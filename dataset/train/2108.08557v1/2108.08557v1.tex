\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cuted}
\usepackage{enumitem}\newcommand{\nicG}[1]{\textbf{\textcolor{red}{#1}}}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{bm}
\usepackage{authblk}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{8622} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders}



\author[1]{Nicola Garau}
\author[1]{Niccolò Bisagno}
\author[1]{Piotr Bródka}
\author[1]{Nicola Conci}
\affil[ ]{\textit {\{nicola.garau, niccolo.bisagno\}@unitn.it}, piotrbrodka95@gmail.com, nicola.conci@unitn.it}
\affil[1]{University of Trento, Via Sommarive, 9, 38123 Povo, Trento TN}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\vspace*{-50pt}
\begin{strip}
    \centering
    \includegraphics[width=0.9\textwidth]{Teaser.pdf}
    \vspace*{-10pt}
    \captionof{figure}{\textbf{[Better seen in color].} Overview of the proposed solution. Two different views of the same subject are shown for each image: (a) 2D/3D Human Pose Estimation (HPE) and (b) 3D Human Mesh Recovery (HMR) methods achieve good accuracy on the front-view  (second row). Changing the viewpoint turns into performance degradation (first row). Our method (c) promotes viewpoint equivariance, showing good results in both the RGB and depth domains.}
    \label{fig:teaser}
\end{strip}

\begin{abstract}
Human Pose Estimation (HPE) aims at retrieving the 3D position of human joints from images or videos. We show that current 3D HPE methods suffer a lack of viewpoint equivariance, namely they tend to fail or perform poorly when dealing with viewpoints unseen at training time. Deep learning methods often rely on either scale-invariant, translation-invariant, or rotation-invariant operations, such as max-pooling. However, the adoption of such procedures does not necessarily improve viewpoint generalization, rather leading to more data-dependent methods.
To tackle this issue, we propose a novel capsule autoencoder network with fast Variational Bayes capsule routing, named DECA.
By modeling each joint as a capsule entity, combined with the routing algorithm, our approach can preserve the joints' hierarchical and geometrical structure in the feature space, independently from the viewpoint. 
By achieving viewpoint equivariance, we drastically reduce the network data dependency at training time, resulting in an improved ability to generalize for unseen viewpoints.
In the experimental validation, we outperform other methods on depth images from both seen and unseen viewpoints, both top-view, and front-view. In the RGB domain, the same network gives state-of-the-art results on the challenging viewpoint transfer task, also establishing a new framework for top-view HPE.
The code can be found at \url{https://github.com/mmlab-cv/DECA}.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Human pose estimation is key for many applications, such as action recognition, animation, gaming, to name a few \cite{kalfaoglu2020late, starke2019neural, shotton2011real}. 
State of the art methods \cite{cao2017realtime,tome2017lifting} that rely on RGB images can correctly localize human joints (e.g. torso, elbows, knees) in images, also in presence of occlusions. However, they tend to fail when dealing with challenging scenarios. The top-view perspective, in particular, turns out to be a difficult task; on the one hand, it causes the largest amount of joints occlusions, and on the other hand, it suffers the scarcity of suitable training data, as shown in Fig. \ref{fig:teaser}. 

When presented with unseen viewpoints, humans display a remarkable ability to estimate human poses, even in the presence of occlusions and unconventional joints configurations. This is not always true in computer vision. In fact, available methods are trained in relatively constrained settings \cite{Joo_2017_TPAMI}, with a limited variability between different viewpoints. Limited data, especially from the top-viewpoint, along with limited capabilities of modeling the hierarchical and geometrical structure of the human pose, results in poor generalization capabilities.

This generalization problem, known as the \textit{viewpoint problem}, depends on how the network activations vary with the change of the viewpoint, usually after a transformation (translation, scaling, rotation, shearing). Convolutional Neural Networks (CNNs) scalar activations are not suitable to effectively manage these viewpoint transformations, thus needing to rely on max-pooling and aggressive data augmentation \cite{cohen2018spherical, haque2016towards,moon2018v2v,xiong2019a2j}. By doing so, CNNs aim at achieving \textit{viewpoint invariance}, defined as 


According to this formulation, applying a viewpoint transformation \textit{T} on the input image , \textit{does not change} the outcome of the network activations.

However, a more desirable property would be to capture and retain the transformation \textit{T} applied to the input image , thus obtaining a network that is aware of the different transformations applied to the input. Being able to model network activations that change in a structured way according to the input viewpoint transformations is also called \textit{viewpoint equivariance} and it is defined as:



This is achieved by introducing \textit{capsules}: groups of neurons that explicitly encode the intrinsic viewpoint-invariant relationship existing between different parts of the same object. 
Capsule networks (CapsNets) can learn part-whole relationships between so-called \textit{entities} across different viewpoints \cite{hinton2011transforming, sabour2017dynamic, hinton2018matrix}, similarly to how our visual cortex system operates, according to the recognition-by-components theory \cite{biederman1987recognition}.
Unlike traditional CNNs, which usually retain viewpoint invariance, capsule networks can explicitly model and jointly preserve a viewpoint transformation \textit{T} through the network activations, achieving \textit{viewpoint equivariance} (Eq. \ref{eq:equivariance}). 

Developing viewpoint-equivariant methods for 3D HPE networks leads to multiple advantages: (i) the learned model is more robust, interpretable, and suitable for real-world applications, (ii) the viewpoint is treated as a learnable parameter, allowing to disentangle the 3D data of the skeleton from each specific view, (iii) the same annotated data can be used to train a network for different viewpoints, thus less training data is required.

In this work, we address the problem of viewpoint-equivariant human pose estimation from single depth or RGB images. Our contribution is summarised as follows:
\begin{itemize}
    \item We present \textbf{a novel Deep viewpoint-Equivariant Capsule Autoencoder architecture (DECA)} which jointly addresses multiple tasks, such as 3D and 2D human pose estimation.

    \item We show how our network works with limited training data, no data augmentation, and across different input domains (RGB and depth images).

    \item We show how the feature space organization, defined by routing the input information to build capsule entities, improves when the tasks are jointly addressed.

    \item We evaluate our method on the ITOP \cite{haque2016towards} dataset for the depth domain and on the PanopTOP31K \cite{garau2021panoptop} dataset for the RGB domain. We establish a new baseline for the viewpoint transfer task and in the RGB domain.
\end{itemize}







\section{Related work}
\label{sec:related}

In recent years, human pose estimation has been a subject of multiple studies, particularly for real-time 2D HPE \cite{cao2017realtime}, 3D HPE \cite{tome2017lifting} and human mesh recovery (HMR) approaches \cite{kolotouros2019learning, kocabas2019vibe}.
In this work, we focus on HPE from single views, using either RGB \cite{cao2017realtime,he2017mask} or depth images \cite{haque2016towards,moon2018v2v,xiong2019a2j}.

\textit{\textbf{Viewpoint-invariant HPE from RGB images.}} 3D HPE usually leverages on additional cues, such as 2D predictions \cite{tome2017lifting, wang20203d, tekin2017learning}, multiple images \cite{zhou2016sparseness}, pre-trained models \cite{katircioglu2018learning} and pose dictionaries \cite{sanzari2016bayesian}. Other recent works aim at end-to-end, learning-based 3D HPE \cite{rogez2019lcr, tian2019densely, liu2020feature}.
 In the RGB domain, common HPE datasets such as Human3.6M \cite{ionescu2014human}, provide images from multiple views, like front-view or side-view, while the top-view component is generally missing. 
It is then evident that the lack of suitable multi-view (top-view in particular) data implies that 
state-of-the-art methods \cite{cao2017realtime,tome2017lifting, kolotouros2019learning,kocabas2019vibe} necessarily perform poorly when presented with an unseen viewpoint at test time, as shown in Fig. \ref{fig:teaser}(a).

\textit{\textbf{Viewpoint-invariant HPE from depth images.}}
Viewpoint invariant HPE methods have been developed using depth images \cite{haque2016towards,moon2018v2v,xiong2019a2j} from top-view and side-view, using datasets like the K2HPD Body Pose Dataset \cite{wang2016human} and the ITOP dataset \cite{haque2016towards}.
To take advantage of the 3D information encoded in 2D depth images, one recent research trend is to resort to 3D deep learning. The paid efforts can be generally categorized into 3D CNN-based and point-set-based families.
To enhance the 3D proprieties of depth data and compute more significant features, current methods rely on 3D CNNs \cite{haque2016towards,moon2018v2v} or 2D CNNs with dense features \cite{xiong2019a2j}.

3D CNN-based methods \cite{haque2016towards,moon2018v2v} perform a voxelization operation on pixels to transform them into 3D objects. To process the 3D data, each network performs costly 3D convolutions on the input data. These operations are responsible for the high computational burden and the difficulty to properly tune a high number of parameters in 3D CNNs.
In the domain of 2D CNNs, Xiong \etal\cite{xiong2019a2j} capture the 3D structure by computing dense features in an ensemble way, thus avoiding computationally intensive CNN layers, but they still rely on a backbone pre-trained network to extract 2D features.
Still, the above-mentioned approaches usually achieve weak viewpoint-invariance but fail to model viewpoint-equivariance. 
Moreover, we argue that the 3D geometry of the data should be interpreted by the network without relying on the voxelization embedding, or a 2D pre-trained feature extraction network. 


\textit{\textbf{Capsule networks for HPE.}} Capsule networks have shown the ability to model the geometric nature of training data thanks to the network structure and features \cite{sabour2017dynamic, hinton2018matrix, kosiorek2019stacked}.
Sabour \etal., introduce a routing algorithm for vector capsules, called \textit{routing-by-agreement} as a better max-pooling substitute.
Hinton \etal \cite{hinton2018matrix} further improve accuracy through a more complex matrix capsule structure and an Expectation-Maximization routing (\textit{EM-routing}) for capsules. Unfortunately, the EM-routing and the  pose matrix embedded in the capsule contribute to increasing the training time, when compared to both CNNs and vector CapsNets. 
Kosiorek \etal \cite{kosiorek2019stacked} introduce for the first time an unsupervised capsule-based autoencoder. Ribeiro \etal in \cite{ribeiro2020capsule} build upon the EM-routing version of capsule by proposing for the first time a Variational Bayes capsule routing (VB routing) fitting a mixture of transforming Gaussians. They present state-of-the-art results using  fewer capsules, achieving both performance gain and network complexity reduction. However, all the mentioned works only consider small datasets, such as MNIST, smallNORB, and CIFAR-10 for benchmarking. 

In the RGB domain, Ramírez \cite{ramirez2020bayesian} tackles the problem of RGB HPE using dynamic vector capsule networks \cite{sabour2017dynamic} to solve the 3D HPE problem in an end-to-end fashion. However, their work only exploits lateral viewpoints from the Human3.6M dataset and only considering RGB data.

In this work, we use matrix capsules \cite{hinton2018matrix}, along with a different capsule routing algorithm and a new encoding-decoding pipeline with GELU activations. We argue that matrix capsules are better suited than vector capsules for the 3D HPE task, as the  pose matrix used for the routing can capture 3D geometry better than a dynamic vector structure.

\section{Method}
\label{sec:method}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Network.pdf}
    \captionof{figure}{\textbf{[Better seen in color].} Overview of the proposed architecture. In light blue, the encoding module (Input, CNN encoder, Capsule layers), in green the interpretable feature space with capsule entities, in light orange the decoding module (fully connected decoders with multiple tasks and self-balancing loss).}
    \label{fig:network}
\end{figure*}

We now analyze the proposed autoencoder, DECA, starting with the capsule encoder and the multi-task decoders.
DECA can be trained end-to-end, without any pre-training or data augmentation, and it works in real-time in the inference phase. An overview of the proposed architecture is shown in Fig. \ref{fig:network}.

\subsection{Capsule encoder}
\label{subsec:encoder}
The encoding module of the network (light blue in Fig. \ref{fig:network}) is divided in: (i) an input pre-processor , (ii) a CNN encoder  and (iii) four layers of Matrix Capsules with Variational Bayes Routing \cite{ribeiro2020capsule}.

 (i)  is a  layer which normalizes the different type of data (RGB images, depth images, top-view, side-view, free-view) in the interval . 


(ii) The normalised input is then forwarded to a CNN encoder , built using four convolutional layers with inputs , instance normalisation and GELU activations \cite{hendrycks2016gaussian}, as shown in Eq. \ref{eq:GELU}.  is the number of channels, which may vary depending on the input.



(iii) The output of the CNN encoder  feeds our capsule layers.
 It has been shown in previous works \cite{sabour2017dynamic, hinton2018matrix, kosiorek2019stacked} that capsules provide a superior understanding of the viewpoint and the relationship between parts and parent objects, thus aiming at true viewpoint equivariance. Given the multiple degrees of freedom of each joint, we adopt the matrix capsules model \cite{hinton2018matrix} instead of vector capsules \cite{sabour2017dynamic}, enriching the description of single joints as hierarchically linked capsule entities. We deploy the novel capsule routing based on Variational Bayes (VB) \cite{ribeiro2020capsule}, which is proven to speed up the training of our matrix capsules layers, at the same time improving performances. The last iteration of the VB routing is also called \textit{ClassRouting} and it is used to route the highest-level information to the last layer of capsules before the feature space .


In our CapsNet, we employ four layers: a \textit{primary capsules} layer encapsulates the output features of  into -dimensional capsules, two \textit{convolutional capsules} layers refine the capsule features, and a final \textit{class capsules} layer encodes the output into a -dimensional features in the latent space , where  is the number of joints, also called .

Given each lower-level capsule  and the corresponding higher-level capsule , we define  as the proposed lower level pose matrix and  as a trainable viewpoint-equivariant transformation matrix such that:
    
where  is the vote coming from lower capsules  for higher capsules . The voting procedure takes place inside the VB routing and it allows each lower capsule  to route its information to a higher capsule  of its choice, thus allowing to build the hierarchical structure typical of CapsNets. 

To promote the viewpoint equivariance in Eq. \ref{eq:equivariance}, we introduce an inverse matrix  in the \textit{class capsules}, which aims at satisfying the Inverse Graphics constraint:

    

 meaning that the learned inverse matrix  effectively acts as an approximated inverse of the rendering operation, as it is commonly found in computer graphics \cite{hinton2011transforming}.
 
At the output of the encoder, each \textit{entity} corresponding to each joint of the skeleton is defined by a flattened vector of  elements, or, in other words, a  matrix, which is sufficient to grasp the complete pose (translation + rotation) of each joint. 

An overview of the capsule encoder is shown in Algorithm \ref{algo:encoder}. In the algorithm,  are weights used for the self-balancing of the loss,  are the convolutional layer weights,  are the activations of each Capsule layer, and  represents parameters used only when in the RGB domain. 

\begin{algorithm}
  \caption{Capsule encoder}
  \SetKwInOut{Input}{inputs}
  \SetKwInOut{Output}{outputs}
  \SetKwProg{CapsuleEncoder}{CapsuleEncoder}{}{}

  \CapsuleEncoder{}{
    \Input{,  batch size of RGB or depth images}
    \Output{
     16-dimensional ;  trainable Inverse Graphics matrix
    }
    \;
    \;
    \ForEach{}{\;
      \;
      \;
    }
    \;
    \ForEach{}{\;
      \;
    }
    \;
    \;
    \;
    
    \KwRet{}\;
  }
 \label{algo:encoder}
\end{algorithm}


\subsection{Multi-task decoders}
\label{subsec:decoders}

Starting from the 16-dimensional entities in the capsule feature space , we design a decoding module (light orange block in Fig. \ref{fig:network}) that allows us to simultaneously retrieve multiple predictions for different tasks from the same feature space .
Each decoder  in the decoding module is configured as an independent fully connected block, with  Dropout and GELU activations \cite{hendrycks2016gaussian}. We employ no weight sharing or layer sharing across the decoders to enforce the multi-task loss, as explained in section \ref{subsec:loss}. 

We define different tasks () with different objectives:

\begin{itemize}
    \item : minimise the distance between ground truth and predicted 3D joints in 3D space ;
    \item : as above, but without relying on 3D joints predictions, and rather predicting 2D joints  as seen from the current viewpoint in camera frame coordinates;
    \item : reconstruct the depth map  of the input RGB image. It is used only in the RGB domain;
    \item  Inverse Graphics loss : learn the inverse graphics matrix  to promote the \textit{de-rendering} of input pixels into isolated capsule entities, as explained in Sec. \ref{subsec:encoder}, Eq. \ref{eq:inverse_graphics}.
\end{itemize}


For each task , a decoder  takes as input the feature space  and it outputs the predictions  to the loss function. For , the  matrix is forwarded to the loss function directly from the encoder.

An overview of the capsule decoders is shown in Algorithm \ref{algo:decoders}. 

\begin{algorithm}
  \caption{Capsule decoders}
  \SetKwInOut{Input}{inputs}
  \SetKwInOut{Output}{outputs}
  \SetKwProg{CapsuleDecoders}{CapsuleDecoders}{}{}

  \CapsuleDecoders{}{
    \Input{ 16-dimensional }
    \Output{
    
    }
    ;\\
    \ForEach{}{\;
      \;
      \;
    }
    
    \KwRet{}\;
  }
 \label{algo:decoders}
\end{algorithm}

\subsection{Self-balancing multi-task loss}
\label{subsec:loss}

Tasks are associated to the different input domains, as follows: 


\begin{table}[!htbp]
\centering
\resizebox{0.2\textwidth}{!}{\begin{tabular}{@{}lllll@{}}
\toprule
 &  &  &  &  \\ \midrule
Depth & \checkmark   & \checkmark   &                           & \checkmark  \\
RGB   & \checkmark   & \checkmark   & \checkmark & \checkmark  \\ \bottomrule
\end{tabular}}
\label{tab:tasks}
\end{table}

Each task is assigned a loss , defined as:


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\begin{itemize}
    \item , : Mean Square Error (MSE) loss for the  and  joints prediction tasks.
    \item : masked L1 loss for the depth estimation task , in the RGB domain, where  is a function that applies the L1 loss only on pixels over a certain depth threshold, to promote the depth estimation over non-background areas.
    \item : inverse graphics loss , which role is to enforce invertibility for the capsule weight matrices. The notation  defines the Frobenius norm of a matrix.
\end{itemize}





Considering  as the set of the employed tasks , the overall balanced loss for all the tasks is expressed as:





where  are the trainable weights associated with each loss in , initialised to 1 in algorithm \ref{algo:encoder}, and  is each loss of the enabled decoders, as defined in Eq. \ref{eq:losses}. 


\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}

\textbf{ITOP Dataset of depth images.} The ITOP dataset \cite{haque2016towards} contains depth images from top and front view. The training split and the test split consist of 40k and 10k images, respectively. The depth images display 15 videos of 20 actors in a constrained setting. The dataset is recorded using two Axus Xtion Pro cameras. The 3D skeleton model consists of 15 joints.

\textbf{PanopTOP31K dataset of depth and RGB images.} The PanopTOP31K dataset \cite{garau2021panoptop} consists of 34k top-view and 34k front view images coming from video sequences of 24 different actors, available both in the RGB and depth domain, for a total of 68k images. The ground truth 3D skeleton consists of 19 joints.

\subsection{Evaluation metrics}

Following the works of \cite{haque2016towards,moon2018v2v,xiong2019a2j}, we choose the mean average precision (mAP) as the evaluation metric for the depth domain. It is defined as the percentage of all predicted joints which fall in an interval smaller than 0.10 meters. In the RGB domain, we use the Mean Per Joint Position Error (MPJPE) in millimeters as in many HPE works \cite{cao2017realtime,tome2017lifting,ramirez2020bayesian}.

\begin{figure*}[]
        \begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{V2V.png}
                \caption{V2V \cite{moon2018v2v}}
                \label{fig:latentV2V}
        \end{subfigure}\begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{1_2D.png}
                \caption{DECA-D1, }
                \label{fig:latent3D}
        \end{subfigure}\begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{2_2D.png}
                \caption{DECA-D2, }
                \label{fig:latent3D+W}
        \end{subfigure}\begin{subfigure}[b]{0.25\textwidth}
                \includegraphics[width=\linewidth]{3_2D.png}
                \caption{DECA-D3, }
                \label{fig:latent3D+2D+W}
        \end{subfigure}\caption{2D representation on the 16-dimensional latent space obtained using t-SNE \cite{van2008visualizing}. Each dot corresponds to an entity  representing a joint  of the skeleton from the test set of ITOP \cite{haque2016towards}. V2V network \cite{moon2018v2v} relies on CNNs, thus is not able to cluster together samples corresponding to the same entity (a). When trained to satisfy only the 3D prediction constraint our DECA-D1 network performs slightly better than V2V (b). The 15 clusters, corresponding to the 15 joints of the skeleton model, are clearly distinguishable in DECA-D2 (c) and DECA-D3 (d), with (d) displaying better cluster separation and fewer outliers.}\label{fig:latent2D}
\end{figure*}

\subsection{Implementation details}

Our network is trained in an end-to-end fashion using Pytorch Lightning. Input images are normalized in the interval  with a resolution of 256x256 pixels for depth images and 256x256 pixels for RGB ones. We do not perform any augmentations on the input datasets. The batch size is set to 128 for ITOP and 128 for PanopTOP31K. We initialize the weights with the Xavier initialization \cite{glorot2010understanding}. The learning rate is set to , the weight decay is set to 0, and Adam is the optimizer of choice. We train our network for 20 epochs on the ITOP dataset and 15 epochs on PanopTOP31K.



\subsection{Feature space entities and ablation study}
\label{sec:ablation}

We report experiments on the top-view of the ITOP dataset \cite{haque2016towards} to validate the 3D representation provided by our network and to show how the multi-tasks decoder influences the overall performances. 

To do so, we deploy 4 configurations, 3 on depth data and 1 on RGB data, with different sets of tasks  of our method:
\begin{itemize}
    \item DECA-D1, with 
    \item DECA-D2, with 
    \item DECA-D3, with 
    \item DECA-R4, with 
\end{itemize}



where the letter  or  indicates the depth or RGB domains, and the number defines how many tasks are assigned to the network.
Since we are evaluating the performances on the 3D HPE, the  is used for all the different configurations.

\textbf{Loss effectiveness analysis}. 
The results are reported in the last 3 columns of Table \ref{tab:results_itop}. As shown in the Table, increasing the number of tasks in  generally leads to an increase in the network's performances. DECA-D1 already achieves similar results to the state-of-the-art, thanks to the CapsNets' capability to interpret the geometrical nature of the input data. When the inverse graphics loss  is employed (DECA-D2 and DECA-D3), the enforced invertibility of the weights matrix leads to an immediate gain in performances. 
In DECA-D3, the introduction of the  loss leads to an additional improvement in terms of accuracy. Hence, we argue that the network performances improve when more tasks are given because we achieve a better representation of the entities in the latent space.



\textbf{Latent space analysis}. 
To analyze the latent space, we use the features of the test set extracted after the capsule modules. Each feature  is linearised to obtain a vector of length .  At this stage, each entity  corresponding to each joint  is defined by dividing each feature vector by the number of joints, resulting in vectors of length . For visualisation purposes, we use \textit{t-SNE} \cite{van2008visualizing} to project the entities on a 2-dimensional space. The results are displayed in Fig. \ref{fig:latent2D}. We compare our latent space against the publicly available version of the V2V \cite{moon2018v2v} encoder/decoder structure. We show how our DECA network can better cluster and separate each entity  with respect to V2V. Our solution provides a better organization of the latent space, with bigger inter-class margins and fewer outliers. The latent space organization improves drastically when we employ the  task (DECA-D2), thus enforcing the inverse graphics constraint. In DECA-D3 we add the  task. The resulting organization of the latent space improves, thus further establishing a correlation between the growing number of tasks and the improvement in performances.




\begin{table*}[]
\resizebox{\textwidth}{!}{\begin{tabular}{@{}lcccccccc|cccccccccc@{}}
\toprule
           & \multicolumn{8}{c}{\textbf{ITOP front-view}}                                                               & \multicolumn{10}{c|}{\textbf{ITOP top-view}}                              
           \\ \midrule
Body part  & RF\cite{shotton2011real}    & RTW\cite{yub2015random}            & IEF\cite{carreira2016human}   & VI \cite{haque2016towards}            & REN9x6x6\cite{guo2017towards}       & V2V\cite{moon2018v2v}            & A2J\cite{xiong2019a2j}   & DECA-D3        & RF\cite{shotton2011real}    & RTW\cite{yub2015random}            & IEF\cite{carreira2016human}   & VI \cite{haque2016towards}            & REN9x6x6\cite{guo2017towards}       & V2V\cite{moon2018v2v}            & A2J\cite{xiong2019a2j}    & DECA-D1 & DECA-D2        & DECA-D3        \\ \midrule
Head       & 63.80 & 97.80          & 96.20 & 98.10          & \textbf{98.70} & 98.29          & 98.54 & 93.87          & 95.40 & \textbf{98.40} & 83.80 & 98.10          & 98.20    & \textbf{98.40} & 98.38 & 94.41   & 95.31          & 95.37          \\
Neck       & 86.40 & 95.80          & 85.20 & 97.50          & \textbf{99.40} & 99.07          & 99.20 & 97.90          & 98.50 & 82.20          & 50.00 & 97.60          & 98.90    & 98.91          & 98.91 & 98.86   & \textbf{99.16} & 98.68          \\
Shoulders  & 83.30 & 94.10          & 77.20 & 96.50          & 96.10          & \textbf{97.18} & 96.23 & 95.22          & 89.00 & 91.80          & 67.30 & 96.10          & 96.60    & 96.87          & 96.26 & 96.12   & \textbf{97.51} & 96.57          \\
Elbows     & 73.20 & 77.90          & 45.40 & 73.30          & 74.70          & 80.42          & 78.92 & \textbf{84.53} & 57.40 & 80.10          & 40.20 & \textbf{86.20} & 74.40    & 79.16          & 75.88 & 76.86   & 81.67          & 84.07          \\
Hands      & 51.30 & \textbf{70.50} & 30.90 & 68.70          & 55.20          & 67.26          & 68.35 & 56.49          & 49.10 & 76.90          & 39.00 & \textbf{85.50} & 50.70    & 62.44          & 59.35 & 44.41   & 45.97          & 54.33          \\
Torso      & 65.00 & 93.80          & 84.70 & 85.60          & 98.70          & 98.73          & 98.52 & \textbf{99.04} & 80.50 & 68.20          & 30.50 & 72.90          & 98.10    & 97.78          & 97.82 & 99.46   & \textbf{99.70} & 99.46          \\
Hip        & 50.80 & 80.30          & 83.50 & 72.00          & 91.80          & 93.23          & 90.85 & \textbf{97.42} & 20.00 & 55.70          & 38.90 & 61.20          & 85.50    & 86.91          & 86.88 & 97.84   & \textbf{97.87} & 97.42          \\
Knees      & 65.70 & 68.80          & 81.80 & 69.00          & 89.00          & 91.80          & 90.75 & \textbf{94.56} & 2.60  & 53.90          & 54.00 & 51.60          & 70.00    & 83.28          & 79.66 & 88.01   & 88.19          & \textbf{90.84} \\
Feet       & 61.30 & 68.40          & 80.90 & 60.80          & 81.10          & 87.60          & 86.91 & \textbf{92.04} & 0.00  & 28.70          & 62.40 & 51.50          & 41.60    & 69.62          & 58.34 & 79.30   & \textbf{83.53} & 81.88          \\
Upper Body & -     & -              & -     & \textbf{84.00} & -              & -              & -     & 83.03          & -     & -              & -     & \textbf{91.40} & -        & -              & -     & 78.51   & 80.60          & 83.00          \\
Lower Body & -     & -              & -     & 67.30          & -              & -              & -     & \textbf{95.30} & -     & -              & -     & 54.70          & -        & -              & -     & 89.96   & 91.27          & \textbf{91.39} \\
Mean       & 65.80 & 80.50          & 71.00 & 77.40          & 84.90          & 88.74          & 88.00 & \textbf{88.75} & 47.40 & 68.20          & 51.20 & 75.50          & 75.50    & 83.44          & 80.5  & 83.85   & 85.58          & \textbf{86.92} \\ \bottomrule
\end{tabular}}
\caption{Comparison with the state-of the art for ITOP front-view and top-view (metric: 0.1m mAP).}
\label{tab:results_itop}
\end{table*}

\subsection{Comparison with state-of-the-art methods}

\textbf{Depth data: ITOP dataset.} We compare our DECA against common state-of-the-art method for human pose estimation on depth images \cite{shotton2011real, yub2015random, carreira2016human, haque2016towards, guo2017towards, moon2018v2v, xiong2019a2j}. The results are reported in Tab. \ref{tab:results_itop}. 
Our DECA outperforms existing methods on the front-view task, improving the accuracy by a wide margin on the more challenging top viewpoint. In general, we consistently perform better than other methods on most of the joints and the average. The gain of our method is particularly large when dealing with the lower body, which is often occluded in the top-view.


\textbf{Depth data: Viewpoint-equivariant ITOP.} We test DECA on the viewpoint transfer task, meaning training on one viewpoint, either top-view or front-view, and testing on the other one, unseen at training time. The comparison against available state-of-the-art methods \cite{shotton2011real, yub2015random, carreira2016human, haque2016towards} are reported in Tab. \ref{tab:viewpoint_transfer_ITOP}. We consistently outperform other methods by a wide margin, thus making a step forward toward viewpoint equivariance. 
While other methods provide only the best subset of viewpoint transfer results (Tab. \ref{tab:viewpoint_transfer_ITOP}), omitting entirely the train on top and test on front scenario, we provide results for all the joints and all the viewpoint transfer combinations in Tab. \ref{tab:viewpoint_transfer_ITOP_both}. Our DECA achieves better results than the top-most of the other methods on many different joints (e.g. shoulders, lower body). In Tab \ref{tab:viewpoint_transfer_ITOP_both}, training DECA on top-view or front-view achieves comparable lower body accuracy. This means that when the network is trained on top view, where the lower body is mostly occluded, it can retrieve the occluded joints from previously unseen front views, and vice versa. This shows how our network has learned the viewpoint as a parameter, and it is thus able to generalize in a similar fashion in all the viewpoint transfer combinations. 

\begin{table}[]
\centering
\resizebox{.5\textwidth}{!}{\begin{tabular}{@{}lccccc@{}}
\toprule
           & \multicolumn{5}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}ITOP\\Train on front, test on top\end{tabular}}} \\ \midrule
Body part  & RF \cite{shotton2011real}                    & RTW \cite{yub2015random}                  & IEF \cite{carreira2016human}                   & VI \cite{haque2016towards}                    & DECA-D3                       \\ \midrule
Head       & 48.10                 & 1.50                 & 47.90                 & 55.60                 & 46.27                          \\
Neck       & 5.90                  & 8.10                 & 39.00                 & 40.90                 & \textbf{73.14}                 \\
Torso      & 4.70                  & 3.90                 & 41.90                 & 35.00                 & \textbf{85.94}                 \\
Upper Body & 19.70                 & 2.20                 & 23.90                 & 29.40                 & \textbf{45.00}                 \\
Full Body  & 10.80                 & 2.00                 & 17.40                 & 20.40                 & \textbf{51.85}                 \\ \bottomrule
\end{tabular}}
\caption{Comparison with the state-of the art for the ITOP viewpoint transfer task (metric: 0.1m mAP). Training on front-view, validating on front-view, testing on top-view (top-view data is unseen in validation).}
\label{tab:viewpoint_transfer_ITOP}
\end{table}


\begin{table}[]
\centering
\resizebox{.3\textwidth}{!}{\begin{tabular}{@{}lcc@{}}
\toprule
           & \multicolumn{2}{c}{\textbf{DECA-D3}}                                                                                                                                 \\ \midrule
Body part  & \begin{tabular}[c]{@{}c@{}}Train on front,\\ test on top\end{tabular} & \begin{tabular}[c]{@{}c@{}}Train on top,\\ test on front\end{tabular} \\ \midrule
Head       & 46.27                                                                           & 18.51                                                                              \\
Neck       & 73.14                                                                           & 44.77                                                                              \\
Shoulders  & 69.02                                                                           & 25.18                                                                              \\
Elbows     & 43.87                                                                           & 16.23                                                                              \\
Hands      & 9.41                                                                            & 2.19                                                                               \\
Torso      & 85.94                                                                           & 68.63                                                                              \\
Hip        & 72.15                                                                           & 64.75                                                                              \\
Knees      & 49.31                                                                           & 68.15                                                                              \\
Feet       & 42.46                                                                           & 46.12                                                                              \\
Upper Body & 45.00                                                                           & 18.81                                                                              \\
Lower Body & 59.11                                                                           & 60.95                                                                              \\
Mean       & 51.85                                                                           & 38.48                                                                              \\ \bottomrule
\end{tabular}}
\caption{DECA-D3 complete results for the ITOP viewpoint transfer tasks (metric: 0.1m mAP). Test data is unseen during validation for both the cases.}
\label{tab:viewpoint_transfer_ITOP_both}
\end{table}


\textbf{RGB data: Viewpoint-equivariant PanopTop31K.} 
To the best of our knowledge, we are the first to tackle the problem of viewpoint transfer between top-view and front-view in the RGB domain. We report results with training and testing on both seen and unseen viewpoints in Tab. \ref{tab:results_rgb}. The chosen metric is the mean per-joint projection error (MPJPE). We report results with and without the Procrustes alignment \cite{goodall1991procrustes} of the predicted poses. It is interesting to notice how DECA can reduce the gap between the same viewpoint results and the results of the viewpoint transfer tasks. In the case of viewpoint transfer, we train on viewpoint A, validate on the same viewpoint A and test on viewpoint B.


\begin{table*}[]
\centering
\resizebox{.89\textwidth}{!}{\begin{tabular}{@{}lcccccccc@{}}
\toprule
            & \multicolumn{8}{c}{\textbf{DECA-R4}}                                                                                                                                                                                                                                                                                                                                                                              \\ \midrule
            & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Train on front,\\ test on front\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Train on top,\\ test on top\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Train on front,\\ test on top\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Train on top,\\ test on front\end{tabular}}} \\ \midrule
Body part   & No Procrustes                                      & Procrustes                                      & No Procrustes                                    & Procrustes                                    & No Procrustes                                     & Procrustes                                     & No Procrustes                                     & Procrustes                                     \\ \midrule
Neck        & 4.02                                               & 2.37                                            & 4.55                                             & 2.51                                          & 16.02                                             & 4.16                                           & 8.21                                              & 5.06                                           \\
Nose        & 5.66                                               & 3.75                                            & 6.98                                             & 3.89                                          & 16.83                                             & 7.67                                           & 10.72                                             & 6.76                                           \\
Body Center & 0.56                                               & 4.63                                            & 1.23                                             & 3.63                                          & 1.01                                              & 31.20                                          & 0.83                                              & 11.59                                          \\
Shoulders   & 4.56                                               & 2.76                                            & 5.14                                             & 3.07                                          & 17.43                                             & 5.33                                           & 8.51                                              & 5.35                                           \\
Elbows      & 9.82                                               & 7.14                                            & 9.64                                             & 7.51                                          & 29.70                                             & 18.52                                          & 23.20                                             & 15.47                                          \\
Hands       & 13.88                                              & 10.82                                           & 14.02                                            & 12.34                                         & 47.01                                             & 38.29                                          & 36.78                                             & 28.25                                          \\
Hips        & 18.75                                              & 4.87                                            & 2.71                                             & 3.89                                          & 5.10                                              & 30.07                                          & 3.64                                              & 10.88                                          \\
Knees       & 9.54                                               & 5.14                                            & 7.59                                             & 4.84                                          & 52.98                                             & 28.65                                          & 20.11                                             & 9.28                                           \\
Feet        & 11.53                                              & 5.08                                            & 9.83                                             & 5.10                                          & 69.18                                             & 28.75                                          & 26.36                                             & 11.07                                          \\
Eyes        & 6.19                                               & 4.00                                            & 7.44                                             & 3.79                                          & 19.33                                             & 11.00                                          & 11.40                                             & 7.45                                           \\
Ears        & 5.50                                               & 3.73                                            & 7.15                                             & 3.74                                          & 23.56                                             & 13.00                                          & 11.22                                             & 7.16                                           \\
Upper Body  & 6.93                                               & 5.21                                            & 7.66                                             & 5.46                                          & 23.69                                             & 16.56                                          & 15.54                                             & 11.60                                          \\
Lower Body  & 7.65                                               & 5.03                                            & 6.71                                             & 4.61                                          & 42.42                                             & 29.16                                          & 16.71                                             & 10.41                                          \\
Mean        & 7.16                                               & 5.15                                            & 7.36                                             & 5.19                                          & 29.60                                             & 20.54                                          & 15.91                                             & 11.22                                          \\ \bottomrule
\end{tabular}}
\caption{DECA-R4 results on the PanopTOP31K RGB dataset, with and without the Procrustes transformation  \cite{goodall1991procrustes} (metric: MPJPE). Tasks: (i) 3D pose estimation from the front and top viewpoints (ii) viewpoint transfer for both front and top views. Test data is unseen during validation for both the viewpoint transfer tasks.}
\label{tab:results_rgb}
\end{table*}

\begin{figure*}[!ht]
\centering
\begin{minipage}[c]{.1\textwidth}
  \vspace*{\fill}
  \centering
  \includegraphics[height=1cm]{inputRT.png}
  \subcaption{}
  \label{fig:qual_irt}\par\vfill
  \includegraphics[height=3cm]{inputRF.png}
  \subcaption{}
  \label{fig:qual_irf}
\end{minipage}\begin{minipage}[c]{.4\textwidth}
  \vspace*{\fill}
  \centering
  \includegraphics[height=4cm]{gt_depth.png}
  \subcaption{GT}
  \label{fig:qual_gt}
\end{minipage}\begin{minipage}[c]{.2\textwidth}
  \vspace*{\fill}
  \centering
  \includegraphics[height=2cm]{pred_depth_top.png}
  \subcaption{\{T\};\{T\}}
  \label{fig:qual_pt}\par\vfill
  \includegraphics[height=2cm]{pred_depth_front.png}
  \subcaption{\{F\};\{F\}}
  \label{fig:qual_pf}
\end{minipage}\begin{minipage}[c]{.2\textwidth}
  \vspace*{\fill}
  \centering
  \includegraphics[height=2cm]{pred_depth_top_trans.png}
  \subcaption{\{T\};\{F\}}
  \label{fig:qual_ptvt}\par\vfill
  \includegraphics[width=2cm,height=2cm]{pred_depth_front_trans.png}
  \subcaption{\{F\};\{T\}}
  \label{fig:qual_pfvt}
\end{minipage}
\caption{DECA-R4 qualitative results on the PanopTOP31K dataset. On the left (\subref{fig:qual_irt}, 
\subref{fig:qual_irf}) the types of input accepted by DECA (top-view or front-view). DECA can also accept inputs in the depth domain. In the center (\subref{fig:qual_gt}), the corresponding 3D ground truth. On the right, the possible combinations of training/testing experiments. \textbf{T} stands for \textbf{top} and \textbf{F} stands for \textbf{front}. As an example, in (\subref{fig:qual_ptvt}), \textit{\{T\};\{F\}} means that DECA has been trained exclusively on \textbf{top} data and tested on previously unseen (not even at validation time) \textbf{front} data.}
\label{fig:qualitative}
\end{figure*}

\subsection{Qualitative results}
In Fig. \ref{fig:qualitative} we show some qualitative results from DECA-R4 configuration on RGB data. We deploy our network training and testing on all the possible viewpoint combinations. The network takes as input either the top-view RGB (Fig. \ref{fig:qual_irt}) image or the front view (Fig. \ref{fig:qual_irf}) one. When trained and tested on the same viewpoint (Fig. \ref{fig:qual_pt}, \ref{fig:qual_pf}), the network produces similar outputs, thus confirming its ability to deal with the challenging top-view scenario. When training on the top view and testing on the front one (Fig. \ref{fig:qual_ptvt}), the network can accurately retrieve the positions of the lower body joints. DECA can retrieve parts of the body mostly occluded ad training time, thus displaying its generalization capabilities. When training on the front view and testing on the top one (Fig. \ref{fig:qual_pfvt}), the network can retrieve the positions of the upper body joints, which are visible in both images but from different perspectives, proving that DECA can internally model the viewpoint.

\section{Conclusions}
\label{sec:conclusions}

We presented DECA, a deep viewpoint-equivariant method for human pose estimation on single RGB/depth images using capsule autoencoders. We show how CapsNets are better suited to deal with the 3D nature of raw data and how they allow taking a step forward to viewpoint equivariance. We have shown how our method can effectively generalize and achieve state-of-the-art results in both RGB and depth domains, as well as in the viewpoint transfer task. 
In future work, we aim at improving hands pose estimation and employing matrix capsules on bigger RGB datasets.




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
