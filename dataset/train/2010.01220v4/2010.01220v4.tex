\changeS{When testing domain adaption performance, we distinguish two cases: a) the capabilities of the model to address domain shift issues, i.e., the case of reducing the shift between training and test data; and 2) the capabilities of the model to learn generalizable features that can be employed, without any additional tuning.}\\
\\
\noindent {\bf \changeS{Domain-shift.}} \changeS{To assess the performance of our hierarchical domain adaptation approach in tackling the problem of domain shift, we run a set of experiments by selecting different combinations of datasets to be employed as source domain (used in a supervised way during training) and target domain, used in an unsupervised way during training; as test set, an unseen portion from the target domain is used.
The assumption in these experiments is to perform unsupervised learning on the test domain through our hierarchical gradient reversal approach before testing on it (on the appropriate test split not used for unsupervised learning). } 

\changeS{In particular, we compare the performance of our base model in the three scenarios:} 
\begin{itemize}
    \item \changeS{\emph{Domain generalization}, i.e., the model trained supervisedly on the source domain and directly tested on the target domain, with no additional information on the test dataset used during training;
    \item \emph{Domain adaptation}, i.e., the model trained  with unsupervised adaptation on the target domain, enabled through the hierarchy of GRL layers as in our full model in Fig.~\ref{fig:HDS}};
    \item \changeS{\emph{Transfer learning}, i.e., the model (with gradient reversal disabled) trained on the source dataset and then fine-tuned (in a supervised way) on the target dataset. This scenario represents the upper bound of the evaluation and is, of course, out of the scope of pure domain adaptation, since target domain labels are available at training time.}
\end{itemize}

\changeS{Tab.~\ref{tab:unsupervisedDA} shows the results for different combinations of source and target domains. 
Two main patterns of results can be identified, depending on whether DHF1K is employed as source domain or not. In the former case \changeS{(top block of Tab.~\ref{tab:unsupervisedDA})}, it can be noticed that the employment of gradient reversal layers improves performance over all target datasets, compared to simply training on the source dataset.
When instead DHF1K is employed as target domain \changeS{(second and third blocks in Tab.~\ref{tab:unsupervisedDA})}, the use of gradient reversal layers degrades performance. This may be due to the specific characteristics of Hollywood2 and UCF Sports, which were collected in a task-driven experiments while DHF1K in a free-viewing one. Furthermore, the limited variability of spatio-temporal features from videos in Hollywood2, as shown in Fig.~\ref{fig:dataset_overview}, makes harder for the model to move clustered features and to learn more general representations. Similarly, when UCF Sports is used as source domain, the small size of the dataset makes it easier for the model to focus on the supervised saliency prediction task (on which it can easily achieve a low training loss), rather than minimizing the domain adaptation loss.
Overall, as expected, the highest performance are obtained in the transfer learning regime.}\\

\noindent \changeS{ {\bf Learning generalizable features.} We also test the capabilities of the model to learn general features by using, in the domain adaptation stream, a target dataset different from the one used for test. We specifically compute performance when training on DHF1K, adapting the learned features to LEDOV, and testing on never seen datasets (UCF and Hollywood2). Performance are reported in Table~\ref{tab:dhf1k-ledov}, which reports how the performance gain of HDS, when empowered with hierarchical gradient reversal modules, is higher than in the case of domain-shift experiments (see Table~\ref{tab:unsupervisedDA}). This demonstrates that our hierarchical domain adaptation mechanism is better at learning salience features that generalize well on multiple data domains than at addressing the domain-shift for a given dataset. }

\begin{table*}[ht!]
        \centering
        \begin{tabular}{c|lccccc}
            \toprule
            \multicolumn{7}{c}{\textbf{Source dataset: DHF1K}}\\ 
            \midrule
            \textbf{Target dataset} &\textbf{Approach} & \textbf{NSS} & \textbf{CC} & \textbf{SIM} & \textbf{AUC-J} & \textbf{s-AUC} \\
            \midrule
            \multirow{3}{*}{UCF Sports} &Domain Generalization  &  2.483 & 0.537 & 0.442 & 0.890 & 0.744 \\
            &Domain Adaptation  & 2.514 &  0.539    & 0.448 &  0.893 & 0.750 \\
            &Transfer Learning  &\textbf{ 3.001} &  \textbf{0.594}   & \textbf{0.493}    &  \textbf{0.913} & \textbf{0.773} \\
            \midrule
            \multirow{3}{*}{Hollywood2} &Domain Generalization  &  3.063 & 0.625 & 0.505 & 0.925 & 0.779 \\
            &Domain Adaptation  & 3.101 &  0.632   & 0.510 &  0.925 & 0.785 \\
            &Transfer Learning  & \textbf{3.426}    &  \textbf{0.668}    &  \textbf{0.558}    &  \textbf{0.927} & \textbf{0.797} \\
            \midrule
            \multicolumn{7}{c}{} \\
            \toprule
            \multicolumn{7}{c}{\textbf{Source dataset: UCF Sports}}\\ 
            \midrule
            \textbf{Target dataset} &\textbf{Approach} & \textbf{NSS} & \textbf{CC} & \textbf{SIM} & \textbf{AUC-J} & \textbf{s-AUC} \\
            \midrule
            \multirow{3}{*}{DHF1K} &Domain Generalization  &  2.237 & 0.405 & 0.325 & 0.880 & 0.658 \\
            &Domain Adaptation  & 2.160 &  0.392    & 0.309 &  0.877 & 0.658 \\
            &Transfer Learning  & \textbf{2.688}    &  \textbf{0.477}    & \textbf{0.374}   &  \textbf{0.896} & \textbf{0.700} \\
            \midrule
            \multirow{3}{*}{Hollywood2} &Domain Generalization  &  2.469 & 0.517 & 0.433 & 0.899 & 0.727 \\
            &Domain Adaptation  & 2.386 &  0.503   & 0.422 &  0.896 & 0.727 \\
            &Transfer Learning  & \textbf{3.298}    &  \textbf{0.657}    & \textbf{0.533}    &  \textbf{0.925} & \textbf{0.794} \\
            \midrule
            \multicolumn{7}{c}{} \\
            \toprule
            \multicolumn{7}{c}{\textbf{Source dataset: Hollywood2}}\\ 
            \midrule
            \textbf{Target dataset} &\textbf{Approach} & \textbf{NSS} & \textbf{CC} & \textbf{SIM} & \textbf{AUC-J} & \textbf{s-AUC} \\
            \midrule
            \multirow{3}{*}{DHF1K} &Domain Generalization  &  2.467 & 0.445 & 0.338 & 0.893 & 0.690 \\
            &Domain Adaptation  & 2.461 &  0.447    & 0.338 &  0.894 & 0.696 \\
            &Transfer Learning  & \textbf{2.753}    &  \textbf{0.487}    & \textbf{0.384}    & \textbf{0.898} & \textbf{0.697} \\
            \midrule
            \multirow{3}{*}{UCF Sports} &Domain Generalization  &  2.476 & 0.538 & 0.442 & 0.885 & 0.756 \\
            &Domain Adaptation  & 2.389 &  0.522   & 0.431 &  0.882 & 0.746 \\
            &Transfer Learning  & \textbf{2.780}    & \textbf{0.576}    & \textbf{0.486}    & \textbf{0.887} & \textbf{0.762} \\
            \bottomrule
        \end{tabular}
        \caption{\textbf{Analysis of domain-shift capabilities.} Performance evaluation in the \emph{domain generalization} (supervised training on source; testing on target) and \emph{domain adaptation} (supervised training on source; unsupervised training and test on target) scenarios. Upper-bound performance is measured by the \emph{transfer learning} scenario (supervised training on source and fine-tuning on target).} 
        \label{tab:unsupervisedDA}
    \end{table*}

\begin{table}[ht!]
    \centering
    \small\addtolength{\tabcolsep}{-2pt}
    \begin{tabular}{clcccc}
    \toprule
    \multicolumn{6}{c}{\textbf{source: DHF1K -  target: LEDOV }}\\
    \midrule
    Test & \textbf{Setting} & \textbf{NSS} & \textbf{CC} & \textbf{SIM} & \textbf{AUC-J} \\
    \midrule
    \multirow{2}{*}{UCF} & Generaliz.   & 2.494 & 0.536 & 0.442  & 0.889            \\
    & Adaptation   &  \textbf{2.584}  &  \textbf{0.555}  &  \textbf{0.452} & \textbf{0.900}      \\
    \midrule
    \multirow{2}{*}{Hollywood2} & Generaliz.               & 3.011            & 0.622            & 0.502 & 0.922 \\
    & Adaptation     & \textbf{3.066}   & \textbf{0.623} &  \textbf{0.505}  & \textbf{0.926}   \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Analysis of generalization capabilities.} Domain adaptation performance, respectively, with source: DHF1K, target: LEDOV; test: UCF Sports and Hollywood2. Best results in bold.}
    \label{tab:dhf1k-ledov}
\end{table}

    \begin{table*}[ht!]
        \centering
        \begin{tabular}{c|lccccc}
            \toprule
            \multicolumn{7}{c}{\textbf{Train datasets: DHF1K, UCF Sports, Hollywood2}} \\ 
            \midrule
            \textbf{Test Dataset} &\textbf{Approach} & \textbf{NSS} & \textbf{CC} & \textbf{SIM} & \textbf{AUC-J} & \textbf{s-AUC} \\
            \midrule
            \multirow{3}{*}{DHF1K} & Single-source & 2.806 & 0.489 & 0.403 & 0.904 & 0.705 \\
            & Multi-source   & 2.811 & 0.491 & 0.403 & 0.893 & \textbf{0.708} \\
            & Domain-specific  & \textbf{2.875} &  \textbf{0.500} & \textbf{0.406} &  \textbf{0.910} & 0.707 \\
            \midrule
            \multirow{3}{*}{UCF Sports} & Single-source & 2.803 & 0.589 & 0.489 & 0.879 & 0.759 \\
            & Multi-source  &  2.922 & 0.594 & 0.498 & 0.882 & 0.767 \\
            &Domain-specific  & \textbf{3.114} &  \textbf{0.604} & \textbf{0.507} &  \textbf{0.904} & \textbf{0.768} \\
            \midrule
            \multirow{3}{*}{Hollywood2} & Single-source & 3.235 & 0.660 & 0.528 & 0.919 & 0.778 \\
            &Multi-source  &  3.349 & 0.665 & 0.551 & 0.922 & 0.797 \\
            &Domain-specific  & \textbf{3.352} &  \textbf{0.670}   & \textbf{0.551} &  \textbf{0.936} & \textbf{0.807} \\
            \bottomrule
        \end{tabular}
        \caption{Performance evaluation on the multi-source and domain specific learning scenarios.}
        \label{tab:supervised}
    \end{table*}































































