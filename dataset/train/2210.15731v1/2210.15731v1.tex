\documentclass{esannV2p}
\usepackage{graphicx}
\usepackage[latin1]{inputenc}
\usepackage{amssymb,amsmath,array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage[square,numbers]{natbib}
\usepackage{enumitem}
\usepackage{xfrac}
\usepackage{watermark}
\watermark{\thispageheading{\sffamily Proceedings of the 30th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2022), pp. 491-496 doi:10.14428/esann/2022.ES2022-58}}
\setcounter{page}{491}

\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}


\newcommand\rot{\rotatebox{90}}
\definecolor{bestresult}{gray}{0.9}
\newcommand{\best}[1]{\colorbox{bestresult}{#1}}
\setlength{\fboxsep}{2pt}

\begin{document}
\title{Beyond Homophily with\\ Graph Echo State Networks}

\author{Domenico Tortorella and Alessio Micheli
\vspace{.3cm}\\
University of Pisa - Department of Computer Science \\
Largo B. Pontecorvo 3, 56127 Pisa - Italy
}

\maketitle

\begin{abstract}
Graph Echo State Networks (GESN) have already demonstrated their efficacy and efficiency in graph classification tasks.
However, semi-supervised node classification brought out the problem of over-smoothing in end-to-end trained deep models, which causes a bias towards high homophily graphs.
We evaluate for the first time GESN on node classification tasks with different degrees of homophily, analyzing also the impact of the reservoir radius.
Our experiments show that reservoir models are able to achieve better or comparable accuracy with respect to fully trained deep models that implement ad hoc variations in the architectural bias, with a gain in terms of efficiency.
\end{abstract}

\section{Introduction}
Graphs provide a useful structure to represent relations between entities, such as paper citations or web page networks.
A plethora of neural models have been proposed to solve graph-, edge-, and node-level tasks \cite{Bacciu2020}, most of them sharing an architecture structured in layers that perform local aggregations of node features.
This architectural bias, where node features are progressively smoothed in deeper layers via local aggregation \cite{Li2018}, is the source of most of the issues that graph neural models are facing.
This bias towards locally homogeneous graphs is more apparent in node classification tasks, where graphs presenting a significant number of inter-class edges, i.e. a low \emph{homophily} degree, present a challenge to convolutive models.
Graph Echo State Network (GESN) \cite{Gallicchio2010} is an efficient model within the reservoir computing (RC) paradigm.
In RC, input data is encoded via a randomly-initialized reservoir, while only a linear readout requires training.
GESN has already been successfully applied to graph-level classification tasks \cite{Gallicchio2020}.
In this paper, we analyze for the first time its application to node classification tasks, focusing in particular on the efficacy in tackling low-homophily graphs.

\section{Node classification and homophily}\label{sec:background}
Let  denote a graph with node feature vectors  for each node .
We also denote by  the set of nodes within  hops of node , and by  the graph adjacency matrix.
The goal of a semi-supervised node classification task is to learn a model from a subset of graph nodes with known target labels , in order to infer the node labels  for the remaining nodes  using the network structure and input features .
Most common graph convolutional models are structured in  layers, where each layer learns an embedding for each node based on an increasingly large receptive field.
These layers can be formalized as \cite{Xu2019}

where node embeddings  of layer  are obtained by aggregating the previous embeddings  of node 's -hop neighbors via , and then combined with the node's previous embeddings  via ; for , .
The final layer  either directly predicts the one-hot encoding of target label , or is followed by an MLP that serves this purpose.
The whole model is trained end-to-end by typically minimizing the cross-entropy loss.

The choice of functions in \eqref{eq:gnn-layer} determines the architectural bias of the model.
For example, GCN \cite{Kipf2017} layers are defined as ,
where  is the normalized adjacency matrix,  are learnable weights, and  is the row stack of node features for layer .
It has been shown that stacking more than three or four layers of graph convolution causes a degradation in accuracy \cite{Li2018}, since representations  converge asymptotically to a fixed point of  as  increases, or more generally, to a low-frequency subspace of the graph spectrum.
This problem is known as \emph{oversmoothing}.
Indeed, by acting as a low-pass filter, GCNs are biased in favor of tasks whose graphs present a high degree of homophily, that is nodes in the same neighborhood mostly share the same class \citep{Zhu2020}.
Formally, homophily in a graph can be quantified \citep{Zhu2020} as the intra-class edges ratio

Changes in the model architectural bias have been proposed to improve classification on low homophily graphs.
Some solutions individuated by \cite{Zhu2020} are:
\begin{enumerate}[itemsep=0mm]
  \item separate ego and neighborhood representations in \eqref{eq:gnn-layer}, by aggregating on open node neighborhoods  and combining by concatenation;
  \item extend aggregation to multi-hop neighborhoods , , e.g. as in graph convolutions with Chebyshev polynomial filters \cite{Defferrard2016};
  \item exploit also the representations  computed at each intermediate layer  to make predictions, e.g. as in Jumping-Knowledge networks \cite{Xu2018}.
\end{enumerate}
H2GCN \cite{Zhu2020} incorporates all three architectural solutions.
Alternative solutions include altering the graph structure to improve the homophily degree, in order to increase the ratio of intra-class edges in node neighborhoods \cite{Gasteiger2019,Topping2022}.

\section{Reservoir computing for graphs}\label{sec:graphesn}
Reservoir computing is a paradigm for the efficient design of recurrent neural networks.
Input data is encoded by a randomly initialized reservoir, while only the task prediction layer requires training.
Graph Echo State Networks (GESNs) extended the reservoir computing paradigm to graph-structured data \cite{Gallicchio2010}, and have already demonstrated their effectiveness in graph classification tasks \cite{Gallicchio2020}.
Node embeddings are recursively computed by the dynamical system

where  and  are the input-to-reservoir and the recurrent weights, respectively (input bias is omitted).
Equation \eqref{eq:graphesn} is iterated over  until the system state converges to fixed point , which is used as the embedding.
The existence of a fixed point is guaranteed by the Graph Embedding Stability (GES) property \cite{Gallicchio2020}, which also guarantees independence from the system's initial state .
A necessary condition \cite{Tortorella2022} for the GES property is , where  denotes the spectral radius of a matrix, i.e. its largest absolute eigenvalue, and  is the graph spectral radius.
This condition also provides the best estimate of the system bifurcation point, i.e. the threshold beyond which \eqref{eq:graphesn} becomes asymptotically unstable.
Reservoir weights are randomly initialized from a uniform distribution in , and then rescaled to the desired input scaling and reservoir spectral radius, without requiring any training.
While in graph-level task node features are aggregated to provide global embeddings, for node classification tasks we directly apply a linear readout to node embeddings , where the weights  are trained by ridge regression on one-hot encodings of target classes .

The contractivity of \eqref{eq:graphesn} is a sufficient condition for the GES property \cite{Tortorella2022}.
However, the contractivity of graph convolution layers has also been linked to the degradation of representativeness in deep models \cite{Topping2022}.
Graph rewiring solutions to the homophily bias, such as \cite{Gasteiger2019}, greatly increase the edges of a graph, which in turn leads to an increase of  and a decrease in contractivity.
Therefore, in our experiments we will explore also values of the reservoir radius beyond the stability threshold, in this case by arbitrarily fixing the number of iterations of \eqref{eq:graphesn} to .
Indeed, we can interpret the  iterations of \eqref{eq:graphesn} as equivalent to  graph convolution layers with weights shared among layers and input skip connections.
While in deep GCNs convergence to a fixed point of the graph convolution operator, due to stacking too many layers, has been linked to the oversmoothing issue \cite{Li2018}, GESNs can in principle avoid that by selecting a reservoir radius .

\section{Experiments and discussion}\label{sec:experiments}

\begin{table}[h!]
  \footnotesize
  \centering
  \begin{tabular}{@{}l@{}c@{\hspace{-1pt}}c@{}c@{}c@{}c@{\hspace{-1pt}}c@{}c@{}c@{}c@{}}
    \toprule
    & \textsf{Texas} & \textsf{Wisconsin} & \textsf{Actor} & \textsf{Squirrel} & \textsf{Chameleon} & \textsf{Cornell} & \textsf{Citeseer} & \textsf{Pubmed} & \textsf{Cora} \\
    \midrule
    \textbf{Homo.} &  &  &  &  &  &  &  &  &  \\
    \textbf{Nodes} &  &  &  &  &  &  &  &  &  \\
    \textbf{Edges} &  &  &  &  &  &  &  &  &  \\
    \textbf{Radius} &  &  &  &  &  &  &  &  &  \\
    \textbf{Featur.} &  &  &  &  &  &  &  &  &  \\
    \textbf{Classes} &  &  &  &  &  &  &  &   & \\
    \midrule
    GCN &  &  &  &  &  &  & \best{} &  & \best{} \\
    +JK &  &  &  &  &  &  &  &  &  \\
    +Cheby &  &  &  &  &  &  & \best{} &  & \best{} \\
    H2GCN & \best{} & \best{} & \best{} &  &  & \best{} & \best{} & \best{} & \best{} \\
    \midrule MLP & \best{} & \best{} & \best{} &  &  & \best{} &  &  &  \\
    \midrule GESN & \best{} & \best{} &  & \best{} & \best{} & \best{} &  & \best{} & \best{} \\
    \bottomrule
  \end{tabular}
  \caption{Node classification accuracy on low and high homophily graphs (average and standard deviation; results of fully trained models reported from \cite{Zhu2020}; results within one standard deviation of the best accuracy are highlighted).}
  \label{tab:experiments}
  \vspace{-12pt}
\end{table}

\begin{figure}\centering
  \begin{minipage}{0.4\linewidth}
    \centering
    \includegraphics[scale=.7, trim=0cm 0.6cm 0cm 0.6cm, clip]{radii.eps}
    \caption{Reservoir radii selected on each task.}
    \label{fig:radii}
  \end{minipage}
  \hfill
  \begin{minipage}{0.55\linewidth}
    \centering
    \includegraphics[scale=.69]{texas.eps}
    \caption{Impact of reservoir radius and units on classification accuracy for Texas.}
    \label{fig:texas}
  \end{minipage}
\end{figure}

\begin{figure}\centering
  \includegraphics[scale=0.7, trim=1.6cm 0.4cm 1cm 0.3cm, clip]{experiments.eps}
  \caption{Impact of reservoir radius and units on classification accuracy for Squirrel () and Cora (), with and without input features.}
  \label{fig:experiments}
\end{figure}

We evaluate GESN on six node classification tasks with low homophily degree () and three tasks with high homophily degree ().
We adopt the same  scaffold splits 48\%/32\%/20\% of \cite{Zhu2020}, averaging results in each fold over  different reservoir initializations.
We explore a number of units ranging from  to , input scaling factors from  to , readout regularization values from  to , and reservoir radii  with steps of  (up to  with larger steps for Squirrel and Chameleon).
Embeddings are computed with at most  iterations of equation \eqref{eq:graphesn}.

Accuracy results are reported in Table \ref{tab:experiments}, while Fig. \ref{fig:radii} shows the reservoir radii selected in the  splits.
We can observe three different behaviors, exemplified in Fig. \ref{fig:texas} and  \ref{fig:experiments} (top).
The number of reservoir units plays a significant role, offering best results when it is closer to the number of input features.
For Texas, Wisconsin, Actor, and Cornell, the performances of GESN are closer to the accuracies of MLP, which uses only node features , and H2GCN, with reservoir radii : in this case, the graph connectivity appears to be of no use.
While Squirrel and Chameleon present a low homophily degree, graph convolution models fare better than MLP: in this case graph connectivity needs to be taken into account.
On these two tasks, GESN improves upon the best model accuracy by  and , respectively, with reservoir radii selected in the range --.
Finally, on high homophily tasks (Citeseer, Pubmed, Cora) GESN performs generally in line with graph convolution models, which in turn do better than MLP; reservoir radii are selected in the range --.

We observe how the best accuracy results are for reservoir radii well above the stability threshold, which are required when the graph connectivity needs to be leveraged in classifying nodes.
To support our conclusion, in Fig. \ref{fig:experiments} (bottom) we report the accuracy on Squirrel and Cora where input features have been removed.
We observe that for stable embeddings (), accuracy significantly drops below the level reached by having input features, while it reaches almost the same levels of accuracy for the values of  selected with features, which are well beyond the region where GESN stability is guaranteed.

Finally, we underline the efficiency of GESN.
Only the linear readout's  parameters require training, against the additional  parameters of models that need to be trained end-to-end through many gradient descent epochs (for further time comparisons, see \cite{Gallicchio2020}).
The time required to compute node embeddings and train the readout for a model of  units takes from  to  seconds on a GPU Nvidia Tesla V100, depending on graph size.

\section{Conclusion}\label{sec:conclusion}
For the first time, we have applied Graph Echo State Networks to the task of node classification.
Experiments on nine graphs with different degrees of homophily have shown a classification accuracy generally in line with most fully trained models, with extraordinary improvements over two low homophily tasks.
Furthermore, contrary to the theory and experiments that demonstrated the crucial role of system stability in applying GESNs to graph-level tasks, our experiments have shown that node embeddings computed in regions well beyond the theoretical stability threshold are better suited to represent the graph structure.
Future work will analyze more in-depth the embedding space structure, the role of reservoir radius in conditioning the filtering properties of GESN, and the impact of reservoir spectrum.


\begin{footnotesize}


\bibliographystyle{unsrtnat}
\bibliography{bibliography}

\end{footnotesize}


\end{document}
