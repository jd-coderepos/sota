



We develop a differentiable framework for self-supervised disentanglement of 3D pose and foreground appearance from in-the-wild video frames of human activity.







Our self-supervised 
framework builds on the conventional encoder-decoder architecture (Sec.~\ref{sec:3_2}). Here, the encoder produces a set of local 3D vectors from an input RGB image. This is then processed through a series of 3D transformations, adhering to the 3D pose articulation constraints to obtain a set of 2D coordinates (camera projected, non-spatial 2D pose). 
In Sec.~\ref{sec:3_1}, we define a set of part based representations followed by carefully designed differentiable transformations required to bridge the representation gap between the non-spatial 2D pose and the spatial part maps. This serves three important purposes. First, their spatial nature facilitates compatible input pose conditioning for the fully-convolutional decoder architecture. Second, it enables the decoder to selectively synthesize only FG human appearance ignoring the variations in the background. 
Third, it facilitates a novel way to encode the 2D joint and part association using a single template puppet model. 
Finally, Sec.~\ref{sec:3_3} describes the proposed self-supervised paradigm which makes use of the pose-aware spatial part maps for simultaneous discovery of 3D pose and part segmentation using image pairs from wild videos.
















\subsection{Joint-anchored spatial part representation}\label{sec:3_1}
One of the major challenges in unsupervised pose or landmark detection is to map the model-discovered landmarks to the standard landmark conventions. This is essential to facilitate the 
subsequent task-specific pipelines, which expect the input pose to follow a certain convention. Prior works~\cite{rhodin2018unsupervised,kundu2020ksp} rely on paired supervision to learn this mapping. In absence of such supervision, we aim to encode this convention in a \textit{canonical part dictionary} where the association of 2D joints with respect to the body parts is extracted from a \textit{single} manually annotated puppet template (Fig.~\ref{fig:main1}{\color{red}C}, top panel). This can be interpreted as a 2D human puppet model, which can approximate any human pose deformation via independent spatial transformation of body parts while keeping intact the anchored joint associations.

\vspace{1.5mm}
\textbf{Canonical maps.} We extract \textit{canonical part maps},  (here, : limb index and : total number of limbs or parts), where we perform erosion followed by Gaussian blurring of binary part segments to account for the associated \textit{shape} uncertainty (\ie body shape or apparel shape variations). We represent , 
where  is the space of spatial indices.  In addition, we also extract \textit{canonical shape uncertainty maps}  to specifically highlight only the uncertain regions (Fig.~\ref{fig:main1}{\color{red}C}, bottom panel). The two anchored joint locations for each limb  and its corresponding part map  are denoted as ,  , except for the \textit{torso} which is represented using 4 joints. 


\vspace{1.5mm}
\textbf{Part deformation model.} For a given 2D pose  with  being the total number of joints, \textit{part-wise pose maps} are obtained as independent spatial-transformations of the \textit{canonical part maps}, \ie . Here,  represents an affine transformation of the spatial indices , whose rotation, scale, and translation parameters are obtained as a function of , where ,  denote the joint locations associated with the limb  in pose . Similarly, we also compute the \textit{part-wise shape uncertainty maps} as . Note that,  and  are unaware of inter-part occlusion in the absence of limb-depth information. Following this, we obtain single-channel maps (see Fig.~\ref{fig:main1}{\color{red}D}), \ie 

\textbf{a)} \textit{shape uncertainty map} as , and 

\textbf{b)} \textit{single-channel FG-BG map} as . 

The above formalization bridges the representation gap between the raw joint locations,  and the output spatial maps , , and , thereby facilitating them to be used as differentiable spatial maps for the subsequent self-supervised learning. 









\vspace{1.5mm}
\textbf{Depth-aware part segmentation.} For 3D deformable objects, a reliable 2D part segmentation can be obtained with the help of following attributes, \ie a) 2D skeletal pose, b) part-shape information, and c) knowledge of inter-part occlusion. Here, the 2D skeletal pose and the knowledge of inter-part occlusion can be extracted by accessing camera transformation of the corresponding 3D pose representation. Let, the depth of the 2D joints in  with respect to the camera be denoted as  and .  We obtain a scalar depth value associated with each limb  as, .  
We use these depth values to alter the strength of depth-unaware \textit{part-wise pose maps},  at each spatial location,  by modulating the strength of part map intensity as being inversely proportional to the depth values. This is realized in the following steps: 

\textbf{a)} 
, 



\textbf{b)} , and 

\textbf{c)}
. 

Here,  indicates the spatial-channel dedicated for the background. Additionally, a non-differentiable 2D part-segmentation map (see Fig.~\ref{fig:main1}{\color{red}E}) is obtained as,

. 


\subsection{Self-supervised pose network}\label{sec:3_2}
The architecture for self-supervised pose and appearance disentanglement consists of a series of pre-defined differentiable transformations facilitating discovery of a constrained latent pose representation. As opposed to imposing learning based constraints~\cite{habibie2019wild}, 
we devise a way around where the 3D pose articulation constraints (\ie knowledge of joint connectivity and bone-length) are directly applied via structural means, implying guaranteed constraint imposition. 



\begin{figure*}\begin{center}
    \vspace{-1mm}
	\includegraphics[width=1.0\linewidth]{image_final/cvpr_fig_2.pdf}
	\vspace{-5.0mm}
	\caption{An overview of data-flow pipelines for the proposed self-supervised objectives. Images close to the output heads show the network output for the given set of exemplar input tuple. Here, the color of transformation blocks are consistent with Fig.~\ref{fig:main1}{\color{red}A}. 
	}
 	\label{fig:main2}    
    \vspace{-6mm}
\end{center}
\end{figure*}



\noindent
\textbf{a) Encoder network.} As shown in Fig.~\ref{fig:main1}{\color{red}A}, the encoder  takes an input image  and outputs three disentangled factors, a) a set of local 3D vectors: , b) camera parameters: , and c) a FG appearance: .

As compared to spatial 2D geometry~\cite{rhodin2018unsupervised,lorenz2019unsupervised}, discovering the inherent 3D human pose is a highly challenging task considering the extent of associated non-rigid deformation, and rigid camera variations~\cite{akhter2015pose,kundu2019unsupervised}. To this end, we define a canonical coordinate system , where \textit{face-vector} of the skeleton is canonically aligned along the +ve X-axis, thus making it completely view-invariant. Here, the \textit{face-vector} is defined as the perpendicular direction of the plane spanning the neck, left-hip and right-hip joints. As shown in Fig.~\ref{fig:main1}{\color{red}B}, in , except the pelvis, neck, left-hip and right-hip, all other joints are defined at their respective parent relative local coordinate systems (\ie parent joint as the origin with axis directions obtained by performing Gram-Schmidt orthogonalization of the parent-limb vector and the \textit{face-vector}). Accordingly, we define a recursive forward kinematic transformation  to obtain the canonical 3D pose from the local limb vectors, \ie , which accesses a constant array of limb length magnitudes~\cite{zhou2016deep}. 


Here, the camera extrinsics,  (3 rotation angles and 3 restricted translations ensuring that the camera-view captures all the skeleton joints in ) is obtained at the encoder output, whereas a fixed perspective camera projection is applied to obtain the final 2D pose representation, \ie . A part based deformation operation on this 2D pose  (Sec.~\ref{sec:3_1}) is shown as , where  and  (following the depth aware operations on ). Finally,  denotes the entire series of differentiable transformations, \ie , as shown in Fig.~\ref{fig:main1}{\color{red}A}. Here,  denotes composition operation.


\noindent
\textbf{b) Decoder network.} The decoder takes a concatenated representation of the FG appearance,  and pose,  as input to obtain two output maps, i) a reconstructed image , and ii) a predicted part segmentation map  via a bifurcated CNN decoder (see Fig.~\ref{fig:main2}{\color{red}A}). The common decoder branch,  consists of a series of up-convolutional layers conditioned on the spatial pose map  at each layer's input (\ie multi-scale pose conditioning).
Whereas,  and  follow up-convolutional layers to their respective outputs. 

\subsection{Self-supervised training objectives}\label{sec:3_3}
The prime design principle of our self-supervised framework is to leverage the interplay between the pose and appearance information by forming paired input images of either consistent pose or appearance. 

Given a pair of source and target image, , sampled from the same video \ie with consistent FG appearance, the shared encoder extracts their respective pose and appearance as  and  (see Fig.~\ref{fig:main2}{\color{red}A}). We denote the decoder outputs as  while the decoder takes in pose,  with appearance,  (this notation is consistent in later sections). Here,  is expected to depict the person in the target pose . Such a cross-pose transfer setup is essential to restrict leakage of pose information 
through the appearance. Whereas, the low dimensional bottleneck of  followed by the series of differentiable transformations prevents leakage of appearance through pose~\cite{jakab2018unsupervised}. 

To effectively operate on wild video frames (\ie beyond the in-studio fixed camera setup~\cite{rhodin2018unsupervised, jakab2018unsupervised}), we aim to utilize the pose-aware, spatial part representations as a means to disentangle the FG from BG. Thus, we plan to reconstruct  with a constant BG color  and segmented FG appearance (see Fig.~\ref{fig:main2}{\color{red}A}). Our idea stems from the concept of co-saliency detection~\cite{zhang2016co,hsu2018unsupervised}, where the prime goal is to discover the common or salient FG from a given set of two or more images. Here, the part appearances belonging to the model predicted part regions have to be consistent across  and  for a successful self-supervised pose discovery.




\textbf{Access to unpaired 3D/2D pose samples.} We denote  and  as a 3D pose and its projection (via random camera), sampled from an \textit{unpaired} 3D pose dataset , respectively. 
Such samples can be easily collected without worrying about BG or FG diversity in the corresponding camera feed (\ie a single person activity). We use these samples to further
constrain the latent space towards realizing a plausible 3D pose distribution.


\vspace{1mm}\noindent
\textbf{a) Image reconstruction objective.} 
Unlike~\cite{rhodin2018unsupervised,jakab2018unsupervised}, we do not have access to the corresponding ground-truth representation for the predicted , giving rise to an increased possibility of producing degenerate solutions or \textit{mode-collapse}. In such cases, the model focuses on fixed background regions as the common region between the two images, specifically for in-studio datasets with limited BG variations. One way to avoid such scenarios is to select image pairs with completely diverse BG (\ie select image pairs with high L2 distance in a sample video-clip). 

To explicitly restrict the model from inculcating such BG bias, we incorporate content-based regularization. An {uncertain} pseudo FG mask is used to establish a one-to-one correspondence between the reconstructed image  and the target image . This is realized through a spatial-mask,  which highlights the salient regions (applicable for any image frame) or regions with diverse motion cues (applicable for frames captured in a fixed camera). We formulate a pseudo (\textit{uncertain}) reconstruction objective as, 

\vspace{-5mm}


Here,  denotes pixel-wise weighing and  is a balancing hyperparameter. Note that, 
the final loss is computed as average over all the spatial locations, .
This loss enforces a self-supervised consistency between the pose-aware part maps,  and the salient common FG to facilitate a reliable pose estimate.

As a novel direction, we utilize  
to form a pair of image predictions  following simultaneous appearance invariance and pose equivariance (see Fig.~\ref{fig:main2}{\color{red}B}). Here, a \textit{certain} reconstruction objective is defined as, 


\vspace{-5mm}



\vspace{1mm}\noindent
\textbf{b) Part-segmentation objective.} 
Aiming to form a consistency between the true pose  and the corresponding part segmentation output, we formulate, 

\vspace{-5mm}




\begin{algorithm}[!b]
\vspace{-2mm}
\SetAlgoLined
: Trainable parameters of the Encoder \\   
: Trainable parameters of the Decoder \\
 (includes , , and )

\For {}{
\uIf{}{
 Update  by optimizing  and  in  separate \textit{Adagrad} optimizers on frozen .
}
 \Else{
 Update  by optimizing  and  in  separate \textit{Adagrad} optimizers on frozen .
 }
 


 Update  by optimizing , , and   in separate \textit{Adagrad} optimizers.
 }
 \vspace{1mm}
\caption{Self-supervised learning with the proposed adaptation via decoupled energy minimization. 
\vspace{-2mm}} \label{algo:1}
\end{algorithm}



Here, , and  denote the pixel-wise cross-entropy and self-entropy respectively. Moreover, we confidently enforce segmentation loss with respect to one-hot map  (Sec.~\ref{sec:3_1}) only at the \textit{certain} regions, while minimizing the Shanon's entropy for the regions associated with \textit{shape uncertainty} as captured in . Here, the limb depth required to compute  is obtained from  (Fig.~\ref{fig:main2}{\color{red}C}).

In summary, the above self-supervised objectives form a consistency among , , and ; 

\textbf{a)}  enforces consistency between  and , 

\textbf{b)}  enforces consistency between  (via ) and , 


\textbf{c)}  enforces consistency between  (via ) and . 

However, the model inculcates a discrepancy between the predicted pose and the true pose distributions. It is essential to bridge this discrepancy as  and  rely on true pose , whereas  relies on the predicted pose . Thus, we employ an adaptation strategy to guide the model towards realizing a plausible pose prediction.




\vspace{1mm}\noindent
\textbf{c) Adaptation via energy minimization.} 
Instead of employing an ad-hoc adversarial discriminator~\cite{chen2019unsupervised,yang20183d}, we devise a simpler yet effective decoupled energy minimization strategy~\cite{han2018co,jaderberg2017decoupled}. 
We avoid a direct encoder-decoder interaction during gradient back-propagation, by updating
the encoder parameters, while freezing the decoder parameters and vice-versa. However, this is performed while enforcing a reconstruction loss at the output of the \textit{secondary} encoder in a cyclic auto-encoding scenario (see Fig.~\ref{fig:main2}{\color{red}C}). The two energy functions are formulated as  and , where  and .

The decoder parameters are updated to realize a faithful , as the frozen encoder expects  to match its input distribution of real images (\ie ) for an effective energy minimization. Here, the encoder can be perceived as a frozen energy network as used in energy-based GAN~\cite{zhao2016energy}. A similar analogy applies while updating the encoder parameters with gradients from the frozen decoder. Each alternate energy minimization step is preceded by an overall optimization of the above consistency objectives, where both encoder and decoder parameters are updated simultaneously (see Algo.~\ref{algo:1}).