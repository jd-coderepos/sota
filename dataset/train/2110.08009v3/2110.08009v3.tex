\begin{center}
\hrule
\Huge{
    \textsc{Supplementary Material}}
    \vspace{0.3cm}
\hrule
\end{center}
\vspace{0.3cm}











The following appendices support the main paper and are organized as follows: 
Appendix~\ref{appendix:maso} addresses some details on the background of continuous piecewise affine deep networks, that were omitted in the main paper.
Appendices~\ref{appendix:figures} and \ref{appendix:architecture} provide additional figures and training details for all the experiments that were studied in the main text, and Appendix~\ref{appendix:proofs} provides the proofs of all the theoretical results. {\bf Due to size constraints, the high-quality batch of samples are provided in the supplementary files}.


\section{Background on Continuous Piecewise Affine Deep Networks}
\label{appendix:maso}
A {\em max-affine spline operator} (MASO) concatenates independent {\em max-affine spline} (MAS) functions, with each MAS formed from the point-wise maximum of  affine mappings
\citep{magnani2009convex,hannah2013multivariate}. For our purpose each MASO will express a DN layer and is thus an operator producing a  dimensional vector from a  dimensional vector and is formally given by

\vspace{-0.2em}
where  are the slopes and 
 
are the offset/bias parameters and the maximum is taken coordinate-wise. For example, a layer comprising a fully connected operator with weights  and biases  followed by a ReLU activation operator corresponds to a (single) MASO with .
Note that a MASO is a {\em continuous piecewise-affine} (CPA) operator \citep{wang2005generalization}.

The key background result for this paper is that {\em the layers of DNs constructed from piecewise affine operators (e.g., convolution, ReLU, and max-pooling) are MASOs}
\citep{balestriero2018spline,reportRB}:

making the entire DGN a composition of MASOs.
\\
The CPA spline interpretation enabled from a MASO formulation of DGNs provides a powerful global geometric interpretation of the network mapping based on a partition of its input space  into polyhedral regions and a per-region affine transformation producing the network output. The partition regions are built up over the layers via a {\em subdivision} process and are closely related to Voronoi and power diagrams \citep{balestriero2019geometry}. We now propose to greatly extend such  insights to carefully characterize and understand DGNs as well as provide theoretical justifications to various observed behaviors e.g. mode collapse.


\section{Uniform and Gaussian Manifold Distributions}
\label{sec:appendix_distributions}


We now demonstrate the use of the above result by considering practical examples for which we are able to gain insights into the DGN data modeling and generation. We consider the two most common cases: (i) the latent distribution is set as   and (ii) the latent distribution is set as  (on the hypercube of dimension ). We obtain the following result by direct application of Thm.~\ref{thm:general_density}.


\begin{cor}
\label{cor:gaussian_density}
The generated density distribution  of the Gaussian and uniform densities are given by

\end{cor}


The two above formulae provide a precise description of the produced density given that the latent space density is Gaussian or Uniform. In the Gaussian case, the per-region slope matrices act upon the  distance by rescaling it from the coordinates of  and the per-region offset parameters  are the mean against which the input  is compared against. In the Uniform case, the change of volume (recall Eq.~\ref{eq:volume}) is the only quantity that impacts the produced density. We will heavily rely on this observation for the next section where we study how to produce a uniform sampling onto the CPA manifold of an affine spline.


We derive the analytical form for the case of Gaussian and Uniform latent distribution in Appendix~\ref{appendix:distribution}.
From the analytical derivation of the generator density distribution, we obtain its differential entropy.



\begin{cor}
\label{cor:entropy}
The differential Shannon entropy of the output distribution  of the DGN is given by

\end{cor}




As a result, the differential entropy of the output distribution  corresponds to the differential entropy of the latent distribution  plus a convex combination of the per-region volume changes. It is thus possible to optimize the latent distribution  to better fit the target distribution entropy as in \cite{ben2018gaussian} and whenever the prior distribution is fixed, any gap between the latent and output distribution entropy imply the need for high change in volumes between  and .


\section{Per-Region Affine Mappings}
\label{appendix:affine}

For completeness we also provide that analytical form of the per-region affine mappings

where  is the pointwise derivative of the activation function of layer  based on its input , which we note as a function of  directly. For precise definitions of those operators see \cite{balestriero2020mad}.
The  operator simply puts the given vector into a diagonal square matrix. For convolutional layers (or else) one can simply replace the corresponding  with the correct slope matrix parametrization as discussed in Sec.~\ref{sec:background}. Notice that since the employed activation functions  are piecewise affine, their derivative is piecewise constant, in particular with values  with  for ReLU,  for absolute value, and in general with  for Leaky-ReLU for . 


\section{Number of Samples and Uniformity}
\label{sec:NvsK}

Exact uniformity is reached when the Monte Carlo samples have covered each region of the DGN partition boundary. For large state-of-the-art models this condition requires sampling on the order of millions. However, we conducted an experiment to see how the number of samples really impacted the uniformity of the generated manifold as follows. We compute precision and recall metrics [4] for StyleGAN2 with  generated samples obtained from  Monte Carlo samples based on our sampling strategy by varying . We use  and  ranging from  to . Based on the metrics, we identify that increasing beyond  no longer impacts the metrics, showing that this number of monte carlo samples is enough to converge (approximately) to the uniform sampling in that case, see Fig.~\ref{fig:evoluion}.

We report here the Jacobian computation times for Tensorflow 2.5 with CUDA 11 and Cudnn 8 on an NVIDIA Titan RTX GPU. For StyleGAN2 pixel space, 5.03s/it; StyleGAN2 style-space, 1.12s/it; BigGAN 5.95s/it; ProgGAN 3.02s/it. For NVAE on Torch 1.6 it takes 20.3s/it. Singular value calculation for StyleGAN2 pixel space takes .005s/it, StyleGAN2 style space .008s/it, BigGAN .001s/it, ProgGAN .004s/it and NVAE .02s/it on NumPy.

\begin{figure}[t!]
    \centering
    \begin{minipage}{0.39\linewidth}
    \includegraphics[width=\linewidth]{images/evoluionN.png}
    \end{minipage}
    \begin{minipage}{0.49\linewidth}
    \caption{Evolution of the precision/recall curves for varying number of samples  form the monte-carlo sampling against the number of samples  for StyleGAN2.}
    \label{fig:evoluion}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.39\linewidth}
    \includegraphics[width=\linewidth]{prd/stylegan_prd70k.pdf}
    \end{minipage}
    \begin{minipage}{0.49\linewidth}
    \caption{Precision-recall curves for  samples from Vanilla StyleGAN2 and MaGNET StyleGAN2}
    \label{fig:styleganpr}
    \end{minipage}
\end{figure}


\section{Extra Figures}
\label{appendix:figures}





This section contains samples from our proposed methods, more samples along with attribute data and pretrained weights are available at our project \texttt{link}.


\begin{figure}[t!]
    \centering
    \begin{minipage}{0.39\linewidth}
    \includegraphics[width=\linewidth]{bvae/bvae_8_hue_distribution_train.pdf}
    \end{minipage}
    \begin{minipage}{0.49\linewidth}
    \caption{Depiction of the imbalance hue distribution applied to color the MNIST digits.}
    \label{fig:hue_original}
    \end{minipage}
\end{figure}


\begin{figure}[t!]
    \centering
    \begin{minipage}{0.04\linewidth}
    \rotatebox{90}{\parbox{4.5cm}{\small Number of training samples having  (x-axis) samples at most  away}}
    \end{minipage}
    \begin{minipage}{0.47\linewidth}
    \includegraphics[width=\linewidth]{nvae/nn_hist_05gamma_vanilla.pdf}
    \end{minipage}
    \begin{minipage}{0.47\linewidth}
    \includegraphics[width=\linewidth]{nvae/nn_hist_05gamma_magnet.pdf}
    \end{minipage}
    {\small number of samples within -ball radius}
    \caption{\small Reprise of Fig.~\ref{fig:mnist_unitest}.  Vanilla NVAE {\bf Left}, MaGNET NVAE {\bf Right} }
    \label{fig:mnist_unitest1}
\end{figure}


\begin{figure}[t!]
    \centering
    \begin{minipage}{0.04\linewidth}
    \rotatebox{90}{\parbox{4.5cm}{\small Number of training samples having  (x-axis) samples at most  away}}
    \end{minipage}
    \begin{minipage}{0.47\linewidth}
    \includegraphics[width=\linewidth]{nvae/nn_hist_15gamma_vanilla.pdf}
    \end{minipage}
    \begin{minipage}{0.47\linewidth}
    \includegraphics[width=\linewidth]{nvae/nn_hist_15gamma_magnet.pdf}
    \end{minipage}
    {\small number of samples within -ball radius}
    \caption{\small Reprise of Fig.~\ref{fig:mnist_unitest}. Vanilla NVAE {\bf Left}, MaGNET NVAE {\bf Right} }
    \label{fig:mnist_unitest2}
\end{figure}

\clearpage
\begin{figure}[h]
    \centering
    \begin{minipage}{0.3\linewidth}
    \includegraphics[width=\linewidth]{stylegan2/large_batch/vanilla_245_2.png}
    \end{minipage}
    \begin{minipage}{0.3\linewidth}
    \includegraphics[width=\linewidth]{stylegan2/large_batch/magnet_pixel_245.png}
    \end{minipage}
    \begin{minipage}{0.3\linewidth}
    \includegraphics[width=\linewidth]{stylegan2/large_batch/magnet_style_245_2.png}
    \end{minipage}
    
    \caption{Random batches of  samples from a StyleGAN2 trained on FFHQ, generated via standard sampling (left), MaGNET sampling in the pixel-space (middle) and MaGNET sampling in the style-space.}
    \label{fig:styleganfull}
\end{figure}


\clearpage
\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\linewidth}
    \includegraphics[width=\linewidth]{progGAN/prog_gan_naive.png}
    \end{minipage}
    \begin{minipage}{0.48\linewidth}
    \includegraphics[width=\linewidth]{progGAN/prog_gan_magnet.png}
    \end{minipage}
    \caption{Random Samples from vanilla progGAN (left) and MaGNET progGAN (right) trained on the CelebA-HQ dataset. Samples are sorted by gender \& age and color coded by gender as visually predicted by the Microsoft Cognitive API. Samples not recognized by the API are color coded as white at the bottom. }
    \label{fig:progganfull}
\end{figure}

\newpage
\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bigGAN/collie_montage.png}
    \caption{Random Samples from vanilla BigGAN (left) and MaGNET BigGAN (right) from the \emph{Collie} class of Imagenet.}
    \label{fig:collie}
\end{figure}



\newpage
\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bigGAN/siamese_montage.png}
    \caption{Random Samples from vanilla BigGAN (left) and MaGNET BigGAN (right) from the \emph{Siamese} class of Imagenet.}
    \label{fig:siamese}
\end{figure}

\newpage
\clearpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{bigGAN/tabby_montage.png}
    \caption{Random Samples from vanilla BigGAN (left) and MaGNET BigGAN (right) from the \emph{Tabby} class of Imagenet.}
    \label{fig:tabby}
\end{figure}




\newpage

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{stylegan2/style_attribs.pdf}
    \small
    \caption{Facial Attributes of  StyleGAN2 samples using vanilla sampling, MaGNET style-space sampling and  MaGNET pixel-space sampling. We see that MaGNET style-space increases uniformity in gender and age distributions whereas MaGNET pixel-space yields more variations in physical attributes and accessories.}
    \label{fig:stylegan2_attributes}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{progGAN/proggan_attribs.pdf}
    \small
    \caption{Facial Attributes of  ProgGAN samples using standard sampling and MaGNET sampling.}
    \label{fig:stylegan2_attributes}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{nvae/mnist.png}
    \caption{Random Samples from vanilla NVAE (left) and MaGNET NVAE (right) trained on the MNIST dataset.}
    \label{fig:mnistnvae}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{nvae/cifar.png}
    \caption{Random Samples from vanilla NVAE (left) and MaGNET NVAE (right) trained on the CIFAR dataset.}
    \label{fig:cifarnvae}
\end{figure}

\clearpage





\section{Extra Tables}
\label{appendix:tables}

\begin{table}[h]
\centering
\caption{FID obtained by StyleGAN2 (config-f) trained on FFHQ using standard and MaGNET sampling (pixel-space) for varying degrees of truncation (). (Left) FID score obtained for 50,000 samples generated by a mixture of MaGNET and standard sampling. MaGNET samples can be used to increase diversity of the model, resulting in better FID than current state-of-the-art. (Right) FID score for varying truncation without mixing.}
\begin{tabular}{cccrcccr}
\cline{1-4} \cline{6-8}
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Truncation\\ \end{tabular}} & \begin{tabular}[c]{@{}c@{}}Sampling\\ Method\end{tabular} & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Percent\\ Mixture\end{tabular}} & \multicolumn{1}{c}{FID ()} &  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Truncation\\ \end{tabular}} & \begin{tabular}[c]{@{}c@{}}Sampling\\ Method\end{tabular} & \multicolumn{1}{c}{FID ()} \\ \cline{1-4} \cline{6-8} 
\multirow{2}{*}{1} & Standard & - & 2.74 &  & \multirow{2}{*}{.5} & Standard & 58.33 \\
 & MaGNET & 4.1\% & \textbf{2.66} &  &  & MaGNET & \textbf{54.47} \\
\multirow{2}{*}{.9} & Standard & - & 5.05 &  & \multirow{2}{*}{.4} & Standard & 83.84 \\
 & MaGNET & 20\% & \textbf{4.29} &  &  & MaGNET & \textbf{82.41} \\
\multirow{2}{*}{.8} & Standard & - & 10.94 &  & \multirow{2}{*}{.3} & Standard & \textbf{112.08} \\
 & MaGNET & 33\% & \textbf{8.57} &  &  & MaGNET & 112.89 \\
\multirow{2}{*}{.7} & Standard & - & 21.34 &  & \multirow{2}{*}{.2} & Standard & \textbf{142.27} \\
 & MaGNET & 100\% & \textbf{19.41} &  &  & MaGNET & 144.93 \\
\multirow{2}{*}{.6} & Standard & - & 36.98 &  & \multirow{2}{*}{.1} & Standard & \textbf{176.20} \\
 & MaGNET & 100\% & \textbf{33.19} &  &  & MaGNET & 178.75 \\ \cline{1-4} \cline{6-8} 
\end{tabular}
\label{tab:fidMagnet}
\end{table}









\section{Algorithms}
\label{appendix:online_alg}

\begin{algorithm}[]
	\SetAlgoLined
	\KwIn{Latent space domain, ; Generator ; Number of regions to sample ; Number of samples ;}
\KwOut{MaGNET Samples, ;}
	 Initialize,  \;
    \For{}{
     \;
     Get Slope Matrix, \;
     Get volume scalar at , \;
     \;
     }
\For{}{
	
	\;
	
	}
    
  
	
	\caption{MaGNET Sampling as described in Sec.~\ref{sec:uniformity}}
	\label{alg:magnet}
	
\end{algorithm}



\begin{algorithm}[]
	\SetAlgoLined
	\KwIn{Latent space domain, ; Generator ;  change of volume scalars ;}
\KwOut{MaGNET Sample, ;}
	\While{True}{
      Sample \;
      Sample \;
      Get Slope Matrix, \;
      Get volume scalar at , \;
    \uIf{   }{
    \vspace{.5em}
    \;
      \textbf{break}\;
    }\textbf{end}
    


  }
	
	\caption{Online Rejection Sampling algorithm for MaGNET}
	\label{alg:RS_magnet}
	
\end{algorithm}


\section{Architecture, Hardware and Implementation Details}
\label{appendix:architecture}


All the experiments were run on a Quadro RTX 8000 GPU, which has 48 GB of high-speed GDDR6 memory and 576 Tensor cores. For the software details we refer the reader to the provided codebase. In short, we employed TF2 (2.4 at the time of writing), all the usual Python scientific libraries such as NumPy and PyTorch. We employed the official repositories of the various models we employed with official pre-trained weights.
As a note, most of the architectures can not be run on GPUs with less or equal to 12 GB of memory.

For StyleGAN2, we use the official config-e provided in the GitHub StyleGAN2 repo\footnote{https://github.com/NVlabs/stylegan2}, unless specified. We use the recommended default of  as the interpolating stylespace truncation, to ensure generation quality of faces for the qualitative experiments. For BigGAN we use the BigGAN-deep architecture with no truncation, available on TFHub\footnote{https://tfhub.dev/deepmind/biggan-deep-256/1}. We also use the NVAE\footnote{https://github.com/NVlabs/NVAE} and ProgGAN\footnote{https://github.com/tkarras/progressive\_growing\_of\_gans} models and weights from their respective official implementations. For the Jacobian determinant calculation of images w.r.t latents, we first use a random orthogonal matrix to project generated images into a lower dimensional space, calculate the Jacobian of the projection w.r.t the latents and calculate the singular values of the jacobian to estimate the volume scalar. We use a projection of 256 dimensions for StyleGAN2-pixel, ProgGAN and BigGAN, and 128 dimensions for NVAE. To estimate the volume scalar we use the top 30, 20, 15 singular values for StyleGAN2 MaGNET pixel, ProgGAN and BigGAN; 40 for StyleGAN2 MaGNET style, and 30 for NVAE. 


\section{Proofs}
\label{appendix:proofs}















































\subsection{Proof of Lemma~\ref{lemma:volume}}
\label{proof:volume}

\begin{proof}
In the special case of an affine transform of the coordinate given by the matrix  the well known result from demonstrates that the change of volume is given by
 (see Theorem 7.26 in \cite{rudin2006real}). However in our case the mapping is a rectangular matrix as we span an affine subspace in the ambient space  making  not defined. 


First, we shall note that in the case of a Riemannian manifold (as is the produced surface from the per-region affine mapping) the volume form used in the usual change of variable formula can be defined via the square root of the determinant of the metric tensor. Now,  for a surface of intrinsic dimension  embedded in Euclidean space of dimension  (in our case, the per-region affine mapping produces an affine subspace) parametrized by the mapping  (in our case this mapping is simply the affine mapping  for each region) the metric tensor is given by   with  the jacobian/differential operator (in our case  for each region). This result is also known as Sard's theorem \citep{spivak2018calculus}. We thus obtain that the change of volume from the region  to the affine subspace  is given by  which can also be written as follows with  the svd-decomposition of the matrix :

leading to

\end{proof}




\subsection{Proof of Theorem~\ref{thm:general_density}}
\label{proof:general_density}


\begin{proof}
We will be doing the change of variables , also notice that  .
First, we know that
 which is well defined based on our full rank assumptions. We then proceed by


Let now prove the Etape 1 step by proving that   where we lighten notations as  and  is the svd-decomposition of :

with the above it is direct to see that  as follows

which gives the desired result.
\end{proof}


\subsection{Proof of Corollary~\ref{cor:gaussian_density}}
\label{appendix:distribution}

\begin{proof}
We now demonstrate the use of Thm.~\ref{thm:general_density} where we consider that the latent distribution is set as  . 
We obtain that

giving the desired result that is reminiscent of Kernel Density Estimation (KDE) \citep{rosenblatt1956remarks} and in particular adaptive KDE \citep{breiman1977variable}, where a partitioning of the data manifold is performed on each cell ( in our case) different kernel parameters are used.
\end{proof}

\begin{proof}
We now turn into the uniform latent distribution case on a bounded domain  in the DGN input space. By employing again Thm.~\ref{thm:general_density}, the given formula one can directly obtain that the output density is given by 

\end{proof}



\subsection{Proof of Thm.~\ref{thm:2}}
\label{proof:thm2}

As we assume successful training, then regardless of the actual distribution , the DGN will learn the correct underlying manifold, and learn the best approximation to  as possible onto this manifold. Now, applying MaGNET sampling i.e. Sec.~\ref{sec:uniformity} is equivalent to sampling from a distribution  such that after DGN mapping, that distribution is uniform on the learned manifold (see Thm.~\ref{thm:general_density}). As we assumed that regardless of  the DGN approximates correctly the true manifold, and as we then adapt the sampling distribution  to always obtain uniform sampling on that manifold, we see that this final sampling becomes invariant upon the data distribution (on the manifold) leading to the desired result.



%
