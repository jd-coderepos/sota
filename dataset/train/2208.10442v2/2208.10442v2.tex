\documentclass{article}

\usepackage[numbers]{natbib}
\usepackage[preprint]{neurips_2022}

\usepackage[dvipsnames]{xcolor}         \definecolor{linkColor}{rgb}{0.18,0.39,0.62}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks=true,linkcolor=linkColor,citecolor=linkColor,filecolor=linkColor,urlcolor=linkColor]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{capt-of}
\usepackage{tabularx}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{scalerel}
\usepackage[inline]{enumitem}
\usepackage{listings}
\usepackage{varwidth}
\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\newcommand{\crefrangeconjunction}{--}
\usepackage{stmaryrd}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{pifont}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\tx}[1]{``\textit{#1}''}
\newcommand{\sptk}[1]{\texttt{[#1]}}




\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{officeblue}{RGB}{0,102,204}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{mybrickred}{RGB}{182,50,28}
\newcommand\mybox[2][]{\tikz[overlay]\node[inner sep=1pt, anchor=text, rectangle, rounded corners=0mm,#1] {#2};\phantom{#2}}
\definecolor{fillcolor}{RGB}{216,217,252}
\newcommand\bg[1]{\mybox[fill=blue!20]{#1}}
\newcommand\rg[1]{\mybox[fill=red!20]{#1}}
\newcommand\graybox[1]{\mybox[fill=gray!20]{#1}}



\newcommand*\AlgCommentInLine[1]{{\color{deepblue}{ \textit{#1}}}}
 

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}\xspace}\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}\xspace}


\usepackage{pifont}\newcommand{\cmark}{{\color{blue}\ding{51}}}\newcommand{\xmark}{{\color{black}\ding{55}}}

\newcommand\our{\textsc{BEiT-3}}
\newcommand\vlmo{\textsc{VLMo}}
\newcommand\mome{\textsc{MoME}}
\newcommand\multiway{Multiway Transformers}
\newcommand\beit{\textsc{BEiT}}
\newcommand{\tblidx}[1]{{\small \texttt{[#1]}}}
\newcommand{\boxAP}{{AP}}
\newcommand{\maskAP}{{AP}}

\title{Image as a Foreign Language: \beit{} Pretraining for All Vision and Vision-Language Tasks}


\author{Wenhui Wang\thanks{~Equal contribution.  Corresponding author.}, ~~Hangbo Bao\footnotemark[1], ~~Li Dong\footnotemark[1], ~~Johan Bjorck,~~Zhiliang Peng,~~Qiang Liu \\\textbf{Kriti Aggarwal,}~~\textbf{Owais Khan Mohammed,}~~\textbf{Saksham Singhal,}~~\textbf{Subhojit Som,}~~\textbf{Furu Wei} \\
Microsoft Corporation \\
\url{https://aka.ms/beit-3}
}



\begin{document}

\maketitle

\vspace{-6mm}
\begin{abstract}
A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model \textbf{\our{}}, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce \multiway{} for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked ``language'' modeling on images ({\textbf{Imglish}}), texts (English), and image-text pairs (``parallel sentences'') in a unified manner. Experimental results show that \our{} obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).
\end{abstract}


\vspace{-5mm}
\begin{figure}[h]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.9\textwidth]{figure/radar_compare.pdf}
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{\our{} achieves state-of-the-art performance on a broad range of tasks compared with other customized or foundation models. I2T/T2I is short for image-to-text/text-to-image retrieval. 
}
\label{fig:radar_compare}
\end{figure}

\begin{table*}
\centering
\small
\begin{tabular}{@{}llllll@{}}
\toprule
\bf Category & \bf Task & \bf Dataset & \bf Metric & \bf Previous SOTA & \bf \our{} \\
\midrule
\multirow{3}{*}{Vision} & Semantic Segmentation & ADE20K & mIoU & 61.4 (FD-SwinV2) & \bf 62.8 (\textcolor{Green}{+1.4}) \\
\cmidrule{2-6}
& Object Detection & COCO & AP & 63.3 (DINO) & \bf 63.7
(\textcolor{Green}{+0.4}) \\
& Instance Segmentation & COCO & AP & 54.7 (Mask DINO) & \bf 54.8
(\textcolor{Green}{+0.1}) \\
\cmidrule{2-6}
& Image Classification & ImageNet & Top-1 acc. & 89.0 (FD-CLIP) & \bf 89.6 (\textcolor{Green}{+0.6}) \\
\midrule
\multirow{6}{*}{Vision-Language} & Visual Reasoning & NLVR2 & Acc. & 87.0 (CoCa) & \bf 92.6 (\textcolor{Green}{+5.6}) \\
\cmidrule{2-6}
& Visual QA & VQAv2 & VQA acc. & 82.3 (CoCa) & \bf 84.0 (\textcolor{Green}{+1.7}) \\
\cmidrule{2-6}
& Image Captioning & COCO & CIDEr & 145.3 (OFA) & \bf 147.6 (\textcolor{Green}{+2.3}) \\
\cmidrule{2-6}
& \multirow{2}{*}{Finetuned Retrieval} & COCO & \multirow{2}{*}{R@1} & 72.5 (Florence) & \bf 76.0 (\textcolor{Green}{+3.5}) \\
& & Flickr30K & & 92.6 (Florence) & \bf 94.2 (\textcolor{Green}{+1.6}) \\
\cmidrule{2-6}
& Zero-shot Retrieval & Flickr30K & R@1 & 86.5 (CoCa) & \bf 88.2 (\textcolor{Green}{+1.7}) \\
\bottomrule
\end{tabular}
\caption{Overview of \our{} results on various vision and vision-language benchmarks. 
We compare with previous state-of-the-art models, including FD-SwinV2~\citep{fd-swin}, DINO~\citep{dino-od}, Mask DINO~\citep{dino-od}, FD-CLIP~\citep{fd-swin}, CoCa~\citep{coca}, OFA~\citep{ofa}, Florence~\citep{florence}.
We report the average of top- image-to-text and text-to-image results for retrieval tasks.
``'' indicates ImageNet results only using publicly accessible resources.
``'' indicates image captioning results without CIDEr optimization.
}
\label{tab:presota_comparision}
\end{table*}



\section{Introduction: The Big Convergence}
\label{sec:intro}

Recent years have featured a trend toward the big convergence of language~\citep{gpt,bert,unilm}, vision~\citep{beit,beitv2}, and multimodal~\citep{vlmo,clip,coca} pretraining.
By performing large-scale pretraining on massive data, we can easily transfer the models to various downstream tasks.
It is appealing that we can pretrain a general-purpose foundation model that handles multiple modalities.
In this work, we advance the convergence trend for vision-language pretraining from the following three aspects.

First, the success of Transformers~\citep{transformer} is translated from language to vision~\citep{vit} and multimodal~\citep{vilt,vlmo} problems.
The unification of network architectures enables us to seamlessly handle multiple modalities.
For vision-language modeling, there are various ways to apply Transformers due to the different natures of downstream tasks.
For example, the dual-encoder architecture is used for efficient retrieval~\citep{clip}, encoder-decoder networks for generation tasks~\citep{simvlm}, and the fusion-encoder architecture for image-text encoding~\citep{vilt}.
However, most foundation models have to manually convert the end-task formats according to the specific architectures. Moreover, the parameters are usually not effectively shared across modalities.
In this work, we adopt \multiway{}~\citep{vlmo} for general-purpose modeling, i.e., one unified architecture shared for various downstream tasks. The modular network also comprehensively considers modality-specific encoding and cross-modality fusion.

Second, the pretraining task based on masked data modeling has been successfully applied to various modalities, such as texts~\citep{bert}, images~\citep{beit,beitv2}, and image-text pairs~\citep{vlbeit}.
Current vision-language foundation models usually multitask other pretraining objectives (such as image-text matching), rendering scaling-up unfriendly and inefficient.
In contrast, we only use one pretraining task, i.e., mask-then-predict, to train a general-purpose multimodal foundation model.
By regarding the image as a foreign language (i.e., \textit{Imglish}), we handle texts and images in the same manner without fundamental modeling differences.
Consequentially, image-text pairs are utilized as ``parallel sentences'' in order to learn the alignments between modalities.
We also show that the simple yet effective method learns strong transferable representations, achieving state-of-the-art performance on both vision and vision-language tasks.
The prominent success demonstrates the superiority of generative pretraining~\citep{bert,beit}.

Third, scaling up the model size and data size universally improves the generalization quality of foundation models, so that we can transfer them to various downstream tasks.
We follow the philosophy and scale up the model size to billions of parameters.
Moreover, we scale up the pretraining data size in our experiments while only using publicly accessible resources for academic reproducibility.
Although without using any private data, our method outperforms state-of-the-art foundation models that rely on in-house data by a decent margin.
In addition, the scaling up benefits from treating images as a foreign language, as we can directly reuse the pipeline developed for large-scale language model pretraining.

In this work, we take advantage of the above ideas to pretrain a general-purpose multimodal foundation model \our{}.
We pretrain a Multiway Transformer by performing masked data modeling on images, texts, and image-text pairs.
During pretraining, we randomly mask some proportion of text tokens or image patches. The self-supervised learning objective is to recover the original tokens (i.e., text tokens, or visual tokens) given corrupted inputs.
The model is general-purpose in the sense that it can be repurposed for various tasks regardless of input modalities, or output formats.

As shown in Figure~\ref{fig:radar_compare} and Table~\ref{tab:presota_comparision}, \our{} achieves state-of-the-art transfer performance across a broad range of vision and vision-language tasks.
We evaluate \our{} on extensive downstream tasks and datasets, i.e., object detection (COCO), instance segmentation (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).
Specifically, our model outperforms previous strong foundation models~\citep{coca,flamingo,florence} despite that we only use public resources for pretraining and finetuning.
The model also obtains better results than specialized models.
Moreover, \our{} not only performs well on vision-language tasks but also on vision tasks (such as object detection, and semantic segmentation).


\section{\our{}: A General-Purpose Multimodal Foundation Model}
\label{sec:methods}

\begin{figure}[t]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.86\textwidth]{figure/overview-cropped.pdf}
\end{tabular}
\end{center}
\caption{
Overview of \our{} pretraining.
We perform masked data modeling on monomodal (i.e., images, and texts) and multimodal (i.e., image-text pairs) data with a shared Multiway Transformer as the backbone network.
}
\label{fig:overview}
\end{figure}

\begin{figure}[t]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=\textwidth]{figure/multiway-cropped.pdf}
\end{tabular}
\end{center}
\caption{
\our{} can be transferred to various vision and vision-language downstream tasks.
With a shared Multiway Transformer, we can reuse the model as (a)(b) vision or language encoders; (c) fusion encoders that jointly encode image-text pairs for deep interaction; (d) dual encoders that separately encode modalities for efficient retrieval; (e) sequence-to-sequence learning for image-to-text generation.
}
\label{fig:multiway}
\end{figure}

As shown in Figure~\ref{fig:overview}, \our{} is pretrained by masked data modeling on monomodal and multimodal data, using a shared Multiway Transformer network.
The model can be transferred to various vision and vision-language downstream tasks.

\subsection{Backbone Network: \multiway{}}

We use \multiway{}~\citep{vlmo} as the backbone model to encode different modalities.
As shown in Figure~\ref{fig:overview}, each Multiway Transformer block consists of a shared self-attention module, and a pool of feed-forward networks (i.e., modality experts) used for different modalities.
We route each input token to the experts depending on its modality.
In our implementation, each layer contains a vision expert and a language expert. Moreover, the top three layers have vision-language experts designed for fusion encoders.
Refer to Figure~\ref{fig:multiway} (a)(b)(c) for more detailed modeling layouts.
Using a pool of modality experts encourages the model to capture more modality-specific information.
The shared self-attention module learns the alignment between different modalities and enables deep fusion for multimodal (such as vision-language) tasks.

As shown in Figure~\ref{fig:multiway}, the unified architecture enables \our{} to support a wide range of downstream tasks.
For example, \our{} can be used as an image backbone for various vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation.
It can also be finetuned as a dual encoder for efficient image-text retrieval, and a fusion model for multimodal understanding and generation tasks.


\subsection{Pretraining Task: Masked Data Modeling}

We pretrain \our{} via a unified masked data modeling~\citep{vlbeit} objective on monomodal (i.e., images, and texts) and multimodal data (i.e., image-text pairs).
During pretraining, we randomly mask some percentage of text tokens or image patches and train the model to recover the masked tokens.
The unified mask-then-predict task not only learns representations but also learns the alignment of different modalities.
Specifically, text data is tokenized by a SentencePiece tokenizer~\citep{sentencepiece}.
Image data is tokenized by the tokenizer of \beit{} v2~\citep{beitv2} to obtain the discrete visual tokens as the reconstructed targets.
We randomly mask \% tokens of monomodal texts and \% tokens of texts from image-text pairs.
For images, we mask \% of image patches using a block-wise masking strategy as in \beit{}~\citep{beit,beitv2}.

We only use one pretraining task, which makes the training process scaling-up friendly. In contrast, previous vision-language models~\citep{oscar,vinvl,vilt,albef,vlmo,blip,coca} usually employ multiple pretraining tasks, such as image-text contrast, image-text matching, and word-patch/region alignment.
We show that a much smaller pretraining batch size can be used with the mask-then-predict task.
In comparison, contrastive-based models~\cite{clip,align,florence,coca} usually need a very large batch size\footnote{For example, CoCa~\citep{coca} uses k batch size, CLIP~\citep{clip} uses k batch size, and Florence~\citep{florence} uses k batch size. \our{} uses a much smaller k batch size for pretraining.} for pretraining, which brings more engineering challenges, such as GPU memory cost.


\subsection{Scaling Up: \our{} Pretraining}

\paragraph{Backbone Network}

\our{} is a giant-size foundation model following the setup of ViT-giant~\citep{scaling:vit}.
As shown in Table~\ref{tab:model_config}, the model consists of a -layer Multiway Transformer with  hidden size,  intermediate size, and  attention heads.
All layers contain both vision experts and language experts.
Vision-language experts are also employed in the top three Multiway Transformer layers.
The self-attention module is shared across different modalities.
\our{} consists of B parameters in total, including M parameters for vision experts, M parameters for language experts, M parameters for vision-language experts, and M parameters for the shared self-attention module.
Notice that only vision-related parameters (i.e., comparable size as ViT-giant; about 1B) are activated when the model is used as a vision encoder.

\begin{table*}
\centering
\small
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multirow{2}{*}{\bf Model} & \multirow{2}{*}{\bf \#Layers} & \multirow{2}{*}{\bf \tabincell{c}{Hidden \\ Size}} & \multirow{2}{*}{\bf \tabincell{c}{MLP \\ Size}} & \multicolumn{5}{c}{\bf \#Parameters} \\
\cmidrule(lr){5-9}
 & & & & \bf V-FFN & \bf L-FFN & \bf VL-FFN & \bf Shared Attention & \bf Total \\
\midrule
\our{} & 40 & 1408 & 6144 & 692M & 692M & 52M & 317M & 1.9B \\
\bottomrule
\end{tabular}
\caption{Model configuration of \our{}. The architecture layout follows ViT-giant~\citep{scaling:vit}.}
\label{tab:model_config}
\end{table*}

\begin{table*}
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\bf Data & \bf Source & \bf Size \\
\midrule
Image-Text Pair & CC12M, CC3M, SBU, COCO, VG & 21M pairs \\
Image & ImageNet-21K & 14M images \\
Text & English Wikipedia, BookCorpus, OpenWebText, CC-News, Stories & 160GB documents \\
\bottomrule
\end{tabular}
\caption{Pretraining data of \our{}.
All the data are academically accessible.
}
\label{tab:pretraining_data}
\end{table*}


\paragraph{Pretraining Data}
\our{} is pretrained on both monomodal and multimodal data shown in Table~\ref{tab:pretraining_data}.
For multimodal data, there are about M images and M image-text pairs collected from five public datasets: Conceptual 12M (CC12M)~\citep{cc12m}, Conceptual Captions (CC3M)~\citep{gcc}, SBU Captions (SBU)~\citep{sbu}, COCO~\citep{coco} and Visual Genome (VG)~\citep{vg}.
For monomodal data, we use M images from ImageNet-21K and GB text corpora~\citep{unilm2} from English Wikipedia, BookCorpus~\citep{bookcorpus}, OpenWebText\footnote{\url{http://skylion007.github.io/OpenWebTextCorpus}}, CC-News~\citep{roberta}, and Stories~\citep{stories_data}.

\paragraph{Pretraining Settings}
We pretrain \our{} for M steps.
Each batch contains  samples in total, including  images,  texts and  image-text pairs.
The batch size is much smaller than contrastive models~\citep{clip,align,coca}.
\our{} uses  patch size and is pretrained at resolution .
We use the same image augmentation as in \beit{}~\citep{beit}, including random resized cropping, horizontal flipping, and color jittering~\citep{coloraug}.
A SentencePiece tokenizer~\citep{sentencepiece} with k vocab size is employed to tokenize the text data.
We use the AdamW~\citep{adamw} optimizer with ,  and 1e-6 for optimization.
We use a cosine learning rate decay scheduler with a peak learning rate of 1e-3 and a linear warmup of k steps.
The weight decay is .
Stochastic depth~\citep{drop_path} with a rate of  is used.
The BEiT initialization algorithm\footnote{We first randomly initialize the parameters within a small range, e.g., . Next, we rescale the -th Transformer layer's output matrices (i.e., the last linear projection within each sublayer) of self-attention and FFN by .}~\citep{beit} is used to stabilize Transformer training.


\section{Experiments on Vision and Vision-Language Tasks}
\label{sec:exps}

We extensively evaluate \our{} on major public benchmarks for both vision-language and vision tasks.
Table~\ref{tab:presota_comparision} presents the overview of results. 
\our{} obtains state-of-the-art performance on a wide range of vision and vision-language tasks.


\subsection{Vision-Language Downstream Tasks}

We evaluate the capabilities of \our{} on the widely used vision-language understanding and generation benchmarks, including visual question answering~\citep{vqa}, visual reasoning~\citep{nlvr2}, image-text retrieval~\citep{flickr30k,coco}, and image captioning~\citep{coco}.


\begin{table*}[t]
\centering
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multirow{2}{*}{\bf Model} & \multicolumn{2}{c}{\bf VQAv2} & \multicolumn{2}{c}{\bf NLVR2} & \multicolumn{4}{c}{\bf COCO Captioning} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9}
 & test-dev & test-std & dev & test-P & B@4 & M & C & S \\
\midrule
Oscar~\citep{oscar} & 73.61 & 73.82 & 79.12 & 80.37 & 37.4 & 30.7 & 127.8 & 23.5 \\
VinVL~\citep{vinvl} & 76.52 & 76.60 & 82.67 & 83.98 & 38.5 & 30.4 & 130.8 & 23.4 \\
ALBEF~\citep{albef} & 75.84 & 76.04 & 82.55 & 83.14 & - & - & - & - \\
BLIP~\citep{blip} & 78.25 & 78.32 & 82.15 & 82.24 & 40.4 & - & 136.7 & - \\
SimVLM~\citep{simvlm} & 80.03 & 80.34 & 84.53 & 85.15 & 40.6 & 33.7 & 143.3 & \bf 25.4 \\
Florence~\citep{florence} & 80.16 & 80.36 & - & - & - & - & - & - \\
OFA~\citep{ofa} & 82.00 & 82.00 & - & - & 43.9 & 31.8 & 145.3 & 24.8 \\
Flamingo~\citep{flamingo} & 82.00 & 82.10 & - & - & - & - & 138.1 & - \\
CoCa~\citep{coca} & 82.30 & 82.30 & 86.10 & 87.00 & 40.9 & \bf 33.9 & 143.6 & 24.7 \\
\midrule
\bf \our{} & \bf 84.19 & \bf 84.03 & \bf 91.51 & \bf 92.58 & \bf 44.1 & 32.4 & \bf 147.6 & \bf 25.4 \\
\bottomrule
\end{tabular}
\caption{Results of visual question answering, visual reasoning, and image captioning tasks.
We report \textit{vqa-score} on VQAv2 test-dev and test-standard splits, accuracy for NLVR2 development set and public test set (test-P).
For COCO image captioning, we report BLEU@4 (B@4), METEOR (M), CIDEr (C), and SPICE (S) on the Karpathy test split.
For simplicity, we report captioning results without using CIDEr optimization.
}
\label{tbl:results:vqa_nlvr2_captioning}
\end{table*}


\paragraph{Visual Question Answering (VQA)} 

The task requires the model to answer natural language questions about input images.
Following previous work~\citep{bottom_up_attn,vinvl,vilt}, we conduct finetuning experiments on the VQA v2.0 dataset~\citep{vqa} and formulate the task as a classification problem.
The model is trained to predict answers from the  most frequent answer candidates in the training set.
\our{} is finetuned as a fusion encoder to model deep interactions of images and questions for the VQA task.
We concatenate the embeddings of a given question and an image, and then feed the input embeddings into \multiway{} to jointly encode the image-question pair.
The final pooled output is fed into a classifier layer to predict the answer.
The results are present in Table~\ref{tbl:results:vqa_nlvr2_captioning}, \our{} outperforms all previous models by a large margin (more than  points), pushing the state of the art to  with a single model.

\paragraph{Visual Reasoning}
The task needs models to perform joint reasoning about images and natural language descriptions.
We evaluate the model on the popular NLVR2~\citep{nlvr2} benchmark, which is to determine whether a textual description is true about a pair of images.
Following previous work~\citep{vinvl,vilt}, we construct two image-text pairs based on the triplet input.
We finetune \our{} as a fusion encoder to jointly encode the image-text pairs.
The final pooled outputs of the two pairs are concatenated and then fed into a classifier layer to predict the label. 
As shown in Table~\ref{tbl:results:vqa_nlvr2_captioning}, \our{} achieves a new state-of-the-art result for visual reasoning, outperforming CoCa by about  points. 
The performance on NLVR2 reaches above \% for the first time.

\paragraph{Image Captioning}

The task aims to generate a natural language caption for the given image.
We use the COCO~\citep{coco} benchmark, finetune and evaluate the model on Karpathy split~\citep{karpathysplit}. 
Following \textsc{UniLM}~\citep{unilm} and s2s-ft~\citep{s2s-ft}, \our{} is used as a conditional generation model via masked finetuning.
To be more specific, a special self-attention mask is employed for the image captioning task.
Image tokens (i.e., image patches) can only attend to each other bidirectionally within the image sequence.
Tokens of the caption can attention to image tokens, their leftward caption tokens, and themselves.
During finetuning, we randomly mask some percentage of caption tokens.
The model is trained to recover these tokens based on the clues of the image and its leftward caption context.
We also mask the special boundary token \sptk{SEP} to help the model learn to terminate the generation.
For simplicity, \our{} is trained with simple cross-entropy loss, without using CIDEr optimization.
During inference, we generate the caption tokens one by one in an autoregressive manner.
Table~\ref{tbl:results:vqa_nlvr2_captioning} presents the results on COCO captioning.
\our{} outperforms all previous models trained with cross-entropy loss, creating a new state-of-the-art image captioning result.
The results demonstrate the superiority of \our{} for vision-language generation.

\paragraph{Image-Text Retrieval}


\begin{table*}[t]
\centering
\small
\begin{tabular}{@{}l@{\hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} | @{ \hskip2pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{} }
\toprule
\multirow{3}{*}{\bf Model} & \multicolumn{6}{c}{\bf MSCOCO (5K test set)} & \multicolumn{6}{c}{\bf Flickr30K (1K test set)} \\
 & \multicolumn{3}{c}{Image  Text} & \multicolumn{3}{c}{Text  Image} & \multicolumn{3}{c}{Image  Text} & \multicolumn{3}{c}{Text  Image} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
\multicolumn{13}{l}{\textit{Fusion-encoder models}} \\
UNITER~\citep{uniter} & 65.7 & 88.6 & 93.8 & 52.9 & 79.9 & 88.0 & 87.3 & 98.0 & 99.2 & 75.6 & 94.1 & 96.8 \\
VILLA~\citep{villa} & - & - & - & - & - & - & 87.9 & 97.5 & 98.8 & 76.3 & 94.2 & 96.8 \\
Oscar~\citep{oscar} & 73.5 & 92.2 & 96.0 & 57.5 & 82.8 & 89.8 & - & - & - & - & - & - \\
VinVL~\citep{vinvl} & 75.4 & 92.9 & 96.2 & 58.8 & 83.5 & 90.3 & - & - & - & - & - & - \\
\midrule
\multicolumn{13}{l}{\textit{Dual encoder + Fusion encoder reranking}} \\
ALBEF~\citep{albef} & 77.6 & 94.3 & 97.2 & 60.7 & 84.3 & 90.5 & 95.9 & 99.8 & \bf 100.0 & 85.6 & 97.5 & 98.9 \\
BLIP~\citep{blip} & 82.4 & 95.4 & 97.9 & 65.1 & 86.3 & 91.8 & 97.4 & 99.8 & 99.9 & 87.6 & 97.7 & 99.0 \\
\midrule
\multicolumn{13}{l}{\textit{Dual-encoder models}} \\
ALIGN~\citep{align} & 77.0 & 93.5 & 96.9 & 59.9 & 83.3 & 89.8 & 95.3 & 99.8 & \bf 100.0 & 84.9 & 97.4 & 98.6 \\
FILIP~\citep{filip} & 78.9 & 94.4 & 97.4 & 61.2 & 84.3 & 90.6 & 96.6 & \bf 100.0 & \bf 100.0 & 87.1 & 97.7 & 99.1 \\
Florence~\citep{florence} & 81.8 & 95.2 & - & 63.2 & 85.7 & - & 97.2 & 99.9 & - & 87.9 & 98.1 & - \\
\bf \our{} & \bf 84.8 & \bf 96.5 & \bf 98.3 & \bf 67.2 & \bf 87.7 & \bf 92.8 & \bf 98.0 & \bf 100.0 & \bf 100.0 & \bf 90.3 & \bf 98.7 & \bf 99.5 \\
\bottomrule
\end{tabular}
\caption{
Finetuning results of image-to-text retrieval and text-to-image retrieval on COCO and Flickr30K.
Notice that dual-encoder models are more efficient than fusion-encoder-based models for the retrieval tasks.
}
\label{tbl:results:finetuned_retrieval}
\end{table*}


\begin{table*}[t]
\centering
\begin{tabular}{@{}l@{\hskip4pt} @{ \hskip4pt}c@{\hskip4pt} @{\hskip4pt}c@{\hskip4pt} @{\hskip4pt}c@{\hskip4pt} @{\hskip4pt}c@{\hskip4pt} @{\hskip4pt}c@{\hskip4pt} @{\hskip4pt}c@{}}
\toprule
\multirow{3}{*}{\bf Model} & \multicolumn{6}{c}{\bf Flickr30K (1K test set)} \\
 & \multicolumn{3}{c}{Image  Text} & \multicolumn{3}{c}{Text  Image} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
FLAVA~\citep{flava} & 67.7 & 94.0 & - & 65.2 & 89.4 & - \\
CLIP~\citep{clip} & 88.0 & 98.7 & 99.4 & 68.7 & 90.6 & 95.2 \\
ALIGN~\citep{align} & 88.6 & 98.7 & 99.7 & 75.7 & 93.8 & 96.8 \\
FILIP~\citep{filip} & 89.8 & 99.2 & 99.8 & 75.0 & 93.4 & 96.3 \\
Florence~\citep{florence} & 90.9 & 99.1 & - & 76.7 & 93.6 & - \\
Flamingo~\citep{flamingo} & 89.3 & 98.8 & 99.7 & 79.5 & 95.3 & \bf 97.9 \\
CoCa~\citep{coca} & 92.5 & 99.5 & 99.9 & 80.4 & \bf 95.7 & 97.7 \\
\midrule
\bf \our{} & \bf 94.9 & \bf 99.9 & \bf 100.0 & \bf 81.5 & 95.6 & 97.8 \\
\bottomrule
\end{tabular}
\caption{
Zero-shot image-to-text retrieval and text-to-image retrieval on Flickr30K.
}
\label{tbl:results:zeroshot_retrieval}
\end{table*}

The task is to measure the similarity between images and texts.
There are two directions depending on the modality of the retrieved target: image-to-text retrieval, and text-to-image retrieval.
Two popular retrieval benchmarks, i.e., COCO~\citep{coco}, and Flickr30K~\citep{flickr30k}, are used to evaluate the model.
Following previous work~\citep{vinvl,vilt}, we use the Karpathy split~\citep{karpathysplit} for the two benchmarks.
\our{} is finetuned as a dual encoder for efficient image-text retrieval.
Dual-encoder models separately encode images and texts to obtain their representations. Then we calculate the cosine similarity scores of these representations.
Dual-encoder models are more efficient than fusion-encoder models. Because they do not have to jointly encode all possible image-text pairs.

We directly finetune \our{} on COCO and Flickr30K, although the model is not pretrained with image-text contrastive loss.
Surprisingly, \our{} outperforms previous state-of-the-art models only using a small amount of contrastive training.
The results demonstrate that \our{} effectively learns alignments between images and texts via masked data modeling. 
In order to improve the performance, we perform intermediate finetuning with an image-text contrastive objective on the pretraining image-text pairs.
We finetune the model with much fewer steps than pretraining.
Then we use the model to evaluate zero-shot and finetuned image-text retrieval.
The finetuned results are present in Table~\ref{tbl:results:finetuned_retrieval}, dual-encoder \our{} outperforms prior models by a large margin, achieving / absolute improvement on COCO top- image-to-text/text-to-image retrieval, and / absolute improvement on Flickr30K top- image-to-text/text-to-image retrieval.
\our{} also significantly outperforms fusion-encoder-based models, which require more computation cost for inference.
As present in Table~\ref{tbl:results:zeroshot_retrieval}, \our{} also achieves better performance than previous models on Flickr30K zero-shot retrieval.


\subsection{Vision Downstream Tasks}
\label{exp:vision}

In addition to vision-language downstream tasks, \our{} can be transferred to a wide range of vision downstream tasks, including object detection, instance segmentation, semantic segmentation, and image classification.
The number of effective parameters is comparable to ViT-giant~\citep{scaling:vit}, i.e., about 1B, when \our{} is used as a vision encoder.

\paragraph{Object Detection and Instance Segmentation} 

\begin{table*}[t]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{\bf Model} & \multirow{2}{*}{\bf Extra OD Data} & \multirow{2}{*}{\bf \tabincell{c}{Maximum \\ Image Size}} & \multicolumn{2}{c}{\bf COCO test-dev} \\
 & & & \boxAP{} & \maskAP{} \\
\midrule
ViT-Adapter~\citep{vit-adapter} & - & 1600 & 60.1 & 52.1 \\
DyHead~\citep{dyhead} & ImageNet-Pseudo Labels & 2000 & 60.6 & - \\
Soft Teacher~\citep{soft_teacher} & Object365 & - & 61.3 & 53.0 \\
GLIP~\citep{glip} & FourODs & - & 61.5 & - \\
GLIPv2~\citep{glipv2} & FourODs & - & 62.4 & - \\
Florence~\citep{florence} & FLOD-9M & 2500 & 62.4 & - \\
SwinV2-G~\citep{swinv2} & Object365 & 1536 & 63.1 & 54.4 \\
Mask DINO~\citep{mask_dino} & Object365 & 1280 & - & 54.7 \\
DINO~\citep{dino-od} & Object365 & 2000 & 63.3 & - \\
\midrule
\bf \our{} & Object365 & 1280 & \bf 63.7 & \bf 54.8 \\
\bottomrule
\end{tabular}
\caption{Results of object detection and instance segmentation on COCO benchmark.
\our{} uses Cascade Mask R-CNN~\citep{cascade-mask-rcnn} as the detection head.
Our results are reported with multi-scale evaluation.
We report the maximum image size used for training.
FLOD-9M and FourODs also contain Object365.
The results of the comparison systems are from the \href{https://paperswithcode.com/sota/object-detection-on-coco}{paperswithcode.com} leaderboard (timestamp: 08/22/2022).
}
\label{tbl:results:cocood}
\end{table*}

We conduct finetuning experiments on the COCO 2017 benchmark~\citep{coco}, which consists of k training, k validation, and k test-dev images.
We use \our{} as the backbone and follow ViTDet~\citep{vitdet}, including a simple feature pyramid and window attention, for the object detection and instance segmentation tasks.
Following common practices~\citep{swinv2,dino-od}, we first conduct intermediate finetuning on the Objects365~\citep{object365} dataset. Then we finetune the model on the COCO dataset.
Soft-NMS~\citep{soft-nms} is used during inference.
Table~\ref{tbl:results:cocood} compares \our{} with previous state-of-the-art models on COCO object detection and instance segmentation.
\our{} achieves the best results on the COCO test-dev set with a smaller image size used for finetuning, reaching up to  box AP and  mask AP.


\paragraph{Semantic Segmentation} 

\begin{table*}[t]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\multirow{2}{*}{\bf Model} & \multirow{2}{*}{\bf Crop Size} & \multicolumn{2}{c}{\bf ADE20K} \\
 & & mIoU & +MS \\
\midrule
HorNet~\citep{HorNet} &  & 57.5 & 57.9 \\
SeMask~\citep{jain2021semask} &  & 57.0 & 58.3 \\
SwinV2-G~\citep{swinv2} &  & 59.3 & 59.9 \\
ViT-Adapter~\citep{vit-adapter} &  & 59.4 & 60.5 \\
Mask DINO~\citep{mask_dino} & - & 59.5 & 60.8 \\
FD-SwinV2-G~\citep{fd-swin} &  & - & 61.4 \\
\midrule
\bf \our{} &  & \bf 62.0 & \bf 62.8 \\
\bottomrule
\end{tabular}
\caption[Caption protect]{Results of semantic segmentation on ADE20K.
``MS'' is short for multi-scale.
The results of the comparison systems are from the \href{https://paperswithcode.com/sota/semantic-segmentation-on-ade20k}{paperswithcode.com} leaderboard (timestamp: 08/22/2022).
}
\label{tbl:results:ade20k}
\end{table*}

Semantic segmentation aims to predict the label for each pixel of the given image.
We evaluate \our{} on the challenging ADE20K dataset~\citep{ade20k}, which includes  semantic categories.
ADE20K contains k images for training and k images for validation.
We directly follow the task transfer settings of ViT-Adapter~\citep{vit-adapter}.
We use a dense prediction task adapter and employ Mask2Former~\citep{mask2former} as the segmentation framework.
As shown in Table~\ref{tbl:results:ade20k}, \our{} creates a new state-of-the-art result with  mIoU, outperforming FD-SwinV2~\citep{fd-swin} giant model with 3B parameters by  points.
It shows that \our{} achieves superior performance on the dense prediction task.


\paragraph{Image Classification}

\begin{table*}[t]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\bf Model & \bf Extra Data & \bf Image Size & \bf ImageNet \\
\midrule
\multicolumn{4}{l}{~\textit{With extra \textbf{private} image-tag data}} \\
SwinV2-G~\citep{swinv2} & IN-22K-ext-70M &  & \textcolor{lightgray}{90.2} \\
ViT-G~\citep{scaling:vit} & JFT-3B &  & \textcolor{lightgray}{90.5} \\
CoAtNet-7~\citep{coatnet} & JFT-3B &  & \textcolor{lightgray}{90.9} \\
Model Soups~\citep{modelsoups} & JFT-3B &  & \textcolor{lightgray}{91.0} \\
CoCa~\citep{coca} & JFT-3B &  & \textcolor{lightgray}{91.0} \\
\midrule
\multicolumn{4}{l}{~\textit{With only \textbf{public} image-tag data}} \\
\beit{}~\citep{beit} & IN-21K &  & 88.6 \\
CoAtNet-4~\citep{coatnet} & IN-21K &  & 88.6 \\
MaxViT~\citep{maxvit} & IN-21K &  & 88.7 \\
MViTv2~\citep{mvitv2} & IN-21K &  & 88.8 \\
FD-CLIP~\citep{fd-swin} & IN-21K &  & 89.0 \\
\bf \our{} & IN-21K &  & \bf 89.6 \\
\bottomrule
\end{tabular}
\caption{Top-1 accuracy on ImageNet-1K.
}
\label{tbl:results:in1k}
\end{table*}


We evaluate the model on ImageNet-1K~\citep{imagenet}, which contains M training images and k validation images in k classes.
Rather than appending a task layer to the vision encoder~\citep{vit,beit}, we formulate the task as an image-to-text retrieval task.
We use the category names as texts to construct image-text pairs.
\our{} is trained as a dual encoder to find the most relevant label for an image.
During inference, we first compute the feature embeddings of possible class names and the feature embedding of the image.
Their cosine similarity scores are then calculated to predict the most probable label for each image.
Table~\ref{tbl:results:in1k} reports the results on ImageNet-1K. 
We first perform intermediate finetuning on ImageNet-21K, then we train the model on ImageNet-1K.
For a fair comparison, we compare with the previous models only using public image-tag data.
\our{} outperforms prior models, creating a new state-of-the-art result when only using public image-tag data. 


\section{Conclusion}

In this paper, we present \our{}, a general-purpose multimodal foundation model, which achieves state-of-the-art performance across a wide range of vision and vision-language benchmarks.
The key idea of \our{} is that image can be modeled as a foreign language, so that we can conduct masked ``language'' modeling over images, texts, and image-text pairs in a unified way.
We also demonstrate that \multiway{} can effectively model different vision and vision-language tasks, making it an intriguing option for general-purpose modeling.
\our{} is simple and effective, and is a promising direction for scaling up multimodal foundation models.
For future work, we are working on pretraining multilingual \our{} and including more modalities (e.g., audio) in \our{} to facilitate the cross-lingual and cross-modality transfer, and advance the big convergence of large-scale pretraining across tasks, languages, and modalities.
We are also interested in enabling in-context learning capability for multimodal foundation models by combining the strength of \our{} and MetaLM~\cite{metalm}.

\newpage

\bibliographystyle{alpha}
\bibliography{beit3}


\newpage
\appendix


\section{Effects of Intermediate Finetuning for Retrieval}

As shown in Table~\ref{tbl:results:finetuned_retrieval_woitcct}, we directly finetune \our{} on COCO and Flickr30K.
\our{} still outperforms previous state-of-the-art models, even without using image-text contrastive objective during pretraining.
The results demonstrate the effectiveness of masked data modeling for learning cross-modal representations.
Next, we perform intermediate finetuning on the pretraining image-text pairs for  epochs with a k batch size.
The peak learning is 3e-5, with linear warmup over the first epoch.
The image input size is .
The weight decay is set to .
We disable dropout as in pretraining and use drop path with a rate of .
The layer-wise learning rate decay is .
We use the AdamW~\citep{adamw} optimizer with , .

\begin{table*}[h]
\centering
\small
\begin{tabular}{@{}l@{\hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} | @{ \hskip2pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{ \hskip1pt} @{\hskip1pt}c@{} }
\toprule
\multirow{3}{*}{\bf Model} & \multicolumn{6}{c}{\bf MSCOCO (5K test set)} & \multicolumn{6}{c}{\bf Flickr30K (1K test set)} \\
 & \multicolumn{3}{c}{Image  Text} & \multicolumn{3}{c}{Text  Image} & \multicolumn{3}{c}{Image  Text} & \multicolumn{3}{c}{Text  Image} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
\our{} & 82.7 & 96.0 & 98.2 & 65.1 & 86.6 & 92.3 & 97.5 & 99.9 & 100.0 & 89.1 & 98.6 & 99.3 \\
~~+ Intermediate Finetuning & \bf 84.8 & \bf 96.5 & \bf 98.3 & \bf 67.2 & \bf 87.7 & \bf 92.8 & \bf 98.0 & \bf 100.0 & \bf 100.0 & \bf 90.3 & \bf 98.7 & \bf 99.5 \\
\bottomrule
\end{tabular}
\caption{Finetuning results of image-text retrieval on COCO and Flickr30K.
\our{} is directly finetuned on downstream benchmarks without intermediate finetuning on the pretraining data.
}
\label{tbl:results:finetuned_retrieval_woitcct}
\end{table*}


\section{Hyperparameters Used for Pretraining}


\begin{table}[H]
\centering
\small
\begin{tabular}{l|c}
\toprule
\bf Hyperparameters & \bf \our{} \\
\midrule
Layers & 40 \\
Hidden size & 1408 \\
FFN inner hidden size & 6144 \\
Attention heads & 16 \\
Patch size &  \\
Relative positional embeddings & \xmark \\
\midrule
Training steps & 1M \\
Batch size & 6144 \\
AdamW  & 1e-6 \\
AdamW  & (0.9, 0.98) \\
Peak learning rate & 1e-3 \\
Learning rate schedule & Cosine \\
Warmup steps & 10k \\
\midrule
Gradient clipping & 3.0 \\
Dropout & \xmark \\
Drop path & 0.1 \\
Weight decay & 0.05 \\
\midrule
Data Augment & RandomResizeAndCrop \\
Input resolution &  \\
Color jitter & 0.4 \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{
Hyperparameters for pretraining \our{}.
}
\label{tbl:pretrain:hyperparams}
\end{table}


\section{Hyperparameters Used for Finetuning}


\begin{table}[H]
\centering
\small
\begin{tabular}{l|cc}
\toprule
\bf Hyperparameters & \bf NLVR2 & \bf VQAv2 \\
\midrule
Peak learning rate & 1e-3 & 1e-5 \\
Fine-tuning epochs & 20  & 10 \\
Warmup epochs & 5 & 1 \\
Layer-wise learning rate decay & 0.8 & 1.0 \\
Batch size & 256 & 128 \\
AdamW  & \multicolumn{2}{c}{1e-8}  \\
AdamW  & \multicolumn{2}{c}{(0.9, 0.999)} \\
Weight decay & 0.05 & 0.01 \\
Drop path & \multicolumn{2}{c}{0.4} \\
Dropout & \multicolumn{2}{c}{\xmark} \\
Input resolution &  &  \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{
Hyperparameters for fine-tuning \our{} on NLVR2 and VQAv2.
}
\label{tbl:ft:vqa_nlvr2:hyperparams}
\end{table}


\begin{table}[H]
\centering
\small
\begin{tabular}{l|c}
\toprule
\bf Hyperparameters & \bf COCO Captioning \\
\midrule
Peak learning rate & 8e-6 \\
Fine-tuning steps & k \\
Warmup steps & 1600 \\
Layer-wise learning rate decay & 1.0 \\
Batch size & 256 \\
AdamW  & 1e-8  \\
AdamW  & (0.9, 0.999) \\
Weight decay & 0.01 \\
Drop path & 0.3 \\
Dropout & \xmark \\
Input resolution &  \\
Mask prob & 0.6 \\
Label smoothing  & 0.1 \\
Beam size & 3 \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{
Hyperparameters for fine-tuning \our{} on COCO captioning.
}
\label{tbl:ft:captioning:hyperparams}
\end{table}



\begin{table}[H]
\centering
\small
\begin{tabular}{l|cc}
\toprule
\bf Hyperparameters & \bf COCO & \bf Flickr30K \\
\midrule
Peak learning rate & \multicolumn{2}{c}{1e-5} \\
Fine-tuning epochs & 15 & 20 \\
Warmup epochs & 3 & 5 \\
Layer-wise learning rate decay & \multicolumn{2}{c}{0.95} \\
Batch size & \multicolumn{2}{c}{k} \\
AdamW  & \multicolumn{2}{c}{1e-8}  \\
AdamW  & \multicolumn{2}{c}{(0.9, 0.999)} \\
Weight decay & \multicolumn{2}{c}{0.05} \\
Drop path & \multicolumn{2}{c}{0.3} \\
Dropout & \multicolumn{2}{c}{\xmark} \\
Input resolution & \multicolumn{2}{c}{} \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{
Hyperparameters for fine-tuning \our{} on image-text retrieval.
}
\label{tbl:ft:retrieval:hyperparams}
\end{table}


\begin{table}[H]
\centering
\small
\begin{tabular}{l|c}
\toprule
\bf Hyperparameters & \bf ADE20K \\
\midrule
Peak learning rate & 1e-5 \\
Fine-tuning steps & 80k \\
Warmup steps & 1500 \\
Layer-wise learning rate decay & 0.95 \\
Batch size & 16 \\
AdamW  & 1e-8  \\
AdamW  & (0.9, 0.999) \\
Weight decay & 0.05 \\
Drop path & 0.5 \\
Dropout & \xmark \\
Input resolution &  \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{
Hyperparameters for fine-tuning \our{} on semantic segmentation.
}
\label{tbl:ft:semseg:hyperparams}
\end{table}


\begin{table}[H]
\centering
\small
\begin{tabular}{l|cc}
\toprule
\bf Hyperparameters & \bf  Object365 & \bf COCO \\
\midrule
Learning rate & 1e-4 & 5e-5 \\
Fine-tuning epochs & 15 & 20 \\
Warmup steps & \multicolumn{2}{c}{250} \\
Layer-wise learning rate decay & \multicolumn{2}{c}{0.9} \\
Batch size & \multicolumn{2}{c}{64} \\
AdamW  & \multicolumn{2}{c}{1e-8}  \\
AdamW  & \multicolumn{2}{c}{(0.9, 0.999)} \\
Weight decay & \multicolumn{2}{c}{0.1} \\
Drop path & \multicolumn{2}{c}{0.6} \\
Input resolution &  &  \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{
Hyperparameters for fine-tuning \our{} on object detection.
}
\label{tbl:ft:od:hyperparams}
\end{table}


\begin{table}[H]
\centering
\small
\begin{tabular}{l|cc}
\toprule
\bf Hyperparameters & \bf ImageNet-21K & \bf ImageNet-1K \\
\midrule
Peak learning rate & 5e-5 & 3e-5 \\
Fine-tuning epochs & 50 & 15 \\
Warmup epochs & 5 & 3 \\
Layer-wise learning rate decay & 0.85 & 0.95 \\
Batch size & k & k \\
AdamW  & 1e-6 & 1e-8  \\
AdamW  & (0.9, 0.98) & (0.9, 0.999) \\
Weight decay & \multicolumn{2}{c}{0.05} \\
Drop path & \multicolumn{2}{c}{0.4} \\
Dropout & \multicolumn{2}{c}{\xmark} \\
Input resolution &  &  \\
Label smoothing  & \multicolumn{2}{c}{0.1} \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{
Hyperparameters for fine-tuning \our{} on image classification.
}
\label{tbl:ft:imagenet:hyperparams}
\end{table}

\end{document}
