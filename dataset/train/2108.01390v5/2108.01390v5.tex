\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{multirow}
\usepackage{float} 
\usepackage{subfigure}

\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/Title (Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer)
/Author (Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, Xing Sun)
/Conference (AAAI2022)
}




\setcounter{secnumdepth}{0} 







\title{Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer}
\author{Paper ID: 1350}


\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\title{Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer}
\author{Yifan Xu\textsuperscript{\rm 1,3,4}\thanks{The first two authors contributed equally. 
}
, Zhijie Zhang\textsuperscript{\rm 2,3}, Mengdan Zhang\textsuperscript{\rm 3}, Kekai Sheng\textsuperscript{\rm 3}, Ke Li\textsuperscript{\rm 3}, Weiming Dong\textsuperscript{\rm 1,4}\thanks{The co-corresponding authors.}
, \\Liqing Zhang\textsuperscript{\rm 2}, Changsheng Xu\textsuperscript{\rm 1,4}, Xing Sun\textsuperscript{\rm 3}\\}
\affiliations{
\textsuperscript{\rm 1}Institute  of  Automation,  Chinese  Academy  of Sciences  
\textsuperscript{\rm 2}Shanghai Jiao Tong University  
\textsuperscript{\rm 3}Tencent Youtu Lab\\
\textsuperscript{\rm 4}School of Artificial Intelligence, University of Chinese Academy of Sciences\\
\{xuyifan2019, weiming.dong, changsheng.xu\}@ia.ac.cn, zzj506506@sjtu.edu.cn, zhang-lq@cs.sjtu.edu.cn,\\
\{davinazhang, saulsheng, tristanli\}@tencent.com, winfred.sun@gmail.com
}


\begin{document}
\maketitle


\begin{abstract}




Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 
1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers;
2) it usually requires a time-consuming pre-training procedure.
To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token  evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the  simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification. For example, our method accelerates DeiT-S by over 60 throughput while only sacrificing 0.4 top-1 accuracy on ImageNet-1K, outperforming current token pruning methods on both accuracy and efficiency.
Code is available at \textcolor{gray}{\url{https://github.com/YifanXu74/Evo-ViT}}.
\end{abstract}
 
\section{Introduction}
\label{sec:introduction}

Recently, vision transformers (ViTs) have shown strong power on various computer vision tasks~\cite{survey4}.
The reason of introducing the transformer into computer vision lies in its unique properties that convolution neural networks (CNNs) lack, especially the property of modeling long-range dependencies. However, dense modeling of long-range dependencies among image tokens brings computation inefficiency, because images contain large regions of low-level texture and uninformative background.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{Imgs/tech_pipline_v2_2.pdf} \caption{An illustration of technique pipelines for computation reduction via tokens. The dash lines denote iterative training. The first branch: the pipeline of unstructured token pruning~\cite{DynamicViT,PatchSlimming} based on pre-trained models. The second branch: the pipeline of structured compression~\cite{LeViT}. The third branch: our proposed pipeline that performs unstructured updating while suitable for structured compressed models.}
    \label{fig:tech_pipline}
    \vspace{-4mm}
\end{figure} 
Existing methods follow two pipelines to address the inefficiency problem of modeling long-range dependencies among tokens in ViT as shown in the above two pathways of Fig.~\ref{fig:tech_pipline}. The first pipeline, as shown in the second pathway, is to perform structured compression based on local spatial prior,
such as local linear projection~\cite{PVT}, convolutional projection~\cite{PiT}, and shift windows~\cite{SwinT}. Most modern transformers with deep-narrow structures are within this pipeline. However, the structured compressed models treat the informative object tokens and uninformative background tokens with the same priority.
Thus, token pruning, the second pipeline, proposes to identify and drop the uninformative tokens in an unstructured way. \cite{PatchSlimming} improves the efficiency of a pre-trained transformer network by developing a top-down layer-by-layer token slimming approach that can identify and remove redundant tokens based on the reconstruction error of the pre-trained network. The trained pruning mask is fixed for all instances. \cite{DynamicViT} proposes to accelerate a pre-trained transformer network by removing redundant tokens hierarchically, and explores an data-dependent down-sampling strategy via self-distillation.
Despite of the significant acceleration, these unstructured token pruning methods are restricted in two folds due to their incomplete spatial structure and information flow, namely, the inapplicability on structured compressed transformers and inability to train from scratch.


In this paper, as shown in the third pathway of Fig.~\ref{fig:tech_pipline}, we propose to handle the inefficiency problem in a dynamic data-dependent way from the very beginning of the training process while suitable for structured compression methods. We denote uninformative tokens that contribute little to the final prediction but bring computational cost when bridging redundant long-range dependencies as placeholder tokens. Different from structured compression that reduces local spatial redundancy in ~\cite{PVT, LeViT}, we propose to distinguish the informative tokens from the placeholder tokens for each instance in an unstructured and dynamic way, and update the two types of tokens with different computation paths. 
Instead of searching for redundancy and pruning in a pre-trained network such as \cite{PatchSlimming, DynamicViT}, by preserving placeholder tokens, 
the complete spatial structure and information flow can be maintained.
In this way, our method can be a generic plugin in most ViTs of both flat and deep-narrow structures from the very beginning of training.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{Imgs/vis_attn_mini.pdf} \caption{Visualization of class attention in DeiT-T. The \textit{interpretability} comes from ~\cite{Trans_interpretability}.}
    \label{fig:fig1}
    \vspace{-5mm}
\end{figure}

 
Concretely, 
Evo-ViT, a self-motivated slow-fast token evolution approach for dynamic ViTs is proposed in this work. ``Self-motivated'' means that transformers can naturally distinguish informative tokens from placeholder tokens for each instance, since they have insights into global dependencies among image tokens. 
Without loss of generality, we take DeiT~\cite{DeiT} in Fig.~\ref{fig:fig1} as example. We find that the class token of DeiT-T estimates importance of each token for dependency modeling and final classification. Especially in deeper layers (\emph{e.g.}, layer 10), the class token usually augments informative tokens with higher attention scores and has a sparse attention response, which is quite consistent to the visualization result provided by~\cite{Trans_interpretability} for transformer interpretability. In shallow layers (\emph{e.g.}, layer 5), the attention of the class token is relatively scattered but mainly focused on informative regions. Thus, taking advantage of class tokens, informative tokens and placeholer tokens are determined. The preserved placeholer tokens ensure complete information flow in shallow layers of a transformer for modeling accuracy. 
After the two kinds of tokens are determined, they are updated in a slow-fast approach. Specifically, the placeholder tokens are summarized to a representative token that is evolved via the full transformer encoder simultaneously with the informative tokens in a slow and elaborate way. Then, the evolved representative token is exploited to fast update the placeholder tokens for more representative features.












We evaluate the effectiveness of the proposed Evo-ViT method on two kinds of baseline models, namely, transformers of flat structures such as DeiT~\cite{DeiT} and transformers of deep-narrow structures such as LeViT~\cite{LeViT} on ImageNet~\cite{imagenet} dataset.  Our self-motivated slow-fast token evolution method allows
the DeiT model to improve inference throughput by 40\%-60\% and further accelerates the state-of-the-art efficient transformer LeViT while maintaining comparable performance. 

%
 
\section{Related Work}
\paragraph{Vision Transformer.}
Recently, a series of transformer models~\cite{survey1,survey4} are proposed to solve various computer vision tasks. The transformer has achieved promising success in image classification~\cite{ViT,DeiT,ConViT}, object detection~\cite{DETR,SwinT,deformableDETR} and instance segmentation~\cite{sstvos,SETR} due to its significant capability of modeling long-range dependencies. 
Vision Transformer (ViT)~\cite{ViT} is among the pioneering works that achieve state-of-the-art performance with large-scale pre-training. DeiT~\cite{DeiT} manages to tackle the data-inefficiency problem in ViT by simply adjusting training strategies and adding an additional token along with the class token for knowledge distillation. 
To achieve better accuracy-speed trade-offs for general dense prediction, recent works~\cite{T2T,LeViT,PVT} design transformers of deep-narrow structures by adopting sub-sampling operation (\emph{e.g.}, strided down-sampling, local average pooling, convolutional sampling) to reduce the number of tokens in intermediate layers.

\paragraph{Redundancy Reduction.}
Transformers take high computational cost because the multi-head self-attention (MSA) requires quadratic time complexity and the feed forward network (FFN) increases the dimension of latent features. 
The existing acceleration methods for transformers can be mainly categorized into sparse attention mechanism, knowledge distillation~\cite{distilbert}, and pruning.  The sparse attention mechanism includes, for example, low rank factorization~\cite{CoaT,Linformer}, fixed local patterns~\cite{SwinT}, and learnable patterns~\cite{Synthesizer,longformer}. \cite{PS-ViT-ICCV} handles the inefficiency problem by sparse input patch sampling.
\cite{Realformer} proposes to add an evolved global attention to the attention matrix in each layer for a better residual mechanism. Motivated by this work, we propose the evolved global class attention to guide the token selection in each layer.
The closest paradigm to this work is token pruning. \cite{PatchSlimming} presents a top-down layer-by-layer patch slimming algorithm to reduce the computational cost in pre-trained vision transformers. The patch slimming scheme is conducted under a careful control of the feature reconstruction error, so that the pruned transformer network can maintain the original performance with lower computational cost. \cite{DynamicViT} devises a lightweight prediction module to estimate the importance score of each token given the current features of a pre-trained transformer. The module is plugged into different layers to prune placeholder tokens in a unstructured way and is supervised by a distillation loss based on the predictions of the original pre-trained transformer. Different from these pruning works, we proposed to preserve the  placeholder tokens, and update the informative tokens and placeholder tokens with different computation paths; thus our method can achieve better performance and be suitable for various transformers due to the complete spatial structure. 
In addition, the complete information flow allows us to accelerate transformers  with scratch training.  
\section{Preliminaries}
\label{sec:background}


ViT~\cite{ViT} proposes a simple tokenization strategy that handles images by reshaping them into flattened sequential patches and linearly projecting each patch into latent embedding. An extra class token (CLS) is added to the sequence and serves as the global image representation. Moreover, since self-attention in the transformer encoder is position-agnostic and vision applications highly require position information, ViT adds position embedding into each token, including the CLS token. Afterwards, all tokens are passed through stacked transformer encoders and the CLS token is used for final classification.

The transformer is composed of a series of stacked encoders where each encoder consists of two modules, namely, a multi-head self-attention (MSA) module and a feed forward network (FFN) module. The FFN module  contains two linear transformations with an activation function. The residual connections are employed around both MSA and FFN modules, followed by layer normalization (LN).  Given the input  of ViT, the processing of the k-th encoder can be mathematically expressed as

where  and  are CLS and patch tokens respectively and  denotes the position embedding.  and  are the number of patch tokens and the dimension of the embedding.

Specifically, a self-attention (SA) module projects the input sequences into  query, key, value vectors (\emph{i.e.}, ) using three learnable linear mapping ,  and .  Then, a weighted sum over all values in the sequence is computed through:

MSA is an extension of SA. It splits queries, keys, and values for  times and performs the attention function in parallel, then linearly projects their concatenated outputs. 

It is worth noting that one very different design of ViT from CNNs is the CLS token. The CLS token interacts with patch tokens at each encoder and summarizes all the patch tokens for the final representation. We denote the similarity scores between the CLS token and patch tokens as class attention , formulated as:

where  is the query vector of the CLS token. 

\noindent\textbf{Computational complexity.}
In ViT, the computational cost of the MSA and FFN modules are  and , respectively. For pruning methods~\cite{DynamicViT, PatchSlimming}, by pruning  tokens, at least  FLOPS in the FFN and MSA modules can be reduced. Our method can achieve the same efficiency while suitable for scratch training and versatile downstream applications. 
\section{Methodology}
\label{sec:methodlogy}
\subsection{Overview}

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{Imgs/fig_method.pdf} \caption{The overall diagram of the proposed method.}
    \label{fig:method}
    \vspace{-4mm}
\end{figure} In this paper, we aim to handle the inefficiency modeling issue in each input instance from the very beginning of the training process of a versatile transformer. 
As shown in Fig~\ref{fig:method}, the pipeline of Evo-ViT mainly contains two parts: the structure preserving token selection module and the slow-fast token updating module. In the structure preserving token selection module, the informative tokens and the placeholder tokens are determined by the evolved global class attention, so that they can be updated in different manners in the following slow-fast token updating module. Specifically, the placeholder tokens are summarized and updated by a representative token. The long-term dependencies and feature richness of the representative token and the informative tokens are evolved via the MSA and FFN modules.  

We first elaborate on the proposed structure preserving token selection module. Then, we introduce how to update the informative tokens and the placeholder tokens in a slow-fast approach. Finally, the training details, such as the loss and other training strategies, are introduced.








\begin{figure}[t]
    \centering  \subfigure[Inter-layer]{
    \label{fig:cka_score}
    \includegraphics[width=0.43\linewidth]{Imgs/cka_similarity.pdf}}
    \subfigure[Intra-layer]{
    \label{fig:token_correlation}
    \includegraphics[width=0.53\linewidth]{Imgs/within_layer_correlation.pdf}}
    \caption{Two folds that illustrate the difficulty of pruning the shallow layers. (a) The CKA similarity between the final CLS token and token features in each layer. (b) The Pearson correlation coefficient of the token features in each layer.}
    \label{fig:cor_analysis}
    \vspace{-5mm}
\end{figure} 
\subsection{Structure preserving token selection}
\label{sec:cls_attn_analysis}
In this work, we propose to preserve all the tokens and dynamically distinguish informative tokens and placeholder tokens for complete information flow. The reason is that it is  not trivial to prune tokens in shallow and middle layers of a vision transformer, especially in the beginning of the training process. We explain this problem in both inter-layer and intra-layer ways. 
First, shallow and middle layers usually present fast growing capability of feature representation. Pruning tokens brings severe information loss. 
Following Refiner~\cite{Refiner}, we use centered kernel alignment (CKA) similarity~\cite{CKA} to measure similarity of the intermediate token features in each layer and the final CLS token, assuming that the final CLS token is strongly correlated with classification.  As shown in Fig.~\ref{fig:cka_score}, the token features of DeiT-T keep evolving fast when the model goes deeper and the final CLS token feature is quite different from token features in shallow layers. It indicates that the representations in shallow or middle layers are insufficiently encoded, which makes token pruning quite difficult. 
Second, tokens have low correlation with each other in the shallow layers.
We evaluate the Pearson correlation coefficient (PCC) among different patch token queries with respect to the network depth in the DeiT-S model to show 
redundancy. As shown in Fig.~\ref{fig:token_correlation}, the lower correlation with larger variance in the shallow layers also proves the difficulty to distinguish redundancy in shallow features. 


The attention weight is the easiest and most popular approach~\cite{Attention_rollout,EvolvingAttn} to interpret a model’s decisions and to gain insights about the propagation of information among tokens. The class attention weight described in Eqn.~\ref{eq:cls_attn} reflects the information collection and broadcast processes of the CLS token. We find that our proposed evolved global class attention is able to be a simple measure to help dynamically distinguish informative tokens and placeholder tokens in a vision transformer. In Fig.~\ref{fig:cka_score}, the distinguished informative tokens have high CKA correlation with the final CLS token, while the placeholder tokens have low CKA correlation.
As shown in Fig.~\ref{fig:fig1},  the global class attention is able to focus on the object tokens, which is consistent to the visualization results of~\cite{Trans_interpretability}. In the following part of this section, detailed introduction of our structure preserving token selection method is provided.





As discussed in Preliminaries Section, the class attention   is calculated by Eqn.~\ref{eq:cls_attn}. We select  tokens whose scores in the class attention are among the top  as the informative tokens. The remaining  tokens are recognized as placeholder tokens that contain less information. Different from token pruning, the placeholder tokens are kept and fast-updated rather than dropped.

For better capability of capturing the underlying information among tokens in different layers, we propose a global class attention that augments the class attention by evolving it across layers as shown in Fig.~\ref{fig:method}. Specifically, a residual connection between class attention of different layers is designed to facilitate the attention information flow with some regularization effects. Mathematically, 

where  is the global class attention in the k-th layer, and  is the class attention in the k-th layer. We use  for the token selection in the (k+1)-th layer for stability and efficiency. For each layer with token selection, only the global class attention scores of the selected informative tokens are updated.



\subsection{Slow-fast token updating}
Once the informative tokens and the placeholder tokens are determined by the global class attention, we propose to update tokens in a slow-fast way instead of harshly dropping placeholder tokens as \cite{PatchSlimming,DynamicViT}. As shown in Fig.~\ref{fig:method}, informative tokens are carefully evolved via MSA and FFN modules, while placeholder tokens are coarsely summarized and updated via a representative token. 
We introduce our slow-fast token updating strategy mathematically as follows.




For  patch tokens , we first split them into  informative tokens  and  placeholder tokens  by the above-mentioned token selection strategy. Then, the placeholder tokens  are aggregated into  a representative token  , as follows:

where  denotes an aggregating function, such as weighted sum or transposed linear projection~\cite{mlp-mixer}. Here we use weighted sum based on the corresponding global attention score in Eqn.~\ref{eq:global_attn}.






Then, both the informative tokens  and the representative token  are fed into MSA and FFN modules, and their residuals are recorded as  and  for skip connections, which can be denoted by:

Thus, the informative tokens  and the representative token  are updated in a slow and elaborate way.

Finally, the placeholder tokens  are updated in a fast way by the residuals of :

where  denotes an expanding function, such as simple copy in our method. 

It is worth noting that the placeholder tokens are fast updated by the residuals of  rather than the output features. In fact, the fast updating serves as a skip connection for the placeholder tokens.
By utilizing residuals, we can ensure the output features of the slow updating and fast updating modules within the same order of magnitude. 



\subsection{Training Strategies}




\noindent\textbf{Layer-to-stage training schedule.} 
Our proposed token selection mechanism becomes increasingly stable and consistent as the training process. Fig.~\ref{fig:prune_vis} shows that the token selection results of a well-trained transformer turn to be consistent across different layers;  thereby indicating that the transformer tends to augment informative tokens with computing resource as much as possible, namely the full transformer networks.
Thus, we propose a layer-to-stage training strategy for further efficiency. Specifically, we conduct the token selection and slow-fast token updating layer by layer at the first 200 training epochs. During the remaining 100 epochs, we only conduct token selection at the beginning of each stage, and then slow-fast updating is normally performed in each layer. For transformers with flat structure such as DeiT, we manually arrange four layers as one stage. 



\noindent\textbf{Assisted CLS token loss.}
Although many state-of-the-art vision transformers~\cite{PVT,LeViT} remove the CLS token and use the final average pooled features for classification, it is not difficult to add a CLS token in their models for our token selection strategy. We empirically find that the ability of distinguishing two types of tokens of the CLS token as illustrated in Fig.~\ref{fig:fig1} is kept in these models even without supervision on the CLS token. For better stability, we calculate classification losses based on the CLS token together with the final average pooled features during training. Mathematically, 

where  is the ground-truth of  and ;  denotes the transformer model;  is the classification metric function, usually realized by the cross-entropy loss.
During inference, the final average pooled features are used for classification and the CLS token is only used for token selection. 

%
 
\section{Experiments}
\label{sec:experiments}

\subsection{Setup}
In this section, we demonstrate the superiority of the proposed Evo-ViT approach through extensive experiments on the ImageNet-1k~\cite{imagenet} classification dataset. To demonstrate the generalization of our method, we conduct experiments on vision transformers of both flat and deep-narrow structures, \emph{i.e.}, DeiT~\cite{DeiT} and LeViT~\cite{LeViT}. Following ~\cite{LeViT}, we train LeViT with distillation and without batch normalization fusion.
We apply the position embedding in~\cite{PVT} to LeViT for better efficiency.
For overall comparisons with the state-of-the-art methods~\cite{DynamicViT,PatchSlimming,SViTE,IA-RED2}, we conduct the token selection and slow-fast token updating from the fifth layer of DeiT and 
the third layer (excluding the convolution layers) of LeViT, respectively. The selection ratios of informative tokens in all selected layers of both DeiT and LeViT are set to 0.5.
The global CLS attention trade-off  in Eqn.~\ref{eq:global_attn} are set to 0.5 for all layers.
For fair comparisons, all the models are trained for 300 epochs.

\begin{table}
    \centering
    \caption{Comparison with existing token pruning methods on DeiT. The image resolution is  unless specified.  denotes that the image resolution is .}
    \label{tab:prune_compare}
    \small{
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{l|ccc}
    \toprule
    \multirow{2}{*}{Method} & Top-1 Acc. & \multicolumn{2}{c}{Throughput} \\
     & (\%) & (img/s) & (\%) \\
    \midrule
    \multicolumn{4}{c}{DeiT-T} \\
    \midrule
    Baseline~\cite{DeiT} & 72.2 & 2536 & -  \\
    PS-ViT~\cite{PatchSlimming}     &      72.0       &        3563       &       40.5      \\
    DynamicViT~\cite{DynamicViT}    &      71.2       &        3890       &       53.4        \\
    SViTE~\cite{SViTE}              &      70.1       &          2836     &       11.8      \\
    \textbf{Evo-ViT (ours)}         &    72.0         & \textbf{4027}     &  \textbf{58.8}  \\
    \midrule
    \multicolumn{4}{c}{DeiT-S} \\
    \midrule
    Baseline~\cite{DeiT}            &      79.8       &                940        &       -         \\
PS-ViT~\cite{PatchSlimming}     &      79.4       &                1308       &       43.6      \\
    DynamicViT~\cite{DynamicViT}    &      79.3       &                 1479      &       57.3        \\
    SViTE~\cite{SViTE}              &      79.2       &                1215       &       29.3      \\
    IA-RED~\cite{IA-RED2}       &      79.1       &                 1360      &       44.7      \\
    \textbf{Evo-ViT (ours) }        &      79.4   &        \textbf{1510}      &  \textbf{60.6}      \\
\midrule
    \multicolumn{4}{c}{DeiT-B} \\
    \midrule
    Baseline~\cite{DeiT}            &      81.8       &                 299        &       -         \\
    Baseline~\cite{DeiT}      &      82.8       &                87        &       -         \\
    PS-ViT~\cite{PatchSlimming}     &      81.5       &                445        &       48.8      \\
    DynamicViT~\cite{DynamicViT}    &      80.8       &                454        &       51.8      \\
    SViTE~\cite{SViTE}              &      82.2   &                 421        &       40.8      \\
    IA-RED~\cite{IA-RED2}       &      80.9       &                453        &       42.9      \\
    IA-RED~\cite{IA-RED2}    &      81.9       &                129        &       51.5      \\
    \textbf{Evo-ViT (ours) }        &      81.3       &      \textbf{462}        &       \textbf{54.5} \\
    \textbf{Evo-ViT (ours)}   &      82.0       &                \textbf{139}        &       \textbf{59.8}      \\
    \bottomrule
    \end{tabular}
    }
    }
    \vspace{-3mm}
\end{table} 
\subsection{Main Results}
\noindent\textbf{Comparisons with existing pruning methods.}
In Table~\ref{tab:prune_compare}, we compare our method with existing token pruning methods~\cite{DynamicViT,IA-RED2,PatchSlimming,SViTE}. Since token pruning methods are unable to recover the 2D structure and are usually designed for transformers with flat structures, we comprehensively conduct the comparisons based on DeiT~\cite{DeiT} on ImageNet dataset. We report the top-1 accuracy and throughput for performance evaluation. The throughput is measured on a single NVIDIA V100 GPU with batch size fixed to 256, which is the same as the setting of DeiT. Results indicate that our method outperforms previous token pruning methods on both accuracy and efficiency. Our method accelerates the inference throughput by over 60 with negligible accuracy drop (-0.4) on DeiT-S.

\noindent\textbf{Comparisons with state-of-the-art ViTs.}
Owing to the preserved placeholder tokens, our method guarantees the spatial structure that is indispensable for most existing modern ViT networks. Thus, we further apply our method to the state-of-the-art efficient transformer LeViT~\cite{LeViT}, which presents a deep-narrow architecture. As shown in Table~\ref{tab:main_compare}, our method can further accelerate the deep-narrow transformer such as LeViT. We have observed larger accuracy degradation of our method on LeViT than on DeiT. The reason lies that the deeper layers of LeViT have few tokens and therefore have less redundancy due to the shrinking pyramid structure. With dense input, such as the image resolution of 384×384, our method accelerates LeViT with less accuracy degradation and more acceleration ratio, which indicates the effectiveness of our method on dense input.

\begin{table}[t]
    \center
    \caption{Comparison with state-of-the-art vision transformers. The input image resolution is  unless specified.  denotes that the image resolution is .}
    \label{tab:main_compare}
    \small{
    \setlength{\tabcolsep}{2.8mm}{
    \begin{tabular}{l|cccc}
    \toprule
    \multirow{2}{*}{Model}         & Param & Throughput & Top-1 Acc. \\
             & (M) & (img/s) & (\%) \\
    \midrule
LeViT-128S    &     7.8           &     8755   &     74.5      \\
    LeViT-128     &     9.2           &     6109   &     76.2      \\
    LeViT-192     &     10.9          &     4705   &     78.4      \\
    PVTv2-B1      &     14.0          &     1225   &     78.7      \\
    CoaT-Lite Tiny&     5.7           &     1083   &      76.6         \\
PiT-Ti    &     4.9           &      3030      &      73.0         \\ \midrule
\textbf{Evo-LeViT-128S} &     7.8           &     10135  &     73.0      \\
    \textbf{Evo-LeViT-128}  &     9.2           &     8323   &      74.4     \\
    \textbf{Evo-LeViT-192}  &     11.0          &    6148    &      76.8     \\ \midrule\midrule
LeViT-256     &     18.9          &    3357    &      80.1     \\
    LeViT-256&     19.0          &   906     &        81.8   \\
    PVTv2-B2      &     25.4          &   687     &      82.0         \\
PiT-S    &     23.5           &   1266      &      80.9         \\
    Swin-T      &    29.4          &       755     &     81.3      \\
    CoaT-Lite Small&     20.0         &     550      &      81.9         \\\midrule
\textbf{Evo-LeViT-256}  &     19.0          &    4277    &     78.8      \\ 
    \textbf{Evo-LeViT-256}  &     19.2          &    1285    &  81.1     \\ \midrule\midrule
LeViT-384     &     39.1          &     1838   &     81.6      \\
    LeViT-384 &     39.2          &   523     &     82.8      \\
    PVTv2-B3      &    45.2          &      457     &     83.2      \\
PiT-B    &     73.8           &    348       &      82.0         \\ \midrule
\textbf{Evo-LeViT-384}  &     39.3          &    2412    &     80.7      \\
    \textbf{Evo-LeViT-384}  &     39.6          &    712    &   82.2        \\
    \bottomrule
    \end{tabular}
    }
    }
    \vspace{-3mm}
\end{table}
%
 \subsection{Ablation Analysis}
\textbf{Effectiveness of each module.}
To evaluate the effectiveness of each sub-method, we add the following improvements step by step in Tab.~\ref{tab:method_ablation_deit} on transformers of both flat and deep-narrow structures, namely DeiT and LeViT:
\textit{i) Naive selection}. Simply drop the placeholder tokens based on the original class attention in each layer;
\textit{ii) Structure preservation}. Preserve the placeholder tokens but not fast update them;
\textit{iii) Global attention}. Utilize the proposed global class attention instead of vanilla class attention for token selection;
\textit{iv) Fast updating}. Augment the preserved placeholder tokens with fast updating;
\textit{v) Layer-to-stage}. Apply the proposed layer-to-stage training strategy to further accelerate inference.

Results on DeiT indicate that our structure preserving strategy further improves the selection performance due to its capacity of preserving complete information flow. The evolved global class attention enhances the consistency of token selection across layers and achieves better performance. The fast updating strategy has less effect on DeiT than on LeViT. We claim that the performance of DeiT turns to be saturated based on structure preservation and global class attention, while LeViT still has some space for improvement. LeViT exploits spatial pooling for token reduction, which makes unstructured token reduction in each stage more difficult. By using the fast updating strategy, it is possible to collect some extra cues from  placeholder tokens for accurate and augmented feature representations.
We also evaluate the layer-to-stage strategy. Results indicate that it further accelerates inference while maintaining accuracy. 
\begin{table}[t]
\centering
\caption{Method ablation on DeiT and LeViT.}
\label{tab:method_ablation_deit}
\setlength{\tabcolsep}{1mm}
\small{
\begin{tabular}{l|cc|cc}
\toprule
\multirow{3}{*}{Strategy} & \multicolumn{2}{c}{DeiT-T}           & \multicolumn{2}{|c}{LeViT 128S} \\
\cline{2-5}
 & Acc. & Throughput & Acc. & Throughput \\
 & (\%) & (img/s) & (\%) & (img/s) \\
\midrule
baseline                 & 72.2            & 2536               & 74.5            & 8755               \\
+ naive selection        & 70.8            & 3824               & -               & -                  \\
+ structure preservation & 71.6            & 3802               & 72.1            & 9892               \\
+ global attention       & 72.0            & 3730               & 72.5            & 9452               \\
+ fast updating          & 72.0            & 3610               & 73.0            & 9360               \\
+ layer-to-stage         & 72.0            & 4027               & 73.0            & 10135               \\
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\end{table} 




\begin{table}[t]
\centering
\caption{Different token selection strategies on DeiT-T. 
We conduct all sub-sampling methods at the seventh layer and conduct token selection strategies from the fifth layer.}
\label{tab:subsample_strategy_comparision}
\small{
\setlength{\tabcolsep}{2.4mm}{
\begin{tabular}{lcc}
\toprule
Method                 & Acc. (\%) & Throughput (img/s)  \\
\midrule
average pooling        &      69.5       &        3703         \\
max pooling            &      69.8       &        3698         \\
convolution            &      70.2       &        3688         \\ \midrule
random selection       &      66.4       &        3760         \\
last class attention   &      69.7       &        1694         \\
attention column mean  &      71.2       &        3596         \\       
global class attention &      72.0       &        3730         \\
\bottomrule
\end{tabular}
}
}
\vspace{-3mm}
\end{table} \begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Imgs/vis_prune2.pdf} \caption{Token selection results on DeiT-T. The left, middle, and right three columns denote the selection results on a well-trained Evo-ViT, the fifth layer at different training epochs, and Evo-ViT with the proposed layer-to-stage strategy, respectively.
}
    \label{fig:prune_vis}
    \vspace{-3mm}
\end{figure*} 






\noindent\textbf{Different Token Selection Strategy.}
We compare our global-attention-based token selection strategy with several common token selection strategies and sub-sampling methods in Tab.~\ref{tab:subsample_strategy_comparision} to evaluate the effectiveness of our method. All token selection strategies are conducted under our structure preserving strategy without layer-to-stage training schedule.
The token selection strategies include: randomly selecting the informative tokens (\textit{random selection}); Utilizing the class attention of the last layer for selection in all layers via twice inference (\textit{last class attention});  taking the column mean of the attention matrix as the score of each token as proposed in~\cite{kim2021learned} (\textit{attention column mean}).

Results in Tab.~\ref{tab:subsample_strategy_comparision} indicate that our evolved global class attention outperforms the other selection strategies and common sub-sampling methods on both accuracy and efficiency. We have observed obvious performance degradation with last class attention, although the attention in deeper layers is more focused on objects in Fig.~\ref{fig:fig1}. A possible reason is that the networks require some background information to assist classification, while restricting all layers to only focus on objects during the entire training process leads to underfitting on the background features.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{Imgs/ablation_figs_pr.pdf} \caption{Different architecture of the accelerated DeiT-S via our method. We start our token selection from the fifth layer.}
    \label{fig:abl_diff_pr}
    \vspace{-5mm}
\end{figure} 
\noindent\textbf{Visualization.}
We visualize the token selection in Fig.~\ref{fig:prune_vis} to demonstrate performance of our method during both training and testing stages. The visualized models in the left and middle three columns are trained without the layer-to-stage training strategy.
The left three columns demonstrate results on different layers of a well-trained DeiT-T model. 
Results show that our token selection method mainly focuses on objects instead of backgrounds, thereby indicating that our method can effectively discriminate the informative tokens from placeholder tokens. 
The selection results tend to be consistent across layers, which proves the feasibility of our layer-to-stage training strategy.
Another interesting finding is that some missed tokens in the shallow layers are retrieved in the deep layers owing to our structure preserving strategy. 
Take the baseball images as an example, tokens of the bat are gradually picked up as the layer goes deeper. 
This phenomenon is more obvious under our layer-to-stage training strategy in the right three columns. 
We also investigate how the token selection evolves during the training stage in the middle three columns. Results demonstrate that some informative tokens, such as the fish tail, are determined as placeholder tokens at the early epochs. With more training epochs, our method gradually turns to be stable for discriminative token selection.

\noindent\textbf{Consistent keeping ratio.} 
We set different keeping ratio of tokens in each layer to investigate the best acceleration architecture of Evo-ViT. The keeping ratio determines how many tokens are kept as informative tokens. Previous token pruning works~\cite{DynamicViT,PatchSlimming} present a gradual shrinking architecture, in which more tokens are recognized as placeholder tokens in deeper layers. They are restricted in this type of architecture due to direct pruning. Our method allows more flexible token selection owing to the structure preserving slow-fast token evolution. As shown in Fig.~\ref{fig:abl_diff_pr}, we maintain the sum of the number of placeholder tokens in all layers and adjust the keeping ratio in each layer. Results demonstrate that the best performance is reached with a consistent keeping ratio across all layers.
We explain the reason as follows. 
In the above visualization, we find that the token selection results tend to be consistent across layers, indicating that the transformer tends to augment informative tokens with computing resource as much as possible. 
In Fig.~\ref{fig:abl_diff_pr}, at most 50 tokens are passed through the full transformer network when the keeping ratios in all layers are set to , thereby augmenting the most number of informative tokens with the best computing resource, namely, the full transformer network. 
\section{Conclusions}
\label{sec:conclusions}
In this work, we investigate the efficiency of vision transformers by developing a self-motivated slow-fast token evolution (Evo-ViT) method. We propose the structure preserving token selection and slow-fast updating strategies to solve the limitation of token pruning on modern structured compressed transformers and scatch training. Extensive experiments on two popular ViT architectures, \emph{i.e.}, DeiT and LeViT, indicate that our Evo-ViT approach is able to accelerate various transformers significantly while maintaining comparable classification performance.

As for future work, an interesting and worthwhile direction is to extend our method to downstream tasks, such as object detection and instance segmentation. 
 
\section{Acknowledgements}
This work was supported by National Key R\&D Program of China under no. 2020AAA0106200, by Shanghai Municipal Science and Technology Key Project under no. 20511100300, by National Natural Science Foundation of China under nos. 62076162, U20B2070, 61832016 and 62036012, and by CASIA-Tencent Youtu joint research project.

\bibliography{Main}


\end{document}
