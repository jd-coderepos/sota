\appendix
\section{Experimental Results}
\label{appendix:results}

\subsection{Hyperparameter setting}

For reproduction, we show the detailed hyperparameter setting for each method in \tablename~\ref{tab-para-algo-depen} and \ref{tab-para-algo-inde}, for algorithm-dependent and algorithm-independent hyperparameters, respectively.

\begin{table}[!htbp]
    \centering
    \caption{Algorithm dependent parameters.}
    \label{tab-para-algo-depen}
    \begin{adjustbox}{width=0.8\columnwidth,center}
    \begin{tabular}{cccccc}\toprule
Algorithm &  PL (Flex-PL) & UDA (Flex-UDA) & FixMatch (FlexMatch) \\\cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(l){4-4}
Unlabeled Data to Labeled Data Ratio (CIFAR-10/100, STL-10, SVHN)    &  1 & 7 & 7 \\\cmidrule(r){1-1}
\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(l){4-4}
Unlabeled Data to Labeled Data Ratio (ImageNet)    &  - & - & 1 \\\cmidrule(r){1-1}
\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(l){4-4}
Pre-defined Threshold (CIFAR-10/100, STL-10, SVHN) &  0.95  & 0.8 & 0.95\\ \cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(l){4-4}
Pre-defined Threshold (ImageNet) &  -  & - & 0.7\\ \cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(l){4-4}
Temperature &  -  & 0.5 & - \\ 
\bottomrule
    \end{tabular}
    \end{adjustbox}
    
\end{table}

\begin{table}[!htbp]
    \centering
    \caption{Algorithm independent parameters.}
    \label{tab-para-algo-inde}
    \begin{adjustbox}{width=0.8\columnwidth,center}
    \begin{tabular}{cccccc}\toprule
Dataset &  CIFAR-10 & CIFAR-100 & STL-10 & SVHN & ImageNet \\\cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(l){6-6}
Model    &  WRN-28-2 \citep{zagoruyko2016wide} & WRN-28-8 & WRN-37-2~\cite{zhou2020time} & WRN-28-2 & ResNet-50~\citep{he2016deep} \\\cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(l){6-6}
Weight Decay&  5e-4  & 1e-3 & 5e-4 & 5e-4 & 3e-4\\ \cmidrule(r){1-1} \cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(l){6-6}
Batch Size & \multicolumn{4}{c}{64} & \multicolumn{1}{c}{128}\\\cmidrule(r){1-1} \cmidrule(lr){2-5} \cmidrule(l){6-6}
Learning Rate & \multicolumn{5}{c}{0.03}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
SGD Momentum & \multicolumn{5}{c}{0.9}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
EMA Momentum & \multicolumn{5}{c}{0.999}\\\cmidrule(r){1-1} \cmidrule(l){2-6}
Unsupervised Loss Weight & \multicolumn{5}{c}{1}\\
\bottomrule
    \end{tabular}
    \end{adjustbox}
    
\end{table}

\subsection{Class-wise accuracy improvement.} As introduced in the paper, CPL has its ability of improving performance on those hard-to-learn classes by taking into consider the model's learning status. A detailed class-wise accuracy comparison is listed in Table~\ref{tb-class-wise}, where the final accuracies of class 2, 3 and 5 with originally bad performance are improved. 

\begin{table}[h!]
\centering
\caption{Class-wise accuracy comparison on CIFAR-10 40-label split.}
\label{tb-class-wise}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{lcccccccccc}
\toprule
Class Number &0& 1 & 2 & 3&4&5&6&7&8&9 \\  \midrule
FixMatch&  0.964	&0.982&	\textbf{0.697}&	0.852&	0.974&	0.890&	0.987	&0.970 &	0.982 &	0.981   \\ 

FlexMatch &0.967&	0.980&	\textbf{0.921}&	0.866&	0.957 &	0.883&	0.988&	0.975&	0.982 &	0.968
   \\   \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Median error rates}
We also report the median error rates of the last 20 checkpoints by allowing all methods to run the same iterations, following existing work~\cite{sohn2020fixmatch}. There are 1000 iterations between every two checkpoints.
The results in \tablename~\ref{table:main median table} show that our CPL method can dramatically improve the performance of existing SSL algorithms and the \method achieves the best accuracy. These conclusions are in consistency with the results of \tablename 1 in the main text, showing the effectiveness of our proposed CPL algorithm.
\begin{table}[hbp!]
\centering
\caption{Median error rates of the last 20 checkpoints.}
\label{table:main median table}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{@{}l|rrr|rrr|rrr|rr@{}}
\toprule Dataset & \multicolumn{3}{c|}{CIFAR-10}& \multicolumn{3}{c|}{CIFAR-100}& \multicolumn{3}{c|}{STL-10} & \multicolumn{2}{c}{SVHN} \\ \cmidrule(r){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(l){11-12}
 \,Label Amount & \multicolumn{1}{c}{40} & \multicolumn{1}{c}{250}  & \multicolumn{1}{c|}{4000} & \multicolumn{1}{c}{400}  & \multicolumn{1}{c}{2500}  & \multicolumn{1}{c|}{10000} & \multicolumn{1}{c}{40}  & \multicolumn{1}{c}{250}   & \multicolumn{1}{c|}{1000} & \multicolumn{1}{c}{40} & \multicolumn{1}{c}{1000}\\ \cmidrule(r){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10} \cmidrule(l){11-12}
 
 \,PL  & 77.42{\scriptsize 1.19}  & 48.33{\scriptsize 2.43}  & 15.64{\scriptsize 0.29}  & 90.01{\scriptsize 0.21}  & 58.38{\scriptsize 0.42}  & 37.64{\scriptsize 0.16}  & \textbf{76.44}{\scriptsize 0.67}  & 56.90{\scriptsize 2.32}  & 33.57{\scriptsize 0.40}  & 69.05{\scriptsize 6.77}  & \textbf{9.99}{\scriptsize 0.35}  
\\
 \,Flex-PL  & \textbf{76.09}{\scriptsize 2.25}  & \textbf{47.53}{\scriptsize 2.25}  & \textbf{15.30}{\scriptsize 0.24}  & \textbf{86.60}{\scriptsize 0.48}  & \textbf{56.72}{\scriptsize 0.54}  & \textbf{36.20}{\scriptsize 0.20}  & 76.84{\scriptsize 1.04}  & \textbf{53.71}{\scriptsize 2.69}  & \textbf{33.19}{\scriptsize 0.25}  & \textbf{67.20}{\scriptsize 3.99}  & 15.10{\scriptsize 1.33}  
\\ \cmidrule(r){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10} \cmidrule(l){11-12}
 
 \,UDA    & 10.96{\scriptsize 3.68}  & \textbf{5.46}{\scriptsize 0.07}  & 4.60{\scriptsize 0.05}  & \textbf{51.97}{\scriptsize 1.38}  & 29.92{\scriptsize 0.35}  & 23.64{\scriptsize 0.33}  & \textbf{41.11}{\scriptsize 5.21}  & \textbf{10.74}{\scriptsize 1.39}  & 8.00{\scriptsize 0.58}  & \textbf{5.31}{\scriptsize 4.39}  & \textbf{1.97}{\scriptsize 0.04}  
\\
 \,Flex-UDA & \textbf{5.77}{\scriptsize 0.52}  & 5.48{\scriptsize 0.33}  & \textbf{4.52}{\scriptsize 0.07}  & 59.51{\scriptsize 2.70}  & \textbf{29.33}{\scriptsize 0.23}  & \textbf{23.38}{\scriptsize 0.19}  & 61.16{\scriptsize 4.34}  & 10.88{\scriptsize 0.54}  & \textbf{7.16}{\scriptsize 0.20}  & 6.21{\scriptsize 2.84}  & 2.13{\scriptsize 0.09}  
\\ \cmidrule(r){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10} \cmidrule(l){11-12}
 
 \,FixMatch & 7.99{\scriptsize 0.59}  & \textbf{5.12}{\scriptsize 0.33}  & \textbf{4.46}{\scriptsize 0.11}  & 48.95{\scriptsize 1.19}  & 29.19{\scriptsize 0.25}  & 23.06{\scriptsize 0.12}  & 44.70{\scriptsize 6.58}  & 12.34{\scriptsize 2.13}  & 7.38{\scriptsize 0.26}  & \textbf{3.92}{\scriptsize 1.18}  & \textbf{2.06}{\scriptsize 0.01}  
\\
 
 \,FlexMatch & \textbf{5.19}{\scriptsize 0.05}  & 5.33{\scriptsize 0.12}  & 4.47{\scriptsize 0.09}  & \textbf{45.91}{\scriptsize 1.76}  & \textbf{28.11}{\scriptsize 0.20}  & \textbf{23.04}{\scriptsize 0.28}  & \textbf{44.69}{\scriptsize 7.49}  & \textbf{9.27}{\scriptsize 0.49}  & \textbf{6.15}{\scriptsize 0.25}  & 20.81{\scriptsize 5.26}  & 12.90{\scriptsize 2.68}  
\\ \bottomrule
 
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Detailed results}
To comprehensively evaluate the performance of all methods in a classification setting, we further report the precision, recall, f1 score and AUC (area under curve) results on CIFAR-10 dataset. As shown in \tablename~\ref{tb-prf1}, we see that in addition to the reduced error rates, CPL also has the best performance on precision, recall, F1 score, and AUC. These metrics, together with error rates (accuracy), shows the strong performance of our proposed method.

\begin{table}[h]
\centering
\caption{Precision, recall, f1 score and AUC results on CIFAR-10. }
\begin{adjustbox}{width=\columnwidth,center}
\label{tb-prf1}
\begin{tabular}{l|cccc|cccc}
\toprule
Label Amount & \multicolumn{4}{c|}{40 labels}& \multicolumn{4}{c}{4000 labels}\\ \cmidrule(r){1-1} \cmidrule(lr){2-5} \cmidrule(l){6-9} 
Criteria    & \multicolumn{1}{c}{Precision}  &  \multicolumn{1}{c}{Recall}  &	\multicolumn{1}{c}{F1 Score}  &	\multicolumn{1}{c|}{AUC}  &\multicolumn{1}{c}{Precision}  &	\multicolumn{1}{c}{Recall}  &	\multicolumn{1}{c}{F1 score}  &	\multicolumn{1}{c}{AUC} \\
\cmidrule(r){1-1} \cmidrule(lr){2-5} \cmidrule(l){6-9} 

PL    & 0.2539  &  0.2552  &	0.2493  &	0.6542  &0.8498  &	0.8509  &	0.8500  &	0.9833\\

Flex-PL    & \textbf{0.2865}& \textbf{0.2865}	&\textbf{0.2663}	&\textbf{0.6718} &\textbf{0.8544}	&\textbf{0.8545}	&\textbf{0.8542}	&\textbf{0.9843}

 \\\cmidrule(r){1-1} \cmidrule(lr){2-5} \cmidrule(l){6-9} 

UDA    & 0.8759	&0.8408	&0.8086	&0.9775 &0.9557	&0.9559	&0.9557	&0.9985 

 \\

Flex-UDA    & \textbf{0.9482}	&\textbf{0.9485}	&\textbf{0.9482} 	&\textbf{0.9974} &\textbf{0.9576}	&\textbf{0.9577}	&\textbf{0.9576}	&\textbf{0.9986}

\\\cmidrule(r){1-1} \cmidrule(lr){2-5} \cmidrule(l){6-9} 

Fixmatch    & 0.9333	&0.9290	&0.9278	&0.9910 &0.9571	&0.9571	&0.9569	&\textbf{0.9984} 

\\
 
Flexmatch    & \textbf{0.9506}	&\textbf{0.9507}	&\textbf{0.9506}	&\textbf{0.9975} & \textbf{0.9580}	&\textbf{0.9581}	&\textbf{0.9580}	&\textbf{0.9984}

\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\section{\codebase: A PyTorch-based SSL Codebase}
\label{appendix:torchssl}



The PyTorch~\citep{paszke2019pytorch} framework has gained increasing attention in the deep learning research community.
However, the main existing SSL codebase~\citep{Sohn2020fixgit} is based on TensorFlow.
For the convenience and customizability, we re-implement and open source a PyTorch-based SSL toolbox, named \emph{\codebase} \footnote{Our toolbox is partially based on~\cite{Doyup2020git}.} as shown in \figurename~\ref{fig-torchssl}.
\codebase contains eight popular semi-supervised learning methods: -Model~\citep{rasmus2015semi}, Pseudo-Labeling~\citep{lee2013pseudo}, VAT~\citep{miyato2018virtual}, Mean Teacher~\citep{tarvainen2017mean},  MixMatch~\citep{berthelot2019mixmatch}, ReMixMatch~\citep{berthelot2019remixmatch}, UDA~\citep{xie2020unsupervised}, and FixMatch~\citep{sohn2020fixmatch}, along with our proposed method \method. Most of our implementation details are based on~\citep{Sohn2020fixgit}.
More importantly, in addition to the basic SSL methods and components, we implement several techniques to make the results stable under PyTorch framework. For instance, we add synchronized batch normalization~\cite{zhang2018context} to avoid the performance degradation caused by multi-GPU training with small batch size, and a batch norm controller to prevent performance crashes for some algorithms, which is not officially supported in PyTorch. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=.7\textwidth]{figures/fig-torchssl.pdf}
\caption{Components of \codebase.}
    \label{fig-torchssl}
\end{figure}








\subsection{BatchNorm Controller} 
We observed that Mean Teacher can be very unstable if we update BatchNorm for both labeled data and unlabeled data in turn. Other algorithms such as -Model and MixMatch also show the similar instability. Therefore, we use BatchNorm Controller to update BatchNorm only for labeled data if labeled data and unlabeled data are forwarded separately. The code of BatchNorm Controller is as follows. We record the BatchNorm statistics before the forward propagation of unlabeled data and restore them after the propagation is done.  









\subsection{Benchmark results}

We comprehensively run all algorithms in our \codebase on four common datasets in SSL: CIFAR-10, CIFAR-100, SVHN, and STL-10, and report the best error rates in \tablename~\ref{tb-benchmark-cifar10}, \ref{tb-benchmark-cifar100}, \ref{tb-benchmark-stl}, and \ref{tb-benchmark-svhn}, respectively.
These benchmark results provide a reference of using this toolbox.

\begin{table}[h!]
\centering
\caption{Benchmark results on CIFAR-10. The error bars are obtained from three trials. }
\label{tb-benchmark-cifar10}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{lrrr}
\toprule
Algorithms & Error Rate (40 labels) & Error Rate (250 labels) & Error Rate (4000 labels) \\  \cmidrule(r){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4}
-Model~\citep{rasmus2015semi}         &74.34{\scriptsize 1.76}     &46.24{\scriptsize 1.29}  &13.13{\scriptsize 0.59}    \\ 

Pseudo-Labeling~\citep{lee2013pseudo}     & 74.61{\scriptsize 0.26}     & 46.49{\scriptsize 2.20}    & 15.08{\scriptsize 0.19}   \\

VAT~\citep{miyato2018virtual} &74.66{\scriptsize 2.12}  &41.03{\scriptsize 1.79} &10.51{\scriptsize 0.12}   \\

Mean Teacher~\citep{tarvainen2017mean}    &70.09{\scriptsize 1.60}  &37.46{\scriptsize 3.30} &8.10{\scriptsize 0.21}    \\ 

MixMatch~\citep{berthelot2019mixmatch}      &36.19{\scriptsize 6.48} & 13.63{\scriptsize 0.59}  &6.66{\scriptsize 0.26}  \\ 

ReMixMatch~\citep{berthelot2019remixmatch}     &9.88{\scriptsize 1.03} &6.30{\scriptsize 0.05} &4.84{\scriptsize 0.01}       \\

UDA~\citep{xie2020unsupervised}                 & 10.62{\scriptsize 3.75}  & 5.16{\scriptsize 0.06}  & 4.29{\scriptsize 0.07}           \\

FixMatch~\citep{sohn2020fixmatch}            & 7.47{\scriptsize 0.28}  & 4.86{\scriptsize 0.05}  & 4.21{\scriptsize 0.08}           \\
FlexMatch           & 4.97{\scriptsize 0.06}  & 4.98{\scriptsize 0.09}  & 4.19{\scriptsize 0.01}           \\\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[h!]
\centering
\caption{Benchmark results on CIFAR-100.}
\label{tb-benchmark-cifar100}
\begin{adjustbox}{width=\columnwidth,center}

\begin{tabular}{lrrr}
\toprule
Algorithms & Error Rate (400 labels) & Error Rate (2500 labels) & Error Rate (10000 labels) \\  \cmidrule(r){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4}
-Model~\citep{rasmus2015semi}    &86.96{\scriptsize 0.80}  &58.80{\scriptsize 0.66} &36.65{\scriptsize 0.00}   \\ 

Pseudo-Labeling~\citep{lee2013pseudo}     & 87.45{\scriptsize 0.85}  & 57.74{\scriptsize 0.28}   & 36.55{\scriptsize 0.24}   \\

VAT~\citep{miyato2018virtual} &85.20{\scriptsize 1.40}  &46.84{\scriptsize 0.79} &32.14{\scriptsize 0.19}  \\

Mean Teacher\citep{tarvainen2017mean}  &81.11{\scriptsize 1.44} &45.17{\scriptsize 1.06} &31.75{\scriptsize 0.23}   \\ 

MixMatch~\citep{berthelot2019mixmatch}    &67.59{\scriptsize 0.66} &39.76{\scriptsize 0.48}  &27.78{\scriptsize 0.29}  \\ 

ReMixMatch~\citep{berthelot2019remixmatch}    &42.75{\scriptsize 1.05} &26.03{\scriptsize 0.35} &20.02{\scriptsize 0.27}    \\

UDA~\citep{xie2020unsupervised}      &46.39{\scriptsize 1.59} & 27.73{\scriptsize 0.21} & 22.49{\scriptsize 0.23}         \\

FixMatch~\citep{sohn2020fixmatch}    & 46.42{\scriptsize 0.82}   & 28.03{\scriptsize 0.16} &22.20{\scriptsize 0.12}  \\
FlexMatch           & 39.94{\scriptsize 1.62}  & 26.49{\scriptsize 0.20}  & 21.90{\scriptsize 0.15}      \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[h!]
\centering
\caption{Benchmark results on STL-10. }
\label{tb-benchmark-stl}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{lrrr}
\toprule
Algorithms & Error Rate (40 labels) & Error Rate (250 labels) & Error Rate (1000 labels) \\  \cmidrule(r){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4}
-Model~\citep{rasmus2015semi}   &74.31{\scriptsize 0.85}    &55.13{\scriptsize 1.50}  &32.78{\scriptsize 0.40}    \\ 

Pseudo-Labeling~\citep{lee2013pseudo}  & 74.68{\scriptsize 0.99}   & 55.45{\scriptsize 2.43}   & 32.64{\scriptsize 0.71}  \\

VAT~\citep{miyato2018virtual}&74.74{\scriptsize 0.38}   &56.42{\scriptsize 1.97} &37.95{\scriptsize 1.12}  \\

Mean Teacher~\citep{tarvainen2017mean}  &71.72{\scriptsize 1.45} &56.49{\scriptsize 2.75} &33.90{\scriptsize 1.37}   \\ 

MixMatch~\citep{berthelot2019mixmatch}    &54.93{\scriptsize 0.96} &34.52{\scriptsize 0.32}  &21.70{\scriptsize 0.68}  \\ 

ReMixMatch~\citep{berthelot2019remixmatch} &32.12{\scriptsize 6.24} &12.49{\scriptsize 1.28} &6.74{\scriptsize 0.14}     \\

UDA~\citep{xie2020unsupervised} &    37.42{\scriptsize 8.44}  & 9.72{\scriptsize 1.15}  & 6.64{\scriptsize 0.17}       \\

FixMatch~\citep{sohn2020fixmatch} &     35.97{\scriptsize 4.14} & 9.81{\scriptsize 1.04} & 6.25{\scriptsize 0.33}   \\

FlexMatch           & 29.15{\scriptsize 4.16}  & 8.23{\scriptsize 0.39}  & 5.77{\scriptsize 0.18}      \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\begin{table}[t!]
\centering
\caption{Benchmark results on SVHN. }
\label{tb-benchmark-svhn}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{lrrr}
\toprule
Algorithms & Error Rate (40 labels) & Error Rate (250 labels) & Error Rate (1000 labels) \\  \cmidrule(r){1-1} \cmidrule(l){2-2} \cmidrule(l){3-3} \cmidrule(l){4-4}
-Model~\citep{rasmus2015semi}  &67.48{\scriptsize 0.95}  &13.30{\scriptsize 1.12} &7.16{\scriptsize 0.11}   \\ 

Pseudo-Labeling~\citep{lee2013pseudo}  & 64.61{\scriptsize 5.60} & 15.59{\scriptsize 0.95} & 9.40{\scriptsize 0.32}  \\

VAT~\citep{miyato2018virtual}&74.75{\scriptsize 3.38}  &4.33{\scriptsize 0.12} &4.11{\scriptsize 0.20}  \\

Mean Teacher~\citep{tarvainen2017mean} &36.09{\scriptsize 3.98} &3.45{\scriptsize 0.03} &3.27{\scriptsize 0.05}  \\ 

MixMatch~\citep{berthelot2019mixmatch}   &30.60{\scriptsize 8.39} &4.56{\scriptsize 0.32}  &3.69{\scriptsize 0.37}  \\ 

ReMixMatch~\citep{berthelot2019remixmatch} &24.04{\scriptsize 9.13} &6.36{\scriptsize 0.22} &5.16{\scriptsize 0.31}    \\

UDA~\citep{xie2020unsupervised} & 5.12{\scriptsize 4.27} &1.92{\scriptsize 0.05} &1.89{\scriptsize 0.01}    \\

FixMatch~\citep{sohn2020fixmatch} & 3.81{\scriptsize 1.18} &2.02{\scriptsize 0.02}& 1.96{\scriptsize 0.03}      \\
FlexMatch           & 8.19{\scriptsize 3.20}  & 6.59{\scriptsize 2.29}  & 6.72{\scriptsize 0.30}   \\   \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


















\paragraph{}






































