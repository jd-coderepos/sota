\documentclass[a4paper,twocolumn,11pt]{article}

\usepackage[left=2cm,right=2cm]{geometry}

\usepackage{amsfonts,amsmath}
\usepackage{verbatim}
\usepackage{listings}

\usepackage{blkarray}
\makeatletter
\let\BA@quicktrue\BA@quickfalse
\makeatother

\usepackage{dcolumn}
\usepackage{rotating}
\usepackage{graphicx}


\usepackage[hyperref=auto]{biblatex}
\usepackage{dmbiblatex}
\bibliography{lctes25-henry}

\usepackage[inline]{enumitem}


\usepackage{tikz}
\usetikzlibrary{arrows,shadows,shapes,backgrounds,fit,chains}
\usepackage{color}
\usepackage{hyperref}
\usepackage{cleveref}


\lstset{language=C,basicstyle={\fontfamily{ppl}\selectfont\small},mathescape=true}

\lstdefinelanguage{smtlib2}
{morekeywords={assert,check-sat,get-model,declare-fun,and,or,ite,not,true,false,Int,Bool,Real},
sensitive=false,
morecomment=[l]{;},
morestring=[b]",
}

\lstdefinelanguage{llvm}
{morekeywords={},
sensitive=false,
morecomment=[l]{;},
morestring=[b]",
}
\lstnewenvironment{llvm}
{\lstset{language=llvm,
		basicstyle=\fontfamily{ppl}\small,
		commentstyle=\textit,
		keywordstyle=\textbf,
		showstringspaces=false}}
{}


\newcommand{\mytitle}{How to Compute Worst-Case Execution Time by Optimization Modulo Theory and a Clever Encoding of Program Semantics}
\title{\mytitle{\renewcommand*{\thefootnote}{\fnsymbol{footnote}}\footnotemark}}

\author{Julien Henry \and Mihail Asavoae \and David Monniaux \and Claire Ma\"iza}

\hypersetup{pdftitle={\mytitle},pdfauthor={Julien Henry, Mihail Asavoae, David Monniaux, Claire Ma\"iza}}


\newcommand{\ite}{\mathit{ite}}
\newcommand\true{\mathit{true}}
\newcommand\false{\mathit{false}}
\newcommand{\NN}{\mathbb{N}}

\renewcommand{\figureautorefname}{Fig.}
\renewcommand{\equationautorefname}{Formula}

\newcommand{\RegTM}{\texttrademark}

\sloppy

\begin{document}
\maketitle
 
{\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{The research leading to these results has received funding from the French \emph{Agence nationale de la recherche}, grant \href{http://wsept.inria.fr/}{W-SEPT} (ANR-12-INSE-0001), and from from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007--2013) / ERC grant agreement 306595 ``\href{http://stator.imag.fr/}{STATOR}''.

This article was also published in the proceedings of the 2014 ACM SIGPLAN Conference on Languages, Compilers and Tools for Embedded Systems (LCTES).}}


\begin{abstract}
In systems with hard real-time constraints, it is necessary to compute upper bounds on the worst-case execution time (WCET) of programs; the closer the bound to the real WCET, the better.
This is especially the case of synchronous reactive control loops with a fixed clock; the WCET of the loop body must not exceed the clock period.

We compute the WCET (or at least a close upper bound thereof) as the solution of an \emph{optimization modulo theory} problem that takes into account the semantics of the program, in contrast to other methods that compute the longest path whether or not it is feasible according to these semantics.
Optimization modulo theory extends satisfiability modulo theory (SMT) to maximization problems.

Immediate encodings of WCET problems into SMT yield formulas intractable for all current production-grade solvers --- this is inherent to the DPLL(T) approach to SMT implemented in these solvers.
By conjoining some appropriate ``cuts'' to these formulas, we considerably reduce the computation time of the SMT-solver.

We experimented our approach on a variety of control programs, using the OTAWA analyzer both as baseline and as underlying microarchitectural analysis for our analysis, and show notable improvement on the WCET bound on a variety of benchmarks and control programs.
 \end{abstract}

\section{Introduction}\label{sec:intro}
In embedded systems, it is often necessary to ascertain that the worst-case execution time (WCET) of a program is less than a certain threshold.
This is in particular the case for synchronous reactive control loops (infinite loops that acquire sensor values, compute appropriate actions and update, write them to actuators, and wait for the next clock tick) \parencite{Caspi_et_al_Synchronous_2008}: the WCET of the loop body (``step'') must be less than the period of the clock.

Computing the WCET of a program on a modern architecture requires a combination of low-level, microarchitectural reasoning (regarding pipeline and cache states, busses, cycle-accurate timing) and higher-level reasoning (program control flow, loop counts, variable pointers).
A common approach is to apply a form of abstract interpretation to the microarchitecture, deduce worst-case timings for elementary blocks, and reassemble these into the global WCET according to the control flow and maximal iteration counts using integer linear programming (ILP) \parencite{Theiling_et_al_2000,wilhelm-et-TECS08}.


One pitfall of this approach is that the reassembly may take into account paths that cannot actually occur in the real program, possibly overestimating the WCET.
This is because this reassembly is mostly driven by the control-flow structure of the program, and (in most approaches) ignores semantic conditions.
For instance, a control program may (clock-)enable certain parts of the program according to modular arithmetic with respect to time:
\begin{lstlisting}
if (clock /* unrelated code */
if (clock \end{lstlisting}
These arithmetic constraints entail that certain combinations of parts cannot be active simultaneously (sections  and  are mutually incompatible).
If such constraints are not taken into account (as in most approaches), the WCET will be grossly over-estimated.

The purpose of this article is to take such \emph{semantic constraints} into account, in a fully automated and very precise fashion.
Specifically, we consider the case where the program for which WCET is to be determined contains only loops for which small static bounds can be determined (but our approach can also be applied to general programs through summarization, see \autoref{sec:extensions}).
This is very commonly the case for synchronous control programs, such as those found in aircraft fly-by-wire controls \parencite{Souyris_et_al_FM2009}.
Programs of this form are typically compiled into C from high-level data-flow synchronous programming languages such as \textsc{Simulink}\footnote{\textsc{Simulink}{\RegTM} is a block diagram environment for multidomain simulation and model-based design from The Mathworks.},
\textsc{Lustre} or
\textsc{Scade}\footnote{\textsc{Scade}{\RegTM} is a model-based development environment dedicated to critical embedded software, from Esterel Technologies, derived from the academic language \textsc{Lustre}.}
\parencite{Caspi_et_al_Synchronous_2008}.

We compute the WCET of such programs by expressing it as the solution of an \emph{optimization modulo theory} problem. Optimization modulo theory is an extension of \emph{satisfability modulo theory} (SMT) where the returned solution is not just any solution, but one maximizing some objective; in our case, solutions define execution traces of the program, and the objective is their execution time.

Expressing execution traces of programs as solutions to an SMT problem is a classical approach in \emph{bounded model checking}; typically, the SMT problem includes a constraint stating that the execution trace reaches some failure point, and an ``unsatisfiable'' answer means that this failure point is unreachable.
In the case of optimization, the SMT solver has to disprove the existence of solutions greater than the maximum to be returned --- in our case, to disprove the existence of traces of execution time greater than the WCET.
Unfortunately, all currently available SMT solvers take unacceptably long time to conclude on naive encodings of WCET problems.
This is because all these solvers implement variants of the DPLL(T) approach \parencite{Kroening_Strichman}, which has exponential behavior on so-called ``diamond formulas'', which appear in naive encodings of WCET on sequences of if-then-elses.

Computing or proving the WCET by direct, naive encoding into SMT therefore leads to intractable problems, which is probably the reason why, to our best knowledge, it has not been proposed in the literature.
We however show how an alternate encoding, including ``cuts'', makes such computations tractable.
Our contributions are:

\begin{enumerate}
\item The computation of worst-case execution time (WCET), or an over-approximation thereof, by optimization modulo theory. The same idea may also be applicable to other similar problems (e.g. number of calls to a memory allocator).
Our approach exhibits a worst-case path, which may be useful for targeting optimizations so as to lower WCET~\parencite{DBLP:journals/rts/ZhaoKWHM06}.
\item The introduction of ``cuts'' into the encoding so as to make SMT-solving tractable, without any change in the code of the SMT solver. The same idea may extend to other problems with an additive or modular structure.
\end{enumerate}

In \autoref{sec:wcet}, we recall the usual approach for the computation of an upper bound on WCET. In \autoref{sec:bmc}, we recall the general framework of bounded model checking using SMT-solving. In \autoref{sec:cuts}, we explain how we improve upon the ``normal'' SMT encoding of programs so as to make WCET problems tractable, and in \autoref{sec:intractable} we explain (both theoretically and practically) why the normal encoding results in intractable problems. In \autoref{sec:implementation} we describe our implementation and experimental results. We present the related work in section~\ref{sec:related}, we discuss possible extensions and future works in \autoref{sec:extensions}, and then, in \autoref{sec:conclusion} we draw the conclusions.

\section{Worst-Case Execution Time}
\label{sec:wcet}

\newcommand{\uAA}{Micro-architectural Analysis}
\tikzstyle{data}=[fill=black!3]
\tikzstyle{phase}=[fill=blue!7]
\tikzstyle{WCETall}  =[draw, text centered, text width=20mm, rounded corners=3pt]
\tikzstyle{WCETdata} =[WCETall, drop shadow, ellipse, inner sep=1pt, data]
\tikzstyle{WCETphase}=[WCETall, drop shadow, rectangle, phase]
\tikzstyle{legend}   =[rectangle, draw, rounded corners=1pt, text width=20mm]


\begin{figure}\begin{center}
\scalebox{.6}{\begin{tikzpicture}[node distance=13mm, thick]
\node[WCETdata]  (BIN)                               {Input Binary};
\node[WCETphase] (CFGR) [below of=BIN]               {CFG Reconstruction};
\node[WCETdata]  (CFG)  [below of=CFGR]              {Control-flow Graph};
\node[WCETphase] (LBA)  [below of=CFG, yshift=-3mm]  {Loop Bound Analysis};
\node[WCETphase] (VA)   [left of =LBA, xshift=-12mm] {Value Analysis};
\node[WCETphase] (IPA)  [right of=LBA, xshift=+12mm] {Control-flow Analysis};
\node[WCETdata]  (ACFG) [below of=LBA, yshift=-3mm]  {Annotated CFG};
\node[WCETdata]  (BBT)  [below of=ACFG]              {Basic Block Timing Info};
\node[WCETphase] (uAA)  [left of =BBT, xshift=-15mm] {\uAA};
\node[WCETphase] (WPA)  [right of=BBT, xshift=+15mm] {Path\\Analysis};

\begin{pgfonlayer}{background}
    \node[WCETall, inner sep=4pt, fit=(VA) (LBA) (IPA), fill opacity=0.2] (MA) {};
\end{pgfonlayer}

\draw[->] (BIN)  -- (CFGR);
\draw[->] (CFGR) -- (CFG);
\draw[->] (CFG)  -- (MA);
\draw[->] (MA)   -- (ACFG);
\draw[->] (ACFG) -- (uAA);
\draw[->] (ACFG) -- (WPA);
\draw[->] (uAA)  -- (BBT);
\draw[->] (BBT)  -- (WPA);

\node[legend, anchor=north east, xshift=+15mm] (Legend) at (current bounding box.north east) {\textbf{Legend:}\C \Rightarrow x_2 = x_1 + 2) \land (D \Rightarrow x_2 = x_1 + 3)
\land x_2 \geq 10
nnit_i \in \NNT\chi_i \in \{0,1\}i\tau\chi_{i,j} \in \{0,1\}\tau\beta = \max T(\tau)[l,h]\betam:=\lceil\frac{l+h}{2}\rceil\tauT(\tau) \geq ml:=ml:=T(\tau)h:=m-1[l,h]lh\tau_1, \dots, \tau_nt_1,\dots,t_71\dots7\betat_1 + t_2 + \max(t_3, t_4) + t_5 + t_7 \leq \beta\beta422^{42}P_2P_1P_1P_2P_2 \max(t_3, t_4) + t_5DBBDBI \neq BBID \neq BBPbt_bbw_bPbb_1,\dots,b_kbw_b = \max(w_{b_1}+t_{b_1}.\chi_P(b_1), \dots, w_{b_k}+t_{b_k}.\chi_P(b_k))\chi_P(x)1x \in P0w_b[-10000,+10000]ite(b,x,y)xbyxb\_it\_i\_jt\_i\_jc\_i\_jb\_0b\_1t\_0\_115c\_0\_1 + c\_1\_2 + c\_0\_2 \leq 21cost \leq 434336t\_0\_1, cost=15t\_0\_2, cost=14t\_1\_2, cost=6t\_2\_3, cost=12t\_2\_4, cost=11t\_3\_4, cost=6t\_0\_1F_1 \land F_2IF_1 \Rightarrow II \land F_2F_1F_2A ; BAB\phi_A \land \phi_B\phi_A\phi_BABi_1,\dots,i_mo_1,\dots,o_nt_1,\dots,t_pABi_1,\dots,i_m,o_1,\dots,o_nFo_1t_1ABF \land t_1 \geq \betaAt_1 \leq \beta_ABo_1 - t_1 \leq \beta_BAF \land t_1 \geq \beta(\phi_A) \land (\phi_B \land t_1 \geq \beta)B\phi_B \land (\phi_A \land t_1 \geq \beta)nm=5nnnib_ix_ib_ib_ib_iy_i(b_i)_{1 \leq i \leq n}5nm(b_i), \allowbreak (x_i), \allowbreak (y_i)_{1 \leq i \leq n}b_1,\dots,b_n \in \{0,1\}^nb_1,\dots,b_n\alpha_i,\beta_i\{2,3\}i\alpha_i + \beta_i = 5m \leq 5nm > 5nb_1,\dots,b_n2^nb_3 \land \neg b_5 \land b_7b_3 \land \neg b_5x_i+y_i \leq 5nm = 5n\sum_{i\in S} c_i \leq Bc_iBab\neg a \lor \neg b+\infty+\infty+\infty+\infty+\infty+\infty<+\infty+\infty+\infty+\infty+\infty+\inftySSx \leq 0x + y \geq 0y \geq 0TT2nb_ii\sim 2^n$. The ``grouped'' encoding yields instantaneous answers.
This confirms our intuition that groups of associated statements should be abstracted as a unit, be it using interpolants as in Z3 or using precomputed bounds as in our approach.
Unfortunately we have not had the time to attempt encoding large programs into Horn clauses so as to check whether the good performance on toy diamond examples translates to good performance on larger programs.
We however do not see how the possibility that we have of keeping bounds of non-contiguous portions of code would easily translate into Horn clauses.
\end{comment}

\section{Extensions and Future Work}
\label{sec:extensions}
The ``counter encoding'' is best suited for code portions that have a single entry and exit point (in which case they express the timing difference between entry and exit). In contrast, the ``sum encoding'' may be applied to arbitrary subsets of the code, which do not in fact need to be connected in the control-flow graph. One may thus use other heuristic criteria, such as usage of related variables.

A model based on worst-case execution times for every block, to be reassembled into a global worst-case execution time, may be too simplistic: indeed, the execution time of a block may depend on which blocks were executed beforehand, or, for finer modeling, on the value of pointer variables (for determining cache status).

A very general and tempting idea, as suggested earlier in MDD-based model-checking \parencite{metzner-CAV04}, in symbolic execution and bounded model checking by \parencite{DBLP:journals/rts/ChattopadhyayR13,Chu_Jaffar_EMSOFT2011}, in combined abstract interpretation and SAT-solving \parencite{DBLP:conf/rtas/BanerjeeCR13} is to integrate in the same analysis both the non-functional semantics (e.g. caches) and the functional semantics; in our case, we would replace both the micro-architectural analysis (or part of it) and the path analysis by a single pass of optimization modulo SMT.
Because merely encoding the functional semantics and a simplistic timing model already led to intractable formulas, we decided to postpone such micro-architectural modeling until we had solved scalability issues.
We intend to integrate such \emph{non-functional} aspects into the SMT problem in future work.

Detailed modeling of the cache, pipeline, etc. may be too expensive to compute beforehand and encode into SMT. One alternative is to iteratively refine the model with respect to the current ``worst-case trace'': to each basic block one attaches an upper bound on the worst-case execution time, and once a worst-case trace is obtained, a trace analysis is run over it to derive stronger constraints. These constraints can then be incorporated in the SMT encoding before searching for a new longest path.

We have discussed obtaining a tight upper bound on the worst-case operation time of the program from upper bounds on the execution times of the basic blocks. If using lower bounds on the worst-case execution times of the basic blocks, one may obtain a lower bound on the worst-case execution time of the program. Having both is useful to gauge the amount of over-approximation incurred. Also, by applying minimization instead of maximization, one gets bounds on best-case execution time, which is useful for some scheduling applications~\parencite{Wilhelm06}.

On a more practical angle, our analysis is to be connected to analyses both on the high level specification (e.g. providing invariants) and on the object code (micro-architectural timing analysis); this poses engineering difficulties, because typical compilation framework may not support sufficient tracing information.

Our requirement that the program should be loop-free, or at least contain loops with small constant bounds, can be relaxed through an approach similar to that of \textcite{Chu_Jaffar_EMSOFT2011}: the body of a loop can be summarized by its WCET, or more precisely by some summaries involving the cost variables and the scalar variables of the program. Then, this entire loop can be considered as a single block in an analysis of a larger program, with possibly overapproximations in the WCET, depending on how the summaries are produced.

\section{Conclusion}
\label{sec:conclusion}
We have shown that optimization using satisfiability modulo theory (SMT) is a workable approach for bounding the worst-case execution time of loop-free programs (or programs where loops can be unrolled). To our knowledge, this is the first time that such an approach was successfully applied.

Our approach computes an upper bound on the WCET, which may or may not be the actual WCET.
The sources of discrepancy are
\begin{enumerate*}[label={\arabic*)}]
\item the microarchitectural analysis (e.g. the cache analysis does not know whether an access is a hit or a miss),
\item the composition of WCET for basic blocks into WCET for the program, which may lose dependencies on execution history\footnote{This does not apply to some simple microcontroller architectures, without cache or pipeline states, e.g. Atmel AVR{\RegTM} and Freescale{\texttrademark} HCS12.},
\item the encoding of the program into SMT, which may be imprecise (e.g. unsupported constructs replaced by nondeterministic choices).
\end{enumerate*}

We showed that straightforward encodings of WCET problems into SMT yield problems intractable by all current production-grade SMT-solvers (``diamond formulas''), and how to work around this issue using a clever encoding.
We believe this approach can be generalized to other properties, and lead to fruitful interaction between modular abstraction and SMT-solving.

From a practical point of view, our approach integrates with any SMT solver without any modification, which makes it convenient for efficient and robust implementation. It could also integrate various simple analyses for introducing other relevant cuts.

While our redundant encoding brings staggering improvements in analysis time, allowing formerly intractable problems to be solved under one minute, the improvements in the WCET upper bound brought by the elimination of infeasible paths depend on the structure of the program being analyzed.
The improvement on the WCET bound of some industrial examples (18\%, 55\%\dots) is impressive, in a field where improvements are often of a few percents.
This means that, at least for certain classes of programs, it is necessary to take infeasible paths into account.
At present, certain industries avoid using formal verification for WCET because it has a reputation for giving overly pessimistic over-estimates; it seems likely that some of this over-estimation arises from infeasible paths.

Our approach to improving bounds on WCET blends well with other WCET analyses. It can be coupled with an existing micro-architectural analysis, or part of that analysis may be integrated into our approach. It can be combined with precise, yet less scalable analyzes \parencite{DBLP:conf/rtns/KnoopKZ13,Holsti_WCET2008} to summarize inner loops; but may itself be used as a way to summarize the WCET of portion of a larger program.

\printbibliography
\end{document}
