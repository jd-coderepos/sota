\documentclass[sigconf]{acmart}

\usepackage{booktabs} 


\usepackage{caption}
\captionsetup{skip=0pt}
\usepackage{epsfig,subfigure}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{hyperref}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}




\copyrightyear{2018} 
\acmYear{2018} 
\setcopyright{acmcopyright}
\acmConference[MM '18]{2018 ACM Multimedia Conference}{October 22--26, 2018}{Seoul, Republic of Korea}
\acmBooktitle{2018 ACM Multimedia Conference (MM '18), October 22--26, 2018, Seoul, Republic of Korea}
\acmPrice{15.00}
\acmDOI{10.1145/3240508.3240704}
\acmISBN{978-1-4503-5665-7/18/10}

\fancyhead{}



\begin{document}
\title{GestureGAN for Hand Gesture-to-Gesture Translation\\ in the Wild}

\author{Hao Tang, \quad Wei Wang, \quad Dan Xu, \quad Yan Yan, \quad Nicu Sebe}
\affiliation{\institution{Department of Information Engineering and Computer Science, University of Trento, Trento, Italy \\
		Computer Vision Laboratory, \'Ecole Polytechnique F\'ed\'erale de Lausanne, Lausanne, Switzerland \\
		Department of Engineering Science, University of Oxford, Oxford, United Kingdom \\
		Department of Computer Science, Texas State University, San Marcos, USA}
{ \{hao.tang,\,niculae.sebe\}@unitn.it,\,wei.wang@epfl.ch,\,danxu@robots.ox.ac.uk,\,y\_y34@txstate.edu}
}

\begin{abstract}
	
Hand gesture-to-gesture translation in the wild is a challenging task since hand gestures can have arbitrary poses, sizes, locations and self-occlusions. 
Therefore, this task requires a high-level understanding of the mapping between the input source gesture and the output target gesture.
To tackle this problem, we propose a novel hand Gesture Generative Adversarial Network (GestureGAN). 
GestureGAN consists of a single generator  and a discriminator , which takes as input a conditional hand image and a target hand skeleton image. 
GestureGAN utilizes the hand skeleton information explicitly, and learns the gesture-to-gesture mapping through two novel losses, the color loss and the cycle-consistency loss. 
The proposed color loss handles the issue of ``channel pollution'' while back-propagating the gradients.
In addition, we present the Fr\'echet ResNet Distance (FRD) to evaluate the quality of generated images.
Extensive experiments on two widely used benchmark datasets demonstrate that the proposed GestureGAN achieves state-of-the-art performance on the unconstrained hand gesture-to-gesture translation task. 
Meanwhile, the generated images are in high-quality and are photo-realistic, allowing them to be used as data augmentation to improve the performance of a hand gesture classifier. 
Our model and code are available at \url{https://github.com/Ha0Tang/GestureGAN}.
	
\end{abstract}

\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010147.10010178.10010224</concept_id>
	<concept_desc>Computing methodologies~Computer vision</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10010147.10010257</concept_id>
	<concept_desc>Computing methodologies~Machine learning</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer vision}
\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{Generative Adversarial Networks; Image Translation; Hand Gesture}

\maketitle


\section{Introduction}

Hand gesture-to-gesture translation in the wild is a task that converts the hand gesture of a given image to a target gesture with a different pose, size and location while preserving the identity information.
This task has many applications, such as human-computer interactions, entertainment, virtual reality and data augmentation.
However, this task is difficult since it needs (i) handling complex backgrounds with different illumination conditions, objects and occlusions; (ii) a high-level semantic understanding of the mapping between the input and output gestures.

Recently, Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} have shown the potential to solve this challenging task. GAN is a generative model based on game theory, which has achieved impressive performance in many applications, such as high-quality image generation \cite{karras2017progressive}, video generation \cite{yan2017skeleton} and audio generation \cite{oord2016wavenet}.
To generate specific kinds of images, videos and audios,  Mirza et al.~\cite{mirza2014conditional} proposed the Conditional GAN (CGAN), which comprises a vanilla GAN and other external information, such as class labels~\cite{choi2017stargan}, text descriptions~\cite{reed2016learning}, images~ \cite{isola2017image} and object keypoints~\cite{reed2016learning}.

In this paper, we focus on the image-to-image translation task using CGAN.
Image-to-image translation tasks can be divided into two types: paired \cite{isola2017image,ma2017pose,siarohin2017deformable} and unpaired \cite{zhu2017unpaired,yi2017dualgan,anoosheh2017combogan,choi2017stargan}.
However, existing image-to-image translation frameworks are inefficient in the multi-domain image-to-image translation task. 
For instance, given  image domains, pix2pix \cite{isola2017image} and  BiCycleGAN \cite{zhu2017toward} need to train  models.
CycleGAN~\cite{zhu2017unpaired}, DiscoGAN~\cite{kim2017learning} and DualGAN~\cite{yi2017dualgan} need to train  models, or  generator/discriminator pairs since one model has 2 different generator/discriminator pairs for these methods.
ComboGAN~\cite{anoosheh2017combogan} requires  models.
StarGAN \cite{choi2017stargan} needs one model. 
However, for some specific image-to-image translation applications  such as hand gesture-to-gesture translation,  could be arbitrary large since gestures in the wild can have arbitrary poses, sizes, appearances, locations and self-occlusions.

To address these limitations, several works have been proposed to generate images based on object keypoints.
For instance, 
Reed et al. \cite{reed2016generating} present an extension of Pixel Convolutional Neural Networks (PixelCNN) to generate images based on keypoints and text description.
Siarohin et al. \cite{siarohin2017deformable} introduce a deformable Generative Adversarial Network for pose-based human image generation.
Ma et al. \cite{ma2017disentangled} propose a two-stage reconstruction pipeline that learns generates novel person images.
However, the aforementioned methods always have the ``channel pollution'' problem that is frequently occurring in generative models such as PG \cite{ma2017pose} leading to blurred generated images.
To solve this issue, in this paper, we propose a novel Generative Adversarial Network, \textit{i.e.}, GestureGAN which treats each channel independently.
It allows generating high-quality hand gesture images with arbitrary poses, sizes and locations in the wild, and thus reducing the dependence on environment and pre-processing operations. 
GestureGAN only consists of one generator and one discriminator, taking  a conditional hand gesture image and a target hand skeleton image as inputs.
In addition, to better learn the mapping between inputs and outputs, we propose two novel losses, \textit{i.e.}, color loss and cycle-consistency loss.
Note that the color loss can handle the problem of ``channel pollution'', 
making the generated images sharper and having higher quality.
Furthermore, we propose the Fr\'echet ResNet Distance (FRD), which is a novel evaluation metric to evaluate the generated image of GANs.
Extensive experiments on two public benchmark datasets demonstrate that GestureGAN can generate high-quality images with convincing details. 
Thus, these generated images can augment the training data and improve the performance of  hand gesture classifiers.

Overall, the contributions of this paper are as follows:
\begin{itemize}[leftmargin=*]
	\item We propose a novel Generative Adversarial Network, \textit{i.e.}, GestureGAN,  which can generate target hand gesture with arbitrary poses, sizes and locations in the wild. In addition, we present a novel color loss to learn the hand gesture-to-gesture mappings, handling the problem of ``channel pollution''.
	\item We propose an efficient Fr\'echet ResNet Distance (FRD) metric to evaluate the similarity of the real and generated images, which is more consistent with human judgment.
	FRD measures the similarity between the real image and the generated image in a high-level semantic feature space.
	\item  Qualitative and quantitative results demonstrate the superiority of the proposed GestureGAN over the state-of-the-art models on the unconstrained hand gesture-to-gesture translation task. In addition, the generated hand gesture images are of high-quality and are photo-realistic, thus allowing them to be used to boost the performance of hand gesture classifiers.
\end{itemize}



\section{Related Work}
\label{sec:relatewprk}

\begin{figure*}[!t] \tiny
	\centering
\includegraphics[width=0.94\linewidth]{framework1.jpg}
	\caption{Pipeline of the proposed GestureGAN model. GestureGAN consists of a single generator  and a discriminator , which takes as input a conditional hand image and a target hand skeleton image. 
	}
	\label{fig:framework}
\end{figure*}

\noindent\textbf{Generative Adversarial Network (GAN)} \cite{goodfellow2014generative} is an unsupervised learning method and has been proposed by Goodfellow et al.
Recently, GAN has shown outstanding results in various applications, \textit{e.g.}, image generation \cite{karras2017progressive,berthelot2017began}, image editing \cite{shu2017neural,perarnau2016invertible}, video generation \cite{tulyakov2017mocogan,mathieu2015deep}, texture synthesis \cite{li2016precomputed}, music generation \cite{yang2017midinet} and feature learning \cite{xie2017controllable}.
Recent approaches employ the idea of GAN for conditional image generation, such as image-to-image translation \cite{isola2017image,wang2018high}, text-to-image translation \cite{reed2016generative,han2017stackgan}, image inpainting \cite{li2017generative,dolhansky2017eye},
image blending \cite{wu2017gp},  image super-resolution \cite{ledig2016photo}, as well as the applications of other domains like semantic segmentation \cite{luc2016semantic}, object detection \cite{li2017perceptual,wang2017fast}, human parsing \cite{liu2018cross}, face aging \cite{liu2017face} and 3D vision \cite{park2017transformation,wu2016learning}.
The key point success of GANs in computer vision and graphics is the adversarial loss, which allows the model to generate images that are  indistinguishable from real images, and this is exactly the goal that many computer vision and graphics tasks aim to optimize.

\noindent\textbf{Image-to-Image Translation} frameworks use input-output data to learn a parametric mapping between inputs and outputs, \textit{e.g.}, Isola et al. \cite{isola2017image} build the pix2pix model, which uses a conditional GAN to learn a translation function from input to output image domains.
Taigman et al. \cite{taigman2016unsupervised} propose the Domain Transfer Network (DTN) which learns a generative function between one domain and another domain.
Zhu et al. \cite{zhu2017unpaired} introduce the CycleGAN framework, which achieves unpaired image-to-image translation using the cycle-consistency loss.
Moreover, Zhu et al. \cite{zhu2017toward} present the BicycleGAN model based on CycleGAN \cite{zhu2017unpaired} and pix2pix \cite{isola2017image}, which targets multi-modal image-to-image translation.

However, existing image-to-image translation models are inefficient and ineffective.
For example, with  image domains, CycleGAN~\cite{zhu2017unpaired}, DiscoGAN \cite{kim2017learning}, and DualGAN~\cite{yi2017dualgan} need to train  generators and discriminators, while pix2pix \cite{isola2017image} and BicycleGAN \cite{zhu2017toward} have to train  generator/discriminator pairs.
Recently, Anoosheh et al. proposed ComboGAN \cite{anoosheh2017combogan}, which only need to train  generator/discriminator pairs for  different image domains, having a complexity of .
Additionally, Choi et al. \cite{choi2017stargan} propose StarGAN, in which a single generator and discriminator can perform unpaired image-to-image translations for multiple domains.
Although the computational complexity of StarGAN is , this model has only been validated on the face attributes modification task with clear background and face cropping.
More importantly, for some specific image-to-image translation tasks  such as hand gesture-to-gesture translation task, 
the image domains could be arbitrary large, \emph{e.g.}, gesture in the wild can have arbitrary poses, sizes, appearances, locations and self-occlusions.
The aforementioned approaches are not effective for solving these specific situations.

\noindent\textbf{Keypoint/Skeleton Guided Image-to-Image Translation.}
To fix these limitations, several recent works have been proposed to generate person, bird or face images based on object keypoints \cite{reed2016learning,ma2017pose} or human skeleton \cite{yan2017skeleton,siarohin2017deformable}.
For instance,
Di et al.~\cite{di2017gp} propose the Gender Preserving Generative Adversarial Network (GPGAN) to synthesize faces based on facial landmarks.
Reed et al.~\cite{reed2016learning} propose the Generative Adversarial What-Where Network (GAWWN), which generates birds conditioned on both text descriptions and object location.
Ma et al. propose the Pose Guided Person Generation Network (PG) \cite{ma2017pose} and a two-stage reconstruction pipeline \cite{ma2017disentangled}, which achieve person-to-person image translation using a conditional image and a target pose image (note that in these two models images are pre-cropped).
Reed et al. \cite{reed2016generating} present an extension of Pixel Convolutional Neural Networks (PixelCNN) to generate images parts based on keypoints and text description.
Sun et al. \cite{sun2017natural} propose a two-stage framework to  perform head inpainting conditioned on the generated facial landmark in the first stage.
Korshunova et al. \cite{korshunova2016fast} use facial keypoints to define the affine transformations of the alignment and realignment steps for face swap. 
Wei et al.~\cite{wei2017every} propose a Conditional MultiMode Network (CMM-Net) for landmark-guided  smile generation.
Qiao et al.~\cite{qiao2018geometry} present the Geometry-Contrastive Generative Adversarial Network (GCGAN) to generate facial expressions conditioned on geometry information of facial landmarks. 
Song et al. \cite{song2017geometry} propose the Geometry-Guided Generative Adversarial Network (G2GAN) for facial expression synthesis guided by fiducial points.
Yan et al. \cite{yan2017skeleton} propose a method to generate human motion sequence with simple background using CGAN and human skeleton information.
Siarohin et al. \cite{siarohin2017deformable} introduce PoseGAN for pose-based human image generation using human skeleton.

The typical problem with the aforementioned generative models is that they suffer from ``channels pollution'' and thus
they tend to generate blurry results with artifacts.
To handle this problem, we propose GestureGAN, which allows generating high-quality hand gesture image with arbitrary poses, sizes and locations in the wild.

\section{Model Description}
\label{sec:method}

\subsection{GestureGAN Objective}

The goal of vanilla GAN is to train a generator  which learns the mapping between random noise  and image .
The mapping function of GAN,  is learned via the following objective function,

Conditional GANs learn the mapping , where   is the input conditional image.
Generator  is trained to generate image  that cannot be distinguished from ``real'' image  by an adversarially trained discriminator , while the discriminator  is trained as well as possible to detect the ``fake'' images generated by the generator .  
The objective function of the conditional GAN is defined as follows,

where generator  tries to minimize this objective while the discriminator  tries to maximize it. 
Thus, the solution is 
In this paper, we try to learn two mappings through one generator.
The framework of the proposed GestureGAN is shown in Figure \ref{fig:framework}.

\noindent \textbf{Adversarial Loss.} In order to learn the  gesture-to-gesture mapping, we employ the hand skeleton information explicitly.
We exploit OpenPose \cite{simon2017hand} to detect 21 hand keypoints denoted as , where  and  represent pixel coordinates of keypoints.
For each keypoint ,  represents the confidence that the keypoint is correctly localized.
Thus, the adversarial losses of the two mappings   and  are defined respectively, as follows:



where  and  are the hand keypoints of image  and  respectively; 
 represents the concatenate operation.  and  are defined by setting the pixels around the corresponding keypoint  to 1 (white) with the radius of 4 and 0 (black) elsewhere.
In other words, each keypoint is actually represented with pixels in a circle with a radius of 4. 
Therefore, the total adversarial loss based on hand keypoint can be defined as,


In addition, to explore the influence of the confidence score  to the generated image, we define a confidence keypoint image  in which the pixels around the corresponding keypoint  are set to  in a radius of 4 pixels and 0 (black) elsewhere.
Thus, Equation \ref{eqn:adv} can be expressed as .

Moreover, following OpenPose \cite{simon2017hand}, we connect the 21 keypoints (hand joints) to obtain the hand skeleton, denoted as  and .
The hand skeleton image visually contains richer hand structure information than the hand keypoint image.
Next, the adversarial loss based on hand skeleton can be derived from Equation~\ref{eqn:adv}, \textit{i.e.}, .
In hand skeleton image  and , the hand joints are connected by the lines with the width of 4 and with white color.
Next, corresponding to the confidence keypoint image, we have also defined an adversarial loss using confidence hand skeleton as ,
where the line connections in  and  are filled with the confidence score of later point, \textit{e.g.}, if the hand skeleton connects points 1 and 2, thus this line connection is filled with the confidence of point 2, \textit{i.e.},  with the width of 4.

\noindent \textbf{Improved Pixel Loss.} Previous work indicated that mixing the adversarial loss with a traditional loss such as  loss \cite{isola2017image} or  loss \cite{pathak2016context} between the generated image and the ground truth image improves the quality of generated images.
The definition of  and  losses are:


However, we observe that the existing image-to-image translation models such as PG \cite{ma2017pose} cannot retain the holistic color of the input images.
An example is shown in Figure~\ref{fig:first_figure}, where PG is affected by the pollution issue and produces more unrealistic regions. 
Therefore, to remedy this limitation we introduce a novel channel-wise color loss.
Traditional generative models convert a whole image to another, which leads to the ``channel pollution'' problem.
However, the color loss treats ,  and  channels independently and generates only one channel each time, and then these three channels are combined to produce the final image.
Intuitively, since the generation of a three-channel image space is much more complex than the generation of a single-channel image space, leading to higher possibility of artifacts, we independently generate each channel.
The objective of ,  and  channel losses can be defined as follows,
\begin{figure}[!t] \tiny
	\centering
	\includegraphics[width=0.95\linewidth]{comparison_color.jpg}
	\caption{Illustration of the ``channel pollution'' issue on different methods.}
	\label{fig:first_figure}
	\vspace{-0.3cm}
\end{figure}

\vspace{-0.3cm}

where , ,  and  denote the ,  and  channels of image  respectively and similar to ,  and ,  and  represent  and  distance losses.
Thus, the color  and  losses can be expressed as, 


When back-propagating the gradients of the  loss, the partial derivatives are constants, \emph{i.e.}, . 
Therefore, the error from other channels will not influence the current one as the derivative is a constant.
However, for the original  loss, the derivative is not a fixed constant. Actually, the derivative for the variables in one channel is always influenced by the errors from other channels. 
We have listed the gradients of red channel of Equations \ref{equ:l2} and \ref{eqn:color2}. Let  represent the generated target image , we have,
\vspace{-0.2cm}



Therefore, we can calculate the gradient of original  and color  losses, 

\vspace{-0.2cm}


Clearly, in Equation \ref{eqn:gra_l2} the red channel in original  loss is polluted by green and blue channels. As a consequence, the error from other channels will also influence the red channel. 
On the contrary, if we compute the loss for each channel independently, we can avoid such influence as shown in Equation \ref{eqn:gra_colorl2}.

\noindent \textbf{Cycle-Consistency Loss.}
It is worth noting that the CycleGAN~\cite{zhu2017unpaired} is different from pix2pix framework \cite{isola2017image} as the training data in CycleGAN is unpaired. 
The CycleGAN introduces the cycle-consistency loss to enforce forward-backward consistency.
The cycle-consistency loss can be regarded as ``pseudo'' pairs of training data even though we do not have the corresponding data in the target domain which corresponds to the input data from the source domain.
However, in this paper we introduce the cycle-consistency loss for the paired image-to-image translation task.
The cycle loss ensures the consistency between source images and the reconstructed image, and it can be expressed as,

Similar to StarGAN \cite{choi2017stargan} we use the same generator  two times, with the first time to convert an original image into the target one, then to recover the original image from the generated image.

\noindent \textbf{Identity Preserving Loss.} To preserve the person identity after image synthesis, we propose the identity preserving loss, which can be expressed as follows,

where  is a feature extractor. 
The feature extractor employs a VGG network \cite{simonyan2014very} originally pretrained for face recognition.
We minimize the difference between the feature maps which are generated from the real and the generated images via the pretrained CNN for identity preservation. 

\noindent \textbf{Overall Loss.} The final objective of the proposed GestureGAN is,

where,  ,  and  are three hyper-parameters controlling the relative importance of these four losses.
In our experiments, we follow the same setup of pix2pix \cite{isola2017image}.
Instead of using the random noise vector~, we provide noise only in the form of dropout in generator~.


\subsection{Network Architecture}

\noindent\textbf{Generator.}
We adopt the ``U-shaped'' network \cite{isola2017image} as our generator.
U-net has skip connections, which concatenate all channels at layer  with those at layer , where  is the total number of layers.

\noindent\textbf{Discriminator.}
We employ PatchGAN  \cite{isola2017image}  as our discriminator architecture.
The goal of PatchGAN  is to classify each small patch in an image as real or fake.
We run PatchGAN convolutationally across an image, then average all results to calculate the ultimate output of discriminator .

\subsection{Optimization}

We follow the standard optimization method from \cite{goodfellow2014generative} to optimize the proposed GestureGAN, \textit{i.e.}, we alternate between one gradient descent step on discriminator , and one step on generator .
In addition, as suggested in the original GAN paper \cite{goodfellow2014generative}, we train to maximize  rather than to minimize .
Moreover, in order to slow down the rate of  relative to  we divide the objective by 2 while optimizing ,
\vspace{-0.1cm}

where  denotes the Binary Cross Entropy loss function.
We also employ dual discriminators as in Xu et al. \cite{xu2017face}, Nguyen et al. \cite{nguyen2017dual} and CycleGAN \cite{zhu2017unpaired}, which have demonstrated that they improve the ability of discriminator to generate more photo-realistic images.
Thus Equations \ref{eqn:dloss} is modified as: 
\vspace{-0.1cm}

We employ the minibatch SGD algorithm and apply Adam optimizer~\cite{kingma2014adam} as solver. The momentum terms  and   of Adam are 0.5 and 0.999, respectively.  
The initial learning rate for Adam is 0.0002.


\section{Experiments}
\label{sec:experiment}

\subsection{Experimental Setup}
\noindent \textbf{Datasets.}
We evaluate the proposed GestureGAN on two public hand gesture datasets: NTU Hand Digit \cite{ren2013robust} and Creative Senz3D \cite{memo2016head}, which include different hand gestures.
NTU Hand Digit dataset \cite{ren2013robust} contains 10 hand gestures (\textit{e.g.}, decimal digits from 0 to 9) color images
and depth maps collected with a Kinect sensor under cluttered background.
The total images in this dataset are 10 gestures  10 subjects  10 times = 1000 images.
All images are in  resolution.
In our experiment, we only use the RGB images.
We randomly select 84,636 pairs, each of which is comprised of two images of the same person but different gestures.
9,600 pairs are randomly selected for the testing subset and the rest of 75,036 pairs as the training set.
Creative Senz3D dataset \cite{memo2016head} includes static hand gestures performed by 4 people, each performing 11 different gestures repeated 30 times each in
the front of a Creative Senz3D camera.
The overall number of images of this dataset is 4 subjects  11 gestures  30 times = 1320.
All images are in resolution .
In our experiment, we only use the RGB images.
We randomly select 12,800 pairs and 135,504 pairs as the testing and training set, each pair being composed of two images of the same person but different gestures.

\noindent\textbf{Implementation Details.}
For both datasets, we do left-right flip for data augmentation and random crops are disabled in this experiment as was done in PG \cite{ma2017pose}.
For the embedding method, skeleton images are fed into an independent encoder similar to PG \cite{ma2017pose},  then we extract the fully connected layer feature vector to concatenate it with the image embedding at the bottleneck fully connected layer.
For optimization, models are trained with a mini-batch size of 8 for 20 epochs on both datasets.
Hyper-parameters are set empirically with ,  .
 is 0.1 in the beginning and is gradually increased to 0.5.
At inference time, we follow the same settings of PG~\cite{ma2017pose}, Ma et al. \cite{ma2017disentangled} and PoseGAN \cite{siarohin2017deformable} to randomly select the target keypoint or skeleton.
GestureGAN is implemented using the public deep learning framework PyTorch.
To speed up the training and testing processes, we use a Nvidia TITAN Xp GPU with 12G memory.

\begin{figure*}[!t] \tiny
	\centering
	\includegraphics[width=0.95\linewidth]{comparison_result.jpg}
	\caption{Qualitative comparison with PG \cite{ma2017pose}, Ma et al. \cite{ma2017disentangled}, Yan et al. \cite{yan2017skeleton} and PoseGAN \cite{siarohin2017deformable} on the NTU Hand Digit (Top) and the Senz3D (Bottom) datasets. Zoom in for details.}
	\label{fig:comparsion}
\end{figure*}

\begin{table*}[!t] \small
	\centering
	\vspace{-0.2cm}
	\caption{Quantitative results of different models on the NTU Hand Digit and Senz3D datasets. For PSNR and IS measures, higher is better. For MSE evaluation, lower is better.}
	\begin{tabular}{l|c|c|c|c|c|c} \toprule
		\multirow{2}{*}{Model} & \multicolumn{3}{c|}{NTU Hand Digit \cite{liu2016deepfashion}}& \multicolumn{3}{c}{Senz3D \cite{memo2016head}}   \\ \cline{2-7}
		& MSE               & PSNR             & IS              & MSE               & PSNR             & IS     \\ \midrule			
		PG \cite{ma2017pose}  (NIPS 2017)                                        & 116.1049          & 28.2403          & 2.4152          & 199.4384          & 26.5138          & 3.3699 \\ \hline
		Yan et al. \cite{yan2017skeleton} (ACM MM 2017)                              & 118.1239          & 28.0185          & 2.4919          & 175.8647          & 26.9545          & 3.3285 \\ \hline
		Ma et al. \cite{ma2017disentangled} (CVPR 2018)                              & 113.7809          & 30.6487          & 2.4547          & 183.6457          & 26.9451          & 3.3874 \\ \hline 
		PoseGAN \cite{siarohin2017deformable} (CVPR 2018)                            & 113.6487          & 29.5471          & 2.4017          & 176.3481          & 27.3014          & 3.2147 \\ \hline \hline
		GestureGAN (Ours)                                                            & \textbf{105.7286} & \textbf{32.6091} & \textbf{2.5532 }         & \textbf{169.9219} & \textbf{27.9749}          & \textbf{3.4107} \\ \bottomrule		
	\end{tabular}
	\label{tab:baseline}
	\vspace{-0.2cm}
\end{table*}

\begin{figure*}[!t] \tiny
	\centering
	\includegraphics[width=0.95\linewidth]{loss_results.jpg}
	\caption{Qualitative comparison using different components of GestureGAN on the NTU Hand Digit (Top) and the Senz3D (Bottom) datasets.  All: full version of GestureGAN, D: Dual discriminators strategy, I: Identity preserving loss, P: Color loss, C: Cycle-consistency loss. ``-'' means removing. Zoom in for details.}
	\label{fig:loss}
\end{figure*}


\noindent\textbf{Evaluation Metrics.}
Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Inception Score (IS) \cite{salimans2016improved}, Fr\'echet Inception Distance (FID) \cite{heusel2017gans} and the proposed Fr\'echet ResNet Distance (FRD) are employed to evaluate the quality of generated images.
FRD approach provides an alternative method to quantify the quality of synthesis and is similar to the Fr\'echet Inception Distance (FID) proposed by \cite{heusel2017gans}.
FID is a measure of similarity between two datasets of images. 
The authors in \cite{heusel2017gans} have shown that the FID is more robust to noise than IS \cite{salimans2016improved} and  correlates well with the human judgment of visual quality \cite{heusel2017gans}.
To calculate FID \cite{heusel2017gans} between two image domains  and , they first embed both into a feature space  given by an Inception model.
Then viewing the feature space as a continuous multivariate Gaussian as suggested in \cite{heusel2017gans}, the Fr\'echet distance between the two Gaussians to quantify the quality of the data and the definition of FID can be expressed as:

where  and  are the mean and the covariance of the data distribution and model distribution, respectfully.

Unlike FID, which regards the datasets  and  as a  whole, the proposed FRD  is inspired from feature matching methods \cite{tang2016novel,zheng2014packing,zheng2014bayes}, and separately calculates the Fr\'echet distance between generated images and real images from the semantical level.
In this way, images from two domains do not affect each other when computing the Fr\'echet distance.  
Moreover, for FID the number of samples should be greater than the dimension of the coding layer, while the proposed FRD does not have this limitation. 
We denote  and  as images in the   and  domains, respectively.
For calculating FRD, we first embed both images  and  into a feature space  with  dimension given by a ResNet pretrained model \cite{he2016deep}.
We then calculate the Fr\'echet distance between two feature maps  and .
The Fr\'echet distance  is defined as the infimum over all reparameterizations  and  of  of the maximum over all  of the distance in  between  and , where  and  are continuous, non-decreasing surjections of the range .
The proposed FRD is a measure of similarity between the feature vector of the real image   and the feature vector of the generated image  by calculating the Fr\'echet distance between them. The Fr\'echet distance is defined as the minimum cord-length sufficient to join a point traveling forward along  and one traveling forward along , although the rate of travel for each point may not necessarily be uniform. 
Thus, the definition of FRD between two image domain  and  is,

where  is the distance function of ,  is the total number of images in  and  domains.

\subsection{Qualitative \& Quantitative Results}
\noindent \textbf{Comparison against Baselines.}
We compare the proposed GestureGAN with the most related four works,
\textit{i.e.}, PG \cite{ma2017pose}, Yan et al. \cite{yan2017skeleton}, PoseGAN \cite{siarohin2017deformable} and Ma et al. \cite{ma2017disentangled}.
PG \cite{ma2017pose} and Ma et al. \cite{ma2017disentangled} try to generate a person image with different poses based on a conditional person image and a target keypoint image.
Yan et al. \cite{yan2017skeleton} and PoseGAN \cite{siarohin2017deformable} explicitly employ human skeleton information to generate person images.  
Note that Yan et al. \cite{yan2017skeleton} adopt a CGAN to generate motion sequences based on appearance information and skeleton information by exploiting frame level smoothness.
We re-implemented this model to generate a single frame for fair comparison. 
These four methods are paired image-to-image models and comparison results are shown in Figure \ref{fig:comparsion} and Table \ref{tab:baseline}.
As we can see in Figure \ref{fig:comparsion}, GestureGAN produces sharper images with convincing details compared with other baselines.
Moreover, it is obvious that our results in Table \ref{tab:baseline} are consistently much better than baseline methods on both datasets.

\begin{table*}[!t] \small
	\centering
	\vspace{-0.2cm}
	\caption{Ablation study: quantitative results with different components of GestureGAN on the NTU Hand Digit and Senz3D datasets. For PSNR and IS measures, higher is better. For MSE evaluation, lower is better.  All: full version of GestureGAN, D: Dual discriminators strategy, I: Identity preserving loss, P: Color loss, C: Cycle-consistency loss. ``-'' means removing.}
	\begin{tabular}{l|c|c|c|c|c|c} \toprule
		\multirow{2}{*}{Component} & \multicolumn{3}{c|}{NTU Hand Digit \cite{liu2016deepfashion}}& \multicolumn{3}{c}{Senz3D \cite{memo2016head}}   \\ \cline{2-7}
		& MSE      & PSNR    & IS                            & MSE       & PSNR    & IS       \\ \midrule	
		All	                                  & \textbf{105.7286}  & \textbf{32.6091} &2.5532      & \textbf{169.9219} & \textbf{27.9749}          & 3.4107 \\ \hline
		All - D (Dual Discriminators Strategy)         & 118.7830 & 28.0189 & 2.5071                        & 198.0646  & 26.7237 & 3.2740 \\ \hline	
		All - D - I (Identity Preserving Loss)           & 198.7054 & 25.8474 & 2.5438                        & 1319.3957 & 18.3892 & \textbf{4.0784} \\ \hline
		All - D - I - P (Color Loss)          & 406.1478 & 22.1564 & 2.5842                        & 1745.3214 & 14.6598 & 3.4519 \\ \hline 
		All - D - I - P - C (Cycle-Consistency Loss)      & 707.6053 & 20.2684 & \textbf{2.6114}               & 2064.8428 & 15.5426 & 3.2064  \\ \hline	
	\end{tabular}
	\label{tab:result}
	\vspace{-0.2cm}
\end{table*}

\begin{figure}[!t] \tiny
	\centering
	\setcounter{subfigure}{0}
	\subfigure{	\includegraphics[width=0.4\linewidth]{loss_epoch.jpg}}
	\subfigure{\includegraphics[width=0.4\linewidth]{ntu_pair.png}}
	\caption{Convergence loss  in Equation \ref{eqn:overallloss} (Left) and MSE of different gesture pairs on the NTU dataset (Right).}	
	\label{fig:plot_loss_change}
	\vspace{-0.4cm}
\end{figure}

\noindent \textbf{Generated Results of Each Epoch.}
Figure \ref{fig:plot_loss_change} (left) illustrates the convergence loss  of the proposed GestureGAN in Equation \ref{eqn:overallloss}. 
Note that the proposed GestureGAN ensures a very fast yet stable convergence.

\noindent \textbf{Analysis of the Model Components.}
In Figure \ref{fig:loss} we conduct ablations studies of our model.
We gradually remove components of the proposed GestureGAN, \textit{i.e.}, Dual Discriminators (D), Identity Loss (I), Color Loss (P) and Cycle-consistency Loss (C).
We find that removing the color loss and the cycle-consistency loss substantially degrades results, meaning that the color loss and the cycle-consistency loss are critical to our results.
In addition, the results without using the identity loss and the dual discriminators slightly degrade performance.
We also provide quantitative results in Table~\ref{tab:result}, and we can see that the full version of GestureGAN  produces more photo-realistic results that other variants on two measurements except IS.
The reason could be that the datasets we used only include human images which do not fit into ImageNet classes \cite{deng2009imagenet}. 
Moreover, PG \cite{ma2017pose} and other super-resolution works such as \cite{johnson2016perceptual}  also show the fact that sharper results have a lower quantitative value.

\noindent \textbf{User Study.}
Similar to \cite{ma2017pose,zhu2017unpaired,siarohin2017deformable}, we have also provided a user study. 
We follow the same settings as in \cite{isola2017image} to conduct the Amazon Mechanical Turk (AMT) perceptual studies.
The results of  NTU Hand Digit \cite{liu2016deepfashion} and Senz3D \cite{memo2016head} datasets compared with the baseline models PG \protect\cite{ma2017pose}, Ma et al \cite{ma2017disentangled}, Yan et al. \cite{yan2017skeleton} and PoseGAN \cite{siarohin2017deformable} are shown in Table \ref{tab:user}.
Note that the proposed GestureGAN consistently achieves the best performance compared with baselines.  

\noindent \textbf{FID vs. FRD.}
We also compare the performance between FID and the proposed FRD.
The results shown in Table \ref{tab:frd} and we can observe that FRD is more consistent with the human judgment in Table \ref{tab:user} than the FID metric.
Moreover, we observe that the difference in FRD between GestureGAN and the other methods is not as obvious as in the results from the user study in Table \ref{tab:user}.
The reason is that FRD calculates the Fr\'echet distance between the feature maps extracted from the real image and the generated image using CNNs which are trained with semantic labels. Thus, these feature maps are employed to reflect the semantic distance between the images. The semantic distance between the images is not very large considering they are all hands. On the contrary, the user study measures the generation quality from a perceptual level. The difference on the perceptual level is more obvious than on the semantic level, \emph{i.e.}, the generated images with small artifacts show minor difference on the feature level, while being judged with a significant difference from the real images by humans. 

\begin{table}[!t]
	\centering
	\caption{Comparison of AMT perceptual studies (\%) on the NTU Hand Digit and Senz3D datasets.} 
	\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c} \toprule
			Method                                             & NTU Hand Digit \cite{liu2016deepfashion} & Senz3D \cite{memo2016head} \\ \midrule
			PG \cite{ma2017pose}  (NIPS 2017)              & 3.5\%                                    & 2.8\%  \\ \hline
			Yan et al. \cite{yan2017skeleton}  (ACM MM 2017)   & 2.6\%                                    & 2.3\%  \\ \hline
			Ma et al. \cite{ma2017disentangled}  (CVPR 2018)   & 7.1\%                                    & 6.9\% \\ \hline
			PoseGAN \cite{siarohin2017deformable} (CVPR 2018)  & 9.3\%                                    & 8.6\%   \\ \hline
			GestureGAN (Ours)                         & \textbf{26.1}\%                                   & \textbf{22.6}\%   \\ \bottomrule		
	\end{tabular}}
	\label{tab:user}
	\vspace{-0.2cm}
\end{table}

\begin{table}[!t] 
	\centering
	\caption{Comparison of FID and the proposed FRD metrics on the NTU Hand Digit and Senz3D datasets. For both FID and FRD, lower is better.}
	\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c|c} \toprule
			\multirow{2}{*}{Method} & \multicolumn{2}{c|}{NTU Hand Digit \cite{liu2016deepfashion}}& \multicolumn{2}{c}{Senz3D \cite{memo2016head}}   \\ \cline{2-5}
			& FID & FRD                & FID     & FRD    \\ \midrule			
			PG \cite{ma2017pose}  (NIPS 2017)               & 24.2093 & 2.6319             & 31.7333 & 3.0933 \\ \hline
			Yan et al. \cite{yan2017skeleton}  (ACM MM 2017)    & 31.2841 & 2.7453             & 38.1758 & 3.1006 \\ \hline
			Ma et al. \cite{ma2017disentangled}  (CVPR 2018)    & \textbf{6.7661} & 2.6184             & 26.2713 & 3.0846 \\ \hline	
			PoseGAN \cite{siarohin2017deformable} (CVPR 2018)   & 9.6725 & 2.5846             & 24.6712 & 3.0467 \\ \hline		
			GestureGAN (Ours)                                   & 7.5860 & \textbf{2.5223}    & \textbf{18.4595} & \textbf{2.9836} \\  \bottomrule		
	\end{tabular}}
	\label{tab:frd}
	\vspace{-0.3cm}
\end{table}

\noindent \textbf{Data Augmentation.}
The generated images are high-quality and are photo-realistic, and  these images can be used to improve the performance of a hand gesture classifier.
The intuition is that if the generated images are realistic, the classifiers trained on both the real images and the generated images will be able to boost the accuracy of the real images.
In this situation, the generated images work as augmented data.
We employ a pretrained ResNet-50 model \cite{he2016deep} and feed the generated images to fine-tune it. 
For both datasets, we make a split of 70\%/30\% between training and testing sets.
Specifically, the NTU Hand Digit dataset has 700 and 300 images for training and testing set.
For the Senz3D dataset, the numbers of training and testing set are 924 and 396. 
The recognition results for the NTU Hand Digit and the Senz3D datasets are 15\% and 34.34\%, respectively.
The term ``real/real'' in Table~\ref{tab:data_aug} represents the result without data augmentation.
After adding the generated images by different methods to the training set, the performance improves significantly.
Results compared with PG \cite{ma2017pose}, Yan et al.~\cite{yan2017skeleton}, Ma et al. \cite{ma2017disentangled} and PoseGAN \cite{siarohin2017deformable}  are shown in Table \ref{tab:data_aug}.
Clearly, GestureGAN achieves the best result compared with baselines.


\begin{table}[!t]
	\centering
	\caption{Comparison of hand gesture recognition accuracy~(\%) on the NTU Hand Digit and Senz3D datasets.} 
	\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c} \toprule
			Method                                                 & NTU Hand Digit \cite{liu2016deepfashion}   & Senz3D \cite{memo2016head}  \\ \midrule
			real/real                                              & 15.000\%          & 34.343\% \\ \hline
			PG \cite{ma2017pose}  (NIPS 2017)                  & 93.667\%          & 98.737\%  \\ \hline
			Yan et al. \cite{yan2017skeleton}  (ACM MM 2017)       & 95.333\%          & 99.495\%  \\ \hline
			Ma et al. \cite{ma2017disentangled}  (CVPR 2018)       & 95.864\%          & 99.054\% \\ \hline
			PoseGAN \cite{siarohin2017deformable} (CVPR 2018)      & 96.128\%          & 99.549\% \\ \hline
			GestureGAN (Ours)                             & \textbf{96.667\%} & \textbf{99.747\%}  \\ \bottomrule
	\end{tabular}}
	\label{tab:data_aug}
	\vspace{-0.15cm}
\end{table}


\noindent\textbf{Influence of Gesture Size and Distance.} We have also investigated the influence of the gesture size and the distance between source and target gestures. The training samples for the source and the target gesture  from the same person are randomly paired and both gestures have different sizes and distances. Thus, the model is able to learn a robust translation \textit{w.r.t} different hand size and distance. 
We show a qualitative example in Figure~\ref{fig:comparison_size} in which the source images have different sizes and the target gestures have different locations. 
Note that GestureGAN can generate the target gesture from different hand sizes and distances with high quality.
\begin{figure}[!t] \tiny
	\centering
	\includegraphics[width=0.95\linewidth]{comparison_size.jpg}
	\caption{\small{Two samples with different hand sizes and distances.}}
	\label{fig:comparison_size}
	\vspace{-0.3cm}
\end{figure}

\noindent\textbf{Influence of Gesture Pairs.}
To evaluate the influence of gesture pairs, we searched all the translations between every possible category combinations including the translation within each category.
In Figure \ref{fig:plot_loss_change} (right), we show the MSE for the translation from the source to the target gesture types on NTU dataset.
Note that the MSE for the generation of different gesture pairs has a small variance, showing that the influence of different gesture pairs is very low. This proves that our model is stable. 

\section{Conclusions}
\label{sec:conclusions}

In this paper, we focus on a challenging task of hand gesture-to-gesture translation in the wild.
To this end, we propose a novel Generative Adversarial Network (GAN), \textit{i.e.}, GestureGAN, which can generate  hand gestures with different poses, sizes and locations in the wild.
We also propose two novel losses to learn the mapping from the source gesture to the target gesture, \textit{i.e.}, the color loss and the cycle-consistency loss.
It is worth noting that the proposed color loss handles the ``channel pollution'' problem while back-propagating the gradients, which frequently occurs in the existing generative models.
In addition, we present the Fr\'echet ResNet Distance (FRD) metric to evaluate the quality of generated images.
Experimental results show that GestureGAN achieves state-of-the-art performance.
Lastly, the generated images of GestureGAN are of high-quality and are photo-realistic, and they can thus be used to improve the performance of hand gesture classifiers. 
Future work will focus on designing a GAN model which can handle the situation where the background is total different between source and target gestures.

\section*{Acknowledgements} We want to thank the Nvidia Corporation for the donation of the TITAN Xp GPUs used in this work.

\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}
