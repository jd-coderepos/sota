\documentclass[a4paper,11pt]{paper}

\usepackage[margin=1in]{geometry}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{enumerate}
\usepackage[usenames]{xcolor}
\usepackage[noend,linesnumbered,boxed]{algorithm2e}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{paralist}

\usepackage[T1]{fontenc}

\newcommand{\mathset}[1]{\ensuremath {\mathbb {#1}}}
\newcommand{\N}{\mathset {N}}
\newcommand{\eps}{\varepsilon}
\newcommand{\R}{\mathset{R}}
\newcommand{\Q}{\mathset{Q}}
\newcommand{\Z}{\mathset{Z}}
\newcommand{\script}[1]{\ensuremath {\mathcal {#1}}}
\newcommand{\etal}{\textit{et al.}}
\setlength{\fboxsep}{.5pt}

\DeclareMathOperator{\diam}{\text{diam}}
\DeclareMathOperator{\diag}{\text{diag}}
\DeclareMathOperator{\poly}{\text{poly}}
\DeclareMathOperator{\lastone}{\text{lastone}}
\DeclareMathOperator{\dist}{d}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\aff}{aff}
\newcommand{\lowdimapprox}[1][]{(4k+3)(d#1-k-1)+\sqrt{k+1}}

\newtheorem{theorem} {Theorem}[section]
\newtheorem{problem}[theorem]{Problem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{invariant}[theorem]{Invariant}
\newtheorem{remark}[theorem]{Remark}

\newenvironment{alg}{\begin{algorithm}[htbp]}{\end{algorithm}}

\title{Approximate -flat Nearest Neighbor
  Search\thanks{WM and PS
were supported in part by DFG Grants MU 3501/1 and MU 3501/2.
YS was supported by the Deutsche Forschungsgemeinschaft within
the research training group `Methods for Discrete Structures'
(GRK 1408).}}

\author
{
Wolfgang Mulzer\thanks{Institut f\"ur Informatik,
Freie Universit\"at Berlin,
\{\texttt{mulzer,pseiferth,yannikstein}\}\texttt{@inf.fu-berlin.de}.}
\and
Huy L. Nguy\~{\^{e}}n\thanks{Simons Institute, UC Berkeley
\texttt{hlnguyen@cs.princeton.edu}.}
\and
Paul Seiferth\footnotemark[2]
\and
Yannik Stein\footnotemark[2]
}


\begin{document}
\maketitle

\begin{abstract}
Let  be a nonnegative integer.
In the \emph{approximate -flat nearest neighbor}
(-ANN) problem, we are given a set
 of  points in
-dimensional space and a fixed approximation
factor . Our goal is to
preprocess  so that we can efficiently
answer \emph{approximate -flat nearest
neighbor queries}: given a -flat ,
find a point in  whose distance to
 is within a factor   of the
distance between  and the closest
point in . The case  corresponds to
the well-studied approximate nearest neighbor
problem, for which a plethora of results are known,
both in low and high dimensions.
The case  is called \emph{approximate line
nearest neighbor}. In this case, we are aware of only
one provably efficient data structure, due to
Andoni, Indyk, Krauthgamer,
and Nguy\~{\^{e}}n (AIKN)~\cite{AndoniInKrNg09}.
For , we know of no previous results.

We present the first efficient data structure that
can handle approximate nearest neighbor queries for
arbitrary . We use a data structure
for -ANN-queries as a black box, and the performance
depends on the parameters of the -ANN
solution: suppose we have an -ANN structure
with query time  and space requirement
, for .
Then we can answer -ANN queries in time
 and space
.
Here,  is an arbitrary constant and the -notation
hides exponential factors in , , and  and
polynomials in .

Our approach generalizes the techniques of AIKN for
-ANN: we partition  into \emph{clusters} of increasing
radius, and we build a low-dimensional data structure for
a random projection of . Given a query flat , the
query can be answered directly in clusters whose radius is
``small'' compared to  using a grid. For the
remaining points, the low dimensional approximation turns out to
be precise enough.
Our new data structures also give an improvement in the space
requirement over the previous result for -ANN: we can achieve
near-linear space and sublinear query time, a further step
towards practical applications where space  constitutes
the bottleneck.
\end{abstract}
\newpage
\setcounter{page}{1}

\section{Introduction}

Nearest neighbor search is a fundamental
problem in computational geometry, with
countless applications in databases,
information retrieval, computer vision,
machine learning, signal processing,
etc.~\cite{Indyk04}.  Given a set
 of  points in
-dimensional
space, we would like to preprocess 
so that for any query point ,
we can quickly find the point in 
that is closest to .

There are efficient
algorithms if the dimension  is
``small''~\cite{Clarkson88,Meiser93}.
However, as  increases, these
algorithms quickly become inefficient:
either the query time approaches linear
or the space grows exponentially with
. This phenomenon is usually called
the ``curse of dimensionality''.
Nonetheless, if one is satisfied with
just an \emph{approximate} nearest
neighbor whose distance to the query
point  lies within some factor
, , of the distance between
 and the actual nearest neighbor,
there are efficient solutions even for high
dimensions. Several methods are known,
offering trade-offs between the
approximation factor, the space
requirement, and the query time (see,
e.g.,~\cite{Andoni09,AndoniInNgRa14}
and the references therein).

From a practical perspective, it is
important to keep both the query time
and the space small. Ideally, we would
like algorithms with almost linear
(or at least sub-quadratic) space
requirement and sub-linear query time.
Fortunately, there are solutions with
these guarantees. These methods
include \emph{locality sensitive hashing}
(LSH)~\cite{IndykMo98,KushilevitzOsRa98}
and a more recent approach that improves
upon LSH~\cite{AndoniInNgRa14}. Specifically,
the latter algorithm achieves query time
 and space
, where
 is the approximation factor.

Often, however, the query object is more
complex than a single point. Here,
the complexity of the problem is much less
understood. Perhaps the simplest such
scenario occurs when the query object is
a -dimensional flat, for some small
constant . This is called the
\emph{approximate -flat nearest neighbor}
problem~\cite{AndoniInKrNg09}. It constitutes
a natural generalization of approximate
nearest neighbors, which corresponds
to .  In practice, low-dimensional
flats are used to model data subject
to linear variations.  For example, one
could capture the appearance of a
physical object under different lighting
conditions or under different
viewpoints (see~\cite{BasriHaZe07} and the references therein).

So far, the only known algorithm with
worst-case guarantees is for , the
\emph{approximate line nearest neighbor}
problem. For this case, Andoni, Indyk,
Krauthgamer, and Nguy\~{\^{e}}n (AIKN) achieve
sub-linear query time 
and space ,
for arbitrarily small . For the
``dual'' version of the problem, where
the query is a point but the data set
consists of -flats, three results are
known~\cite{BasriHaZe07,Mahabadi15,Magen07}. The
first algorithm is essentially a
heuristic with some control of the
quality of
approximation~\cite{BasriHaZe07}.
The second algorithm provides
provable guarantees and a very fast
query time of
~\cite{Magen07}.
The third result, due to Mahabadi, is very recent
and improves the space requirement of Magen's
result~\cite{Mahabadi15}. Unfortunately, these algorithms
all suffer from very high space requirements, thus limiting
their applicability in practice. In fact, even the basic LSH
approach for  is already too expensive for large
datasets and additional theoretical work and heuristics are
required to reduce the memory usage and make LSH suitable
for this setting~\cite{Panigrahy06,LvJoWaChLi07}.
For , we know of no results in the
theory literature.

\noindent
\textbf{Our results.}
We present the first efficient data structure
for general approximate -flat nearest
neighbor search. Suppose we have a data structure
for approximate \emph{point} nearest neighbor
search with query time  and space
, for some constants .
Then our algorithm achieves
query time 
and space
, where
 can be made arbitrarily small.
The constant factors for the query time depend on
, , and . Our main result is as follows.

\begin{theorem}\label{thm:main}
  Fix an integer  and an approximation factor .
  Suppose we have a data structure
  for approximate \emph{point} nearest neighbor
  search with query time  and space
  , for some constants .
  Let  be a -dimensional -point set.
  For any parameter , we
  can construct a data structure with
   space
  that can answer the following queries in
  expected time :
  given a -flat , find a point 
  with .
\end{theorem}

\begin{center}
\begin{tabular}{|c|c|c|}
  \hline
  Algorithm &  & \\
  \hline
  AINR~\cite{AndoniInNgRa14} & &
   \\
  LSH1~\cite[Theorem~3.2.1]{Andoni09} &  & \\
  LSH2~\cite[Theorem~3.4.1]{Andoni09} &  & \\
  \hline
\end{tabular}
\end{center}
The table above gives an overview of
some approximate point nearest neighbor structures
that can be used in Theorem~\ref{thm:main}.
The result by AINR gives the current best query performance for large
enough values of . For smaller , an approach
using locality sensitive hashing (LSH1) may be preferable.
With another variant of locality sensitive hashing (LSH2),
the space can be made almost linear, at the expense of a slightly
higher query time. The last result (and related practical
results, e.g.,~\cite{LvJoWaChLi07}) is of particular
interest in applications as the memory consumption is a major bottleneck in practice. It also improves
over the previous algorithm by AIKN for line queries.

Along the way towards Theorem~\ref{thm:main},
we present a novel data structure for
-flat \emph{near} neighbor reporting when the dimension  is
constant. The space requirement in this case is
 and the query time is
, where  is the answer set.
We believe that this data structure may be of independent
interest and may lead to further applications.
Our results provide a vast generalization of the result in  AIKN
and shows for the first time
that it is possible to achieve provably efficient
nearest neighbor search for higher-dimensional query objects.


\noindent
\textbf{Our techniques.}
Our general strategy is similar to the approach by AIKN.
The data structure  consists of two  main structures: the
\emph{projection structure} and the \emph{clusters}.
The projection structure works by projecting the point set
to a space of constant dimension and by answering the nearest
neighbor query in that space. As we will see, this suffices
to obtain a rough estimate for the distance, and it can be
used to obtain an exact answer if the point set is ``spread
out''.

Unfortunately, this does not need to be the case.
Therefore, we partition the point set into a sequence
of \emph{clusters}.
A cluster consists of  points and a -flat 
such that all points in the cluster are ``close'' to ,
where  is a parameter to be optimized.
Using a rough estimate from the projection structure, we can
classify the clusters as \emph{small} and \emph{large}.
The points in the large clusters are spread out and
can be handled through projection. The points in the small
clusters are well behaved and can be handled directly
in high dimensions using grids and discretization.

\noindent
\textbf{Organization.}
In order to provide the curious reader with quick gratification,
we will give the main data structure together with the properties
of the cluster and the projection structure in
Section~\ref{sec:mainDS}.  Considering these structures as
black boxes, this already proves Theorem~\ref{thm:main}.

In the remainder of the paper, we describe the details of the
helper structures. The necessary tools are introduced
in Section~\ref{sec:preliminaries}. Section~\ref{sec:clusterstructure}
gives the approximate nearest neighbor algorithm for small clusters.
In Section~\ref{sec:lowdimstructure}, we consider approximate
near neighbor reporting for -flats in constant dimension.
This data structure is then used for the projection
structures in Section~\ref{sec:projectionstructures}.

\section{Main Data Structure and Algorithm Overview}
\label{sec:mainDS}

We describe our main data structure for approximate
-flat nearest neighbor search. It relies on various substructures
that will be described in the following sections.
Throughout,  denotes a -dimensional -point set,
and  is the desired approximation factor.

Let  be a -flat in  dimensions.
The \textit{flat-cluster}  (or cluster for short)
of  with radius  is
the set of all points with distance at most  to , i.e.,
.
A cluster is \emph{full}
if it contains at least  points from , where  is a
parameter to be determined. We call 
\textit{-cluster-free} if there is no full cluster with
radius .
Let  be an arbitrarily small parameter.
Our data structure requires the following three subqueries.
\begin{enumerate}[\bfseries Q1:]
 \item
\label{itm:q1}
Given a query flat ,
find a point   with .
\item
\label{itm:q2}
Assume  is contained in a flat-cluster with radius .
Given a query flat  with  ,
return a point  with .
\item
\label{itm:q3}
Assume  is -cluster free.
Given  a query flat  with
, find the nearest neighbor  to .
\end{enumerate}

Briefly, our strategy is as follows: during the preprocessing
phase, we partition the point set into a set of full
clusters of increasing radii. To answer a query , we first perform
a query of type \textbf{Q1} to obtain an -approximate
estimate  for . Using ,
we identify the ``small'' clusters. These clusters can be
processed using a query of type \textbf{Q2}.
The remaining point set contains no ``small'' full cluster, so we
can process it with a query of type \textbf{Q3}.

We will now describe the properties of the subqueries and the
organization of the data structure in more detail.
The data structure for \textbf{Q2}-queries is called the
\emph{cluster structure}. It is described in
Section~\ref{sec:clusterstructure}, and it has
the following properties.

\begin{theorem}\label{thm:q2}
  Let  be a -dimensional -point set
  that is contained in a flat-cluster of radius .
  Let  be an approximation factor.
  Using space
  , we can build a data
  structure with the following property.
  Given a query -flat  with 
  and an estimate  with
  ,
  we can find
  a -approximate nearest neighbor for  in  in total
  time
  .
\end{theorem}
The data structures for \textbf{Q1} and \textbf{Q3} are very similar,
and we cover them in Section~\ref{sec:projectionstructures}.
They are called \emph{projection structures}, since
they are based on projecting  into a low dimensional
subspace.
In the projected space, we use a
data structure for approximate -flat \textit{near}
neighbor search to be described in
Section~\ref{sec:lowdimstructure}.
The projection structures have the following properties.
\begin{theorem}
 \label{thm:q1}
 Let  be a -dimensional -point set, and let  be
 a small enough constant.
Using space and time , we can obtain
a data structure for the following query:
given a -flat , find a point  with
.
A query needs  time, and
the answer is correct with high probability.
\end{theorem}

\begin{theorem}
 \label{thm:q3}
 Let  be a -dimensional -point set, and let  be
 a small enough constant.
Using space and time , we can obtain
a data structure for the following query:
given a -flat  and  such that
 and such that  is
-cluster-free, find an exact nearest neighbor for
 in . A query needs 
time, and the answer is correct with high probability.
Here,  denotes the size of a full cluster.
\end{theorem}

\subsection{Constructing the Data Structure}

First, we build a projection structure for \textbf{Q1} queries
on . This needs  space,
by Theorem~\ref{thm:q1}.
Then, we repeatedly find the full flat-cluster  with smallest
radius.
The  points in  are removed from , and we build
a cluster structure for \textbf{Q2} queries on this set.
By Theorem~\ref{thm:q2}, this needs
 space.
To find , we check all flats  spanned
by  distinct points of . In Lemma~\ref{lem:pert} below,
we prove that this provides a good enough approximation.
In the end, we have  point sets
 ordered by decreasing radius,
i.e., the cluster for  has the largest radius.
The total space occupied  by the cluster structures is
.

Finally, we build a perfect binary tree  with  leaves
labeled , from left to right.
For a node 
let  be the union of all  assigned to leaves below .
For each  we build
a data structure for  to answer \textbf{Q3} queries.
Since each point is contained in  data structures,
the total size is , by
Theorem~\ref{thm:q3}.
For pseudocode, see Algorithm~\ref{alg:high-dim-preprocessing}.

\begin{alg}
\DontPrintSemicolon
\SetKw{DownTo}{downto}
\SetKwInOut{Input}{Input}
\Input{point set , approximation factor ,
parameter }
\;
\For{ \DownTo }{
For each , consider
the -flat  defined by . Let  be the
radius of the smallest flat-cluster of  with
exactly  points of . \;
Choose the flat  that minimizes
 and set . \;
Remove from  the set  of  points in
 within distance  from .\;
Construct a cluster structure  for the cluster
.\;
}
Build a perfect binary tree  with  leaves,
labeled  from left to right.\;
\ForEach{node }{
  Build data structure for \textbf{Q\ref{itm:q3}}  queries as in
  Theorem~\ref{thm:q3}
  for the set  corresponding to the leaves below .
}
\caption{Preprocessing algorithm. Compared
  with AIKN~\cite{AndoniInKrNg09}, we
  organize the  projection structure in a tree to save space.}
\label{alg:high-dim-preprocessing}
\end{alg}

\subsection{Performing a Query}
Suppose we are given a -flat .
To find an approximate nearest neighbor
for  we proceed similarly as AIKN~\cite{AndoniInKrNg09}. We use
\textbf{Q2} queries on
``small'' clusters  and \textbf{Q3} queries on the remaining points;
for pseudocode, see Algorithm~\ref{alg:high-dim-query}.

\begin{alg}
  \DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
  \Input{query flat }
  \Output{a -approximate nearest neighbor for  in }
  Query the root of  for a
  -approximate nearest neighbor  to .
  \tcc*{type Q\ref{itm:q1}}
  \;
   maximum
   with ,
  or 
  if no such  exists\;

  \For{ \KwTo }{
    \tcc{type Q\ref{itm:q2}; we have
      }
    Query cluster structure  with estimate .
  }
 \tcc{type Q\ref{itm:q3}}
  Query projection structure for a
  -thresholded nearest neighbor of  in
  .
  \Return closest point to  among query results.
 \caption{Algorithm for finding approximate nearest neighbor in
   high dimensions.}
 \label{alg:high-dim-query}
\end{alg}

First, we perform a query of type \textbf{Q1}
to get a -approximate nearest neighbor  for  in time
. Let .
We use  as an estimate to distinguish between
``small'' and
``large'' clusters. Let 
be the largest integer such that the cluster assigned with 
has radius .
For ,
we use  as an estimate for a \textbf{Q2} query
on . Since
 and by Theorem~\ref{thm:q2},
this needs
total time
.

It remains to deal with points in ``large'' clusters.
The goal is to perform a type \textbf{Q3}
query on . For this, we start at the
leaf of  labeled  and walk up to the root.
Each time we encounter a new node  from its right child,
we perform a \textbf{Q3} query on , where  denotes the
left child of .
Let  be all the left children we find in this way. Then clearly
we have   and
.
Moreover, by construction, there is no full cluster with radius
less than
 defined by  vertices of 
for any .
We will see that this implies every  to be
-cluster-free, so
Theorem~\ref{thm:q3} guarantees a total query
time of  for this step.
Among all the points we obtained during the queries,
we return the one that is closest to .
A good trade-off point is achieved for ,
i.e., for .
This gives the bounds claimed in Theorem~\ref{thm:main}.

\noindent
\textbf{Correctness.}
Let  be a point with .
First, suppose that ,
for some . Then, we have
, where 
is the radius of the cluster assigned to .
Since  is a valid -approximate
estimate for ,
a query of type \textbf{Q2} on  gives a -approximate nearest
neighbor, by Theorem~\ref{thm:q2}.
Now, suppose that  for .
Let  be the node of 
with . Then Theorem~\ref{thm:q3} guarantees
that we will find
 when doing a \textbf{Q3} query on .

\section{Preliminaries}
\label{sec:preliminaries}

\noindent
\textbf{Partition Trees.}
Fix an integer constant , and
let  be a -dimensional -point set.
A \emph{simplicial -partition}  for  is a sequence
 of pairs
such that (i) the sets  form a partition of
 with , for
;
(ii) each  is a relatively open simplex with
, for ; and
(iii) every hyperplane  in  \emph{crosses} 
  simplices  in . Here, a hyperplane  \emph{crosses}
  a simplex  if  intersects , but does not
  contain it.
In a classic result, Matou\v{s}ek showed that such a simplicial
partition always exists and that it can be computed
efficiently~\cite{Matousek92,Chazelle00}.

\begin{theorem}[Partition theorem, Theorem~3.1 and Lemma~3.4
  in \cite{Matousek92}]\label{thm:partition}
  For any -dimensional -point set  and for
  any constant , there exists a simplicial
  -partition for . Furthermore, if  is bounded by
  a constant, such a partition can be found in time .\qed
\end{theorem}

Through repeated application of Theorem~\ref{thm:partition},
one can construct a \emph{partition tree} for .
A partition tree  is a rooted tree in which each node
is associated with a pair , such that  is a subset
of  and  is a relatively open simplex that
contains . If , the children of 
constitute a simplicial -partition of . Otherwise,
the node  has  children where each child
corresponds to a point in . A partition tree has
constant degree, linear size, and logarithmic depth.

Given a hyperplane , there is a straightforward query
algorithm to find the highest nodes in  whose
associated simplex does not cross : start at the root and
recurse on all children whose associated simplex crosses ;
repeat until there are no more crossings or until a leaf is reached.
The children of the traversed nodes whose simplices do not cross
 constitute the desired answer. A direct application of
Theorem~\ref{thm:partition} yields a partition tree for which this
query takes time , where  is
a constant that can be made arbitrarily small by increasing .
In 2012, Chan~\cite{Chan12} described a more global
construction that eliminates the  factor.

\begin{theorem}[Optimal Partition
   Trees~\cite{Chan12}]\label{thm:optpartition}
  For any -dimensional -point set ,
  and for any large enough constant ,
  there is a \emph{partition tree}  with the following
  properties:
   (i) the tree  has degree  and depth ;
    (ii) each node is of the form ,
     where  is a subset of  and  a relatively open
     simplex that contains ; (iii) for each node ,
     the simplices of the children of  are contained in 
     and are pairwise disjoint; (iv) the point set associated
     with a node of depth  has size at most ;
     (v) for any hyperplane  in , the number
      of simplices in   that
      intersects at level  obeys the recurrence
     
   Thus,  intersects  simplices in total.
 The tree  can be build in expected time .
\end{theorem}

\noindent
\textbf{-flat Discretization.}
For our cluster structure we must find -flats that are
close to many points. The following lemma shows that it
suffices to check ``few'' -flats for this.
\begin{lemma}\label{lem:pert}
  Let  be a finite point set with ,
  and let  be a -flat. There is a -flat 
  such that  is the affine hull of  points in  and
  ,
  where  and
  .
\end{lemma}
\begin{proof}
  This proof generalizes the proof of Lemma~2.3 by
  AIKN~\cite{AndoniInKrNg09}.

  Let  be the orthogonal projection of  onto .
  We may assume that  is the affine hull of , since otherwise
  we could replace  by  without affecting
  . We choose an orthonormal basis for  such that
   is the linear subspace spanned by the first  coordinates.
  An affine basis for  is constructed as follows: first,
  take a point  whose -coordinate is minimum. Let
   be the projection of  onto , and translate the
  coordinate system such that  is the origin.
  Next, choose  additional points  such
  that  is maximum, where  is
  the projection of  onto , for . That is,
  we choose  additional points such that the volume of the
  -dimensional parallelogram spanned by their projections
  onto  is maximized. The set  is a basis for
  , since the maximum determinant cannot be  by our assumption
  that  is spanned by .

  Now fix some point  and let  be its projection onto
  . We write . Then, the point
  
  lies in . By the triangle inequality, we have
  
  To upper-bound  we first show that all coefficients
   lie in .
  \begin{claim}\label{clm:coeffbound}
    Take ,   and 
    as above. Write . Then for
    , we have , and
     for at least one  .
  \end{claim}
  \begin{proof}
    We first prove that all coefficients  lie in the
    interval . Suppose that  for some
    . We may assume that .
    Using the linearity of the determinant,
    
     contradicting the choice of .

     Furthermore, by our choice of the origin,
     all points in  have a non-negative
     -coordinate.
     Thus, at least one coefficient , ,
     has to be non-negative.
  \end{proof}
  Using Claim~\ref{clm:coeffbound}, we can now bound .
  For , we write
  , where  is orthogonal to .
  Then,
  
  since
  ,
  and since  follows from
  fact that at least one  is non-negative.
  By (\ref{equ:flat_triang}) and (\ref{equ:perp_triang}), we get
  .
\end{proof}

\begin{remark}
  For , the proof of Lemma~\ref{lem:pert} coincides with
  the proof of Lemma~2.3 by AIKN~\cite{AndoniInKrNg09}. In this
  case, one can obtain a
  better bound on  since  is a convex combination
  of  and . This gives .
\end{remark}

\section{Cluster Structure}
\label{sec:clusterstructure}
A \emph{-flat cluster structure} consists of a -flat
 and a set  of  points with
, for all .
Let  be a parametrization of , with
 and  such that the columns
of  constitute an orthonormal basis for  and such that
 is orthogonal to .
We are also given an approximation parameter .
The cluster structure uses a data structure
for approximate point nearest neighbor search as a black box.
We assume that we have such a structure available that
can answer -approximate point nearest neighbor queries
in  dimensions with query time 
and space requirement  for
some constants . As mentioned in the
introduction, the literature offers several data structures
for us to choose from.

The cluster structure distinguishes two cases:
if the query flat  is close to , we can approximate
 by few ``patches'' that are parallel to , such that
a good nearest neighbor for the patches is also
good for . Since the patches are parallel to , they
can be handled through -ANN queries in the orthogonal
space  and low-dimensional
queries inside .
If the query flat is far from , we can approximate 
by its projection onto  and handle the query with a
low-dimensional data structure.

\subsection{Preprocessing}

Let  be the linear subspace of  that is orthogonal
to . Let  be the projection of  onto ,
and let  be the projection of  onto .
We compute a -dimensional partition
tree  for .
As stated in Theorem~\ref{thm:optpartition}, the tree 
has  nodes, and it can be computed in time .

For each node  of , we do the following:
we determine the set  whose projection onto 
gives , and we take the projection  of  onto
. Then, we build a  dimensional -ANN data
structure for , as given by the assumption,
where . See
Algorithm~\ref{alg:preprocessCluster} for pseudocode.

\begin{alg}
\SetKwInOut{Input}{Input}
\SetKwFunction{CreatePartitionStructure}{CreatePartitionStructure}
\SetKwFunction{CreateSlabStructure}{CreateSlabStructure}
\Input{-flat , point set  with
   for all , approximation
  parameter }

   projection of  onto 

   projection of  onto 

  Build a -dimensional partition tree  for 
   as in Theorem~\ref{thm:optpartition}.

  

  \ForEach{node } {
       projection of the points in 
      corresponding to  onto 

      Build a -dimensional -ANN structure for 
      as given by the assumption.
}
 \caption{CreateClusterStructure}
 \label{alg:preprocessCluster}
\end{alg}

\begin{lemma}\label{lem:clusterPrep}
  The cluster structure can be constructed in total
  time  , and it requires
    space.
\end{lemma}
\begin{proof}
By Theorem~\ref{thm:optpartition}, the partition tree can be
built in  time. Thus,
the preprocessing time is dominated by the time to
construct the -ANN data structures at the nodes
of the partition tree .
Since the sets on each level of  constitute a
partition of , and since the sizes of the sets decrease
geometrically, the bounds on the preprocessing time and
space requirement follow directly from our assumption.
Note that by our choice of , the
space requirement and query time for the
ANN data structure change only by a constant factor.
\end{proof}

\subsection{Processing a Query}

We set .
Let  be the query -flat, given as ,
with  and  such
that the columns of  are an orthonormal basis for 
and  is orthogonal to . Our first task is to find bases
for the flats  and  that provide us
with information about the relative position of  and .
For this, we take the matrix
, and we compute a \emph{singular
value decomposition}  of ~\cite[Chapter~7.3]{HornJo13}.
Recall that  and  are orthogonal  matrices and
that 
is a  diagonal matrix
with .
We call  the \emph{singular values}
of . The following lemma summarizes the properties of
the SVD that are relevant to us.
\begin{lemma}\label{lem:svd}
  Let , and let  be a singular
  value decomposition for . Let  be
  the columns of  and  be the columns of .
  Then,
  (i)  is an orthonormal basis
  for  (in the coordinate system induced by );
  (ii)  is an orthonormal basis for 
  (in the coordinate system induced by ): and
  (iii) for , the projection of  onto
   is  and the projection of  onto
   is  (again in the coordinate systems
  induced by  and ).
  In particular, we have .
\end{lemma}
\begin{proof}
  Properties (i) and (ii) follow since  and 
  are orthogonal matrices.
  Property (iii) holds because  describes the projection
  from  onto  (in the coordinate systems induced by  and
  ) and because  describes the
  projection from  onto .
\end{proof}

We reparametrize  according to  and  according to .
More precisely, we set  and , and we write
 and .
The new coordinate system provides
a simple representation for the distances between  and .
We begin with a technical lemma that is a simple corollary of
Lemma~\ref{lem:svd}.
\begin{lemma}\label{lem:matrices}
  Let  be the columns of the matrix ;
  let  be the columns of
  the matrix , and  the
  columns of the matrix .
  Then, (i) for , the vector  is
  the projection of  onto  and the vector 
  is the projection of  onto ;
  (ii) for , we have
   and
   ; and
  (iii) the vectors  are pairwise orthogonal.
  An analogous statement holds for the matrices
  , , and .
\end{lemma}
\begin{proof}
  Properties (i) and (ii) are an immediate consequence of the
  definition of  and  and of Lemma~\ref{lem:svd}.
  The set  is orthogonal
  by Lemma~\ref{lem:svd}(ii). Furthermore, since for any
  , the vector
   lies in  and the vector
   lies in ,
   and  are orthogonal.
  Finally, let . Then,
  
  since we already saw that
  .
  The argument for the other matrices is
  completely analogous.
\end{proof}

The next lemma shows how our choice of bases gives a
convenient representation of the distances between  and
.
\begin{lemma}\label{lem:svddistance}
Take two points  and  such that
. Write  and
.
Then, for any point  with , we have

and for any point  with , we have

\end{lemma}
\begin{proof}
  We show the calculation for . The calculation for
   is symmetric. Let  with 
  be given. Let  be the projection of  onto .
  Then,
  
  where the last equality is due to Pythagoras, since
   lies in ,  lies in , and
   is orthogonal to both  and .
  Now,  we have
  . Similarly, since  is
  the projection of  onto , we have
  . Thus,
  
  using the definition of  and .
  By Lemma~\ref{lem:matrices}, the columns
   of the matrix
   are pairwise orthogonal and for
  , we have .
  Pythagoras gives
  
\end{proof}

\begin{alg}
\SetKwInOut{Input}{Input}
\SetKwFunction{CreatePartitionStructure}{CreatePartitionStructure}
\SetKwFunction{CreateSlabStructure}{CreateSlabStructure}
\Input{query -flat ;
  an estimate  with
  .}

  .

  Compute an SVD  of  with singular values
    .

  \If{} {

    projection of  onto 
    \tcc*{ and  are parallel;  is a point}

      -ANN for  in 

     \Return 
  }

  Reparametrize  according to  and  according to .


 \tcc{Near case}
   set of approximate patches obtained
    by combining Lemma~\ref{lem:tflats} and~\ref{lem:patches}

  

  \ForEach{} {
    G
  }
 \tcc{Far case}
    G

  \Return point in  that minimizes the distance to 
 \caption{QueryClusterStructure}
 \label{alg:queryCluster}
\end{alg}

We now give a brief overview of the query algorithm,
refer to Algorithm~\ref{alg:queryCluster} for pseudocode.
First,  we check for the special case that  and  are
parallel, i.e., that . In
this case, we need to perform only a single -ANN
query in  to obtain the desired result.
If  and  are not parallel, we distinguish two
scenarios: if  is far from , we can approximate  by
its projection  onto . Thus, we take the closest point
 in  to , and we return an approximate nearest neighbor for
 in  according to an appropriate metric derived
from Lemma~\ref{lem:svddistance}. Details can be found in
Section~\ref{sec:far}.
If  is close to , we use Lemma~\ref{lem:svddistance} to
discretize the relevant
part of  into \emph{patches}, such that each patch is
parallel to  and such that the best nearest
neighbor in  for the patches provides an approximate nearest
neighbor for . Each patch can then
be handled essentially by an appropriate nearest neighbor
query in . Details follow in Section~\ref{sec:close}.
 We say  and  are \emph{close}
if , and \emph{far} if
. Recall that we chose .


\subsubsection{Near: }\label{sec:close}

We use our reparametrization of  and  to split the coordinates
as follows: recall that
 are the singular
values of . Pick  such that
, for , and
, for .
For a  matrix , let
 denote the  matrix with the first 
columns of , and  the  matrix with
the remaining  columns of .
Similarly, for a vector , let
 be the vector in  with the  first 
coordinates of , and  the vector in  with the
remaining  coordinates of .

The following lemma is an immediate consequence of
Lemma~\ref{lem:svddistance}. It tells us that
we can partition the directions in  into those that are
almost parallel to  and those that are almost orthogonal
to . Along the orthogonal directions, we discretize
 into few lower-dimensional flats that are almost
parallel to . After that, we approximate these flats
by few patches that are actually parallel to .
These patches are then used to perform the query.

\begin{lemma}\label{lem:orthogDiag}
  Let  be a point and
   with  .
  Write  and .
  Then, .
\end{lemma}
\begin{proof}
  By Lemma~\ref{lem:svddistance} and the choice of ,
  
\end{proof}

Using Lemma~\ref{lem:orthogDiag}, we can discretize
the query  into a set of  -flats that are
almost parallel to the cluster flat .
\begin{lemma}\label{lem:tflats}
  There is a set   of
  
  -flats such
  that the following holds:
  (i) for every , we have
    ;
  (ii) for every  and for every unit vector
    , the projection
    of  onto  has length at least ; and
  (iii) if ,
  then there is an -flat  with
  .
\end{lemma}
\begin{proof}
  Let  be a point in  with
  .  Furthermore, let
  
  Using  and , we
  define a set  of \emph{index vectors} with
  
  and .
  For each , we define the -flat  as
  
  Our desired set of approximate query -flats is now .

  The set  meets properties (i) and (ii)
  by construction, so it remains to verify (iii).
  For this, we take a point  with .
  We write ,  and we define
  .
  We assumed that
  ,
  so Lemma~\ref{lem:orthogDiag} gives
  .
  It follows that by rounding each coordinate of  to
  the nearest multiple of ,
  we obtain an index vector  with
  .
  Hence, considering the point in  with ,
  we get
  
  where in (*) we used that the columns of
   are orthonormal and in (**) we used the
  assumption .
\end{proof}

From now on, we focus on an approximate query -flat
 with .
Our next goal is to approximate   by a set of patches such
that each is parallel to .

\begin{lemma}\label{lem:patches}
  There is a set   of 
  patches such
  that the following holds:
  (i) every  is an -dimensional
    polytope, given by  inequalities;
  (ii) for every ,
  the affine hull of  is parallel to ;
  (iii) if ,
  then there exists  with
  ;
  (iv) for all  and for all , we have
  .
\end{lemma}
\begin{proof}
Let  be the  matrix
whose columns  constitute
the projections of the columns of  onto .
By Lemma~\ref{lem:matrices},
the vectors  are  orthogonal with
, for
, and the columns

of the matrix  also constitute an orthogonal set,
with ,
for . Let 
be a point in  that minimizes the distance to , and write
.  Furthermore, let

We use the  and  to define a set  of
\emph{index vectors} as
.
We have .
For each index vector , we define the patch  as

Our desired set of approximate query patches is now
.
The set  fulfills
properties (i) and (ii) by construction, so it remains to check
(iii). Fix a point . Since , we can
write , where the vector 
represents the coordinates of  in  and the vector 
represents the coordinates of  in .
By Lemma~\ref{lem:svddistance},

where the vector  represents the coordinates of a point in
 that is closest to . By definition of , the last
 coordinates  in   are the same for all points
, so we can conclude that the coordinates for
a closest point to  in  are given by
 and that

Now take a point  in  with  and
write .
Since we assumed , (\ref{equ:distLK})
implies that for , we have
.
Thus, if for , we round
 down to the next multiple of
,
we obtain an index vector  with
.
We set . Considering the point
 in  , we see that

using the properties of the matrix  stated above.
It follows that

since we  assumed . This proves
(iii). Property (iv) is obtained similarly.
Let ,  and let
 be a point in . Write ,
where .
Considering the point  in ,
we see that

Thus,

\end{proof}

Finally, we have a patch , and we are
looking for an approximate nearest neighbor for  in .
The next lemma states how this can be done.
\begin{lemma}\label{lem:patch_NN}
  Suppose that . We can find a point
     with
    
    in total time
    .
\end{lemma}
\begin{proof}
  Let  be the projection  of  onto , and let
   be the projection of  onto .
  Since  and  are parallel,
   is a point, and
   is of the form , with
   and .
  Let , where
   denotes the
  -distance with respect to the coordinate system induced
  by .
  We subdivide the set ,
  into a collection  of
  axis-parallel cubes, each with diameter
  .
  The cubes in 
  have side length ,
  the total number of cubes is
  , and the
  boundaries of the cubes lie on
   hyperplanes.

  We now search the partition tree 
  to find the highest nodes  in
    whose simplices  are completely contained
  in a single cube of .
  This is done as follows: we begin at the root
  of , and we check for all children 
  and for all boundary hyperplanes  of 
  whether the simplex  crosses the boundary .
  If a child  crosses no hyperplane, we label it
  with the corresponding cube in  (or with ).
  Otherwise, we recurse on 
  with all the boundary hyperplanes that it crosses.

  In the end, we have obtained a set  of simplices
  such that each simplex in  is completely contained in
  a cube of .
  The total number of simplices
  in  is , by
  Theorem~\ref{thm:optpartition}.
  For each simplex in ,
  we query the corresponding -ANN structure.
  Let  be the set of the query results.
  For each point , we take the corresponding point
  , and we compute the distance .
  We return a point  that minimizes .
  The query time is dominated by the time for the ANN queries.
  For each , let  be the number
  of points in the corresponding ANN structure. By
  assumption, an ANN-query takes time
  ,
  so the total query time is proportional to
  
  using the fact that  is concave and
  that .

  It remains to prove that approximation bound.
  Take a point  in  with . Since
  we assumed that , the projection
   of  onto  lies in .
  Let  be the simplex in  with
  . Suppose that the ANN-query for 
  returns a point .
  Thus, in , we have
  , where  and  are
  the projections of  and  onto  and
   is the point set stored in the ANN-structure of
  .
  By the definition of , in , we have
  ,
  where  is the projection of  onto .
  By Pythagoras,
  
  recalling that  and .
  Since ,
  the result follows.
\end{proof}
\noindent
Of all the candidate points obtained through querying
patches, we return the one closest to .
The following lemma summarizes the properties of the
query algorithm.
\begin{lemma}\label{lem:nearQuery}
  Suppose that .
  Then the query procedure returns
  a point  with
   in
  total time
  .
\end{lemma}
\begin{proof}
  By Lemmas~\ref{lem:tflats} and~\ref{lem:patches},
  there exists a patch  with .
  For this patch, the algorithm from Lemma~\ref{lem:patch_NN}
  returns a point  with
  .
  Thus, using Lemma~\ref{lem:patches}(iv), we have
  
  and
  by our choice of , we get
    
\end{proof}

\subsubsection{Far: }\label{sec:far}

If , we can approximate  by its
projection  onto  without losing too much.
Thus, we can perform the whole algorithm in .
This is done by a procedure similar to Lemma~\ref{lem:patch_NN}.

\begin{lemma}\label{lem:farNNinQa}
  Suppose we are given an estimate 
  with .
  Then, we can find a point  with
   in
  time .
\end{lemma}
\begin{proof}
Let  be a point in  with
. Write .
Define

If we take a point  with
 and write
, then Lemma~\ref{lem:svddistance} gives

so  .
We subdivide  into copies of the
hyperrectangle
.
Let  be the resulting set of hyperrectangles.
The boundaries of the hyperrectangles in  lie on
 hyperplanes.
We now search the partition tree 
in order to find the highest nodes  in
  whose simplices  are completely contained
in a single hyperrectangle of .
This is done in the same way as in Lemma~\ref{lem:patch_NN}.

This gives a set  of simplices
such that each simplex in  is completely contained in
a hyperrectangle of .
The total number of simplices
in  is , by
Theorem~\ref{thm:optpartition}.
For each simplex ,
we pick an arbitrary point  that lies in
, and we compute . We return
the point  that minimizes the
distance to .
The total query time is .

Now let  be a point in  with ,
and let  be the simplex  that contains .
Furthermore, let  be the point that the algorithm
examines in . Write  and
. Since
 and  lie in the same hyperrectangle and by
Lemma~\ref{lem:svddistance},
  
Since , the result
follows.
\end{proof}

\begin{lemma}\label{lem:farQuery}
  Suppose we are given an estimate 
  with .
  Suppose further that .
  Then we can find a  with
   in
  time .
\end{lemma}
\begin{proof}
  For any point , let  be its projection
  onto . Then, .
  Thus, ,
  and we can apply Lemma~\ref{lem:farNNinQa}. Let
   be the result of this query, and let
   be the corresponding point in . We have
    
  by our choice of .
\end{proof}

By combining Lemmas~\ref{lem:clusterPrep},~\ref{lem:nearQuery},
and~\ref{lem:farQuery}, we obtain Theorem~\ref{thm:q2}.

\section{Approximate -flat Range Reporting in Low Dimensions}
\label{sec:lowdimstructure}

In this section, we present  a data structure for low dimensional
-flat approximate near neighbor reporting.
In Section~\ref{sec:projectionstructures}, we will use it as
a foundation for our projection structures.
The details of the structure are
summarized in Theorem~\ref{thm:lowdimstructure}.
Throughout this section,
we will think of  as a constant, and we will suppress
factors depending on  in the -notation.

\begin{theorem} \label{thm:lowdimstructure}
  Let  be an -point set.
  We can preprocess  into an  space data
  structure for approximate -flat near neighbor queries: given
  a -flat  and a parameter , find a set
   that contains all  with  and no  with
  . The
  query time is .
\end{theorem}

\subsection{Preprocessing}
\label{sec:lowdimpreproc}

Let  be the -dimensional subspace of 
spanned by the first  coordinates, and let  be the
projection of  onto .\footnote{We assume general position:
any two distinct points in  have distinct projections in .}
We build a -dimensional partition tree 
for , as in Theorem~\ref{thm:optpartition}.
If , we also build a \emph{slab structure}
for each node of .
Let  be such a node, and let  be the simplicial partition
for the children of .
Let . A \emph{-slab}  is a closed region in  that is
bounded by two parallel hyperplanes of distance .
The \emph{median hyperplane}  of  is the
hyperplane inside  that is parallel to the
two boundary hyperplanes and has distance  from both.
A -slab  is \emph{full} if there are at least 
simplices  in  with .

\begin{alg}
\SetKwInOut{Input}{Input}
\SetKwFunction{CreateSearchStructure}{CreateSearchStructure}
\SetKwFunction{CreateSlabStructure}{CreateSlabStructure}
\Input{point set }
\If{} {
Store  in a list and \Return.
}
 projection of  onto the subspace  spanned by the
first  coordinates.

 -dimensional partition tree for  as
in Theorem~\ref{thm:optpartition}.

\If{} {
  \ForEach{node }{
 simplicial partition for the children of 

\For{ \KwTo } {


  \CreateSlabStructure{}

  without all simplices inside the
 slab for \\
 }
  }
}
 \caption{CreateSearchStructure}
 \label{alg:createpartitionstructure}
\end{alg}

\begin{alg}
  \DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwFunction{CreateSearchStructure}{CreateSearchStructure}
  \SetKwFunction{CreateSlabStructure}{CreateSlabStructure}
  \Input{}

   vertices of the simplices in 

  For each -subset , find the
  smallest  such that the -slab with median
  hyperplane  is full.

  Let  be the smallest ; let  be the
  corresponding full -slab and  its median
  hyperplane.

  Find the set  of  simplices
  in ; let
  
  and let  be the -dimensional point set
  corresponding to .

   the hyperplane orthogonal to  through 

   projection of  onto 
  \tcc*[r]{ is -dimensional}

  \CreateSearchStructure{}
  \caption{CreateSlabStructure}
  \label{alg:createslabstructure}
\end{alg}

The slab structure for  is constructed in several iterations.
In iteration , we have a current subset 
of pairs in the simplicial partition.
For each -set  of vertices
of simplices in , we determine the smallest
width of a full slab whose median hyperplane is
spanned by . Let  be the smallest among those slabs,
and let  be its median hyperplane.
Let  be the  simplices that lie completely
in .  We remove  and the corresponding point
set 
from  to obtain . Let
 be the
-dimensional point set corresponding to .
We project  onto the -dimensional
hyperplane  that is orthogonal to  and goes through
. We recursively
build a search structure for the -dimensional projected
point set. The th slab structure  at  consists of this
search structure, the hyperplane
, and the width . This process is repeated until less than
 simplices remain; see
Algorithms~\ref{alg:createpartitionstructure}
and~\ref{alg:createslabstructure} for details.


Denote by  the space for a -dimensional search
structure with  points.
The partition tree  has  nodes, so the
overhead for storing the slabs and partitions is linear.
Thus,

where the sum is over all slab structures  and where  is the
number of points in the slab structure . Since every
point appears in  slab structures, and since the
recursion stops for , we get
\begin{lemma}
 \label{lem:space}
 The search structure for  points in  dimensions needs space
.\qed
\end{lemma}

\subsection{Processing a Query}

For a query, we are given a distance threshold  and a
-flat . For the recursion, we will need to query
the search structure with a -dimensional
polytope.  We obtain the initial query polytope by
intersecting the flat  with the bounding box of 
extended by  in each direction. With  slight abuse of
notation, we still call this polytope .

A query for  and  is processed by using the
slab structures for small enough slabs and by recursing in
the partition tree for the remaining points. Details follow.

Suppose we are at some node  of the partition tree, and
let  be the largest integer
with . For , we
recursively query each slab structure  as follows: let
 be the polytope containing the points
in  with distance at most 
from , and let  be the projection of 
onto .  We
query the search structure in  with  and .
Next, we project  onto the
subspace  spanned by the first  coordinates.
Let  be the simplices in  with
distance at most  from the projection.  For each simplex
in , we recursively query the corresponding child in
the partition tree. Upon reaching the bottom of the recursion
(i.e., ), we collect all points within distance
 from  in the set .

\begin{alg}
 \SetKwInOut{Input}{Input}
 \SetKwFunction{Query}{query}
 \SetKwInOut{Output}{Output}
 \Input{polytope , distance threshold }
 \Output{point set }
 

 \If{}{
   
 } \ElseIf{ } {
  \label{alg:querylinesegment:d2start}
 Compute polytope  as described.
 \label{querylinesegment:computerectangle}

 PF_{\diamond}
 \label{alg:querylinesegment:d2end}

 }
 \Else{
   the largest integer with
  

 \For{ \KwTo }{
  \label{alg:querylinesegment:slabqstart}
   projection of  onto  as described

  .\Query{, }
  \label{alg:querylinesegment:queryslabrecursively}
  }

   projection of  onto the subspace 
  spanned by the
  first  coordinates

  simplices in 

  

   \ForEach{}{
     \Delta
  \label{alg:querytrianglerecursive}
    }
 }
 \Return{}\label{alg:querylinesegment:return}
\caption{Find a superset  of all points in  with distance
less than  from a query polytope .}
 \label{alg:querylinesegment}
\end{alg}

If , we approximate the region of interest by the
polytope
,
where  denotes the -metric
in .
Then, we query the partition tree  to find all points
of  that lie inside .
We prove in Lemma~\ref{lem:querytime} that  is
a polytope with
 facets;
see Algorithm~\ref{alg:querylinesegment} for details.
The following two lemmas analyze the correctness and
query time of the algorithm.

\begin{lemma}
\label{lem:distancerror}
 The set  contains all  with   and
 no  with , where
 .
\end{lemma}
\begin{proof}
  The proof is  by induction on the size  of  and on
  the dimension .
  If , we return all points with distance at most
   to . If , we report the
  points inside the polytope
  
  (lines~\ref{alg:querylinesegment:d2start}--\ref{alg:querylinesegment:d2end})
  using .
  Since 
  holds for all , the polytope  contains
  all points with distance at most  from  and no point
  with distance more than  from . Thus,
  correctness also follows in this case.

  \begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.6]{img/distanceerror}
    \caption{The distance error due to the reduction of the dimension
      in the slab structure .}
    \label{fig:distanceerror}
  \end{figure}

  In the general case  ( and  not constant), we prove
  the lemma individually for the slab structures and for the
  partition tree.  Let  be a slab
  structure and  the corresponding -dimensional point set.
  Fix some point  with . To
  query , we take the subpolytope 
  with distance at most  from the
  median hyperplane , and we project it onto . Let
   be this projection.
  Since
  orthogonal projections can only
  decrease distances, we have
  \texttt{.query(}, \texttt{)} by induction.
  Now fix a point \texttt{.query(}, \texttt{)}.
  We must argue that .
  Let  be the projection of  onto 
  and  the closest point to
   in .
  Let  be the corresponding -dimensional
  point (see Fig.~\ref{fig:distanceerror}).
  By triangle inequality and the induction hypothesis,
  
  By construction, we have , so
  , as claimed.

  Consider now a child in the partition tree queried in
  line~\ref{alg:querytrianglerecursive}, and let  be the
  corresponding -dimensional point set. Since
  , the claim follows by induction.
\end{proof}

\begin{lemma}
  \label{lem:querytime}
  The query time is .
\end{lemma}
\begin{proof}
  Let  be the total query time.

  First, let .
  We bound the time to query the partition tree .
  Let  be the projection of  onto .
  Furthermore, let  be the set of nodes in  that
  are visited during a query, and let  be the
  corresponding
  simplices. By construction, all simplices in 
  have distance at most  from .
  Consider the -slab  whose
  median hyperplane contains .
  We partition  into two sets: the nodes
   whose simplices intersect , and the nodes
   whose simplices lie completely in .
  First, since the simplex for each node in  is
  contained in the simplex for its parent node, we observe that
   constitutes a connected subtree of , starting
  at the root.  The nodes of  form several connected subtrees,
  each hanging off a node in . Furthermore, by construction,
  each node from  has at most  children from .
  Let  be the set of nodes in  with level ,
  for ,
  and let .
  By Theorem~\ref{thm:optpartition}, we have
  . Since
  ,
  we get
  
  For any , if we choose
   large enough depending on , this solves to
  .
  Thus, we get
  

  \newcommand{\magicnumber}{((2k+2)(5d_0 - 2k)^{k/2})^{(k+1)/2}}
  For , we use  directly.
  Thus, by Theorem~\ref{thm:optpartition}, the query time 
  is , where
   is the number of facets of  and
   is the answer set.
  We claim that
   is bounded by .
  Recall that  is the Minkowski sum of  and
  the -ball with radius
   as in Algorithm~\ref{alg:querylinesegment}
  line~\ref{querylinesegment:computerectangle}.
  Initially  is the intersection of the query -flat with the
  extended bounding box of . This intersection can
  be described by at most
   oriented half-spaces, where  denotes
  the initial dimension. In each recursive
  step, we intersect  with the  bounding hyperplanes of a slab.
  Therefore, the descriptive complexity of  in the base case is
  at most . By duality and the Upper Bound
  theorem~\cite{Matousek02}, the -description of
   consists of at most  vertices. Using
  that the Minkowski sum of two
  polytopes with  and  vertices has at most
   vertices, we deduce that  has at
  most  vertices.
  Applying the upper bound theorem again, it follows that
  , as claimed.


  Thus, plugging the base case into (\ref{equ:qrec}),
  we get that the overall query time  is  bounded by
  .
\end{proof}

Theorem~\ref{thm:lowdimstructure} follows immediately from
Lemmas~\ref{lem:space}, \ref{lem:distancerror}, and
\ref{lem:querytime}.

\subsection{Approximate -Flat Nearest Neighbor Queries}
\label{sec:approximateNNlowdim}
We now show how to extend our
data structure from Section~\ref{sec:lowdimpreproc}
for approximate -flat nearest neighbor queries with
multiplicative error .
That is, given an -point set
, we want to find for any
given query flat 
a point  with .
We reduce this problem to a near neighbor query by choosing an
appropriate
threshold  that ensures
, using random sampling.
For preprocessing we build the data structure  from
Theorem~\ref{thm:lowdimstructure} for .

Let a query flat  be given.
The \emph{-rank} of a point  is the number of
points in  that are closer to  than .
Let  be a random sample obtained by
taking each point in  independently with probability .
The expected size of  is , and if  is the
closest point
to  in , then the expected -rank of  is . Set
. We query
 with  and  to obtain a set .
If , then
 contains the nearest neighbor. Otherwise,  is a
-approximate nearest neighbor for .
Thus, it suffices to return the nearest neighbor in .
Since with high probability all
points in  have -rank at most ,
we have , and the query time is
.
This establishes the following corollary of
Theorem~\ref{thm:lowdimstructure}.
\begin{corollary}
\label{cor:lowdimapprox}
Let  be an -point set.
We can preprocess  into an
 space data structure
for approximate -flat nearest neighbor queries:
given a flat , find a point
 with .
The expected query time is
.
\end{corollary}

\section{Projection Structures}
\label{sec:projectionstructures}

We now describe how to answer queries of type
\textbf{Q1} and \textbf{Q3} efficiently. Our approach is
to project the points into  random subspace of constant dimension
and to solve the problem there using our data structures from
Theorem~\ref{thm:lowdimstructure} and
Corollary~\ref{cor:lowdimapprox}.
For this, we need a Johnson-Lindenstrauss-type lemma that bounds the
distortion, see Section~\ref{sec:dimReduct}.

Let  be a parameter
and let  be a high dimensional -point set.
Set  and
let  be a random projection from
 to , scaled by .
We obtain  by projecting  using .
We build for  the
data structure  from Corollary~\ref{cor:lowdimapprox} to answer
\textbf{Q1} queries and  from Theorem~\ref{thm:lowdimstructure}
to answer \textbf{Q3} queries.
This needs  space.
For each  we write  for the
-dimensional point  and  for the projected
flat .

\subsection{Dimension Reduction}\label{sec:dimReduct}

We use the following variant of the Johnson-Lindenstrauss-Lemma,
as proved by Dasgupta and Gupta~\cite[Lemma~2.2]{DasguptaGu03}.

\begin{lemma}[JL-Lemma]\label{lem:jl}
  Let , and let  be the projection
  matrix onto a random -dimensional subspace, scaled by a factor
  of .
  Then, for every vector  of unit length and every
  , we have
  \begin{enumerate}
    \item\label{itm:jl-up}
      , and
    \item\label{itm:jl-low}
      .\qed{}
  \end{enumerate}
\end{lemma}

\begin{lemma}\label{lem:jl-kflat}
  Let  be a point and let  be a -flat.
  For , let  be
  the projection matrix into a random -dimensional
  subspace, scaled by . Let  and
  
  be the projections of  and of , respectively.
  Then, for any ,
     (i)
         ; and (ii)
         .
\end{lemma}
\begin{proof}
  Let , and set  and .
  Defining  and ,
  we must bound the probabilities
   for (i) and
   for (ii).

  We begin with (i).  Let 
  be the orthogonal projection of  onto , and let
  .
  Let . Then,
   and .
  By Lemma~\ref{lem:jl}(\ref{itm:jl-up}),
  
  Thus, , as desired.

  For (ii),  choose  orthonormal vectors
   such that
  .
  Set , and
  consider the lattice
  .
  Let  be the projected lattice.
  We next argue that with high probability
  (i) all points in  have
  distance at least  from ; and (ii)
  for , we have .

  To show (i), we partition  into \emph{layers}: for
  , let
  
  Now for any  and
  ,
  Pythagoras gives
  
  Thus, using Lemma~\ref{lem:jl}(\ref{itm:jl-low}),
  
  as . Now we use a union bound to obtain
  
  To show (ii), we use a union bound with
  Lemma~\ref{lem:jl}(\ref{itm:jl-up}). Recalling
  ,
  
  since  for .
  By (\ref{equ:bound_i}) and (\ref{equ:bound_ii}), and recalling
  ,
  the probability that events (i) and (ii) do not both happen
  is at most
  
Suppose (i) and (ii) happen. Fix a point
, and let  be the point in the projected
lattice that is closest to .
By (i), .
By (ii)
and the choice of ,
the -dimensional cube with center  and side length
 contains . This cube has diameter
.
By triangle inequality,
.
\end{proof}

\subsection{Queries of Type Q1}
Let a query flat  be given.
To answer \textbf{Q1} queries, we compute  and query
 with  to obtain a -nearest
neighbor . We return the original point .
To obtain Theorem~\ref{thm:q1}, we argue that if  is a
-nearest neighbor for , then  is
a -nearest neighbor for  with high probability.

Let  be a point with .
Set  and
.
Denote by  the event that .
By Lemma~\ref{lem:jl-kflat},
.
Let  be the event that for all points  with
 we have
.
For a fixed , by setting 
in Lemma~\ref{lem:jl-kflat},
this probability is


for  large enough. By the union bound, we
get , so the event  occurs
with constant probability. Then,  is a
-approximate nearest neighbor for , as desired.


\subsection{Queries of Type Q3}

To answer a query of type \textbf{Q3},
we compute the projection  and query  with parameter
. We obtain
a set  in time
.
Let  be the corresponding -dimensional set.
We return a point  that minimizes .
If , the event  from above implies
that , and we correctly return .

To bound the size of , and thus the running time, we use
that  is -cluster-free.
Let  be the event that for all  with
, we have
.
By the definition of cluster-freeness and the guarantee of
Theorem~\ref{thm:lowdimstructure}, we have  in
the case of .
Using  in Lemma~\ref{lem:jl-kflat}
and doing
a similar calculation as above yields again .
Thus, we can answer queries of type \textbf{Q3} successfully
in time  with constant probability,
as claimed in
Theorem~\ref{thm:q3}.

\section{Conclusion}
We have described the first provably efficient
data structure for general -ANN.
Our main technical contribution consists of two
new data structures: the cluster data structure
for high-dimensional -ANN queries, and the
projection data structure for -ANN queries
in constant dimension. We have only
presented the latter structure for a constant approximation factor
, but we believe that it is possible to extend
it to any fixed approximation factor .
For this, one would need to subdivide the slab structures by
a sufficiently fine sequence of parallel hyperplanes.

Naturally, the most pressing open question is to improve the
query time of our data structure. Also, a further generalization
to more general query or data objects would be of interest.

\section*{Acknowledgments}
This work was initiated while WM, PS, and YS
were visiting the Courant Institute of Mathematical Sciences.
We would like to thank our host Esther Ezra for her hospitality
and many enlightening discussions.

\bibliographystyle{abbrv}
\bibliography{literature}

\end{document}
