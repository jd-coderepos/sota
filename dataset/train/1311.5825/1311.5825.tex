\documentclass{llncs}
\usepackage{alltt}
\usepackage[pdftex]{graphicx}
\usepackage{stmaryrd}
\usepackage{amssymb}
\bibliographystyle{splncs}

\pdfinfo{/Author (David Van Horn and Harry G. Mairson)
         /Title (Flow analysis, linearity, and PTIME)
         /Keywords (complexity, flow analysis, 0CFA, simple closure analysis)}

\newcommand\True{\ensuremath{\mathit{True}}}
\newcommand\False{\ensuremath{\mathit{False}}}
\newcommand\TT{\ensuremath{\mathit{tt}}}
\newcommand\FF{\ensuremath{\mathit{ff}}}
\newcommand\Not{\ensuremath{\mathit{Not}}}
\newcommand\Copy{\ensuremath{\mathit{Copy}}}
\newcommand\And{\ensuremath{\mathit{And}}}

\newcommand{\compose}{\circ}
\newcommand{\loves}{\ensuremath{\vdash}}
\newcommand{\parr}{\bindnasrepma}
\newcommand\lambdap{\ensuremath{\lambda^*}}
\newcommand\unknown{\ensuremath{\mathit{unknown}}}
\newcommand\fv[1]{\ensuremath{\mathbf{fv}(#1)}}
\newcommand\lab[1]{\ensuremath{\mathbf{lab}(#1)}}
\newcommand\dom[1]{\ensuremath{\mathbf{dom}(#1)}}
\newcommand\ev[1]{\ensuremath{\mathcal{E}\sem{#1}}}
\newcommand\evn[2]{\ensuremath{\mathcal{E}{#1}\sem{#2}}}
\newcommand\av[1]{\ensuremath{\mathcal{A}\sem{#1}}}
\newcommand\sem[1]{\ensuremath{\llbracket #1 \rrbracket}}
\newcommand\nc{{\sc nc}}
\newcommand\ptime{{\sc ptime}}
\newcommand\logspace{{\sc logspace}}
\newcommand\cache{\widehat{\mathsf{C}}}
\newcommand\lcache{\mathsf{C}}  \newcommand\env{\widehat{\rho}}
\newcommand\ce{ce}
\newcommand\kmodels[2]{\models^{#1}_{#2}}
\newcommand\Cache{\mathbf{Cache}}
\newcommand\Lab{\mathbf{Lab}}
\newcommand\Lam{\mathbf{Lam}}
\newcommand\Nat{\mathbf{Nat}}
\newcommand\Term{\mathbf{Term}}
\newcommand\LExp{\mathbf{Exp}}
\newcommand\LEnv{\mathbf{Env}}
\newcommand\Exp{\mathbf{Exp}}
\newcommand\Env{\mathbf{Env}}
\newcommand\CEnv{\mathbf{CEnv}}
\newcommand\VEnv{\mathbf{VEnv}}
\newcommand\Var{\mathbf{Var}}
\newcommand\implies{\ensuremath{\Rightarrow}}
\newcommand\restrict{\ensuremath{\!\upharpoonright\!}}

\title{Flow analysis, linearity, and PTIME}
\author{David {Van Horn} \and Harry G.~Mairson}
\institute{
Department of Computer Science\\
Brandeis University\\
Waltham, Massachusetts 02454\\
\email{\{dvanhorn,mairson\}@cs.brandeis.edu}
}

\begin{document}

\mainmatter
\maketitle

\begin{abstract}

Flow analysis is a ubiquitous and much-studied component of compiler
technology---and its variations abound.  Amongst the most well known is
Shivers' 0CFA; however, the best known algorithm for 0CFA requires
time cubic in the size of the analyzed program and is unlikely to be
improved.  Consequently, several analyses have been designed to
approximate 0CFA by trading precision for faster computation.
Henglein's simple closure analysis, for example, forfeits the notion
of directionality in flows and enjoys an ``almost linear'' time
algorithm.  But in making trade-offs between precision and complexity,
what has been given up and what has been gained?  Where do these
analyses differ and where do they coincide?

We identify a core language---the linear -calculus---where
0CFA, simple closure analysis, and many other known approximations or
restrictions to 0CFA are rendered identical.  Moreover, for this core
language, analysis corresponds with (instrumented) evaluation.
Because analysis faithfully captures evaluation, and because the
linear -calculus is complete for \ptime, we derive
\ptime-completeness results for all of these analyses.
\end{abstract}






\section{Introduction}
\label{sec:intro}


Flow analysis \cite{jones-81,sestoft-88,shivers-phd,midtgaard-07} is
concerned with providing sound approximations to the question of
``does a given value flow into a given program point during
evaluation?''  The most approximate analysis will always answer {\em
yes}, which takes no resources to compute---and is of little use.  On
the other hand, the most precise analysis will answer {\em yes} if and
only if the given value flows into the program point during
evaluation, which is useful, albeit uncomputable.  In mediating
between these extremes, every static analysis necessarily gives up
valuable information for the sake of computing an answer within
bounded resources.  Designing a static analyzer, therefore, requires
making trade-offs between precision and complexity. But what exactly
is the analytic relationship between forfeited information and
resource usage for any given design decision?  In other words:
\begin{center}
{\em What are the computationally potent ingredients in a
static analysis?}
\end{center}

The best known algorithms to compute Shivers' 0CFA \cite{shivers-88},
a canonical flow analysis for higher-order programs, are cubic in the
size of the program, and there is strong evidence to suggest that in
general, this cannot be improved \cite{heintze-mcallester-97b}.
Nonetheless, information can be given up in the service of quickly
computing a necessarily less precise analysis, avoiding the ``cubic
bottleneck.''  For example, by forfeiting 0CFA's notion of
directionality, algorithms for Henglein's simple closure analysis run
in near linear time \cite{henglein92d}.  Similarly, by explicitly
bounding the number of passes the analyzer is allowed over the
program, as in Ashley and Dybvig's sub-0CFA \cite{ashley-dybvig}, we
can recover running times that are linear in the size of the program.
But the question remains: Can we do better?  For example, is it
possible to compute these less precise analyses in logarithmic space?
We show that without profound combinatorial breakthroughs (\ptime\ 
\logspace), the answer is no.  Simple closure analysis, sub-0CFA, and
other analyses that approximate or restrict 0CFA, {\em require}---and
are therefore, like 0CFA \cite{vanhorn-mairson-07}, complete
for---polynomial time.


\paragraph{What is flow analysis?}

Flow analysis is {\em the} ubiquitous static analysis of higher-order
programs.  As Heintze and McAllester remark, some form of flow
analysis is used in most forms of analysis for higher-order languages
\cite{heintze-mcallester-97a}.  It answers fundamental questions such
as {\em what values can a given subexpression possibly evaluate to at
run-time?}  Each subexpression is identified with a unique
superscripted {\em label} , which serves to index that program
point.  The result of a flow analysis is a {\em cache}  that
maps program points (and variables) to sets of values.  These analyses
are {\em conservative} in the following sense: if  is in
, then the subexpression label  {\em may} evaluate
to  when the program is run (likewise, if  is in ,
 may be bound to  when the program is run).  But if  is {\em
not} in , we know that  {\em cannot} evaluate to 
and conversely if  evaluates to ,  {\em must} be in
.

For the purposes of this paper and all of the analyses considered
herein, values are (possibly open) -abstractions.  During
evaluation, functional values are denoted with {\em closures}---a
-abstraction together with an {\em environment} that closes
it.  Values considered in the analysis approximate run-time
denotations in the sense that if a subexpression labeled 
evaluates to the closure , then
 is in .

The {\em acceptability} of a flow analysis is often specified as a set
of (in)equations on program fragments.  The most naive way to compute
a satisfying analysis is to initialize the cache with the flow sets
being empty.  Successive passes are then made over the program,
monotonically updating the cache as needed, until the least fixed
point is reached.  The more approximate the analysis, the faster this
algorithm converges on a fixed point.  The key to a fruitful analysis,
then, is ``to accelerate the analysis without losing too much
information'' \cite{ashley-dybvig}.

One way to realize the computational potency of a static analysis is
to subvert this loss of information, making the analysis an {\em
exact} computational tool.  Proving lower bounds on the expressiveness
of an analysis becomes an exercise in hacking, armed with this
newfound tool.  Clearly the more approximate the analysis, the less
there is to work with, computationally speaking, and the more we have
to do to undermine the approximation.  But a fundamental technique
allows us to understand expressivity in static analysis: {\em
linearity}.  This paper serves to demonstrate that linearity renders a
number of the most approximate flow analyses both equivalent and
exact.

\paragraph{Linearity and approximation in static analysis:}

Linearity, the Curry-Howard programming counterpart of linear logic
\cite{girard-ll}, plays an important role in understanding static
analyses.  The reason is straightforward: because static analysis has
to be tractable, it typically approximates normalization, instead of
simulating it, because running the program may take too long.  For
example, in the analysis of simple types---surely a kind of static
analysis---the approximation is that all occurrences of a bound
variable must have the same type (as a consequence, perfectly good
programs are rejected).  A comparable but not identical thing happens
in the case of type inference for ML and bounded-rank intersection
types---but note that when the program is linear, there is no
approximation, and type inference becomes evaluation under another
name.

In the case of flow analysis, similarly, a cache is computed in the
course of an approximate evaluation, which is only an approximation
because each evaluation of an abstraction body causes the same piece
of the cache to be (monotonically) updated.  Again, if the term is
linear, then there is only one evaluation of the abstraction body, and
the flow analysis becomes synonymous with normalization.

\paragraph{Organization of this paper:}

The next section introduces 0CFA in order to provide intuitions and a
point of reference for comparisons with subsequent analyses.  Section
\ref{sec:simple} specifies and provides an algorithm for computing
Henglein's simple closure analysis. Section \ref{sec:linearity}
develops a correspondence between evaluation and analysis for linear
programs.  We show that when the program is linear, normalization
and analysis are synonymous.  As a consequence the normal form of a
program can be {\em read back} from the analysis.  We then show in
Section \ref{sec:circuits} how to simulate circuits, the canonical
\ptime-hard problem, using linear terms.  This establishes the
\ptime-hardness of the analysis.  Finally, we discuss other
monovariant flow analyses and sketch why these analyses remain
complete for \ptime\ and provide conclusions and perspective.


\section{Shivers' 0CFA}
\label{sec:0cfa}

As a starting point, we consider Shivers' 0CFA
\cite{shivers-88,shivers-phd}.\footnote{It should be noted that
Shivers' original {\em zeroth-order control-flow analysis} (0CFA) was
developed for a core CPS-Scheme language, whereas the analysis
considered here is in direct-style.  Sestoft independently developed a
similar flow analysis in his work on globalization
\cite{sestoft-88,sestoft-89}.  See \cite{midtgaard-07,mossin-phd} for
details.}

\paragraph{The Language:} 
A countably infinite set of labels, which include variable names, is
assumed.  The syntax of the language is given by the following
grammar:


All of the syntactic categories are implicitly understood to be
restricted to the finite set of terms, labels, variables, etc.~that
occur in the {\em program of interest}---the program being analyzed.
The set of labels, which includes variable names, in a program
fragment is denoted .  As a convention, programs are assumed
to have distinct bound variable names.

The result of 0CFA is an {\em abstract cache} that maps each program
point (i.e., label) to a set of -abstractions which
potentially flow into this program point at run-time:

Caches are extended using the notation , and
we write  to mean .  It is convenient to sometimes think of caches
as mutable tables (as we do in the algorithm below), so we abuse
syntax, letting this notation mean both functional extension and
destructive update.  It should be clear from context which is implied.

\paragraph{The Analysis:} We present the specification of the analysis here in the style of
Nielson {\em et al.} \cite{nielson-nielson-hankin}.  Each
subexpression is identified with a unique superscripted {\em label}
, which marks that program point;  stores all
possible values flowing to point .  An {\em acceptable} flow
analysis for an expression  is written :

The  relation needs to be coinductively defined since
verifying a judgment  may obligate verification of
 which in turn may require verification of .  The above specification of acceptability, when read as a
table, defines a functional, which is monotonic,
has a fixed point, and  is defined coinductively as the
greatest fixed point of this functional.\footnote{See
\cite{nielson-nielson-hankin} for a thorough discussion of coinduction
in specifying static analyses.}

Writing  means ``the abstract cache 
contains all the flow information for program fragment  at program
point .''  The goal is to determine the {\em least} cache
solving these constraints to obtain the most precise analysis. Caches
are partially ordered with respect to the program of interest:

Since we are concerned only with the least cache (the most precise
analysis) we refer to this as {\em the} cache, or synonymously, {\em
the} analysis.

\paragraph{The Algorithm:}

These constraints can be thought of as an abstract evaluator---
 simply means {\em evaluate} , which
serves {\em only} to update an (initially empty) cache.



The abstract evaluator  is iterated until the finite cache
reaches a fixed point.\footnote{\label{fn:fineprint}A single iteration
of  may in turn make a recursive call  with no change
in the cache, so care must be taken to avoid looping.  This amounts to
appealing to the coinductive hypothesis  in
verifying .  However, we consider this inessential
detail, and it can safely be ignored for the purposes of obtaining our
main results in which this behavior is never triggered.}  Since the
cache size is polynomial in the program size, so is the running time,
as the cache is {\em monotonic}---we put values in, but never take
them out.  Thus the analysis and any decision problems answered by the
analysis are clearly computable within polynomial time.

\paragraph{An Example:} 

Consider the following program, which we will return to discuss
further in subsequent analyses:



The 0CFA is given by the following cache:

where we write  as shorthand for , etc.


\section{Henglein's simple closure analysis}
\label{sec:simple}

Simple closure analysis follows from an observation by Henglein some
15 years ago: he noted that the standard flow analysis can be computed
in dramatically less time by changing the specification of flow
constraints to use equality rather than containment
\cite{henglein92d}.  The analysis bears a strong resemblance to simple
typing: analysis can be performed by emitting a system of equality
constraints and then solving them using {\em unification}, which can
be computed in almost linear time with a union-find datastructure.

Consider a program with both  and  as subexpressions.
Under 0CFA, whatever flows into  and  will also flow into the
formal parameter of all abstractions flowing into , but it is not
necessarily true that whatever flows into  {\em also} flows into
 and {\em vice versa}.  However, under simple closure analysis,
this is the case.  For this reason, flows in simple closure analysis
are said to be {\em bidirectional}.

\paragraph{The Analysis:} 



\paragraph{The Algorithm:} 

We write  to mean
.


The abstract evaluator  is iterated until a fixed point is
reached.\footnote{The fine print of footnote \ref{fn:fineprint}
applies as well.}  By similar reasoning to that given for 0CFA, simple
closure analysis is clearly computable within polynomial time.

\paragraph{An Example:}

Recall the example program of the previous section:


Notice that  is applied to itself and then to , so  will be bound to both  and ,
which induces an equality between these two terms.  Consequently,
wherever 0CFA gives a flow set of  or , simple closure analysis will give .
The simple closure analysis is given by the following cache (new flows
are underlined):






\section{Linearity and normalization}
\label{sec:linearity}

In this section, we show that when the program is {\em linear}---every
bound variable occurs exactly once---analysis and normalization are
synonymous.

First, consider an evaluator for our language, :



We use  to range over {\em environments}, , and let  range over {\em
closures}, each comprising a term and an environment that closes the
term.  The function  is extended to closures and
environments by taking the union of all labels in the closure or in
the range of the environment, respectively.






Notice that the evaluator ``tightens'' the environment in the case of
an application, thus maintaining throughout evaluation that the domain
of the environment is exactly the set of free variables in the
expression.  When evaluating a variable occurrence, there is only one
mapping in the environment: the binding for this variable. Likewise,
when constructing a closure, the environment does not need to be
restricted: it already is.

In a linear program, each mapping in the environment corresponds to
the single occurrence of a bound variable.  So when evaluating an
application, this tightening {\em splits} the environment  into
, where  closes the operator,  closes
the operand, and .





\begin{definition}
Environment  {\em linearly closes}  (or  is a {\em linear closure}) iff  is linear, 
closes , and for all ,  occurs exactly once
(free) in ,  is a linear closure, and for all
 does not occur (free or bound) in . The
{\em size} of a linear closure  is defined as:

\end{definition}

The following lemma states that evaluation of a linear closure cannot
produce a larger value.  This is the environment-based analog to the
easy observation that -reduction {\em strictly} decreases the
size of a linear term.
\begin{lemma}\label{lem:smaller}
If  linearly closes  and , then
.
\end{lemma}
\begin{proof}
Straightforward by induction on , reasoning by case analysis
on .  Observe that the size strictly decreases in the application
and variable case, and remains the same in the abstraction case.  \qed
\end{proof}

\begin{definition}
A cache  {\em respects}  (written ) when,
\begin{enumerate}
\item  linearly closes ,
\item , and
\item .
\end{enumerate}
\end{definition}
Clearly,  when  is closed and linear,
i.e.~ is a linear program.

Assume that the imperative algorithm  of Section
\ref{sec:simple} is written in the obvious ``cache-passing''
functional style.  

\begin{theorem}\label{thm:main}
If , ,
, , and , then
, , and
.
\end{theorem}
An important consequence is noted in Corollary \ref{cor:normal}.

\begin{proof} By induction on , reasoning by case analysis on .
\begin{itemize}
\item Case .

Since  and  linearly closes , thus  and  linearly closes
.  By definition,

Again since , , with which the
assumption  implies

and therefore .  It
remains to show that .
By definition, .  Since  and  do not
occur in  by linearity and assumption, respectively, it
follows that  and the case
holds.

\item Case .

By definition,

and by assumption , so  and therefore
.
By assumptions  and
, it follows that
 and
the case holds.

\item Case . Let

Clearly, for ,  and


By induction, for  and .  From this, it is straightforward to observe that
 and  where  and  are disjoint.  So let
.  It is
clear that .  Furthermore,


By Lemma \ref{lem:smaller},
, therefore 

Let

and by induction, , , and .  Finally, observe that
,
, and
, so the case holds.
\end{itemize}
\qed
\end{proof}
We can now establish the correspondence between analysis and
evaluation.

\begin{corollary}\label{cor:normal}
If  is the simple closure analysis of a linear program
, then  where
 and .
\end{corollary}

By a simple replaying of the proof substituting the containment
constraints of 0CFA for the equality constraints of simple closure
analysis, it is clear that the same correspondence can be established,
and therefore 0CFA and simple closure analysis are identical for
linear programs.

\begin{corollary}
If  is a linear program, then  is the simple closure
analysis of  iff  is the 0CFA of .
\end{corollary}

\paragraph{Discussion:}

Returning to our earlier question of the computationally potent
ingredients in a static analysis, we can now see that when the term is
linear, whether flows are directional and bidirectional is irrelevant.
For these terms, simple closure analysis, 0CFA, and evaluation are
equivalent.  And, as we will see, when an analysis is {\em exact} for
linear terms, the analysis will have a \ptime-hardness bound.


\section{Lower bounds for flow analysis}
\label{sec:circuits}

There are at least two fundamental ways to reduce the complexity of
analysis.  One is to compute more approximate answers, the other is to
analyze a syntactically restricted language.

We use {\em linearity} as the key ingredient in proving lower bounds
on analysis.  This shows not only that simple closure analysis and
other flow analyses are \ptime-complete, but the result is rather
robust in the face of analysis design based on syntactic restrictions.
This is because we are able to prove the lower bound via a highly
restricted programming language---the linear -calculus.  So
long as the subject language of an analysis includes the linear
-calculus, and is exact for this subset, the analysis must be
at least \ptime-hard.

The decision problem answered by flow analysis, described colloquially
in Section \ref{sec:intro}, is formulated as follows:
\begin{description}
\item[Flow Analysis Problem:] Given a closed expression , a term
, and label , is  in the analysis of ?
\end{description}

\begin{theorem}
If analysis corresponds to evaluation on linear terms, the analysis is
\ptime-hard.
\end{theorem}
The proof is by reduction from the canonical \ptime-complete problem
\cite{ladner-75}:
\begin{description}
\item[Circuit Value Problem:] Given a Boolean circuit  of 
inputs and one output, and truth values , is
 accepted by ?
\end{description}

An instance of the circuit value problem can be compiled, using only
logarithmic space, into an instance of the flow analysis problem
following the construction in \cite{vanhorn-mairson-07}.  Briefly, the
circuit and its inputs are compiled into a linear -term,
which simulates  on  via {\em evaluation}---it normalizes
to true if  accepts  and false otherwise.  But since the
analysis faithfully captures evaluation of linear terms, and our
encoding is linear, the circuit can be simulated by flow analysis.

The encodings work like this: \TT\ is the identity on pairs, and \FF\
is the swap.  Boolean values are either  or
, where the first component is the ``real''
value, and the second component is the complement.  


The simplest connective is \Not, which is an inversion on pairs, like
\FF.  A {\em linear} copy connective is defined as:

The coding is easily explained: suppose  is \True, then  is identity and 
twists; so we get the pair .  Suppose 
is \False, then  twists and  is identity; we get
.  

The \And\ connective is defined as:


Conjunction works by computing pairs  and
.  The former is the usual conjuction on the
first components of the Booleans :  can be read as ``if  then , otherwise false
(\FF).''  The latter is (exploiting deMorgan duality) the disjunction of
the complement components of the Booleans: 
is read as ``if  (i.e.~if not ) then true (\TT), otherwise
 (i.e.~not ).''  The result of the computation is equal to
, but this leaves  unused, which
would violate linearity.  However, there is symmetry to this {\em
garbage}, which allows for its disposal.  Notice that, while we do not
know whether  is \TT\ or \FF\ and similarly for , we do know
that {\em one of them is \TT\ while the other is \FF}.  Composing the
two together, we are guaranteed that .  Composing
this again with another twist (\FF) results in the identity function
.  Finally, composing this with 
is just equal to , so , which is the desired result,
but the symmetric garbage has been {\em annihilated}, maintaining
linearity.

This hacking, with its self-annihilating garbage, is an improvement
over that given in \cite{mairson-jfp04} and allows Boolean computation
without K-redexes, making the lower bound stronger, but also
preserving all flows.  In addition, it is the best way to do circuit
computation in multiplicative linear logic, and is how you compute
similarly in non-affine typed -calculus.

We know from Corollary \ref{cor:normal} that normalization and
analysis of linear programs are synonymous, and our encoding of
circuits will faithfully simulate a given circuit on its inputs,
evaluating to true iff the circuit accepts its inputs.  But it does
not immediately follow that the circuit value problem can be reduced
to the flow analysis problem.  Let  be the encoding of
the circuit and its inputs.  It is tempting to think the instance of
the flow analysis problem could be stated:
\begin{center}
is \True\ in  in the analysis
of ?
\end{center}
The problem with this is there may be many syntactic instances of
``\True.''  Since the flow analysis problem must ask about a particular
one, this reduction will not work.  The fix is to use a context which
expects a boolean expression and induces a particular flow (that can
be asked about in the flow analysis problem) iff that expression
evaluates to a true value \cite{vanhorn-mairson-07}.

\begin{corollary}
Simple closure analysis is \ptime-complete.
\end{corollary}

\section{Other monovariant analyses}

In this section, we survey some of the existing monovariant analyses
that either approximate or restrict 0CFA to obtain faster analysis
times.  In each case, we sketch why these analyses are complete for
\ptime.

\subsection{Ashley and Dybvig's sub-0CFA}
\label{sec:sub0}

In \cite{ashley-dybvig}, Ashley and Dybvig develop a general framework
for specifying and computing flow analyses, which can be instantiated
to obtain 0CFA or Jagannathan and Weeks' polynomial CFA
\cite{jagannathan-weeks-95}, for example.  They also develop a class
of instantiations of their framework dubbed {\em sub-0CFA} that is
faster to compute, but less accurate than 0CFA.

This analysis works by explicitly bounding the number of times the
cache can be updated for any given program point.  After this
threshold has been crossed, the cache is updated with a distinguished
 value that represents all possible -abstractions
in the program.  Bounding the number of updates to the cache for any
given location effectively bounds the number of passes over the
program an analyzer must make, producing an analysis that is  in
the size of the program.  Empirically, Ashley and Dybvig observe that
setting the bound to 1 yields an inexpensive analysis with no
significant difference in enabling optimizations with respect to 0CFA.


The idea is the cache gets updated once ( times in general) before
giving up and saying all -abstractions flow out of this
point.  But for a linear term, the cache is only updated at most once
for each program point.  Thus we conclude even when the sub-0CFA bound
is 1, the problem is \ptime-complete.

As Ashley and Dybvig note, for any given program, there exists an
analysis in the sub-0CFA class that is identical to 0CFA (namely by
setting  to the number of passes 0CFA makes over the given
program).  We can further clarify this relationship by noting that for
all linear programs, all analyses in the sub-0CFA class are identical
to 0CFA (and thus simple closure analysis).

\subsection{Subtransitive 0CFA}

Heintze and McAllester \cite{heintze-mcallester-97b} have shown that
the ``cubic bottleneck'' of computing full 0CFA---that is, computing
all the flows in a program---cannot be avoided in general without
combinatorial breakthroughs: the problem is {\sc{2npda}}-hard, for
which the ``the cubic time decision procedure [\dots] has not been
improved since its discovery in 1968.''

Given the unlikeliness of improving the situation in general, Heintze
and McAllester \cite{heintze-mcallester-97a} identify several simpler
flow questions (including the decision problem discussed in the paper,
which is the simplest; answers to any of the other questions imply an
answer to this problem).  They give algorithms for simply typed terms
that answer these restricted flow problems, which under certain
conditions, compute in less than cubic time.

Their analysis is linear with respect to a program's graph, which in
turn, is bounded by the size of the program's type.  Thus, bounding
the size of a program's type results in a linear bound on the running
times of these algorithms.  If this type bound is removed, though, it
is clear that even these simplified flow problems (and their
bidirectional-flow analogs), are complete for \ptime: observe that
every linear term is simply typable, however in our lower bound
construction, the type size is proportional to the size of the circuit
being simulated.  As they point out, when type size is not bounded,
the flow graph may be exponentially larger than the program, in which
case the standard cubic algorithm is preferred.

Independently, Mossin \cite{mossin-98} developed a type-based analysis
that, under the assumption of a constant bound on the size of a
program's type, can answer restricted flow questions such as single
source/use in linear time with respect to the size of the explicitly
typed program.  But again, removing this imposed bound results in
\ptime-completeness.

As Hankin {\em et al.} \cite{hankin-games} point out: both Heintze
and McAllester's and Mossin's algorithms operate on type structure (or
structure isomorphic to type structure), but with either implicit or
explicit -expansion.  For simply typed terms, this can result in
an exponential blow-up in type size.  It is not surprising then, that
given a much richer graph structure, the analysis can be computed
quickly.  In this light, recent results on 0CFA of -expanded,
simply typed programs can be seen as an improvement of the
subtransitive flow analysis since it works equally well for languages
with first-class control and can be performed with only a fixed number
of pointers into the program structure, i.e.~it is computable in
\logspace\ (and in other words, \ptime\ 
\logspace\ up to ) \cite{vanhorn-mairson-07}.






\section{Conclusions and perspective}



When an analysis is {\em exact}, it will be possible to establish a
correspondence with evaluation.  The richer the language for which
analysis is exact, the harder it will be to compute the analysis.  As
an example in the extreme, Mossin \cite{mossin-97} developed a flow
analysis that is exact for simply typed terms.  The computational
resources that may be expended to compute this analysis are {\em ipso
facto} not bounded by any elementary recursive function
\cite{statman}.  However, most flow analyses do not approach this kind
of expressivity.  By way of comparison, 0CFA only captures \ptime, and
yet researchers have still expending a great deal of effort deriving
approximations to 0CFA that are faster to compute.  But as we have
shown for a number of them, they all coincide on linear terms, and so
they too capture \ptime.





We should be clear about what is being said, and not said.  There is a
considerable difference in practice between linear algorithms
(nominally considered efficient) and cubic algorithms (still feasible,
but taxing for large inputs), even though both are polynomial-time.
\ptime-completeness does not distinguish the two.  But if a
sub-polynomial (e.g., \logspace) algorithm was found for this sort of
flow analysis, it would depend on (or lead to) things we do not know
(\logspace\  \ptime).  Likewise, were a parallel implementation of
this flow analysis to run in logarithmic time (i.e., \nc), we would
consequently be able to parallelize every polynomial time algorithm
similarly.

A fundamental question we need to be able to answer is this: what can
be deduced about a long-running program with a time-bounded analyzer?
When we statically analyze exponential-time programs with a
polynomial-time method, there should be a analytic bound on what we
can learn at compile-time: a theorem delineating how exponential time
is being viewed through the compressed, myopic lens of polynomial time
computation.

For example, a theorem due to Statman \cite{statman} says this: let
{\bf P} be a property of simply-typed -terms that we would
like to detect by static analysis, where {\bf P} is invariant under
reduction (normalization), and is computable in elementary time
(polynomial, or exponential, or doubly-exponential, or\dots).  Then
{\bf P} is a {\em trivial} property: for any type , {\bf P} is
satisfied by {\em all} or {\em none} of the programs of type .
Henglein and Mairson \cite{henglein-mairson-popl91} have complemented
these results, showing that if a property is invariant under
-reduction for a class of programs that can encode all Turing
Machines solving problems of complexity class {\sc{f}} using
reductions from complexity class {\sc{g}}, then any superset is either
{\sc{f}}-complete or trivial.  Simple typability has this property for
linear and linear affine -terms
\cite{henglein-mairson-popl91,mairson-jfp04}, and these terms are
sufficient to code all polynomial-time Turing Machines.

We would like to prove some analogs of these theorems, with or without
the typing condition, but weakening the condition of ``invariant under
reduction'' to some {\em approximation} analogous to the
approximations of flow analysis, as described above.  We are motivated
as well by yardsticks such as Shannon's theorem from information
theory \cite{shannon}: specify a bandwidth for communication and an
error rate, and Shannon's results give bounds on the channel capacity.
We too have essential measures: the time complexity of our analysis,
the asymptotic differential between that bound and the time bound of
the program we are analyzing.  There ought to be a fundamental result
about what information can be yielded as a function of that
differential.  At one end, if the program and analyzer take the same
time, the analyzer can just run the program to find out everything.
At the other end, if the analyzer does no work (or a constant amount
of work), nothing can be learned.  Analytically speaking, what is in
between?



\paragraph{Acknowledgments:}

We are grateful to Olin Shivers and Matt Might for a long, fruitful,
and ongoing dialogue on flow analysis.  We thank the anonymous
reviewers for insightful comments.  The first author also thanks the
researchers of the Northeastern University Programming Research Lab
for the hospitality and engaging discussions had as a visiting
lecturer over the last year.

\bibliography{simpleclosure}
\end{document}
