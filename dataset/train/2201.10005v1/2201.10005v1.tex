\documentclass[nohyperref]{article}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{makecell}
\usepackage{multicol}
\usepackage[multiple]{footmisc}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage[accepted]{icml2022}

\newcommand\todo[1]{\textcolor{red}{[TODO: #1]}}

\begin{document}


\twocolumn[
\icmltitle{Text and Code Embeddings by Contrastive Pre-Training}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Arvind Neelakantan}{equal,comp}
\icmlauthor{Tao Xu}{equal,comp}
\icmlauthor{Raul Puri}{comp}
\icmlauthor{Alec Radford}{comp}
\icmlauthor{Jesse Michael Han}{comp}
\icmlauthor{Jerry Tworek}{comp}
\icmlauthor{Qiming Yuan}{comp}
\icmlauthor{Nikolas Tezak}{comp}
\icmlauthor{Jong Wook Kim}{comp}
\icmlauthor{Chris Hallacy}{comp}
\icmlauthor{Johannes Heidecke}{comp}
\icmlauthor{Pranav Shyam}{comp}
\icmlauthor{Boris Power}{comp}
\icmlauthor{Tyna Eloundou Nekoul}{comp}
\icmlauthor{Girish Sastry}{comp}
\icmlauthor{Gretchen Krueger}{comp}
\icmlauthor{David Schnurr}{comp}
\icmlauthor{Felipe Petroski Such}{comp}
\icmlauthor{Kenny Hsu}{comp}
\icmlauthor{Madeleine Thompson}{comp}
\icmlauthor{Tabarak Khan}{comp}
\icmlauthor{Toki Sherbakov}{comp}
\icmlauthor{Joanne Jang}{comp}
\icmlauthor{Peter Welinder}{comp}
\icmlauthor{Lilian Weng}{comp}
\end{icmlauthorlist}

\icmlaffiliation{comp}{OpenAI}

\icmlcorrespondingauthor{Arvind Neelakantan}{arvind@openai.com}

\icmlkeywords{Unsupervised Learning, Embeddings, Contrastive Learning}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search. 
\end{abstract}


\section{Introduction}
\begin{figure}[h]
\centering
\includegraphics[width=0.35\textwidth]{summary.pdf}
\caption{Average performance of unsupervised  models of different sizes across 22 tasks consisting of linear-probe classification, text search, and sentence similarity tasks.}
\label{fig:summary}
\end{figure}
Deep unsupervised learning with generative and embedding models has seen dramatic success in the past few years. Generative models \cite{elmo, t5, wavenet, dalle, gpt-3, codex} are trained to maximize the likelihood of observed data while embedding models are trained to distinguish observed data from noise \cite{inbatch,Oord,clip,align,simcse,contreiver}.  Generative models have been shown to produce realistic content and benefit many downstream applications, reducing the need for labeled training datasets. In generative models, the information about the input is typically distributed over multiple hidden states of the model. While some generative models \cite{vae,Kiros} can learn a single representation of the input, most autoregressive Transformer \cite{transformer} models do not \cite{t5, gpt-3, codex, dalle}. However, learning such a representation (or embedding) is necessary for many tasks. Systems that search over millions or billions of items require each entry to be embedded as a dense representation and build an index in advance to save computational costs at query time. These embeddings are useful features for classification tasks and can also enable data visualization applications via techniques such as clustering. Embedding models are explicitly optimized to learn a low dimensional representation that captures the semantic meaning of the input \cite{clip,align,declutr,simcse,contreiver}.   

In this work, we train embedding models using a contrastive learning objective with in-batch negatives \cite{inbatch,contrastive} on unlabeled data. The input is encoded with a Transformer encoder \cite{transformer} and we leverage naturally occurring paired data to construct training data with no explicit labels. Text embedding models are trained on paired text data where we consider neighboring pieces of text on the Internet as positive pairs. Code embedding models treat the top-level docstring in a function along with its implementation as a (text, code) pair. The training signal of the contrastive objective on its own is not sufficient to learn useful representations and we overcome this by initializing our model with other pre-trained models \cite{gpt-3,codex}. Finally, we find that it is critical to use a sufficiently large batch to achieve the optimal performance. We show that this simple recipe combining pre-trained model initialization, large-batch contrastive learning and training at scale, can produce text and code embeddings that possess a broad range of capabilities.

We train a series of unsupervised text embedding models () of different sizes, ranging from 300M to 175B parameters, and observe a consistent performance improvement with increasing model sizes (Figure \ref{fig:summary}). On classification accuracy averaging across 7 linear-probe classification tasks in SentEval \cite{senteval}, our largest unsupervised model achieves new state-of-the-art results with a relative improvement of 4\% and 1.8\% over the previous best unsupervised \cite{declutr} and supervised \cite{simcse} text embedding models, respectively.

Text embedding in previous work was studied under different domains, varying in data, training objective and model architecture. Precisely, sentence embedding \cite{sbert,simcse,declutr} and neural information retrieval \cite{ORQA, REALM, dpr, e2e, contreiver} have remained different research topics evaluated on distinct benchmarks, even though both aim to learn high-quality text representation.
However, we find the same model that achieves good performance on sentence embedding benchmarks, as discussed above, is also able to obtain impressive results on large-scale information retrieval. When evaluated on the MSMARCO passage ranking task \cite{msmarco} to search over 4M passages,  gets a relative improvement of 23.4\% over previous best unsupervised methods \cite{bm25}.  On the task of searching on 21M documents from Wikipedia,  obtains a relative improvement of 14.7\%, and 10.6\% over previous unsupervised methods \cite{contreiver} for Natural Questions \cite{nq} and TriviaQA \cite{trivia}, respectively. On TriviaQA, our unsupervised method is even competitive with fine-tuned models. 

Next, we train code embedding models () using the same recipe. Our models learn via (text, code) pairs, extracted from open source code. We evaluate our model on CodeSearchNet \cite{codesearchnet}, a commonly used code search benchmark, where the task is to find the most relevant code snippet given a natural language query. Our models achieve new state-of-the-art results with a 20.8\% relative improvement over the previous best result \cite{Guo}. Unlike text embedding models, we observe no performance improvement on code search when increasing the number of parameters of  from 300M to 1.2B.

Finally, we experiment with fine-tuning our models on several supervised datasets and study the transfer learning performance. When fine-tuned on NLI (Natural Language Inference) datasets, we see a further boost in linear-probe classification, outperforming the previous best transfer method \cite{simcse} by 2.2\%. On SST-2 sentiment classification \cite{sst}, we find that our representations are sufficiently descriptive that even a simple -NN classifier achieves results comparable to a linear-probe classifier. Interestingly, zero-shot performance with our embeddings outperforms the supervised neural network models introduced along with the release of the SST-2 dataset.  We also fine-tune the unsupervised model on MSMARCO and evaluate it on a suite of zero-shot search tasks in the BEIR benchmark \cite{beir}. In the transfer setting, our models achieve a 5.2\% relative improvement over previous methods \cite{contreiver} and is comparable even with methods \cite{colbert,splade,mini} that demand substantially more computation at test time.


\section{Approach}
\label{sec:approach}

Our models are trained with a contrastive objective on paired data. In this section, we present more details on the model architecture and the training objective. The training set consists of paired samples, , where  corresponds to a positive example pair, indicating that  and  are semantically similar or contextually relevant.

\subsection{Model}
\label{sec:model}

\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{fig2-model.pdf}
\caption{The encoder  maps input  to embedding . Special tokens, \texttt{[SOS]} and \texttt{[EOS]}, are appended to the start and end of the input sequence respectively. The last layer hidden state corresponding to the token \texttt{[EOS]} is extracted as the embedding of the input sequence.}
\label{fig:model}
\end{figure}

Given a training pair , a Transformer \cite{transformer} encoder  is used to process  and  independently. The encoder maps the input to a dense vector representation or embedding (Figure \ref{fig:model}). We insert two special token delimiters, \texttt{[SOS]} and \texttt{[EOS]}, to the start and end of the input sequence respectively. The hidden state from the last layer corresponding to the special token \texttt{[EOS]} is considered as the embedding of the input sequence.


\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{fig3-training.pdf}
\caption{The encoder  maps inputs  and , to embeddings,  and  independently. The similarity score between  and  is defined as the cosine similarity between these two embedding vectors.}
\label{fig:train}
\end{figure}

The Transformer encoder maps the input,  and , to embeddings,  and  respectively and the similarity between two inputs is quantified by the cosine similarity between their embeddings,  and  (Figure \ref{fig:train}).


where  is an operation to concatenate two strings together. We found that using different delimiters leads to more stable training. For , we use `['  as  and `]' as , while we use `\{' and `\}' as  and  respectively for . 

\subsection{Training Objective}
\label{sec:training}

The paired samples in the training set are contrasted against in-batch negatives \cite{contrastive,inbatch}. Contrastive learning with in-batch negatives has been widely used for unsupervised representation learning in prior work \cite{clip,align,Chen,contreiver}. For each example in a mini-batch of  examples, the other  in the batch are used as negative examples. The usage of in-batch negatives enables re-use of computation both in the forward and the backward pass making training highly efficient. The \texttt{logits} for one batch is a  matrix, where each entry  is given by,



where  is a trainable temperature parameter.

Only entries on the diagonal of the matrix are considered positive examples. The final training loss is the sum of the cross entropy losses on the row and the column direction, as described in the following numpy style pseudo code.



\begin{small}
\begin{verbatim}
labels = np.arange(M)
l_r = cross_entropy(logits, labels, axis=0)
l_c = cross_entropy(logits, labels, axis=1)
loss = (l_r + l_c) / 2
\end{verbatim}
\end{small}

We initialize our models with pre-trained generative language models.  is initialized with GPT models \cite{gpt-3} and  is initialized with Codex models \cite{codex}. When fine-tuning our models (Section \ref{sec:results}), the supervised training data like NLI datasets contain explicit negative examples and they are used along with the in-batch negatives. 

\section{Results}
\label{sec:results}

\begin{table}[]
\centering

\setlength{\tabcolsep}{4.5pt}
\begin{tabular}{l|r|r|r}
\Xhline{2.5\arrayrulewidth}
Model & Parameters &  Embed Dimensions & Batch size  \\ 
\Xhline{1\arrayrulewidth} & \-1.5ex]
BERT \cite{bert} & 78.7 & 86.2 & 94.4 & 88.7 & 84.4 & 92.8 & 69.4 & 84.9 \\
SimCSE \cite{simcse} & 84.7 & 88.6 & 95.4 & 87.5 & 89.5 & 95.0 & 72.4 & 87.6 \\
DECLUTR \cite{declutr} & 85.2 & 90.7 & 95.8 & 88.5 & 90.0 & 93.2 & \textbf{74.6} & 88.3 \\ \hline & \-1.5ex]
SBERT \cite{sbert} & 84.9 & 90.1 & 94.5 & 90.3 & 90.7 & 87.4 & 75.9 & 87.7 \\
SimCSE \cite{simcse} & 88.4 & 92.5 & 95.2 & 90.1 & 93.3 & 93.8 & 77.7 & 90.2 \\ \hline & \-1.5ex]
Zero-shot                   & 88.1 \\
Zero-shot with prompting    & 89.1 \\
-NN                      & 93.3 \\
Linear-probe                & 95.7 \\
Full fine-tuned SOTA        & 97.5 \\
\Xhline{2.5\arrayrulewidth}
\end{tabular}
\caption{Comparison of different classification strategies using the 6B  model fine-tuned on NLI data for SST-2 binary sentiment task \cite{sst}. Our zero-shot results are better than the \% accuracy obtained by supervised neural networks reported along with the release of the dataset \cite{sst}.}
\label{table:sst}
\end{table}


\subsubsection{Sentence Similarity}
\label{sec:sts}

On sentence similarity tasks in SentEval, we find that our models perform worse than previous SOTA methods (Table \ref{table:sts}). Sentence similarity is not a completely well-defined downstream task (e.g. are the sentences, `Jack loves Jill' and `Mary loves chocolates', similar?).\footnote{\url{https://twitter.com/yoavgo/status/1431299645570011142}}\footnote{\url{https://twitter.com/yoavgo/status/1483565266575540225?s=20}} For example, \citet{similarity} argue that two objects can be infinitely similar or dissimilar \cite{Vervaeke2012RelevanceRA}. A possible explanation for why our models perform better than prior work on search and classification but not on these tasks is that our models might not be optimized for the specific definition used by these sentence similarity benchmarks. It is important to note that previous embedding search methods do not report performance on sentence similarity tasks \cite{dpr, e2e, contreiver}.  More discussion on this phenomenon is presented in Section \ref{sec:behavior}.  



\begin{table}[]
\small
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{lcccccc}
\multicolumn{1}{r}{} STS & -12 & -13 & -14 & -15 & -16 & Avg \\ 
\Xhline{2.5\arrayrulewidth}
\multicolumn{7}{c}{Unsupervised} \\ 
\Xhline{1\arrayrulewidth} \-1.5ex]
SimCSE \cite{simcse} & \textbf{77.5} & \textbf{87.3} & \textbf{82.4} & \textbf{86.7} & 83.9 & \textbf{83.6} \\
\texttt{cpt-text} S & 72.8 & 80.6 & 78.7 & 84.7 & 82.0 & 79.8 \\
\texttt{cpt-text} M & 73.7 & 80.2 & 78.9 & 85.0 & 82.8 & 80.1 \\
\texttt{cpt-text} L & 71.8 & 79.7 & 79.0 & 85.8 & 84.0 & 80.1 \\
\texttt{cpt-text} XL & 72.3 & 80.3 & 78.9 & 85.1 & \textbf{85.1} & 80.3 \\
\Xhline{2.5\arrayrulewidth}
\end{tabular}
\caption{\texttt{cpt-text} performs worse than the previous best sentence embedding method on sentence similarity tasks. We investigate this result in more detail in Section \ref{sec:behavior}.}
\label{table:sts}
\end{table}


\subsection{Text Search}

Previous work on training embedding methods for search typically requires fine-tuning on a particular text search dataset \cite{dpr, e2e, Qu}. It is also common to have a multi-step setup where fine-tuned models rely on an expensive query and document cross-attention encoder in the final step \cite{Qu, mini}. In contrast, we push the limits of using a \textit{single} embedding model for large-scale semantic search.


\subsubsection{Large-Scale Search}
\label{sec:large}

First, we evaluate our models on several large-scale text search benchmarks. MSMARCO \cite{msmarco} requires the model to search over 4M documents while Natural Questions (NQ) \cite{nq} and TriviaQA \cite{trivia} involve searching over 21M Wikipedia documents. We use the FAISS library \cite{Johnson} to build the vector indices for approximate -nearest neighbor search. The \textit{same} unsupervised model discussed previously achieves impressive performance on semantic search. Table \ref{table:large} demonstrates that \texttt{cpt-text} outperforms prior unsupervised approaches by a big margin and larger model sizes consistently lead to improved performance. Surprisingly, on TriviaQA, our model is even competitive with fine-tuned models.

\begin{table}[]
\setlength{\tabcolsep}{4.5pt}
\centering
\begin{tabular}{lccc}
& MSMARCO  & NQ & TriviaQA  \\ 
\Xhline{2.5\arrayrulewidth}
Fine-tuned SOTA  & 44.3 & 84.8, 89.8  & 84.1, 87.8 \\
\Xhline{2.5\arrayrulewidth} \-1.5ex]
BM25 & 18.4 & 62.9, 78.3 & 76.4, 83.2 \\
ICT & - & 50.9, 66.8 & 57.5, 73.6 \\
MSS & - & 59.8, 74.9 & 68.2, 79.4 \\
Contriever & - & 67.2, 81.3 & 74.2, 83.2 \\
\Xhline{1\arrayrulewidth} \-1.5ex]
BM25 \cite{bm25} & \textbf{65.6} &	32.5 &	23.6 &	31.5 &	\textbf{36.7} &	78.9 &	66.5 &	\textbf{21.3} &	\textbf{31.3} &	\textbf{60.3} &	\textbf{75.3} &	\textbf{47.6} \\
Contriever \cite{contreiver} & 27.4 &	31.7 &	24.5 &	37.9 &	19.3 &	\textbf{83.5} &	64.9 &	15.5 &	29.2 &	48.1 &	68.2 &	40.9 \\  
\Xhline{1\arrayrulewidth} & \-1.5ex]
TAS-B \cite{tas} & 48.1 &	31.9 &	30.0 &	42.9 &	16.2 &	83.5 &	64.3 &	22.8 &	38.4 &	58.4 &	70.0 &	46.0 \\
ANCE \cite{Xiong} & 65.4 &	23.7 &	29.5 &	41.5 &	24.0 &	85.2 &	50.7 &	19.8 &	28.1 &	45.6 &	66.9 &	43.7 \\
Contriever \cite{contreiver}  & 59.6 &	32.8 &	32.9 &	44.6 &	23.0 &	\textbf{86.5} &	67.7 &	23.7 &	41.3 &	63.8 &	75.8 &	50.2 \\ 
\Xhline{1\arrayrulewidth} \-1.5ex]
docT5query \cite{doct5} & 71.3 & 32.8 & 29.1 & 34.9 & \textbf{34.7} & 80.2 & 67.5 & 20.1 & 33.1 & 58.0 & 71.4 & 48.5 \\
BM25+CE \cite{mini} & \textbf{75.7} & 35.0 & 34.7 & 31.1 & 27.1 & 82.5 & 68.8 & \textbf{25.3} & 39.2 & 70.7 & 81.9 & 52.0 \\
ColBERT v2 \cite{colbert} & 73.8 & 33.8 & 35.6 & 46.3 & 26.3 & 85.2 & 69.3 & 17.6 & \textbf{44.6} & 66.7 & 78.5 & 52.5 \\
Splade v2 \cite{splade} & 71.0 & 33.4 & 33.6 & 47.9 & 27.2 & 83.8 & 69.3 & 23.5 & 43.5 & 68.4 & \textbf{78.6} & 52.7 \\
\Xhline{2.5\arrayrulewidth}
\end{tabular}
\caption{Comparison of  to previous methods on 11 zero-shot search tasks in the BEIR evaluation suite \cite{beir}. Results are reported both in the unsupervised data setting and in the transfer data setting.  outperforms previous best embedding methods \cite{Xiong,tas,contreiver} in both the settings. In the unsupervised setting, BM25 \cite{bm25} still achieves the best performance while in the transfer setting  is competitive with methods that use substantially more compute at test time \cite{mini,colbert,splade}.}
\label{table:beir}
\end{table*}


\subsection{Code Search}
\label{sec:code}

We evaluate our code embedding models on the code search task using the CodeSearchNet benchmark \cite{codesearchnet}. Given a natural language query, the model is expected to retrieve the relevant code block among 1K candidates. The models are evaluated on 6 programming languages and our model achieves state-of-the-art results (Table \ref{table:code}). Unlike with text embeddings, we do not see a performance improvement with increased model size for code embeddings.

\begin{table}[]
\small
\setlength{\tabcolsep}{4pt}
\centering
\tabcolsep=0.105cm
\begin{tabular}{lccccccc}
& Go & Ruby & Python & Java & JS & PHP & Avg. \\
\Xhline{2.5\arrayrulewidth} \-1.5ex]
\texttt{cpt-code} S & \textbf{97.7} & \textbf{86.3} & 99.8 & 94.0 & 86.0 & 96.7 & 93.4 \\
\texttt{cpt-code} M & 97.5 & 85.5 & \textbf{99.9} & \textbf{94.4} & \textbf{86.5} & \textbf{97.2} & \textbf{93.5} \\
\Xhline{2.5\arrayrulewidth}
\end{tabular}
\caption{Comparison of  on code search across 6 programming languages \cite{codesearchnet} with CodeBERT \cite{codebert} and GraphCodeBERT \cite{Guo}. The task requires finding the relevant code block among 1K candidates for a given natural language query.  performs substantially better than previous methods on all the languages.}
\label{table:code}
\end{table}

We also evaluate on a harder setting of finding the relevant code block among 10K candidates instead of 1K. Here, we compare the performance of \texttt{cpt-text} models against \texttt{cpt-code} models (Table \ref{table:large_code}). It is interesting to see that text embedding performs fairly well in code search especially in Python. We see a drop in performance for code embedding models with increased distractors and still don’t see bigger models giving a boost in search performance. 

\begin{table}[]
\small
\setlength{\tabcolsep}{4pt}
\centering
\tabcolsep=0.11cm
\begin{tabular}{lccccccc}
 & Go & Ruby & Python & Java & JS & PHP & Avg. \\ 
\Xhline{2.5\arrayrulewidth} \-1.5ex]
\texttt{cpt-code} S & \textbf{90.4} & 80.6 & 98.8 & \textbf{81.9} & \textbf{76.1} & \textbf{85.3} & \textbf{85.5} \\
\texttt{cpt-code} M & 90.0 & \textbf{89.1} & \textbf{98.9} & 81.1 & 75.6 & 85.1 & 85.0 \\
\Xhline{2.5\arrayrulewidth}
\end{tabular}
\caption{Comparison of \texttt{cpt-code} vs \texttt{cpt-text} on large scale code search \cite{codesearchnet}. The task is to retrieve the relevant code block among 10K candidates for a given natural language query. It is interesting to note that  performs quite well on Python code search without explicitly training on (text, code) pairs.}
\label{table:large_code}
\end{table}


\subsection{Analysis}

\subsubsection{Effect of Batch Size}
\label{sec:bs}
Our ablation study highlights the effect of the model's batch size on the final performance. Table \ref{table:bs_effect} compares the performance of S (300M) \texttt{cpt-text} model trained with different batch sizes on the NQ development set. Since we train with in-batch negatives, a larger batch increases the chances of having hard negatives in a batch, resulting in a significant performance boost.

\begin{table}[]
\centering
\begin{tabular}{l|c}
\Xhline{2.5\arrayrulewidth}
Batch Size & MRR@10 \\ 
\hline  1536       & 71.4   \\ 
12288      & 84.7   \\ 
\Xhline{2.5\arrayrulewidth}
\end{tabular}
\caption{Performance of the \texttt{cpt-text} 300M model on NQ dev set given different training batch sizes.}
\label{table:bs_effect}
\end{table}


\subsubsection{Training Behavior}
\label{sec:behavior}

We observe that as we train our models for longer, the performance on search and classification tasks increases while the performance on sentence similarity tasks decreases (Figure \ref{fig:behavior}). As discussed previously, sentence similarity is not a well defined task. A hypothesis is that search tasks and sentence similarity tasks might have contradicting definitions. For example, a sentence and its negation could be considered as relevant during search, but  not ``similar" in sentence similarity tasks. It is also important to note that previous embedding search methods do not report performance on sentence similarity tasks \cite{dpr, e2e, contreiver} and previous sentence embedding methods do not evaluate on search tasks \cite{sbert,declutr,simcse}. When deciding the model checkpoints to use for evaluation, we assigned higher importance to search and classification tasks as they are commonly associated with clearly defined real-world applications while sentence similarity tasks are less so. 



\begin{figure}[h]
\centering
\includegraphics[width=8cm]{behavior.pdf}
\caption{Performance of M (1.2B)  model on classification, search and sentence similarity tasks at different training steps. While the performance on search and classification improves with longer training, the performance on sentence similarity degrades.}
\label{fig:behavior}
\end{figure}


\section{Related Work}
The goal of representation learning \cite{bengio} is to learn an embedding space in which similar examples stay close to each other while dissimilar ones are far apart \cite{hadsell}. In contrastive learning, the learning procedure is formulated as a classification problem given similar and dissimilar candidates \cite{chopra,Gutmann,Schroff,inbatch,Oord}. Recent work relies on contrastive objective to learn representations for images \cite{wu,He,Chen,Zbontar}, text, or both jointly \cite{Lu,Sun,Kim,clip,Khosla}. In self-supervised contrastive learning, positive samples can be collected in various approaches including by creating an augmented version of the original input without modifying the semantic meaning \cite{simcse}, by grouping samples within the same context \cite{declutr,contreiver}, or by collecting data about the same object from different views \cite{tian}. 

Learning word embeddings is a well studied research area \cite{brown,Gutmann,mikolov,pennington}. Learning low-dimensional representations of larger text pieces, denser than raw term-based vectors, has been studied extensively as well \cite{deerwester,contrastive}. Most of the recent models for learning sentence embeddings rely on supervised NLI datasets, using entailment pairs as positive examples and contradiction pairs as (hard) negatives. SBERT \cite{sbert} trained a siamese network to learn a representation where sentence similarity is estimated by the cosine similarity between embeddings. \citet{Li} improves the embedding space to be isotropic via normalizing flows. The whitening operation is another alternative operation to improve the isotropy of the embedding space \cite{Su}. It is typical to initialize such models with a pre-trained language model \cite{bert} before training on NLI datasets.

Several methods have been studied for unsupervised or self-supervised sentence embedding learning \cite{Logeswaran,Zhang,simcse}. Common approaches consider sentences within the same context as semantically similar samples \cite{Kiros,Logeswaran}. To create positive training pairs with augmented samples, a diverse set of text augmentation operations have been explored, including lexicon-based distortion \cite{Wei}, synonym replacement \cite{Kobayashi}, back-translation \cite{Fang}, cut-off \cite{Shen} and dropout \cite{simcse}. However, unsupervised sentence embedding models still perform notably worse than supervised sentence encoders.

Large-scale text search based on dense embeddings and neural information retrieval (neural IR) have the potential to generalize better than keyword matching in classic IR systems. Neural IR systems encode documents at the indexing stage and then perform nearest neighbor search \cite{Johnson} at query time \cite{Lin}. Neural IR models are usually learned by fine-tuning a pre-trained language model on supervised search corpus \cite{ORQA,REALM,Karpukhin,Lewis}. Many SOTA search models combine classical IR with neural IR in a staged setup, where the candidates are first narrowed down by BM25 keyword search \cite{bm25} and then re-ranked by joint query and document neural encoders \cite{Nogueira,Qu}. \citet{Xiong} proposed ANCE, a contrastive learning framework for learning text representations for dense retrieval using mined hard negatives.  Other unsupervised retriever methods use the Inverse Cloze Task or masked salient spans to achieve significant improvement on ODQA tasks \cite{e2e}. In comparison to most prior work, we find that with a large enough batch size, it is possible to achieve good search performance without using supervised data. Finally, the recently published Contriever \cite{contreiver} is most similar to our work on learning text embeddings for text search using contrastive learning on unlabeled data.  

Semantic code search refers to the task of retrieving code relevant to a query in natural language. The CodeSearchNet challenge \cite{codesearchnet} presents a set of benchmark code search tasks in different programming languages, as well as a simple baseline model to predict embeddings of query and code via contrastive learning on a dataset of (text, code) pairs. ContraCode  \cite{Jain} uses a contrastive learning task of identifying functionally similar programs, where the functionally similar samples are generated via source-to-source compiler transformations. CodeBERT \cite{codebert} learns to predict semantic similarity with a pre-trained language model and GraphCodeBERT \cite{Guo} further improves the performance on the CodeSearchNet benchmark by adding pre-training tasks on code structure. 

\section{Broader Impacts}
Prior research has shown that text representation models encode the biases present in their training data, including those which are discriminatory towards protected groups such as Black people or women \cite{Bolukbasi,Caliskan, May,Zhao,Rudinger}. Biases encoded in embedding models may cause representational harms\footnote{ Representational harms occur when systems reinforce the subordination of some groups along the lines of identity, e.g. stereotyping or denigration \cite{Crawford}.} by reinforcing existent societal biases in the text corpus, and further propagating them in downstream tasks of embedding models. 

Therefore, we encourage further research on two research agendas: (a) developing robust evaluation methodologies for multiple classes of bias in training data and pre-trained models, and (b) developing and improving methods for mitigating encoded bias, including fine-tuning to reduce bias in pre-trained models \cite{Caliskan,May,Bolukbasi,Liang,parkbias, Solaiman}. Until we have robust evaluation methodology, it is important to restrict and monitor the use of the model in downstream applications. Particularly for those where risk of representational harm is great and those where biased representations may influence the allocation of resources and opportunities to people.

Our embedding models are trained with large batch sizes and require substantial computation resources. While this training regime is environmentally and computationally costly, there are promising paths forward to amortize and offset these costs while allowing users to benefits from the capabilities of these models. For example, safe public access to large pre-trained language models, and efficient training pipelines that leverage improved model architectures and training schemes. We encourage further research and implementation efforts in these areas.



\section{Conclusion}
We showed that contrastive pre-training on unsupervised data with a sufficiently large batch size can lead to high quality vector representations of text and code. Our models achieved new state-of-the-art results in linear-probe classification, text search and code search. We find that our models underperformed on sentence similarity tasks and observed unexpected training behavior with respect to these tasks.  Finally, we discussed the broader impact of our work on society. 
\bibliography{main}
\bibliographystyle{icml2022}

\end{document}
