
\begin{figure*}[t]
\centering
\includegraphics[width=0.82\textwidth, keepaspectratio, trim={0.75cm 2cm 0.75cm 3.5cm}, clip]{figures/DynGraphIE.pdf}
\caption{
Overview of our \sys\ model. Dotted arcs indicate confidence weighted graph edges. Solid lines indicate the final predictions.
}\label{fig:model:mtl}
\vspace{-1em}
\end{figure*} 
\section{Model}


\subsection{Overview}
\paragraph{Problem Definition} The input is a document represented as a sequence of words , from which we derive
, the set of all possible within-sentence word sequence spans (up to length ) in the document.
The output contains three structures: 
the entity types  for all spans , 
the relations  for all span pairs  within the same sentence, and the coreference links  for all spans in  across sentences.  
 In particular, {\it Entity recognition} is to predict the best entity type  for every candidate span . {\it Relation extraction} is to predict  the best relation type  given an ordered pair of spans . {\it Coreference resolution} is to predict the best antecedent   given a span . 


\paragraph{Our Model} We develop a general information extraction framework (called \sys) to identify and classify  entities, relations, and coreference in a multi-task setup (sketeched in Figure \ref{fig:model:mtl}). \sys\  enumerates all phrase spans in a sentence, encodes them in a vector space representation through local context, and refines the span representations using global information by propagating the entity and relation type confidences through a {\it graph of spans}. At each  training step, the graph of spans are dynamically constructed by selecting the most confident entity spans and linking these nodes with confidence-weighted relation types and coreferences.  Then, the span representations are refined using global context from  propagations of  neighboring relation types and corefered entities.  Unlike the previous multi-task framework~\cite{luan2018multi} in which the only interaction between tasks is the shared first-layer LSTM capturing local context, our model uses the global context through the span graph to  allow entity and relation type confidences to propagate through the graph to iteratively refine the result. The refined span representations are used in a multi-task framework to predict entity types, relation types, and coreference links. \hanna{I think an example would help here -- or referring back to the figure}


 

\subsection{Span Representation Refinement through Dynamic Graph Construction}
 The  span graph facilitates propagating broader context through soft coreference and relation links to update span representations.  \luanyi{changed higher order to broader context}.
 The nodes in the span graph consist of spans  with vector representations . The edges between spans are weighted according to the coreference and relation scores, which are trained according to the neural architecture explained in Section~\ref{sec:model_architecture}. In the following we explain how coreference and relation links can update span representations.  
 
 
 \paragraph{Coreference Propagation} For the whole document, we define a beam  consisting of  spans that are most likely to be coreference mentions.  We consider  to be a matrix of real values that indicate coreference confidence scores in the -th iteration.  is of size . For the coreference graph,  the edges in the graph is single directional which connect the current  span  with all its potential antecedents  in the coreference beam, where .  The edge between two nodes  and  are weighted according to the coreference confidence score . 
The following equation shows how the coreference context span representation  is computed by propagating the co-referred span representations  weighted by their coreference scores  . 

\luanyi{not sure if  is going to be misleading, since  is not probability, but scores of real values} \luheng{Maybe? Also I'm a bit confused by the direction of coreference links here. Is  defined for all pairs in the beam, or just (mention, antecedent pairs) (where ?). In general I was just wondering if you treat coreference links as directed.}

\paragraph{Relation Propagation}  For each sentence, we define a beam  consisting of  candidate entity spans that are mostly likely to be involved in a relation.  Unlike coreference graph, the weights of relation edges capture different relation types.  Therefore for the -th iteration, we use a tensor  of real value that captures relation confidences of each relation type.  has size , where  indicates the number of relation types.
For the relation graph,  the edges in the graph connect two entity spans  and  in the relation beam according to  the predicted relation scores , which is a vector of size  indicating the confidence score of each relation type. \luanyi{maybe redundant, not sure about the notation of  as a vector.}
The following equation shows how the relation context span representation   is affected by propagating span representations from the neighboring spans. 


where  is a trainable relation transformation matrix \luheng{or just: linear projection matrix?},  
 is a non-linear function to select the most important relations. Due to the fact that only a small number of entities in the relation beam have the actual relationship with the target span, connecting edges between all possible span pairs would introduce lots of noise to the new representation.
Therefore, we choose  to be the ReLU function  that  removes the effect of null-type relation and prune out unconfident edges by setting the dimension of negative relation scores  to 0. \hanna{why just null-type? -- better explanation here} \luanyi{explained, maybe need rewording} \luheng{we might need to mention the 0 NULL score thing in 2.1 overview so it's not an information overload here.} The matrix  helps to transform the embedding  according to each relation type confidence scores between span pairs. \luheng{Ppl might wonder why you used a scalar weight for coref and a vector weight for relation, is there a natural explanation for this? Basically  can be think of as a ``vectorized weight'', right?}
\hanna{I agree with Luheng, it would be good if you use the same formulation...we can vectorize for coref as well}

\paragraph{Updating Span Representations with Gating} In order to compute the new span representations, we define a gating vector , where  to determine whether to keep the previous span representation  or to integrate new information from coreference or relation context span representation . More formally, 

where  is a trainable parameter for the gating function.  is the element-wise sigmoid function.
\luanyi{use  for gating params, not explaining iteration process in this section}


\subsection{Model Architecture}
\label{sec:model_architecture}
\sys\ consists of the following components and layers. Figure \ref{fig:model:mtl} shows a high-level overview of the \sys\ framework. 


\paragraph{Token Representation Layer}
We apply a bidirectional LSTM at the token level taking the concatenated character, GLoVe~\cite{pennington2014glove} word embeddings and ELMo embeddings \cite{peters2018deep} as input. The token representation is obtained by stacking the forward and backward LSTM hidden states.

\paragraph{Span Representation Layer}
For a span , its vector representation  is constructed by concatenating the left and right end points of  from the BiLSTM outputs, an attention-based soft ``headword", and embedded span width
features, following \cite{Lee2017EndtoendNC}.


\paragraph{Coreference Propagation Layer}
The span embedding from span enumeration layer is used to initialize the span representation at . 
At each iteration , we first compute the expected span representation  of each span   according to the equation \ref{eq:coref}. We then update the new span representation  by learning the gating vector  according to the equation .  By iterating this process  times, the final span representation  includes higher order captures information from antecedents that are further in the document. \luanyi{This is really the same process as Kenton, we probably don't want to overclaim it.} 

\paragraph{Relation Propagation Layer}
The relation propagation layer is initialized by updated representation from  coreference propagation layer .  At each iteration , we first compute the expected span representation  of each span   according to the equation \ref{eq:relation}. We then update the new span representation  by learning the gating function  according to the equation . Information can be integrated from multiple relation paths by iterate this process  times. This can be regarded as capturing information of walking  steps in a relational graph, which have been commonly used for knowledge graph reasoning.  \luanyi{cite TransE or any link prediction paper. Not sure if this claim makes sense.} 





\subsection{Training}
\paragraph{Objective function}
\luheng{Another trick to make things more clear is to use ``text'' for super/subscripts of names, and regular math font for variables e.g.  because here  stands for ``relation'' and is not a variable.}
\luanyi{Concerns of this section after talking to luheng: 1, we never mention how we get  and  until here, which would be very hard to understand for people who haven't read our previous paper. 2, we probably need to mention scores   are pruned while 
 means confidence scale}
Given the input document , we use the final representation updated by coreference and relation graph layer 
to predict entity  and relation . For entity and relation, we pass  to a linear classifier to produce per-class scores  and  for token . We compute locally normalized probabilities for entity and relation using the softmax function: 
and . \luanyi{we should really find a better notation for scoring functions  and , }. \luheng{So if we use another letter for ``spans'', we will be able to use  for all the scores ... :) we can also use greek letters such as  ...}
Coreference is normalized by maximizing the score of gold antecedent over all candidate antecedents in the coreference beam. \luanyi{do we still need the equation for coref?} \luheng{It's good to write them concretely for now to make sure everything is consistent. We can always prune them or move them to supplementary later.}


The loss function is defined as a weighted sum of the  log-likelihood of all three tasks:


where ,  and  are gold structures of the entity types, relations and coreference respectively.
The task weights , , and  are introduced as hyper-parameters to control the importance of each task.
\luanyi{mention coref data doesn't have be overlapped annotations, and we use coreference as an auxiliary task}

























%
