\section{Results}
\label{sec:results}
To show the importance of hard attention for Visual QA, we first compare HAN to existing soft attention (SAN) architectures on VQA-CP v2, and exploring the effect of varying degrees of hard attention by directly controlling the number of attended spatial cells in the convolutional map. 
We then examine AdaHAN, which adaptively chooses the number of attended cells, and briefly investigate the effect of network depth and pretraining.
Finally, we present qualitative results, and also provide results on CLEVR to show the method's generality.

\subsection{Datasets}
\textbf{VQA-CP v2.} This dataset~\cite{agrawal2017don}
consists of about 121K (98K) images, 438K (220K) questions, and 4.4M (2.2M) answers in the train (test) set; and it is created so that the distribution of the answers between train and test splits differ, and hence the models cannot excessively rely on the language prior \cite{agrawal2017don}. As expected, \cite{agrawal2017don} show that performance of all Visual QA approaches they tested drops significantly between train to test sets. 
The dataset provides a standard train-test split, and also breaks questions into different question types: those where the answer is yes/no, those where the answer is a number, and those where the answer is something else. Thus, we report accuracy on each question type as well as the overall accuracy for each network architecture.
\newline
\textbf{CLEVR.} This synthetic dataset~\cite{johnson2017clevr} consists of 100K images of 3D rendered objects like spheres and cylinders, and roughly 1m questions that were automatically generated with a procedural engine.  While the visual task is relatively simple, solving this dataset requires reasoning over complex relationships between many objects.

\subsection{Effect of Hard Attention}
We begin with the most basic hard attention architecture, which applies hard attention and then does sum pooling over the attended cells, followed by a small MLP.
For each experiment, we take the top $k$ cells, out of $100$, according to our $L2$-norm criterion, where $k$ ranges from $16$ to $100$ (with $100$, there is no attention, and the whole image is summed).  
Results are shown in the top of Table~\ref{table:han-mhsa-attention_numbers}.  
Considering that the hard attention 
selects only a subset of the input cells, 
we might expect that the algorithm would lose important information and be unable to recover.
In fact, however, the performance is almost the same with less than half of the units attended.
Even with just $16$ units, the performance loss is less than 1\%, suggesting that hard attention is quite capable of capturing the important parts of the image. 


\begin{table}
\begin{center}
\begin{tabular}{llrrrr}
\toprule
 & Percentage\,\, & Overall\,\, & Yes/No\,\, & Number\,\, & Other \\
 & of cells\,\,  & & & & \\
 \cmidrule(ll){1-2}\cmidrule(lr){3-6}
 HAN+sum & 16\% & 26.99 & 40.53 & 11.38 & 24.15 \\
 HAN+sum & 32\% & 27.43 & 41.05 & 11.38 & 24.68 \\
 HAN+sum & 48\% & 27.94 & 41.35 & 11.93 & 25.27 \\
 HAN+sum & 64\% & 27.80 & 40.74 & 11.29 & 25.52 \\
 sum & 100\% & 27.96 & 43.23 & 12.09 & 24.29 \\ 
\midrule
 HAN+pairwise & 16\% & 26.81 & 41.24 & 10.87 & 23.61 \\
 HAN+pairwise & 32\% & 27.45 & 40.91 & 11.48 & 24.75 \\
 HAN+pairwise & 48\% & 28.23 & 41.23 & 11.40 & 25.98 \\
 Pairwise & 100\%  & 28.06 & 44.10 & 13.20 & 23.71 \\
 \midrule
  SAN \cite{agrawal2017don,yang2015stacked} & - & 24.96 & 38.35 & 11.14 & 21.74 \\
  SAN (ours) & - & 26.60 & 39.69 & 11.25 & 23.92 \\
  SAN+pos (ours) & - & 27.77 & 40.73 & 11.31 & 25.47 \\
  \midrule
  GVQA \cite{agrawal2017don} & - & 31.30 & 57.99 & 13.68 & 22.14 \\
\bottomrule
\end{tabular}
\end{center}
\caption{
Comparison between different number of attended cells (percentage of the whole input), and aggregation operation. We consider a simple summation, and non-local pairwise computations as the aggregation tool. 
}
\label{table:han-mhsa-attention_numbers}
\end{table}

The fact that hard attention can work is interesting itself, but it should be especially useful for models that devote significant processing to each attended cell.  
We therefore repeat the above experiment with the non-local pairwise aggregation mechanism described in \autoref{sec:method}, which computes activations for every pair of attended cells, and therefore scales quadratically with the number of attended cells.
These results are shown in the middle of Table~\ref{table:han-mhsa-attention_numbers}, where we can see that hard attention (48 entitties) actually boosts performance over an analogous model without hard attention.

Finally, we compare standard soft attention baselines in the bottom of \autoref{table:han-mhsa-attention_numbers}.
In particular, we include previous results using a basic soft attention network~\cite{agrawal2017don,yang2015stacked}, as well as our own re-implementation of the soft attention pooling algorithm presented in \cite{agrawal2017don,yang2015stacked} with the same features used in other experiments.  
Surprisingly, soft attention does not outperform basic sum pooling, even with careful implementation that outperforms the previously reported results with the same method on this dataset; in fact, it performs slightly worse. 
The non-local pairwise aggregation performs better than SAN on its own, although the best result includes hard attention.
Our results overall are somewhat worse than the state-of-the-art~\cite{agrawal2017don}, but this is likely due to several architectural decisions not included here, such as a split pathway for different kinds of questions, special question embeddings, and the use of the question extractor.

\begin{table*}\begin{center}
\begin{tabular}{llrrrr}
\toprule
 & Percentage\,\, & Overall\,\, & Yes/No\,\, & Number\,\, & Other \\
 & of cells & & & \\
 \cmidrule(l){1-1}\cmidrule(lr){2-5}
 AdaHAN+sum & 25.66\% & 27.40 & 40.70 & 11.13 & 24.86 \\
 AdaHAN+pairwise  & 32.63\% & 28.65 & 52.25 & 13.79 & 20.33 \\
 \midrule
 HAN+sum  & 32\% & 27.43 & 41.05 & 11.38 & 24.68 \\
 HAN+sum  & 48\% & 27.94 & 41.35 & 11.93 & 25.27 \\
 HAN+pairwise  & 32\% & 27.45 & 40.91 & 11.48 & 24.75 \\
 HAN+pairwise  & 48\% & 28.23 & 41.23 & 11.40 & 25.98 \\
\bottomrule
\end{tabular}
\end{center}
\caption{
Comparison between different adaptive hard-attention techniques with average number of attended parts,  and aggregation operation. We consider a simple summation, and the non-local pairwise aggregation. 
Since AdaHAN adaptively selects relevant features, based on the fixed threshold $\frac{1}{w*h}$, we report here the average number of attended parts.
}
\label{table:adaptive_han-attention_numbers}
\end{table*}

\subsection{Adaptive hard attention}
Thus far, our experiments have dealt with networks that have a fixed threshold for all images.  
However, some images and questions may require reasoning about more entities than others.
Therefore, we explore a simple adaptive method, where the network chooses how many cells to attend to for each image.
Table~\ref{table:adaptive_han-attention_numbers} shows results, where AdaHAN refers to our adaptive mechanism.  We can see that on average, the adaptive mechanism uses surprisingly few cells: 25.66 out of 100 when sum pooling is used, and 32.63 whenever the non-local pairwise aggregation mechanism is used.
For sum pooling, this is on-par with a non-adaptive network that uses more cells on average (HAN+sum 32); for the non-local pairwise aggregation mechanism, just 32.63 cells are enough to outperform our best non-adaptive model, which uses roughly $50\%$ more cells.
This shows that even very simple methods of adapting hard attention to the image and the question can lead to both computation and performance gains, suggesting that more sophisticated methods will be an important direction for future work.

\subsection{Effects of network depth}
In this section, we briefly analyze an important architectural choice: the number of layers used on top of the pretrained embeddings.  
That is, before the question and image representations are combined, we perform a small amount of processing to ``align'' the information, so that the embedding can easily tell the relevance of the visual information to the question.
Table~\ref{table:han-sum-attention_numbers} shows the results of removing the two layers which perform this function.  
We consistently see a drop of about 1\% without the layers, suggesting that deciding which cells to attend to requires different information than the classification-tuned ResNet is designed to provide.

\begin{table*}[tb]
\begin{center}
\begin{tabular}{lllrrrr}
\toprule
 & Percentage\,\, & Number\,\, & Overall\,\, & Yes/No\,\, & Number\,\, & Other \\
 & of cells\,\, & of layers\,\,  & & & & \\
 \cmidrule(ll){1-3}\cmidrule(lr){4-7}
 HAN+sum & 25\% & 0 & 26.38 & 43.21 & 13.12 & 21.17 \\
 HAN+sum & 50\% & 0 & 26.75 & 41.42 & 10.94 & 23.38 \\
 HAN+sum & 75\%   & 0 & 26.82 & 41.30 & 11.48 & 23.42 \\
\midrule
 HAN+sum & 25\% & 2 & 26.99 & 40.53 & 11.38 & 24.15 \\
 HAN+sum & 50\% & 2 & 27.43 & 41.05 & 11.38 & 24.68 \\
 HAN+sum & 75\% & 2 & 27.94 & 41.35 & 11.93 & 25.27 \\
\bottomrule
\end{tabular}
\end{center}
\caption{
Comparison between different number of the attended 
cells as the percentage of the whole input. The results are reported on VQA-CP v2. The second column denotes the 
percentage of the attended input.
The third column denotes number of layers of the MLP (Equations \ref{eq:emb_x} and \ref{eq:emb_q}).
}
\label{table:han-sum-attention_numbers}
\end{table*}

\subsection{Implementation Details.}
All our models use the same LSTM size $512$ for questions embeddings, and the last convolutional layer of the ImageNet pre-trained ResNet-101 \cite{he2015deep} (yielding 10-by-10 spatial representation, each with $2048$ dimensional cells) for image embedding. We also use MLP with $3$ layers of sizes: $1024, 2048, 1000$, as a classification module. We use ADAM for optimization \cite{kingma2014adam}. We use a distributed setting with two workers computing a gradient over a batch of $128$ elements each. We normalize images by dividing them by their norm. We do not perform a hyper-parameter search as there is no separated validation set available. Instead, we rather choose default hyper-parameters based on our prior experience on Visual QA datasets. We trained our models until we notice a saturation on the training set. Then we evaluate these models on the test set. Our tables show the performance of all the methods wrt. the second digits precision obtained by rounding.

\autoref{table:han-mhsa-attention_numbers} shows SAN's \cite{yang2015stacked} results reported by \cite{agrawal2017don} together with our in-house implementation (denoted as ``ours''). Our implementation has 2 attention hops, $1024$ dimensional multimodal embedding size, a fixed learning rate $0.0001$, and ResNet-101. In these experiments we pool the attended representations by weighted average with the attention weights.
Our in-house implementation of the non-local pairwise mechanism strongly resembles implementations of \cite{wang2017non}, and \cite{vaswani2017attention}. We use $2$ heads, with embedding size $512$.
In \autoref{eq:emb_x} and \autoref{eq:emb_q}, we use $d := 2048$ (the same as dimensionality as the image encoding) and two linear layers with RELU that follows up each layer.


\subsection{Qualitative Results.}
\label{sec:qualitative}
One advantage of our formulation is that it is straightforward to visualize the masks of attended cells given questions and images, which we show in \autoref{fig:qualitative} and \autoref{fig:qualitative2}.
In general, relevant objects are usually attended, and that significant portions of the irrelevant background is suppressed. Although some background might be kept, we hypothesize the context matters in answering some questions.

In \autoref{fig:qualitative}, we show results with our different hard-attention mechanisms (HAN or AdaHAN), and different aggregation operations (summation or pairwise). We can see that the important objects are attended together with some context, which we hypothesize can also be important in correctly answering questions. These masks
are occasionally useful for diagnosing behavior. For instance, as row 2 and column 3 suggest, the network may answer the question correctly but likely for wrong reasons. 
We can also see broad differences between the network architectures.  For instance, the sum pooling method (row 2) is much more spatially constrained than the pairwise pooling version (row 1), even though the adaptive attention can select an arbitrarily large region.   We hypothesize that more visual features may unnecessarily interfere during the summation, and hence a more spatially sparse representation is preferred, or that sum pooling struggles to integrate across complex scenes.  The support is also not always contiguous: non-adaptive hard attention with 16 entities (row 4) in particular distributes its attention widely. 



In \autoref{fig:qualitative2}, we show results with our best-performing model on VQA-CP: adaptive hard attention mechanism tied with a non-local, pairwise aggregation mechanism (AdaHAN+pairwise). The qualitative behaviour of this mechanism subsumes various fixed hard-attention variants, and with a variable spatial support tends to be better qualitatively and quantitatively than others. 
Interestingly, the topology of the attended parts of AdaHAN+pairwise differs from image to image. For instance, for the question ``Are this lions?'' (1st row, 1st column), the two attended regions are separated and quite localized.  However, for ``Is that an airplane in image?'' (1st row, 2nd column), the attended regions are contiguous and cover almost whole image.  The shape of the train in the image (1st row, 3rd column), despite of its elongated shape, is quite well captured by our method.  Similarly, we can observe that the attended regions overlap with the shape of a boat (1st row, 4th column), even though the method ultimately gets the question wrong.



\begin{figure*}[p]
\begin{center}
\begin{tabular}{l@{\ }l@{\ }c@{\ }c@{\ }c@{\ }c}
\toprule
 & \multicolumn{1}{c}{Are those drinking} & 
\multicolumn{1}{c}{Can cars cross} & 
\multicolumn{1}{c}{What color is} &
\multicolumn{1}{c}{What is building} \\
& \multicolumn{1}{c}{glasses next to} & 
\multicolumn{1}{c}{this bridge?} & 
\multicolumn{1}{c}{her skirt?} & 
\multicolumn{1}{c}{facade made} \\
& \multicolumn{1}{c}{flower pot?} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{from?}
\\\midrule
\rotatebox{90}{AdaHAN+pair.} & \multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-are_those_drinking_glasses_next_to_flower_pot-yes-yes__yes-1}} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-can_cars_cross_this_bridge-no-no__no-1} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_color_is_her_skirt-white-white__white-1} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_is_building_facade_made_from-brick-brick__brick-1}
\\
& \multicolumn{1}{c}{\textcolor{darkgreen}{yes}} & 
\multicolumn{1}{c}{\textcolor{darkgreen}{no}} & 
\multicolumn{1}{c}{\textcolor{darkgreen}{white}} &
\multicolumn{1}{c}{\textcolor{darkgreen}{brick}}
\\\midrule
\rotatebox{90}{AdaHAN+sum} & \multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/adah-sum/609494-2-are_those_drinking_glasses_next_to_flower_pot-no-yes__yes-0.png}} &
\includegraphics[width=0.2\linewidth]{fig/adah-sum/609494-2-can_cars_cross_this_bridge-yes-no__no-0} &
\includegraphics[width=0.2\linewidth]{fig/adah-sum/609494-2-what_color_is_her_skirt-white-white__white-1} &
\includegraphics[width=0.2\linewidth]{fig/adah-sum/609494-2-what_is_building_facade_made_from-brick-brick__brick-1}
\\
& \multicolumn{1}{c}{\textcolor{red}{no}} & 
\multicolumn{1}{c}{\textcolor{red}{yes}} & 
\multicolumn{1}{c}{\textcolor{darkgreen}{white}} &
\multicolumn{1}{c}{\textcolor{darkgreen}{brick}}
\\\midrule
\rotatebox{90}{HAN+pair. (32)} & \multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/han-32-rn/606906-4-are_those_drinking_glasses_next_to_flower_pot-yes-yes__yes-1.png}} &
\includegraphics[width=0.2\linewidth]{fig/han-32-rn/606906-4-can_cars_cross_this_bridge-no-no__no-1} &
\includegraphics[width=0.2\linewidth]{fig/han-32-rn/606906-4-what_color_is_her_skirt-black-white__white-0} &
\includegraphics[width=0.2\linewidth]{fig/han-32-rn/606906-4-what_is_building_facade_made_from-stone-brick__brick-0.png}
\\
& \multicolumn{1}{c}{\textcolor{darkgreen}{yes}} & 
\multicolumn{1}{c}{\textcolor{darkgreen}{no}} & 
\multicolumn{1}{c}{\textcolor{red}{black}} &
\multicolumn{1}{c}{\textcolor{orange}{stone}}
\\\midrule
\rotatebox{90}{HAN+pair. (16)} & \multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/han-16-rn/606906-3-are_those_drinking_glasses_next_to_flower_pot-yes-yes__yes-1}} &
\includegraphics[width=0.2\linewidth]{fig/han-16-rn/606906-3-can_cars_cross_this_bridge-no-no__no-1} &
\includegraphics[width=0.2\linewidth]{fig/han-16-rn/606906-3-what_color_is_her_skirt-blue-white__white-0} &
\includegraphics[width=0.2\linewidth]{fig/han-16-rn/606906-3-what_is_building_facade_made_from-stone-brick__brick-0}
\\
& \multicolumn{1}{c}{\textcolor{darkgreen}{yes}} & 
\multicolumn{1}{c}{\textcolor{darkgreen}{no}} & 
\multicolumn{1}{c}{\textcolor{red}{blue}} &
\multicolumn{1}{c}{\textcolor{orange}{stone}}
\\\hline
\bottomrule

\end{tabular}
\end{center}
\caption{
Qualitative comparison between our variants of the hard attention mechanism together with different aggregation methods. The first row shows AdaHAN+pairwise (AdaHAN+pair. in the figure), the second row shows AdaHAN+sum, the third row shows HAN+pairwise with fixed $32$ entities, 
and the last row shows HAN+pairwise with fixed $16$  entities, covering 32\% and 16\% of the input respectively. In the images, attended regions are highlighted while unattended are darkened. The green denotes correct answers, the red incorrect, and orange denotes partial consensus between the human answers. This figure illustrates various strengths of the proposed methods. Best viewed on a display.
}
\label{fig:qualitative}
\end{figure*}


\begin{figure*}[p]
\begin{center}
\def\arraystretch{1}
\begin{tabular}{l@{\ }l@{\ }c@{\ }c@{\ }c@{\ }c}
\toprule
 & \multicolumn{1}{c}{{\footnotesize Are this lions?}} & 
\multicolumn{1}{c}{{\footnotesize Is that}} & 
\multicolumn{1}{c}{{\footnotesize How many} } &
\multicolumn{1}{c}{{\footnotesize How many boats}} \\
& \multicolumn{1}{c}{} & 
\multicolumn{1}{c}{{\footnotesize an airplane}} & 
\multicolumn{1}{c}{{\footnotesize cars are on}} & 
\multicolumn{1}{c}{{\footnotesize are there?}} \\
& \multicolumn{1}{c}{} & 
\multicolumn{1}{c}{{\footnotesize  in image?}} & 
\multicolumn{1}{c}{{\footnotesize train tracks?}} & 
\multicolumn{1}{c}{}
\\
\rotatebox{90}{AdaHAN+pair.} & 
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-are_this_lions-no-no__no-1} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-is_that_an_airplane_in_image-no-no__no-1.png} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-how_many_cars_are_on_train_tracks-2-1__0-0} &
\multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-how_many_boats_are_there-2-1__1-0}} 
\\
& \multicolumn{1}{c}{\textcolor{darkgreen}{no}} & 
\multicolumn{1}{c}{\textcolor{darkgreen}{no}} & 
\multicolumn{1}{c}{\textcolor{red}{2} (1)} &
\multicolumn{1}{c}{\textcolor{red}{2} (1)}
  \\\midrule
 & \multicolumn{1}{c}{About how high} & 
 \multicolumn{1}{c}{Any chains on} &
\multicolumn{1}{c}{About how old} & 
\multicolumn{1}{c}{What color is} \\
& \multicolumn{1}{c}{is man jumping?} &
\multicolumn{1}{c}{hydrant?} &
\multicolumn{1}{c}{is boy on} & 
\multicolumn{1}{c}{this train?}  \\
& \multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{surfboard?} & 
\multicolumn{1}{c}{}
\\
\rotatebox{90}{AdaHAN+pair.} & \multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-about_how_high_is_man_jumping-3_feet-6_feet__2_ft-1}} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-any_chains_on_hydrant-no-no__no-1} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-about_how_old_is_boy_on_surfboard-4-4__3-0_6} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_color_is_this_train-green-yellow__red_and_yellow-0_6}
\\
& \multicolumn{1}{c}{\textcolor{darkgreen}{3 feet}} &
\multicolumn{1}{c}{\textcolor{darkgreen}{no}} &
\multicolumn{1}{c}{\textcolor{orange}{4}} & 
\multicolumn{1}{c}{\textcolor{orange}{yellow}} 
  \\\midrule
 & \multicolumn{1}{c}{Would this type} & 
 \multicolumn{1}{c}{What is} &
\multicolumn{1}{c}{What is he} & 
\multicolumn{1}{c}{What time does} \\
& \multicolumn{1}{c}{of train be used} &
\multicolumn{1}{c}{in his hand?} &
\multicolumn{1}{c}{holding up?} & 
\multicolumn{1}{c}{clock read?}  \\
& \multicolumn{1}{c}{as commuter?} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}
\\
\rotatebox{90}{AdaHAN+pair.} & \multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-would_this_type_of_train_be_used_as_commuter-no-no__no-1}} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_is_in_his_hand-phone-phone__phone-1} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_is_he_holding_up-fruit-fruit__piece_of_fruit-0_9} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_time_does_clock_read-noon-425__425-0}
\\
& \multicolumn{1}{c}{\textcolor{darkgreen}{no}} &
\multicolumn{1}{c}{\textcolor{darkgreen}{phone}} &
\multicolumn{1}{c}{\textcolor{orange}{fruit}} & 
\multicolumn{1}{c}{\textcolor{red}{noon} (4:25)} 
  \\\midrule
 & \multicolumn{1}{c}{What is girl} & 
 \multicolumn{1}{c}{Are people } &
\multicolumn{1}{c}{Are all these} & 
\multicolumn{1}{c}{What kind} \\
& \multicolumn{1}{c}{trying to catch?} &
\multicolumn{1}{c}{sunbathing at} &
\multicolumn{1}{c}{people moving?} & 
\multicolumn{1}{c}{of animals}  \\
& \multicolumn{1}{c}{} & 
\multicolumn{1}{c}{beach in photo?} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{are these?}
\\
\rotatebox{90}{AdaHAN+pair.} & \multicolumn{1}{c}{\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_is_girl_trying_to_catch-kite-kite__kite-1}} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-are_people_sunbathing_at_beach_in_photo-no-no__no-1.png} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-are_all_these_people_moving-yes-yes__no-1} &
\includegraphics[width=0.2\linewidth]{fig/adah-rn/609494-3-what_kind_of_animals_are_these-bear-bears__bear-0_9.png}
\\
& \multicolumn{1}{c}{\textcolor{darkgreen}{kite}} &
\multicolumn{1}{c}{\textcolor{darkgreen}{no}} &
\multicolumn{1}{c}{\textcolor{darkgreen}{yes}} & 
\multicolumn{1}{c}{\textcolor{orange}{bear}} 
\\\hline
\bottomrule

\end{tabular}
\end{center}
\caption{
We show additional results with our AdaHAN+pairwise. In the images, the attended regions are highlighted while the unattended are darkened. Green denotes correct, and red incorrect answers. Orange denotes partial consensus. Best viewed on display.
}
\label{fig:qualitative2}
\end{figure*}

\def\arraystretch{1.2}


\subsection{End-to-end Training.}
\label{sec:end2end_training}
Since our network uses hard attention, which has zero gradients almost everywhere, one might suspect that it will become more difficult to train the lower-level features, or worse, that untrained features might prevent us from bootstrapping the attention mechanism.
Therefore, we also trained HAN+sum (with 16\% of the input cells) end-to-end together with a relatively small convolutional neural network initialized from scratch.  
We compare our method against our implementation of the SAN method trained using the same simple convolutional neural network. 
We call the models: simple-SAN, and simple-HAN.
\newline
\noindent \textbf{Analysis.}
In our experiments, simple-SAN achieves about $21\%$ performance on the test set. Surprisingly, simple-HAN+sum achieves about $24\%$ performance on the same split, on-par with the performance of normal SAN that uses more complex and deeper visual architecture \cite{simonyan2014very}; the results are reported by \cite{agrawal2017don}. This result shows that the hard attention mechanism can indeed be tightly coupled within the training process, and that the whole procedure does not rely heavily on the properties of the ImageNet pre-trained networks.
In a sense, we see that a discrete notion of entities also ``emerges'' through the learning process, leading to efficient training.
\newline
\noindent \textbf{Implementation Details.}
In our experiments we use a simple CNN built of: 1 layer with 64 filters and 7-by-7 filter size followed up by 2 layers with 256 filters and 2 layers with 512 filters, all with 3-by-3 filter size. We use strides 2 for all the layers. 
\begin{figure}[!htb]
\begin{center}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{fig/han_rn_12_hours}
  \caption{HAN+RN (purple), RN (green)}
  \label{fig:han_rn_12_hours}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.87\linewidth]{fig/hard_attention-vs-st}
  \caption{HAN+RN (orange), ST+RN (blue)}
  \label{fig:han_vs_st}
\end{subfigure}\end{center}
\caption{Validation accuracy plots on CLEVR of the methods under the same hyper-parameters setting~\cite{santoro2017simple}. (a)  HAN+RN ($0.25$ of the input cells)  and standard RN (all input cells) trained for 12 hours to measure the efficiency of the methods. (b) Our approaches to hard attention: the proposed one (orange), and the straight-through estimator (blue). }
\end{figure}



\subsection{CLEVR}
To demonstrate the generality of our hard attention method, particularly in domains that are visually different from the VQA images, 
we experiment with a synthetic Visual QA dataset termed CLEVR \cite{johnson2017clevr}, using a setup similar to the one used for VQA-CP and \cite{santoro2017simple}.  
Due to the visual simplicity of CLEVR, we follow up the work of~\cite{santoro2017simple}, and instead of relying on the ImageNet pre-trained features, we train our HAN+sum and HAN+RN (hard attention with relation network) architectures end-to-end together with a relatively small CNN (following~\cite{santoro2017simple}).
\newline
\noindent \textbf{Analysis.}
As reported in prior work~\cite{santoro2017simple,johnson2017clevr}, the soft attention mechanism used in SAN does not perform well on the CLEVR dataset, and achieves only $68.5\%$~\cite{johnson2017clevr} (or $76.6\%$~\cite{santoro2017simple}) performance. In contrast, relation network, which also realizes a non-local and pairwise computational model, essentially solves this task, achieving $95.5\%$ performance on the test set.
Surprisingly, our HAN+sum achieves 
89.7\%
performance even without a relation network, and HAN+RN (\ie, relation network is used as an aggregation mechanism) achieves 
$93.9\%$ on the test set.
These results show the mechanism can readily be used with other architectures on another dataset with different visuals. Training with HAN requires far less computation than the original relation network~\cite{santoro2017simple}, although performance is slightly below relation network's 95.5\%. \autoref{fig:han_rn_12_hours} compares computation time: HAN+RN and relation network are trained for 12 hours under the same hyper-parameter set-up. Here, HAN+RN achieves around 90\% validation accuracy, whereas relation network only 70\%.


Owing to hard-attention, we are able to train larger models, which we call HAN+sum$^{+}$, HAN+RN$^{+}$, and HAN+RN$^{++}$.
These models use larger CNN and LSTM, and HAN+RN$^{++}$ also uses higher resolution of the input (see Implementation Details below). The models
achieve 94.7\%, 96.9\% and 98.8\% respectively.
The relation network with hard attention operates on $k^2$ selected input cells, instead of all $n^2$ cells of the original RN. 
All our experiments except HAN+RN$^{++}$ use only one fourth of the input cells ($\frac{k}{n} = 0.25$, where $n = 64$). HAN+RN$^{++}$ uses a larger spatial tensor (14x14), and the same number of input cells as the original RN~\cite{santoro2017simple}, with $k = 64$ yielding ($\frac{k}{n} = \frac{64}{196})$ around 33\% of the input cells. Dealing with only the fraction of the input data helps the whole network to train faster.

\autoref{table:detailed_clevr} gives more context regarding the results on the CLEVR dataset that we are aware of, and compares our method with other approaches to answer questions about CLEVR images. Our best performing method, denoted by HAN+RN$^{++}$ that uses a deeper model and operates on larger input tensor than the original RN~\cite{santoro2017simple}, is very competitive to alternative approaches such us  FiLM~\cite{perez2017film}, TbD~\cite{mascharka2018transparency}, or MAC~\cite{hudson2018compositional}; and as \cite{santoro2017simple} and \cite{mascharka2018transparency} (TbD+hres) have noted increasing the spatial resolution definitely helps in achieving better performance.
As we can see in \autoref{table:detailed_clevr}, all the approaches seem to struggle with difficult counting questions, and RN is significantly worse on the {\it Compare Numbers} questions. In the remaining question types, HAN+RN$^{++}$ is either on par or even better than TbD+hres that uses larger spatial resolution, deep pre-trained image CNN, more specialized modules, and requires an ``expert layout''~\cite{explainable2018eccv}. 
Here, we keep the conceptual simplicity of the original RN~\cite{santoro2017simple} coupled with our simple mechanism of selecting important features, as well as we trained the whole architecture end-to-end and from scratch.
Finally, through a visual inspection, we have observed that the fraction of input cells that we have experimented with ($k=16$ for 8x8 spatial tensor, and $k=64$ for 14x14 spatial tensor) is sufficient to cover all the important objects in the image, and thus the mechanism resembles more the saliency mechanism. It is worth noting, the hard-attention mechanism often selects a few cells that correspond to the object as this is sufficient to recognize the object's properties such as size, material, color, type, and spatial location.


\noindent\textbf{Straight-Through Estimator.}
As an alternative to our hard attention, we have also implemented a few variants of the straight-through estimator~\cite{bengio2013estimating}, which is a method introduced to deal with non-differentiable neural modules.
In a nutshell, during the forward pass we employ steps that are non-differentiable or have gradients that are zero almost everywhere (e.g., hard thresholding), but in the backward pass, we introduce skip-connections that the back-propagation mechanism uses to bypass these steps.
For the purpose of gracefully implementing this mechanism in TensorFlow, we have implemented the estimator as follows\footnote{Credit goes to Sergey Ioffe for pointing out the general expression that we have adapted for our purpose.}. Let $\bs{x} \in \mathbb{R}^{n \times d}$ be spatial input, with $n$ spatial cells, each $d$-dimensional. All our estimators have the form
\begin{align*}
\bs{x} \cdot (g(\bs{x}) + \text{stop}(\mathbbm{1}\left\{g(\bs{x}) > t(g(\bs{x}), k)\right\} - g(\bs{x}) ))
\end{align*}
Here, $\cdot$ is the element-wise multiplication, $\text{stop}(\bs{y})$ prevents from propagating the gradient through $\bs{y}$,  $t(\bs{y},k)$ returns the $k$-th largest element of the vector $\bs{y}$, $\mathbbm{1}\left\{\mathcal{P}\right\}$ outputs 1 if the predicate $\mathcal{P}$ is true and 0 otherwise, and $g$ produces a spatial mask similar to the soft attention mask, i.e. $g(\bs{y}) \in \mathbb{R}^{n \times 1}$. In all our experiments, $g = \mu \circ f$ is the composition of the normalization function (e.g. softmax) $\mu$ and an MLP $f$ with one hidden layer of dimension $\frac{d}{2}$, and one ReLU between the hidden and the output layers. For $\mu$, we investigate identity, sigmoid or softmax. Only the latter two yield results significantly better than $60\%$, but we still find the results either under-performing to our hard-attention approach, or very unstable. For instance, \autoref{fig:han_vs_st} shows our best results (accuracy- and stability-wise) with straight-through. Moreover, our formulation of the straight-through still requires to have gradients back-propagated through all the cells, even though they are ignored in the forward-pass, and hence the method lacks the computational benefits of our hard-attention mechanism.


\noindent\textbf{Implementation Details.}
 In the experiments with HAN+Sum, and HAN+RN we follow the same setup as \cite{santoro2017simple}. However, we have made slight changes with our larger models: HAN+Sum$^{+}$, HAN+RN$^{+}$, and HAN+RN$^{++}$.
  HAN+Sum$^{+}$, HAN+RN$^{+}$, and HAN+RN$^{++}$ use an LSTM with 256 hidden units and 64 dimensional word embedding (jointly trained from scratch together with the whole architecture) for language.  For the image, we use a CNN with 4 layers, each with stride 2, 3x3 kernel size, ReLU non-linearities, and 128 features at each spatial cell.  Our classifier  is an MLP with a single hidden layer (1024 dimensional),  drop-out 50\%, and a single ReLU. Function $g_\theta$ defined in~\cite{santoro2017simple} is an MLP with four hidden layers (each 256 dimensional) and ReLUs. We also find that, before the sum-pooling in HAN+Sum$^{+}$, and before the pairwise aggregation in HAN+RN$^{+/++}$ it is worthwhile to process the multimodal embedding with a 1-by-1 convolution (we use 4 layers, with ReLUs, and 256 features). We use $l_2$-norm on all the weights as the regularization. For hard-attention, we have also found batch-normalization in the image CNN to be crucial to achieve a good performance. Moreover, batch-normalization before 1-by-1 convolutions is also helpful, but not critical. The other hyper-parameters are identical to the ones presented in~\cite{santoro2017simple}.

\begin{table}
\centering
\begin{threeparttable}
\begin{tabular}{l | C{14mm} | C{10mm} C{10mm} C{14mm} C{14mm} C{14mm}}
\toprule
Model & \textbf{Overall} & Count & Exist & Compare Numbers & Query Attribute & Compare Attribute \\
\hline
Human~\cite{johnson2017clevr}  & 92.6 & 86.7 & 96.6 & 86.5 & 95.0 & 96.0 \\
\hline
Q-type baseline~\cite{johnson2017clevr,antol2015vqa}  & 41.8 & 34.6 & 50.2 & 51.0 & 36.0 & 51.3  \\
LSTM-only~\cite{johnson2017clevr,ren2015image,antol2015vqa,gao2015you,malinowski2015ask} & 46.8 & 41.7 & 61.1 & 69.8 & 36.8 & 51.8  \\
CNN$+$LSTM~\cite{johnson2017clevr,ren2015image,antol2015vqa,gao2015you,malinowski2015ask}  & 52.3 & 43.7 & 65.2 & 67.1 & 49.3 & 53.0  \\
SAN~\cite{johnson2017clevr,yang2015stacked}       & 68.5 & 52.2 & 71.1 & 73.5 & 85.3 & 52.3  \\
SAN*~\cite{santoro2017simple,yang2015stacked}     & 76.6 & 64.4 & 82.7 & 77.4 & 82.6 & 75.4  \\
LBP-SIG~\cite{chen2017sva}  & 78.0 & 61.3 & 79.6 & 80.7 & 88.6 & 76.3  \\
N2NMN~\cite{hu2017learning}  & 83.7 & 68.5 & 85.7 & 85.0 & 90.0 & 88.9 \\
PG+EE (700k)$^-$~\cite{johnson2017inferring} & 96.9 & 92.7 & 97.1 & 98.7 & 98.1 & 98.9 \\
RN~\cite{santoro2017simple} & 95.5 & 90.1 & 97.8 & 93.6 & 97.9 & 97.1 \\
Hyperbolic RN~\cite{gulcehre2018hyperbolic} & 95.7 & - & - & - & - & - \\
Object RN**~\cite{desta2018object} & 94.5 & 93.6 & 94.7 & 93.3 & 95.2 & 94.4 \\
Stack-NMNs**~\cite{explainable2018eccv} & 96.6 (93.0) & - & - & - & - & - \\
FiLM~\cite{perez2017film} & 97.6 & 94.5 & 99.2 & 93.8 & 99.2 & 99.0 \\
DDRprog$^-$~\cite{suarez2018ddrprog} & 98.3 & 96.5 & 98.8 & 98.4 & 99.1 & 99.0 \\
MAC~\cite{hudson2018compositional} & 98.9 & 97.2 & 99.5 & 99.4 & 99.3 & 99.5 \\
TbD$^{-}$~\cite{mascharka2018transparency} & 98.7 & 96.8 & 98.9 & 99.1 & 99.4 & 99.2 \\
TbD+hres$^{-}$~\cite{mascharka2018transparency} & 99.1 & 97.6 & 99.2 & 99.4 & 99.5 & 99.6 \\
\hline
HAN+Sum$^{+}$ (Ours) & 94.7 & 88.9 & 97.3 & 88.0 & 98.1 & 97.0 \\
HAN+RN$^{+}$ (Ours) & 96.9 & 92.8 & 98.6 & 94.9 & 98.9 & 98.2 \\
HAN+RN$^{++}$ (Ours) & 98.8 & 97.2 & 99.6 & 96.9 & 99.6 & 99.6 \\
\bottomrule\addlinespace[1ex]
\end{tabular}
\end{threeparttable}
\caption{Results, in $\%$, on CLEVR. 
SAN denotes the SAN~\cite{yang2015stacked} implementation of \cite{johnson2017clevr}.
SAN* denotes the SAN implementation of \cite{santoro2017simple}.
Object RN**~\cite{desta2018object} and Stack-NMNs**~\cite{explainable2018eccv} report the results only on the validation set, whereas others report on the test set. Overall performance of Stack-NMNs**~\cite{explainable2018eccv} is measured with the ``expert layout'' (similar to N2NMN) yielding 96.6 and without it (93.0).
DDRprog$^-$~\cite{suarez2018ddrprog}, PG+EE (700k)$^-$~\cite{johnson2017inferring}, TbD$^-$, and TbD+hres$^-$~\cite{mascharka2018transparency} are trained with a privileged state-description, while others are trained directly from images-questions-answers. TbD+hres~\cite{mascharka2018transparency} uses high-resolution (28x28) spatial tensor, while majority uses either 8x8 or 14x14. 
HAN+Sum/RN$^{+}$ denotes a larger relational model, or a different hyper-parameters setup, than the model of \cite{santoro2017simple}. 
HAN+RN$^{++}$ denotes HAN+RN$^{+}$ with larger input images with spatial dimensions 224x224 as opposed to 128x128, and larger image tensors with spatial dimension 14x14 as opposed to 8x8. 
}
\label{table:detailed_clevr}
\end{table}