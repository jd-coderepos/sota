
\documentclass[10pt]{article} \usepackage[accepted]{tmlr}



\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} \usepackage{placeins}
\usepackage{amsmath,amssymb,amsfonts,amscd}
\usepackage[all,cmtip]{xy}
\usepackage{enumerate}
\usepackage{latexsym}
\usepackage{fullpage}
\usepackage{tabularx}




\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{authblk}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{lipsum}

\usepackage{graphicx}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}



\newtheorem{propo}{Proposition}[section]
\newtheorem{coro}[propo]{Corollary}
\newtheorem{thm}{Theorem}
\newtheorem{asmp}{Assumption}
\newtheorem{conj}[propo]{Conjecture}
\newtheorem{fact}[propo]{Fact}
\newtheorem{claim}[propo]{Claim}
\newcommand{\ip}[2]{\left\langle #1, #2 \right \rangle}
\newcommand{\rev}[1]{#1}
\newcommand{\defeq}{\vcentcolon=}

\DeclareMathOperator*{\Multi}{Multi}
\def\real{\mathbb R}
\def\eps{\varepsilon}
\def\tukey{\text{Tukey}}
\def\Cov{\text{Cov}}
\def\Dir{\text{Dir}}
\def\Var{\text{Var}}

\def\E{\mathbb E}
\def\I{\mathbf I}
\def\F{\mathbf F}
\def\P{\mathbf P}
\def\Q{\mathbf Q}
\def\T{\mathbf T}
\newcommand{\reals}{\mathbb{R}}

\def\a{\mathbf a}
\def\b{\mathbf b}
\def\p{\mathbf p}
\def\q{\mathbf q}
\def\s{\mathbf s}
\def\t{\mathbf t}
\def\w{\mathbf{w}}
\def\x{\mathbf{x}}
\def\y{\mathbf{y}}



\newcommand{\Weihao}[1]{{\color{cyan}[Weihao: #1]}}
\newcommand{\adnote}[1]{\textcolor{purple}{[AD: #1]}}
\newcommand{\rsnote}[1]{\textcolor{purple}{[RS: #1]}}


\RequirePackage{latexsym}
\RequirePackage{amsmath}
\RequirePackage{amsthm}
\RequirePackage{amssymb}
\RequirePackage{bm}
\RequirePackage{stmaryrd}
\RequirePackage{environ}
\RequirePackage{graphics}
\usepackage{dsfont}


\newcount\Comments  \Comments=1   \newcommand{\kibitz}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\usepackage{color}
\definecolor{red}{rgb}{1,0,0}
\newcommand{\owner}[1]{\kibitz{red}      {[CURRENT OWNER: #1]}}

\newcommand{\assign}[2][1]{\kibitz{red}      {[TODO(#1):#2]}}


\RequirePackage{xspace}
\newcommand{\Blackbox}{Black-box\xspace}
\newcommand{\blackbox}{black-box\xspace}


\makeatletter
\@ifpackageloaded{dsfont}{}{\newcommand{\mathds}[1]{\mathbb{#1}}}
\@ifpackageloaded{mathrsfs}{}{\newcommand{\mathscr}[1]{\mathcal{#1}}}
\makeatother



\NewEnviron{resize}[2][!]{\resizebox{#2}{#1}{\BODY}} 
\NewEnviron{rescale}[2][]{\scalebox{#2}[#1]{\BODY}}

\makeatletter
\newcommand{\newreptheorem}[2]{
  \newtheorem*{rep@#1}{\rep@title} 
  \newenvironment{rep#1}[1]{\def\rep@title{#2 \ref*{##1}}\begin{rep@#1}}{\end{rep@#1}}
}
\makeatother
\newreptheorem{theorem}{Theorem}
\newreptheorem{claim}{Claim}

\usepackage{thm-restate}



\newcommand{\ab}{{\bm{a}}}
\newcommand{\bb}{{\bm{b}}}
\newcommand{\cbb}{{\bm{c}}}
\newcommand{\db}{{\bm{d}}}
\newcommand{\eb}{{\bm{e}}}
\newcommand{\fb}{{\bm{f}}}
\newcommand{\gb}{{\bm{g}}}
\newcommand{\hb}{{\bm{h}}}
\newcommand{\ib}{{\bm{i}}}
\newcommand{\jb}{{\bm{j}}}
\newcommand{\kb}{{\bm{k}}}
\newcommand{\lb}{{\bm{l}}}
\newcommand{\mb}{{\bm{m}}}
\newcommand{\nbb}{{\bm{n}}}
\newcommand{\ob}{{\bm{o}}}
\newcommand{\pb}{{\bm{p}}}
\newcommand{\qb}{{\bm{q}}}
\newcommand{\rb}{{\bm{r}}}
\newcommand{\sbb}{{\bm{s}}}
\newcommand{\tb}{{\bm{t}}}
\newcommand{\ub}{{\bm{u}}}
\newcommand{\vb}{{\bm{v}}}
\newcommand{\wb}{{\bm{w}}}
\newcommand{\xb}{{\bm{x}}}
\newcommand{\yb}{{\bm{y}}}
\newcommand{\zb}{{\bm{z}}}

\newcommand{\ba}{\ab}
\newcommand{\bc}{\cbb}
\newcommand{\bd}{\db}
\newcommand{\be}{\eb}
\newcommand{\bbf}{\fb}
\newcommand{\bff}{\fb}
\newcommand{\bg}{\gb}
\newcommand{\bh}{\hb}
\newcommand{\bi}{\ib}
\newcommand{\bj}{\jb}
\newcommand{\bk}{\kb}
\newcommand{\bl}{\lb}
\newcommand{\bbm}{\mb}
\newcommand{\bmm}{\mb}
\newcommand{\bn}{\nbb}
\newcommand{\bo}{\ob}
\newcommand{\bp}{\pb}
\newcommand{\bq}{\qb}
\newcommand{\br}{\rb}
\newcommand{\bs}{\sbb}
\newcommand{\bt}{\tb}
\newcommand{\bu}{\ub}
\newcommand{\bv}{\vb}
\newcommand{\bw}{\wb}
\newcommand{\bx}{\xb}
\newcommand{\by}{y^n}
\newcommand{\bz}{\zb}

\newcommand{\atil}{\tilde{a}}
\newcommand{\btil}{\tilde{b}}
\newcommand{\ctil}{\tilde{c}}
\newcommand{\dtil}{\tilde{d}}
\newcommand{\etil}{\tilde{e}}
\newcommand{\ftil}{\tilde{f}}
\newcommand{\gtil}{\tilde{g}}
\newcommand{\htil}{\tilde{h}}
\newcommand{\itil}{\tilde{i}}
\newcommand{\jtil}{\tilde{j}}
\newcommand{\ktil}{\tilde{k}}
\newcommand{\ltil}{\tilde{l}}
\newcommand{\mtil}{\tilde{m}}
\newcommand{\ntil}{\tilde{n}}
\newcommand{\otil}{\tilde{o}}
\newcommand{\ptil}{\tilde{p}}
\newcommand{\qtil}{\tilde{q}}
\newcommand{\rtil}{\tilde{r}}
\newcommand{\stil}{\tilde{s}}
\newcommand{\ttil}{\tilde{t}}
\newcommand{\util}{\tilde{u}}
\newcommand{\vtil}{\tilde{v}}
\newcommand{\wtil}{\tilde{w}}
\newcommand{\xtil}{\tilde{x}}
\newcommand{\ytil}{\tilde{y}}
\newcommand{\ztil}{\tilde{z}}

\newcommand{\at}{\tilde{a}}
\newcommand{\bti}{\tilde{b}}
\newcommand{\ct}{\tilde{c}}
\newcommand{\dt}{\tilde{d}}
\newcommand{\et}{\tilde{e}}
\newcommand{\ft}{\tilde{f}}
\newcommand{\gt}{\tilde{g}}
\newcommand{\hti}{\tilde{h}}
\newcommand{\htt}{\tilde{h}}
\newcommand{\iti}{\tilde{i}}
\newcommand{\itt}{\tilde{i}}
\newcommand{\jt}{\tilde{j}}
\newcommand{\kt}{\tilde{k}}
\newcommand{\lt}{\tilde{l}}
\newcommand{\mt}{\tilde{m}}
\newcommand{\nt}{\tilde{n}}
\newcommand{\ot}{\tilde{o}}
\newcommand{\pti}{\tilde{p}}
\newcommand{\ptt}{\tilde{p}}
\newcommand{\qt}{\tilde{q}}
\newcommand{\rt}{\tilde{r}}
\newcommand{\st}{\tilde{s}}
\newcommand{\tti}{\tilde{t}}
\newcommand{\ttt}{\tilde{t}}
\newcommand{\ut}{\tilde{u}}
\newcommand{\vti}{\tilde{v}}
\newcommand{\vtt}{\tilde{v}}
\newcommand{\wt}{\tilde{w}}
\newcommand{\xt}{\tilde{x}}
\newcommand{\yt}{\tilde{y}}
\newcommand{\zt}{\tilde{z}}

\newcommand{\abtil}{\tilde{\ab}}
\newcommand{\bbtil}{\tilde{\bb}}
\newcommand{\cbtil}{\tilde{\cbb}}
\newcommand{\dbtil}{\tilde{\db}}
\newcommand{\ebtil}{\tilde{\eb}}
\newcommand{\fbtil}{\tilde{\fb}}
\newcommand{\gbtil}{\tilde{\gb}}
\newcommand{\hbtil}{\tilde{\hb}}
\newcommand{\ibtil}{\tilde{\ib}}
\newcommand{\jbtil}{\tilde{\jb}}
\newcommand{\kbtil}{\tilde{\kb}}
\newcommand{\lbtil}{\tilde{\lb}}
\newcommand{\mbtil}{\tilde{\mb}}
\newcommand{\nbtil}{\tilde{\nbb}}
\newcommand{\obtil}{\tilde{\ob}}
\newcommand{\pbtil}{\tilde{\pb}}
\newcommand{\qbtil}{\tilde{\qb}}
\newcommand{\rbtil}{\tilde{\rb}}
\newcommand{\sbtil}{\tilde{\sbb}}
\newcommand{\tbtil}{\tilde{\tb}}
\newcommand{\ubtil}{\tilde{\ub}}
\newcommand{\vbtil}{\tilde{\vb}}
\newcommand{\wbtil}{\tilde{\wb}}
\newcommand{\xbtil}{\tilde{\xb}}
\newcommand{\ybtil}{\tilde{\yb}}
\newcommand{\zbtil}{\tilde{\zb}}

\newcommand{\batil}{\tilde{\ab}}
\newcommand{\bctil}{\tilde{\cbb}}
\newcommand{\bdtil}{\tilde{\db}}
\newcommand{\betil}{\tilde{\eb}}
\newcommand{\bftil}{\tilde{\fb}}
\newcommand{\bgtil}{\tilde{\gb}}
\newcommand{\bhtil}{\tilde{\hb}}
\newcommand{\bitil}{\tilde{\ib}}
\newcommand{\bjtil}{\tilde{\jb}}
\newcommand{\bktil}{\tilde{\kb}}
\newcommand{\bltil}{\tilde{\lb}}
\newcommand{\bmtil}{\tilde{\mb}}
\newcommand{\bntil}{\tilde{\nbb}}
\newcommand{\botil}{\tilde{\ob}}
\newcommand{\bptil}{\tilde{\pb}}
\newcommand{\bqtil}{\tilde{\qb}}
\newcommand{\brtil}{\tilde{\rb}}
\newcommand{\bstil}{\tilde{\sbb}}
\newcommand{\bttil}{\tilde{\tb}}
\newcommand{\butil}{\tilde{\ub}}
\newcommand{\bvtil}{\tilde{\vb}}
\newcommand{\bwtil}{\tilde{\wb}}
\newcommand{\bxtil}{\tilde{\xb}}
\newcommand{\bytil}{\tilde{\yb}}
\newcommand{\bztil}{\tilde{\zb}}

\newcommand{\bat}{\tilde{\ab}}
\newcommand{\bbt}{\tilde{\bb}}
\newcommand{\bct}{\tilde{\cbb}}
\newcommand{\bdt}{\tilde{\db}}
\newcommand{\bet}{\tilde{\eb}}
\newcommand{\bft}{\tilde{\fb}}
\newcommand{\bgt}{\tilde{\gb}}
\newcommand{\bht}{\tilde{\hb}}
\newcommand{\bit}{\tilde{\ib}}
\newcommand{\bjt}{\tilde{\jb}}
\newcommand{\bkt}{\tilde{\kb}}
\newcommand{\blt}{\tilde{\lb}}
\newcommand{\bmt}{\tilde{\mb}}
\newcommand{\bnt}{\tilde{\nbb}}
\newcommand{\boti}{\tilde{\ob}}
\newcommand{\boot}{\tilde{\ob}}
\newcommand{\bpt}{\tilde{\pb}}
\newcommand{\bqt}{\tilde{\qb}}
\newcommand{\brt}{\tilde{\rb}}
\newcommand{\bst}{\tilde{\sbb}}
\newcommand{\btti}{\tilde{\tb}}
\newcommand{\btt}{\tilde{\tb}}
\newcommand{\but}{\tilde{\ub}}
\newcommand{\bvt}{\tilde{\vb}}
\newcommand{\bwt}{\tilde{\wb}}
\newcommand{\bxt}{\tilde{\xb}}
\newcommand{\byt}{\tilde{\yb}}
\newcommand{\bzt}{\tilde{\zb}}

\newcommand{\ahat}{\hat{a}}
\newcommand{\bhat}{\hat{b}}
\newcommand{\chat}{\hat{c}}
\newcommand{\dhat}{\hat{d}}
\newcommand{\ehat}{\hat{e}}
\newcommand{\fhat}{\hat{f}}
\newcommand{\ghat}{\hat{g}}
\newcommand{\hhat}{\hat{h}}
\newcommand{\ihat}{\hat{i}}
\newcommand{\jhat}{\hat{j}}
\newcommand{\khat}{\hat{k}}
\newcommand{\lhat}{\hat{l}}
\newcommand{\mhat}{\hat{m}}
\newcommand{\nhat}{\hat{n}}
\newcommand{\ohat}{\hat{o}}
\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}
\newcommand{\shat}{\hat{s}}
\newcommand{\that}{\hat{t}}
\newcommand{\uhat}{\hat{u}}
\newcommand{\vhat}{\hat{v}}
\newcommand{\what}{\hat{w}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\zhat}{\hat{z}}
\newcommand{\pihat}{\hat{\pi}}

\newcommand{\abhat}{\hat{\ab}}
\newcommand{\bbhat}{\hat{\bb}}
\newcommand{\cbhat}{\hat{\cb}}
\newcommand{\dbhat}{\hat{\db}}
\newcommand{\ebhat}{\hat{\eb}}
\newcommand{\fbhat}{\hat{\fb}}
\newcommand{\gbhat}{\hat{\gb}}
\newcommand{\hbhat}{\hat{\hb}}
\newcommand{\ibhat}{\hat{\ib}}
\newcommand{\jbhat}{\hat{\jb}}
\newcommand{\kbhat}{\hat{\kb}}
\newcommand{\lbhat}{\hat{\lb}}
\newcommand{\mbhat}{\hat{\mb}}
\newcommand{\nbhat}{\hat{\nb}}
\newcommand{\obhat}{\hat{\ob}}
\newcommand{\pbhat}{\hat{\pb}}
\newcommand{\qbhat}{\hat{\qb}}
\newcommand{\rbhat}{\hat{\rb}}
\newcommand{\sbhat}{\hat{\sb}}
\newcommand{\tbhat}{\hat{\tb}}
\newcommand{\ubhat}{\hat{\ub}}
\newcommand{\vbhat}{\hat{\vb}}
\newcommand{\wbhat}{\hat{\wb}}
\newcommand{\xbhat}{\hat{\xb}}
\newcommand{\ybhat}{\hat{\yb}}
\newcommand{\zbhat}{\hat{\zb}}

\newcommand{\bahat}{\hat{\ab}}
\newcommand{\bchat}{\hat{\bc}}
\newcommand{\bdhat}{\hat{\db}}
\newcommand{\behat}{\hat{\eb}}
\newcommand{\bfhat}{\hat{\fb}}
\newcommand{\bghat}{\hat{\gb}}
\newcommand{\bhhat}{\hat{\hb}}
\newcommand{\bihat}{\hat{\ib}}
\newcommand{\bjhat}{\hat{\jb}}
\newcommand{\bkhat}{\hat{\kb}}
\newcommand{\blhat}{\hat{\lb}}
\newcommand{\bmhat}{\hat{\mb}}
\newcommand{\bnhat}{\hat{\nbb}}
\newcommand{\bohat}{\hat{\ob}}
\newcommand{\bphat}{\hat{\pb}}
\newcommand{\bqhat}{\hat{\qb}}
\newcommand{\brhat}{\hat{\rb}}
\newcommand{\bshat}{\hat{\sbb}}
\newcommand{\bthat}{\hat{\tb}}
\newcommand{\buhat}{\hat{\ub}}
\newcommand{\bvhat}{\hat{\vb}}
\newcommand{\bwhat}{\hat{\wb}}
\newcommand{\bxhat}{\hat{\xb}}
\newcommand{\byhat}{\hat{\yb}}
\newcommand{\bzhat}{\hat{\zb}}

\newcommand{\abar}{\bar{a}}
\newcommand{\bbar}{\bar{b}}
\newcommand{\cbar}{\bar{c}}
\newcommand{\dbar}{\bar{d}}
\newcommand{\ebar}{\bar{e}}
\newcommand{\fbar}{\bar{f}}
\newcommand{\gbar}{\bar{g}}
\newcommand{\hbr}{\bar{h}}
\newcommand{\ibar}{\bar{i}}
\newcommand{\jbar}{\bar{j}}
\newcommand{\kbar}{\bar{k}}
\newcommand{\lbar}{\bar{l}}
\newcommand{\mbar}{\bar{m}}
\newcommand{\nbar}{\bar{n}}
\newcommand{\obr}{\bar{o}}
\newcommand{\pbar}{\bar{p}}
\newcommand{\qbar}{\bar{q}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\sbar}{\bar{s}}
\newcommand{\tbar}{\bar{t}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\vbar}{\bar{v}}
\newcommand{\wbar}{\bar{w}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\abbar}{\bar{\ab}}
\newcommand{\bbbar}{\bar{\bb}}
\newcommand{\cbbar}{\bar{\cb}}
\newcommand{\dbbar}{\bar{\db}}
\newcommand{\ebbar}{\bar{\eb}}
\newcommand{\fbbar}{\bar{\fb}}
\newcommand{\gbbar}{\bar{\gb}}
\newcommand{\hbbar}{\bar{\hb}}
\newcommand{\ibbar}{\bar{\ib}}
\newcommand{\jbbar}{\bar{\jb}}
\newcommand{\kbbar}{\bar{\kb}}
\newcommand{\lbbar}{\bar{\lb}}
\newcommand{\mbbar}{\bar{\mb}}
\newcommand{\nbbar}{\bar{\nbb}}
\newcommand{\obbar}{\bar{\ob}}
\newcommand{\pbbar}{\bar{\pb}}
\newcommand{\qbbar}{\bar{\qb}}
\newcommand{\rbbar}{\bar{\rb}}
\newcommand{\sbbar}{\bar{\sbb}}
\newcommand{\tbbar}{\bar{\tb}}
\newcommand{\ubbar}{\bar{\ub}}
\newcommand{\vbbar}{\bar{\vb}}
\newcommand{\wbbar}{\bar{\wb}}
\newcommand{\xbbar}{\bar{\xb}}
\newcommand{\ybbar}{\bar{\yb}}
\newcommand{\zbbar}{\bar{\zb}}

\newcommand{\babar}{\bar{\ab}}
\newcommand{\bcbar}{\bar{\bc}}
\newcommand{\bdbar}{\bar{\db}}
\newcommand{\bebar}{\bar{\eb}}
\newcommand{\bfbar}{\bar{\fb}}
\newcommand{\bgbar}{\bar{\gb}}
\newcommand{\bhbar}{\bar{\hb}}
\newcommand{\bibar}{\bar{\ib}}
\newcommand{\bjbar}{\bar{\jb}}
\newcommand{\bkbar}{\bar{\kb}}
\newcommand{\blbar}{\bar{\lb}}
\newcommand{\bmbar}{\bar{\mb}}
\newcommand{\bnbar}{\bar{\nbb}}
\newcommand{\bobar}{\bar{\ob}}
\newcommand{\bpbar}{\bar{\pb}}
\newcommand{\bqbar}{\bar{\qb}}
\newcommand{\brbar}{\bar{\rb}}
\newcommand{\bsbar}{\bar{\sbb}}
\newcommand{\btbar}{\bar{\tb}}
\newcommand{\bubar}{\bar{\ub}}
\newcommand{\bvbar}{\bar{\vb}}
\newcommand{\bwbar}{\bar{\wb}}
\newcommand{\bxbar}{\bar{\xb}}
\newcommand{\bybar}{\bar{\yb}}
\newcommand{\bzbar}{\bar{\zb}}

\newcommand{\Atil}{\tilde{A}}
\newcommand{\Btil}{\tilde{B}}
\newcommand{\Ctil}{\tilde{C}}
\newcommand{\Dtil}{\tilde{D}}
\newcommand{\Etil}{\tilde{E}}
\newcommand{\Ftil}{\tilde{F}}
\newcommand{\Gtil}{\tilde{G}}
\newcommand{\Htil}{\tilde{H}}
\newcommand{\Itil}{\tilde{I}}
\newcommand{\Jtil}{\tilde{J}}
\newcommand{\Ktil}{\tilde{K}}
\newcommand{\Ltil}{\tilde{L}}
\newcommand{\Mtil}{\tilde{M}}
\newcommand{\Ntil}{\tilde{N}}
\newcommand{\Otil}{\tilde{O}}
\newcommand{\Ptil}{\tilde{P}}
\newcommand{\Qtil}{\tilde{Q}}
\newcommand{\Rtil}{\tilde{R}}
\newcommand{\Stil}{\tilde{S}}
\newcommand{\Ttil}{\tilde{T}}
\newcommand{\Util}{\tilde{U}}
\newcommand{\Vtil}{\tilde{V}}
\newcommand{\Wtil}{\tilde{W}}
\newcommand{\Xtil}{\tilde{X}}
\newcommand{\Ytil}{\tilde{Y}}
\newcommand{\Ztil}{\tilde{Z}}

\newcommand{\At}{\tilde{A}}
\newcommand{\Bt}{\tilde{B}}
\newcommand{\Ct}{\tilde{C}}
\newcommand{\Dt}{\tilde{D}}
\newcommand{\Et}{\tilde{E}}
\newcommand{\Ft}{\tilde{F}}
\newcommand{\Gt}{\tilde{G}}
\newcommand{\Ht}{\tilde{H}}
\newcommand{\It}{\tilde{I}}
\newcommand{\Jt}{\tilde{J}}
\newcommand{\Kt}{\tilde{K}}
\newcommand{\Lt}{\tilde{L}}
\newcommand{\Mt}{\tilde{M}}
\newcommand{\Nt}{\tilde{N}}
\newcommand{\Ot}{\tilde{O}}
\newcommand{\Pt}{\tilde{P}}
\newcommand{\Qt}{\tilde{Q}}
\newcommand{\Rt}{\tilde{R}}
\newcommand{\St}{\tilde{S}}
\newcommand{\Tt}{\tilde{T}}
\newcommand{\Ut}{\tilde{U}}
\newcommand{\Vt}{\tilde{V}}
\newcommand{\Wt}{\tilde{W}}
\newcommand{\Xt}{\tilde{X}}
\newcommand{\Yt}{\tilde{Y}}
\newcommand{\Zt}{\tilde{Z}}

\newcommand{\Atilde}{\widetilde{A}}
\newcommand{\Btilde}{\widetilde{B}}
\newcommand{\Ctilde}{\widetilde{C}}
\newcommand{\Dtilde}{\widetilde{D}}
\newcommand{\Etilde}{\widetilde{E}}
\newcommand{\Ftilde}{\widetilde{F}}
\newcommand{\Gtilde}{\widetilde{G}}
\newcommand{\Htilde}{\widetilde{H}}
\newcommand{\Itilde}{\widetilde{I}}
\newcommand{\Jtilde}{\widetilde{J}}
\newcommand{\Ktilde}{\widetilde{K}}
\newcommand{\Ltilde}{\widetilde{L}}
\newcommand{\Mtilde}{\widetilde{M}}
\newcommand{\Ntilde}{\widetilde{N}}
\newcommand{\Otilde}{\widetilde{O}}
\newcommand{\Ptilde}{\widetilde{P}}
\newcommand{\Qtilde}{\widetilde{Q}}
\newcommand{\Rtilde}{\widetilde{R}}
\newcommand{\Stilde}{\widetilde{S}}
\newcommand{\Ttilde}{\widetilde{T}}
\newcommand{\Utilde}{\widetilde{U}}
\newcommand{\Vtilde}{\widetilde{V}}
\newcommand{\Wtilde}{\widetilde{W}}
\newcommand{\Xtilde}{\widetilde{X}}
\newcommand{\Ytilde}{\widetilde{Y}}
\newcommand{\Ztilde}{\widetilde{Z}}

\newcommand{\Abar}{\bar{A}}
\newcommand{\Bbar}{\bar{B}}
\newcommand{\Cbar}{\bar{C}}
\newcommand{\Dbar}{\bar{D}}
\newcommand{\Ebar}{\bar{E}}
\newcommand{\Fbar}{\bar{F}}
\newcommand{\Gbar}{\bar{G}}
\newcommand{\Hbar}{\bar{H}}
\newcommand{\Ibar}{\bar{I}}
\newcommand{\Jbar}{\bar{J}}
\newcommand{\Kbar}{\bar{K}}
\newcommand{\Lbar}{\bar{L}}
\newcommand{\Mbar}{\bar{M}}
\newcommand{\Nbar}{\bar{N}}
\newcommand{\Obar}{\bar{O}}
\newcommand{\Pbar}{\bar{P}}
\newcommand{\Qbar}{\bar{Q}}
\newcommand{\Rbar}{\bar{R}}
\newcommand{\Sbar}{\bar{S}}
\newcommand{\Tbar}{\bar{T}}
\newcommand{\Ubar}{\bar{U}}
\newcommand{\Vbar}{\bar{V}}
\newcommand{\Wbar}{\bar{W}}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}

\newcommand{\Ahat}{\hat{A}}
\newcommand{\Bhat}{\hat{B}}
\newcommand{\Chat}{\hat{C}}
\newcommand{\Dhat}{\hat{D}}
\newcommand{\Ehat}{\hat{E}}
\newcommand{\Fhat}{\hat{F}}
\newcommand{\Ghat}{\hat{G}}
\newcommand{\Hhat}{\hat{H}}
\newcommand{\Ihat}{\hat{I}}
\newcommand{\Jhat}{\hat{J}}
\newcommand{\Khat}{\hat{K}}
\newcommand{\Lhat}{\hat{L}}
\newcommand{\Mhat}{\hat{M}}
\newcommand{\Nhat}{\hat{N}}
\newcommand{\Ohat}{\hat{O}}
\newcommand{\Phat}{\hat{P}}
\newcommand{\Qhat}{\hat{Q}}
\newcommand{\Rhat}{\hat{R}}
\newcommand{\Shat}{\hat{S}}
\newcommand{\That}{\hat{T}}
\newcommand{\Uhat}{\hat{U}}
\newcommand{\Vhat}{\hat{V}}
\newcommand{\What}{\hat{W}}
\newcommand{\Xhat}{\hat{X}}
\newcommand{\Yhat}{\hat{Y}}
\newcommand{\Zhat}{\hat{Z}}

\newcommand{\Ab}{\bm{{A}}}
\newcommand{\Bb}{\bm{{B}}}
\newcommand{\Cb}{\bm{{C}}}
\newcommand{\Db}{\bm{{D}}}
\newcommand{\Eb}{\bm{{E}}}
\newcommand{\Fb}{\bm{{F}}}
\newcommand{\Gb}{\bm{{G}}}
\newcommand{\Hb}{\bm{{H}}}
\newcommand{\Ib}{\bm{{I}}}
\newcommand{\Jb}{\bm{{J}}}
\newcommand{\Kb}{\bm{{K}}}
\newcommand{\Lb}{\bm{{L}}}
\newcommand{\Mb}{\bm{{M}}}
\newcommand{\Nb}{\bm{{N}}}
\newcommand{\Ob}{\bm{{O}}}
\newcommand{\Pb}{\bm{{P}}}
\newcommand{\Qb}{\bm{{Q}}}
\newcommand{\Rb}{\bm{{R}}}
\newcommand{\Sbb}{\bm{{S}}}
\newcommand{\Tb}{\bm{{T}}}
\newcommand{\Ub}{\bm{{U}}}
\newcommand{\Vb}{\bm{{V}}}
\newcommand{\Wb}{\bm{{W}}}
\newcommand{\Xb}{\bm{{X}}}
\newcommand{\Yb}{\bm{{Y}}}
\newcommand{\Zb}{\bm{{Z}}}

\newcommand{\bA}{\bm{{A}}}
\newcommand{\bB}{\bm{{B}}}
\newcommand{\bC}{\bm{{C}}}
\newcommand{\bD}{\bm{{D}}}
\newcommand{\bE}{\bm{{E}}}
\newcommand{\bF}{\bm{{F}}}
\newcommand{\bG}{\bm{{G}}}
\newcommand{\bH}{\bm{{H}}}
\newcommand{\bI}{\bm{{I}}}
\newcommand{\bJ}{\bm{{J}}}
\newcommand{\bK}{\bm{{K}}}
\newcommand{\bL}{\bm{{L}}}
\newcommand{\bM}{\bm{{M}}}
\newcommand{\bN}{\bm{{N}}}
\newcommand{\bO}{\bm{{O}}}
\newcommand{\bP}{\bm{{P}}}
\newcommand{\bQ}{\bm{{Q}}}
\newcommand{\bR}{\bm{{R}}}
\newcommand{\bS}{\bm{{S}}}
\newcommand{\bT}{\bm{{T}}}
\newcommand{\bU}{\bm{{U}}}
\newcommand{\bV}{\bm{{V}}}
\newcommand{\bW}{\bm{{W}}}
\newcommand{\bX}{\bm{{X}}}
\newcommand{\bY}{\bm{{Y}}}
\newcommand{\bZ}{\bm{{Z}}}

\newcommand{\Abtil}{\tilde{\Ab}}
\newcommand{\Bbtil}{\tilde{\Bb}}
\newcommand{\Cbtil}{\tilde{\Cb}}
\newcommand{\Dbtil}{\tilde{\Db}}
\newcommand{\Ebtil}{\tilde{\Eb}}
\newcommand{\Fbtil}{\tilde{\Fb}}
\newcommand{\Gbtil}{\tilde{\Gb}}
\newcommand{\Hbtil}{\tilde{\Hb}}
\newcommand{\Ibtil}{\tilde{\Ib}}
\newcommand{\Jbtil}{\tilde{\Jb}}
\newcommand{\Kbtil}{\tilde{\Kb}}
\newcommand{\Lbtil}{\tilde{\Lb}}
\newcommand{\Mbtil}{\tilde{\Mb}}
\newcommand{\Nbtil}{\tilde{\Nb}}
\newcommand{\Obtil}{\tilde{\Ob}}
\newcommand{\Pbtil}{\tilde{\Pb}}
\newcommand{\Qbtil}{\tilde{\Qb}}
\newcommand{\Rbtil}{\tilde{\Rb}}
\newcommand{\Sbtil}{\tilde{\Sbb}}
\newcommand{\Tbtil}{\tilde{\Tb}}
\newcommand{\Ubtil}{\tilde{\Ub}}
\newcommand{\Vbtil}{\tilde{\Vb}}
\newcommand{\Wbtil}{\tilde{\Wb}}
\newcommand{\Xbtil}{\tilde{\Xb}}
\newcommand{\Ybtil}{\tilde{\Yb}}
\newcommand{\Zbtil}{\tilde{\Zb}}

\newcommand{\bAt}{\tilde{\Ab}}
\newcommand{\bBt}{\tilde{\Bb}}
\newcommand{\bCt}{\tilde{\Cb}}
\newcommand{\bDt}{\tilde{\Db}}
\newcommand{\bEt}{\tilde{\Eb}}
\newcommand{\bFt}{\tilde{\Fb}}
\newcommand{\bGt}{\tilde{\Gb}}
\newcommand{\bHt}{\tilde{\Hb}}
\newcommand{\bIt}{\tilde{\Ib}}
\newcommand{\bJt}{\tilde{\Jb}}
\newcommand{\bKt}{\tilde{\Kb}}
\newcommand{\bLt}{\tilde{\Lb}}
\newcommand{\bMt}{\tilde{\Mb}}
\newcommand{\bNt}{\tilde{\Nb}}
\newcommand{\bOt}{\tilde{\Ob}}
\newcommand{\bPt}{\tilde{\Pb}}
\newcommand{\bQt}{\tilde{\Qb}}
\newcommand{\bRt}{\tilde{\Rb}}
\newcommand{\bSt}{\tilde{\Sbb}}
\newcommand{\bTt}{\tilde{\Tb}}
\newcommand{\bUt}{\tilde{\Ub}}
\newcommand{\bVt}{\tilde{\Vb}}
\newcommand{\bWt}{\tilde{\Wb}}
\newcommand{\bXt}{\tilde{\Xb}}
\newcommand{\bYt}{\tilde{\Yb}}
\newcommand{\bZt}{\tilde{\Zb}}

\newcommand{\bAtil}{\tilde{\Ab}}
\newcommand{\bBtil}{\tilde{\Bb}}
\newcommand{\bCtil}{\tilde{\Cb}}
\newcommand{\bDtil}{\tilde{\Db}}
\newcommand{\bEtil}{\tilde{\Eb}}
\newcommand{\bFtil}{\tilde{\Fb}}
\newcommand{\bGtil}{\tilde{\Gb}}
\newcommand{\bHtil}{\tilde{\Hb}}
\newcommand{\bItil}{\tilde{\Ib}}
\newcommand{\bJtil}{\tilde{\Jb}}
\newcommand{\bKtil}{\tilde{\Kb}}
\newcommand{\bLtil}{\tilde{\Lb}}
\newcommand{\bMtil}{\tilde{\Mb}}
\newcommand{\bNtil}{\tilde{\Nb}}
\newcommand{\bOtil}{\tilde{\Ob}}
\newcommand{\bPtil}{\tilde{\Pb}}
\newcommand{\bQtil}{\tilde{\Qb}}
\newcommand{\bRtil}{\tilde{\Rb}}
\newcommand{\bStil}{\tilde{\Sbb}}
\newcommand{\bTtil}{\tilde{\Tb}}
\newcommand{\bUtil}{\tilde{\Ub}}
\newcommand{\bVtil}{\tilde{\Vb}}
\newcommand{\bWtil}{\tilde{\Wb}}
\newcommand{\bXtil}{\tilde{\Xb}}
\newcommand{\bYtil}{\tilde{\Yb}}
\newcommand{\bZtil}{\tilde{\Zb}}

\newcommand{\bAtilde}{\widetilde{\Ab}}
\newcommand{\bBtilde}{\widetilde{\Bb}}
\newcommand{\bCtilde}{\widetilde{\Cb}}
\newcommand{\bDtilde}{\widetilde{\Db}}
\newcommand{\bEtilde}{\widetilde{\Eb}}
\newcommand{\bFtilde}{\widetilde{\Fb}}
\newcommand{\bGtilde}{\widetilde{\Gb}}
\newcommand{\bHtilde}{\widetilde{\Hb}}
\newcommand{\bItilde}{\widetilde{\Ib}}
\newcommand{\bJtilde}{\widetilde{\Jb}}
\newcommand{\bKtilde}{\widetilde{\Kb}}
\newcommand{\bLtilde}{\widetilde{\Lb}}
\newcommand{\bMtilde}{\widetilde{\Mb}}
\newcommand{\bNtilde}{\widetilde{\Nb}}
\newcommand{\bOtilde}{\widetilde{\Ob}}
\newcommand{\bPtilde}{\widetilde{\Pb}}
\newcommand{\bQtilde}{\widetilde{\Qb}}
\newcommand{\bRtilde}{\widetilde{\Rb}}
\newcommand{\bStilde}{\widetilde{\Sbb}}
\newcommand{\bTtilde}{\widetilde{\Tb}}
\newcommand{\bUtilde}{\widetilde{\Ub}}
\newcommand{\bVtilde}{\widetilde{\Vb}}
\newcommand{\bWtilde}{\widetilde{\Wb}}
\newcommand{\bXtilde}{\widetilde{\Xb}}
\newcommand{\bYtilde}{\widetilde{\Yb}}
\newcommand{\bZtilde}{\widetilde{\Zb}}

\newcommand{\Abbar}{\bar{\Ab}}
\newcommand{\Bbbar}{\bar{\Bb}}
\newcommand{\Cbbar}{\bar{\Cb}}
\newcommand{\Dbbar}{\bar{\Db}}
\newcommand{\Ebbar}{\bar{\Eb}}
\newcommand{\Fbbar}{\bar{\Fb}}
\newcommand{\Gbbar}{\bar{\Gb}}
\newcommand{\Hbbar}{\bar{\Hb}}
\newcommand{\Ibbar}{\bar{\Ib}}
\newcommand{\Jbbar}{\bar{\Jb}}
\newcommand{\Kbbar}{\bar{\Kb}}
\newcommand{\Lbbar}{\bar{\Lb}}
\newcommand{\Mbbar}{\bar{\Mb}}
\newcommand{\Nbbar}{\bar{\Nb}}
\newcommand{\Obbar}{\bar{\Ob}}
\newcommand{\Pbbar}{\bar{\Pb}}
\newcommand{\Qbbar}{\bar{\Qb}}
\newcommand{\Rbbar}{\bar{\Rb}}
\newcommand{\Sbbar}{\bar{\Sbb}}
\newcommand{\Tbbar}{\bar{\Tb}}
\newcommand{\Ubbar}{\bar{\Ub}}
\newcommand{\Vbbar}{\bar{\Vb}}
\newcommand{\Wbbar}{\bar{\Wb}}
\newcommand{\Xbbar}{\bar{\Xb}}
\newcommand{\Ybbar}{\bar{\Yb}}
\newcommand{\Zbbar}{\bar{\Zb}}

\newcommand{\bAbar}{\bar{\Ab}}
\newcommand{\bBbar}{\bar{\Bb}}
\newcommand{\bCbar}{\bar{\Cb}}
\newcommand{\bDbar}{\bar{\Db}}
\newcommand{\bEbar}{\bar{\Eb}}
\newcommand{\bFbar}{\bar{\Fb}}
\newcommand{\bGbar}{\bar{\Gb}}
\newcommand{\bHbar}{\bar{\Hb}}
\newcommand{\bIbar}{\bar{\Ib}}
\newcommand{\bJbar}{\bar{\Jb}}
\newcommand{\bKbar}{\bar{\Kb}}
\newcommand{\bLbar}{\bar{\Lb}}
\newcommand{\bMbar}{\bar{\Mb}}
\newcommand{\bNbar}{\bar{\Nb}}
\newcommand{\bObar}{\bar{\Ob}}
\newcommand{\bPbar}{\bar{\Pb}}
\newcommand{\bQbar}{\bar{\Qb}}
\newcommand{\bRbar}{\bar{\Rb}}
\newcommand{\bSbar}{\bar{\Sbb}}
\newcommand{\bTbar}{\bar{\Tb}}
\newcommand{\bUbar}{\bar{\Ub}}
\newcommand{\bVbar}{\bar{\Vb}}
\newcommand{\bWbar}{\bar{\Wb}}
\newcommand{\bXbar}{\bar{\Xb}}
\newcommand{\bYbar}{\bar{\Yb}}
\newcommand{\bZbar}{\bar{\Zb}}

\newcommand{\Abhat}{\hat{\Ab}}
\newcommand{\Bbhat}{\hat{\Bb}}
\newcommand{\Cbhat}{\hat{\Cb}}
\newcommand{\Dbhat}{\hat{\Db}}
\newcommand{\Ebhat}{\hat{\Eb}}
\newcommand{\Fbhat}{\hat{\Fb}}
\newcommand{\Gbhat}{\hat{\Gb}}
\newcommand{\Hbhat}{\hat{\Hb}}
\newcommand{\Ibhat}{\hat{\Ib}}
\newcommand{\Jbhat}{\hat{\Jb}}
\newcommand{\Kbhat}{\hat{\Kb}}
\newcommand{\Lbhat}{\hat{\Lb}}
\newcommand{\Mbhat}{\hat{\Mb}}
\newcommand{\Nbhat}{\hat{\Nb}}
\newcommand{\Obhat}{\hat{\Ob}}
\newcommand{\Pbhat}{\hat{\Pb}}
\newcommand{\Qbhat}{\hat{\Qb}}
\newcommand{\Rbhat}{\hat{\Rb}}
\newcommand{\Sbhat}{\hat{\Sbb}}
\newcommand{\Tbhat}{\hat{\Tb}}
\newcommand{\Ubhat}{\hat{\Ub}}
\newcommand{\Vbhat}{\hat{\Vb}}
\newcommand{\Wbhat}{\hat{\Wb}}
\newcommand{\Xbhat}{\hat{\Xb}}
\newcommand{\Ybhat}{\hat{\Yb}}
\newcommand{\Zbhat}{\hat{\Zb}}

\newcommand{\bAhat}{\hat{\Ab}}
\newcommand{\bBhat}{\hat{\Bb}}
\newcommand{\bChat}{\hat{\Cb}}
\newcommand{\bDhat}{\hat{\Db}}
\newcommand{\bEhat}{\hat{\Eb}}
\newcommand{\bFhat}{\hat{\Fb}}
\newcommand{\bGhat}{\hat{\Gb}}
\newcommand{\bHhat}{\hat{\Hb}}
\newcommand{\bIhat}{\hat{\Ib}}
\newcommand{\bJhat}{\hat{\Jb}}
\newcommand{\bKhat}{\hat{\Kb}}
\newcommand{\bLhat}{\hat{\Lb}}
\newcommand{\bMhat}{\hat{\Mb}}
\newcommand{\bNhat}{\hat{\Nb}}
\newcommand{\bOhat}{\hat{\Ob}}
\newcommand{\bPhat}{\hat{\Pb}}
\newcommand{\bQhat}{\hat{\Qb}}
\newcommand{\bRhat}{\hat{\Rb}}
\newcommand{\bShat}{\hat{\Sbb}}
\newcommand{\bThat}{\hat{\Tb}}
\newcommand{\bUhat}{\hat{\Ub}}
\newcommand{\bVhat}{\hat{\Vb}}
\newcommand{\bWhat}{\hat{\Wb}}
\newcommand{\bXhat}{\hat{\Xb}}
\newcommand{\bYhat}{\hat{\Yb}}
\newcommand{\bZhat}{\hat{\Zb}}

\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{{\mathcal{S}}}
\newcommand{\Tcal}{{\mathcal{T}}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand{\Acaltil}{\tilde{\cA}}
\newcommand{\Bcaltil}{\tilde{\cB}}
\newcommand{\Ccaltil}{\tilde{\cC}}
\newcommand{\Dcaltil}{\tilde{\cD}}
\newcommand{\Ecaltil}{\tilde{\cE}}
\newcommand{\Fcaltil}{\tilde{\cF}}
\newcommand{\Gcaltil}{\tilde{\cG}}
\newcommand{\Hcaltil}{\tilde{\cH}}
\newcommand{\Icaltil}{\tilde{\cI}}
\newcommand{\Jcaltil}{\tilde{\cJ}}
\newcommand{\Kcaltil}{\tilde{\cK}}
\newcommand{\Lcaltil}{\tilde{\cL}}
\newcommand{\Mcaltil}{\tilde{\cM}}
\newcommand{\Ncaltil}{\tilde{\cN}}
\newcommand{\Ocaltil}{\tilde{\cO}}
\newcommand{\Pcaltil}{\tilde{\cP}}
\newcommand{\Qcaltil}{\tilde{\cQ}}
\newcommand{\Rcaltil}{\tilde{\cR}}
\newcommand{\Scaltil}{\tilde{\cS}}
\newcommand{\Tcaltil}{\tilde{\cT}}
\newcommand{\Ucaltil}{\tilde{\cU}}
\newcommand{\Vcaltil}{\tilde{\cV}}
\newcommand{\Wcaltil}{\tilde{\cW}}
\newcommand{\Xcaltil}{\tilde{\cX}}
\newcommand{\Ycaltil}{\tilde{\cY}}
\newcommand{\Zcaltil}{\tilde{\cZ}}

\newcommand{\cAtil}{\tilde{\cA}}
\newcommand{\cBtil}{\tilde{\cB}}
\newcommand{\cCtil}{\tilde{\cC}}
\newcommand{\cDtil}{\tilde{\cD}}
\newcommand{\cEtil}{\tilde{\cE}}
\newcommand{\cFtil}{\tilde{\cF}}
\newcommand{\cGtil}{\tilde{\cG}}
\newcommand{\cHtil}{\tilde{\cH}}
\newcommand{\cItil}{\tilde{\cI}}
\newcommand{\cJtil}{\tilde{\cJ}}
\newcommand{\cKtil}{\tilde{\cK}}
\newcommand{\cLtil}{\tilde{\cL}}
\newcommand{\cMtil}{\tilde{\cM}}
\newcommand{\cNtil}{\tilde{\cN}}
\newcommand{\cOtil}{\tilde{\cO}}
\newcommand{\cPtil}{\tilde{\cP}}
\newcommand{\cQtil}{\tilde{\cQ}}
\newcommand{\cRtil}{\tilde{\cR}}
\newcommand{\cStil}{\tilde{\cS}}
\newcommand{\cTtil}{\tilde{\cT}}
\newcommand{\cUtil}{\tilde{\cU}}
\newcommand{\cVtil}{\tilde{\cV}}
\newcommand{\cWtil}{\tilde{\cW}}
\newcommand{\cXtil}{\tilde{\cX}}
\newcommand{\cYtil}{\tilde{\cY}}
\newcommand{\cZtil}{\tilde{\cZ}}

\newcommand{\cAt}{\tilde{\cA}}
\newcommand{\cBt}{\tilde{\cB}}
\newcommand{\cCt}{\tilde{\cC}}
\newcommand{\cDt}{\tilde{\cD}}
\newcommand{\cEt}{\tilde{\cE}}
\newcommand{\cFt}{\tilde{\cF}}
\newcommand{\cGt}{\tilde{\cG}}
\newcommand{\cHt}{\tilde{\cH}}
\newcommand{\cIt}{\tilde{\cI}}
\newcommand{\cJt}{\tilde{\cJ}}
\newcommand{\cKt}{\tilde{\cK}}
\newcommand{\cLt}{\tilde{\cL}}
\newcommand{\cMt}{\tilde{\cM}}
\newcommand{\cNt}{\tilde{\cN}}
\newcommand{\cOt}{\tilde{\cO}}
\newcommand{\cPt}{\tilde{\cP}}
\newcommand{\cQt}{\tilde{\cQ}}
\newcommand{\cRt}{\tilde{\cR}}
\newcommand{\cSt}{\tilde{\cS}}
\newcommand{\cTt}{\tilde{\cT}}
\newcommand{\cUt}{\tilde{\cU}}
\newcommand{\cVt}{\tilde{\cV}}
\newcommand{\cWt}{\tilde{\cW}}
\newcommand{\cXt}{\tilde{\cX}}
\newcommand{\cYt}{\tilde{\cY}}
\newcommand{\cZt}{\tilde{\cZ}}

\newcommand{\cAtilde}{\widetilde{\cA}}
\newcommand{\cBtilde}{\widetilde{\cB}}
\newcommand{\cCtilde}{\widetilde{\cC}}
\newcommand{\cDtilde}{\widetilde{\cD}}
\newcommand{\cEtilde}{\widetilde{\cE}}
\newcommand{\cFtilde}{\widetilde{\cF}}
\newcommand{\cGtilde}{\widetilde{\cG}}
\newcommand{\cHtilde}{\widetilde{\cH}}
\newcommand{\cItilde}{\widetilde{\cI}}
\newcommand{\cJtilde}{\widetilde{\cJ}}
\newcommand{\cKtilde}{\widetilde{\cK}}
\newcommand{\cLtilde}{\widetilde{\cL}}
\newcommand{\cMtilde}{\widetilde{\cM}}
\newcommand{\cNtilde}{\widetilde{\cN}}
\newcommand{\cOtilde}{\widetilde{\cO}}
\newcommand{\cPtilde}{\widetilde{\cP}}
\newcommand{\cQtilde}{\widetilde{\cQ}}
\newcommand{\cRtilde}{\widetilde{\cR}}
\newcommand{\cStilde}{\widetilde{\cS}}
\newcommand{\cTtilde}{\widetilde{\cT}}
\newcommand{\cUtilde}{\widetilde{\cU}}
\newcommand{\cVtilde}{\widetilde{\cV}}
\newcommand{\cWtilde}{\widetilde{\cW}}
\newcommand{\cXtilde}{\widetilde{\cX}}
\newcommand{\cYtilde}{\widetilde{\cY}}
\newcommand{\cZtilde}{\widetilde{\cZ}}

\newcommand{\Acalhat}{\hat{\cA}}
\newcommand{\Bcalhat}{\hat{\cB}}
\newcommand{\Ccalhat}{\hat{\cC}}
\newcommand{\Dcalhat}{\hat{\cD}}
\newcommand{\Ecalhat}{\hat{\cE}}
\newcommand{\Fcalhat}{\hat{\cF}}
\newcommand{\Gcalhat}{\hat{\cG}}
\newcommand{\Hcalhat}{\hat{\cH}}
\newcommand{\Icalhat}{\hat{\cI}}
\newcommand{\Jcalhat}{\hat{\cJ}}
\newcommand{\Kcalhat}{\hat{\cK}}
\newcommand{\Lcalhat}{\hat{\cL}}
\newcommand{\Mcalhat}{\hat{\cM}}
\newcommand{\Ncalhat}{\hat{\cN}}
\newcommand{\Ocalhat}{\hat{\cO}}
\newcommand{\Pcalhat}{\hat{\cP}}
\newcommand{\Qcalhat}{\hat{\cQ}}
\newcommand{\Rcalhat}{\hat{\cR}}
\newcommand{\Scalhat}{\hat{\cS}}
\newcommand{\Tcalhat}{\hat{\cT}}
\newcommand{\Ucalhat}{\hat{\cU}}
\newcommand{\Vcalhat}{\hat{\cV}}
\newcommand{\Wcalhat}{\hat{\cW}}
\newcommand{\Xcalhat}{\hat{\cX}}
\newcommand{\Ycalhat}{\hat{\cY}}
\newcommand{\Zcalhat}{\hat{\cZ}}

\newcommand{\cAhat}{\hat{\cA}}
\newcommand{\cBhat}{\hat{\cB}}
\newcommand{\cChat}{\hat{\cC}}
\newcommand{\cDhat}{\hat{\cD}}
\newcommand{\cEhat}{\hat{\cE}}
\newcommand{\cFhat}{\hat{\cF}}
\newcommand{\cGhat}{\hat{\cG}}
\newcommand{\cHhat}{\hat{\cH}}
\newcommand{\cIhat}{\hat{\cI}}
\newcommand{\cJhat}{\hat{\cJ}}
\newcommand{\cKhat}{\hat{\cK}}
\newcommand{\cLhat}{\hat{\cL}}
\newcommand{\cMhat}{\hat{\cM}}
\newcommand{\cNhat}{\hat{\cN}}
\newcommand{\cOhat}{\hat{\cO}}
\newcommand{\cPhat}{\hat{\cP}}
\newcommand{\cQhat}{\hat{\cQ}}
\newcommand{\cRhat}{\hat{\cR}}
\newcommand{\cShat}{\hat{\cS}}
\newcommand{\cThat}{\hat{\cT}}
\newcommand{\cUhat}{\hat{\cU}}
\newcommand{\cVhat}{\hat{\cV}}
\newcommand{\cWhat}{\hat{\cW}}
\newcommand{\cXhat}{\hat{\cX}}
\newcommand{\cYhat}{\hat{\cY}}
\newcommand{\cZhat}{\hat{\cZ}}

\newcommand{\Acalbar}{\bar{\cA}}
\newcommand{\Bcalbar}{\bar{\cB}}
\newcommand{\Ccalbar}{\bar{\cC}}
\newcommand{\Dcalbar}{\bar{\cD}}
\newcommand{\Ecalbar}{\bar{\cE}}
\newcommand{\Fcalbar}{\bar{\cF}}
\newcommand{\Gcalbar}{\bar{\cG}}
\newcommand{\Hcalbar}{\bar{\cH}}
\newcommand{\Icalbar}{\bar{\cI}}
\newcommand{\Jcalbar}{\bar{\cJ}}
\newcommand{\Kcalbar}{\bar{\cK}}
\newcommand{\Lcalbar}{\bar{\cL}}
\newcommand{\Mcalbar}{\bar{\cM}}
\newcommand{\Ncalbar}{\bar{\cN}}
\newcommand{\Ocalbar}{\bar{\cO}}
\newcommand{\Pcalbar}{\bar{\cP}}
\newcommand{\Qcalbar}{\bar{\cQ}}
\newcommand{\Rcalbar}{\bar{\cR}}
\newcommand{\Scalbar}{\bar{\cS}}
\newcommand{\Tcalbar}{\bar{\cT}}
\newcommand{\Ucalbar}{\bar{\cU}}
\newcommand{\Vcalbar}{\bar{\cV}}
\newcommand{\Wcalbar}{\bar{\cW}}
\newcommand{\Xcalbar}{\bar{\cX}}
\newcommand{\Ycalbar}{\bar{\cY}}
\newcommand{\Zcalbar}{\bar{\cZ}}

\newcommand{\cAbar}{\bar{\cA}}
\newcommand{\cBbar}{\bar{\cB}}
\newcommand{\cCbar}{\bar{\cC}}
\newcommand{\cDbar}{\bar{\cD}}
\newcommand{\cEbar}{\bar{\cE}}
\newcommand{\cFbar}{\bar{\cF}}
\newcommand{\cGbar}{\bar{\cG}}
\newcommand{\cHbar}{\bar{\cH}}
\newcommand{\cIbar}{\bar{\cI}}
\newcommand{\cJbar}{\bar{\cJ}}
\newcommand{\cKbar}{\bar{\cK}}
\newcommand{\cLbar}{\bar{\cL}}
\newcommand{\cMbar}{\bar{\cM}}
\newcommand{\cNbar}{\bar{\cN}}
\newcommand{\cObar}{\bar{\cO}}
\newcommand{\cPbar}{\bar{\cP}}
\newcommand{\cQbar}{\bar{\cQ}}
\newcommand{\cRbar}{\bar{\cR}}
\newcommand{\cSbar}{\bar{\cS}}
\newcommand{\cTbar}{\bar{\cT}}
\newcommand{\cUbar}{\bar{\cU}}
\newcommand{\cVbar}{\bar{\cV}}
\newcommand{\cWbar}{\bar{\cW}}
\newcommand{\cXbar}{\bar{\cX}}
\newcommand{\cYbar}{\bar{\cY}}
\newcommand{\cZbar}{\bar{\cZ}}

\newcommand{\sA}{\mathscr{A}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}}
\newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}}
\newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}}
\newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}}
\newcommand{\sZ}{\mathscr{Z}}

\newcommand{\sAbar}{\bar{\sA}}
\newcommand{\sBbar}{\bar{\sB}}
\newcommand{\sCbar}{\bar{\sC}}
\newcommand{\sDbar}{\bar{\sD}}
\newcommand{\sEbar}{\bar{\sE}}
\newcommand{\sFbar}{\bar{\sF}}
\newcommand{\sGbar}{\bar{\sG}}
\newcommand{\sHbar}{\bar{\sH}}
\newcommand{\sIbar}{\bar{\sI}}
\newcommand{\sJbar}{\bar{\sJ}}
\newcommand{\sKbar}{\bar{\sK}}
\newcommand{\sLbar}{\bar{\sL}}
\newcommand{\sMbar}{\bar{\sM}}
\newcommand{\sNbar}{\bar{\sN}}
\newcommand{\sObar}{\bar{\sO}}
\newcommand{\sPbar}{\bar{\sP}}
\newcommand{\sQbar}{\bar{\sQ}}
\newcommand{\sRbar}{\bar{\sR}}
\newcommand{\sSbar}{\bar{\sS}}
\newcommand{\sTbar}{\bar{\sT}}
\newcommand{\sUbar}{\bar{\sU}}
\newcommand{\sVbar}{\bar{\sV}}
\newcommand{\sWbar}{\bar{\sW}}
\newcommand{\sXbar}{\bar{\sX}}
\newcommand{\sYbar}{\bar{\sY}}
\newcommand{\sZbar}{\bar{\sZ}}

\newcommand{\sAhat}{\hat{\sA}}
\newcommand{\sBhat}{\hat{\sB}}
\newcommand{\sChat}{\hat{\sC}}
\newcommand{\sDhat}{\hat{\sD}}
\newcommand{\sEhat}{\hat{\sE}}
\newcommand{\sFhat}{\hat{\sF}}
\newcommand{\sGhat}{\hat{\sG}}
\newcommand{\sHhat}{\hat{\sH}}
\newcommand{\sIhat}{\hat{\sI}}
\newcommand{\sJhat}{\hat{\sJ}}
\newcommand{\sKhat}{\hat{\sK}}
\newcommand{\sLhat}{\hat{\sL}}
\newcommand{\sMhat}{\hat{\sM}}
\newcommand{\sNhat}{\hat{\sN}}
\newcommand{\sOhat}{\hat{\sO}}
\newcommand{\sPhat}{\hat{\sP}}
\newcommand{\sQhat}{\hat{\sQ}}
\newcommand{\sRhat}{\hat{\sR}}
\newcommand{\sShat}{\hat{\sS}}
\newcommand{\sThat}{\hat{\sT}}
\newcommand{\sUhat}{\hat{\sU}}
\newcommand{\sVhat}{\hat{\sV}}
\newcommand{\sWhat}{\hat{\sW}}
\newcommand{\sXhat}{\hat{\sX}}
\newcommand{\sYhat}{\hat{\sY}}
\newcommand{\sZhat}{\hat{\sZ}}

\newcommand{\sAtilde}{\widetilde{\sA}}
\newcommand{\sBtilde}{\widetilde{\sB}}
\newcommand{\sCtilde}{\widetilde{\sC}}
\newcommand{\sDtilde}{\widetilde{\sD}}
\newcommand{\sEtilde}{\widetilde{\sE}}
\newcommand{\sFtilde}{\widetilde{\sF}}
\newcommand{\sGtilde}{\widetilde{\sG}}
\newcommand{\sHtilde}{\widetilde{\sH}}
\newcommand{\sItilde}{\widetilde{\sI}}
\newcommand{\sJtilde}{\widetilde{\sJ}}
\newcommand{\sKtilde}{\widetilde{\sK}}
\newcommand{\sLtilde}{\widetilde{\sL}}
\newcommand{\sMtilde}{\widetilde{\sM}}
\newcommand{\sNtilde}{\widetilde{\sN}}
\newcommand{\sOtilde}{\widetilde{\sO}}
\newcommand{\sPtilde}{\widetilde{\sP}}
\newcommand{\sQtilde}{\widetilde{\sQ}}
\newcommand{\sRtilde}{\widetilde{\sR}}
\newcommand{\sStilde}{\widetilde{\sS}}
\newcommand{\sTtilde}{\widetilde{\sT}}
\newcommand{\sUtilde}{\widetilde{\sU}}
\newcommand{\sVtilde}{\widetilde{\sV}}
\newcommand{\sWtilde}{\widetilde{\sW}}
\newcommand{\sXtilde}{\widetilde{\sX}}
\newcommand{\sYtilde}{\widetilde{\sY}}
\newcommand{\sZtilde}{\widetilde{\sZ}}

\newcommand{\sAt}{\tilde{\sA}}
\newcommand{\sBt}{\tilde{\sB}}
\newcommand{\sCt}{\tilde{\sC}}
\newcommand{\sDt}{\tilde{\sD}}
\newcommand{\sEt}{\tilde{\sE}}
\newcommand{\sFt}{\tilde{\sF}}
\newcommand{\sGt}{\tilde{\sG}}
\newcommand{\sHt}{\tilde{\sH}}
\newcommand{\sIt}{\tilde{\sI}}
\newcommand{\sJt}{\tilde{\sJ}}
\newcommand{\sKt}{\tilde{\sK}}
\newcommand{\sLt}{\tilde{\sL}}
\newcommand{\sMt}{\tilde{\sM}}
\newcommand{\sNt}{\tilde{\sN}}
\newcommand{\sOt}{\tilde{\sO}}
\newcommand{\sPt}{\tilde{\sP}}
\newcommand{\sQt}{\tilde{\sQ}}
\newcommand{\sRt}{\tilde{\sR}}
\newcommand{\sSt}{\tilde{\sS}}
\newcommand{\sTt}{\tilde{\sT}}
\newcommand{\sUt}{\tilde{\sU}}
\newcommand{\sVt}{\tilde{\sV}}
\newcommand{\sWt}{\tilde{\sW}}
\newcommand{\sXt}{\tilde{\sX}}
\newcommand{\sYt}{\tilde{\sY}}
\newcommand{\sZt}{\tilde{\sZ}}

\newcommand{\dA}{\mathds{A}}
\newcommand{\dB}{\mathds{B}}
\newcommand{\dC}{\mathds{C}}
\newcommand{\dD}{\mathds{D}}
\newcommand{\dE}{\mathds{E}}
\newcommand{\dF}{\mathds{F}}
\newcommand{\dG}{\mathds{G}}
\newcommand{\dH}{\mathds{H}}
\newcommand{\dI}{\mathds{I}}
\newcommand{\dJ}{\mathds{J}}
\newcommand{\dK}{\mathds{K}}
\newcommand{\dL}{\mathds{L}}
\newcommand{\dM}{\mathds{M}}
\newcommand{\dN}{\mathds{N}}
\newcommand{\dO}{\mathds{O}}
\newcommand{\dP}{\mathds{P}}
\newcommand{\dQ}{\mathds{Q}}
\newcommand{\dR}{\mathds{R}}
\newcommand{\dS}{\mathds{S}}
\newcommand{\dT}{\mathds{T}}
\newcommand{\dU}{\mathds{U}}
\newcommand{\dV}{\mathds{V}}
\newcommand{\dW}{\mathds{W}}
\newcommand{\dX}{\mathds{X}}
\newcommand{\dY}{\mathds{Y}}
\newcommand{\dZ}{\mathds{Z}}

\newcommand{\dAbar}{\bar{\dA}}
\newcommand{\dBbar}{\bar{\dB}}
\newcommand{\dCbar}{\bar{\dC}}
\newcommand{\dDbar}{\bar{\dD}}
\newcommand{\dEbar}{\bar{\dE}}
\newcommand{\dFbar}{\bar{\dF}}
\newcommand{\dGbar}{\bar{\dG}}
\newcommand{\dHbar}{\bar{\dH}}
\newcommand{\dIbar}{\bar{\dI}}
\newcommand{\dJbar}{\bar{\dJ}}
\newcommand{\dKbar}{\bar{\dK}}
\newcommand{\dLbar}{\bar{\dL}}
\newcommand{\dMbar}{\bar{\dM}}
\newcommand{\dNbar}{\bar{\dN}}
\newcommand{\dObar}{\bar{\dO}}
\newcommand{\dPbar}{\bar{\dP}}
\newcommand{\dQbar}{\bar{\dQ}}
\newcommand{\dRbar}{\bar{\dR}}
\newcommand{\dSbar}{\bar{\dS}}
\newcommand{\dTbar}{\bar{\dT}}
\newcommand{\dUbar}{\bar{\dU}}
\newcommand{\dVbar}{\bar{\dV}}
\newcommand{\dWbar}{\bar{\dW}}
\newcommand{\dXbar}{\bar{\dX}}
\newcommand{\dYbar}{\bar{\dY}}
\newcommand{\dZbar}{\bar{\dZ}}

\newcommand{\dAhat}{\hat{\dA}}
\newcommand{\dBhat}{\hat{\dB}}
\newcommand{\dChat}{\hat{\dC}}
\newcommand{\dDhat}{\hat{\dD}}
\newcommand{\dEhat}{\hat{\dE}}
\newcommand{\dFhat}{\hat{\dF}}
\newcommand{\dGhat}{\hat{\dG}}
\newcommand{\dHhat}{\hat{\dH}}
\newcommand{\dIhat}{\hat{\dI}}
\newcommand{\dJhat}{\hat{\dJ}}
\newcommand{\dKhat}{\hat{\dK}}
\newcommand{\dLhat}{\hat{\dL}}
\newcommand{\dMhat}{\hat{\dM}}
\newcommand{\dNhat}{\hat{\dN}}
\newcommand{\dOhat}{\hat{\dO}}
\newcommand{\dPhat}{\hat{\dP}}
\newcommand{\dQhat}{\hat{\dQ}}
\newcommand{\dRhat}{\hat{\dR}}
\newcommand{\dShat}{\hat{\dS}}
\newcommand{\dThat}{\hat{\dT}}
\newcommand{\dUhat}{\hat{\dU}}
\newcommand{\dVhat}{\hat{\dV}}
\newcommand{\dWhat}{\hat{\dW}}
\newcommand{\dXhat}{\hat{\dX}}
\newcommand{\dYhat}{\hat{\dY}}
\newcommand{\dZhat}{\hat{\dZ}}

\newcommand{\dAtilde}{\widetilde{\dA}}
\newcommand{\dBtilde}{\widetilde{\dB}}
\newcommand{\dCtilde}{\widetilde{\dC}}
\newcommand{\dDtilde}{\widetilde{\dD}}
\newcommand{\dEtilde}{\widetilde{\dE}}
\newcommand{\dFtilde}{\widetilde{\dF}}
\newcommand{\dGtilde}{\widetilde{\dG}}
\newcommand{\dHtilde}{\widetilde{\dH}}
\newcommand{\dItilde}{\widetilde{\dI}}
\newcommand{\dJtilde}{\widetilde{\dJ}}
\newcommand{\dKtilde}{\widetilde{\dK}}
\newcommand{\dLtilde}{\widetilde{\dL}}
\newcommand{\dMtilde}{\widetilde{\dM}}
\newcommand{\dNtilde}{\widetilde{\dN}}
\newcommand{\dOtilde}{\widetilde{\dO}}
\newcommand{\dPtilde}{\widetilde{\dP}}
\newcommand{\dQtilde}{\widetilde{\dQ}}
\newcommand{\dRtilde}{\widetilde{\dR}}
\newcommand{\dStilde}{\widetilde{\dS}}
\newcommand{\dTtilde}{\widetilde{\dT}}
\newcommand{\dUtilde}{\widetilde{\dU}}
\newcommand{\dVtilde}{\widetilde{\dV}}
\newcommand{\dWtilde}{\widetilde{\dW}}
\newcommand{\dXtilde}{\widetilde{\dX}}
\newcommand{\dYtilde}{\widetilde{\dY}}
\newcommand{\dZtilde}{\widetilde{\dZ}}

\newcommand{\dAt}{\tilde{\dA}}
\newcommand{\dBt}{\tilde{\dB}}
\newcommand{\dCt}{\tilde{\dC}}
\newcommand{\dDt}{\tilde{\dD}}
\newcommand{\dEt}{\tilde{\dE}}
\newcommand{\dFt}{\tilde{\dF}}
\newcommand{\dGt}{\tilde{\dG}}
\newcommand{\dHt}{\tilde{\dH}}
\newcommand{\dIt}{\tilde{\dI}}
\newcommand{\dJt}{\tilde{\dJ}}
\newcommand{\dKt}{\tilde{\dK}}
\newcommand{\dLt}{\tilde{\dL}}
\newcommand{\dMt}{\tilde{\dM}}
\newcommand{\dNt}{\tilde{\dN}}
\newcommand{\dOt}{\tilde{\dO}}
\newcommand{\dPt}{\tilde{\dP}}
\newcommand{\dQt}{\tilde{\dQ}}
\newcommand{\dRt}{\tilde{\dR}}
\newcommand{\dSt}{\tilde{\dS}}
\newcommand{\dTt}{\tilde{\dT}}
\newcommand{\dUt}{\tilde{\dU}}
\newcommand{\dVt}{\tilde{\dV}}
\newcommand{\dWt}{\tilde{\dW}}
\newcommand{\dXt}{\tilde{\dX}}
\newcommand{\dYt}{\tilde{\dY}}
\newcommand{\dZt}{\tilde{\dZ}}

\renewcommand{\vec}[1]{\mathbf{\boldsymbol{#1}}}

\newcommand{\avec}{\vec{a}}
\newcommand{\bvec}{\vec{b}}
\newcommand{\cvec}{\vec{c}}
\newcommand{\dvec}{\vec{d}}
\newcommand{\evec}{\vec{e}}
\newcommand{\fvec}{\vec{f}}
\newcommand{\gvec}{\vec{g}}
\newcommand{\hvec}{\vec{h}}
\newcommand{\ivec}{\vec{i}}
\newcommand{\jvec}{\vec{j}}
\newcommand{\kvec}{\vec{k}}
\newcommand{\lvec}{\vec{l}}
\newcommand{\mvec}{\vec{m}}
\newcommand{\nvec}{\vec{n}}
\newcommand{\ovec}{\vec{o}}
\newcommand{\pvec}{\vec{p}}
\newcommand{\qvec}{\vec{q}}
\newcommand{\rvec}{\vec{r}}
\newcommand{\svec}{\vec{s}}
\newcommand{\tvec}{\vec{t}}
\newcommand{\uvec}{\vec{u}}
\newcommand{\vvec}{\vec{v}}
\newcommand{\wvec}{\vec{w}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\yvec}{\vec{y}}
\newcommand{\zvec}{\vec{z}}

\newcommand{\va}{\vec{a}}
\newcommand{\vecb}{\vec{b}}
\newcommand{\vc}{\vec{c}}
\newcommand{\vd}{\vec{d}}
\newcommand{\ve}{\vec{e}}
\newcommand{\vf}{\vec{f}}
\newcommand{\vg}{\vec{g}}
\newcommand{\vh}{\vec{h}}
\newcommand{\vi}{\vec{i}}
\newcommand{\vj}{\vec{j}}
\newcommand{\vk}{\vec{k}}
\newcommand{\vl}{\vec{l}}
\newcommand{\vm}{\vec{m}}
\newcommand{\vn}{\vec{n}}
\newcommand{\vo}{\vec{o}}
\newcommand{\vp}{\vec{p}}
\newcommand{\vq}{\vec{q}}
\newcommand{\vr}{\vec{r}}
\newcommand{\vs}{\vec{s}}
\newcommand{\vt}{\vec{t}}
\newcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{z}}

\newcommand{\Avec}{\vec{A}}
\newcommand{\Bvec}{\vec{B}}
\newcommand{\Cvec}{\vec{C}}
\newcommand{\Dvec}{\vec{D}}
\newcommand{\Evec}{\vec{E}}
\newcommand{\Fvec}{\vec{F}}
\newcommand{\Gvec}{\vec{G}}
\newcommand{\Hvec}{\vec{H}}
\newcommand{\Ivec}{\vec{I}}
\newcommand{\Jvec}{\vec{J}}
\newcommand{\Kvec}{\vec{K}}
\newcommand{\Lvec}{\vec{L}}
\newcommand{\Mvec}{\vec{M}}
\newcommand{\Nvec}{\vec{N}}
\newcommand{\Ovec}{\vec{O}}
\newcommand{\Pvec}{\vec{P}}
\newcommand{\Qvec}{\vec{Q}}
\newcommand{\Rvec}{\vec{R}}
\newcommand{\Svec}{\vec{S}}
\newcommand{\Tvec}{\vec{T}}
\newcommand{\Uvec}{\vec{U}}
\newcommand{\Vvec}{\vec{V}}
\newcommand{\Wvec}{\vec{W}}
\newcommand{\Xvec}{\vec{X}}
\newcommand{\Yvec}{\vec{Y}}
\newcommand{\Zvec}{\vec{Z}}

\newcommand{\Amat}{\Ab}
\newcommand{\Bmat}{\Bb}
\newcommand{\Cmat}{\Cb}
\newcommand{\Dmat}{\Db}
\newcommand{\Emat}{\Eb}
\newcommand{\Fmat}{\Fb}
\newcommand{\Gmat}{\Gb}
\newcommand{\Hmat}{\Hb}
\newcommand{\Imat}{\Ib}

\newcommand{\Vmat}{\Vb}
\newcommand{\Wmat}{\Wb}
\newcommand{\Xmat}{\Xb}
\newcommand{\Ymat}{\Yb}
\newcommand{\Zmat}{\Zb}
\newcommand{\VV}{\mathrm{Var}}


\newcommand{\yvecbar}{\bar{\vec{y}}}
\newcommand{\wvecbar}{\bar{\vec{w}}}
\newcommand{\xvecbar}{\bar{\vec{x}}}
\newcommand{\yvectil}{\tilde{\vec{y}}}
\newcommand{\yvechat}{\hat{\vec{y}}}






\ifx\BlackBox\undefined
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  \fi

\ifx\QED\undefined
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}
\fi

\ifx\proof\undefined
\newenvironment{proof}{\par\noindent{\bf Proof\ }}{\hfill\BlackBox\\[2mm]}
\fi

\ifx\Proof\undefined
\newcommand{\Proof}{\noindent{\bf Proof.}~}
\fi

\ifx\theorem\undefined
\newtheorem{theorem}{Theorem}
\fi

\ifx\example\undefined
\newtheorem{example}{Example}
\fi

\ifx\property\undefined
\newtheorem{property}{Property}
\fi

\ifx\lemma\undefined
\newtheorem{lemma}{Lemma}
\fi

\ifx\proposition\undefined
\newtheorem{proposition}{Proposition}
\fi

\ifx\remark\undefined
\newtheorem{remark}{Remark}
\fi

\ifx\corollary\undefined
\newtheorem{corollary}{Corollary}
\fi

\ifx\definition\undefined
\newtheorem{definition}{Definition}
\fi

\ifx\conjecture\undefined
\newtheorem{conjecture}{Conjecture}
\fi

\ifx\axiom\undefined
\newtheorem{axiom}[theorem]{Axiom}
\fi

\ifx\claim\undefined
\newtheorem{claim}{Claim}
\fi

\ifx\assumption\undefined
\newtheorem{assumption}{Assumption}
\fi






\newcommand{\done}{\mathds{1}} \newcommand{\dAA}{\mathds{A}} 
\newcommand{\BB}{\mathds{B}} 
\newcommand{\CC}{\mathds{C}} \newcommand{\DD}{\mathds{D}} 
\newcommand{\EE}{\mathds{E}} \newcommand{\FF}{\mathds{F}} \newcommand{\GG}{\mathds{G}} 
\newcommand{\HH}{\mathds{H}} \newcommand{\II}{\mathds{I}} \newcommand{\JJ}{\mathds{J}} 
\newcommand{\KK}{\mathds{K}} \newcommand{\LL}{\mathds{L}} 
\newcommand{\MM}{\mathds{M}} \newcommand{\NN}{\mathbb{N}} \newcommand{\OO}{\mathds{O}} 
\newcommand{\PP}{\mathds{P}} \newcommand{\QQ}{\mathds{Q}} \newcommand{\RR}{\mathbb{R}} \newcommand{\dSS}{\mathbb{S}} 
\newcommand{\RRbar}{\overline{\RR}} \newcommand{\TT}{\mathds{T}} 
\newcommand{\UU}{\mathds{U}} 
\newcommand{\WW}{\mathds{W}} 
\newcommand{\XX}{\mathds{X}}
\newcommand{\YY}{\mathds{Y}}
\newcommand{\ZZ}{\mathds{Z}} 



\newcommand*{\mini}{\operatorname*{minimize}}
\newcommand*{\maxi}{\operatorname*{maximize}}
\newcommand*{\argmin}{\operatorname*{argmin}}
\newcommand*{\argmax}{\operatorname*{argmax}}
\newcommand*{\arginf}{\operatorname*{arginf}}
\newcommand*{\argsup}{\operatorname*{argsup}}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

\newcommand{\co}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{co}\else\operatorname{co}\left(#1\right)\fi}
\newcommand{\core}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{core}\else\operatorname{core}\left(#1\right)\fi}
\newcommand{\diag}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{diag}\else\operatorname{diag}\left(#1\right)\fi}
\newcommand{\dom}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{dom}\else\operatorname{dom}\left(#1\right)\fi}
\newcommand{\expect}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{\mathds{E}}\else\operatorname{\mathds{E}}\ssbr{#1}\fi}
\newcommand{\lap}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{\bf Lap}\else\operatorname{\bf Lap}\left(#1\right)\fi}
\newcommand{\prob}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{\mathds{P}}\else\operatorname{\mathds{P}}\ssbr{#1}\fi}
\newcommand{\proj}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{\mathcal{P}}\else\operatorname{\mathcal{P}}\left(#1\right)\fi}
\newcommand{\prox}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{prox}\else\operatorname{prox}\left(#1\right)\fi}
\newcommand{\rank}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{rank}\else\operatorname{rank}\left(#1\right)\fi}
\newcommand{\sgn}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{sign}\else\operatorname{sign}\left(#1\right)\fi}
\newcommand{\sign}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{sign}\else\operatorname{sign}\left(#1\right)\fi}
\newcommand{\tr}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{tr}\else\operatorname{tr}\left(#1\right)\fi}
\newcommand{\trace}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{tr}\else\operatorname{tr}\left(#1\right)\fi}
\newcommand{\traj}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{Traj}\else\operatorname{Traj}\left(#1\right)\fi}
\newcommand{\var}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{\mathds{V}}\else\operatorname{\mathds{V}}\left[#1\right]\fi}
\newcommand{\vect}[1][]{\def\tst{#1}\ifx\tst\empty\operatorname{vec}\else\operatorname{vec}\left(#1\right)\fi}

\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\const}{\mathrm{constant}}
\newcommand{\ri}{\operatorname{ri}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\intr}{\operatorname{int}}
\newcommand{\emp}{\operatorname{emp}}
\newcommand{\nnz}{\operatorname{nnz}}
\newcommand{\eproof}{$\null\hfill\blacksquare$}
\renewcommand{\O}{{\cO}}
\newcommand{\softO}[1]{{\widetilde{\cO}}\rbr{#1}}

\newcommand{\Co}[1]{\co[#1]}
\newcommand{\Core}[1]{\core[#1]}
\newcommand{\Diag}[1]{\diag[#1]}
\newcommand{\Dom}[1]{\dom[#1]}
\newcommand{\Expect}[2][]{\underset{#1}{\EE}\ssbr{{#2}}}
\newcommand{\Ind}[1]{\ind[#1]}
\newcommand{\Lap}[1]{\lap[#1]}
\newcommand{\Prob}[1]{\prob[#1]}
\newcommand{\Proj}[2][]{\mathcal{P}_{#1}\sbr{#2}}
\newcommand{\Prox}[2][]{\operatorname{prox}_{#1}\rbr{#2}}
\newcommand{\Rank}[1]{\rank[#1]}
\newcommand{\Sgn}[1]{\sgn[#1]}
\newcommand{\Sign}[1]{\sign[#1]}
\newcommand{\Tr}[1]{\trace[#1]}
\newcommand{\Trace}[1]{\trace[#1]}
\newcommand{\Traj}[1]{\traj[#1]}
\newcommand{\Vect}[1]{\vect[#1]}



\newcommand{\eq}[1]{(\ref{#1})}
\newcommand{\mymatrix}[2]{\left[\begin{array}{#1} #2 \end{array}\right]}


\newcommand{\mychoose}[2]{\left(\begin{array}{c} #1 \\ #2 \end{array}\right)}
\newcommand{\mydet}[1]{\det\left[ #1 \right]}
\newcommand{\myspan}[1]{\mathrm{span}\cbr{#1}}
\newcommand{\smallfrac}[2]{{\textstyle \frac{#1}{#2}}}
\newcommand{\pwrt}[1]{\frac{\partial}{\partial #1}}
\newcommand{\ppwrt}[1]{\frac{\partial^2}{(\partial #1)^2}}
\newcommand{\aleq}{\preccurlyeq}
\newcommand{\ageq}{\succcurlyeq}



\newcommand{\ea}{\emph{et al.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\iid}{\emph{iid}}
\newcommand{\cf}{\emph{cf.}\ }
\newcommand{\wrt}{\emph{w.r.t.}\ }



\newcommand{\rbr}[1]{\left({#1}\right)}
\newcommand{\sbr}[1]{\left[{#1}\right]}
\newcommand{\cbr}[1]{\left\{{#1}\right\}}
\newcommand{\nbr}[1]{\left\|{#1}\right\|}
\newcommand{\abr}[1]{\left\langle{#1}\right\rangle}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand{\floor}[1]{\left\lfloor {#1} \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil {#1} \right\rceil}
\newcommand{\inner}[2]{\left\langle {#1},{#2} \right\rangle}
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\newcommand{\frob}[1]{\norm{#1}_\text{F}}
\newcommand{\fro}[1]{\norm{#1}_\text{F}}
\newcommand{\onenorm}[1]{\norm{#1}_1}
\newcommand{\twonorm}[1]{\norm{#1}_2}
\newcommand{\infnorm}[1]{\norm{#1}_{\infty}}
\newcommand{\trnorm}[1]{\norm{#1}_{\text{tr}}}
\newcommand{\ccc}[1]{\left|\!\left|\!\left|{#1}\right|\!\right|\!\right|}
\newcommand{\bsd}[1]{\left\llbracket{#1}\right\rrbracket}
\newcommand{\ssbr}[1]{\left\llbracket{#1}\right\rrbracket}
\newcommand{\sembrack}[1]{\left\llbracket{#1}\right\rrbracket}



\newcommand{\one}{{\bm{1}}}  \newcommand{\zero}{{\bm{0}}} \newcommand{\bone}{{\bm{1}}}  \newcommand{\bzero}{{\bm{0}}} \def\b0{\bm{0}} \newcommand{\half}{\frac{1}{2}}
\newcommand{\sqrttwo}{\sqrt{2}}
\newcommand{\invsqrttwo}{\frac{1}{\sqrt{2}}}
\newcommand{\rmd}{\ensuremath{\mathrm{d}}}
\newcommand*{\diff}[1][]{\mathop{}\!{\mathrm{d}^{#1}}}



\newcommand{\sigmab}{\bm{\sigma}}
\newcommand{\Sigmab}{\mathbf{\Sigma}}

\newcommand{\val}{\vec{\alpha}}
\newcommand{\valpha}{\vec{\alpha}}
\newcommand{\AL}{\vec{\alpha}}
\newcommand{\vbeta}{\vec{\beta}}
\newcommand{\vga}{\vec{\gamma}}
\newcommand{\vgamma}{\vec{\gamma}}
\newcommand{\vde}{\vec{\delta}}
\newcommand{\vdelta}{\vec{\delta}}
\newcommand{\veps}{\vec{\epsilon}}
\newcommand{\vepsilon}{\vec{\epsilon}}
\newcommand{\vze}{\vec{\zeta}}
\newcommand{\vzeta}{\vec{\zeta}}
\newcommand{\veta}{\vec{\eta}}
\newcommand{\vth}{\vec{\theta}}
\newcommand{\vtheta}{\vec{\theta}}
\newcommand{\viota}{\vec{\iota}}
\newcommand{\vkappa}{\vec{\kappa}}
\newcommand{\vlambda}{\vec{\lambda}}
\newcommand{\vmu}{\vec{\mu}}
\newcommand{\vnu}{\vec{\nu}}
\newcommand{\vxi}{\vec{\xi}}
\newcommand{\vpi}{\vec{\pi}}
\newcommand{\vrho}{\vec{\rho}}
\newcommand{\vsig}{\vec{\sigma}}
\newcommand{\vsigma}{\vec{\sigma}}
\newcommand{\vtau}{\vec{\tau}}
\newcommand{\vupsilon}{\vec{\upsilon}}
\newcommand{\vphi}{\vec{\phi}}
\newcommand{\vchi}{\vec{\chi}}
\newcommand{\vpsi}{\vec{\psi}}
\newcommand{\vomega}{\vec{\omega}}

\newcommand{\bal}{\vec{\alpha}}
\newcommand{\balpha}{\vec{\alpha}}
\newcommand{\bbeta}{\vec{\beta}}
\newcommand{\bga}{\vec{\gamma}}
\newcommand{\bgamma}{\vec{\gamma}}
\newcommand{\bde}{\vec{\delta}}
\newcommand{\bdelta}{\vec{\delta}}
\newcommand{\beps}{\vec{\epsilon}}
\newcommand{\bepsilon}{\vec{\epsilon}}
\newcommand{\bze}{\vec{\zeta}}
\newcommand{\bzeta}{\vec{\zeta}}
\newcommand{\boldeta}{\vec{\eta}}
\newcommand{\bth}{\vec{\theta}}
\newcommand{\btheta}{\vec{\theta}}
\newcommand{\biota}{\vec{\iota}}
\newcommand{\bkappa}{\vec{\kappa}}
\newcommand{\blambda}{\vec{\lambda}}
\newcommand{\bmu}{\vec{\mu}}
\newcommand{\bnu}{\vec{\nu}}
\newcommand{\bxi}{\vec{\xi}}
\newcommand{\bpi}{\vec{\pi}}
\newcommand{\brho}{\vec{\rho}}
\newcommand{\bsig}{\vec{\sigma}}
\newcommand{\bsigma}{\vec{\sigma}}
\newcommand{\btau}{\vec{\tau}}
\newcommand{\bupsilon}{\vec{\upsilon}}
\newcommand{\bphi}{\vec{\phi}}
\newcommand{\bchi}{\vec{\chi}}
\newcommand{\bpsi}{\vec{\psi}}
\newcommand{\bomega}{\vec{\omega}}

\newcommand{\ALbar}{\bar{\val}}
\newcommand{\balbar}{\bar{\val}}
\newcommand{\balphabar}{\bar{\val}}
\newcommand{\bbetabar}{\bar{\vbeta}}
\newcommand{\bgabar}{\bar{\vgamma}}
\newcommand{\bgammabar}{\bar{\vgamma}}
\newcommand{\bdebar}{\bar{\vdelta}}
\newcommand{\bdeltabar}{\bar{\vdelta}}
\newcommand{\bepsbar}{\bar{\vepsilon}}
\newcommand{\bepsilonbar}{\bar{\vepsilon}}
\newcommand{\bzebar}{\bar{\vzeta}}
\newcommand{\bzetabar}{\bar{\vzeta}}
\newcommand{\boldetabar}{\bar{\veta}}
\newcommand{\bthbar}{\bar{\vtheta}}
\newcommand{\bthetabar}{\bar{\vtheta}}
\newcommand{\biotabar}{\bar{\viota}}
\newcommand{\bkappabar}{\bar{\vkappa}}
\newcommand{\blambdabar}{\bar{\vlambda}}
\newcommand{\bmubar}{\bar{\vmu}}
\newcommand{\bnubar}{\bar{\vnu}}
\newcommand{\bxibar}{\bar{\vxi}}
\newcommand{\bpibar}{\bar{\vpi}}
\newcommand{\brhobar}{\bar{\vrho}}
\newcommand{\bsigbar}{\bar{\vsigma}}
\newcommand{\bsigmabar}{\bar{\vsigma}}
\newcommand{\btaubar}{\bar{\vtau}}
\newcommand{\bupsilonbar}{\bar{\vupsilon}}
\newcommand{\bphibar}{\bar{\vphi}}
\newcommand{\bchibar}{\bar{\vchi}}
\newcommand{\bpsibar}{\bar{\vpsi}}
\newcommand{\bomegabar}{\bar{\vomega}}

\newcommand{\ALhat}{\hat{\val}}
\newcommand{\balhat}{\hat{\val}}
\newcommand{\balphahat}{\hat{\val}}
\newcommand{\bbetahat}{\hat{\vbeta}}
\newcommand{\bgahat}{\hat{\vgamma}}
\newcommand{\bgammahat}{\hat{\vgamma}}
\newcommand{\bdehat}{\hat{\vdelta}}
\newcommand{\bdeltahat}{\hat{\vdelta}}
\newcommand{\bepshat}{\hat{\vepsilon}}
\newcommand{\bepsilonhat}{\hat{\vepsilon}}
\newcommand{\bzehat}{\hat{\vzeta}}
\newcommand{\bzetahat}{\hat{\vzeta}}
\newcommand{\boldetahat}{\hat{\veta}}
\newcommand{\bthhat}{\hat{\vtheta}}
\newcommand{\bthetahat}{\hat{\vtheta}}
\newcommand{\biotahat}{\hat{\viota}}
\newcommand{\bkappahat}{\hat{\vkappa}}
\newcommand{\blambdahat}{\hat{\vlambda}}
\newcommand{\bmuhat}{\hat{\vmu}}
\newcommand{\bnuhat}{\hat{\vnu}}
\newcommand{\bxihat}{\hat{\vxi}}
\newcommand{\bpihat}{\hat{\vpi}}
\newcommand{\brhohat}{\hat{\vrho}}
\newcommand{\bsighat}{\hat{\vsigma}}
\newcommand{\bsigmahat}{\hat{\vsigma}}
\newcommand{\btauhat}{\hat{\vtau}}
\newcommand{\bupsilonhat}{\hat{\vupsilon}}
\newcommand{\bphihat}{\hat{\vphi}}
\newcommand{\bchihat}{\hat{\vchi}}
\newcommand{\bpsihat}{\hat{\vpsi}}
\newcommand{\bomegahat}{\hat{\vomega}}

\newcommand{\ALtil}{\tilde{\val}}
\newcommand{\baltil}{\tilde{\val}}
\newcommand{\balphatil}{\tilde{\val}}
\newcommand{\bbetatil}{\tilde{\vbeta}}
\newcommand{\bgatil}{\tilde{\vgamma}}
\newcommand{\bgammatil}{\tilde{\vgamma}}
\newcommand{\bdetil}{\tilde{\vdelta}}
\newcommand{\bdeltatil}{\tilde{\vdelta}}
\newcommand{\bepstil}{\tilde{\vepsilon}}
\newcommand{\bepsilontil}{\tilde{\vepsilon}}
\newcommand{\bzetil}{\tilde{\vzeta}}
\newcommand{\bzetatil}{\tilde{\vzeta}}
\newcommand{\boldetatil}{\tilde{\veta}}
\newcommand{\bthtil}{\tilde{\vtheta}}
\newcommand{\bthetatil}{\tilde{\vtheta}}
\newcommand{\biotatil}{\tilde{\viota}}
\newcommand{\bkappatil}{\tilde{\vkappa}}
\newcommand{\blambdatil}{\tilde{\vlambda}}
\newcommand{\bmutil}{\tilde{\vmu}}
\newcommand{\bnutil}{\tilde{\vnu}}
\newcommand{\bxitil}{\tilde{\vxi}}
\newcommand{\bpitil}{\tilde{\vpi}}
\newcommand{\brhotil}{\tilde{\vrho}}
\newcommand{\bsigtil}{\tilde{\vsigma}}
\newcommand{\bsigmatil}{\tilde{\vsigma}}
\newcommand{\btautil}{\tilde{\vtau}}
\newcommand{\bupsilontil}{\tilde{\vupsilon}}
\newcommand{\bphitil}{\tilde{\vphi}}
\newcommand{\bchitil}{\tilde{\vchi}}
\newcommand{\bpsitil}{\tilde{\vpsi}}
\newcommand{\bomegatil}{\tilde{\vomega}}

\newcommand{\ALtilde}{\widetilde{\val}}
\newcommand{\baltilde}{\widetilde{\val}}
\newcommand{\balphatilde}{\widetilde{\val}}
\newcommand{\bbetatilde}{\widetilde{\vbeta}}
\newcommand{\bgatilde}{\widetilde{\vgamma}}
\newcommand{\bgammatilde}{\widetilde{\vgamma}}
\newcommand{\bdetilde}{\widetilde{\vdelta}}
\newcommand{\bdeltatilde}{\widetilde{\vdelta}}
\newcommand{\bepstilde}{\widetilde{\vepsilon}}
\newcommand{\bepsilontilde}{\widetilde{\vepsilon}}
\newcommand{\bzetilde}{\widetilde{\vzeta}}
\newcommand{\bzetatilde}{\widetilde{\vzeta}}
\newcommand{\boldetatilde}{\widetilde{\veta}}
\newcommand{\bthtilde}{\widetilde{\vtheta}}
\newcommand{\bthetatilde}{\widetilde{\vtheta}}
\newcommand{\biotatilde}{\widetilde{\viota}}
\newcommand{\bkappatilde}{\widetilde{\vkappa}}
\newcommand{\blambdatilde}{\widetilde{\vlambda}}
\newcommand{\bmutilde}{\widetilde{\vmu}}
\newcommand{\bnutilde}{\widetilde{\vnu}}
\newcommand{\bxitilde}{\widetilde{\vxi}}
\newcommand{\bpitilde}{\widetilde{\vpi}}
\newcommand{\brhotilde}{\widetilde{\vrho}}
\newcommand{\bsigtilde}{\widetilde{\vsigma}}
\newcommand{\bsigmatilde}{\widetilde{\vsigma}}
\newcommand{\btautilde}{\widetilde{\vtau}}
\newcommand{\bupsilontilde}{\widetilde{\vupsilon}}
\newcommand{\bphitilde}{\widetilde{\vphi}}
\newcommand{\bchitilde}{\widetilde{\vchi}}
\newcommand{\bpsitilde}{\widetilde{\vpsi}}
\newcommand{\bomegatilde}{\widetilde{\vomega}}

\newcommand{\ALt}{\tilde{\val}}
\newcommand{\balt}{\tilde{\val}}
\newcommand{\balphat}{\hat{\val}}
\newcommand{\bbetat}{\tilde{\vbeta}}
\newcommand{\bgat}{\tilde{\vgamma}}
\newcommand{\bgammat}{\tilde{\vgamma}}
\newcommand{\bdet}{\tilde{\vdelta}}
\newcommand{\bdeltat}{\tilde{\vdelta}}
\newcommand{\bepst}{\tilde{\vepsilon}}
\newcommand{\bepsilont}{\tilde{\vepsilon}}
\newcommand{\bzet}{\tilde{\vzeta}}
\newcommand{\bzetat}{\tilde{\vzeta}}
\newcommand{\boldetat}{\tilde{\veta}}
\newcommand{\btht}{\tilde{\vtheta}}
\newcommand{\bthetat}{\tilde{\vtheta}}
\newcommand{\biotat}{\tilde{\viota}}
\newcommand{\bkappat}{\tilde{\vkappa}}
\newcommand{\blambdat}{\tilde{\vlambda}}
\newcommand{\bmut}{\tilde{\vmu}}
\newcommand{\bnut}{\tilde{\vnu}}
\newcommand{\bxit}{\tilde{\vxi}}
\newcommand{\bpit}{\tilde{\vpi}}
\newcommand{\brhot}{\tilde{\vrho}}
\newcommand{\bsigt}{\tilde{\vsigma}}
\newcommand{\bsigmat}{\tilde{\vsigma}}
\newcommand{\btaut}{\tilde{\vtau}}
\newcommand{\bupsilont}{\tilde{\vupsilon}}
\newcommand{\bphit}{\tilde{\vphi}}
\newcommand{\bchit}{\tilde{\vchi}}
\newcommand{\bpsit}{\tilde{\vpsi}}
\newcommand{\bomegat}{\tilde{\vomega}}

\newcommand{\albar}{\bar{\al}}
\newcommand{\alphabar}{\bar{\al}}
\newcommand{\betabar}{\bar{\beta}}
\newcommand{\gabar}{\bar{\gamma}}
\newcommand{\gammabar}{\bar{\gamma}}
\newcommand{\debar}{\bar{\delta}}
\newcommand{\deltabar}{\bar{\delta}}
\newcommand{\epsbar}{\bar{\epsilon}}
\newcommand{\epsilonbar}{\bar{\epsilon}}
\newcommand{\zebar}{\bar{\zeta}}
\newcommand{\zetabar}{\bar{\zeta}}
\newcommand{\etabar}{\bar{\eta}}
\newcommand{\thbar}{\bar{\theta}}
\newcommand{\thetabar}{\bar{\theta}}
\newcommand{\iotabar}{\bar{\iota}}
\newcommand{\kappabar}{\bar{\kappa}}
\newcommand{\lambdabar}{\bar{\lambda}}
\newcommand{\mubar}{\bar{\mu}}
\newcommand{\nubar}{\bar{\nu}}
\newcommand{\xibar}{\bar{\xi}}
\newcommand{\pibar}{\bar{\pi}}
\newcommand{\rhobar}{\bar{\rho}}
\newcommand{\sigbar}{\bar{\sigma}}
\newcommand{\sigmabar}{\bar{\sigma}}
\newcommand{\taubar}{\bar{\tau}}
\newcommand{\upsilonbar}{\bar{\upsilon}}
\newcommand{\phibar}{\bar{\phi}}
\newcommand{\chibar}{\bar{\chi}}
\newcommand{\psibar}{\bar{\psi}}
\newcommand{\omegabar}{\bar{\omega}}

\newcommand{\alhat}{\hat{\al}}
\newcommand{\alphahat}{\hat{\al}}
\newcommand{\betahat}{\hat{\beta}}
\newcommand{\gahat}{\hat{\gamma}}
\newcommand{\gammahat}{\hat{\gamma}}
\newcommand{\dehat}{\hat{\delta}}
\newcommand{\deltahat}{\hat{\delta}}
\newcommand{\epshat}{\hat{\epsilon}}
\newcommand{\epsilonhat}{\hat{\epsilon}}
\newcommand{\zehat}{\hat{\zeta}}
\newcommand{\zetahat}{\hat{\zeta}}
\newcommand{\etahat}{\hat{\eta}}
\newcommand{\thhat}{\hat{\theta}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\iotahat}{\hat{\iota}}
\newcommand{\kappahat}{\hat{\kappa}}
\newcommand{\lambdahat}{\hat{\lambda}}
\newcommand{\muhat}{\hat{\mu}}
\newcommand{\nuhat}{\hat{\nu}}
\newcommand{\xihat}{\hat{\xi}}
\newcommand{\rhohat}{\hat{\rho}}
\newcommand{\sighat}{\hat{\sigma}}
\newcommand{\sigmahat}{\hat{\sigma}}
\newcommand{\tauhat}{\hat{\tau}}
\newcommand{\upsilonhat}{\hat{\upsilon}}
\newcommand{\phihat}{\hat{\phi}}
\newcommand{\chihat}{\hat{\chi}}
\newcommand{\psihat}{\hat{\psi}}
\newcommand{\omegahat}{\hat{\omega}}

\newcommand{\alphatil}{\tilde{\alpha}}
\newcommand{\betatil}{\tilde{\beta}}
\newcommand{\gatil}{\tilde{\gamma}}
\newcommand{\gammatil}{\tilde{\gamma}}
\newcommand{\deltatil}{\tilde{\delta}}
\newcommand{\epstil}{\tilde{\epsilon}}
\newcommand{\epsilontil}{\tilde{\epsilon}}
\newcommand{\zetil}{\tilde{\zeta}}
\newcommand{\zetatil}{\tilde{\zeta}}
\newcommand{\etatil}{\tilde{\eta}}
\newcommand{\thetatil}{\tilde{\theta}}
\newcommand{\iotatil}{\tilde{\iota}}
\newcommand{\kappatil}{\tilde{\kappa}}
\newcommand{\lambdatil}{\tilde{\lambda}}
\newcommand{\mutil}{\tilde{\mu}}
\newcommand{\nutil}{\tilde{\nu}}
\newcommand{\xitil}{\tilde{\xi}}
\newcommand{\pitil}{\tilde{\pi}}
\newcommand{\rhotil}{\tilde{\rho}}
\newcommand{\sigtil}{\tilde{\sigma}}
\newcommand{\sigmatil}{\tilde{\sigma}}
\newcommand{\tautil}{\tilde{\tau}}
\newcommand{\upsilontil}{\tilde{\upsilon}}
\newcommand{\phitil}{\tilde{\phi}}
\newcommand{\chitil}{\tilde{\chi}}
\newcommand{\psitil}{\tilde{\psi}}
\newcommand{\omegatil}{\tilde{\omega}}

\newcommand{\alphat}{\tilde{\alpha}}
\newcommand{\betat}{\tilde{\beta}}
\newcommand{\gammat}{\tilde{\gamma}}
\newcommand{\deltat}{\tilde{\delta}}
\newcommand{\epsilont}{\tilde{\epsilon}}
\newcommand{\epst}{\tilde{\epsilon}}
\newcommand{\zetat}{\tilde{\zeta}}
\newcommand{\etat}{\tilde{\eta}}
\newcommand{\thetat}{\tilde{\theta}}
\newcommand{\iotat}{\tilde{\iota}}
\newcommand{\kappat}{\tilde{\kappa}}
\newcommand{\lambdat}{\tilde{\lambda}}
\newcommand{\mut}{\tilde{\mu}}
\newcommand{\nut}{\tilde{\nu}}
\newcommand{\xit}{\tilde{\xi}}
\newcommand{\pit}{\tilde{\pi}}
\newcommand{\rhot}{\tilde{\rho}}
\newcommand{\sigt}{\tilde{\sigma}}
\newcommand{\sigmat}{\tilde{\sigma}}
\newcommand{\taut}{\tilde{\tau}}
\newcommand{\upsilont}{\tilde{\upsilon}}
\newcommand{\phit}{\tilde{\phi}}
\newcommand{\chit}{\tilde{\chi}}
\newcommand{\psit}{\tilde{\psi}}
\newcommand{\omegat}{\tilde{\omega}}

\newcommand{\Gammabar}{\bar{\Gamma}}
\newcommand{\Deltabar}{\bar{\Delta}}
\newcommand{\Thetabar}{\bar{\Theta}}
\newcommand{\Lambdabar}{\bar{\Lambda}}
\newcommand{\Xibar}{\bar{\Xi}}
\newcommand{\Pibar}{\bar{\Pi}}
\newcommand{\Sigmabar}{\bar{\Sigma}}
\newcommand{\Upsilonbar}{\bar{\Upsilon}}
\newcommand{\Phibar}{\bar{\Phi}}
\newcommand{\Psibar}{\bar{\Psi}}
\newcommand{\Omegabar}{\bar{\Omega}}

\newcommand{\Gammahat}{\hat{\Gamma}}
\newcommand{\Deltahat}{\hat{\Delta}}
\newcommand{\Thetahat}{\hat{\Theta}}
\newcommand{\Lambdahat}{\hat{\Lambda}}
\newcommand{\Xihat}{\hat{\Xi}}
\newcommand{\Pihat}{\hat{\Pi}}
\newcommand{\Sigmahat}{\hat{\Sigma}}
\newcommand{\Upsilonhat}{\hat{\Upsilon}}
\newcommand{\Phihat}{\hat{\Phi}}
\newcommand{\Psihat}{\hat{\Psi}}
\newcommand{\Omegahat}{\hat{\Omega}}

\newcommand{\Gammatil}{\tilde{\Gamma}}
\newcommand{\Deltatil}{\tilde{\Delta}}
\newcommand{\Thetatil}{\tilde{\Theta}}
\newcommand{\Lambdatil}{\tilde{\Lambda}}
\newcommand{\Xitil}{\tilde{\Xi}}
\newcommand{\Pitil}{\tilde{\Pi}}
\newcommand{\Sigmatil}{\tilde{\Sigma}}
\newcommand{\Upsilontil}{\tilde{\Upsilon}}
\newcommand{\Phitil}{\tilde{\Phi}}
\newcommand{\Psitil}{\tilde{\Psi}}
\newcommand{\Omegatil}{\tilde{\Omega}}

\newcommand{\Gammatilde}{\widetilde{\Gamma}}
\newcommand{\Deltatilde}{\widetilde{\Delta}}
\newcommand{\Thetatilde}{\widetilde{\Theta}}
\newcommand{\Lambdatilde}{\widetilde{\Lambda}}
\newcommand{\Xitilde}{\widetilde{\Xi}}
\newcommand{\Pitilde}{\widetilde{\Pi}}
\newcommand{\Sigmatilde}{\widetilde{\Sigma}}
\newcommand{\Upsilontilde}{\widetilde{\Upsilon}}
\newcommand{\Phitilde}{\widetilde{\Phi}}
\newcommand{\Psitilde}{\widetilde{\Psi}}
\newcommand{\Omegatilde}{\widetilde{\Omega}}

\newcommand{\Gammat}{\tilde{\Gamma}}
\newcommand{\Deltat}{\tilde{\Delta}}
\newcommand{\Thetat}{\tilde{\Theta}}
\newcommand{\Lambdat}{\tilde{\Lambda}}
\newcommand{\Xit}{\tilde{\Xi}}
\newcommand{\Pit}{\tilde{\Pi}}
\newcommand{\Sigmat}{\tilde{\Sigma}}
\newcommand{\Upsilont}{\tilde{\Upsilon}}
\newcommand{\Phit}{\tilde{\Phi}}
\newcommand{\Psit}{\tilde{\Psi}}
\newcommand{\Omegat}{\tilde{\Omega}}

\newcommand{\valbar}{\bar{\val}}
\newcommand{\valhat}{\hat{\val}}
\newcommand{\valtil}{\tilde{\val}}
\newcommand{\vthtil}{\tilde{\vth}}
\newcommand{\vthhat}{\hat{\vth}}
\newcommand{\vthbar}{\bar{\vth}}
\newcommand{\vbetatil}{\tilde{\vbeta}}
\newcommand{\vbetahat}{\hat{\vbeta}}

\newcommand{\alphavec}{\val}
\newcommand{\alphavecbar}{\bar{\val}}
\newcommand{\alphavechat}{\hat{\val}}
\newcommand{\alphavectil}{\tilde{\val}}
\newcommand{\betavec}{\vec{\beta}}
\newcommand{\gammavec}{\vec{\gamma}}
\newcommand{\deltavec}{\vec{\delta}}
\newcommand{\etavec}{\vec{\eta}}
\newcommand{\phivec}{\vec{\phi}}
\newcommand{\psivec}{\vec{\psi}}
\newcommand{\thetavec}{\vec{\theta}}
\newcommand{\muvec}{\vec{\mu}}
\newcommand{\xivec}{\vec{\xi}}
\newcommand{\chivec}{\vec{\chi}}
\newcommand{\lambdavec}{\vec{\lambda}}

\newcommand{\alphab}{\boldsymbol{\alpha}}
\newcommand{\phib}{\boldsymbol{\phi}}
\newcommand{\epsilonb}{\boldsymbol{\epsilon}}
\newcommand{\betab}{\boldsymbol{\beta}}
\newcommand{\gammab}{\boldsymbol{\gamma}}
\newcommand{\thetab}{\boldsymbol{\theta}}
\newcommand{\mub}{\boldsymbol{\mu}}
\newcommand{\xib}{\boldsymbol{\xi}}
\newcommand{\Deltab}{\boldsymbol{\Delta}}
\newcommand{\Pib}{\boldsymbol{\Pi}}
\newcommand{\etab}{\boldsymbol{\eta}}
\newcommand{\taub}{\boldsymbol{\tau}}
\newcommand{\lambdab}{\boldsymbol{\lambda}}
\newcommand{\elltil}{\tilde{\ell}}
\newcommand{\rhob}{\boldsymbol{\rho}}

\newcommand{\delhat}{\hat{\delta}}
\newcommand{\delbar}{\bar{\delta}}

\newcommand{\lambdavectil}{\tilde{\vec{\lambda}}}




\newcommand{\Poisson}{\mathrm{Poisson}}

\def\*#1{\mathbf{#1}}
\def\_#1{\mathcal{#1}}
\def\-#1{\mathbb{#1}}
\def\=#1{\pmb{#1}}

\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\mat}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmat}[1]{\pa{\mat{#1}}}
\newcommand{\bmat}[1]{\bra{\mat{#1}}}

\newcommand{\Ibest}{I_{\text{best}}}
\newcommand{\gap}{\textsc{gap}}

\newcommand{\ind}[1]{\bone\left\{ #1 \right\}}

\def\EE{{\mathbb{E}}}\def\PP{{\mathbb{P}}}

\newcommand{\RegExp}{\mathsf{R}}
\newcommand{\RegEmp}{\widehat{\mathsf{R}}}
\newcommand{\thetals}{\hat{\pmb{\theta}}_{\mathrm{ls}}}
\newcommand{\thetawls}{\hat{\pmb{\theta}}_{\mathrm{wls}}}
\newcommand{\thetamle}{\hat{\pmb{\theta}}_{\mathrm{mle}}}
\newcommand{\thetaopt}{\pmb{\theta}^{*}}
\newcommand{\thetamleone}{\hat{\theta}_{\mathrm{mle}}}
\newcommand{\thetaoptone}{\theta^{*}}
\newcommand{\thetalsone}{\hat{\theta}_{\mathrm{ls}}}
\newcommand{\thetabet}{\hat{\pmb{\theta}}(\pmb{\beta})}
\newcommand{\ball}{\-B}
\newcommand{\sphere}{\-S}
\newcommand{\subg}{\mathrm{subG}}
\newcommand{\im}{i_{min}}
\newcommand{\eig}{\mathrm{eig}}
\newcommand{\lmax}{\lambda_{\mathrm{max}}}
\newcommand{\lmin}{\lambda_{\mathrm{min}}}
\newcommand{\dtheta}{\nabla_{\thetab}}
\newcommand{\stir}{\mathrm{Stirling}}
\newcommand{\eptilde}{\tilde{\pmb{\epsilon}}}
\newcommand{\rs}[1]{\textcolor{red}{[RS: #1]}}
\newcommand{\tmax}{w}
\newcommand{\xmax}{R}
\newcommand{\Rel}{\mathsc{Rel}}
\newcommand{\Relhat}{\widehat{\mathsc{Rel}}}
\newcommand{\pred}{\widehat{y}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\ours}{TiDE}


\usepackage{pdfpages}

 

\title{Long-term Forecasting with \\TiDE: Time-series Dense Encoder}

\author{\name Abhimanyu Das \email abhidas@google.com \\
      \addr Google Research
      \AND
      \name Weihao Kong \email weihaokong@google.com \\
      \addr Google Research
      \AND
      \name Andrew Leach \email andrewleach@google.com \\
      \addr Google Cloud
      \AND
      \name Shaan Mathur \email shaanmathur@google.com \\
      \addr Google Cloud
      \AND
      \name Rajat Sen\email senrajat@google.com \\
      \addr Google Research
      \AND
      \name Rose Yu\email q6yu@ucsd.edu \\
      \addr University of California, San Diego
      }



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\def\month{MM}  \def\year{YYYY} \def\openreview{\url{https://openreview.net/forum?id=pCbC3aQB5W}} 


\begin{document}

\maketitle

\begin{abstract}
Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, \underline{Ti}me-series \underline{D}ense \underline{E}ncoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we  show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.\blfootnote{Author names are arranged in alphabetical order of last names.}
\end{abstract}


\section{Introduction}
Long-term forecasting, which is to predict several steps into the future  given  a long context or look-back, is one of the most fundamental problems in time series analysis, with broad applications in energy, finance, and transportation. Deep learning models \citep{wu2021autoformer,nie2022time} have emerged as a popular approach for forecasting rich, multivariate, time series data, often outperforming classical statistical approaches such as ARIMA or GARCH \citep{box2015time}. In several forecasting competitions such as the M5  competition \citep{makridakis2020m5} and IARAI Traffic4cast contest \citep{kreil2020surprising}, almost all the winning solutions are based on deep neural networks.

Various neural network architectures have been explored for forecasting, ranging from recurrent neural networks to convolutional networks to graph neural networks.
For sequence modeling tasks in domains such as language, speech and vision, Transformers \citep{vaswani2017attention} have emerged as the most successful model, even outperforming recurrent neural networks (LSTMs)\citep{hochreiter1997long}.
Subsequently, there has been a surge of Transformer-based forecasting papers \citep{wu2021autoformer,zhou2021informer,zhou2022fedformer} in the time-series community that have claimed state-of-the-art (SoTA) forecasting performance for long-horizon tasks. However, recent work~\citep{zeng2022transformers} has shown that these Transformerss-based architectures may not be as powerful as one might expect for time series forecasting, and can  be easily outperformed  by a simple linear model on forecasting benchmarks. Such a linear model however has deficiencies since it is ill-suited for modeling non-linear dependencies among the time-series sequence and the time-independent covariates. Indeed, a very recent paper~\citep{nie2022time} proposed a new Transformer-based architecture that  obtains SoTA performance for deep neural networks on the standard multivariate forecasting benchmarks.



In this paper, we present a simple and effective deep learning architecture for long-term forecasting that obtains  superior performance when compared to existing SoTA neural network based models on the   time series forecasting benchmarks. Our Multi-Layer Perceptron (MLP)-based model is embarrassingly  simple without any self-attention, recurrent or convolutional mechanism. Therefore, it enjoys a linear computational scaling in terms of the context and horizon lengths unlike many Transformer-based solutions.


The \textit{main contributions} of this work are as follows:

\begin{itemize}
\item We propose the \underline{Ti}me-series \underline{D}ense \underline{E}ncoder (TiDE) model architecture for long-term time series forecasting.  TiDE encodes the past of a time-series along with covariates using dense MLPs and then decodes  time-series along with future covariates, again using dense MLPs.  

\item We analyze a simplified linear analogue of our model and prove that this linear model can achieve near optimal error rate in linear dynamical systems (LDS)~\citep{kalman1963mathematical} when the design matrix of the LDS has maximum singular value bounded away from $1$. We empirically verify this on a simulated dataset where the linear model outperforms LSTMs and Transformers.  

\item On popular real-world long-term forecasting benchmarks, our model achieves better or similar performance compared to prior neural network based baselines ($>$10\% lower Mean Squared Error on the largest dataset). At the same time, TiDE is 5x faster in terms of inference and more than 10x faster in training when compared to the best Transformer based model.
\end{itemize}







\section{Background and Related Work}
\label{sec:rwork}
Models for long-term forecasting can be broadly divided into either \textit{multivariate} models or \textit{univariate} models.

Multivariate models use the past of all the interrelated time-series variables and predict the future of all the time-series as a joint function of those pasts. This includes the classical VAR models~\citep{zivot2006vector}. We will mostly focus on the prior work on neural network based models for long-term forecasting. LongTrans~\citep{li2019enhancing} uses attention layer with LogSparse design to capture local information with near linear space and computational complexity. Informer~\citep{zhou2021informer} uses the ProbSparse self-attention mechanism to achieve sub-quadratic dependency on the length of the context. Autoformer~\citep{wu2021autoformer} uses trend and seasonal decomposition with sub-quadratic self attention mechanism. FEDFormer~\citep{zhou2022fedformer} uses a frequency enchanced structure while Pyraformer~\citep{liu2021pyraformer} uses pyramidal self-attention that has linear complexity and can attend to different granularities. The common theme among the above works is the use of sub-quadratic approximations for the full self attention mechanism, that have been also been used in other domains~\citep{wang2020linformer}.

On the other hand, univariate models predict the future of a time-series variable as a function of only the past of the same time-series and covariate features. In other words, the past of other time-series is not part of the input during inference. There are two kinds of univariate models, \textit{local} and \textit{global}. Local univariate models are usually trained per time-series variable and inference is done per time-series as well. Different variables have different models. Classical models like AR, ARIMA,  exponential smoothing models~\citep{mckenzie1984general} and the Box-Jenkins methodology~\citep{box1968some} belong in this category. We would refer the reader to~\citep{box2015time} for an in depth discussion of these methods.

Global univariate models ignore the variable information and train one shared model for all the time series on the whole dataset. This category mainly includes deep learning based architectures like~\citep{salinas2020deepar}. In the context of long-term forecasting, recently it was observed that a simple linear global univariate model can outperform the transformer based multivariate approaches for long-term forecasting~\citep{zeng2022transformers}. DLinear~\citep{zeng2022transformers} learns a  linear mapping from context to horizon, pointing to deficiencies in sub-quadratic approximations to the self-attention mechanism. Indeed, a very recent model, PatchTST~\citep{nie2022time} has shown that feeding contiguous patches of time-series as tokens to the vanilla self-attention mechanism can beat the performance of DLinear in long-term forecasting benchmarks.
MLPs have been used for time-series forecasting in the popular N-BEATS model~\citep{oreshkinn} and later extended in a follow up work~\citep{challu2022nhits} that uses multi-rate sampling for better efficiency. However, these methods do not explicitly mention supporting covariates and fall short of PatchTST in long-horizon benchmarks.

Recent work has improved the efficacy of RNNs~\citep{kag2020rnns, lukovsevivcius2022time, rusch2020coupled, li2019deep} and applied parameter efficient SSMs~\citep{guefficiently, gupta2022diagonal} to modeling long range dependencies in sequences. They have demonstrated improvement over some transformer based architectures on sequence modeling benchmarks including speech and 1-D pixel level image classification tasks. We compare our method to S4 model~\citep{guefficiently}, which is the only such method that has been applied to global univariate and global multivariate forecasting.

Note that all categories of models can be fairly compared on the task of multivariate long-term forecasting if they are evaluated on the same test set for the same task, which is the protocol that we follow in Section~\ref{sec:exp}.


\section{Problem Setting}
\label{sec:psetting}
Before we describe the problem setting we will need to setup some general notation.

\subsection{Notation}
We will denote matrices by bold capital letters like $\bX \in \reals^{N \times T}$. The slice notation $i:j$ denotes the set $\{i, i+1, \cdots j\}$ and $[n]:= \{1, 2, \cdots, n\}$. The individual rows and columns are always treated as column vectors unless otherwise specified. We can also use sets to select sub-matrices i.e $\bX[\cI, \cJ]$ denotes the sub-matrix with rows in $\cI$ and columns in $\cJ$. $\bX[:, j]$ means selecting the $j$-th column while $\bX[i, :]$ means the $i$-th row. The notation $[\bv; \bu]$ will denote the concatenation of the two column vectors and the same notation can be used for matrices along a dimension.

\subsection{Multivariate Forecasting}
\label{sec:probfor}
In this section we first abstract out the core problem in long-term multivariate forecasting. There are $N$ time-series in the dataset. The look-back of the $i$-th time-series will be denoted by $\*y^{(i)}_{1:L}$, while the horizon is denoted by $\*y^{(i)}_{L+1:L+H}$. The task of the forecaster is to predict the horizon time-points given access to the look-back.

In many forecasting scenarios, there might be dynamic and static covariates that are known in advance. With slight abuse of notation, we will use $\bx^{(i)}_t \in \reals^{r}$ to denote the $r$-dimensional dynamic covariates of time-series $i$ at time $t$. For instance, they can be global covariates (common to all time-series) such as day of the week, holidays etc or specific to a time-series for instance the discount of a particular product on a particular day in a demand forecasting use case. We can also have static attributes of a time-series denoted by $\ba^{(i)}$ such as features of a product in retail that do not change with time. In many applications, these covariates are vital for accurate forecasting and a good model architecture should have provisions to handle them.

The forecaster can be thought of as a function that maps the history $\*y^{(i)}_{1:L}$, the dynamic covariates $\bx^{(i)}_{1:L+H}$ and the static attributes  $\ba^{(i)}$ to an accurate prediction of the future, i.e.,
\begin{equation}
    f: \left( \left\{\*y^{(i)}_{1:L}\right\}_{i=1}^{N}, \left\{\bx^{(i)}_{1:L+H}\right\}_{i=1}^{N}, \left\{\ba^{(i)}\right\}_{i=1}^N \right) \longrightarrow \left\{\hat{\*y}^{(i)}_{L+1:L+H}\right\}_{i=1}^{N}.
    \label{eq:all}
\end{equation}

The accuracy of the prediction will be measured by a metric that quantifies their closeness to the actual values. For instance, if the metric is Mean Squared Error (MSE), then the goodness of fit is measured by,
\begin{align}
    \mathrm{MSE} \left(\left\{\*y^{(i)}_{L+1:L+H}\right\}_{i=1}^{N},  \left\{\hat{\*y}^{(i)}_{L+1:L+H}\right\}_{i=1}^{N}\right) = \frac{1}{NH} \sum_{i=1}^{N}\norm{\*y^{(i)}_{L+1:L+H} - \hat{\*y}^{(i)}_{L+1:L+H}}_2^2.
    \label{eq:mse}
\end{align}
 
\section{Model}
\label{sec:model}

Recently, it has been observed that simple linear models~\citep{zeng2022transformers} can outperform Transformers based models in several long-term forecasting benchmarks. On the other hand, linear models will fall short when there are inherent non-linearities in the dependence of the future on the past. Furthermore, linear models would not be able to model the dependence of the prediction on the covariates as evidenced by the fact that~\citep{zeng2022transformers} do not use time-covariates as they hurt performance. 

In this section, we introduce a simple and efficient MLP based architecture for long-term time-series forecasting. In our model we add non-linearities in the form of MLPs in a manner that can handle past data and covariates. The model is dubbed \ours~(\underline{Ti}me-series \underline{D}ense \underline{E}ncoder) as it encodes the past of a time-series along with covariates using dense MLP's and then decodes the encoded time-series along with future covariates.

\begin{figure*}[tbh!]
    \centering
    \includegraphics[width=\linewidth]{tide_arch_final.pdf}
    \caption{Overview of TiDE architecture. The dynamic covariates per time-point are mapped to a lower dimensional space using a feature projection step. Then the encoder combines the look-back along with the projected covariates with the static attributes to form an encoding. The decoder maps this encoding to a vector per time-step in the horizon. Then a temporal decoder combines this vector (per time-step) with the projected features of that time-step in the horizon to form the final predictions. We also add a global linear residual connection from the look-back to the horizon.}
    \label{fig:arch}
\end{figure*}

An overview of our architecture has been presented in Figure~\ref{fig:arch}. Our model is applied in a channel independent manner (the term was used in~\citep{nie2022time}) i.e the input to the model is the past and covariates of one time-series at a time $\left(\*y^{(i)}_{1: L}, \bx^{(i)}_{1:L}, \ba^{(i)}\right)$ and it maps it to the prediction of that time-series $\hat{\*y}^{(i)}_{L+1: L+H}$. Note that the weights of the model are trained globally using the whole dataset. A key component in our model is the MLP residual block towards the right of the figure. 

{\bf Residual Block.} We use the residual block as the basic layer in our architecture. It is an MLP with one hidden layer with ReLU activation. It also has a skip connection that is fully linear. We use dropout on the linear layer that maps the hidden layer to the output and also use layer norm at the output.

We separate the model into encoding and decoding sections. The encoding section has a novel feature projection step followed by a dense MLP encoder. The decoder section consists of a dense decoder followed by a novel temporal decoder. Note that the dense encoder (green block with $n_e$ layers) and decoder blocks (yellow block with $n_d$ layers) in Figure~\ref{fig:arch} can be merged into a single block. For the sake of exposition we keep them separate as we tune the hidden layer size in the two blocks separately. Also the last layer of the decoder block is unique in the sense that its output dimension needs to be $H \times p$ before the reshape operation. 

\subsection{Encoding}
\label{sec:encoder}

The task of the encoding step is to map the past and the covariates of a time-series to a dense representation of the features. The encoding in our model has two key steps.

{\bf Feature Projection.} We use a residual block to map $\bx^{(i)}_t$ at each time-step (both in the look-back and the horizon) into a lower dimensional projection of size $\tilde{r} \ll r$ (\texttt{temporalWidth}). This operation can be described as,
\begin{align}
    \tilde{\bx}^{(i)}_t = \mathrm{ResidualBlock} \left( \bx^{(i)}_t\right).
\end{align}

This is essentially a dimensionality reduction step since flattening the dynamic covariates for the whole look-back and horizon would lead to an input vector of size $(L + H)r$ which can be prohibitively large. On the other hand, flattening the reduced features would only lead to a dimension of $(L + H)\tilde{r}$.

{\bf Dense Encoder.}  As the input to the dense encoder, we stack and flatten all the past and  future projected covariates, concatenate them with the static attributes and the past of the time-series. Then we map them to an embedding using an encoder which contains multiple residual blocks. This can be written as,
\begin{align}
    \be^{(i)} = \mathrm{Encoder} \left(\*y^{(i)}_{1:L}; \tilde{\bx}^{(i)}_{1:L+H}; \ba^{(i)} \right)
\end{align}
The encoder internal layer sizes are all set to \texttt{hiddenSize} and the total number of layers in the encoder is set to $n_e$ (\texttt{numEncoderLayers}).

\subsection{Decoding}
\label{sec:dec}
The decoding in our model maps the encoded hidden representations into future predictions of time series. It also comprises of two operations, dense decoder and temporal decoder.

{\bf Dense Decoder.} The first decoding unit is a stacking of several residual blocks like the encoder with the same hidden layer sizes. It takes as an input the encoding $\be^{(i)}$ and maps it to a vector $\bg^{(i)}$ of size $H \times p$ where $p$ is the \texttt{decoderOutputDim}. This vector is then reshaped to a matrix $\bD^{(i)} \in \reals^{d \times H}$. The $t$-th column i.e $\bd^{(i)}_t$ can be thought of as the decoded vector for the $t$-th time-period in the horizon for all $t \in [H]$. This whole operation can be described as,
\begin{align*}
    \bg^{(i)} &= \mathrm{Decoder} \left( \be^{(i)} \right)~~~\in \reals^{p.H} \\
    \bD^{(i)} &= \mathrm{Reshape} \left( \bg^{(i)} \right)~~~\in \reals^{p \times H}.
\end{align*}
The number of layers in the dense decoder is $n_d$ (\texttt{numDecoderLayers}).

{\bf Temporal Decoder.} Finally, we use the temporal decoder to generate the final predictions. The temporal decoder is just a residual block with output size $1$ that maps the decoded vector $\bd^{(i)}_t$ at $t$-th horizon time-step concatenated with the projected covariates $\tilde{\bx}^{(i)}_{L+t}$ i.e
\begin{align*}
    \hat{\*y}^{(i)}_{L+t} = \mathrm{TemporalDecoder} \left( \bd^{(i)}_t; \tilde{\bx}^{(i)}_{L+t}\right)~~~\forall t \in [H].
\end{align*}

This operation adds a "highway" from the future covariates at time-step $L + t$ to the prediction at time-step $L+t$. This can be useful if some covariates have a strong direct effect on a particular time-step's actual value. For instance, in retail demand forecasting a holiday like Mother's day might strongly affect the sales of certain gift items. Such signals can be lost or take longer for the model to learn in absence of such a highway. We denote the hyperparameter controlling the hidden size of the temporal decoder as \texttt{temporalDecoderHidden}. 

Finally, we add a \textit{global residual connection} that linearly maps the look-back $\*y^{(i)}_{1:L}$ to a vector the size of the horizon which is added to the prediction $\hat{\*y}^{(i)}_{L+1:L+H}$. This ensures that a purely linear model like the one in~\citep{zeng2022transformers} is always a subclass of our model.

{\bf Training and Evaluation.} The model is trained using mini-batch gradient descent where each batch consists of a \texttt{batchSize} number of time-series and the corresponding look-back and horizon time-points. We use MSE as the training loss. Each epoch consists of all look-back and horizon pairs that can be constructed from the training period i.e. two mini-batches can have overlapping time-points. This was standard practice in all prior work on long-term forecasting~\citep{zeng2022transformers, liu2021pyraformer, wu2021autoformer, li2019enhancing}. 

The model is evaluated on a test set on every (look-back, horizon) pair that can be constructed from the test set. This is usually known as rolling validation/evaluation. A similar evaluation on a validation set can be used optionally used to tune parameters for model selection.


\section{Experimental Results}
\label{sec:exp}
In this section we present our main experimental results on popular long-term forecasting benchmarks. We also perform an ablation study that shows the usefulness of the temporal decoder.

\subsection{Long-Term Time-Series Forecasting}

{\bf Datasets.} We use seven commonly used long-term forecasting benchmark datasets: Weather, Traffic, Electricity and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). We  refer the reader to~\citep{wu2021autoformer} for a detailed discussion on the datasets. In Table~\ref{tab:datasets} we provide some statistics about the datasets. Note that Traffic and Electricity are the largest datasets with $>800$ and $>300$ time-series each having tens of thousands of time-points. Since we are only interested in long-term forecasting results in this section, we omit the shorter horizon ILI dataset.

\begin{table}[ht!]
\centering
\begin{tabular}{l|c|c|c}
\toprule
Dataset & \#Time-Series & \#Time-Points & Frequency \\
\midrule
Electricity & 321 & 26304 & 1 Hour \\
Traffic & 862 & 17544 & 1 Hour \\
Weather & 21 & 52696 & 10 Minutes \\
ETTh1   & 7 & 17420 & 1 Hour \\
ETTh2   & 7 & 17420 & 1 Hour \\
ETTm1   & 7 & 69680 & 15 Minutes \\
ETTm2   & 7 & 69680 & 15 Minutes \\
\bottomrule
\end{tabular}
\caption{Summary of datasets.}
\label{tab:datasets}
\end{table}

\begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|cc|cc|cc|cc|ccc}
			\cline{2-21}
			&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\ours} & \multicolumn{2}{c|}{PatchTST/64}& \multicolumn{2}{c|}{N-HiTS} & \multicolumn{2}{c|}{DLinear}& \multicolumn{2}{c|}{FEDformer}& \multicolumn{2}{c|}{Autoformer}& \multicolumn{2}{c|}{Informer}& \multicolumn{2}{c|}{Pyraformer}& \multicolumn{2}{c}{LogTrans}& \\
			\cline{2-21}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-21}
			&\multirow{4}*{\rotatebox{90}{Weather}} & 96 & 0.166 & 0.222 & \textbf{0.149} & \textbf{0.198} & 0.158 & \textbf{0.195} & 0.176 & 0.237 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354 & 0.405 & 0.896 & 0.556 & 0.458 & 0.490 \\
            &\multicolumn{1}{c|}{}& 192 & 0.209 & 0.263 & \textbf{0.194} & \textbf{0.241} & 0.211 & 0.247 & 0.220 & 0.282 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419 & 0.434 & 0.622 & 0.624 & 0.658 & 0.589 \\
            &\multicolumn{1}{c|}{}& 336 & 0.254 & 0.301 & \textbf{0.245} & \textbf{0.282} & 0.274 & 0.300 & 0.265 & 0.319 & 0.339 & 0.377 & 0.351 & 0.391 & 0.583 & 0.543  & 0.739 & 0.753 & 0.797 & 0.652 \\
            &\multicolumn{1}{c|}{}& 720  & \textbf{0.313} & 0.340 & \textbf{0.314} & \textbf{0.334} & 0.401 & 0.413 & 0.323 & 0.362 & 0.389 & 0.409 & 0.415 & 0.426 & 0.916 & 0.705 & 1.004 & 0.934 & 0.869 & 0.675 \\
			\cline{2-21}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96  & \textbf{0.336} & 0.253  & 0.360 & \textbf{0.249} & 0.402 & 0.282 & 0.410 & 0.282 & 0.576 & 0.359 & 0.597 & 0.371 & 0.733 & 0.410 & 2.085 & 0.468 & 0.684 & 0.384 \\
            &\multicolumn{1}{c|}{} & 192 & \textbf{0.346} & \textbf{0.257} & 0.379 & \textbf{0.256} & 0.420 & 0.297 & 0.423 & 0.287 & 0.610 & 0.380 & 0.607 & 0.382 & 0.777 & 0.435 & 0.867 & 0.467 & 0.685 & 0.390 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.355} & \textbf{0.260} & 0.392 & 0.264  & 0.448 & 0.313 & 0.436 & 0.296 & 0.608 & 0.375 & 0.623 & 0.387 & 0.776 & 0.434 & 0.869 & 0.469 & 0.734 & 0.408 \\
            &\multicolumn{1}{c|}{}& 720 & \textbf{0.386} & \textbf{0.273} & 0.432 & 0.286  & 0.539 & 0.353 & 0.466 & 0.315 & 0.621 & 0.375 & 0.639 & 0.395 & 0.827 & 0.466 & 0.881 & 0.473 & 0.717 & 0.396 \\
            \cline{2-21}
        	&\multirow{4}*{\rotatebox{90}{Electricity}}& 96 & \textbf{0.132} & 0.229  & \textbf{0.129} & \textbf{0.222} & 0.147 & 0.249 & 0.140 & 0.237 & 0.186 & 0.302 & 0.196 & 0.313 & 0.304 & 0.393 & 0.386 & 0.449 & 0.258 & 0.357 \\
			&\multicolumn{1}{c|}{}& 192 & \textbf{0.147} & 0.243 & \textbf{0.147} & \textbf{0.240} & 0.167 & 0.269 & 0.153 & 0.249 & 0.197 & 0.311 & 0.211 & 0.324 & 0.327 & 0.417 & 0.386 & 0.443 & 0.266 & 0.368 \\
			&\multicolumn{1}{c|}{}& 336 & \textbf{0.161} & 0.261 & 0.163 & \textbf{0.259} & 0.186 & 0.290 & 0.169 & 0.267 & 0.213 & 0.328 & 0.214 & 0.327 & 0.333 & 0.422 & 0.378 & 0.443 & 0.280 & 0.380 \\
			&\multicolumn{1}{c|}{}& 720 &  \textbf{0.196} & 0.294 & \textbf{0.197} & \textbf{0.290} & 0.243 & 0.340 & 0.203 & 0.301 & 0.233 & 0.344 & 0.236 & 0.342 & 0.351 & 0.427 & 0.376 & 0.445 & 0.283 & 0.376 \\
			\cline{2-21}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96  & \textbf{0.375} & 0.398 & 0.379 & {0.401}  & 0.378 & \textbf{0.393} & {0.375} & \textbf{0.399} & 0.376 & 0.415 & 0.435 & 0.446 & 0.941 & 0.769 & 0.664 & 0.612 & 0.878 & 0.740 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.412} & 0.422 & {0.413} & 0.429 & 0.427 & 0.436 & \textbf{0.412} & \textbf{0.420} & 0.423 & 0.446 & 0.456 & 0.457 & 1.007 & 0.786 & 0.790 & 0.681 & 1.037 & 0.824 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.435} & \textbf{0.433} & \textbf{0.435} & 0.436 & 0.458 & 0.484 & 0.439 & 0.443 & 0.444 & 0.462 & 0.486 & 0.487 & 1.038 & 0.784 & 0.891 & 0.738 & 1.238 & 0.932 \\
            &\multicolumn{1}{c|}{}& 720 & 0.454 & \textbf{0.465} & \textbf{0.446} & \textbf{0.464} & 0.472 & 0.561 & 0.501 & 0.490 & 0.469 & 0.492 & 0.515 & 0.517 & 1.144 & 0.857 & 0.963 & 0.782 & 1.135 & 0.852 \\
			\cline{2-21}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96  & \textbf{0.270} & \textbf{0.336} & 0.274 & \textbf{0.337} & 0.274 & 0.345 & 0.289 & 0.353 & 0.332 & 0.374 & 0.332 & 0.368 & 1.549 & 0.952 & 0.645 & 0.597 & 2.116 & 1.197 \\
            &\multicolumn{1}{c|}{}& 192  & \textbf{0.332} & 0.380 & {0.338} & \textbf{0.376}  & 0.353 & 0.401 & 0.383 & 0.418 & 0.407 & 0.446 & 0.426 & 0.434 & 3.792 & 1.542 & 0.788 & 0.683 & 4.315 & 1.635 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.360} & 0.407 & 0.363 & \textbf{0.397} & 0.382 & 0.425 & 0.448 & 0.465 & 0.400 & 0.447 & 0.477 & 0.479 & 4.215 & 1.642 & 0.907 & 0.747 & 1.124 & 1.604 \\
            &\multicolumn{1}{c|}{}& 720 &  0.419 & 0.451 & \textbf{0.393} & \textbf{0.430} & 0.625 & 0.557 & 0.605 & 0.551 & 0.412 & 0.469 & 0.453 & 0.490 & 3.656 & 1.619 & 0.963 & 0.783 & 3.188 & 1.540 \\
			\cline{2-21}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96  & 0.306 & 0.349 & \textbf{0.293} & \textbf{0.346} & 0.302 & 0.350 & 0.299 & {0.343} & 0.326 & 0.390 & 0.510 & 0.492 & 0.626 & 0.560 & 0.543 & 0.510 & 0.600 & 0.546 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.335} & \textbf{0.366} & \textbf{0.333} & 0.370 & 0.347 & 0.383 & 0.335 & \textbf{0.365} & 0.365 & 0.415 & 0.514 & 0.495 & 0.725 & 0.619 & 0.557 & 0.537 & 0.837 & 0.700 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.364} & \textbf{0.384} & {0.369} & {0.392} & 0.369 & 0.402 & {0.369} & 0.386 & 0.392 & 0.425 & 0.510 & 0.492 & 1.005 & 0.741 & 0.754 & 0.655 & 1.124 & 0.832 \\
            &\multicolumn{1}{c|}{}& 720 & \textbf{0.413} & \textbf{0.413} & 0.416 & 0.420 & 0.431 & 0.441 & 0.425 & {0.421} & 0.446 & 0.458 & 0.527 & 0.493 & 1.133 & 0.845 & 0.908 & 0.724 & 1.153 & 0.820 \\
			\cline{2-21}
			&\multirow{4}*{\rotatebox{90}{ETTm2}} & 96  & \textbf{0.161} & \textbf{0.251} & {0.166} & {0.256} & 0.176 & 0.255 & 0.167 & 0.260 & 0.180 & 0.271 & 0.205 & 0.293 & 0.355 & 0.462& 0.435 & 0.507 & 0.768 & 0.642 \\
            &\multicolumn{1}{c|}{}& 192  & \textbf{0.215} & \textbf{0.289} & {0.223} & {0.296} & 0.245 & 0.305 & 0.224 & 0.303 & 0.252 & 0.318 & 0.278 & 0.336 & 0.595 & 0.586 & 0.730 & 0.673 & 0.989 & 0.757 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.267} & \textbf{0.326} & 0.274 & 0.329 & 0.295 & 0.346 & 0.281 & 0.342 & 0.324 & 0.364 & 0.343 & 0.379 & 1.270 & 0.871 & 1.201 & 0.845 & 1.334 & 0.872 \\
            &\multicolumn{1}{c|}{}& 720  & \textbf{0.352} & \textbf{0.383} & 0.362 & \textbf{0.385} & 0.401 & 0.413 & 0.397 & 0.421 & 0.410 & 0.420 & 0.414 & 0.419 & 3.001 & 1.267 & 3.625 & 1.451 & 3.048 & 1.328 \\
			\cline{2-21}
		\end{tabular}
	}
	\caption[]{Multivariate long-term forecasting results with our model. $T\in \{96, 192, 336, 720\}$ for all datasets. The best results including the ones that cannot be statistically distinguished from the best mean numbers are in \textbf{bold}. We calculate standard error intervals for our method over 5 runs. The rest of the numbers are taken from the results from~\citep{nie2022time}\footnotemark. All metrics are reported on standard normalized datasets. \rev{We provide the standard errors for our method in Table~\ref{tab:conf} in Appendix~\ref{app:moreexp}.}}
	\label{tab:supervised}
\end{table*}


\footnotetext{Note that there was a bug in the original \href{https://github.com/yuqinie98/PatchTST/issues/7}{test dataloader} that affected the result significantly in smaller datasets like ETTh1 and ETTh2. We report the PatchTST results after correcting this bug in the dataloader.}

{\bf Baselines and Setup.}
We choose SOTA Transformers based models  for time-series including Fedformer~\citep{zhou2022fedformer}, Autoformer~\citep{wu2021autoformer}, Informer~\citep{zhou2021informer}, Pyraformer~\citep{liu2021pyraformer} and LongTrans~\citep{li2019enhancing}. Recently, DLinear~\citep{zeng2022transformers} showed that simple linear models can outperform the above methods and therefore DLinear serves as an important baseline. We include N-HiTS~\citep{challu2022nhits} which is an improvement over the famous NBeats~\citep{oreshkinn} model.
Finally we compare with PatchTST ~\citep{nie2022time} where they showed that vanilla Transformers applied to time-series patches can be very effective. The results for all Transformer based baselines are reported from~\citep{nie2022time}. 

For each method, the look-back window was tuned in $\{24, 48, 96, 192, 336, 720\}$. We report the DLinear numbers directly from the original paper~\citep{zeng2022transformers}. For our method we always use context length of $720$ for all horizon lengths in $\{96, 192, 336, 720\}$. All models were trained using MSE as the training loss. In all the datasets, the train:validation:test ratio is 7:1:2 as dictated by prior work. Note that all the experiments are performed on standard normalized datasets (using the mean and the standard deviations in the training period) in order to be consitent with prior work~\citep{wu2021autoformer}.

{\bf Our Model.} We use the  architecture described in Figure~\ref{fig:arch}. We tune our hyper-parameters using the validation set rolling validation error. We provide details about our hyper-parameters in Appendix~\ref{app:params}. As global dynamic covariates, we use simple time-derived features like minute of the hour, hour of the day, day of the week etc which are normalized similar to~\citep{alexandrov2020gluonts}. Note that these features were turned off in DLinear since it was observed to hurt the performance of the linear model, however our model can easily handle such features. Our model is trained in Tensorflow~\citep{abadi2016tensorflow} and we optimize using the default settings of the Adam optimizer~\citep{kingma2014adam}. We provide our implementation in the supplementary with scripts to reproduce the results in Table~\ref{tab:supervised}. The code can be found \href{https://github.com/google-research/google-research/tree/master/tide}{here}.

{\bf Results. } We present Mean Squared Error (MSE) and Mean Absolute Error (MSE) for all datasets and methods in Table~\ref{tab:supervised}. For our model we report the mean metric out of 5 independent runs for each setting. The bold-faced numbers are from the best model or within statistical significance of the best model in terms of two standard error intervals. Note that different predictive statistics are optimal for different target metrics~\citep{awasthibenefits, gneiting2011making} and therefore we should look at a target metric that is closely aligned with the training loss in this case. Since all models were trained using MSE let us focus on that column for comparisons. 

We can see that \ours, PatchTST, N-HiTS and DLinear are much better than the other baselines in all datasets. \rev{This can be attributed to the fact that sub-quadratic approximations to the full self-attention mechanism is perhaps not best suited for long term forecasting. The same was observed in the PatchTST~\citep{nie2022time} where it was shown that full self attention over patches was much more effective even when applied in a channel dependent manner. For a more in depth discussion about the pitfalls of sub-quadratic attention approximation in the context of forecasting, we refer the reader to Section 3 of~\citep{zeng2022transformers}.} In Appendix~\ref{sec:theory}, we prove that a linear analogue of our model can be optimal for predicting linear dynamical systems when compared against sequence models, thus shedding some light on why our model and even simpler models like DLinear can be so competitive for long context and/or horizon forecasting.

Further, we outperform DLinear  significantly  in all settings except for horizon 192 in ETTh1 where the performances are equal. This shows the value of the additional non-linearity in our model. In some datasets like Weather and ETTh1, N-HiTS performs similar to TiDE and PatchTST for horizon 96, but fails to uphold the performance for longer horizons.
In all datasets except Weather, we either outperform PatchTST or perform within its statistical significance for most horizons. In the Weather dataset, PatchTST peforms the best for horizons 96-336 while our model is the most performant for horizon 720. In the biggest dataset (Traffic), we significantly outperform PatchTST in all settings. For instance, for horizon 720 our prediction is 10.6\% better than PatchTST in  MSE. \rev{We provide additional results in Appendix~\ref{app:exp_new} including a comparison with the S4 model~\citep{guefficiently} in Table~\ref{tab:new}.}

\subsection{Demand Forecasting}
\label{sec:m5}
\rev{
In order to showcase our model's ability to handle static attrubutes and complex dynamic covariates we use the M5 forecasting competition benchmarks~\citep{makridakis2022m5}. We follow the convention in the example notebook\footnote{\url{https://github.com/awslabs/gluonts/blob/dev/examples/m5_gluonts_template.ipynb}} released by the authors of~\citep{alexandrov2020gluonts}. The dataset consists of more than 30k time-series with static attributes like hierarchical categories and dynamic covariates like promotions. More details of the setup are available in Appendix~\ref{app:exp_new}.

 We present the competition metric (WRMSSE) results on the test set corresponding to the private leader-board in Table~\ref{tab:m5}. We compare with DeepAR~\citep{salinas2020deepar} whose implementation can handle all covariates and also PatchTST (the best model from Table~\ref{tab:supervised}). Note that the implementation of PatchTST~\citep{nie2022time} does not handle covariates. We report the score over 3 independent runs along with the corresponding standard errors. We can see that PatchTST performs poorly as it does not use covariates. Our model using all the covariates outperforms DeepAR (that also uses all the covariates) by as much as 20\%. For the sake of ablation, we also provide the metric for our model that uses only date derived features as covariates. There is a degradation in performance from not using the dataset specific covariates but even so this version of the model also outperforms the other baselines.


\begin{table}[!ht]
    \centering
    \begin{tabular}{l|c|c}
\toprule 
Model & Covariates & Test WRMSSE \\ 
\midrule 
TiDE & Static + Dynamic & $\mathbf{0.611 \pm 0.009}$ \\ 
TiDE & Date only & $0.637 \pm 0.005$ \\ 
DeepAR & Static + Dynamic & $0.789 \pm 0.025$ \\ 
PatchTST & None & $0.976 \pm 0.014$ \\ 
\bottomrule
\end{tabular}
    \caption{\rev{M5 forecasting results on the private test set. We report the competition metric (averaged across three runs) for each model. We also list the covariates used by all models.}}
    \label{tab:m5}
\end{table}

}
\subsection{Training and Inference Efficiency} 

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{inftime_v2.pdf}
    \caption{Inference time per batch in microseconds} \label{fig:inf}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{ttime_v2.pdf}
    \caption{Training time for one epoch in seconds.}\label{tab:training}
  \end{subfigure}
  \caption{In (a) we show the inference time per batch on the electricity dataset. In (b) we show the corresponding training times for one epoch. In both the figures the y-axis is plotted in log-scale. Note that the PatchTST model ran out of GPU memory for look-back $L \geq 1440$.}
  \label{fig:timings}
\end{figure}

In the previous section we have seen that \ours~outperforms all methods except PatchTST by a large margin while it performs better or comparable to PatchTST in all datasets except Weather. Next, we would like to demonstrate that \ours~is much more efficient than PatchTST in terms of both training and inference times.

Firstly, we would like to note that inference scales as $\tilde{O}(n_eh^2 + hL)$ for the encoder in \ours, where $n_e$ is the number of layers in the encoder, $h$ is the hidden size of the internal layers and $L$ is the look-back. On the other hand, inference in PatchTST encoder would scale as $\tilde{O}(K n_aL^2/P^2)$, where $K$ is the size of the key in self-attention, $P$ is the patch-size and $n_a$ is the number of attention layers. The quadratic scaling in $L$ can be prohibitive for very long contexts. Also, the amount of memory required is quadratic in $L$ for the vanilla Transformer architecture used in PatchTST~\footnote{Note that sub-quadratic memory attention mechanism does exist~\citep{dao2022flashattention} but has not been used in PatchTST. The quadratic computation seems to be unavoidable since approximations like Pyraformer seem to perform much worse than PatchTST.}.

We demonstrate these effects in practice in Figure~\ref{fig:timings}. 
For the comparison to be fair we carry out the experiment using the data loader in the Autoformer~\citep{wu2021autoformer} code base that was used in all subsequent papers. We use the electricity dataset with batch size $8$, that is each batch has a shape of $8 \times 321 \times L$ because the electricity dataset has $321$ time-series. We report the inference time for one batch and the training time for one epoch for \ours~and PatchTST as the look-back ($L$) is increased from $192$ to $2880$. We can see that there is an order of magnitude difference in inference time. The differences in training time is even more stark with PatchTST being much more sensitive to the look-back size. Further PatchTST runs out of memory for $L \geq 1440$. Thus our model achieves better or similar accuracy while being much more computation and memory efficient. All the experiments in this section were performed using a single NVIDIA T4 GPU on the same machine with 64 core Intel(R) Xeon(R) CPU @ 2.30GHz.

\subsection{Ablation Study}
\label{sec:ablation}
\paragraph{Temporal Decoder.} The use of the temporal decoder for adaptation to future covariates is perhaps one of the most interesting components of our model. Therefore in this section we would like to show the usefulness of that component with a semi-synthetic example using the electricity dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{elec_hv3.png}
    \caption{We plot the actuals vs the predictions from TiDE with and without the temporal decoder after just one epoch of training on the modified electricity dataset. The red part of the horizontal line indicates an event of Type A occuring.}
    \label{fig:ablation}
\end{figure}

We derive a new dataset from the electricity dataset, where we add numerical features for two kinds of events. When an event of Type A occurs the value of a time-series is increased by a factor which is uniformly chosen between $[3, 3.2]$.  When an event of Type B occurs the value of a time-series is decreased by a factor which is uniformly chosen between $[2, 2.2]$. Only 80\% of the time-series are affected by these events and the time-series id's that fall in this bracket are chosen randomly. There are 4 numerical covariates that indicate Type A and 4 that indicate Type B. When Type A event occurs the Type A covariates are drawn from an isotropic Gaussian with mean $[1.0, 2.0, 2.0, 1.0]$ and variance $0.1$ for every coordinate. On the other hand in the absence of Type A events Type A covariates are drawn from an isotropic Gaussian with mean $[0.0, 0.0, 0.0, 0.0]$. Thus these covariates serve as noisy indicators of the event. We follow a similar pattern for Type B events and covariates but with different means. Whenever these events occur they occur for 24 contiguous hours.

In order to showcase that the use of the temporal decoder can learn such patterns derived from the covariates faster, we plot the predictions from the TiDE model with and without the temporal decoder after just one epoch of training on the modified electricity dataset in Figure~\ref{fig:ablation}. The red part of the horizontal line indicates the occurrence of Type A events. We can see that the use of temporal decoder has a slight advantage during that time-period. But more importantly, in the time instances following the event the model without the temporal decoder is thrown off possibly because it has not yet readjusted its past to what it should have been without the event. This effect is negligible in the model which uses the temporal decoder, even after just one epoch of training.

\begin{figure}[h]
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{context_ablatation.png}
\caption{\rev{We plot the Test MSE on the traffic dataset as a function of different context sizes for three different horizon length tasks. Each plot is an average of 5 runs with the 2 standard error interval plotted.}}
\label{fig:context}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{cc|c|cc|cc}
			\cline{2-7}
			&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\ours} & \multicolumn{2}{c}{TiDE (no res.)} \\
			\cline{2-7}
			&\multirow{4}*{Electricity} & 96 & $\mathbf{0.132 \pm 0.003}$ &	$\mathbf{0.229 \pm 0.001}$ & $0.136 \pm 0.001$ & $0.235 \pm 0.002$ \\
            &\multicolumn{1}{c|}{}& 192 & $\mathbf{0.147 \pm 0.003}$ & $\mathbf{0.243 \pm 0.001}$ & $0.153 \pm 0.001$ & $0.253 \pm 0.001$  \\
            &\multicolumn{1}{c|}{} & 336 & $\mathbf{0.161 \pm 0.001}$ & $\mathbf{0.261 \pm 0.002}$ & $0.172 \pm 0.003$ & $0.274 \pm 0.002$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.196 \pm 0.002$	& $0.294 \pm 0.001$ & $0.196 \pm 0.003$ & $0.295 \pm 0.002$  \\
			\cline{2-7}
		\end{tabular}}
\caption{\rev{We perform an ablation study by presenting results from our model without any residual connections, on the electricity benchmark. We average over 5 runs for all the numbers and present the corresponding standard errors.}}
\label{tab:nores}
\end{minipage}
\end{figure}


\rev{
\paragraph{Context Size.} In Figure~\ref{fig:context} we study the dependence of prediction accuracy with context size on the Traffic dataset. We plot the results for multiple horizon length tasks and show that in all cases our methods performance becomes better with increasing context size as expected. This is contrast to some of the transformer based methods like Fedformer, Informer as shown in ~\citep{zeng2022transformers}.

\paragraph{Residual Connections.} In Table~\ref{tab:nores} we perform an ablation study of the residual connections on the electricity dataset. In the model dubbed TiDE (no res) we remove all the residual connections including the ones in the residual block as well as the global linear residual connection. In horizons 96-336 we see a statistically significant drop in performance without the residual connections.
}

\section{Conclusion}
\label{sec:conclusion}
We propose a simple MLP based encoder decoder model that matches or supersedes the performance of prior neural network baselines on popular long-term forecasting benchmarks. At the same time, our model is 5-10x faster than the best Transformer based baselines. Our study shows that self attention might not be necessary to learn the periodicity and trend patterns at least for these long-term forecasting benchmarks. 

Our theoretical analysis partly explains why this could be the case by proving that linear models can achieve near optimal rate when the ground truth is generated from a linear dynamical system. However, for future work it would be interesting to rigorously analyze MLPs and Transformer \rev{(including non-linearity)} under some simple mathematical model for time-series data and potentially quantify the (dis)advantages of these architectures for different levels of seasonality and trends. \rev{Also, note that transformers are generally more parameter efficient than MLPs while being much more memory and compute intensive. This could be a limitation while training extremely large scale pre-trained models but the benefits of that line of work are not immediately clear in time-series forecasting and is beyond the scope of this work.}


\FloatBarrier
\bibliographystyle{plainnat}
\bibliography{references, rose-ref}

\clearpage
\appendix

\section{Theoretical Analysis under Linear Dynamical Systems}
\label{sec:theory}
To gain insights into our design, we will now analyze the simplest linear analogue of our model. In our model if all the residual connections are active and the size of the encoding is greater than or equal to the length of the horizon, then it reduces to a linear map from the context and the covariates to the horizon. We study this version for the case when the data is generated from a Linear Dynamical System (LDS), that has been a popular mathematical model for systems evolving with time~\citep{kalman1963mathematical}. We will prove that under some conditions, a linear model that maps the past and the covariates of a finite context to the future can be optimal for prediction in a LDS.

\subsection{Theoretical Results}
\label{sec:theoryp1}
We formally define a linear dynamical system (LDS) as follows,
\begin{definition}
A linear dynamical system (LDS) is a map from a sequence of input vectors $x_1, \ldots, x_T \in \reals^n$ to output (response) vectors $y_1, \ldots, y_T \in \reals^m$ of the form
\begin{align}
h_{t+1} &= A h_t + B x_t + \eta_t \\
y_t &= C h_t + D x_t + \xi_t,
\end{align}
where $h_0, \ldots, h_T \in \reals^d$ is a sequence of hidden states, $A,B,C,D$ are matrices of appropriate dimension, and $\eta_t \in \reals^d, \xi_t \in \reals^m$ are (possibly stochastic) noise vectors. The $x_t$'s can be thought of as covariates for the time-series $y_t$.
\end{definition}
Given an LDS with parameter $\Theta = (A,B,C,D,h_0 = 0)$, we define the LDS predictor as follows,
\begin{definition}[LDS predictor]
\begin{align}
\hat y_t &= y_{t-1} + (CB + D)x_t - Dx_{t-1} + \sum_{i=1}^{t-1} C(A^i - A^{i-1})Bx_{t-i}
\end{align}
\end{definition}

For a fixed sequence length $T$, we consider a roll-out of the system $\{ (x_t, y_t) \}_{t=1}^T$ to be a single example. In particular, we define $(X = (x_1, y_1, \ldots, x_{T-1}, y_{T-1}, x_T), Y = y_T)$ where $X$ contains be the full information that is available for the model to predict observation $y_T$. Trained on $N$ i.i.d. samples $\{(X_i, Y_i)\}$, the goal of the model is to predict $Y_i$ from $X_i$. 

We assume the samples satisfy $\norm{x_t}_2, \norm{y_{t}}_2 \leq c$ for a constant $c$. We compete against the class of functions in $\Hcal$ restricted to contain LDSs with parameters $\Theta = (A,B,C,D,h_0 = 0)$ such that $0 \preccurlyeq A \preccurlyeq \gamma \cdot I$ where $\gamma<1$ is a constant and $\norm{B}_F, \norm{C}_F, \norm{D}_F\le c$ for an absolute constant $c$. For error metrics, we consider squared loss function $\ell_{X,Y} = \norm{h(x_1, y_1, \ldots, x_{T-1}, y_{T-1}, x_T)-y_T}^2$. For an empirical sample set $S$, let $\ell_S(h) = \frac{1}{|S|} \sum_{(X,Y) \in S} \ell_{X,Y}(h)$. Similarly, for a distribution $\Dcal$, let $\ell_\Dcal(h) = \E_{(X,Y) \sim \Dcal} [\ell_{X,Y}(h)]$.

Now we define our auto-regressive hypothesis class. Let $\tilde X\in \reals^{(k+1)n+m}$ be the concatenated vector
\[\bmat{ x_{t-k} & x_{t-k+1} & \cdots & x_{t-1} & \; x_t \; & \; y_{t-1} },\] and $f_M(\tilde X) = M \tilde X, M\in \reals^{m\times (k+1)n+m}$. Our hypothesis class is defined as $\hat\Hcal =  \{f_M | \norm{M}_F\le O(1)\}$. 

We are ready to state our main theorem.

\begin{proposition}[Generalization bound of learning LDS with auto-regressive algorithm]
\label{thm:main}
Choose any $\eps > 0$.
Let $S = \{ (X_i, Y_i) \}_{i=1}^N$ be a set of i.i.d. training samples from a distribution $\Dcal$. Let $\hat h \defeq \argmin_{h \in \hat\Hcal} \ell_S(h)$ with a choice of $k = \Theta(\log(1/\eps) )$.
Let $h^* \defeq \argmin_{h^* \in \Hcal} \ell_\Dcal(h)$ be the loss minimizer over the set of $LDS$ predictors. Then,
with probability at least $1 - \delta$, it holds that
\[ \ell_\Dcal( \hat h ) - \min_{h \in \Hcal} \ell_\Dcal( h )
\leq \eps + \frac{ O\pa{\log(1/ \eps)\sqrt{\log 1/\delta} } }{\sqrt{N}}.
\]
\end{proposition}

The above result shows that the linear autoregressive predictor with a short look-back window is competitive against the best LDS predictor where the largest eigenvalue of the transition matrix $A$ is strictly smaller than $1$. In Appendix~\ref{sec:sims} we compare linear models with LSTM's and Transformers on long-term forecasting tasks on data generated by LDS, thus validating our theoretical results.


\begin{proof}[Proof of Proposition~\ref{thm:main}]
The proof proceeds as follows: First we show that an auto-regressive model with look by window length $\Omega(\log(1/\eps))$ can approximate an LDS with error $\eps$. Second, we prove a simple bound on the Rademacher complexity of the class of auto-regressive model we considered, which implies a generalization bound of our algorithm. Combining both results yields our main result. 
\begin{proposition}[Approximating LDS with auto-regressive model]
\label{thm:main-appx-relaxation}
Let $\hat{y}_T$ be the predictions made by an LDS $\Theta = (A,B,C,D,h_0 = 0)$. Then, for any $\eps > 0$, with a choice of $k = \Omega \rbr{\log (1/\eps)}$, there exists an $M_\Theta \in \RR^{m \times (k+1)n+m}$ such that
\[ \norm{M_{\Theta} \tilde X - y_T}^2 \leq \norm{\hat y_T - y_T}^2 + \eps. \]
\end{proposition}

\begin{proof}
This proposition is an analog of Theorem 3 in~\citep{hazan2017learning}. We construct $M_{\Theta}$ as the block matrix
\[\bmat{ M^{(k-1)} & M^{(k-2)} & \cdots & M^{(1)} & M^{(x')} & M^{(x)} & M^{(y)} },\]
where the blocks' dimensions are chosen to align with $\tilde X_t$, the concatenated vector
\[\bmat{ x_{t-k} & x_{t-k+1} & \cdots & x_{t-1} & \; x_t \; & \; y_{t-1} },\]
so that the prediction is the block matrix-vector product
\[ M_{\Theta} \tilde X_t = \sum_{j=1}^{k-1} M^{(j)} x_{t-1-j} + M^{(x')} x_{t-1} + M^{(x)} x_t + M^{(y)} y_{t-1}. \]

Our construction is as follows:
\begin{itemize}
\item $M^{(j)} = C(A^{j+1}-A^{j})B$, for each $1 \leq j \leq k-1$.
\item $M^{(x')} = C(A-I)B - D,\quad M^{(x)} = CB + D,\quad M^{(y)} = I_{m\times m}$.
\end{itemize}
The prediction of the LDS is by definition
\begin{align*}
\hat y_t &= y_{t-1} + (CB + D)x_t - Dx_{t-1} + \sum_{i=1}^{t-1} C(A^i - A^{i-1})Bx_{t-i}
\end{align*}
We conclude that
$$
\hat{y}_t = M_{\Theta} \tilde X_t + \sum_{i=k+2}^{T} C(A^i-A^{i-1})Bx_{t-i}.
$$
This implies 
\begin{align*}
&\norm{M_{\Theta} \tilde X_t - y_t}^2\\
\leq& \norm{\hat y_t - y_t}^2 + 2\norm{\hat y_t - y_t}\norm{\sum_{i=k+2}^{T} C(A^i-A^{i-1})Bx_{t-i}} +  \norm{\sum_{i=k+2}^{T} C(A^i-A^{i-1})Bx_{t-i}}^2\\
\le& \norm{\hat y_t - y_t}^2 + O(\frac{\gamma^{k+2}+\gamma^{2k+4}}{1-\gamma}) \le \norm{\hat y_t - y_t} + \eps
\end{align*}
\end{proof}
The empirical Rademacher complexity of $\hat \Hcal$ on $N$ samples, with this restriction that $\norm{M}=O(1)$, satisfies
\[ \Rcal_N(\hat \Hcal) \leq O\pa{ \frac{ 1}{\sqrt{N}} }. \]
It's easy to check that $\norm{M_{\Theta}}_F\le O\pa{\sqrt{\sum_{i=0}^k\gamma^i}} = O(1)$, which falls in the feasible set of our algorithm. The maximum loss $\ell_{\max}$ of the hypothesis in model class $\hat{\Hcal}$ is bounded by $O(k)$. The lipschitz constant of loss function in the matrix $M$ is 
$G_\mathrm{max} \le \norm{M \tilde X_t - y_t}_2 \cdot \norm{\tilde X_t}_2 \leq O(k)$


With all of these facts in hand, a standard Rademacher complexity-dependent generalization bound holds in the improper hypothesis class $\hat\Hcal$ (see, e.g. \citep{bartlett2002rademacher}):
\begin{lemma}[Generalization via Rademacher complexity]
\label{lem:rademacher}
With probability at least $1 - \delta$, it holds that
\begin{align*}
\ell_\Dcal( \hat h ) - \ell_\Dcal( \hat h^* ) &\leq
G_\mathrm{max} \Rcal_N(\hat\Hcal)
+ \ell_\mathrm{max} \sqrt{ \frac{8 \ln 2/\delta}{N} }
\end{align*}
\end{lemma}
With the stated choice of $k$,
an upper bound for the RHS of Lemma~\ref{lem:rademacher} is
\[\frac{ O\pa{ \log(1 / \eps)\sqrt{\log 1/\delta} } }{\sqrt{N}}.\]
Combining this with the approximation result (Proposition~\ref{thm:main-appx-relaxation})
yields the theorem.
\end{proof}

\subsection{Experimental Results on Synthetic Datasets}
\label{sec:sims}
{\bf Dataset.} We evaluate several models on a synthetic dataset generated from a linear dynamical system. The transition matrix $A$ is a $30\times 30$ dimension Wishart random matrix normalized to have operator norm equals $0.95$. The noise $\eta_t$ in the state transition follows from a $30$-dimensional Gaussian distribution. The input at each time-step $x_t$ follows from a $5$-dimensional standard Gaussian distribution, which is observable to the model. Finally, We add seasonality of $6$ different periodicity by adding cosine signal as the input to the linear dynamical system, but hidden from the prediction model. We generate $4$ different time series which shares the same model parameter, input $x_t$ and seasonality input, with the only difference coming from the randomness in the state transition. 
We set look-back window to be length $320$, and horizon also to length $320$. For each time-series, we use the first $1640$ steps for training, next $740$ steps for validation, and the final $740$ steps for testing, which results in $4000$ examples for training, $400$ examples for validation, and $400$ examples for testing. 

{\bf Baselines and Setup.}
We evaluate three models on our synthetic dataset: linear, long short-term memory (LSTM) and Transformer. Our linear model is a direct linear map between the history in look-back window and future. We use a one layer LSTM with dimension 128. For Transformer, we use a two layer self-attention layers with dimension 128, combined with a one hidden layer feed-forward network with 128 hidden units.

{\bf Results.} We present Mean Squared Error (MSE) for all models in Table~\ref{tab:linear_result}. For all models, we report the mean and standard deviation out of 3 independent runs for each setting. The bold-faced numbers are from the best model or within statistical significance of the best model in terms of two standard error intervals. We also plot the actuals (ground truth) vs the predictions from Linear, LSTM and Transformer models. We see that Transformer captures the lower frequency seasonality of the time-series but seems to not be able to leverage the inputs/covariates to predict the short term variation of the value, LSTM seems to not capture the trend/seasonality correctly, while Linear model's prediction is the closest to the truth, which matches the metric result in Table~\ref{tab:linear_result}.
\begin{table}[ht!]
\centering
\begin{tabular}{l|c}
\toprule
Model & MSE \\
\midrule
Linear & $\mathbf{0.510\pm0.001}$\\
LSTM & $1.455\pm0.455$  \\
Transformer & $0.731\pm0.041$  \\
\bottomrule
\end{tabular}
\caption{Mean Squared Error (MSE) of Linear, LSTM and Transformer models on synthetic time-series.}
\label{tab:linear_result}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{linear_pred_sample.png}
    \caption{We plot the actuals vs the predictions from Linear, LSTM and Transformer models.}
    \label{fig:linear_pred_sample}
\end{figure}



\section{More Experimental Details}
\label{app:moreexp}
\subsection{Additional Experiments}
\rev{
\paragraph{Comparison against S4.} In Table~\ref{tab:new} we present the results of our model along side that of the S4 model~\citep{guefficiently}. The numbers of S4 are directly taken from Table-14 of the original paper. It can be seen that TiDE vastly outperforms the S4 model on the time-series benchmarks.

\paragraph{M5 Forecasting.} We will now provide more details about the M5 forecasting experiments. We follow the setup used in the notebook linked in Section~\ref{sec:exp} that was released by the authors of~\citep{alexandrov2020gluonts}. The list of dynamic features include date derived features, promotion features like snap\textunderscore CA, snap\textunderscore TX, snap\textunderscore WI, even\textunderscore type\textunderscore 1 and event\textunderscore type\textunderscore 2. It also includes static attributes like category\textunderscore id, store\textunderscore id, department\textunderscore id and item\textunderscore id. The categorical features are embedded into learnable embeddings. 

DeepAR~\citep{salinas2020deepar} has suggested using the zero-inflated negative binomial loss likelihood as the loss function for sparse count data in the dataset. Therefore we use this loss function for our model and the DeepAR model. All models are trained to a maximum of 100 epochs with an early stopping patience of 5.
}

\begin{table*}
\centering
		\begin{tabular}{cc|c|cc}
			\cline{2-5}
			&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c}{\ours} \\
			\cline{2-5}
			&\multirow{4}*{Electricity} & 96 & ${0.132 \pm 0.003}$ &	${0.229 \pm 0.001}$  \\
            &\multicolumn{1}{c|}{}& 192 & ${0.147 \pm 0.003}$ & ${0.243 \pm 0.001}$  \\
            &\multicolumn{1}{c|}{} & 336 & ${0.161 \pm 0.001}$ & ${0.261 \pm 0.002}$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.196 \pm 0.002$	& $0.294 \pm 0.001$ \\
			\cline{2-5}
			&\multirow{4}*{Traffic} & 96 & ${0.336 \pm 0.001}$ &	${0.253 \pm 0.001}$  \\
            &\multicolumn{1}{c|}{}& 192 & ${0.346 \pm 0.001}$ & ${0.257 \pm 0.002}$  \\
            &\multicolumn{1}{c|}{} & 336 & ${0.355 \pm 0.001}$ & ${0.260 \pm 0.001}$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.386 \pm 0.002$	& $0.273 \pm 0.0005$ \\
			\cline{2-5}
			&\multirow{4}*{Weather} & 96 & ${0.166 \pm 0.0005}$ &	${0.222 \pm 0.0005}$  \\
            &\multicolumn{1}{c|}{}& 192 & ${0.209 \pm 0.002}$ & ${0.263 \pm 0.0001}$  \\
            &\multicolumn{1}{c|}{} & 336 & ${0.254 \pm 0.002}$ & ${0.301 \pm 0.0001}$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.313 \pm 0.001$	& $0.340 \pm 0.0002$ \\
			\cline{2-5}
			&\multirow{4}*{ETTm2} & 96 & ${0.161 \pm 0.0002}$ &	${0.251 \pm 0.0003}$  \\
            &\multicolumn{1}{c|}{}& 192 & ${0.215 \pm 0.0001}$ & ${0.289 \pm 0.0004}$  \\
            &\multicolumn{1}{c|}{} & 336 & ${0.267 \pm 0.0001}$ & ${0.326 \pm 0.0002}$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.352 \pm 0.0002$	& $0.383 \pm 0.0002$ \\
			\cline{2-5}
			&\multirow{4}*{ETTm1} & 96 & ${0.306 \pm 0.0001}$ &	${0.349 \pm 0.0002}$  \\
            &\multicolumn{1}{c|}{}& 192 & ${0.335 \pm 0.0002}$ & ${0.366 \pm 0.0002}$  \\
            &\multicolumn{1}{c|}{} & 336 & ${0.364 \pm 0.0004}$ & ${0.384 \pm 0.0001}$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.413 \pm 0.0001$	& $0.413 \pm 0.0001$ \\
            \cline{2-5}
            &\multirow{4}*{ETTh1} & 96 & ${0.375 \pm 0.0003}$ &	${0.398 \pm 0.0002}$  \\
            &\multicolumn{1}{c|}{}& 192 & ${0.412 \pm 0.0002}$ & ${0.422 \pm 0.0001}$  \\
            &\multicolumn{1}{c|}{} & 336 & ${0.435 \pm 0.0001}$ & ${0.433 \pm 0.0001}$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.454 \pm 0.0003$	& $0.465 \pm 0.0001$ \\
            \cline{2-5}
            &\multirow{4}*{ETTh2} & 96 & ${0.270 \pm 0.0005}$ &	${0.336 \pm 0.0007}$  \\
            &\multicolumn{1}{c|}{}& 192 & ${0.332 \pm 0.001}$ & ${0.380 \pm 0.002}$  \\
            &\multicolumn{1}{c|}{} & 336 & ${0.360 \pm 0.001}$ & ${0.407 \pm 0.001}$ \\
            &\multicolumn{1}{c|}{}& 720 & $0.419 \pm 0.005$	& $0.451 \pm 0.002$ \\
            \cline{2-5}
		\end{tabular}
\caption{\rev{We provide standard error bars for our method over 5 independent runs.}}
\label{tab:conf}
\end{table*}

\label{app:exp_new}
\begin{table*}[t]
	\centering
	
		\begin{tabular}{cc|c|cc|cc}
			\cline{2-7}
			&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{\ours} & \multicolumn{2}{c}{S4} \\
			\cline{2-7}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE\\
			\cline{2-7}
			&\multirow{2}*{Weather} & 336 & 0.254 & 0.301 & 0.531 & 0.539 \\
            &\multicolumn{1}{c|}{}& 720 & 0.313 & 0.340 & 0.578 & 0.578  \\
			\cline{2-7}
			&\multirow{2}*{ETTh1} & 336 & 0.435 & 0.433 & 1.407 & 0.910 \\
            &\multicolumn{1}{c|}{}& 720 & 0.454 & 0.465 & 1.162 & 0.842  \\
			\cline{2-7}
			&\multirow{2}*{ETTh2} & 336 & 0.360 & 0.407 & 0.531 & 0.539 \\
            &\multicolumn{1}{c|}{}& 720 & 0.419 & 0.451 & 2.650 & 1.340  \\
			\cline{2-7}
			&\multirow{2}*{Electricity} & 336 & 0.161 & 0.261 & 0.531 & 0.539 \\
            &\multicolumn{1}{c|}{}& 720 & 0.196 & 0.294 & 0.578 & 0.578  \\
			\cline{2-7}
		\end{tabular}
	\caption[]{\rev{We benchmark our model's performance against that of S4. The S4 results are taken from Table~14 of the original paper~\citep{guefficiently}.}}
	\label{tab:new}
\end{table*}


\subsection{Data Loader}
\label{app:data}
Each training batch consists of a look-back $\bY[\cB, t-L:t-1]$ and a horizon $\bY[\cB, t:t+H-1]$. Here, $t$ can range from $L+1$ to $H$ steps before the end of the training set. $\cB$ denotes the indices of the time-series in the batch and the \texttt{batchSize} can be set as a hyper-parameter. When \texttt{batchSize} is greater than $N$, all the time-series are loaded in a batch.

We also load time derived features as covariates. The time-stamps corresponding to time-indices $t - L : t+H-1$ are converted to periodic features like minute of the hour, hour of the day, day of the week etc normalized to the scale $[-0.5, 0.5]$ as done in GluonTS~\citep{alexandrov2020gluonts}. In total we have 8 such features, many of which can stay constant depending on the granularity of the dataset.

\subsection{Hyperparameters}
\label{app:params}

Recall that in Section~\ref{sec:model}, we had the following hyper-parameters \texttt{temporalWidth}, \texttt{hiddenSize}, \texttt{numEncoderLayers}, \texttt{numDecoderLayers}, \texttt{decoderOutputDim} and \texttt{temporalDecoderHidden}. We also have hyper-parameters \texttt{layerNorm} and \texttt{dropoutLevel} that denote the global model level layer norm on/off and the probability of dropout. We also tune the maximum \texttt{learningRate} which is the input to a cosine decay learning rate schedule. In all our experiments \texttt{batchSize} is fixed to $512$ and \texttt{temporalWidth} is fixed to $4$. We also tune whether reversible instance normalization~\citep{kim2021reversible} is turned on or off. The tuning range of the hparams are provided in Table~\ref{tab:hptuning}. We use the validation loss to tune the hyper-parameters per dataset.



\begin{table}[ht]
\centering
\begin{tabular}{c|c}
\toprule
Parameter & Range \\
\midrule
\texttt{hiddenSize} &  [256, 512, 1024]\\
\hline
\texttt{numEncoderLayers} & [1, 2, 3] \\
\hline
\texttt{numDecoderLayers} & [1, 2, 3] \\
\hline
\texttt{decoderOutputDim}& [4, 8, 16, 32] \\
\hline
\texttt{temporalDecoderHidden} & [32, 64, 128] \\
\hline
\texttt{dropoutLevel} & [0.0, 0.1, 0.2, 0.3, 0.5] \\
\hline
\texttt{layerNorm} & [True, False] \\
\hline
\texttt{learningRate} & Log-scale in [1e-5, 1e-2]\\
\hline
\texttt{revIn} & [True, False] \\
\bottomrule
\end{tabular}
\caption{Ranges of different hyper-paramaters}
\label{tab:hptuning}
\end{table}

We report the specific hyper-parameters chosen for each dataset in Table~\ref{tab:allhparams}.

\begin{table*}[!ht]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    Dataset & \texttt{hiddenSize} & \texttt{numEncoderLayers} & \texttt{numDecoderLayers} & \texttt{decoderOutputDim} & \texttt{temporalDecoderHidden} & \texttt{dropoutLevel} & \texttt{layerNorm} & \texttt{learningRate} & \texttt{revIn} \\
    \midrule
    Traffic & 256 & 1 & 1 & 16 & 64 & 0.3 & False & 6.55e-5 & True \\ \hline
    Electricity & 1024 & 2 & 2 & 8 & 64 & 0.5 & True & 9.99e-4 & False \\ \hline
    ETTm1 & 1024 & 1 & 1 & 8 & 128 & 0.5 & True & 8.39e-5 & False \\ \hline
    ETTm2 & 512 & 2 & 2 & 16 & 128 & 0.0 & True & 2.52e-4 & True \\ \hline
    ETTh1 & 256 & 2 & 2 & 8 & 128 & 0.3 & True & 3.82e-5 & True \\ \hline
    ETTh2 & 512 & 2 & 2 & 32 & 16 & 0.2 & True & 2.24e-4 & True \\ \hline
    Weather & 512 & 1 & 1 & 8 & 16 & 0.0 & True & 3.01e-5 & False \\
    \bottomrule
  \end{tabular}
  }
  \caption{The hyper-parameters for different experimental settings.}\label{tab:allhparams}
\end{table*}

\end{document}
