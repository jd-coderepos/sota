\ifmuteappendixcite
\renewcommand{\cite}[1]{}
\fi

\ifstandalonesupplement



\setcounter{tocdepth}{2}
{\hypersetup{linkcolor=black}
\tableofcontents
}
\clearpage

\else





\fi

We summary the contents of the appendix as follows. Appendix \ref{app:bench-details} describes additional details of our meta-benchmark, including discussion on the definition, pseudo ground-truth, simulation, dataset and instantiations for novel hardware and task. Appendix \ref{app:solution-details} provides additional details of our proposed solutions, including scheduling, tracking and forecasting. Finally, Appendix \ref{app:addmethods} includes additional baselines for a more thorough evaluation.

\section{Benchmark Details}
\label{app:bench-details}



\subsection{Additional Discussion on the Benchmark Definition}
\label{app:benchdef}

In 
\ifstandalonesupplement
    Section 3.1 (main text),
\else
    Section~\ref{sec:formaldef},
\fi
we defined our benchmark as evaluation over a discrete set of frames. One might point out that a continuous definition is more consistent with the notion of estimating the state of the world at all time instants for streaming perception. First, we note that it is possible to define a continuous-time counterpart, where the ground truth can be obtained via polynomial interpolation and the algorithm prediction can be represented as a function of time (e.g., simply derived from extrapolating the discrete output). Also in
\ifstandalonesupplement
    Eq 4 (main text),
\else
    Eq~\ref{eq:eval},
\fi
the aggregation function (implicit in ) could be integration. However, our choice of a discrete definition is mainly for two reasons: (1) we believe a high-frame-rate data stream is able to approximate the continuous evaluation; (2) most existing single-frame metrics (, e.g., average-precision) is defined with a discrete set of input and we prefer that our streaming metric is compatible with these existing metrics. 

\subsection{Pseudo Ground Truth}
\label{app:pseudo-gt}
We use manually obtained ground-truth for bounding-box-based object detection. As we point out in the main text, one could make use of pseudo ground truth by simply running an (expensive but accurate) off-line detector to generate detections that could be used to evaluate on-line streaming detectors.

Here, we analyze the effectiveness of pseudo ground truth detection as a proxy for ground-truth.
We adopt the state-of-the-art detector --- Hybrid Task Cascade (HTC)~\cite{chen2019hybrid} for computing the offline pseudo ground truth. As shown in 
\ifstandalonesupplement
    Table 1 (main text),
\else
    Table~\ref{tab:det},
\fi
this offline detector dramatically outperforms all real-time streaming methods by a large margin. As shown in the main text, pseudo-streaming AP correlates extraordinarily well with ground-truth-streaming AP, with a normalized correlation coefficient of 0.9925. This suggests that pseudo ground truth can be used to rank streaming perception algorithms. 

We emphasize that since we have constructed Argoverse-HD by deliberately annotating high frame rate  bounding boxes, {\em we use real ground truth for evaluating detection performance}. However, obtaining such high-frame-rate annotations for instance segmentation is expensive. Hence we make use of pseudo ground-truth instance masks (provided by HTC) to benchmark streaming instance segmentation (Section~\ref{app:instseg}).



\subsection{Simulation}
\label{app:simulation}



In true hardware-in-the-loop benchmarking, the output timestamp  is simply the wall-clock time at which an algorithm produces an output. While we hold this as the gold-standard, one can dramatically simplify benchmarking by making use of simulation, where  is computed using runtimes of different modules. For example,  for a single-frame detector on a single GPU can be simulated by adding its runtime to the time when it starts processing a frame. Complicated perception stacks require considering runtimes of all modules (we model those that contribute  1 ms) in order to accurately simulate timestamps.

\pbf{Modeling runtime distribution} Existing latency analysis \cite{redmon2016you,Liu2016SSDSS,lin2017focal} usually reports only the mean runtime of an algorithm. However, empirical runtimes are in fact {\em stochastic} (Fig.~\ref{fig:runtime}), due to the underlying operating system scheduling and even due to the algorithm itself (e.g., proposal-based detectors often take longer when processing a scene with many objects). Because scene-complexity is often correlated across time, runtimes will also be correlated (a long runtime for a given frame may also hold for the next frame).


We performed a statistical analysis of runtimes, and found that a {\em marginal} empirical distribution to work well.
We first run the algorithm over the entire dataset to get the empirical distribution of runtimes. At test time, we randomly sample a runtime when needed from the empirical distribution, without considering the correlation across time. Empirically, we found that the results (streaming AP) from a simulated run is within the variance of a real run.

\begin{figure}[]
\centering
\includegraphics[width=0.7\linewidth]{Fig/runtime-hist.pdf}

\caption{Runtime distribution for an object detector. Note that runtime is not constant, and this variance needs to be modeled in a simulation. This plot is obtained by running RetinaNet (ResNet 50) \cite{lin2017focal} on Argoverse 1.1 \cite{Argoverse} with input scale 0.5.}
\label{fig:runtime}
\end{figure}

\pbf{Simulation for non-existent hardware/algorithm}

Through simulation, our evaluation protocol does not directly depend on hardware, but on a collection of runtime distributions for different modules (known as a {\em runtime profile}). One thus has the freedom to alter the distributions. For example, we can simulate a faster algorithm simply by scaling down the runtime profile.
\ifstandalonesupplement
    Table 3 (main text),
\else
    Table~\ref{tab:track},
\fi
uses simulation to evaluate the streaming performance of a non-existent tracker that runs twice as fast as the actual implementation on-hand. The reduced runtime could have arisen from better hardware; one can run the benchmark on a Geforce GTX 1080 Ti GPU and simulate the performance on a Tesla V100 GPU. We find that Tesla V100 makes our detectors run 16\% faster, implying we can scale runtime profiles accordingly. For example, Mask R-CNN R50 @ s0.5 produces a simulated-streaming AP of 12.652 while the real-streaming AP (on a V100) is 12.645, suggesting that effectivness of simulated benchmarking. 


\pbf{Infinite GPUs}

In simulation, we are not restricted by the number of physical GPUs present in a system. Therefore, we are able to perform analysis in the infinite GPU setting. In this setting, each detector or visual tracker runs on a different device without any interference with each other. Equivalently, we run a new GPU job on an existing device as long as it is idle. As a result, the simulation also provides information on how many GPUs are required for a particular infinite GPU experiment in practice  (\ie, the maximum number of concurrent jobs). We summarize the number of GPUs required for the experiments in the main text in Table~\ref{tab:ifiniteGPU}. This implies that our streaming benchmark can be used to inform hardware design of {\em future} robotic platforms.

\begin{table}[]
\small
\centering
\caption{Summary of the experiments in the infinite GPU settings (in the main text) and the number of GPUs needed in practice to achieve this performance (\ie, the maximum number of concurrent jobs). This suggest that our simulation can also identify the optimal hardware configuration
}
\begin{tabular}{lc}
\toprule
Method   &  of GPUs \\
\midrule
Det
\ifstandalonesupplement
    (Table 1,
\else
    (Table~\ref{tab:det},
\fi
 row 8) & 4 \\
Det + Associate + Forecast
\ifstandalonesupplement
    (Table 2,
\else
    (Table~\ref{tab:forecast},
\fi
 row 3) & 4 \\
Det + Visual Track
\ifstandalonesupplement
    (Table 3,
\else
    (Table~\ref{tab:track},
\fi
 row 4) & 9 \\
Det + Visual Track + Forecast
\ifstandalonesupplement
    (Table 3,
\else
    (Table~\ref{tab:track},
\fi
 row 5) & 9 \\
\bottomrule
\end{tabular}
\label{tab:ifiniteGPU} 
\end{table}

\pbf{Runtime-induced variance}
As mentioned in the previous section, runtime is stochastic and has a variance up to 11.1\% (standard deviation normalized by mean). Fortunately, such a variance does not transfer to the variance of our streaming metric. Empirically, we found that the variance of streaming AP of different runs (by varying the random seed) is around 0.5\% for the same algorithm. In comparison, independent training runs of Mask R-CNN \cite{He2017MaskR} on MS COCO \cite{lin2014microsoft} with the {\em same random seed}
yield a variance of 0.3\% on the AP (cudnn back-propagation is stochastic by default) \cite{Li2020BudgetTrain}. Since the stochastic noise of streaming evaluation is at the same scale as CNN training, we ignore runtime-induced variance for our evaluation.

\subsection{Dataset Annotation and Comparison}
\label{app:datasetcompare}

Based on the publicly available video dataset \href{https://www.argoverse.org/}{Argoverse 1.1} \cite{Argoverse}, we build our dataset with high-frame-rate annotations for streaming evaluation --- Argoverse-HD (High-frame-rate Detection). One key feature is that the annotation follows MS COCO \cite{lin2014microsoft} standards, thus allowing direct evaluation of COCO pre-trained models on this self-driving vehicle dataset. The annotation is done at 30 FPS without any interpolation used. Unlike some self-driving vehicle datasets where only cars on the road are annotated \cite{Voigtlaender19CVPR_MOTS}, we also annotate background objects since they can potentially enter the drivable area. Of course, objects that are too small are omitted and our minimum size is  or  (based on the aspect ratio of the object). We outsourced the annotation job to \href{https://scale.com/}{Scale AI}. In Table~\ref{tab:datasetcompare}, we compare our annotation with existing datasets:
\href{https://detrac-db.rit.albany.edu/Tracking}{DETRAC} \cite{DETRAC:CoRR:WenDCLCQLYL15},
\href{http://www.vision.rwth-aachen.de/page/mots}{KITTI-MOTS} \cite{Voigtlaender19CVPR_MOTS},
\href{http://www.vision.rwth-aachen.de/page/mots}{MOTS} \cite{Voigtlaender19CVPR_MOTS},
\href{https://sites.google.com/site/daviddo0323/projects/uavdt}{UAVDT} \cite{Du2018TheUA},
\href{https://waymo.com/open/about/}{Waymo} \cite{sun2019scalability}, and
\href{https://youtube-vos.org/dataset/vis/}{Youtube-VIS} \cite{Yang2019VideoIS}.

\begin{table}[]
\small
\centering
\caption{Comparison of 2D video object detection datasets. For surveillance camera setups, the cameras are either stationary or have limited motion. For ego-vehicle setups, the scene dynamics evolve quickly, as (1) the ego-vehicle is traveling fast, and (2) other objects are much closer to the camera and thus have a higher speed in the image space. Our contributed dataset (annotation) is a high-frame-rate and high-resolution multi-class one compared to existing datasets
}
\label{tab:datasetcompare}
\addtolength{\tabcolsep}{0.2em}
\adjustbox{width=1\linewidth}{
\begin{tabular}{lcccccc}
\toprule
Name        & Camera Setup     & Image Res & Image FPS & Annot FPS & Classes & Boxes \\
\midrule
DETRAC      & Survelliance     &    & 30        & 6         & 4             & 1.21M       \\
KITTI-MOTS  & Ego-Vehicle      &   & 10        & 10        & 2             & 46K         \\
MOTS        & Generic          &  & 30        & 30        & 2             & 30K         \\
UAVDT       & UAV Survelliance &   & 30        & 30        & 1             & 842K        \\
Waymo       & Ego-Vehicle      &  & 10        & 10        & 4             & 11.8M       \\
Youtube-VIS & Generic          &   & 30        & 6         & 40            & 131K        \\
\midrule
Argoverse-HD (Ours)        & Ego-Vehicle      &  & 30        & 30        & 8             & 250K \\
\bottomrule
\end{tabular}
}
\addtolength{\tabcolsep}{-0.2em} 
\end{table}

\subsection{Experiment Settings}
\pbf{Platforms} The CPU used in our experiments is Xeon Gold 5120, and the GPU is Geforce GTX 1080 Ti. The software environment is PyTorch 1.1 with CUDA 10.0. 

\pbf{Timing} The setup which we time single-frame algorithms mimics the scenario in real-world applications. The offline pipeline involves several steps: loading data from the disk, image pre-processing, neural network forward pass, and result post-processing. Our timing excludes the first step of loading data from the disk. This step is mainly for dataset-based evaluation. In actual embodied applications, data come from sensors instead of disks. This is implemented by loading the entire video to the main memory before the evaluation starts. In summary, our timing (\eg, the last column of
\ifstandalonesupplement
    Table 1)
\else
    Table~\ref{tab:det})
\fi
 starts at the time when the algorithm receives the image in the main memory, and ends at the time when the results are available in the main memory (instead of in the GPU memory).
 
\subsection{Alternate Task: Instance Segmentation}
\label{app:instseg}

\begin{table*}[]
\small
\centering
\caption{Instance segmentation overhead compared with object detection. This table lists runtimes of several methods with and without the mask head, and their differences are the extra cost which one has to pay for instance segmentation. All numbers are milliseconds except the scale column and the last column. The average overhead is 17ms or 13\%}
\label{tab:mask-overhead}
\addtolength{\tabcolsep}{0.22em}
\begin{tabular}{cccccc}
\toprule
Method & Scale & w/o Mask & w/ Mask & Overhead & Overhead \\
\midrule
 & 0.2 & 34.3 & 41.4 & 7.1 & 21\% \\
 & 0.25 & 36.1 & 44.3 & 8.2 & 23\% \\
Mask R-CNN ResNet 50 & 0.5 & 56.7 & 65.6 & 8.8 & 16\% \\
 & 0.75 & 92.7 & 101.0 & 8.3 & 9\% \\
 & 1.0 & 139.6 & 147.7 & 8.1 & 6\% \\
\midrule
 & 0.2 & 38.4 & 46.4 & 7.9 & 21\% \\
 & 0.25 & 40.9 & 48.7 & 7.8 & 19\% \\
Mask R-CNN ResNet 101 & 0.5 & 68.8 & 76.4 & 7.6 & 11\% \\
 & 0.75 & 119.7 & 127.1 & 7.5 & 6\% \\
 & 1.0 & 183.8 & 190.8 & 7.0 & 4\% \\
\midrule
 & 0.2 & 60.9 & 66.0 & 5.1 & 8\% \\
 & 0.25 & 59.2 & 69.1 & 9.9 & 17\% \\
Cascade MRCNN ResNet 50 & 0.5 & 80.0 & 95.4 & 15.3 & 19\% \\
 & 0.75 & 118.1 & 133.8 & 15.7 & 13\% \\
 & 1.0 & 164.6 & 181.9 & 17.3 & 10\% \\
\midrule
 & 0.2 & 66.4 & 71.0 & 4.6 & 7\% \\
 & 0.25 & 65.4 & 75.2 & 9.7 & 15\% \\
Cascade MRCNN ResNet 101 & 0.5 & 92.2 & 106.6 & 14.4 & 16\% \\
 & 0.75 & 143.4 & 159.2 & 15.8 & 11\% \\
 & 1.0 & 208.2 & 225.1 & 16.9 & 8\% \\
\bottomrule
\end{tabular}
\addtolength{\tabcolsep}{-0.22em}
\end{table*}

In the main text, we propose a meta-benchmark and mention that it can be instantiated with different tasks. In this section, we include {\em full benchmark evaluation} for streaming instance segmentation.

Instance segmentation is a more fine-grained task than object detection. This creates challenges for streaming evaluation as annotation becomes more expensive and forecasting is not straight-forward. We address these two issues by leveraging pseudo ground truth and warping masks according to the forecasted bounding boxes.

Another issue which we observed is that off-the-shelf pipelines are usually designed for benchmark evaluation or visualization. First, similar to object detection, we adopt GPU image pre-processing by default. Second, we found that more than 90\% of the time within the mask head of Mask R-CNN is spent on transforming masks from the RoI space to the image space and compressing them in a format to be recognized by the COCO evaluation toolkit. Clearly, compression can be disabled for streaming perception. We point out that mask transformation can also be disabled. In practice, masks are used to tell if a specific point or region contains the object. Instead of transforming the mask (which involves object-specific image resizing operations), we can transform the query points or regions, which is simply a linear transformation over points or control points. Therefore, our timing does not include RoI-to-image transformation or mask compression. Furthermore, this also implies that we do not pay an additional cost for masks in forecasting, since only the box coordinates are updated but the masks remain in the RoI space.

For the instance segmentation benchmark, we use the same dataset and the same method HTC \cite{chen2019hybrid} for the pseudo ground truth as for detection, and we include 4 methods: Mask R-CNN \cite{He2017MaskR}
and Cascade Mask R-CNN~\cite{Cai2018CascadeRD} with ResNet 50 and ResNet 101 backbones. Since these are hybrid methods that produce both instance boxes and masks, we can measure the overhead of including masks as the difference between runtime with and without the mask head in
\ifstandalonesupplement
    Table 1.
\else
    Table~\ref{tab:mask-overhead}.
\fi
We find that the average overhead is around 13\%. We include the streaming evaluation in Tables~\ref{tab:mask} and~\ref{tab:forecast-mask} (with forecasting).


\begin{table*}[]
\small
\centering
\caption{Streaming evaluation for instance segmentation. We find that {\em many of our observations for object detection still hold for instance segmentation}: (1) AP drops significantly when moving from offline to real time, (2) the optimal ``sweet spot'' is not the fastest algorithm but the algorithm with runtime more than the unit frame interval, and (3) both our dynamic scheduling and infinite GPUs further boost the performance. Note that the absolute numbers might appear higher than the tables in the main text since we use pseudo ground truth here}
\label{tab:mask}
\adjustbox{width=1\linewidth}{
\begin{tabular}{lllccccccc}
\toprule
ID & Method & Detector & AP & AP & AP & AP & AP & AP & Runtime \\
\midrule
1 & Accurate (Offline) & Cascade MRCNN R50 @ s1.0 & 63.1 & 63.0 & 60.9 & 47.9 & 81.6 & 69.4 & 225.1 \\
\midrule
2 & Accurate & Cascade MRCNN R50 @ s1.0 & 11.8 & 11.5 & 8.1 & 5.4 & 20.4 & 11.1 & 225.1 \\
3 & Fast & Mask R-CNN R50 @ s0.2 & 8.3 & 16.5 & 2.1 & 0.0 & 13.6 & 8.3 & \textbf{41.4} \\
4 & Optimized & Mask R-CNN R50 @ s0.5 & \textbf{17.2} & \textbf{19.9} & \textbf{13.8} & \textbf{5.2} & \textbf{31.8} & \textbf{15.1} & 65.6 \\
\midrule
5 & + Scheduling
\ifstandalonesupplement
    (Alg. 1)
\else
    (Alg. \ref{alg:1})
\fi
& Mask R-CNN R50 @ s0.5 & 18.3 & 21.4 & 14.9 & 5.8 & 33.5 & 16.4 & 65.5 \\
\midrule
6 & + Infinite GPUs & Mask R-CNN R50 @ s0.75 & 20.6 & 20.0 & 19.0 & 9.1 & 38.4 & 18.2 & 100.8    \\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[]
\small
\centering
\caption{Streaming evaluation for instance segmentation with forecasting. Despite that we only forecast boxes and warp masks accordingly, we still observe significant improvement from forecasting for mask AP. The optimized algorithm for row 1 is Mask R-CNN ResNet 50 @ s0.5, and for row 2 is Mask R-CNN ResNet 50 @ s0.75}
\label{tab:forecast-mask}
\adjustbox{width=1\linewidth}{
\begin{tabular}{llcccccc}
\toprule
ID & Method                                             & AP            & AP         & AP        & AP        & AP        & AP       \\
\midrule
1  & Detection + Scheduling + Association + Forecasting \  & 24.1 & 32.4  & 23.0   & 6.0   & 43.7   & 22.0    \\
\midrule
2  & + Infinite GPUs                                    & 29.2 & 30.7  & 30.2   & 11.4  & 53.0   & 26.7    \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Alternate Hardware: Tesla V100}
\label{app:v100}

In the main text, we propose a meta-benchmark and mention that it can be instantiated with different hardware platforms. In this section, we include {\em full benchmark evaluation} for streaming detection with Tesla V100 (a faster GPU than GTX 1080~Ti used in the main text).

While our benchmark is hardware dependent, the method of evaluation generalizes across hardware platforms, and our conclusions largely hold when the hardware environment changes. We follow the same setup as in the experiments in the main text,
except that we use Tesla V100 from Amazon Web Services (EC2 instance of type \texttt{p3.2xlarge}). We provide the results for detection, forecasting, and tracking in Tables~\ref{tab:det-v100-aws}, \ref{tab:forecast-v100-aws}, and \ref{tab:track-v100-aws}, respectively. We see that {\em the improvement due to better hardware is largely orthogonal to the algorithmic improvement} proposed in the main text.

\begin{table*}[!h]
\small
\centering
\caption{Performance of detectors for streaming perception on Tesla V100 (a faster GPU than the Geforce GTX 1080 Ti used in the main text). By comparing with
\ifstandalonesupplement
    Table 1 
\else
    Table~\ref{tab:det}
\fi
in the main text, we see that runtime is shortened and the AP is increased due to the boost of hardware performance. Different from
\ifstandalonesupplement
    Table 1,
\else
    Table~\ref{tab:det},
\fi
we only consider GPU image pre-processing here for simplicity. Interestingly, with additional computation power, Tesla V100 enables more expensive models like input scale 0.75 (row 4) and Cascade Mask R-CNN (row 5) to be the optimal configurations (detector and scale) under their corresponding settings. Note that the improvement from our dynamic scheduler is orthogonal to the boost from hardware performance}
\label{tab:det-v100-aws}
\adjustbox{width=1\linewidth}{
\begin{tabular}{lllccccccc}
\toprule
ID & Method & Detector & AP & AP & AP & AP & AP & AP & Runtime \\
\midrule
1 & Accurate (Offline) & HTC @ s1.0 & 38.0 & 64.3 & 40.4 & 17.0 & 60.5 & 38.5 & 338.0 \\
\midrule
2 & Accurate & HTC @ s1.0 & 8.2 & 12.3 & 5.1 & 1.6 & 15.3 & 7.6 & 338.0 \\
3 & Fast & RetinaNet R50 @ s0.25 & 6.4 & 17.3 & 0.6 & 0.0 & 11.9 & 6.0 & \textbf{43.3} \\
4 & Optimized & Mask R-CNN R50 @ s0.75 & 13.0 & 22.2 & 9.5 & \textbf{2.3} & \textbf{27.6} & 10.9 & 72.1 \\
5 & + Scheduling
\ifstandalonesupplement
    (Alg. 1)
\else
    (Alg. \ref{alg:1})
\fi
& Cascade MRCNN R50 @ s0.5 & \textbf{14.0} & \textbf{28.8} & \textbf{9.9} & 1.0 & 26.8 & \textbf{12.2} & 60.2 \\
\midrule
6 & + Infinite GPUs & Mask R-CNN R50 @ s1.0 & 15.9 & 24.1 & 13.2 & 4.9 & 34.2 & 13.3 & 98.8    \\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[!h]
\vspace{-1em}
\small
\centering
\caption{Streaming perception with joint detection, association, and forecasting on Tesla V100 (corresponding to
\ifstandalonesupplement
    Table 2
\else
    Table~\ref{tab:forecast}
\fi
in the main text). We observe similar boost as in the detection only setting (Table~\ref{tab:det-v100-aws}). The ``re-optimize detection'' step finds that Mask R-CNN R50 @ s1.0 outperforms Cascade Mask R-CNN R50 @ s0.5 with forecasting (row2), and it also happens to be the optimal detector with infinite GPUs (row 3)}
\label{tab:forecast-v100-aws}
\adjustbox{width=1\linewidth}{
\begin{tabular}{llcccccc}
\toprule
ID & Method                                             & AP            & AP         & AP        & AP        & AP        & AP       \\
\midrule
1  & Detection + Scheduling + Association + Forecasting & 18.2          & 42.7          & 16.1          & 1.1          & 30.9          & 17.7          \\
2  & + Re-optimize Detection                            & \textbf{19.6} & \textbf{33.0} & \textbf{19.2} & \textbf{5.3} & \textbf{38.5} & \textbf{17.9} \\
\midrule
3  & + Infinite GPUs                                    & 22.9          & 38.7          & 23.1          & 6.9          & 43.8          & 21.2     \\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[!h]
\vspace{-1em}
\small
\centering
\caption{Streaming perception with joint detection, visual tracking, and forecasting on Tesla V100 (corresponding to
\ifstandalonesupplement
    Table 3
\else
    Table~\ref{tab:track}
\fi
in the main text). We find the similar conclusions that visual tracking with forecasting does not outperform association with forecasting in the single GPU case and achieves comparable performance in the infinite GPU case}
\label{tab:track-v100-aws}
\adjustbox{width=0.8\linewidth}{
\begin{tabular}{llcccccc}
\toprule
ID & Method                                                             & AP   & AP & AP & AP & AP & AP \\
\midrule
1  & Detection + Visual Tracking     & 12.6          & 21.5          & 9.0           & 2.2          & 27.1          & 10.5          \\
2  & + Forecasting                   & \textbf{18.0} & \textbf{34.7} & \textbf{16.8} & \textbf{3.2} & \textbf{36.0} & \textbf{16.4} \\
\midrule
3  & + Infinite GPUs w/o Forecasting  \qquad \qquad & 14.4          & 24.2          & 11.2          & 2.8          & 30.6          & 12.0          \\
4  & + Forecasting                   & \textbf{22.8} & \textbf{38.6} & \textbf{23.0} & \textbf{6.9} & \textbf{43.7} & \textbf{21.0} \\
\bottomrule
\end{tabular}
}
\end{table*}


\clearpage

\section{Solution Details}
\label{app:solution-details}




\subsection{Dynamic Scheduling}
\label{app:dynamicschedule}

In the main text, we propose the dynamic scheduling algorithm \ifstandalonesupplement
    (Alg. 1)
\else
    (Alg. \ref{alg:1})
\fi
to reduce temporal aliasing. Such an algorithm is counter-intuitive in that it minimizes latency by sometimes sitting idle. In this subsection, we provide additional theoretical analysis and empirical results for algorithm scheduling. We first introduce the framework to study algorithm scheduling for streaming perception.
Next, we show theoretically that our dynamic scheduling outperforms naive idle-free scheduling for any constant runtime larger than the frame interval and any long-enough sequence length. Lastly, we verify empirically the superiority of our dynamic scheduling.

To study algorithm scheduling, we assume no concurrency (\ie, a single job at a time) and that jobs are not interruptible. For notational simplicity, we assume a fixed input frame rate where frame  is the frame available at time  (\ie, zero-based indexing), and therefore  can be used to denote both frame index and time. {\em We assume that time (time axis, runtime, and latency) is represented in the units of the number of frames}. We also assume  to be a {\em single-frame} algorithm, and the streaming algorithm  is thus composed of  and a scheduling policy. No tracking or forecasting is used in the discussion below.
Let  be the input frame index that was processed to generate output : if , then . We denote the runtime of  as . 

\begin{figure}[!b]
\centering
\includegraphics[width=0.7\linewidth]{Fig/temporal-mismatch.pdf}

\caption{Temporal mismatch for single-frame algorithms. Take  
(query index )
as an example (highlighted in orange): when the benchmark queries for , the latest prediction is , whose input index is 0, thus leading to a temporal mismatch of 3 (frames).}
\label{fig:temporalmismatch}
\end{figure}

{\noindent \bf Definition (Temporal Mismatch)} When the benchmark queries for the state of the world at frame , the temporal mismatch is , where .
If there is no output available, . We denote the average temporal mismatch over the entire sequence as .

Intuitively, the temporal mismatch measures the latency of a streaming algorithm  in the unit of the number of frames (Fig.~\ref{fig:temporalmismatch}). This latency is typically higher than the runtime of the single-frame algorithm  itself due to the blocking effect of consecutive execution blocks. For example, in Figure~\ref{fig:temporalmismatch}, although runtime , the average mismatch  for . Note that we define  if there is no output available. To avoid the degenerate case where an algorithm processes nothing and yields a zero cumulative temporal mismatch, we assume that all schedules start processing the first frame immediately at .

\pbf{MDP} Naive idle-free scheduling processes the next available frame immediately after the previous execution is finished. However, a scheduler can {\em choose} when and which frames to process. Selection among such choices over the data sequence can be modeled as a decision policy under a {\em Markov decision process (MDP)}. An MDP formulation allows one to compute the expected future cumulative mismatch for a given policy under stochastic runtimes .
In theory, one may also be able to compute the optimal schedule (that minimizes expected cumulative mismatches) through policy search algorithms. 
However, Figure~\ref{fig:runtime} shows that practical runtime profiles have low variance and are unimodal. If one assumes that runtimes are deterministic and fixed at a constant value, we will now show that our shrinking-tail policy outperforms idle-free over a range of runtimes  and sequence lengths . We believe that constant runtime is a reasonable assumption for our setting, and empirically verify so after our theoretical analysis. 

\begin{figure}[!b]
\centering
\includegraphics[width=1\linewidth]{Fig/shrinking-tail-geq-0_5.pdf}

\caption{Mismatch is the same for the shrinking-tail policy with different runtime  and  as long as , , and .
}
\label{fig:shrinking-tail-geq-0.5}
\end{figure}

\pbf{Pattern analysis} Crucially, constant runtimes ensure that all transitions are deterministic, allowing for a global view of the sequence. Our key observation is that the {\em global sequence will contain repeating mismatch patterns}. Analysis of one such pattern sheds light on the cumulative mismatch of the entire sequence. For example,  under idle-free repeats itself every 2 processing blocks. However, different patterns emerge for different values of  and for different policies.
We assume that  to avoid the trivial schedule where an algorithm consistently finishes before the next frame arrives. We write  and  for the average temporal mismatch  for the idle-free and shrinking-tail policies, respectively. Our analysis is based on the concept of {\em tail}:
. We denote  as  for short. Note that the integral part of runtime does not contribute to the temporal quantization effect, and we thus focus on the discussion of  for simplicity.
We split our analysis into 3 different cases:
, , and .

\pbf{Case 1} The first is a special case where . It can be easily verified that idle-free is equivalent to shrinking-tail, and thus .


\pbf{Case 2}
Now we inspect the case with . Since , the shrinking-tail policy will output true (waiting) after processing the first frame. The waiting aligns the execution again with the integral time step, and thus for the subsequent processing blocks, it also outputs true (waiting). In summary, shrinking-tail always outputs true in this case, and its pattern in mismatch is agnostic to the specific runtime  (Fig.~\ref{fig:shrinking-tail-geq-0.5}). Let  denote   with runtime ,
then we can draw the conclusion that 
 for , , and .

\begin{figure}[!b]
\centering
\includegraphics[width=1\linewidth]{Fig/shrinking-tail-0_5.pdf}

\caption{For , shrinking-tail achieves less cumulative mismatch than idle-free. Note that each policy has its own repeating period and shrinking-tail always achieves 1 less cumulative mismatch within each common period.
}
\label{fig:shrinking-tail-0.5}
\end{figure}

We then focus on a particular case of . As shown in Figure~\ref{fig:shrinking-tail-0.5}, idle-free repeats itself in a period of 3 frames, and shrinking-tail repeats itself in a period of 2 frames. Together, they form a joint pattern that repeats itself in a period of 6 frames (their least common multiple). The diagram shows that within each common period, the difference of cumulative mismatch between idle-free and shrinking-tail is increased by 1. And it is the same for all common periods. Therefore, if  for some positive integer  (intuitively, the entire sequence is a multiple of several common periods), . Additionally, Figure~\ref{fig:shrinking-tail-0.5} enumerates all possible cases, where the sequence ends before a common period is over or in the middle of a common period. All these cases have . 

Next, it is straightforward to see thatthe cumulative mismatch will not decrease if one increases the runtime  of :  if . 
Therefore, for , we have




\pbf{Case 3}

The last case with  (\ie, ) is more complicated than previous cases because the underlying repeating pattern never exactly repeats itself. To address this issue, we must introduce several new concepts to characterize such near-repeating patterns. We first observe a special type of execution block:

{\noindent \bf  Definition (Shrinking-Tail Block)} Denoting the start and the end time of an execution block as  and , a {\em shrinking-tail block} is an execution block such that . As shown in Figure~\ref{fig:shrinking-tail-block}, a shrinking-tail block increases temporal mismatch.

\begin{figure}[]
\centering
\includegraphics[width=0.65\linewidth]{Fig/tail-shrinking-block2.pdf}

\caption{A shrinking-tail execution block (orange) increases temporal mismatch.
}
\label{fig:shrinking-tail-block}
\end{figure}


{\noindent \bf  Definition (Shrinking-Tail Cycle)} A sequence of execution blocks can be divided into segments by a shrinking-tail block or an idle gap. A {\em shrinking-tail cycle} is a set of queries covered by the segment between these dividers. Specifically, the cycle starts from the 0-th query, the last query of a shrinking-tail block, or the query at the end of an idle gap. The cycle ends either when the sequence ends or the next cycle starts. The length of a cycle is the number of queries it covers. 


\begin{figure}[]
\centering
\includegraphics[width=1\linewidth]{Fig/shrinking-tail-cycle.pdf}

\caption{Shrinking-tail cycle. Intuitively, blocks within each shrinking-tail cycle has tails increasing ( and ). It ends when the tail decreases or there is an idle gap, and thus the tail ``shrinks''.
}
\label{fig:shrinking-tail-cycle}
\end{figure}



As shown in Figure~\ref{fig:shrinking-tail-cycle}, shrinking-tail cycles are small segments of the entire sequence and they may have different lengths. Note that the definitions of both shrinking-tail block and cycle are agnostic to , but we only refer to them during our discussion for .
Instead of comparing  for idle-free and shrinking-tail directly,
we compare them for each cycle (denoted as  and  respectively). First, we observe that a shrinking-tail cycle starts with either a shrinking-tail block or an idle gap and ends with consecutive tail-increasing blocks. Second, we observe that most queries have a mismatch of 2 for both policies (\eg, Cycle 2's queries 20 to 21 and Cycle 4's queries 18 to 19 in Fig.~\ref{fig:shrinking-tail-cycle}), and that the second query in a cycle is always 3 due to having a shrinking-tail block or an idle-gap before it.  This is the rounding effect when adding multiple fractional numbers. The difference between the two policies is thus the mismatch of the first query. For , the first query of idle-free has a mismatch of 3,
while shrinking-tail has a mismatch of 2. 
Intuitively, given that the majority of queries are with mismatch 2, the number of queries with mismatch 3 determines the relationship between : . Therefore, when the sequence length is long enough, the policy with a smaller  leads to a smaller overall cumulative mismatch.

Now, we present a more formal analysis on the above statement. To quantify the cycle patterns, we first quantify the number of consecutive tail-increasing blocks. Let the number of consecutive tail-increasing blocks be  and the tail of the first block covered by the cycle be  (in the case where the first block starts after an idle gap, we define  to be 0). We first observe that . Also,  has its own range for each policy. For idle-free, , and for shrinking-tail, . Taking Figure~\ref{fig:shrinking-tail-cycle} for example (), Cycle 1 has  and , Cycle 2 has  and , and Cycle 4 has  and .

Since  might vary from cycle to cycle, we introduce a reference quantity that is constant and can be used to measure the length of cycles.
Let  be the  when , \ie, , and  be the length of a cycle. For idle-free policy,  or . The variable length in cycles is due to variable  between cycles. When , we have the first type of cycle with length  (denoted as ); when , we have the second type of cycle of length  (denoted as ). The starting cycle in a sequence is always the first type, while the ensuing cycles can be either the first or second type. Note that it is possible that all cycles are the first type. For example, when , each cycle resets itself and  for all cycles. For shrinking-tail policy, each cycle resets itself (whose length denoted as ). Note that  denotes the length of the 3 {\em types} of cycles and Cycle 1, 2, 3, ... in the figures denote specific cycle {\em instances}.
From the above analysis, we can see

Therefore,


\begin{figure}[!b]
\centering
\includegraphics[width=1\linewidth]{Fig/shrinking-tail-start.pdf}

\caption{The first cycles for both policies have different mismatch patterns.
}
\label{fig:schedule-starting}
\end{figure}

Next, we explain how to infer the relationship between  from that between . To analyze the mismatch of the whole sequence, we need to inspect the boundary cases at the start and the end of the sequence, where the cycle-based analysis might not hold. As shown in Figure~\ref{fig:schedule-starting}, the first cycles for both policies have different mismatch patterns due to empty detection at the first two queries. Compared to regular cycles in Figure~\ref{fig:shrinking-tail-cycle}, the first cycle has 6 and 5 less total mismatch for idle-free and shrinking-tail policy respectively. Let , , and  be the number of complete cycles of type , , and  in a sequence, respectively,  be the number of residual queries at the end of the sequence that do not complete a cycle, and  be the total mismatch of these  queries, then we have

Note that the above holds only when  and  (the sequence is at least one cycle long for both policies). If  is smaller or equal to one cycle, the two policies are equivalent and . When  is large enough (\eg, , the  terms dominate Eq~\ref{eq:deltaif} and Eq~\ref{eq:deltast}, and due to Eq~\ref{eq:deltac}, we have , which shows that the shrinking-tail policy is superior.
Formally, when , where  is some constant depending on , .

\pbf{Summary of the theoretical analysis} Considering all 3 cases, we can draw the conclusion that  when  is large enough (greater than  if , and no requirement otherwise). By achieving less average mismatch, shrinking-tail outperforms idle-free.


\subsubsection{Practical Performance of Dynamic Scheduling}

We apply our dynamic schedule
\ifstandalonesupplement
    (Alg. 1)
\else
    (Alg. \ref{alg:1})
\fi
to a wide suite of detectors under the same settings as our main experiments and summarize the results in Table~\ref{tab:effectofds}. In practice, runtime is stochastic due to complicated software and hardware scheduling or running an input adaptive model, but we find the theoretical results obtained under constant runtime assumption generalizes to most of the practical cases under our experiment setting.

\begin{table*}[]
\small
\centering
\caption{Empirical performance comparison before and after using
\ifstandalonesupplement
    Alg. 1.
\else
    Alg. \ref{alg:1}.
\fi
We see that our shrinking-tail policy consistently boosts the streaming performance for different detectors and for different input scales. We also observe some failure cases (last two rows), where runtime is close to one frame duration. This is because our theoretical analysis assumes constant runtime, while it is dynamic in practice. Hence, the variance in runtime when it is a boundary value can make a noticeable difference on the performance
}\label{tab:effectofds}
\adjustbox{width=1\linewidth}{
\begin{tabular}{lcccc}
\toprule
Method & AP (Before) & AP (After) & Runtime (ms) & Runtime (frames) \\
\midrule
SSD @ s0.5 & 9.7 & 9.7 & 66.7 & 2.0 \\
RetinaNet R50 @ s0.5 & 10.9 & 11.6 & 54.5 & 1.6 \\
RetinaNet R101 @ s0.5 & 9.9 & 9.9 & 66.8 & 2.0 \\
Mask R-CNN R101 @ s0.5 & 11.0 & 11.1 & 68.8 & 2.1 \\
Cascade MRCNN R50 @ s0.5 & 11.3 & 11.7 & 80.0 & 2.4 \\
Cascade MRCNN R101 @ s0.5 & 10.3 & 11.1 & 92.2 & 2.8 \\
HTC @ s0.5 & 7.9 & 8.0 & 240.8 & 7.2 \\
\midrule
Mask R-CNN R50 @ s0.25 & 7.7 & 7.8 & 36.1 & 1.1 \\
Mask R-CNN R50 @ s0.5 & 12.0 & 13.0 & 56.7 & 1.7 \\
Mask R-CNN R50 @ s0.75 & 11.5 & 12.6 & 92.7 & 2.8 \\
Mask R-CNN R50 @ s1.0 & 10.6 & 10.7 & 139.6 & 4.2 \\
\midrule
RetinaNet R50 @ s0.25 & 6.9 & 6.8 & 33.4 & 1.0 \\
Mask R-CNN R50 @ s0.2 & 6.5 & 6.3 & 34.3 & 1.0 \\
\bottomrule
\end{tabular}
}
\end{table*}


\subsection{Additional Details for Forecasting}
\label{app:forecasting}

We use an asynchronous Kalman filter for our forecasting module. The state representation which we choose is , where  are the top-left coordinates, and width and height of the bounding box, and the remaining four are their derivatives. The state transition is assumed to be linear. We also test with the representation used in SORT \cite{Bewley2016_sort}, which assumes that the area (the product of the width and the height) varies linearly instead of that each of the width and the height varies linearly. We find that such a representation produces lower numbers in AP.

As explained in the main text, Kalman filter needs to be asynchronous and time-varying for streaming perception.
Let  be the time-varying intervals between updates or prediction steps, we pick the transition matrix to be:

and the process noise to be

Intuitively, the process noise is larger the longer between the updates.

All forecasting modules are implemented on the CPU and thus can be parallelized while the detector runs on the GPU. Our batched (over multiple objects) implementation of the asynchronous Kalman filter takes ms for the update step and ms for the prediction step, which are relatively very small overheads compared to detector runtimes. For scalable evaluation, we assume zero runtime for the association and forecasting module, and implement forecasting as post-processing of the detection outputs. One might wonder that a simulated post-processing run and an actual real-time parallel execution might have different final APs. We have also implemented the latter for verification purposes. For most settings the differences are within 1\%. Although for some settings the difference can reach 3\%, we find such fluctuation does not affect the relative rankings.


\subsection{Additional Details for Visual Tracking}
\label{app:tracking}

For our tracking experiments
\ifstandalonesupplement
    (Section 4.4 in the main text),
\else
    (Section~\ref{sec:exp-tracking}),
\fi
we adapt and modify the state-of-the-art multi-object tracker \cite{Bergmann2019TrackingWB}. A component breakdown in Fig.~\ref{fig:tracker} explains how this tracker works and why it has the potential to achieve better performance under the streaming setting.

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{Fig/tracker-explained.pdf}
\vspace{-0.5em}
\caption{Multi-object visual tracker. The advantage of a visual tracker is that it runs faster than a detector and thus yields lower latency for streaming perception. The multi-object tracker used here is modified from \cite{Bergmann2019TrackingWB}. It is mostly the same as a two-stage detector, except that its box head uses the last known object location as input in place of region proposals. Therefore, we get a computation saving by not running the RPN head. Runtime is measured for Mask R-CNN (ResNet 50) with input scale 0.5.}
\label{fig:tracker}
\vspace{-1em}
\end{figure}

\subsection{Evaluation of Our Meta-Detector {\em Streamer}}
\label{app:metaalg}

\begin{table*}[]
\small
\centering
\caption{Performance boost after applying Streamer. ``(B)'' standards for ``Before'', and ``(A)'' standards for ``After''. The evaluation setting is the same as
\ifstandalonesupplement
    Table 1 
\else
    Table~\ref{tab:det}
\fi
in the main text. This table assumes a {\em single} GPU, and an infinite GPU counterpart can be found in Table~\ref{tab:meta-alg-inf-gpu}. Under this setting, we observe significant improvement in AP, ranging from 5\% to 78\%, and averaging at 34\%
}
\label{tab:meta-alg-single-gpu} 
\addtolength{\tabcolsep}{0.2em}
\vspace{-0.5em}
\begin{tabular}{lccccccc}
\toprule
Method & Scale & AP(B) & AP(A) & Boost & AP(B) & AP(A) & Boost \\
\midrule
                   & 0.2  & 9.5  & 10.4 & 9\%  & 23.5 & 28.6 & 21\% \\
                   & 0.25 & 9.3  & 10.6 & 14\% & 23.9 & 31.5 & 32\% \\
SSD                & 0.5  & 9.7  & 13.5 & 40\% & 20.0 & 32.4 & 62\% \\
                   & 0.75 & 6.0  & 10.7 & 78\% & 11.5 & 19.8 & 72\% \\
                   & 1.0  & 4.2  & 7.3  & 76\% & 7.3  & 12.5 & 72\% \\
\midrule
                   & 0.2  & 6.0  & 6.3  & 5\%  & 18.1 & 21.3 & 17\% \\
                   & 0.25 & 6.9  & 7.5  & 9\%  & 19.8 & 26.2 & 33\% \\
RetinaNet R50      & 0.5  & 10.9 & 14.2 & 30\% & 24.1 & 38.3 & 59\% \\
                   & 0.75 & 10.8 & 16.1 & 50\% & 20.2 & 32.9 & 63\% \\
                   & 1.0  & 9.9  & 14.1 & 42\% & 16.7 & 24.7 & 48\% \\
\midrule
                   & 0.2  & 5.4  & 5.9  & 9\%  & 14.7 & 19.8 & 35\% \\
                   & 0.25 & 6.5  & 7.4  & 14\% & 18.2 & 25.8 & 42\% \\
RetinaNet R101     & 0.5  & 9.9  & 13.0 & 31\% & 21.5 & 33.6 & 56\% \\
                   & 0.75 & 9.9  & 14.5 & 47\% & 18.1 & 27.7 & 53\% \\
                   & 1.0  & 8.9  & 12.7 & 42\% & 14.7 & 22.0 & 50\% \\
\midrule
                   & 0.2  & 6.5  & 7.2  & 11\% & 18.0 & 25.1 & 40\% \\
                   & 0.25 & 7.7  & 9.1  & 19\% & 20.1 & 29.9 & 49\% \\
Mask R-CNN R50     & 0.5  & 12.0 & 16.7 & 39\% & 24.3 & 39.9 & 64\% \\
                   & 0.75 & 11.5 & 17.8 & 54\% & 19.5 & 33.3 & 71\% \\
                   & 1.0  & 10.6 & 15.0 & 42\% & 16.6 & 25.0 & 50\% \\
\midrule
                   & 0.2  & 6.3  & 7.2  & 14\% & 16.7 & 24.1 & 45\% \\
                   & 0.25 & 7.6  & 9.0  & 17\% & 19.3 & 28.5 & 48\% \\
Mask R-CNN R101    & 0.5  & 11.0 & 15.2 & 39\% & 21.6 & 35.4 & 64\% \\
                   & 0.75 & 10.0 & 15.3 & 52\% & 16.8 & 28.0 & 67\% \\
                   & 1.0  & 8.8  & 12.4 & 42\% & 13.7 & 21.2 & 55\% \\
\midrule
                   & 0.2  & 6.2  & 7.8  & 25\% & 15.4 & 25.5 & 66\% \\
                   & 0.25 & 7.5  & 9.6  & 28\% & 18.4 & 30.1 & 63\% \\
Cascade MRCNN R50  & 0.5  & 11.3 & 16.4 & 45\% & 22.6 & 37.5 & 66\% \\
                   & 0.75 & 10.9 & 16.7 & 54\% & 18.6 & 29.8 & 60\% \\
                   & 1.0  & 10.1 & 15.7 & 55\% & 15.4 & 25.3 & 64\% \\
\midrule
                   & 0.2  & 6.1  & 7.3  & 20\% & 15.2 & 23.1 & 52\% \\
                   & 0.25 & 7.4  & 9.5  & 28\% & 17.6 & 29.6 & 69\% \\
Cascade MRCNN R101 & 0.5  & 10.3 & 15.4 & 49\% & 20.5 & 34.1 & 66\% \\
                   & 0.75 & 9.5  & 14.7 & 54\% & 16.1 & 26.1 & 62\% \\
                   & 1.0  & 8.8  & 12.9 & 46\% & 13.7 & 21.8 & 59\% \\
\midrule
                   & 0.2  & 5.6  & 6.8  & 22\% & 12.0 & 17.0 & 42\% \\
                   & 0.25 & 6.3  & 8.3  & 31\% & 13.0 & 19.8 & 53\% \\
HTC                & 0.5  & 7.9  & 10.8 & 38\% & 13.3 & 19.9 & 49\% \\
                   & 0.75 & 7.1  & 8.6  & 22\% & 11.4 & 14.8 & 30\% \\
                   & 1.0  & 6.4  & 7.2  & 12\% & 9.6  & 11.4 & 18\% \\
\bottomrule
\end{tabular}
\addtolength{\tabcolsep}{-0.2em}
\end{table*}

\begin{table*}[]
\small
\centering
\caption{Performance boost after applying Streamer. ``(B)'' standards for ``Before'', and ``(A)'' standards for ``After''. The evaluation setting is the same as
\ifstandalonesupplement
    Table 1 
\else
    Table~\ref{tab:det}
\fi
in the main text. This table assumes {\em infinite} GPUs, and a single GPU counterpart can be found in Table~\ref{tab:meta-alg-single-gpu}. Under this setting, we observe significant improvement in AP, ranging from 4\% to 80\%, and averaging at 32\%
}
\label{tab:meta-alg-inf-gpu} 
\addtolength{\tabcolsep}{0.2em}
\vspace{-0.5em}
\begin{tabular}{lccccccc}
\toprule
Method & Scale & AP(B) & AP(A) & Boost & AP(B) & AP(A) & Boost \\
\midrule
                   & 0.2  & 9.9  & 10.6 & 7\%  & 25.5 & 29.4 & 15\% \\
                   & 0.25 & 9.6  & 10.7 & 12\% & 24.9 & 31.7 & 27\% \\
SSD                & 0.5  & 11.3 & 14.7 & 30\% & 24.1 & 35.4 & 47\% \\
                   & 0.75 & 8.0  & 13.3 & 66\% & 14.6 & 25.6 & 76\% \\
                   & 1.0  & 5.5  & 9.8  & 80\% & 10.0 & 16.5 & 65\% \\
\midrule
                   & 0.2  & 6.1  & 6.3  & 4\%  & 18.6 & 21.3 & 15\% \\
                   & 0.25 & 7.1  & 7.6  & 8\%  & 21.4 & 27.1 & 26\% \\
RetinaNet R50      & 0.5  & 12.3 & 14.7 & 20\% & 28.1 & 40.1 & 42\% \\
                   & 0.75 & 13.1 & 18.0 & 37\% & 24.3 & 37.8 & 56\% \\
                   & 1.0  & 11.7 & 17.3 & 48\% & 19.5 & 31.3 & 60\% \\
\midrule
                   & 0.2  & 5.5  & 6.0  & 9\%  & 15.3 & 20.1 & 32\% \\
                   & 0.25 & 6.7  & 7.5  & 12\% & 18.8 & 26.1 & 38\% \\
RetinaNet R101     & 0.5  & 11.3 & 14.0 & 24\% & 25.3 & 38.1 & 50\% \\
                   & 0.75 & 11.8 & 17.0 & 44\% & 21.3 & 34.3 & 61\% \\
                   & 1.0  & 10.8 & 16.3 & 51\% & 18.2 & 28.2 & 55\% \\
\midrule
                   & 0.2  & 6.7  & 7.4  & 10\% & 20.0 & 26.2 & 31\% \\
                   & 0.25 & 7.8  & 9.2  & 17\% & 20.8 & 30.1 & 45\% \\
Mask R-CNN R50     & 0.5  & 13.9 & 17.4 & 26\% & 29.0 & 42.6 & 47\% \\
                   & 0.75 & 14.4 & 20.3 & 40\% & 24.3 & 38.5 & 59\% \\
                   & 1.0  & 12.4 & 18.7 & 51\% & 19.4 & 31.4 & 62\% \\
\midrule
                   & 0.2  & 6.5  & 7.3  & 13\% & 17.4 & 24.3 & 40\% \\
                   & 0.25 & 7.9  & 9.1  & 15\% & 20.5 & 28.9 & 41\% \\
Mask R-CNN R101    & 0.5  & 11.9 & 16.2 & 36\% & 23.7 & 38.4 & 62\% \\
                   & 0.75 & 12.4 & 18.5 & 49\% & 20.3 & 35.3 & 74\% \\
                   & 1.0  & 10.6 & 16.2 & 53\% & 16.9 & 27.7 & 64\% \\
\midrule
                   & 0.2  & 7.0  & 7.9  & 13\% & 18.9 & 26.5 & 40\% \\
                   & 0.25 & 8.5  & 9.9  & 16\% & 22.3 & 31.7 & 42\% \\
Cascade MRCNN R50  & 0.5  & 12.9 & 17.6 & 37\% & 26.0 & 41.2 & 58\% \\
                   & 0.75 & 13.2 & 19.9 & 51\% & 22.1 & 36.5 & 65\% \\
                   & 1.0  & 12.6 & 19.8 & 57\% & 19.0 & 31.8 & 67\% \\
\midrule
                   & 0.2  & 6.8  & 7.9  & 17\% & 17.8 & 26.6 & 49\% \\
                   & 0.25 & 8.3  & 9.8  & 18\% & 21.0 & 31.7 & 50\% \\
Cascade MRCNN R101 & 0.5  & 12.6 & 17.0 & 35\% & 25.0 & 38.5 & 54\% \\
                   & 0.75 & 11.4 & 17.7 & 56\% & 19.0 & 32.7 & 72\% \\
                   & 1.0  & 10.5 & 16.6 & 59\% & 16.7 & 27.6 & 65\% \\
\midrule
                   & 0.2  & 6.3  & 8.0  & 27\% & 14.0 & 21.8 & 55\% \\
                   & 0.25 & 7.3  & 9.8  & 34\% & 15.7 & 25.5 & 62\% \\
HTC                & 0.5  & 9.2  & 13.7 & 50\% & 16.3 & 26.9 & 65\% \\
                   & 0.75 & 8.2  & 11.4 & 39\% & 13.2 & 20.5 & 55\% \\
                   & 1.0  & 7.4  & 9.3  & 25\% & 11.1 & 15.8 & 43\% \\
\bottomrule
\end{tabular}
\addtolength{\tabcolsep}{-0.2em}
\end{table*}

Streamer is introduced in Section
\ifstandalonesupplement
    4.3
\else
    \ref{sec:streamer}
\fi
in the main text. Given a detector and an input scale, Streamer automatically schedules the detector and employs forecasting to compensate for some of its latency. In the single GPU case, our dynamic schedule
\ifstandalonesupplement
    (Alg. 1)
\else
    (Alg.~\ref{alg:1})
\fi
is used and in the infinite GPU case, idle-free scheduling
\ifstandalonesupplement
    (Fig. 4c)
\else
    (Fig.~\ref{fig:compconstraint}c)
\fi
is used. Proper scheduling requires the knowledge of runtime of the algorithm, which is known in the case of benchmark evaluation. When applied in the wild, we can optionally track runtime of the algorithm on unseen data and adjust the scheduling accordingly. The forecasting module is implemented with asynchronous Kalman filter (Section~\ref{app:forecasting}).

Streamer has several key features. First, it enables synchronous processing for an asynchronous problem. Under the commonly studied settings (both offline and online), computation is synchronous in that the outputs and the inputs have a natural one-to-one correspondence. Therefore, many existing temporal reasoning models assume that the inputs are at a uniform rate and each input corresponds to an output \cite{Donahue2015LongtermRC,Girdhar2018ABB,Feichtenhofer2019SlowFastNF}. In the real-time setting, however, such a relationship does not exist due to the latency of the algorithm, \ie, the number of outputs can be arbitrary. Streamer converts detectors with arbitrary runtimes into systems that output at a designated fixed rate. In short, it abstracts away the asynchronous nature of the problem and therefore allows downstream synchronous processing. Second, by adopting forecasting, Streamer significantly boosts the performance of streaming perception. In Tables~\ref{tab:meta-alg-single-gpu} and \ref{tab:meta-alg-inf-gpu}, we evaluate the detection AP before and after applying our meta-detector. We observe relative improvement from 4\% to 80\% with an average of 33\% in detection AP under 80 different settings (8 detectors  5 image scales  2 compute models). Note that the difference of this evaluation and benchmark evaluation in the main text is that we fix the detector and input scale here, while benchmark evaluation searches over the best configuration of detectors and input scales. For the infinite GPU settings, we discount the boost from additional compute itself.











\subsection{Implementation Details}

\pbf{Detectors}
We experiment with a large suite of object detectors: SSD \cite{Liu2016SSDSS}, RetinaNet \cite{lin2017focal}, Mask R-CNN \cite{He2017MaskR}, Cascade Mask R-CNN \cite{Cai2018CascadeRD}, and HTC \cite{chen2019hybrid}. The ``optimized" and ``re-optimized" rows in all tables represent the optimal configuration over all detectors and all input scales of 0.2, 0.25, 0.5, 0.75, and 1.0. We adopt mmdetection codebase \cite{mmdetection} (one of the fastest open-source implementation for Mask R-CNN) for object detectors. Note that for all detectors, the implementation has reproduced both the accuracy and runtime reported in the original papers.

\begin{figure}[!b]
\centering
\includegraphics[width=0.65\linewidth]{Fig/schedule-forecast.pdf}
\caption{Scheduling for linear forecasting. The scheduling is similar as with the Kalman filter case in that both are asynchronous. The difference is that linear forecasting does not explicitly maintain a state representation but only stores two latest detection results. Association takes place immediately after a new detection result becomes available, and it links the bounding boxes in two consecutive detection results and computes a velocity estimate. Forecasting takes place right before the next time step, and it uses linear extrapolation to produce an output as the estimation of the current world state. The equations represent the computation for reporting to benchmark query at .  is a simplified representation for object location. At this time, only detection results for frame 0 and 1 are available, but through association and forecasting, the algorithm can make a better prediction for the current world state.}
\label{fig:linear-forecast}
\end{figure}

\pbf{Potentially better implementation} We acknowledge that there are additional bells and whistles to reduce runtime of object detectors, which might further improve the results on our benchmark. We focus on general techniques instead of device- or application-specific ones. For example, we have explored GPU image pre-processing, which is applicable to all GPUs. Another implementation technique is to use half-precision floating-point numbers (FP16), which we have not explored, since it will only pay off for certain GPUs that have been optimized for FP16 computation (it is reported that FP16 yields only marginal testing time speed boost on 1080 Ti \cite{chen2019simpledet}).





\section{Additional Baselines}
\label{app:addmethods}

\subsection{Forecasting Baselines}
\label{app:addforecasting}





We have also tested linear extrapolation (\ie, constant velocity) and quadratic extrapolation for forecasting detection results. We include an illustration of linear forecasting in Fig.~\ref{fig:linear-forecast}, and the quadratic counterpart is a straight-forward extension that involves three latest detection results. Though they produce inferior results than Kalman filter, we include the results in Table~\ref{tab:more-forecast} for completeness. 

\begin{table*}[]
\small
\centering
\caption{Comparison of different forecasting methods for streaming perception. We see that both linear and Kalman filter forecasting methods significantly improve the streaming performance. Kalman filter further outperforms the linear forecasting. The quadratic forecasting decreases the AP, suggesting that high-order extrapolation is not suitable for this task. The detection used here is Mask R-CNN ResNet 50 @ s0.5 with dynamic scheduling 
\ifstandalonesupplement
    (Alg. 1)
\else
    (Alg. \ref{alg:1})
\fi
}
\label{tab:more-forecast}
\addtolength{\tabcolsep}{0.22em}
\begin{tabular}{llcccccc}
\toprule
ID & Method                                             & AP            & AP         & AP        & AP        & AP        & AP       \\
\midrule
1 & No Forecasting & 13.0 & 26.6 & 9.2 & 1.1 & 26.8 & 11.1 \\
2 & Linear (constant velocity) & 15.7 & 38.1 & 13.8 & 1.1 & 30.2 & 14.8 \\
3 & Quadratic & 9.7 & 23.8 & 6.6 & 0.4 & 21.4 & 7.9 \\
4 & Kalman filter & \textbf{16.7} & \textbf{39.8} & \textbf{14.9} & \textbf{1.2} & \textbf{31.2} & \textbf{16.0}  \\
\bottomrule
\end{tabular}
\addtolength{\tabcolsep}{-0.2em}
\vspace{-2em}
\end{table*}


\subsection{An End-to-End Baseline}
\label{app:e2ebaseline}

In the main text, we break down the streaming detection task into detection, tracking, and forecasting for modular analysis. Alternatively, it is also possible to train a model that directly outputs detection results in the future. F2F \cite{Luc2018PredictingFI} is one such model. Building upon Mask R-CNN, it does temporal reasoning and forecasting at the level of FPN feature maps. Note that no explicit tracking is performed. In this section, we compare against this end-to-end baseline in both offline and streaming settings.

\begin{table*}[]
\small
\centering
\caption{Standard offline forecasting evaluation for the end-to-end method F2F \cite{Luc2018PredictingFI}. The goal is to forecast 3 frames into the future. Surprisingly, the more expensive F2F method performs worse than the simpler Kalman filter in terms of the overall AP}
\label{tab:f2foffline}
\begin{tabular}{llcccccc}
\toprule
ID & Method                                             & AP            & AP         & AP        & AP        & AP        & AP       \\
\midrule
1  & None (copy last) & 13.4          & 24.3          & 10.9          & 1.9          & 27.9          & 11.3          \\
2  & Linear           & 16.3          & 34.8          & 16.8          & 1.8          & 32.9          & 14.3          \\
3  & Kalman filter    & \textbf{19.1} & 40.3          & 19.8          & \textbf{2.6} & \textbf{35.8}          & \textbf{17.7} \\
4  & F2F              & 18.3          & \textbf{41.0} & \textbf{20.0} & 2.5          & 33.9 & 17.1     \\
\bottomrule
\end{tabular}
\end{table*}


In the offline setting, the algorithm is given  frames as input history, and outputs detection results for  frames ahead. This is the same as the evaluation in \cite{Luc2018PredictingFI}. We set both  and  to be 3, as the optimal detector in our forecasting experiments
\ifstandalonesupplement
    (Table 2)
\else
    (Table~\ref{tab:forecast})
\fi
has runtime of 2.78 frames. Since F2F forecasts at the FPN feature level, it is agnostic to second stage tasks. In our evaluation, we focus on the bounding box detection task instead of instance segmentation. Also, we conduct experiments on Argoverse-HD, consistent with the setting in our other experiments. Due to a lack of annotation, we adopt pseudo ground truth (Section~\ref{app:pseudo-gt}) for training (data from the original training set of Argoverse 1.1~\cite{Argoverse}). We implement our own version of F2F based on mmdetection (instead of Detectron as done in \cite{Luc2018PredictingFI}). We train the model for 12 epochs end-to-end (a 50\% longer schedule than combined stages in \cite{Luc2018PredictingFI}). For a fair comparison, we also finetuned the detectors on Argoverse with the same pseudo ground truth. For Mask R-CNN ResNet 50 at scale 0.5, it boosts the offline box AP from 19.4 to 22.9. We use this finetuned detector in our method to compare against F2F. The results are summarized in Table~\ref{tab:f2foffline}. We see that an end-to-end solution does not immediately boost the performance. We believe that it is still an open problem on how to effectively replace tracking and forecasting with an end-to-end solution.

In the streaming setting, 
F2F can be viewed as a detector that compensates its own latency. The results are summarized in Table~\ref{tab:f2fstreaming}. We see that F2F is too expensive compared with other streaming solutions, showing that forecasting can help only if it is fast under our evaluation. Note that the detectors (row 1--2) are not finetuned as in the offline case, which means that they can be further improved.


\begin{table*}[]
\vspace{-1em}
\small
\centering
\caption{Streaming evaluation for the end-to-end method F2F \cite{Luc2018PredictingFI}. The setting is the same as the experiments in the main text. Rows 1 and 2 are the optimized detector and the Kalman filter forecasting solution from the main text. The underlying detectors used are Mask R-CNN ResNet 50 at scale 0.5 and scale 0.75 respectively. Row 3 suggests that F2F has a low streaming AP, due to its forecasting module being very expensive (last column, runtime in milliseconds). For diagnostics purpose, we assume F2F to run as fast as our optimized detector (row 4), and arm it with our scheduling algorithm (row 5). But even so, F2F still under-performs the simple Kalman filter solution}
\label{tab:f2fstreaming}
\begin{tabular}{llccccccc}
\toprule
ID & Method                             & AP            & AP         & AP        & AP        & AP        & AP       & Runtime       \\
\midrule
1  & Detection                          & 12.0          & 24.3          & 7.9           & 1.0          & 25.1          & 10.1          & \textbf{56.7} \\
2  & + Scheduling
\ifstandalonesupplement
    (Alg. 1)
\else
    (Alg. \ref{alg:1})
\fi
+ KF \  & \textbf{17.8} & \textbf{33.3} & \textbf{16.3} & \textbf{3.2} & \textbf{35.2} & \textbf{16.5} & 92.7          \\
3  & F2F                                & 6.2           & 11.1          & 3.4           & 0.8          & 13.1          & 5.2           & 321.6         \\
4  & F2F (Simulated Fast)               & 14.1          & 29.1          & 12.7          & 1.9          & 28.9          & 12.0          & 92.7          \\
5  & + Scheduling
\ifstandalonesupplement
    (Alg. 1)
\else
    (Alg. \ref{alg:1})
\fi
& 15.6          & 33.0          & 15.2          & 2.1          & 30.7          & 13.9          & 92.7     \\
\bottomrule
\end{tabular}
\end{table*}







