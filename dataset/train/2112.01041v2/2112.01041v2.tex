\section{Experimental Results}
\label{exp}

In this section, we empirically validate various properties of N-ImageNet.
With its large scale and label diversity, N-ImageNet is not only a useful benchmark to assess various event-based representations, but can also boost the performance of existing algorithms via pre-training (Section~\ref{performance}).
In Section~\ref{sec:exp_robustness}, we investigate the robustness of event-based classifiers against diverse external conditions, along with the efficacy of our proposed event representation, DiST.

\paragraph{Event Representations for Object Recognition}
We introduce the event representations used throughout our experiments.
The representations are inputs to the object recognition algorithms, while the backbone classifier is fixed to ResNet34~\cite{resnet}.
This is because most event-based object recognition algorithms~\cite{est, matrix_lstm} only differ in the input event representation and share a similar classification backbone.
Eleven event representations are selected for evaluation on N-ImageNet and its variants, as shown in Table~\ref{val_results}.

Two of the representations are learned from the data, namely MatrixLSTM~\cite{matrix_lstm} and Event Spike Tensor (EST)~\cite{est}.
After the events are passed through LSTM~\cite{lstm} for MatrixLSTM and multilayer perceptrons for EST, the outputs are further voxelized to form an image-like representation.

The remaining representations can be classified based on how the timestamps are handled.
Two of these representations discard temporal information, and only use the locations of events.
Binary event image~\cite{binary_image_2, gesture_1} is generated by assigning 1 to pixels with events, and 0 to others.
Event histogram~\cite{event_driving} is an extension of the former, additionally keeping the event count of each pixel.

Three representations use the raw timestamps to leverage temporal information.
Timestamp image~\cite{timestamp_image} caches the newest timestamp for each pixel.
Event image~\cite{ev_gait, evflownet} is a richer representation that concatenates the event histogram~\cite{event_driving} and timestamp image~\cite{timestamp_image}.
Time surface~\cite{hots} extends the timestamp image~\cite{timestamp_image} in a slightly different manner, by passing each timestamp through an exponential filter.
This allows the surface to place more weight on the newest events, which enhances the sharpness of the representation.
The aforementioned representations with raw timestamps can be vulnerable to camera speed changes, as pointed out in Section~\ref{robust}.

We further include representations targeted to enhance robustness.
HATS~\cite{hats} improves the robustness against event camera noise.
Specifically, the outliers are smoothed by aggregating neighboring pixels of the time surface~\cite{hots}.
We use a slightly modified version of HATS~\cite{hats} for more competitive results, and the details are provided in the supplementary material.
Surface of active events with sort normalization~\cite{ace}, which we will refer to as sorted time surface, is robust against camera speed changes as the sorting generates relative timestamps. 
DiST, as explained in Section~\ref{robust}, is robust against both event camera noise and speed changes.
We additionally report results on the variant of DiST without sorting, namely the Discounted Timestamp Image (DiT).
Evaluation on DiT can shed light on the importance of the sorting operation in DiST.

\paragraph{Implementation Details}
All inputs are reshaped into a  grid to restrict GPU memory consumption and shorten inference time.
All models are trained from scratch with a learning rate of , except for the learned representations (MatrixLSTM~\cite{matrix_lstm} and EST~\cite{est}).
The weights are initialized with ImageNet pre-training for these representations to fully replicate the training setup specified in the original works.
We train these models with a learning rate of .
Further information regarding experimental details is provided in the supplementary material.


\begin{table}[]
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
\toprule
Representation & Description & \begin{tabular}[c]{@{}c@{}}\# of\\ Channels\end{tabular} & Accuracy(\%) \\ \Xhline{2\arrayrulewidth}
MatrixLSTM~\cite{matrix_lstm} & \begin{tabular}[c]{@{}c@{}}Learned with\\ LSTM\end{tabular} & 3 & 32.21 \\ \hline
\begin{tabular}[c]{@{}l@{}}Event Spike \\ Tensor~\cite{est}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Learned with\\ MLP\end{tabular} & 18 & \textbf{48.93} \\ \hline
\begin{tabular}[c]{@{}l@{}}Binary Event \\ Image~\cite{binary_image_2} \end{tabular} & \begin{tabular}[c]{@{}c@{}}Binarized\\ event occurence\end{tabular} & 2 & 46.36 \\ \hline
\begin{tabular}[c]{@{}l@{}} Event\\ Histogram~\cite{event_driving}
\end{tabular} & Event counts & 2 & 47.73 \\ \hline
Event Image~\cite{ev_gait} & \begin{tabular}[c]{@{}c@{}}Event counts and\\ newest timestamps\end{tabular} & 4 & 45.77 \\ \hline
Time Surface~\cite{hots} & \begin{tabular}[c]{@{}c@{}}Exponential of\\ newest timestamps\end{tabular} & 2 & 44.32 \\ \hline
HATS~\cite{hats} & \begin{tabular}[c]{@{}c@{}}Aggregated\\ newest timestamps\end{tabular} & 2 & 47.14 \\ \hline
\begin{tabular}[c]{@{}l@{}} Timestamp\\ Image~\cite{timestamp_image}
\end{tabular} & \begin{tabular}[c]{@{}c@{}}Newest \\ timestamps\end{tabular} & 2 & 45.86 \\ \hline
\begin{tabular}[c]{@{}l@{}}Sorted Time \\ Surface~\cite{ace}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sorted newest\\ timestamps\end{tabular} & 2 & 47.90 \\ \hline
DiT & \begin{tabular}[c]{@{}c@{}}Discounted\\ newest timestamps\end{tabular} & 2 & 46.1 \\ \hline
DiST & \begin{tabular}[c]{@{}c@{}}Sorted discounted\\ timestamps \end{tabular} & 2 & \textbf{48.43} \\ \bottomrule
\end{tabular}
}
\caption{N-ImageNet validation accuracy evaluated on various event representations.}
\label{val_results}
\end{table}


\if 0
\begin{table}[]
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c}
\toprule
Representation & Description & Accuracy(\%) \\ \Xhline{3\arrayrulewidth}
MatrixLSTM~\cite{matrix_lstm} & \begin{tabular}[c]{@{}c@{}}Events passed\\ through LSTM\end{tabular} & 32.21 \\ \hline
\begin{tabular}[c]{@{}l@{}}Event Spike \\ Tensor~\cite{est}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Events passed\\ through MLP\end{tabular} & \textbf{48.93} \\ \hline
\begin{tabular}[c]{@{}l@{}}Binary Event \\ Image~\cite{binary_image_2} \end{tabular} & \begin{tabular}[c]{@{}c@{}}Binarized\\ event occurence\end{tabular} & 46.36 \\ \hline
\begin{tabular}[c]{@{}l@{}} Event\\ Histogram~\cite{event_driving}
\end{tabular} & Event counts & 47.73 \\ \hline
Event Image~\cite{ev_gait} & \begin{tabular}[c]{@{}c@{}}Event counts and\\ newest timestamps\end{tabular} & 45.77 \\ \hline
Time Surface~\cite{hots} & \begin{tabular}[c]{@{}c@{}}Exponential of\\ newest timestamps\end{tabular} & 44.32 \\ \hline
HATS~\cite{hats} & \begin{tabular}[c]{@{}c@{}}Aggregated\\ newest timestamps\end{tabular} & 47.14 \\ \hline
\begin{tabular}[c]{@{}l@{}} Timestamp\\ Image~\cite{timestamp_image}
\end{tabular} & \begin{tabular}[c]{@{}c@{}}Newest \\ timestamps\end{tabular} & 45.86 \\ \hline
\begin{tabular}[c]{@{}l@{}}Sorted Time \\ Surface~\cite{ace}\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sorted\\ newest timestamps\end{tabular} & 47.90 \\ \hline
\begin{tabular}[c]{@{}l@{}}Discounted\\ Timestamp Image\end{tabular} & \begin{tabular}[c]{@{}c@{}}Discounted\\ newest timestamps\end{tabular} & 46.1 \\ \hline
\begin{tabular}[c]{@{}l@{}}Discounted Sorted\\ Timestamp Image\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sorted discounted\\ timestamps \end{tabular} & \textbf{48.43} \\ \bottomrule
\end{tabular}
}
\caption{N-ImageNet validation accuracy evaluated on various event representations.}
\label{val_results}
\end{table}
\fi 
\subsection{Evaluation Results with N-ImageNet}

\paragraph{Event-based Object Recognition}
\label{performance}
Table~\ref{val_results} displays the evaluation results of existing event-based object recognition algorithms on N-ImageNet.
The accuracy of the best performing model on N-ImageNet (48.9\%), is far below that of the state-of-the-art model on ImageNet~\cite{imagenet_sota} (90.2\%).
The clear gap indicates that mastering N-ImageNet is still a long way to go.

Other examined models also exhibit a stark contrast in their reported accuracy on existing benchmarks and performance on N-ImageNet.
For example, the test accuracy of the event histogram~\cite{asynet} on N-Cars is 94.5\%, and the test accuracy of MatrixLSTM~\cite{matrix_lstm} on N-Caltech101 is 86.6\%.
These models show a validation accuracy of around  in N-ImageNet, further supporting the difficulty of N-ImageNet.
N-ImageNet is a large-scale, fine-grained benchmark (Table~\ref{b_as_b}) compared to any other existing benchmark and the inherent challenge will foster development in event classifiers that could readily function in the real world.

\paragraph{Assessment on Representations}
The evaluation of various representations on N-ImageNet allows us to make a systematic assessment of different design choices to handle event-based data.
Interestingly, the performance of representations without temporal information (binary event image~\cite{gesture_1,binary_image_2} and event histogram~\cite{event_driving}) are superior to representations directly using raw timestamps (timestamp image~\cite{timestamp_image}, time surface~\cite{hots}, and event image~\cite{ev_gait,evflownet}).
The wide variations in raw timestamps deteriorate the generalization capacity of representations that directly utilize this information.
This notion is further supported by the fact that the representations using relative timestamps (sorted time surface~\cite{ace} and DiST) outperform those using raw timestamps.

It should also be noted that our proposed robust representation, DiST, successfully generalizes to large-scale datasets such as N-ImageNet, and shows performance on par with strong learned representations.
EST~\cite{est} is the best performing model in Table~\ref{val_results}, capable of learning highly expressive encodings of event data, thanks to its event aggregation using multilayer perceptrons.
The performance of DiST is very close to that of EST, although it does not incorporate any learnable module in its event representation.
The suppression of noise from discounting, and the resilience to variations in camera speed from using relative timestamps help DiST to generalize.
If we either omit the discount (sorted time surface~\cite{ace}) or the sorting mechanism (DiT), the performance is inferior to DiST, indicating the importance of the discounting and sorting operations.
We further investigate the robustness of DiST in Section~\ref{sec:exp_robustness}.


\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{\begin{tabularx}{1.25\columnwidth}{l|c|c|c|c}
\toprule
Dataset & N-Cars & CIFAR10-DVS & ASL-DVS & N-Caltech101 \\ \midrule
\# of classes & 2 & 10 & 24 & 101 \\ \midrule
Random & 90.80 & 62.57 & 29.57 & 68.12 \\ 
ImageNet & 91.48 & 70.36 & 53.43 & 80.88 \\ 
N-ImageNet & \textbf{94.73} & \textbf{73.72} & \textbf{58.28} & \textbf{86.81} \\ \bottomrule
\end{tabularx}
}
\caption{Test accuracy of N-ImageNet pretrained models on existing event-based object recognition benchmarks, compared with ImageNet pretraining and random initialization.}
\label{major_stats}
\end{table}

\if0
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{\begin{tabularx}{1.25\columnwidth}{l|cccc}
\toprule
Dataset & N-Cars & CIFAR10-DVS & ASL-DVS & N-Caltech101 \\ \midrule
\# of classes & 2 & 10 & 24 & 101 \\ \midrule\midrule
\begin{tabular}[c]{@{}l@{}}Random\\ Initialization\end{tabular} & 90.80 & 62.57 & 29.57 & 68.12 \\ \midrule
\begin{tabular}[c]{@{}l@{}}ImageNet\\ Pretraining\end{tabular} & 91.48 & 70.36 & 53.43 & 80.88 \\ \midrule
\begin{tabular}[c]{@{}l@{}}N-ImageNet\\ Pretraining\end{tabular} & \textbf{94.73} & \textbf{73.72} & \textbf{58.28} & \textbf{86.81} \\ \bottomrule
\end{tabularx}
}
\caption{Test accuracy of N-ImageNet pretrained models on existing event-based object recognition benchmarks. N-ImageNet pretraining is compared with random initialization and ImageNet pretraining.}
\label{major_stats}
\end{table}
\fi \begin{figure}
    \makebox[\columnwidth][c]{
        \includegraphics[width=0.5\columnwidth]{assets/images/plot_pre-train_ASL-DVS.png}
        \includegraphics[width=0.5\columnwidth]{assets/images/plot_pre-train_N-Cars.png}
    }
    \makebox[\columnwidth][c]{
        \includegraphics[width=0.5\columnwidth]{assets/images/plot_pre-train_N-Caltech101.png}
        \includegraphics[width=0.5\columnwidth]{assets/images/plot_pre-train_CIFAR10-DVS.png}
    }
    \caption{Test accuracy of N-ImageNet pretrained models in resource constrained settings. Each model is trained for 5 epochs with varying amounts of training data.}
    \label{fig:pretrain_four}
\end{figure}

\paragraph{Efficacy of N-ImageNet Pre-Training}
\label{pre-training}
Apart from being a challenging benchmark, the main motivation of N-ImageNet is to provide a large reservoir of event data to pre-train powerful representations for downstream tasks, echoing the role of ImageNet in conventional images.
We validate the effectiveness of N-ImageNet pre-training by observing the capacity to generalize in new, unseen datasets.
Four standard event camera datasets are used for evaluation: N-Caltech101~\cite{n_caltech}, N-Cars~\cite{hats}, CIFAR10-DVS~\cite{cifar_dvs}, and ASL-DVS~\cite{asl_dvs}.
For seven event representations from Table~\ref{val_results}, ResNet34~\cite{resnet} is pre-trained on N-ImageNet and compared with ImageNet pre-training and random initialization.
The seven representations selected are as follows: binary event image~\cite{binary_image_2}, event histogram~\cite{event_driving}, timestamp image~\cite{timestamp_image}, event image~\cite{ev_gait}, time surface~\cite{hots}, sorted time surface~\cite{ace}, and DiST.
In experiments explicated below, we report the averaged test accuracy of the seven representations on each dataset.
Additional details about the experimental setup are specified in the supplementary material.

Table~\ref{major_stats} displays the average test accuracy after training a fixed number of epochs for different initialization schemes.
Note that we only use 800 samples from ASL-DVS~\cite{asl_dvs} for training, as using the whole dataset made all model performances saturate near 99\%.
Networks initiated with N-ImageNet pre-trained weights outperform models from other initialization schemes by a large margin.
Notably, the benefits of pre-training intensify as the number of classes in the datasets increases.
This could be attributed to the fine-grained labels of N-ImageNet, which help models to generalize in challenging datasets where numerous labels are present.
Furthermore, N-ImageNet pre-trained models outperform its competitors in N-Cars and ASL-DVS, which are recordings of real-world objects.
This indicates that although N-ImageNet contains events from monitor displayed images, models pre-trained on it could seamlessly generalize to recognizing real-world objects.

As a practical extension to the previous experiment, we validate the generalization capability of N-ImageNet pre-trained models under resource-constrained settings.
We train the same set of models for 5 epochs with initialization schemes from the previous experiment, under varying numbers of training samples.
Figure~\ref{fig:pretrain_four} shows that N-ImageNet pre-training incurs a large performance improvement across all four evaluated datasets.
The performance gain is further increased when the number of training samples is small.
Such results imply that N-ImageNet pre-training provides strong semantic priors that enable object recognition algorithms to quickly adapt to new datasets, even with a few labeled samples.

\if 0
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
Factor & \multicolumn{2}{c|}{Trajectory} & \multicolumn{2}{c}{Brightness} \\ \hline
Change Amount & Small & Big & \multicolumn{1}{c}{Small} & Big \\ \hline
\begin{tabular}[c]{@{}l@{}}Validation Dataset\\ Number\end{tabular} & 1, 2 & 3, 4, 5 & \multicolumn{1}{c}{7, 8} & 6, 9 \\ \Xhline{2\arrayrulewidth}
MatrixLSTM~\cite{matrix_lstm} & 30.67 & 25.42 & \multicolumn{1}{c}{28.91} & 23.60 \\ \hline
\begin{tabular}[c]{@{}l@{}}Event Spike \\ Tensor~\cite{est}\end{tabular} & 38.82 & 27.28 & \multicolumn{1}{c}{24.89} & 22.36 \\ \hline
\begin{tabular}[c]{@{}l@{}}Binary Event \\ Image~\cite{binary_image_2}\end{tabular} & 36.03 & 30.36 & \multicolumn{1}{c}{30.94} & 25.54 \\ \hline
Event Histogram~\cite{event_driving} & 37.56 & 31.12 & \multicolumn{1}{c}{33.01} & 27.72 \\ \hline
Event Image~\cite{ev_gait} & 35.29 & 30.03 & \multicolumn{1}{c}{32.26} & 27.04 \\ \hline
Time Surface~\cite{hots} & 36.84 & 32.74 & \multicolumn{1}{c}{34.19} & 28.74 \\ \hline
HATS~\cite{hats} & 38.22 & 31.53 & \multicolumn{1}{c}{33.26} & 28.22 \\ \hline
Timestamp Image~\cite{timestamp_image} & 37.62 & 32.43 & \multicolumn{1}{c}{33.27} & 28.04 \\ \hline
\begin{tabular}[c]{@{}l@{}}Sorted Time \\ Surface~\cite{ace}\end{tabular} & 38.34 & 31.95 & \multicolumn{1}{c}{33.47} & 28.38 \\ \hline
\begin{tabular}[c]{@{}l@{}}Discounted\\ Timestamp Image\end{tabular} & 37.89 & 31.79 & \multicolumn{1}{c}{32.66} & 28.42 \\ \hline
\begin{tabular}[c]{@{}l@{}}Discounted Sorted\\ Timestamp Image\end{tabular} & \textbf{40.15} & \textbf{34.42} & \multicolumn{1}{c}{\textbf{35.87}} & \textbf{30.88} \\ \bottomrule
\end{tabular}
}
\caption{Mean accuracy measured on N-ImageNet variants with changes in camera trajectory and brightness.}
\label{degrad_stats}
\end{table}
\fi

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabularx}{1.05\columnwidth}{l|c|c|c|c}
\toprule
Factor & \multicolumn{2}{c|}{Trajectory} & \multicolumn{2}{c}{Brightness} \\ \midrule
Change Amount & Small & Big & \multicolumn{1}{c|}{Small} & Big \\ \midrule
\begin{tabular}[c]{@{}l@{}}Validation Dataset\\ Number\end{tabular} & 1, 2 & 3, 4, 5 & \multicolumn{1}{c|}{7, 8} & 6, 9 \\ \midrule
MatrixLSTM~\cite{matrix_lstm} & 33.00 & 25.62 & \multicolumn{1}{c|}{28.91} & 23.60 \\ 
Event Spike Tensor~\cite{est} & 36.97 & 32.35 & \multicolumn{1}{c|}{24.89} & 22.36 \\ 
Binary Event Image~\cite{binary_image_2} & 36.68 & 31.82 & \multicolumn{1}{c|}{30.94} & 25.54 \\ 
Event Histogram~\cite{event_driving} & 38.72 & 32.49 & \multicolumn{1}{c|}{33.01} & 27.72 \\ 
Event Image~\cite{ev_gait} & 36.52 & 30.96 & \multicolumn{1}{c|}{32.26} & 27.04 \\ 
Time Surface~\cite{hots} & 37.82 & 33.46 & \multicolumn{1}{c|}{34.19} & 28.74 \\ 
HATS~\cite{hats} & 38.95 & 33.28 & \multicolumn{1}{c|}{33.26} & 28.22 \\ 
Timestamp Image~\cite{timestamp_image} & 38.31 & 33.70 & \multicolumn{1}{c|}{33.27} & 28.04 \\ 
Sorted Time Surface~\cite{ace} & 38.92 & 33.69 & \multicolumn{1}{c|}{33.47} & 28.38 \\ 
DiT & 38.21 & 33.61 & \multicolumn{1}{c|}{32.66} & 28.42 \\ 
DiST & \textbf{40.88} & \textbf{35.85} & \multicolumn{1}{c|}{\textbf{35.87}} & \textbf{30.88} \\ \bottomrule
\end{tabularx}
}
\caption{Mean accuracy measured on N-ImageNet variants with changes in camera trajectory and brightness.}
\label{degrad_stats}
\end{table} 
\subsection{Robust Event-Based Object Recognition}
\label{sec:exp_robustness}

\paragraph{\textbf{Validation Accuracy of N-ImageNet Variants}} 
Using the N-ImageNet variants created under various external conditions as described in Section~\ref{robust_datasets}, we examine the robustness of event-based object recognition algorithms.
Table~\ref{degrad_stats} shows the validation accuracy averaged over the trajectory-modified datasets and brightness-modified datasets.
All models displayed in Table~\ref{val_results} are evaluated on the N-ImageNet variants.
We group datasets according to the variation factor, i.e., brightness and trajectory, and the amount of discrepancy between the original setup and the modified setup.

All tested models exhibit a consistent deterioration in performance when evaluated on the N-ImageNet variants.
Furthermore, the amount of performance degradation intensifies as the amount of environment change increases, as shown in Table~\ref{degrad_stats}.
These observations imply that many event-based object recognition algorithms are biased on their training setups, and thus fail to fully generalize in challenging, unseen environments.
In spite of the consistent performance drop however, DiST outperforms its competitors under all external variations shown in Table~\ref{degrad_stats}.
Notably, the ablated versions of DiST, i.e. sorted time surface~\cite{ace}, DiT, and timestamp image~\cite{timestamp_image}, all perform poorly compared to DiST.
Along with the validation accuracy on the original N-ImageNet, this reinforces the necessity of both the discounting and sorting modules of DiST.
DiST's capacity to generalize in unseen environmental conditions demonstrates its effectiveness as a robust representation for event-based object recognition.

\paragraph{\textbf{Representation Consistency}} To further investigate the robustness of DiST, we quantify the content-wise consistency of various event representations.
Seven representations from Table~\ref{val_results} are compared against DiST.
Other three representations (MatrixLSTM~\cite{matrix_lstm}, EST~\cite{est}, event image~\cite{ev_gait}) are omitted as they have different number of channels, which may incur unfair comparison.
For each representation, we assess the structural similarity index measure (SSIM) between the original representation from N-ImageNet and the representation obtained from N-ImageNet variants.
To further elaborate, suppose  and  are event sequences derived from the same image in the ImageNet validation dataset.
Let  and  be the event representations obtained from  and  respectively.
We report , which measures how consistent each representations are amidst external condition changes.

As displayed in Figure~\ref{fig:ssim}, the contents of DiST are more consistent than other competing representations, which can be observed from its highest SSIM value.
The contribution of discounting is greater than that of sorting in representation consistency, which can be seen from the SSIM difference of DiT and DiST.
However, sorting is crucial for robust object recognition, as can be observed from Table~\ref{degrad_stats}, where a clear gap exists between DiT and DiST.
Thus, the interplay between discounting and sorting as a whole enhances the robustness of DiST, further leading to improved performance in N-ImageNet variants.

Apart from the robustness of DiST, it must be noted that the N-ImageNet variants serve as the first benchmark for quantifying robustness in event-based classifiers.
Although DiST shows a consistent improvement from previous models in robustness, it does not fully recover the original N-ImageNet validation accuracy reported in Table~\ref{val_results}.
We expect the N-ImageNet variants to spur future work in robust representations for event-based object recognition.

\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{assets/images/plot_ssim_exp_included.png}
\caption{Structural similarity measure (SSIM) between the representations from N-ImageNet and its variants, grouped by changes in motion and brightness. 
High SSIM indicates that the structure of the representation is stable under external variations.
Note that `Time' and `Exp' denote timestamp image~\cite{timestamp_image} and time surface~\cite{hots}, respectively.
}
\label{fig:ssim}
\end{figure}
