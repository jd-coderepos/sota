\documentclass[11pt]{article}

\def\full{1}


\def\showauthornotes{1}
\def\showtableofcontents{1}
\def\showkeys{0}
\def\showdraftbox{0}
\def\showcolorlinks{1}
\def\usemicrotype{1}
\def\showfixme{0}



\usepackage{etex}








\usepackage{xspace,enumerate}

\usepackage[dvipsnames]{xcolor}

\usepackage[T1]{fontenc}
\usepackage[full]{textcomp}



\usepackage[american]{babel}




\usepackage{mathtools}




\newcommand\hmmax{0} \usepackage{bm}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{itheorem}{Theorem}


\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem{subclaim}{Claim}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem*{conjecture*}{Conjecture}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem*{hypothesis*}{Hypothesis}


\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{protocol}[theorem]{Protocol}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}





\usepackage[letterpaper,
top=1.3in,
bottom=1.3in,
left=1.6in,
right=1.6in]{geometry}




\usepackage[varg]{txfonts} \renewcommand{\mathbb}{\varmathbb}
\renewcommand{\Bbbk}{\varBbbk}



\ifnum\showkeys=1
\usepackage[color]{showkeys}
\fi



\ifnum\showcolorlinks=1
\usepackage[
pagebackref,
letterpaper=true,
colorlinks=true,
urlcolor=blue,
linkcolor=blue,
citecolor=OliveGreen,
]{hyperref}
\fi

\ifnum\showcolorlinks=0
\usepackage[
pagebackref,
letterpaper=true,
colorlinks=false,
pdfborder={0 0 0}
]{hyperref}
\fi


\usepackage{prettyref}



\newcommand{\savehyperref}[2]{\texorpdfstring{\hyperref[#1]{#2}}{#2}}

\newrefformat{eq}{\savehyperref{#1}{\textup{(\ref*{#1})}}}
\newrefformat{lem}{\savehyperref{#1}{Lemma~\ref*{#1}}}
\newrefformat{def}{\savehyperref{#1}{Definition~\ref*{#1}}}
\newrefformat{thm}{\savehyperref{#1}{Theorem~\ref*{#1}}}
\newrefformat{cor}{\savehyperref{#1}{Corollary~\ref*{#1}}}
\newrefformat{cha}{\savehyperref{#1}{Chapter~\ref*{#1}}}
\newrefformat{sec}{\savehyperref{#1}{Section~\ref*{#1}}}
\newrefformat{app}{\savehyperref{#1}{Appendix~\ref*{#1}}}
\newrefformat{tab}{\savehyperref{#1}{Table~\ref*{#1}}}
\newrefformat{fig}{\savehyperref{#1}{Figure~\ref*{#1}}}
\newrefformat{hyp}{\savehyperref{#1}{Hypothesis~\ref*{#1}}}
\newrefformat{alg}{\savehyperref{#1}{Algorithm~\ref*{#1}}}
\newrefformat{rem}{\savehyperref{#1}{Remark~\ref*{#1}}}
\newrefformat{item}{\savehyperref{#1}{Item~\ref*{#1}}}
\newrefformat{step}{\savehyperref{#1}{step~\ref*{#1}}}
\newrefformat{conj}{\savehyperref{#1}{Conjecture~\ref*{#1}}}
\newrefformat{fact}{\savehyperref{#1}{Fact~\ref*{#1}}}
\newrefformat{prop}{\savehyperref{#1}{Proposition~\ref*{#1}}}
\newrefformat{prob}{\savehyperref{#1}{Problem~\ref*{#1}}}
\newrefformat{claim}{\savehyperref{#1}{Claim~\ref*{#1}}}
\newrefformat{relax}{\savehyperref{#1}{Relaxation~\ref*{#1}}}
\newrefformat{red}{\savehyperref{#1}{Reduction~\ref*{#1}}}
\newrefformat{part}{\savehyperref{#1}{Part~\ref*{#1}}}





\newcommand{\Sref}[1]{\hyperref[#1]{\S\ref*{#1}}}

\usepackage{nicefrac}
\newcommand{\flatfrac}[2]{#1/#2}
\newcommand{\varflatfrac}[2]{#1\textfractionsolidus#2}

\let\nfrac=\nicefrac
\let\ffrac=\flatfrac


\newcommand{\half}{\nicefrac12}
\newcommand{\onequarter}{\nicefrac14}
\newcommand{\threequarter}{\nicefrac34}






\ifnum\usemicrotype=1
\usepackage{microtype}
\fi


\ifnum\showauthornotes=1
\newcommand{\Authornote}[2]{{\sffamily\small\color{red}{[#1: #2]}}}
\newcommand{\Authorcomment}[2]{{\sffamily\small\color{gray}{[#1: #2]}}}
\newcommand{\Authorstartcomment}[1]{\sffamily\small\color{gray}[#1: }
\newcommand{\Authorendcomment}{]\rmfamily\normalsize\color{black}}
\newcommand{\Authorfnote}[2]{\footnote{\color{red}{#1: #2}}}
\newcommand{\Authorfixme}[1]{\Authornote{#1}{\textbf{??}}}
\newcommand{\Authormarginmark}[1]{\marginpar{\textcolor{red}{\fbox{\Large #1:!}}}}
\else
\newcommand{\Authornote}[2]{}
\newcommand{\Authorcomment}[2]{}
\newcommand{\Authorstartcomment}[1]{}
\newcommand{\Authorendcomment}{}
\newcommand{\Authorfnote}[2]{}
\newcommand{\Authorfixme}[1]{}
\newcommand{\Authormarginmark}[1]{}
\fi

\newcommand{\Dnote}{\Authornote{D}}
\newcommand{\Dcomment}{\Authorcomment{D}}
\newcommand{\Dstartcomment}{\Authorstartcomment{D}}
\newcommand{\Dendcomment}{\Authorendcomment}
\newcommand{\Dfixme}{\Authorfixme{D}}
\newcommand{\Dmarginmark}{\Authormarginmark{D}}
\newcommand{\Dfnote}{\Authorfnote{D}}

\newcommand{\Pnote}{\Authornote{P}}
\newcommand{\Pfnote}{\Authorfnote{P}}

\newcommand{\PRnote}{\Authornote{PR}}
\newcommand{\PRfnote}{\Authorfnote{PR}}

\newcommand{\PTnote}{\Authornote{PT}}
\newcommand{\PTfnote}{\Authorfnote{PT}}

\newcommand{\PGnote}{\Authornote{PG}}
\newcommand{\PGfnote}{\Authorfnote{PG}}

\newcommand{\Bnote}{\Authornote{B}}
\newcommand{\Bfnote}{\Authorfnote{B}}

\newcommand{\Mnote}{\Authornote{M}}
\newcommand{\Mfnote}{\Authorfnote{M}}

\newcommand{\Rnote}{\Authornote{M}}
\newcommand{\Rfnote}{\Authorfnote{M}}

\newcommand{\Nnote}{\Authornote{N}}
\newcommand{\Nfnote}{\Authorfnote{N}}

\newcommand{\Snote}{\Authornote{S}}
\newcommand{\Sfnote}{\Authorfnote{S}}




\newcommand{\redmarginmarker}{\marginpar{\textcolor{red}{\fbox{\Large !}}}}

\newcommand{\FIXME}{\textbf{\textcolor{red}{FIXME!!}}\redmarginmarker}

\ifnum\showfixme=0
\renewcommand{\FIXME}{}
\fi


\usepackage{boxedminipage}

\newenvironment{mybox}
{\center \noindent\begin{boxedminipage}{1.0\linewidth}}
{\end{boxedminipage}
\noindent
}


\newcommand{\paren}[1]{(#1)}
\newcommand{\Paren}[1]{\left(#1\right)}
\newcommand{\bigparen}[1]{\big(#1\big)}
\newcommand{\Bigparen}[1]{\Big(#1\Big)}
\newcommand{\brac}[1]{[#1]}
\newcommand{\Brac}[1]{\left[#1\right]}
\newcommand{\bigbrac}[1]{\big[#1\big]}
\newcommand{\Bigbrac}[1]{\Big[#1\Big]}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\Abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\bigabs}[1]{\big\lvert#1\big\rvert}
\newcommand{\Bigabs}[1]{\Big\lvert#1\Big\rvert}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\Card}[1]{\left\lvert#1\right\rvert}
\newcommand{\bigcard}[1]{\big\lvert#1\big\rvert}
\newcommand{\Bigcard}[1]{\Big\lvert#1\Big\rvert}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\Set}[1]{\left\{#1\right\}}
\newcommand{\bigset}[1]{\big\{#1\big\}}
\newcommand{\Bigset}[1]{\Big\{#1\Big\}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\Norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bignorm}[1]{\big\lVert#1\big\rVert}
\newcommand{\Bignorm}[1]{\Big\lVert#1\Big\rVert}
\newcommand{\normt}[1]{\norm{#1}_2}
\newcommand{\Normt}[1]{\Norm{#1}_2}
\newcommand{\bignormt}[1]{\bignorm{#1}_2}
\newcommand{\Bignormt}[1]{\Bignorm{#1}_2}
\newcommand{\snormt}[1]{\norm{#1}^2_2}
\newcommand{\Snormt}[1]{\Norm{#1}^2_2}
\newcommand{\bigsnormt}[1]{\bignorm{#1}^2_2}
\newcommand{\Bigsnormt}[1]{\Bignorm{#1}^2_2}
\newcommand{\snorm}[1]{\norm{#1}^2}
\newcommand{\Snorm}[1]{\Norm{#1}^2}
\newcommand{\bigsnorm}[1]{\bignorm{#1}^2}
\newcommand{\Bigsnorm}[1]{\Bignorm{#1}^2}
\newcommand{\normo}[1]{\norm{#1}_1}
\newcommand{\Normo}[1]{\Norm{#1}_1}
\newcommand{\bignormo}[1]{\bignorm{#1}_1}
\newcommand{\Bignormo}[1]{\Bignorm{#1}_1}
\newcommand{\normi}[1]{\norm{#1}_\infty}
\newcommand{\Normi}[1]{\Norm{#1}_\infty}
\newcommand{\bignormi}[1]{\bignorm{#1}_\infty}
\newcommand{\Bignormi}[1]{\Bignorm{#1}_\infty}
\newcommand{\iprod}[1]{\langle#1\rangle}
\newcommand{\Iprod}[1]{\left\langle#1\right\rangle}
\newcommand{\bigiprod}[1]{\big\langle#1\big\rangle}
\newcommand{\Bigiprod}[1]{\Big\langle#1\Big\rangle}


\newcommand{\Esymb}{\mathbb{E}}
\newcommand{\Psymb}{\mathbb{P}}
\newcommand{\Vsymb}{\mathbb{V}}

\DeclareMathOperator*{\E}{\Esymb}
\DeclareMathOperator*{\Var}{\Vsymb}
\DeclareMathOperator*{\ProbOp}{\Psymb}

\renewcommand{\Pr}{\ProbOp}

\newcommand{\prob}[1]{\Pr\set{#1}}
\newcommand{\Prob}[2][]{\Pr_{{#1}}\Set{#2}}

\newcommand{\ex}[1]{\E\brac{#1}}
\newcommand{\Ex}[2][]{\E_{{#1}}\Brac{#2}}
\newcommand{\varex}[1]{\E\paren{#1}}
\newcommand{\varEx}[1]{\E\Paren{#1}}

\newcommand{\given}{\;\middle\vert\;}








\newcommand{\suchthat}{\;\middle\vert\;}

\newcommand{\tensor}{\otimes}

\newcommand{\where}{\text{where}}
\newcommand{\textparen}[1]{\text{(#1)}}
\newcommand{\using}[1]{\textparen{using #1}}
\ifx\because\undefined
\newcommand{\because}[1]{\textparen{because #1}}
\else
\renewcommand{\because}[1]{\textparen{because #1}}
\fi
\newcommand{\by}[1]{\textparen{by #1}}


\newcommand{\sge}{\succeq}
\newcommand{\sle}{\preceq}

\newcommand{\lmin}{\lambda_{\min}}
\newcommand{\lmax}{\lambda_{\max}}

\newcommand{\signs}{\set{1,-1}}
\newcommand{\varsigns}{\set{\pm 1}}
\newcommand{\maximize}{\mathop{\textrm{maximize}}}
\newcommand{\minimize}{\mathop{\textrm{minimize}}}
\newcommand{\subjectto}{\mathop{\textrm{subject to}}}

\renewcommand{\ij}{{ij}}

\newcommand{\symdiff}{\Delta}
\newcommand{\varsymdiff}{\bigtriangleup}

\newcommand{\bits}{\{0,1\}}
\newcommand{\sbits}{\{\pm1\}}

\renewcommand{\labelitemi}{--}

\newcommand{\listoptions}{\labelsep0mm\topsep-0mm\itemindent-6mm\itemsep0mm}
\newcommand{\displayoptions}[1]{\abovedisplayshortskip#1mm\belowdisplayshortskip#1mm\abovedisplayskip#1mm\belowdisplayskip#1mm}





\newcommand{\super}[2]{#1^{\paren{#2}}}
\newcommand{\varsuper}[2]{#1^{\scriptscriptstyle\paren{#2}}}

\newcommand{\tensorpower}[2]{#1^{\tensor #2}}


\newcommand{\inv}[1]{{#1^{-1}}}

\newcommand{\dual}[1]{{#1^*}}



\newcommand{\vbig}{\vphantom{\bigoplus}}

\newcommand{\sm}{\setminus}

\newcommand{\defeq}{\stackrel{\mathrm{def}}=}

\newcommand{\seteq}{\mathrel{\mathop:}=}

\newcommand{\from}{\colon}

\newcommand{\bigmid}{~\big|~}
\newcommand{\Bigmid}{~\Big|~}
\newcommand{\Mid}{\;\middle\vert\;}

\renewcommand{\vec}[1]{{\bm{#1}}}
\newcommand{\bvec}[1]{\bar{\vec{#1}}}
\newcommand{\pvec}[1]{\vec{#1}'}
\newcommand{\tvec}[1]{{\tilde{\vec{#1}}}}

\newcommand{\mper}{\,.}
\newcommand{\mcom}{\,,}

\newcommand\bdot\bullet

\newcommand{\trsp}[1]{{#1}^\dagger}

\newcommand{\Ind}{\mathbb I}






\renewcommand{\th}{\textsuperscript{th}\xspace}
\newcommand{\st}{\textsuperscript{st}\xspace}
\newcommand{\nd}{\textsuperscript{nd}\xspace}
\newcommand{\rd}{\textsuperscript{rd}\xspace}


\DeclareMathOperator{\Inf}{Inf}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\SDP}{SDP}
\DeclareMathOperator{\sdp}{sdp}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\LP}{LP}
\DeclareMathOperator{\OPT}{OPT}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\qpoly}{qpoly}
\DeclareMathOperator{\qpolylog}{qpolylog}
\DeclareMathOperator{\qqpoly}{qqpoly}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\Conv}{Conv}

\DeclareMathOperator{\rank}{rank}

\DeclareMathOperator*{\median}{median}
\DeclareMathOperator*{\Median}{Median}

\DeclareMathOperator*{\varsum}{{\textstyle \sum}}
\DeclareMathOperator{\tsum}{{\textstyle \sum}}






\newcommand{\diffmacro}[1]{\,\mathrm{d}#1}

\newcommand{\dmu}{\diffmacro{\mu}}
\newcommand{\dgamma}{\diffmacro{\gamma}}
\newcommand{\dlambda}{\diffmacro{\lambda}}
\newcommand{\dx}{\diffmacro{x}}
\newcommand{\dt}{\diffmacro{t}}





\newcommand{\ie}{i.e.,\xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\Eg}{E.g.,\xspace}
\newcommand{\phd}{Ph.\,D.\xspace}
\newcommand{\msc}{M.\,S.\xspace}
\newcommand{\bsc}{B.\,S.\xspace}

\newcommand{\etal}{et al.\xspace}

\newcommand{\iid}{i.i.d.\xspace}



\newcommand\naive{na\"{\i}ve\xspace}
\newcommand\Naive{Na\"{\i}ve\xspace}
\newcommand\naively{na\"{\i}vely\xspace}
\newcommand\Naively{Na\"{\i}vely\xspace}



\newcommand{\Erdos}{Erd\H{o}s\xspace}
\newcommand{\Renyi}{R\'enyi\xspace}
\newcommand{\Lovasz}{Lov\'asz\xspace}
\newcommand{\Juhasz}{Juh\'asz\xspace}
\newcommand{\Bollobas}{Bollob\'as\xspace}
\newcommand{\Furedi}{F\"uredi\xspace}
\newcommand{\Komlos}{Koml\'os\xspace}
\newcommand{\Luczak}{\L uczak\xspace}
\newcommand{\Kucera}{Ku\v{c}era\xspace}
\newcommand{\Szemeredi}{Szemer\'edi\xspace}
\newcommand{\Hastad}{H{\aa}stad\xspace}

\newcommand{\Z}{\mathbb Z}
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\Rnn}{\R_+}
\newcommand{\varR}{\Re}
\newcommand{\varRnn}{\varR_+}






\newcommand{\problemmacro}[1]{\texorpdfstring{\textsc{#1}}{#1}\xspace}

\newcommand{\uniquegames}{\problemmacro{Unique Games}}
\newcommand{\maxcut}{\problemmacro{Max Cut}}
\newcommand{\multicut}{\problemmacro{Multi Cut}}
\newcommand{\vertexcover}{\problemmacro{Vertex Cover}}
\newcommand{\balancedseparator}{\problemmacro{Balanced Separator}}
\newcommand{\maxtwosat}{\problemmacro{Max -Sat}}
\newcommand{\maxthreesat}{\problemmacro{Max -Sat}}
\newcommand{\maxthreelin}{\problemmacro{Max -Lin}}
\newcommand{\threesat}{\problemmacro{-Sat}}
\newcommand{\labelcover}{\problemmacro{Label Cover}}
\newcommand{\maxksat}{\problemmacro{Max -Sat}}
\newcommand{\mas}{\problemmacro{Maximum Acyclic Subgraph}}
\newcommand{\kwaycut}{\problemmacro{-way Cut}}
\newcommand{\sparsestcut}{\problemmacro{Sparsest Cut}}
\newcommand{\betweenness}{\problemmacro{Betweenness}}
\newcommand{\uniformsparsestcut}{\problemmacro{Uniform Sparsest Cut}}
\newcommand{\grothendieckproblem}{\problemmacro{Grothendieck Problem}}
\newcommand{\maxfoursat}{\problemmacro{Max -Sat}}
\newcommand{\maxkcsp}{\problemmacro{Max -CSP}}
\newcommand{\maxdicut}{\problemmacro{Max DiCut}}
\newcommand{\maxcutgain}{\problemmacro{Max Cut Gain}}
\newcommand{\smallsetexpansion}{\problemmacro{Small-Set Expansion}}
\newcommand{\minbisection}{\problemmacro{Min Bisection}}
\newcommand{\minimumlineararrangement}{\problemmacro{Minimum Linear Arrangement}}
\newcommand{\maxtwolin}{\problemmacro{Max -Lin}}
\newcommand{\gammamaxlin}{\problemmacro{-Max -Lin}}
\newcommand{\basicsdp}{\problemmacro{Basic Sdp}}
\newcommand{\dgames}{\problemmacro{-to- Games}}
\newcommand{\maxclique}{\problemmacro{Max Clique}}
\newcommand{\densestksubgraph}{\problemmacro{Densest -Subgraph}}

\newcommand{\maxbisection}{\problemmacro{Max Bisection}}
\newcommand{\mincut}{\problemmacro{Min Cut}}




\newcommand{\cA}{\mathcal A}
\newcommand{\cB}{\mathcal B}
\newcommand{\cC}{\mathcal C}
\newcommand{\cD}{\mathcal D}
\newcommand{\cE}{\mathcal E}
\newcommand{\cF}{\mathcal F}
\newcommand{\cG}{\mathcal G}
\newcommand{\cH}{\mathcal H}
\newcommand{\cI}{\mathcal I}
\newcommand{\cJ}{\mathcal J}
\newcommand{\cK}{\mathcal K}
\newcommand{\cL}{\mathcal L}
\newcommand{\cM}{\mathcal M}
\newcommand{\cN}{\mathcal N}
\newcommand{\cO}{\mathcal O}
\newcommand{\cP}{\mathcal P}
\newcommand{\cQ}{\mathcal Q}
\newcommand{\cR}{\mathcal R}
\newcommand{\cS}{\mathcal S}
\newcommand{\cT}{\mathcal T}
\newcommand{\cU}{\mathcal U}
\newcommand{\cV}{\mathcal V}
\newcommand{\cW}{\mathcal W}
\newcommand{\cX}{\mathcal X}
\newcommand{\cY}{\mathcal Y}
\newcommand{\cZ}{\mathcal Z}

\newcommand{\bbB}{\mathbb B}
\newcommand{\bbS}{\mathbb S}
\newcommand{\bbR}{\mathbb R}
\newcommand{\bbZ}{\mathbb Z}
\newcommand{\bbI}{\mathbb I}
\newcommand{\bbQ}{\mathbb Q}
\newcommand{\bbP}{\mathbb P}
\newcommand{\bbE}{\mathbb E}

\newcommand{\sfE}{\mathsf E}


\renewcommand{\leq}{\leqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\ge}{\geqslant}


\ifnum\showdraftbox=1
\newcommand{\draftbox}{\begin{center}
  \fbox{\begin{minipage}{2in}\begin{center}\Large\textsc{Working Draft}\\Please do not distribute\end{center}\end{minipage}}\end{center}
\vspace{0.2cm}}
\else
\newcommand{\draftbox}{}
\fi




\let\epsilon=\varepsilon

\numberwithin{equation}{section}






\newcommand{\MYstore}[2]{\global\expandafter \def \csname MYMEMORY #1 \endcsname{#2}}

\newcommand{\MYload}[1]{\csname MYMEMORY #1 \endcsname }

\newcommand{\MYnewlabel}[1]{\newcommand\MYcurrentlabel{#1}\MYoldlabel{#1}}

\newcommand{\MYdummylabel}[1]{}

\newcommand{\torestate}[1]{\let\MYoldlabel\label \let\label\MYnewlabel #1\MYstore{\MYcurrentlabel}{#1}\let\label\MYoldlabel }

\newcommand{\restatetheorem}[1]{\let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{theorem*}[Restatement of \prettyref{#1}]
    \MYload{#1}
  \end{theorem*}
  \let\label\MYoldlabel
}

\newcommand{\restatelemma}[1]{\let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{lemma*}[Restatement of \prettyref{#1}]
    \MYload{#1}
  \end{lemma*}
  \let\label\MYoldlabel
}

\newcommand{\restateprop}[1]{\let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{proposition*}[Restatement of \prettyref{#1}]
    \MYload{#1}
  \end{proposition*}
  \let\label\MYoldlabel
}

\newcommand{\restatefact}[1]{\let\MYoldlabel\label
  \let\label\MYdummylabel
  \begin{fact*}[Restatement of \prettyref{#1}]
    \MYload{#1}
  \end{fact*}
  \let\label\MYoldlabel
}

\newcommand{\restate}[1]{\let\MYoldlabel\label
  \let\label\MYdummylabel
  \MYload{#1}
  \let\label\MYoldlabel
}




\newcommand{\addreferencesection}{
  \phantomsection
  \addcontentsline{toc}{section}{References}
}




\newcommand{\la}{\leftarrow}
\newcommand{\sse}{\subseteq}
\newcommand{\ra}{\rightarrow}
\newcommand{\e}{\epsilon}
\newcommand{\eps}{\epsilon}
\newcommand{\eset}{\emptyset}





\let\origparagraph\paragraph
\renewcommand{\paragraph}[1]{\origparagraph{#1.}}




\allowdisplaybreaks





\sloppy





\newcommand{\cclassmacro}[1]{\texorpdfstring{\textbf{#1}}{#1}\xspace}
\newcommand{\Ptime}{\cclassmacro{P}}
\newcommand{\NP}{\cclassmacro{NP}}
\newcommand{\SUBEXP}{\cclassmacro{SUBEXP}}



\newcommand{\dict} {\textsf{DICT}}


\newcommand{\orv}[1]{\lowercase{#1}}

\newcommand{\mrv}[1]{\bm{\lowercase{#1}}}

\newcommand{\erv}[1]{\mathcal{#1}}


\newcommand{\lcf}[1]{\mathsf{\uppercase{#1}}}

\newcommand{\orf}[1]{\mathcal{\uppercase{#1}}}

\newcommand{\mrf}[1]{\bm{\mathcal{\uppercase{#1}}}}

\newcommand{\opl}[1]{{\uppercase{#1}}}

\newcommand{\mpl}[1]{\bm{\uppercase{#1}}}

\newcommand{\msf}[1]{\mathsf{#1}}
\newcommand{\mcl}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mv}[1]{\mathbf{#1}}
\newcommand{\gwrel} {\textsf{GW}\xspace}

\newcommand{\round}{\msf{Round}}
\newcommand{\struncate}{f_{[-1,1]}}
\newcommand{\T}{\mathrm{T}}
 
\newcommand{\RM}{\ensuremath{\text{\sf RM}}}

\let\pref=\prettyref
\let\tp=\tensorpower
\newcommand{\arank}{\rank^*}

\DeclareMathOperator{\cond}{\Phi}

\renewcommand{\vol}{\mu}
\newcommand{\sst}{\substack}
\newcommand{\GF}[1]{\mathbb F_{#1}}


\DeclareMathOperator{\TC}{Test_{\mc{C}}}
\DeclareMathOperator{\TRM}{Test_{\RM}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\vc}[1]{(-1)^{#1}}
\newcommand{\mb}[1]{#1} \newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mf}[1]{\mathbb{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\ol}[1]{\ensuremath{\overline{#1}}}
\newcommand{\wt}{{\sf wt}}
\newcommand{\eat}[1]{}

\newcommand{\maxlintwo}{{\mathsf{MaxLin}}(2)}
\newcommand{\LH}{\mathsf{LH}}
\newcommand{\SA}{\mathsf{SA}}
\newcommand{\NS}{\mathsf{NS}}


\setcounter{page}{1}

\usepackage{pdfpages}

\ifnum\full=1
\parskip=1ex
\else
\parskip=0.1ex
\fi


\title{Approximating CSPs with Global Cardinality Constraints Using SDP Hierarchies}

\author{Prasad Raghavendra \and Ning Tan}


\date{}


\begin{document}

\maketitle
\draftbox
\thispagestyle{empty}



\begin{abstract}



	This work is concerned with approximating constraint
satisfaction problems (CSPs) with an
	additional global cardinality constraints.  For example, \maxcut is a boolean CSP where the input is a
	graph  and the goal is to find a cut  that maximizes the number
	of crossing edges, .  The \maxbisection problem is a variant of \maxcut
	with an additional global constraint that each side of the cut
	has exactly half the vertices, i.e., .
	Several other natural optimization problems like \minbisection and approximating Graph Expansion can be
	formulated as CSPs with global constraints.

	In this work, we formulate a general approach towards
	approximating CSPs with global constraints using SDP
	hierarchies.  To demonstrate the approach we present the
	following results:

	\begin{itemize}
	\item Using the Lasserre hierarchy, we present an algorithm
		that runs in time  that given
		an instance of \maxbisection with value ,
		finds a bisection with value .
		This approximation is near-optimal (up to constant
		factors in ) under the Unique Games Conjecture.
    \item By a computer-assisted proof, we show that the same
algorithm also achieves a 0.85-approximation for \maxbisection,
improving on the previous bound of 0.70 (note that it is \uniquegames
hard to approximate better than a 0.878 factor). The same algorithm also yields a 0.92-approximation for \maxtwosat with cardinality constraints.
	\item   For every CSP with a global cardinality constraints, we present a generic conversion from integrality gap
		instances for the Lasserre hierarchy to a {\it dictatorship
		test} whose soundness is at most integrality gap.
		Dictatorship testing gadgets are central to hardness
		results for CSPs, and a generic conversion of the
		above nature lies at the core of the tight Unique Games
		based hardness result for CSPs. \cite{Raghavendra08}

\end{itemize}
\end{abstract}
\clearpage



\ifnum\showtableofcontents=1
{
\tableofcontents
\thispagestyle{empty}
\clearpage
 }
\fi


\setcounter{page}{1}
\ifnum\full=0 \vspace{-8pt}\fi

\section{Introduction} \label{sec:intro}

Constraint Satisfaction Problems (CSP) are a class of fundamental
optimization problems that have been extensively studied in
approximation algorithms and hardness of approximation.  In  a
constraint satisfaction problem, the input consists of a set of
variables taking values over a fixed finite domain (say ) and
a set of {\it local} constraints on them.  The constraints are {\it
local} in that each of them depends on at most  variables for some
fixed constant .  The goal is to find an assignment to the
variables that satisfies the maximum number of constraints.

Over the last two decades, there has been much progress in understanding the approximability of
CSPs.  On the algorithmic front, semidefinite programming (SDP) has been
used with great success in approximating several well-known CSPs such
as \maxcut \cite{GoemansW95}, \maxtwosat \cite{CharikarMM07a} and
\maxthreesat \cite{KarloffZ97}.  More recently, these
algorithmic results have been unified and generalized to the entire class of constraint
satisfaction problems \cite{RaghavendraS09b}.  With the development of PCPs and long code based reductions, tight
hardness results matching the SDP based algorithms have been shown for
some CSPs such as Max-3-SAT \cite{Hastad01}.  In a surprising
development under the Unique Games Conjecture, semidefinite programming based algorithms have been shown
to be optimal for \maxcut \cite{KhotKMO07}, \maxtwosat
\cite{Austrin07a} and more generally every constraint satisfaction problem \cite{Raghavendra08}.

Unfortunately, neither SDP based algorithms nor the hardness
results extend satisfactorily to optimization problems with {\it
non-local} constraints.  Part of the reason is that the nice framework of SDP based approximation algorithms and matching
hardness results crucially rely on the {\it locality} of the constraints
involved.  Perhaps the simplest non-local constraint would be to
restrict the cardinality of the assignment, i.e., the number of ones
in the assignment.  Variants of CSPs with even a single cardinality constraint are not well-understood.
Optimization problems of this nature, namely constraint satisfaction problems with global cardinality constraints are the primary focus of this work.
Several important problems such as \maxbisection, \minbisection,
\smallsetexpansion can be formulated as CSPs with a single global cardinality
constraint.

As an illustrative example, let us consider the \maxbisection problem
which is also part of the focus of this work.  The \maxbisection problem is
a variant of the much well-studied \maxcut problem
\cite{GoemansW95,KhotKMO07}.  In the \maxcut
problem the goal is to partition the vertices of the input graph in to
two sets while maximizing the number of crossing edges.  The
\maxbisection problem includes an additional cardinality constraint
that both sides of the partition have exactly half the vertices of the
graph.
The seemingly mild cardinality constraint appears to change the nature
of the problem.  While \maxcut\ admits a factor  approximation
algorithm~\cite{GoemansW95}, the best known
approximation factor for \maxbisection\ equals
~\cite{FeigeL06}, improving on previous bounds of
~\cite{FriezeJ97}, ~\cite{Ye01}, and
~\cite{HalperinZ02}. These algorithms proceed by rounding
the natural semidefinite programming relaxation analogous to the
Goemans-Williamson SDP for \maxcut.  Guruswami \etal
\cite{GuruswamiMRSZ11} showed that this natural SDP relaxation has a large integrality gap: the SDP
optimum could be  whereas every bisection might only cut less than
 fraction of the edges!  In particular, this implies that none
of these algorithms guarantee a solution with value close to  even
if there exists a perfect bisection in the graph.  More recently,
using a combination of graph-decomposition, bruteforce enumeration and
SDP rounding, Guruswami \etal \cite{GuruswamiMRSZ11} obtained an algorithm that
outputs a  bisection on a graph that has a
bisection of value .

A simple approximation preserving
reduction from \maxcut\ shows that \maxbisection\ is no easier to
approximate than \maxcut\ (the reduction is simply to take two
disjoint copies of the \maxcut\ instance). Therefore, the factor
 NP-hardness \cite{Hastad01,TrevisanSSW00} and the factor  Unique-Games
hardness for \maxcut~\cite{KhotKMO07} also applies to the
\maxbisection problem.  In fact, a stronger hardness result of factor 
was shown in ~\cite{HolmerinK04} assuming .
Yet, these hardness results for \maxbisection are far from matching the
best known approximation algorithm that only achieves a  factor.

\paragraph{SDP Hierarchies}

Almost all known approximation algorithms for constraint satisfaction
problems are based on a fairly minimal SDP relaxation of the problem.
In fact, there exists a simple semidefinite program with linear number
of constraints (see \cite{Raghavendra08, RaghavendraS09b}) that yields the
best known approximation ratio for every CSP. This leaves open the possibility that stronger SDP relaxations such as
those obtained using the Lovasz-Schriver, Sherali-Adams and Lasserre
SDP hierarchies yield better approximations for CSPs.  Unfortunately,
there is evidence suggesting that the stronger SDP relaxations yield
no better approximation for CSPs than the simple semidefinite program
suggested in \cite{Raghavendra08,RaghavendraS09b}.  First, under the
Unique Games Conjecture, it is \NP-hard to approximate any CSP to a
factor better than that yielded by the simple semidefinite program
\cite{Raghavendra08}.  Moreover, a few recent works \cite{KhotS09,
Tulsiani09, RaghavendraS09c} have constructed
integrality gap instances for strong SDP relaxations of CSPs, obtained via Sherali-Adams and Lasserre
hierarchies.  For instance, the integrality gap instances in
\cite{KhotS09,RaghavendraS09c} demonstrate that up to 
rounds of the Sherali-Adams SDP hierarchy yields no better approximation
to \maxcut than the simple Goemans-Williamson semidefinite program
\cite{GoemansW95}.

The situation for CSPs with cardinality constraints promises
to be different.  For the \balancedseparator problem -- a CSP with a
global cardinality constraint, Arora \etal \cite{AroraRV04} obtained
an improved approximation of  by appealing to a
stronger SDP relaxation with triangle inequalities.  In case of
\maxbisection, one of the components of the algorithm of
\cite{GuruswamiMRSZ11} is a {\it brute-force search} -- a technique
that could quite possibly be carried out using SDP hierarchies.

Despite their promise, there are only a handful of applications of SDP hierarchies
in to approximation algorithms, most notably to approximating graph
expansion \cite{AroraRV04}, graph coloring and hypergraph independent
sets.  Moreover, there are few general techniques to round solutions to
SDP hierarchies, and analyze their integrality gap.

In an exciting development, fairly general techniques to round
solutions to SDP hierarchies (particularly the Lasserre hierarchy) has
emerged in recent works by Barak \etal \cite{BarakRS11} and Guruswami
and Sinop \cite{GuruswamiS11}.
Both these works (concurrently and independently) developed a fairly
general approach to round solutions to the Lasserre hierarchy using
an appropriate notion of local-global correlations in the SDP solution.
As an application of the technique, both the works obtain a
subexponential time algorithm for the Unique
Games problem using the Lasserre SDP hierarchy.  These works also
demonstrate several interesting applications of the technique.

Barak \etal \cite{BarakRS11} obtain an algorithm for arbitrary
-CSPs with an approximation guarantee depending on the spectrum of the
input graph.  Specifically, the result implies a quasi-polynomial time approximation scheme for every -CSP on
low threshold rank graphs, namely graphs with few large eigenvalues.

Guruswami and Sinop \cite{GuruswamiS11} obtain a general algorithm to
optimize quadratic integer programs with positive semidefinite forms
and global linear constraints.  Several interesting
problems including {\it -CSPs with  global cardinality constraints} such
as \maxbisection, \minbisection and \balancedseparator fall in to
the framework of \cite{GuruswamiS11}.  However, the approximation
guarantee of their algorithm depends on the spectrum of the input
graph, and is therefore effective only on the special class of
low threshold rank graphs.

\ifnum\full=0 \vspace{-8pt}\fi
\subsection{Our Results}
\ifnum\full=0 \vspace{-8pt}\fi
In this paper, we develop a general approach to approximate CSPs with
global cardinality constraints using the Lasserre SDP hierarchy.

We illustrate the approach with an improved approximation algorithm
for the \maxbisection and balanced \maxtwosat problems.  For the
\maxbisection problem, we show the following result.

\begin{theorem} \label{thm:max-bisection}
For every , there exists an algorithm for \maxbisection
that runs in time  and obtains the following
approximation guarantees,
\begin{itemize}
	\item The output bisection has value at least  times the
		optimal max bisection.
	\item For every , given an instance  with a
		bisection of value , the algorithm
		outputs a bisection of value at least .
\end{itemize}
\end{theorem}

Note that the approximation guarantee of  on
instances with  is nearly optimal (up to constant factors
in the )
under the Unique Games Conjecture.  This follows from the
corresponding hardness of \maxcut and the reduction from \maxcut to
\maxbisection.

Our approach is robust in that it also yields similar
approximation guarantees to the more general -\maxcut problem
where the goal is to find a cut with exactly -fraction of
vertices on one side of the cut.  More generally, the algorithm also
generalizes to a weighted version of \maxbisection, where the vertices
have weights and the cut has approximately half the weight on each
side.  \footnote{Note that in the weighted case, finding any exact bisection is at least as
hard as subset-sum problem.}

The same algorithm also yields an approximation to the complementary
problem of \minbisection.  Formally, we obtain the following
approximation algorithm for \minbisection and
-\balancedseparator.

\begin{theorem} \label{thm:minbisection}
For every , there exists an algorithm running in time
, which given a graph with a bisection (-balanced separator) cutting
-fraction of the edges, finds a bisection (-balanced separator) cutting at most
-fraction of edges.
\end{theorem}

Towards showing a matching hardness results for CSPs with cardinality
constraints, we construct a {\it dictatorship test} for these
problems.  Dictatorship testing gadgets lie at the heart of all
optimal hardness of approximation results for CSPs (both
-hardness and unique games based hardness results).  In fact,
using techniques from the work of Khot \etal \cite{KhotKMO07}, any
dictatorship test for a CSP yields a corresponding unique games based
hardness result.  More generally, a large fraction of hardness of
approximation results (not necessarily CSPs) have an underlying
dictatorship testing gadget.

Building on earlier works, Raghavendra \cite{Raghavendra08} exhibited
a generic reduction that starts with an arbitrary integrality gap
instance for certain SDP relaxation of a CSP to a dictatorship test for the same CSP.
In turn, this implied optimal hardness results matching the
integrality gap of the SDP under the unique games conjecture.
Using techniques from \cite{Raghavendra08}, we exhibit a generic
reduction from integrality gap instances to the Lasserre SDP
relaxation of a CSP with cardinality constraints, to a dictatorship
test for the same.  While the reduction applies in general for every
CSP with cardinality constraints, for the sake of exposition, we
present the special case of \maxbisection.  For \maxbisection, we show
the following.
\begin{theorem}(Informal Statement)
For every , given an integrality gap instance for
-round Lasserre SDP for \maxbisection, with SDP
value  and optimum integral value , there exists a dictatorship
test for \maxbisection with completeness  and
soundness .
\end{theorem}
\ifnum\full=0
The formal statement of the result and its proof is deferred to the
full version of the paper.
\fi
\ifnum\full=1
The formal statement of the result  and its proof is presented in
\pref{sec:gaptodict}.
\fi
Unfortunately, this dictatorship test does not yet translate in to a
corresponding hardness result for \maxbisection.
First, observe that the framework of Khot \etal \cite{KhotKMO07} to
show unique games based hardness results does not apply to
\maxbisection due to the global constraint on the instance.
This is the same reason why the unique games conjecture is not known to imply
hardness results for \balancedseparator.   The reason being that the hard instances of these problems are
required to have certain global structure (such as expansion in case of
\balancedseparator).  In case of \maxbisection, a hard instance must
not decompose in to sets of small size ( vertices), else
the global balance condition can be easily satisfied by appropriately
flipping the cut in each set independently.  Gadget reductions from a unique games instance
preserve the global properties of the unique games instance such as lack of expansion. Therefore, showing
hardness for \balancedseparator or \maxbisection problems require a
stronger assumption such as unique games with expansion or the Small
Set expansion hypothesis \cite{RaghavendraS10}.

\ifnum\full=0 \vspace{-8pt}\fi

\section{Overview of Techniques} \label{sec:overview}

In this section, we outline the our approach to approixmating the
\maxbisection problem.  The techniques are fairly general and can be applied to other CSPs with global cardinality constraints.


\paragraph{Global Correlation}
For the sake of exposition, let us recall the Goemans and Williamson algorithm
for \maxcut.
Given a graph , the Goemans-Williamson SDP relaxation for
\maxcut assigns a unit vector  for every vertex , so as to maximize the average squared length  of the edges.  Formally, the SDP relaxation is given by,

The rounding scheme picks a random halfspace passing through the
origin and outputs the partition of the vertices induced by the
halfspace.  The value of the cut returned is guaranteed to be within a
-factor of the SDP value.

The same algorithm would be an approximation for \maxbisection if the
cut returned by the algorithm was near-balanced, i.e., .  Indeed, the expected number of vertices on either side of the
partition is , since each vertex  falls on a given
side of a random halfspace with probability .

If the balance of the partition returned is concentrated around its
expectation then the Goemans and Williamson algorithm would yield a
-approximation for \maxbisection.   However, the balance of the
partition need not be concentrated, simply because the values taken by
vertices could be highly correlated with each other!


\paragraph{SDP Relaxation}
To exploit the correlations between the vertices we use a -round Lasserre SDP
\cite{Lasserre01} of \maxbisection for a sufficiently large constant .  On a high
level, the solutions to a Lasserre's SDP hierarchy are vectors
that {\it locally behave} like a distribution over integral solutions.
The -round Lasserre SDP has the following properties similar to
a true distribution over integral solutions.
\begin{itemize}
\item {\it Marginal Distributions} For any subset  of vertices with , the SDP will
yield a distribution  on partial assignments to the vertices
().  The marginals of ,  for a pair of
subsets  and  are consistent on their intersection .
\item {\it Conditioning} Analogous to a true distribution over
integral solutions, for any subset  with 
and a partial assignment , the SDP solution can
be conditioned on the event that  is assigned .
\end{itemize}
A detailed description of the Lasserre's SDP hierarchy for \maxbisection and other CSPs will be given in \pref{sec:prelim}.

\paragraph{Measuring Correlations}

  In this work, we will use mutual
information as a measure of correlation between two random variables.
We refer the reader to \pref{sec:prelim} for the definitions of
Shannon entropy and mutual information.  The correlation between vertices  and  is given by

where the random variables  are sampled using the local distribution
 associated with the Lasserre SDP.
An SDP solution will be termed {\it -independent} if the average
mutual information between random pairs of vertices is at most
, i.e., .

For most natural rounding schemes such as the halfspace-rounding, the variance
of the balance of the cut returned is
directly related to the average correlation between random pairs
of vertices in the graph.  In other words, if the rounding scheme is
applied to an -independent SDP solution then the variance of
the balance of the cut is at most .


\paragraph{Obtaining Uncorrelated SDP Solutions}



Intuitively, if it is the case that globally all the vertices are
highly correlated, then conditioning on the value of a vertex should
reveal information about the remaining vertices, therefore reducing
the total entropy of all the vertices.

Formally, let us suppose the -round Lasserre SDP solution is not
-independent, i.e.,  .
Let us pick a vertex  at random, sample its value  and condition the SDP
solution to the event .  This conditioning reduces the
average entropy of the vertices () by at least
 in expectation.  If the conditioned SDP solution is
-independent we are done, else we repeat the process.

The intital average entropy  is at most , and
the quantity always remains non-negative.  Therefore, within
 conditionings, the SDP solution will be
-independent.  Starting with a -round Lasserre SDP
solution, this process produces a  round -independent Lasserre
SDP solution for some .








\paragraph{Rounding Uncorrelated SDP Solutions}
Given an -independent SDP solution, for many natural rounding
schemes the balance of the output cut is concentrated around its
expectation.  Hence it suffices to construct rounding schemes that
output a balanced cut in expectation.  We exhibit a simple rounding
scheme that preserves the bias of each vertex individually, thereby
preserving the global balance property.   The details of the rounding algorithm will be described in \pref{sec:rounding}.

\ifnum\full=0 \vspace{-8pt}\fi
\section{Preliminaries} \label{sec:prelim}

\ifnum\full=1
\paragraph{Constraint Satisfaction Problem with Global Cardinality Constraints}

In this section we formally define CSPs with global constraints.

\begin{definition} [Constraint Satisfaction Problems with Global Cardinality Constraints]
A constraint satisfaction problem with global cardinality constraints is specified by  where  is a finite domain, \} is a set of payoff functions. The maximum number of inputs to a payoff function is denoted by . The map  is the cardinality function which satisfies . For any , the solution should contain  fraction of the variables with value .

\end{definition}
\begin{remark}
Although some problems (\eg \balancedseparator) do not fix the cardinalities to be some specific quantities, they can be easily reduced to the above case.
\end{remark}

\begin{definition}
An instance  of constraint satisfaction problems with global
cardinality constraints  is given by  where
\begin{itemize}
\item : variables taking values over 
\item  consists of the payoffs applied to subsets  of size at most 
\item Nonnegative weights  satisfying . Thus we may interpret  as a probability distribution on the subsets. By , we denote a set  chosen according to the probability distribution 
\item An assignment should satisfy that the number of variables with value  is  (we may assume this is an integer).
\end{itemize}
\end{definition}
\fi
Here we give a few examples of CSPs with global cardinality
constraints.
\ifnum\full=0
We defer the formal definition of a CSP with global
cardinality constraint to the full version.
\fi
\begin{definition} [Max(Min) Bisection]
Given a (weighted) graph  with  even, the goal is to partition the vertices into two equal pieces such that the number (total weights) of edges that cross the cut is maximized (minimized).
\end{definition}
More generally, in an -\maxcut problem, the goal is to find a
partition having  vertices on one side, while cutting the
maximum number of edges.  Furthermore, one could allow weights on the
vertices of the graph, and look for cuts with exactly
-fraction of the weight on one side.  Most of our techniques
generalize to this setting.

Throughout this work, we will have a weighted graph  with weights
 on the vertices.  The weights on the vertices are assumed to form
a probability distribution.  Hence the notation  refers to
a random vertex sampled from the distribution .

\begin{definition} [Edge Expansion]
Given a graph (w.l.o.g, we may assume it is a unweighted regular graph) , and , the goal is to find a set  such that  and the edge expansion of :  is minimized.
\end{definition}


\paragraph{Information Theoretic Notions}
\begin{definition} \label{def:entropy}
Let  be a random variable taking values over . The \emph{entropy} of  is defined as

\end{definition}
\begin{definition}\label{def:mutualinfo}
Let  and  be two jointly distributed variables taking values over . The \emph{mutual information} of  and  is defined as

\end{definition}
\begin{definition}\label{def:conditionalentropy}
Let  and  be two jointly distributed variables taking values over . The \emph{conditional entropy} of  conditioned on  is defined as

\end{definition}
\ifnum\full=1
We also give two well-known theorems in information theory below.
\begin{theorem} \label{thm:entropy-info}
Let  and  be two jointly distributed variables taking value on , then

\end{theorem}
\begin{theorem} \label{thm:dataprocessing} (Data Processing
Inequality)
Let  be random variables such that  and , \ie  is fully determined by  and  is fully determined by , then

\end{theorem}
\fi

\paragraph{Lasserre SDP Hierarchy for Globally Constrained CSPs} \label{subsec:lasserresdp}

\ifnum\full=1
Let  be a CSP with global constraints
and  be an instance of
 on variables . A solution to the
-round Lasserre SDP consists of vectors  for all
vertex sets  with  and local assignments
. Also for each subset  with , there is a distribution  on . For two subsets
 such that , we require that the corresponding
distributions  and  are consistant when restricted to . A Lasserre solution is feasible if for any , , , we have

The SDP also has a vector  that denotes the constant .  The global cardinality constraints can be written in terms of the
marginals of each variable. Specifically, for every  with  and , we have

The objective of the SDP is to maximize



\fi
\ifnum\full=0
Consider a CSP with global constraint over an alphabet .  For the purpose of this extended abstract, .
A -rounds Lasserre solution consists of vectors  for
all vertex sets  with  and local assignments
. Also for each subset  with , there is a distribution  on . For two subsets  such that , we require that  and  are consistant when restricted on . A Lasserre solution is feasible if for any , , , we have

Also, the marginal distribution of each variable should match the cardinality constraints. That is, for any  with  and , we have

The objective of the SDP is to maximize

\fi


While the complete description of the Lasserre SDP hierarchy is
somewhat complicated, there are few properties of the hierarchy that
we need. The most
important property is the existence of consistent local
marginal distributions  whose first two
moments match the inner products of the vectors. We
stress that even though the local distributions are consistent, there
might not exist a global distribution that agrees with all of them.  The second property of the -round Lasserre SDP
solution is that although the variables are not jointly distributed,
one can still \emph{condition} on the assignment to any given
variable to obtain a solution to the  round Lasserre's SDP that
corresponds to the \emph{conditioned distribution}.


\ifnum\full=0 \vspace{-8pt}\fi
\section{Globally Uncorrelated SDP Solutions} \label{sec:uncorrelatedsol}

As remarked earlier, it is easy to round SDP solutions to a CSP with
cardinality constraint if the variables behave like {\it independent} random
variables.  In this section, we show a very simple procedure that
starts with a solution to the -round Lasserre SDP and produces
a solution to the -round Lasserre SDP with the additional property
that globally the variables are somewhat "uncorrelated".  To this end, we define the
notion of {\it -independence} for SDP solutions below.
We remark that all the definitions and results in this section can be applied to all CSPs.


\begin{definition} \label{def:alphaindependentsol}
Given a solution to the -round Lasserre SDP relaxation, it is said
to be -independent if

where  is the local distribution associated with the
pair of vertices .
\end{definition}
\ifnum\full=1
\begin{remark}
We stress again that the variables in the SDP solution are not jointly distributed. However, the notion is still well-defined here because of the locality of mutual information: it only depends on the joint distribution of two variables, which is guaranteed to exist by the SDP. Also,  in the expression can be replaced with  for arbitrary  with  and  because of the consistency of local distributions.
\end{remark}
\fi

The notion of -independence of random variables using mutual
information, easily translates in to more familiar notion of
statistical distance.  Specifically, we have the following relation.
\begin{fact} \label{fact:statdist}
Let  and  be two jointly distributed random variables on 
then,

in particular for all 

As a consequence, if  and  are two random variables defined on ,

\end{fact}
\ifnum\full=1
For the sake of completeness, we include the proof of this observation
in \pref{app:informationtheory}.
\fi
Now we describe the procedure of getting an -independent
-rounds Lasserre's solution.  A similar argument was concurrently discovered in \cite{BarakRS11}.
Here we reproduce the argument in information theoretic terms, while
\cite{BarakRS11} present the argument in terms of covariance.  The
information theoretic argument is somewhat robust and cleaner in that it is
independent of the sample space involved.

\begin{mybox}
\begin{algorithm}

{\bf Input}: A feasible solution to the  round Lasserre SDP
relaxation as described in \pref{sec:prelim} for .

{\bf Output}: An -independent solution to the  round Lasserre SDP
relaxation.\\

Sample indices  independently according to .  Set .

Until the SDP solution is -independent repeat
\begin{itemize}\itemsep=0ex
	\item	Sample the variable  from its marginal distribution after the
	first  fixings, and condition the SDP solution on the outcome.
\item .
\end{itemize}
\end{algorithm}
\end{mybox}


The following lemma shows that there exists  such that the
resulting solution is -independent after -conditionings with high probability.
\begin{lemma} \label{lem:existlowmi}
There exists  such that 
\end{lemma}

\begin{proof}
By linearity of expectation, we have that for any 

adding the equalities from  to , we get

The lemma follows from the fact that for each , .
\end{proof}


\begin{theorem}  \label{thm:alphaindependentsol}
For every  and positive integer , there exists an algorithm running in time
 that finds an
-independent solution to the -round Lasserre SDP,
with an SDP objective value of at least , where 
denotes the optimum value of the -round Lasserre SDP relaxation.
\end{theorem}

\begin{proof}
Pick .  Solve the  round Lasserre
SDP solution, and use it as input to the conditioning algorithm
described earlier.  Notice that the algorithm respects the marginal
distributions provided by the SDP while sampling the values to
variables.  Therefore, the expected objective value of the SDP
solution after conditioning is exactly equal to the SDP objective
value before conditioning. Also notice that the SDP value is at most
. Therefore, the probability of the SDP value dropping by at least
 due to conditioning is at most .

Also, by \pref{lem:existlowmi} and Markov Inequality, the probability
of the algorithm failing to find a -independent soluton is at most .
Therefore, by union bound, there exists a fixing such that the SDP
value is maintained up to , and the solution after
conditioning is -independent. Moreover, this particular fixing
can be found using brute-force search.

\end{proof}









\ifnum\full=0 \vspace{-8pt}\fi
\section{Rounding Scheme for \maxbisection} \label{sec:rounding}

In this section, we present and analyze a natural rounding scheme for
\maxbisection.  Given an globally uncorrelated SDP solution to a
-round Lasserre SDP relaxation of \maxbisection, the rounding
scheme will output a cut with the approximation guarantees outlined in
\pref{thm:max-bisection}.
The same rounding scheme also yields a -approximation algorithm for arbitrary globally constrained \maxtwosat problem.

\paragraph{Constructing Goemans-Williamson type SDP solution}
In the -round Lasserre SDP for \maxbisection, there are two orthogonal vectors
 and  for each variable .  This can be used to
obtain a solution to the Goemans-Williamson SDP solution by simply defining .  The following proposition is an easy consequence,
\begin{proposition}
Let  where . Then, for each edge , .
\end{proposition}
\ifnum\full=1
\begin{proof}

\end{proof}
\fi
Let  be the component of  orthogonal to the  vector, i.e.,

Using  and , we get  and .
We remark that  is the crucial component that captures the \emph{correlation} between  and other variables. To formalize this, we show the following lemma.
\begin{lemma} \label{lem:ipbound}
Let  and  be the unit vectors constructed above,  and  be the components of  and  that orthogonal to . Then

\end{lemma}
\begin{proof}  Let  and
.  Notice that

By applying \pref{fact:statdist}, we get

\end{proof}
Henceforth we will switch from the alphabet   to 
\footnote{The mapping is given by  and }. After this transformation, we can interpret the inner product  as the \emph{bias} of vertex .
\ifnum\full=0 \vspace{-8pt}\fi
\subsection{Rounding Scheme}
\ifnum\full=0 \vspace{-8pt}\fi

Roughly speaking, the algorithm applies a hyperplane rounding on
the vectors  associated with the vertices
.  However, for each vertex , the algorithm shifts
the hyperplane according to the bias of that vertex.

\begin{mybox}
\begin{algorithm} \label{alg:rounding}
Given: A set of unit vectors  where , where  is the component of  orthogonal to .

Pick a random Gaussian vector  orthogonal to  with coordinates distributed as .
For every ,
\begin{enumerate}
\item
Project  on the direction of , \ie
,
where  is the normalized vector or . Note that  is also a standard Gaussian variable.
\item
Pick threshold  as follows:


\item
If , set , otherwise set .
\end{enumerate}
\end{algorithm}
\end{mybox}

Notice that, the threshold  is chosen so that individually the bias of  is exactly . Therefore, the expected balance of the rounded solution matches the intended value.
The analysis of the rounding algorithm consists of two parts: first we
show that the cut returned by the rounding algorithm has high expected
value, then we show the that the balance of the cut is concentrated
around its expectation.

\ifnum\full=0 \vspace{-8pt}\fi
\subsection{Analysis of the Cut Value}
\ifnum\full=0 \vspace{-8pt}\fi
Analyzing the cut value of the rounding scheme is fairly standard
albeit a bit technical.  The analysis is {\it local} as in the case of
other algorithms for CSPs, and reduces to bounding the probability
that a given edge is cut.  The probability that a given edge  is
cut corresponds to a probability of an event related to two correlated
Gaussians.

By using numerical techniques, we were able to show that the
cut value is at least  times the SDP optimum.  Analytically, we
show the following asymptotic relation.
\begin{lemma} \label{lem:rooteps}
Let , be two unit vectors satisfying , then the probability of them being separated by \pref{alg:rounding} is at most .
\end{lemma}
\ifnum\full=1
The proof of this lemma is fairly technical and is deferred to \pref{app:cutvalue}.
\fi
\ifnum\full=0
The proof of this lemma is fairly technical and is deferred to the
full version.
\fi
\ifnum\full=0 \vspace{-8pt}\fi
\subsection{Analysis of the Balance}
\ifnum\full=0 \vspace{-8pt}\fi
In this section we show that the balance of the rounded solution will be highly concentrated. We prove this fact by bounding the variance of the balance. Specifically, we show that if the SDP solution is -independent, then the variance of the balance can be bounded above by a function of .

The proof in this section is information theoretical -- although this approach gives sub-optimal bound, but the proof itself is very simple and clean.


\begin{lemma} \label{lem:lowmi}
Let  and  be two vectors in the SDP solution that satisfy . Let  and  be the rounded solution of  and , then



\end{lemma}


\begin{proof}
Since


It implies that one of the three quantities in the equation above is at most .
If it is the case that  or  (w.l.o.g we can assume it's the first case), then we have

We may assume , therefore .
Notice that our rounding scheme preserves the bias individually, which implies  is a highly biased binary variable, hence

Now let's assume it's the case that .
Let  and  as described in the rounding scheme, and . Hence  and  are two jointly distributed
standard Gaussian variables with covariance matrix  .

The mutual information of  and  is


Notice that  is fully dependent on , therefore by the data
processing inequality (\pref{thm:dataprocessing}), we have 
\end{proof}


\begin{theorem}
Given an -independent solution to 2-rounds Lasserre's SDP hierarchy. Let  be the rounded solution after applying \pref{alg:rounding}. Define , then


\end{theorem}

\begin{proof}

\end{proof}

\begin{corollary} \label{cor:balance}
Given an -independent solution to 2-rounds Lasserre's SDP hierarchy . The rounding algorithm will find an -balanced (that is, the balance of the cut differs from the expected value by at most  fraction of the total weights) with probability at least .
\end{corollary}






\ifnum\full=0 \vspace{-8pt}\fi
\subsection {Wrapping Up}
\ifnum\full=0 \vspace{-8pt}\fi
Here we present the proofs of the main theorems of this work.
\paragraph{Proof of \pref{thm:minbisection}}
Suppose we're given a \minbisection instance  with value at
most  and constant . By setting
 and applying \pref{thm:alphaindependentsol}, we
will get an -independent solution with value at most
. By \pref{lem:rooteps} and the concavity of the
function , the expected size of the cut returned by
\pref{alg:rounding} is at most
. Therefore, with
constant probability (say 1/2), the cut returned by the rounding
algorithm has size at most . Also, by
\pref{cor:balance}, the cut will be -balanced with
probability at least . Therefore, by union bound, the
algorithm will return an -balanced cut with value at most
 with constant probability. Notice that
this probability can be amplified to  by running the algorithm
 times. Given such a cut, we can simply move  fraction of the vertices with least degree from the larger side to the smaller side to get an exact bisection -- this process will increase the value of the cut by at most . Therefore, in this case, we get a bisection of value at most . Hence, the expected value of the bisection returned by the rounding algorithm is at most .
\paragraph{Proof of \pref{thm:max-bisection}}
The proof is similar in the case of \maxbisection. The only difference is that we have to use the fact that the rounding scheme is balanced, \ie  . Hence, by \pref{lem:rooteps}, for any edge  with value  in the SDP solution, the algorithm separates them with probability at least . The rest of the proof is identical.




\iffalse
\begin{theorem}
Given a graph  admits a bisection (with respect to weight vector ) with value  and , there exists a randomized algorithm that finds a cut  satisfying:
\begin{enumerate}
\item

\item
The size of the cut is at least 
\end{enumerate}
The algorithm is based on rounding a  Lasserre's SDP hierarchy.
\end{theorem}
\fi

Using a computer-assisted proof, we can show that the approximation
ratio of this algorithm for \maxbisection is between  and .
Thus further narrowing down the gap between approximation and inapproximability of \maxbisection.
Using the same algorithm, we obtain a -approximation for globally constrained \maxtwosat. It is known that under the Unique Games Conjecture, \maxtwosat is NP-Hard to approximate within .


\ifnum\full=1

\section{Dictatorship Tests from Globally Uncorrelated SDP Solutions}

\label{sec:gaptodict}

A dictatorship test  for the \maxbisection problem
	consists of a graph on the set of vertices
	.  By convention, the graph 
	is a weighted graph where the edge weights form a
	probability distribution (sum up to ).  We will
	write  to denote an edge
	sampled from the graph  (here ).

A cut of the  graph can be thought of as a boolean function
	.  The value	of a cut  given by
		
	is the probability that , are on
different sides of the cut.
		It is also useful to define  for non-boolean
	functions  that take values
	in the interval .  To this end, we will interpret a
	value  as a random variable that
	takes  values.  Specifically, we think of a number  as the following random variable
	
	With this interpretation, the natural definition of
	 for such a function is as follows:
	
	Indeed, the above expression is equal to the expected value of the
	cut obtained by randomly rounding the values of the function
	 to  as
	described in Equation \eqref{eq:cutrounding}.

	We will construct a dictatorship test for the weighted version
	of \maxbisection.  In particular, each vertex  of \dict is
	associated a weight , and the weights  form a
	probability distribution over  (sum up to ).  The
	balance condition on the cut can now be expressed as
	.

The dictatorship test  can be easily
	transformed in to a dictatorship test  for unweighted
	\maxbisection.  The idea is to replace each vertex  with a cluster  of 
	vertices for some large integer .  For every edge
	 in , connect every pair of vertices
	in the corresponding clusters  with
	edge of the same weight.  Given any bisection  of the graph 
	with value , define .  By slightly correcting the balance of , it is
	easy to obtain a bisection 
	satisfying
	
	Conversely, given a bisection  of
	, assign  fraction of vertices of
	 to be  and the rest to .  The resulting
	partition of  is very close to balanced (up to
	rounding errors), and can be modified in to a bisection with
	value .


	The {\it dictator cuts} are given by the functions
	 for some .
	The dictatorship test graph is so constructed that each
	dictator cut will yield a bisection and the  of the test  is the minimum value of a dictator cut, i.e.,
	
	The soundness of the dictatorship test is the value of
	bisections of  that are {\it far from every dictator}.  We will formalize the notion of being
	{\it far from every dictator} using the notion of influences.

\paragraph{Influences and Noise Operators}

	To this end, we recall the definitions of influences and noise
	operators.  Let  denote the probability space with atoms
 and a distribution  on them.  Then, the influences and
noise operators for functions over the product space  are
defined as follows.
	\begin{definition}[Influences]
	The {\it influence} of the \th coordinate on a function
	 under a distribution 
	over  is given by
	.
	\end{definition}
	
\begin{definition}
  For , define the operator  on
 as,

where each coordinate  of  is  equal
to 
\textrm{ with probability } and a random element from
 with probability .
\end{definition}

\paragraph{Invariance Principle}

The following invariance principle is an immediate consequence of
Theorem  in the work of Isaksson and Mossel \cite{IsakssonM09}.

\begin{theorem}(Invariance Principle \cite{IsakssonM09}) \label{thm:invariance}
  Let  be a finite probability space with the least non-zero
  probability of an atom at least .  Let  be an ensemble of random variables
  over .  Let  be an ensemble of Gaussian random variables satisfying the following conditions:
  
  Let .  Let  denote a
  multilinear polynomial  and let .  Let the variance of ,  be
  bounded by  and all the influences are smaller than , i.e.,   for all .

If  is a Lipschitz-continous function with
Lipschitz constant  (with respect to the  norm)  then
    
    for some constant .
\end{theorem}


\paragraph{Construction}

	Let  be an arbitrary instance of \maxbisection.  Let
	 denote a {\it globally
	uncorrelated} feasible SDP solution for two rounds of the
	Lasserre hierarchy.  Specifically, for every pair of vertices
	, there exists a distribution  over  assignments that match the SDP inner
products.  In other words, there exists 
valued random variables  such that

Furthermore, the correlation between random pair of vertices is at
most , i.e., 

	Starting from  along with the SDP solution  and a parameter  we
	construct a dictatorship test .  The
	dictatorship test gadget is exactly the same as
	the construction by Raghavendra \cite{Raghavendra08} for the \maxcut
	problem.  For the sake of completeness, we include the details
	below.

	\begin{mybox}
 (\maxbisection)
\ifnum\full=0
\itemsep=0ex
\fi
The set of vertices of  consists of the
-dimensional hypercube .  The distribution of edges in
 is the one induced by the following sampling
procedure:
\begin{itemize} \itemsep=0ex
\item Sample an edge  in the graph .
\item Sample  times independently from the distribution  to
	obtain  and
	, both in
	.
\item  Perturb each coordinate of  and 
	independently with probability  to obtain
	
	respectively.  Formally, for each ,
	
\item Output the edge . \end{itemize}
The weights on the vertices of  is given by

\end{mybox}

  We will show the following theorem about the completeness and
soundness of the dictatorship test.

\begin{theorem} \label{thm:gaptodict}
There exist absolute constants  such that for all  there exists  such that following holds. Given a graph  and a
-independent SDP solution   for the two round Lasserre SDP for \maxbisection, the
dictatorship test  is such that
\begin{itemize}
	\item The {\it dictator cuts} are bisections with value within
		 of the SDP value, i.e.,
		
	\item If  is a bisection of
		 () and all its influences are at most , i.e.,
		 then,
		
\end{itemize}
\end{theorem}

\begin{proof}
The analysis of the dictatorship test is along the lines of the
corresponding proof for \maxcut in \cite{Raghavendra08}.

\paragraph{Completeness}
First, the dictatorship test gadget is exactly the same as that
constructed for \maxcut in \cite{Raghavendra08}.  Therefore from \cite{Raghavendra08}, the
fraction of edges cut by the dictators is at least .
To finish the proof of completeness, we need to show that the dictator
cuts are indeed {\it balanced}.  However, this is an easy calculation
since the balance of the  dictator cut is given by,

where the last equality uses the fact that the SDP solution satisfies
the balance condition.

\paragraph{Soundness}
Let  be a balanced cut all of whose
influences are at most .  As in \cite{Raghavendra08}, we will
use the function  to round the SDP solution .  The
rounding algorithm is exactly the same as the one in
\cite{Raghavendra08}.  For the sake of completeness, we reproduce the
rounding scheme below.

\begin{mybox}
       Scheme
      \paragraph{Truncation Function} Let  be a Lipschitz-continous
function such that for all , .  Let  denote the Lipschitz constant of the function
.



\paragraph{Bias}  For each vertex , let the bias of vertex
 be  and let  be the component of  orthogonal to the vector
.
\paragraph{Scheme}
	Sample  vectors  with each coordinate
      being i.i.d normal random variable.


      For each  do
      \begin{itemize}	
        \itemsep=0ex
      \item For all ,
        compute the projection  of the vector
	 as follows:
        
	and let 



\item   Let  denote the multilinear polynomial
	corresponding to the function  under the distribution
	 and let .
	Evaluate  with  as
	inputs to obtain , i.e., .
\item 	Round  to  by using the Lipschitz-continous truncation function
	.
	
\item   Assign the vertex  to be  with probability
	       and  with the remaining probability.
      \end{itemize}
 \end{mybox}

  Let  denote the
  expected value of the cut returned by the rounding scheme
 on the SDP solution  for the
\maxbisection instance .

Again, by appealing to the soundness analysis in \cite{Raghavendra08},
we conclude that the fraction of edges cut by the resulting partition
is lower bounded by

for an absolute constant .  To finish the proof, we need to argue that if the SDP solution  is -independent, then the resulting partition is close to
balanced with high probability.

First, note that the expected balance of the cut is given by,


Fix a vertex . By construction, the random variables
 and  have
matching moments up to order two for each .  Therefore, by applying the invariance
principle of Isaksson and Mossel \cite{IsakssonM09} with the smooth
function  and the multilinear polynomial  yields
the following inequality,

Since the cut  is balanced we can write,

In the previous calculation, the first equality uses the fact that  for  while the second equality uses the fact that
.
Therefore, we get the following bound on the expected value of the
balance of the cut, 


Finally, we will show that the balance of the cut is concentrated
around its expectation.  To this end, we first show the following
continuity of the rounding algorithm.

\begin{lemma} \label{lem:rounding-cont}
	For each  and any vector  satisfying
	, if  denotes the output
	of the rounding scheme  with  instead
	of  then,

for some function of  ( suffices).
\end{lemma}
\begin{proof}
	Let  denote the projections of the vector  along
the directions .  The
output of the rounding scheme on  is given by
.  Recall that the output of the rounding scheme is given by .

The result is a consequence of the fact that the function
 is Lipschitz continous.  Since the
variance of  is at most , the sum of squares
of coefficients of  is at most .  Therefore, all the
 coefficients of  are bounded by  in absolute value.

The proof is a simple hybrid argument, where we replace  by  one by
one.  The details of the proof are deferred to the full version.
\end{proof}

\begin{lemma} \label{lem:covariance}
For every ,

for some function  of  ( suffices).
\end{lemma}
\begin{proof}
Set  for a unit vector 
orthogonal to  and .  Note that  is orthogonal to  and satisfies
.
Let  denote the output of the rounding with  instead of
. Since  is orthogonal to  all their projections are
independent random variables, which implies that,
.
Moreover, by \pref{lem:rounding-cont} we have,
.
Combining these inequalities and using Cauchy-Schwartz, we finish the proof as follows,

\end{proof}

To finish the proof, now we bound the variance of the balance of the
cut returned using \pref{lem:covariance}. The variance
of the balance of the cut returned is given by,

For a -independent SDP solution, the above quantity is at most
.  This gives the desired result.
\end{proof}


\fi


\ifnum\full=0
\clearpage
\bibliographystyle{abbrv}
\bibliography{refs-groth}
\vspace{2ex}
\Large{{\bf APPENDIX:} {\sc Full version of the paper follows.}}
\includepdf[pages=-]{max-bisection-full.pdf}
\end{document}
