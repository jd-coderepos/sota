\pdfoutput=1
\newif\ifFull
\Fulltrue
\ifFull
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\else
\documentclass{sig-alternate-05-2015}
\fi
\clubpenalty=8000
\hyphenpenalty=1000

\renewcommand{\emph}[1]{\textit{\textbf{#1}}}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[noend]{algorithmic}
\usepackage{cite}
\usepackage{url}

\newtheorem{theorem}{Theorem}\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{property}[theorem]{Property}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{counterexample}[theorem]{Counterexample}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{openproblem}[theorem]{Open Problem}
\newtheorem{assumption}[theorem]{Assumption}
\ifFull
\newenvironment{proof}{\noindent{\bf Proof:}}{\hspace*{\fill}\rule{6pt}{6pt}\bigskip}
\fi

\begin{document}






\ifFull\else
\CopyrightYear{2016} 
\setcopyright{acmlicensed}
\conferenceinfo{SPAA'16,}{July 11--13, 2016, Pacific Grove, CA, USA.}
\isbn{978-1-4503-4210-0/16/07}\acmPrice{\nnxb(b,M,E)tMlEMEtlt=23l=8t=52l=11t=112l=15tl1X=\{x_1,x_2,\ldots,x_n\}nS_n^*S_n^*S_nS_n^*S_n^*S_nS_nn\oplusxy\epsilon_{x,y}\epsilonnXXTvv\oplusXT\oplusX\oplusnxys = x\oplus yx+y = s+e_sse_sxy\addXnO(1)88nC(X)XnnnnnMBO(n)\{-1,0,1\}S_n^*nS^*_nO(\log n)nS^*_nO(\log^2 n \log\log\log C(X))O(n\log C(X))C(X)XS^*_nO({\rm sort}(n))(n){\rm sort}(n)=O((n/B)\log_{M/B} (n/B))MBS^*_nO({\rm scan}(n)){\rm scan}(n){scan}(n)=O(n/B)BM\Omega(\sigma(n))\sigma(n)n\sigma(n)O(\log n)lnS^*_nO(\sigma(n/p)p+n/p)O(n)ppo(n)nxb(b,M,E)tMlEt=52l=11nXt+2^{l-1}+\lceil\log n\rceilt+2^{l-1}XY(y_k,y_{k-1},\ldots,y_0)yYYy_0Xy_iyyX(\alpha,\beta)RY_i[-\alpha,\beta]\alpha,\beta\ge 2tR2^{t-1}>2y_it-1R=2^{t_0-1}>2t_0\ge 2\alpha\betayzP_i = Y_i+Z_iW_i=P_i-C_{i+1}RC_{i+1}C_{i+1}\in\{-1,0,1\}W_i[-(\alpha-1),\,\beta-1]S_i = W_i + C_is_i(\alpha,\beta) \alpha=\beta=R-1R>2yz(\alpha,\beta)S_i[-\alpha,\beta]yz(\alpha,\beta)P_i[-2\alpha,2\beta]s=y+z(\alpha,\beta)S_i[-\alpha,\beta]-R+1<P_i<R-1S_i[-\alpha,\beta]C_i\{-1,0,1\}C_{i+1}=0P_iP_i\ge R-1C_{i+1}=1W_i\ge -1 \ge -(\alpha-1)\alpha\ge 2S_i\ge -\alphaP_i\le -R+1C_{i+1}=-1W_i\le 1 \le \beta-1\beta\ge 2S_i\le \beta\alpha=\beta=R-1(\alpha,\beta)0k0k+1\Omega(\log n)nRW_iC_iP_iP_iy_iy_{i+1}y_iy_iy_iy_{i+1}y_iy_iYYl(\alpha,\beta)YYi_0<i_1<\cdots<i_jiy_iY'=(y_{i_{j_1}},\ldots,y_{i_{0}})Z'=(z_{i_{j_2}},\ldots,z_{i_{0}})Y'Z'(\alpha,\beta)Y'Z'R(\alpha,\beta)Y'\gamma\gammaY'Y''\gammaY'(\alpha,\beta)O(\log n)O(n\log n)T\lceil\log n\rceilix_iXO(\log n)O(n)x_ix_i(\alpha,\beta)x_i'O(1)O(n)O(1)Rx_i'E(v)TTveE(v)vO(\log n)O(n\log n)E(v)O(\log n)O(n\log n)O(n\log n)TTO(\log n)O(n\log n)(\alpha,\beta)T((R/2)-1,(R/2)-1)-101O(\log n)O(n)O(\log n)O(n)nO(\log n)O(n\log n)nO(1)nnCDC=D\tau\log nc_iC(-1,1,\tau c_i)(-1)\times 2^{\tau c_i}d_iD(1,1,\tau d_i)1\times 2^{\tau d_i}C=DC=DCD(-1,1,\tau c_i)(1,1,\tau d_i)c_i=d_i\tau > \log nn(-1,1,\tau c_i)(1,1,\tau d_i)c_i=d_iC=D\Omega(n\log n)O(\log n)\Omega(\log n)O(1)n(\alpha,\beta)Y_1Y_2mY_1Y_2O(\log m)O(m/\log m)XO(n\log C(X))C(X)=1\log C(X)=01O(\log n)TXx_i(\alpha,\beta)TTO(\log^2 n)O(n\log n)r=2rrr\leftarrow r^2ryy_{i_r}yyE_{i_r}y_{i_r}\epsilon\epsilon_{\rm min}y_{i_r}yny\lceil \log n\rceilE_{i_r}n\epsilon_{\rm min}y_{i_r}y_{i_r}XnXO(\log^2 n\log\log\log C(X))O(n \log C(X))C(X)XXrO(\log C(X))X_1=\sum |x_i|X_2=|\sum x_i|\log C(X)\delta1X_11X_2\delta\Theta(\delta+\log n)rO(\log C(X))\Omega(\log n)rO(\log\log\log C(X)))rO(n\log C(X))O(\log^2 n)nnXn(\alpha,\beta)O({\rm scan}(n))O({\rm sort}(n))(\alpha,\beta)Sy_{i,j}y_{i,j}SSS(\alpha,\beta)O({\rm scan}(n))SnSS((R/2)-1,(R/2)-1)O({\rm scan}(n))nXnO({\rm sort}(n))O(1)\sigma(n)MnO({\rm scan}(n))\sigma(n)\le Mnx_ix_i(r(x_i),x_i)r(x_i)[1,p]prrXppo(n)O(n/p)r_ii\in[1,p]r_ipp[1, p]\deltaC(X)=1\delta\deltaO(p)O(1)p\delta\delta\delta\delta\delta\delta\deltan$ floating-point numbers.  Our algorithms are designed for
a number of parallel models, including the PRAM, external-memory,
and MapReduce models. The primary design paradigm of our methods
is that of
converting the input values to an intermediate representation, called
a sparse superaccumulator, summing the values exactly in this representation,
and then converting this exact sum to a faithfully-rounded floating-point
representation.
\ifFull
We are able to achieve significant parallelization by utilizing a novel
intermediate floating-point superaccumulator representation that is carry-free.
\fi
Our experimental evaluation shows that our MapReduce algorithm can achieve
up to 80X performance speedup as compared to the state-of-the-art sequential algorithm.
The MapReduce algorithm yields lineary scalability with both the input dataset and
number of cores in the cluster.

\subsection*{Acknowledgments}
This research was supported in part by
the National Science Foundation under grant 1228639,
and an AWS in Education Grant.
We would like to thank Wayne Hayes for several helpful discussions
concerning the topics of this paper.

{\raggedright 
\small
\bibliographystyle{abbrv} 
\bibliography{refs} 
}


\end{document}
