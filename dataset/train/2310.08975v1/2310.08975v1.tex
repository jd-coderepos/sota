
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{pdfpages}

\title{ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models}





\author{{\bf Haoran Luo\textsuperscript{\rm 1}, Haihong E\textsuperscript{\rm 1}\thanks{\ \ Corresponding author.}  , Zichen Tang\textsuperscript{\rm 1}, Shiyao Peng\textsuperscript{\rm 1}, Yikai Guo\textsuperscript{\rm 3},} \\{\bf Wentai Zhang\textsuperscript{\rm 1}, Chenghao Ma\textsuperscript{\rm 1}, Guanting Dong\textsuperscript{\rm 2}, Meina Song\textsuperscript{\rm 1}, Wei Lin\textsuperscript{\rm 4}} \\
         \textsuperscript{1}School of Computer Science, Beijing University of Posts and Telecommunications, China \\ 
         \textsuperscript{2}School of Artificial Intelligence, Beijing University of Posts and Telecommunications, China \\ 
         \textsuperscript{3}Beijing Institute of Computer Technology and Application 
         \ \textsuperscript{4}Inspur Group Co., Ltd., China \\ 
         \texttt{\{luohaoran, ehaihong\}@bupt.edu.cn}}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
Knowledge Base Question Answering (KBQA) aims to derive answers to natural language questions over large-scale knowledge bases (KBs), which are generally divided into two research components: knowledge retrieval and semantic parsing. However, three core challenges remain, including inefficient knowledge retrieval, retrieval errors adversely affecting semantic parsing, and the complexity of previous KBQA methods. In the era of large language models (LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2. ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then retrieving and replacing entities and relations through an unsupervised retrieval method, which improves both generation and retrieval more straightforwardly. Experimental results reveal that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and ComplexWebQuestions (CWQ). This work also provides a new paradigm for combining LLMs with knowledge graphs (KGs) for interpretable and knowledge-required question answering. 
Our code is publicly available at \url{https://github.com/LHRLAB/ChatKBQA}.

\end{abstract}

\section{Introduction}

Knowledge Base Question Answering (KBQA) is a classical NLP task to answer natural language questions based on facts over a large-scale knowledge base (KB), such as Freebase~\citep{Freebase}, Wikidata~\citep{Wikidata}, and DBpedia~\citep{DBpedia}, which are composed of structured knowledge graphs (KGs) built from triples consisting of (head entity, relation, tail entity). Previous KBQA methods primarily focused on addressing two core issues: knowledge retrieval~\citep{KR} and semantic parsing~\citep{SP}. Knowledge retrieval mainly aims to locate the most relevant entities, relations, or triples according to the question from KB, narrowing the scope of consideration. Then, semantic parsing essentially converts the question from unstructured natural language into a structured logical form (such as S-expression~\citep{GrailQA}), which can then be converted into an executable graph database query language (such as SPARQL~\citep{SPARQL}) to obtain precise answers and interpretable paths.

Previous KBQA work~\citep{SR,UniK-QA,StructGPT} proposed different knowledge retrieval methods with technologies of named entity recognition (NER)~\citep{BERT}, entity linking~\citep{ELQ} or subgraph retrieval~\citep{SR}. Then, they leveraged the retrieved factual triples to directly derive answers to questions using a seq2seq model such as T5~\citep{T5}. Others~\citep{RnG-KBQA,GMT-KBQA,TIARA} performed semantic parsing to generate a logical form after retrieving relevant triples and then executed the according converted SPARQL query over KB to fetch the answers. Moreover, \citet{DECAF} combined these two approaches to further improve the accuracy of the KBQA task.

\begin{figure*}[h!t]
\centering
\includegraphics[width=13.7cm]{F1.drawio.pdf}
\caption{Comparison of examples of previous retrieve-then-generate KBQA framework (left) and our proposed generate-then-retrieve KBQA framework, ChatKBQA (right). Note that labels such as ``\texttt{g.g.a}" etc. in the computational graph are acronyms for relation names such as ``\texttt{government.government\_position\_held.appointed\_by}". }
\label{f1}
\vspace{-3mm}
\end{figure*}

Despite this, three main challenges remain, as shown on the left side of Figure~\ref{f1}. (1) \textbf{Low retrieval efficiency.} Traditional methods first identify the span of candidate entities and then do entity retrieval and relation retrieval. Since the structure of natural language problems differs from KB facts, most approaches require training dedicated models for extraction and linking inefficiently. (2) \textbf{Incorrect retrieval results will mislead semantic parsing.} Previous methods have utilized retrieved triples also as input of reference to the seq2seq model along with the original question. However, since the retrieved triples aren't always accurate, they adversely impact semantic parsing outcomes. Additionally, if there are numerous retrieved triples, the seq2seq model requires a much longer context length. (3) \textbf{Multiple processing steps make KBQA a redundantly complex task.} Previous work decomposed the KBQA task into multiple sub-tasks~\citep{GMT-KBQA,TIARA}, forming a complex pipeline processing, which made reproduction and migration challenging. In the era when large language models (LLMs)~\citep{GPT4,LLMsurvey,LLMKGsurvey} are restructuring traditional NLP tasks~\citep{Flan-T5,gptner,InstructUIE}, a more straightforward solution utilizing LLMs to reformulate the traditional KBQA paradigm is promising.

To overcome these challenges, we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework based on fine-tuning open-source LLMs, such as Llama-2-7B~\citep{Llama2}, ChatGLM2-6B~\citep{GLM} and Baichuan2-7B~\citep{Baichuan2}. As illustrated on the right side of Figure~\ref{f1}, ChatKBQA proposes a straightforward method for KBQA: it first generates a logical form, followed by the retrieval of entities and relations, aiming to avoid the influence of retrieval on logical form generation and enhance retrieval efficiency. In the generation phase, we fine-tune open-source LLMs using instruction tuning techniques~\citep{InstructGPT} to equip them with the capability to perceive and generate in logical form format. Owing to the exceptional learning ability of LLMs, beam search results indicate that, approximately 74\% of the test questions, once transformed into logical form, already match the ground truth. Furthermore, with entities and relations masked (skeleton), over 91\% of the samples possess a logical form structure consistent with the ground truth. In the retrieval phase, we propose an unsupervised retrieval method for entities and relations, which conducts phrase-level semantic retrieval in the entity set and relation set of the KB for entities and relations in the logical form and replaces them in the respective positions. The overall ChatKBQA framework is straightforward and user-friendly, where the LLM part can be replaced with currently open-source LLMs, and the retrieval part can be swapped with other semantic matching models flexibly, ensuring excellent performance with plug-and-play characteristics.

To test the performance of our proposed framework, we conduct experiments on two standard KBQA datasets, WebQSP~\citep{WebQSP} and ComplexWebQuestions (CWQ)~\citep{CWQ}, with both settings of using and not using golden entities. The experimental results demonstrate that ChatKBQA achieves a new state-of-the-art performance in the KBQA task. We also set up additional experiments to validate whether our generate-then-retrieve approach improves generation results and retrieval efficiency. 
Finally, we also discuss how insights from this framework lead us to envision future combinations of LLMs and KGs.


\section{Related Work}
\label{s2}
\textbf{Knowledge Base Question Answering.}
Existing Knowledge Base Question Answering (KBQA) methods can be broadly categorized into Information Retrieval-based (IR-based) and Semantic Parsing-based (SP-based) methods. (1) IR-based methods primarily retrieve relevant factual triples or text from Knowledge Bases (KBs) based on natural language questions, forming a subgraph to determine answers. Examples include KV-Mem~\citep{KV-Mem}, which enhances document reading by using distinct encoding during memory access; GRAFT-Net~\citep{GRAFT-Net} extracts answers from a question-specific subgraph consisting of text and KB entities; and PullNet~\citep{PullNet}, EmbedKGQA~\citep{EmbedKGQA}, NSM~\citep{NSM}, TransferNet~\citep{TransferNet}, Relation Learning~\citep{RelationLearning}, Subgraph Retrieval~\citep{SubgraphRetrieval}, UniK-QA~\citep{UniK-QA}, and SKP~\citep{SKP} introduce innovative mechanisms and frameworks to optimize subgraph-related KBQA models. (2) On the other hand, SP-based methods focus on translating questions into logical forms executable against KBs, such as SPARQL, query graph, and S-expression. Some SP-based approaches, such as STAGG~\citep{STAGG}, UHop~\citep{UHop}, Topic Units~\citep{TopicUnits}, TextRay~\citep{TextRay}, QGG~\citep{QGG}, EMQL~\citep{EMQL}, GrailQA Ranking~\citep{GrailQA}, Rigel~\citep{Rigel}, UniKGQA~\citep{UniKGQA}, BeamQA~\citep{BeamQA}, HGNet~\citep{HGNet}, StructGPT~\citep{StructGPT}, and PanGu~\citep{PanGu}, utilize strategies of step-wise query graph generation and search for semantic parsing. Alternatively, other SP-based methods, like ReTraCk~\citep{ReTraCk}, CBR-KBQA~\citep{CBR-KBQA}, RnG-KBQA~\citep{RnG-KBQA}, Program Transfer~\citep{ProgramTransfer}, TIARA~\citep{TIARA}, ArcaneQA~\citep{ArcaneQA}, GMT-KBQA~\citep{GMT-KBQA}, Uni-Parser~\citep{Uni-Parser}, UnifiedSKG~\citep{UnifiedSKG}, DECAF~\citep{DECAF}, and FC-KBQA~\citep{FC-KBQA} employ sequence-to-sequence models to generate S-expressions completely and offer various enhancements to the semantic parsing process.

In this paper, our proposed ChatKBQA is a straightforward KBQA framework based on fine-tuned LLMs, which employs an SP-based KBQA method and generates S-expressions completely, adopting a Generate-then-Retrieve approach. It aims to address the issues of low knowledge retrieval efficiency, compromised logical form generation, and complexity that existed in previous methods.

\textbf{Large Language Models.} With the launch of ChatGPT and GPT-4~\citep{GPT4}, displaying the prowess of decoder-only large language models (LLMs) with a vast number of parameters that exhibit emergent phenomena, many traditional NLP tasks are becoming simplified~\citep{LLMsurvey}. Subsequently, open-source models like Llama-2-7B~\citep{Llama2}, ChatGLM2-6B~\citep{GLM} and Baichuan2-7B~\citep{Baichuan2} emerged and can be supervised fine-tuned (SFT) using instruction-tuning technologies~\citep{instructiontuning} such as LoRA~\citep{LoRA}, QLoRA~\citep{QLoRA}, P-Tuning v2~\citep{P-Tuningv2}, and Freeze~\citep{Freeze}, enhancing the capabilities of LLMs for specific tasks. While LLMs often produce hallucinations~\citep{hallucinations}. Technologies like Chain-of-Thought (CoT)~\citep{CoT}, Tree of Thoughts (ToT)~\citep{ToT}, Graph-of-Thought (GoT)~\citep{GoT}, and Program of Thoughts (PoT)~\citep{PoT} have mitigated these hallucinations to some extent, but factual errors still occur~\citep{LLMKGsurvey}. Therefore, generating explainable graph queries over KBs by LLMs is a promising approach for interpretable and externally knowledge-required question answering. 

In this paper, for the first time, ChatKBQA employs the instruction-tuning technique to fine-tune open-source LLMs, achieving impressive semantic parsing. It combines LLMs' powerful semantic parsing capabilities with KGs' interpretability advantages, introducing a new Graph Query of Thought (GQoT) paradigm for LLM+KG applications.


\textbf{Knowledge Retrieval for KBQA.} General retrieval methods are typically divided into traditional lexical models, such as BM25~\citep{BM25}, and dense retrieval models, such as Dense Passage Retrieval (DPR)~\citep{DPR}, SimCSE~\citep{SimCSE}, and Contriever~\citep{Contriever}. In the KBQA task, to better utilize knowledge related to the question from knowledge bases, efficient retrieval algorithms are needed to fetch the most relevant knowledge. ELQ~\citep{ELQ} and FACC1~\citep{FACC1} are commonly used entity retrieval methods. Both CBR-KBQA and RnG-KBQA employ ELQ for dense-retrieval-based entity linking, while GMT-KBQA complements ELQ retrieval results with FACC1, enhancing the coverage of candidate entities. For relation retrieval, GMT-KBQA employs a two-stage relation retrieval module using a bi-encoder and cross-encoder architecture, and utilizes FAISS~\citep{FAISS} for neighboring relation retrieval. TIARA leverages a cross-encoder to learn interactive representations between the question and schema, ranking classes and relations based on matching scores.

In this paper, our KBQA framework, ChatKBQA, retrieves corresponding entity and relation positions after generating the logical form by FACC1 and SimCSE, respectively, in an unsupervised manner. It is superior to previous methods that conducted knowledge retrieval directly on natural language questions.

\section{Preliminaries}

In this section, we define the two basic concepts of our work: the knowledge base and the logical form, followed by the problem statement for the knowledge base question answering task.

\textbf{Definition 1: Knowledge Base (KB).} A KB  is an RDF graph consisting of the triples  where  is an entity,  is a relation , and  can be an entity or a literal. Each entity  in the entity set  is represented by a unique ID, e.g., \texttt{.id="m.0fm2h"}, and can be queried to get the English label of the entity as \texttt{.label="Benjamin\ Netanyahu"}. Each relation  in the set of relations  consists of multiple levels of labels, e.g. \texttt{="government.government\_position\_held.appointed\_by"}. Besides, a literal  is usually ``integer" (e.g., \texttt{="32"}), ``float" (e.g., \texttt{="3.2"}), ``year" (e.g., \texttt{="1999"}), ``year\&month" (e.g., \texttt{="1999-12"}), or ``date" (e.g., \texttt{="1999-12-31"}).

\textbf{Definition 2: Logical Form. }A logical form is a structured representation of a natural language problem. Taking the S-expression as an example, a logical form usually consists of projection and various operators. Projection operation represents a one-hop query of a triple  on  or , where,  is denoted as \texttt{(JOIN  )}, while  is denoted as \texttt{(JOIN (R ) )}. Various operators include ``AND" \texttt{(AND  )} to denote taking the intersection of  and , ``COUNT" \texttt{(COUNT )} to denote counting , ``ARGMAX" \texttt{(ARGMAX  )} to denote taking the max literal obtained after the projection of  in the  relation, ``ARGMIN" \texttt{(ARGMIN  ) } to denote taking the min literal obtained after the projection of the  relation for , ``GT" \texttt{(GT  )} means to take the portion of  that is greater than , ``GE" \texttt {(GE  )} to denote taking the part of  greater than or equal to , ``LT" \texttt{(LT  )} to denote taking the part of  less than , ``LE" \texttt{(LE  )} to denote taking the part of  which is less than or equal to , where  or  denote a sublayer logical form.

\SetKwFunction{Convert}{Convert}
\SetKwFunction{Execute}{Execute}
\SetKwFunction{Sp}{Sp}
\textbf{Problem Statement. }For KBQA task, given a natural language question , and a knowledge base , we need to first convert  into a logical form , where  is a semantic parsing function. Then convert  to the equivalent SPARQL query , where  is the fixed conversion function. Finally the final set of answers  is obtained by executing  against ,where  is the query execution function.









\section{Methodology}

In this section, we first present an overview of the ChatKBQA framework and then introduce efficient fine-tuning on open-source large language models (LLMs), logical form generation by fine-tuned LLMs, unsupervised retrieval for entities and relations, and interpretable query execution.

\subsection{Overview of ChatKBQA}

ChatKBQA is a generate-then-retrieve KBQA framework for knowledge base question answering (KBQA) using fine-tuned open source LLMs. First, the ChatKBQA framework needs to efficiently fine-tune an open-source LLM based on the (natural language question, logical form) pairs in the KBQA dataset by instruction tuning. The fine-tuned LLM is then used to convert the new natural language questions to according candidate logical forms by semantic parsing. Then, ChatKBQA retrieves the entities and relations in these logical forms at the phrase level, and searches for the logical forms that can be executed against KB after being converted to SPARQL. Finally, the SPARQL obtained after conversion is utilized to get the final set of answers and achieve interpretable and knowledge-required answers to natural language questions.


\begin{figure*}[h!t]
\centering
\includegraphics[width=13.7cm]{F2.drawio.pdf}
\caption{The overview of ChatKBQA framework for generate-then-retrieve KBQA method with fine-tuned LLMs and unsupervised retrieval for entities and relations in candidate logical forms.}
\label{f2}
\end{figure*}

\subsection{Efficient Fine-Tuning on LLMs }


To construct the instruction fine-tuning training data, ChatKBQA first converts the SPARQL corresponding to the natural language questions of the test set in the KBQA dataset into equivalent logical forms, and then replaces the entity IDs (e.g., ``\texttt{m.06w2sn5}") in these logical forms with the corresponding entity tags (e.g., ``\texttt{[ Justin Bieber ]}"), to let LLMs understand entity labels better than meaningless entity IDs. We then combine the natural language question (e.g. ``\texttt{What is the name of justin bieber brother?}") and the processed corresponding logical form (e.g. ``\texttt{(AND (JOIN [ people , person , gender ] [ Male ]) (JOIN (R [ people , sibling relationship , sibling ]) (JOIN (R [ people , person , sibling s ]) [ Justin Bieber ])))}") as ``input'' and ``output'' respectively, and add ``instruction'' as ``\texttt{Generate a Logical Form query that retrieves the information corresponding to the given question.}" constitutes the instruction fine-tuning training data for open source LLMs.

In order to reduce the cost of fine-tuning LLMs with a large number of parameters, ChatKBQA utilizes Parameter Efficient Fine-Tuning (PEFT)~\citep{PEFT} methods to fine-tune only a small number of model parameters and achieve performance comparable to full fine-tuning. Among them, LoRA~\citep{LoRA} reduces the memory footprint of large language models with changing weights during fine-tuning by using a low-rank approximation. QLoRA~\citep{QLoRA} further reduces memory by back-propagating the gradient into a frozen 4-bit quantized model, while maintaining the performance of the full 16-bit fine-tuning task. P-tuning v2~\citep{P-Tuningv2} employs a prefix-tuning approach that incorporates fine-tunable parameters at each layer in front of the inputs. Freeze~\citep{Freeze} speeds up the model's convergence by fine-tuning only the fully-connected layer parameters of the last few layers of the Transformer while freezing all other parameters. ChatKBQA can switch between all the above efficient fine-tuning methods as well as LLMs, such as Llama-2-7B~\citep{Llama2} and ChatGLM2-6B~\citep{GLM}.

\subsection{Logical Form Generation by Fine-tuned LLMs}

Through fine-tuning, the LLMs have somewhat mastered the semantic parsing ability to transform from natural language questions to logical forms. So, we use the fine-tuned LLMs to do semantic parsing on the new questions in the test set, and we find that about 63\% of the samples are already identical to the ground truth logical forms. When we use beam search, the list of candidate logical forms  output by our LLMs contains about 74\% of the samples with ground truth logical forms, which demonstrates that fine-tuned LLMs have good learning and parsing capabilities for semantic parsing tasks. Further, if we replace the entities and relationships in the generated candidate logical forms with "\texttt{[]}" (e.g., "\texttt{(AND (JOIN [] []) (JOIN (R []) (JOIN (R []) [])))} ") form the skeleton of logical forms, then the skeleton of the ground truth logical form, the percentage of samples appearing in the candidate skeleton is more than 91\%. This suggests that we only need to replace these entities and relations at the corresponding positions in the logical form with those existing in KB, and then the performance can be further improved.

\subsection{Unsupervised Retrieval for Entities and Relations}

Because of the good generative ability of fine-tuned LLMs for logical form skeletons, in the retrieval phase, we adopt an unsupervised retrieval method, which takes the entities and relations in the candidate logical forms and passes them through phrase-level semantic retrieval and replacement, to get the final logical form transformed into SPARQL executable against KB.




\begin{wrapfigure}{r}{0.42\textwidth}
\vspace{-4mm}
\centering
\begin{minipage}{0.42\textwidth}
    \begin{algorithm}[H] \setstretch{0.8}
    \scriptsize
\SetKwProg{Fn}{Function}{}{}
    \SetKwFunction{TopKwithThreshold}{TopKwithThreshold}
    \SetKwFunction{PermuteByEntity}{PermuteByEntity}
    \SetKwFunction{PermuteByRelation}{PermuteByRelation}
    \SetKwFunction{SimiEntities}{SimiEntities}
    \SetKwFunction{SimiRelations}{SimiRelations}
    \SetKwFunction{Neighborhood}{Neighborhood}
    \SetKwFunction{Convert}{Convert}
    \SetKwFunction{Execute}{Execute}
    \SetKwFunction{Sp}{Sp}
    
    \SetKw{continue}{continue}
    \SetKw{return}{return}
    
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
    \Input{Candidate logical form list generated from LLM , top-k threshold , probability threshold , the entity set of knowledge base }
    \Output{The equivalent SPARQL query }
    
    \;
    \ForEach{}{
        \ForEach{}{
            \;
            \ForEach{}{
                \;
                \;
            }
            \;
            \;
        }
        \;
        \;
    }
    \;
    \ForEach{}{
        \ForEach{}{
            \;
            \ForEach{}{
                \;
                \;
            }
            \;
            \;
        }
        \;
        \;
    }
    \ForEach{}{
        \;
        \If{ is valid to execute}{
            \return \;
        }
    }
    \;
    
    \label{algo:er_retrival}
    \caption{Retrieval}
\end{algorithm}
\end{minipage}
\vspace{-9mm}
\end{wrapfigure}



Specifically, as shown in the Algorithm~\ref{algo:er_retrival}, the input is the generated candidate logical form list , and we traverse each of these logical forms  in order. First, we perform the entity retrieval. For each entity  in , we compute the similarity  with the label of each entity  in the knowledge base  entity set . We sort the retrieved entities based on the similarities, take the top  and greater than the threshold  to get the retrieval result for that entity . Function  performs permutation on the retrieved entities at each position, and we get the result  after entity retrieval. Based on probabilities in , we take top  and greater than threshold  to get a new candidate logical form list .

Then, we perform the relation retrieval. Similar to entity retrieval, but different in that for each relation  in , we compute the similarity  with each candidate relation  according to the neighborhood of entity set of the logical form . We also sort the retrieved relations according to the similarities, take the top  and greater than the threshold  to get the retrieval result . By permuting the retrieval results of the relations at each position, we get the result  after relation retrieval and then take top  and greater than threshold  to get a new list of candidate logical forms .

Given a query, unsupervised retrieval methods require no additional training to select the top k semantically most similar to the candidate set as the set of retrieved answers. BM25~\citep{BM25} uses term frequencies and inverse document frequencies to rank documents based on their relevance to a given query. SimCSE~\citep{SimCSE} and Contriever~\citep{Contriever} are unsupervised dense information retrieval using comparative learning models. ChatKBQA can switch between all the above unsupervised retrieval methods for entity retrieval and relation retrieval. 

\subsection{Interpretable Query Execution}

After retrieval, we get a final candidate logical form list , which we sequentially iterate through the logical form  and convert to the equivalent of the SPARQL query . When the first  that can be executed against KB  is found, we execute to get the final answer set . With this approach, we can also get a complete reasoning path for natural language questions based on SPARQL query with good interpretability. To summarize, ChatKBQA proposes a thought taking both the advantages of using LLMs to do natural language semantic parsing for graph query generation and calling external KBs to interpretably reason with queries, which we name Graph Query of Thoughts (GQoT), a promising LLM+KG combination paradigm to better utilize the external knowledge, improve Q\&A's interpretability, and avoid LLM's hallucinations.



\section{Experiments}


This section presents the experimental setup, results, and analysis. We answer the following research questions (RQs):
\textbf{RQ1}: Does ChatKBQA outperform other KBQA methods?
\textbf{RQ2}: Does the main components of ChatKBQA work?
\textbf{RQ3}: Why use Generate-then-Retrieve method instead of Retrieve-then-Generate method?
\textbf{RQ4}: Why use fine-tuned open-source LLMs instead of calling ChatGPT or training traditional T5 models?
\textbf{RQ5}: Does the Generate-then-Retrieve method improve the efficiency of retrieval?
\textbf{RQ6}: Is ChatKBQA plug-and-play?



\subsection{Experimental Setup}



\begin{wraptable}{r}{0.5\textwidth}
\vspace{-10mm}
\footnotesize
\setlength{\tabcolsep}{0.2mm}{
\begin{tabular}{lcccccc}
\toprule
                                 & \multicolumn{3}{c}{\textbf{WebQSP}}                                                               & \multicolumn{3}{c}{\textbf{CWQ}}                                             \\ \cline{2-7} 
\multirow{-2}{*}{\textbf{Model}} & \textbf{F1}   & \textbf{Hits@1}  & \multicolumn{1}{c|}{\textbf{Acc}}  & \textbf{F1}   & \textbf{Hits@1} & \textbf{Acc}  \\ \hline
KV-Mem                           & 34.5          & 46.7                                         & \multicolumn{1}{c|}{-}             & 15.7          & 21.1                                         & -             \\
STAGG                           & 71.7          & -                                            & \multicolumn{1}{c|}{63.9}          & -             & -                                            & -             \\
GRAFT-Net                        & 62.8          & 67.8                                         & \multicolumn{1}{c|}{-}             & 32.7          & 36.8                                         & -             \\
UHop                             & 68.5          & -                                            & \multicolumn{1}{c|}{-}             & 29.8          & -                                            & -             \\
Topic Units                     & 67.9          & 68.2                                         & \multicolumn{1}{c|}{-}             & 36.5          & 39.3                                         & -             \\
TextRay                         & 60.3          & 72.2                                         & \multicolumn{1}{c|}{-}             & 33.9          & 40.8                                         & -             \\
PullNet                          & -             & 68.1                                         & \multicolumn{1}{c|}{-}             & -             & 47.2                                         & -             \\
QGG                             & 74.0          & 73.0                                         & \multicolumn{1}{c|}{-}             & 40.4          & 44.1                                         & -             \\
EmbedKGQA*                       & -             & 66.6                                         & \multicolumn{1}{c|}{-}             & -             & 44.7                                         & -             \\
EmQL*                            & -             & 75.5                                         & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
NSM+h*                          & 67.4          & 74.3                                         & \multicolumn{1}{c|}{-}             & 44.0          & 48.8                                         & -             \\
GrailQA Ranking*                & 70.0          & -                                            & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
ReTraCk*                         & 74.7          & 74.6                                         & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
TransferNet                     & -             & 71.4                                         & \multicolumn{1}{c|}{-}             & -             & 48.6                                         & -             \\
Relation Learning               & 64.5          & 72.9                                         & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
Rigel*                           & -             & 73.3                                         & \multicolumn{1}{c|}{-}             & -             & 48.7                                         & -             \\
CBR-KBQA                         & 72.8          & -                                            & \multicolumn{1}{c|}{69.9}          & 70.0          & 70.4                                         & 67.1          \\
Subgraph Retrieval*              & 64.1          & 69.5                                         & \multicolumn{1}{c|}{-}             & 47.1          & 50.2                                         & -             \\
RnG-KBQA                         & 75.6          & -                                            & \multicolumn{1}{c|}{71.1}          & -             & -                                            & -             \\
Program Transfer*                & 76.5          & 74.6                                         & \multicolumn{1}{c|}{-}             & 58.7          & 58.1                                         & -             \\
TIARA*                           & 78.9          & 75.2                                         & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
UniK-QA                          & 79.1          & -                                            & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
ArcaneQA                         & 75.6          & -                                            & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
GMT-KBQA                         & 76.6          & -                                            & \multicolumn{1}{c|}{73.1}          & 77.0          & -                                            & 72.2          \\
Uni-Parser*                      & 75.8          & -                                            & \multicolumn{1}{c|}{71.4}          & -             & -                                            & -             \\
UnifiedSKG                       & 73.9          & -                                            & \multicolumn{1}{c|}{-}             & 68.8          & -                                            & -             \\
UniKGQA*                         & 72.2          & 77.2                                         & \multicolumn{1}{c|}{-}             & 49.4          & 51.2                                         & -             \\
DECAF                            & 78.8          & 82.1                                         & \multicolumn{1}{c|}{-}             & -             & 70.4                                         & -             \\
BeamQA*                          & -             & 73.4                                         & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
HGNet*                           & 76.6          & 76.9                                         & \multicolumn{1}{c|}{70.7}          & 68.5          & 68.9                                         & 57.8          \\
SKP                              & -             & 79.6                                         & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
ChatGPT*                         & 61.2          & -                                            & \multicolumn{1}{c|}{-}             & 64.0          & -                                            & -             \\
StructGPT*                       & 72.6          & -                                            & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\
FC-KBQA                          & 76.9          & -                                            & \multicolumn{1}{c|}{-}             & 56.4          & -                                            & -             \\
PanGu                            & 79.6          & -                                            & \multicolumn{1}{c|}{-}             & -             & -                                            & -             \\ \hline\hline
ChatKBQA (ours)                        & 79.8          & 83.2                                         & \multicolumn{1}{c|}{73.8}          & 77.8          & 82.7                                         & 73.3          \\
ChatKBQA* (ours)                      & \textbf{83.5} & \textbf{86.4}                                & \multicolumn{1}{c|}{\textbf{77.8}} & \textbf{81.3} & \textbf{86.0}                                & \textbf{76.8} \\ \bottomrule
\end{tabular}}
\caption{\label{t4}
KBQA result comparison of ChatKBQA with other baselines on WebQSP and CWQ datasets. * denotes using oracle entity linking annotations. The results of the models are mainly taken from their original paper. For our proposed ChatKBQA framework, we display the results of the best setup on WebQSP and CWQ, respectively. The best results in each metric are in \textbf{bold}. 
}
\vspace{-15mm}
\end{wraptable}


\textbf{Datasets. } All experiments are conducted on two standard KBQA datasets: WebQuestionsSP (WebQSP)~\citep{WebQSP} containing 4,737 natural language questions with SPARQL queries and ComplexWebQuestions (CWQ)~\citep{CWQ} containing 34,689 natural language questions with SPARQL queries. Both datasets are based on Freebase~\citep{Freebase} KB. 


\textbf{Baselines. } We compare ChatKBQA with numerous KBQA baseline methods, including KV-Mem~\citep{KV-Mem}, STAGG~\citep{STAGG}, GRAFT-Net~\citep{GRAFT-Net}, UHop~\citep{UHop}, Topic Units~\citep{TopicUnits}, TextRay~\citep{TextRay} and all other KBQA methods in Section~\ref{s2}. 



\textbf{Evaluation Metrics. }
Following previous work~\citep{GMT-KBQA,TIARA,DECAF}, we use  score, Hits@1, and Accuracy (Acc) to denote coverage of all the answers, single top-ranked answer, and strict exact-match accuracy, respectively.

\textbf{Hyperparameters and Enviroment. }
We fine-tune LLMs 100 epochs on WebQSP and 10 epochs on CWQ with batch size 4 and learning rate 5e-5. All experiments were done on a single NVIDIA A40 GPU (48GB), with results averaged from five randomly seeded experiments. 



\subsection{Main Result (RQ1)}

For the KBQA task, Table~\ref{t4} lists the experimental results for our proposed generate-then-retrieve ChatKBQA framework, with the best setup of LoRA~\citep{LoRA} fine-tuning Llama-2-7B~\citep{Llama2} (beam size = 15) on WebQSP, Llama-2-13B~\citep{Llama2} (beam size = 8) on CWQ, with SimCSE~\citep{SimCSE} for unsupervised retrieval, and other baseline models. We can see that ChatKBQA has a significant improvement over all existing KBQA methods on both WebQSP and CWQ datasets. The  scores, Hits@1, and Acc are improved by about 4, 4, and 4 percentage points on WebQSP and about 4, 16, and 4 percentage points on CWQ, respectively, compared to the previous best results, which reflects ChatKBQA's superior KBQA capability to reach the new state-of-the-art performance.



\begin{figure}[h!t]
    \vspace{-3mm}
    \centering
    \subfigure[\label{a}]{
        \includegraphics[width=0.235\textwidth, height=0.18\textwidth]{output1.pdf}
    }
    \subfigure[\label{b}]{
        \includegraphics[width=0.20\textwidth, height=0.18\textwidth]{output2.pdf}
    }
\subfigure[\label{c}]{
        \includegraphics[width=0.245\textwidth, height=0.18\textwidth]{output4.pdf}
    }
    \subfigure[\label{d}]{
        \includegraphics[width=0.25\textwidth, height=0.18\textwidth]{output3.pdf}
    }
	\caption{(a) Ablation study in ChatKBQA generation phase. (b) Ablation study in ChatKBQA retrieval phase. (c) Comparison with other language models in ChatKBQA generation phase. (d) Comparison of retreival efficiency between retrieval from nature language questions (NL-R) and generated logical forms (AG-R) in ChatKBQA retrieval phase.}
	\label{f3}
\end{figure}


\subsection{Ablation Study (RQ2)}

In order to validate the effectiveness of the generation and retrieval phases of ChatKBQA separately, we ablate the two phases separately. For the generation phase, we use 20\%, 40\%, 60\%, and 80\% of the training data for fine-tuning versus full training set fine-tuning. 

As shown in Figure~\ref{a}, the effect of KBQA gets better as the training volume increases, proving the effectiveness of fine-tuning. For the retrieval phase, in order to validate entity retrieval (ER) and relation retrieval (RR) separately, we removed ER or RR from the framework and obtained three simplified variants (\textbf{ChatKBQA w/o ER}, \textbf{ChatKBQA w/o RR} and \textbf{ChatKBQA w/o ER,RR}) at four different beam sizes for comparison. As shown in Figure~\ref{b}, an increase in beam size significantly increases the effect, and both ER and RR contribute to the results, with ER contributing more to the results than RR because ChatKBQA w/o RR is better than ChatKBQA w/o ER.

\subsection{Generate-then-Retrieve Or Retrieve-then-Generate (RQ3)}

\begin{wraptable}{r}{0.5\textwidth}
\vspace{-3mm}
\scriptsize
\centering
\setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Fine-tuning Settings}} & \multicolumn{4}{c}{\textbf{WebQSP}}                               \\ \cline{2-5} 
                                      & \textbf{Max Token} & \textbf{EM} \% & \textbf{BM} \% & \textbf{SM} \% \\ \hline
Llama-2-7B w/o R             & \textbf{512}     & \textbf{63.5}     & \textbf{74.7 }         & \textbf{91.1}              \\ \hline
Llama-2-7B w Top1 R          & 612     & 58.5     & 72.3          & 88.4              \\
Llama-2-7B w Top2 R          & 712     & 59.7     & 73.6          & 89.0              \\
Llama-2-7B w Top5 R          & 1012    & 55.6     & 68.3          & 85.3              \\
Llama-2-7B w Top10 R         & 2012    & 53.1     & 67.9          & 84.8              \\ \bottomrule
\end{tabular}
}
\caption{\label{t2}
Comparison of whether or not utilizing retrieval results before fine-tuning Llama-2-7B for logical form generation in ChatKBQA. 
}
\vspace{-3mm}
\end{wraptable}

In order to verify that our proposed LLMs-based Generate-then-Retrieve method is better than previous Retrieve-then-Generate methods, we take the knowledge fragments obtained in DECAF~\citep{DECAF} by designing instructions to add Top1, Top2, Top5, and Top10 retrieval results to the instruction, respectively, and comparing them with the fine-tuning of Llama-2-7B without doing retrieval. 
As shown in Table~\ref{t2}, we find that without retrieval is better than with retrieval in the logical form generation in terms of extract match ratio (EM), match after beam search ratio (BM), and skeleton match ratio (SM), due to the fact that the information obtained from retrieval will have the erroneous interfering information and increase Max Token of instruction, which leads to catastrophic forgetting of the original problem for LLMs and increases the difficulty of training. At the same time, we observe that Llama-2-7B fine-tuning without retrieval achieves a BM of 74.7\% and SM hits 91.1\%, with good performance because of LLM's well-learned schema of entities and relations, which provides the basis for the retrieval after generation.


\subsection{Comparison with ChatGPT and T5 in Generation Phase (RQ4)}

To illustrate why ChatKBQA chooses to fine-tune open-source generative LLMs such as Llama-2-7B and ChatGLM2-6B, we replace the LLMs in the generative part of the logical form with ChatGPT, GPT-4~\citep{GPT4}, T5~\citep{T5}, and Flan-T5~\citep{Flan-T5}, respectively, and observe their results in Extract Match (EM) and Skeleton Match without beam (SM) results. 
As shown in Figure~\ref{c}, ChatGPT and GPT-4, although having large parametric quantities, cannot generate standardized logical forms well because they aren't open source to be fine-tuned, whereas T5 and Flan-T5 can capture the skeletons well after fine-tuning, but the EM is only about 10\%, which is much worse than the 63\% of Llama-2-7B, and therefore does not guarantee subsequent unsupervised entity and relation retrieval. Fine-tuned open-source LLMs such as Llama-2-7B~\citep{Llama2} and ChatGLM2-6B~\citep{GLM} show stronger semantic parsing ability than models such as T5 and ChatGPT, and can generate higher-quality logical forms in both EM \& SM.

\subsection{Analysis of Efficiency of Retrieval in Retrieval Phase (RQ5)}

To embody the Generate-then-Retrieve method improving the efficiency of retrieval, we compare entity retrieval (ER) and retrieval (RR) after logical form generation (AG-R) with traditional retrieval from natural language questions (NL-R). We define the efficiency of retrieval as the average similarity ranging [0,1] between the text to be retrieved and the set of retrieved answers, which is scored by different retrieval models. Note that BM25 needs to be scored and then mapped to the similarity range of [0,1] by the mapping function. 

As Figure~\ref{d} shows, all three retrieval methods SimCSE~\citep{SimCSE}, Contriever~\citep{Contriever}, and BM25~\citep{BM25} consider AG-R to be more efficient than NL-R for both ER and RR, with the gap being more pronounced for RR. This is due to the fact that NL-R still needs to determine the location of the entities or relations. However, this step has been completed in AG-R after LLM generates the logical forms. Moreover, the generated logical form has fewer kinds of relations than entities in general, and the logical form is generally predicted directly and accurately, whereas, in natural language problems, the relations are generally represented implicitly, which makes the situation completely different in terms of the efficiency of AG-R over NL-R on RR, which reinforces the benefits of the Generate-then-Retreive approach.



\subsection{Plug-and-Play Characteristics (RQ6)}

\begin{wraptable}{r}{0.5\textwidth}
\vspace{-2mm}
\scriptsize
\centering
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{3}{c}{\textbf{ChatKBQA Framework}}                    & \multicolumn{3}{c}{\textbf{WebQSP}}                                                                        \\ \hline
\textbf{LLMs} & \textbf{Tuning} & \textbf{Retrieval} & \textbf{F1}                  & \textbf{Hits@1} & \textbf{Acc}                 \\ \hline
Baichuan2-7B  & LoRA                   & SimCSE                    & 79.1                         & 81.5                                         & 74.1                         \\
Baichuan2-13B & LoRA                   & SimCSE                    & 79.4                         & 82.1                                         & 74.4                         \\
ChatGLM2-6B   & LoRA                   & SimCSE                    & 79.8                         & 82.7                                         & 74.5                         \\
Llama-2-7B    & LoRA                   & SimCSE                    & 80.0                         & 82.4                                         & 75.2                         \\
\textbf{Llama-2-13B}   & \textbf{LoRA}                   & \textbf{SimCSE}                    & \textbf{82.6}                & \textbf{85.2}                                & \textbf{77.5}                \\ \hline
Llama-2-13B   & QLoRA                  & SimCSE                    & 81.9 & 85.0                 & 76.9 \\
ChatGLM2-6B   & P-Tuning v2            & SimCSE                    & 74.6 & 77.8                 & 70.6 \\
Llama-2-13B   & Freeze                 & SimCSE                    & 81.7 & 84.7                 & 76.8 \\ \hline
Llama-2-13B   & LoRA                   & Contriever                & 81.5 & 83.6                 & 76.8 \\
Llama-2-13B   & LoRA                   & BM25                      & 79.8 & 80.5                 & 72.7 \\ \bottomrule
\end{tabular}
}
\caption{\label{framework}
Plug-and-play performance comparison of ChatKBQA framework for replacing LLMs, tuning methods, and unsupervised retrieval methods, respectively, with the beam size all set as 8.
}
\vspace{-2mm}
\end{wraptable}

\begin{table*}[t]

\end{table*}

ChatKBQA is a KBQA framework based on LLMs with plug-and-play characteristics that can flexibly replace three parts: LLM, efficient tuning method, and unsupervised retrieval method. We choose Llama-2-13B~\citep{Llama2} for LLM, LoRA~\citep{LoRA} for the tuning method, and SimCSE~\citep{SimCSE} for the retrieval method as the basic variant, setting the beam size for all variants to 8 for comparison on a single A40 GPU. We replace Baichuan2-7B~\citep{Baichuan2}, Baichuan2-13B~\citep{Baichuan2}, ChatGLM2-6B~\citep{GLM}, Llama-2-7B~\citep{Llama2} in the LLM part, QLoRA~\citep{QLoRA}, P-Tuning v2~\citep{P-Tuningv2}, Freeze~\citep{Freeze} in the tuning part, and Contriever~\citep{Contriever}, BM25~\citep{BM25} in the retrieval part. Due to the plug-and-play characteristics of ChatKBQA, as the LLMs and the methods of tuning and retrieval are upgraded, the KBQA task will be solved better with good flexibility and extensibility.







\section{Conclusion}

In this work, we present ChatKBQA, a generate-then-retrieve framework for Knowledge Base Question Answering (KBQA) that leverages the power of modern fine-tuned large language models (LLMs). By focusing on the generation of logical forms prior to retrieval, our method offers a significant shift from traditional approaches, addressing inherent challenges such as retrieval inefficiencies and the misleading influence of retrieval errors in semantic parsing with fine-tuned open-source LLMs and unsupervised retrieval methods. Our experimental results are based on two standard KBQA benchmarks, WebQSP and CWQ, confirming that ChatKBQA achieves a new state-of-the-art performance in the KBQA domain. Moreover, the simplicity and flexibility of our framework, especially its plug-and-play characteristics, make it a promising direction for integrating LLMs with knowledge graphs for more interpretative and knowledge-required question-answering tasks.











\subsubsection*{Acknowledgments}
This work is supported by the National Science Foundation of China (Grant No. 62176026) and the Beijing Natural Science Foundation (M22009). This work is also supported by the BUPT Excellent Ph.D. Students Foundation and the BUPT Postgraduate Innovation \& Entrepreneurship Project led by Haoran Luo.




\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\appendix














\end{document}
