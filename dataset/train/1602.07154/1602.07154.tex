\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[T1]{fontenc}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphics}
\usepackage{mathtools}
\usepackage{fullpage}

\newcommand{\Order}{\mathrm{O}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\cl}[1]{\overline{#1}}
\newcommand{\Rank}{\textsc{Ranking}}
\newcommand{\KVV}{\textsc{KVV}}
\newcommand{\Greedy}{\textsc{Greedy}}
\newcommand{\eps}{\varepsilon}

\newcommand{\OPT}{\textsc{opt}}
\newcommand{\opt}{\OPT}
\newcommand{\ALG}{\textsc{alg}}
\newcommand{\alg}{\ALG}
\newcommand{\ADV}{\textsc{adv}}
\newcommand{\adv}{\ADV}
\newcommand{\profit}{\textsc{profit}}
\newcommand{\OFF}{\textsc{off}}
\newcommand{\off}{\OFF}
\newcommand{\qStr}{{q\text{-}\textsc{sgkh}}}
\newcommand{\cStr}{{c!\text{-}\textsc{sgkh}}}
\newcommand{\twoStr}{{2\text{-}\textsc{sgkh}}}
\newcommand{\mat}{\textsc{mat}}
\newcommand{\OO}{\ensuremath{{O}}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\poly}{poly}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}

\bibliographystyle{plainurl}


\title{On the Power of Advice and Randomization for Online Bipartite Matching \footnote{Research supported in part by the ANR projects ANR-11-BS02-0015,  ANR-15-CE40-0015, ANR-12-BS02-005, by the Icelandic Research Fund grants-of-excellence no.\ 120032011 and 152679-051 and by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme no.\ 648032.}}

\author{Christoph D\"{u}rr\thanks{Sorbonne Universités, UPMC Univ Paris 06, CNRS, LIP6, Paris, France}
\and
Christian Konrad\thanks{Reykjavik University, Reykjavik, Iceland}
\and
Marc Renault\thanks{IRIF, CNRS,  Universit\'{e} Paris Diderot, Paris, France}}


\begin{document}

\maketitle

\begin{abstract}
While randomized online algorithms have access to a sequence of uniform random bits, deterministic
online algorithms with advice have access to a sequence of {\em advice bits}, i.e., bits that are set
by an all-powerful oracle prior to the processing of the request sequence.
Advice bits are at least as helpful as random bits, but how helpful are they? In this work, we investigate the
power of advice bits and random bits for online maximum bipartite matching (\textsc{MBM}).

The well-known Karp-Vazirani-Vazirani algorithm \cite{kvv90} is an optimal
randomized -competitive algorithm for \textsc{MBM} that requires access to
 uniform random bits. We show that  advice
bits are necessary and  sufficient in order to obtain a
-competitive deterministic advice algorithm. Furthermore,
for a large natural class of deterministic advice algorithms, we prove that
 advice bits are required in order to improve on the -competitiveness of
the best deterministic online algorithm, while it is known that 
bits are sufficient \cite{bkkk11}.

Last, we give a randomized online algorithm that uses  random bits, for integers ,
and a competitive ratio that approaches  very quickly as  is increasing.
For example if , then the difference between  and the achieved competitive ratio
is less than .
\end{abstract}

\section{Introduction}
\paragraph*{Online Bipartite Matching}
The maximum bipartite matching problem (\textsc{MBM}) is a well-studied problem in the area of online
algorithms \cite{kvv90,bm08,djk13}. Let  be a bipartite graph with
 and , for some integers . We
assume  allowing bounds to be stated as simple functions of  rather than of  and .
The -vertices together with their incident edges
arrive online, one at a time, in
some adversarial chosen order . Upon arrival of a vertex ,
the online algorithm has to irrevocably decide to which of its incident (and yet unmatched) -vertices
it should be matched.
The considered quality measure is the well-established {\em competitive ratio}
\cite{st85}, where
the performance of an online algorithm is compared to the performance of the best
offline algorithm: A randomized online algorithm  for \textsc{MBM}
is -competitive if the matching  output by  is such that
, where the expectation is taken over the random coin flips,
and  is a maximum matching.

In 1990, Karp, Vazirani and Vazirani \cite{kvv90} initiated research on
online \textsc{MBM} and presented a -competitive randomized algorithm denoted \KVV. It chooses
a permutation  of the -vertices uniformly at random and then runs
the algorithm ,
which matches each incoming -vertex  to the free incident -vertex  of minimum rank
(i.e.,  for all free incident vertices ).
If there is no free -vertex, then  remains unmatched.
They showed that no online algorithm has a better competitive ratio than , implying
that \KVV~is optimal.
For deterministic online algorithms, it is well-known that the \textsc{Greedy} matching algorithm,
which can be seen as running  using a fixed arbitrary , is -competitive,
and is optimal for the class of deterministic online algorithms.

\paragraph*{Improving on } Additional assumptions are needed in order to improve on the
competitive ratio . For example, Feldman et al.~\cite{fmmm09} introduced the online stochastic matching
problem, where a bipartite graph  and a probability distribution  is
given to the algorithm. The request sequence then consists of vertices of  that are
drawn according to . Feldman et al.\ showed that the additional knowledge can be used to
improve the competitive ratio to , which has subsequently
been further improved \cite{bk10,mgs11}. Another example is a work by Mahdian and Yan \cite{my11}, who considered
the classical online bipartite matching problem with a random arrival order of vertices. They analysed the
\textsc{KVV}~algorithm for this situation and proved that it is -competitive.

\paragraph*{Online Algorithms with Advice}
It is a common theme in online algorithms to equip an algorithm with additional knowledge that
allows it to narrow down the set of potential future requests and, thus, design algorithms that have
better competitive ratios as compared to algorithms that have no knowledge about the future. Additional
knowledge can be provided in many different ways, e.g.\ access to lookahead \cite{hs92,g95}, probability
distributions about future requests \cite{fmmm09,my11},
or even by giving an isomorphic copy of the
input graph to the algorithm beforehand \cite{h99}. Dobrev et al. \cite{DRP2008} and later Emek et al. \cite{efkr11} first quantified
the amount of additional knowledge (advice) given to an online algorithm in an information theoretic sense.
They showed that a specific problem requires at least  bits of advice, for some function , in order
to achieve optimality  \cite{DRP2008} or in order to achieve a particular competitive ratio \cite{efkr11}.
Advice lower bounds are meaningful in practice as they apply to any potential type of
additional information that could be given to an algorithm.

In the advice model, a computationally all-powerful oracle is given the entire request sequence and
computes an advice string that is provided to the algorithm.
Algorithms with advice are not usually designed with practical considerations in mind but to show a
theoretical limit on what can be done. As such, the algorithms are often impractical due to the nature of
the advice or the complexity in calculating the advice.
However, from a theoretical perspective, advice algorithms are necessary to determine the exact advice
complexity of online problems (how many advice bits are necessary and sufficient) and thus provide limits on
the achievable and more practically relevant lower bounds.





\paragraph*{Our Objective and Previous Results} Our objectives are to determine the advice complexity
of \textsc{MBM} and to investigate the power of random and advice bits for this problem.

A starting point is a result of B\"{o}ckenhauer et al.~\cite{bkkk11}, who gave a method that allows the
transformation of a randomized online algorithm into a deterministic one with advice with a similar
approximation ratio. More precisely, given a randomized online algorithm \textbf{A} for a minimization problem
 with approximation factor  and possible inputs  of length ,
B\"{o}ckenhauer et al.\ showed that a -competitive deterministic online algorithm \textbf{B}
with 
bits
\footnote{Throughout the paper, logarithms, where the base is omitted, are implicitly binary logarithms.}
of advice can be deduced from \textbf{A}, for any ,
where  is the binary logarithm in this paper.
The calculation of the advice and the computations executed by \textbf{B} require exponential time, since
\textbf{A} has to be simulated on all potential inputs  on all potential
random coin flips.

The technique of Böckenhauer et al. \cite{bkkk11} can also be applied to maximization problems such as \textsc{MBM}\footnote{It is
straightforward to adapt the proof of Theorem~5 of \cite{bkkk11} accordlingly. For completeness, a proof is given in the full
version of this paper.}.
Applied to the  algorithm, we obtain the following theorem.  For the proof see appendix~\ref{appendix:boeckenhauer}.

\begin{theorem} \label{thm:bockenhauer}
 There is a deterministic online algorithm with  bits
 of advice for \textsc{MBM} with competitive ratio , for any .
\end{theorem}

This result is complemented by a recent result of Mikkelsen~\cite{m15}, who showed that
for {\em repeatable problems} (see \cite{m15} for details) such as \textsc{MBM},
no deterministic online algorithm with advice sub-linear in 
has a substantially better competitive ratio than any randomized algorithm without advice.
Thus, using
 advice bits, a -competitive deterministic algorithm can be obtained,
and no algorithm using  advice bits can substantially improve on this result. Furthermore,
Miyazaki~\cite{m14} showed that  advice bits are necessary
and sufficient in order to compute a maximum matching.

\paragraph*{Our Results on Online Algorithms with Advice}
Consider a deterministic online algorithm with  bits of advice for \textsc{MBM}. Our previous exposition
of related works shows that the ranges  and 
are well understood. In this work, we thus focus on the ranges
 and . Our first set of results concerns
-competitive deterministic advice algorithms. We show:
\begin{enumerate}
 \item There is a deterministic -competitive online algorithm, using
  advice bits for \textsc{MBM}.

 \item Every deterministic -competitive online algorithm for \textsc{MBM} uses
  bits of advice.
\end{enumerate}
Our lower bound result is obtained by a reduction from the {\em string guessing game} of B\"{o}ckenhauer et
al.~\cite{bhkkss14}, a problem that is difficult even in the presence of a large number of advice bits.
This technique has repeatedly been applied for obtaining advice lower bounds, e.g.\ \cite{arrs13,GuptaKL13,BoyarKLL14,adkrr15,BoyarKLL16,BianchiBBKP16}.
Our algorithm simulates an augmenting-paths-based algorithm by Eggert et al. \cite{ekms11}, that has originally
been designed for the data streaming model, with the help of advice bits.
It is fundamentally
different to the  algorithm, however, inspired by the simplicity of , we are particularly interested in
the following class of algorithms:
\begin{definition}[\textsc{Ranking}-algorithm]
 An online algorithm \textbf{A} for \textsc{MBM} is called -algorithm if it follows the
 steps: (1) Determine a ranking ; (2) Return .
\end{definition}
The \KVV~algorithm is a \Rank-algorithm, where in step (1), the permutation  is chosen uniformly
at random. The algorithm
described in Theorem~\ref{thm:bockenhauer} is a deterministic \Rank-algorithm with  bits
of advice that computes the permutation  from the available advice bits.
While we cannot answer the question how many advice bits are needed for deterministic online algorithms
in order to obtain a competitive ratio strictly larger than  (and thus to improve on \textsc{Greedy}),
we make progress concerning \Rank~algorithms:
\begin{enumerate}
\setcounter{enumi}{2}
 \item Every -algorithm that chooses  from a set of at most
  permutations, for a small constant , has approximation factor
 at most , for any .
\end{enumerate}
The previous result implies that every -competitive deterministic online
\textsc{Ranking}-algorithm requires  advice bits.

Next, since the computation of the advice and the algorithm of Theorem~\ref{thm:bockenhauer} are not efficient,
we are interested in fast and simple \textsc{Ranking} algorithms. We identify a subclass of \textsc{Ranking} algorithms,
denoted \textsc{Category} algorithms, that leads to interesting results, both as deterministic algorithms with advice and
randomized algorithms without advice.

\begin{definition}[\textsc{Category}-algorithm]
 A -algorithm \textbf{A} is called a \textsc{Category}-algorithm if it follows the steps:
 \begin{itemize}
  \item Determine a category function  for some integer  with ;
  \item Let  be the unique permutation of the
-vertices such that for two vertices   if
and only if  or .
 \item Return .
 \end{itemize}
\end{definition}
Categories can be seen as coarsened versions of rankings, where multiple items with adjacent ranks are
grouped into the same category and within a category, the natural ordering by vertex identifier is used.
We prove the following:
\begin{enumerate}
\setcounter{enumi}{3}\item
 There is a deterministic -competitive online \textsc{Category}-algorithm,
 using  bits of advice (and thus two categories).
\end{enumerate}
The oracle determines the categories depending on whether a -vertex would be matched by a run of \textsc{Greedy}.
We believe that this type of advice is particularly interesting since it does not require the oracle to compute an
optimal solution.

\paragraph*{Our Results on Randomized Algorithms}
Last, we consider randomized algorithms with limited access to random bits. The \KVV-algorithm selects a
permutation  uniformly at random, and, since there are  potential permutations,
 random bits are required in order to obtain a uniform choice.
We are interested in randomized algorithms that employ fewer random bits. We
consider the class of randomized \textsc{Category}-algorithms, where the categories of the -vertices
are chosen uniformly at random. We show:
\begin{enumerate}
\setcounter{enumi}{4} \item
 There is a randomized \textsc{Category}-algorithm using  random bits
 with approximation factor , for any integer .
\end{enumerate}
For , the competitive ratio evaluates to . It approaches  very quickly, for example,
for  the absolute difference between the competitive ratio and  is less than
.
Our analysis is based on the analysis of the \textsc{KVV} algorithm by Birnbaum and Mathieu \cite{bm08}
and uses a result by Konrad et al.~\cite{kmm12}
concerning the performance of the \textsc{Greedy}
algorithm on a randomly sampled subgraph which was originally developed in the context of streaming algorithms.

The results as described above are summarized in Table~\ref{tab:results}.

\begin{table}[ht]
\small
\begin{center}
 \begin{tabularx}{\textwidth}{|llX|}
\hline
Deterministic ratio & \# of advice bits & Description and Authors    \\
\hline
 &  & (Miyazaki \cite{m14}) \\
 &  & Application of Eggert et al. \cite{ekms11} (here) \\
 &  & LB holds for any online algorithm (here) \\
 &  & LB holds for any online algorithm (Mikkelsen \cite{m15}) \\
 &   & Exp. time \textsc{Ranking}-alg. (Böckenhauer et al. \cite{bkkk11}) \\
 &  & \textsc{Category}-algorithm using two categories (here) \\
 &  &  LB holds for \textsc{Ranking}-algorithms (here) \\
\hline
Randomized ratio & \# of random bits & Description and Authors    \\
\hline
 &  & \textsc{KVV} algorithm (Karp, Vazirani, Vazirani \cite{kvv90}) \\
 &  &   \textsc{Category}-algorithm using  categories (here)\\
\hline
\end{tabularx}
\caption{Overview of our results, sorted with decreasing competitiveness. \label{tab:results}}
\end{center}
\end{table}

\paragraph*{Models for Online Algorithms with Advice}
The two main models for online computation with advice are the per-request model of Emek
et at.~\cite{efkr11} and the tape model of B\"{o}ckenhauer et al.~\cite{bkkkm09}.
Both models were inspired by the original model proposed by Dobrev et al.~\cite{DRP2008}.
In the model of Emek et at.~\cite{efkr11}, a bit string of a fixed length is received by the
algorithm with each request for a total amount of advice that is at least linear in the size of
the input. For this work, we use the tape model of B\"{o}ckenhauer et al.~\cite{bkkkm09}, where the
algorithm has access to an infinite advice string that it can access at any time (see
Section~\ref{sec:prelim} for a formal definition), allowing for advice that is sub-linear in
the size of the input. Many online problems have been studied in the setting of online algorithms
with advice (e.g. metrical task system \cite{efkr11}, -server
problem \cite{efkr11,bkkk11,RenaultR15,GuptaKL13}, paging \cite{DRP2008,bkkkm09}, bin packing
problem \cite{RenaultRS15,BoyarKLL16,adkrr15}, knapsack problem \cite{BockKKR14}, reordering buffer management
problem \cite{arrs13}, list update problem \cite{BoyarKLL14}, minimum spanning tree
problem \cite{BianchiBBKP16} and others). Interestingly, a variant of the algorithm with advice for
list update problem of \cite{BoyarKLL14} was used to gain significant improvements in the compression
rates for Burrows-Wheeler transform compression schemes \cite{KamaliL14}.
The information-theoretic lower bound techniques for online algorithms with advice proposed by
Emek et al.~\cite{efkr11} applies to randomized algorithms and uses a reduction to a matching
pennies game (essentially equivalent to the string guessing game). The reduction technique using the string
guessing game of B\"{o}ckenhauer et al.~\cite{bhkkss14} is a refinement specifically for deterministic
algorithms of the techniques of Emek et al.


\paragraph*{Outline} Preliminaries are discussed in Section~\ref{sec:prelim}. Our
-competitive algorithm and a related advice lower bound are presented in
Section~\ref{sec:one-minus-eps}. Then, in Section~\ref{sec:lb-rank}, we give the advice lower
bound for -competitive -algorithms. Last, in Section~\ref{sec:cat-algos}, we consider our randomized \textsc{Category} algorithm
and our -competitive advice \textsc{Category} algorithm.


\section{Preliminaries}\label{sec:prelim}
Unless stated otherwise, we consider a bipartite input graph  with  and
, for integers  such that .
The neighbourhood of a vertex  in graph  is denoted by .
Let  be a matching in . We denote the set of vertices matched in  by . For a vertex ,
 denotes the vertex that is matched to  in . Generally, we write  to denote a maximum
matching, i.e., a matching of largest cardinality. For ,
 denotes the size of a maximum matching in , the subgraph induced by .

\paragraph*{The \Rank~Algorithm}
Given permutations 
and , we write  to denote the output matching of the
 algorithm when the -vertices arrive in the order given by , and the -vertices are ranked
according to . We may write  to denote  if  and 
are clear from the context.

\paragraph*{The \textsc{Greedy} Matching Algorithm}  processes the edges
of a graph in arbitrary order and inserts the current edge  into an initially empty matching  if
 is a matching. It computes a maximal matching which is of size at least .


\paragraph*{Category Algorithms} For an integer , let  be an assignment
of categories to the -vertices. Then let  be the unique permutation of the
-vertices such that for two vertices   if
and only if  or . The previous definition of 
is based on the natural ordering of the -vertices. This gives a certain stability to the resulting permutation,
since changing the category of a single vertex  does not affect the relative order of the vertices
.




\paragraph*{The Tape Advice Model} For a given request sequence  of length  for a maximization problem,
an \emph{online algorithm with advice} in the \emph{tape advice model} computes the output sequence
, where  is a function of the requests from  to
 of  and the infinite binary advice string . Algorithm  has an advice complexity of  if,
for all  and any input sequence of length ,  reads no more than  bits from .




\section{Deterministic -competitive Advice Algorithms} \label{sec:one-minus-eps}



\subsection{Algorithm With  Bits of Advice}



The main idea of our online algorithm is the simulation of an augmenting-paths-based algorithm
with the help of advice bits. We employ the deterministic algorithm of Eggert et al.~\cite{ekms11} that has
been designed for the data streaming model. It computes a -approximate
matching, using  passes over the edges of the input graph, where each pass 
is used to compute a matching  in a subgraph , for
some subsets  and , using the \textsc{Greedy}
matching algorithm.
In the first pass,  is computed in  and thus constitutes a -approximation.
Let .
Then,  phases follow, where in each phase, a set of disjoint augmenting
paths is computed using  applications of the \textsc{Greedy} matching
algorithm (and thus  passes per phase).
At the end of a phase,  is augmented using the augmenting-paths found in this phase. Upon
termination of the algorithm,  constitutes a -approximation (see \cite{ekms11} for
the analysis).

The important property that allows us to translate this algorithm into an online algorithm with advice
is the simple observation that the computed matching  is a subset of . For every ,
we encode the vertices
 and  that constitute the vertices of  using  advice bits.
Furthermore, for every vertex , we also encode the index 
of the matching  that contains the edge that is incident to  in the final matching 
(if  is not matched in , then we set ). Last, using  bits, we encode the integers
 and , using a self-delimited encoding. Parameters  are required in order to determine the word size
that allows the storage of the indices , and to determine the subgraphs . The total number of
advice bits is hence .

After having read the advice bits, our online algorithm computes the  \textsc{Greedy}
matchings  simultaneously in the background while receiving the requests. Upon arrival of an
, we match it to the  such that  incident to  if ,
and we leave it unmatched if . We thus obtain the following theorem:

\begin{theorem} \label{thm:ub-advice}
 For every , there is a -competitive deterministic online algorithm for \textsc{MBM}
 that uses  bits of advice.
\end{theorem}



\subsection{ Advice Lower Bound}
We complement the advice algorithm of the previous section with an 
advice lower bound for -competitive deterministic advice algorithms.
To show this, we make use of the lower bound techniques
of \cite{bhkkss14} using the {\em string guessing game}, which is defined as follows.

\begin{definition}{~\textup{\cite{bhkkss14}}.} The {\em string guessing problem with known history}
over an alphabet  of size  () is an online minimization problem. The input consists
of  and a request sequence  of the characters, in order, of an  length string.
An online algorithm  outputs a sequence  such that 
for some computable function .  An important aspect of this problem is that the algorithm needs to produce its output character \emph{before} the corresponding request: request  is revealed immediately after the algorithm outputs . The cost of  is the Hamming distance between  and .
\end{definition}

In \cite{bhkkss14}, the following lower bound on the number of advice bits is shown for .

\begin{theorem}{\textup{\cite{bhkkss14}}}\label{thm:BockThm}
  Consider an input string of length  for . The minimum number of advice bits for any
  deterministic online algorithm that is correct for more than  characters,
  for , is
  , where  is the
  -ary entropy function.
\end{theorem}

First, we define a sub-graph that is used in the construction of the lower bound sequence.

\begin{definition}
 A bipartite graph is {\em -semi complete}, if it is isomorphic to
  with  and
 .
\end{definition}

The following lemma presents the reduction from  to \textsc{MBM}.

\begin{lemma}\label{lem:reduceMat}
  For an integer , suppose that there is a deterministic -competitive online algorithm for
  \textsc{MBM}, using  bits of advice, where .
  Then, there exists a deterministic algorithm for , using  bits of advice, that is correct
  for at least  characters of the -length string.
\end{lemma}

\begin{proof}
  Let  be a deterministic -competitive online algorithm for \textsc{MBM},
  using  bits of advice, with , for an
  integer . We will present an algorithm  that, in an online manner,
  will generate a request sequence  based on its input,  (of length ), that can be processed
  by . Further, the advice received by  will be the advice that
   requires for . As shown below, the length of  is , hence
   requires  bits of advice. The solution produced by  on
   will define the output produced by .

  Suppose first that the entire input sequence  is known in advance (we will argue later how to get around
  this assumption). Let  be an enumeration of all the permutations of length ,
and let  be a bijection between , the alphabet of the  problem,
and an index of a permutation in . The request sequence  has a length of ,
consisting of  distinct -semi-complete graphs, where each graph is based on a request of .
That is, for each request  in , we append  requests to  that correspond to
the -vertices of a -semi-complete graph, where the indices of the -vertices
are permuted according to the permutation .


Since  is not known in advance,
we must construct  in an online manner while predicting the requests . For each
request , the procedure is as follows:

Let  be the -length prefix of . Note that when predicting
request , requests  have already been revealed, and
 can thus be constructed.
The algorithm  simulates  on  followed by
another -semi-complete graph  such that, for ,
when vertex  is revealed, the -vertices incident to  correspond exactly
to the unmatched -vertices of  in the current matching of . By construction,
 computes a perfect matching in .
The computed perfect matching corresponds to a permutation  at some index  of , and algorithm
 outputs  as a prediction for .

Consider a run of  on . If  computes
a perfect matching on the th semi-complete graph, then our algorithm predicted  correctly.
Similarly, if this matching is not perfect, then
our algorithm failed to predict . Let  be the total number of imperfect matchings,
let  denote the matching computed by  on , and let
 denote a perfect matching in the graph given by . Then:

\end{proof}

We prove now the main lower bound result of this section.
\begin{theorem}\label{thm:lbEps}
  For an integer , any deterministic online algorithm with advice for \textsc{MBM}
  requires at least  bits of advice to be
  -competitive for , where  is the -ary
  entropy function and .
\end{theorem}

\begin{proof}
  For , let  be a deterministic -competitive
  online algorithm for \textsc{MBM}, using  bits of advice.
  By Lemma~\ref{lem:reduceMat}, there exists an algorithm for  that uses
   bits of advice and is correct for at least  characters of the -length
  input string. The bounds on  and  imply
  . Thus, Theorem~\ref{thm:BockThm} implies
   and, hence,
  
\end{proof}

Setting  for all , we get the following corollary. Note that,
as  approaches  from below,  also approaches  from below and  approaches .

\begin{corollary} \label{cor:lb-advice}
  For any , any -competitive deterministic online algorithm with advice
  for \textsc{MBM} requires  bits of advice.
\end{corollary}

\section{Advice Lower Bound for \textsc{Ranking} Algorithms} \label{sec:lb-rank}
Let  be rankings.
We will show that there is a -vertex graph  and an arrival order  such that
, for every  and every constant
, while  contains a perfect matching. Furthermore, the construction is such that
.

The key property required for our lower bound is the fact that we can partition the set of -vertices
into disjoint subsets , each of large enough size, such that for every
 with  and , the sequence  is
monotonic, for every . In other words, the ranks of the nodes  appear
in the rankings  in either increasing or decreasing order. For each set ,
we will construct a vertex-disjoint subgraph  on which  computes a matching that is close to
a -approximation. The subgraphs  are based on graph  that we define next.

\paragraph*{Construction of }
We construct now graph  with , for some even integer ,
on which {\Rank} computes a matching that is close to a -approximation, provided that the 
vertices are ranked in either increasing or decreasing order.

Let  be so that  arrives before  in .
Let  be so that  (which implies ). Then, for 
we define , and for 
we define .
The graph  is illustrated in Figure~\ref{fig:lb-ranking}. It has the following properties:
\begin{enumerate}
 \item If the sequence  is increasing, then .
 \item If the sequence  is decreasing, then .
 \item  has a perfect matching (of size ).
\end{enumerate}



\begin{figure}
{\small
\begin{center}
\includegraphics[height=2.6cm]{figs/lb-ranking.pdf}

\vspace{-3.03cm}
\textbf{V} \hspace{0.25cm} \textbf{U} \hspace{0.25cm} \textbf{V} \hspace{0.25cm} \textbf{U} \hspace{10.6cm} 

\vspace{0.23cm}


 \hspace{0.25cm}  \hspace{0.30cm}  \hspace{0.25cm}  \hspace{10.6cm} 

\vspace{0.32cm}

 \hspace{0.25cm}  \hspace{0.30cm}  \hspace{0.25cm}  \hspace{10.6cm} 

\vspace{0.32cm}

 \hspace{0.25cm}  \hspace{0.30cm}  \hspace{0.25cm}  \hspace{10.6cm} 

\vspace{0.32cm}

 \hspace{0.25cm}  \hspace{0.30cm}  \hspace{0.25cm}  \hspace{10.6cm} 


\vspace{-0.1cm}

\hspace{3.8cm} \begin{minipage}{2cm}
\textsc{Ranking}: increasing ranks
\end{minipage} \hspace{1.4cm}
\begin{minipage}{2cm}
\textsc{Ranking}: decreasing ranks
\end{minipage} \hspace{1.2cm}
\begin{minipage}{2.5cm}
perfect matching


\end{minipage}


\end{center}
}
\caption{Left: -vertices arrive in order .
'\textsc{Ranking}: increasing ranks' shows the resulting matching when
.
'\textsc{Ranking}: decreasing ranks' shows the resulting matching when
.
Right: Perfect matching.
\label{fig:lb-ranking}}
\end{figure}




\paragraph*{Lower Bound Proof}
We prove first that we can appropriately partition the -vertices that allow us to define the
graphs . Our prove relies on the well-known Erd\H{o}s-Szekeres theorem \cite{es87} that we state in the
form we need first.

\begin{theorem}[Erd\H{o}s-Szekeres \cite{es87}] \label{thm:es}
 Every sequence of distinct integers of length  contains a monotonic (either increasing or
 decreasing) subsequence of length .
\end{theorem}



\begin{lemma} \label{lem:decomp}
Let  be an arbitrary small constant. Then for any  permutations
 with
,
there is a partition of  into subsets  such that:
\begin{enumerate}
 \item  for every ,
 \item ,
 \item For every  with , and every , the sequence
 is monotonic.
\end{enumerate}
\end{lemma}
\begin{proof}
 Let . We iteratively remove subsets  from  until . The remaining elements then define
 set . Thus, by construction, Item~2 is fulfilled.


 Suppose that we have already defined sets . We show how to obtain set .
 Let  ( if ). Note that .
 By Theorem~\ref{thm:es}, there is a
 subset 
 with  such that the sequence
  is monotonic. Then, again by Theorem~\ref{thm:es}, there is
 a subset  with 
 such that the sequences  are monotonic, for every . Similarly,
 we obtain that there is a subset  with
  such that the sequences
  are monotonic, for every .

 In order to guarantee Item~1, we solve the inequality
 
for , and we obtain . This completes the proof.
\end{proof}

Equipped with the previous lemma, we are ready to prove our lower bound result.
\begin{theorem} \label{thm:lb-ranking}
 Let  be an arbitrary constant. For any 
 permutations  with
 and arrival order ,
there is a graph  such that for every :

while  contains a perfect matching.
\end{theorem}
\begin{proof}
Let .
Let  denote the hard instance graph.
Let  denote the partition of  according to Lemma~\ref{lem:decomp} with
respect to value .
Then, partition  into sets  such that  and for
,  . Graph  is the disjoint union of subgraphs
 and , for . Subgraph 
is an arbitrary graph that contains a perfect matching. If  is even, then
 is an isomorphic copy of . If  is odd, then
 is the disjoint union of an isomorphic copy of  and one edge.
Then,
 
\end{proof}


\section{Category Algorithms} \label{sec:cat-algos}
\subsection{Randomized Category Algorithm} \label{sec:rand-cat}
In this section, we analyse the following randomized -algorithm:
\begin{algorithm}[H]
 \begin{algorithmic}
  \REQUIRE , integer parameter 
  \STATE For every  random number in 
  \STATE  permutation on  such that  iff
   or   and , for every 
  \RETURN 
 \end{algorithmic}
 \caption{Randomized Category Algorithm \label{alg:rand}}
\end{algorithm}


\paragraph*{Considering Graphs with Perfect Matchings}
First, similar to \cite{bm08}, we argue that the worst-case performance ratio of Algorithm~\ref{alg:rand} is obtained
if the input graph contains a perfect matching. It requires the following observation:

\begin{theorem}[Monotonicity \cite{gm08,kvv90}]
 Consider a fixed arrival order  and ranking  for an input graph .
 Let  for some vertex . Let  be the arrival order/ranking
 when restricted to vertices . Then,  and 
 are either identical or differ by a single alternating path starting at .
\end{theorem}
The previous theorem shows that the size of the matching produced by Algorithm~\ref{alg:rand} is monotonic
with respect to vertex removals. Hence, if  is the graph obtained from  by removing all vertices that are
not matched by a maximum matching in , then the performance ratio of  on  cannot be better than on .
We can thus assume that the input graph  has a perfect matching and .


\paragraph*{Analysis: General Idea}
Let , and denote the matching computed by the algorithm by .
The important quantities to consider for the analysis of Algorithm~\ref{alg:rand} are the probabilities:

i.e., the probability that a randomly chosen -vertex of category  is matched by the algorithm. Determining
lower bounds for the quantities  is enough in order to bound the expected matching size, since

We will first prove a bound on  using a previous result of Konrad et al. \cite{kmm12}.
Then, using similar ideas as Birnbaum and Mathieu \cite{bm08}, we will prove inequalities of the form
, for some function  which allow us to bound the probabilities
.

\paragraph*{Bounding }
Let  be an arbitrary bipartite graph and let  be a uniform and random sample of 
such that a node  is in  with probability . Konrad et al. showed in \cite{kmm12} that when
running \textsc{Greedy} on the subgraph induced by vertices , a relatively large fraction
of the -vertices will be matched, for any order in which the edges of the input graph
are processed that is independent of the choice of . More precisely, they prove the following theorem
( denotes the output of \textsc{Greedy} on subgraph  if
edges of  are considered in the order given by ):
\begin{theorem}[\cite{kmm12}]
 Let  be a bipartite graph,  a maximum matching, and let  be a uniform and independent random sample
 of  such that every vertex belongs to  with probability , . Then for any edge arrival
 order , 
\end{theorem}
In , the vertices  are always preferred over vertices . Thus,
the matching  is identical to the matching obtained
when running  on the subgraph induced by . Since the previous theorem
holds for any edge arrival order (that is independent from the choice of ), we can apply
the theorem (setting ) and we obtain:

Since , we obtain .

\paragraph*{Bounding } The key idea of the analysis of Birnbaum and Mathieu for
the \textsc{KVV}-algorithm is the observation that, if a -vertex of rank  is not matched by the
algorithm, then its partner in an optimal matching is matched to a vertex of rank smaller than .
Applied to our algorithm, if a -vertex of category  is not matched, then its optimal
partner  is matched to a -vertex that belongs to a category . Thus:

The following lemma is similar to a clever argument by Birnbaum and Mathieu \cite{bm08}.
\begin{lemma} \label{lem:birnbaum-mathieu}

\end{lemma}
\begin{proof}
 Let  be uniformly distributed and let  be the respective ranking. Pick now a random
  and create new categories  such that  and for all
 . Let  be the ranking given by .

 Let . Suppose that in a run of ,  is matched
 to a vertex  with  and  remains unmatched. Then, we will show that
 in the run of ,  is matched to a vertex  with . This implies
 our result.

 First, suppose that  remains unmatched in . Then, 
 and the claim is trivially true. Suppose now that  is matched in . Then, similar to
 the argument of \cite{bm08}, it can be seen that  and  differ only by
 one alternating path  starting at  such that for all ,
  (1) ,
  (2) , and
  (3) .
Property (3) implies . Thus if the category  of the node that  is matched
to in  is , then the category  of the node that  is matched to in  is
also at most .
\end{proof}


The right side of Inequality~\ref{eqn:918} can be computed explicitly as follows:

This, together with Inequalities~\ref{eqn:456} and \ref{eqn:918}, yields .
We obtain:
\begin{theorem}\label{thm:rand}
 Let  be an integer. Then Algorithm~\ref{alg:rand} is a randomized online algorithm for
 \textsc{MBM} with competitive ratio
 
 that uses  random bits.
\end{theorem}
\begin{proof}
 Following \cite{bm08}, the inequality  yields
 ,
 where  and . According to Equality~\ref{eqn:299},
 we need to bound  from below. Quantity  is minimized if ,
 for all , which yields
 
 The result follows by plugging  into Equality~\ref{eqn:299}.
\end{proof}


\subsection{Advice Category Algorithm}
Let  be the identity function, and let . It is well-known
that  might be as poor as a -approximation. Intuitively, -vertices that are not matched
in  are ranked too high in  and have therefore no chance of being matched. We therefore assign
category  to -vertices that are not matched in , and category  to all other nodes, see
Algorithm~\ref{alg:advice-category}. We will prove that this strategy gives a -approximation algorithm.

\begin{algorithm}
 \begin{algorithmic}
  \STATE \textbf{Computation of advice bits}
  \STATE  permutation such that , ,  maximum matching
   \STATE 
  \STATE \textbf{Online Algorithm with Advice} \COMMENT{Function  is provided using  advice bits}
  \STATE  permutation on  such that  iff  or   and , for every 
  \RETURN 
 \end{algorithmic}
\caption{Category-Advice Algorithm \label{alg:advice-category}}
\end{algorithm}

Our analysis requires a property of  that has been previously used, e.g., in \cite{bm08}.
 \begin{lemma}[Upgrading unmatched vertices, Lemma~4 of \cite{bm08}] \label{lem:upgrade}
  Let  be a ranking and let . Let  be a vertex that is not matched in .
  Let  be the ranking obtained from  by changing the rank of  to any rank that is smaller
  than  (and shifting the ranks of other vertices accordingly), and let
  . Then, every vertex  matched in  to a vertex 
  is matched in  to a vertex  with .
 \end{lemma}



\begin{theorem} \label{thm:adv-cat}
 Alg.~\ref{alg:advice-category} is a -competitive online algorithm
 for \textsc{MBM} using  advice bits.
\end{theorem}
\begin{proof}
 Let  denote the matching computed by the algorithm.
 Let ,  be the subsets of vertices that are matched in .
 Further, let  and  (the vertices not matched in ).
See Figure~\ref{fig:adv-cat} for an illustration of these quantities.

\begin{figure}[H]
{\small
\begin{center}
\includegraphics[height=2cm]{figs/cat-advice.pdf}

 \hspace{1.9cm} 

\vspace{-2.3cm}

 \hspace{1.5cm}  \hspace{1.2cm}  \hspace{1.5cm} 

\vspace{0.6cm}

 \hspace{1.5cm}  \hspace{1.2cm}  \hspace{1.5cm} 

\vspace{-1.6cm}
 \hspace{3.2cm} 

\vspace{0.25cm}
  \hspace{3.3cm} 

\vspace{0.59cm}
  \hspace{3.3cm} 

\end{center}
}
\caption{Quantities employed in the analysis of Algorithm~\ref{alg:advice-category}.}
\label{fig:adv-cat}
\end{figure}


 Then, for , let . Let
 .
 Then,  since  (the input graph does
 not contain any edges between  and  since otherwise some of them would also be contained in ).
 This setting is illustrated in Figure~\ref{fig:adv-cat} in the appendix. We will bound now the sizes of  and  separately:
 \begin{itemize}
  \item \textit{Bounding .} Since -vertices are preferred over -vertices in
   and since there are no edges between  and ,  is a maximal matching between
  and . Since , we have 


 \item \textit{Bounding .} By Lemma~\ref{lem:upgrade}, all -vertices are matched in . Thus,
 

 \item \textit{Bounding .} The algorithm finds a maximal matching between  and
 . Since , we have
 ,
 and thus 
 \end{itemize}

\noindent We combine the previous bounds and we obtain:
 
Next, note that  and . We thus
obtain . Since
, we obtain . Furthermore,
Lemma~\ref{lem:upgrade} implies , and hence
 which is at least .
\end{proof}


\bibliography{permGuess}


\newpage
\appendix


\section{The Construction of B\"{o}ckenhauer et al. for Maximization Problems}
\label{appendix:boeckenhauer}

B\"{o}ckenhauer et al. \cite{bkkk11} showed that a deterministic advice algorithm can be obtained
from a randomized algorithm for a minimization problem. We provide a similar theorem for maximization
problems. The proof follows the proof of \cite{bkkk11} and is provided only for completeness of our work.
\label{sec:min-to-max}
\begin{theorem}
  For a maximization online problem , let  be the set of all possible inputs of
  length . Suppose that  is a randomized algorithm with a expected competitive ratio
  . Then, for any fixed , , it is possible to construct a deterministic
  algorithm that uses a total advice of size
   and that has a competitive ratio of at least
  , where  and .
\end{theorem}

\begin{proof}
  Let  be the maximum number of random bits used by  for a sequence of length . Hence, there are  possible random bit strings. First, we construct a subset, , of the  random strings with size  such that, for any input , there exists a random string  such that , using  as its random string, has a competitive ratio of at least .

Let  be a matrix with  rows and  columns, where  is the competitive ratio of  on input  given random string . For any input, the expected competitive ratio of  is at least . So, for any , ,

This implies that there exists a random bit string  at some index  such that

The random bit string  will be included in the set . Let  be the set of input indexes such that, for input , .  Hence,


Combining \eqref{eq:matUpper} and \eqref{eq:matLower}, we have that 
That is, there are at least  inputs, where , using the random string , has a competitive ratio of at least . Each input sequence corresponding to an index in  is said to be \emph{covered} by the bit string .

Let  be the matrix  with the column corresponding to the string  removed and all the rows at the indexes in  removed. Note that there are  rows in , for any row  in ,  as, by definition, the contribution of the random string  to a remaining row in  was less than . Using , this process can be repeated. After  iterations of this process, all the input sequence are covered by a bit string in .

A deterministic algorithm with advice  is defined as follows. Prior to serving a request,  reads  advice bits containing the length of the sequence, , encoded as a self-delimited encoding (cf.~\cite{BockKKR14}). Then,  computes the set  and reads  bits of advice containing an index in  to the string  that covers the input sequence. Finally,  simulates  on the input sequence with  as the random bit string.
\end{proof}


\end{document}