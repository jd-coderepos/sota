\documentclass{llncs}




\usepackage{figlatex}\graphicspath{{fig/}}

\usepackage{subfigure}
\usepackage[english]{babel}
\usepackage{color,amssymb, url,amsfonts, amsmath}	
\usepackage{tikz,xcolor,pgf,verbatim,xspace}\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage{paralist}
\usepackage[utf8]{inputenc}
\newcommand{\qrm}{\texttt{qr\_mumps}\xspace}
\newcommand{\mumps}{\texttt{MUMPS}\xspace}
\newcommand{\superlu}{\texttt{SuperLU}\xspace}
\newcommand{\pastix}{\texttt{PaStiX}\xspace}

\usetikzlibrary{calc, fit, shapes,arrows}
\usetikzlibrary{decorations,decorations.pathreplacing,patterns,shapes.multipart}
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}






\newcommand{\inte}[4][]{\int_{#2}^{#1} \! #3 \ \mathrm #4}
\def\mystrut(#1){\vrule height #1 depth 0pt width 0pt}   


	\newcommand{\newparskip}{\bigskip}
	\renewcommand{\labelitemi}{$\bullet$}
\newtheorem{defi}[theorem]{Definition}
	 \newtheorem{prop}[theorem]{Property}
	

\newcommand{\LG}[1]{\mathcal{L}_{#1}}
\newcommand{\para}[2]{#1 \mathop{\parallel} #2}
\newcommand{\seri}[2]{#1 \mathop{;} #2}
\newcommand{\s}{\mathcal S\xspace}
\newcommand{\spm}{\ensuremath{\mathcal{S}_{\mathrm{PM}}}\xspace}
\newcommand{\mpm}{\ensuremath{M^{\mathrm{PM}}}\xspace}
\newcommand{\slb}{\ensuremath{\mathcal{S}_{\mathrm{LB}}}\xspace}
\newcommand{\sopt}{\ensuremath{\mathcal{S}_{\mathrm{OPT}}}\xspace}
\newcommand{\uopt}{\ensuremath{u_{\mathrm{OPT}}}\xspace}
\newcommand{\Aopt}{\ensuremath{A_{\mathrm{OPT}}}\xspace}
\newcommand{\mopt}{\ensuremath{{M}_{\mathrm{OPT}}}\xspace}
\newcommand{\opt}{\ensuremath{\mathit{OPT}}\xspace}
\newcommand{\p}{\mathcal P}
\newcommand{\A}{\ensuremath{\mathcal A}\xspace}
\newcommand{\B}{\ensuremath{\mathcal B}\xspace}
\newcommand{\C}{\ensuremath{\mathcal C}\xspace}
\newcommand{\I}{\ensuremath{\mathcal I}\xspace}
\newcommand{\J}{\ensuremath{\mathcal J}\xspace}
\newcommand{\T}{\Delta}
\newcommand{\D}[1]{\Delta_{\mathcal{#1}}}
\newcommand{\tl}{\tau}
\newcommand{\R}{\ensuremath{\mathcal{R}}\xspace}
\newcommand{\rat}{r}
\newcommand{\fctofproc}{{processor profile}\xspace}
\newcommand{\frtrd}{\ensuremath{\left(\frac{4}{3}\right)^\alpha}}
\newcommand{\mideal}{\ensuremath{{M}_{\mathrm{ideal}}}\xspace}
\newcommand{\saideal}{\ensuremath{{\Sigma}_{\mathrm{ideal}}}\xspace}
\newcommand{\sa}{\ensuremath{\sum_{i\in A} x_i}}
\newcommand{\ao}{\ensuremath{A_{\mathrm{O}}}}
\newcommand{\sab}{\ensuremath{\sum_{i\in \bar A} x_i}}
\newcommand{\sao}{\ensuremath{\sum_{i\in \ao} x_i}}
\newcommand{\saob}{\ensuremath{\sum_{i\in \bar \ao} x_i}}
\newcommand{\saalp}{\ensuremath{\left(\sum_{i\in A} x_i\right)^\alpha}}
\newcommand{\sabalp}{\ensuremath{\left(\sum_{i\in \bar A} X_i\right)^\alpha}}
\newcommand{\saopt}{\ensuremath{\mathcal{\Sigma}_{\mathrm{OPT}}}\xspace}
\newcommand{\puisa}[1]{\left(#1\right)^\alpha}
\newcommand{\eg}{\varepsilon_\gamma}
\newcommand{\ek}{\varepsilon_{\kappa}}
\newcommand{\el}{\varepsilon_\lambda}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\midset}{\mathrel{}\middle|\mathrel{}}
\newcommand{\ssnir}{ {\sc Subset Sum Non-Integer Restricted}\xspace}
\newcommand{\ssni}{ {\sc Subset Sum Non-Integer}\xspace}
\newcommand{\pqres}{ {\sc $(p,q)$-Scheduling Restricted}\xspace}
\newcommand{\pqsched}{ {\sc $(p,q)$-Scheduling}\xspace}

\newcommand{\divis}{{\sc Divisible}\xspace}
\newcommand{\propo}{{\sc Proportional}\xspace}


\newif\iflong
\longfalse


\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}
\title{Scheduling Trees of Malleable Tasks\\ for Sparse Linear
  Algebra} 



\titlerunning{Scheduling Trees of Malleable Tasks}

\author{Abdou Guermouche\inst{1} \and Loris Marchal\inst{2} \and Bertrand Simon\inst{2} \and Fr\'ed\'eric Vivien\inst{2}}
\institute{University of Bordeaux and INRIA,\\
     200 rue de la vieille Tour,  Talence, France\\
     \url{abdou.guermouche@labri.fr}
\and
CNRS, INRIA and University of Lyon,\\
     LIP, ENS Lyon, 46 allée d'Italie, Lyon, France\\
     \url{{loris.marchal,bertrand.simon,frederic.vivien}@ens-lyon.fr}}

\maketitle


\begin{abstract}
  Scientific\blfootnote{This work was supported by the ANR SOLHAR project
  funded by the French National Research Agency.} workloads are often described by directed acyclic task
  graphs. This is in particular the case for multifrontal
  factorization of sparse matrices ---the focus of this paper--- whose
  task graph is structured as a tree of parallel tasks. Prasanna and
  Musicus~\cite{prasmus,prasmus2} advocated using the concept of
  \emph{malleable} tasks to model parallel tasks involved in matrix
  computations.  In this powerful model each task is processed on a
  time-varying number of processors.  Following Prasanna and Musicus,
  we consider malleable tasks whose speedup is $p^\alpha$, where $p$
  is the fractional share of processors on which a task executes, and
  $\alpha$ ($0 < \alpha \leq 1$) is a task-independent parameter.
  Firstly, we use actual experiments on multicore platforms to
  motivate the relevance of this model for our application. Then, we
  study the optimal time-minimizing allocation proposed by Prasanna
  and Musicus using optimal control theory. We greatly simplify their
  proofs by resorting only to pure scheduling arguments. Building on
  the insight gained thanks to these new proofs, we extend the study
  to distributed (homogeneous or heterogeneous) multicore
  platforms. We prove the NP-completeness of the corresponding
  scheduling problem, and we then propose some approximation
  algorithms.
\end{abstract}







\section{Introduction}
\label{sec:intro}


Parallel workloads are often modeled as directed acyclic task graphs,
or DAGs, where nodes represent tasks and edges represent dependencies
between tasks. Task graphs arise from many scientific domains, such as
image processing, genomics, and geophysical simulations. In this
paper, we focus on task graphs coming from sparse linear algebra, and
especially from the factorization of sparse matrices using the
multifrontal method. Liu~\cite{liu:90} explains that the computational
dependencies and requirements in Cholesky and LU factorization of
sparse matrices using the multifrontal method can be modeled as a task
tree, called the \emph{assembly tree}. We therefore focus on
dependencies that can be modeled as a tree.


In the abundant existing literature, several variants of the task
graph scheduling problem are addressed, depending on the ability to
process a task in parallel: tasks are either \emph{sequential} (not
amenable to parallel processing), \emph{rigid} (requesting a given
number of processors), \emph{moldable} (able to cope with any fixed
number of processors) or even \emph{malleable} (processed on a variable
number of processors) in the terminology of~Drozdowski~\cite[chapter
25]{handbook}. When considering moldable and malleable tasks, one has
to define how the processing time of a task depends on the number of
allocated  processors.  Under some general
assumptions, Jansen and Zhang~\cite{jansen05} derive a
3.29~approximation algorithm for arbitrary precedence constraints,
which is improved in a 2.62~approximation in the particular case of a
series-parallel precedence graph by Lepere et
al.~\cite{lepere02}. However, although polynomial, these algorithms
relies on complex optimization techniques, which makes them difficult
to implement in a practical setting.

In this study, we consider a special case of malleable tasks, where
the speedup function of each task is $p^\alpha$, where $p$ is the
number of processors allocated to the task, and $0<\alpha\leq 1$ is a
global parameter.  In particular, when the share of processors $p_i$
allocated to a task $T_i$ is constant, its processing time is given by
$L_i/ p_i^\alpha$, where $L_i$ is the sequential duration of
$T_i$. The case $\alpha=1$ represents the unrealistic case of a
perfect linear speed-up, and we rather concentrate on the case
$\alpha<1$ which takes into consideration the cost of the
parallelization. In particular $\alpha<1$ accounts for the cost of
intra-task communications, without having to decompose the tasks in
smaller granularity sub-tasks with explicit communications, which
would make the scheduling problem intractable. This model has been
advocated by Prasanna and Musicus~\cite{prasmus2} for matrix
operations, and we present some new motivation for this model in our
context. As in~\cite{prasmus2}, we also assume that it is possible to
allocate non-integer shares of processors to tasks. This amounts to
assume that processors can share their processing time among
tasks. When task $A$ is allocated 2.6 processors and task $B$ 3.4
processors, one processor dedicates 60\% of its time to $A$ and 40\%
to $B$. Note that this is a realistic assumption, for example, when
using modern task-based runtime systems such as StarPU~\cite{starpu},
KAAPI~\cite{kaapi}, or PaRSEC~\cite{parsec}. This allows to simplify
the scheduling problem and to derive optimal allocation algorithms.

Our objective is to minimize the total processing time of a tree of
malleable tasks. Initially, we consider a homogeneous platform
composed of $p$ identical processors. To achieve our goal, we take
advantage of two sources of parallelism: the \emph{tree parallelism}
which allows tasks independent from each others (such as siblings) to
be processed concurrently, and the \emph{task parallelism} which
allows a task to be processed on several processors. A solution to
this problem describes both in which order tasks are processed and
which share of computing resources is allocated to each task.

In~\cite{prasmus,prasmus2}, the same problem has been addressed by
Prasanna and Musicus for series-parallel graphs (or SP-graphs). Such
graphs are built recursively as series or parallel composition of two
smaller SP-graphs. Trees can be seen as a special-case of
series-parallel graphs, and thus, the optimal algorithm proposed
in~\cite{prasmus,prasmus2} is also valid on trees. They use optimal
control theory to derive general theorems for any strictly increasing
speedup function. For the particular case of the speedup function
$p^\alpha$, Prasanna and Musicus prove some properties of the unique
optimal schedule which allow to compute it efficiently. Their results
are powerful (a simple optimal solution is proposed), but to obtain
these results they had to transform the problem in a shape which is
amenable to optimal control theory. Thus, their proofs do not provide
any intuition on the underlying scheduling problem, yet it seems
tractable using classic scheduling arguments.

In this paper, our contributions are the following:
\begin{compactitem}
\item In Sect.~\ref{sec:motiv}, we show that the model of malleable
  tasks using the $p^\alpha$ speed-up function is justified in the
  context of sparse matrix factorization.
\item In Sect.~\ref{sec:shared}, we propose a new and simpler proof
  for the results of ~\cite{prasmus,prasmus2} on series-parallel
  graphs, using pure scheduling arguments. 
\item In Sect.~\ref{sec:dist}, we extend the previous study on
  distributed memory machines, where tasks cannot be distributed
  across several distributed nodes. We provide NP-completeness
  results and approximation algorithms.
\end{compactitem}



\section{Validation of the Malleable Task Model}
\label{sec:motiv}

In this section, we evaluate the model proposed by
Prasanna and Musicus in~\cite{prasmus,prasmus2} for our target
application. This model states that the instantaneous speedup of a task processed on
$p$ processors is $p^\alpha$. Thus, the processing time of a task
$T_i$ of size $L_i$ which is allocated a share of processors $p_i(t)$
at time $t$ is equal to the smallest value $C_i$ such that
$
{ \int_{0}^{C_i} \left(p_i(t)\right)^{\alpha} dt} \ \geq\  L_i,
$
where $\alpha$ is a task-independent constant. When the share
of processors $p_i$ is constant, $C_i = L_i/p_i^\alpha$. Our goal is
(i) to find whether this formula well describes the evolution of the
task processing time for various shares of processors and (ii) to
check that different tasks of the same application have the same
$\alpha$ parameter.  We target a modern multicore platform composed of
a set of nodes each including several multicore processors. For the
purpose of this study we restrict ourselves to the single node case
for which the communication cost will be less dominant. In this
context, $p_i(t)$ denotes the number of \emph{cores} dedicated to task
$T_i$ at time $t$.

We consider applications having a tree-shaped task
graph constituted of parallel tasks. This kind of
execution model can be met in sparse direct solvers where the matrix
is first factorized before the actual solution is computed. For
instance, either the multifrontal method~\cite{dure:83} as implemented
in \mumps~\cite{mumps11} or \qrm~\cite{buttari10}, or the supernodal
approach as implemented in \superlu~\cite{superlu} or
in \pastix~\cite{pastix}, are based on tree-shaped task graphs (namely the
assembly tree~\cite{aglp:87}).  Each task in this tree is a partial
factorization of a dense sub-matrix or of a sparse panel. In order to
reach good performance, these factorizations are performed using tiled
linear algebra routines (BLAS): the sub-matrix is decomposed into 2D
tiles (or blocks), and optimized BLAS kernels are used to perform the
necessary operations on each tile. Thus, each task can be seen as a
task graph of smaller granularity sub-tasks.







As computing platforms evolve quickly and become more complex (e.g.,
because of the increasing use of accelerators such as GPUs or Xeon
Phis), it becomes interesting to rely on an optimized dynamic runtime
system to allocate and schedule tasks on computing resources. These
runtime systems (such as StarPU~\cite{starpu}, KAAPI~\cite{kaapi}, or
PaRSEC~\cite{parsec}) are able to process a task on a prescribed
subset of the computing cores that may evolve over time. This
motivates the use of the malleable task model, where the share of
processors allocated to a task vary with time. This approach has been
recently used and evaluated~\cite{hugo14} in the context of the \qrm
solver using the StarPU runtime system.

In order to assess whether tasks used within sparse direct
solvers fit the model introduced by Prasanna and Musicus
in~\cite{prasmus2} we conducted an experimental study on several dense
linear algebra tasks. We used a test platform composed of 4 Intel
E7-4870 processors having 10 cores each clocked at 2.40~GHz and having
30~MB of L3 cache for a total of 40 cores. The platform is equipped
with 1~TB of memory with uniform access. We considered dense
operations which are representative of what can be met in sparse
linear algebra computations, namely the standard frontal matrix
factorization kernel used in the \qrm solver. We used either
block-columns of size 32 (1D partitioning) or square blocks of size
256 (2D partitioning). All experiments were made using the StarPU
runtime.






\begin{figure}[bt]
  \centering
\subfigure[\mystrut(1.2em) Timings and model (lines) with 1D partitioning]{\label{fig.qrm1D}\includegraphics[width=0.6\linewidth]{qrm1d_model_log_log.fig}}
\hspace{0.5cm}
\raisebox{65pt}{\subfigure[\mystrut(1.2em)Values of $\alpha$]{\label{fig.qrmalpha}\scalebox{0.9}{\begin{tabular}[c]{|c|c|c|}
      \hline
      matrix & ~~1D~~ & ~~2D~~ \\
      \hline
      5000x1000~~  & 0.78 & 0.93\\
      \hline
      10000x2500~~ & 0.88 & 0.95 \\
      \hline
      20000x5000~~ & 0.89 & 0.94 \\
      \hline
    \end{tabular}}
}
}
\caption{Timings and $\alpha$ values for \qrm frontal matrix factorization kernel}
  \label{fig.qrm}
\end{figure}

Figure~\ref{fig.qrm1D} presents the timings obtained when processing
the \qrm frontal matrix factorization kernel on a varying number of
processors. The logarithmic scales show that the $p^\alpha$ speedup
function models well the timings, except for small matrices when $p$
is large. In those cases, there is not enough parallelism in tasks to
exploit all available cores. We performed linear regressions on
the portions where $p\leq 10$ to compute $\alpha$ for
different task sizes (Fig.~\ref{fig.qrmalpha}). We performed
the same test for 2D partitioning and computed the corresponding
$\alpha$ values (using $ p\leq 20$). We notice that the value of
$\alpha$ does not vary significantly with the matrix size, which
validates our model. The only notable exception is for the smallest
matrix (5000x1000) with 1D partitioning: it is hard to efficiently use
many cores for such small matrices. In all cases, when the number of
processors is larger than a threshold the performance deteriorates and
stalls. Our speedup model is only valid below this threshold, 
which threshold increases with the matrix size. This is not a
problem as the allocation schemes developed in the next sections
allocate large numbers of processors to large tasks at the top of the
tree and smaller numbers of processors for smaller tasks. In other
words, we produce allocations that always respect the validity
thresholds of the model.
 Finally, note that the value of $\alpha$ depends on the
 parameters of the problem (type of factorization, partitioning, block
 size, etc.). It has to be determined for each kernel and each set of
 blocking parameters. 

\section{Model and Notations}
\label{sec:model}

We assume that the number of available computing resources may vary
with time: $p(t)$ gives the (possibly rational) total number of processors
available at time $t$, also called the \fctofproc. For the sake of
simplicity, we consider that $p(t)$ is a step function. Although our
study is motivated by an application running on a single multicore
node (as outlined in the previous section), we use the term
\emph{processor} instead of \emph{computing core} in the following
sections for readability and consistency with the scheduling
literature.

We consider an in-tree $G$ of $n$ malleable tasks $T_1, \ldots, T_n$.
$L_i$ denotes the length, that is the sequential processing time, of
task $T_i$.  As motivated in the previous section, we assume that the
speedup function for a task allocated $p$ processors is $p^\alpha$,
where $0 < \alpha \leq 1$ is a fixed parameter.
A schedule $\s$ is a set of nonnegative piecewise continuous functions
$\big\{ p_i(t)\ \big|\ i\in I\big\}$ representing the time-varying
share of processors allocated to each task. During a time interval
$\T$, the task $T_i$ performs an amount of work equal to $ \inte{\T\
}{p_i(t)^\alpha}{dt}$. Then, $T_i$ is completed when the total work
performed is equal to its length $L_i$. The completion time of task
$T_i$ is thus the smallest value $C_i$ such that $
\int_0^{C_i}{p_i(t)^\alpha}{dt} \geq L_i$.
We define $w_i(t)$ as the ratio of the
work of the task $T_i$ that is done during the time interval $[0,t]$:
$w_i(t) = \inte[t]{0}{p_i(x)^\alpha}{dx} \big/ L_i $.  A schedule is a
valid solution if and only if:
\begin{compactitem}
\item it does not use more processors than available: $\forall t,  \sum_{i\in I} p_i(t) \leq p(t)$;
\item it completes all the tasks: $\exists\tl,\ \forall i \in I, \ \ w_i(\tl)=1$;
\item and it respects precedence constraints:
  $\forall i\in I, \forall t$, if $p_i(t)>0$ then, $\forall j \in I$,
  if $j$ is a child of $i$, $w_j(t)=1$.
\end{compactitem}
The makespan $\tl$ of a schedule is computed as $\min \{t \ | \
\forall i\ w_i(t) = 1\}$. Our objective is to construct a valid
schedule with optimal, i.e., minimal, makespan.



Note that because of the speedup function $p^\alpha$, the computations
in the following sections will make a heavy use of the functions
$f:x\mapsto x^\alpha$ and $g:x\mapsto x^{(1/\alpha)}$. We assume that
we have at our disposal a polynomial time algorithm to compute both
$f$ and $g$. We are aware that this assumption is very likely to be
wrong, as soon as $\alpha<1$, since $f$ and $g$ produce irrational
numbers. However, without these functions, it is not even possible to
compute the makespan of a schedule in polynomial time and, hence, the
problem is not in NP. Furthermore, this allows us to avoid the
complexity due to number computations, and to concentrate on the most
interesting combinatorial complexity, when proving NP-completeness
results and providing approximation algorithms. In practice, any
implementation of $f$ and $g$ with a reasonably good accuracy will be
sufficient to perform all computations including the computation
of makespans.



In the next section, following Prasanna and Musicus, we will not
consider trees but more general graphs: \emph{series-parallel graphs}
(or SP graphs). An SP graph is recursively defined as a single task,
the series composition of two SP graphs, or the parallel composition
of two SP graphs. A tree can easily be transformed into an SP graph by joining the
leaves according to its structure, the resulting graph is then called a \emph{pseudo-tree}.  We will use
$(\para ij)$ to represent the parallel composition of tasks $T_i$ and
$T_j$ and $(\seri ij)$ to represent their series composition. 
Thanks to the construction of pseudo-trees, an algorithm which solves
the previous scheduling problem on SP-graphs also gives an optimal
solution for trees.





\section{Optimal Solution for Shared-Memory Platforms}

\label{sec:shared}





The purpose of this section is to give a simpler proof of the results
of~\cite{prasmus,prasmus2} using only scheduling arguments. We
consider an SP-graph to be scheduled on a shared-memory platform
(each task can be distributed across the whole platform). We assume
that $\alpha<1$ and prove the uniqueness of 
 the optimal schedule.

Our objective is to prove that any SP graph $G$ is \emph{equivalent}
to a single task $T_G$ of easily computable length: for any \fctofproc
$p(t)$, graphs $G$ and $T_G$ have the same makespan. We prove that the
ratio of processors allocated to any task $T_i$, defined by
$\rat_i(t)= p_i(t)/p(t)$, is constant from the
moment at which $T_i$ is initiated to the moment at which it is
terminated. We also prove that in an optimal schedule, the two
subgraphs of a parallel composition terminate at the same
time and each receives a constant total ratio of processors throughout
its execution.
We then prove that these properties imply that the optimal schedule is unique and obeys
to a {\it flow conservation} property: the shares of processors
allocated to two subgraphs of a series composition are equal. When
considering a tree, this means that the whole schedule is defined by
the ratios of processors allocated to the leaves. Then, all the children of a node $T_i$ terminate at the
same time, and its ratio is the sum of its children
ratios.


We first need to define the length $\LG{G}$ associated to a graph $G$, which
will be proved to be the length of the task $T_G$. Then, we state a few lemmas
before proving the main theorem. We only present here sketches of the proofs,
the detailed versions can be found in \cite{RR-ipdps-2014}.

\begin{defi}
\label{def.eq-task}
We recursively define the length $\LG{G}$ associated to a SP graph $G$:
\begin{inparaitem}
	\item $\LG{T_i} = L_i$ \hfill 
	\item $\LG{\seri{G_1}{G_2}} = \LG{G_1} + \LG{G_2}$ \hfill
	\item $\LG{\para{G_1}{G_2}} = \left(\LG{G_1}^{1/\alpha} + \LG{G_2}^{1/\alpha}\right)^\alpha$
\end{inparaitem}
\end{defi}

\newcommand{\Q}{\mathcal{Q}}



\begin{lemma}
  \label{lem:allproc}
  An allocation minimizing the makespan uses all the processors at any time. 
\end{lemma}



We call a \emph{clean interval} with regard to a schedule $\s$ an
interval during which no task is completed in $\s$.

\begin{lemma}
  \label{lem:esc}
  When the number of available processors is constant, any optimal schedule
  allocates a constant number of processors per task on any clean
  interval.
\end{lemma}




\begin{proof}
  By contradiction, we assume that there exists an optimal schedule
  $\mathcal{P}$ of makespan $M$, a task $T_j$ and a clean interval
  $\T=[t_1,t_2]$ such that $T_j$ is not allocated a constant number of
  processors on $\T$. By definition of clean intervals, no task
  completes during $\T$. $|\T|=t_2-t_1$ denotes the duration of $\T$,
  $I$ the set of tasks that receive a non-empty share of processors
  during $\T$, and $p$ the constant number of available processors.
	




  We want to show that there exists a valid schedule with a makespan
  smaller than $M$. To achieve this, we define an intermediate and
  not necessarily valid schedule $\Q$, which
  nevertheless respects the resource constraints (no more than
  $p$ processors are used at time $t$). This schedule is
  equal to $\mathcal{P}$ except on $\T$.
The constant share of processors allocated to task $T_i$ on $\T$ in
  $\mathcal{Q}$ is defined by $q_i = \frac{1}{|\T|}\int_\T
  p_i(t)dt$.  For all $t$, we have $\sum_{i\in I} p_i(t) = p$ because
  of Lemma~\ref{lem:allproc}. We get $\sum_{i\in I} q_i = p$. So
  $\mathcal{Q}$ respects the resource constraints.
  Let $W_i^\T(\mathcal{P})$ (resp. $W_i^\T(\mathcal{Q})$) denote the
  work done on $T_i$ during $\T$ under schedule $\mathcal{P}$
  (resp. $\mathcal{Q}$).
    We have 
  \begin{align*}	
    W_i^\T(\mathcal{P}) &= \int_\T p_i(t)^\alpha dt = |\T|
    \int_{[0,1]}  p_i(t_1+ t |\T|)^\alpha dt\\
W_i^\T(\mathcal{Q}) &= \int_\T  \left(\frac{1}{|\T|}\int_\T p_i(t) dt\right) ^\alpha dx
    =  |\T| \left(\int_{[0,1]} p_i(t_1+t |\T|) dt\right) ^\alpha
  \end{align*}
  As $\alpha<1$, the function $x\mapsto x^\alpha$ is concave and then,
  by Jensen inequality \cite{Hardy}, $W_i^\T(\mathcal{P}) \leq
  W_i^\T(\mathcal{Q})$.  Moreover, as $x\mapsto x^\alpha$ is
  \emph{strictly} concave, this inequality is an equality if and only
  if the function $t\mapsto p_i(t_1+t|\T|)$ is equal to a constant on
  $[0,1[$ except on a subset of $[0,1[$ of null measure \cite{Hardy}.
  Then, by definition, $p_j$ is not constant on $\Delta$, and
  cannot be made constant by modifications on a set of null measure.
  We thus have $W_j^\T(\mathcal{P}) < W_j^\T(\mathcal{Q})$.
Therefore, $T_j$ is allocated too many processors under
  $\mathcal{Q}$.  It is then possible to distribute this surplus among
  the other tasks during $\T$, so that the work done during $\T$ in
  $\mathcal{P}$ can be terminated earlier. This remark implies that
  there exists a valid schedule with a makespan smaller than $M$;
  hence, the contradiction.\qed
\end{proof}


We recall that  $\rat_i(t)= p_i(t)/p(t)$ is the instantaneous ratio of
processors allocated to a task $T_i$ .

\begin{lemma}
\label{lem:rate}
Let $G$ be the parallel composition of two tasks, $T_1$ and $T_2$. If
$p(t)$ is a step function, in any optimal schedule $r_1(t)$ is
constant and equal to $\pi_1 =
{1}\left/\left({1+\left({L_2}/{L_1}\right)^
      {1/\alpha}}\right)\right. = L_1^{1/\alpha} \left/ \LG{\para
    12}^{1/\alpha} \right.$ up to the completion of $G$.
\end{lemma}


\begin{proof}
First, we prove that $r_1(t)$ is constant on any optimal
schedule.

We consider an optimal schedule $\s$, and two consecutive time intervals $A$ and
$B$ such that $p(t)$ is constant and equal to $p$ on $A$ and $q$ on $B$, and
$\s$ does not complete before the end of $B$. Suppose also that $|A|p^\alpha =
|B|q^\alpha$ (shorten one interval otherwise), where $|A|$ and
$|B|$ are the durations of intervals $A$ and $B$. By Lemma \ref{lem:esc}, 
$r_1(t)$ has constant values $r_1^A$ on $A$ and $r_1^B$ on
$B$. Suppose by contradiction that $r_1^A \neq r_1^B$.

We want to prove that $\s$ is not optimal, and so that we can do the
same work as $\s$ does on $A\cup B$ in a smaller makespan. We
set $r_1 = \left.\left({r_1^A+r_1^B}\right) \right/{2}$.  We define
the schedule $\s'$ as equal to $\s$ except on $A \cup B$ where the ratio
allocated to $T_1$ is $r_1$ (see Fig. \ref{fig:rarb}).
\begin{figure}[tb]
\centering
\ifdefined\s
\else
\newcommand{\s}{S}
\fi

\newcommand{\ya}{30pt}
\newcommand{\yb}{50pt}

\newcommand{\yf}{.5*\ya+.5*\yb}

\newcommand{\topy}{70pt}
\newcommand{\topx}{100pt}
\newcommand{\midx}{60pt}

\newcommand{\midlx}{50pt}

\newcommand{\scalerat}{0.8}
\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]


\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};

\node (mra) at (\midx,\ya) {};
\node (mla) at (0,\ya) {};

\node (mlb) at (\midx,\yb) {};
\node (mrb) at (\topx,\yb) {};

\node (mlf) at (0,\yf) {};
\node (mrf) at (\topx,\yf) {};

\node (title) at (\topx/2, \topy+15pt) {$\s$};


\draw (bl)--(tl)--(tr)--(br)--(bl);
\draw (mla)--(mra)
      (mlb)--(mrb)
      (bm)--(tm);

\path (tl)--+(10pt,-10pt) node[label] {$T_2$};
\path (0,\ya)--+(10pt,-10pt) node[label] {$T_1$};
\path (\midx,\yb)--+(10pt,-10pt) node[label] {$T_1$};
\path (tm)--+(10pt,-10pt) node[label] {$T_2$};
\path (mla)--+(-10pt,-0) node[label] {$r_1^A$};
\path (mrb)--+(+10pt,-0) node[label] {$r_1^B$};
\draw[dotted] (tl)--+(-15pt,-0) node[left] {$1$};
\draw[dotted] (bl)--+(-15pt,-0) node[left] {$0$};

\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\midx,0)--(0,0) node [midway,yshift=-0.4cm] 
{ $A$};
\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\topx,0)--(\midx,0) node [midway,yshift=-0.4cm] 
{$B$};

\end{tikzpicture}
\raisebox{40pt}{\huge$\Rightarrow$}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]


\draw (bl)--(tl)--(tr)--(br)--(bl);
\draw (mlf)--(mrf)
      (bm)--(tm);

\path (tl)--+(10pt,-10pt) node[label] {$T_2$};
\path (0,\yf)--+(10pt,-10pt) node[label] {$T_1$};
\path (\midx,\yf)--+(10pt,-10pt) node[label] {$T_1$};
\path (tm)--+(10pt,-10pt) node[label] {$T_2$};
\path (mlf)--+(-10pt,-0) node[label] {$r_1$};
\draw[dotted] (tl)--+(-15pt,-0) node[left,label] {$1$};
\draw[dotted] (bl)--+(-15pt,-0) node[left,label] {$0$};

\path (title) node[label] {$\s'$};

\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\midx,0)--(0,0) node [midway,yshift=-0.4cm] 
{ $A$};
\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\topx,0)--(\midx,0) node [midway,yshift=-0.4cm] 
{$B$};

\end{tikzpicture}}
 \caption{Schedules $\s$ and $\s'$ on $A\cup B$. The abscissae represent the time and the ordinates the ratio of processing power}
\label{fig:rarb}
\end{figure}
\\
The work $W_1$ on task $T_1$ under $\s$ and $W'_1$ under $\s'$ during $A\cup B$ are
equal to:
$$W_1 = |A|p^\alpha \left(r_1^A\right)^\alpha + |B|q^\alpha \left(r_1^B\right)^\alpha
\qquad W'_1 = r_1^\alpha \left(|A|p^\alpha + |B|q^\alpha\right)$$
Then, with the concavity inequality and the fact that $|B|q^\alpha =
|A|p^\alpha$, we can deduce that $W_1'>W_1$ and symmetrically that $W_2'>W_2$.

Therefore, $\s'$ performs strictly more work for each task during $A\cup B$ than
$\s$. Thus, as in Lemma~\ref{lem:esc}, $\s$ is not optimal. So
$r_1(t)$ is constant in optimal schedules.



There remains to prove that in an optimal schedule $\s$, $r_1(t) =
\pi_1$; hence, the optimal schedule is unique.  As $p(t)$ is a step
function, we define the sequences $\left(A_k\right)$ and
$\left(p_k\right)$ such that $A_k$ is the duration of the $k$-th
step of the function $p(t)$ and $p(t)=p_k>0$ on $A_k$. The sum of the
durations of the $A_k$'s is the makespan of $\s$.
Then, if we note $V = \sum_k |A_k| p_k^\alpha$ and $r_1$ the value of $r_1(t)$,
we have:
$$
\begin{array}{c}\displaystyle
L_1 = \sum_k |A_k| r_1^\alpha p_k^\alpha = r_1^\alpha V
\qquad\displaystyle\text{and}\qquad 
 L_2 = \sum_k |A_k| (1-r_1)^\alpha p_k^\alpha = (1-r_1)^\alpha V
\end{array}
$$
Then, $\ r_1 = {1}\left/\left({1+\left({L_2}/{L_1}\right)^
      {1/\alpha}}\right)\right.  = \pi_1$.\qed
\end{proof}



\begin{lemma}
  \label{lem:ratequiv}
  Let $G$ be the parallel composition of tasks $T_1$ and $T_2$, with
  $p(t)$ a step function, and $\s$ an optimal schedule. Then, the
  makespan of $G$ under $\s$ is equal to the makespan of the task
  $T_G$ of length $\LG G = \LG{\para 12}$.
\end{lemma}

\begin{proof}  
  We characterize $p(t)$ by the sequences $(A_k)$ and $(p_k)$ as in the proof of
  Lemma~\ref{lem:rate}. We know by Lemma~\ref{lem:rate} that the share allocated
  to $T_1$ is constant and equal to $\pi_1p_k$ on each interval $A_k$.
Then, by summing the work done on each interval for both tasks, one can prove
  that they are completed simultaneously, and that this completion time is the
  same as that of task $T_G$ under the same processor profile.\qed
\end{proof}



\begin{theorem}
  \label{th:step}
  For every graph $G$, if $p(t)$ is a step function, $G$ has the same
  optimal makespan as its equivalent task $T_G$ of length $\LG
  G$ (computed as in Definition~\ref{def.eq-task}). Moreover, there is a unique optimal schedule, and it can be
  computed in polynomial time.
\end{theorem}

\begin{proof}
  In this proof, we only consider optimal schedules. Therefore, when
  the makespan of a graph is considered, this is implicitly its
  optimal makespan.
We first remark that in any optimal schedule, as $p(t)$ is a step
  function and because of Lemma \ref{lem:esc}, only step functions are
  used to allocate processors to tasks, and so Lemma
  \ref{lem:ratequiv} can be applied on any subgraph of $G$ without
  checking that the \fctofproc is also a step function for this
  subgraph.
We now prove the result by induction on the structure of $G$.
  \begin{itemize}
  \item $G$ is a single task. The result is immediate.

  \item $G$ is the series composition of $G_1$ and $G_2$. By
    induction, $G_1$ (resp. $G_2$) has the same makespan as task
    $T_{G_1}$ (resp. $T_{G_2}$) of length $\LG {G_1}$ (resp. $\LG {G_2}$) under any
    \fctofproc. Therefore, the makespan of $G$ is equal to $\LG G = \LG {\seri {G_1}{G_2}} =
    \LG {G_1} + \LG {G_2}$.
The unique optimal schedule of $G$ under $p(t)$ processors is the
    concatenation of the optimal schedules of $G_1$ and $G_2$.


  \item $G$ is the parallel composition of $G_1$ and $G_2$. By
    induction, $G_1$ (resp. $G_2$) has the same makespan as task
    $T_{G_1}$ (resp. $T_{G_2}$) of length $\LG {G_1}$ (resp. $\LG {G_2}$) under any
    \fctofproc.
Consider an optimal schedule $\s$ of $G$ and let $p_1(t)$ be the
    \fctofproc allocated to $G_1$. Let $\tilde \s$ be the schedule of
    $(\para{T_{G_1}}{T_{G_2}})$ that allocates $p_1(t)$ processors to
    $T_{G_1}$. $\tilde \s$ is optimal and achieves the same makespan as $\s$
    for $G$ because $T_{G_1}$ and $G_1$ (resp. $T_{G_2}$ and $G_2$) have the
    same makespan under any \fctofproc.  Then, by Lemma
    \ref{lem:ratequiv}, $\tilde \s$ (so $\s$) achieves the same makespan as
    the optimal makespan of the task $T_G$ of length $\LG {\para{G_1}{G_2}} = \LG G$.
Moreover, by Lemma \ref{lem:rate} applied on $(\para{T_{G_1}}{T_{G_2}})$,
    we have $p_1(t) = \pi_1 p(t)$. By induction, the unique optimal
    schedules of $G_1$ and $G_2$ under respectively $p_1(t)$ and
    $(p(t)-p_1(t))$ processors can be computed. Therefore, there is a
    unique optimal schedule of $G$ under $p(t)$ processor: the
    parallel composition of these two schedules.  
  \end{itemize}
  
  Therefore, there is a unique optimal schedule for $G$ under $p(t)$. Moreover,
  it can be computed in polynomial time. We describe here the algorithm to
  compute the optimal schedule of a tree $G$, but
  it can be extended to treat SP-graphs. The length of the equivalent
  task of each subtree of $G$ can be computed in polynomial time by a
  depth-first search of the tree (assuming that raising a number to the power
  $\alpha$ or $1/\alpha$ can be done in polynomial time). Hence, the ratios
  $\pi_1$ and $\pi_2$ for each parallel composition can also be computed in
  polynomial time. Finally, these ratios imply the computation in linear time of
  the ratios of the processor profile that should be allocated to each task
  after its children are completed, which describes the optimal schedule.\qed
\end{proof}





\section{Extensions to Distributed Memory}
\label{sec:dist}

The objective of this section is to extend the previous results to the
case where the computing platform is composed of several nodes with
their own private memory. In order to avoid the large communication overhead
of processing a task on cores distributed across several
nodes, we forbid such a multi-node execution: the tasks of the tree can
be distributed on the whole platform but each task has to be processed on a
single node. We prove that this additional constraint, denoted by
\R, renders the problem much more difficult. We concentrate first
on platforms with two homogeneous nodes and then with two heterogeneous
nodes.

\subsection{Two Homogeneous Multicore Nodes}
\label{sec:dist-hom}

In this section, we consider a multicore platform composed of two
equivalent nodes having the same number of computing cores $p$. We
also assume that all the tasks $T_i$ have the same speedup function
$p_i^\alpha$ on both nodes.
We first show that finding a schedule with minimum makespan is weakly
NP-complete, even for independent tasks:

\begin{theorem}
  Given two homogenous nodes of $p$ processors, $n$ independent tasks
  of sizes $L_1, ..., L_n$ and a bound $T$, the problem of finding a
  schedule of the $n$ tasks on the two nodes that respects \R, and
  whose makespan is not greater than $T$, is (weakly) NP-complete for
  all values of the $\alpha$ parameter defining the speedup function.
\end{theorem}

The proof relies on the Partition problem, which is known to be weakly
(i.e., binary) NP-complete~\cite{gareyjohnson}, and uses tasks of
length $L_i=a_i^\alpha$, where the $a_i$'s are the numbers from the
instance of the Partition problem. We recall that we assume that
functions $x\mapsto x^\alpha$ and $x\mapsto x^{1/\alpha}$ can be
computed in polynomial time.  
\iflong
\todo[inline]{faire une courte preuve de
  NP-complétude}
\else
Details can be found in the companion research report~\cite{RR-ipdps-2014}.
\fi

We also provide a constant ratio approximation algorithm. We recall
that a $\rho$-approximation provides on each instance a solution whose
objective $z$ is such that $z \leq \rho z*$, where $z*$ is the optimal
value of the objective on this instance.

\begin{theorem}
\label{th.43approx}
  There exists a polynomial time \frtrd-approximation algorithm for
  the makespan minimization problem when scheduling a tree of malleable tasks on
  two homogenous nodes.
\end{theorem}

\iflong\else Due to lack of space, we refer the interested reader to
the companion research report
for the complete description of the
algorithm and proof~\cite{RR-ipdps-2014}.  The proof of the
approximation ratio consists in comparing the proposed solution to the
optimal solution on a single node made of $2p$ processors, denoted
\spm. Such an optimal solution can be computed as proposed in the
previous section, and is a lower bound on the optimal makespan on 2
nodes with $p$ processors. The general picture of the proposed
algorithm is the following. First, the root of the tree is arbitrarily
allocated to
the $p$ processors of one of the two nodes. Then, the subtrees $S_i$'s
rooted at the root's children are considered. If none of these
subtrees is allocated more than $p$ processors in \spm, then we show
how to ``pack'' the subtrees on the two nodes and bound the slow-down
by \frtrd. On the contrary, if one of the $S_i$'s is allocated more
than $p$ processors in \spm, then we allocate $p$ processors to its
root, and recursively call the algorithm on  its children and on the
remaining subtrees.
\fi

\iflong
The proof of this theorem is done by induction on the structure of the
tree and relies on the following lemmas. The approximation algorithm
is summarized in Algorithm~\ref{alg:hybapp}.  Two initialization cases
are depicted. One of them is easily handled, and the second one is
handled by Lemma \ref{lem:x<1}. The heredity property is proved in the
theorem, and uses Lemmas
\ref{lem:muopt},\ref{lem:uopt},\ref{lem:slb}. Lemma
\ref{lem:chainroot} allows the restriction to a slightly simpler class
of graphs.


Let \sopt be a makespan-optimal schedule, and \mopt be its makespan.
We consider the optimal schedule $\spm$ of $G$ on $2p$ processors
without the constraint \R. $\spm$ is then a PM so a PFC schedule. Its
makespan is $\mpm_{2p} = \LG G \big/ (2p)^\alpha$, which is then a
lower bound of the optimal makespan with the restriction \R.

One can observe that a $2^\alpha$ approximation is immediate: a
solution is the PM schedule of $G$ under only $p$ processors, whose
makespan is $\mpm_p = \LG G \big / p^\alpha$. As the optimal makespan
is not smaller than $\mpm_{2p}$, $\mpm_p$ is indeed a
$2^\alpha$-approximation.




Let $\{c_i\}$, for $i\in [1,n_c]$, be the set of children of the root
of $G$, and let $C_i$ be the subtree of $G$ rooted at $c_i$ and
including its descendants. We can suppose than the indices are ordered
such that the $\LG{C_i}$'s are in decreasing order. We denote
$\sigma_c= \sum_{i=1}^{n_c} \LG{C_i}^{1/\alpha}$.

We denote $x=\frac{2\LG{C_1}^{1/\alpha}}{\sigma_c}$, which means that
$xp$ processors are dedicated to $C_1$ in \spm.

The following lemma, whose proof is immediate, allows to restrict the
following discussion on a slightly simpler class of graphs:

\begin{lemma}
\label{lem:chainroot}
We can suppose without loss of generality that the length of the root
of $G$ is $0$ and the root has at least two children. Otherwise, the
chain starting at the root can be aggregated in a single task of
length $0$ before finding the schedule on this modified graph
$\tilde{G}$. It is then immediate to adapt it to the original graph,
by allocating $p$ processors to each task of this chain.
\end{lemma}





\begin{lemma}
\label{lem:x<1}
If we have $\ x \leq 1$, then a $ \frtrd$-approximation is computable in polynomial time.
\end{lemma}


\begin{proof}

Let $p_i$ be the share allocated to $C_i$ in \spm. Each $p_i$ is constant because $\spm$ is PFC by definition. By hypothesis, we have $p_i\leq p$ for all $i$, as $\LG{C_1}$ is the largest $\LG{C_i}$ and its share is equal to $xp \leq p$.

 If $n_c=2$, both $p_1$ and $p_2$ are not larger than $p$ so equal $p$. Therefore, the schedule $\spm$ respects restriction $\R$, is then optimal and so is a \frtrd approximation. 
 
 Otherwise, we have $n_c\geq 3$ and we partition the indices $i$ in three sets $S_1$, $S_2$, $S_3$ such that the sum $\Sigma_k$ of $p_i$'s corresponding to each set $S_k$ is not greater than $p$:
$ \forall k \in \{1,2,3\},\ \Sigma_k = \sum_{i \in S_k} p_i \leq p$
 , which is always possible because no $p_i$ is greater than $p$ and the sum of all $p_i$'s is $2p$. Indeed, we just have to iteratively place the largest $p_i$ in the set that has the lowest $\Sigma_k$. If a $\Sigma_k$ exceeds $p$, it was at least equal to $p/2$ at the previous step, and both other $\Sigma_k$ also: the sum of all $p_i$'s then exceeds $2p$, which is impossible.
 
 Then, we place the set with the largest $\Sigma_k$, say $S_1$, on one half of the processing power, and aggregate the two smallest, $S_2\cup S_3$ in the other half. We now compute the PM schedule of $S_1$ with $p$ processors and $S_2\cup S_3$ with $p$ processors. The makespan is then $M = \max{\left(\LG{S_1},\LG{\para{S_2}{S_3}}\right)}\big/p^\alpha= \LG{\para{S_2}{S_3}}\big/p^\alpha$.
  Indeed, we have $\Sigma_1 \leq p \leq \Sigma_2+\Sigma_3$ and $\LG{S_1}\big/\Sigma_1^\alpha = \LG{\para{S_2}{S_3}}\big/(\Sigma_2+\Sigma_3)^\alpha$, as these quantities represent the makespan of each subpart of the tree in $\spm$, and all subtrees $C_i$ terminate simultaneously in \spm.  So $\LG{S_1}<\LG{\para{S_2}{S_3}}$.
  
  
 We know that $\Sigma_1 \geq \max\left(\Sigma_2,\Sigma_3\right)$ and $\Sigma_1+\Sigma_2+\Sigma_3 =2p$, so $\Sigma_1\geq \frac 23p$, then $\Sigma_2+\Sigma_3\leq \frac 43 p$.
Therefore, in $\spm$, $\Sigma_2+\Sigma_3 \leq \frac 43p$ processors are allocated to $S_2\cup S_3$.
Then, the makespan of $\spm$ verifies $\mpm_{2p} \geq \LG{\para{S_2}{S_3}} \big/ \left( \frac 43 p \right)^\alpha$, and so $M/\mpm_{2p} \leq \frtrd$. Therefore, the schedule is indeed a \frtrd approximation.\qed
\end{proof} 




\begin{defi}
\label{def:Rq}
For any $0<q\leq p$, let $\R_q$ be the constraint that forces $q$ processors to be allocated to $c_1$.
\end{defi}


\begin{figure}[Ht]
\centering
\newcommand{\midy}{50pt}
\newcommand{\topy}{80pt}
\newcommand{\midx}{70pt}
\newcommand{\midlx}{40pt}
\newcommand{\topx}{100pt}

\newcommand{\bota}{55pt}
\newcommand{\leftb}{80pt}
\newcommand{\scalerat}{0.8}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]

\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (0,\bota)-- ++(\leftb,0)
      (\leftb,0) -- ++(0,\bota);

\node (title) at (\topx/2, \topy+15pt) {$\s_u$};

\path (tl)--+(10pt,-10pt) node[label] {$c_1$};
\path (0,\bota)--+(20pt,-10pt) node[label] {$C_1\setminus c_1$};
\path (tm)--+(10pt,-10pt) node[label] {$B_u$};


\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.4cm] 
{\footnotesize $p$};
\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.4cm] 
{\footnotesize $p$};
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [midway,yshift=0.2cm]
{\footnotesize $u$};
\draw [gray,<->]
(0,15pt)--(\leftb,15pt) node [midway,yshift=0.3cm] 
{\footnotesize $v_u$};

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left,xshift=-0.3cm] 
{\footnotesize $\Delta_{u,1}$};
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize $\Delta_{u,2}$};

\draw[gray] (\leftb,\bota) -- (\topx,\bota);
\node at (\leftb+10pt,\bota-10pt)  {$\bar B_u$};

\end{tikzpicture}}
\hfill
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]
\renewcommand{\midlx}{70pt}
\renewcommand{\leftb}{\midlx}
\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midlx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (0,\bota)-- ++(\leftb,0)
      (\leftb,0) -- ++(0,\bota);


\node (title) at (\topx/2, \topy+15pt) {$\s_{\mathrm{PM}}$};

\path (tl)--+(10pt,-10pt) node[label] {$c_1$};
\path (0,\bota)--+(20pt,-10pt) node[label] {$C_1\setminus c_1$};
\path (tm)--+(15pt,-10pt) node[label] {$B_{xp}$};

\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.4cm] 
{\footnotesize $p$};
\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.4cm] 
{\footnotesize $p$};
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [near start,yshift=0.2cm]
{\footnotesize $xp$};

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left,xshift=-0.3cm] 
{\footnotesize $\Delta_{xp,1}$};
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize $\Delta_{xp,2}$};

\draw[gray] (\leftb,\bota) -- (\topx,\bota);
\node at (\leftb+15pt,\bota-10pt)  {$\bar B_{xp}$};

\end{tikzpicture}}
 \caption{A schedule $\s_u$, for $u<p$ on the left and the schedule $\spm=\s_{xp}$ on the right}
\label{fig:hybsu}
\end{figure}

\begin{defi}
\label{def:su}
We denote by $B$ the subgraph $G\setminus \mathit{root} \setminus C_1$.

We define the schedule $\s_u$ parametrized by $u\in]0,p]\cup\{xp\}$,
which respects $\R_u$ but not $\R$. It allocates a constant share
$u\leq p$ of processors to $c_1$ until it is terminated. Meanwhile,
$2p-u$ processors are allocated to schedule a part $B_u$ of $B$. $B_u$
may contain fractions of tasks. Before, the rest of the graph, which
is composed of $C_1\setminus c_1$ and of the potential remaining part
$\bar B_u$ of $B$, is scheduled on $2p$ processors by a PM schedule,
regardless of the $\R$ constraint. We denote by $v_u$ the share
allocated to $C_1\setminus c_1$ and by $M_u$ the makespan of the
schedule. See Fig.~\ref{fig:hybsu} for an illustration. 
Let $G_{u,1}$ be the graph $\para{c_1}{B_{u}}$ and $G_{u,2}$ be the graph $\para{\left(C_1\setminus c_1\right)}{\bar B_u}$.
We denote by $\Delta_{u,1}$  (resp. $\Delta_{u,2}$) the execution time of $G_{u,1}$ (resp. $G_{u,2}$) in $\s_u$. Then, $M_u=\Delta_{u,1}+\Delta_{u,2}$.

One can note that the PM schedule $\spm$ is equal to $\s_{xp}$, where $u=v_u=xp$.



\end{defi}
 
 
 
\begin{lemma}
\label{lem:muopt}
For any $u\in]0,p]$, under the constraint $\R_u$, the makespan-optimal schedule is $M_u$.
\end{lemma} 
 
\begin{proof}
Let $\s$ be the the makespan-optimal schedule that respects the constraint $\R_u$. We want to show that $\s= \s_u$.

First, suppose that $c_1$ terminates before $B$ in $\s$. This means that a time range is dedicated to schedule $B$ at the end of the schedule. We can modify slightly $\s$ by moving this time range to the beginning of the schedule. This is possible as there is no heredity constraint between $B$ and $C_1$, and the same tasks of $B$ can be performed in the new processor profile, by a PM schedule on $B$. So we now assume that the schedule terminates at the execution of $c_1$.

Because of $\R_u$, $\s$ must allocate $u$ processors to $c_1$ at the end of the schedule. In parallel to $c_1$, only $B$ can be executed, and before the execution of $c_1$, both subgraphs $B$ and $C_1\setminus c_1$ can be executed.

Suppose that $\s$ differs from $\s_u$ in the execution of $B$ in parallel to $c_1$. Consider the schedule of $C_1$ fixed, and let $p_b(t)$ be the number of processors allocated to $B$ at the time $t$ in $\s$. As $B_u$ is scheduled according to PM ratios in $\s_u$, and $\s$ differs from this schedule, in $\s$, $B$ is not scheduled according to PM ratios under the processor profile $p_b(t)$. Then, this schedule can be modified to schedule $B$ in a smaller makespan, and then to schedule the whole graph $G$ in a smaller makespan, which contradicts the makespan-optimality of $\s$.

So $\s$ and $\s_u$ are equal during the time interval $\Delta_{u,1}$. Then, it remains to schedule the graph $G_{u,2}=\para{\left(C_1\setminus c_1\right)}{\bar B_u}$, which has a unique optimal schedule, the PM schedule, that is followed by $\s_u$. Therefore, $\s=\s_u$.\qed
\end{proof} 
 

\begin{lemma}
\label{lem:uopt}
$\s_p$ is the makespan-optimal schedule among the $\s_w$ for $w\in]0,p]$, i.e., we have $p=\argmin_{w\in]0,p]}\left(M_w\right)$.

\end{lemma}



\begin{proof}
Let $\displaystyle \uopt=\argmin_{w\in]0,p]}\left(M_w\right)$. We will prove here that $\uopt=p$.

For the sake of simplification, we denote in this proof $\uopt$ by $u$, $v_{u}$ by $v$, $\Delta_{u,1}$ by $\Delta_{1}$ and $\Delta_{u,2}$ by $\Delta_{2}$. We will then consider the schedule $\s_u$, which is makespan-optimal among the $\s_w$, for $w\in]0,p]$.

Suppose by contradiction that $u<p$. We will build an other schedule $\bar{\s}$ following the constraint $\R_{\bar{u}}$ for a certain $\bar{u}$ respecting $u<\bar{u}<p$, that will give a contradiction with the optimality of $\s_u$. 

\newparskip

The following paragraphs prove the inequality $v>p$, which can be intuitively deduced by an observation of the schedules.

As we have $x> 1$, we know that $\LG{C_1\setminus c_1} > \LG{\bar B_{xp}} = \LG{B} - \LG{B_{xp}} > \LG{B}-c_1$. The first inequality holds because in $\s_{xp}$, the subgraphs $C_1\setminus c_1$ and $\bar B_{xp}$ are scheduled in parallel, and each subgraph is scheduled according to the PM ratios. Then, each subgraph {\color{red} can be replaced by its equivalent task}.   Moreover, $xp$ (resp. $(2-x)p$) processors are allocated to  $C_1\setminus c_1$ (resp. $\bar B_{xp}$). As $x>1$, more processors are allocated to $C_1\setminus c_1$, so $\LG{C_1\setminus c_1} > \LG{\bar B_{xp}}$. 
By the same reasoning between $c_1$ and $B_{xp}$ in $\s_{xp}$, we get $\LG{B_{xp}}<c_1$ and the second inequality holds. See Fig.~\ref{fig:hybsu} for an illustration.

With similar arguments between the subgraphs $c_1$ and $B_u$ in the schedule $\s_u$, and using the hypothesis $u<p$, we get $\LG{B_u} > c_1$. The difference with the previous case is that the share of processors allocated to both subgraphs is not computed by the PM ratios, but as $B_u$ is scheduled under $(2p-u)$ processors with the PM ratios, {\color{red} it can still be replaced by its equivalent task}.

Combining these two inequalities, we have $\LG{\bar B_u} < \LG{B}-c_1 < \LG{C_1\setminus c_1}$, and by using the same reasoning in the other way with the parallel execution of $C_1\setminus c_1$ and $\bar B_u$ in $\s_u$, we finally prove $v>p$.

\begin{figure}[!ht]
\centering
\newcommand{\midy}{60pt}
\newcommand{\topy}{90pt}
\newcommand{\midx}{60pt}
\newcommand{\midlx}{30pt}
\newcommand{\topx}{91pt}

\newcommand{\bota}{70pt}
\newcommand{\leftb}{75pt}
\newcommand{\scalerat}{0.8}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]

\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (0,\bota)-- ++(\leftb,0)
      (\leftb,0) -- ++(0,\bota);


\path (tl)--+(10pt,-10pt) node[label] {$c_1$};
\path (0,\bota)--+(20pt,-10pt) node[label] {$C_1\setminus c_1$};
\path (tm)--+(10pt,-10pt) node[label] {$B$};

\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.6cm] 
{\footnotesize $p$};
\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.6cm] 
{\footnotesize $p$};
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [midway,yshift=0.3cm]
{\footnotesize $u$};
\draw [gray,<->]
(0,15pt)--(\leftb,15pt) node [midway,yshift=0.3cm] 
{\footnotesize $v$};

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left,xshift=-0.3cm] 
{\footnotesize $\Delta_{1}$};
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize $\Delta_{2}$};

\phantom{
\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx,-10pt)--(\topx/2,-10pt) node [midway,yshift=-0.6cm] 
{\footnotesize $p$};
}
\end{tikzpicture}}
\hfill
\renewcommand{\midlx}{35pt}
\renewcommand{\leftb}{65pt}
\renewcommand{\bota}{70pt}
\renewcommand{\topy}{100pt}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]

\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (\midlx,\bota)-- (\leftb,\bota)
      (\leftb,0) -- ++(0,\bota);

\draw(0,\bota+10pt)-- ++(\midlx,0);


\path (tl)--+(10pt,-10pt) node[label] {$c_1$};
\path (0,\bota)--+(20pt,-10pt) node[label] {$C_1\setminus c_1$};
\path (tm)--+(10pt,-10pt) node[label] {$B$};

\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.6cm] 
{\footnotesize $p$};
\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.6cm] 
{\footnotesize $p$};
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [midway,yshift=0.3cm]
{\footnotesize $\bar u=u+\varepsilon\Delta_{2}$};
\draw [gray,<->]
(0,15pt)--(\leftb,15pt) node [midway,yshift=0.3cm,fill=white] 
{\footnotesize $\bar v=v-\varepsilon\Delta_{1}$};

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left, xshift=-0.3cm] 
{\footnotesize $\Delta_{1}$};
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize $\bar M - \Delta_{1}$};
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=2pt,yshift=0pt]
(\topx,\topy) -- (\topx,10pt) node [midway,right, xshift=0.3cm,align=left] 
{\footnotesize $\bar\Delta$ of\\ length\\ $M_u$};

\draw[pattern=north west lines, pattern color=gray] (\leftb,0) rectangle (\topx,18pt);

\end{tikzpicture}}
 \caption{Schedules $\s_u$ (left) and  $\bar\s$(right), assuming that $B$ begins after $C_1$ in $\bar\s$}
\label{fig:hybsb}
\end{figure}

\newparskip

Let $\varepsilon>0$ small enough such that $u+\varepsilon\Delta_{2}<p$ and $v-\varepsilon\Delta_{1}>p$. Let $\bar{u}=u+\varepsilon\Delta_{2}$ and $\bar{v}=v-\varepsilon\Delta_{1}$. One can note that $0<u<\bar{u}<p<\bar{v}<v$.

Let $\bar{\s}$ be the schedule allocating $\bar{u}$ processors to $C_1$ during a time $\Delta_{1}$ at the end of the schedule, and $\bar{v}$ processors to $C_1$ before. 
The subgraph $B$ is scheduled following PM ratios in parallel to $C_1$, in such a way that it terminates at the same time as $c_1$ and there is no idle time after the beginning of its execution. The subgraph $C_1$ is scheduled in the same way as $B$, following PM ratios as soon as its execution begins. 
 One can note that $B$ and $C_1$ do not necessarily begin simultaneously. See Fig. \ref{fig:hybsb} for an illustration of the case where $B$ begins after $C_1$. Let $\bar M$ be the makespan of $\bar{\s}$.




As $\bar u>u$, $c_1$ is completed in a time smaller than $\Delta_1$ in $\bar\s$, so $\bar \s$ respects the constraint $\R_{\bar u}$.
Then, by Lemma \ref{lem:muopt}, as $\bar\s \neq \s_{\bar u}$, we know that $\bar M> M_{\bar u}$. In addition, by the definition of $M_u$, we get $\bar M> M_{\bar u}\geq M_u$.

We can assume without loss of generality that $C_1$ and $B$ are both a unique task. This can be reached by replacing them by their equivalent task, which does not change their execution time. We do not lose generality by this transformation here because both subgraphs are scheduled under the PM rules. For example, we could not consider the equivalent task of the total graph $G$ because constraints of the form $\R_w$, with $w\neq xp$,  are followed, and so the PM rules are not respected.


Let $\bar\Delta$ be the interval of time ending when $\s_u$ terminates and having a length of $M_u$. See Fig. \ref{fig:hybsb} for an illustration.

Let $\bar W_C$ (resp. $\bar W_B$) be the total length of the task $C_1$ (resp. $B$) that is executed in $\bar{\s}$ during $\bar\Delta$. Similarly, we define $W_C$ (resp. $W_B$) the total length of the task $C_1$ (resp. $B$) that is executed in $\s_u$. These two last quantities are equal to:
\begin{align*}
 W_C &= \Delta_{1}u^\alpha+\Delta_{2}v^\alpha \left( = \LG{C_1}\right)\\
W_B &= \Delta_{1}(2p-u)^\alpha+\Delta_{2}(2p-v)^\alpha \left( = \LG{B}\right)
\end{align*}



As $\bar M> M_u$, we must not have simultaneously $\bar W_C\geq W_C$ and $\bar W_B\geq W_B$. Indeed, if it was the case, then all the tasks of $G$ would be completed by $\bar\s$ in a makespan smaller than $M_u$, which is a contradiction.
For each task, we separate two cases.

If $C_1$ begins in $\bar\s$ during $\bar\Delta$, then $\bar W_C=\LG{C}=W_C$ because the execution of $C_1$ would hold entirely in $\bar\Delta$.

Otherwise, we have:
$$\bar W_C = \Delta_{1}\bar{u}^\alpha+\Delta_{2}\bar{v}^\alpha$$

We know that $0<u<\bar{u}<\bar{v}<v$. 
Therefore, by the concavity of the function $t\mapsto t^\alpha$, we have the inequality below. The plot on the right illustrates the inequality. Indeed, the slope of the red segment corresponding to $u$ and $\bar u$ is larger than the other one.

\vspace{.25em}

\begin{minipage}{.36\columnwidth}
\begin{align*}
\frac{\bar{u}^\alpha-u^\alpha}{\bar u - u} &> \frac{v^\alpha-\bar{v}^\alpha}{ v -\bar v}
\end{align*}
\end{minipage}
\begin{minipage}{.58\columnwidth}
\tikzset{
xmin/.store in=\xmin, xmin/.default=-3, xmin=-3,
xmax/.store in=\xmax, xmax/.default=3, xmax=3,
ymin/.store in=\ymin, ymin/.default=-3, ymin=-3,
ymax/.store in=\ymax, ymax/.default=3, ymax=3,
}
\newcommand {\grille}
{\draw[help lines, dashed] (.001,.001) grid (1,1);}
\newcommand {\axes} {
\draw[->] (\xmin,0) -- (\xmax,0);
\draw[->] (0,\ymin) -- (0,\ymax);
}
\newcommand {\fenetre}
{\clip (\xmin,\ymin) rectangle (\xmax,\ymax);}
\newcommand{\fx}[1]{3*#1^3}
\newcommand{\ux}{.4}
\newcommand{\ubx}{.53}
\newcommand{\vbx}{.85}
\newcommand{\vx}{.95}
\begin{tikzpicture}[scale=1.3,xmin=0,xmax=3.3,ymin=0,ymax=1]
\axes \draw[blue] plot[smooth,domain=0:1] (\fx{\x},\x);
\draw (0,\ymax) node[left] {\textit{ \small $t^\alpha$}} (\xmax,0) node[below] {\textit{\small $t$}};
\node[red,below] at (\fx{\ux},0) {$u\strut$};
\node[red,below] at (\fx{\ubx},0) {$\bar u\strut$};
\node[red,below] at (\fx{\vbx},0) {$\bar v\strut$};
\node[red,below] at (\fx{\vx},0) {$v\strut$};

\draw[red,very thick,|-|] (\fx{\ux},\ux) edge (\fx{\ubx},\ubx)
 (\fx{\vx},\vx) -- (\fx{\vbx},\vbx);

\foreach \aa in {\ux,\ubx,\vx,\vbx}
{
\draw[red, dashed]  (\fx{\aa},0) -- +(0,\aa);
}
\end{tikzpicture}
 \end{minipage}
\vspace{.5em}

Then, we derive from this inequality the fact that $\bar W_C$ is larger than $W_C$:
\begin{align*}
\frac{\bar{u}^\alpha-u^\alpha}{\bar u - u} &> \frac{v^\alpha-\bar{v}^\alpha}{ v -\bar v}\\
\frac{\bar{u}^\alpha-u^\alpha}{\varepsilon\Delta_{2}} &> \frac{v^\alpha-\bar{v}^\alpha}{\varepsilon\Delta_{1}}\\
{\Delta_{1}}\left(\bar{u}^\alpha-u^\alpha\right) &> \Delta_{2}\left(v^\alpha-\bar{v}^\alpha\right)\\
\Delta_{1}\bar{u}^\alpha+\Delta_{2}\bar{v}^\alpha &> \Delta_{1}u^\alpha+\Delta_{2}v^\alpha\\
\bar W_C &> W_C
\end{align*}

Therefore, in any case, we have $\bar W_C \geq W_C$

\newparskip

Then, we treat similarly the subgraph $B$.
If $B$ begins in $\bar\s$ during $\bar\Delta$, then $\bar W_B=\LG{B}=W_B$.

Otherwise, we have:

$$\bar W_B = \Delta_{1}(2p-\bar{u})^\alpha+\Delta_{2}(2p-\bar{v})^\alpha$$

Similarly, we know that $2p-u>2p-\bar{u}>2p-\bar{v}>2p-v>0$. Therefore, by the concavity of the function $t\mapsto t^\alpha$, we have:
\begin{align*}
\frac{(2p-{u})^\alpha-(2p-\bar u)^\alpha}{\bar u - u} &< \frac{(2p-\bar v)^\alpha-(2p-{v})^\alpha}{v - \bar v}\\
\frac{(2p-{u})^\alpha-(2p-\bar u)^\alpha}{\varepsilon\Delta_{2}} &< \frac{(2p-\bar v)^\alpha-(2p-{v})^\alpha}{\varepsilon\Delta_{1}}\\
\frac{(2p-\bar{u})^\alpha-(2p-u)^\alpha}{\varepsilon\Delta_{2}} &> \frac{(2p-v)^\alpha-(2p-\bar{v})^\alpha}{\varepsilon\Delta_{1}}\\
{\Delta_{1}}\left((2p-\bar{u})^\alpha-(2p-u)^\alpha\right) &> \Delta_{2}\left((2p-v)^\alpha-(2p-\bar{v})^\alpha\right)\\
\Delta_{1}(2p-\bar{u})^\alpha+\Delta_{2}(2p-\bar{v})^\alpha &> \Delta_{1}(2p-u)^\alpha+\Delta_{2}(2p-v)^\alpha\\
\bar W_B &> W_B
\end{align*}


Then, in any case, we have both $\bar W_C\geq W_C$ and $\bar W_B\geq W_B$, so we get the contradiction.

Therefore, we have $u\geq p$ and so $u=p$.\qed
\end{proof}


\begin{lemma}
\label{lem:slb}

The makespan of $\s_p$ is a lower bound of $\mopt$.

\end{lemma}

\begin{proof}
One can note that in $\sopt$, a constant share $u_*\leq p$ processors must be allocated to $c_1$ due to \R, as in $\s_{u_*}$. Indeed, if this share is not constant, because of the concavity of the function $t\mapsto t^\alpha$, it would be better to always allocate the mean value to $c_1$. This would allow to terminate earlier both $c_1$ and the tasks executed in parallel to $c_1$ on the same part. {\color{red} See Lemma ???.}
 
Let $\R'$ be the constraint that enforces a schedule to respect one of the constraint $\R_w$, for any $w\in]0,p]$. Otherwise stated, $\R'$ enforces a schedule to allocate a constant share $w$ not larger than $p$ to $c_1$.

Let $\sopt'$ be the makespan-optimal schedule respecting $\R'$, and let $\mopt'$ be its makespan. As $\sopt$ respects $\R'$, we have $\mopt\geq \mopt'$.

Furthermore, there exists $u'\leq p$ such that $\sopt'=\s_{u'}$. Therefore, $\mopt'\geq \min_{w\in]0,p]}M_{w}$, and, by Lemma \ref{lem:uopt}, we get $\mopt'\geq M_{p}$.

Finally, we have $\mopt\geq M_p$.\qed
\end{proof}




We prove the following theorem by induction on $n$.  We are now able
to prove Theorem~\ref{th.43approx}, by induction on the tree
structure. The corresponding approximation algorithm is described in
Algorithm~\ref{alg:hybapp}.

 
\begin{proof}[of Theorem~\ref{th.43approx}]
First, we treat the initialization case, where $n=1$. An optimal schedule is the one that allocates $p$ processors to the unique task.

Then, we treat the cases that do not need the heredity property.
\begin{itemize}
\item if $x\geq 1$ and $c_1$ is a leaf, no schedule can have a makespan smaller than $x^\alpha \mpm_{2p}$, as no more than $p$ processors can be allocated to $c_1$. Then, the schedule that only differs from \spm by reducing the share allocated to $c_1$ to $p$ is makespan-optimal.
\item if $x\leq 1$, the result is given by Lemma \ref{lem:x<1}. 
\end{itemize}

Now, we suppose the result true for $m<n$. The case remaining is when $c_1$ is not a leaf and $x>1$. Consider such a graph $G$ of $n$ nodes.





We consider the schedule $\s_p$, whose makespan $M_p$ is a lower bound of \mopt as stated in Lemma \ref{lem:slb}.  




We now build the schedule $\s$, which achieves a $\frtrd$-approximation respecting $\R$.
At the end of the schedule, $G_{p,1}$ is scheduled as in $\s_p$. At the beginning of the schedule, we use the heredity property to derive from $\s_p$ a schedule of $G_{p,2}$ that follows the \R constraint.


More formally, we have $G_{p,2}$, which is the parallel composition $\para{(C_1\setminus c_1)}{\bar B_p}$, composed of at most $n-1$ nodes. So, by induction, a schedule $\s^r$ achieving a \frtrd-approximation can be computed for $G_{p,2}$. This means that its makespan $M^r$ is at most $\frtrd \Delta_{p,2}$, as $\s_p$ completes $G_{p,2}$ with PM ratios in a time $\Delta_{p,2}$, which is then the optimal time.

Consider the schedule $\s$ of $G$ that schedules $G_{p,2}$ as in $\s^r$, then schedules $G_{p,1}$ as in $\s_p$. The time necessary to complete $G_{p,1}$ is then equal to $\Delta_{p,1}$.
The makespan $M$ of $\s$ respects then:
 \begin{align*}
 M &= \Delta_{p,1} + M^r \leq \Delta_{p,1} + \frtrd \Delta_{p,2} \\ 
 &\leq \frtrd \left(\Delta_{p,1} + \Delta_{p,2}\right)\leq \frtrd M_{p} \leq \frtrd \mopt
 \end{align*}
 
  Then, $\s$ is a $\frtrd$-approximation.\qed
\end{proof}




\begin{figure}[Ht]
\begin{algorithmic}[1]
 \Require{A graph $G$, the parameter $p$ of the processor platform $\mathcal{P}$}
 \Ensure{A schedule $\s$ of $G$ on $\mathcal{P}$ that is a \frtrd-approximation of the makespan}
\Function{HybApp}{$G$,$p$}
\State $\tilde{G} \gets G$
\State Modify $G$ as in Lemma \ref{lem:chainroot} 
\State Compute the PM schedule $\spm$ of $G$ on $2p$ processors
\State Compute the $c_i$'s, the $C_i$'s, $B$, and $x$
 \If{$x\geq 1$ and $c_1$ is a leaf}
 \State	Build $\s$: shrink from $\spm$ the share of processors allocated to $c_1$ to $p$ processors
 	
 \ElsIf{$x\leq1$}
 \State	Build $\s$: map the $C_i$'s as in Lemma \ref{lem:x<1}, and compute the PM schedule on each part
 	
 \Else
 \Comment{we have $x>1$ and $c_1$ is not a leaf}
\State Compute the schedule $\s_p$ and partition $G$ in $G_{p,1}$ and $G_{p,2}$ as in Definition \ref{def:su}
\State $\s^r \gets\ \Call{HybApp}{G_{p,2},p}$
\State Build $\s$: schedule $G_{p,2}$ as in $\s^r$ then $G_{p,1}$ as in $\s_p$
 \EndIf
 \State Adapt the schedule $\s$ to the original graph $\tilde{G}$ if $G\neq\tilde G$
 \State \Return $\s$
 \EndFunction
\end{algorithmic}
 \caption{Approximation algorithm for the hybrid problem}
 \label{alg:hybapp}
\end{figure}




\paragraph{Worst case of the algorithm}

Let $G$ be the parallel composition of $3$ identical subtrees $G_1$, $G_2$, $G_3$ (plus the root of length 0). Each subtree $G_i$ is composed of $3$ tasks: a root $T_i$ of length $\epsilon$, and $2$ leaves $T_{i,1}$ and $T_{i,2}$ each of length $L$.

The algorithm follows the second case, and place $2$ of these trees on one part. The makespan obtained is then
$$M_1 = \frac{L}{\left(\frac p4\right)^\alpha} + \frac{\epsilon}{\left(\frac p2\right)^\alpha}$$

Now, consider the schedule that places two roots on one part, but $3$ tasks $T_{i,j}$ on each part at the beginning. Its makespan is then
$$M_2 = \frac{L}{\left(\frac p3\right)^\alpha} + \frac{\epsilon}{\left(\frac p2\right)^\alpha}$$

When $\epsilon$ is close to $0$, we get
$$\frac{M_1}{M_2} = \frtrd$$

So the approximation ratio is tight. 
\fi 

\subsection{Two Heterogeneous Multicore Nodes}
\label{sec:dist-het}

We suppose here that the computing platform is made of two processors
of different processing capabilities: the first one is made of $p$
cores, while the second one includes $q$ cores. We also assume that
the parameter $\alpha$ of the speedup function is the same on both
processors. As the problem gets more complicated, we concentrate here
on $n$ independent tasks,  of lengths $L_1, ..., L_n$. Thanks to the
homogenous case presented above, we already know that scheduling
independent tasks on two nodes is NP-complete.

\iflong \else This problem is close to the \textsc{Subset Sum} problem. Given $n$
numbers, the optimization version of \textsc{Subset Sum} considers a
target $K$ and aims at finding the subset with maximal sum smaller
than or equal to $K$.  There exists many approximation schemes for
this problem. In particular, Kellerer et
al.~\cite{kellerer2003efficient} propose a fully polynomial
approximation scheme (FPTAS). Based on this result, an approximation
scheme can be derived for our problem.
\begin{theorem}
  There exists an FPTAS for the problem of scheduling independent
  malleable tasks on two heterogeneous nodes, provided that, for each
  task, $L_i^{1/\alpha}$ is an integer.
\end{theorem}

The proof is complex and detailed in~\cite{RR-ipdps-2014}. The
assumption on the $L_i^{1/\alpha}$s is needed to apply the FPTAS of
\textsc{Subset Sum}, which is valid only on integers.
\fi 

\iflong \begin{proof}
We reduce the problem to {\sc Subset sum}, which is known to be NP-Complete \cite{gareyjohnson}.

Consider an instance $\mathcal{I}$ to {\sc Subset sum}. We have a set $X=\{x_i\}, i\in[1,n]$, a number $s$, and we want to know if there exists a subset of $X$ that sums to $s$, assuming that the total sum is larger.

We construct an instance $\mathcal{J}$ to $(p,q)$-scheduling. Let $T_i$ have lengths $L_i = x_i^{\alpha}$, $p=s$, $q=\sum x_i -p$, and $T=1$. We recall that raising a number to the power $\alpha$ is assumed feasible, so the computation of the $L_i$s is done in polynomial time. Note that $T$ is the optimal makespan without the $(p,q)$ constraint, and so only the PM schedule can be a solution.

It now remains to prove that $\mathcal{I}$ is satisfiable if and only if $\mathcal{J}$ is satisfiable.
Let $p_i$ be the share of processors allocated to task $T_i$ in the PM schedule. We have $$\displaystyle p_i = (p+q) \frac{L_i^{1/\alpha}}{ \sum_k L_k^{1/\alpha}} = x_i$$

Then, $\mathcal{J}$ is satisfiable if and only if a subset of the  $p_i$ sums to $s$. This is equivalent to say that a subset of $X$ sums to $s$, i.e., that $\mathcal{I}$ is satisfiable.\qed
\end{proof}



We will denote by $A$ the subset of the indices of the tasks allocated
to the $p$-part, and by $\bar A$ the complementary of $A$. Then, the
schedule that partition the tasks according to the subset $A$ and
performing a PM schedule on both parts is denoted by $\s_A$.



The $(p,q)$-scheduling problem is strictly equivalent to {\sc Subset sum} only when we restrict ourselves to instances where the PM schedule is compatible with the $(p,q)$-constraint. In the general case, the minimum-makespan schedule is reached when  the total idle time is minimized and the schedule is PFC.

This is equivalent to pack all the tasks in two sets $A$ and $B$ such that the difference between the time needed to complete $A$ with $p$ processors and the one needed to complete $\bar A$ with $q$ processors is minimized, i.e.:

$$\mathop{\mathrm{minimize}}\limits_{A} \left(\frac{\sum_{i\in A} L_i^{1/\alpha}}{p}\right)^\alpha - \left(\frac{\sum_{i\in \bar A} L_i^{1/\alpha}}{q}\right)^\alpha$$
$$ \mathop{\mathrm{minimize}}\limits_{A} \left(q^\alpha\left(\sum_{i\in A} x_i\right)^\alpha - p^\alpha\left(\sum_{i\in \bar A} x_i\right)^\alpha\right) \quad \textit{ where $x_i=L_i^{1/\alpha}$}$$

Note that the makespan of the schedule associated to the subset $A$ is:

$$M_A=\max\left(\frac{\saalp}{p^\alpha}, \frac{\sabalp}{q^\alpha}\right)$$



Consider an instance of $(p,q)$-scheduling. Let $S$ be $\sum_i x_i$, where $x_i=L_i^{1/\alpha}$ and $X$ be the set $\left\{x_i, i\in[1,n]\strut\right\}$.


\begin{lemma}
A tight lower bound of the optimal makespan is $ \mideal = \puisa{\frac{S}{p+q}}$. In such a schedule, we have $\sa = \frac{pS}{p+q}$. We denote this quantity by $\saideal$.
\end{lemma}

\begin{proof}
$\mideal$ is the makespan of the PM schedule, which is a lower bound of the optimal makespan, and can be reached, as in the proof of Theorem \ref{th:pqnpc}.\qed
\end{proof}

We note $r =\max\left(\frac qp,\frac pq\right)$.


For $0<\kappa<1$, a $\kappa$-approximation of {\sc Subset sum} returns a subset $A$ of $X$ such that the sum of its elements ranges between $\kappa OPT$ and $OPT$, where $$\displaystyle OPT = \max\limits_{A\  \mid\  \sum_A x_i \leq s} \sum_{A} x_i$$

For $\lambda>1$, a $\lambda$-approximation of the $(p,q)$-scheduling problem returns a schedule such that its makespan is not larger than $\lambda$ times the optimal makespan.
We note $\varepsilon_\lambda=\lambda^{1/\alpha}-1$ and $\varepsilon_\kappa = 1-\kappa$.

An AS \A resolving {\sc Subset Sum} is defined as followed. Given an instance $\mathcal I$ of {\sc Subset Sum} and a parameter $0<\kappa<1$, it computes a solution to $\mathcal I$ achieving a $\kappa$-approximation in a time complexity $f_\A(n,\ek)$. 

An AS \B resolving $(p,q)$-scheduling is defined as followed. Given an instance $\mathcal J$ of the $(p,q)$-scheduling problem and a parameter $\lambda>1$, it computes a solution to $\mathcal J$ achieving a $\lambda$-approximation in a time complexity $f_\B(\mathcal J,\el)$.

\newcommand{\mtgamrap}{\max\left(3,\left\lceil\frac{1}{\ek}-4\right\rceil\right)}

\begin{remark}
There exist a FPTAS for {\sc Subset Sum}.



In [Kellerer et al'02] "An efficient FPTAS for the subset sum" a FPTAS of time complexity $O\left(\min\left(n/\ek,n+1/\ek^2\log(1/\ek)\right)\right)$ and space complexity $0(n+1/\ek)$ is proposed.

\end{remark}



\begin{defi}
\label{def:pqrest}
The \pqres problem is defined from the \pqsched problem by replacing the entries $L_i$ by $x_i = L_i^{1/\alpha}$, and is restricted to the case where the $x_i$ are integers.
\end{defi}






\begin{theorem}
\label{th:pqptas}
Given a AS \A of {\sc Subset Sum} of time complexity $(n,\ek) \mapsto f_\A(n,\ek)$, Algorithm \ref{alg:pqapp} performs a AS to \pqres with time complexity $(n,p,q,\alpha,\lambda) \mapsto O\left(f_\A\left(n,\frac{\el}{r}\right)\right)$.
\end{theorem}

\begin{coro}
The \pqres problem admits a AS of time complexity $O\left(\min\left(\frac{nr}{\lambda^{1/\alpha}-1},n+\left(\frac{r}{\lambda^{1/\alpha}-1}\right)^2\log(\frac{r}{\lambda^{1/\alpha}-1})\right)\right)$ and space complexity $0\left(n+\frac{r}{\lambda^{1/\alpha}-1}\right)$.

Indeed, parametrized by the FPTAS of [Kellerer'02], Algorithm \ref{alg:pqapp} is an AS of the \pqres problem of such a complexity.

\end{coro}


\begin{proof}

Let $\mathcal{I}$ be an instance of \pqres, $\lambda>1$ and \A be an AS of {\sc Subset Sum}.

We recall that raising a number to the power $\alpha$ or $1/\alpha$ is assumed feasible.

If $\lambda \geq (1+r)^{\alpha}$, it suffices to compute the PM schedule on the largest part of the platform.
We assume in the following that $\lambda < (1+r)^\alpha$.


We define $\kappa = \left(1-\frac{1}{r}(\lambda^{1/\alpha}-1)\right)$, so that $\ek=\frac{\el}{r}$. One can check that $0<\kappa<1$.

A tight lower bound on the makespan is $\mideal = \puisa{\frac{S}{p+q}}$. Indeed, it represents the makespan of the PM schedule on $p+q$ processors, which can respect the constraint for some values of the $x_i$s. 


 Let \sopt be an optimal schedule of $\mathcal I$. If we denote by $\ao$ the subset of the tasks allocated to the $p$-part of the platform, we have either:

\begin{equation}
\label{eq:apq}
\sao \leq \saideal \leq \frac{p S}{p+q} = p \mideal^{1/\alpha} \quad\text{or}\quad 
\saob = S - \sao \leq \frac{q S}{p+q} = q \mideal^{1/\alpha}
\end{equation}

We first suppose that the left inequality holds. The other case is treated at the end of the proof.


Then,
$$\puisa{\frac{\sao}{p}} \leq \mideal$$

So the $p$-part of the schedule terminates before the ideal schedule. Therefore, the $q$-part of the schedule terminates after the $p$-part as $\mopt\geq\mideal$.
We denote $\saopt = \sao$.


Then, the makespan $\mopt$ is equal to the time needed to complete the tasks of $\bar \ao$. Then we have 
$$\mopt = \puisa{\frac{\saob}{q}}  = \puisa{\frac{S-\sao}{q}} $$

Let $\Lambda$ be the set of subsets f $X$: 


$$\Lambda = \left\{ A\subset X \midset  (1-\ek) \saopt \leq \sa \leq \saopt \right\}$$


We now prove in the following paragraphs that a subset $A\in \Lambda$ is computed by the algorithm \A launched on $X$, $s=\saideal=\frac{pS}{p+q}$, and $\ek$. First, we recall that the $x_i$ are assumed to be integers in the formulation of \pqres;

We know that $\sao=\saopt$, so $A_O \in \Lambda$.
Then, there does not exist any subset $A$ of $X$ such that $\saopt < \sa \leq \saideal$, because the associated schedule $\s_A$ would have a makespan smaller than $\mopt$, which contradicts the optimality of $\sopt$.

So $A_O$ is an optimal solution to the instance submitted to $\A$.
Therefore, \A launched on this instance with the parameter $\gamma$ will return a set $A\in \Lambda$ in time $f_\A(n,\eg)$, and so the claim is proved.


\newparskip


Let $A$ be an element of $\Lambda$.
We know that the makespan $M_A$ of the corresponding schedule $\s_A$ allocating the tasks corresponding to $A$ on the $p$-part is:

$$M_A=\puisa{\max\left(\frac{\sa}{p}, \frac{\sab}{q}\right)}$$


We have

$$ \sab = S - \sa$$

and 

$$\sa \geq \kappa \saopt$$

So

$$\frac{\sab}{q} \leq \frac{ S- \kappa \saopt}{q}$$

and 

$$ \sa \leq \saopt \leq \saideal$$


This last inequality implies that the tasks allocated to the $p$-part of the platform are terminated before $\mideal$, and so necessarily before the tasks allocated to $q$-part. Therefore, we have $$M_A = \puisa{\frac{\sab}{q}}$$.

and so
$$
\left(\frac{M_A}{\mopt}\right)^{1/\alpha} 
\leq \frac{S-\kappa\saopt}{S-\saopt}\\ $$

Then, as $0<\kappa<1$ and $\saopt\leq\saideal$, we get

\begin{align*}
\left(\frac{M_A}{\mopt}\right)^{1/\alpha} 
&\leq \frac{S-\kappa\saideal}{S-\saideal}\\
&\leq \frac{1-\frac{\kappa p}{p+q}}{1-\frac{p}{p+q}}\\
&\leq \frac{p+q-\kappa p}{q}\\
&\leq 1 + \frac{p}{q} (1-\kappa)
\end{align*}

Then, $r$ is an upper bound of $\frac{p}{q}$, so

$$ \frac{M_A}{\mopt} \leq \puisa{1+ r\left(1-\kappa\right)}$$

Finally, by the definition of $\kappa$, we get

$$ \frac{M_A}{\mopt} \leq \lambda$$


\newparskip

We have supposed so far that the left inequality of (\ref{eq:apq}) holds. Otherwise, the second one holds. Note that both hypotheses only differ by an exchange of the roles of $p$ and $q$. Then, as the problem is strictly symmetric in $p$ and $q$, by an analogue reasoning, one can prove that \A launched on $X$, $\frac{qS}{p+q}$, $\ek$ returns a set $B$ in 

$$\Lambda' = \left\{ B\subset X \midset  (1-\ek) \sum_{i\in\bar \ao} x_i \leq \sum_{i\in B} x_i \leq \sum_{i\in\bar \ao} x_i\right\}$$

and that the schedule that associates $B$ to the $q$-part of the processors has a makespan smaller than $\lambda \mopt$. Indeed, we needed to obtain this conclusion that $r\geq \frac{p}{q}$, and as we also have $r\geq \frac qp$, the  same method works in this case.

\newparskip

To conclude, Algorithm \ref{alg:pqapp} launched with the parameter $\lambda$ computes a set $A\in \Lambda$ and a set $B\in \Lambda'$, then returns the schedule that has the minimum makespan between $\s_A$ and $\s_{\bar B}$. Therefore, regardless of which inequality of (\ref{eq:apq}) holds, the returned schedule has a makespan smaller than $\lambda\mopt$, and so Algorithm \ref{alg:pqapp} achieves a $\lambda$-approximation.\qed 
\end{proof}


Formally, the algorithm is the following.

\begin{figure}[!ht]


\SetEndCharOfAlgoLine{;}

\begin{algorithm}[H]

\SetKwFunction{pqapp}{PQApp}
\SetKwBlock{Fct}{Function \pqapp{$G$,$p$,$q$,$\lambda$}}{}
\SetKwInOut{KwOut}{Output}
\SetKwInOut{KwIn}{Input}

\SetKwIF{AFct}{AElseIf}{AElse}{if}{then}{else if}{else}{endif}
\SetVline
\Fct{
 \KwIn{A graph $G$ composed of $n$ independent tasks $T_i$ of length $L_i$, the parameters $p$ and $q$ of the processor platform $\mathcal{P}$, and the requested approximation ratio $\lambda$}
 \KwOut{A schedule $\s$ of $G$ on $\mathcal{P}$ that is a $\lambda$-approximation of the makespan}
\If{$\lambda > (1+r)^\alpha$}{
	\Return{the PM schedule on the largest part}}

 $A \gets \A\left(X,\frac{pS}{p+q}, \frac{\el}r\right) \quad ; \quad$ 
 $B \gets \A\left(X,\frac{qS}{p+q}, \frac{\el}r\right)$\;
\Return{the schedule with the minimum makespan between $\s_A$ and $\s_{\bar B}$}
 }
 \vspace*{-1em}
 \caption{Approximation scheme for the $(p,q)$-scheduling problem \mystrut(1.5em)}
 \label{alg:pqapp}
\end{algorithm}
\end{figure}





\fi 





























   











\section{Conclusion}

In this paper, we have studied how to schedule trees of malleable
tasks whose speedup function on multicore platforms is $p^\alpha$. We
have first motivated the use of this model for sparse matrix
factorizations by actual experiments. When using factorization kernels
actually used in sparse solvers, we show that the speedup follows the
$p^\alpha$ model for reasonable allocations. On the machine used for
our tests, $\alpha$ is in the range 0.85--0.95. Then, we proposed a
new proof of the optimal allocation derived by Prasanna and
Musicus~\cite{prasmus,prasmus2} for such trees on single node
multicore platforms. Contrarily to the use of optimal control theory
of the original proofs, our method relies only on pure scheduling
arguments and gives more intuitions on the scheduling problem. Based
on these proofs, we proposed several extensions for two multicore
nodes: we prove the NP-completeness of the scheduling problem and propose
a \frtrd-approximation algorithm for a tree of malleable tasks on two
homogeneous nodes, and an FPTAS for independent malleable tasks on two
heterogeneous nodes. 



The perspectives to extend this work follow two main
directions. First, it would be interesting to extend the
approximations proposed for the heterogeneous case to a number of nodes
larger than two, and to more heterogeneous nodes, for which the value
of $\alpha$ differs from one node to another. This is a promising
model for the use of accelerators (such as GPU or Xeon Phi). The
second direction concerns an actual implementation of the PM
allocation scheme in a sparse solver. 


\bibliographystyle{splncs03}
\bibliography{europar}
\end{document}
