\documentclass{llncs}




\usepackage{figlatex}\graphicspath{{fig/}}

\usepackage{subfigure}
\usepackage[english]{babel}
\usepackage{color,amssymb, url,amsfonts, amsmath}	
\usepackage{tikz,xcolor,pgf,verbatim,xspace}\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage{paralist}
\usepackage[utf8]{inputenc}
\newcommand{\qrm}{\texttt{qr\_mumps}\xspace}
\newcommand{\mumps}{\texttt{MUMPS}\xspace}
\newcommand{\superlu}{\texttt{SuperLU}\xspace}
\newcommand{\pastix}{\texttt{PaStiX}\xspace}

\usetikzlibrary{calc, fit, shapes,arrows}
\usetikzlibrary{decorations,decorations.pathreplacing,patterns,shapes.multipart}
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}






\newcommand{\inte}[4][]{\int_{#2}^{#1} \! #3 \ \mathrm #4}
\def\mystrut(#1){\vrule height #1 depth 0pt width 0pt}   


	\newcommand{\newparskip}{\bigskip}
	\renewcommand{\labelitemi}{}
\newtheorem{defi}[theorem]{Definition}
	 \newtheorem{prop}[theorem]{Property}
	

\newcommand{\LG}[1]{\mathcal{L}_{#1}}
\newcommand{\para}[2]{#1 \mathop{\parallel} #2}
\newcommand{\seri}[2]{#1 \mathop{;} #2}
\newcommand{\s}{\mathcal S\xspace}
\newcommand{\spm}{\ensuremath{\mathcal{S}_{\mathrm{PM}}}\xspace}
\newcommand{\mpm}{\ensuremath{M^{\mathrm{PM}}}\xspace}
\newcommand{\slb}{\ensuremath{\mathcal{S}_{\mathrm{LB}}}\xspace}
\newcommand{\sopt}{\ensuremath{\mathcal{S}_{\mathrm{OPT}}}\xspace}
\newcommand{\uopt}{\ensuremath{u_{\mathrm{OPT}}}\xspace}
\newcommand{\Aopt}{\ensuremath{A_{\mathrm{OPT}}}\xspace}
\newcommand{\mopt}{\ensuremath{{M}_{\mathrm{OPT}}}\xspace}
\newcommand{\opt}{\ensuremath{\mathit{OPT}}\xspace}
\newcommand{\p}{\mathcal P}
\newcommand{\A}{\ensuremath{\mathcal A}\xspace}
\newcommand{\B}{\ensuremath{\mathcal B}\xspace}
\newcommand{\C}{\ensuremath{\mathcal C}\xspace}
\newcommand{\I}{\ensuremath{\mathcal I}\xspace}
\newcommand{\J}{\ensuremath{\mathcal J}\xspace}
\newcommand{\T}{\Delta}
\newcommand{\D}[1]{\Delta_{\mathcal{#1}}}
\newcommand{\tl}{\tau}
\newcommand{\R}{\ensuremath{\mathcal{R}}\xspace}
\newcommand{\rat}{r}
\newcommand{\fctofproc}{{processor profile}\xspace}
\newcommand{\frtrd}{\ensuremath{\left(\frac{4}{3}\right)^\alpha}}
\newcommand{\mideal}{\ensuremath{{M}_{\mathrm{ideal}}}\xspace}
\newcommand{\saideal}{\ensuremath{{\Sigma}_{\mathrm{ideal}}}\xspace}
\newcommand{\sa}{\ensuremath{\sum_{i\in A} x_i}}
\newcommand{\ao}{\ensuremath{A_{\mathrm{O}}}}
\newcommand{\sab}{\ensuremath{\sum_{i\in \bar A} x_i}}
\newcommand{\sao}{\ensuremath{\sum_{i\in \ao} x_i}}
\newcommand{\saob}{\ensuremath{\sum_{i\in \bar \ao} x_i}}
\newcommand{\saalp}{\ensuremath{\left(\sum_{i\in A} x_i\right)^\alpha}}
\newcommand{\sabalp}{\ensuremath{\left(\sum_{i\in \bar A} X_i\right)^\alpha}}
\newcommand{\saopt}{\ensuremath{\mathcal{\Sigma}_{\mathrm{OPT}}}\xspace}
\newcommand{\puisa}[1]{\left(#1\right)^\alpha}
\newcommand{\eg}{\varepsilon_\gamma}
\newcommand{\ek}{\varepsilon_{\kappa}}
\newcommand{\el}{\varepsilon_\lambda}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\midset}{\mathrel{}\middle|\mathrel{}}
\newcommand{\ssnir}{ {\sc Subset Sum Non-Integer Restricted}\xspace}
\newcommand{\ssni}{ {\sc Subset Sum Non-Integer}\xspace}
\newcommand{\pqres}{ {\sc -Scheduling Restricted}\xspace}
\newcommand{\pqsched}{ {\sc -Scheduling}\xspace}

\newcommand{\divis}{{\sc Divisible}\xspace}
\newcommand{\propo}{{\sc Proportional}\xspace}


\newif\iflong
\longfalse


\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}
\title{Scheduling Trees of Malleable Tasks\\ for Sparse Linear
  Algebra} 



\titlerunning{Scheduling Trees of Malleable Tasks}

\author{Abdou Guermouche\inst{1} \and Loris Marchal\inst{2} \and Bertrand Simon\inst{2} \and Fr\'ed\'eric Vivien\inst{2}}
\institute{University of Bordeaux and INRIA,\\
     200 rue de la vieille Tour,  Talence, France\\
     \url{abdou.guermouche@labri.fr}
\and
CNRS, INRIA and University of Lyon,\\
     LIP, ENS Lyon, 46 allée d'Italie, Lyon, France\\
     \url{{loris.marchal,bertrand.simon,frederic.vivien}@ens-lyon.fr}}

\maketitle


\begin{abstract}
  Scientific\blfootnote{This work was supported by the ANR SOLHAR project
  funded by the French National Research Agency.} workloads are often described by directed acyclic task
  graphs. This is in particular the case for multifrontal
  factorization of sparse matrices ---the focus of this paper--- whose
  task graph is structured as a tree of parallel tasks. Prasanna and
  Musicus~\cite{prasmus,prasmus2} advocated using the concept of
  \emph{malleable} tasks to model parallel tasks involved in matrix
  computations.  In this powerful model each task is processed on a
  time-varying number of processors.  Following Prasanna and Musicus,
  we consider malleable tasks whose speedup is , where 
  is the fractional share of processors on which a task executes, and
   () is a task-independent parameter.
  Firstly, we use actual experiments on multicore platforms to
  motivate the relevance of this model for our application. Then, we
  study the optimal time-minimizing allocation proposed by Prasanna
  and Musicus using optimal control theory. We greatly simplify their
  proofs by resorting only to pure scheduling arguments. Building on
  the insight gained thanks to these new proofs, we extend the study
  to distributed (homogeneous or heterogeneous) multicore
  platforms. We prove the NP-completeness of the corresponding
  scheduling problem, and we then propose some approximation
  algorithms.
\end{abstract}







\section{Introduction}
\label{sec:intro}


Parallel workloads are often modeled as directed acyclic task graphs,
or DAGs, where nodes represent tasks and edges represent dependencies
between tasks. Task graphs arise from many scientific domains, such as
image processing, genomics, and geophysical simulations. In this
paper, we focus on task graphs coming from sparse linear algebra, and
especially from the factorization of sparse matrices using the
multifrontal method. Liu~\cite{liu:90} explains that the computational
dependencies and requirements in Cholesky and LU factorization of
sparse matrices using the multifrontal method can be modeled as a task
tree, called the \emph{assembly tree}. We therefore focus on
dependencies that can be modeled as a tree.


In the abundant existing literature, several variants of the task
graph scheduling problem are addressed, depending on the ability to
process a task in parallel: tasks are either \emph{sequential} (not
amenable to parallel processing), \emph{rigid} (requesting a given
number of processors), \emph{moldable} (able to cope with any fixed
number of processors) or even \emph{malleable} (processed on a variable
number of processors) in the terminology of~Drozdowski~\cite[chapter
25]{handbook}. When considering moldable and malleable tasks, one has
to define how the processing time of a task depends on the number of
allocated  processors.  Under some general
assumptions, Jansen and Zhang~\cite{jansen05} derive a
3.29~approximation algorithm for arbitrary precedence constraints,
which is improved in a 2.62~approximation in the particular case of a
series-parallel precedence graph by Lepere et
al.~\cite{lepere02}. However, although polynomial, these algorithms
relies on complex optimization techniques, which makes them difficult
to implement in a practical setting.

In this study, we consider a special case of malleable tasks, where
the speedup function of each task is , where  is the
number of processors allocated to the task, and  is a
global parameter.  In particular, when the share of processors 
allocated to a task  is constant, its processing time is given by
, where  is the sequential duration of
. The case  represents the unrealistic case of a
perfect linear speed-up, and we rather concentrate on the case
 which takes into consideration the cost of the
parallelization. In particular  accounts for the cost of
intra-task communications, without having to decompose the tasks in
smaller granularity sub-tasks with explicit communications, which
would make the scheduling problem intractable. This model has been
advocated by Prasanna and Musicus~\cite{prasmus2} for matrix
operations, and we present some new motivation for this model in our
context. As in~\cite{prasmus2}, we also assume that it is possible to
allocate non-integer shares of processors to tasks. This amounts to
assume that processors can share their processing time among
tasks. When task  is allocated 2.6 processors and task  3.4
processors, one processor dedicates 60\% of its time to  and 40\%
to . Note that this is a realistic assumption, for example, when
using modern task-based runtime systems such as StarPU~\cite{starpu},
KAAPI~\cite{kaapi}, or PaRSEC~\cite{parsec}. This allows to simplify
the scheduling problem and to derive optimal allocation algorithms.

Our objective is to minimize the total processing time of a tree of
malleable tasks. Initially, we consider a homogeneous platform
composed of  identical processors. To achieve our goal, we take
advantage of two sources of parallelism: the \emph{tree parallelism}
which allows tasks independent from each others (such as siblings) to
be processed concurrently, and the \emph{task parallelism} which
allows a task to be processed on several processors. A solution to
this problem describes both in which order tasks are processed and
which share of computing resources is allocated to each task.

In~\cite{prasmus,prasmus2}, the same problem has been addressed by
Prasanna and Musicus for series-parallel graphs (or SP-graphs). Such
graphs are built recursively as series or parallel composition of two
smaller SP-graphs. Trees can be seen as a special-case of
series-parallel graphs, and thus, the optimal algorithm proposed
in~\cite{prasmus,prasmus2} is also valid on trees. They use optimal
control theory to derive general theorems for any strictly increasing
speedup function. For the particular case of the speedup function
, Prasanna and Musicus prove some properties of the unique
optimal schedule which allow to compute it efficiently. Their results
are powerful (a simple optimal solution is proposed), but to obtain
these results they had to transform the problem in a shape which is
amenable to optimal control theory. Thus, their proofs do not provide
any intuition on the underlying scheduling problem, yet it seems
tractable using classic scheduling arguments.

In this paper, our contributions are the following:
\begin{compactitem}
\item In Sect.~\ref{sec:motiv}, we show that the model of malleable
  tasks using the  speed-up function is justified in the
  context of sparse matrix factorization.
\item In Sect.~\ref{sec:shared}, we propose a new and simpler proof
  for the results of ~\cite{prasmus,prasmus2} on series-parallel
  graphs, using pure scheduling arguments. 
\item In Sect.~\ref{sec:dist}, we extend the previous study on
  distributed memory machines, where tasks cannot be distributed
  across several distributed nodes. We provide NP-completeness
  results and approximation algorithms.
\end{compactitem}



\section{Validation of the Malleable Task Model}
\label{sec:motiv}

In this section, we evaluate the model proposed by
Prasanna and Musicus in~\cite{prasmus,prasmus2} for our target
application. This model states that the instantaneous speedup of a task processed on
 processors is . Thus, the processing time of a task
 of size  which is allocated a share of processors 
at time  is equal to the smallest value  such that

where  is a task-independent constant. When the share
of processors  is constant, . Our goal is
(i) to find whether this formula well describes the evolution of the
task processing time for various shares of processors and (ii) to
check that different tasks of the same application have the same
 parameter.  We target a modern multicore platform composed of
a set of nodes each including several multicore processors. For the
purpose of this study we restrict ourselves to the single node case
for which the communication cost will be less dominant. In this
context,  denotes the number of \emph{cores} dedicated to task
 at time .

We consider applications having a tree-shaped task
graph constituted of parallel tasks. This kind of
execution model can be met in sparse direct solvers where the matrix
is first factorized before the actual solution is computed. For
instance, either the multifrontal method~\cite{dure:83} as implemented
in \mumps~\cite{mumps11} or \qrm~\cite{buttari10}, or the supernodal
approach as implemented in \superlu~\cite{superlu} or
in \pastix~\cite{pastix}, are based on tree-shaped task graphs (namely the
assembly tree~\cite{aglp:87}).  Each task in this tree is a partial
factorization of a dense sub-matrix or of a sparse panel. In order to
reach good performance, these factorizations are performed using tiled
linear algebra routines (BLAS): the sub-matrix is decomposed into 2D
tiles (or blocks), and optimized BLAS kernels are used to perform the
necessary operations on each tile. Thus, each task can be seen as a
task graph of smaller granularity sub-tasks.







As computing platforms evolve quickly and become more complex (e.g.,
because of the increasing use of accelerators such as GPUs or Xeon
Phis), it becomes interesting to rely on an optimized dynamic runtime
system to allocate and schedule tasks on computing resources. These
runtime systems (such as StarPU~\cite{starpu}, KAAPI~\cite{kaapi}, or
PaRSEC~\cite{parsec}) are able to process a task on a prescribed
subset of the computing cores that may evolve over time. This
motivates the use of the malleable task model, where the share of
processors allocated to a task vary with time. This approach has been
recently used and evaluated~\cite{hugo14} in the context of the \qrm
solver using the StarPU runtime system.

In order to assess whether tasks used within sparse direct
solvers fit the model introduced by Prasanna and Musicus
in~\cite{prasmus2} we conducted an experimental study on several dense
linear algebra tasks. We used a test platform composed of 4 Intel
E7-4870 processors having 10 cores each clocked at 2.40~GHz and having
30~MB of L3 cache for a total of 40 cores. The platform is equipped
with 1~TB of memory with uniform access. We considered dense
operations which are representative of what can be met in sparse
linear algebra computations, namely the standard frontal matrix
factorization kernel used in the \qrm solver. We used either
block-columns of size 32 (1D partitioning) or square blocks of size
256 (2D partitioning). All experiments were made using the StarPU
runtime.






\begin{figure}[bt]
  \centering
\subfigure[\mystrut(1.2em) Timings and model (lines) with 1D partitioning]{\label{fig.qrm1D}\includegraphics[width=0.6\linewidth]{qrm1d_model_log_log.fig}}
\hspace{0.5cm}
\raisebox{65pt}{\subfigure[\mystrut(1.2em)Values of ]{\label{fig.qrmalpha}\scalebox{0.9}{\begin{tabular}[c]{|c|c|c|}
      \hline
      matrix & ~~1D~~ & ~~2D~~ \\
      \hline
      5000x1000~~  & 0.78 & 0.93\\
      \hline
      10000x2500~~ & 0.88 & 0.95 \\
      \hline
      20000x5000~~ & 0.89 & 0.94 \\
      \hline
    \end{tabular}}
}
}
\caption{Timings and  values for \qrm frontal matrix factorization kernel}
  \label{fig.qrm}
\end{figure}

Figure~\ref{fig.qrm1D} presents the timings obtained when processing
the \qrm frontal matrix factorization kernel on a varying number of
processors. The logarithmic scales show that the  speedup
function models well the timings, except for small matrices when 
is large. In those cases, there is not enough parallelism in tasks to
exploit all available cores. We performed linear regressions on
the portions where  to compute  for
different task sizes (Fig.~\ref{fig.qrmalpha}). We performed
the same test for 2D partitioning and computed the corresponding
 values (using ). We notice that the value of
 does not vary significantly with the matrix size, which
validates our model. The only notable exception is for the smallest
matrix (5000x1000) with 1D partitioning: it is hard to efficiently use
many cores for such small matrices. In all cases, when the number of
processors is larger than a threshold the performance deteriorates and
stalls. Our speedup model is only valid below this threshold, 
which threshold increases with the matrix size. This is not a
problem as the allocation schemes developed in the next sections
allocate large numbers of processors to large tasks at the top of the
tree and smaller numbers of processors for smaller tasks. In other
words, we produce allocations that always respect the validity
thresholds of the model.
 Finally, note that the value of  depends on the
 parameters of the problem (type of factorization, partitioning, block
 size, etc.). It has to be determined for each kernel and each set of
 blocking parameters. 

\section{Model and Notations}
\label{sec:model}

We assume that the number of available computing resources may vary
with time:  gives the (possibly rational) total number of processors
available at time , also called the \fctofproc. For the sake of
simplicity, we consider that  is a step function. Although our
study is motivated by an application running on a single multicore
node (as outlined in the previous section), we use the term
\emph{processor} instead of \emph{computing core} in the following
sections for readability and consistency with the scheduling
literature.

We consider an in-tree  of  malleable tasks .
 denotes the length, that is the sequential processing time, of
task .  As motivated in the previous section, we assume that the
speedup function for a task allocated  processors is ,
where  is a fixed parameter.
A schedule  is a set of nonnegative piecewise continuous functions
 representing the time-varying
share of processors allocated to each task. During a time interval
, the task  performs an amount of work equal to . Then,  is completed when the total work
performed is equal to its length . The completion time of task
 is thus the smallest value  such that .
We define  as the ratio of the
work of the task  that is done during the time interval :
.  A schedule is a
valid solution if and only if:
\begin{compactitem}
\item it does not use more processors than available: ;
\item it completes all the tasks: ;
\item and it respects precedence constraints:
  , if  then, ,
  if  is a child of , .
\end{compactitem}
The makespan  of a schedule is computed as . Our objective is to construct a valid
schedule with optimal, i.e., minimal, makespan.



Note that because of the speedup function , the computations
in the following sections will make a heavy use of the functions
 and . We assume that
we have at our disposal a polynomial time algorithm to compute both
 and . We are aware that this assumption is very likely to be
wrong, as soon as , since  and  produce irrational
numbers. However, without these functions, it is not even possible to
compute the makespan of a schedule in polynomial time and, hence, the
problem is not in NP. Furthermore, this allows us to avoid the
complexity due to number computations, and to concentrate on the most
interesting combinatorial complexity, when proving NP-completeness
results and providing approximation algorithms. In practice, any
implementation of  and  with a reasonably good accuracy will be
sufficient to perform all computations including the computation
of makespans.



In the next section, following Prasanna and Musicus, we will not
consider trees but more general graphs: \emph{series-parallel graphs}
(or SP graphs). An SP graph is recursively defined as a single task,
the series composition of two SP graphs, or the parallel composition
of two SP graphs. A tree can easily be transformed into an SP graph by joining the
leaves according to its structure, the resulting graph is then called a \emph{pseudo-tree}.  We will use
 to represent the parallel composition of tasks  and
 and  to represent their series composition. 
Thanks to the construction of pseudo-trees, an algorithm which solves
the previous scheduling problem on SP-graphs also gives an optimal
solution for trees.





\section{Optimal Solution for Shared-Memory Platforms}

\label{sec:shared}





The purpose of this section is to give a simpler proof of the results
of~\cite{prasmus,prasmus2} using only scheduling arguments. We
consider an SP-graph to be scheduled on a shared-memory platform
(each task can be distributed across the whole platform). We assume
that  and prove the uniqueness of 
 the optimal schedule.

Our objective is to prove that any SP graph  is \emph{equivalent}
to a single task  of easily computable length: for any \fctofproc
, graphs  and  have the same makespan. We prove that the
ratio of processors allocated to any task , defined by
, is constant from the
moment at which  is initiated to the moment at which it is
terminated. We also prove that in an optimal schedule, the two
subgraphs of a parallel composition terminate at the same
time and each receives a constant total ratio of processors throughout
its execution.
We then prove that these properties imply that the optimal schedule is unique and obeys
to a {\it flow conservation} property: the shares of processors
allocated to two subgraphs of a series composition are equal. When
considering a tree, this means that the whole schedule is defined by
the ratios of processors allocated to the leaves. Then, all the children of a node  terminate at the
same time, and its ratio is the sum of its children
ratios.


We first need to define the length  associated to a graph , which
will be proved to be the length of the task . Then, we state a few lemmas
before proving the main theorem. We only present here sketches of the proofs,
the detailed versions can be found in \cite{RR-ipdps-2014}.

\begin{defi}
\label{def.eq-task}
We recursively define the length  associated to a SP graph :
\begin{inparaitem}
	\item  \hfill 
	\item  \hfill
	\item 
\end{inparaitem}
\end{defi}

\newcommand{\Q}{\mathcal{Q}}



\begin{lemma}
  \label{lem:allproc}
  An allocation minimizing the makespan uses all the processors at any time. 
\end{lemma}



We call a \emph{clean interval} with regard to a schedule  an
interval during which no task is completed in .

\begin{lemma}
  \label{lem:esc}
  When the number of available processors is constant, any optimal schedule
  allocates a constant number of processors per task on any clean
  interval.
\end{lemma}




\begin{proof}
  By contradiction, we assume that there exists an optimal schedule
   of makespan , a task  and a clean interval
   such that  is not allocated a constant number of
  processors on . By definition of clean intervals, no task
  completes during .  denotes the duration of ,
   the set of tasks that receive a non-empty share of processors
  during , and  the constant number of available processors.
	




  We want to show that there exists a valid schedule with a makespan
  smaller than . To achieve this, we define an intermediate and
  not necessarily valid schedule , which
  nevertheless respects the resource constraints (no more than
   processors are used at time ). This schedule is
  equal to  except on .
The constant share of processors allocated to task  on  in
   is defined by .  For all , we have  because
  of Lemma~\ref{lem:allproc}. We get . So
   respects the resource constraints.
  Let  (resp. ) denote the
  work done on  during  under schedule 
  (resp. ).
    We have 
  
  As , the function  is concave and then,
  by Jensen inequality \cite{Hardy}, .  Moreover, as  is
  \emph{strictly} concave, this inequality is an equality if and only
  if the function  is equal to a constant on
   except on a subset of  of null measure \cite{Hardy}.
  Then, by definition,  is not constant on , and
  cannot be made constant by modifications on a set of null measure.
  We thus have .
Therefore,  is allocated too many processors under
  .  It is then possible to distribute this surplus among
  the other tasks during , so that the work done during  in
   can be terminated earlier. This remark implies that
  there exists a valid schedule with a makespan smaller than ;
  hence, the contradiction.\qed
\end{proof}


We recall that   is the instantaneous ratio of
processors allocated to a task  .

\begin{lemma}
\label{lem:rate}
Let  be the parallel composition of two tasks,  and . If
 is a step function, in any optimal schedule  is
constant and equal to  up to the completion of .
\end{lemma}


\begin{proof}
First, we prove that  is constant on any optimal
schedule.

We consider an optimal schedule , and two consecutive time intervals  and
 such that  is constant and equal to  on  and  on , and
 does not complete before the end of . Suppose also that  (shorten one interval otherwise), where  and
 are the durations of intervals  and . By Lemma \ref{lem:esc}, 
 has constant values  on  and  on
. Suppose by contradiction that .

We want to prove that  is not optimal, and so that we can do the
same work as  does on  in a smaller makespan. We
set .  We define
the schedule  as equal to  except on  where the ratio
allocated to  is  (see Fig. \ref{fig:rarb}).
\begin{figure}[tb]
\centering
\ifdefined\s
\else
\newcommand{\s}{S}
\fi

\newcommand{\ya}{30pt}
\newcommand{\yb}{50pt}

\newcommand{\yf}{.5*\ya+.5*\yb}

\newcommand{\topy}{70pt}
\newcommand{\topx}{100pt}
\newcommand{\midx}{60pt}

\newcommand{\midlx}{50pt}

\newcommand{\scalerat}{0.8}
\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]


\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};

\node (mra) at (\midx,\ya) {};
\node (mla) at (0,\ya) {};

\node (mlb) at (\midx,\yb) {};
\node (mrb) at (\topx,\yb) {};

\node (mlf) at (0,\yf) {};
\node (mrf) at (\topx,\yf) {};

\node (title) at (\topx/2, \topy+15pt) {};


\draw (bl)--(tl)--(tr)--(br)--(bl);
\draw (mla)--(mra)
      (mlb)--(mrb)
      (bm)--(tm);

\path (tl)--+(10pt,-10pt) node[label] {};
\path (0,\ya)--+(10pt,-10pt) node[label] {};
\path (\midx,\yb)--+(10pt,-10pt) node[label] {};
\path (tm)--+(10pt,-10pt) node[label] {};
\path (mla)--+(-10pt,-0) node[label] {};
\path (mrb)--+(+10pt,-0) node[label] {};
\draw[dotted] (tl)--+(-15pt,-0) node[left] {};
\draw[dotted] (bl)--+(-15pt,-0) node[left] {};

\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\midx,0)--(0,0) node [midway,yshift=-0.4cm] 
{ };
\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\topx,0)--(\midx,0) node [midway,yshift=-0.4cm] 
{};

\end{tikzpicture}
\raisebox{40pt}{\huge}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]


\draw (bl)--(tl)--(tr)--(br)--(bl);
\draw (mlf)--(mrf)
      (bm)--(tm);

\path (tl)--+(10pt,-10pt) node[label] {};
\path (0,\yf)--+(10pt,-10pt) node[label] {};
\path (\midx,\yf)--+(10pt,-10pt) node[label] {};
\path (tm)--+(10pt,-10pt) node[label] {};
\path (mlf)--+(-10pt,-0) node[label] {};
\draw[dotted] (tl)--+(-15pt,-0) node[left,label] {};
\draw[dotted] (bl)--+(-15pt,-0) node[left,label] {};

\path (title) node[label] {};

\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\midx,0)--(0,0) node [midway,yshift=-0.4cm] 
{ };
\draw [decorate,decoration={brace,amplitude=5pt},yshift=-2pt,xshift=0pt]
(\topx,0)--(\midx,0) node [midway,yshift=-0.4cm] 
{};

\end{tikzpicture}}
 \caption{Schedules  and  on . The abscissae represent the time and the ordinates the ratio of processing power}
\label{fig:rarb}
\end{figure}
\\
The work  on task  under  and  under  during  are
equal to:

Then, with the concavity inequality and the fact that , we can deduce that  and symmetrically that .

Therefore,  performs strictly more work for each task during  than
. Thus, as in Lemma~\ref{lem:esc},  is not optimal. So
 is constant in optimal schedules.



There remains to prove that in an optimal schedule , ; hence, the optimal schedule is unique.  As  is a step
function, we define the sequences  and
 such that  is the duration of the -th
step of the function  and  on . The sum of the
durations of the 's is the makespan of .
Then, if we note  and  the value of ,
we have:

Then, .\qed
\end{proof}



\begin{lemma}
  \label{lem:ratequiv}
  Let  be the parallel composition of tasks  and , with
   a step function, and  an optimal schedule. Then, the
  makespan of  under  is equal to the makespan of the task
   of length .
\end{lemma}

\begin{proof}  
  We characterize  by the sequences  and  as in the proof of
  Lemma~\ref{lem:rate}. We know by Lemma~\ref{lem:rate} that the share allocated
  to  is constant and equal to  on each interval .
Then, by summing the work done on each interval for both tasks, one can prove
  that they are completed simultaneously, and that this completion time is the
  same as that of task  under the same processor profile.\qed
\end{proof}



\begin{theorem}
  \label{th:step}
  For every graph , if  is a step function,  has the same
  optimal makespan as its equivalent task  of length  (computed as in Definition~\ref{def.eq-task}). Moreover, there is a unique optimal schedule, and it can be
  computed in polynomial time.
\end{theorem}

\begin{proof}
  In this proof, we only consider optimal schedules. Therefore, when
  the makespan of a graph is considered, this is implicitly its
  optimal makespan.
We first remark that in any optimal schedule, as  is a step
  function and because of Lemma \ref{lem:esc}, only step functions are
  used to allocate processors to tasks, and so Lemma
  \ref{lem:ratequiv} can be applied on any subgraph of  without
  checking that the \fctofproc is also a step function for this
  subgraph.
We now prove the result by induction on the structure of .
  \begin{itemize}
  \item  is a single task. The result is immediate.

  \item  is the series composition of  and . By
    induction,  (resp. ) has the same makespan as task
     (resp. ) of length  (resp. ) under any
    \fctofproc. Therefore, the makespan of  is equal to .
The unique optimal schedule of  under  processors is the
    concatenation of the optimal schedules of  and .


  \item  is the parallel composition of  and . By
    induction,  (resp. ) has the same makespan as task
     (resp. ) of length  (resp. ) under any
    \fctofproc.
Consider an optimal schedule  of  and let  be the
    \fctofproc allocated to . Let  be the schedule of
     that allocates  processors to
    .  is optimal and achieves the same makespan as 
    for  because  and  (resp.  and ) have the
    same makespan under any \fctofproc.  Then, by Lemma
    \ref{lem:ratequiv},  (so ) achieves the same makespan as
    the optimal makespan of the task  of length .
Moreover, by Lemma \ref{lem:rate} applied on ,
    we have . By induction, the unique optimal
    schedules of  and  under respectively  and
     processors can be computed. Therefore, there is a
    unique optimal schedule of  under  processor: the
    parallel composition of these two schedules.  
  \end{itemize}
  
  Therefore, there is a unique optimal schedule for  under . Moreover,
  it can be computed in polynomial time. We describe here the algorithm to
  compute the optimal schedule of a tree , but
  it can be extended to treat SP-graphs. The length of the equivalent
  task of each subtree of  can be computed in polynomial time by a
  depth-first search of the tree (assuming that raising a number to the power
   or  can be done in polynomial time). Hence, the ratios
   and  for each parallel composition can also be computed in
  polynomial time. Finally, these ratios imply the computation in linear time of
  the ratios of the processor profile that should be allocated to each task
  after its children are completed, which describes the optimal schedule.\qed
\end{proof}





\section{Extensions to Distributed Memory}
\label{sec:dist}

The objective of this section is to extend the previous results to the
case where the computing platform is composed of several nodes with
their own private memory. In order to avoid the large communication overhead
of processing a task on cores distributed across several
nodes, we forbid such a multi-node execution: the tasks of the tree can
be distributed on the whole platform but each task has to be processed on a
single node. We prove that this additional constraint, denoted by
\R, renders the problem much more difficult. We concentrate first
on platforms with two homogeneous nodes and then with two heterogeneous
nodes.

\subsection{Two Homogeneous Multicore Nodes}
\label{sec:dist-hom}

In this section, we consider a multicore platform composed of two
equivalent nodes having the same number of computing cores . We
also assume that all the tasks  have the same speedup function
 on both nodes.
We first show that finding a schedule with minimum makespan is weakly
NP-complete, even for independent tasks:

\begin{theorem}
  Given two homogenous nodes of  processors,  independent tasks
  of sizes  and a bound , the problem of finding a
  schedule of the  tasks on the two nodes that respects \R, and
  whose makespan is not greater than , is (weakly) NP-complete for
  all values of the  parameter defining the speedup function.
\end{theorem}

The proof relies on the Partition problem, which is known to be weakly
(i.e., binary) NP-complete~\cite{gareyjohnson}, and uses tasks of
length , where the 's are the numbers from the
instance of the Partition problem. We recall that we assume that
functions  and  can be
computed in polynomial time.  
\iflong
\todo[inline]{faire une courte preuve de
  NP-complétude}
\else
Details can be found in the companion research report~\cite{RR-ipdps-2014}.
\fi

We also provide a constant ratio approximation algorithm. We recall
that a -approximation provides on each instance a solution whose
objective  is such that , where  is the optimal
value of the objective on this instance.

\begin{theorem}
\label{th.43approx}
  There exists a polynomial time \frtrd-approximation algorithm for
  the makespan minimization problem when scheduling a tree of malleable tasks on
  two homogenous nodes.
\end{theorem}

\iflong\else Due to lack of space, we refer the interested reader to
the companion research report
for the complete description of the
algorithm and proof~\cite{RR-ipdps-2014}.  The proof of the
approximation ratio consists in comparing the proposed solution to the
optimal solution on a single node made of  processors, denoted
\spm. Such an optimal solution can be computed as proposed in the
previous section, and is a lower bound on the optimal makespan on 2
nodes with  processors. The general picture of the proposed
algorithm is the following. First, the root of the tree is arbitrarily
allocated to
the  processors of one of the two nodes. Then, the subtrees 's
rooted at the root's children are considered. If none of these
subtrees is allocated more than  processors in \spm, then we show
how to ``pack'' the subtrees on the two nodes and bound the slow-down
by \frtrd. On the contrary, if one of the 's is allocated more
than  processors in \spm, then we allocate  processors to its
root, and recursively call the algorithm on  its children and on the
remaining subtrees.
\fi

\iflong
The proof of this theorem is done by induction on the structure of the
tree and relies on the following lemmas. The approximation algorithm
is summarized in Algorithm~\ref{alg:hybapp}.  Two initialization cases
are depicted. One of them is easily handled, and the second one is
handled by Lemma \ref{lem:x<1}. The heredity property is proved in the
theorem, and uses Lemmas
\ref{lem:muopt},\ref{lem:uopt},\ref{lem:slb}. Lemma
\ref{lem:chainroot} allows the restriction to a slightly simpler class
of graphs.


Let \sopt be a makespan-optimal schedule, and \mopt be its makespan.
We consider the optimal schedule  of  on  processors
without the constraint \R.  is then a PM so a PFC schedule. Its
makespan is , which is then a
lower bound of the optimal makespan with the restriction \R.

One can observe that a  approximation is immediate: a
solution is the PM schedule of  under only  processors, whose
makespan is . As the optimal makespan
is not smaller than ,  is indeed a
-approximation.




Let , for , be the set of children of the root
of , and let  be the subtree of  rooted at  and
including its descendants. We can suppose than the indices are ordered
such that the 's are in decreasing order. We denote
.

We denote , which means that
 processors are dedicated to  in \spm.

The following lemma, whose proof is immediate, allows to restrict the
following discussion on a slightly simpler class of graphs:

\begin{lemma}
\label{lem:chainroot}
We can suppose without loss of generality that the length of the root
of  is  and the root has at least two children. Otherwise, the
chain starting at the root can be aggregated in a single task of
length  before finding the schedule on this modified graph
. It is then immediate to adapt it to the original graph,
by allocating  processors to each task of this chain.
\end{lemma}





\begin{lemma}
\label{lem:x<1}
If we have , then a -approximation is computable in polynomial time.
\end{lemma}


\begin{proof}

Let  be the share allocated to  in \spm. Each  is constant because  is PFC by definition. By hypothesis, we have  for all , as  is the largest  and its share is equal to .

 If , both  and  are not larger than  so equal . Therefore, the schedule  respects restriction , is then optimal and so is a \frtrd approximation. 
 
 Otherwise, we have  and we partition the indices  in three sets , ,  such that the sum  of 's corresponding to each set  is not greater than :

 , which is always possible because no  is greater than  and the sum of all 's is . Indeed, we just have to iteratively place the largest  in the set that has the lowest . If a  exceeds , it was at least equal to  at the previous step, and both other  also: the sum of all 's then exceeds , which is impossible.
 
 Then, we place the set with the largest , say , on one half of the processing power, and aggregate the two smallest,  in the other half. We now compute the PM schedule of  with  processors and  with  processors. The makespan is then .
  Indeed, we have  and , as these quantities represent the makespan of each subpart of the tree in , and all subtrees  terminate simultaneously in \spm.  So .
  
  
 We know that  and , so , then .
Therefore, in ,  processors are allocated to .
Then, the makespan of  verifies , and so . Therefore, the schedule is indeed a \frtrd approximation.\qed
\end{proof} 




\begin{defi}
\label{def:Rq}
For any , let  be the constraint that forces  processors to be allocated to .
\end{defi}


\begin{figure}[Ht]
\centering
\newcommand{\midy}{50pt}
\newcommand{\topy}{80pt}
\newcommand{\midx}{70pt}
\newcommand{\midlx}{40pt}
\newcommand{\topx}{100pt}

\newcommand{\bota}{55pt}
\newcommand{\leftb}{80pt}
\newcommand{\scalerat}{0.8}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]

\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (0,\bota)-- ++(\leftb,0)
      (\leftb,0) -- ++(0,\bota);

\node (title) at (\topx/2, \topy+15pt) {};

\path (tl)--+(10pt,-10pt) node[label] {};
\path (0,\bota)--+(20pt,-10pt) node[label] {};
\path (tm)--+(10pt,-10pt) node[label] {};


\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.4cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.4cm] 
{\footnotesize };
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [midway,yshift=0.2cm]
{\footnotesize };
\draw [gray,<->]
(0,15pt)--(\leftb,15pt) node [midway,yshift=0.3cm] 
{\footnotesize };

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left,xshift=-0.3cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize };

\draw[gray] (\leftb,\bota) -- (\topx,\bota);
\node at (\leftb+10pt,\bota-10pt)  {};

\end{tikzpicture}}
\hfill
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]
\renewcommand{\midlx}{70pt}
\renewcommand{\leftb}{\midlx}
\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midlx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (0,\bota)-- ++(\leftb,0)
      (\leftb,0) -- ++(0,\bota);


\node (title) at (\topx/2, \topy+15pt) {};

\path (tl)--+(10pt,-10pt) node[label] {};
\path (0,\bota)--+(20pt,-10pt) node[label] {};
\path (tm)--+(15pt,-10pt) node[label] {};

\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.4cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=4pt},yshift=-3pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.4cm] 
{\footnotesize };
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [near start,yshift=0.2cm]
{\footnotesize };

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left,xshift=-0.3cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize };

\draw[gray] (\leftb,\bota) -- (\topx,\bota);
\node at (\leftb+15pt,\bota-10pt)  {};

\end{tikzpicture}}
 \caption{A schedule , for  on the left and the schedule  on the right}
\label{fig:hybsu}
\end{figure}

\begin{defi}
\label{def:su}
We denote by  the subgraph .

We define the schedule  parametrized by ,
which respects  but not . It allocates a constant share
 of processors to  until it is terminated. Meanwhile,
 processors are allocated to schedule a part  of . 
may contain fractions of tasks. Before, the rest of the graph, which
is composed of  and of the potential remaining part
 of , is scheduled on  processors by a PM schedule,
regardless of the  constraint. We denote by  the share
allocated to  and by  the makespan of the
schedule. See Fig.~\ref{fig:hybsu} for an illustration. 
Let  be the graph  and  be the graph .
We denote by   (resp. ) the execution time of  (resp. ) in . Then, .

One can note that the PM schedule  is equal to , where .



\end{defi}
 
 
 
\begin{lemma}
\label{lem:muopt}
For any , under the constraint , the makespan-optimal schedule is .
\end{lemma} 
 
\begin{proof}
Let  be the the makespan-optimal schedule that respects the constraint . We want to show that .

First, suppose that  terminates before  in . This means that a time range is dedicated to schedule  at the end of the schedule. We can modify slightly  by moving this time range to the beginning of the schedule. This is possible as there is no heredity constraint between  and , and the same tasks of  can be performed in the new processor profile, by a PM schedule on . So we now assume that the schedule terminates at the execution of .

Because of ,  must allocate  processors to  at the end of the schedule. In parallel to , only  can be executed, and before the execution of , both subgraphs  and  can be executed.

Suppose that  differs from  in the execution of  in parallel to . Consider the schedule of  fixed, and let  be the number of processors allocated to  at the time  in . As  is scheduled according to PM ratios in , and  differs from this schedule, in ,  is not scheduled according to PM ratios under the processor profile . Then, this schedule can be modified to schedule  in a smaller makespan, and then to schedule the whole graph  in a smaller makespan, which contradicts the makespan-optimality of .

So  and  are equal during the time interval . Then, it remains to schedule the graph , which has a unique optimal schedule, the PM schedule, that is followed by . Therefore, .\qed
\end{proof} 
 

\begin{lemma}
\label{lem:uopt}
 is the makespan-optimal schedule among the  for , i.e., we have .

\end{lemma}



\begin{proof}
Let . We will prove here that .

For the sake of simplification, we denote in this proof  by ,  by ,  by  and  by . We will then consider the schedule , which is makespan-optimal among the , for .

Suppose by contradiction that . We will build an other schedule  following the constraint  for a certain  respecting , that will give a contradiction with the optimality of . 

\newparskip

The following paragraphs prove the inequality , which can be intuitively deduced by an observation of the schedules.

As we have , we know that . The first inequality holds because in , the subgraphs  and  are scheduled in parallel, and each subgraph is scheduled according to the PM ratios. Then, each subgraph {\color{red} can be replaced by its equivalent task}.   Moreover,  (resp. ) processors are allocated to   (resp. ). As , more processors are allocated to , so . 
By the same reasoning between  and  in , we get  and the second inequality holds. See Fig.~\ref{fig:hybsu} for an illustration.

With similar arguments between the subgraphs  and  in the schedule , and using the hypothesis , we get . The difference with the previous case is that the share of processors allocated to both subgraphs is not computed by the PM ratios, but as  is scheduled under  processors with the PM ratios, {\color{red} it can still be replaced by its equivalent task}.

Combining these two inequalities, we have , and by using the same reasoning in the other way with the parallel execution of  and  in , we finally prove .

\begin{figure}[!ht]
\centering
\newcommand{\midy}{60pt}
\newcommand{\topy}{90pt}
\newcommand{\midx}{60pt}
\newcommand{\midlx}{30pt}
\newcommand{\topx}{91pt}

\newcommand{\bota}{70pt}
\newcommand{\leftb}{75pt}
\newcommand{\scalerat}{0.8}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]

\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (0,\bota)-- ++(\leftb,0)
      (\leftb,0) -- ++(0,\bota);


\path (tl)--+(10pt,-10pt) node[label] {};
\path (0,\bota)--+(20pt,-10pt) node[label] {};
\path (tm)--+(10pt,-10pt) node[label] {};

\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.6cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.6cm] 
{\footnotesize };
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [midway,yshift=0.3cm]
{\footnotesize };
\draw [gray,<->]
(0,15pt)--(\leftb,15pt) node [midway,yshift=0.3cm] 
{\footnotesize };

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left,xshift=-0.3cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize };

\phantom{
\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx,-10pt)--(\topx/2,-10pt) node [midway,yshift=-0.6cm] 
{\footnotesize };
}
\end{tikzpicture}}
\hfill
\renewcommand{\midlx}{35pt}
\renewcommand{\leftb}{65pt}
\renewcommand{\bota}{70pt}
\renewcommand{\topy}{100pt}
{\begin{tikzpicture}[scale =\scalerat,
every node/.style={inner sep=0pt, minimum size=0pt, anchor=center, transform shape},
label/.style={}
]

\node (bl) at (0,0) {};
\node (tl) at (0,\topy) {};
\node (br) at (\topx,0) {};
\node (tr) at (\topx,\topy) {};
\node (ml) at (0,\midy) {};
\node (mm) at (\midlx,\midy) {};
\node (mr) at (\midx,\midy) {};
\node (bml) at (\midlx,0) {};
\node (bm) at (\midx,0) {};
\node (tm) at (\midx,\topy) {};
\draw (bl)--(tl)--(tr)--(br)-- (bl) --cycle ;
\draw[gray,dashed] (\topx/2,0) -- ++(0,\topy);
\draw(\midlx,\topy) -- (\midlx,\bota);
\draw (\midlx,\bota)-- (\leftb,\bota)
      (\leftb,0) -- ++(0,\bota);

\draw(0,\bota+10pt)-- ++(\midlx,0);


\path (tl)--+(10pt,-10pt) node[label] {};
\path (0,\bota)--+(20pt,-10pt) node[label] {};
\path (tm)--+(10pt,-10pt) node[label] {};

\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx/2,0)--(0,0) node [midway,yshift=-0.6cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=8pt},yshift=-5pt,xshift=0pt]
(\topx,0)--(\topx/2,0) node [midway,yshift=-0.6cm] 
{\footnotesize };
\draw [gray,<->, yshift=4pt]
(0,\topy)--(\midlx,\topy) node [midway,yshift=0.3cm]
{\footnotesize };
\draw [gray,<->]
(0,15pt)--(\leftb,15pt) node [midway,yshift=0.3cm,fill=white] 
{\footnotesize };

\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,\bota) -- (0,\topy) node [midway,left, xshift=-0.3cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=-2pt,yshift=0pt]
(0,0) -- (0,\bota) node [midway,left,xshift=-0.3cm] 
{\footnotesize };
\draw [gray,decorate,decoration={brace,amplitude=5pt},xshift=2pt,yshift=0pt]
(\topx,\topy) -- (\topx,10pt) node [midway,right, xshift=0.3cm,align=left] 
{\footnotesize  of\\ length\\ };

\draw[pattern=north west lines, pattern color=gray] (\leftb,0) rectangle (\topx,18pt);

\end{tikzpicture}}
 \caption{Schedules  (left) and  (right), assuming that  begins after  in }
\label{fig:hybsb}
\end{figure}

\newparskip

Let  small enough such that  and . Let  and . One can note that .

Let  be the schedule allocating  processors to  during a time  at the end of the schedule, and  processors to  before. 
The subgraph  is scheduled following PM ratios in parallel to , in such a way that it terminates at the same time as  and there is no idle time after the beginning of its execution. The subgraph  is scheduled in the same way as , following PM ratios as soon as its execution begins. 
 One can note that  and  do not necessarily begin simultaneously. See Fig. \ref{fig:hybsb} for an illustration of the case where  begins after . Let  be the makespan of .




As ,  is completed in a time smaller than  in , so  respects the constraint .
Then, by Lemma \ref{lem:muopt}, as , we know that . In addition, by the definition of , we get .

We can assume without loss of generality that  and  are both a unique task. This can be reached by replacing them by their equivalent task, which does not change their execution time. We do not lose generality by this transformation here because both subgraphs are scheduled under the PM rules. For example, we could not consider the equivalent task of the total graph  because constraints of the form , with ,  are followed, and so the PM rules are not respected.


Let  be the interval of time ending when  terminates and having a length of . See Fig. \ref{fig:hybsb} for an illustration.

Let  (resp. ) be the total length of the task  (resp. ) that is executed in  during . Similarly, we define  (resp. ) the total length of the task  (resp. ) that is executed in . These two last quantities are equal to:




As , we must not have simultaneously  and . Indeed, if it was the case, then all the tasks of  would be completed by  in a makespan smaller than , which is a contradiction.
For each task, we separate two cases.

If  begins in  during , then  because the execution of  would hold entirely in .

Otherwise, we have:


We know that . 
Therefore, by the concavity of the function , we have the inequality below. The plot on the right illustrates the inequality. Indeed, the slope of the red segment corresponding to  and  is larger than the other one.

\vspace{.25em}

\begin{minipage}{.36\columnwidth}

\end{minipage}
\begin{minipage}{.58\columnwidth}
\tikzset{
xmin/.store in=\xmin, xmin/.default=-3, xmin=-3,
xmax/.store in=\xmax, xmax/.default=3, xmax=3,
ymin/.store in=\ymin, ymin/.default=-3, ymin=-3,
ymax/.store in=\ymax, ymax/.default=3, ymax=3,
}
\newcommand {\grille}
{\draw[help lines, dashed] (.001,.001) grid (1,1);}
\newcommand {\axes} {
\draw[->] (\xmin,0) -- (\xmax,0);
\draw[->] (0,\ymin) -- (0,\ymax);
}
\newcommand {\fenetre}
{\clip (\xmin,\ymin) rectangle (\xmax,\ymax);}
\newcommand{\fx}[1]{3*#1^3}
\newcommand{\ux}{.4}
\newcommand{\ubx}{.53}
\newcommand{\vbx}{.85}
\newcommand{\vx}{.95}
\begin{tikzpicture}[scale=1.3,xmin=0,xmax=3.3,ymin=0,ymax=1]
\axes \draw[blue] plot[smooth,domain=0:1] (\fx{\x},\x);
\draw (0,\ymax) node[left] {\textit{ \small }} (\xmax,0) node[below] {\textit{\small }};
\node[red,below] at (\fx{\ux},0) {};
\node[red,below] at (\fx{\ubx},0) {};
\node[red,below] at (\fx{\vbx},0) {};
\node[red,below] at (\fx{\vx},0) {};

\draw[red,very thick,|-|] (\fx{\ux},\ux) edge (\fx{\ubx},\ubx)
 (\fx{\vx},\vx) -- (\fx{\vbx},\vbx);

\foreach \aa in {\ux,\ubx,\vx,\vbx}
{
\draw[red, dashed]  (\fx{\aa},0) -- +(0,\aa);
}
\end{tikzpicture}
 \end{minipage}
\vspace{.5em}

Then, we derive from this inequality the fact that  is larger than :


Therefore, in any case, we have 

\newparskip

Then, we treat similarly the subgraph .
If  begins in  during , then .

Otherwise, we have:



Similarly, we know that . Therefore, by the concavity of the function , we have:



Then, in any case, we have both  and , so we get the contradiction.

Therefore, we have  and so .\qed
\end{proof}


\begin{lemma}
\label{lem:slb}

The makespan of  is a lower bound of .

\end{lemma}

\begin{proof}
One can note that in , a constant share  processors must be allocated to  due to \R, as in . Indeed, if this share is not constant, because of the concavity of the function , it would be better to always allocate the mean value to . This would allow to terminate earlier both  and the tasks executed in parallel to  on the same part. {\color{red} See Lemma ???.}
 
Let  be the constraint that enforces a schedule to respect one of the constraint , for any . Otherwise stated,  enforces a schedule to allocate a constant share  not larger than  to .

Let  be the makespan-optimal schedule respecting , and let  be its makespan. As  respects , we have .

Furthermore, there exists  such that . Therefore, , and, by Lemma \ref{lem:uopt}, we get .

Finally, we have .\qed
\end{proof}




We prove the following theorem by induction on .  We are now able
to prove Theorem~\ref{th.43approx}, by induction on the tree
structure. The corresponding approximation algorithm is described in
Algorithm~\ref{alg:hybapp}.

 
\begin{proof}[of Theorem~\ref{th.43approx}]
First, we treat the initialization case, where . An optimal schedule is the one that allocates  processors to the unique task.

Then, we treat the cases that do not need the heredity property.
\begin{itemize}
\item if  and  is a leaf, no schedule can have a makespan smaller than , as no more than  processors can be allocated to . Then, the schedule that only differs from \spm by reducing the share allocated to  to  is makespan-optimal.
\item if , the result is given by Lemma \ref{lem:x<1}. 
\end{itemize}

Now, we suppose the result true for . The case remaining is when  is not a leaf and . Consider such a graph  of  nodes.





We consider the schedule , whose makespan  is a lower bound of \mopt as stated in Lemma \ref{lem:slb}.  




We now build the schedule , which achieves a -approximation respecting .
At the end of the schedule,  is scheduled as in . At the beginning of the schedule, we use the heredity property to derive from  a schedule of  that follows the \R constraint.


More formally, we have , which is the parallel composition , composed of at most  nodes. So, by induction, a schedule  achieving a \frtrd-approximation can be computed for . This means that its makespan  is at most , as  completes  with PM ratios in a time , which is then the optimal time.

Consider the schedule  of  that schedules  as in , then schedules  as in . The time necessary to complete  is then equal to .
The makespan  of  respects then:
 
 
  Then,  is a -approximation.\qed
\end{proof}




\begin{figure}[Ht]
\begin{algorithmic}[1]
 \Require{A graph , the parameter  of the processor platform }
 \Ensure{A schedule  of  on  that is a \frtrd-approximation of the makespan}
\Function{HybApp}{,}
\State 
\State Modify  as in Lemma \ref{lem:chainroot} 
\State Compute the PM schedule  of  on  processors
\State Compute the 's, the 's, , and 
 \If{ and  is a leaf}
 \State	Build : shrink from  the share of processors allocated to  to  processors
 	
 \ElsIf{}
 \State	Build : map the 's as in Lemma \ref{lem:x<1}, and compute the PM schedule on each part
 	
 \Else
 \Comment{we have  and  is not a leaf}
\State Compute the schedule  and partition  in  and  as in Definition \ref{def:su}
\State 
\State Build : schedule  as in  then  as in 
 \EndIf
 \State Adapt the schedule  to the original graph  if 
 \State \Return 
 \EndFunction
\end{algorithmic}
 \caption{Approximation algorithm for the hybrid problem}
 \label{alg:hybapp}
\end{figure}




\paragraph{Worst case of the algorithm}

Let  be the parallel composition of  identical subtrees , ,  (plus the root of length 0). Each subtree  is composed of  tasks: a root  of length , and  leaves  and  each of length .

The algorithm follows the second case, and place  of these trees on one part. The makespan obtained is then


Now, consider the schedule that places two roots on one part, but  tasks  on each part at the beginning. Its makespan is then


When  is close to , we get


So the approximation ratio is tight. 
\fi 

\subsection{Two Heterogeneous Multicore Nodes}
\label{sec:dist-het}

We suppose here that the computing platform is made of two processors
of different processing capabilities: the first one is made of 
cores, while the second one includes  cores. We also assume that
the parameter  of the speedup function is the same on both
processors. As the problem gets more complicated, we concentrate here
on  independent tasks,  of lengths . Thanks to the
homogenous case presented above, we already know that scheduling
independent tasks on two nodes is NP-complete.

\iflong \else This problem is close to the \textsc{Subset Sum} problem. Given 
numbers, the optimization version of \textsc{Subset Sum} considers a
target  and aims at finding the subset with maximal sum smaller
than or equal to .  There exists many approximation schemes for
this problem. In particular, Kellerer et
al.~\cite{kellerer2003efficient} propose a fully polynomial
approximation scheme (FPTAS). Based on this result, an approximation
scheme can be derived for our problem.
\begin{theorem}
  There exists an FPTAS for the problem of scheduling independent
  malleable tasks on two heterogeneous nodes, provided that, for each
  task,  is an integer.
\end{theorem}

The proof is complex and detailed in~\cite{RR-ipdps-2014}. The
assumption on the s is needed to apply the FPTAS of
\textsc{Subset Sum}, which is valid only on integers.
\fi 

\iflong \begin{proof}
We reduce the problem to {\sc Subset sum}, which is known to be NP-Complete \cite{gareyjohnson}.

Consider an instance  to {\sc Subset sum}. We have a set , a number , and we want to know if there exists a subset of  that sums to , assuming that the total sum is larger.

We construct an instance  to -scheduling. Let  have lengths , , , and . We recall that raising a number to the power  is assumed feasible, so the computation of the s is done in polynomial time. Note that  is the optimal makespan without the  constraint, and so only the PM schedule can be a solution.

It now remains to prove that  is satisfiable if and only if  is satisfiable.
Let  be the share of processors allocated to task  in the PM schedule. We have 

Then,  is satisfiable if and only if a subset of the   sums to . This is equivalent to say that a subset of  sums to , i.e., that  is satisfiable.\qed
\end{proof}



We will denote by  the subset of the indices of the tasks allocated
to the -part, and by  the complementary of . Then, the
schedule that partition the tasks according to the subset  and
performing a PM schedule on both parts is denoted by .



The -scheduling problem is strictly equivalent to {\sc Subset sum} only when we restrict ourselves to instances where the PM schedule is compatible with the -constraint. In the general case, the minimum-makespan schedule is reached when  the total idle time is minimized and the schedule is PFC.

This is equivalent to pack all the tasks in two sets  and  such that the difference between the time needed to complete  with  processors and the one needed to complete  with  processors is minimized, i.e.:




Note that the makespan of the schedule associated to the subset  is:





Consider an instance of -scheduling. Let  be , where  and  be the set .


\begin{lemma}
A tight lower bound of the optimal makespan is . In such a schedule, we have . We denote this quantity by .
\end{lemma}

\begin{proof}
 is the makespan of the PM schedule, which is a lower bound of the optimal makespan, and can be reached, as in the proof of Theorem \ref{th:pqnpc}.\qed
\end{proof}

We note .


For , a -approximation of {\sc Subset sum} returns a subset  of  such that the sum of its elements ranges between  and , where 

For , a -approximation of the -scheduling problem returns a schedule such that its makespan is not larger than  times the optimal makespan.
We note  and .

An AS \A resolving {\sc Subset Sum} is defined as followed. Given an instance  of {\sc Subset Sum} and a parameter , it computes a solution to  achieving a -approximation in a time complexity . 

An AS \B resolving -scheduling is defined as followed. Given an instance  of the -scheduling problem and a parameter , it computes a solution to  achieving a -approximation in a time complexity .

\newcommand{\mtgamrap}{\max\left(3,\left\lceil\frac{1}{\ek}-4\right\rceil\right)}

\begin{remark}
There exist a FPTAS for {\sc Subset Sum}.



In [Kellerer et al'02] "An efficient FPTAS for the subset sum" a FPTAS of time complexity  and space complexity  is proposed.

\end{remark}



\begin{defi}
\label{def:pqrest}
The \pqres problem is defined from the \pqsched problem by replacing the entries  by , and is restricted to the case where the  are integers.
\end{defi}






\begin{theorem}
\label{th:pqptas}
Given a AS \A of {\sc Subset Sum} of time complexity , Algorithm \ref{alg:pqapp} performs a AS to \pqres with time complexity .
\end{theorem}

\begin{coro}
The \pqres problem admits a AS of time complexity  and space complexity .

Indeed, parametrized by the FPTAS of [Kellerer'02], Algorithm \ref{alg:pqapp} is an AS of the \pqres problem of such a complexity.

\end{coro}


\begin{proof}

Let  be an instance of \pqres,  and \A be an AS of {\sc Subset Sum}.

We recall that raising a number to the power  or  is assumed feasible.

If , it suffices to compute the PM schedule on the largest part of the platform.
We assume in the following that .


We define , so that . One can check that .

A tight lower bound on the makespan is . Indeed, it represents the makespan of the PM schedule on  processors, which can respect the constraint for some values of the s. 


 Let \sopt be an optimal schedule of . If we denote by  the subset of the tasks allocated to the -part of the platform, we have either:



We first suppose that the left inequality holds. The other case is treated at the end of the proof.


Then,


So the -part of the schedule terminates before the ideal schedule. Therefore, the -part of the schedule terminates after the -part as .
We denote .


Then, the makespan  is equal to the time needed to complete the tasks of . Then we have 


Let  be the set of subsets f : 





We now prove in the following paragraphs that a subset  is computed by the algorithm \A launched on , , and . First, we recall that the  are assumed to be integers in the formulation of \pqres;

We know that , so .
Then, there does not exist any subset  of  such that , because the associated schedule  would have a makespan smaller than , which contradicts the optimality of .

So  is an optimal solution to the instance submitted to .
Therefore, \A launched on this instance with the parameter  will return a set  in time , and so the claim is proved.


\newparskip


Let  be an element of .
We know that the makespan  of the corresponding schedule  allocating the tasks corresponding to  on the -part is:




We have



and 



So



and 




This last inequality implies that the tasks allocated to the -part of the platform are terminated before , and so necessarily before the tasks allocated to -part. Therefore, we have .

and so


Then, as  and , we get



Then,  is an upper bound of , so



Finally, by the definition of , we get




\newparskip

We have supposed so far that the left inequality of (\ref{eq:apq}) holds. Otherwise, the second one holds. Note that both hypotheses only differ by an exchange of the roles of  and . Then, as the problem is strictly symmetric in  and , by an analogue reasoning, one can prove that \A launched on , ,  returns a set  in 



and that the schedule that associates  to the -part of the processors has a makespan smaller than . Indeed, we needed to obtain this conclusion that , and as we also have , the  same method works in this case.

\newparskip

To conclude, Algorithm \ref{alg:pqapp} launched with the parameter  computes a set  and a set , then returns the schedule that has the minimum makespan between  and . Therefore, regardless of which inequality of (\ref{eq:apq}) holds, the returned schedule has a makespan smaller than , and so Algorithm \ref{alg:pqapp} achieves a -approximation.\qed 
\end{proof}


Formally, the algorithm is the following.

\begin{figure}[!ht]


\SetEndCharOfAlgoLine{;}

\begin{algorithm}[H]

\SetKwFunction{pqapp}{PQApp}
\SetKwBlock{Fct}{Function \pqapp{,,,}}{}
\SetKwInOut{KwOut}{Output}
\SetKwInOut{KwIn}{Input}

\SetKwIF{AFct}{AElseIf}{AElse}{if}{then}{else if}{else}{endif}
\SetVline
\Fct{
 \KwIn{A graph  composed of  independent tasks  of length , the parameters  and  of the processor platform , and the requested approximation ratio }
 \KwOut{A schedule  of  on  that is a -approximation of the makespan}
\If{}{
	\Return{the PM schedule on the largest part}}

  
 \;
\Return{the schedule with the minimum makespan between  and }
 }
 \vspace*{-1em}
 \caption{Approximation scheme for the -scheduling problem \mystrut(1.5em)}
 \label{alg:pqapp}
\end{algorithm}
\end{figure}





\fi 





























   











\section{Conclusion}

In this paper, we have studied how to schedule trees of malleable
tasks whose speedup function on multicore platforms is . We
have first motivated the use of this model for sparse matrix
factorizations by actual experiments. When using factorization kernels
actually used in sparse solvers, we show that the speedup follows the
 model for reasonable allocations. On the machine used for
our tests,  is in the range 0.85--0.95. Then, we proposed a
new proof of the optimal allocation derived by Prasanna and
Musicus~\cite{prasmus,prasmus2} for such trees on single node
multicore platforms. Contrarily to the use of optimal control theory
of the original proofs, our method relies only on pure scheduling
arguments and gives more intuitions on the scheduling problem. Based
on these proofs, we proposed several extensions for two multicore
nodes: we prove the NP-completeness of the scheduling problem and propose
a \frtrd-approximation algorithm for a tree of malleable tasks on two
homogeneous nodes, and an FPTAS for independent malleable tasks on two
heterogeneous nodes. 



The perspectives to extend this work follow two main
directions. First, it would be interesting to extend the
approximations proposed for the heterogeneous case to a number of nodes
larger than two, and to more heterogeneous nodes, for which the value
of  differs from one node to another. This is a promising
model for the use of accelerators (such as GPU or Xeon Phi). The
second direction concerns an actual implementation of the PM
allocation scheme in a sparse solver. 


\bibliographystyle{splncs03}
\bibliography{europar}
\end{document}
