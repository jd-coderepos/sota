\documentclass[acmtog]{acmart}
\acmSubmissionID{221s2}

\usepackage{soul}
\usepackage{booktabs} 

\citestyle{acmauthoryear}




\usepackage[ruled]{algorithm2e} \renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}

\acmJournal{TOG}
\acmVolume{39}
\acmNumber{6}
\acmArticle{263} \acmYear{2020}
\acmMonth{12}



\acmDOI{10.1145/3414685.3417806}



\newcommand*{\ShowNotes}{}
\ifdefined\ShowNotes
  \newcommand{\colornote}[3]{{\color{#1}{#2 #3}\normalfont}}
\else
  \newcommand{\colornote}[3]{}
\fi




\begin{document}
\title{MeshWalker: Deep Mesh Understanding by Random Walks}

\author{Alon Lahav}
\affiliation{\institution{Technion – Israel Institute of Technology}
}
\email{alon.lahav2@gmail.com}
\author{Ayellet Tal}
\affiliation{\institution{Technion – Israel Institute of Technology}
}
\email{ayellet@ee.technion.ac.il}




\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010371.10010396.10010402</concept_id>
       <concept_desc>Computing methodologies~Shape analysis</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010258.10010259</concept_id>
       <concept_desc>Computing methodologies~Supervised learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Shape analysis}
\ccsdesc[500]{Computing methodologies~Supervised learning}


\begin{abstract}
Most attempts to represent  3D shapes for deep learning have focused on volumetric grids, multi-view images and point clouds.
In this paper we look at the most popular representation of 3D shapes in computer graphics---a triangular mesh---and ask how it can be utilized within deep learning.
The few attempts to answer this question propose to adapt convolutions \& pooling to suit {\em Convolutional Neural Networks (CNNs)}.
This paper proposes a very different approach, termed {\em MeshWalker} 
to learn the shape directly from a given mesh.
The key idea is to represent the mesh by random walks along the surface, which "explore" the mesh's geometry and topology.
Each walk is organized as a list of vertices, which in some manner imposes regularity on the mesh.
The walk is fed into  a {\em Recurrent Neural Network (RNN)} that "remembers" the history of the walk.
We show that our approach achieves state-of-the-art results for two fundamental shape analysis tasks: shape classification and semantic segmentation.
Furthermore, even a very small number of examples suffices for learning.
This is highly important, since large datasets of meshes are difficult to acquire.
\end{abstract}


\keywords{Deep Learning, Random Walks}

\begin{teaserfigure}
\centering  
\fbox{\includegraphics[width=0.97\textwidth, trim={0cm 4.0cm 0cm 3.0cm}, clip]{images/teaser-fig.pdf}}
  \caption{{\bf Classification by MeshWalker.}
    This figure shows classification results as the walk (in green) proceeds along the surface of a camel ( faces) from SHREC11~\cite{lian2011shape}.
    The initial point was randomly chosen on the neck.
    After  steps (left),  being the number of vertices, the system is uncertain regarding the class, and the highest probability predictions are for the flamingo class and for the hand class (out of  classes).
    After continuing the random walk along the body and the front leg for  steps, the probability of being a horse is higher than before, but the camel already has quite a high probability.
    Finally, after  steps (right) and walking also along the hump, the system correctly classifies the model as a camel.
     }
\label{fig:teaser}
\end{teaserfigure}

\maketitle

\section{Introduction}
\label{sec:introduction}

The most-commonly used representation of surfaces in computer graphics is a polygonal mesh, due to its numerous benefits, including efficiency and high-quality.
Nevertheless, in the era of deep learning, this representation is often bypassed because of its irregularity, which does not suit {\em Convolutional Neural Networks (CNNs)}.
Instead, 3D data is often represented as volumetric grids~\cite{maturana2015voxnet, DBLP:journals/corr/AlvarZB16, roynard2018classification, ben20183dmfv} or multiple 2D projections~\cite{su2015multi, boulch2017unstructured, 
Feng_2018_CVPR, DBLP:journals/corr/abs-1811-01571, kanezaki2018rotationnet}.
In some recent works point clouds are utilized and new ways to convolve or pool are proposed~\cite{atzmon2018point, xu2018spidercnn, li2018pointcnn, hua2018pointwise, thomas2019kpconv}.

Despite the benefits of these representations, they miss the notions of neighborhoods and connectivity and might not be as good for capturing local surface properties.
Recently, several works have proposed to maintain the potential of the mesh representation, while still utilizing neural networks.
FeaStNet~\cite{verma2018feastnet} proposes a graph neural network in which the neighborhood of each vertex for the convolution operation is calculated dynamically based on its features.
MeshCNN~\cite{hanocka2019meshcnn} defines pooling and convolution layers over the mesh edges.
MeshNet~\cite{feng2019meshnet} treats the faces of a mesh as the basic unit and extracts their spatial and structural features individually to offer the ﬁnal semantic representation.
LRF-Conv~\cite{yang2020continuous} learns descriptors directly from the raw mesh by defining new continuous convolution kernels that provide robustness to sampling.
All these methods redefine the convolution operation, and by doing so, are able to fit the unordered structure of a mesh to a CNN framework.

We propose a novel and fundamentally different approach,  named {\em MeshWalker}.
As in previous approaches that learn directly from the mesh data, the basic question is how to impose regularity on the unordered data.
Our key idea is to represent the mesh by random walks on its surface.
These walks explore the local geometry of the surface, as well as its global one.
Every walk is fed into a {\em Recurrent Neural Network (RNN)}, that "remembers" the walk's history.

In addition to simplicity, our approach has three important benefits. 
First, we will show that even a small dataset suffices for training.
Intuitively, we can generate multiple random walks for a single model; these walks provide multiple explorations of the model. 
This may be considered as equivalent to using different projections of 3D objects in the case of image datasets.
Second, as opposed to CNNs, RNNs are inherently robust to sequence length.
This is vital in the case of meshes, as datasets include objects of various granularities.
Third, the meshes need not be watertight or have a single connected component; our approach can handle any triangular mesh.

Our approach is general and can be utilized to address a variety of shape analysis tasks.
We demonstrate its benefit in two basic applications: mesh classification and mesh semantic segmentation.
Our results are superior to those of state-of-the-art approaches on common datasets and on highly non-uniform meshes.
Furthermore, when the training set is limited in size, the accuracy improvement over the state-of-the-art methods is highly evident.

Hence, this paper makes three contributions:
\begin{enumerate}
    \item 
    We propose a novel representation of meshes for neural networks: random walks on surfaces.
    \item
    We present an end-to-end learning framework that realizes this representation within RNNs.
    We show that this framework works well even when the dataset is very small.
    This is important in the case of 3D, where large datasets are seldom available and are difficult to generate.
    \item 
    We demonstrate the benefits of our method in two key applications: 3D shape classification and semantic segmentation.
\end{enumerate}

\section{Related Work}
\label{sec:relatedwork}
Our work is at the crossroads of three fields, as discussed below.

\subsection{Representing 3D objects for Deep Neural Networks}
\label{subsec:related_work_3d_representations}
A variety of representations of 3D shapes have been proposed in the context of deep learning.
The main challenge is how to re-organize the shape description such that it could be processed within deep learning frameworks. 
Hereafter we briefly review the main representations;
see~\cite{gezawa2020review} for a recent excellent survey.
 
\paragraph{Multi-view 2D projections.}
This representation is essentially a set of 2D images, each of which is a rendering of the object from a different viewpoint~\cite{su2015multi, kalogerakis20173d, qi2016volumetric, sarkar2018learning, gomez2017lonchanet, johns2016pairwise, zanuttigh2017deep, bai2016gift, wang2019dominant, kanezaki2018rotationnet, feng2018gvcnn, he2018triplet, han20193d2seqviews}.
The major benefit of this representation is that it can naturally utilize any image-based CNN. 
In addition, high-resolution inputs can be easily handled. 
However,  it is not easy to determine the optimal number of views; if that number is large, the computation might be costly.
Furthermore, self-occlusions might be a drawback.

\paragraph{Volumetric grids.}
These grids are analogous to the 2D grids of images.
Therefore, the main benefit of this representation is that operations that are applied on 2D grids can be extended to 3D  in a straightforward manner~\cite{wu20153d, brock2016generative, tchapmi2017segcloud, fanelli2011real, maturana2015voxnet, wang2019normalnet, sedaghat2016orientation, zhi2018toward}.
The primary drawbacks of volumetric grids are their limited resolution and the heavy computation cost needed.

\paragraph{Point clouds.}
This representation consists of a set of 3D points, sampled from the object's surface. 
The simplicity, close relationship to data acquisition, and the ease of conversion from other representations, make  point clouds an attractive representation.
Therefore, a variety of recent works proposed successful techniques for point cloud shape analysis using neural networks~\cite{qi2017pointnet, qi2017pointnet++, wang2019dynamic, guerrero2018pcpnet, williams2019deep, atzmon2018point, li2018pointcnn, liu2019relation, xu2019geometry, zhu2019random}.
These methods attempt to learn a representation for each point, using its neighbors (Euclidean-wise) either by multi layer perceptions or by convolutional layers.
Some also  define novel pooling layers.
Point cloud representations might fall short in applications when the connectivity is highly meaningful (e.g. segmentation) or when the salient information is concentrated in small specific areas.

\paragraph{Triangular meshes.}
This representation is the most widespread representation in computer graphics and the focus of our paper.
The major challenge of using meshes within deep learning frameworks is the irregularity of the representation---each vertex has a different number of neighbors, at different distances.

The pioneering work of~\cite{masci2015geodesic} introduces deep learning of local features and shows
 how to make the convolution operations intrinsic to the mesh.
In~\cite{poulenard2018multi} a new convolutional layer is defined, which allows the propagation of geodesic information throughout the network layers.
FeaStNet~\cite{verma2018feastnet} proposes a graph neural network in which the neighborhood of each vertex for the convolution operation is calculated dynamically based on its features.
Another line of works exploits the fact that local patches are approximately Euclidean. 
The 3D  manifolds are then parameterized  in 2D, where standard CNNs are utilized~\cite{henaff2015deep, sinha2016deep, boscaini2016learning, maron2017convolutional, ezuz2017gwcnn, haim2019surface}.
A different approach is to apply a linear map to a spiral of neighbors~\cite{gong2019spiralnet++, lim2018simple}, which works well for meshes with a similar graph structure.


\begin{figure*}[tb]
\centering 
\begin{tabular}{ccc}
\includegraphics[height=0.33\textwidth]{images/2nd_fig_a.png} &
\includegraphics[height=0.33\textwidth]{images/2nd_fig_b.png} &
\includegraphics[height=0.33\textwidth]{images/2nd_fig_c.png}  \\
(a)  walks on the surface & (b) Classification: Samples from the class the input belongs to& (c) Semantic segmentation
\end{tabular}
\caption{{\bf Outline.}
To explore a mesh, walks on its surface are generated and study the surface both locally and globally (a).
These walks provide sufficient information to perform shape analysis tasks, such as classification and segmentation.
Specifically,~(b) shows samples from the class to which MeshWalker correctly classified the model from~(a) and~(c) shows the resulting segmentation.
The models are from SHREC11~\cite{lian2011shape}.
}
\label{fig:outline} 
\end{figure*}


Two approaches were recently introduced:
MeshNet~\cite{feng2019meshnet} treats faces of a mesh as the basic unit and  extracts their spatial and structural features individually, to offer the final semantic representation.
MeshCNN~\cite{hanocka2019meshcnn} is based on a very unique idea of using the edges of the mesh to perform pooling and convolution. 
The convolution operations exploit the regularity of edges---having  edges of their incidental triangles.
An edge collapse operation is used for pooling, which maintains surface topology and generates new mesh connectivity for further convolutions.

\subsection{Classification}
\label{subsec:related_work_classification}
Object classification refers to the task of classifying a given shape into one of pre-defined categories.
Before deep learning methods became widespread, the main challenges were finding good descriptors and good distance functions between these descriptors.
According to the thorough  review of~\cite{lian2013comparison}, the methods could be roughly classified into algorithms employing local features~\cite{johnson1999using, lowe2004distinctive, liu2006shape, sun2009concise, ovsjanikov2009shape}, topological structures~\cite{hilaga2001topology, sundar2003skeleton, tam2007deformable}, isometry-invariant global geometric properties~\cite{reuter2005laplace, jain2007spectral, mahmoudi2009three}, 
direct shape matching, or canonical forms~\cite{memoli2005theoretical, memoli2007use, bronstein2006efficient, elad2003bending}.

Many of the recent techniques already use deep learning for classification.
They are described in Section~\ref{subsec:related_work_3d_representations}, for instance~\cite{hanocka2018alignet, qi2017pointnet, qi2017pointnet++, li2018pointcnn, ezuz2017gwcnn, bronstein2011shape, feng2019meshnet, thomas2019kpconv, liu2019relation, velivckovic2017graph, wang2019graph, kipf2016semi, perozzi2014deepwalk}.

\subsection{Semantic segmentation}
\label{subsec:related_work_segmentatio}
Mesh segmentation is a key ingredient in many computer graphics tasks, including modeling, animation and a variety of shape analysis tasks.
The goal is to determine, for the basic elements of the mesh (vertex, edge or face), to which segment they belong.
Many approaches were proposed, including region growing~\cite{chazelle1997strategies, lavoue2005new, zhou2004decomposing, koschan2003perception, sun2002triangle, katz2005mesh},
clustering~\cite{shlafman2002metamorphosis, katz2003hierarchical, gelfand2004shape, attene2006hierarchical}, spectral analysis \cite{alpert1995spectral, gotsman2003graph, liu2004segmentation, zhang2005mesh} and more.
See~\cite{Attene_06,shamir2008survey,rodrigues2018part} for excellent surveys of segmentation methods.

Lately, deep learning has been utilized for this task as well.
Each proposed approach handles a specific shape representation, as described in Section~\ref{subsec:related_work_3d_representations}.
These approaches include among others ~\cite{hanocka2018alignet, qi2017pointnet, qi2017pointnet++, li2018pointcnn, yang2020continuous, haim2019surface, maron2017convolutional, qi2017pointnet++, guo20153d}. 


\section{MeshWalker outline}
\label{sec:outline}

Imagine an ant walking on a surface; it will "climb" on ridges and go through valleys. 
Thus, it will explore the local geometry of the surface, as well as the global terrain. 
Random walks have been shown to incorporate both global and local information about a given object~\cite{Lai:2008:FMS:1364901.1364927,lovasz1993random, grady2006random, noh2004random}. 
This information may be invaluable for shape analysis tasks, nevertheless, random walks have not been used to represent meshes within a deep learning framework before. 

Given a polygonal mesh, we propose to randomly walk through the vertices of the mesh, along its edges,
as shown in Fig.~\ref{fig:outline}(a).
In our ant analogy, the longer the walk, the more information is acquired by the ant.
But how shall this information be accumulated?
We propose to feed this representation into a Recurrent Neural Network (RNN) framework, which aggregates properties of the walk.
This aggregated information will enable the ant to perceive the shape of the mesh.
This is particularly beneficial for shape analysis tasks that require both the 3D global structure and some local information of the mesh, as demonstrated in Fig.~\ref{fig:outline}(b-c).

Algorithm~\ref{alg:MeshWalkerTraining} describes the training procedure of our proposed {\em MeshWalker} approach. 
A defining property of it is that the \textbf{same} piece of algorithm is used for every vertex along the walk (i.e., each vertex the ant passes through).
The algorithm  iterates on the following:
A mesh is first extracted from the dataset (it could be a mesh that was previously extracted).
A vertex is chosen randomly as the head of the walk and then a random walk is generated.
This walk is the input to an RNN model.
Finally, the RNN model's parameters~ are updated by minimizing the {\em Softmax} cross entropy loss , using Adam optimizer~\cite{kingma2014adam}. 

\begin{algorithm}[t]
\SetAlgoNoLine
\KwIn{Labeled mesh dataset,  }
\KwOut{---RNN model parameters}
 random parameters\;
 MeshPreprocessing()\;
\Repeat{Convergence} 
    {
         random mesh  and label(s) \;
         random starting vertex\;
        \;
        \;
\;
    }
\caption{MeshWalker Training}
\label{alg:MeshWalkerTraining}
\end{algorithm}


Section~\ref{sec:model} elaborates on the architecture of our MeshWalker learning model, as well as on each of the  ingredients of the iterative step.
Section~\ref{subsec:preprocessing} explains the mesh pre-processing step, which essentially performs mesh simplification, and provides implementation details.

\section{Learning to walk over a surface}
\label{sec:model}
This section explains how to realize Algorithm~\ref{alg:MeshWalkerTraining}.
It begins by elaborating on the construction of a random walk on a mesh.
It then proceeds to describe the network that learns from walks in order to understand meshes.


\begin{figure*}[ht]
\centering 
\includegraphics[width=0.9\textwidth]{images/network-9} 
\caption{{\bf Network architecture.} 
The network consists of three components: 
The first component (FC layers) changes the feature space; the second component (RNN layers) aggregates the information along the walk; and the third component (an FC layer) predicts the outcome of the network. 
For classification, the prediction of the last vertex of the walk is considered and {\em Softmax} is applied to its resulting vector (the bottom-right orange circle, classified as a camel).
For segmentation (not shown in this figure), the network is similar.
However, {\em Softmax} is applied to each of the resulting vectors of the vertices (the orange circles in the right column); each vertex is classified into a segment.
}
 \label{fig:NetworkArch}
\end{figure*}

\subsection{What is a walk?}
Walks provide a novel way to organize the mesh data.
A {\em walk} is a sequence of vertices (not necessarily adjacent), each of which is associated with basic information.

\paragraph{Walk generation.}
We adopt a very simple strategy to generate walks, out of many possible ones.
Recall that we are given the first vertex  of a walk.
Then, to generate the walk , the other vertices are iteratively added, as follows.
Given the current vertex of the walk, the next vertex is chosen randomly from its adjacent vertices (those that belong to its one-ring neighbors).

If such a vertex does not exist (as all the neighbors already belong to the walk),  the walk is tracked backwards until an un-visited neighbor is found; this neighbor is added to the walk. 
In this case, the walk is not a linear sequence of vertices connected via edges, but rather a tree.
If the mesh consists of multiple connected component, it is possible that the walk reaches a dead-end.
In this case, a new random  un-visited vertex is chosen and the walk generation proceeds as before.
We note that in all cases, the input to the RNN is a sequence of vertices, arranged by their discovery order.
In practice,  the length of the walk is set by default to , where  is number of vertices. 

\paragraph{Walk representation.}
 Once the walk  is determined, the representation  of this walk should be defined; this would be the input to the RNN. 
Each vertex is represented as the 3D translation from the previous vertex in the walk ().
This is inline with the deep learning philosophy, which prefers end-to-end learning instead of hand-crafted features that are separated from a classifier, 
We note that we also tried other representations, including vertex coordinates, normals, and curvatures, but the results did not improve.

\paragraph{Walks at inference time.}
At inference, several walks are being used for each mesh.
Each walk produces a vector of probabilities to belong to the different classes (in the case of classification).
These vectors are averaged to produce the final result.
To understand the importance of averaging, let us consider the walks on the camel in Fig.~\ref{fig:teaser}.
Since walks are generated randomly, we expect some of them to explore atypical parts of the model, such as the legs, which are similar to horse legs.
Other walks, however, are likely to explore unique parts, such as the hump or the head.
The average result will most likely be the camel, as will be shown in Section~\ref{sec:applications}.



\subsection{Learning from walks}
\label{subsec:learning}
Once walks are defined, the next challenge is 
to distillate the information accumulated along a walk into a single descriptor vector.
Hereafter we  discuss the network architecture and the training. 

\paragraph{Network architecture.}
The model consists of three sub-networks, as illustrated in  Fig.~\ref{fig:NetworkArch}.
The first sub-network is given the current vertex of the walk and learns a new feature space, i.e.  it transforms the 3D input feature space into a 256D feature space.
This is done by two fully connected (FC) layers, followed by an {\em instance normalization}~\cite{ulyanov2016instance} layer and  {\em ReLu} as nonlinear activation;
both empirically outperform other alternatives.


The second sub-network is the core of our approach.
It utilizes a recurrent neural network (RNN) whose defining property is being able to "remember" and accumulate knowledge.
Briefly, a recurrent neural network~\cite{graves2008novel,hochreiter1997long,cho2014learning} is a  connectionist model that contains a self-connected hidden layer. 
The benefit of self-connection is that the ‘memory’ of previous inputs remains in the network’s internal state, allowing it to make use of past context.
In our setting, the RNN gets as input a feature vector  (the result of the previous sub-network), learns the hidden states that describe the walk up to the current vertex, and outputs a state vector that contains the  information gathered along the walk. 

Another benefit of RNNs, which is crucial in our case, is not being confined to fixed-length inputs or outputs.
Thus, we can use the model to inference on a walk of a certain length, which may differ from walk lengths the model was trained on.

To implement the RNN part of our model, we use three {\em Gated Recurrent Unit (GRU)} layers of~\cite{cho2014learning}.
Briefly, the goal of an GRU layer is to accumulate only the important information from the input sequence and to forget the non-important information.

Formally, let  be the input at time  and  be the hidden state at time ;
let the {\em reset gate}  and the {\em update gate}  be two vectors, which jointly decide which information should be passed from time - to time .
To realize GRU's goal, the network performs the following calculation, which sets the hidden state at time .
Its final content is based on updating the hidden state in the previous time (the {\em update gate}  determines which information should be passed) and on its candidate memory content :

where  is an element-wise multiplication.
Here,  is defined as:

That is, when the reset gate is close to , the hidden state ignores the previous hidden state and resets with the current input only.
This effectively allows the hidden state to drop any information that will later be found to be irrelevant.

Finally,  the {\em reset gate}  and the {\em update gate}  are defined as:


where  is a logistic Sigmoid function. 
 and  are trainable weight matrices and   are trainable bias vectors.
The initial hidden state  is set to .

GRU  outperforms a vanilla RNN, due to its ability to both remember the important information along the sequence and to forget unimportant content.
Furthermore, it is capable of processing long sequences, similarly to the {\em Long Short-Term Memory (LSTM)}~\cite{hochreiter1997long}.
Being able to  accumulate information from long sequences is vital for grasping the shape of a 3D model, which usually consists of thousands of vertices. 
We chose GRU over LSTM due to its simplicity and its smaller computational requirements.
For comparison, LSTM would require  trainable parameters in our case, whereas  uses .
Furthermore, the inference time is smaller---for instance, a single -steps walk takes  using LSTM and  using GRU.

The third sub-network in Fig.~\ref{fig:NetworkArch} predicts the object class in case of classification,  
or the vertex segment in case of semantic segmentation. 
It consists of a single fully connected (FC) layer on top of the state vector calculated in the previous sub-network.
More details on the architectures \& the implementation are given in Section~\ref{sec:experiments}.

\paragraph{Loss calculation.}
The {\em Softmax} cross entropy loss is used on the output of the third part of the network.
In the case of the classification task, only the last step of the walk is used as input to the loss function, since it accumulates all prior information from the walk. 
In Fig.~\ref{fig:NetworkArch}, this is the bottom-right orange component.

In the case of the segmentation task,
each vertex has its own predicted segment class.
Each of the orange components in Fig.~\ref{fig:NetworkArch} classifies the segment that the respected vertex belongs to. 
Since at the beginning of the walk the results are not trustworthy (as the mesh is not yet well understood), for the loss calculation in the training process we consider the segment class predictions only for the vertices that belong to the second half of the walk.


\section{Applications: Classification \& Segmentation}
\label{sec:applications}
MeshWalker is a general approach, which may be applied to a variety of applications.
We demonstrate its performance for two fundamental tasks in shape analysis: mesh classification and mesh semantic segmentation. 
Our results are compared against the {\em reported} SOTA results for recently-used datasets, hence the methods we compare against vary according to the specific dataset.


\subsection{Mesh classification}
\label{subsec:classification}
Given a mesh, the goal is to classify it into one of pre-defined classes.
For the given mesh we generate multiple random walks.
These walks are run through the trained network. 
For each walk, the network predicts the probability of this mesh to belong to each class.
These prediction vectors are averaged into a single prediction vector.
In practice we use  walks; Section~\ref{sec:experiments} will discuss the robustness of MeshWalker to the number of walks. 

To test our algorithm, we applied our method to three recently-used datasets: SHREC11~\cite{lian2011shape}, engraved cubes ~\cite{hanocka2019meshcnn} and ModelNet40~\cite{wu20153d}, which differ from each other in the number of classes, the number of objects per class, as well as the type of shapes they contain.
As common, the accuracy is defined as the ratio of correctly predicted meshes.

\paragraph{SHREC11.}
This dataset consists of  classes, with 20 examples per class.
Typical classes are camels, cats, glasses, centaurs, hands etc.
Following the setup of~\cite{ezuz2017gwcnn}, we split the objects in each class into  (/)  training examples and   (/) testing examples.

Table~\ref{tbl:shrec11} compares the performance, where each result is the average of the results of  randoms splits (of  or of ). 
When the split is  objects for training and  for testing, the advantage of our method is apparent. 
When  objects are used for training and only  for testing, we get the same accuracy as that of the current state-of-the-art. 
In Section~\ref{subsec:size_training_set} we show that indeed the smaller the training dataset, the more advantageous our approach is.

\begin{table}[htb]\caption{{\bf Classification on SHREC11~\cite{lian2011shape}.}
 Split- and Split- are the number of training models per class (out of  models in the class).
 In both cases our method achieves state-of-the-art results, yet it is most advantageous for a small training dataset (Split-).
 (We have not found point cloud-based networks that were tested on SHREC11).
 }
\begin{center}
 \begin{tabular}
 {||l l c c||} 
 \hline
 Method & Input & Split-6  & Split-\\ [0.5ex] 
 \hline\hline\hline
 MeshWalker (ours) & Mesh & \textbf{98.6\%} & \textbf{97.1\%} \\ 
 \hline
 MeshCNN~\cite{hanocka2019meshcnn} & Mesh & \textbf{98.6\%} & 91.0\% \\
 \hline
 GWCNN~\cite{ezuz2017gwcnn} & Mesh & 96.6\% & 90.3\% \\
 \hline
 SG~\cite{bronstein2011shape} & Mesh & 70.8\% & 62.6\% \\
 \hline
 \hline
\end{tabular}
\label{tbl:shrec11}
\end{center}
\end{table}

\paragraph{Cube engraving.}
This dataset contains  objects, with / training/testing split.
Each object is a cube "engraved" with a shape at a random face in a random location, as demonstrated in Fig.~\ref{fig:cube}.
The engraved shape belongs to a dataset of  classes (e.g., car, heart, apple, etc.), each contains roughly  shapes. 
This dataset was created in order to demonstrate that using meshes, rather than point clouds, may be critical for 3D shape analysis.

Table~\ref{tbl:cubes} provides the results.
It demonstrates the benefit of our method over state-of-the-art methods.

\begin{figure}[htb] 
\centering 
\includegraphics[width=0.48\textwidth]{images/Engraved_cubes.PNG}
\caption{{\bf Engraved cubes dataset.} 
This image is courtesy of~\cite{hanocka2019meshcnn}.
}
\label{fig:cube} 
\end{figure}

\begin{table}[htb]\caption{{\bf Classification on Cube Engraving~\cite{hanocka2019meshcnn}.} 
Our results outperform those of state-of-the-art algorithms.}
\begin{center}
\begin{tabular}{||l l c||} 
\hline
 Method & Input & accuracy\\ [0.5ex] 
 \hline\hline\hline
 MeshWalker (ours) & Mesh & \textbf{98.6\%} \\ 
 \hline
 MeshCNN~\cite{hanocka2019meshcnn} & Mesh & 92.16\% \\
 \hline\hline
 PointNet++~\cite{qi2017pointnet++} & Point cloud & 64.26\% \\
 \hline
\end{tabular}
\label{tbl:cubes}
\end{center}
\end{table}


\paragraph{ModelNet40.}
This commonly-used dataset contains  CAD models from  categories, out of which  models are used for training and  models are used for testing.
Unlike previous datasets, many of the objects contain multiple components and are not necessarily watertight, making this dataset prohibitive for some mesh-based methods.
However, such models can be handled by MeshWalker since as explained before, if the walk gets into a dead-end during backtracking, it jumps to a new random location.

Table~\ref{tbl:modelnet} shows that our results  outperform those of mesh-based state-of-the-art methods.
We note that without  classes that are cross-labeled (desk/table \& plant/flower-pot/vase) our method's accuracy is .
The table shows that multi-views approaches are excellent for this dataset. 
This is due to relying on networks that are pre-trained on a large number of images.
However, they might fail for other datasets, such as the engraved cubes, and do not suit other shape analysis tasks, such as semantic segmentation.


\begin{table}[htb]\caption{{\bf Classification on ModelNet40~\cite{wu20153d}.}
MeshWalker is competitive with other mesh-based methods. 
Multi-view methods  are advantageous for this dataset, possibly due to relying on pre-trained networks for image classification and to naturally handling multiple components and non--watertight models, which characterize many meshes in this dataset.
}
\begin{center}
 \begin{tabular}{||l l c||} 
 \hline
 Method & Input & Accuracy \\ [0.5ex] 
 \hline\hline\hline
 MeshWalker (ours) & mesh & 92.3\% \\ 
 \hline
 MeshNet~\cite{feng2019meshnet} & mesh & 91.9\% \\
 \hline
 SNGC~\cite{haim2019surface} & mesh & 91.6\%  \\
 \hline
 \hline
 KPConv~\cite{thomas2019kpconv} & point cloud & 92.9\% \\
 \hline
 PointNet~\cite{qi2017pointnet} & point cloud & 89.2\%  \\
 \hline
 RS-CNN~\cite{liu2019relation} & point cloud & 93.6\% \\
 \hline
 \hline
 RotationNet~\cite{kanezaki2018rotationnet} & multi-views & \textbf{97.3\%} \\
 \hline
 GVCNN~\cite{feng2018gvcnn} & multi-views & 93.1\% \\
 \hline
 3D2SeqViews~\cite{han20193d2seqviews} & multi-views & 93.4\% \\
 \hline
\end{tabular}
\label{tbl:modelnet}
\end{center}
\end{table}


\begin{figure*}[tb]
\centering
\begin{tabular}{cccc}
\includegraphics[height=0.33\textwidth]{images/full_train-19-my.png}&
\includegraphics[height=0.33\textwidth]{images/full_train-19-meshcnn}&
\includegraphics[height=0.33\textwidth]{images/full_train-20-my.png}&
\includegraphics[height=0.33\textwidth]{images/full_train-20-meshcnn}\\
(a) Ours & (b) \cite{hanocka2019meshcnn} & (c) Ours & (d) \cite{hanocka2019meshcnn}
\end{tabular}
\caption{{\bf Qualitative results for human shape segmentation from~\cite{maron2017convolutional}. }
Our system avoids mis-classifications, not mixing lower legs with lower arms or hands with feet.
We note that for most shapes in the dataset, both systems produce equally-good results.
}
\label{fig:human_body_seg_visualization}
\end{figure*}


\subsection{Mesh semantic segmentation}
\label{subsec:segmentation}
Shape segmentation is an important building block for many applications in shape analysis and synthesis.
The goal is to determine, for every vertex, the segment it belongs to.
We tested  MeshWalker on two datasets: COSEG~\cite{wang2012active} and human-body Segmentation~\cite{maron2017convolutional}. 

Given mesh, multiple  random walks are generated (in practice,   \# segment classes; see the discussion in Section~\ref{sec:experiments}).
These walks are run through the trained network, which predicts the probabilities of belonging to the segments.
Similarly to the training process, only vertices of the second half of each walk are considered trustworthy.
For each vertex, the predictions of the walks it belongs to are averaged.
Then, as post-processing, we consider the average prediction of the vertex neighbors and add  this average with  weight. 
Finally, the prediction for each vertex is the argmax-ed.

Formally, let  be the set of walks performed on a mesh.
Let  be the vector that is the {\em Softmax} output for vertex  from walk~ (if walk~ does not visit ,  is set to a -vector).
Let  be the list of the vertices adjacent to  and  be the size of this list.
The predicted label,   of vertex  is defined as (where  finds the maximum vector entry):


We follow the accuracy measure proposed in~\cite{hanocka2019meshcnn}:
Given the prediction for each edge, the accuracy is defined as the percentage of the correctly-labeled edges, weighted by their length.
Since MeshWalker predicts the segment of the vertices, if the predictions of the endpoints of the edge agree, the edge gets the endpoints' label; otherwise,  the label with the higher prediction is chosen.
The overall accuracy is the average over all meshes.

\paragraph{Human-body segmentation.}
The dataset consists of  training models from SCAPE~\cite{anguelov2005scape}, FAUST~\cite{bogo2014faust}, MIT~\cite{vlasic2008articulated} and Adobe Fuse~\cite{Adobe2016}.
The test set consists of  humans from SHREC'07 \cite{giorgi2007shape} .
The meshes are manually segmented into eight labeled segments according to~\cite{kalogerakis2010learning}. 


\begin{table}[t]\caption{{\bf Human-body segmentation results on~\cite{maron2017convolutional}.}
The accuracy is  calculated on edges of the simplified meshes.
}
\begin{center}
 \begin{tabular}{||l c||} 
 \hline
 Method &  Edge Accuracy\\ [0.5ex] 
 \hline\hline\hline
 MeshWalker  & \textbf{94.8}\% \\ 
 \hline
 MeshCNN & 92.3\%  \\
 \hline
\end{tabular}
\label{tbl:human_body_segmentation_on_edge}
\end{center}
\end{table}

\begin{table}[htb]\caption{{\bf Human-body segmentation results on~\cite{maron2017convolutional}.} 
The reported results are on the original meshes;
For MeshCNN, the results shown are ours.
Our results outperform those of state-of-the-art algorithms.
}
\begin{center}
 \begin{tabular}{||l l c||} 
 \hline
 Method &  Input & Face\\ [0.5ex] 
   &    & Accuracy\\ [0.5ex] 
 \hline\hline\hline
 MeshWalker (ours)  & Mesh & \textbf{92.7}\% \\ 
 \hline
 MeshCNN~\cite{hanocka2019meshcnn} & Mesh & 89.0\% \\
 \hline
 LRF-Conv~\cite{yang2020continuous} & Mesh & 89.9\% \\
 \hline
SNGC~\cite{haim2019surface} & Mesh & 91.3\%  \\
 \hline
 Toric Cover~\cite{maron2017convolutional} & Mesh & 88.0\%  \\
 \hline
 GCNN~\cite{masci2015geodesic} & Mesh & 86.4\%  \\
 \hline
 MDGCNN & Mesh & 89.5\%  \\
 ~\cite{poulenard2018multi} & &   \\
 \hline
 \hline
 PointNet++~\cite{qi2017pointnet++} & Point cloud & 90.8\% \\
 \hline
 DynGraphCNN~\cite{wang2019dynamic} & Point cloud & 89.7\%  \\
 \hline
\end{tabular}
\label{tbl:human_body_segmentation}
\end{center}
\end{table}


There are two common measures of segmentation results, according to the correct classification of faces~\cite{haim2019surface} or of  edges \cite{hanocka2019meshcnn}.
Tables~\ref{tbl:human_body_segmentation_on_edge} and~\ref{tbl:human_body_segmentation} compare our results to those of previous works, according to the reported measure and the type of objects (simplified or not).
Since our method is trained on simplified meshes, to get results on the original meshes, we apply a simple projection to the original meshes jointly with boundary smoothing, as in~\cite{katz2003hierarchical}.
In both measures, MeshWalker outperforms other methods. 
Fig.~\ref{fig:human_body_seg_visualization} presents qualitative examples where the difference between the resulting segmentations is evident.






\begin{figure*}[htb]
\centering
\begin{tabular}{ccc}
\includegraphics[height=0.17\textwidth]{images/coseg-vases__full_train-55-my}
\includegraphics[height=0.17\textwidth]{images/coseg-vases-full_train-72-my}&
\includegraphics[height=0.17\textwidth]{images/coseg-aliens__full_train-63-my}
\includegraphics[height=0.17\textwidth]{images/coseg-aliens__full_train-89-my}&
\includegraphics[height=0.17\textwidth]{images/coseg-chairs__full_train-374-my}
\includegraphics[height=0.17\textwidth]{images/coseg-chairs__full_train-394-my}\\
(a) Vases & (b) Aliens & (c) Chairs 
\end{tabular}
\caption{{\bf Qualitative results of segmentation for meshes from COSEG~\cite{wang2012active}.} 
} 
\label{fig:coseg_body_seg_visualization}
\end{figure*}


\paragraph{COSEG segmentation.}
This dataset contains three large classes: aliens, vases and chairs with ,  and  shapes,
respectively. 
Each category is split into / train/test sets.
Fig.~\ref{fig:coseg_body_seg_visualization} presents some qualitative results, where it can be seen that our method performs very well.
Table~\ref{tbl:coseg} shows the accuracy of our results, where the results of the competitors are reported in~\cite{hanocka2019meshcnn}.
Our method achieves state-of-the-art results for all categories.

\begin{table}[htb]\caption{{\bf Segmentation results on COSEG~\cite{wang2012active}.}
Our method achieves state-of-the-art results for all categories.
} 
\begin{center}
 \begin{tabular}{||l c c c c||} 
 \hline
 Method & Vases  & Chairs & Telealiens & Mean\\ [0.5ex] 
 \hline\hline\hline
 MeshWalker (ours) & \textbf{98.7}\% & \textbf{99.6}\% & \textbf{99.1}\% & \textbf{99.1}\% \\ 
 \hline
 MeshCNN & 97.3\% & \textbf{99.6}\% & 97.6\% & 98.2\% \\
 \hline\hline
 PointNet++ & 94.7\% & 98.9\% & 79.1\% & 90.9\% \\
 \hline
 PointCNN~\cite{li2018pointcnn} & 96.4\% & 99.3\% & 97.4\% & 97.7\% \\
 \hline
\end{tabular}
\label{tbl:coseg}
\end{center}
\end{table}



\section{Experiments}
\label{sec:experiments}


\subsection{Ablation study}

\paragraph{Size of the training dataset.}
\label{subsec:size_training_set}
How many training models are needed in order to achieve good performance?
In the 3D case this question is especially important, since creating a dataset is costly.
Table~\ref{tbl:train_size_alien_coseg} shows the accuracy of our model for the {\em COSEG} dataset, when trained on different dataset sizes.
As expected, the larger the dataset, the better the results.
However, even when using only  shapes for training, the results are pretty good (). 
This outstanding result can be explained by the fact that 
we can produce many random walks for each mesh, hence the actual number of training examples is large. 
This result is consistent across all categories and datasets.
Table~\ref{tbl:train_size_human_body} shows a similar result for the  {\em human-body segmentation} dataset.

\begin{table}[htb]\caption{{\bf Analysis of the training dataset size (COSEG segmentation).} 
"Full" training is ,  and  shapes for tele-aliens, vases and chairs, respectively.
As expected, the larger the dataset, the better the results.
However, even if the training dataset is very small, our results are good.
}
\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
 \# training shapes & Vases & Chairs & Tele-aliens & Mean \\ [0.5ex] 
 \hline\hline\hline
 Full &  &  &  &  \\ 
 \hline\hline
 32         &  &  &  &  \\
 \hline
 16         &  &  &  &  \\
 \hline
 8          &  &  &  &  \\
 \hline
 4          &  &  &  &  \\
 \hline
 2          &  &  &  &  \\
 \hline
 1          &  &  &  &  \\
 \hline
\end{tabular}
\label{tbl:train_size_alien_coseg}
\end{center}
\end{table}

\begin{table}[htb]\caption{{\bf Analysis of the training dataset size (human-body segmentation).}
As before, the performance of our method degrades gracefully with the size of the training set.
We note that the results of MeshCNN are not reported in their paper, but rather the results of  new runs of their system.
}
\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 \# training shapes & MeshWalker & MeshCNN \\ [0.5ex] 
   & (ours) & \cite{hanocka2019meshcnn} \\
\hline\hline\hline
 381 (full) &  &  \\ 
 \hline\hline
 16 &  &  \\
 \hline
 4 &  &  \\
 \hline
 2 &  &  \\
 \hline
\end{tabular}
\label{tbl:train_size_human_body}
\end{center}
\end{table}


\paragraph{Walk length.}
Fig.~\ref{fig:teaser} has shown that the accuracy of our method depends on the walk length.
What would be an ideal length for our system to "understand" a shape?
Fig.~\ref{fig:walk_length_analysis} analyzes the influence of the length on the task of classification for {\em SHREC11}.
As expected, the accuracy increases with length.
However, it can be seen that when we use at least  walks per mesh, a walk whose length is  suffices to get excellent results.
Furthermore, there is a trade-off between the number of walks we use and the length of these walks.
Though the exact length depends both on the task in hand and on the dataset, this correlation is consistent across datasets and tasks.

\begin{figure}[tb] 
\centering 
\includegraphics[width=0.45\textwidth]
{images/acc_per_step_shrec11.png} 
\caption{{\bf Walk length analysis.} 
The accuracy increases with walk length, for classification on {\em SHREC11}.
Here, the  axis is number of vertices along the walk, normalized by number of mesh vertices.
This figure illustrates trade-off between the number of walks we use and the length of these walks.
As the walk begins, using many walks is not beneficial since the RNN has not accumulated enough information yet. However, after e.g.~0.3V, two walks are better than a single 0.6V-length walk. This is because they explore different mesh regions. 
}
\label{fig:walk_length_analysis} 
\end{figure}

\paragraph{Number of walks.}
How many walks are needed at inference time?
Table~\ref{tbl:accuracy_per_n_walks} shows that the more walks, the better the accuracy. However, even very few walks result in very good accuracy.
In particular, on SHREC11, even with a single walk the accuracy is .
For the Engraved-Cubes dataset, more walks are needed, since the model is engraved on a single cube facet, which certain walks might not get to.
Even in this difficult case,  walks already achieve  accuracy.
We note that the STD is between  for a single walk to  for  walks.
As expected, the more walks used, the more stable the results are and the smaller the STD is.


\begin{table}[tb]\caption{{\bf Number of walks analysis.}
The accuracy improves with the number of walks per shape (demonstrated on  datasets). 
}
\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 \# Walks & SHREC11 Acc & Eng.Cubes Acc\\ [0.5ex] 
 \hline\hline\hline
  &  & \%  \\ 
 \hline
  &  & \%   \\ 
 \hline
  &  & \%   \\ 
 \hline
  &  & \%   \\ 
 \hline
  &  & \%   \\ 
 \hline
  &  & \%   \\ 
 \hline
  \end{tabular}
\label{tbl:accuracy_per_n_walks}
\end{center}
\end{table}

\paragraph{Robustness.}
We use various rotations within data augmentation, hence robustness to orientations.
In particular, to test the robustness to rotation, we rotated the models in the Human-body segmentation dataset and in SHREC11 classification dataset  times for each axis, by increments of .
For each of these rotated versions of the datasets we applied the same testing as before.
For both datasets, there was no difference in the results.
Furthermore, the meshes are normalized, hence robustness to scaling. 

Our approach is inherently robust to different triangulations, as random walks (representing the same mesh) may vary greatly anyhow. 
Specifically, we generated a modified version of the COSEG segmentation dataset by randomly perturbing  of the vertex positions, realized as a shift towards a random vertex in its -ring. 
The performance degradation is less than .

\begin{figure*}
\centering
\begin{tabular}{cccccc}
\includegraphics[width=0.15\textwidth]{images/t-SNE_input}&
\includegraphics[width=0.15\textwidth]{images/t-SNE_fc1}&
\includegraphics[width=0.15\textwidth]{images/t-SNE_fc2}&
\includegraphics[width=0.15\textwidth]{images/t-SNE_gru1}&
\includegraphics[width=0.15\textwidth]{images/t-SNE_gru2}&
\includegraphics[width=0.15\textwidth]{images/t-SNE_gru3}\\
(a) input &
(b) FC1 &
(c) FC2 &
(d) GRU1&
(e) GRU2 &
(f) GRU3
\end{tabular}
\caption{{\bf t-SNE of the internal layers.}
This is a visualization of the output of the different layers for the human-body segmentation task.
It can be seen how the semantic meaning of the layers' output starts to evolve after the first GRU layer and gets better in the next two layers.
} 
\label{fig:t_sne_all_layers}
\end{figure*}

\subsection{Implementation}
\label{subsec:implementation}

\paragraph{Mesh pre-processing: simplification \& data augmentation.}
\label{subsec:preprocessing}
All the meshes used for training are first simplified into roughly the same number of faces~\cite{garland1997surface, hoppe1997view} ({\em MeshProcessing} procedure in Algorithm~\ref{alg:MeshWalkerTraining}).
Simplification is analogous to the initial resizing of images.
It reduces the network capacity required for training. 
Moreover, we could use several simplifications for each mesh as a form of data augmentation for training and for testing. 
For instance, for ModelNet40 we use ,  and  faces.
The meshes are normalized into a unit sphere, if necessary.


In addition, we augment the training data and add diversity by rotating the models. 
As part of batch preparation, each model is randomly rotated in each  axis prior to each training iteration.


\paragraph{t-SNE analysis.}
Does the network produce meaningful features?  
Fig.~\ref{fig:t_sne_all_layers} opens the network's "black box" and shows the t-SNE  projection to 2D of the multi-dimensional features after each stage of our learning framework, applied to the human-body segmentation task.
Each feature vector is colored by its correct label. 

In the input layer all the classes are mixed together. 
The same behavior is noticed after the first two fully-connected layers, since no information is shared between the vertices up to this stage. 
In the next three GRU layers, semantic meaning evolves:
The features are structured as we get deeper in the network.
In the last RNN layer the features are meaningful, as the clusters are evident. 
This visualization demonstrates the importance of the RNN hierarchy.


Fig.~\ref{fig:t-sne_shrec} reveals another invaluable property of our walks.
It shows the t-SNE visualization of walks for classification of objects from  categories of  SHREC11.
Each feature vector is colored by its correct label; its shape (rectangle, triangle etc) represents the object the walk belongs to.
Not only clusters of shapes from the same category clearly emerge, but also walks that belong to the same object are grouped together!
This is another indication to the quality of our proposed features.

\paragraph{Computation time.}
Training takes between  hours (for classification on SHREC11) to  hours (for segmentation on human-body), using GTX 1080 TI graphics card.
At inference, a -step walk, which is typical for  SHREC11, takes about  milliseconds.
When we use  walks per shape, the running time would be  milliseconds. 
Remeshing takes e.g.  seconds from  faces to  or  from  face  to  faces.
We note that our method is easy to parallelize, as every walk could be processed on a different processor, which is yet another benefit of our approach.


\begin{figure}[tb] 
\centering 
\includegraphics[width=0.35\textwidth]{images/shrec11-tsne.png} 
\caption{
{\bf t-SNE analysis for classification.}
This figure shows feature hierarchy:
Meshes that belong to the same category (indicated by the color) are clustered together.
Furthermore, walks that belong to the same mesh  (indicated by the shape of the 2D point) are also clustered.
}
\label{fig:t-sne_shrec} 
\end{figure}


\paragraph{Training configurations.}
We implemented our network using {\em TensorFlow V2}. 
The network architecture is given in Table~\ref{tbl:configuration}.
The source code is available on
\url{"https://github.com/AlonLahav/MeshWalker"}.

\begin{table}[htb]\caption{{\bf Training configuration}}
\begin{center}
 \begin{tabular}{||l c||} 
 \hline
 Layer & Output Dimension \\ [0.5ex] 
 \hline\hline\hline
 Vertex description &   \\ 
 Fully Connected &   \\ 
 Instance Normalization &  \\ 
 ReLU &  \\
 Fully Connected &   \\ 
 Instance Normalization &  \\ 
 ReLU &  \\
 GRU &  \\
 GRU &  \\
 GRU &  \\
 Fully Connected & \# of classes  \\ 
 \hline
\end{tabular}
\label{tbl:configuration}
\end{center}
\end{table}

Optimization: 
To update the network weights, we use Adam optimizer~\cite{kingma2014adam}. 
The learning rate is set in a cyclic way, as suggested  by~\cite{smith2017cyclical}.
The initial and the maximum learning rates are set to  and  respectively.
The cycle size is  iterations.

Batch strategy:
Walks are grouped into batches of  walks each.
For mesh classification, the walks are generated from different meshes, whereas for semantic segmentation each batch is composed of  walks on  meshes.

Training iterations:
We train for k, k, k, k, k iterations for SHREC11, COSEG, human-body segmentation, engraved-cubes and ModelNet40 datasets, respectively.
This is so since for the loss to converge fast, many of the walks should cover the salient parts of the shape, which distinguish it from other classes/segments.
When this is not the case, more iterations are needed in order for the few meaningful walks to influence the loss.
This is the case for instance in the engraved cubes dataset, where the salient information lies on a single facet.

\subsection{Limitations}
\label{subsec:limitations}

Fig.~\ref{fig:limitation_segmentation} shows a failure of our algorithm, where parts of the hair were wrongly classified as  a torso.
 This is the case since the training data does not contain enough models with hair to learn from.
In general, learning-based algorithms rely on good training data, which is not always available.


\begin{figure}[htb]
\centering
\begin{tabular}{ccc}
\includegraphics[height=0.22\textwidth]{images/gt_human_seg_06}&
\includegraphics[height=0.22\textwidth]{images/full_train-06-my}&
\hspace{-0.1in}\includegraphics[height=0.22\textwidth]{images/full_train-06-meshcnn}\\
(a) Ground truth & (b) Ours & \hspace{-0.1in}(c) \cite{hanocka2019meshcnn}
\end{tabular}
\caption{{\bf Limitation. }
Our algorithm fails to classify the hair due to not having sufficient similar shapes in the dataset.
}
\label{fig:limitation_segmentation}
\end{figure}

Another limitation is handling large meshes.
The latter require long walks, which in turn might lead to run-time and memory issues.
In this paper, this is solved by simplifying the meshes and then projecting the segmentation results onto the original meshes.
(For classification, this is not a concern, as simplified meshes may be used).


\section{Conclusion}
\label{sec:conclusion}
This paper has introduced a novel approach for representing meshes within deep learning schemes. 
The key idea is to represent the mesh by random walks on its surface, which intuitively explore the shape of the mesh.
Since walks are described by the order of visiting mesh vertices, they suit deep learning.

Utilizing this representation, the paper has proposed an end-to-end learning framework, termed {\em MeshWalker}.
The random walks are fed into a Recurrent Neural Network (RNN), that "remembers" the walk’s history (i.e. the geometry of the mesh).
Prior works indicated that RNNs are unsuitable for point clouds due to both the unordered nature of the data and the number of vertices used to represent a shape. Surprisingly, we have shown that RNNs work extremely well for meshes, through the concept of random walks. 

Our approach is general, yet simple.
It has several additional benefits.
Most notably, it works well even for extremely small datasets. e.g.  even  meshes per class suffice  to get good results. 
In addition, the meshes are not require to be watertight or to consist of a single component (as demonstrated by ModelNet40~\cite{wu20153d}); some other mesh-based approaches impose these conditions and require the meshes to be manifolds.

Last but not least, the power of this approach has been demonstrated for two key tasks in shape analysis: mesh classification and mesh semantic segmentation. 
In both cases, we present  state-of-the-art  results.

An interesting question for future work is whether there are optimal walks for meshes, rather than random walks.
For instance, are there good starting points of walks?
Additionally, reinforcement learning could be utilized to learn good walks.
Exploring other applications, such as shape correspondence,
is another intriguing future direction.
Another interesting practical future work would be to work on the mesh as is, without simplification as pre-processing.

\begin{acks}
  We gratefully acknowledge the support of the Israel Science Foundation (ISF) 1083/18 amd PMRI -- Peter Munk Research Institute -- Technion.
\end{acks}


\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

 
\end{document}