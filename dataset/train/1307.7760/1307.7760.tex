\documentclass[11pt]{myclass}

\usepackage{mystyle}
\usepackage{subfigure}
\usepackage{mathtools}

\title{Geometric Inference on Kernel Density Estimates}
\author{
Jeff M. Phillips\thanks{Thanks to supported by NSF CCF-1350888, IIS-1251019, and ACI-1443046.}
     \\ {\small\textsl{jeffp@cs.utah.edu}} \\  {\small University of Utah} \and 
Bei Wang\thanks{Thanks to supported by  INL 00115847 via DOE DE-AC07ID14517, 
                                                            DOE NETL DEEE0004449, 
                                                            DOE DEFC0206ER25781, 
                                                            DOE DE-SC0007446, and
                                                            NSF 0904631.}
      \\ {\small\textsl{beiwang@sci.utah.edu}} \\  {\small University of Utah} \and
Yan Zheng \\ {\small\textsl{yanzheng@cs.utah.edu}} \\  {\small University of Utah}
}



\begin{document}

\begin{titlepage}
\maketitle 
\begin{abstract}
We show that geometric inference of a point cloud can be calculated by examining its kernel density estimate with a Gaussian kernel.  This allows one to consider kernel density estimates, which are robust to spatial noise, subsampling, and approximate computation in comparison to raw point sets.  This is achieved by examining the sublevel sets of the \emph{kernel distance}, which isomorphically map to superlevel sets of the kernel density estimate.  We prove new properties about the kernel distance, demonstrating stability results and allowing it to inherit reconstruction results from recent advances in distance-based topological reconstruction.  Moreover, we provide an algorithm to estimate its topology using weighted Vietoris-Rips complexes.
\end{abstract}
\end{titlepage}





\section{Introduction}
\label{sec:intro}

Geometry and topology have become essential tools in modern data analysis: geometry to handle spatial noise and topology to identify the core structure.  Topological data analysis (TDA) has found applications spanning protein structure analysis~\cite{EFFL95,LEFSS98} to heart modeling~\cite{GCZQMA13} to leaf science~\cite{PSMHW11}, and is the central tool of identifying quantities like connectedness, cyclic structure, and intersections at various scales.  Yet it can suffer from spatial noise in data, particularly outliers.  

When analyzing point cloud data, classically these approaches consider -shapes~\cite{Edelsbrunner1993}, where each point is replaced with a ball of radius , and the union of these balls is analyzed.  More recently a distance function interpretation~\cite{CC12} has become more prevalent where the union of -radius balls can be replaced by the sublevel set (at value ) of the Hausdorff distance to the point set.  Moreover, the theory can be extended to other distance functions to the point sets, including the \emph{distance-to-a-measure}~\cite{ChazalCohen-SteinerMerigot2011} which is more robust to noise.  

This has more recently led to statistical analysis of TDA.  These results show not only robustness in the function reconstruction, but also in the topology it implies about the underlying dataset.  This work often operates on persistence diagrams which summarize the persistence (difference in function values between appearance and disappearance) of all homological features in single diagram.  A variety of work has developed metrics on these diagrams and probability distributions over them~\cite{MMH11,TMMH14}, and robustness and confidence intervals on their landscapes~\cite{Bub14,FLRWBS14,CFLRSW13} (summarizing again the most dominant persistent features~\cite{CFLRW14}).  Much of this work is independent of the function and data from which the diagram is generated, but it is now more clear than ever, it is most appropriate when the underlying function is robust to noise, e.g., the distance-to-a-measure~\cite{ChazalCohen-SteinerMerigot2011}.  

\begin{figure}[h!]
\includegraphics[width=0.3\linewidth]{im-10000-05.pdf}
\hspace{0.01\linewidth}
\includegraphics[width=0.32\linewidth]{pd-10000-05.pdf}
\hspace{0.01\linewidth}
\includegraphics[width=0.32\linewidth]{pd-10000-dist-05.pdf}
\vspace{-.11in}
\\
\includegraphics[width=0.30\linewidth]{im-10000-core-05.pdf}
\hspace{0.01\linewidth}
\includegraphics[width=0.32\linewidth]{pd-10000-core-05.pdf}
\hspace{0.01\linewidth}
\includegraphics[width=0.32\linewidth]{pd-10000-kd-05.pdf}
\vspace{-.11in}
\caption{\label{fig:circleline}
\small \sffamily 
Example with  points in  generated on a circle or line with  noise;  of points are uniform background noise.  The generating function is reconstructed with \kde\ with  (upper left), and its persistence diagram based on the superlevel set filtration is shown (upper middle).  
A coreset~\cite{big-kde} of the same dataset with only  points (lower left) and persistence diagram (lower middle) are shown, again using \kde.  This associated confidence interval contains the dimension  homology features (red triangles) suggesting they are noise; this is because it models data as iid -- but the coreset data is not iid, it subsamples more intelligently.   
We also show persistence diagrams of the original data based on the sublevel set filtration of the standard distance function (upper right, with no useful features due to noise) and the kernel distance (lower right).}
\end{figure}



A very recent addition to this progression is the new TDA package for R~\cite{FKLM14}; it includes built in functions to analyze point sets using Hausdorff distance, distance-to-a-measure, -nearest neighbor density estimators, kernel density estimates, and kernel distance.  The example in Figure \ref{fig:circleline} used this package to generate persistence diagrams.  
While, the stability of the Hausdorff distance is classic~\cite{CC12,Edelsbrunner1993}, and the distance-to-a-measure~\cite{ChazalCohen-SteinerMerigot2011} and -nearest neighbor distances have been shown robust to various degrees~\cite{BCCSDR11}, this paper is the first to analyze the stability of kernel density estimates and the kernel distance in the context of geometric inference.
Some recent manuscripts show related results.  
Bobrowski \etal~\cite{BMT14} consider kernels with finite support, and describe approximate confidence intervals on the superlevel sets, which recover approximate persistence diagrams.  
Chazal \etal~\cite{CFLMRW14} explore the robustness of the kernel distance in bootstrapping-based analysis.

In particular, we show that the kernel distance and kernel density estimates, using the Gaussian kernel, inherit some reconstruction properties of distance-to-a-measure, that these functions can also be approximately reconstructed using weighted (Vietoris-)Rips complexes~\cite{BuchetChazalOudot2013}, and that under certain regimes can infer homotopy of compact sets.  
Moreover, we show further robustness advantages of the kernel distance and kernel density estimates, including that they possess small coresets~\cite{Phillips2013,big-kde} for persistence diagrams and inference.  

\vspace{-.1in}










\subsection{Kernels, Kernel Density Estimates, and Kernel Distance}
\label{sec:kernel}
A \emph{kernel} is a non-negative similarity measure ; more similar points have higher value.  For any fixed , a kernel  can be normalized to be a probability distribution; that is .  
For the purposes of this article we focus on the Gaussian kernel defined as 
. \footnote{
 is normalized so that  for .  
The choice of coefficient  is not the standard normalization, but it is perfectly valid as it scales everything by a constant.  It has the property that  for  small.}

A \emph{kernel density estimate}~\cite{Sil86,Sco92,DG84,DL01} is a way to estimate a continuous distribution function over  for a finite point set ; they have been studied and applied in a variety of contexts, for instance, under subsampling~\cite{Phillips2013,big-kde,BFLRSW13}, motion planning~\cite{PEKK12}, multimodality~\cite{Sil81,EFR12}, and surveillance~\cite{EDHD02}, road reconstruction~\cite{BE12}.
Specifically, 


The \emph{kernel distance}~\cite{HB05,glaunesthesis,
JoshiKommarajuPhillips2011,PhillipsVenkatasubramanian2011} (also called \emph{current distance} or \emph{maximum mean discrepancy}) is a metric~\cite{Muller1997,SGFSL10} between two point sets ,  (as long as the kernel used is characteristic~\cite{SGFSL10}, a slight restriction of being positive definite~\cite{Aronszajn1950,Wah99}, this includes the Gaussian and Laplace kernels).
Define a similarity between the two point sets as 
 
Then the kernel distance between two point sets is defined as \vspace{-1mm}

When we let point set  be a single point , then .  

Kernel density estimates can apply to any measure  (on ) as 
 
The similarity between two measures is 
 
where  is the product measure of  and  (), and then the kernel distance between two measures  and  is still a metric, defined as  
 
When the measure  is a Dirac measure at  ( for , but integrates to ), then .  
Given a finite point set , we can work with the empirical measure  defined as , where  is the Dirac measure on , and .  


If  is positive definite, it is said to have the reproducing property~\cite{Aronszajn1950,Wah99}.
This implies that  is an inner product in some reproducing kernel Hilbert space (RKHS) .  Specifically, there is a lifting map  so that , and moreover the entire set  can be represented as , which is a single element of  and has a norm .  A single point  also has a norm  in this space.




\subsection{Geometric Inference and Distance to a Measure: A Review}
\label{sec:ball}

Given an unknown compact set  and a finite point cloud  that comes from  under some process, geometric inference aims to recover topological and geometric properties of  from . The offset-based (and more generally, the distance function-based) approach for geometric inference reconstructs a geometric and topological approximation of  by offsets from  (e.g. \cite{ChazalCohen-SteinerLieutier2009b,ChazalCohen-SteinerLieutier2009,ChazalCohen-SteinerMerigot2011,ChazalLieutier2005,ChazalLieutier2006}).  


Given a compact set , we can define a \emph{distance function}  to  ; a common example is  (i.e. -shapes).
The offsets of  are the sublevel sets of , denoted .
Now an approximation of  by another compact set  (e.g. a finite point cloud) can be quantified by the Hausdorff distance  of their distance functions. 
The intuition behind the inference of topology is that if  is small, thus  and  are close, and subsequently, ,  and  carry the same topology for an appropriate scale . 
In other words, to compare the topology of offsets  and , 
we require Hausdorff stability with respect to their distance functions  and .  
 An example of an offset-based topological inference result is formally stated as follows (as a particular version of the reconstruction Theorem 4.6 in \cite{ChazalCohen-SteinerLieutier2009}), 
 where the \emph{reach} of a compact set , , is defined as the minimum distance between  and its medial axis \cite{Merigot2010}. 

\begin{theorem}[Reconstruction from  \cite{ChazalCohen-SteinerLieutier2009}]
\label{thm:recon-fP}
Let  be compact sets such that  and . Then  and  are homotopy equivalent for sufficiently small  (e.g., ) if .
\end{theorem}



Here  ensures that the topological properties of  and  are the same, and the  parameter ensures  and  are close.  
Typically  is tied to the density with which a point cloud  is sampled from .  

For function  to be \emph{distance-like} it should satisfy the following properties:
\begin{itemize} \denselist
\item (D1)   is -Lipschitz:  For all , .  
\item (D2)   is -semiconcave:  The map  is concave.  
\item (D3)    is proper:  tends to the infimum of its domain (e.g., ) as  tends to infinity.  
\end{itemize}
In addition to the Hausdorff stability property stated above, as explained in \cite{ChazalCohen-SteinerMerigot2011},  is distance-like.  These three properties are paramount for geometric inference (e.g. \cite{ChazalCohen-SteinerLieutier2009,Lieutier2004}).  
(D1) ensures that  is differentiable almost everywhere and the medial axis of  has zero -volume~\cite{ChazalCohen-SteinerMerigot2011}; and (D2) is a crucial technical tool, e.g., in proving the existence of the flow of the gradient of the distance function for topological inference \cite{ChazalCohen-SteinerLieutier2009}. 




\paragraph{Distance to a measure.}
Given a probability measure  on  and a parameter  smaller than the total mass of ,
the \emph{distance to a measure}  \cite{ChazalCohen-SteinerMerigot2011} is defined for any point  as 

It has been shown in \cite{ChazalCohen-SteinerMerigot2011} that  is a distance-like function (satisfying (D1), (D2), and (D3)), and:
\begin{itemize} 
\item (M4)   [Stability] For probability measures  and  on  and , then .
\end{itemize}
Here  is the \emph{Wasserstein distance}~\cite{Villani2003}:  

between two measures, where  measures the amount of mass transferred from location  to location  and  is a transference plan \cite{Villani2003}.  



Given a point set , the sublevel sets of  can be described as the union of balls \cite{GuibasMerigotMorozov2011}, and then one can algorithmically estimate the topology (e.g., persistence diagram) with weighted alpha-shapes \cite{GuibasMerigotMorozov2011} and weighted Rips complexes \cite{BuchetChazalOudot2013}. 

\subsection{Our Results}
\label{sec:results}

We show how to estimate the topology (e.g., approximate persistence diagrams, infer homotopy of compact sets) using superlevel sets of the kernel density estimate of a point set .  
We accomplish this by showing that a similar set of properties hold for the kernel distance with respect to a measure ,  
(in place of distance to a measure ),  defined as 

This treats  as a probability measure represented by a Dirac mass at .  
Specifically, we show  is distance-like (it satisfies (D1), (D2), and (D3)), so it inherits reconstruction properties of .  Moreover, it is stable with respect to the kernel distance:
\begin{itemize}
\item (K4) [Stability] If  and  are two measures on , then 
. 
\end{itemize}

In addition, we show how to construct these topological estimates for  using weighted Rips complexes, following power distance machinery introduced in \cite{BuchetChazalOudot2013}.  That is, a particular form of power distance permits a multiplicative approximation with the kernel distance.  

We also describe further advantages of the kernel distance.  
(i) Its sublevel sets conveniently map to the superlevel sets of a kernel density estimate.  
(ii) It is Lipschitz with respect to the smoothing parameter  when the input  is fixed.  
(iii) As  tends to  for any two probability measures , the kernel distance is bounded by the Wasserstein distance:  .  
(iv) It has a small coreset representation, which allows for sparse representation and efficient, scalable computation.  In particular, an \emph{-kernel sample}~\cite{JoshiKommarajuPhillips2011,Phillips2013,big-kde}  of  is a finite point set whose size only depends on  and such that .  These coresets preserve inference results and persistence diagrams.  


\section{Kernel Distance is Distance-Like} 
\label{sec:prop}

In this section we prove  satisfies (D1), (D2), and (D3); hence it is distance-like.  
Recall we use the -normalized Gaussian kernel .  For ease of exposition, unless otherwise noted, we will assume  is fixed and write  instead of .  




\subsection{Semiconcave Property for }

\begin{lemma}[D2]
 is -semiconcave: the map  is concave. \label{lem:semi}

\end{lemma}
\begin{proof}
Let .  The proof will show that the second derivative of  along any direction is nonpositive.  
We can rewrite 

Note that both  and  are absolute constants, so we can ignore them in the second derivative.  
Furthermore, by setting , the second derivative of  is nonpositive if the second derivative of  is nonpositive for all .  
First note that the second derivative of  is a constant  in every direction.  
The second derivative of  is symmetric about , so we can consider the second derivative along any vector ,

This reaches its maximum value at  where it is ; this follows setting the derivative of  to , (), substituting .  
\end{proof}

We also note in Appendix \ref{app:dist-like} that semiconcavity follows trivially in the RKHS .  

\subsection{Lipschitz Property for }
\label{sec:lipschitz}

We generalize a (folklore, see \cite{ChazalCohen-SteinerMerigot2011}) relation between semiconcave and Lipschitz functions and prove it for completeness.  
A function  is -semiconcave if the function  is concave.  

\begin{lemma}
Consider a twice-differentiable function  and a parameter .  
If  is -semiconcave, then  is -Lipschitz.  
\label{lem:semi-Lip}
\end{lemma}
\begin{proof}
The proof is by contrapositive; we assume that  is not -Lipschitz and then show  cannot be -semiconcave.  
By this assumption, then in some direction , there is a point  such that .  

Now we examine  at , and specifically its second derivative in direction . 

Since , then  and  is not -semiconcave at .  
\end{proof}

We can now state the following lemma as a corollary of Lemma \ref{lem:semi-Lip} and Lemma \ref{lem:semi}.  

\begin{lemma}[D1]
 is -Lipschitz on its input. 
\label{lem:Lipschitz}
\end{lemma}


\subsection{Properness of }

Finally, for  to be distance-like, we need to show it is proper when its range is restricted to be less than . Here, the value of  depends on  not on  since .
This is required for a distance-like version \cite{ChazalCohen-SteinerMerigot2011}, Proposition 4.2) of the Isotopy Lemma (\cite{Grove1993}, Proposition 1.8).  

\begin{lemma}[D3] 
 is proper. 
\label{lem:proper}
\end{lemma}


We delay this technical proof to Appendix \ref{app:dist-like}.  
The main technical difficulty comes in mapping standard definitions and approaches for distance functions to our function  with a restricted range.  

Also by properness (see discussion in Appendix \ref{app:dist-like}), Lemma \ref{lem:proper} also implies that  is a closed map and its levelset at any value  is compact. 
This also means that the sublevel set of  (for ranges ) is compact. Since the levelset (sublevel set) of  corresponds to the levelset (superlevel set) of , we have the following corollary.  
\begin{corollary}
The superlevel sets of  for all ranges with threshold , are compact. 
\end{corollary}
The result in \cite{EFR12} shows that given a measure  defined by a point set  of size , the  has polynomial in  modes; hence the superlevel sets of  are compact in this setting.  The above corollary is a more general statement as it holds for any measure.  








\section{Power Distance using Kernel Distance}
\label{sec:powerdistance}
A \emph{power distance} using  is defined with a point set  and a metric  on ,  

A point  takes the distance under  to the closest , plus a weight from ; thus a sublevel set of  is defined by a union of balls.  
We consider a particular choice of the distance  which leads to a kernel version of power distance


In Section \ref{subsec:topo-est} we use  to adapt the construction introduced in \cite{BuchetChazalOudot2013} to approximate the persistence diagram of the sublevel sets of , using a weighted Rips filtration of . 

Given a measure , let , and let  be a point set that contains .  We show below, in Theorem \ref{thm:powK-up} and Theorem \ref{thm:powK-low}, that .  
However, constructing  exactly seems quite difficult.  
We also attempt to use  in place of  (see Section \ref{app:pow-justP}), but are not able to obtain useful bounds.  

Now consider an empirical measure  defined by a point set .  
We show (in Theorem \ref{thm:phat+} in Appendix \ref{app:phat+}) how to construct a point   (that approximates ) such that  
for any .  For a point set , the \emph{median concentration}  is a radius such that no point  has more than half of the points of  within , 
and the \emph{spread}  is the ratio between the longest and shortest pairwise distances.  
The runtime is polynomial in  and  assuming  is bounded, and  that  and  are constants.  

Then we consider , where  is found with  in the above construction.   Then we can provide the following multiplicative bound, proven in Theorem \ref{thm:powK-up-hat}.  The lower bound holds independent of the choice of  as shown in Theorem \ref{thm:powK-low}.  

\begin{theorem}
\label{thm:powK-bnd}
For any point set  and point , with empirical measure  defined by ,  then 

\end{theorem}





\subsection{Kernel Power Distance for a Measure }
First consider the case for a kernel power distance  where  is an arbitrary measure.  

\begin{theorem}
\label{thm:powK-low}
For measure , point set , and , 

\end{theorem}
\begin{proof}
Let .  Then we can use the triangle inequality and  to show

\end{proof}






\begin{lemma}
\label{lem:pK-up}
For measure , point set , point , and point   then

\end{lemma}
\begin{proof}
Again, we can reach this result with the triangle inequality. 

\end{proof}


Recall the definition of a point .  
\begin{lemma}
\label{lem:p+}
For any measure  and point  we have .  
\end{lemma}
\begin{proof}
Since  is a point in ,   and thus .  Then by triangle inequality of  to see that 
. 
\end{proof}
 
\begin{theorem}
\label{thm:powK-up}
For any measure  in  and any point , using the point  then 
.  
\end{theorem}
\begin{proof}
Combine Lemma \ref{lem:pK-up} and Lemma \ref{lem:p+} as

\end{proof}



We now need two properties of the point set  to reach our bound, namely, the spread  and the median concentration . Typically  is not too large, and it makes sense to choose  so , or at least .  





\begin{theorem}
\label{thm:powK-up-hat}
Consider  any point set  of size , with measure , spread , and median concentration .  We can construct a point set  in  time such that for any point , 
  
\end{theorem}
\begin{proof}
We use Theorem \ref{thm:phat+} to find a point  such that . Thus for any , using the triangle inequality

Now combine this with Lemma \ref{lem:pK-up} and Lemma \ref{lem:p+} as

\end{proof}






\section{Reconstruction and Topological Estimation using Kernel Distance}
\label{sec:recon}

Now applying distance-like properties from Section \ref{sec:prop} and the power distance properties of Section \ref{sec:powerdistance} we can apply known reconstruction results to the kernel distance.  

\subsection{Homotopy Equivalent Reconstruction using }

We have shown that the kernel distance function  is a distance-like function.
Therefore the reconstruction theory for a distance-like function \cite{ChazalCohen-SteinerMerigot2011} (which is an extension of results for compact sets \cite{ChazalCohen-SteinerLieutier2009}) 
holds in the setting of . 
We state the following two corollaries for completeness, whose proofs follow from the proofs of Proposition 4.2 and Theorem 4.6 in \cite{ChazalCohen-SteinerMerigot2011}.  
Before their formal statement, we need some notation adapted from \cite{ChazalCohen-SteinerMerigot2011} to make these statements precise.
Let  be a distance-like function. A point  is an -critical point if  with , . 
Let  denote the sublevel set of , and let  denote all points at levels in the range .  
For , the \emph{-reach} of  is the maximum  such that  has no -critical point, denoted as .   
When ,  coincides with \emph{reach} introduced in \cite{Federer1959}. 


\begin{theorem}[Isotopy lemma on ]
Let  be two positive numbers such that  has no critical points in .  
Then all the sublevel sets  are isotopic for . 
\end{theorem}

\begin{theorem}[Reconstruction on ]
Let  and  be two kernel distance functions such that
.  
Suppose  for some .
Then , and , 
the sublevel sets  and  are homotopy equivalent for . 
\label{thm:reconstruct}
\end{theorem}










\subsection{Constructing Topological Estimates using }
\label{subsec:topo-est}
In order to actually construct a topological estimate using the kernel distance , one needs to be able to compute quantities related to its sublevel sets, in particular, to compute the persistence diagram of the sub-level sets filtration of .  
Now we describe such tools needed for the kernel distance based on machinery recently developed by Buchet et al.  \cite{BuchetChazalOudot2013}, which shows how to approximate the persistent homology of distance-to-a-measure for any metric space via a power distance construction. Then using similar constructions, we can use the weighted Rips filtration to approximate the persistence diagram of the kernel distance. 

To state our results, first we require some technical notions and assume basic knowledge on persistent homology (see \cite{EdelsbrunnerHarer2008,EdelsbrunnerHarer2010} for a readable background). 
Given a metric space  with the distance , a set  and a function , the (general) \emph{power distance}  associated with  is defined as 
 
Now given the set  and its corresponding power distance , one could use the weighted Rips filtration to approximate the persistence diagram of , under certain restrictive conditions proven in Appendix \ref{subsec:approximation}.  
Consider the sublevel set of , . It is the union of balls centered at points  with radius  for each .
The weighted \v{C}ech complex  for parameter  is the union of simplices  such that . 
The weighted Rips complex  for parameter  is the maximal complex whose -skeleton is the same as .  
The corresponding weighted Rips filtration is denoted as . 

Setting  and given point set  described in Section \ref{sec:powerdistance},  consider the weighted Rips filtration  based on the kernel power distance, . 
We view the persistence diagrams on a logarithmic scale, that is, we change coordinates of points following the mapping .  denotes the corresponding bottleneck distance between persistence diagrams.  We now state a corollary of Theorem \ref{thm:powK-bnd}.
\begin{corollary}
\label{cor:wRip}
The weighted Rips filtration  can be used to  approximate the persistence diagram of  such that 
.
\end{corollary}
\begin{proof}
To prove that two persistence diagrams are close, one could prove that their filtration are interleaved \cite{ChazalCohen-SteinerGlisse2009}, 
that is, two filtrations  and  are \emph{-interleaved} if for any , .  


First, Lemmas \ref{lemma:q-tame} and \ref{lemma:q-tame-rips} prove that the persistence diagrams  and  are well-defined. 
Second, the results of Theorem \ref{thm:powK-bnd} implies an  multiplicative interleaving. 
Therefore for any , 

On a logarithmic scale (by taking the natural log of both sides), such interleaving becomes addictive, 

Theorem 4 of \cite{ChazalSilvaGlisse2013} implies 

In addition, by the Persistent Nerve Lemma (\cite{ChazalOudot2008}, Theorem 6 of \cite{Sheehy2012}, an extension of the Nerve Theorem \cite{Hatcher2002}), 
the sublevel sets filtration of , which correspond to unions of balls of increasing radius, has the same persistent homology as the nerve filtration of these balls (which, by definition, is the \v{C}ech filtration).
Finally, there exists a multiplicative interleaving between weighted Rips and \v{C}ech complexes (Proposition 31 of \cite{ChazalSilvaGlisse2013}), 

We then obtain the following bounds on persistence diagrams,  

We use triangle inequality to obtain the final result: 

\end{proof}

Based on Corollary \ref{cor:wRip}, we have an algorithm that approximates the persistent homology of the sublevel set filtration of  by constructing the weighted Rips filtration corresponding to the kernel-based power distance and computing its persistent homology. 
For memory efficient computation, sparse (weighted) Rips filtrations could be adapted by considering simplices on subsamples at each scale \cite{Sheehy2013,ChazalSilvaGlisse2013}, although some restrictions on the space apply. 









\subsection{Distance to the Support of a Measure vs. Kernel Distance}
\label{sec:infer}
Suppose  is a uniform measure on a compact set  in . 
We now compare the kernel distance  with the distance function  to the support  of . We show how  approximates , and thus allows one to infer geometric properties of  from samples from . 

A generalized gradient and its corresponding flow associated with a distance function are described in \cite{ChazalCohen-SteinerLieutier2009} and later adapted for distance-like functions in \cite{ChazalCohen-SteinerMerigot2011}. 
Let  be a distance function associated with a compact set  of . 
It is not differentiable on the medial axis of . 
A \emph{generalized gradient function}  coincides with the usual gradient of  where  is differentiable, and is defined everywhere and can be integrated into a continuous flow  that points away from .   
Let  be an integral (flow) line.  
The following lemma shows that when close enough to , that  is strictly increasing along any .  The proof is quite technical and is thus deferred to Appendix \ref{app:infer}.  

\begin{lemma}
Given any flow line  associated with the generalized gradient function ,  is strictly monotonically increasing along  for  sufficiently far away from the medial axis of , for  and . 
Here  denotes a ball of radius ,  ,  and suppose .  
\label{lem:monotonicity}
\end{lemma}

The strict monotonicity of  along the flow line under the conditions in Lemma \ref{lem:monotonicity} makes it possible to define a deformation retract of the sublevel sets of  onto sublevel sets of . Such a deformation retract defines a special case of homotopy equivalence between the sublevel sets of  and sublevel sets of . 
Consider a sufficiently large point set  sampled from , and its induced measure .  
We can then also invoke Theorem \ref{thm:reconstruct} and a sampling bound (see Section \ref{sec:algo} and Lemma \ref{lem:KD-samp-nosq}) to show homotopy equivalence between the sublevel sets of  and .  



Note that Lemma \ref{lem:monotonicity} uses somewhat restrictive conditions related to the reach of a compact set, however we believe such conditions could be further relaxed to be associated with the concept of -reach as described in \cite{ChazalCohen-SteinerLieutier2009}. 


\section{Stability Properties for the Kernel Distance to a Measure}

\begin{lemma}[K4]
For two measures  and  on  we have
. 
\end{lemma}
\begin{proof}
Since  is a metric, then by triangle inequality,  for any  we have 
 and 
. 
Therefore for any  we have , proving the claim.  
\end{proof}

Both the Wasserstein and kernel distance are \emph{integral probability metrics}~\cite{SGFSL10}, so (M4) and (K4) are both interesting, but not easily comparable.  We now attempt to reconcile this.  

\subsection{Comparing  to }
\begin{lemma}
There is no Lipschitz constant  such that for any two probability measures  and  we have .  
\end{lemma}
\begin{proof}
Consider two measures  and  which are almost identical: the only difference is some mass of measure  is moved from its location in  a distance  in . 
The Wasserstein distance requires a transportation plan that moves this  mass in  back to where it was in  with cost  in .  
On the other hand,   is bounded.  
\end{proof}

We conjecture for any two probability measures  and  that .  This would show that  is at least as stable as  since a bound on  would also bound , but not vice versa.  
Alternatively, this can be viewed as  is less discriminative than ; we view this as a positive in this setting, as it is mainly less discriminative towards outliers (far away points).  
Here we only show that this property for a special case and as .  
To simplify notation, all integrals are assumed to be over the full domain .  

\paragraph{Two Dirac masses.}
We first consider a special case when  is a Dirac mass at a point  and  is a Dirac mass at a point .  That is they are both single points.  We can then write .  Figure \ref{fig:Euc-bound} illustrates the result of this lemma.  

\begin{lemma}
\label{lem:E2DK}
For any points  it always holds that .  
When  then .  
\end{lemma}
\begin{proof}
First expand  as

Now using that  for 

and 

where the last inequality holds when .  
\end{proof}

\begin{figure}
\begin{center}\includegraphics[width=.9\linewidth]{kernel-dist.pdf}\end{center}
\caption{\label{fig:Euc-bound}
\small \sffamily
Showing that , where the second inequality holds for .  The kernel distance  is shown for  in purple, blue, and red, respectively.  
}
\end{figure}



\paragraph{One Dirac mass.}
Consider the case where one measure  is a Dirac mass at point .    

\begin{lemma}
Consider two probability measures  and  on  where  is represented by a Dirac mass at a point .  
Then  for any , where the equality only holds when  is also a Dirac mass at .
\label{lem:2pts}
\end{lemma}
\begin{proof}
Since both  and  are metrics and hence non-negative, we can instead consider their squared versions:  and 
\vspace{-2mm}

Now use the bound  for  to approximate 

The inequality becomes an equality only when  for all , and since they are both metrics, this is the only location where they are both .  
\end{proof}


\paragraph{General case.}
Next we show that if  is not a unit Dirac, then this inequality holds in the limit as  goes to infinity.  The technical work is making precise how  and how this compares to bounds on  and .  

For simpler exposition, we assume  is a probability measure, that is ; otherwise we can normalize  at the appropriate locations, and all of the results go through.  

\begin{lemma}
For any  we have

\label{lem:Kflip}
\end{lemma}
\begin{proof}
We use the Taylor expansion of .
Then it is easy to see

\end{proof}

This lemma illustrates why the choice of coefficient of  is convenient.  Since then  acts like , and becomes closer as  increases.  
Define  to represent the mean point of measure ;
  to represent the variance of the measure ; and  
.

\begin{lemma}
For any  we have

\label{lem:mean+Var}
\end{lemma}
\begin{proof} 

\end{proof}


\begin{lemma} 
For probability measures  and  on ,  

\label{lem:decomp-kap}
\end{lemma}
\begin{proof}
We use Lemma \ref{lem:Kflip} to expand

After shifting the  term outside, we can use Lemma \ref{lem:mean+Var} (twice) to rewrite

\end{proof}


\begin{theorem}
For any two probability measures  and  defined on  
  
\label{thm:DK-means}
\end{theorem}
\begin{proof} 
First expand 

Finally we observe that since all terms of  are divided by  or larger powers of .  Thus as  increases  approaches  and  approaches , completing the proof.  
\end{proof}

Now we can relate  to  through .  
The next result is a known lower bounds for the Earth movers distance~\cite{Coh99}[Theorem 7].  We reprove it in Appendix \ref{app:stability} for completeness.

\begin{lemma} 
For any probability measures  and  defined on  we have

\label{lem:EMD}
\end{lemma}


We can now combine these results to achieve the following theorem.  

\begin{theorem}
\label{thm:DK-W2}
For any two probability measures  and  defined on  
  and   Thus
.  
\end{theorem}







\subsection{Kernel Distance Stability with Respect to }
\label{subsection:sigma}
We now explore the Lipschitz properties of  with respect to the noise parameter .  
We argue any distance function that is robust to noise needs some parameter to address how many outliers to ignore or how far away a point is that is an  outlier.  For instance, this parameter in  is  which controls the amount of measure  to be used in the distance.  

Here we show that  has a particularly nice property, that it is Lipschitz with respect to the choice of  for any fixed .  The larger  the more effect outliers have, and the smaller  the less the data is smoothed and thus the closer the noise needs to be to the underlying object to effect the inference.  

\begin{lemma}
Let .  We can bound
, 
 and 
 over any choice of .  
\label{lem:h-max}
\end{lemma}
\begin{proof}
The first bound follows from  and  for . 

Next we define


Now to solve the first part, we differentiate  with respect to  to find its maximum over all choices of .  

Where  at ,  and as  approaches .  Thus the maximum must occur at one of these values.  Both  and , while , proving the first part.  

To solve the second part, we perform the same approach on .  

Thus  at  and as  goes to  for .  Both  and .  The minimum occurs at .  
The maximum occurs at .  
\end{proof}



\begin{theorem}
\label{thm:sigma-Lip}
For any measure  defined on  and ,  is -Lipschitz with respect to , for .
\end{theorem}
\begin{proof}
Recall that  is the product measure of any  and , and
define  as .  
It is now useful to define a function  as  

So  and we can write another function as

Now to prove   is -Lipschitz, we can show that  is -semiconcave with respect to , and apply Lemma \ref{lem:semi-Lip}.  This boils down to showing the second derivative of  is always non-positive.  




First we note that since  for any product distribution  of two distributions  and , including when  or  is a Dirac mass, then 

Thus since  is in  for all choices of , , and , then  and .  This bounds the third term in , we now need to use a similar approach to bound the first and second terms.  

Let , so we can apply Lemma \ref{lem:h-max}.







Then we complete the proof using the upper bound of each item of 

\end{proof}

\paragraph{Lipschitz in  for .}
We show that there is no Lipschitz property for , with respect to  that is independent of the measure .  
Consider a measure  for point set  consisting of two points at  and at .
Now consider .  When  then  is constant.  But for  for , then  and 
, which is maximized as  approaches  with an infimum of .  
If  points are at  and  point at , then the infimum of the first derivative of  is .  
Hence for a measure  defined by a point set, the infimum of  and, hence a lower bound on the Lipschitz constant is  where .  









\section{Algorithmic and Approximation Observations}
\label{sec:algo}

\paragraph{Kernel coresets.}
The kernel distance is robust under random samples~\cite{JoshiKommarajuPhillips2011}.  
Specifically, if  is a point set randomly chosen from  of 
size  then 

with probability at least .  We call such a subset  and -kernel sample of .  
Furthermore, it is also possible to construct -kernel samples  with even smaller size of ~\cite{Phillips2013}; in particular in  the required size is .  
Exploiting the above constructions, recent work~\cite{big-kde} builds a data structure to allow for efficient approximate evaluations of  where .  

These constructions of  also immediately imply that  since , and both the first and third term incur at most  error in converting to  and , respectively (see Lemma \ref{lem:KD-samp}).  
Thus, an -kernel sample  of  implies that  (see Lemma \ref{lem:KD-samp-nosq}).

This implies algorithms for geometric inference on enormous noisy data sets.  
Moreover, if we assume an input point set  is drawn iid from some underlying, but unknown distribution , we can bound approximations with respect to .  


\begin{corollary}
Consider a measure  defined on , a kernel , and a parameter .  
We can create a coreset  of size  or randomly sample  points so, with probability at least , any sublevel set  is homotopy equivalent to  for  and .  
\end{corollary}
\begin{proof}
Those bounds are obtained by constructing an -kernel sample~\cite{JoshiKommarajuPhillips2011,Phillips2013}, which guarantees  via Lemma \ref{lem:KD-samp-nosq}.  
Then since , with Theorem \ref{thm:reconstruct} any sublevel set  is homotopy equivalent to .  
\end{proof}

\paragraph{Stability of persistence diagrams.}
Furthermore, the stability results on persistence diagrams \cite{Cohen-SteinerEdelsbrunnerHarer2007} hold for kernel density estimates and kernel distance of  and  (where  is a coreset of  with the same size bounds as above).  If , then , where  is the bottleneck distance between persistence diagrams.  Combined with the coreset results above, this immediately implies the following corollaries.  

\begin{corollary}
Consider a measure  defined on  and a kernel .  
We can create a core set  of size  or randomly sample  points which will have the following properties with probability at least .  
\begin{itemize}
\item .
\item .
\end{itemize}
\end{corollary}


\begin{corollary}
Consider a measure  defined on  and a kernel .  
We can create a coreset  of size  or randomly sample  points which will have the following property with probability at least .  
\begin{itemize}
\item .
\end{itemize}
\end{corollary}



Another bound was independently derived to show an upper bound on the size of a random sample  such that  in \cite{BFLRSW13};  this can, as above, also be translated into bounds for  and .  This result assumes  and is parametrized by a bandwidth parameter  that retains that  for all  using that  and .  This ensures that  is -Lipschitz and that  for any .  Then their bound requires
 random samples.  

To compare directly against the random sampling result we derive from Joshi \etal~\cite{JoshiKommarajuPhillips2011}, for kernel  then .  Hence, our analysis requires , and is an improvement when  or  is not known or bounded, as well as in some other cases as a function of , , , and .   





\section{Discussion}
\label{app:discuss}
We mention here a few other interesting aspects of our results and observations about topological inference using the kernel distance.  They are related to how the noise parameter  affects the idea of scale, and a few more experiments, including with alternate kernels.  


\subsection{Noise and Scale}
Much of geometric and topological reconstruction grew out of the desire to understand shapes at various scales.  A common mechanism is offset based; e.g., -shapes~\cite{Edelsbrunner1993} represent the scale of a shape with the  parameter controlling the offsets of a point cloud.  
There are two parameters with the kernel distance:  controls the offset through the sublevel set of the function, and  controls the noise. 
We argue that any function which is robust to noise must have a parameter that controls the noise (e.g.  for  and  for ).
Here  clearly defines some sense of scale in the setting of density estimation~\cite{Sil86} and has a geometrical interpretation, while  represents a fraction of the measure and is hard to interpret geometrically, as illustrated by the lack of a Lipschitz property for  with respect to .  

There are several experiments below, in Section \ref{sec:exp}, 
from which several insights can be drawn.  
One observation is that even though there are two parameters  and  that control the scale, the interesting values typically have  very close to .  
Thus, we recommend to first set  to control the scale at which the data is studied, and then explore the effect of varying  for values near .  
Moreover, not much structure seems to be missed by not exploring the space of both parameters; 
Figure \ref{fig:gamma-v-sigma} shows that fixing one (of  and ) and varying the other can provide very similar superlevel sets.  
However, it is possible instead to fix  and explore the persistent topological features in the data \cite{EdelsbrunnerLetscherZomorodian2002,EdelsbrunnerHarer2008} (those less affected by smoothing) by varying .  
On the other hand, it remains a challenging problem to study two parameter persistent homology \cite{CarlssonZomorodian2007,CarlssonSinghZomorodian2009} under the setting of kernel distance (or kernel density estimate). 













\subsection{Experiments}
\label{sec:exp}

We consider measures  defined by a point set .  
To experimentally visualize the structures of the superlevel sets of kernel density estimates, or equivalently sublevel sets of the kernel distance, we do the simplest thing and just evaluate  at every grid point on a sufficiently dense grid.  


\paragraph{Grid approximation.}
Due to the -Lipschitz property of the kernel distance, well chosen grid points have several nice properties.  
We consider the functions up to some resolution parameter , consistent with the parameter used to create a coreset approximation .  
Now specifically, consider an axis-aligned grid  with edge length  so no point  is further than  from some grid point .  
Since  when , we only need to consider grid points  which are within  of some point  (or , of coreset  of )~\cite{JoshiKommarajuPhillips2011,big-kde}.  This is at most  grid points total for  a fixed constant.  
Furthermore, due to the -Lipschitz property of , when considering a specific level set at 
\begin{itemize}
\item a point  such that  is no further than  from some  such that , and 
\item every ball  centered at some point  of radius  so that all  has  has some representative point  such that , and hence . 
\end{itemize} 
Thus ``deep" regions and spatially thick features are preserved, however thin passageways or layers that are near the threshold , even if they do not correspond to a critical point, may erroneously become disconnected, causing phantom components or other topological features.  However, due to the Lipschitz property, these can be different from  by at most , so the errors will have small deviation in persistence.



\paragraph{Varying parameter  or .}
\label{sec:exp}

\begin{figure}
\begin{center}
\includegraphics[width=.4\linewidth]{test-005-gamma-2.pdf}
\hspace{10mm}
\includegraphics[width=.4\linewidth]{test-005-sigma-2.pdf}
\end{center}

\caption{\label{fig:gamma-v-sigma} \small \sffamily
Sublevel sets for the kernel distance while varying the isolevel , for fixed  (left) and for fixed isolevel  but variable  (right), with Gaussian kernel.  The variable values of  and  are chosen to make the plots similar.}  
\end{figure}

We demonstrate the geometric inference on a synthetic dataset in  where  points are chosen near a circle centered at  with radius  or along a line segment from  to .  Each point has Gaussian noise added with standard deviation .  The remaining  points are chosen uniformly from .  
We use a Gaussian kernel with .  
Figure \ref{fig:gamma-v-sigma} shows 
(left) various sublevel sets  for the kernel distance at a fixed  and (right) various superlevel sets for a fixed , but various values of , where



\noindent
This choice of  and  were made to highlight how similar the isolevels can be.  



\begin{figure}
\includegraphics[width=.24\linewidth]{kde-lap-2.pdf}
\hspace{0.3mm}
\includegraphics[width=.24\linewidth]{kde-tri-2.pdf}
\hspace{0.3mm}
\includegraphics[width=.24\linewidth]{kde-epan-2.pdf}
\hspace{0.3mm}
\includegraphics[width=.24\linewidth]{kde-ball-2.pdf}

\caption{\label{fig:TEB}  
\small \sffamily
Alternate kernel density estimates for the same dataset as Figure \ref{fig:gamma-v-sigma}.  From left to right, they use the Laplace, triangle, Epanechnikov, and the ball kernel.}
\end{figure}



\paragraph{Alternative kernels.}
We can choose kernels other than the Gaussian kernel in the kernel density estimate, for instance 
\begin{itemize} \denselist
\item the Laplace kernel , 
\item the triangle kernel , 
\item the Epanechnikov kernel , or 
\item the ball kernel (.   
\end{itemize}
Figure \ref{fig:TEB} chooses parameters to make them comparable to the Figure \ref{fig:gamma-v-sigma}(left).  
Of these only the Laplace kernel is \emph{characteristic}~\cite{SGFSL10} making the corresponding version of the kernel distance a metric.  Investigating which of the above reconstruction theorems hold when using the Laplace or other kernels is an interesting question for future work.  


Additionally, normal vector information and even -forms can be used in the definition of a kernel~\cite{glaunesthesis,Vaillant2005,DurrlemanPennecTrouve2008,
DurrlemanPennecTrouve2007,GlaunesJoshi2006,JoshiKommarajuPhillips2011}; this variant is known as the \emph{current distance}.  In some cases it retains its metric properties and has been shown to be very useful for shape alignment in conjunction with medical imaging.  






\subsection{Open Questions}
\label{sec:open}
This work shows it is possible to prove formal reconstruction results using kernel density estimates and the kernel distance.  But it also opens many interesting questions.  

\begin{itemize}
\item For what other types of kernels can we show reconstruction bounds?  The Laplace and triangle kernels are natural choices.  For both the coresets results match those of the Gaussian kernel.  The kernel distance under the Laplace kernel is also a metric, but is not known to be for the triangle kernel.  Yet, the triangle kernel would be interesting since it has bounded support, and may lend itself to easier computation.  

\item The power distance construction in Section \ref{sec:powerdistance} requires a point , which approximates the point with minimum kernel distance.  This is intuitively because it is possible to construct a point set  (say points lying on a circle with no points inside) such that the point  which minimizes the kernel distance and maximizes the kernel density estimate is far from any point in the point set.  
For one, can  be constructed efficiently without dependence on  or ?  

But more interestingly, can we generally approximate the persistence diagram without creating a simplicial complex on a subset of the input points?  We do describe some bounds on using a grid-based technique in Section \ref{sec:exp}, but this is also unsatisfying since it essentially requires a low-dimensional Euclidean space.  

\item Since  is Lipschitz in  and , it may make sense to understand the simultaneous stability of both variables.  What is the best way to understand persistence over both parameters?  

\item We provided some initial bound comparing the kernel distance under the Gaussian kernel and the Wasserstein  distance.  Can we show that under our choice of normalization that , unconstrained?  More generally, how does the kernel distance under other kernels compare with other forms of Wasserstein and other distances on measures?  
\end{itemize}








\paragraph{Acknowledgements}
The authors thank Don Sheehy, Fr\'{e}d\'{e}ric Chazal and the rest of the Geometrica group at  INRIA-Saclay for enlightening discussions on geometric and topological reconstruction.  
We also thank Don Sheehy for personal communications regarding the power distance constructions, 
and Yusu Wang for ideas towards Lemma \ref{lem:monotonicity}.  Finally, we are also indebted to the anonymous reviewers for many detailed suggestions leading to improvements in results and presentation.  


\bibliographystyle{plain}
\bibliography{kernel-refs}





\newpage
\appendix













\section{Details on Distance-Like Properties of Kernel Distance}
\label{app:dist-like}

We provide further details on distance-like properties of the kernel distance.  

\subsection{Semiconcave Properties of Kernel Distance}


We also note that semiconcavity follows quite naturally and simply in the RKHS  for .  

\begin{lemma}
\label{lemma:RKHS-concave}
 is -semiconcave in : the map  is concave. 
\end{lemma}
\begin{proof}
We can write 

Now 

Since the above is twice-differentiable, we only need to show that its twice-differential is non-positive. 
By definition, for a fixed ,  and  are both constant. 
Suppose  and ,
we have .
Since the RKHS  is a vector space with well-defined norm , 
the above is a concave parabolic function.  
\end{proof}

However, this semiconcavity in  is not that useful. 
For unit weight elements , an element  such that  is a weighted point set with a point at  with weight  and another at  with weight . 
Lemma \ref{lemma:RKHS-concave} only implies that 
.  


\subsection{Kernel Distance is Proper}

We use two more general, but equivalent definitions of a proper map.  
Definition (i): A continuous map  between two topological spaces is \emph{proper} if and only if the inverse image of every compact subset in  is compact in  (\cite{Lee2000}, page 84; \cite{Lee2003}, page 45). 
Definition (ii): a continuous map  between two topological manifolds is proper if and only if for every sequence  in  that escapes to infinity,  escapes to infinity in  (\cite{Lee2003}, Proposition 2.17).
Here, for a topological space , a sequence  in  \emph{escapes to infinity} if for every compact set , there are at most finitely many values of  for which  (\cite{Lee2003}, page 46). 


\begin{lemma}[Lemma \ref{lem:proper}] 
 is proper. 
\label{lem:proper-app}
\end{lemma}
\begin{proof}
To prove that  is proper, we prove the following two claims: 
(a) A continuous function  (where c is a constant) is proper, if for any sequence  in  that escapes to infinity, the sequence 
 tends to  (approaches  in the limit);
(b) Let  and one needs to show that for any sequences  that escapes to infinity, the sequence  tends to ; or equivalently, 
 tends to .

We prove claim (a) by proving its contrapositive.   
If a continuous function  is not proper, then there exists a sequence  in  that escapes to infinity, such that the sequence  does not tend to . 
Suppose  is not proper, this implies that there exists a constant  such that  is not compact (based on properness definition (i)) and therefore either not closed or unbounded. 
We first show that  is closed. 
We make use of the following theorem (\cite{KolmogorovFominSilverman1975}, page 88, Theorem 10'):
A mapping  of a topological space  into a topological space  is continuous if and only if the pre-image  of every closed set  is closed in .  
Since  is continuous, it implies that the pre-image of every closed set  is closed in . Therefore,  is closed, therefore it must be unbounded.
Since every unbounded sequence contains a monotone subsequence
that has either  or  as a limit, 
therefore  contains a subsequence  that tends to an infinite limit. 
In addition, as elements in  escapes to infinity,  tends to  and does not tend to .
Therefore (a) holds by contraposition. 

To prove claim (b), we need to show that for any sequence  that escapes to infinity,  tends to .  
For each , define a radius  and define a ball  that is centered at the origin  and has radius .  As  goes to infinity,  increases until for any fixed arbitrary ,  we have  and thus .  Furthermore, let , so .  Thus also as  goes to infinity,  increases until for any  we have .  
We now decompose .  
Thus for any , as  goes to infinity, the first term is at most  since all  and the second term is at most  since  and .  
Since these results hold for all , as  goes to infinity and  goes to , 
 goes to .  

Combine (a) with (b) and the fact that  is a continuous (in fact, Lipschitz) function, we obtained the properness result. 
\end{proof}



\section{-Approximation of the Kernel Distance}
\label{app:KD-approx}

Here we make explicit the way that an -kernel sample approximated the kernel distance.  
Recall that if  is an -kernel sample of , then .  


\begin{lemma}
\label{lem:KD-samp}
If  is an -kernel sample of , then . 
\end{lemma}
\begin{proof}
First expand .  
Replacing  with , 
the first term is unaffected.  The second term is bounded, 

Similar results hold by switching  with  in the above inequality, that is, 
.   
And for the third term we have similar inequality, .
Combining all three terms,  we have the desired result: 
  . 
\end{proof}


\begin{lemma}
\label{lem:KD-samp-nosq}
If  is an -kernel sample of , then . 
\end{lemma}
\begin{proof}
By Lemma \ref{lem:KD-samp} this condition on  implies that .  
We then use a basic fact for values  and .  
\\
 .  This follows since
.

We now prove the main result as an upper and lower bound using for any .  
We first use  and expand  to obtain


Now we use  and expand  to obtain

Hence for any  we have .  
\end{proof}







\section{Power Distance Constructions}
\label{app:power}
Recall we want to consider the following \emph{power distance} using  (as weight) for a measure  associated with a subset  and metric  on ,

We consider a particular choice of the distance metric  which leads to a kernel version of the power distance


Recall that .  In this section, we will always use the notation , and when  or  are points (e.g.  is a Dirac mass at  and  is a Dirac mass at ), then we will just write .  This will be especially helpful when we apply the triangle inequality in several places.  



\subsection{Kernel Power Distance on Point Set }
\label{app:pow-justP}
Given a set  defining a measure of interest , it is of interest to consider if  is multiplicatively bounded by .  Theorem \ref{thm:powK-low} shows that the lower bound holds.  In this section we try to provide a multiplicative approximation upper bound.  

Let .
We can start with Lemma \ref{lem:pK-up} which reduces the problem finding a multiplicative upper bound for  in terms of .  
However, we are not able to provide very useful bounds, and they require more advanced techniques that the previous section.  In particular, they will only apply for points  when  is large enough; hence not well-approximating the minima of .  

For simplicity, we write  as . 

The difficult case is when  is very small, and hence  is very small.  So we start by developing tools to upper bound  using , a point which only provides a worse approximation that .  

We first provide a general result in a Hilbert space (a refinement of a vector space \cite{Daume2004}), and then next apply it to our setting in the RKHS.  




\begin{lemma}
\label{lem:vec-shrink}
Consider a set  of vectors in a Hilbert space endowed with norm  and inner product .  Let each  have norm .  
Consider weights  such that  and .  Let .  Let .
Then
 
\end{lemma}
\begin{proof}
Recall elementary properties of inner product space: 
, 
, 
. 
By definition of , 
for any , 

We can decompose  (based on linearity of an inner product space) as 

The last inequality holds by .  
Then since  we can solve for  as 

\end{proof}


\begin{lemma}
\label{lem:HK-shrink}
Let , then
.
\end{lemma}
\begin{proof}
Let  map points in  to the reproducing kernel Hilbert space (RKHS)  defined by kernel .  This space has norm  defined on a set of points  and inner product .  Let  be the representation of a set of points  in .  
Note that .  
We can now apply Lemma \ref{lem:vec-shrink} to  with weights  and , and norm .  Hence
.
\end{proof}


\begin{lemma}
\label{lem:outa-root}
For any  and any , then .
\end{lemma}
\begin{proof}
We expand the square of the desired result

After subtracting  from both sides, it is equivalent to .  This holds since  and  are always nonnegative.  
\end{proof}


\begin{lemma}
\label{lem:sq-bnd}
 for . 
\end{lemma}
\begin{proof}
Refer to Figure \ref{fig:kern-pow} for geometric intuition in this proof.  
Let  be a measure that is  for all ; thus it has a norm .  
We can measure the distance from  to  and , noting that  and .  Thus by triangle inequality, Lemma \ref{lem:HK-shrink}, and Lemma \ref{lem:outa-root},   




\begin{figure}
\centering{\includegraphics{kern-pow.pdf}}
\vspace{-5mm}
\caption{\label{fig:kern-pow}
\small \sffamily 
Illustration of , , , , and  as vectors in a RKHS.  Note we have omitted the  and  maps to unclutter the notation.  
}
\end{figure}

We now assume that  and show this is not possible.  
First observe that .  
These expressions imply that , and thus 

a contradiction.  
The last steps follows by setting 

and solving for , 

Since , so we have .  
\end{proof}


Recall that an -kernel sample  of  satisfies



\begin{theorem}
\label{thm:powK-up-eh}
If  then . 
If  is an -kernel sample of  then 
.  
\end{theorem}
\begin{proof}
We combine Lemma \ref{lem:sq-bnd} with Lemma \ref{lem:pK-up} to achieve

\emph{Aside:  Note that the first  is squared and the second is not.}
If  then  we have

Let .  We have 


Since , via Lemma \ref{lem:KD-samp}. 
We obtain, 

\end{proof}




\subsection{Approximating the Minimum Kernel Distance Point}
\label{app:phat+}
The goal in this section is to find a point that approximately minimizes the kernel distance to a point set .  We assume here  contains  points and describes a measure made of  Dirac mass at each  with weight  (this is the empirical measure  defined in Section \ref{sec:kernel}).  
Let . 
Since , for simplicity in notation, we work with point set  instead of  for the remaining of this section. 
That is, we define .  
Note that  is chosen over all of , as the bound in Theorem \ref{thm:powK-up-eh} is not sufficient when choosing a point from .  
In particular, for any , we want a point  such that .  

Note that Agarwal \etal~\cite{AHKS13} provide an algorithm that with high probability finds a point  such that  in time .  However this point  is \emph{not} sufficient for our purpose
(that is,  does not satisfy the condition  ),  since  yields  

since in general it is not true that , as would be required.  

First we need some structural properties. 
For each point , define a radius \}, where  is a ball of radius  centered at .  In other words, it is the largest radius such that at most half of points in  are within .  
Let  be the point in  such that .  
In other words,  is a point such that no more than  points in  satisfy .  
Finally it is useful to define  which is  where ; in particular .  

We now need to lower bound  in terms of .
Lemma \ref{lem:sq-bnd} already provides a bound in terms of the closest point for any .  We follow a similar construction here.  


\begin{lemma}
\label{lem:vec-shrink2}
Consider a set  of vectors in a Hilbert space endowed with norm  and inner product .  Let each  have norm .  
Consider weights  such that  and .  Let .  Define a partition of  with  and  such that  is the smallest set such that , and for all  and  we have .  Let .
Then
 
\end{lemma}
\begin{proof}
For ease of notation, we assume that  for all , and let .    Let .  
Let  be a norm  vector that has .
Since   and , 
let  and  (for . 
By definition, we also have . 
We can decompose  as 

The last inequality holds by .  
Then since  we can solve for  as 

\end{proof}


\begin{lemma}
\label{lem:HK-shrink2}
Using  as defined above, then
.
\end{lemma}
\begin{proof}
Let  map points in  to the reproducing kernel Hilbert space (RKHS)  defined by kernel .  This space has norm  defined on a set of points  and inner product .  Let  be the representation of a set of points  in .  
Note that .  
We can now apply Lemma \ref{lem:vec-shrink2} to  with weights  and , and norm .  
Finally note that we can use  since  represents the set of points which are further or equal to  than is .   
In addition, by the property of RKHS, . 
Hence
.
\end{proof}





\begin{lemma}
\label{lem:sq-bnd2}
.  
\end{lemma}
\begin{proof}
Refer to Figure \ref{fig:kern-pow} for geometric intuition in this proof.  
Let  be a measure that is  for all ; thus it has a norm .  
We can measure the distance from  to  and , noting that  and .  Thus by triangle inequality, Lemma \ref{lem:HK-shrink2}, and Lemma \ref{lem:outa-root} 

\end{proof}



Now we place a net  on ; specifically, it is a set of points such that for some  that
 (we refer to this inequality as the \emph{net condition}, therefore,  is a set of points such that some points in it satisfy the net condition).  Since  is -Lipschitz, we have 
. 
This ensures that some point  satisfies , and can serve as .  
In other words,  is guaranteed to contain some point  that can serve as .  

Note that  must be in , the convex hull of .  Otherwise, moving to the closest point on  decreases the distance to all points, and thus increases , which cannot happen by definition of .  Let  be the diameter of  (the distance between the two furthest points).  Clearly for some  we must have .  

Also note that  must be within a distance  to some , otherwise for , we can bound
 which means  is not a maximum.  
The first inequality is by definition of , the second by assuming .  


Let  be the ball centered at  with radius .  
Let .  
So  must be in .  
We describe a net  construction for one ball ; that is for any  such that  is the closest point to , then some point  satisfies  .  Thus if this point , the correct property holds, and we can use the corresponding  as .  
Then , and is at most  times the size of .  
Let  be the smallest integer  such that .  
The net  will be composed of , where . 

Before we proceed with the construction, we need an assumption:  That  is a bounded quantity, it is not too small.  That is,  no point has \emph{more} than half the points within an absolute radius .  We call  the \emph{median concentration}.  

\begin{lemma}
\label{lem:N0}
A net  can be constructed of size  so that all points  satisfy  for some .
\end{lemma}
If , then such a point satisfies the net condition, that is there is a point  such that   
.  
\begin{proof}
For all points , they must have , otherwise  is completely inside , and cannot have enough points.   
Within  we place the net  so that all points  satisfy  for some .  
Now , and since  (for , via Lemma \ref{lem:E2DK}), thus the net ensures if , then some  is sufficiently close to .  

Since  fits in a squared box of side length , then we can describe  as an axis-aligned grid with  points along each axis.  We define two cases to bound .  When  then we can set

Otherwise, 

Then we need .  
\end{proof}

When  we still need to handle the case for  where the annulus .  
For a point  if  then .  We only worry about the net  on  for these points where  is the closest point, the others will be handled by another  for  and .  

Recall  is the smallest integer  such that .  

\begin{lemma}
\label{lem:Ann-net}
A net  can be constructed of size  so that all points  where ,  satisfy  for some .  
\end{lemma}

If , then such a point satisfies the net condition, that is there is a point  such that   
.  


\begin{proof}
We now consider the  annuli  which cover .  Each  has volume .  For any  we have , so the Euclidean distance to the nearest  can be at most . Thus we can cover  with a net  of size  based on two cases again.  If  then

Otherwise

Since , then
the total size of , the union of all of these nets, is .  
In the first case  dominates the cost and in the second case it is .  
\end{proof}

Thus the total size of  is  where .  
It just remains to bound .  Given that no more than  points are collocated on the same spot (which already holds by  being a bounded quantity), then for all , .  
The value  is known as the \emph{spread} of a point set, and it is common to assume it is an absolute bounded quantity related to the precision of coordinates, where  is not too large.  Thus we can bound .  

\begin{theorem}
\label{thm:phat+}
Consider a point set  with  points, spread , and median concentration .  
For any , in time  we can find a point  such that .  
\end{theorem}
\begin{proof}
Using Lemma \ref{lem:N0} and Lemma \ref{lem:Ann-net} we can build a net  of size  such that some  satisfies .  Lemma \ref{lem:sq-bnd2} ensures that this  satisfies  since  is -Lipschitz.  

We can find such a  and set it as  by evaluating  for all  and taking the one with largest value.  This takes  for each . 
\end{proof}


We claim that in many realistic settings .  In such a case the algorithm runs in  time.  
If , then \emph{over} half of the measure described by  will essentially behave as a single point.  In many settings  is drawn uniformly from a compact set , so then choosing  so that more than half of  has negligible diameter compared to  will cause that data to be over smoothed.  
In fact, the definition of  can be modified so that this radius never contains more than any  points for any constant , and the bounds do not change asymptotically.  





\section{Details on Reconstruction Properties of Kernel Distance}
\label{app:recon}

In this section we provide the full proof for some statements from Section \ref{sec:recon}.  

\subsection{Topological Estimates using Kernel Power Distance}
\label{subsec:approximation}
For persistence diagrams of sublevel sets filtration of  and the weighted Rips filtration  to be well-defined, we need the technical condition (proved in Lemma \ref{lemma:q-tame} and \ref{lemma:q-tame-rips}) that they are -tame. 
Recall a filtration  is \emph{-tame} if for any , the homomorphism between 
 and  induced by the canonical inclusion has finite rank \cite{ChazalCohen-SteinerGlisse2009,ChazalSilvaGlisse2013}. 

\begin{lemma}
The sublevel sets filtration of  is -tame. 
\label{lemma:q-tame}
\end{lemma}
\begin{proof}
The proof resembles the proof of -tameness for distance to measure sublevel sets filtration (Proposition 12, \cite{BuchetChazalOudot2013}). 
We have shown that  is -Lipschitz and proper. Its properness property implies that any sublevel set  (for ) is compact. 
Since  is triangulable (i.e. homeomorphic to a locally finite simplicial complex), there exists a homeomorphism  from  to a locally finite simplicial complex . 
For any , since  is compact, we consider the restriction of  to a finite simplicial complex  that contains . 
The function  is continuous on , 
therefore its sublevel set filtration is -tame based on 
Theorem 2.22 of \cite{ChazalSilvaGlisse2013}, which states that the sublevel sets filtration of a continuous function (defined on a realization of a finite simplicial complex) is -tame. 
Extending the above construction to any , the sublevel sets filtration of  is therefore -tame. 
As homology is preserved by homeomorphisms , this implies that the sublevel sets filtration of  is -tame. 
\end{proof}

Setting , Lemma \ref{lemma:q-tame} implies that the sublevel sets filtration of  is also -tame. 

\begin{lemma}
The weighted Rips filtration  is -tame for compact subset . 
\label{lemma:q-tame-rips}
\end{lemma}
\begin{proof}
Since  is compact subset of ,  is -tame based on Proposition 32 of \cite{ChazalSilvaGlisse2013}, which states that the weighted Rips filtration with respect to a compact subset  in metric space and its corresponding weight function is -tame. 
\end{proof}

Setting , , Lemma \ref{lemma:q-tame-rips} implies that the weighted Rips filtration  is well-defined. 



\subsection{Inference of Compact Set  with the Kernel Distance}
\label{app:infer}

Suppose  is a uniform measure on a compact set  in . 
We now compare the kernel distance  with the distance function  to the support  of . We show how  approximates , and thus allows one to infer geometric properties of  from samples from . 

For a point , the distance function  measures the minimum distance between  and any point in , . 
The point  that realizes the minimum in the definition of  is the \emph{orthogonal projection} of  on .  The location of the points  that have more than one projection on  is the \emph{medial axis} of  \cite{Merigot2010}, denoted as . Since  resides in the unbounded component , it is referred to as the \emph{outer medial axis} similar to the concept found in \cite{Dey2007}. The \emph{reach} of  is the minimum distance between a point in  and a point in its medial axis, denoted as . 
Similarly, one could define the medial axis of  (i.e. 
the \emph{inner medial axis} which resides in the interior of ) following definitions in \cite{Lieutier2004}, and denote its associated reach as . The concepts of reach associated with the inner and outer medial axis of  capture curvature information of the compact set. 


Recall that a generalized gradient and its corresponding flow to a distance function are described in \cite{ChazalCohen-SteinerLieutier2009} and later adapted for distance-like functions in \cite{ChazalCohen-SteinerMerigot2011}. 
Let  be a distance function associated with a compact set  of . 
It is not differentiable on the medial axis of . 
It is possible to define a \emph{generalized gradient function}  coincides with the usual gradient of  where  is differentiable, and is defined everywhere and can be integrated into a continuous flow .   
Such a flow points away from , towards local maxima of  (that belong to the medial axis of ) \cite{Merigot2010}. 
The integral (flow) line  of this flow starting at point in  can be parameterized by arc length, , and we have .

\begin{figure}
\includegraphics[width=0.3 \linewidth]{Bx+S.pdf}
\hspace{.2in}
\includegraphics[width=0.3\linewidth]{Bx+S+H.pdf}
\hspace{.2in}
\includegraphics[width=0.3 \linewidth]{Rover6.pdf}
\caption{\label{fig:inference}
Illustrations of the geometric inference of  from  at three scales.}
\end{figure}

\begin{lemma}[Lemma \ref{lem:monotonicity}]
Given any flow line  associated with the generalized gradient function ,  is strictly monotonically increasing along  for  sufficiently far away from the medial axis of , for  and . 
Here  denotes a ball of radius ,  ,  and suppose .  
\label{lem:monotonicity-app}
\end{lemma}
\begin{proof}
Since  is always positive, and  where  is a constant that depends only on , , and , then it is sufficient to show that  is strictly monotonically decreasing along .  

Let  be the negative of the direction of the flow line  at  (i.e  is a unit vector that points towards ). We show that  is strictly monotonically increasing along . 
Informally, we will observe that all parts of  that are ``close'' to  are in the direction , and that these parts dominate the gradient of  along .  
We now make this more formal by describing two quantities,  and , illustrated in Figure \ref{fig:inference}.  

For a point , let ; since  is not on the medial axis of ,  is uniquely defined and  points in the direction of . 
First, we claim that there exists a ball  of radius  incident to  that is completely contained in .
This holds since . 
In addition, since , no part in  is further than  from .
Second, we claim that no part of  within  ()  of  (this includes ) is in the halfspace  with boundary passing through  and outward normal defined by .  
To see this, let  be the center of a ball with radius  that is incident to  but not in , refer to such a ball as . This implies that points ,  and  are colinear. Then a ball centered at  with radius  should intersect  outside of , and in the worst case, on the boundary of . 
This holds as long as ; see Figure \ref{fig:inference}.  
Define .    


Now we examine the contributions to the directional derivative of  along the direction of  from points in  and , respectively. 
Such a directional derivative is denoted as . 
Recall  and  is a uniform measure on , . 
For any point , we define  
. 
Therefore . 

We now examine the contribution to  from points in , 
.  First, for all points , since , we have . 
Second, at least half of points  (that covers half the volume of ) is at least  away from , and correspondingly for these points . 
We have . Given , we have .   
Denote . 

We now examine the contribution to  from points in , 
. For any point  (including ), 
. 
Let  so we have . 
Since this bound on  is maximized at , under the condition , we can set  to achieve the bound  for  (that is, for all ).  
Now we have , leading to . Denote . 

Since only the points  could possibly reside in  and thus can cause  to be negative, we just need to show that .  
This can be confirmed by plugging in , and using some algebraic manipulation.  
\end{proof}







\section{Lower Bound on Wasserstein Distance}
\label{app:stability}

We note the next result is a known lower bounds for the Earth movers distance~\cite{Coh99}[Theorem 7].  We reprove it here for completeness.  

\begin{lemma}[\textbf{Lemma.} \ref{lem:EMD}]
For any probability measures  and  defined on  we have

\end{lemma}
\begin{proof}
Let  describes the optimal transportation plan from  to .  Also let  be the unit vector from  to .   Then we can expand 

The first inequality follows since  is the length of a projection and thus must be at most .  
The second inequality follows since that projection describes the squared length of mass  along the direction between the two centers  and , and the total sum of squared length of unit mass moved is exactly .  Note the left-hand-side of the second inequality could be larger since some movement may cancel out (e.g. a rotation).  
\end{proof}







\end{document}