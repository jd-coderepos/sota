\documentclass{article} \usepackage{iclr2023_conference_arxiv,times}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsmath}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[algo2e,ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{eqparbox}
\renewcommand{\algorithmiccomment}[1]{\eqparbox{COMMENT}{\color{gray}\#\space#1}}
\usepackage{elasticrow}
\usepackage{tikz}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{pbox}
\usepackage{verbatim}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\usepackage{enumitem}

\setcitestyle{numbers,square}

\def\imagepadding{0.2cm}
\def\imagepaddingy{0.15cm}
\def\imagepaddingtiny{0.1cm}
\def\imagepaddingtinyy{0.05cm}

\newcommand{\method}{Loci}
\newcommand{\changed}{\textcolor{black}}

\title{\changed{Learning What and Where: \\ Disentangling Location and Identity Tracking Without Supervision}}





\author{Manuel Traub \\
  Neuro-Cognitive Modeling\\
  University of Tübingen\\
  \And 
  Sebastian Otte\\
  Neuro-Cognitive Modeling\\
  University of Tübingen\\
  \And 
  Tobias Menge\\
  Neuro-Cognitive Modeling\\
  University of Tübingen\\
  \AND
  Matthias Karlbauer\\
  Neuro-Cognitive Modeling\\
  University of Tübingen\\
  \And 
  Jannik Thümmel\\
  ML in Climate Science\\
  University of Tübingen\\
  \And 
  Martin V. Butz\\
  Neuro-Cognitive Modeling\\
  University of Tübingen\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\thispagestyle{fancy}
\fancyhead[l]{Accepted at ICLR 2023}

\begin{abstract}
Our brain can almost effortlessly decompose visual data streams into background and salient objects. Moreover, it can anticipate object motion and interactions, which are crucial abilities for conceptual planning and reasoning.
Recent object reasoning datasets, such as CATER, have revealed fundamental shortcomings of current vision-based AI systems, particularly when targeting explicit object representations, object permanence, and object reasoning. 
Here we introduce a self-supervised LOCation and Identity tracking system (Loci), which excels on the CATER tracking challenge.
Inspired by the dorsal and ventral pathways in the brain, Loci tackles the binding problem by processing separate, slot-wise encodings of `what' and `where'.
Loci's predictive coding-like processing encourages active error minimization, such that individual slots tend to encode individual objects.
Interactions between objects and object dynamics are processed in the disentangled latent space.
Truncated backpropagation through time combined with forward eligibility accumulation significantly speeds up learning and improves memory efficiency. 
Besides exhibiting superior performance in current benchmarks, Loci effectively extracts objects from video streams and separates them into location and Gestalt components.
We believe that this separation offers a representation that will facilitate effective planning and reasoning on conceptual levels.
\footnote{Source Code: \url{https://github.com/CognitiveModeling/Loci}}
\end{abstract}

\section{Introduction}
Human reasoning about visual scenes is characterized by segmenting a scene into individual entities and their interactions \citep{Barsalou:1999,Butz:2017,Duncan:1984}. This ability presents a non-trivial challenge for computational models of cognition \citep{greff2020binding,Malsburg:1995}: the binding problem \citep{Treisman:1996}.
Originally proposed and discussed mainly in cognitive neuroscience, the binding problem describes the fundamental challenge of how to bind features into objects, thereby segregating them from the background, and encoding them by means of compressed stable neural attractors \citep{Barsalou:2003a,Jordan:1986,Kaltenberger:2022,Rao:1999}.

Recent years have seen revolutionary progress in the ability of connectionist models to operate on complex natural images and videos \citep{Gatys:2016,Krizhevsky:2012,Meinhardt:2022}.
Yet, neural network models are not capable of addressing the binding problem in its full generality \citep{greff2020binding}. Indeed, recent work on synthetic object reasoning datasets like CLEVR, CLEVRER, or CATER \cite{girdhar2019CATER,johnson2017clevr,yi2019clevrer} suggests that state-of-the-art video reasoning systems \cite{carreira2017quo,wang2018non,yang2020temporal} still struggle to model fundamental physical object properties, such as hollowness, blockage, or object permanence---concepts that children develop during the first few months of their lives \cite{Baillargeon:1985,Mandler:2004}.

In a comprehensive review on the binding problem in the context of neural networks, \citet{greff2020binding} define three main challenges for solving the problem: \emph{representation}, \emph{segregation}, and \emph{composition}.
\emph{Representation} refers to the challenge to effectively represent the essential properties of an object, including its appearance and potential dynamics. We will refer to these properties as the `Gestalt' of an object \citep{Koffka:2013,Wagemans2012ACO1, Wagemans2012ACO2}.
Object representations should be separable to avoid interference.
Moreover, their location and shape should be disentangled to enable compositional recombinations.
Meanwhile, they should share a common format to enable general purpose reasoning. 
\emph{Segregation} describes the challenge to extract particular object encodings from a perceived scene. 
This extraction should be done context- and task-dependently in order to identify currently relevant entities. 
The essential feature of a good segregation is that it allows effective dynamic predictions of the whole, rather than the parts.
\emph{Composition} characterizes the challenge to develop object representations that enable meaningful re-combinations of object properties.
Moreover, these properties should enable the prediction of object interaction consequences, depending on the respective object properties. 
As a result, compositional representations allow conceptual reasoning about both object properties as well as relations and interactions between objects. 

We introduce a novel \textbf{Loc}ation and \textbf{I}dentity tracking system.
\changed{While observing video stream data, Loci disentangles object identities (`what') from their spatial properties (`where') in a fully unsupervised, autoregressive manner.
It is motivated by the distinction between}
ventral and dorsal processing pathways in our brain \cite{Mishkin:1983,Ungerleider:1994}.
\changed{
The key contribution of Loci lies in how object-specific information is disentangled and recombined: 
\begin{enumerate}[label=\textbf{(\roman*)}]
    \item Loci fosters slot-respective object persistence over time thanks to a novel combination of slot-specific input channels, temporal slot-interactive predictions via self-attention \citep{vaswani2017attention} followed by GateL0RD-RNN \citep{gumbsch2021sparsely}, and object permanence-oriented loss regularization techniques.
    \item Our slot-decoding strategy combines object-specific Gestalt codes with parameterized Gaussians in a, to the best of our knowledge, novel manner. This combination fosters the emergent explication of an object's size, its position, and current occlusions.
    \item We improve sample and memory efficiency by training Loci's recurrent modules by means of time-local backpropagation combined with forward propagation of eligibility traces.
\end{enumerate}
As a main result, we observe superior performance on the CATER benchmark: Loci outperforms previous methods by a large margin with an order of magnitude fewer parameters.  
Additional evaluations on moving MNIST, an aquarium video footage, as well as on the CLEVRER benchmark underline Loci's contribution towards a self-organized, disentangled identification and localization of objects as well as an effective processing of object interaction dynamics from video data.
}
\section{Related Work}
Previous work by \cite{locatello18assumptions} has emphasized that, in general, unsupervised object representation learning is impossible because infinitely many variable models are consistent with the data.
Inductive biases are thus necessary to ensure the effective learning of a system that segregates a visual stream of information into effective, compositional representations \cite{greff2020binding}.
Accordingly, we review related work in the light of the binding problem and their relation to the proposed Loci system.

\paragraph{Representation}
A powerful choice of an encoding format is the formulation of `slots', which share the encoding module but keep the codes of individual objects separate from one another.
To ensure a common format between the slot-wise encodings, 
typically, slot-respective encoder modules share their weights \citep{bertinetto2016fully, Locatello:2020, vaswani2017attention}.
To assign individual objects to individual slots, though, the system needs to break slot symmetry.
Recurrent neural networks have been used to disentangle encodings or assignments \citep{burgess19monet, engelcke19genesis, eslami16air,  greff19multiobject, Locatello:2020}.
Other mechanisms explicitly separate spatial slot locations \citep{crawford19pineau, jiang20scalor, lin20space}, which we also do in Loci. 
However, instead of treating every spatial location as a potential object, each slot has a spotlight, which is designed to approximate the object's center. 
\changed{To further foster}
a compositional object representation,
Loci enforces disentanglement of \changed{`what' from `where' by separating an object's Gestalt code---mainly representing shape and surface pattern---from its} location, size (visual extent), and priority (current visibility with respect to other objects).
\changed{This stronger disentanglement and more complex `where' representation is related to work that models selective visual attention, realizing partially size-invariant tracking of one particular entity, such as a pedestrian \cite{Denil:2012,Kahou:2017,Ranzato:2014}.
Advancing this work, Loci tracks multiple objects in parallel, imposes interactive, object-specific spot-lights, and enables more compressed, object-specific appearance representations due to its novel way of combining `what' and `where' for decoding.}

\paragraph{Segregation}
Segregating object instances from images is traditionally solved via bounding box detection \citep{liu2016ssd,redmon2016you}, where more advanced techniques extract additional masks for instance segmentation \citep{carion2020end,dai2016instance,he2017mask}. Through slot-attention mechanisms, recent unsupervised approaches partition image regions into separate slots to represent distinct objects \citep{greff2019multi,locatello2020object}. To segregate objects from images, \citet{burgess2019monet} and \citet{von2020towards} combine a soft attention (mask) approach with encoder, recurrent attention, and decoder modules, where slots compete for encoding individual objects. 
Loci pursues a similar approach but segregates objects even further to encourage object-respective `what' and `where' representations, where the latter additionally disentangle location, size, and approximate depth.
Slot-respective masks compete via softmax attention to actively minimize the prediction error of the visual content.
Moreover, a pre-trained background model separates potentially interesting from uninteresting regions.
While we keep the background modeling rather simple in this work, more advanced techniques may certainly be applied in the near future 
\citep{ehrhardt2020relate,nguyen2020blockgan,niemeyer2021giraffe,van2020investigating}.

\paragraph{Composition}
Compositional reasoning in our model builds on two modules, which process object-to-object interactions and object dynamics.
Object-to-object interactions are modelled using Multi-Head Self-Attention (MHSA) \citep{vaswani2017attention}, in close relation with \citep{elsayed22savi++,kipf21savi}. 
Alternative approaches employ message passing neural networks to simulate object interactions \citep{battaglia2016interaction,chang2016compositional,janner2018reasoning}. An other promising approach uses Neural Production Systems to dynamically select a learnable rule to model object interactions \citep{alias2021neural}.
Object dynamics are modelled using a recurrent GateL0RD module \citep{gumbsch2021sparsely}. 
GateL0RD is specifically designed to apply latent state updates sparsely, which encourages the maintenance of stable object encodings over long periods of time.
Previous approaches have also employed recurrent structures to propagate slot-wise encodings forward in time \citep{jiang20scalor,kosiorek18sqair}.
Although some previous works have combined recurrent structures with attention \citep{goyal2019recurrent,goyal2020object}, recent slot-attention frameworks tend to adopt fully auto-regressive designs based on transformers without explicit internal state maintenance \citep{elsayed22savi++,kipf21savi,meinhardt21trackformer}.


\paragraph{Tracking models}
While our primary goal is to separate object location and Gestalt representations, a successful extraction of object positions will likely facilitate object tracking; a wide area of research on its own \citep{weis2021benchmarking}. 
Modern state-of-the-art methods for object tracking rely on features extracted via attention modules that are typically applied autoregressively on individual frames \citep{chen21transt,meinhardt21trackformer}. 
Again, these approaches do not explicitly foster a separation of `what' from `where', which potentially limits their applicability and accuracy in distractive environments, where humans still maintain high tracking skills as recently shown in \citep{linsley21tracking}. Loci maintains separate object encodings and thus copes with the presence of distractors more readily.
The tracking model proposed in \cite{linsley21tracking} is similar in spirit to Loci, extracting object encodings and propagating them with a competition mechanism and a recurrent module. However, to model complex objects their model relies on additional supervision.
Disentangling movement and appearance is a common principle in video models with a notion of optical flow \citep{liu21objectness}. But optical flow based methods \changed{are not designed to deal} with occlusions as they only represent parts of the scene that are currently visible.
In contrast, Loci is able to maintain the encoding of an object even under occlusion. 


\paragraph{The CATER benchmark}
We evaluate  Loci mainly on the CATER challenge \cite{girdhar2019CATER}.
Previous SOTA methods on this challenge, like Multi-Hopper or OpNet \cite{shamsian2020learning,zhou2021hopper}, have focused on reusing well-established neural network building blocks. 
Others have attempted to build their system on top of a supervised pre-trained bounding-box-based object detector \cite{carion2020end,zhu2018distractor}.
\changed{Most recently, current best results were reported using a video-level TFCNet \citep{zhang2022tfcnet}, which makes efficient use of spatio-temporal convolutions on long videos.} 
In contrast, Loci effectively combines an entity-oriented representational format with several kinds of interactive neural processing modules. 
\changed{While Loci disentangles location from identity when} trained in a fully self-supervised manner, it can be further fine-tuned with supervision to minimize location-specific prediction error. 
\changed{As a consequence of the strongly inductive bias focused design, our method achieves strong performance with a fraction of the parameters of other models.}

\section{Methods}

The core idea of Loci involves two major aspects: First, distributing the visual input across recruitable slot-modules, which try to explain the ongoing scene dynamics in a compositional, mutually exclusive manner. A similar principle was shown to be a powerful inductive bias for emergent time series decomposition \cite{Otte:2019}.
Second, splitting the encoding of a moving entity within a slot-module into its `what' and `where' components. 
\changed{These components are represented as a latent Gestalt vector (`what'), which can intuitively be understood as a symbolic encoding of an object’s appearance, including its shape, color, size, texture, and other kinds of visual properties, and a latent spatial code (`where'), which explicates location, size, and relative priority \cite{Mishkin:1983,Ungerleider:1994}.}


Moreover, we use the Siamese network approach implemented in slot-attention \cite{Locatello:2020}, sharing weights across slots.
In fact, Loci employs the same encoding and same decoding network for every slot starting from the raw image.
Each encoder slot  generates output Gestalt vectors  and position codes .
Entity dynamics and interactions are then processed based on these pairs of latent Gestalt and location encodings. 
We predict the future states of latent encodings in a transition module and allow cross-slot interactions via multi-head self-attention.

\autoref{fig:arch} shows the main components of Loci; information processing in one iteration from left to right. 
A detailed algorithmic description is provided in Appendix~\ref{sec:fullLoci}.
Here, we provide an overview and then detail the loss computation and training procedure. 


\begin{figure}[!b]
\includegraphics[width=\textwidth]{figures/loci/loci.pdf}
  \caption{Loci's main processing loop has three components:
\changed{
  Individual entitites  are processed by a \emph{slot-wise encoder module} to generate the disentangled latent Gestalt code  and position code .
The \emph{transition module} consists of  alternating blocks of multi-head self-attention (modeling slot-to-slot interactions) and GateL0RD (predicting within-slot dynamics).
The resulting Gestalt and position codes are finally combined by the \emph{slot-wise decoder} into entity-respective RGB reconstructions  and masks . 
  Note that the red-colored tensors ,  are calculated as complements from  and  .
  }}
  \label{fig:arch}
\end{figure}




\subsection{Loci Overview}
The slot-wise encoder is based on a conventional ResNet architecture. 
The input consists of eight main features, which encode the image frame  at current iteration , the prediction error map , a common background mask  as well as, specifically for every slot , a content mask , a position map , and maps for both the other masks  and the other positions  (cf. Figure~\ref{fig:arch}).
The computation path of Gestalt and position for an individual slot is tree-shaped, starting with a shared ResNet-trunk after which the path is split into four pathways, which compute Gestalt, location, size, and priority. 
This encoder design encourages that separately moving entities are encoded in individual ResNet activity dynamics as well as that each slot encodes `what' (Gestalt) and `where' (location, size, and priority) of the slot-encoded moving entity separately but conjointly.
Thereby, the error map  encourages active error minimization, following the principle of predictive coding fostering attractor dynamics \citep{Jordan:1986,Rao:1999}.
Appendix~\ref{sec:fullLoci} specifies further details.

The transition module then predicts location  and Gestalt dynamics  as well as interactions between the slot-encoded entities. 
This module is loosely based on the architecture of a transformer-like encoder \citep{vaswani2017attention}, where several multi-head attention layers are stacked together with residual feed forward layers in between.
We replace the residual feed forward layers with residual GateL0RD \citep{gumbsch2021sparsely} layers. 
GateL0RD is a recent gated recurrent neural network module, which is very well-suited to learn and distinctively encode sparse interaction events (events can be considered as ``structured, describable, memorable units of
experience.'' \cite{Baldwin:2021}, cf. \cite{Zacks2007}).
In accordance with our entity-focused processing approach, we apply a Siamese GateL0RD version, operating on the individual slot level while still receiving information form other slots via the attention layers.

At the end of the transition module, the Gestalt code is pushed through a binarization layer, which is inspired by the principle of vector quantization \citep{van2017neural}. 
This layer enforces an information bottleneck and thus contributes to the development of disentangled entity codes.

The slot-wise decoder recombines the `what' and `where' output from the transition module.
To do so, first the potential influence of each Gestalt code  is computed over the output range by means of a 2d isotropic Gaussian parametrized by the Position code , yielding density maps .
Next, a priority--based attention is applied to account for the fact that only one slot-object can be visible at any location (transparent objects are left for future work,
see Algorithm-\ref{algo:atten} in Appendix~\ref{sec:fullLoci}).
As a result, when two slots cover the same location, the one with the lower priority will have its feature maps set to zero.
The rest of the decoder is based on a conventional ResNet, which increases the resolution back to the video size. 
Like the encoder, the decoder shares weights across all slots. 
Eventually, the decoder outputs predictions of RGB entity reconstructions  and individual mask predictions  for each slot .
The masks from all slots and from the background are then combined to construct the final output image prediction  and competitive mask predictions .

To fully reconstruct the image, Loci uses its simple background module, which generates background image estimates  and a background mask . 
In case of a static background, a Gaussian mixture model over the whole training set is used and a flat background mask set to the bias value  .
For more dynamic backgrounds, we employ a simple auto encoder.





\subsection{Training}

Loci is trained using a binary cross-entropy loss () pixel-wise on the frame prediction applying rectified Adam optimization \cite[RAdam, cf.][]{liu2019variance}. 
Several regularizing losses are added to foster object permanence. 
Additionally, to speed-up learning, we use truncated backpropagation trough time and enhance the gradients with an e-prop-like accumulation of previous neural activities \cite{bellec2019biologically}. 

Empirical evaluations showed that backpropagating the gradients through the networks' inputs creates instabilities in the training process. 
Thus, we detach the gradients for the latent states.
As a result, the only part of the network that needs to backpropagate gradients trough time are the GateL0RD layers. 
Here, we found that using the described combination of e-prop and backpropagation is not only comparable in terms of network accuracy, it also greatly decreases training wall-clock time, as it allows the use of truncated backpropagation with length~1, effectively updating the weights after each input frame (see supplementary material for details).

Another important aspect for successful training is the use of a warmup phase, where we mask the target of the network with a foreground mask computed with a threshold : 

where  is the background model estimate, detailed above. Additionally an black background is used instead of  to construct the next frame prediction during this phase.
The foreground mask together with the zeroed background enforces the network to initially focus on reconstructing the foreground objects only. This supports learning to use the position-constrained encoding and decoding mechanism. After around 30\,000 updates---when the network has sufficiently learned to use the position encodings---we switch from the masked foreground reconstruction to the full reconstruction.
    
    
\paragraph{Object Permanence Loss}
To encourage object permanence, an additional loss is computed based on \autoref{eq:Lo}, which favors a slot that keeps encoding the same object, even if the object is temporarily occluded and thus invisible:

where  and  denote location and Gestalt encoding in slot ,  refers to the RGB part of the decoder network, while  denotes the Gestalt code averaged around the last time step in which the entity was visible, and  denotes the mask of slot  at time step . 
As a result,  is only applied when the object becomes invisible, which is the case when  is approaching zero. 
 
\paragraph{Time Persistence Loss}
A second mechanism to enforce object permanence and to also regularize the network towards temporal consistent slot encodings, is a time persistent regularization loss: 

where again  refers to the RGB part of the decoder network and  is the center position in the image.  essentially penalizes large visual changes in the decoded object between two consecutive time steps.

\paragraph{Position Change Loss}
In order to encourage the network to predict small slot-position changes over time, a simple  regularization loss is based on the position change between two time steps:


\paragraph{Supervision Loss}
Finally, for the experiments that are using a supervised target object location signal for fine-tuning, detailed in \autoref{eq:Ls}, we added a gating network  that operates on the latent Gestalt codes  before binarization and predicts a single softmax probability, which is used to decide which entity corresponds to the Snitch in the CATER dataset. The location of the selected entity is then used in an -loss to foster regression to the target location provided in the dataset.


The final loss for the network results from adding the individual loss components, denoting the unsupervised and supervised losses, respectively:



\section{Experiments \& Results}
We performed unsupervised training in all experiments. 
Only for the CATER challenge we additionally evaluated a version where we fine-tuned the network via supervision using \autoref{eq:Ls}. 


\subsection{CATER-Tracking Challenge}

\begin{figure}[b!]
  \includegraphics[width=\textwidth]{figures/CATER-challenge}
  \vspace{-.5cm}
  \caption{The CATER Snitch localization challenge: The task is to locate the yellow spherical object called Snitch within the last frame. The challenge is that the Snitch might be contained and moved by a cone. So its location has to be inferred by recognizing and remembering a containment event and then tracking the position of the container. Image adapted from \cite{girdhar2019CATER}.}
\label{fig:CATER}
\end{figure}

In the CATER tracking challenge (cf. \autoref{fig:CATER}), the task is to locate a unique yellow object, called Snitch, at the end of a video sequence. During the video, different geometric objects rotate, move, and hop in a scripted random way. By doing so, cone objects can engulf other objects and move them to another location before releasing them again, which can lead to situations where the Snitch remains hidden in the last frame of the video. Therefore, the challenge is not only to \emph{recognize containment} events, but also to \emph{track} the specific cone that contains the Snitch. For classification purposes, the 3D Snitch position is partitioned into a  grid, resulting in a total of 36 classes. In order to account for the fact that a small location error could lead to miss-classification, a common reported metric is the  grid distance.
We additionally report the continuous  distance with grid length 1, such that the distance is comparable to, but more informative than the  grid distance.


\begin{table}[t]
    \caption{Quantitative results of the CATER Snitch localization challenge. Referenced results are from \cite{zhou2021hopper} and from the other system-respective papers.}
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{ l c C C C C }
        \toprule
         Method & Parameters (M) & Top 1 & Top 5 &  (grid) &  \\
        \midrule
         Random& - & 2.8 & 13.8 & 3.9 & \\
         Transformer \citep{vaswani2017attention} & 15.01 & 13.7 & 39.9 & 3.53 & \\
         SINet \citep{ma2018attend} & 138.69 & 21.1 & 47.1 & 3.14 & \\
         TSN (RGB) + LSTM \citep{wang2016temporal} && 25.6 & 67.2 & 2.6 & \\
         DaSiamRPN \citep{zhu2018distractor} && 33.9 & 40.8 & 2.4 & \\
         I3D-50 + LSTM \citep{carreira2017quo} && 60.2 &81.8 & 1.2 & \\
         Hopper-transformer \citep{zhou2021hopper}  & 15.01 & 61.1 & 86.6 & 1.42 & \\
         TSM-50 \citep{lin2019tsm} && 64.0 &85.7 & 0.93 & \\
         TPN-101 \citep{yang2020temporal}&& 65.3 & 83.0 & 1.09 & \\
         Hopper-sinet \citep{zhou2021hopper}  & 139.22 & 69.1 & 91.8 & 1.02 & \\
         Inferno \citep{castrejon2021inferno} & -  & 71.7 & 88.9 & - & \\
         Hopper-multihop \citep{zhou2021hopper} &6.39 & 73.2 & 93.8 & 0.85 & \\
         Aloe \citep{ding2020object} & & 74.0 & 94.0 & 0.44 & \\
         OPNet \citep{shamsian2020learning} & -  & 74.8  & - & 0.54 & \\
         TFC V3D Depthwise \citep{zhang2022tfcnet} & 24.64  & 79.7 & 95.5 & 0.47 & \\
         \textbf{Loci supervised (ours) } & \textbf{2.96}  & \textbf{90.7} & \textbf{98.5}  & \textbf{0.14} & 0.14 \\
         \midrule
         Loci unsupervised (ours) & 4.14 &  78.4 & 92.0 & 0.45 & 0.44 \\
        \bottomrule
    \end{tabularx}
    \label{tab:CATERresults}
\end{table}
\begin{figure}[t]
    
    frame 191 ~~~~~~~~~~~~~~~~~~~~~~
    frame 194 ~~~~~~~~~~~~~~~~~~~~~~
    frame 287 ~~~~~~~~~~~~~~~~~~~~~~
    frame 291
    
\vskip\imagepaddingy
    \begin{elasticrow}[\imagepadding]
        \elasticfigure{imgs/gp-highlited-0011-191.jpg}
        \elasticfigure{imgs/gp-highlited-0011-194.jpg}
        \elasticfigure{imgs/gp-highlited-0011-287.jpg}
        \elasticfigure{imgs/gp-highlited-0011-291.jpg}
    \end{elasticrow}
    \vskip\imagepaddingy
    \begin{elasticrow}[\imagepadding]
        \elasticfigure{imgs/gp-object-0011-191-04.jpg}
        \elasticfigure{imgs/gp-object-0011-194-04.jpg}
        \elasticfigure{imgs/gp-object-0011-287-04.jpg}
        \elasticfigure{imgs/gp-object-0011-291-04.jpg}
    \end{elasticrow}
    
    \caption{Object permanence shown on a CATER tracking example. While the network (after completely unsupervised training) struggles to keep the shape of the Snitch when contained for a longer time span, color and, importantly, the position of the Snitch are preserved during containment.  
    \textbf{Top row}: tracked objects visualized trough colored masks. \textbf{Bottom row}: RGB representation of the Snitch.} \label{fig:cater_object_permanence}
\end{figure}



As described in \citet{girdhar2019CATER}, we split the dataset with a ratio of 70:30 into a training and test set and further put aside 20\% of the training set as a validation set, leaving 56\% of the original data for training. 
To blend-in the supervision loss, we first set the supervision factor  for the first 4 epochs, fostering mostly unsupervised training, and then set  for the duration of the training process and to  during the last epochs, to give the Snitch location a weak pull towards the target location.

In order to produce labels from the purely unsupervisedly trained network, we train a separate small classifier with around 17k parameters, which only operates on the latent states of the trained Loci network with a correction and a gating network. The correction network computes a residual for the location of each entity. The gating network computes softmax probabilities, which are used to select the location belonging to the Snitch object similar to \autoref{eq:Ls}. To prevent supervised gradient flow, the classifier network module is trained once the unsupervised training has finished. Eventually, the location and Gestalt codes are computed for the whole dataset and extracted into a separate data file, from which the classifier module is trained, again using a 70:30 train/test split.



As shown in \autoref{tab:CATERresults}, Loci not only surpasses all previous methods by a large margin---achieving a top 1 accuracy of 90.7\% and a top 5 accuracy of 98.5\% with an  distance of 0.14---it also surpasses most of the existing work when the main architecture is trained purely unsupervised. 
Loci learns object permanence merely by means of inductive bias and regularization. This is shown in \autoref{fig:cater_object_permanence}, where Loci keeps the representation of the contained Snitch in memory. While the shape details blur over time, importantly, the locating is correctly tracked to reconstruct the Snitch once it is revealed.
\autoref{fig:snitch_supervised} also demonstrates the tracking performance in case of several co-occurring occlusions and double contained situations, where two cones are covering the Snitch. Here, the location output of the supervised gating network is marked in the video frames.

\begin{figure}[t!]
    \begin{elasticrow}[\imagepadding]
        \elasticfigure{imgs/gp-snitch-0004-113.jpg}
        \elasticfigure{imgs/gp-snitch-0004-121.jpg}
        \elasticfigure{imgs/gp-snitch-0004-129.jpg}
        \elasticfigure{imgs/gp-snitch-0004-145.jpg}
        \elasticfigure{imgs/gp-snitch-0004-155.jpg}
        \elasticfigure{imgs/gp-snitch-0004-209.jpg}
    \end{elasticrow}
    \vskip\imagepaddingy
    \begin{elasticrow}[\imagepadding]
        \elasticfigure{imgs/gp-snitch-0012-038.jpg}
\elasticfigure{imgs/gp-snitch-0012-084.jpg}
        \elasticfigure{imgs/gp-snitch-0012-144.jpg}
        \elasticfigure{imgs/gp-snitch-0012-151.jpg}
        \elasticfigure{imgs/gp-snitch-0012-184.jpg}
        \elasticfigure{imgs/gp-snitch-0012-223.jpg}
    \end{elasticrow}

    
    \caption{Two challenging CATER tracking examples with several co-occurring containments / occlusions: Video frames overlaid with the predicted Snitch location (network trained with supervision).} 
    \label{fig:snitch_supervised}
\end{figure}

In additional studies (detailed in Appendix~\ref{sec:lesion_studies}), we furthermore \changed{investigate Loci's object permanence and also} demonstrate the effective disentanglement of position, Gestalt, size, and priority codes, which underlines Loci's ability to separate position and Gestalt of objects from a scene without supervision.



\subsection{Moving MNIST}

The moving MNIST (MMNIST) challenge is a dataset for video prediction \cite{srivastava2015unsupervised}. The task is to predict the motion of two moving digits, with a maximum size of  pixels, moving independently inside a  pixel window and bouncing off the walls in a predictable manner. While the dataset is usually implemented by simply bouncing the entire  MNIST sub-image within the  window, we first crop each digit to its actual appearance in order to obtain a realistic bounding box of the digit and thus to generate more realistic bouncing effects (once the actual number touches the border, instead of its superficial bounding box). This removes the undesired bias in the dataset to remember each digit's individual bounding box, individually, which is then the only way to correctly predict a bounce off a wall. Instead, now the network can predict the bounce based on the pixel information alone.

While the task was originally formulated to predict the next 10 frames after receiving the first 10 frames, we also evaluate the ability to generate temporal closed-loop predictions for up to 100 frames after being only trained to predict 10 frames. 
We compare Loci to the state-of-the-art approach PhyDNet, also designed to disentangle `what' from `where'; more specifically: Physical dynamics from unknown factors such as the digit's Gestalt code. While using the original code provided by the authors, we did not reach the same reported performance of PhyDNet using our unbiased MMNIST dataloader.
Nevertheless, PhyDNet reaches a high accuracy for the 10 frame prediction, which is only slightly topped by Loci. For extended temporal predictions, though, PhyDNet quickly dissolves the digits, while Loci preserves the Gestalt of each digit over the 100 predicted frames, as shown in \autoref{fig:moving_mnist}.
\autoref{tab:mmnistresults} shows a qualitative comparison between Loci and PhyDNet: While Loci consistently outperforms PhyDNet for the structural similarity index measure (SSIM, \cite{wang2004image}), PhyDNet has a lower MSE than Loci after 30 frames. This might be due to the blurring of PhyDNet, which accounts for the uncertainty in the position estimate at the cost of the digits shape.


\newcommand{\alignedlabel}[2]{\parbox{#1}{\tiny\centering{\strut #2}}
}

\newlength{\alen}
\setlength{\alen}{0.62cm}

\begin{table}[t!]
    \caption{Moving MNIST prediction accuracy PhyDNet vs Loci. Both networks where trained using the same dataloader to predict the next 90 frames given an input sequence of 10 frames.}
    \label{tab:mmnistresults}
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{ l C C C C C C C C C C}
        \toprule
         & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\
        \cmidrule(l{0pt}r{1.5pt}){2-3}
        \cmidrule(l{1.5pt}r{1.5pt}){4-5}
        \cmidrule(l{1.5pt}r{1.5pt}){6-7}
        \cmidrule(l{1.5pt}r{1.5pt}){8-9}
        \cmidrule(l{1.5pt}r{0pt}){10-11}
        Method & MSE & SSIM & MSE  & SSIM & MSE & SSIM & MSE  & SSIM & MSE  & SSIM  \\
        \midrule
        PhyDnet \citep{guen2020disentangling} & 35.6 & 0.913 & 58.6 & 0.830 & \textbf{80.7} & 0.739 & \textbf{113.9} & 0.594 & \textbf{155.1} & 0.484 \\
Loci & \textbf{30.7} & \textbf{0.923} & \textbf{52.5} & \textbf{0.881} & 98.3 & \textbf{0.814} & 165.1 & \textbf{0.720} & 221.2 & \textbf{0.639} \\
        \bottomrule
        
    \end{tabularx}
\end{table}
\begin{figure*}[t!]
    \alignedlabel{\alen}{t=10}\hfill
    \alignedlabel{\alen}{t=11}\hfill
    \alignedlabel{\alen}{t=12}\hfill
    \alignedlabel{\alen}{t=14}\hfill
    \alignedlabel{\alen}{t=16}\hfill
    \alignedlabel{\alen}{t=18}\hfill
    \alignedlabel{\alen}{t=20}\hfill
    \alignedlabel{\alen}{t=23}\hfill
    \alignedlabel{\alen}{t=26}\hfill
    \alignedlabel{\alen}{t=29}\hfill
    \alignedlabel{\alen}{t=33}\hfill
    \alignedlabel{\alen}{t=37}\hfill
    \alignedlabel{\alen}{t=42}\hfill
    \alignedlabel{\alen}{t=48}\hfill
    \alignedlabel{\alen}{t=58}\hfill
\alignedlabel{\alen}{t=68}\hfill
    \alignedlabel{\alen}{t=78}\hfill
    \alignedlabel{\alen}{t=88}\hfill
    \alignedlabel{0.55cm}{t=100}

    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/GroundTruth-10-000}
        \elasticfigure{imgs/GroundTruth-10-001}
        \elasticfigure{imgs/GroundTruth-10-002}
        \elasticfigure{imgs/GroundTruth-10-004}
        \elasticfigure{imgs/GroundTruth-10-006}
        \elasticfigure{imgs/GroundTruth-10-008}
        \elasticfigure{imgs/GroundTruth-10-010}
        \elasticfigure{imgs/GroundTruth-10-013}
        \elasticfigure{imgs/GroundTruth-10-016}
        \elasticfigure{imgs/GroundTruth-10-019}
        \elasticfigure{imgs/GroundTruth-10-023}
        \elasticfigure{imgs/GroundTruth-10-027}
        \elasticfigure{imgs/GroundTruth-10-032}
        \elasticfigure{imgs/GroundTruth-10-038}
        \elasticfigure{imgs/GroundTruth-10-048}
\elasticfigure{imgs/GroundTruth-10-058}
        \elasticfigure{imgs/GroundTruth-10-068}
        \elasticfigure{imgs/GroundTruth-10-078}
        \elasticfigure{imgs/GroundTruth-10-090}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/PhyDNet-10-000}
        \elasticfigure{imgs/PhyDNet-10-001}
        \elasticfigure{imgs/PhyDNet-10-002}
        \elasticfigure{imgs/PhyDNet-10-004}
        \elasticfigure{imgs/PhyDNet-10-006}
        \elasticfigure{imgs/PhyDNet-10-008}
        \elasticfigure{imgs/PhyDNet-10-010}
        \elasticfigure{imgs/PhyDNet-10-013}
        \elasticfigure{imgs/PhyDNet-10-016}
        \elasticfigure{imgs/PhyDNet-10-019}
        \elasticfigure{imgs/PhyDNet-10-023}
        \elasticfigure{imgs/PhyDNet-10-027}
        \elasticfigure{imgs/PhyDNet-10-032}
        \elasticfigure{imgs/PhyDNet-10-038}
        \elasticfigure{imgs/PhyDNet-10-048}
\elasticfigure{imgs/PhyDNet-10-058}
        \elasticfigure{imgs/PhyDNet-10-068}
        \elasticfigure{imgs/PhyDNet-10-078}
        \elasticfigure{imgs/PhyDNet-10-090}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/Loci-10-000}
        \elasticfigure{imgs/Loci-10-001}
        \elasticfigure{imgs/Loci-10-002}
        \elasticfigure{imgs/Loci-10-004}
        \elasticfigure{imgs/Loci-10-006}
        \elasticfigure{imgs/Loci-10-008}
        \elasticfigure{imgs/Loci-10-010}
        \elasticfigure{imgs/Loci-10-013}
        \elasticfigure{imgs/Loci-10-016}
        \elasticfigure{imgs/Loci-10-019}
        \elasticfigure{imgs/Loci-10-023}
        \elasticfigure{imgs/Loci-10-027}
        \elasticfigure{imgs/Loci-10-032}
        \elasticfigure{imgs/Loci-10-038}
\elasticfigure{imgs/Loci-10-048}
        \elasticfigure{imgs/Loci-10-058}
        \elasticfigure{imgs/Loci-10-068}
        \elasticfigure{imgs/Loci-10-078}
        \elasticfigure{imgs/Loci-10-090}
    \end{elasticrow}
    
    \caption{Comparison between ground truth \textbf{top row}, PhyDNet \textbf{center row} and Loci \textbf{bottom row} for a prediction of up to 90 frames. Both PhyDNet and Loci were trained on 10 frame prediction. While in PhyDNet, the appearance of the digits dissolves after a few frames beyond the initial training distribution, Loci manages to keep the Gestalt code and the location accurate until the fourth collision at around frame 42. The Gestalt codes remain stable until the end of the considered 100 time steps.} 
    \label{fig:moving_mnist}
\end{figure*}




\subsection{Other Datasets \& Supplementary Material}
Apart from a video footage for the main experiments and further algorithmic details on Loci, we provide additional insights and tests in the supplementary material: We evaluate the real world tracking performance of Loci on a ten hour aquarium footage found on YouTube Additionally, we examine Gestalt preservation and indicators of intuitive physics in closed loop predictions on the CLEVRER dataset \citep{yi2019clevrer}.
Furthermore, several ablation studies are provided. 

\section{Conclusion and Future Work}

We presented Loci---a novel location and identity disentangling artificial neural network, which learns to process object-distinct position and Gestalt encodings. 
Loci significantly exceeds current state-of-the-art architectures on the CATER challenge, while requiring fewer network parameters and less supervision. 
Loci thus offers a critical step towards solving the binding problem \cite{greff2020binding,Treisman:1996,Malsburg:1995}.
We particularly hope that the mechanisms proposed in this work bear potential to enrich the field of object representation learning and highlight the importance of well-chosen inductive biases.


Currently, Loci operates on static background estimates and cameras, which we denote as its main limitation. 
We intend to extend the background model to incorporate rich and varying backgrounds more flexibly in future work. Potential avenues for this extension may include an explicit, separate encoding of ego-motion and depth \citep{Li:2021}.
Furthermore, image reconstructions may be fused with subsequent image inputs in an information-driven manner.  
We also expect to create even more compact object-interaction encodings following theories of event-predictive cognition and related conceptions and models in computational neuroscience \citep{Butz:2016,Butz:2021,Franklin:2020,Stawarczyk:2021}.
Moreover, we are excited to explore the potential of Loci to address problems of intuitive physics and causality \citep{Michotte:1946,Pearl:2018,Schoelkopf:2021}, seeing that Loci offers a suitable compositional structure \citep{greff2020binding,Lake:2017} to enable symbolic reasoning. 
Finally, we hope that Loci will also be well-combinable with reinforcement learning and model-predictive planning in system control setups to pursue active, goal-directed environmental interactions. 

\section{Acknowledgement}
This work received funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 as well as from the Cyber Valley in Tübingen, CyVy-RF-2020-15. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Manuel Traub and Matthias Karlbauer, and the Alexander von Humboldt Foundation for supporting Martin Butz and Sebastian Otte. We also thank Simon Frank for the Gestalt-Code analysis in \autoref{fig:cluster_analysis}.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\appendix
\clearpage
\section{Appendix}

As additional content, we first provide details on further evaluations we conducted with Loci. 
In order to evaluate the tracking performance of Loci in a real world example, we trained it on a 10 hour aquarium footage found on YouTube. The task poses the additional challenge to cope with backgrounds that are not fully stationary. The results are shown in \autoref{fig:aquarium2} and demonstrate that Loci is able to track 15+ objects in a complex real world environment.
\begin{figure*}[b!]
 \begin{elasticrow}[\imagepadding]
     \elasticfigure{imgs/gp-highlited-0000-1470.jpg}
     \elasticfigure{imgs/gp-highlited-0000-1485.jpg}
     \elasticfigure{imgs/gp-highlited-0000-1500.jpg}
 \end{elasticrow}
 \vskip\imagepaddingy
 \begin{elasticrow}[\imagepadding]
     \elasticfigure{imgs/gp-highlited-0000-1515.jpg}
     \elasticfigure{imgs/gp-highlited-0000-1530.jpg}
     \elasticfigure{imgs/gp-highlited-0000-1545.jpg}
 \end{elasticrow}
 \vskip\imagepaddingy
 \begin{elasticrow}[\imagepadding]
     \elasticfigure{imgs/gp-highlited-0000-1560.jpg}
     \elasticfigure{imgs/gp-highlited-0000-1575.jpg}
     \elasticfigure{imgs/gp-highlited-0000-1590.jpg}
 \end{elasticrow}
 \caption{Fully unsupervised real world tracking example trained on 10 hour aquarium footage.} 
 \label{fig:aquarium2}
\end{figure*}

The Gestalt preserving performance of Loci for closed loop predictions are also demonstrated exemplary on the CLEVRER dataset in \autoref{fig:clevrer_prediction}. Here, the effects of collisions of different geometric objects are predicted into the future. While the location deviates  visibly over time, Loci is able to preserve the Gestalt code also for more complex objects in a closed loop setup, which is considered specifically challenging for RNNs.

\begin{figure}[htb!]
    t = 10 ~~~~~~~~~~~~
    t = 20 ~~~~~~~~~~~~
    t = 30 ~~~~~~~~~~~~
    t = 40 ~~~~~~~~~~~~
    t = 50 ~~~~~~~~~~~~
    t = 60 ~~~~~~~~~~~~
    t = 70 
    \vskip\imagepaddingtiny
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/gp-input-0001-070.jpg}
        \elasticfigure{imgs/gp-input-0001-080.jpg}
        \elasticfigure{imgs/gp-input-0001-090.jpg}
        \elasticfigure{imgs/gp-input-0001-100.jpg}
        \elasticfigure{imgs/gp-input-0001-110.jpg}
        \elasticfigure{imgs/gp-input-0001-120.jpg}
        \elasticfigure{imgs/gp-input-0001-130.jpg}
    \end{elasticrow}
    \vskip\imagepaddingtiny
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/gp-output-0001-070.jpg}
        \elasticfigure{imgs/gp-output-0001-080.jpg}
        \elasticfigure{imgs/gp-output-0001-090.jpg}
        \elasticfigure{imgs/gp-output-0001-100.jpg}
        \elasticfigure{imgs/gp-output-0001-110.jpg}
        \elasticfigure{imgs/gp-output-0001-120.jpg}
        \elasticfigure{imgs/gp-output-0001-130.jpg}
    \end{elasticrow}
    \vskip\imagepaddingtiny
    \vskip\imagepaddingtiny
    \vskip\imagepaddingtiny
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/gp-input-0017-070.jpg}
        \elasticfigure{imgs/gp-input-0017-080.jpg}
        \elasticfigure{imgs/gp-input-0017-090.jpg}
        \elasticfigure{imgs/gp-input-0017-100.jpg}
        \elasticfigure{imgs/gp-input-0017-110.jpg}
        \elasticfigure{imgs/gp-input-0017-120.jpg}
        \elasticfigure{imgs/gp-input-0017-130.jpg}
    \end{elasticrow}
    \vskip\imagepaddingtiny
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/gp-output-0017-070.jpg}
        \elasticfigure{imgs/gp-output-0017-080.jpg}
        \elasticfigure{imgs/gp-output-0017-090.jpg}
        \elasticfigure{imgs/gp-output-0017-100.jpg}
        \elasticfigure{imgs/gp-output-0017-110.jpg}
        \elasticfigure{imgs/gp-output-0017-120.jpg}
        \elasticfigure{imgs/gp-output-0017-130.jpg}
    \end{elasticrow}
    \vskip\imagepaddingtiny
    \vskip\imagepaddingtiny
    \vskip\imagepaddingtiny
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/gp-input-0016-070.jpg}
        \elasticfigure{imgs/gp-input-0016-080.jpg}
        \elasticfigure{imgs/gp-input-0016-090.jpg}
        \elasticfigure{imgs/gp-input-0016-100.jpg}
        \elasticfigure{imgs/gp-input-0016-110.jpg}
        \elasticfigure{imgs/gp-input-0016-120.jpg}
        \elasticfigure{imgs/gp-input-0016-130.jpg}
    \end{elasticrow}
    \vskip\imagepaddingtiny
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/gp-output-0016-070.jpg}
        \elasticfigure{imgs/gp-output-0016-080.jpg}
        \elasticfigure{imgs/gp-output-0016-090.jpg}
        \elasticfigure{imgs/gp-output-0016-100.jpg}
        \elasticfigure{imgs/gp-output-0016-110.jpg}
        \elasticfigure{imgs/gp-output-0016-120.jpg}
        \elasticfigure{imgs/gp-output-0016-130.jpg}
    \end{elasticrow}

    \caption{Three examples of predicting object dynamics from the CLEVRER\citep{yi2019clevrer} benchmarks. Loci runs 70 time-steps in closed loop after 50 times-steps of teacher forcing. Top rows show ground truth wile bottom rows show Loci's closed loop predictions.} 
    \label{fig:clevrer_prediction}
\end{figure}

\subsection{CATER Evaluation Details}

In order to produce the results from \autoref{tab:CATERresults} we trained five networks with different initial seeds and and then evaluated each network five times with different initial seeds. In \autoref{tab:CATERresults} we reported the mean results over the five evaluation runs from our best performing network (Network~3 from \autoref{tab:CATERresults_runs}).

\begin{table}[htb]
    \caption{Evaluations on the CATER Snitch challenge for supervised training. Table shows five networks trained with different initial seeds, each evaluated five times with different initial seeds.}
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{ l C C C C }
        	\toprule
         Network & Top 1 & Top 5 &  (grid) &  \\
        \midrule
1 & 91.7 & 99.0 & 0.118 & 0.122 \\
 & 90.8 & 98.8 & 0.129 & 0.126 \\
 & 91.1 & 98.8 & 0.119 & 0.121 \\
 & 91.4 & 98.9 & 0.122 & 0.123 \\
 & 91.3 & 98.8 & 0.123 & 0.126 \\
 \hline
2 & 91.1 & 98.6 & 0.129 & 0.133 \\
 & 90.4 & 98.3 & 0.144 & 0.141 \\
 & 89.8 & 98.3 & 0.151 & 0.144 \\
 & 90.0 & 98.5 & 0.149 & 0.140 \\
 & 89.5 & 98.2 & 0.164 & 0.151 \\
 \hline
3 & 89.6 & 97.8 & 0.173 & 0.170 \\
 & 90.1 & 98.0 & 0.159 & 0.157 \\
 & 89.5 & 97.9 & 0.174 & 0.164 \\
 & 88.9 & 97.5 & 0.174 & 0.167 \\
 & 89.8 & 98.1 & 0.162 & 0.157 \\
 \hline
4 & 91.5 & 99.1 & 0.112 & 0.118 \\
 & 90.4 & 98.9 & 0.133 & 0.128 \\
 & 91.0 & 98.7 & 0.125 & 0.129 \\
 & 90.5 & 99.1 & 0.125 & 0.124 \\
 & 90.8 & 98.8 & 0.127 & 0.126 \\
 \hline
5 & 91.7 & 98.2 & 0.130 & 0.139 \\
 & 91.4 & 98.4 & 0.135 & 0.139 \\
 & 91.2 & 98.0 & 0.146 & 0.150 \\
 & 91.8 & 98.3 & 0.130 & 0.140 \\
 & 91.3 & 98.1 & 0.150 & 0.151 \\
\bottomrule
    \end{tabularx}
    \label{tab:CATERresults_runs}
\end{table}

\begin{table}[htb]
    \caption{Evaluations on the CATER Snitch challenge for supervised training, statistics of the 25 evaluations presented in \autoref{tab:CATERresults_runs}}
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{ l C C C C }
	\toprule
    Metric & min & mean & std & max \\
    \midrule
    Top1 & 88.9 & 90.7 & 0.8 & 91.8 \\
    Top5 & 97.5 & 98.5 & 0.4 & 99.1 \\
     (grid)   &   0.112 & 0.140 & 0.019 & 0.174 \\
       &  0.118 & 0.139 & 0.015 &  0.170 \\
    \bottomrule
    \end{tabularx}
    \label{tab:CATERresults_stats}
\end{table}

\subsection{Backgound Model}\label{app:subsec:background_model}
In order to compute an background model for datasets with a simple static background like CATER, CLEVR or CLEVRER we use a simple Gaussian Mixture Model. The specific function used is \textit{createBackgroundSubtractorMOG2} from OpenCV \cite{opencv_library} which we use with a learningrate of 0.00001 to compute an background image based on the training set.

For more complex backgrounds like in the aquarium example where the camera is still static, we use an autoencoder ResNet that encodes the input image into a latent vector with the same size as the Gestalt code. This latent background code is then run through an residual GateL0rd in order to capture temporal dynamics, like the chancing water level, and is then run through an ResNet decoder.
The background autoencoder is pretrained using a L1 reconstruction loss in order to focus on the most dominant features of the background and to not care so much about foreground objects.




\subsection{Ablations}\label{ablations}

Using the binary cross-entropy with non binary targets, while producing valid gradients, gives little insights into the network's actual performance.
In order to better compare different designs and to also take the objects into focus, we use a modified -loss for our ablation studies that is computed based on \autoref{eq:l2_obj}:



Here the MSE is masked with the error between the input and the background. As a result, we get a much higher error when the network produces a tracking error compared to a background reconstruction error. The  loss takes into account instances, where, for example, the network overlooks an object or, makes a mistake in the prediction of the movement of an object.
It thus offers itself as a good metric for comparing the performance of different architectures during training.


\subsubsection{GateL0RD vs LSTM}

As shown in \autoref{fig:gatel0rd_vs_lstm}, using GateL0RD within the predictor network significantly increases the -Loss. Thus, it appears that GateL0RD's piece-wise constant latent state regularization mechanism indeed suitably biases the network towards assuming object permanence.

\begin{figure}[t]
  \includegraphics[width=\textwidth]{gatel0rd-vs-lstm/gatel0rd-vs-lstm}
  \vspace{-.75cm}
  \caption{Comparison between GateL0RD and LSTM modules within the predictor part of the network.}
  \label{fig:gatel0rd_vs_lstm}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{e-prop-vs-bptt/e-prop-vs-bptt}
  \vspace{-.75cm}
  \caption{Comparison between different sequence lengths for truncated back-propagation trough time vs back-propagation and e-prop.}
  \label{fig:eprop_vs_bptt}
\end{figure}

\subsubsection{Input channels}

In \autoref{fig:input_ablations}, we show the networks performance in terms of  when we zero out different input channels.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/ablations/input-ablations.pdf}
  \vspace{-.75cm}
  \caption{Importance of different input channels.}
  \label{fig:input_ablations}
\end{figure}

\subsubsection{Regularization losses}

In \autoref{fig:loss_ablations}, we show the networks performance in terms of  without certain regularization losses.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/ablations/loss-ablations.pdf}
    \vspace{-.75cm}
  \caption{Importance of regularization losses.}
  \label{fig:loss_ablations}
\end{figure}


\subsubsection{E-prop vs back-propagation through time}
\label{sec:eprop_vs_backprop}
In \autoref{fig:eprop_vs_bptt} we compared the  loss for truncated back-propagation through time with different sequence lengths against a version where we used back-propagation together with e-prop. In all experiments the networks are trained on the full Cater sequence with 300 frames, which are fed into the network sequentially. We then updated and afterwards detached the gradients using truncated back-propagation through time with different time intervals.  
Using e-prop informed gradients not only drastically sped up training time, but it was also more sample-efficient while achieving the same performance as truncated back-propagation through time with an interval of three time steps. Especially interesting is the experiment where we tested truncated back-propagation with a sequence length of one, without using e-prop's informed gradients. In this case, the performance is significantly worse than when using e-prop's informed gradients.


\subsubsection{Lesion studies}\label{sec:lesion_studies}
In order to quantify the effect of the Gestalt and position codes, we conducted an ablation by adding normally distributed noise with standard deviations  to the Gestalt and position codes provided by the encoder--right before they were forwarded to the transition module---and calculated the resulting accuracies and errors, which are reported in \autoref{fig:position-Gestalt_noise_ablation_cater} \changed{and \autoref{fig:position-Gestalt_noise_ablation_mmnist} for the CATER and moving MNIST benchmarks, respectively}. Furthermore, exemplary images of according position, priority, size, and Gestalt code manipulations are presented in \autoref{fig:clevrer_prediction2} and \autoref{fig:moving_mnist}, again for the CATER and moving MNIST benchmarks. Effectively, manipulating the according codes result in changes in position, priority, size, or Gestalt code, demonstrating the disentangled encoding of object features in the respective latent codes. \changed{In particular, while the Gestalt-code manipulation results in changes of the actual numbers in the moving MNIST experiment under conservation of positions (first vs. third row of \autoref{fig:moving_mnist}), the position-code manipulation alters the position of the numbers in space without modifying the object shape (first vs. fourth row of \autoref{fig:moving_mnist}).}

\begin{figure}[htb!]
    \centering
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/lesions/cater_average_l1_distance_im}
        \elasticfigure{imgs/lesions/cater_average_l2_distance_im}
        \elasticfigure{imgs/lesions/cater_average_object_loss_im}
        \elasticfigure{imgs/lesions/cater_average_top_1_accuracy_im}
        \elasticfigure{imgs/lesions/cater_average_top_5_accuracy_im}
    \end{elasticrow}
    
    \vspace{0.2cm}
    
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/lesions/cater_average_l1_distance_plt}
        \elasticfigure{imgs/lesions/cater_average_l2_distance_plt}
        \elasticfigure{imgs/lesions/cater_average_object_loss_plt}
        \elasticfigure{imgs/lesions/cater_average_top_1_accuracy_plt}
        \elasticfigure{imgs/lesions/cater_average_top_5_accuracy_plt}
    \end{elasticrow}
    \caption{Gestalt-position lesion study on the CATER benchmark. Top: Metrics resulting from different \textit{noise} intensities \textit{added} to the \textit{Gestalt} and \textit{position codes}. Bottom: Explicit visualization of the bottom-most row (manipulating position only, blue), left-most column (manipulating Gestalt only, orange), and diagonal from bottom left to top right in the plots at the top (manipulating both position and Gestalt, purple).}
    \label{fig:position-Gestalt_noise_ablation_cater}
\end{figure}

\begin{figure}[htb!]
    \centering
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/lesions/mmnist_mse_im}
        \elasticfigure{imgs/lesions/mmnist_ssim_im}
        \elasticfigure{imgs/lesions/mmnist_mse_plt}
        \elasticfigure{imgs/lesions/mmnist_ssim_plt}
    \end{elasticrow}
    \caption{\changed{Same analysis as \autoref{fig:position-Gestalt_noise_ablation_cater}, applied to the moving MMNIST dataset.}}
    \label{fig:position-Gestalt_noise_ablation_mmnist}
\end{figure}

\begin{figure}[htb!]
    \raggedright{
    ground truth ~~~~~~~~~~
position + noise ~~~~~ 
    priority + noise ~~~~~
    size + noise ~~~~~~~~~~~
    Gestalt + noise ~~~}
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/leasion-gt.jpg}
\elasticfigure{imgs/leasion-pos.jpg}
        \elasticfigure{imgs/leasion-prio.jpg}
        \elasticfigure{imgs/leasion-std.jpg}
        \elasticfigure{imgs/leasion-gestalt.jpg}
    \end{elasticrow}

    \caption{To study the disentanglement of the different parts of our learned codes, we perform a lesion study. The principle of the study is as follows: For each component code (position, priority, size, and Gestalt) we add gaussian noise to the code before applying the decoder while keeping all other codes as predicted by the model.} 
    \label{fig:clevrer_prediction2}
\end{figure}

\begin{figure*}[t!]
\begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-010}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-011}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-012}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-013}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-014}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-015}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-016}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-017}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-018}
        \elasticfigure{imgs/mnist-leasion/gt/gp-output-0000-019}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-010}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-011}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-012}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-013}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-014}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-015}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-016}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-017}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-018}
        \elasticfigure{imgs/mnist-leasion/std/gp-output-0000-019}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-010}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-011}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-012}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-013}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-014}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-015}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-016}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-017}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-018}
        \elasticfigure{imgs/mnist-leasion/ges/gp-output-0000-019}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-010}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-011}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-012}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-013}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-014}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-015}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-016}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-017}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-018}
        \elasticfigure{imgs/mnist-leasion/pos/gp-output-0000-019}
    \end{elasticrow}
    \caption{\changed{Lesion study on the moving mnist closed loop predictions (frames  to ). \textbf{First row:} undisturbed prediction. \textbf{Second row:}  scalled gaussian noise added to the position (size) code, which is then cliped to . \textbf{Third row:} gaussian noise added to the Gestalt code witch is then rounded and clipped to . \textbf{Fourth row}  scalled gaussian noise added to the position (x,y) code which is then clipped to }}
    \label{fig:moving_mnist_leasion}
\end{figure*}

\clearpage
\subsection{\changed{Gestalt Code Investigation}}
\changed{
Here we investigate the latent landscape of the Gestalt codes learned by Loci when trained on the CATER challenge. To do this, we first selected a fixed amount (450) Gestalt codes created by Loci using the CATER dataset. We then clustered these Gestalt codes using a Gaussian Mixture model and then performed a dimensionality reduction using t-SNE. 
}
\begin{figure*}[t!]
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/Color3_0}
        \elasticfigure{imgs/Color3_1}
        \elasticfigure{imgs/Color3_2}
        \elasticfigure{imgs/Color3_3}
        \elasticfigure{imgs/Color3_4}
        \elasticfigure{imgs/Color3_5}
        \elasticfigure{imgs/Color3_6}
        \elasticfigure{imgs/Color3_7}
        \elasticfigure{imgs/Color3_8}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/Form3_0}
        \elasticfigure{imgs/Form3_1}
        \elasticfigure{imgs/Form3_2}
        \elasticfigure{imgs/Form3_3}
        \elasticfigure{imgs/Form3_4}
        \elasticfigure{imgs/Form3_5}
        \elasticfigure{imgs/Form3_6}
        \elasticfigure{imgs/Form3_7}
        \elasticfigure{imgs/Form3_8}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \begin{elasticrow}[\imagepaddingtiny]
        \elasticfigure{imgs/Form_Color3_0}
        \elasticfigure{imgs/Form_Color3_1}
        \elasticfigure{imgs/Form_Color3_2}
        \elasticfigure{imgs/Form_Color3_3}
        \elasticfigure{imgs/Form_Color3_4}
        \elasticfigure{imgs/Form_Color3_5}
        \elasticfigure{imgs/Form_Color3_6}
        \elasticfigure{imgs/Form_Color3_7}
        \elasticfigure{imgs/Form_Color3_8}
    \end{elasticrow}
    \vskip\imagepaddingtinyy
    \caption{\changed{Traversing the Gestalt code manifold: Using a Gaussian Mixture together with t-SNE we find distinct clusters in the latent Gestalt code. Traversing the main axis of variance given by a PCA we found a color axis (\textbf{top rob}) and a shape axis (\textbf{middle row}).  Since these axes are disentangled they can be used in an additive manner to simultaneously change the color and shape (\textbf{bottom row})}}
    \label{fig:cluster_analysis}
\end{figure*}
\changed{
In some of the found clusters mainly the color of the objects was different, while in others the color and the shape varied. 
}

\changed{
The images from \autoref{fig:cluster_analysis} where created by using a a Principal component analysis (PCA) to calculate the main axes of variance in different clusters. A random Gestalt code was chosen from a clusters and was modified by subtracting or adding a portion of the axes or in other words, the Gestalt code was used as a starting point and then new Gestalt codes were sampled walking down and up the selected axes. These new Gestalt codes are not binary anymore but were still clipped at 0 and 1, to create meaningful inputs to the decoder. We were able to identify an axis, that only varies the color of the objects, while the shape stays the same, and one axis, which only varies the shape, while the color stays the same. Since these axes are disentangled they can be used in an additive manner to simultaneously change the color and shape of objects.
}
\subsection{\changed{Object Permanence Evaluation}}

\changed{In order to analysed Loci's object permanence abilities we evaluated how the number of time steps the Snitch was hidden affects Loci's ability to correctly locate it. As shown in \autoref{fig:snitch_contained}, where the tracking accuracy is plotted for progressively extended periods of Snitch containment. For both the supervised and unsupervised tracking, the location error remains constant after an initial increase, even over extended time spans.}

\begin{figure}[t!]
  \includegraphics[width=.49\textwidth]{snitch-contained-l2/unsupervised-plot}
  \includegraphics[width=.49\textwidth]{snitch-contained-l2/supervised-plot}
  \caption{\changed{Comparison between unsupervised (\textbf{left}) and supervised (\textbf{right}) tracking, while the Snitch is contained progressively longer. The x-axis depicts the number of frames the Snitch has been contained. The left y-axis shows the  distance to the target position, while the right y-axis shows the number of times a Snitch was contained that long within the test-set (green shaded area). The thick line represents the median  distance, while the thin line represents the mean  distance. The weaker colored area shows the 90-10-quantile, while the stronger one corresponds to the 75-25-quantile.}}
\label{fig:snitch_contained}
\end{figure}

\subsection{e-prop for GateL0RD}
 
 GateL0RD is defined in \autoref{eq:gate_l0rd_1} till \autoref{eq:gate_l0rd_6} following \citet{gumbsch2021sparsely} with the gating network , the candidate network , and the cell state .
Next, we detail how we applied e-prop in GateL0RD.

\subsubsection{GateL0RD Model}




\subsubsection{e-prop updates:}

\paragraph{core derivative and eligiblity determination:}


\paragraph{gating network forward eligibility propagation:}




\paragraph{candidate network  forward eligibility propagation:}






Note that the partial derivatives  in \autoref{eq:eprop:gating} and \autoref{eq:eprop:candidate} address the current time step  only. Also note that  and  carry true gradient information of the current time step, backprogated through the feedforward connections of the architecture. In contrast, original e-prop uses local approximations of the learning signal only, which does not allow to stack multiple layers without losing the exact local gradient. 














\begin{table}[htb]
    \caption{Evaluations of Loci's foreground segmentation masks. Trained networks from the Cater challenge are evaluated on CLEVR with mask by running the single CLEVR images 30 iterations through the network and then comparing the masks using the Intersecion over Union (IoU) metric. We compare a mean per mask accuracy (mask avg) and size weighted average that represents how many pixels where segmented correctly (pixel avg).}
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{ l C C }
        	\toprule
         Network & mask avg (\%) & pixel avg (\%) \\
        \midrule
        1 & 83.3 & 96.8 \\
        2 & 84.6 & 97.0 \\
        3 & 84.3 & 97.1 \\
        4 & 84.1 & 96.7 \\
        5 & 85.6 & 97.2 \\
\bottomrule
    \end{tabularx}
    \label{tab:CATERresults_runs3}
\end{table}



\section{Loci Algorithm}
\label{sec:fullLoci}
Loci processes a sequence of RGB video-encoding images .
Processing is mostly done slot-wise, whereby the system is initialized with a variable number of  processing slots.
Its main components consist of a \emph{slot-wise encoder}, a \emph{transition module}, and a \emph{slot-wise decoder}. 
Moreover, a background processing module is implemented.
The slot-wise encoder is implemented by a tree-structured ResNet-based processing encoder (see Figure~\ref{fig:encoder_flow_chart}).
The transition module processes \emph{slot-wise temporal dynamics} and \emph{between-slot interaction dynamics} (see Figure~\ref{fig:predictor_flow_chart}).
The slot-wise decoder is again implemented by a ResNet (see Figure~\ref{fig:decoder_flow_chart}).
For simple backgrounds, we use a Gaussian Mixture Model to obtain a default background estimate , which is used for the whole training set.
For more complex backgrounds we use an additional Auto-Encoder Module.

In the remainder, we denote scalar values by lower-case letters, tensors by upper-case letters, and vectors by bold letters.
Moreover, we denote slot-specific activities with a subscript  and time by the superscript .
We drop  for temporary values.

We now first define data and neural encoding sizes and types used throughout Loci's processing pipeline. 
We then specify neural activity initialization.
Finally, we detail the unfolding overall processing loop.

\subsection{Slot-wise encoder}
\paragraph{Inputs} The encoder inputs at each time step  consist of: 
\begin{itemize}
    \item RGB input image ,
    \item MSE map  (pixel-wise mean squared error between  and ),
    \item Slot-specific RGB image reconstructions ,
    \item Slot-specific mask predictions ,
    \item Slot-specific mask complements 
    \item Slot-specific isotropic Gaussian position map predictions ,
    \item Slot-specific Gaussian position map complements 
    \item Background mask , which is equivalent to 
\end{itemize}

\paragraph{Outputs} Based on these inputs, the slot-wise encoder network generates latent codes, which are forwarded to the transition module:
\begin{itemize}    
    \item Slot-specific Gestalt codes ,
    \item Slot-specific position codes  encode an isotropic Gaussian  and a slot-priority code ,
\end{itemize}
where  denotes the size of the Gestalt code and .

\subsection{Transition module}
A transition module is used to process interaction dynamics within and between these slot-respective codes and creates a prediction for the next state, which is fed into the decoder.
The input to the transition module equals , .
It is processed across slots and per slot in the respective layers:
Multi-Head Attention predicts slot interactions (across slots), while
GateL0RD predicts slot-specific dynamics (per slot).
In our main CATER implementation we use two attention layers with two heads each with GateL0RD layers in between.

\paragraph{Outputs} The outputs of the transition module  and  have the same size as its inputs. Additionally, recurrent, slot-respective hidden states  are maintained in the time-recurrent GateL0RD layers:
\begin{itemize}    
    \item Slot-specific position codes ,
    \item Slot-specific Gestalt codes ,
    \item Slot-specific GateL0RD-layer-respective hidden states ,
\end{itemize}
where  denotes the size of the recurrent latent states.

\subsection{Slot-wise decoder}
\paragraph{Inputs} The outputs of all slots from the transition module  and  then act as the input to the decoder.
\paragraph{Outputs} The output of the decoder includes the slot-respective masks and RGB reconstructions: 
\begin{itemize}
    \item Slot-specific mask outputs ,
    \item Slot-specific RGB image reconstructions ,
\end{itemize}
which are then used as part of the input at the next iteration as specified above.

We generate the combined reconstructed image  by summing all slot estimates  and the background estimate  weighted with their corresponding masks  and , as specified further in Algorithm~\ref{algo:loci}.

\subsection{Sequence Initialization}
\label{sec:algini}
At time step  we determine the network's inputs based on randomly generated, fictive position and Gestalt estimates for each slot , which are then fed through the Loci decoder module.
Initial position and Gestalt codes  and  are sampled from 
an isotropic Gaussian distribution  and
a factorized Gaussian distribution 
with learnable parameters 
,  and 
, , respectively.
The third position code value of , that is, the Gaussian standard deviation , is set to , where   denotes the number of pixels in a row, effectively setting  to one pixel distance.
The fourth position code value, that is, the priority value , is set to its index value , 
inducing an ordered priority, which biases initial random slot assignments and thus bootstraps initial slot-assignment progress.

Based on the initial codes, we generate estimates of the output mask , reconstruction  and Gaussian positions  by calling the slot-wise decoder (see Algorithm~\ref{algo:loci} line~\ref{algo:loci_decoder} and following): 
.
We finally determine the first RGB image reconstruction .

The hidden states of the recurrent neural network GateL0RD are initialized to zero, that is, .


\subsection{Main processing loop}
Loci first runs the main processing loop for ten time steps with the first video image of a particular image sequence. It thereby bootstraps the objects into individual slots, somewhat similar to previous slot-attention approaches \cite{Locatello:2020}.
After the ten initial time steps, Loci keeps re-initializing positions  of a slot  to random values (as specified above) given that  has been smaller than  for all time points until . This induces an initial active search process.
For longer video sequences, such as the Aquarium footage, this search process was also used for invisible slots, which increased the number of tracked objects, but negatively influenced object permanence.

\begin{algorithm}[b!]
   \caption{\texttt{Loci-Algorithm} (main processing loop)}
   \label{algo:loci}
\begin{algorithmic}[1]
    \vspace{.1cm}
    \STATE \textbf{Inputs:} Input video , static background 
    \STATE \textbf{Network parameters:} , , 
    \STATE \textbf{Additional parameters:}
    initialization parameters ; background threshold , which is encoded as a uniform offset mask ; number of slots ; processing steps 
    \\\vspace{-.1cm}
    \hrulefill
    \\\vspace{.1cm}
    \STATE  Initialize  \COMMENT{see Section~\ref{sec:algini} for details}
    \vspace{-.3cm}
    \FOR{}
            \vspace{.1cm}
        \STATE \COMMENT{\underline{Pre-processing:}}
\STATE 
        \STATE \texttt{compute complements} , 
            \vspace{.1cm}
        \STATE \COMMENT{\underline{Slot-wise encoder:}}
\STATE 
\STATE 
\STATE 
            \STATE 
\vspace{.1cm}
        \STATE \COMMENT{\underline{Transition module:}}
\STATE  
            \vspace{.1cm}
        \STATE \COMMENT{\underline{Gestalt binarization:}}
        \STATE  
        \STATE 
            \vspace{.1cm}
        \STATE\COMMENT{\label{algo:loci_decoder}\underline{Slot-wise decoder:}}
\STATE \COMMENT{ encodes all pixel positions normalized to ,  the number of pixels in a row}
\STATE  
            \vspace{-.2cm}
            \STATE 
            \STATE 
                \vspace{.1cm}
            \STATE \COMMENT{\underline{Post-processing:}}
\STATE 
            \STATE 
    \ENDFOR
\STATE \textbf{return} 
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t!]
   \caption{\texttt{Priority-based-Attention}}
   \label{algo:atten}
\begin{algorithmic}[1]
    \vspace{.1cm}
    \STATE \textbf{Inputs:} 
    Gestalt: , 
    Gaussian 2d position: ,
    priority: 
    \STATE \textbf{Additional parameters:} values of the learnable  are initially set to , while   induces a default slot-order bias.
    \\\vspace{-.1cm}
    \hrulefill
    \\\vspace{.1cm}
    \STATE  \COMMENT{Scale priorities and add Noise}
    \STATE \COMMENT{Subtract Gaussian attention from other slots, scaled by priority ( denotes the sigmoid)}
    \STATE  
    \STATE  \COMMENT{}
    \STATE \textbf{return} 
\end{algorithmic}
\end{algorithm}


Note that the prediction error is calculated across the three RGB channels. 
It determines the MSE between the input  and the static background , multiplied per Hadamard-product with the forth square root MSE between the input  and the predicted input .
The fourth square root emphasizes small differences, encouraging accurate encodings of individual entities.

In the transition module we apply a single trainable parametric bias neuron alpha, as proposed in \citet{bachlechner2021rezero}, instead of layer-normalization. 
Alpha is initialized to zero.
Its current value is multiplied with the output vector before the residual parts of the transition module. 
These alpha-residuals enforce the predictor to initially compute the identity function.
The mechanism bootstraps Loci's learning progress by initially focusing it on developing decoder-suitable Gestalt encodings.







\begin{figure}[ht!]
  \centering
  \includegraphics[width=\textwidth,keepaspectratio]{figures/encoder-flow-chart-v2.pdf}
  \caption{Encoder module diagram (identical for all slots )}
  \label{fig:encoder_flow_chart}
\end{figure}

\begin{figure}[!b]
  \centering
  \includegraphics[width=\textwidth,keepaspectratio]{figures/predictor-flow-chart-v2.pdf}
  \caption{Transition module diagram (identical for all slots )}
  \label{fig:predictor_flow_chart}
\end{figure}


\begin{figure}[htb!]
  \centering
  \includegraphics[width=\textwidth,keepaspectratio]{figures/decoder-flow-chart-v2.pdf}
  \caption{Decoder module diagram (identical for all slots )}
  \label{fig:decoder_flow_chart}
\end{figure}


 



%
 
\end{document}
