\documentclass{bmvc2k}

\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{amssymb}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{subcaption}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{xcolor, soul}
\usepackage{todonotes}
\usepackage{fancyhdr,graphicx,amsmath,amssymb}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{cleveref}



\hyphenation{
  acad-e-my
  acad-e-mies
  ac-cu-sa-tive
  acro-nym
  acro-nyms
  acu-punc-ture
  acu-punc-tur-ist
  ad-a-mant
  add-a-ble
  add-i-ble
  adren-a-line
  aero-space
  af-ter-thought
  af-ter-thoughts
  agron-o-mist
  agron-o-mists
  al-ge-bra-i-cal-ly
  al-ma-nac
  al-ma-nacs
  al-ma-nack
  al-ma-nacks
  anach-ro-nism
  anach-ro-nis-tic
  an-a-lect
  an-a-lects
  an-a-lyse
  an-a-lysed
  analy-ses
  analy-sis
  an-eu-rysm
  an-eu-rysms
  an-eu-rys-mal
  an-iso-trop-ic
  an-iso-trop-i-cal-ly
  an-isot-ro-pism
  an-isot-ropy
  an-ni-ver-sary
  an-ni-ver-saries
  anom-a-ly
  anom-a-lies
  an-o-nym-i-ty
  anon-y-mous
  anon-y-mously
  anti-bi-ot-ic
  anti-bi-ot-ics
  anti-deriv-a-tive
  anti-deriv-a-tives
  anti-holo-mor-phic
  an-tin-o-my
  an-tin-o-mies
  anti-nu-clear
  anti-nu-cle-on
  an-tip-o-des
  anti-rev-o-lu-tion-ary
  a-peri-odic
  apoth-e-o-ses
  apoth-e-o-sis
  ap-pen-di-ces
  ap-pen-dix
  ap-pen-dixes
  ar-chi-me-dean
  ar-chi-pel-ago
  ar-chi-pel-a-gos
  ar-chive
  ar-chives
  ar-chiv-ing
  ar-chiv-ist
  ar-chiv-ists
  ar-che-typ-al
  ar-che-type
  ar-che-types
  ar-che-typ-i-cal
  arc-tan-gent
  arc-tan-gents
  areas
  a-spher-ic
  a-spher-i-cal
  as-sign-a-ble
  as-sign-or
  as-sign-ors
  as-sist-ant
  as-sist-ance
  as-sist-ant-ship
  as-sist-ant-ships
  as-trol-o-ger
  as-trol-o-gers
  as-tron-o-mer
  as-tron-o-mers
  asymp-to-matic
  as-ymp-tot-ic
  asyn-chro-nous
  ath-er-o-scle-ro-sis
  at-mos-phere
  at-mos-pheres
  at-tri-bute
  at-trib-uted
  at-trib-ut-able
  au-to-ma-tion
  au-tom-a-ton
  au-tom-a-ta
  auto-num-ber-ing
  au-ton-o-mous
  auto-re-gres-sion
  auto-re-gres-sive
  auto-round-ing
  av-oir-du-pois
  awar-ded
  awe-struck
  back-pedal
  back-pedals
  back-pedal-ing
  back-scratcher
  back-scratch-ing
  band-lead-er
  band-lead-ers
  bank-roll
  bank-rolls
  bank-rupt
  bank-rupts
  bank-rupt-cy
  bank-rupt-cies
  bar-onies
  base-line-skip
  ba-thym-e-try
  bathy-scaphe
  bean-ies
  be-drag-gle
  be-drag-gled
  bed-rid-den
  bed-rock
  be-dwarf
  be-dwarfs
  be-hav-iour
  be-hav-iours
  bevies
  bib-lio-graph-i-cal
  bib-li-og-ra-phy-style
  bib-units
  bi-dif-fer-en-tial
  big-gest
  big-shot
  big-shots
  bill-able
  bio-in-for-mat-ics
  bio-mass
  bio-math-e-mat-ics
  bio-med-i-cal
  bio-med-i-cine
  bio-rhythms
  bio-weap-ons
  bio-weap-on-ry
  bit-map
  bit-maps
  bland-er
  bland-est
  blind-er
  blind-est
  blondes
  blue-print
  blue-prints
  bo-lom-e-ter
  bo-lom-e-ters
  book-sell-er
  book-sell-ers
  bool-ean
  bool-eans
  bor-no-log-i-cal
  bot-u-lism
  brusquer
  buf-fer
  buf-fers
  bun-gee
  bun-gees
  busier
  busi-est
  bussing
  butted
  buzz-word
  buzz-words
  cache-abil-ity
  cache-able
  ca-coph-o-ny
  ca-coph-o-nies
  call-er
  call-ers
  cam-era-men
  cart-wheel
  cart-wheels
  ca-tarrh
  ca-tarrhs
  ca-tas-tro-phe
  ca-tas-tro-phes
  cat-a-stroph-ic
  cat-a-stroph-i-cally
  ca-tas-tro-phism
  cat-e-noid
  cat-e-noids
  cau-li-flow-er
  cha-me-leon
  cha-me-leons
  chan-cery
  cha-ot-ic
  chap-ar-ral
  char-treuse
  chemo-kine
  chemo-kines
  chemo-ther-apy
  chemo-ther-a-pies
  chloro-meth-ane
  chloro-meth-anes
  cho-les-teric
  cig-a-rette
  cig-a-rettes
  cinque-foil
  co-asso-cia-tive
  coch-lea
  coch-leas
  coch-lear
  co-designer
  co-designers
  co-gnac
  co-gnacs
  co-gno-scen-ti
  co-ker-nel
  co-ker-nels
  col-lin-ea-tion
  col-o-phon
  col-o-phons
  col-um-bine
  col-um-bines
  col-um-nar
  col-umns
  com-par-and
  com-par-ands
  com-pen-dium
  com-po-nent-wise
  comp-trol-ler
  comp-trol-lers
  com-put-able
  com-put-abil-ity
  con-form-able
  con-form-ist
  con-form-ists
  con-form-ity
  con-ge-ries
  con-gress
  con-gresses
  con-struc-ted
  con-struc-ti-ble
  con-struc-ti-bil-ity
  con-tract-able
  con-trib-ute
  con-trib-utes
  con-trib-uted
  copy-right-able
  co-re-la-tion
  co-re-la-tions
  co-re-li-gion-ist
  co-re-li-gion-ists
  co-re-op-sis
  co-re-spon-dent
  co-re-spon-dents
  co-se-cant
  co-semi-sim-ple
  co-tan-gent
  cour-age
  cou-ra-geous
  cou-ra-geous-ly
  cour-ses
  co-work-er
  co-work-ers
  crank-case
  crank-shaft
  cric-ket
  cric-kets
  croc-o-dile
  croc-o-diles
  cross-hatch
  cross-hatched
  cross-hatch-ing
  cross-over
  cryp-to-gram
  cryp-to-grams
  cuff-link
  cuff-links
  cui-sines
  cu-nei-form
  cus-tom-er
  cus-tom-ers
  cus-tom-iz-a-ble
  cus-tom-ize
  cus-tom-izes
  cus-tom-ized
  cy-ber-virus
  cy-ber-viruses
  cy-ber-wea-pon
  cy-ber-wea-pons
  cy-to-kine
  cy-to-kines
  dachs-hund
  dam-sel-fly
  dam-sel-flies
  dactyl-o-gram
  dactyl-o-graph
  data-base
  data-bases
  data-flow
  data-path
  data-paths
  data-point
  data-points
  data-set
  data-sets
  data-type
  data-types
  date-stamp
  date-stamps
  de-allo-cate
  de-allo-cates
  de-allo-cated
  de-allo-ca-tion
  de-allo-ca-tions
  de-clar-able
  de-fin-i-tive
  de-lec-ta-ble
  demi-semi-qua-ver
  demi-semi-qua-vers
  de-moc-ra-cy
  dem-o-crat
  dem-o-crats
  dem-o-crat-ic
  de-moc-ra-tism
  de-mon-stra-ble
  de-mon-stra-bly
  dem-on-strate
  dem-on-strates
  dem-on-strated
  dem-on-stra-ting
  dem-on-stra-tion
  dem-on-stra-tions
  demos
  der-i-va-tion
  der-i-va-tions
  der-i-va-tion-al
  de-riv-a-tive
  de-riv-a-tives
  dia-lect
  dia-lects
  di-a-lec-tal
  dia-lec-tic
  dia-lec-tics
  di-a-lec-ti-cal
  dia-lec-ti-cian
  dia-lec-ti-cians
  di-a-tom
  di-a-toms
  di-a-to-ma-ceous
  dif-fract
  dif-fracts
  dif-frac-tion
  dif-frac-tions
  di-lem-ma
  di-lem-mas
  direr
  dire-ness
  dis-par-and
  dis-par-ands
  dis-traught-ly
  dis-trib-ut-able
  dis-trib-ute
  dis-trib-utes
  dis-trib-uted
  dis-trib-ut-ing
  dis-trib-u-tive
  dis-util-ity
  dou-ble-space
  dou-ble-spaced
  dou-ble-spac-ing
  dou-ble-talk
  doll-ish
  drift-age
  driv-ers
  drom-e-dary
  drom-e-daries
  drop-let
  drop-lets
  dro-soph-i-la
  du-op-o-list
  du-op-o-lists
  du-op-o-ly
  du-op-o-lies
  dys-lexia
  dys-lec-tic
  dys-topia
  east-end-ers
  eco-sys-tem
  eco-sys-tems
  eco-nom-ics
  econ-o-mies
  econ-o-mist
  econ-o-mists
  egg-shell
  egg-shells
  ei-gen-class
  ei-gen-classes
  ei-gen-val-ue
  ei-gen-val-ues
  elec-tro-en-cephalo-gram
  electro-mechan-i-cal
  electro-mechano-acoustic
  elec-tro-pho-re-sis
  elec-tro-pho-ret-ic
  elit-ist
  elit-ists
  end-game
  en-dos-copy
  en-dos-copies
  en-tre-pre-neur
  en-tre-pre-neurs
  en-tre-pre-neur-ial
  ephem-era
  ephem-eris
  ephem-er-i-des
  epi-graphs
  ep-i-neph-rine
  eps-to-pdf
  equa-ted
  equi-di-men-sional
  equi-vari-ant
  equi-vari-ance
  er-go-nom-ic
  er-go-nom-ics
  er-go-nom-i-cally
  es-sence
  es-sences
  eu-sta-chian
  ever-si-ble
  evert
  everts
  evert-ed
  evert-ing
  ex-plan-a-tory
  ex-pli-cit
  ex-pli-cit-ly
  ex-quis-ite
  ex-tra-or-di-nary
  face-lift
  face-lifts
  face-lift-ing
  fall-ing
  fermi-ons
  fi-du-ciary
  fi-du-ciar-ies
  figu-rine
  figu-rines
  fi-nite-ly
  fla-gel-lum
  fla-gel-la
  flam-ma-bles
  fledg-ling
  flow-chart
  flow-charts
  fluor-os-copies
  fluor-os-copy
  font-enc
  for-mi-da-ble
  for-mi-da-bly
  for-syth-ia
  forth-right
  fra-gil-i-ty
  free-loader
  free-loaders
  free-sia
  free-sias
  friend-lier
  friend-li-est
  fri-vol-ity
  fri-vol-i-ties
  friv-o-lous
  front-end
  front-ends
  ga-lac-tic
  gal-axy
  gal-ax-ies
  gaz-et-teer
  gaz-et-teers
  gas-om-e-ter
  ge-o-des-ic
  ge-o-det-ic
  ge-om-eter
  ge-om-eters
  geo-met-ric
  geo-met-rics
  ge-o-strophic
  geo-ther-mal
  ge-ot-ro-pism
  giga-nodes
  gno-mon
  gno-mons
  gran-di-ose
  grand-uncle
  grand-uncles
  graph-eme
  graph-emes
  gra-phe-mic
  gra-phe-mics
  gra-phe-tic
  gra-phe-tics
  grapho-lin-guis-tic
  grapho-lin-guis-tics
  griev-ance
  griev-ances
  griev-ous
  griev-ous-ly
  group-like
  hair-style
  hair-styles
  hair-styl-ist
  hair-styl-ists
  half-life
  half-lives
  half-space
  half-spaces
  half-tone
  half-tones
  half-way
  har-bin-ger
  har-bin-gers
  har-le-quin
  har-le-quins
  hatch-eries
  hei-nous
  he-lio-pause
  he-lio-trope
  hemi-demi-semi-qua-ver
  hemi-demi-semi-qua-vers
  he-mo-glo-bin
  he-mo-phil-ia
  he-mo-phil-iac
  he-mo-phil-iacs
  hemo-rhe-ol-ogy
  he-pat-ic
  he-pat-ica
  her-maph-ro-dite
  her-maph-ro-dit-ic
  he-roes
  het-ero-in-ter-face
  het-ero-in-ter-faces
  hexa-dec-i-mal
  hip-po-po-ta-mus
  holo-deck
  holo-decks
  ho-lo-no-m
  ho-lo-no-my
  ho-lo-no-mies
  ho-meo-mor-phic
  ho-meo-mor-phism
  ho-meo-stat-ic
  ho-meo-stat-ics
  ho-meo-sta-sis
  hom-o-nym
  hom-o-nyms
  hom-o-nym-ic
  ho-mon-y-mous
  ho-mon-y-my
  ho-moph-o-nous
  ho-moph-o-ny
  ho-mo-thetic
  horse-rad-ish
  hot-bed
  hot-beds
  hounds-teeth
  hounds-tooth
  hy-dro-ther-mal
  hy-per-elas-tic-ity
  hy-phen-a-tion
  hy-phen-a-tions
  hy-po-elas-tic-ity
  hy-po-thal-a-mus
  ico-nog-ra-pher
  ico-nog-ra-phers
  icon-o-graph-ic
  ico-nog-ra-phy
  ideals
  ideo-graphs
  id-i-o-lect
  id-i-o-lects
  idio-syn-crasy
  idio-syn-cra-sies
  idio-syn-cratic
  idio-syn-crat-i-cal-ly
  ig-nit-er
  ig-nit-ers
  ig-ni-tor
  ignore-spaces
  il-li-quid
  il-li-quid-ity
  im-mu-ni-za-tion
  im-mu-no-mod-u-la-to-ry
  im-ped-ance
  im-ped-ances
  in-du-bi-ta-ble
  in-fin-ite-ly
  in-fin-i-tes-i-mal
  in-fra-struc-ture
  in-fra-struc-tures
  input-enc
  in-sig-nif-i-cant
  in-stall-er
  in-stall-ers
  in-teg-rity
  in-ter-dis-ci-pli-nary
  inter-ele-ment
  in-ter-ga-lac-tic
  in-ter-view-ee
  in-ter-view-ees
  in-trac-ta-ble
  in-trac-ta-bil-ity
  in-utile
  in-util-i-ty
  ir-ra-tio-nal
  ir-re-duc-ible
  ir-re-duc-ibly
  ir-rev-o-ca-ble
  iso-geo-met-ric
  iso-geo-met-rics
  iso-ther-mal
  iso-trop-ic
  isot-ropy
  itin-er-ary
  itin-er-ar-ies
  je-re-mi-ad
  je-re-mi-ads
  jun-ior
  key-note
  key-notes
  key-stroke
  key-strokes
  kilo-nodes
  kiln-ing
  lab-y-rinth
  lab-y-rinths
  lab-y-rin-thi-an
  lab-y-rin-thine
  lac-i-est
  lam-en-ta-ble
  land-scap-er
  land-scap-ers
  lar-cen-ist
  lar-cen-ists
  lar-ce-ny
  lar-ce-nies
  leaf-hop-per
  leaf-hop-pers
  leaf-let
  leaf-lets
  le-ga-cy
  le-ga-cies
  leg-ate
  leg-ates
  le-ga-tion
  le-ga-tions
  let-ter-space
  let-ter-spaces
  let-ter-spaced
  let-ter-spac-ing
  leu-ko-cyte
  leu-ko-cytes
  leu-ko-tri-ene
  leu-ko-tri-enes
  life-span
  life-spans
  life-style
  life-styles
  lift-off
  light-weight
  lim-ou-sines
  line-backer
  line-spacing
  li-on-ess
  li-quid-ity
  lith-o-graphed
  lith-o-graphs
  lo-bot-omy
  lo-bot-om-ize
  loges
  long-est
  look-ahead
  lo-quac-ity
  love-struck
  Lua-TeX
  Lua-LaTeX
  macro-eco-nomic
  macro-eco-nomics
  macro-econ-omy
  mac-ros
  Make-Index
  mal-invest-ment
  mal-invest-ments
  mal-a-prop-ism
  mal-a-prop-isms
  man-slaugh-ter
  man-u-script
  man-u-scripts
  mar-gin-al
  mark-up
  math-e-ma-ti-cian
  math-e-ma-ti-cians
  mattes
  med-ic-aid
  medi-ocre
  medi-oc-ri-ties
  mega-fau-na
  mega-fau-nal
  mega-lith
  mega-liths
  mega-nodes
  mensch
  meta-bol-ic
  me-tab-o-lism
  me-tab-o-lisms
  me-tab-o-lite
  me-tab-o-lites
  meta-form
  meta-forms
  meta-lan-guage
  meta-lan-guages
  meta-phor
  meta-phors
  meta-phor-i-cal
  meta-phor-i-cal-ly
  meta-sta-bil-ity
  meta-stable
  meta-table
  meta-tables
  metem-psy-cho-sis
  meth-od
  meth-od-o-lo-gy
  meth-od-o-lo-gies
  meth-od-o-logical
  meth-od-o-logically
  me-trop-o-lis
  me-trop-o-les
  met-ro-pol-i-tan
  met-ro-pol-i-tans
  micro-eco-nomic
  micro-eco-nomics
  micro-econ-omy
  micro-en-ter-prise
  micro-en-ter-prises
  mi-cro-fiche
  mi-cro-fiches
  micro-organ-ism
  micro-organ-isms
  mi-cro-struc-ture
  mid-after-noon
  mill-age
  mil-li-liter
  mil-li-liters
  mimeo-graphed
  mimeo-graphs
  mim-ic-ries
  mine-sweeper
  mine-sweepers
  min-is
  mini-sym-po-sium
  mini-sym-po-sia
  mi-nut-er
  mi-nut-est
  mis-chie-vous-ly
  mi-sers
  mi-sog-a-my
  mne-mon-ic
  mne-mon-ics
  mod-el-ling
  mo-lec-u-lar
  mol-e-cule
  mol-e-cules
  mon-archs
  money-len-der
  money-len-ders
  mono-chrome
  mono-en-er-getic
  mon-oid
  mon-oph-thong
  mon-oph-thongs
  mono-pole
  mono-poles
  mo-nop-oly
  mono-space
  mono-spaced
  mono-spacing
  mono-spline
  mono-splines
  mono-strofic
  mo-not-o-nies
  mo-not-o-nous
  mo-ron-ism
  mos-qui-to
  mos-qui-tos
  mos-qui-toes
  mud-room
  mud-rooms
  mul-ti-fac-eted
  mul-ti-plic-able
  mul-ti-plic-ably
  multi-user
  name-space
  name-spaces
  neo-field
  neo-fields
  neo-nazi
  neo-nazis
  neph-ews
  neph-rite
  neph-ritic
  new-est
  news-let-ter
  news-let-ters
  nil-po-tent
  node-list
  node-lists
  no-name
  non-ar-chi-me-dean
  non-ar-ith-met-ic
  non-emer-gency
  non-equi-vari-ance
  none-the-less
  non-euclid-ean
  non-iso-mor-phic
  non-pseudo-com-pact
  non-smooth
  non-uni-form
  non-uni-form-ly
  non-zero
  not-with-stand-ing
  nu-cleo-syn-the-sis
  nu-cleo-tide
  nu-cleo-tides
  nut-crack-er
  nut-crack-ers
  oer-steds
  off-line
  off-load
  off-loads
  off-loaded
  oli-gop-o-list
  oli-gop-o-lists
  oli-gop-o-ly
  oli-gop-o-lies
  om-ni-pres-ent
  om-ni-pres-ence
  ono-mat-o-poe-ia
  ono-mat-o-po-et-ic
  op-er-and
  op-er-ands
  orang-utan
  orang-utans
  or-tho-don-tist
  or-tho-don-tists
  or-tho-ker-a-tol-ogy
  over-view
  over-views
  ox-id-ic
  pad-ding
  page-rank
  pain-less-ly
  pal-ette
  pal-ettes
  pa-rab-ola
  pa-rab-olas
  par-a-bol-ic
  pa-rab-o-loid
  par-a-digm
  par-a-digms
  para-chute
  para-chutes
  para-graph-er
  para-le-gal
  par-al-lel-ism
  para-mag-net-ism
  para-medic
  para-medics
  pa-ram-e-tri-za-tion
  pa-ram-e-trize
  para-mil-i-tary
  para-mount
  pass-over
  path-o-gen-ic
  peev-ish
  peev-ish-ness
  pen-al-ize
  pen-al-ty
  pen-al-ties
  pen-ta-gon
  pen-ta-gons
  peri-spome-na
  peri-spome-non
  pe-tro-le-um
  phe-nom-e-non
  phi-lat-e-list
  phi-lat-e-lists
  phi-los-o-pher
  phi-los-o-phers
  phi-los-o-phies
  pho-neme
  pho-nemes
  pho-ne-mic
  phos-phor-ic
  pho-to-graphs
  pho-to-off-set
  phys-ics
  pic-a-dor
  pic-a-dors
  pipe-line
  pipe-lines
  pipe-lin-ing
  pi-ra-nhas
  placa-ble
  plant-hop-per
  plant-hop-pers
  pla-teau
  pla-teaus
  pleas-ance
  plug-in
  plug-ins
  pluri-sub-har-monic
  pol-ter-geist
  poly-an-dry
  poly-an-drous
  poly-dac-tyl
  poly-dac-tyl-lic
  po-lyg-a-mist
  po-lyg-a-mists
  polyg-on-i-za-tion
  po-lyg-y-ny
  po-lyg-y-nous
  po-lyn-ya
  po-lyn-yas
  pol-yp
  pol-yps
  po-lyph-o-ny
  po-lyph-o-nous
  poly-phon-ic
  poly-se-my
  pome-gran-ate
  pome-gran-ates
  poro-elas-tic
  por-ous
  por-ta-ble
  post-am-ble
  post-am-bles
  post-hu-mous
  post-script
  post-scripts
  pos-tur-al
  po-ta-ble
  po-ta-to
  po-ta-toes
  pre-am-ble
  pre-am-bles
  pre-dict-able
  predi-lec-tion
  predi-lec-tions
  pre-fers
  pre-loaded
  prem-ise
  prem-ises
  pre-par-ing
  pre-pend
  pre-pends
  pre-pend-ed
  pre-pend-ing
  prep-o-si-tion
  prep-o-si-tions
  prep-o-si-tional
  pre-print
  pre-prints
  pre-proces-sor
  pre-proces-sors
  pres-ent-ly
  pre-split-ting
  pre-wrap
  pre-wrapped
  priest-esses
  pret-ty-prin-ter
  pret-ty-prin-ting
  pro-ce-dur-al
  process
  pro-cur-ance
  prog-e-nies
  prog-e-ny
  pro-gram-mable
  pro-kary-ote
  pro-kary-otes
  pro-kary-ot-ic
  prom-i-nent
  pro-mis-cu-ous
  prom-is-sory
  prom-ise
  prom-ises
  pro-pel-ler
  pro-pel-lers
  pro-pel-ling
  pro-hib-i-tive
  pro-hib-i-tive-ly
  pro-sciut-to
  pros-ta-glan-din
  pros-ta-glan-dins
  pro-style
  pro-styles
  pro-test-er
  pro-test-ers
  pro-tes-tor
  pro-tes-tors
  pro-to-lan-guage
  pro-to-typ-al
  pro-vi-der
  pro-vi-ders
  prov-ince
  prov-inces
  pro-vin-cial
  pro-virus
  pro-viruses
  prow-ess
  pseu-do-dif-fer-en-tial
  pseu-do-fi-nite
  pseu-do-fi-nite-ly
  pseu-do-forces
  pseu-dog-ra-pher
  pseu-do-group
  pseu-do-groups
  pseu-do-num-ber
  pseu-do-num-bers
  pseu-do-nym
  pseu-do-nyms
  pseu-do-word
  pseu-do-words
  psy-che-del-ic
  psychs
  pu-bes-cence
  pur-ges
  quad-ding
  qua-drat-ic
  qua-drat-ics
  quad-ra-ture
  quad-ri-lat-er-al
  quad-ri-lat-er-als
  quad-ri-pleg-ic
  quad-ru-ped
  quad-ru-peds
  quad-ru-pole
  quad-ru-poles
  quaint-er
  quaint-est
  qua-si-equiv-a-lence
  qua-si-equiv-a-lences
  qua-si-equiv-a-lent
  qua-si-hy-po-nor-mal
  qua-si-rad-i-cal
  qua-si-resid-ual
  qua-si-smooth
  qua-si-sta-tion-ary
  qua-si-topos
  qua-si-tri-an-gu-lar
  qua-si-triv-ial
  quin-tes-sence
  quin-tes-sences
  quin-tes-sen-tial
  QWERTY
  rab-bit-ry
  ra-di-og-ra-phy
  raff-ish
  raff-ish-ly
  rag-ged
  ram-shackle
  rav-en-ous
  re-allo-cate
  re-allo-cates
  re-allo-cated
  re-arrange
  re-arranges
  re-arranged
  re-arrange-ment
  re-arrange-ments
  rec-i-proc-i-ties
  rec-i-proc-i-ty
  rec-tan-gle
  rec-tan-gles
  rec-tan-gu-lar
  re-di-rect
  re-di-rect-ion
  re-duc-ible
  re-echo
  re-edu-cate
  ref-u-gee
  ref-u-gees
  re-imple-ment
  re-imple-ments
  re-imple-mented
  re-imple-men-ta-tion
  ren-ais-sance
  re-phrase
  re-phrases
  re-phrased
  re-po-si-tion
  re-po-si-tions
  re-print
  re-prints
  re-print-ed
  re-stor-able
  retro-fit
  retro-fit-ted
  re-us-able
  re-use
  re-wire
  re-wrap
  re-wrapped
  re-write
  rhi-noc-er-os
  right-eous
  right-eous-ness
  ring-leader
  ring-leaders
  ro-bot
  ro-bots
  ro-botic
  ro-bot-ics
  roof-top
  roof-tops
  round-ish
  round-ish-ness
  round-table
  round-tables
  run-nable
  sales-clerk
  sales-clerks
  sales-woman
  sales-women
  sa-lient
  sal-mo-nel-la
  sal-ta-tion
  sar-sa-par-il-la
  sat-el-lite
  sat-el-lites
  sauer-kraut
  sca-lar
  sca-lars
  scat-o-log-i-cal
  scene-shift-er
  scene-shift-ing
  sched-ul-ing
  schiz-o-phrenic
  schnau-zer
  school-child
  school-child-ren
  school-teacher
  school-teach-ers
  scru-ti-ny
  scyth-ing
  sell-er
  sell-ers
  sec-re-tar-iat
  sec-re-tar-iats
  sem-a-phore
  sem-a-phores
  se-mes-ter
  semi-def-i-nite
  semi-di-rect
  semi-ho-mo-thet-ic
  semi-ring
  semi-rings
  semi-sim-ple
  semi-skilled
  semi-sta-ble
  sem-itic
  ser-geant
  ser-geants
  sero-epi-de-mi-o-log-i-cal
  ser-vo-me-chan-i-cal
  ser-vo-mech-a-nism
  ser-vo-mech-a-nisms
  ses-qui-pe-da-lian
  set-up
  set-ups
  se-vere-ly
  shap-able
  shape-able
  shoe-string
  shoe-strings
  shop-lift-er
  shop-lift-ing
  show-hy-phens
  side-step
  side-steps
  side-swipe
  sign-age
  single-space
  single-spaced
  single-spacing
  SI-units
  sky-scraper
  sky-scrapers
  sln-uni-code
  smoke-stack
  smoke-stacks
  snor-kel-ing
  so-le-noid
  so-le-noids
  solute
  solutes
  sov-er-eign
  sov-er-eigns
  spa-ces
  spe-cious
  spec-tros-co-py
  spell-er
  spell-ers
  spell-ing
  spe-lunk-er
  spend-thrift
  spher-oid
  spher-oids
  spher-oid-al
  sphin-ges
  spic-i-ly
  spin-or
  spin-ors
  spokes-man
  spokes-men
  spokes-per-son
  spokes-per-sons
  spokes-woman
  spokes-women
  sports-cast
  sports-cast-er
  spor-tive-ly
  sports-wear
  sports-writer
  sports-writers
  spright-lier
  squea-mish
  stand-alone
  star-tling
  star-tling-ly
  sta-tis-tics
  stealth-ily
  steeple-chase
  stereo-graph-ic
  ste-ril-i-ty
  sto-chas-tic
  stop-list
  stop-lists
  strange-ness
  strap-hanger
  strat-a-gem
  strat-a-gems
  stretch-abil-ity
  stretch-i-er
  strip-tease
  strong-est
  strong-hold
  stu-pid-er
  stu-pid-est
  style-sheet
  style-sheets
  sub-dif-fer-en-tial
  sub-ex-pres-sion
  sub-ex-pres-sions
  sub-node
  sub-nodes
  sub-scrib-er
  sub-scrib-ers
  sub-se-lect
  sub-se-lects
  sub-se-lected
  sub-se-lec-tion
  sub-se-lec-tions
  sub-tables
  sum-ma-ble
  super-deri-va-tion
  super-deri-va-tions
  super-ego
  super-egos
  super-el-lipse
  super-el-lipses
  super-ellip-ti-cal
  super-ellip-ti-cally
  super-ellip-ti-cal-ness
  su-pra-ordi-nate
  su-prem-a-cist
  su-prem-a-cists
  sur-gery
  sur-ger-ies
  sur-ges
  sur-veil-lance
  swim-ming-ly
  symp-to-matic
  syn-chro-ni-city
  syn-chro-mesh
  syn-chro-nous
  syn-chro-tron
  syn-o-nym
  syn-o-nyms
  syn-on-y-mous
  syn-on-y-my
  tab-leau
  tab-leaux
  taff-rail
  take-over
  take-overs
  talk-a-tive
  ta-pes-try
  ta-pes-tries
  tar-pau-lin
  tar-pau-lins
  te-leg-ra-pher
  te-leg-ra-phers
  tele-ki-net-ic
  tele-ki-net-ics
  tele-ro-bot-ics
  tell-er
  tell-ers
  tem-po-rar-ily
  ten-ure
  test-bed
  tera-nodes
  text-height
  text-length
  text-width
  thal-a-mus
  ther-mal
  ther-mo-elas-tic
  ther-mo-elas-tic-ity
  ther-mom-eter
  ther-mom-eters
  ther-mo-nu-clear
  thesis
  theses
  ti-ger
  ti-gers
  time-stamp
  time-stamps
  ti-ta-nate
  tool-kit
  tool-kits
  topo-graph-i-cal
  topo-iso-mer-ase
  topo-iso-mer-ases
  toques
  trac-ta-ble
  trai-tor-ous
  trans-at-lan-tic
  trans-ceiver
  trans-ceivers
  tran-sept
  tran-septs
  trans-gress
  trans-pacific
  trans-par-en-cy
  trans-par-en-cies
  trans-pile
  trans-piles
  trans-piled
  trans-pi-ler
  trans-pi-lers
  trans-pil-ing
  trans-ver-sal
  trans-ver-sals
  trans-ves-tite
  trans-ves-tites
  tra-pe-zium
  trap-e-zoid
  trap-e-zoids
  trap-e-zoi-dal
  tra-vers-a-ble
  tra-ver-sal
  tra-ver-sals
  treach-eries
  tribes-man
  trig-o-no-met-ric
  trig-o-nom-e-try
  trip-let
  trip-lets
  tri-plex
  tri-plex-es
  trou-ba-dour
  tur-key
  tur-keys
  turn-around
  turn-arounds
  typ-al
  uber-mensch
  un-at-tached
  un-err-ing-ly
  un-friend-ly
  un-friend-li-er
  un-in-stan-ti-at-ed
  un-pre-dict-a-ble
  un-pre-dict-a-bly
  vaguer
  vaude-ville
  vic-ars
  vil-lain-ess
  vis-ual
  vis-ual-ly
  vi-vip-a-rous
  voice-print
  vspace
  wad-ding
  wall-flower
  wall-flow-ers
  warm-er
  warm-est
  waste-water
  wave-guide
  wave-guides
  wave-let
  wave-lets
  weap-ons
  weap-on-ry
  web-like
  web-log
  web-logs
  week-night
  week-nights
  weight-lift-er
  weight-lift-ing
  wheel-chair
  wheel-chairs
  which-ever
  white-sided
  white-space
  white-spaces
  wide-spread
  wing-span
  wing-spans
  wing-spread
  witch-craft
  word-spac-ing
  work-around
  work-arounds
  work-horse
  work-horses
  wrap-around
  wrap-arounds
  wretch-ed
  wretch-ed-ly
  yes-ter-year
  Alex-an-der
  Alex-an-dria
  Alex-an-drine
  al-ge-brai-sche
  Al-gon-quian
  Al-gon-quin
  Al-le-ghe-ny
  An-da-lu-cia
  An-da-lu-cian
  An-da-lu-sia
  An-da-lu-sian
  Apol-lo-dorus
  Arbor-Text
  Ar-kan-sas
  Ath-ens
  Auf-lage
  Aus-tral-asian
  auto-ma-ti-sier-ter
  Beb-chuk
  Be-die-nung
  Bembo
  bi-blio-gra-phi-sche
  Big-elow
  Bos-ton
  Bow-ditch
  Bring-hurst
  Brink-mann
  Brown-ian
  Bruns-wick
  Bu-da-pest
  Burck-hardt
  Can-a-da
  Ca-na-di-an
  Cara-theo-dory
  Car-ib-bean
  Car-ne-gie
  Charles-ton
  Char-lottes-ville
  Ches-ter
  Chiang
  Chi-ca-go
  Chich-es-ter
  Cohen
  Co-lum-bi-an
  Co-lum-bia
  com-pa-rai-son
  Czecho-slo-va-kia
  Del-a-ware
  Dijk-stra
  Dor-ches-ter
  Dorf-leit-ner
  Drechs-ler
  Duane
  dy-na-mi-sche
  Eijk-hout
  Engle
  Engel
  Eng-lish
  Euler-ian
  Evan-ston
  Feb-ru-ary
  Fest-schrift
  Flor-i-da
  Flor-i-d-ian
  For-schungs-in-sti-tut
  Frank-lin
  Free-BSD
  funk-tsional
  Gauss-ian
  Geof-frey
  Geo-gebra
  Ge-sell-schaft
  Ghost-script
  Ghost-View
  Gott-fried
  Gott-lieb
  Gran-jon
  Grass-mann-ian
  Greifs-wald
  Grothen-dieck
  Grund-leh-ren
  Ha-da-mard
  Hai-fa
  Ham-il-ton
  Ham-il-to-nian
  Hara-lam-bous
  Has-kell
  Hed-rick
  Hegel-ian
  Hel-sinki
  Her-mit-ian
  Hew-lett
  Hibbs
  Hil-bert
  Hoef-ler
  Hoek-water
  Hok-kai-do
  Huber
  Image-Magick
  Jac-kow-ski
  Jan-u-ary
  Ja-pa-nese
  Java-Script
  Je-re-miah
  Jo-seph
  Jung-ian
  Kad-om-tsev
  Kan-sas
  Karls-ruhe
  Keynes-ian
  Knuth-ian
  Kor-te-weg
  Krishna
  Krish-na-ism
  Krish-nan
  Kron-ecker
  Kunst-aka-de-mie
  Lan-cas-ter
  La-place
  Le-gendre
  Leices-ter
  Les-ter
  Lip-schitz
  Lip-schitz-ian
  Loj-ban
  Lou-i-si-ana
  Lucas
  MacBeth
  Mac-OS
  Ma-gel-lan
  Mal-a-ya-lam
  Man-ches-ter
  Mar-kov-ian
  Markt-ober-dorf
  Mass-a-chu-setts
  Max-i-mil-ian
  Max-well
  Mes-o-po-ta-mia
  Mes-o-po-ta-mian
  Meth-od-ist
  Meth-od-ism
  Mi-cro-soft
  Min-kow-ski
  Min-ne-ap-o-lis
  Min-ne-sota
  Moj-ca
  Mon-aco
  Mon-ta-gnard
  Mon-ta-gnards
  Mont-real
  Mora-wetz
  Mor-dell
  Mos-cow
  Nach-rich-ten
  Nash-ville
  Net-BSD
  Net-scape
  Nich-o-las
  Nietz-sche
  Nij-me-gen
  Noe-ther-ian
  Noord-wijker-hout
  North-amp-ton
  Noto-wi-digdo
  No-vem-ber
  Obst-feld
  Open-BSD
  Open-Office
  Oreo-pou-los
  Pack-ard
  Paki-stan
  Pala-tino
  Pa-ler-mo
  Pe-trov-ski
  Phin-eas
  Pfaff-ian
  Phil-a-del-phia
  phi-lo-so-phi-sche
  Por-tu-guese
  Poin-care
  Po-ten-tial-glei-chung
  Po-to-mac
  Pres-by-terian
  Pres-by-terians
  Pyong-yang
  Py-thag-o-ras
  Py-thag-o-re-an
  Ra-dha-krish-nan
  raths-kel-ler
  Ravi-kumar
  Reich-lin
  Rich-ard
  Rie-mann-ian
  Robes-pierre
  Ro-ches-ter
  Ryd-berg
  Schim-mel-pfen-nig
  Scholar-TeX
  schot-ti-sche
  Schro-din-ger
  Schwa-ba-cher
  Schwarz-schild
  Schweid-nitz
  Schwert
  Sep-tem-ber
  Shore-ditch
  Singa-pore
  Singa-po-re-an
  Singa-po-re-ans
  Skoup
  South-amp-ton
  Stokes-sche
  Stutt-gart
  Sus-que-han-na
  Syl-ves-ter
  Tau-ber-ian
  tech-ni-sche
  Ten-nes-see
  Thiruv-ananda-puram
  Tol-ches-ter
  To-ma-szew-ski
  To-Unicode
  Toyo-ta
  ty-po-graphique
  Ukrain-ian
  Ven-e-zu-e-la
  Ven-e-zu-e-lan
  ver-all-ge-mei-nerte
  Ver-ei-ni-gung
  Ver-tei-lun-gen
  Vid-ias-sov
  Vieth
  viiith
  viith
  Wahr-schein-lich-keits-theo-rie
  Wein-stein
  Werk-zeuge
  Wer-ner
  Wer-ther-ian
  Wiki-pe-dia
  Wil-czyn-ski
  Will-iam
  Will-iams
  Win-ches-ter
  Wing-dings
  Wirt-schaft
  wis-sen-schaft-lich
  Wolff-ian
  Wolfs-kehl
  Wood-row
  Xerox
  xiiith
  xviiith
  xviith
  xxiiird
  xxiind
  Ying-yong Shu-xue Ji-suan
  Zea-land
  Zeit-schrift
  acryl-amide
  acryl-amides
  acryl-alde-hyde
  am-phet-a-mine
  am-phet-a-mines
  ATP-ase
  ATP-ases
  cor-ti-co-steroid
  di-chloro-meth-ane
  eth-ane
  eth-yl-am-ine
  eth-yl-ate
  eth-yl-ated
  eth-yl-ene
  ethy-nyl
  ethy-nyl-a-tion
  fluoro-car-bon
  meth-am-phet-a-mine
  meth-ane
  meth-yl-am-mo-nium
  meth-yl-ate
  meth-yl-ated
  meth-yl-a-tion
  meth-yl-ene
  nitro-meth-ane
  nor-ep-i-neph-rine
  ortho-nitro-toluene
  para-di-methyl-benzene
  para-fluoro-toluene
  para-methyl-anisole
  phe-nol-phthalein
  phenyl-ala-nine
  phtha-lam-ic
  phthal-ate
  phthi-sis
  poly-ene
  poly-ethy-lene
  poly-pep-tide
  poly-pep-tides
  poly-styrene
  poly-vinyl
  tetra-butyl-ammo-nium
  tri-ethyl-amine
}

 \newtheorem*{remark}{Assumption}


\newcommand{\cfnote}[1]{\textbf{\textcolor{red}{chennote:#1}}}
\newcommand{\yiannis}[1]{
    \sethlcolor{green}
    \hl{\textbf{YP:} \textit{#1}}
}



\title{SSR: An Efficient and Robust Framework for Learning with Unknown Label Noise}

\addauthor{Chen Feng}{https://sites.google.com/view/mr-chenfeng}{1}
\addauthor{Georgios Tzimiropoulos}{https://ytzimiro.github.io/}{1}
\addauthor{Ioannis Patras}{https://sites.google.com/view/ioannispatras}{1}

\addinstitution{
 School of Electronic Engineering and Computer Science\\
 Queen Mary University of London\\
 London, UK
}

\runninghead{Chen et al}{SSR: An Efficient and Robust Framework for LULN}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\begin{document}

\maketitle
\begin{abstract}
Despite the large progress in supervised learning with neural networks, there are significant challenges in obtaining high-quality, large-scale and accurately labelled datasets. In such a context, how to learn in the presence of noisy labels has received more and more attention. As a relatively complex problem, in order to achieve good results, current approaches often integrate components from several fields, such as supervised learning, semi-supervised learning, transfer learning and resulting in complicated methods. Furthermore, they often make multiple assumptions about the type of noise of the data. This affects the model robustness and limits its performance under different noise conditions. 
In this paper, we consider a novel problem setting, \textit{\textbf{L}earning with \textbf{U}nknown \textbf{L}abel \textbf{N}oise}~(\textbf{LULN}), that is, learning when both the degree and the type of noise are unknown. Under this setting, unlike previous methods that often introduce multiple assumptions and lead to complex solutions, we propose a simple, efficient and robust framework named \textit{\textbf{S}ample \textbf{S}election and \textbf{R}elabelling}~(\textbf{SSR}), that with a minimal number of hyperparameters achieves SOTA results in various conditions. At the heart of our method is a sample selection and relabelling mechanism based on a \textbf{n}on-\textbf{p}arametric \textbf{K}NN classifier~(NPK)  and a \textbf{p}arametric \textbf{m}odel \textbf{c}lassifier~(PMC) , respectively, to select the clean samples and gradually relabel the noisy samples.
Without bells and whistles, such as model co-training, self-supervised pre-training and semi-supervised learning, and with robustness concerning the settings of its few hyper-parameters, our method significantly surpasses previous methods on both CIFAR10/CIFAR100 with synthetic noise and real-world noisy datasets such as WebVision, Clothing1M and ANIMAL-10N. Code is available at \url{https://github.com/MrChenFeng/SSR_BMVC2022}.
\end{abstract}

\section{Introduction}
It is now commonly accepted that supervised learning with deep neural networks can provide excellent solutions for a wide range of problems, so long as there is sufficient availability of labelled training data and computational resources. However, these results have been mostly obtained using well-curated datasets in which the labels are of high quality. In the real world, it is often costly to obtain high-quality labels, especially for large-scale datasets. A common approach is to use semi-automatic methods to obtain the labels (e.g. ``webly-labelled'' images where the images and labels are obtained by web-crawling). While such methods can greatly reduce the time and cost of manual labelling, they also lead to low-quality noisy labels. 

In such settings, noise is one of the following two types: closed-set noise where the true labels belong to one of the given classes (Set B in~\cref{fig:noise_example}) and open-set noise where the true labels do not belong to the set of labels of the classification problem (Set C in~\cref{fig:noise_example}). To deal with different types of noise, two main types of methods have been proposed, which we name here as probability-consistent methods and probability-approximate methods.
\begin{wrapfigure}{r}{0.4\textwidth}
\begin{center}
    \includegraphics[width=0.4\textwidth]{figures/threesets.png}
  \end{center}
    \caption{Different ``tigers''.}
\label{fig:noise_example}
\end{wrapfigure}
Probability-consistent methods usually model noise patterns directly and propose corresponding probabilistic adjustment techniques, e.g., robust loss functions~\citep{mae, generalized_cross_entropy, symmetric_cross_entropy} and noise corrections based on noise transition matrix~\citep{noiseadaptation}. However, accurate modelling of noise patterns is non-trivial, and often cannot even model open-set noise. Also, due to the necessary simplifications of probabilistic modelling, such methods often perform poorly with heavy and complex noise. 
More recently, probability-approximate methods, that is methods that do not model the noise patterns explicitly become perhaps the dominant paradigm, especially ones that are based on sample selection. Earlier methods often reduce the influence of noise samples by selecting a clean subset and training only with it~\citep{coteaching, coteaching+, mentornet, whentohow}. Recent methods tend to further employ semi-supervised learning methods, such as MixMatch~\citep{mixmatch}, to fully explore the entire dataset by treating the selected clean subset as labelled samples and the non-selected subset as unlabeled samples~\citep{dividemix, moit}. These methods, generally, do not consider the presence of open-set noise in the dataset, since most current semi-supervised learning methods can not deal with open-set noise appropriately. To address this, several methods~\citep{evidentialmix, ngc} extend the sample selection idea by further identifying the open-set noise and excluding it from the semi-supervised training. 

In general, the above methods make assumptions about the pattern of the noise, such as the confidence penalty specifically for asymmetric noise in DivideMix~\citep{dividemix}. However, these mechanisms are often detrimental when the noise pattern does not meet the assumptions -- for example, explicitly filtering open-set noise in the absence of open-set noise may result in clean hard samples being removed. Furthermore, due to the complexity of combining multiple modules, the above methods usually need to adjust complex hyperparameters according to the type and degree of noise. 




In this paper, we consider a novel problem setting --- \textit{\textbf{L}earning with \textbf{U}nknown \textbf{L}abel \textbf{N}oise}~(\textbf{LULN}), that is, learning when both the degree and the type of noise are unknown.
Striving for simplicity and robustness, we propose a simple method for \textbf{LUNL}, namely \textit{\textbf{S}ample \textbf{S}election and \textbf{R}elabelling}~(\textbf{SSR})~(\cref{3_3}), with two components that are clearly decoupled: a selection mechanism that identifies clean samples with correct labels, and a relabelling mechanism that aims to recover correct labels of wrongly labelled noisy samples. These two major components are based on the two simple and necessary assumptions for \textbf{LULN}, namely, that samples with highly-consistent annotations with their neighbours are often clean, and that very confident model predictions are often trustworthy. Once a well-labelled subset is constructed this way we use the most basic supervised training scheme with a cross-entropy loss. Optionally, a feature consistency loss can be used for all data so as to deal better with open-set noise. 

Without bells and whistles, such as semi-supervised learning, self-supervised model pre-training and model co-training, our method is shown to be robust to the values of its very few hyperparameters through extensive experiments and ablation studies and to consistently outperform the state-of-the-art by a large margin in various datasets.


\section{Related Works}
This paper mainly focuses on the probability-approximate methods, especially methods based on sample selection. For a detailed introduction to probability-consistent methods described above, please refer to the review papers~\citep{review1, review2}. We note that we do not consider utilizing an extra clean validation dataset, such as meta-learning-based methods~\citep{mwnet, famus} do. 


\paragraph{Clean sample selection} Most sample selection methods fall into two main categories: 
\begin{itemize}[itemsep=0em]
\item \textit{Prediction-based methods.} Most of the recent sample selection methods do so, by relying on the predictions of the model classifier, for example on the per-sample loss~\citep{lossmodellingbmm, dividemix} or model prediction~\citep{selfie, whentohow}. However, the prediction-based selection is often unstable and easily leads to confirmation bias, especially in heavy noise scenarios. A few works focus on improving the sample selection quality of these methods~\citep{xia2021sample, zhou2020robust}. To identify open-set noise, several methods utilize the Shannon entropy of the model predictions of different samples~\citep{evidentialmix, albert2022addressing}. Open-set noise samples that do not belong to any class should have a relatively average model prediction (larger entropy value).

\item \textit{Feature-based methods.} Instead of selecting samples based on the model prediction, some works try to utilize the feature representations for sample selection. \citet{topofilter,ngc} try to build a KNN graph and identify clean samples through connected sub-graphs. \citet{deep-knn} selects clean samples with a KNN classifier in the prediction logit space, while \citet{moit} proposes an iterative KNN to alleviate the effect of noisy labels.
\end{itemize}



Our work falls in the second category. However, unlike existing methods that use complex variants of neighbouring algorithms, in our pursuit of simplicity and robustness, we use the simplest KNN classification and show that this is sufficient. 

\paragraph{Fully exploiting the whole dataset} To fully utilise the whole dataset during training and more specifically the non-selected subset, recent methods usually apply semi-supervised training methods (e.g., MixMatch~\citep{mixmatch}), by considering the selected subset as labelled and the non-selected subset as unlabeled~\citep{dividemix}. However, most current semi-supervised learning methods can not deal with open-set samples properly. How to properly do semi-supervised learning in this setting is often referred to as open-set semi-supervised learning~\citep{MTC, openmatch}. In this paper, instead of adopting complex semi-supervised learning schemes, we adopt a simple relabeling and selection scheme in order to construct a clean and well-labelled subset and then train with a simple cross-entropy loss on the clean, well-labelled set and optionally, with a feature consistency loss on the whole dataset that possibly contains open-set noise and samples that cannot be well relabelled.




\section{Methodology}
\label{3}
\subsection{Problem formulation}
\label{3_1}
Let us denote with , a training set with the corresponding one-hot vector labels , where  is the number of classes and  is the number of samples.
For convenience, let us also denote the label of each sample  corresponding to the one-hot label vector  as . 
Finally, let us denote the true labels with . Clearly, for an open-set noisy label it is the case that , while for closed-set noisy samples .

We view the classification network as an encoder  that extracts a feature representation and a \textbf{p}arametric \textbf{m}odel \textbf{c}lassifier~(PMC)  that deals with the classification problem in question. We also define a \textbf{n}on-\textbf{p}arametric \textbf{K}NN classifier~(NPK)  based on the feature representations from encoder .
For brevity, we define  as the feature representation of sample , and  and  as the prediction vectors from PMC  and NPK , respectively. Following recent works~\citep{dividemix, ngc, moit, coteaching, coteaching+}, we adopt an iterative scheme for our method consisting of two stages: 1.~sample selection~(\cref{alg:sel}) and relabelling~(\cref{alg:re}), and 2.~model training~(\cref{alg:train}) in \textbf{Algorithm 1}. 




\begin{figure}[t]
    \begin{center}
    \includegraphics[width=1.0\textwidth]{figures/method_new.png}
    \end{center}
\caption{A toy example of \textbf{SSR}~(\cref{3_3}) with a noisy animal dataset.}
    \label{fig:ssr}
\end{figure}

\begin{algorithm}[H]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\Input{dataset , sample selection threshold , sample relabelling threshold , weight of feature consistency loss , max epochs }
 \While{}{
    Generate  with \cref{eqr} \tcc*[r]{sample relabelling} \label{alg:re}
    Generate  with \cref{eq3} and \cref{eq4} \tcc*[r]{sample selection} \label{alg:sel}
    Model training with \cref{eq10} \tcc*[r]{model training} \label{alg:train}
}
\caption{\textbf{SSR}. }
\label{alg}
\end{algorithm}


\subsection{Sample selection and relabelling}
\label{3_3}
For a better exposition, we first introduce our sample selection mechanism. Please note, that we actually relabel the samples before each sample selection. 

\paragraph{Clean sample selection by balanced neighbouring voting}

\label{3_3_1}
Our sample selection is based on the consistency, as quantified by a measure , between the label ~\footnote{Please note, we use the labels ~(\cref{eqr}) that a relabelling mechanism provides as mentioned above.} of sample  and an (adjusted) distribution, , of the labels in its neighbourhood in the feature space. More specifically, let us denote the similarity between the representations  and  of any two samples  and  by . By default, we used the cosine similarity, that is, .
Let us also denote by  the index set of the  nearest neighbours of sample  in  based on the calculated similarity. Then, for each sample , we can calculate the normalised label distribution  in its neighbourhood, and a balanced version, , of it that takes into consideration/compensates for the distribution  of the labels in the dataset. More specifically,

where we denote with  the vector whose entries are the inverses of the entries of the vector  --- in this way we alleviate the negative impact of possible class imbalances in sample selection.



The vector  can be considered as the (soft) prediction of the NPK  classifier. We then, define a consistency measure  between the sample's label  (\cref{3_1}) and the prediction  of the NPK as 
that is the ratio of the value of the distribution  at the label ~(\cref{eqr}) divided by the value of its highest peak . 
Roughly speaking, a high consistency measure  at a sample  means that its neighbours agree with its current label  --- this indicates that  is likely to be correct. By setting a threshold  to , a clean subset  can be extracted. In our method, we set  by default, that is, we consider a sample  to be clean only when its neighbours' voting  is consistent with its current label .



\paragraph{Noisy sample relabelling by classifier thresholding}
\label{3_3_2}



Our sample relabelling scheme aims at adding well-labelled samples to the training pool and is based on the PMC classifier . Specifically, we "relabel" all samples for which the classifier is confident, that is all samples  for which the prediction  of the classifier PMC  exceeds a threshold . Formally,


Please note, that similarly to~\cref{3_1}, we denote the one-hot label corresponding to  as  --- this will be used in~\cref{eq3}. 
By setting a high , a highly confident sample  will be relabelled --- this can in turn further enhance the quality of sample selection. Note, that this scheme typically avoids mis-relabelling open-set noise samples as those tend not to have highly confident predictions. In this way, our method can deal with open-set noise datasets effectively even though we do not explicitly propose a mechanism for them.


\paragraph{On choices of sample selection and relabelling}
\label{3_3_3}
In this work, we utilize two different classifiers for the two stages of our scheme: for sample selection the NPK  based on nearest neighbours in the feature space  ((\cref{eq3} and \cref{eq4}) and for sample relabelling the PMC  classifier (\cref{eqr}). Here, we justify/comment on this choice.


\textit{Why not PMC  for sample selection?}
\quad Most previous works rely on the PMC  itself to select clean samples, i.e., typically, samples with small losses. However, it is well known that such methods are not robust to complex and heavy noise. Also, these methods often require a warmup stage before sample selection --- this requires prior knowledge about the difficulty and the noise ratio of the dataset. For example, a warmup stage of 50 epochs under heavy noise on the CIFAR10 dataset may result in overfitting before sample selection, while a warmup stage of 5 epochs on the mildly noisy CIFAR100 dataset may not be enough. In this paper, we rely on the smoothness of feature representations and use the NPK  for sample selection -- even a randomly initialized encoder can provide quite meaningful neighbouring relations therefore enabling us to train the model from scratch and also improves the sample selection stability and performance in heavy and complex noisy scenarios. For a more detailed discussion, please refer to \texttt{Supplementary C}.


\textit{Why not NPK  for sample relabelling?}
\quad Due to the existence of noisy labels, we found that it is very difficult to make a proper choice of  and rely on the NPK  for sample relabeling, especially in the early iterations. By contrast, in our scheme, PMC  is always trained with a relatively clean subset and can perform sample relabeling more accurately. Furthermore, relabeling samples on which the classifier is confident leads to smaller gradients and smoother learning from easier samples first --- even when the newly assigned labels are wrong, the influence is smaller.





\subsection{Model training}
\label{3_4}
In the training stage, we use the most basic form of supervised learning, i.e., using the cross-entropy loss on the clean subset selected in the first stage --- this updates both the encoder  and the PMC . With our sample relabelling mechanism, the size of the clean subset grows progressively by including more and more relabeled closed-set noise in training. Optionally, we use a feature consistency loss that enforces consistency between the feature representations of different augmentations of the same sample --- this updates the encoder  and helps to learn a strong feature space on which the selection mechanism of the first stage can rely. 

\paragraph{Supervised training using the clean subset}
For each sample  in the selected subset , we train the encoder  and PMC  with common cross-entropy loss, that is, .
Moreover, to deal with the possible class imbalance in the selected subset, we simply over-sample minority classes. In the ablations study, we report the effect of balancing -- the over-sampling and also the balanced sample selection in~\cref{eq3}.
\paragraph{Optional: feature consistency regularization using all samples} 
Although our relabeling method can progressively relabel and introduce closed-set noise samples into training, open-set samples can also improve generalization. Motivated by commonly used prediction consistency regularization methods, we propose a feature consistency loss ~\citep{simsiam}. Specifically, with a projector  and predictor , we minimize the cosine distance\footnote{In \texttt{Supplementary D} we investigate the use of the L2 distance.} between two different augmented views ( and ) of the same sample . That is,

where  and .In summary, the overall training objective is to minimize a weighted sum of  and , that is 

We set . For brevity, we name our method as SSR when , and SSR+ when .



\section{Experiments}
\label{4}

\subsection{Overview}
\label{4_1}
In this section, we conduct extensive experiments on two standard benchmarks with synthetic label noise, CIFAR-10 and CIFAR-100, and three real-world datasets, Clothing1M~\citep{clothing1mdataset}, WebVision~\citep{webvision}, and ANIMAL-10N~\citep{selfie}. For brevity, we define abbreviated names for the corresponding noise settings, such as "" for 50\% symmetric noise, "" for 40\% asymmetric noise and "" for 30\% total noise with 50\% open-set noise.~(more dataset and implementation details can be found in \texttt{Supplementary A and B}). In~\cref{4-2}, we conduct extensive ablation experiments to show the great performance and robustness of our sample selection and relabelling mechanism w.r.t its hyperparameters with different noise types, noise ratios and dataset. In~\cref{4-3} and~\cref{4-4}, we compare our method with the state-of-the-art in synthetic noisy datasets and real-world noisy datasets. 

\subsection{Ablations study}
\label{4-2}

\paragraph{Quality of sample selection and relabelling} In~\cref{fig:selectionandrelabeling}, we investigate the quality of sample selection and relabeling under different noise types and ratios on the CIFAR10 noisy dataset. We set  for , ,  and  noise, while  for  and  noise -- please refer to \texttt{Supplementary A} for more details on noise. We set  in all experiments.


\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=1.0\textwidth]{figures/total.png}
\end{center}
\caption{Effect of our sample selection and relabelling method with various noise settings. (a) The proportion of relabeled samples; (c) The corrected clean samples ratio within the relabeled part; (c)F-score of sample selection.}
\label{fig:selectionandrelabeling}
\end{figure}



To summarise, we found that the number of the relabeled samples is highly related to the value of  across different noise ratios (\cref{fig:selectionandrelabeling}(a)) for closed-set noise only dataset, for e.g, a lower  leads to more relabeled samples across different noise settings~(, ,  and ). For datasets containing also open-set noise~( and ), our relabeling mechanism is more conservative~(nearly no relabelling), thus alleviating the negative impact of open-set noise. In~\cref{fig:selectionandrelabeling}(b), we show that we obtain very high relabelling accuracy with different noise and relabelling volumes, e.g., only  samples have correct labels originally for  symmetric noise while >95\% samples are correctly relabelled. In~\cref{fig:selectionandrelabeling}(c), we report the F-score of our sample selection. Please note that our sample selection is more challenging compared to previous sample selection methods. While previous methods usually focus on identifying clean subsets and noisy subsets based on the original labels, our method involves the relabeling of samples, which takes the risk of introducing more errors while increasing the number of clean subsets. Even so, we see that the F-score of our sample selection works well~(> 0.95 in most cases).

\paragraph{Robustness to hyper-parameters} In this section, we conduct extensive ablation studies to show the robustness of the values of the few hyperparameters with different noise types, noise ratios, and datasets. The choice of  controls the sample relabelling quality and proportion. Roughly speaking, the lower the , the more samples will be relabelled. Similarly, the choice of  controls the sample selection quality and proportion -- the lower the , the more samples will be selected for the training stage. We set  for  ablations and  for  ablations. We also investigate the effects of  --- the size of neighborhood of NPK  during the sample selection stage, with  and .
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/hyperparameters.png}
\end{center}
\caption{Classification accuracy with different hyper-parameters on CIFAR10 datasets. (a); (b); (c) .}
\label{fig:hyperparameters}
\end{figure}


In~\cref{fig:hyperparameters}(a) we report results with . Removing sample selection () leads to severe degradation especially for a high noise ratio (90\% symmetric noise), while a relatively high  gives consistently high performance. In~\cref{fig:hyperparameters}(b), we report the performance with different  on the synthetic CIFAR10 noisy dataset. Our method achieves consistently superior performance than the state-of-the-art with different . In~\cref{fig:hyperparameters}(c), we report results with different  for the CIFAR10 dataset with 40\% asymmetric noise since it is more challenging and realistic. Except for extremely small , our method is stable and consistently better than the state-of-the-art.

\paragraph{Effect of balancing strategies}
\label{effect}
To alleviate the possible class imbalance in the dataset, we proposed two balancing strategies, one in the sample selection~[\cref{eq3}] and one in the model training stage~[Over-sampling minority class], respectively. In~\cref{Balancing_trciks} we investigate the effect of using data balancing or not, on CIFAR10 with 40\% asymmetric noise and also on a well-known real-world imbalanced noisy dataset, Clothing1M. It can be seen that the effect is small but positive.

\begin{table}[htbp]
\begin{center}
\resizebox{0.45\textwidth}{!}{\begin{tabular}[t]{@{}lcc@{}}
\toprule
Method           & Clothing1M                      & 40\% asym CIFAR10               \\ \midrule
SSR               & \textbf{74.83} & \textbf{95.5} \\
w/o balancing & 74.12                           & 94.9                           \\ \bottomrule
\end{tabular}
}
\end{center}
\caption{Effect of class balancing.}
\label{Balancing_trciks}
\end{table}


\subsection{Evaluation with synthetic noisy datasets}
\label{4-3}
In this section, we compare our method to the most recent state-of-the-art methods and we show that it achieves consistent improvements in all datasets and at all noise types and ratios.

\begin{table*}[htbp]
\begin{center}
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{@{}l|ccccc|cccc@{}}
\toprule
Dataset                                & \multicolumn{5}{c|}{CIFAR10}                                                                                                                                      & \multicolumn{4}{c}{CIFAR100}                                                                                                 \\ \midrule
Noise type                             & \multicolumn{4}{c|}{Symmetric}                                                                                                                    & Assymetric    & \multicolumn{4}{c}{Symmetric}                                                                                                \\ \midrule
Noise ratio                            & \multicolumn{1}{c|}{20\%}          & \multicolumn{1}{c|}{50\%}          & \multicolumn{1}{c|}{80\%}          & \multicolumn{1}{c|}{90\%}          & 40\%          & \multicolumn{1}{c|}{20\%}          & \multicolumn{1}{c|}{50\%}          & \multicolumn{1}{c|}{80\%}          & 90\%          \\ \midrule
Cross-Entropy                          & \multicolumn{1}{c|}{86.8}          & \multicolumn{1}{c|}{79.4}          & \multicolumn{1}{c|}{62.9}          & \multicolumn{1}{c|}{42.7}          & 85.0          & \multicolumn{1}{c|}{62.0}          & \multicolumn{1}{c|}{46.7}          & \multicolumn{1}{c|}{19.9}          & 10.1          \\
Co-teaching+~\citep{coteaching+}       & \multicolumn{1}{c|}{89.5}          & \multicolumn{1}{c|}{85.7}          & \multicolumn{1}{c|}{67.4}          & \multicolumn{1}{c|}{47.9}          & -             & \multicolumn{1}{c|}{65.6}          & \multicolumn{1}{c|}{51.8}          & \multicolumn{1}{c|}{27.9}          & 13.7          \\
F-correction~\citep{loss_correction}   & \multicolumn{1}{c|}{86.8}          & \multicolumn{1}{c|}{79.8}          & \multicolumn{1}{c|}{63.3}          & \multicolumn{1}{c|}{42.9}          & 87.2          & \multicolumn{1}{c|}{61.5}          & \multicolumn{1}{c|}{46.6}          & \multicolumn{1}{c|}{19.9}          & 10.2          \\
PENCIL~\citep{pencil}                  & \multicolumn{1}{c|}{92.4}          & \multicolumn{1}{c|}{89.1}          & \multicolumn{1}{c|}{77.5}          & \multicolumn{1}{c|}{58.9}          & 88.5          & \multicolumn{1}{c|}{69.4}          & \multicolumn{1}{c|}{57.5}          & \multicolumn{1}{c|}{31.1}          & 15.3          \\
LossModelling~\citep{lossmodellingbmm} & \multicolumn{1}{c|}{94.0}          & \multicolumn{1}{c|}{92.0}          & \multicolumn{1}{c|}{86.8}          & \multicolumn{1}{c|}{69.1}          & 87.4          & \multicolumn{1}{c|}{73.9}          & \multicolumn{1}{c|}{66.1}          & \multicolumn{1}{c|}{48.2}          & 24.3          \\
DivideMix*~\citep{dividemix}            & \multicolumn{1}{c|}{96.1}          & \multicolumn{1}{c|}{94.6}          & \multicolumn{1}{c|}{93.2}          & \multicolumn{1}{c|}{76.0}          & 93.4          & \multicolumn{1}{c|}{77.3}          & \multicolumn{1}{c|}{74.6}          & \multicolumn{1}{c|}{60.2}          & 31.5          \\
ELR+*~\citep{elr}                        & \multicolumn{1}{c|}{95.8}          & \multicolumn{1}{c|}{94.8}          & \multicolumn{1}{c|}{93.3}          & \multicolumn{1}{c|}{78.7}          & 93.0          & \multicolumn{1}{c|}{77.6}          & \multicolumn{1}{c|}{73.6}          & \multicolumn{1}{c|}{60.8}          & 33.4          \\
RRL~\citep{rrl}                        & \multicolumn{1}{c|}{95.8}          & \multicolumn{1}{c|}{94.3}          & \multicolumn{1}{c|}{92.4}          & \multicolumn{1}{c|}{75.0}          & 91.9          & \multicolumn{1}{c|}{79.1}          & \multicolumn{1}{c|}{74.8}          & \multicolumn{1}{c|}{57.7}          & 29.3          \\
NGC~\citep{ngc}                        & \multicolumn{1}{c|}{95.9}          & \multicolumn{1}{c|}{94.5}          & \multicolumn{1}{c|}{91.6}          & \multicolumn{1}{c|}{80.5}          & 90.6          & \multicolumn{1}{c|}{79.3}          & \multicolumn{1}{c|}{75.9}          & \multicolumn{1}{c|}{62.7}          & 29.8          \\
AugDesc*~\citep{augdesc}                & \multicolumn{1}{c|}{96.3}          & \multicolumn{1}{c|}{95.4}          & \multicolumn{1}{c|}{93.8}          & \multicolumn{1}{c|}{91.9}          & 94.6          & \multicolumn{1}{c|}{79.5}          & \multicolumn{1}{c|}{77.2}          & \multicolumn{1}{c|}{66.4}          & 41.2          \\
C2D*~\citep{c2d}                        & \multicolumn{1}{c|}{96.4}          & \multicolumn{1}{c|}{95.3}          & \multicolumn{1}{c|}{94.4}          & \multicolumn{1}{c|}{93.6}          & 93.5          & \multicolumn{1}{c|}{78.7}          & \multicolumn{1}{c|}{76.4}          & \multicolumn{1}{c|}{67.8}          & 58.7          \\ \midrule
SSR(ours)                                    & \multicolumn{1}{c|}{96.3}          & \multicolumn{1}{c|}{95.7}          & \multicolumn{1}{c|}{95.2}          & \multicolumn{1}{c|}{94.6}          & 95.1          & \multicolumn{1}{c|}{79.0}              & \multicolumn{1}{c|}{75.9}              & \multicolumn{1}{c|}{69.5}              &  61.8             \\
SSR+(ours)                                   & \multicolumn{1}{c|}{\textbf{96.7}} & \multicolumn{1}{c|}{\textbf{96.1}} & \multicolumn{1}{c|}{\textbf{95.6}} & \multicolumn{1}{c|}{\textbf{95.2}} & \textbf{95.5} & \multicolumn{1}{c|}{\textbf{79.7}} & \multicolumn{1}{c|}{\textbf{77.2}} & \multicolumn{1}{c|}{\textbf{71.9}} & \textbf{66.6} \\ \bottomrule
\end{tabular}
}
\end{center}
\caption{Evaluation on CIFAR-10 and CIFAR-100 with closed-set noise. Methods marked with an asterisk employ semi-supervised learning, model co-training or model pre-training.}
\label{close}
\end{table*}

\paragraph{Evaluation with controlled closed-set noise}
In this section, we compare SSR/SSR+ to the most competitive recent works. Table~\ref{close} shows results on CIFAR10 and CIFAR100 --- we note again for SSR/SSR+ this is without the use of model cotraining or pre-training. It is clear that our method far outperforms them~(e.g. 66.6\% accuracy on CIFAR100 with 90\% symmetric noise), not only in the case of symmetric noise but also in the more realistic asymmetric synthetic noise settings.

\begin{table*}[t]
\begin{center}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{@{}cccc|cccc|c@{}}
\toprule
CE    & F-correction \citep{loss_correction} & ELR \citep{elr} & RRL \citep{rrl} & C2D* \citep{c2d} & DivideMix* \citep{dividemix} & ELR+* \citep{elr} & AugDesc* \citep{augdesc} & SSR+(ours)     \\ \midrule
69.21 & 69.84                                & 72.87           & 74.30           & 74.84            & 74.76                        & 74.81             & \textbf{75.11}           & \textbf{74.83} \\ \bottomrule
\end{tabular}
}
\end{center}
\caption{Testing accuracy~(\%) on Clothing1M (methods with * utilized model cotraining).}
\label{clothing1m}
\end{table*}

\paragraph{Evaluation with combined open-set noise and closed-set noise}
Table~\ref{openset} shows the performance of our method in a more complex combined noise scenario. Previous methods that are specially designed for open-set noise degrade rapidly when the open-set noise ratio is decreased from 1 to 0.5~\citep{rog,iterativeopenset}. Also, the performance of the method without considering open-set noise like DivideMix~\citep{dividemix} decreases when the open-set noise ratio is increased. EDM~\citep{evidentialmix} modifies the method of DivideMix to deal with combined noise, however, reports results that are considerably lower than ours. 

\begin{table}[htbp]
\begin{minipage}{0.5\linewidth}
\begin{center}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{@{}l|c|cc|cc@{}}
\toprule
\multirow{2}{*}{Method}    & Noise ratio & \multicolumn{2}{c|}{0.3}                           & \multicolumn{2}{c}{0.6}                            \\ \cmidrule(l){2-6} 
                           & Open ratio  & \multicolumn{1}{c|}{0.5}           & 1             & \multicolumn{1}{c|}{0.5}           & 1             \\ \midrule
\multirow{2}{*}{ILON~\citep{iterativeopenset}}      & Best        & \multicolumn{1}{c|}{87.4}          & 90.4          & \multicolumn{1}{c|}{80.5}          & 83.4          \\
                           & Last        & \multicolumn{1}{c|}{80.0}          & 87.4          & \multicolumn{1}{c|}{55.2}          & 78.0          \\ \midrule
\multirow{2}{*}{RoG~\citep{rog}}       & Best        & \multicolumn{1}{c|}{89.8}          & 91.4          & \multicolumn{1}{c|}{84.1}          & 88.2          \\
                           & Last        & \multicolumn{1}{c|}{85.9}          & 89.8          & \multicolumn{1}{c|}{66.3}          & 82.1          \\ \midrule
\multirow{2}{*}{DivideMix~\citep{dividemix}} & Best        & \multicolumn{1}{c|}{91.5}          & 89.3          & \multicolumn{1}{c|}{91.8}          & 89.0          \\
                           & Last        & \multicolumn{1}{c|}{90.9}          & 88.7          & \multicolumn{1}{c|}{91.5}          & 88.7          \\ \midrule
\multirow{2}{*}{EDM~\citep{evidentialmix}}       & Best        & \multicolumn{1}{c|}{94.5}          & 92.9          & \multicolumn{1}{c|}{93.4}          & 90.6          \\
                           & Last        & \multicolumn{1}{c|}{94.0}          & 91.9          & \multicolumn{1}{c|}{92.8}          & 89.4          \\ \midrule
\multirow{2}{*}{SSR(ours)}       & Best        & \multicolumn{1}{c|}{96.0}              &  95.7             & \multicolumn{1}{c|}{93.8}              &  93.1             \\
                           & Last        & \multicolumn{1}{c|}{95.9}              & 95.6              & \multicolumn{1}{c|}{93.7}              &  93.1          \\ \midrule
\multirow{2}{*}{SSR+(ours)}      & Best        & \multicolumn{1}{c|}{\textbf{96.3}} & \textbf{96.1} & \multicolumn{1}{c|}{\textbf{95.2}} & \textbf{94.0} \\
                           & Last        & \multicolumn{1}{c|}{\textbf{96.2}} & \textbf{96.0} & \multicolumn{1}{c|}{\textbf{95.2}} & \textbf{93.9} \\ \bottomrule
\end{tabular}}
\end{center}
\caption{Evaluation on CIFAR10 with combined noise.}
\label{openset}

\end{minipage}
\hfill
\begin{minipage}{0.5\linewidth}
\begin{center}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{@{}l|cc|cc@{}}
\toprule
\multirow{2}{*}{Methods}    & \multicolumn{2}{c}{WebVision}   & \multicolumn{2}{c}{ILSVRC2012}  \\ \cmidrule(l){2-5} 
                            & Top1           & Top5           & Top1           & Top5           \\ \midrule
Co-teaching~\citep{coteaching}                 & 63.58          & 85.20          & 61.48          & 84.70          \\
DivideMix~\citep{dividemix} & 77.32          & 91.64          & 75.20          & 90.84          \\
ELR+~\citep{elr}            & 77.78          & 91.68          & 70.29          & 89.76          \\
NGC~\citep{ngc}             & 79.16          & 91.84          & 74.44          & 91.04          \\
LongReMix~\citep{cordeiro2021longremix}                   & 78.92          & 92.32          & -              & -              \\
RRL~\citep{rrl}             & 76.3           & 91.5           & 73.3           & 91.2           \\
SSR+(ours)                  & \textbf{80.92} & \textbf{92.80} & \textbf{75.76} & \textbf{91.76} \\ \bottomrule
\end{tabular}}
\end{center}
\caption{Testing accuracy~(\%) on Webvision.}
\label{webvision}

\begin{center}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{@{}ccccc@{}}
\toprule
Cross-Entropy  & \begin{tabular}[c]{@{}c@{}}SELFIE  \\ \citep{selfie}\end{tabular} & \begin{tabular}[c]{@{}c@{}}PLC \\ \citep{PLC}\end{tabular} & \begin{tabular}[c]{@{}c@{}}NCT  \\ \citep{nestedcoteaching}\end{tabular} & SSR+(ours)              \\ \midrule
79.4 & 81.8                                                   & 83.4                                            & 84.1                                                           & \textbf{88.5} \\ \bottomrule
\end{tabular}}
\end{center}
\caption{Testing accuracy on ANIMAL-10N. }
\label{animal10n}
\end{minipage}
\end{table}


\subsection{Evaluation with real-world noisy datasets}
\label{4-4}
Finally, in~\cref{clothing1m},~\cref{webvision} and~\cref{animal10n} we show results on the Clothing1M, WebVision and ANIMAL-10N datasets, respectively. To summarize, our method achieves better or competitive performance in relation to the current state-of-the-art in both large-scale web-crawled datasets and small-scale human-annotated noisy datasets.




\section{Conclusions}
\label{5}

In this paper we propose an efficient \textit{\textbf{S}ample \textbf{S}election and \textbf{R}elabelling}~(SSR) framework for \textit{\textbf{L}earning with \textbf{U}nknown \textbf{L}abel \textbf{N}oise}~(\textbf{LULN}). Unlike previous methods that try to integrate many different mechanisms and regularizations, we strive for a concise, simple and robust method. The proposed method does not utilize complicated mechanisms such as semi-supervised learning, model co-training and model pre-training, and is shown with extensive experiments and ablation studies to be robust to the values of its few hyper-parameters, and to consistently and by large surpass the state-of-the-art in various datasets. 

\vspace{5pt}
{\setlength{\parindent}{0cm}
\textbf{Acknowledgments:} This work was supported by the EU H2020 AI4Media No. 951911 project.
}

\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Albert et~al.(2022)Albert, Ortego, Arazo, O'Connor, and
  McGuinness]{albert2022addressing}
Paul Albert, Diego Ortego, Eric Arazo, Noel~E O'Connor, and Kevin McGuinness.
\newblock Addressing out-of-distribution label noise in webly-labelled data.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pages 392--401, 2022.

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O’Connor, and
  McGuinness]{lossmodellingbmm}
Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In \emph{International Conference on Machine Learning}, pages
  312--321. PMLR, 2019.

\bibitem[Bahri et~al.(2020)Bahri, Jiang, and Gupta]{deep-knn}
Dara Bahri, Heinrich Jiang, and Maya Gupta.
\newblock Deep k-nn for noisy labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  540--550. PMLR, 2020.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Chen and He(2021)]{simsiam}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15750--15758, 2021.

\bibitem[Chen et~al.(2021)Chen, Shen, Hu, and Suykens]{nestedcoteaching}
Yingyi Chen, Xi~Shen, Shell~Xu Hu, and Johan~AK Suykens.
\newblock Boosting co-teaching with compression regularization for label noise.
\newblock \emph{arXiv preprint arXiv:2104.13766}, 2021.

\bibitem[Cordeiro et~al.(2021)Cordeiro, Sachdeva, Belagiannis, Reid, and
  Carneiro]{cordeiro2021longremix}
Filipe~R Cordeiro, Ragav Sachdeva, Vasileios Belagiannis, Ian Reid, and Gustavo
  Carneiro.
\newblock Longremix: Robust learning with high confidence samples in a noisy
  label environment.
\newblock \emph{arXiv preprint arXiv:2103.04173}, 2021.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and Le]{autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 113--123, 2019.

\bibitem[Feng and Patras(2022)]{feng2022adaptive}
Chen Feng and Ioannis Patras.
\newblock Adaptive soft contrastive learning.
\newblock \emph{arXiv preprint arXiv:2207.11163}, 2022.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{mae}
Aritra Ghosh, Himanshu Kumar, and PS~Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Goldberger and Ben-Reuven(2016)]{noiseadaptation}
Jacob Goldberger and Ehud Ben-Reuven.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock 2016.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{coteaching}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock \emph{arXiv preprint arXiv:1804.06872}, 2018.

\bibitem[Han et~al.(2020)Han, Yao, Liu, Niu, Tsang, Kwok, and
  Sugiyama]{review2}
Bo~Han, Quanming Yao, Tongliang Liu, Gang Niu, Ivor~W Tsang, James~T Kwok, and
  Masashi Sugiyama.
\newblock A survey of label-noise representation learning: Past, present and
  future.
\newblock \emph{arXiv preprint arXiv:2011.04406}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{preactresnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9729--9738, 2020.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and Fei-Fei]{mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  2304--2313. PMLR, 2018.

\bibitem[Lee et~al.(2019)Lee, Yun, Lee, Lee, Li, and Shin]{rog}
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo~Li, and Jinwoo Shin.
\newblock Robust inference via generative classifiers for handling noisy
  labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  3763--3772. PMLR, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Socher, and Hoi]{dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2002.07394}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Xiong, and Hoi]{rrl}
Junnan Li, Caiming Xiong, and Steven Hoi.
\newblock Learning from noisy data with robust representation learning.
\newblock 2020{\natexlab{b}}.

\bibitem[Li et~al.(2017)Li, Wang, Li, Agustsson, and Van~Gool]{webvision}
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van~Gool.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock \emph{arXiv preprint arXiv:1708.02862}, 2017.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and Fernandez-Granda]{elr}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{arXiv preprint arXiv:2007.00151}, 2020.

\bibitem[Malach and Shalev-Shwartz(2017)]{whentohow}
Eran Malach and Shai Shalev-Shwartz.
\newblock Decoupling" when to update" from" how to update".
\newblock \emph{arXiv preprint arXiv:1706.02613}, 2017.

\bibitem[Nishi et~al.(2021)Nishi, Ding, Rich, and Hollerer]{augdesc}
Kento Nishi, Yi~Ding, Alex Rich, and Tobias Hollerer.
\newblock Augmentation strategies for learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 8022--8031, 2021.

\bibitem[Ortego et~al.(2021)Ortego, Arazo, Albert, O'Connor, and
  McGuinness]{moit}
Diego Ortego, Eric Arazo, Paul Albert, Noel~E O'Connor, and Kevin McGuinness.
\newblock Multi-objective interpolation training for robustness to label noise.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 6606--6615, 2021.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{loss_correction}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1944--1952, 2017.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{mwnet}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{International conference on machine learning}, pages
  4334--4343. PMLR, 2018.

\bibitem[Sachdeva et~al.(2021)Sachdeva, Cordeiro, Belagiannis, Reid, and
  Carneiro]{evidentialmix}
Ragav Sachdeva, Filipe~R Cordeiro, Vasileios Belagiannis, Ian Reid, and Gustavo
  Carneiro.
\newblock Evidentialmix: Learning with combined open-set and closed-set noisy
  labels.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pages 3607--3615, 2021.

\bibitem[Saito et~al.(2021)Saito, Kim, and Saenko]{openmatch}
Kuniaki Saito, Donghyun Kim, and Kate Saenko.
\newblock Openmatch: Open-set consistency regularization for semi-supervised
  learning with outliers.
\newblock \emph{arXiv preprint arXiv:2105.14148}, 2021.

\bibitem[Simonyan and Zisserman(2014)]{vgg19}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Song et~al.(2019)Song, Kim, and Lee]{selfie}
Hwanjun Song, Minseok Kim, and Jae-Gil Lee.
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5907--5915. PMLR, 2019.

\bibitem[Song et~al.(2022)Song, Kim, Park, Shin, and Lee]{review1}
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee.
\newblock Learning from noisy labels with deep neural networks: A survey.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem[Wang et~al.(2018)Wang, Liu, Ma, Bailey, Zha, Song, and
  Xia]{iterativeopenset}
Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le~Song, and
  Shu-Tao Xia.
\newblock Iterative learning with open-set noisy labels.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8688--8696, 2018.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and
  Bailey]{symmetric_cross_entropy}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 322--330, 2019.

\bibitem[Wu et~al.(2020)Wu, Zheng, Goswami, Metaxas, and Chen]{topofilter}
Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen.
\newblock A topological filter for learning with label noise.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21382--21393, 2020.

\bibitem[Wu et~al.(2021)Wu, Wei, Jiang, Mao, Tang, and Li]{ngc}
Zhi-Fan Wu, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, and Yu-Feng
  Li.
\newblock Ngc: A unified framework for learning with open-world noisy data.
\newblock \emph{arXiv preprint arXiv:2108.11035}, 2021.

\bibitem[Xia et~al.(2021)Xia, Liu, Han, Gong, Yu, Niu, and
  Sugiyama]{xia2021sample}
Xiaobo Xia, Tongliang Liu, Bo~Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi
  Sugiyama.
\newblock Sample selection with uncertainty of losses for learning with noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2106.00445}, 2021.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{clothing1mdataset}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2691--2699, 2015.

\bibitem[Xu et~al.(2021)Xu, Zhu, Jiang, and Yang]{famus}
Youjiang Xu, Linchao Zhu, Lu~Jiang, and Yi~Yang.
\newblock Faster meta update strategy for noise-robust deep learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 144--153, 2021.

\bibitem[Yi and Wu(2019)]{pencil}
Kun Yi and Jianxin Wu.
\newblock Probabilistic end-to-end noise correction for learning with noisy
  labels.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7017--7025, 2019.

\bibitem[Yu et~al.(2020)Yu, Ikami, Irie, and Aizawa]{MTC}
Qing Yu, Daiki Ikami, Go~Irie, and Kiyoharu Aizawa.
\newblock Multi-task curriculum framework for open-set semi-supervised
  learning.
\newblock In \emph{European Conference on Computer Vision}, pages 438--454.
  Springer, 2020.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{coteaching+}
Xingrui Yu, Bo~Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{International Conference on Machine Learning}, pages
  7164--7173. PMLR, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{Mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Zheng, Wu, Goswami, and Chen]{PLC}
Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, and Chao Chen.
\newblock Learning with feature-dependent label noise: A progressive approach.
\newblock \emph{arXiv preprint arXiv:2103.07756}, 2021.

\bibitem[Zhang and Sabuncu(2018)]{generalized_cross_entropy}
Zhilu Zhang and Mert~R Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock \emph{arXiv preprint arXiv:1805.07836}, 2018.

\bibitem[Zheltonozhskii et~al.(2021)Zheltonozhskii, Baskin, Mendelson,
  Bronstein, and Litany]{c2d}
Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex~M Bronstein, and
  Or~Litany.
\newblock Contrast to divide: Self-supervised pre-training for learning with
  noisy labels.
\newblock \emph{arXiv preprint arXiv:2103.13646}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Wang, and Bilmes]{zhou2020robust}
Tianyi Zhou, Shengjie Wang, and Jeff Bilmes.
\newblock Robust curriculum learning: from clean label detection to noisy label
  self-correction.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\end{thebibliography}

\appendix
\section*{Supplementary A: Dataset details}
\label{sup:a}
\paragraph{Synthetic noisy dataset}
CIFAR10 and CIFAR100 both consist of 50K images. Following the standard practice, for CIFAR10 and CIFAR100, we evaluate our method with two types of artificial noise: \textit{symmetric noise} by randomly replacing labels of all samples using a uniform distribution; and \textit{asymmetric noise} by randomly exchanging labels of visually similar categories, such as Horse  Deer and Dog  Cat. For closed-set noise only dataset, we test with 20\%, 50\%, 80\% and 90\% symmetric noise and 40\% asymmetric noise following DivideMix~\citep{dividemix}. For datasets including also open-set noise, following settings in EDM~\citep{evidentialmix}, we test with 30\%, 60\% total noise ratio and 50\%, 100\% open-set noise ratio on CIFAR10 dataset. The total noise ratio denotes the total proportion of noisy samples in the population of samples while the open-set noise ratio denotes the proportion of open-set noise in the noisy samples. The closed-set noise is generated as \textit{symmetric noise} while the open-set noise is randomly sampled from CIFAR100.

\paragraph{Real-world noisy dataset}
WebVision~\citep{webvision} is a large-scale dataset of 1000 classes of images crawled from the Web. Following previous work~\citep{mentornet, dividemix, moit}, we compare baseline methods on the top 50 classes from Google images Subset of WebVision. The noise ratio is estimated to be around~20\%. 
ANIMAL-10N~\citep{selfie} is a smaller and recently proposed real-world dataset consisting of 10 classes of animals, that are manually labelled with an error rate that is estimated to be approximately~8\%. ANIMAL-10N has similar size characteristics to the CIFAR datasets, with 50000 train images and 10000 test images.
Clothing1M~\citep{clothing1mdataset} is a large-scale dataset of 14 classes of clothing images crawled from online shopping websites, consisting of 1 million noisy images. The noise ratio is estimated to be around~38.5\%. 

\section*{Supplementary B: Implementation details}
\label{sup:b}
We use a PresActResNet-18~\citep{preactresnet} as the backbone for all CIFAR10/100 experiments following previous works. Unlike previous methods that use specific warmup settings for CIFAR10/CIFAR100, we train the model from scratch with  in all experiments. We set  for higher noise ratio ---  and  noise,  for remain settings in all CIFAR experiments except in the corresponding ablation part. We train all modules with the same SGD optimizer for 300 epochs with a momentum of 0.9 and a weight decay of 5e-4. The initial learning rate is 0.02 and is controlled by a cosine annealing scheduler. The batchsize is fixed as 128. 

For WebVision, we use InceptionResNetv2 following~\citep{dividemix}. We train the network with SGD optimizer for 150 epochs with a momentum of 0.9 and a weight decay of 1e-4. The initial learning rate is 0.01 and reduced by a factor of 10 after 50 and 100 epochs. The batchsize is fixed as 32. 
For Clothing1M, we use ResNet50 following~\citep{dividemix} with ImageNet pretrained weights. We train the network with SGD optimizer for 150 epochs with a momentum of 0.9 and weight decay of 1e-3. The initial learning rate is 0.002 and reduced by a factor of 10 after 50 and 100 epochs. The batchsize is fixed as 32. 
For ANIMAL-10N, we use VGG-19~\citep{vgg19} with batch-normalization following~\citep{selfie}. We train the network with SGD optimizer for 150 epochs with a momentum of 0.9 and weight decay of 5e-4. The initial learning rate is 0.02 and reduced by a factor of 10 after 50 and 100 epochs. The batchsize is fixed as 128. For all real-world noisy datasets, we train the model from scratch with , while  is fixed as 0.95.

Following recent works, in this work, we define three augmentation strategies: original image which we denote with `none' augmentation for testing, random cropping+horizontal flipping which we denote as `weak' augmentation, and `strong' augmentation  the one that further combines the augmentation policy from \citep{autoaugment}. For  we use 'strong' augmentation with mixup interpolations~\citep{Mixup} while for , we use 'weak' augmentation for  and 'strong' augmentation for  in eq.(4).
For mixup interpolation, following DivideMix~\citep{dividemix}, we set  as 4 for beta mixture for the CIFAR10/CIFAR100 datasets, and as 0.5 for the real-world noisy dataset. 

\section*{Supplementary C: Evaluation on different choices for sample selection}
\label{sup:c}

In this section, we extensively compare the performance of different sample selection mechanisms under different noise ratios and modes. More specifically, with the PMC  and NPK , we compare the performance of these two classifiers in two different selection modes. Pre-defined mode means the number of noisy samples is known. That is, given a symmetric noisy dataset with noise ratio as , we select the top  percent of the samples as clean according to its loss value or prediction confidence. Automatic mode means that there is no information about the ratio of noise. In this case, we utilize the consistency measure  using NPK in our method and the GMM-based loss modelling for PMC. Note that here we only considered two simple modes for sample selection and one variant in each mode respectively. There are many different variants proposed, however, we just aim to show the robustness of NPK over PMC here.  For a fair and clear comparison, we also show results of training with the whole dataset and clean subset as the bottom baseline and top baseline, respectively. Note, that in order to compare only the effect of the sample selection part, we exclude sample relabeling, strong data augmentation and optional feature consistency loss here.

\begin{table}[htbp]
\begin{center}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{@{}lllccccc@{}}
\toprule
Methods                      &                            &      & 50\% sym       & 80\% sym        & 40\% asym      & 60\% all(50\% open) & 60\% all(100\% open) \\ \midrule
\multirow{4}{*}{Automatic}   & \multirow{2}{*}{PMC}       & Last & 88.41          & 52.19          & 48.88          & 83.91               & \textbf{85.24}       \\ \cmidrule(lr){3-3}
                             &                            & Best & 88.52          & 64.03          & 79.24          & 84.08               & \textbf{85.32}       \\ \cmidrule(l){2-8} 
                             & \multirow{2}{*}{NPK(Ours)} & Last & \textbf{88.82} & \textbf{68.67} & \textbf{88.20} & \textbf{85.27}      & 84.72                \\ \cmidrule(lr){3-3}
                             &                            & Best & \textbf{88.88} & \textbf{69.57} & \textbf{88.99} & \textbf{85.30}      & 85.17                \\ \midrule
\multirow{4}{*}{Pre-defined} & \multirow{2}{*}{PMC}       & Last & 87.85          & 65.93          & \textbf{84.33} & 85.75               & 86.16                \\ \cmidrule(lr){3-3}
                             &                            & Best & 87.98          & 66.33          & \textbf{85.41} & 85.76               & 86.48                \\ \cmidrule(l){2-8} 
                             & \multirow{2}{*}{NPK}       & Last & \textbf{88.20} & \textbf{72.25} & 75.47          & \textbf{85.84}      & \textbf{86.43}       \\ \cmidrule(lr){3-3}
                             &                            & Best & \textbf{88.27} & \textbf{72.66} & 83.36          & \textbf{86.10}      & \textbf{86.53}       \\ \midrule
Whole dataset                &                            & Last & 58.93          & 27.59          & 76.68          & 59.92               & 80.99                \\ \cmidrule(lr){3-3}
                             &                            & Best & 80.86          & 61.55          & 85.41          & 79.26               & 84.18                \\ \midrule
Clean subset                 &                            & Last & 92.46          & 87.64          & 93.40          & 90.70               & 90.62                \\ \cmidrule(lr){3-3}
                             &                            & Best & 92.53          & 87.85          & 93.56          & 90.87               & 90.82                \\ \bottomrule
\end{tabular}
}
\end{center}
\caption{Results with different sample selection mechanisms.}
\label{tab:sampleselection}
\end{table}

In~\cref{tab:sampleselection}, we can see that the NPK-based selection achieved better performance compared to PMC-based selection regardless of the mode, and that our choice can significantly improve the baseline~(Whole dataset) without knowing the noise ratio. 

\section*{Supplementary D: Distance metric for feature consistency loss}
\label{sup:d}
In Section 3.3, we use the cosine distance as the distance metric for feature consistency loss. Here we also experiment with the use of the L2 distance. The results are shown in~\cref{distancemetric}, where it can be seen that there are small differences, but that the cosine similarity is in general better.

\begin{table}[htbp]
\begin{center}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{@{}lcccc@{}}
\toprule
Training                       & 50\%~sym       & 90\%~sym        & 40\%~asym      & 60\% all~(50\% open-set) \\ \midrule
SSR+(negative cosine similarity) & \textbf{96.1} & \textbf{95.2} & 95.5           & \textbf{95.2}          \\
SSR+(L2 distance)                & 96.0          & 94.7           & \textbf{96.1} & 94.3                   \\ \bottomrule
\end{tabular}}
\end{center}
\caption{SSR+ with different distance metric for feature consistency loss}
\label{distancemetric}
\end{table}

\section*{Supplementary E: Computational cost analysis}
\label{sup:e}
In~\cref{tab:time}, we report the running time of each step of our model on the datasets that we have experimented with. It can be seen that the time of sample selection and relabelling is almost negligible compared to the time of gradient propagation.

\begin{table}[htbp]
\begin{center}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{Dataset(Size)} & \multirow{2}{*}{Model training} & \multicolumn{3}{c}{Sample selection and relabelling}       \\ \cmidrule(l){3-5} 
                               &                                 & Feature extraction & sample selection & sample relabelling \\ \midrule
CIFAR(50K)                     & 112s                            & 9s                 & 1.23s            & ~0s                 \\
WebVision(~65K)                & 587s                            & 109s               & 1.48s            & ~0s                \\
Clothing1M(32K)                & 575s                            & 57s                & 0.79s            & ~0s                \\ \bottomrule
\end{tabular}}
\end{center}
\caption{Computational cost analysis.}
\label{tab:time}
\end{table}

\end{document}