\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}





\usepackage[preprint]{neurips_2022}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{multirow}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{appendix}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcommand{\cgaphl}[2]{
\fontsize{8pt}{1em}\selectfont{\textbf{#2}}
}
\newcommand{\cgaphlp}[2]{
\fontsize{8pt}{-.5em}\selectfont{(\textbf{#2})}
}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother


\title{RTFormer: Efficient Design for Real-Time Semantic Segmentation with Transformer}





\author{Jian Wang\thanks{Equal Contribution.}\quad Chenhui Gou\quad Qiman Wu\quad Haocheng Feng\\
\textbf{Junyu Han\quad Errui Ding\quad Jingdong Wang\thanks{Corresponding author.}}\\
Baidu VIS\quad Australian National University(ANU)\\
\texttt{\{wangjian33, wuqiman, fenghaocheng, hanjunyu, dingerrui, wangjingdong\}@baidu.com}\\
\texttt{u7194588@anu.edu.au}\\
}


\begin{document}


\maketitle

\begin{abstract}
  
Recently, transformer-based networks have shown impressive results in semantic segmentation. Yet for real-time semantic segmentation, pure CNN-based approaches still dominate in this field, due to the time-consuming computation mechanism of transformer. We propose RTFormer, an efficient dual-resolution transformer for real-time semantic segmenation, which achieves better trade-off between performance and efficiency than CNN-based models. To achieve high inference efficiency on GPU-like devices, our RTFormer leverages GPU-Friendly Attention with linear complexity and discards the multi-head mechanism. Besides, we find that cross-resolution attention is more efficient to gather global context information for high-resolution branch by spreading the high level knowledge learned from low-resolution branch. Extensive experiments on mainstream benchmarks demonstrate the effectiveness of our proposed RTFormer, it achieves state-of-the-art on Cityscapes, CamVid and COCOStuff, and shows promising results on ADE20K. Code is available at PaddleSeg\cite{liu2021paddleseg}: \href{https://github.com/PaddlePaddle/PaddleSeg}{https://github.com/PaddlePaddle/PaddleSeg}.

\end{abstract}

\begin{wrapfigure}[17]{r}{6cm}
\centering
\includegraphics[width=1.0\linewidth]{figures/efficient.png}
\caption{\textbf{Accuracy(mIoU\%) vs. Inference Speed(FPS) on CamVid\cite{CamVid} test set.} Our methods are presented in red dots while other methods are presented in blue dots.}
\label{fig:intro}
\end{wrapfigure}

\section{Introduction}

Semantic segmentation is a fundamental computer vision task which usually serves as critical perception module in autonomous driving, mobile applications, robot sensing and so on. In pace with the development of these applications, the demand of executing semantic segmentation in real-time grows stronger increasingly. Existing real-time segmentation methods mainly focus on exploiting CNN architectures, including designing high-efficiency backbones and decoders\cite{zhao2018icnet, yu2018bisenet, yu2021bisenet, hong2021deep, li2020semantic, fan2021rethinking, chao2019hardnet, orvsic2021efficient} by handcraft and exploring neural architecture search methods to find better trade-off between accuracy and efficiency\cite{zhang2019customizable, lin2020graph, li2019partial, chen2019fasterseg}. And significant improvement has been achieved by these great works so far.

More recently, vision transformers have been drawn lots of attention for their strong visual recognition capability\cite{vit, Deit, swin-v1, wang2021pyramid, wang2022pvt}. And inherited from them, a series of transformer-based architectures like\cite{zheng2021rethinking, xie2021segformer, yuan2021hrformer, ranftl2021vision} are proposed and show very promising performance on general semantic segmentation task. Comparing with CNN-based networks, the main distinction of these transformer-based architectures is the heavy usage of self-attention, and self-attention is good at capturing long-range context information which is essential in semantic segmentation. So, we consider that attention structure should also be effective in real-time semantic segmentation task. 

But up to now, only a few works\cite{xie2021segformer} have explored the application of attention in this field, and the state-of-the-arts are still dominated by CNN-based architectures. We suppose the main obstacles of applying attention in real-time setting might come from the following two aspects. One is that the computation properties of most existing types of attention are not quite inference friendly for GPU-liked devices, such as the quadratic complexity and the multi-head mechanism. The quadratic complexity introduces large computation burden when processing high resolution features, especially in dense prediction tasks like semantic segmentation. Although several works such as\cite{wang2021pyramid, xie2021segformer} shrink down the size of keys and values, the property of quadratic complexity remains. While the multi-head mechanism splits the matrix multiplication into multiple groups, which makes the attention operation to be time consuming on GPU-like devices, analogue to the situation that executing group convolution. The other is that conducting attention only on high resolution feature map itself like\cite{xie2021segformer, yuan2021hrformer} may not be the most effective way for capturing long-range context with high level semantic information, as a single feature vector from high resolution feature map has limited receptive field.

We propose a novel transformer block, named RTFormer block, as shown in Figure~\ref{fig:GFDA}, which aims to achieve better trade-off between performance and efficiency on GPU-like devices with transformer. For the low-resolution branch, a newly proposed GPU-Friendly Attention, derived from external attention\cite{guo2021beyond}, is adopted. It inherits the linear complexity property from external attention, and alleviates the weakness of multi-head mechanism for GPU-like devices by means of discarding the channel split within matrix multiplication operations. Instead, it enlarges the number of external parameters and splits the second normalization within double-norm operation proposed by external attention into multiple groups. This enables GPU-Friendly Attention to be able to maintain the superiority of multi-head mechanism to some extent. For the high-resolution branch, we adopt cross-resolution attention instead of only conducting attention within high-resolution feature itself. Besides, unlike the parallel formulation of multi-resolution fusion from\cite{wang2020deep, hong2021deep, yuan2021hrformer}, we arrange the two resolution branches into a stepped layout. Therefore, the high-resolution branch can be enhanced more effectively by the assistant of the high level global context information learned from low-resolution branch. Based on the proposed RTFormer block, we construct a new real-time semantic segmentation network, named RTFormer. In order to learn enough local context, we still use convolution blocks at the earlier stages and place RTFormer block at the last two stages. By taking extensive experiments, we find RTFormer can make use of global context more effectively and achieve better trade-off than previous works. Figure~\ref{fig:intro} shows the comparison between RTFormer with other methods on CamVid. Finally, we summarize the contribution of RTFormer as following three aspects:
\begin{itemize}
\item A novel RTFormer block is proposed, which achieves better trade-off between performance and efficiency on GPU-like devices for semantic segmentation task.
\item A new network architecture RTFormer is proposed, which can make full use of global context for improving semantic segmentation by utilizing attention deeply without lost of efficiency.
\item RTFormer achieves state-of-the-art on Cityscapes, CamVid and COCOStuff, and show promising performance on ADE20K. In addition, it provides a new perspective for practice on real-time semantic segmentation task.
\end{itemize}

\section{Related Work}

\paragraph{Generic Semantic segmentation.}
Traditional segmentation methods utilized the hand-crafted features to solve the pixel-level label assigning problems,e.g., threshold selection\cite{4310076},the super-pixel\cite{6205760} and the graph algorithm\cite{937505}. With the success of deep learning, a series of methods\cite{deeplabV3,SegNet,zhao2017pyramid} based on FCN (fully convolutional neural network) \cite{DBLP:journals/corr/LongSD14} achieve superior performance on various benchmarks. These methods improved FCN from different aspects. The Deeplabv3 \cite{deeplabV3} and the PSPNet\cite{zhao2017pyramid} enlarge the receptive field and fused different level features by introducing the atrous spatial pyramid pooling module and the pyramid pooling module. The SegNet\cite{SegNet} recovers the high-resolution map through the encoder-decoder structure. HRNet \cite{wang2020deep} introduces a multi-resolution architecture which maintains high-resolution feature maps all through the network. OCRNet \cite{yuan2020object} enhances the feature outputted from backbone by querying global context.

\vspace{-2mm}
\paragraph{Real-time Semantic segmentation.}
To solve the real-time segmentation problem, various methods\cite{zhao2018icnet, yu2018bisenet, yu2021bisenet,hong2021deep,fan2021rethinking,chen2019fasterseg} have been proposed. ICNet\cite{zhao2018icnet} solves this problem by using a well-designed multi-resolution image cascade network. FasterSeg\cite{chen2019fasterseg} utilizes neural architecture search (NAS) to reach the goal of balancing high accuarcy and low latency.
BiSeNetV1\cite{yu2018bisenet} and BiSeNetV2 \cite{yu2021bisenet} adopt a two-stream paths network and a feature fusion module to achieve a well balance between speed and segmentation performance. STDC\cite{fan2021rethinking} rethinks and improves BiSeNet by proposing a single-steam structure with detail guidance module. DDRNets\cite{hong2021deep} achieves better performance by designing a two deep branch network with multiple bilateral fusions and a Deep Aggregation Pyramid Pooling Module.

\vspace{-2mm}
\paragraph{Attention mechanism.}
The attention mechanism has been vigorous developed in computer vision field \cite{wang2018non,yuan2018ocnet,hu2018relation,hu2018squeeze,fu2019dual,xie2018attentional}. SE block proposed by \cite{hu2018squeeze} applies the attention function to channels and improves the representation capability of the network. \cite{hu2018relation} uses an attention-module to model object relation and help the objection detection. \cite{wang2018non} presents a non-local operation which can capture the long-range dependencies and shows promising results on video classification task. \cite{xie2018attentional} uses attention in point cloud recognition task. Self-attention is a special case of the attention mechanism that has been widely used in recent years\cite{yuan2018ocnet, fu2019dual, wang2020hierarchical}. However, the quadratic complexity limits its usage. Some works\cite{guo2021beyond, wang2020linformer, xiong2021nystromformer} reform self-attention to achieve linear complexity. But they are still not friendly for inference on GPU. Inspired by external attention \cite{guo2021beyond}, we developed a GPU-Friendly attention module that has low latency and high performance on GPU-like devices.

\vspace{-2mm}
\paragraph{Transformer in Semantic segmentation.}
Very recently, transformer shows promising performance in semantic segmentation. DPT \cite{ranftl2021vision} applies transformer as encoder to improve the performance of dense prediction task. SETR \cite{zheng2021rethinking} proposes a sequence-to sequence method and achieves impressing result. SETR uses pretrained VIT \cite{vit-G} as its backbone and has no downsampling in spatial resolution. However, it is difficult to use it for real-time segmentation task due to its heavy backbone and very high resolution. SegFormer \cite{xie2021segformer} increases efficiency by introducing a hierarchical transformer encoder and a lightweight all MLP decoder. Compared with SETR, SegFormer has both higher efficiency and higher performance. However, the efficiency of SegFormer is still relatively low compared to some state-of-the-art CNN based real-time segmentation model. By introducing our RTFormer block, our method can take advantage of the attention mechanism while achieving the real-time speed.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{figures/GFDA.pdf}
    \caption{Illustration of RTFormer block. For low resolution, GPU-Friendly Attention is applied. And for high resolution, we use Cross-resolution Attention which draws K and V from low resolution branch. Besides, we make up FFN with two  convolution layers.}
    \label{fig:GFDA}
\end{figure}


\section{Methodology}
In this section, we elaborate the details of our proposed approach. We first describe the RTFormer block, then we present how to construct RTFormer based on RTFormer block.

\subsection{RTFormer block}


RTFormer block is dual-resolution module which inherits the multi-resolution fusion paradigm from\cite{wang2020deep, hong2021deep, yuan2021hrformer}. In contrast to the previous works, RTFormer block is comprised of two types of attention along with their feed forward network, and arranged as stepped layout, as shown in Figure~\ref{fig:GFDA}. In the low-resolution branch, we use a GPU-Friendly Attention to capture high level global context. While in the high-resolution branch, we introduce a cross-resolution attention to broadcast the high level global context learned from low-resolution branch to each high-resolution pixel, and the stepped layout is served to feed more representative feature from the low-resolution branch into the cross-resolution attention.

\begin{wrapfigure}[25]{r}{7cm}
	\centering
	\includegraphics[width=1.0\linewidth]{figures/GFA.pdf}
	\caption{Comparison between Multi-Head External Attention and GPU-Friendly Attention. Multi-head external attention splits the matrix multiplication into several groups while our GPU-Friendly Attention makes matrix multiplication to be integrated which is more friendly for GPU-like devices.}
	\label{fig:GFA}
\end{wrapfigure}


\vspace{-2mm}
\paragraph{GPU-Friendly Attention.}

Comparing the different existing types of attention, we find that external attention\cite{guo2021beyond}(EA) can be a potential choice for being executed on GPU-like devices due to its gratifying property of linear complexity, and our GPU-Friendly Attention(GFA) is derived from it. Thus, before detailing GFA, we review EA first. Let  denotes an input feature, where  is the number of elements(or pixels in images) and  is the feature dimension, then the formulation of EA can be expressed as:



where  are learnable parameters,  is the parameter dimension,  is the Double Normalization operation proposed by\cite{guo2021beyond}. And the multi-head version of EA can be expressed as:



where ,  and  is the number of heads, while  is the th head of . As shown in upper part of Figure~\ref{fig:GFA}, the multi-head mechanism generates  attention maps for improving upon the capacity of EA, and this makes the matrix multiplication be splitted into several groups, which is similar as group convolution. Although EA uses shared  and  for different heads, which can speed up the calculation a lot, the splitted matrix multiplication remains.

To avoid the latency reduction on GPU-like devices due to the multi-head mechanism, we propose a simple and effective GPU-Friendly Attention. It evolves from the basic external attention expressed by Equation~\ref{equ:EA}, which can be formulated as:



where ,  and  denotes Grouped Double Normalization, which splits the second normalization of the original double normalization into  groups, as shown in the left lower part of Figure~\ref{fig:GFA}. From Equation~\ref{equ:GFA} we can find that GFA has two main improvements. On the one hand, it makes the matrix multiplication to be integrated, which is quite friendly for GPU-like devices. Benefit from this, we can enlarge the size of external parameters from  to . Therefore, more parameters can be tuned for improving the performance. On the other hand, it maintains the superiority of multi-head mechanism to some extent by taking advantage of the grouped double normalization. For intuitive comprehension, it can be regarded that GFA also generates  different attention maps for capturing different relations between tokens, but more feature elements are involved for computing similarity and all the attention maps contribute to the final output.

\vspace{-2mm}
\paragraph{Cross-resolution Attention.}

Multi-resolution fusion has been proven to be effective for dense prediction task. And for the design of multi-resolution architecture, we can intuitively apply the GFA in different resolution branches independently, and exchange features after the convolution module or attention module being executed like\cite{wang2020deep, yuan2021hrformer}. But in high-resolution branch, pixels focus on local information more than high level global context. Thus, we suppose that directly conducting attention on high-resolution feature map for learning global context is not effective enough. To obtain the global context more effectively, we propose a cross-resolution attention, which aims to make full use of the high level semantic information learned from low-resolution branch. As exhibited in Figure~\ref{fig:GFDA}, unlike GFA, cross-resolution attention is adopted in high-resolution branch for gathering global context. And the calculation of this cross-resolution attention is expressed as:



where ,  denote the feature maps on high-resolution branch and low-resolution branch respectively,  is a set of matrix operations including splitting, permutation and reshaping,  means the feature dimension of high-resolution branch. It is worth to explain that, the feature map , denoted as cross-feature in the following text, is computed from  by function  which is composed of pooling and convolution layers. And the spatial size of  indicates the number of tokens generated from the . Experimentally, we only adopt softmax upon the last axis of attention map for normalization, as a single softmax performs better than double normalization when the key and value are not external parameters. Specially, for fast inference on GPU-like devices, multi-head mechanism is also discarded here.

\vspace{-2mm}
\paragraph{Feed Forward Network.}

In the previous transformer-based segmentation methods like\cite{xie2021segformer, yuan2021hrformer}, the Feed Forward Network(FFN) is typically consist of two MLP layers and a depth-wise  convolution layer, where the depth-wise  layer works for supplementing position encoding or enhancing locality. Besides, the two MLP layers expand the hidden dimension to be two or four times of the input dimension. This type of FFN can achieve better performance with relative less parameters. But in the scenario where latency on GPU-like devices should be considered, the typical structure of FFN is not very efficient. In order to balance the performance and efficiency, we adopt two  convolution layers without dimension expansion in the FFN of RTFormer block. And it shows even better result than the typical FFN configuration.

\begin{figure}
\includegraphics[scale=0.5]{figures/total.png}
    \caption{Illustrating the RTFormer architecture. We place RTFormer block at the last two stages which indicated by pink block and use convolution blocks at the earlier stages which indicated by blue block. Besides, we add a DAPPM module for segmentation head, drawing on the successful experience from\cite{hong2021deep}.}
    \label{fig:RTFormer}
\end{figure}


\subsection{RTFormer}

Figure~\ref{fig:RTFormer} illustrates the overall architecture of RTFormer.

\vspace{-2mm}
\paragraph{Backbone Architecture.}
For extracting enough local information which is needed by high-resolution feature map, we combines convolution layers with our proposed RTFormer block to construct RTFormer. Concretely, we let RTFormer start from a stem block consist of two  convolution layers and make up the first two stages with several successive basic residual blocks\cite{he2016deep}. Then, from stage, we use dual-resolution modules which enable feature exchange between high-resolution and low-resolution branches, inspired by\cite{hong2021deep}. And for the high-resolution branches of the last three stages, the feature strides keep as  unchanged, while for the low-resolution branches, the feature strides are , ,  respectively. Specially, we arrange the dual-resolution module into stepped layout for boosting the semantic representation of high-resolution feature with the help of the output of low-resolution branch. Most importantly, we construct the stage and stage with our proposed RTFormer block which is illustrated in Figure~\ref{fig:GFDA} for efficient global context modeling, while the stage is still composed by basic residual blocks.

\vspace{-2mm}
\paragraph{Segmentation Head.}
For the segmentation head of RTFormer, we add a DAPPM module after low-resolution output feature, drawing on the successful experience from\cite{hong2021deep}. And after fusing the output of DAPPM with high-resolution feature, we obtain the output feature map with stride=. Finally, this output feature is passed into a pixel-level classification head for predicting dense semantic labels. And the classification head is consist of a  convolution layer and a  convolution layer, with the hidden feature dimension being same with input feature dimension.

\begin{table}
\centering
\caption{\textbf{Detailed configurations of architecture variants of RTFormer.}}
\begin{tabular}{c|c|c|c}
\toprule
Models & \#Channels & \#Blocks & Spatial size of cross-feature\\
\hline
RTFormer-Slim &  &  & \\
RTFormer-Base &  &  & \\
\bottomrule
\end{tabular}
\label{sample-tableArchitecture}
\end{table}

\vspace{-2mm}
\paragraph{Instantiation.}
We instantiate the architecture of RTFormer with RTFormer-Slim and RTFormer-Base, and the detailed configurations are recorded in Table~\ref{sample-tableArchitecture}. For the number of channels and number of blocks, each array contains  elements, which are corresponding to the  stages respectively. Especially, the elements with two numbers are corresponding to the dual-resolution stages. For instance,  means the number of channels is  for high-resolution branch and  for low-resolution branch. While  means the number of basic convolution blocks is  for high-resolution branch and  for low-resolution branch. It is worth to be noted that, the last two elements in block number array denote the number of RTFormer blocks, and they are both  for RTFormer-Slim and RTFormer-Base. The spatial sizes of cross-feature are set as 64() and 144() for RTFormer-Slim and RTFormer-Base respectively.


\section{Experiments}
In this section, we valid RTFormer on Cityscapes\cite{Cityscapes}, Camvid\cite{CamVid}, ADE20K\cite{ADE20K} and COCOStuff\cite{caesar2018coco}. We first introduce the datasets with their training details. Then, we compare RTFormer with state-of-the-art real-time methods on Cityscapes and CamVid. Besides, more experiments on ADE20K\cite{ADE20K} and COCOStuff\cite{caesar2018coco} are summarised to further prove the generality of our method. Finally, ablation studies of different design modules within RTFormer block on ADE20K\cite{ADE20K} are provided.

\subsection{Implementation Details}
Before finetuning on semantic segmentation, all models are pretrained on ImageNet\cite{deng2009imagenet}. And the training details for ImageNet\cite{deng2009imagenet} will be provided in the supplementary material. We apply mIoU and FPS as the metrics for performance and efficiency respectively, and the FPS is measured on RTX 2080Ti without tensorrt acceleration by default.

\vspace{-2mm}
\paragraph{Cityscapes.}
Cityscapes\cite{Cityscapes} is a widely-used urban street scene parsing dataset, which contains 19 classes used for semantic segmentation task. And it has 2975, 500 and 1525 fine annotated images for training, validation, and testing respectively. We train all models using the AdamW optimizer with the initial learning rate 0.0004 and the weight decay of 0.0125. We adopt the poly learning policy with the power of 0.9 to drop the learning rate and implement the data augmentation method including random cropping into , random scaling in the range of 0.5 to 2.0, and random horizontal flipping. All models are trained with 484 epochs (about 120K iterations), a batch size of 12, and syncBN on four V100 GPUs. For a fair comparison with other algorithms, online hard example mining(OHEM) is not used.

\vspace{-2mm}
\paragraph{CamVid.}
CamVid\cite{CamVid} contains 701 densely annotated frames and the resolution of each frame is . These frames are divided into 367 training images, 101 validation images, and 233 testing images. CamVid\cite{CamVid} have 32 categories which has the subset of 11 classes are used for segmentation experiments.
We merge the training set and validation set for training and evaluate our models on the testing set. We set the initial learning rate to 0.001 and the weight decay to 0.05. The power of poly learning policy is set to 1.0. We train all models for 968 epochs. Data augmentation includes color jitter, random horizontal flipping, random cropping into  and random scaling of [288, 1152]. Unlike previous methods\cite{fan2021rethinking}, we do not pretrain our model on Cityscapes\cite{Cityscapes}. All other training details are the same as for Cityscapes\cite{Cityscapes}.

\vspace{-2mm}
\paragraph{ADE20K.}
ADE20K\cite{ADE20K} is a scene parsing dataset covering 150 fine-grained semantic concepts, which split 20K, 2K, and 3K images for training, validation, and testing, respectively. Our models are trained with a batch size of 16 for 160k iterations. And we set the initial learning rate to 0.0001 and the weight decay to 0.05, and the other training settings are identical to those for Cityscapes\cite{Cityscapes}.

\vspace{-2mm}
\paragraph{COCOStuff.}
COCOStuff\cite{caesar2018coco} is a dense annotated dataset derived from COCO. It contains 10K images (9K for training and 1K for testing) with respect to 182 categories, including 91 thing and 91 stuff classes. And 11 of the thing classes have no annotations. We train RTFormer 110 epochs on COCOStuff with AdamW optimizer, and the initial learning rate and weight decay are set as 0.0001 and 0.05 respectively. In the training phase, we first resize the short side of image to  and randomly crop  patch for augmentation. While in the testing phase, we resize all images into . Other training settings are identical to Cityscapes.

\begin{table}
\small
\caption{\textbf{Comparisons with other state-of-the-art real-time methods on Cityscapes and CamVid.} Performances are measured with a single crop of ,  for Cityscapes and CamVid respectively. \#Params refers to the number of parameters. FPS is calculateted under the same input scale as performance measuring. In this table, * means we retrain this method follows its original training setting, and  means we measure the FPS on single RTX 2080Ti GPU.
  }
  \label{sample-tableSOTA}
  \resizebox{\textwidth}{!}{
    \centering
  \begin{tabular}{l|c|c|c|cc|cc}\shline
    \multirow{2}{*}{Method} & 
    \multirow{2}{*}{Encoder}&
    \multirow{2}{*}{\#Params}&
    \multirow{2}{*}{GPU}&
    \multicolumn{2}{c|}{\textbf{Cityscapes}} & \multicolumn{2}{c}{\textbf{CamVid}}\\
    \cmidrule(l{0pt}r{0pt}){5-8}& & & &FPS &val mIoU(\%) &FPS &test mIoU(\%) \\
    \midrule
    ICNet \cite{Zhao_2018_ECCV} &- &-   &TitanX M &30.3   &67.7 &27.8 &67.1\\
DFANet A \cite{li2019dfanet} &Xception A &7.8M &TitanX &100.0 &-   &120.0 &64.7 \\
    DFANet B \cite{li2019dfanet} &Xception B  & 4.8M &TitanX &120.0  &-   &160.0  &59.3     \\
    \midrule
CAS  \cite{zhang2019customizable}  & -   &- &TitanX &108.0 &71.6   &169.0 &71.2     \\
    GAS \cite{lin2020graph}     &-   &- &TitanX &108.4 &72.4 &153.1 &72.8 \\
    \midrule
    DF1-Seg-d8 \cite{li2019partial}&DF1 &- &GTX 1080Ti &136.9 &72.4  &-   &- \\
    DF1-Seg \cite{li2019partial}   &DF1  &-  &GTX 1080Ti&106.4    &74.1  &- &-  \\
    DF2-Seg1 \cite{li2019partial}  &DF2    &-   &GTX 1080Ti &67.2 &75.9 &- &-\\
    DF2-Seg2 \cite{li2019partial}  &DF2   &-  &GTX 1080Ti &56.3 &76.9  &- &-\\
\midrule
BiSeNet1 \cite{yu2018bisenet} &Xception39    &5.8M &GTX 1080Ti  &105.8 &69.0  &175.0 &65.6     \\
    BiSeNet2 \cite{yu2018bisenet} &ResNet18     &49.0M  &GTX 1080Ti &65.5 &74.8   &116.3 &68.7     \\
    BiSeNetV2\cite{yu2021bisenet}  &-   &- &GTX 1080Ti &156.0 &73.4 &124.5 & 72.4\\
    BiSeNetV2-L \cite{yu2021bisenet} &-  &-  &GTX 1080Ti &47.3     &75.8  &32.7 &73.2 \\
\midrule
    SFNet   \cite{li2020semantic}
&ResNet18  &12.9M &RTX 2080Ti &- &-  &62.9 &73.8\\
    \midrule
    FasterSeg \cite{chen2019fasterseg} &- &4.4M &RTX 2080Ti &136.2 &73.1 &- &71.1\\
    \midrule
    STDC1-Seg75\cite{fan2021rethinking} &STDC1 &14.2M &RTX 2080Ti &74.6 &74.5 &- &-\\
    STDC2-Seg75\cite{fan2021rethinking} &STDC2 &22.2M &RTX 2080Ti &73.5 &77.0 &- &-\\
    STDC1-Seg\cite{fan2021rethinking} &STDC1 &14.2M &RTX 2080Ti &- &- &125.6 &73.0\\
    STDC2-Seg\cite{fan2021rethinking} &STDC2 &22.2M &RTX 2080Ti &- &- &100.5 &73.9\\
    \midrule
    DDRNet-23-Slim \cite{hong2021deep} &- &5.7M &RTX 2080Ti &101.0 &76.1 &217.0 &74.7\\
    DDRNet-23 \cite{hong2021deep} &- &20.1M &RTX 2080Ti &38.3 &78.9* &97.1 &76.3\\
    \midrule
    \textbf{RTFormer-Slim}(Ours) &- &4.8M &RTX 2080Ti &110.0 &76.3 &190.7 &81.4\\
    \textbf{RTFormer-Base}(Ours) &- &16.8M &RTX 2080Ti &39.1 &79.3 &94.0 &82.5\\
\bottomrule
  \end{tabular}
}
\end{table}


\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/Camvid_vis.png}
    \caption{\textbf{Qualitative results on CamVid\cite{CamVid} testing set.} Compared to DDRNet-23\cite{fu2019dual}, RTFormer predicts masks with finer details and reduces long-range errors as highlighted in white.}
    \label{fig:vis_camvid}
\end{figure}


\subsection{Comparison with State-of-the-arts}
In this part, we compare our RTFormer with state-of-the-art methods on Cityscapes\cite{Cityscapes} and CamVid\cite{CamVid}. Table~\ref{sample-tableSOTA} shows our results including parameters, FPS and mIoU for Cityscapes\cite{Cityscapes} and CamVid\cite{CamVid}.

\vspace{-2mm}
\paragraph{Results.} On Cityscapes\cite{Cityscapes}, our RTFormer owns the best speed-accuracy trade-off among all other real-time methods. For example, our RTFormer-Slim achieves  mIoU at  FPS which is faster and provides better mIoU compared to STDC2-Seg75\cite{fan2021rethinking} and DDRNet-23-Slim\cite{fu2019dual}. Besides, our RTFormer-Base achieves  FPS and  mIoU which establishes new state-of-the-art result. Further more, using only ImageNet\cite{deng2009imagenet} pre-training, our method achieves  mIoU at  FPS on CamVid\cite{CamVid}, significantly outperforms all other real-time methods including STDC2-Seg\cite{fan2021rethinking} which uses additional Cityscapes\cite{Cityscapes} pre-training. Moreover, Our RTFormer-Slim yields  mIoU at  FPS with only M, which is faster and better than other models like STDC2-Seg\cite{fan2021rethinking} at FPS and DDRNet-23\cite{hong2021deep} at FPS. Figure~\ref{fig:vis_camvid} shows the qualitative results on CamVid\cite{CamVid} testing set, where RTFormer-base provides better detail than DDRNet-23\cite{hong2021deep}, especially for the Column Pole class , which requires more global context. In summary, these results demonstrate the superiority of RTFormer in real-time semantic segmentation in terms of accuracy, latency, and model size.


\begin{table}
  \caption{\textbf{Comparisons with other state-of-the-art real-time methods on
ADE20K.} The \#Params, FLOPs and FPS are measured at resolution . \#Params refers to the number of parameters. In this table, * means that we retrain this model by ourself on ADE20K\cite{ADE20K}.  means we measure the FPS using single RTX 2080Ti GPU. Method without  is using its reported \#Params, FLOPs, FPS and mIoU.} 
  \label{sample-tableADE20K}
  \centering
  \begin{tabular}{l|c|c|c|c|c}
    \toprule
Method & Encoder & \#Params
    & FLOPs &FPS &val mIoU(\%)\\
    \midrule
    FCN  \cite{long2015fully}  &MobileNetV2 &9.8M &39.0G  &64.4 &19.7\\
    PSPNet\cite{zhao2017pyramid}&MobileNetV2 &13.7M &52.9G &57.7 &29.6 \\
    DeepLabV3+ \cite{chen2018encoder} &MobileNetV2 &15.4M &69.4G &43.1 &34.0 \\
    SegFormer \cite{xie2021segformer}&MiT-B0 &3.8M &8.4G &50.5 &37.4 \\
DDRNet-23-Slim \cite{hong2021deep} &- &5.6M  &18.2G & 189.1 &33.3*\\
    DDRNet-23\cite{hong2021deep} &- &20.1M &71.6G &71.2  &38.8* \\
    \midrule
    \textbf{RTFormer-Slim}(Ours) &- &4.8M &17.5G & 187.9 &36.7 \\
    \textbf{RTFormer-Base}(Ours) &- &16.8M &67.4G & 71.4 &42.1 \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Generalization Capability}

To further prove the effectiveness of our RTFormer on more generalized scene, we show additional results on ADE20K\cite{ADE20K} and COCOStuff\cite{caesar2018coco}.

\vspace{-2mm}
\paragraph{Results.}
Table~\ref{sample-tableADE20K} presents our result on ADE20K\cite{ADE20K}. Our RTFormer-Base archieves the superior mIoU of  and with FPS, which outperforms all other methods. For instances, in contrast to DDRNet-23-Slim\cite{hong2021deep}, RTFormer-Slim achieves better mIoU  and maintains nearly the same speed. Figure~\ref{fig:vis_ade} shows qualitative results on ADE20K validation set. Compared with DDRNet-23\cite{hong2021deep}, our RTFormer shows better details and context information. In summary, these results demonstrate that RTFormer also shows very promising performance on real-time semantic segmentation in generalized scene. While on COCOStuff, as shown in Table~\ref{sample-tableCOCOStuff}, our RTFormer-Base achieves  mIoU at  FPS, which outperforms the DDRNet-23 about  with a comparable inference speed and sets a new state-of-the-art.

\begin{table}
  \caption{\textbf{Comparisons with other state-of-the-art real-time methods on
COCOStuff.} The \#Params, FLOPs and FPS are measured at resolution .}
  \centering
  \begin{tabular}{l|c|c|c|c|c}
    \toprule
Method 
& GPU
    & \#Params
    & FLOPs &FPS &test mIoU(\%)\\
    \midrule
    PSPNet\cite{zhao2017pyramid}
    & -
    & - & - & 6.6 & 32.6 \\
    ICNet\cite{Zhao_2018_ECCV}
    & TitanX M
    & - & - & 35.7 & 29.1 \\
    BiSeNetV2\cite{yu2021bisenet}
    & GTX 1080Ti
    & - & - & 87.9 & 25.2 \\
    BiSeNetV2-L\cite{yu2021bisenet}
    & GTX 1080Ti
    & - & - & 42.5 & 28.7 \\
    \midrule
    DDRNet-23 
    & RTX 2080Ti
    & 20.1M & 28.1G & 146.1 & 32.1 \\
    \midrule
    \textbf{RTFormer-Base}
    & RTX 2080Ti
    & 16.8M & 26.6G & 143.3 & 35.3 \\
    \bottomrule
\end{tabular}
  \label{sample-tableCOCOStuff}
\end{table}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/ADE20K_vis3.png}
    \caption{\textbf{Qualitative results on ADE20K\cite{ADE20K} validation set.} As shown, RTFormer is good at focusing on global context.}
    \label{fig:vis_ade}
\end{figure}

\subsection{Ablation study on ADE20K}

\paragraph{Training Setup.} We provide ablation results with RTFormer-Slim. To make quick evaluations, we train RTFormer-slim from scratch with the initial learning rate being set to 0.001, and the other training settings are same with experiments on ADE20K\cite{ADE20K} above. More experimental details and analyses are elaborated in the supplementary material.

\vspace{-2mm}
\paragraph{Comparison on different types of attention.}
To verify the effectiveness of our proposed attentions, we replace the attentions used in RTFormer block with different types and combinations. As shown in Table~\ref{table-abliation-attention}, we give the results of different combinations of multi-head self attention, multi-head external attention, GPU-Friendly attention and cross-resolution attention. For instance, "GFA+CA" means using GFA in low-resolution branch and CA in high-resolution branch. In addition, we adjust the hyper parameter  in multi-head external attention by , where  is a reduction ratio. We can find that GPU-Friendly attention outperforms all settings of multi-head external attention and is faster than the best one when , and meanwhile, GPU-Friendly attention is much more efficient than multi-head self attention with comparable performance. That indicates GPU-Friendly attention achieves better trade-off between performance and efficiency than both multi-head self attention and multi-head external attention on GPU-like devices. When we introduce cross-resolution attention, the performance improves further, while the FPS only drops less than .

\vspace{-2mm}
\paragraph{Comparison on different types of FFN.}
Table~\ref{tab:FFN} illustrates the results of typical FFN which is consist of two MLP layers and a  depth-wise convolution layer and our proposed FFN containing two  convolution layers. It is shown that our proposed FFN outperforms typical FFN not only on mIoU but also on FPS. That indicates that our proposed FFN is more suitable in the scenario when latency on GPU-like devices should be considered.

\begin{table}[!t]\scriptsize
\setlength{\fboxrule}{0pt}
\caption{\textbf{Ablation studies on different types of attention, FFN and different settings of hyper parameters.}}
	\label{table-abliation-attention}
	\vspace{-4pt}
	\begin{center}
	\begin{subtable}[t]{0.45\textwidth}
	\begin{center}
\caption{Comparison on different types of attention. SA, EA, GFA, CA denote Self Attention, External Attention, GPU-Friendly Attention and Cross-resolution Attention respectively.}
        \label{table-abliation-attention}
		\vspace{-4pt}
		\setlength{\tabcolsep}{1mm}
		\resizebox{0.8\textwidth}{!}{
            \centering
            \begin{tabular}{l|c|c}
             \toprule
                Attention
                &FPS
                &mIoU(\%) \\
                \midrule
                SA+SA &97.4 &32.7\\
                EA+EA (r=1) &180.8 &32.2\\
                EA+EA (r=0.125) &196.9 &31.9\\
                EA+EA (r=0.25) &189.6 &32.0\\
                GFA+GFA &189.8 &32.8 \\
                GFA+CA  &187.9 &33.0\\
                \bottomrule
            \end{tabular}
		}
		\label{tab:GFDAttention Abaltion}
	\end{center}
    \end{subtable}
\hfill
\begin{subtable}[t]{0.499\textwidth}
	\begin{center}
\caption{Comparison of different types of FFN. The typical FFN is composed of two MLP layers and a  depth-wise convolution layer, and our FFN design is two  convolution layers.}
	    \label{table-abliation-FFN}
	    \vspace{9pt}
		\setlength{\tabcolsep}{1.0mm}
		\resizebox{0.65\textwidth}{!}{
		 \label{sample-table}
          \centering
\begin{tabular}{l|c|c}
             \toprule
            Method &FPS &mIoU(\%)\\
            \midrule
            Typical FFN &178.5 &32.15\\
            Our FFN &187.9 &33.0\\
            \bottomrule
            \end{tabular}
		}
	    \label{tab:FFN}
	\end{center}
\end{subtable}

\begin{subtable}[t]{0.455\textwidth}
	\begin{center}
\caption{Comparison of different number of groups in Grouped Double Normalization.}
	    \label{table-abliation-group}
	    \vspace{-2pt}
		\setlength{\tabcolsep}{1.0mm}
		\resizebox{0.75\textwidth}{!}{
          \centering
         \begin{tabular}{c|c|c}
            \toprule
\# of Groups & FPS &mIoU(\%)\\
\midrule
            ,  &189.8 &32.2\\
            ,  &189.8 &32.3\\
            ,  &189.8 &32.8\\
            \bottomrule
            \end{tabular}
 		}
	    \label{tab:attention}
	\end{center}
\end{subtable}
\hfill
\begin{subtable}[t]{0.495\textwidth}\scriptsize
	\begin{center}
\caption{Comparison of different spatial size of the cross-feature in Cross-resolution Attention.}
	    \label{table-abliation-param}
	    \vspace{-2pt}
		\setlength{\tabcolsep}{1.0mm}
		\resizebox{0.75\textwidth}{!}{
          \centering
          \begin{tabular}{c|c|c}
            \toprule
            \# of Parameters &FPS&mIoU(\%)\\
\midrule
             &191.6 &32.85\\
             &189.8 &33.00\\
             &175.6  &32.94\\
            \bottomrule
            \end{tabular}
 		}
	    \label{tab:groups}
	\end{center}
\end{subtable}
\hfill

\end{center}
\label{tab_1}
\end{table}

\vspace{-2mm}
\paragraph{Influence of the number of groups within grouped double normalization.} 
We study the influence of the number of group in grouped double normalization under the setting of using GPU-Friendly Attention for both branches. And Table~\ref{table-abliation-group} shows the results of different configurations. For example, "+" means using  groups in low-resolution branch and  groups in high-resolution. Specially, when the number of groups is set to , grouped double normalization degrades to the original double normalization. Here, the best mIoU is achieved when the numbers of groups are  and , which illustrates that the grouped double normalization performs better than the original double normalization. And it is worth to be noted that, changing the number of groups in grouped double normalization does not affect the inference efficiency, which makes GPU-Friendly attention being able to keep high FPS when the number of groups is large.

\vspace{-2mm}
\paragraph{Influence of the spatial size of cross-feature in Cross-resolution Attention.}
We also investigate the spatial size of cross-feature in cross-resolution attention, including applying , , and . As presented in Table~\ref{table-abliation-param},  spatial size of cross-feature for RTFormer-Slim is the best according to the trade-off between FPS and mIoU. To some extent, it indicates that the spatial size of cross-feature which is close to the dimension of high-resolution feature is appropriate, as the high-resolution feature dimension of RTFormer-Slim is  which equals to .

\section{Conclusion}
In this paper, we present RTFormer which can  efficiently capture the global context to improve the real-time semantic segmentation performance. Extensive experiments demonstrate that our method not only achieves new state-of-the-art results on common datasets for real-time segmentation but also shows superior performance on challenging dataset for general semantic segmentation. Due to the efficiency of RTFormer, we hope our method can encourage new design of real-time semantic segmentation with transformer. One limitation is that while our RTFormer-Slim only has M parameters, more parameter efficiency may be needed in a chip of edge device. We leave it for future work.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\clearpage
\appendix

\begin{wraptable}[18]{r}{5cm}
\caption{Training settings on ImageNet classification.}
\footnotesize
\begin{tabular}{l|l}
\toprule
config & value \\
\hline
optimizer & AdamW \\
base learning rate & 0.0005\\
weight decay & 0.04\\
optimizer momentum &  \\
batch size & 1024 \\
learning rate schedule & cosine decay \\
minimum learning rate & 5e-6 \\
warmup epochs & 5 \\
warmup learning rate & 5e-7 \\
training epochs & 300  \\
augmentation & RandAug(9, 0.5) \\
color jitter & 0.4 \\
mixup & 0.2 \\
cutmix & 1.0 \\
random erasing & 0.25 \\
drop path & 0.0\\
\bottomrule
\end{tabular}
\label{supp-tab:imagenet-settings}
\end{wraptable}

\section{ImageNet Pre-training}

RTFormer is consist of several convolution blocks and RTFormer blocks, and RTFormer block contains different types of attention. Thus, we pre-train RTFormer on ImageNet-1K\cite{deng2009imagenet} mainly following the settings of training transformer network\cite{liu2021swin}, and the detail configuration is provided in Table~\ref{supp-tab:imagenet-settings}.

Table~\ref{supp-tab:imagenet-result} shows the performance of RTFormer on ImageNet classification. Both RTFormer-Slim and RTFormer-Base outperform the corresponding DDRNet variants. In addition, RTFormer-Base achieves the best performance among the existing backbones adopted in real-time semantic segmentation task.

\section{More Experiments}

In this section, we extend the ablation study about different types of attention. Firstly, we supplement experimental details about different types of attention, meanwhile, we introduce more variants of attention for analysis. Then, we analyse the results of different types of attention in detail.

\subsection{Experimental Details.}

The self-attention used for comparison is following \cite{xie2021segformer}. In contrast to the traditional self-attention, this type of self-attention shrinks the spatial size of key and value as  of the input feature, which can reduce the computation cost caused by the large input resolution. We set  for the self-attention in high-resolution branch, while  for low-resolution branch, following the settings for feature maps with stride= and stride= in\cite{xie2021segformer}.

For both multi-head self-attention and multi-head external attention, which are denoted as SA and EA in Table~\ref{supp-tab:ablation-attention}, we set the number of heads as  and  for high-resolution and low-resolution branches respectively. Similarly, for the GPU-Friendly attention, we set the number of groups as  and  separately for high-resolution and low-resolution branches. For the case of GFA+CA, the number of groups of the GPU-Friendly attention in low-resolution is still set as , while the cross-resolution attention has no multi-head calculation.

Especially for multi-head external attention, we give several results with different hyper parameters for comprehensive comparison. The first three results of multi-head external attention are with  respectively. When , the parameter dimension of multi-head external attention  in low-resolution branch is , which is identical to the setting in\cite{guo2021beyond}. And the other two results are used for showing more variations of the trade-off between performance and inference speed. In addition, an extra result with ,  is given, where  is the number of base feature dimension in network( for RTFormer-Slim by default). For GPU-Friendly attention, we set  constantly.

Further more, we also compare with the attentions proposed in Linformer \cite{wang2020linformer} and Nystr{\"o}mformer\cite{xiong2021nystromformer}. For linformer attention, we directly give a result without hyper parameter modification. While for nystr{\"o}mformer attention, we give two results denoted as NA() and NA(), which differs in the number of landmark points.

\begin{table}[t]
\centering
\caption{\textbf{Classification accuracy on the ImageNet validation set.} Performances are measured with a single  crop. ``\#Params'' refers to the number of parameters. ``FLOPs'' is calculated under the input scale of .}
\begin{tabular}{l|c|c|c}
\toprule
Method & \#Params & FLOPs & Top-1 Acc. \\
\midrule
ResNet-18\cite{he2016deep} & 11.2M & 1.8G & 69.0 \\
RestNet-50\cite{he2016deep} & 23.5M & 3.7G & 75.3 \\
DF1\cite{li2019partial} & 8.0M & 0.7G & 69.8 \\
DF2\cite{li2019partial} & 17.5M & 1.7G & 73.9 \\
MobileNetV2\cite{sandler2018mobilenetv2} & 3.4M & 0.3G & 72.0 \\
MobileNetV3\cite{howard2019searching} & 5.4M & 0.2G & 75.2 \\
Efficient-Net-B0\cite{tan2019efficientnet} & 5.3M & 0.4G & 76.3 \\
STDC1\cite{fan2021rethinking} & 8.4M & 0.8G & 73.9 \\
STDC2\cite{fan2021rethinking} & 12.5M & 1.4G & 76.4 \\
\midrule
DDRNet-23-slim\cite{hong2021deep} & 7.6M & 1.0G & 70.2 \\
DDRNet-23\cite{hong2021deep} & 28.2M & 3.9G & 75.9 \\
\midrule
\textbf{RTFormer-Slim} & 5.3M & 0.8G & 72.3 \\
\textbf{RTFormer-Base} & 20.5M & 3.0G & 77.4 \\
\bottomrule
\end{tabular}
\label{supp-tab:imagenet-result}
\end{table}

\begin{table}[t]
	\begin{center}
        \caption{\textbf{Comparison among different types of attention on ADE20K.} SA, EA, GFA, CA, LA, NA denote multi-head self-attention, multi-head external attention, GPU-Friendly attention, cross-resolution attention, linformer attention and nystr{\"o}mformer attention respectively. For example, GFA+CA means adopting GFA in low-resolution branch and CA in high-resolution branch.  is a ratio for adjusting the parameter dimension  in multi-head external attention.  is the number of base feature dimension in network ( by default). NA(), NA() denote the nystr{\"o}mformer attention with  and  landmark points respectively.}
        \setlength{\tabcolsep}{1mm}
        \centering
        \begin{tabular}{l|c|c|c}
        \toprule
            Attention
            &GPU
&FPS
            &val mIoU(\%) \\
            \midrule
            SA+SA &RTX 2080Ti &97.4 &32.7\\
            EA+EA (r=0.125) &RTX 2080Ti &196.9 &31.9\\
            EA+EA (r=0.25) &RTX 2080Ti &189.6 &32.0\\
            EA+EA (r=1) &RTX 2080Ti &180.8 &32.2\\
            EA+EA(r=1,C=36)	&RTX 2080Ti	&134.8 &32.8\\
            LA+LA &RTX 2080Ti &167.6 &32.4\\
            NA(32)+NA(32) &RTX 2080Ti &77.6 &32.9\\
            NA(64)+NA(64) &RTX 2080Ti &72.2 &33.0\\
            GFA+GFA &RTX 2080Ti &189.8 &32.8\\
            GFA+CA &RTX 2080Ti &187.9 &33.0\\
        \bottomrule
        \end{tabular}
		\label{supp-tab:ablation-attention}
	\end{center}
\end{table}

\subsection{Analyses.}

As illustrated in Table~\ref{supp-tab:ablation-attention}, we can find that multi-head self-attention achieves  mIoU, which performs better than multi-head external attentions with different settings of . But, the inference speed of multi-head self-attention is not competitive, which is mainly caused by the quadratic complexity and multi-head mechanism.

Multi-head external attention can achieve a good inference speed, which is benefit from its linear complexity and the design of sharing external parameter for multiple heads. Associated with the above two properties, multi-head external attention adopts a low parameter dimension (), which reduces the total computation cost further. However, the performance of multi-head external attention is suboptimal, as the network capacity is limited by those designs. Yet, the multi-head mechanism still remains, which is not friendly for running on GPU-like devices and leads to a relative worse efficiency than single head situation. As a example, when we let  to be equal to , the performance is still worse than multi-head self-attention, and the inference speed drops about FPS than .

The linformer attention achieves linear complexity by projecting the keys and values to a space where token length is fixed. But it is still built upon multi-head mechanism. The nystr{\"o}mformer attention repurposes the nystr{\"o}m method for approximating self-attention computation, and it achieves linear complexity by adopting landmark points to reconstruct the softmax matrix. However, it splits the original softmax matrix computation into several parts which causes the suboptimal inference efficiency on GPU-like devices. Besides of the splitting operation, nystr{\"o}mformer also has the problem brought by the vanilla multi-head mechanism.

While, GPU-Friendly attention, which is derived from multi-head external attention, can achieve both relative good performance and inference speed. It is because that, GPU-Friendly attention discards the multi-head mechanism and makes the matrix multiplication to be integrated and friendly for GPU calculation. Meanwhile, the grouped double normalization in GFA helps to maintain the capacity for learning diverse information which can be regarded as an extension of multi-head mechanism. Therefore, the external parameters can be enlarged for increasing the network capacity without great loss of inference speed.

Further more, when the basic feature dimension  is enlarged from  to  for EA+EA(), the mIoU increases to 32.8, while the FPS drops from 180.8 to 134.8. From this result, we can conclude that the network equipped with GFA+GFA is faster than EA+EA about 41\% when they achieve the same performance, and this improvement is considerable.

Finally, the combination of GPU-Friendly attention and cross-resolution attention improves the performance further, and it outperforms other types and combinations of attentions in both accuracy and efficiency, which validates the effectiveness of our proposed attentions.


\end{document}