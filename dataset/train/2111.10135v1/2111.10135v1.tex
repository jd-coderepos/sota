\section{Experiments}

\subsection{Dataset and Metrics}
\label{exp:data}
SWiG~\cite{pratt2020grounded} dataset is composed of 75k, 25k and 25k images for the train, development and test set respectively.
There are $\vert \mathcal V \vert = 504$ verbs, $\vert \mathcal R \vert = 190$ roles, and $1 \le \vert \mathcal R_v \vert \le 6$ semantic roles per verb.
We use about $10k$ nouns, the number of noun classes in the train set.
The annotation for each image consists of a verb, a bounding box for each semantic role, and three nouns (from three annotators) for each semantic role.

The predicted verb and grounded nouns are measured by five metrics: \textit{verb}, \textit{value}, \textit{value-all}, \textit{grounded-value}, and \textit{grounded-value-all}.
The \textit{verb} metric denotes a verb prediction accuracy.
The \textit{value} metric denotes a noun prediction accuracy from its semantic role.
The \textit{value-all} metric denotes that all nouns corresponding to semantic roles are correctly predicted.
The \textit{grounded-value} metric denotes a grounded noun prediction accuracy for its semantic role.
Note that the grounded noun prediction is considered correct if it correctly predicts noun and bounding box.
The bounding box prediction is considered correct if it correctly predicts bounding box existence and the predicted box has an Intersection-over-Union (IoU) value of at least 0.5 with the ground-truth box.
The \textit{grounded-value-all} metric denotes that all grounded nouns corresponding to semantic roles are correctly predicted. 
The requirements for each metric are summarized in Table~\ref{table:metric}.
Because the number of roles per verb is different and the number of images per verb could be different, all above metrics are calculated for each verb and then averaged over them. 

Since these metrics depend heavily on the verb accuracy, the metrics are reported in 3 settings: \textbf{top-1 predicted verb}, \textbf{top-5 predicted verbs} and \textbf{ground-truth verb}.
In \textbf{top-1 predicted verb} setting, five metrics are reported: a top-1 predicted verb accuracy, two noun metrics and two grounded noun metrics.
If the top-1 predicted verb is incorrect, the noun and grounded noun metrics are considered incorrect.
In \textbf{top-5 predicted verbs} setting, five metrics are reported: a top-5 predicted verbs accuracy, two noun metrics and two grounded noun metrics.
If the ground-truth verb is not included in the top-5 predicted verbs, the noun and grounded noun metrics are
considered incorrect, too.
In \textbf{ground-truth verb} setting, four metrics are reported: two noun metrics and two grounded noun metrics.
From the ground-truth verb assumed to be known, noun and grounded noun predictions are taken from the model by conditioning on the ground-truth verb.

\begin{table}[t]
\centering
\caption{Requirements for each metric.} 
\centering 
\label{table:metric}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c} 
    \hline 
    \multicolumn{1}{c|}{} & \multicolumn{5}{c}{requirement} \\
    \hline 
    & \multirow{2}{*}{correct verb}
    & correct noun
    & correct nouns
    & correct bounding box
    & correct bounding boxes
    \\
    metric &
    &
    for a semantic role & 
    for all semantic roles &
    for a semantic role &
    for all semantic roles
    \\
\hline 
\hline
\textit{verb} & \Checkmark &  &  & \\ 
\hline
\textit{value} & \Checkmark & \Checkmark &  & \\
\hline
\textit{value-all} & \Checkmark & \Checkmark & \Checkmark & \\
\hline
\textit{grounded-value} & \Checkmark & \Checkmark &  &\Checkmark \\
\hline
\textit{grounded-value-all} & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ 
\hline 
\end{tabular}}
\end{table}

\subsection{Implementation Details}
\label{exp:detail}

Following previous work~\cite{pratt2020grounded}, we use ImageNet-pretrained ResNet-50 backbone~\cite{resnet} except Feature Pyramid Network (FPN) ~\cite{lin2017_fpn}.
The ResNet-50 backbone produces the image features $X_{img} \in \mathbb R ^ {c \times h \times w}$ from the input image where $c=2048$.
The hidden dimensions of each semantic role query, verb token and image feature are $512$ ($d=512$).
The verb embedding dimension and role embedding dimension are $256$ ($d_v=d_r=256$).
We use learnable 2D embeddings for the positional encodings.
The number of heads for all MHSA and MHA blocks is $8$.
We use 2 fully connected layers with ReLU activation function for the four followings: the FFN blocks in the encoder and decoder, the verb classifier, the noun classifier, and the bounding box existence predictor.
The size of hidden dimensions are $2048$, $2d$, $2d$, and $2d$, respectively.
The dropout rates are $0.15$, $0.3$, $0.3$, and $0.2$, respectively.
The bounding box regressor is 3 fully connected layers with ReLU activation function and $2d$ hidden dimensions, using $0.2$ dropout rate.
The label smoothing regularization~\cite{szegedy2016rethinking} is used for the target verb and noun labels with 
label smoothing factor $0.3$ and $0.2$, respectively.
We use AdamW~\cite{loshchilov2018decoupled} optimizer with the learning rate $10^{-4}$ ($10^{-5}$ for the backbone),
weight decay $10^{-4}$, $\beta_{1}=0.9$ and $\beta_{2}=0.999$.
We set the max gradient clipping value to $0.1$ and train the BatchNorm layers in the backbone.
The training epoch is 40 with batch size 16 per GPU on four 12GB TITAN Xp GPUs, which takes about 20 hours.
The loss coefficients are $\lambda_v=\lambda_n=1$ and $\lambda_{exist}=\lambda_{L_1}=\lambda_{GIoU}=5$.

\noindent\textbf{Data Augmentation:} Random Color Jittering, Random Gray Scaling, Random Scaling and Random Horizontal Flipping are used.
The hue, saturate and brightness scale in random color jittering set to $0.1$.
The scale of random gray scaling sets to $0.3$.
The scales of random scaling set to $0.5$, $0.75$ and $1.0$.
The probability of random horizontal flipping sets to $0.5$.

\noindent\textbf{Final Noun Loss:} In SWiG, three noun annotations exist per role. For each noun annotation, we calculate the loss~(Eq. \ref{eq:loss_n}). The final noun loss is the summation of the three noun losses. 

\noindent \textbf{Batch Training:} 
The number of semantic roles ranges from 1 to 6 depending on the frame of a verb.
In GSRTR, the semantic role queries are constructed as much as the number of semantic roles.
To ensure batch training, zero padding is used for each output of grounded noun prediction branches. 
We ignore the padded outputs in the loss computation.

\subsection{Experiment Results}
\textbf{Quantitative Comparison with Previous Work:}
Table~\ref{table:result} quantitatively compares our model with previous work on the \emph{dev} and \emph{test} splits of SWiG dataset.
In all evaluation metrics, GSRTR achieves the state-of-the-art accuracy.
In the \emph{dev} set, compared with JSL, GSRTR achieves the top-1 predicted verb and top-5 predicted verbs accuracies of 41.06\% (+1.46\%p) and 69.46\% (+1.75\%p), respectively.
In ground-truth verb setting, GSRTR achieves the value and grounded-value accuracies 74.27\% (+0.74\%p) and 58.33\% (+0.83\%p), respectively.
Note that previous work uses two ResNet-50 backbones and FPN, while our GSRTR only uses a single ResNet-50 backbone without FPN. 
Existing models in~\cite{pratt2020grounded} have about 108 million parameters, but our GSRTR only has about 83 million parameters.
Although GSRTR has less backbone capacity and less parameters, it achieves the state-of-the-art accuracy in every evaluation metric. In addition, the reason for the small improvement by GSRTR in terms of grounded-value metrics is that these metrics require correct predictions of verb, noun and bounding box as shown in Table~\ref{table:metric}.

Existing models in~\cite{pratt2020grounded} are trained separately in terms of verb prediction part and grounded noun prediction part, while our GSRTR is trained in an end-to-end manner.
For this reason, it is difficult to fairly compare the training time of ours with existing models.
However, we can reasonably guess that GSRTR takes less training time than others.
GSRTR takes about 20 hours with four 12GB TITAN Xp GPUs for whole training, but other models take about 20 hours with four 24GB TITAN RTX GPUs only for training of grounded noun prediction part.
For the comparison of inference time, we compare GSRTR with JSL which was the previous state-of-the-art.
We evaluate the models on the \emph{test} set in the same environment with one 2080Ti GPU.
GSRTR takes 21.69 ms (46.10 FPS) and JSL takes 80.00 ms (12.50 FPS) on the average of 10 trials.
\label{exp:result}

\noindent
\textbf{Effect of Verb Embedding Concatenation:}
We also quantitatively show the effect of verb embedding concatenation in the semantic role query.
If we do not concatenate the verb embedding (\emph{i.e.}, $d_v=0$ and $d_r=d$), the accuracies in the ground-truth verb setting decrease by around $1.3 \sim 2.3$\%p  (GSRTR w/o VE in Table~\ref{table:result}).
It demonstrates that the verb embedding concatenation is helpful for grounded noun prediction.

\begin{table}[!t]
    \centering
    \caption{
        Quantitative evaluation on the SWiG dataset.
    }
    \label{table:result}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|l|ccccc|ccccc|cccc}
        \hline
        \multicolumn{2}{c|}{}
            & \multicolumn{5}{c|}{top-1 predicted verb}
            & \multicolumn{5}{c|}{top-5 predicted verbs}
            & \multicolumn{4}{c}{ground-truth verb}  
        \\
        \hline
            &  
            &       &       &       & grnd & grnd
            &       &       &       & grnd & grnd
            &       &       & grnd & grnd  
        \\
        set & model 
            & verb & value & value-all & value & value-all
            & verb & value & value-all & value & value-all
            & value & value-all & value & value-all
        \\
        \hline
        \hline
        \multirow{4}{*}{dev} & ISL \cite{pratt2020grounded}
            & 38.83 & 30.47 & 18.23 & 22.47 & 7.64
            & 65.74 & 50.29 & 28.59 & 36.90 & 11.66
            & 72.77 & 37.49 & 52.92 & 15.00 
        \\
            & JSL \cite{pratt2020grounded}
            & 39.60 & 31.18 & 18.85 & 25.03 & 10.16
            & 67.71 & 52.06 & 29.73 & 41.25 & 15.07
            & 73.53 & 38.32 & 57.50 & 19.29
        \\
        \cline{2-16}
            & GSRTR w/o VE (Ours)
            & 40.81 & 32.05 & 19.31 & 25.64 & 10.31
            & 69.33 & 53.09 & 29.78 & 42.01 & 15.36
            & 72.55 & 37.07 & 57.00 & 18.93
        \\
            & GSRTR (Ours)
            & \textbf{41.06} & \textbf{32.52} & \textbf{19.63} & \textbf{26.04} & \textbf{10.44}
            & \textbf{69.46} & \textbf{53.69} & \textbf{30.66} & \textbf{42.61} & \textbf{15.98}
            & \textbf{74.27} & \textbf{39.24} & \textbf{58.33} & \textbf{20.19} 
        \\
        \hline
        \hline
        \multirow{4}{*}{test} & ISL \cite{pratt2020grounded}
            & 39.36 & 30.09 & 18.62 & 22.73 & 7.72
            & 65.51 & 50.16 & 28.47 & 36.60 & 11.56
            & 72.42 & 37.10 & 52.19 & 14.58 
        \\
            & JSL \cite{pratt2020grounded}
            & 39.94 & 31.44 & 18.87 & 24.86 & 9.66
            & 67.60 & 51.88 & 29.39 & 40.60 & 14.72
            & 73.21 & 37.82 & 56.57 & 18.45 
        \\    
        \cline{2-16}
            & GSRTR w/o VE (Ours)
            & 40.61 & 31.87 & 19.01 & 25.21 & 9.69
            & 69.75 & 53.25 & 29.67 & 41.65 & 14.93
            & 72.32 & 36.75 & 56.03 & 18.02        
        \\
            & GSRTR (Ours)
            & \textbf{40.63} & \textbf{32.15} & \textbf{19.28} & \textbf{25.49} & \textbf{10.10}       
            & \textbf{69.81} & \textbf{54.13} & \textbf{31.01} & \textbf{42.50} & \textbf{15.88}
            & \textbf{74.11} & \textbf{39.00} & \textbf{57.45} & \textbf{19.67}      
        \\
        \hline
    \end{tabular}}
\end{table}

\begin{figure}[!t]
    \centering
        \includegraphics[width=1\textwidth]{Arxiv_Figures/sketching_0626.jpg}
\caption{
    Role Attention Map on Image Features for a $Sketching$ image from the MHA block in each decoder layer.
    The left labels are the semantic roles of the verb $Sketching$.
    The rightmost column images and labels are predicted bounding boxes and nouns of our model.
}
\label{fig:role_img}
\end{figure}

\noindent
\textbf{Role Attention Map on Image Features:}
In Figure~\ref{fig:role_img}, each column shows the difference of attention maps among semantic roles.
For example, at Layer 6, the role $Agent$ focuses on the woman, and the role $Place$ focuses on the road and yard.
Each row shows the transition of attention maps through the decoder layers.
For example, in the role $Material$, the attention map gradually focuses on the paper in the image through the decoder layers.
It shows that the semantic role queries can focus on the region related to them.

\noindent
\textbf{Visualization on Role Relations:}
In Figure~\ref{fig:roles}, two images show different context for a verb $Swinging$.
The role $Agent$ and $Carrier$ in Fig.~\ref{fig:roles}(a) focus on the role $Place$, \emph{i.e.}, the forest ($Place$) is highly related to the monkey ($Agent$) and the vine ($Carrier$) given the verb $Swinging$.
Meanwhile, the role $Place$ in Fig.~\ref{fig:roles}(b) focuses on the role $Carrier$, \emph{i.e.}, the golf club ($Carrier$) is highly related to the golf course ($Place$) given the verb $Swinging$.
It shows that the relations among roles can be adaptively captured depending on the context of a given image.

\begin{figure}[!t]
    \centering
    \begin{tabular}{c@{\hskip 0in}c@{\hskip 0in}}
            \includegraphics[height=0.235\textwidth]{Arxiv_Figures/swing_a.jpg}
        &
            \includegraphics[height=0.235\textwidth]{Arxiv_Figures/swing_c.jpg}
        \\
       (a)&(b)
    \end{tabular}
\caption{
    Visualization on Role Relations for two \textit{Swinging} images. 
    We visualize the attention scores between semantic role pairs computed in the MHSA block of the last decoder layer. 
    Attention scores are represented as column-wise sum to 1.
}
\label{fig:roles}
\end{figure}
\begin{figure}[!t]
    \centering
        \includegraphics[width=\textwidth]{Arxiv_Figures/tugging_0626.jpg}
\caption{
    Verb Token Attention Map on Image Features for three $Tugging$ images.
    Each row consists of an image and attention maps from the MHSA block in each encoder layer.
}
\label{fig:v_img}
\end{figure}

\noindent
\textbf{Verb Token Attention Map on Image Features:}
In Figure~\ref{fig:v_img}, 
the rightmost column shows the semantic regions where the verb token focuses on are similar.
The verb token can capture the key feature (\eg, tugged item) to infer the salient action.
Each row shows the transition of attention maps through the encoder layers,
\eg, focusing on the tugged item gradually.
