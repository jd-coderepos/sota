


In order to compare the performance of the traditional BLAS \dgemm\ routine and the various implementations of
\strassen, we define the \emph{effective} \GFLOPS{} metric for
$m\times k \times n$ matrix multiplication, similar to \cite{StrassenBenson,PPL2,StrassenLipshitz}:
\begin{equation}
  \text{\emph{effective} GFLOPS} = \frac{2\cdot m\cdot n\cdot k}{\text{time (in seconds)}}\cdot 10 ^{-9}.
  \label{e:egflops1}
\end{equation}
We next derive a model to predict the execution time $T$ and the \emph{effective} \GFLOPS\ of the traditional
BLAS \dgemm\ and the various implementations of \strassen.  
Theoretical predictions allow us to compare and contrast different implementation decisions, help with 
performance debugging, and (if sufficiently accurate) can be used to 
choose the right member of the family of implementations as a function of the number of threads used and/or problem size.
\NoShow{In future research, we envision also using the estimated cost for task scheduling on the heterogeneous or multi-core parallel
architecture.  More on this in the conclusion.
}




\begin{figure}[!t]
\centering
{\footnotesize
  \begin{tabular}{l| ccrrr}
  \whline
                                        & type
                                        & $\tau$
                                        & \dgemm{}
                                        & one-level 
                                        & two-level  \\
\hline
  $T_{a}^{\times}$                      & -
                                        & $\tau_{a}$
                                        & $ 2 m  n  k $
                                        & $7\times2\frac{m}{2}\frac{n}{2}\frac{k}{2}$
                                        & $49\times2\frac{m}{4}\frac{n}{4}\frac{k}{4}$ \\
$T_{a}^{A_{+}}$                           & -
                                        & $\tau_{a}$
                                        & -
                                        & $5\times2\frac{m}{2} \frac{k}{2}$
                                        & $95\times2\frac{m}{4}\frac{k}{4}$ \\
  $T_{a}^{B_{+}}$                           & -
                                        & $\tau_{a}$
                                        & -
                                        & $5\times2\frac{k}{2} \frac{n}{2}$
                                        & $95\times2\frac{k}{4}\frac{n}{4}$ \\
  $T_{a}^{C_{+}}$                           & -
                                        & $\tau_{a}$
                                        & -
                                        & $12\times2\frac{m}{2}\frac{n}{2}$
                                        & $154\times2\frac{m}{4}\frac{n}{4}$ \\
  \hline
  $T_{m}^{A_{\times}}$                  & \texttt{r}
                                        & $\tau_{b}$
                                        & $mk \lceil \frac{n}{n_c} \rceil$
                                        & $\frac{m}{2} \frac{k}{2} \lceil \frac{n/2}{n_c} \rceil$
                                        & $\frac{m}{4} \frac{k}{4} \lceil \frac{n/4}{n_c} \rceil$ \\
  $T_{m}^{{\widetilde A}_{\times}}$     & \texttt{w}
                                        & $\tau_{b}$
                                        & $mk \lceil \frac{n}{n_c} \rceil$
                                        & $\frac{m}{2} \frac{k}{2} \lceil \frac{n/2}{n_c} \rceil$
                                        & $\frac{m}{4} \frac{k}{4} \lceil \frac{n/4}{n_c} \rceil$ \\
  $T_{m}^{B_{\times}}$                  & \texttt{r}
                                        & $\tau_{b}$
                                        & $nk$
                                        & $\frac{n}{2} \frac{k}{2}$
                                        & $\frac{n}{4} \frac{k}{4}$ \\
  $T_{m}^{{\widetilde B}_{\times}}$       & \texttt{w}
                                        & $\tau_{b}$
                                        & $nk$
                                        & $\frac{n}{2} \frac{k}{2}$
                                        & $\frac{n}{4} \frac{k}{4}$ \\
  $T_{m}^{C_{\times}}$ \fromto{}{(*)}                 & \texttt{r/w}
                                        & $\tau_{b}$
                                        & $2mn\lceil\frac{k}{k_c}\rceil$
                                        & $2\frac{m}{2}\frac{n}{2}\lceil\frac{k/2}{k_c}\rceil $
                                        & $2\frac{m}{4}\frac{n}{4}\lceil\frac{k/4}{k_c}\rceil $ \\
  \hline
  $T_{m}^{A_{+}}$                       & \texttt{r/w}
                                        & $\tau_{b}$
                                        & $mk$
                                        & $\frac{m}{2} \frac{k}{2}$
                                        & $\frac{m}{4} \frac{k}{4}$ \\
  $T_{m}^{B_{+}}$                       & \texttt{r/w}
                                        & $\tau_{b}$
                                        & $nk$
                                        & $\frac{n}{2} \frac{k}{2}$
                                        & $\frac{n}{4} \frac{k}{4}$ \\
  $T_{m}^{C_{+}}$                       & \texttt{r/w}
                                        & $\tau_{b}$
                                        & $mn$
                                        & $\frac{m}{2} \frac{n}{2}$
                                        & $\frac{m}{4} \frac{n}{4}$ \\


  \whline
  \end{tabular}
}


\vspace{0.2in}

\centering
{\footnotesize
  \begin{tabular}{l  c | rrrrrr}
  \whline
  & & $N_{m}^{A_{\times}}$  &    $N_{m}^{B_{\times}}$  &    $N_{m}^{C_{\times}}$  &    $N_{m}^{A_{+}}$  &    $N_{m}^{B_{+}}$  & $N_{m}^{C_{+}}$ \\
\hline
                    \dgemm\ &        & 1    & 1     & 1     & -    & -     & -     \\
\hline  
\multirow{ 3 }{*}{one-level}  & ABC    & 12   & 12    & 12    & -    & -     & -     \\
                            & AB     & 12   & 12    & 7     & -    & -     & 36    \\
                            & Naive  & 7    & 7     & 7     & 19   & 19    & 36    \\
\hline                                          
\multirow{ 3 }{*}{two-level}  & ABC    & 194  & 194   & 154   & -    & -     & -     \\
                            & AB     & 194  & 194   & 49    & -    & -     & 462   \\
                            & Naive  & 49   & 49    & 49    & 293  & 293   & 462   \\
  \whline
  \end{tabular}
}
\caption{
The top table shows theoretical run time breakdown analysis of BLAS \dgemm{} and various implementations of \strassen{}.
The time shown in the first column for \dgemm{},
one-level \strassen{}, two-level \strassen{}
can be computed separately by multiplying the parameter in $\tau$ column with the number in the corresponding entries.
\fromto{}{{Due to the software prefetching effects, the row marked with $(*)$ needs to be multiplied by an additional parameter $\lambda\in[0.5,1]$, which denotes
the prefetching efficiency.}}
The bottom table shows
the coefficient mapping table for computing $T_m$ in the performance model.
}
\label{tab:breakdown}
\end{figure}


\begin{figure*}[htp!]
	\centering
	\includegraphics[width=0.43\textwidth]{figures/model_square_1core.pdf}
    \includegraphics[width=0.43\textwidth]{figures/outsquare_1core.pdf}\\
    \includegraphics[width=0.43\textwidth]{figures/model_rankk_1core.pdf}
    \includegraphics[width=0.43\textwidth]{figures/outrankk_1core.pdf}
    \includegraphics[width=0.43\textwidth]{figures/model_fixk_1core.pdf}
    \includegraphics[width=0.43\textwidth]{figures/outfixk1024_1core.pdf}
    \NoShow{\\
	\includegraphics[width=0.43\textwidth]{figures/model_square_10core.pdf}
		\includegraphics[width=0.43\textwidth]{figures/outsquare_10core.pdf}
	}
\caption{Performance of the various implementations on an Intel\textregistered\ Xeon\textregistered\ E5 2680 v2 (Ivybridge) processor.
		Left: modeled performance. Right: actual performance.
		The range of the y-axis does not start at zero to make the graphs more readable and 28.32 marks theoretical peak performance for this architecture.
	}
	\label{fig:model}
\end{figure*} 

\subsubsection*{Assumption}
Our performance model assumes that the underlying architecture has a modern \fromto{cache}{memory} hierarchy \fromto{with 
a small piece of fast memory (L1/L2/LLC cache) and a large chunk of slow memory (DRAM).}{with fast caches and relatively slow main memory (DRAM).}
We\fromto{ also}{} assume the latency for accessing the fast memory can be ignored \fromto{}{(either because it can be overlapped with computation or because it can be amortized over sufficient computation)} while the latency of loading from main memory is exposed.
For memory store operations, our model assumes that a lazy write-back policy guarantees the time for storing
into fast memory can be hidden.
The slow memory operations for BLAS \dgemm\ and the various implementation of \strassen{} consist of three
parts: (1) memory packing in (adapted) \dgemm\ routine; (2) reading/writing the submatrices of $C$ in (adapted) \dgemm\ routine; and (3) reading/writing of
the temporary buffer that are part of  \XXXstrassen\ and \ABXstrassen, outside (adapted) \dgemm{} routine.
Based on these assumptions, the execution time is dominated by the arithmetic operations and the slow memory operations.

\subsubsection*{Notation}
\fromto{We define $\tau_{a}$, $\tau_{b}$, $T_{a}$, $T_{m}$ and $T$ here.}{}
Parameter $\tau_{a}$  denotes the time (in seconds) of one \underline{a}rithmetic (floating point) operation, i.e., the reciprocal of the theoretical peak \GFLOPS\ of the system.
Parameter $\tau_{b}$ (\underline{b}andwidth, memory operation) denotes the amortized time (in seconds) of a unit (one double precision floating point number,
or eight bytes) of contiguous data movement from DRAM to cache.
\fromto{}{{In practice, \[\tau_{b}=\frac{8 \mbox{(Bytes)}}{\mbox{bandwidth (in GBytes/s)}}\cdot 10^{-9}.\]
For single core, we need to further multiply it by the number of channels.}}

The total execution time (in seconds), $ T $, is broken down into 
the time for \underline{a}rithmetic operations, $T_{a}$, and
\underline{m}emory operations:
\begin{equation}
  T=T_{a}+T_{m}.
  \label{e:etotal}
\end{equation}
\NoShow{
With this, \eref{e:egflops1} can be rewritten as
\begin{equation}
  \text{\emph{effective} GFLOPS} = \frac{2\cdot m\cdot n\cdot k}{T_{a}+T_{m}}\cdot 10 ^{-9}.
  \label{e:egflops2}
\end{equation}
}





\subsubsection*{Arithmetic Operations}
We break down $T_{a}$ into separate terms: 
\begin{equation}
  T_{a} = T_{a}^{\times} + T_{a}^{A_{+}} + T_{a}^{B_{+}} + T_{a}^{C_{+}}, 
  \label{e:fptime}
\end{equation}
where $T_{a}^{\times}$ is the arithmetic time for submatrix multiplication, and $T_{a}^{A_{+}}$, $T_{a}^{B_{+}}$, $T_{a}^{C_{+}}$ 
denote the arithmetic time of extra additions with submatrices of $A$, $B$, $C$, respectively. For \dgemm\, since there are no
extra additions, $T_{a} = 2mnk \cdot\tau_{a}$. For one-level \strassen{}, $T_a$ is comprised of 7 submatrix multiplications, 5 extra additions
of submatrices of $A$ and $B$, and 12 extra additions of submatrices of $C$. Therefore, 
\fromto{$T_{a} = \tau_{a}\cdot(1.75mnk+2.5mk+2.5kn+6mn) $}{$T_{a} = (1.75mnk+2.5mk+2.5kn+6mn) \cdot \tau_{a} $}.
\fromto{\color{red} I prefer the $ \tau_a $ etc. at the end of the formula.}{}
Note that the matrix addition actually involves 2 floating point operations for each entry because they are cast as\fromto{``{\tt axpy}'' or bottom level}{}
{\tt FMA} instructions.
Similar analyses can be applied to compute $T_{a}$ of a two-level \strassen\ implementation.
A full analysis is summarized in \figref{tab:breakdown}.

\subsubsection*{Memory Operations}
The total data movement overhead is determined by both the original matrix sizes $m$, $n$, $k$, and block sizes
$m_C$, $n_C$, $k_C$ in our implementation \figref{fig:side_by_side}(right).
We characterize each memory operation term in \figref{tab:breakdown} by its read/write type and the amount of memory (one unit$=$double precision floating number size$=$eight bytes) involved in the movement.
We decompose $T_{m}$ into
\begin{multline}
  T_{m} = N_{m}^{A_{\times}} \cdot T_{m}^{A_{\times}} + N_{m}^{B_{\times}} \cdot T_{m}^{B_{\times}} + N_{m}^{C_{\times}} \cdot T_{m}^{C_{\times}}  
  + N_{m}^{A_{+}} \cdot T_{m}^{A_{+}} + N_{m}^{B_{+}} \cdot T_{m}^{B_{+}} + N_{m}^{C_{+}} \cdot T_{m}^{C_{+}}, 
\end{multline}
where $T_{m}^{A_{\times}}$, $T_{m}^{B_{\times}}$ are the data movement time for reading from submatrices of $A$, $B$, respectively, for packing
 inside \GOTO{} \gemm\ algorithm (\figref{fig:side_by_side});
$T_{m}^{C_{\times}}$ is the data movement time for loading \emph{and} storing submatrices of $C$ inside \gemm\ algorithm;
$T_{m}^{A_{+}}$, $T_{m}^{B_{+}}$, $T_{m}^{C_{+}}$ are the data movement time for loading \emph{or} storing submatrices of $A$, $B$, $C$, respectively, related to the temporary buffer as part of \XXXstrassen\ and \ABXstrassen{}; the $N_m^X$s denote the corresponding coefficients, which are also tabulated in \figref{tab:breakdown}.



All write operations ($T_{m}^{{\widetilde A}_{\times}}$, $T_{m}^{{\widetilde B}_{\times}}$ for storing submatrices of $A$, $B$, respectively, into packing buffers) are omitted because our assumption of lazy write-back policy with fast memory. Notice that memory operations can recur multiple times depending on the loop in which they reside.
For instance, for two-level \strassen{},
\fromto{$T_{m}^{C_{\times}}=2\tau_{b}\lceil\frac{k/4}{k_c}\rceil \frac{m}{4}\frac{n}{4}$}
{$T_{m}^{C_{\times}}=2\lceil\frac{k/4}{k_c}\rceil \frac{m}{4}\frac{n}{4}\tau_{b}$}
denotes the cost of reading and writing the $\frac{m}{4}\times \frac{n}{4}$ submatrices of $C$ as intermediate result inside the micro-kernel. This is a step function proportional to $k$, because submatrices of $C$ are used to accumulate the \rankk{} update in the 5th loop in \figref{fig:side_by_side}(right).




\subsection{Discussion}
From the analysis summarized in~\figref{tab:breakdown} we can 
make predications about the relative performance of the various implementations.  It helps to also view the predictions as graphs, 
which we give in~\figref{fig:model}, using parameters that capture the architecture described in Section~\ref{sec:perf:single_node}.

\begin{itemize}
	\item 
	Asymptotically, the two-level \strassen\ implementations outperform corresponding one-level \strassen\ implementations, which in turn outperform the traditional \dgemm\ implementation.
	\NoShow{\item
	Asymptotically, when $ m $, $ n $, and $ k $ are all large, \XXXstrassen\ may actually outperform the other implementations. 
	This is because the use of temporary buffers 
	However, this is for matrices large enough that we do not see this in~\figref{fig:model}.
    }
	\item
	The graph for \mbox{$ m = k = n $, 1 core}, shows that for smaller square matrices, \ABCstrassen\ outperforms \ABXstrassen, but for larger square matrices this trend reverses.  This holds for both one-level and two-level \strassen{}.   The reason is that for small $ k $ \ABCstrassen\ reduced the number of times the temporary matrix $ M $ needs to be brought in from memory to be added to submatrices of $ C $.  For large $ k $, it increases the number of times the elements of those submatrices of $ C $ themselves are moved in and out of memory.
	\item
	The graph for \mbox{$ m = n = 16000$, $k$ varies, 1 core,} is particularly interesting: it shows that for $ k $ equal to the appropriate multiple of $ k_C $ ($k=2k_C$ for one-level and $ k=4k_C$ for two-level) \ABCstrassen\ performs dramatically better than the other implementations, as expected.
\end{itemize}
\NoShow{7. (Wrong) Conclusion:
\begin{itemize}
\item 0--2$k_c$: \dgemm\
\item 2$k_c$--$const\times n_c$: \ABCstrassen{} or \ABXstrassen{}
\item $n>const\times n_c$: \XXXstrassen{}
\end{itemize}
}The bottom line: depending on the problem size, a different implementation 
may have its advantages.



\NoShow{The performance model help us understand of why the different implementations perform as observed. It establishes that we understand what is going on as the data is being moved within the fast and slow memory. 
}



















