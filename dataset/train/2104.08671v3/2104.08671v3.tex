\documentclass[sigconf]{acmart}
\usepackage{tabularx}
\usepackage{bm}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{rightsretained}

\acmSubmissionID{163}



\begin{document}

\title[When Does Pretraining Help?]{When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset of 53,000+ Legal Holdings}

\author{Lucia Zheng}
\authornote{These authors contributed equally to this work.}
\email{zlucia@stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }

\author{Neel Guha}
\authornotemark[1]
\email{nguha@stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }
 
\author{Brandon R. Anderson}
\email{banderson@law.stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }

\author{Peter Henderson}
\email{phend@stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }
 
\author{Daniel E. Ho}
\email{dho@law.stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }

\renewcommand{\shortauthors}{Zheng and Guha, et al.}

\begin{abstract}
While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique.  We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help.  To address this, we first present CaseHOLD (Case \underline{H}oldings \underline{O}n \underline{L}egal \underline{D}ecisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on  CaseHOLD and existing legal NLP datasets.  While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (on a corpus of 3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2\% on F1, representing a 12\% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage in resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.
\end{abstract}

\copyrightyear{2021}
\acmYear{2021}
\acmConference[ICAIL'21]{Eighteenth International Conference for Artificial Intelligence and Law}{June 21--25, 2021}{São Paulo, Brazil}
\acmBooktitle{Eighteenth International Conference for Artificial Intelligence and Law (ICAIL'21), June 21--25, 2021, São Paulo, Brazil}\acmDOI{10.1145/3462757.3466088}
\acmISBN{978-1-4503-8526-8/21/06}

\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10010405.10010455.10010458</concept_id>
        <concept_desc>Applied computing~Law</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10010147.10010178.10010179</concept_id>
        <concept_desc>Computing methodologies~Natural language processing</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10010147.10010257.10010293.10010294</concept_id>
        <concept_desc>Computing methodologies~Neural networks</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Law}
\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[500]{Computing methodologies~Neural networks}

\keywords{law, natural language processing, pretraining, benchmark dataset}

\maketitle

\section{Introduction}
How can rapid advances in Transformer-based architectures be leveraged to address problems in law?  One of the most significant advances in natural language processing (NLP) has been the advent of ``pretrained'' (or self-supervised) language models, starting with Google's BERT model \cite{devlin-etal-2019-bert}. Such models are pretrained on a large corpus of general texts --- Google Books and Wikipedia articles --- resulting in significant gains on a wide range of fine-tuning tasks with much smaller datasets and have inspired a wide range of applications and extensions \cite{liu2019roberta, rogers2021primer}. 

One of the emerging puzzles for law has been that while \emph{general} pretraining (on the Google Books and Wikipedia corpus) boosts performance on a range of legal tasks, there do not appear to be any meaningful gains from \emph{domain-specific} pretraining (domain pretraining) using a corpus of law. Numerous studies have attempted to apply comparable Transformer architectures to pretrain language models on law, but have found marginal or insignificant gains on a range of legal tasks~\cite{chalkidis-etal-2020-legal, Elwany2019, zhong2020does, zhong2020jec}.  These results would seem to challenge a fundamental tenet of the legal profession: that legal language is \emph{distinct} in vocabulary, semantics, and reasoning \cite{mellinkoff2004language,mertz2007language, tiersma1999legal}.  Indeed, a common refrain for the first year of U.S.\ legal education is that students should learn the ``language of law'': ``Thinking like a lawyer turns out to
depend in important ways on speaking (and reading, and writing) like a lawyer.'' \cite{mertz2007language}.

We hypothesize that the puzzling failure to find substantial gains from domain pretraining in law stem from the fact that existing fine-tuning tasks may be too easy and/or fail to correspond to the domain of the pretraining corpus task. We show that  existing legal NLP tasks, Overruling (whether a sentence overrules a prior case, see Section \ref{sec:overruling}) and Terms of Service (classification of contractual terms of service, see Section \ref{sec:tos}), are simple enough for naive baselines (BiLSTM) or BERT (without domain-specific pretraining) to achieve high performance.  Observed gains from domain pretraining are hence relatively small. Because U.S.\ law lacks any benchmark task that is comparable to the large, rich, and challenging datasets that have fueled the general field of NLP (e.g., SQuAD~\cite{Rajpurkar2016}, GLUE~\cite{Wang2018}, CoQA~\cite{reddy2019coqa}), we present a new dataset that simulates a fundamental task for lawyers: identifying the legal \emph{holding} of a case. Holdings are central to the common law system. They represent the governing legal rule when the law is applied to a particular set of facts. The holding is precedential and what litigants can rely on in subsequent cases. So central is the identification of holdings that it forms a canonical task for first-year law students to identify, state, and reformulate the holding. 

This CaseHOLD dataset (Case \underline{H}oldings \underline{o}n \underline{L}egal \underline{D}ecisions) provides 53,000+ multiple choice questions with prompts from a judicial decision and multiple potential holdings, one of which is correct, that could be cited.  We construct this dataset using the rules of case citation \cite{bluebook}, which allow us to match a proposition to a source through a comprehensive corpus of U.S.\ case law from 1965 to the present. Intuitively, we extract all legal citations and use the ``holding statement,'' often provided in parenthetical propositions accompanying U.S.\ legal citations, to match context to holding \cite{arredondo2017harvesting}.

CaseHOLD extracts the context, legal citation, and holding statement and matches semantically similar, but inappropriate, holding propositions. This turns the identification of holding statements into a multiple choice task.

In Table \ref{tab:casehold}, we show a citation example from the CaseHOLD dataset. The Citing Text (prompt) consists of the context and legal citation text, Holding Statement 0 is the correct corresponding holding statement, Holding Statements 1-4 are the four similar, but incorrect holding statements matched with the given prompt, and the Label is the 0-index label of the correct holding statement answer. For simplicity, we use a fixed context window that may start mid-sentence.

\begin{table}[ht]
    \centering
    \small
    \caption{CaseHOLD example}
    \vspace{-0.15in}
    \begin{tabularx}{\columnwidth}{X}
         \toprule
         Citing Text (prompt) \\
         \midrule
         They also rely on Oswego Laborers' Local 214 Pension Fund v. Marine Midland Bank, 85 N.Y.2d 20, 623 N.Y.S.2d 529, 647 N.E.2d 741 (1996), which held that a plaintiff ``must demonstrate that the acts or practices have a broader impact on consumers at large." Defs.' Mem. at 14 (quoting Oswego Laborers', 623 N.Y.S.2d 529, 647 N.E.2d at 744). As explained above, however, Plaintiffs have adequately alleged that Defendants' unauthorized use of the DEL MONICO's name in connection with non-Ocinomled restaurants and products caused consumer harm or injury to the public, and that they had a broad impact on consumers at large inasmuch as such use was likely to cause consumer confusion. See, e.g., CommScope, Inc. of N.C. v. CommScope (U.S.A) Int'l Grp. Co., 809 F. Supp.2d 33, 38 (N.D.N.Y 2011) (\textless\text{HOLDING}\textgreater); New York City Triathlon, LLC v. NYC Triathlon \\
         \midrule
         Holding Statement 0 (correct answer) \\
         \midrule
         holding that plaintiff stated a 349 claim where plaintiff alleged facts plausibly suggesting that defendant intentionally registered its corporate name to be confusingly similar to plaintiffs CommScope trademark \\
         \midrule
         Holding Statement 1 (incorrect answer) \\
         \midrule
         holding that plaintiff stated a claim for breach of contract when it alleged the government failed to purchase insurance for plaintiff as agreed by contract \\
         \midrule
         Holding Statement 2 (incorrect answer) \\
         \midrule
         holding that the plaintiff stated a claim for tortious interference \\
         \midrule
         Holding Statement 3 (incorrect answer) \\
         \midrule
         holding that the plaintiff had not stated a claim for inducement to breach a contract where she had not alleged facts sufficient to show the existence of an enforceable underlying contract \\
         \midrule
         Holding Statement 4 (incorrect answer) \\
         \midrule
         holding plaintiff stated claim in his individual capacity \\
         \bottomrule
    \end{tabularx}
    \label{tab:casehold}
    \vspace{-0.15in}
\end{table}

We show that this task is difficult for conventional NLP approaches (BiLSTM F1 = 0.4 and BERT F1 = 0.6), even though law students and lawyers are able to solve the task at high accuracy. We then show that there are substantial and statistically significant performance gains from domain pretraining with a custom vocabulary (which we call Legal-BERT), using all available case law from 1965 to the present (a 7.2\% gain in F1, representing a 12\% relative boost from BERT). We then experimentally assess conditions for gains from domain pretraining with CaseHOLD and find that the size of the fine-tuning task is the principal other determinant of gains to domain-specific pretraining.

The code, the legal benchmark task datasets, and the Legal-BERT models presented here can be found at: \url{https://github.com/reglab/casehold}.

Our paper informs how researchers should decide when to engage in data and resource-intensive pretraining. Such decisions pose an important tradeoff, as cost estimates for fully pretraining BERT can be upward of \t\pm 1.96 \times \text{standard error}0.910 \pm 0.0120.958 \pm 0.0050.958 \pm 0.0050.963 \pm 0.007\bm{0.974} \pm 0.0050.712 \pm 0.0200.722 \pm 0.0150.773 \pm 0.0190.750 \pm 0.018\bm{0.787} \pm 0.0130.399 \pm 0.0050.613 \pm 0.0050.623 \pm 0.0030.680 \pm 0.003\bm{0.695} \pm 0.003tp<0.001p<0.001p<0.00117.6\% \pm 3.73\pm 1.96 \times \text{standard error}xx =xx\pm1.96 \times \textbf{standard error}0.8\% \pm 0.1541M.  Deciding whether to pretrain itself can hence have  significant ethical, social, and environmental implications \cite{Ben:Geb:McM:21}. Our research suggests that many easy tasks in law may not require domain pretraining, but that gains are most likely when ground truth labels are scarce and the task is sufficiently in-domain. Because estimates of domain-specificity across tasks using DS score match our qualitative understanding, this heuristic can also be deployed to determine whether pretraining is worth it. Our results suggest that for other high DS and adequately difficult legal tasks, experimentation with custom, task relevant approaches, such as leveraging corpora from task-specific domains and applying tokenization / sentence segmentation tailored to the characteristics of in-domain text, may yield substantial gains. Bender et al. \cite{Ben:Geb:McM:21} discuss the significant environmental costs associated in particular with transferring an existing large language models to a new task or developing new models, since these workflows require retraining to experiment with different model architectures and hyperparameters. DS scores provide a quick metric for future practitioners to evaluate when resource intensive model adaptation and experimentation may be warranted on other legal tasks. DS scores may also be readily extended to estimate the domain-specificity of tasks in other domains with existing pretrained models like SciBERT and BioBERT \cite{beltagy-etal-2019-scibert, lee_biobert_2019}.

In sum, we have shown that a new benchmark task, the CaseHOLD dataset, and a comprehensively pretrained Legal-BERT model illustrate the conditions for domain pretraining and suggests that language models, too, can embed what may be unique to legal language. 

\begin{acks}
    We thank Devshi Mehrotra and Amit Seru for research assistance, Casetext for the Overruling dataset, Stanford's Institute for Human-Centered Artificial Intelligence (HAI) and Amazon Web Services (AWS) for cloud computing research credits, and Pablo Arredondo, Matthias Grabmair, Urvashi Khandelwal, Christopher Manning, and Javed Qadrud-Din for helpful comments. 
\end{acks}

















\bibliographystyle{ACM-Reference-Format}
\bibliography{legal-benchmarks}



\end{document}
