

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{times}
\usepackage{epsfig}
\usepackage{tabularx}
\usepackage{subfigure} 
\usepackage{caption}
\captionsetup[subfigure]{labelformat=empty}
\newcommand{\mycomment}[1]{}
\newcommand{\reffig}[1]{Fig.~\ref{#1}}

\usepackage{float}
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\newcommand{\Conv}{\mathop{\scalebox{1.5}{\raisebox{-0.2ex}{}}}}\def\etal{\emph{et al}.}
\def\tabularxcolumn#1{m{#1}}
\renewcommand{\tabularxcolumn}[1]{>{\small}m{#1}}
\newcommand{\refsec}[1]{Sec.~{\ref{#1}}}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}

\renewcommand{\thesubfigure}{\thefigure(\alph{subfigure})}

\begin{document}
\pagestyle{headings}
\mainmatter


\title{Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs}
\titlerunning{Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation}
\captionsetup[subfigure]{labelformat=empty}
\authorrunning{Siddhartha Chandra \& Iasonas Kokkinos}
\author{Siddhartha Chandra \hfill Iasonas Kokkinos\\
{\tt \small siddhartha.chandra@inria.fr} \hfill {\tt \small iasonas.kokkinos@ecp.fr}
}
\institute{
INRIA GALEN \& Centrale Sup\'elec, Paris, France\\
}

\maketitle


\begin{abstract}
	In this work we propose a structured prediction technique that combines the virtues of Gaussian Conditional Random Fields (G-CRF)
	with Deep Learning:
	(a) our structured prediction task has a unique  global optimum that is obtained exactly from the solution of a linear system 
	(b) the gradients of our model parameters 
	are analytically computed using closed form expressions, in contrast to the memory-demanding
	contemporary deep structured prediction approaches \cite{crfrnn,Vemulapalli_2016_CVPR} that rely on back-propagation-through-time,
	(c) our pairwise terms do not have to be simple hand-crafted expressions, 
	as in the line of works building on the DenseCRF~\cite{crfrnn,deeplab1}, but can rather be `discovered' from data through deep 
	architectures, and 
	(d) out system can trained in an end-to-end manner. 
	Building on standard tools from numerical analysis we develop very efficient algorithms for inference and learning,
	as well as a customized technique adapted to the semantic segmentation task. 
	This efficiency allows us to explore more sophisticated architectures for structured prediction in deep learning: 
	we introduce multi-resolution architectures to couple information across scales in a joint optimization framework, yielding systematic improvements. 
	We demonstrate the utility of our approach on the challenging VOC PASCAL 2012 image segmentation benchmark, 
	showing substantial improvements over strong baselines. We make all of our code and experiments available at \url{https://github.com/siddharthachandra/gcrf}.
\end{abstract}

\section{Introduction}
\label{sec:Introduction}
Over the last few years deep learning has resulted in dramatic progress in the task of semantic image segmentation. 
Early works on using CNNs as feature extractors \cite{lecun13,zoomout,hariharan} and combining them with standard superpixel-based front-ends gave substantial improvements 
over well-engineered approaches that used hand-crafted features. The currently mainstream approach is relying on `Fully' Convolutional Networks (FCNs) \cite{fcnn,farabet2012scene}, where CNNs are trained to provide fields of outputs used for pixelwise labeling.




\begin{figure*}[b!]
\centering
 \subfigure[Schematic of a fully convolutional neural network with a G-CRF module]{\includegraphics[width=0.95\textwidth]{figures/fullnet.pdf}}\\
\subfigure[\scriptsize{Input Image}]{\includegraphics[width=0.24\linewidth]{figures/newpw2/rgb.png}}
\subfigure[\scriptsize{Person unary}]{\includegraphics[width=0.24\linewidth]{figures/newpw2/unary1.png}}
  \subfigure[\scriptsize{QO Output}]{\includegraphics[width=0.24\linewidth]{figures/newpw2/posterior.png}}
\subfigure[\scriptsize{Person Probability}]{\includegraphics[width=0.24\linewidth]{figures/newpw2/probs.png}}



\caption{\small{(a) shows a detailed schematic representation of our fully convolutional neural network with a G-CRF module.
 The G-CRF module is shown as the box outlined by dotted lines. The factor graph inside the G-CRF module shows a connected neighbourhood. 
 The white blobs represent pixels, red blobs represent unary factors, the green and blue squares represent vertical and horizontal connectivity factors.
  The input image is shown in (b). The network populates the unary terms (c), and horizontal and vertical pairwise
 terms.  The G-CRF module collects the unary and pairwise terms from the network and proposes an image hypothesis, i.e. scores (d) after inference. These scores 
 are finally converted to probabilities using the Softmax function (e), which are then thresholded to obtain the segmentation. It can
be seen that while the unary scores in (c) 
miss part of the torso because it is occluded behind the hand. 
The flow of information from the neighbouring region in the image, via the pairwise terms, encourages pixels in the occluded region to take the same label as the rest of the torso (d). Further it can be seen that the person boundaries are more pronounced in the output (d) due to pairwise constraints between pixels corresponding to the person and background classes.
 }}
 \label{fig:full}
\end{figure*}
A dominant research direction for improving semantic segmentation with deep learning is the combination of the powerful classification capabilities of FCNs with structured prediction  \cite{crfrnn,Vemulapalli_2016_CVPR,deeplab1,schwing,vemulapalli,IonescuVS15}, 
which aims at improving classification by capturing interactions between  predicted labels. 
One of the first works in the direction of combining deep networks with structured prediction was \cite{deeplab1} which advocated the use of densely-connected conditional random fields (DenseCRF)~\cite{densecrf} to post-process an FCNN output so as to obtain a sharper segmentation the preserves image boundaries. This was then used by Zheng \etal~\cite{crfrnn} who combined DenseCRF with a CNN
into a single Recurrent Neural Network (RNN), accommodating the DenseCRF post processing in an end-to-end training procedure. 


Most approaches for semantic segmentation perform structured prediction using approximate inference and learning \cite{schwing,couprie2012multi}.
For instance the techniques of \cite{crfrnn,Vemulapalli_2016_CVPR,deeplab1,vemulapalli} perform mean-field inference for a fixed number of 10 iterations.
Going for higher accuracy with more iterations 
could mean longer computation and eventually also memory bottlenecks:  back-propagation-through-time operates on  the intermediate `unrolled inference' results that have to be stored in  (limited) GPU memory. Furthermore, the non-convexity of the mean field objective means more iterations would only guarantee  convergence to a local minimum. 
The authors in \cite{Adelaide} use piecewise training with CNN-based pairwise potentials and three iterations of inference, while those in \cite{DPN} use highly-sophisticated modules, effectively learning to approximate mean-field inference. 
In  these two works a more pragmatic approach to inference is taken, considering it as a sequence of operations that need to be learned \cite{crfrnn}. 
These `inferning'-based approaches of combining learning and inference may be liberating, in the sense that one acknowledges and accommodates the approximations in the inference through end-to-end training. We show however here that exact inference and learning is feasible, while not making compromises in the model's expressive power. 





Motivated by \cite{TappenLAF07,rtf}, our starting point in this work is the observation that a particular type of graphical model, the Gaussian Conditional Random Field (G-CRF), 
allows us to perform exact and efficient Maximum-A-Posteriori (MAP) inference. 
Even though Gaussian Random Fields are unimodal and as such less expressive, Gaussian \emph{Conditional} Random Fields are unimodal \emph{conditioned on the data}, effectively
reflecting the fact that given the image one solution dominates the posterior distribution.
The G-CRF model thus allows us to construct rich expressive structured prediction models that still lend
themselves to efficient inference. In particular, the log-likelihood of the G-CRF posterior has the form of a quadratic energy function which captures unary and pairwise interactions between random
variables. There are two advantages to using a quadratic function: (a) unlike the energy of general graphical models, a quadratic  function has a unique global minimum if the system matrix is
positive definite, and (b) this unique minimum can be efficiently found by solving a system of linear equations.
We can actually discard the probabilistic underpinning of the G-CRF and understand G-CRF inference as an energy-based model, casting structured prediction as quadratic optimization (QO). 

G-CRFs were exploited for instance in the regression tree fields model of Jancsary \etal~\cite{rtf}  where decision trees were used to construct G-CRF's and address a host of  vision tasks, including inpainting, segmentation and pose estimation.
In independent work \cite{Vemulapalli_2016_CVPR} proposed a similar approach for the task of image segmentation with CNNs, where 
as  in \cite{Adelaide,DPN,vu2015context} FCNs are augmented with discriminatively trained convolutional layers that model and enforce pairwise consistencies between neighbouring regions.

One major difference to \cite{Vemulapalli_2016_CVPR}, as well as other prior works~\cite{crfrnn,deeplab1,vemulapalli,Adelaide,DPN}, is that
we use exact inference and do not use back-propagation-through-time
during training. In particular building on the insights of \cite{TappenLAF07,rtf}, we observe that the MAP solution, as well as  the gradient of our objective with respect to the inputs of our structured prediction module can be obtained through the solution of linear systems.
Casting the learning and inference tasks in terms of linear systems allows us to exploit the wealth of tools from numerical analysis. As we show in Sec.~ \ref{sec:optimization}, for Gaussian CRFs sequential/parallel mean-field inference amounts to solving a linear system using the classic Gauss-Seidel/Jacobi algorithms respectively. Instead of these under-performing methods we use conjugate gradients which allow us to perform exact inference and back-propagation in a small 
number (typically 10) iterations, with a negligible cost (s for the general case in Sec.~\ref{section:formulation}, and s for the simplified formulation in Sec.~\ref{sec:simpleqo}) when implemented on the GPU. 



 
Secondly, building further on the connection between MAP inference and linear system solutions, we  propose memory- and time-efficient algorithms for weight-sharing (Sec.~\ref{sec:simpleqo}) and multi-scale inference
 (Sec.~\ref{sec:multires}).  In particular, in Section \ref{sec:simpleqo} we show that one can further reduce the memory footprint and computation demands of our method by introducing a Potts-type structure in the pairwise term. This results in multifold accelerations, while delivering  results that are competitive to the ones obtained with the unconstrained pairwise term. 
In Sec.~\ref{sec:multires} we show that our approach allows us to work with arbitrary neighbourhoods that go beyond 
the common connected neighbourhoods. In particular we  explore the merit of using multi-scale networks, 
  where variables computed from different image scales interact with each other. This gives rise to a flow of information across different-sized neighborhoods. We show experimentally that this yields substantially 
  improved results over single-scale baselines. 
  
  


  




\mycomment{
\begin{figure}[t]
 \includegraphics[width=\linewidth]{figures/teaser.pdf}
 \caption{Graphical model for the image segmentation task. The input image is divided into patches by a grid with dotted lines. 
 The image region in each grid cell represents a random variable. 
 The white circles represent the unary interactions, and the white
 solid lines represent the pairwise interactions between neighbouring random variables. Our framework allows learning of these unary
 and pairwise terms from the data, and uses these terms in an energy minimization formulation to infer the optimal labeling for the
 image. Our framework allows the modeling of arbitrary neighbourhoods, and arbitrary differentiable loss functions for learning.}
 \label{fig:teaser}
\end{figure}
}


\mycomment{
In this work we focus our attention on the challenging task of pixel-level image segmentation. Figure \ref{fig:teaser} shows the graphical model for the image segmentation
task. Each patch in the image is a random variable, which assumes one out of a set of candidate labels. The solid lines represent pairwise interactions between the random variables, and the circles represent the unary
terms. While the end objective of this task is to predict one label per pixel,
we pose it as a vector-valued labeling task. For each pixel, our framework predicts a vector of scores
reflecting the classifier's confidence in the pixel belonging to each of the candidate classes. We use a softmax function on these scores to convert them into probabilities, and finally the pixel is assigned the most
probable class label. 
}




In \refsec{section:formulation} we describe
our approach in detail, and derive the expressions for weight update rules for parameter learning that are
used to train our networks in an end-to-end manner.
In  \refsec{sec:optimization} we analyze the efficiency of the linear system solvers and present our multi-resolution structured prediction algorithm. 
In \refsec{sec:experiments} we report consistent improvements over well-known baselines and state-of-the-art results on the VOC PASCAL  test set. 





\mycomment{
In contrast, our framework does not assume a certain
parametric distribution for these unary and pairwise interactions. 
While our approach brings about performance improvements over the baseline alone, 
our empirical evaluations in Sec.~ \ref{sec:experiments} reveal that using D-CRF alongside our approach yields a further performance boost.
}





\mycomment{
Recent years have seen a furore of activity on semantic segmentation benchmarks from the deep learning community.
The top  methods on the \emph{VOC PASCAL 2012 image segmentation benchmark} all use deep networks.
Deep learning methods owe their success to large repositories of annotated data (PASCAL, COCO), millions of parameters,
multi-scale architectures that learn richer representations, and the proliferation of high-end GPUs.
}



\mycomment{
Our approach also draws inspiration from the \emph{deep variational model} by Ranftl and Pock~\cite{pock}.
They combine CNNs with a global inference model which uses an  regularizer. The deep variational model proposes
a smooth approximation to the minimum s-t problem for inferring pixel-wise labels, and exploits an L-BGFS optimizer. 
In contrast, we use a quadratic energy function, whose unique global minimum is the solution of a system of linear 
equations~\cite{conjugategradient}.
This simplicity allows us to use fast GPU-based linear system solvers.
}

\newcommand{\refeq}[1]{Eq.~\ref{#1}}
\section{Quadratic Optimization Formulation}
\label{section:formulation}




We now describe our approach. 
Consider an image  containing  pixels. Each pixel  can take a label . 
Although our objective is to assign discrete labels to the pixels, we phrase our problem as a continuous inference task. Rather than performing a discrete inference task that delivers one label per variable, we use
a continuous function of the form  which gives a score for each pairing of a pixel to a label. 
This score can be intuitively
understood as being proportional to the log-odds for the pixel  taking the label , if a `softmax' unit is used to post-process .

We denote the pixel-level ground-truth labeling by a discrete valued vector  where , and the inferred hypothesis by
a real valued vector , where . 
Our formulation is posed as an energy minimization problem. In the following subsections, we
describe the form of the energy function, the inference procedure, and the parameter learning approach, followed by some technical
details pertinent to using our framework in a fully convolutional neural network. Finally, we describe a simpler formulation with pairwise weight sharing  which 
achieves competitive performance while being substantially faster. Even though our inspiration was from the probabilistic approach to structured prediction (G-CRF),
from now on we treat our structured prediction technique as a Quadratic Optimization (QO) module, and will refer to it as QO henceforth.

\subsection{Energy of a hypothesis} We define the energy of a hypothesis in terms of a function of the following form:



\noindent where  denotes the symmetric  matrix of pairwise terms, and  denotes the  vector of unary terms.
In our case, as shown in Fig.~\ref{fig:full}, the pairwise terms  and the unary terms  are learned from the data using a fully convolutional network. In particular and as illustrated in 
Fig.~\ref{fig:full},
 and  are the outputs of the pairwise and unary streams of our network, computed by a forward pass on the input image. These unary 
and pairwise terms are then combined by the QO module to give the final per-class scores for each pixel in the image. As we show below, during training we can easily obtain the gradients of the output with respect to the  and  terms, allowing us to train the whole network end-to-end. 

Eq. \ref{eqn:energy} is a standard way of expressing the energy of a system with unary and pair-wise interactions among the random variables \cite{rtf} in a vector labeling task. 
We chose this function primarily because it has a unique global minimum and allows for exact inference, alleviating the need for approximate inference. 
Note that in order to make the matrix  strictly positive definite, we add to it  times the Identity Matrix , where  is a design parameter set empirically in the experiments.

\mycomment{
\begin{figure}[]
	\centering
\includegraphics[width=.6\textwidth]{figures/matrix.pdf}
	\caption{Toy example illustrating the matrices  and  showing up in our cost function. The input image (a) contains
		black and white pixels.  (b) shows matrix , containing the unary terms: red for black pixels, blue otherwise. (c) illustrates ,   containing the pairwise terms
		for a connected neighbourhood: blue for two pixels having the same colour, red for different colours, white if pixels are not neighbours.}
	\label{fig:matrix}
	\hfill
	
\end{figure}
Figure \ref{fig:matrix} illustrates a toy example, showing the construction procedure of matrices  and .
In this example we consider a  input image containing white and black pixels. The goal here is to model discontinuities in pixel colour among neighbouring pixels.
The matrix , in  \reffig{fig:matrix}(a) containing the unary terms is red for black, and blue for white pixels.
The matrix , shown in   \reffig{fig:matrix}(c), contains the pairwise terms between every pair of pixels.
We assume a connected neighbourhood. The pairwise term is shown to be blue for any two pixels which have the same colour, and red if they have different colours.
 A white pairwise term indicates the two pixels are not neighbours of each other. Kindly note that the width and height of matrices  and  respectively are dimensional, where  is the number of
 pixels times the number of labels. This is a one-label toy example, hence  is the number of pixels.
 As shown in figure \ref{fig:full} in our case, these matrices,  and  are outputs of convolutional layers of a deep network.
 }
 
\subsection{Inference} Given  and , inference involves solving
for the value of  that minimizes the energy function in Eq.~\ref{eqn:energy}.
 If () is symmetric positive definite, then  has a unique global minimum \cite{conjugategradient} at:

As such, inference  is exact and efficient, only involving a system of linear equations.

\subsection{Learning A and B}
\label{subsec:learningAB}
Our model parameters  and  are learned in an end-to-end fashion via the back-propagation method.
In the back-propagation training paradigm each module or \emph{layer} in the network receives
the derivative of the final loss  with respect to its output , denoted by
, from the layer above.  is also referred to as the
gradient of . The module then computes the gradients of its inputs and propagates them down through the
network to the layer below.

To learn the parameters  and  via back-propagation, we require the expressions of gradients of  and , i.e.  and  respectively. We now derive these expressions.


\subsubsection{Derivative of Loss with respect to B}
\label{subsubsection:derivative_b}
To compute the derivative of the loss with respect to B, we use the chain rule of differentiation: .
Application of the chain rule yields the following closed form expression, which is a system of linear equations:

When training a deep network, the right hand side  is delivered by the layer above, and the derivative on the left hand side is sent to the unary layer below. 

\subsubsection{Derivative of Loss with respect to A}
The expression for the gradient of  is derived by using the chain rule of differentiation again: . 

Using the expression , substituting 
 ,
 and simplifying the right hand side,
 we arrive at the following expression:
 

where  denotes the kronecker product.
Thus, the gradient of  is given by the negative of the kronecker product of the output  and the gradient of .

\subsection{Softmax Cross-Entropy Loss}
Please note that while in this work we use the QO module as the penultimate layer of the network, followed by the
softmax cross-entropy loss, it can be used at any stage in a network and not only as the final classifier.
We now give the expressions for the softmax cross-entropy loss and its derivative for sake of completeness.

The image hypothesis is a scoring function of the form . For brevity, we denote the hypothesis concerning a single pixel by .
The softmax probabilities for the labels are then given by 

These probabilities are penalized by the cross-entropy loss
defined as
, 
where  is the ground truth indicator function for the ground truth label , i.e.  if , and  otherwise.
Finally the derivative of the softmax-loss with respect to the input is given by:
.


\subsection{Quadratic Optimization with Shared Pairwise Terms}
\label{sec:simpleqo}
We now describe a simplified \emph{QO} formulation with shared pairwise terms which is significantly faster in practice than the one described above.
We denote by  the pairwise energy term for pixel  taking the label , and pixel  taking the label .
In this section, we propose a \emph{Potts}-type pairwise model, described by the following equation:


In simpler terms, unlike in the general setting, the pairwise terms here depend on whether the pixels take the same label or not, and not on the particular labels they take.
Thus, the pairwise terms are \emph{shared} by different pairs of classes. 
While in the general setting we learn  pairwise terms, here we learn only  terms. To derive the inference and gradient equations after this simplification,
we rewrite our inference equation  as,


where , denotes the vector of scores for all the pixels for the class . The per-class unaries are denoted by , and the pairwise terms  are 
shared between each pair of classes. The equations that follow are derived by specializing the general inference (Eq.~\ref{eqn:linearSolver}) and gradient equations (Eq.~\ref{eqn:dldbres},\ref{eqn:dlda2_1}) to this particular setting.
Following simple manipulations, the inference procedure becomes a two step process where we first compute the sum of our 
scores , followed by , i.e. the scores for the class  as:

\noindent\begin{tabularx}{\textwidth}{@{}XX@{}}
 &
 
\end{tabularx}

Derivatives of the unary terms with respect to the loss are obtained by solving:

\noindent\begin{tabularx}{\textwidth}{@{}XX@{}}
 &
 
\end{tabularx}

Finally, the gradients of  are computed as 

Thus, rather than solving a system with , we solve  systems with . In our case, where  for  object classes and  background class, 
this simplification empirically reduces the inference time by a factor of , and the overall training time by a factor of .
We expect even larger acceleration for the MS-COCO dataset which has  semantic classes. 
Despite this simplification, the results are competitive to the general setting as shown in Sec.~\ref{sec:experiments}.
 \section{Linear Systems for Efficient and Effective Structured Prediction}
\label{sec:optimization}
Having identified that both the inference problem in \refeq{eqn:linearSolver} and computation of pairwise gradients in \refeq{eqn:dldbres} require the
solution of a linear system of equations, we 
now discuss  methods for accelerated inference that rely on standard numerical analysis techniques for linear systems  \cite{PressTVF92,Golub96}. 
Our  main contributions consist in (a) using fast linear system solvers that exhibit 
 fast convergence (\refsec{sec:solvers})  and (b) performing inference on multi-scale graphs by constructing block-structured linear systems (\refsec{sec:multires}).
 
 Our contributions in (a) indicate that standard conjugate gradient based linear system solvers can be up to 2.5 faster than the solutions one could get by a 
 naive application of parallel mean-field when implemented on the GPU. Our contribution in (b)  aims at accuracy rather than efficiency, and is experimentally validated in \refsec{sec:experiments}
 
 
\subsection{Fast Linear System Solvers}
\label{sec:solvers}

\begin{figure}[]
\centering

\begin{minipage}{0.4\textwidth} 

\centering
\begin{minipage}{\linewidth} \centering
    \begin{tabular}{l|r }
    \hline
    Method & Iterations \\ \hline Jacobi & 24.8 \\ \hline
    Gauss Siedel & 16.4 \\ \hline
    GMRES & 14.8 \\ \hline
    Conjugate Gradient & 13.2 \\ \hline
    \end{tabular}\2mm]
\end{minipage} \begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{figures/plot_solvers2.pdf}
\
x^{(k+1)}_i \gets  \frac{1}{a_{ii}} \left\lbrace b_i - \sum_{j \neq i} a_{ij}x^{(k)}_j \right\rbrace \text{.}
\label{eqn:jacobi}

x^{(k+1)}_i \gets \frac{1}{a_{ii}} \left\lbrace b_i - \sum_{j < i} a_{ij}x^{(k+1)}_j - \sum_{j > i} a_{ij}x^{(k)}_j\right\rbrace \text{.}
\label{eqn:gauss}

\mu_i \gets - \frac{1}{\Theta_{ii}} \left\lbrace \theta_i + \sum_{j \neq i} \Theta_{ij}\mu_j \right\rbrace \text{,}
\label{eqn:meanfield}
1mm]


\noindent \textbf{Acknowledgements}
This work has been funded by the EU Projects MOBOT FP7-ICT-2011-600796 and I-SUPPORT 643666 \#2020.

\begin{figure*}
 \centering
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000012.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000012_b0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000012_b1.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000012_a0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000012_a1.png}\\
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000046.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000046_b0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000046_b1.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000046_a0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000046_a1.png}\\
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000166.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000166_b1.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000166_b0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000166_a0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000166_a1.png}\\
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000186.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000186_b0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000186_b1.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000186_a0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000186_a1.png}\\
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000198.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000198_b0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000198_b1.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000198_a0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000198_a1.png}\\
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000209.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000209_b0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000209_b1.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000209_a0.png}
\includegraphics[width=0.18\textwidth]{figures/qualitative/2008_000209_a1.png}\\
\hspace{0mm} (a) Image \hspace{10mm} (b) Basenet \hspace{2mm} (c) Basenet + DCRF \hspace{3mm}  (d)  QO \hspace{0mm} (e) QO + DCRF \hspace{10mm}
\caption{Visual results on the VOC PASCAL 2012 test set. The first column shows the colour image, the second column shows the basenet predicted segmentation,
the third column shows the basenet output after Dense CRF post processing. The fourth column shows the
 QO predicted segmentation, and the final column shows the QO output after Dense CRF post processing.
It can be seen that our multi-resolution
network captures the finer details better than the basenet: the tail of the airplane in the first image, the person's body in the second image, the aircraft fan in the third image, the road between the
car's tail in the fourth image, and the wings of the aircraft in the final image, all indicate this. While Dense CRF post-processing quantitatively improves performance, it tends to miss very fine details.}
\label{fig:visual}
\end{figure*}

{\small
\bibliographystyle{splncs}
\bibliography{egbib}
}
\end{document}
