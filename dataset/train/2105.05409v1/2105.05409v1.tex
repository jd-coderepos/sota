
We conduct extensive experiments on our dataset FoodSeg103 and implement our proposed ReLeM by incorporating three baseline methods of semantic segmentation. 
Below, we first elaborate the experimental settings and the results of an ablation study.
Then, we show the performance gaps of the top model in the typical semantic segmentation task and our food image segmentation task.
We also evaluate the model adaptability using the Asian food data splits in our FoodSeg154.
Lastly, we provide some qualitative results of our best segmentation models.

\subsection{Implementation Details}
\noindent{\textbf{Dataset Settings}}  In our experiments, we use FoodSeg103 for in-domain training and testing, and use the additional Asian food set for out-domain testing. We randomly divide FoodSeg103 dataset into two splits: training set and testing set,  according to the 7:3 ratio. Our training set contains 4,983 images with 29,530 ingredient masks, while testing set contains 2,135 images with 12,567 ingredient masks. For ReLeM training, we use the training set of Recipe1M+ to learn the recipe representations (with test images in FoodSeg103 hidden from training). 

\noindent{\textbf{Segmenter Settings}} We conduct experiments based on two types of vision encoders: ResNet-50~\cite{he2016deep} based on convolutional neural networks, and ViT-16/B~\cite{dosovitskiy2021an} based on vision transformer. ResNet-50 is initialized from the pre-training model on ImageNet-1k~\cite{deng2009imagenet}, which is widely used in multiple vision tasks~\cite{krizhevsky2012imagenet,ren2015faster,chen2017deeplab}. ViT-16/B~\cite{dosovitskiy2021an} is a transformer-based model, which is initialized from the pre-training model on ImageNet-21k. ViT-16/B contains 12 transformer encoders with 12-head self-attention modules. We use the bilinear interpolation method to reinitialize the pre-trained positional embedding. In this paper, we use three types of segmentors: CCNet~\cite{huang2018ccnet}, FPN~\cite{kirillov2019panoptic} and SeTR~\cite{SETR}. CCNet and FPN are based on ResNet-50, while SeTR is based on ViT-16/B. 
Notably, SeTR extracts feature maps from  transformer encoders, followed by two sets of convolution layers for prediction. Other components of the segmentors follow the default settings with random initialization.

\noindent{\textbf{ReLeM Settings}} We use two types of vision encoders in ReLeM: ResNet-50 and ViT-16/B, which follow the same setting as Segmenter. In text preprocessing step, we use the skip-instruction models from the pre-trained weights in \cite{Salvador-LCME-arXiv2018}.

\noindent{\textbf{Learning Parameters of Segmenter}} Each image will be resized into a fixed size of  pixels with a ratio range from 0.5 to 2.0. A  patch is cropped from the resized images, and random horizontal flipping and color jitter are applied. We trained the models with 80k iterations based on 8 images per batch, and optimized the models by SGD solvers, with a momentum as 0.9 and weight decay as 0.0005. For CCNet and FPN, we set the initial learning rate to 1e-3, while for SeTR we set initial learning rate to 1e-3. According to the general settings~\cite{wang2021pyramid,huang2018ccnet}, the learning rate is decayed by a power of 0.9 according to the polynomial decay schedule. For simplicity, we do not apply hard negative mining during training, and our framework is based on the widely used platform mmsegmentation~\cite{mmseg2020}. All experiments were conducted on 4 Tesla-V100 GPU cards.

\noindent{\textbf{Learning Parameters of ReLeM}} Each input image are resized into a size of  pixels and a  patch is cropped from the resized images as the input of the vision encoder. The model is trained for 720 epochs and each batch contains 160 images. We use Adam solver~\cite{kingma2014adam} to optimize the models, with a learning rate of 1e-4, Here we follow a two-stage optimization strategy. We first freeze the weights of the vision encoder and optimize the text encoder. After the text encoder converges, we start to train the vision encoder and freeze the parameters of the text encoder.

\begin{figure*}[t]
	\centering
	\subfigure[Source Image]{
		\includegraphics[width=0.18\textwidth]{figure/figure_file/vis_1.pdf}}
	\subfigure[Ground Truth] {
		\includegraphics[width=0.18\textwidth]{figure/figure_file/vis_2.pdf}}
	\subfigure[ReLeM-CCNet] {
		\includegraphics[width=0.18\textwidth]{figure/figure_file/vis_3.pdf}}
	\subfigure[CCNet] {
		\includegraphics[width=0.18\textwidth]{figure/figure_file/vis_4.pdf}}
	\subfigure[Labels-Masks] {
		\includegraphics[width=0.18\textwidth]{figure/figure_file/vis_5.pdf}}
	\caption{Visualization results on FoodSeg103. ReLeM-CCNet can make more accurate predictions.} 	\label{fig:vis}
\end{figure*} 

\vspace{-2mm}
\subsection{Results and Observations}
The experiment results of CCNet, FPN and SeTR on FoodSeg103 are shown in Table \ref{tab:ab}. 


\begin{table}[tph]
		\centering
		\resizebox{0.45\textwidth}{!}{
		\begin{tabular}{|l|c|c|c|c|}
			\hline \hline
			Methods\;\; & mIoU &  mAcc  &Model Size\\
			\hline
			CCNet~\cite{huang2018ccnet} (ResNet-50)  &35.5 & 45.3 &381M\\
			ReLeM-CCNet (LSTM) &\textbf{36.8} &  47.4 &381M\\
ReLeM-CCNet (Transformer) & 36.0 & 46.5 &381M\\
\hline
			FPN~\cite{kirillov2019panoptic} (ResNet-50) &27.8 & 38.2 &218M\\
			ReLeM-FPN (LSTM)  &\textbf{29.1} &39.8 &218M\\
			ReLeM-FPN (Transformer)  &  28.9& 39.7 &218M\\
			\hline
			SeTR~\cite{SETR}, (ViT-16/B) &  41.3 & 52.7 &723M\\
			ReLeM-SeTR (LSTM)  & \textbf{43.9}&57.0&723M\\
			ReLeM-SeTR (Transformer) &43.2  &55.7  &723M\\
			\hline
			\hline
		\end{tabular}}
		\caption{Semantic segmentation results of our ReLeM plugged into three baseline methods (on the FoodSeg103 dataset). We implement two variants of ReLeM using LSTM and Transformer, respectively, to encode recipes. 
}\label{tab:ab}
		\vspace{-4mm}
	\end{table} \vspace{-1mm}
\if 0
The Segmenters of CCNet and FPN achieve significant improvements when incorporating with either LSTM-based or transformer-based ReLeM.
This confirms that ReLeM is effective in enhancing the convolution based semantic segmentation models.
However, it does not help in the newly proposed transformer-based model SeTR.
We believe this is due to the transformer backbone being more data-hungry than convolutional model~\cite{dosovitskiy2021an}.
Meanwhile, we observe that SeTR achieves the top performance given its attention weights have been pre-trained on JFT-300M~\cite{sun2017revisiting} with billions of object images, through self-supervised learning.
Besides, we can see that the performance of using LSTM-based ReLeM is consistently superior than using transformer-based ReLeM across all the model configurations. 
\fi
The Segmenters of all CCNet, FPN and SeTR achieve significant improvements when incorporating with either LSTM-based or transformer-based ReLeM (1.3\%, 1.3\% and 2.6\% improvement). 
This confirms that ReLeM is effective in enhancing both convolution based and transformer based semantic segmentation models.
Besides, we can see that the performance of using LSTM-based ReLeM is consistently superior than using transformer-based ReLeM across all the model configurations. 
\subsection{Comparing FoodSeg103 with Cityscapes} 
We compare the food image segmentation task with conventional semantic segmentation to compare the degree of difficulty of the two types of segmentation tasks.
We include three types of state-of-the-art segmentation algorithms, CCNet, SeTR and FPN.  They are evaluated on FoodSeg103 and Cityscapes~\cite{Cordts2016Cityscapes} datasets. Cityscapes contains around 5,000 images captured on the streets of German cities, and 20 types of objects as segmentation targets.
As we can see from Table~\ref{tab:cityscape}, all baseline methods achieve satisfactory results on Cityscapes, but suffer significant performance drops on our FoodSeg103. This indirectly shows the greater level of difficulty in the food image segmentation problem.

\begin{table}[thp]
		\centering
		\resizebox{0.43\textwidth}{!}
		{
		\begin{tabular}{|l|c|c|c|}
			\hline 
			Methods\;\; & Cityscapes & FoodSeg103 & \emph{gap}\\
			\hline
			CCNet &79.0 & 35.0 & 34.0 \\
			Sem-FPN &74.5 &27.8 & 46.7 \\
			SeTR &77.9 &41.3 &36.6 \\
			\hline
			\hline
		\end{tabular}}
		\caption{
		Semantic segmentation results on Cityscape~\cite{Cordts2016Cityscapes} and our FoodSeg103, showing that our FoodSeg103 is much more challenging than the object image dataset for the task of semantic segmentation.
		}
		\vspace{-5mm}
		\label{tab:cityscape}
	\end{table} 
\begin{table}[thp]
		\centering
		\resizebox{0.45\textwidth}{!}
		{
		\begin{tabular}{|l|c|c|c|}
			\hline 
			Methods\;\;  &mIoU & mAcc & aAcc \\
			\hline
			CCNet &  28.6 & 47.8 & 78.9\\
			ReLeM-CCNet & 29.2 & 47.5 &79.3\\
			CCNet-Finetune &41.3 & 53.8 & 87.7\\
			ReLeM-CCNet-Finetune &47.1 &59.5&85.5\\
			\hline
			FPN & 21.9 & 41.7 & 75.5\\
			ReLeM-FPN & 22.9 &42.3 & 77.0\\
			FPN-Finetune & 27.1&38.0&82.6\\
			ReLeM-FPN-Finetune &30.8 &40.7 & 78.9\\
			\hline
			\hline
		\end{tabular}}
		\caption{Cross-domain adaptation results. We use LSTM based ReLeM.}
		\vspace{-5mm}
		\label{tab:adapt}
	\end{table} 
\vspace{-3mm}
\subsection{Qualitative Examples}
In Figure \ref{fig:vis}, we show some qualitative results of using CCNet and ReLeM-CCNet on the testing set of FoodSed103. 
The first two rows clearly show that ReLeM-CCNet produces more accurate and detailed predictions than the vanilla CCNet, demonstrating the effectiveness of ReLeM. 
In the last row, we show a failure case.
It is actually a hard example with no clear boundaries among different ingredients.

\subsection{Cross-Domain Evaluation}
We conduct an out-domain model evaluation using the Asian food data set in FoodSeg154. 
With the model trained on FoodSeg103, we adapt it to the subset of FoodSeg154, the Asian food data set. Specifically, the Asia food set is evenly divided into the training and testing splits. 
We fine-tune the trained model on the training set and then run the model on the testing data.
In Table \ref{tab:adapt}, 
we show the performances of three models trained with the following settings:
1) without ReLeM, 2) with ReLeM and 3) with ReLeM and fine-tuned on the training split of the Asian food set. 
For the first two settings, we only evaluate the 62 classes in Asian food set overlapped with FoodSeg103, and for the last setting, we evaluate 112 classes (all).
From the results in Table~\ref{tab:adapt}, we observe that using ReLeM consistently outperforms baselines in both cases---with and without model fine-tuning on the training split of Asian food data. 



