\documentclass[11pt]{article}

\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{0.25in}
\setlength{\marginparwidth}{0.07 true in}
\setlength{\topmargin}{-0.5in}
\addtolength{\headsep}{0.25in}
\setlength{\textheight}{8.5 true in}
\setlength{\textwidth}{6.0 true in}
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{libertine}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{multicol}
\usepackage{booktabs}

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[algo2e,linesnumbered]{algorithm2e}
\usepackage{stackengine}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{multicol}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage[table]{xcolor}
\definecolor{mydarkred}{rgb}{0.6,0,0}
\definecolor{mydarkgreen}{rgb}{0,0.6,0}
\usepackage[colorlinks,
linkcolor=mydarkred,
citecolor=mydarkgreen]{hyperref}
\usepackage{url}
\usepackage{bm}
\usepackage{pifont}
\usepackage{stfloats}
\usepackage[T1]{fontenc}
\usepackage{arydshln}
\usepackage{colortbl}
\usepackage{balance}


\usepackage{subfig}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{asm}{Assumption}
\newtheorem{prop}{Proposition}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{s}{>{\hsize=.3\hsize}Y}
\newcolumntype{t}{>{\hsize=1.5\hsize}X}
\newcolumntype{u}{>{\hsize=0.8\hsize}Y}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{stfloats}


\title{Understanding and Improving Early Stopping for Learning with Noisy Labels}




\author{
  Yingbin Bai \footnotemark[1] ,
  Erkun Yang \footnotemark[1] ,
  Bo Han, \\
  Yanhua Yang, 
  Jiatong Li, 
  Yinian Mao,
  Gang Niu,
  Tongliang Liu, \\

  \small{The University of Sydney;}
  \small{The University of North Carolina at Chapel Hill.} \\
  \small{Hong Kong Baptist University;}
  \small{Xidian University}
  \small{Meituan Inc.}
  \small{RIKEN;}
}
\date{}
\begin{document}



\maketitle
\begin{abstract}
The memorization effect of deep neural network (DNN) plays a pivotal role in many state-of-the-art label-noise learning methods. To exploit this property, the early stopping trick, which stops the optimization at the early stage of training, is usually adopted. Current methods generally decide the early stopping point by considering a DNN as a whole. However, a DNN can be considered as a composition of a series of layers, and we find that the latter layers in a DNN are much more sensitive to label noise, while their former counterparts are quite robust. Therefore, selecting a stopping point for the whole network may make different DNN layers antagonistically affected each other, thus degrading the final performance. In this paper, we propose to separate a DNN into different parts and progressively train them to address this problem. Instead of the early stopping, which trains a whole DNN all at once, we initially train former DNN layers by optimizing the DNN with a relatively large number of epochs. During training, we progressively train the latter DNN layers by using a smaller number of epochs with the preceding layers fixed to counteract the impact of noisy labels. We term the proposed method as progressive early stopping (PES). Despite its simplicity, compared with the early stopping, PES can help to obtain more promising and stable results. Furthermore, by combining PES with existing approaches on noisy label training, we achieve state-of-the-art performance on image classification benchmarks.
\end{abstract}

\footnotetext[1]{Equal contribution}


\newpage


\section{Introduction}
Deep networks have revolutionized a wide variety of tasks, such as image processing, speech recognition, and language modeling~\cite{goodfellow2016deep}, However, this highly relies on the availability of large annotated data, which may not be feasible in practice. Instead, many large datasets with lower quality annotations are collected from online queries~\cite{cha2012social} or social-network tagging~\cite{liu2011noise}. Such annotations inevitably contain mistakes or \emph{label noise}. As deep networks have large model capacities, they can easily memorize and eventually overfit the noisy labels, leading to poor generalization performance~\cite{zhang2016understanding}. Therefore, it is of great importance to develop a methodology that is robust to noisy annotations.


Existing methods on learning with noisy labels (LNL) can be mainly categorized into two groups: model-based and model-free algorithms. Methods in the first category main model noisy labels with the noise transition matrix~\cite{Patrini2017forward, Xu2019DMI, Xie2020UDA, xia2020parts} and use it to correct loss functions. With perfectly estimated noise transition matrix, models trained with corrected losses can approximate to the models trained with clean labels. However, current methods are usually fragile to estimate the noise transition matrix for heavy noisy data and are also hard to handle a large number of classes~\cite{han2018co}. The second type explores the dynamic process of optimization policies, which relates to the memorization effectif clean labels are of majority within the noisy labels for each class, deep networks tend to first memorize and fit majority (clean) patterns and then overfit minority (noisy) patterns~\cite{Arpit2017Look}. 

Recently, based on this phenomenon, many methods~\cite{han2018co, tanaka2018joint, Wang2018Iterative, Li2020DivideMix, Liu2020ELR} have been proposed and achieved promising performance. To exploit the memorization effect, when the double descent phenomenon \cite{belkin2019reconciling,nakkiran2020deep,ishida2020we} cannot be guaranteed to occur, a core issue is to study when to stop the optimization of the network. While stopping the training for too few epochs can avoid overfitting to noisy labels, it can also make the network underfit to clean labels. Current methods~\cite{tanaka2018joint, Nguyen2020SELF} usually adopt an early stopping strategy, which decides the stopping point by considering the network as a whole. However, since DNNs are usually optimized with stochastic gradient descent (SGD) with backpropagation, supervisory signals will gradually propagate through the whole network from latter layers to former layers.  
Noting that the latter layers are closer to  noisy signals, we hypothesize that  noisy labels may have more severe impacts for the latter layers, which is different from current understanding~\cite{Li2020DivideMix, Liu2020ELR} that the network should be optimized all at once. 


To empirically verify the above hypothesis, we analyze the impact of noisy labels on representations from different layers with different training epochs. To quantitatively measure the impact of noisy labels from intermediate layers, we first train the whole network on noisy data with different training epochs and fix the parameters for the selected layer and previous layers. We then  reinitialize and optimize the rest layers with clean data, and the 
final classification performance is adopted to evaluate the impact of noisy labels. For the final layer, we directly report the overall classification performance. As illustrated in Figure~\ref{fig:1}, we can see that latter layers always achieve the best performance at relatively smaller epoch numbers and then exhibit stronger performance drops with additional training epochs, which verifies the hypothesis that noisy data may have more severe impacts for latter layers. 
 With this understanding, we can infer that the early stopping, which optimizes the network all at once, may fail to fully exploit the memorization effect and induce sub-optimal performance.

To address the above problem, we propose to optimize a DNN by considering it as a composition of several DNN parts and present a novel progressive early stopping (PES) method.  Specifically, we initially train former DNN layers by optimizing them with a relatively large number of epochs. Then, to alleviate the impact of noisy labels for latter layers, we reinitialize and progressively train latter DNN layers by using smaller numbers of epochs with preceding DNN layers fixed. Since different layers are progressively trained with different early stopping epochs, we term the proposed method as progressive early stopping (PES).  Despite its simplicity, compared with normal early stopping trick, PES can help to better exploit the memorization effect and obtain more promising and stable results. Moreover, since the model size and training epochs are gradually reduced during the optimization procedure, the training time of PES is only slightly greater than that of the normal early stopping. Finally, by combining PES with existing approaches on noisy label training tasks, we establish new state-of-the-art (SOTA) results on CIFAR-10 and CIFAR-100 with synthetic noise. We also achieve competitive results on one dataset with real-world noise: Clothing-1M~\cite{Xiao2015Clothing}.

\begin{figure}[!t]
\vspace{-10px}
\centering
\subfloat[Symmetric 50\%]{{\includegraphics[width=0.33\textwidth]{images/sym.png}}}
\subfloat[Pairflip 45\%]{{\includegraphics[width=0.33\textwidth]{images/pair.png}}}
\subfloat[Instance 40\%]{{\includegraphics[width=0.33\textwidth]{images/inst.png}}}
\caption{We adopt ResNet-18 as the model and evaluate the impact of noisy labels on the representations from the -th layer, the -th layer, and the final layer. The curves present the mean of five runs and the
best performances are indicated with dotted vertical lines.} 
\label{fig:1}
\vspace{-10px}
\end{figure}


The rest of the paper is organized as follows. In Section~\ref{sec:3}, we first introduce the proposed progressive early stopping and then present the details of the proposed algorithm by combining our method with existing approaches on noisy label training tasks. Section~\ref{sec:exp} shows the experimental results of our proposed method.  Related works are
briefly reviewed in Section~\ref{sec:rel}. Finally, concluding remarks are given in Section~\ref{sec:con}.



\section{Proposed Method}
\label{sec:3}
Let  be the distribution of a pair of random variables , where  indicates the variable of instances,  is the variable of labels,  denotes the feature space, and  is the number of classes. In many real-world problems, examples independently drawn from the distribution  are unavailable. Before being observed, the clean labels are usually randomly corrupted into noisy labels. Let  be the distribution of the noisy example , where  indicates the variable of noisy labels. For label-noise learning, we can only access a sample set  independently drawn from . The aim is to learn a robust classifier from the noisy sample set that can classify test instances accurately. 

In the following, we first elaborate on the proposed progressive early stopping (PES). Then, based on PES, we provide a learning algorithm that learns with confident examples and semi-supervised learning techniques.

\subsection{Progressive Early Stopping}
When trained with noisy labels, if clean labels are of majority within each noisy class, deep networks tend to first fit clean labels during an early learning stage before eventually memorizing the wrong labels, which can be explained by the memorization effect.

Many current methods utilize this property to counteract the influence of noisy labels by stopping the optimization at an early learning phase. Specifically, a deep classifier can be obtained by optimizing the following objective function with a relatively small epoch number :

where  is a deep classifier with model parameters  and  is the cross-entropy loss. When trained with noisy data, early learning regularization (ELR) \cite{Liu2020ELR} reveals that, for the most commonly used cross-entropy loss,  the gradient is well correlated with the correct direction at the early learning phase. Therefore, with a properly defined small epoch number , the classifier can have higher accuracy than at initialization. While, if we continue to optimize the deep model after  epochs, the classifier will be able to memorize more noise labels. Therefore, it is critical to select a proper epoch number  to utilize the memorization effect and alleviate the influence of noisy labels.

Current methods typically select the epoch number  by considering the network as a whole. However, as Figure \ref{fig:1} makes clear, the impact of noisy labels on different DNN layers are different, which implies that the traditional early stopping trick, which optimizes the whole network all at once, may make different DNN layers to be antagonistically affected by each other, thus degrading the final model performance. 
\begin{figure}[!t]
\centering
\subfloat[Symmetric 50\%]{\includegraphics[width=0.33\textwidth]{images/sym_75.png}} 
\subfloat[Pairflip 45\%]{\includegraphics[width=0.33\textwidth]{images/pair_75.png}} 
\subfloat[Instance 40\%]{\includegraphics[width=0.33\textwidth]{images/inst_75.png}}
\caption{Performance of the traditional early stopping trick and the proposed PES on CIFAR-10 with different types of label noise. The lines present the mean of five runs.}
\label{fig:2}
\end{figure}


To this end, we propose to separate a DNN into different parts and progressively train layers in different parts with different training epochs. Specifically, assume that the whole network  can be constituted with  DNN parts


\indent where  is the -th DNN part and  is the corresponding output. The output of the last part  is the prediction. The network  can also be represented as . To counteract the impact of noisy labels, We initially optimize the parameter  for the first part by training the whole network for  epochs with the following objective

Then, we keep the obtained parameter  fixed and progressively learn the -th () DNN part with the parameters for preceding DNN parts fixed. The training procedure is conducted with   epochs by optimizing the following objective

We gradually optimize the -th DNN part with the obtained parameter  fixed, the optimization is continued until all the parameters have been optimized. As elaborated above, latter DNN parts are more sensitive to noisy labels than their former counterparts. Therefore, for the above initializing optimization in Eq.~\eqref{eq:3} and the following  steps of optimization in Eq.~\eqref{eq:4}, we gradually reduce the training epochs (i.e. ) to better exploit the memorization effect. 
After optimization, we can obtain the final network as . 
Since this model is obtained by progressively exploiting the early stopping strategies for different DNN parts, we term the proposed method as progressive early stopping (PES). 

To explicitly verify the effectiveness of the proposed PES method, we conduct several pilot experiments, which compare the traditional early stopping and the proposed PES with label noise from different types and different levels. The results are illustrated in Figure~\ref{fig:2}, from which we can see that, compared with models trained with traditional early stopping,  models trained with PES can achieve superior classification accuracy with smaller variations in all the cases. Current state-of-the-art methods~\cite{Li2020DivideMix} usually adopt these models as base models to distill confident examples and then utilize semi-supervised learning techniques by considering confident examples as labeled data and other noisy examples as unlabeled data to further improve the results. The final performance still heavily relies on the base model trained with noisy labels. By improving the performance of the base model, our method combined with semi-supervised learning techniques is able to establish new state-of-the-art results. In the following subsections, we will elaborate on how to utilize PES to distill confident examples and further combine it with semi-supervised learning techniques.

\subsection{Learning with Confident Examples}
Based on the deep network optimized with progressive early stopping, we can select confident examples to facilitate the model training. Here, confident examples refer to examples that have high probabilities with clean labels. In this paper, we treat examples whose predictions are consistent with given labels as confident examples. In addition, to make the results more robust,  we generate two different augmentations for any given input and use the average prediction to decide its predicted label.  Formally, we can obtain the confident example set  as




where  indicates normal data augmentation operation including horizontal random flip and random crops, and  is the predicted probability of  belonging to class . Note that  is a stochastic transformation, so the two terms in Eq \eqref{eq:5} are not identical. The average prediction of augmented examples provides a more stable prediction and is found empirically to improve performance. After obtaining the confident example set, one can easily train a classifier by considering confident examples as clean data. However, since the number of confident examples for different classes can vary greatly, directly training the model with the obtained confident example set may introduce a severe class imbalance problem. To this end, we adopt a weighted classification loss

\indent where  is the corresponding class weight. Assuming that   denotes  the cardinality of the confident example  set belonging to the -th class. Then, we can set  to indicate the corresponding class importance.


\subsection{Combining with Semi-Supervised Learning}
\label{sec:2.3}
Training with only confident examples neglects the rest data and may suffer from insufficient training examples. To tackle this problem, we further resort to semi-supervised learning techniques by considering confident examples as labeled data and other noisy examples as unlabeled data. Specifically, the labeled data set and unlabeled data set can be obtained as

\indent where the labeled data set  is the same as that in Eq \eqref{eq:6}, and  is the rest unlabeled data set. Similar to~\cite{Li2020DivideMix}, we adopt MixMatch~\cite{Berthelot2019MixMatch} as the semi-supervised learning framework to train the final classification models. For more details about semi-supervised learning, we refer to ~\cite{Berthelot2019MixMatch}. The whole learning algorithm is summarized in Algorithm~\ref{algorithm1}.


\begin{algorithm}[!tp]
{\bfseries Input}:  Neural network with trainable parameters , Noisy training dataset , Number of training epochs for different part: , and training epochs  for refining with confident examples.

\For{ }{
     {Optimize network parameter}   with Eq.~\eqref{eq:3}; \\
}

\For{l = 2, \dots, L}{
 {Froze}  and re-initialize ; \\
\For{ }{
     {Optimize network parameter}   with Eq.~\eqref{eq:4}; \\
}
}
Unfroze ;\\

\For{ }{
{Extract} confident example set  and unlabeled set  with classifier  by Eq.~\eqref{eq:7};\\
	 Training the classifier  with MixMatch loss on  and ;}
     {Evaluate} the obtained classifier . \\
\caption{Progressive Early Stopping with Semi-Supervised Learning}
\label{algorithm1}
\end{algorithm}



\section{Experiments}
\label{sec:exp}
\subsection{Datasets and Implementation Details}
\label{sec:impl}
\textbf{Datasets:} We evaluate our method on two synthetic datasets, CIFAR-10 and CIFAR-100 \cite{krizhevsky2009CIFAR} with different levels of symmetric, pairflip, and instance-dependent label noise (abbreviated as instance label noise) and a real-world dataset Clothing-1M \cite{Xiao2015Clothing}. Both CIFAR-10 and CIFAR-100 contain 50k training images and 10k test images of size . Following previous works~\cite{han2018co, xia2019anchor, Liu2020ELR, xia2021robust}, symmetric noise is generated by uniformly flipping labels for a percentage of the training dataset to all possible labels. Pairflip noise flips noisy labels into their adjacent class. And, instance noise is generated by image features. More details about the synthetic label noise are given in the \emph{supplementary material}. For the flipping rate, it can include \cite{han2018co, xia2019anchor} or ex-include \cite{Li2020DivideMix, Liu2020ELR} true labels. We use the flipping rate including correct labels in Table \ref{tab:cifar_sym} to compare with results in \cite{Li2020DivideMix}, and use without correct labels in the rest of the experiments. Clothing-1M \cite{Xiao2015Clothing} is a large-scale dataset with real-world noisy labels, whose images are clawed from the online shopping websites, and labels are generated based on surrounding texts. It contains 1 million training images, and 15k validation images, and 10k test images with clean labels. \\

\renewcommand{\arraystretch}{1.15}
\begin{table}[!tp]
\fontsize{8.5}{10}\selectfont
\centering
\caption{Preliminary analysis of the performance and the quality of extracted confident examples on CIFAR-10. The mean and standard deviation are computed over five runs.}
\vspace{5pt}
\scalebox{1}{
\begin{tabular}{c | c | c | c | c | c | c}
\Xhline{2\arrayrulewidth}
	Metrics                         & Methods  & Sym-20\%         & Sym-50\%        & Pair-45\%       & Inst-20\%       & Inst-40\%          \\ \hline
\multirow{2}{*}{Test Accuracy} 	    & Early Stopping   & 82.552.46   & 70.761.24  & 60.625.59  & 84.410.90  & 74.732.65    \\ & PES              & \textbf{85.871.59}   & \textbf{75.871.33}  & \textbf{62.402.34}  & \textbf{86.580.45}  & \textbf{77.071.18}    \\ \hline
\multirow{2}{*}{Label Precision} 	& Early Stopping       & 98.810.15   & 94.650.19  & 72.535.26  & \textbf{98.700.43}  & \textbf{90.771.87}    \\ & PES              & \textbf{98.960.09}   & \textbf{95.460.14}  & \textbf{72.992.27}  & 98.520.19  & 90.630.92    \\ \hline
\multirow{2}{*}{Label Recall} 	    & Early Stopping   & 88.512.26   & 75.181.00  & 67.845.06  & 90.371.01  & 82.153.17    \\ & PES              & \textbf{92.671.43}   & \textbf{81.031.83}  & \textbf{71.062.27}  & \textbf{93.240.60}  & \textbf{85.910.68}    \\ \hline
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\label{tab:pre}
\end{table}


\noindent \textbf{Baselines:} Semi-supervised learning may strongly boost the performance, we separately compare our method with approaches with or without semi-supervised learning. For the comparison with baselines with semi-supervised learning, we combine our proposed method with MixMatch used in \cite{Li2020DivideMix} as indicated in Subsection~\ref{sec:2.3}. (1) Approaches without semi-supervised learning: Co-teaching \cite{han2018co}, Forward \cite{Patrini2017forward}, Joint Optim \cite{tanaka2018joint}, T-revision \cite{xia2019anchor}, DMI \cite{Xu2019DMI}, and CDR \cite{xia2021robust}. (2) Methods with semi-supervised learning: M-correction \cite{Arazo2019UnsupervisedLabel}, DivideMix \cite{Li2020DivideMix}, and ELR+ \cite{Liu2020ELR}. We also adopt cross-entropy and MixUp \cite{Zhang2017MixUp} as baselines.

\textbf{Network structure and optimization:} Our method is implemented by PyTorch v1.6. Baseline methods are implemented based on public codes with hyper-parameters set according to the original papers. For DivideMix and ELR+, we evaluate the test accuracy with the first network. To better demonstrate the robustness of our algorithm, we keep the hyper-parameters fixed for different types of label noise. More technique details are given in the \emph{supplementary material}.

For experiments without semi-supervised learning, we follow \cite{xia2019anchor}, and use ResNet-18 \cite{He2016ResNet} for CIFAR-10 and ResNet-34 for CIFAR-100. we adopt SGD with  momentum as the optimizer and train the network for 200 epochs. The initial learning rate is set as  and decayed with a factor of  at the 100th and 150th epoch respectively, and weight decay set . We split networks into three parts, the layers above block 4 as part 1, the block 4 of ResNet as part 2, and the final layer as part 3. We set  as 25 for CIFAR-10 and 30 for CIFAR-100,  as 7, and  as 5 and employ Adam optimizer with a learning rate of  for  and . 

For experiments with semi-supervised learning, we follow the setting of \cite{Li2020DivideMix} with PreAct Resnet-18. For hyper-parameters from MixMatch, we set them according to the original paper \cite{Berthelot2019MixMatch}. The network is trained for  epochs. For optimization, we use a single cycle of \textit{cosine annealing} \cite{Loshchilov2017CosineLr} with the learning rate begins from  and ends at  . We set the final layer as part 2, the rest as part 1. We set  as 20 for CIFAR-10 and 35 for CIFAR-100, and  as 5, and employ Adam optimizer with a learning rate of  for . 

For Clothing-1M \cite{Xiao2015Clothing}, we follow the previous work \cite{tanaka2018joint}, and employ a ResNet-50 \cite{He2016ResNet} pre-trained on ImageNet~\cite{krizhevsky2012imagenet}. We train the network with CE loss for 6 epochs and use SGD with  momentum as the optimizer with a weight decay of . The initial learning rate is  and is decayed by a factor of  at the 3rd and 4th epoch respectively. We set the final layer as part 2, the rest as part 1. We set  as 3, and  as 3, and employ Adam optimizer with a learning rate of  for . 

\subsection{Preliminary Experiments}
In Figure \ref{fig:2}, we can observe that with the PES trick, the performance of classifiers is generally improved compared with that the traditional early stopping trick. In this section, we further carefully analyze the quality of extracted labels by examining them from three aspects, i.e., test accuracy, label precision, and label recall. Here, label precision indicates the ratio of the number of extracted confident examples with correct labels in the total confident example set, and label recall represents the ratio of the number of confident examples with correct labels among the total correctly labeled examples. Specifically, we train a neural network on CIFAR-10  with different kinds and levels of label noise for  epochs respectively and report the performance for each case before and after the proposed PES is applied. 

Results in Table \ref{tab:pre} clearly show that, compared with the early stopping, PES can help to obtain higher accuracies, precisions, and recalls for most cases.  For instance-dependent label noise, PES can achieve higher recall values with comparable label precision values. Note that models with high recall values can help to collect more confident examples, which is critical for learning with confident examples and semi-supervised learning. Therefore, by enhancing the performance of the initial model, PES can help to improve the final classification performance in all cases, which is also verified by the experiments in Section~\ref{sec:3.3}. 

\subsection{Classification Accuracy Evaluation}
\label{sec:3.3}
\textbf{Synthetic datasets.}  We first verify the effectiveness of our proposed method without semi-supervised learning techniques on two synthetic datasets: CIFAR-10 and CIFAR-100. For both of these two datasets, we leave 10\% of data with noisy labels as noisy validation set. Results are presented in Table \ref{tab:cifar_wo}, which shows that our proposed  method can consistently outperform all other baselines across various settings by a large margin. 


\begin{table}[!tp]
\centering
\fontsize{8.5}{10}\selectfont
\caption{Comparison with state-of-the-art methods without semi-supervised learning on CIFAR-10 and CIFAR-100. The mean and standard deviation computed over five runs are presented.}
\vspace{5pt}
\scalebox{1}{{
\begin{tabular}{c | c | c | c | c | c | c}
\Xhline{2\arrayrulewidth}
\multirow{2}{*}{Dataset}  & \multirow{2}{*}{Method}             &  \multicolumn{2}{c |}{Symmetric}       & Pairflip             & \multicolumn{2}{c }{Instance}            \\ \cline{3-7}
	                      &                 &     20\%          &     50\%           &   45\%               &     20\%          &   40\%      \\ \hline
\multirow{7}{*}{CIFAR10} & CE              & 84.000.66    & 75.511.24     & 63.346.03       & 85.100.68    & 77.002.17    \\ 
	                      & Co-teaching     & 87.160.11    & 72.800.45     & 70.111.16       & 86.540.11    & 80.980.39    \\ 
	                      & Forward         & 85.630.52    & 77.920.66     & 60.151.97       & 85.290.38    & 74.723.24    \\ 
	                      & Joint Optim     & 89.700.11    & 85.000.17     & 82.631.38       & 89.690.42    & 82.620.57    \\ 
	                      & T-revision      & 89.630.13    & 83.400.65     & 77.066.47       & 90.460.13    & 85.373.36    \\ 
	                      & DMI             & 88.180.36    & 78.280.48     & 57.6014.56      & 89.140.36    & 84.781.97    \\ 
	                      & CDR             & 89.720.38    & 82.640.89     & 73.670.54       & 90.410.34    & 83.071.33    \\ \cline{2-7}
                          & Ours                                & \textbf{92.380.40}  & \textbf{87.450.35}  & \textbf{88.431.08} & \textbf{92.690.44}  & \textbf{89.730.51} \\ 
                          \hline \hline
\multirow{7}{*}{CIFAR100} & CE            & 51.430.58    & 37.693.45     & 34.102.04       & 52.191.42      & 42.261.29    \\ 
	                      & Co-teaching   & 59.280.47    & 41.370.08     & 33.220.48       & 57.240.69      & 45.690.99    \\ 
	                      & Forward       & 57.750.37    & 44.661.01     & 27.880.80       & 58.760.66      & 44.500.72    \\ 
	                      & Joint Optim   & 64.550.38    & 50.220.41     & 42.610.61       & 65.150.31      & 55.570.41    \\    
	                      & T-revision    & 65.401.07    & 50.241.45     & 41.101.95       & 60.710.73      & 51.540.91    \\ 
	                      & DMI           & 58.730.70    & 44.251.14     & 26.900.45       & 58.050.20      & 47.360.68    \\ 
	                      & CDR           & 66.520.24    & 55.300.96     & 43.871.35       & 67.330.67      & 55.940.56    \\ \cline{2-7}
                          & Ours          & \textbf{68.890.45}  & \textbf{58.902.72}  & \textbf{57.181.44}  & \textbf{70.490.79} & \textbf{65.681.41}    \\ 
\Xhline{2\arrayrulewidth}
\end{tabular}
}}
\label{tab:cifar_wo}
\end{table}


\begin{table}[!tp]
\fontsize{8.5}{10}\selectfont
\centering
\caption{Comparison with state-of-the-art methods with semi-supervised learning on CIFAR-10 and CIFAR-100 with symmetric label noise from different levels. Results with * are token from \cite{Li2020DivideMix}. The mean and standard deviation are computed over three runs.}
\vspace{5pt}
\scalebox{1}{
{
\begin{tabular}{c | c | c | c | c | c | c }
\Xhline{2\arrayrulewidth}
	Dataset             &  \multicolumn{3}{c |}{CIFAR-10}                  & \multicolumn{3}{c}{CIFAR-100}                              \\ \hline
	Methods / Noise     & Sym-20\%       & Sym-50\%       & Sym-80\%       & Sym-20\%           & Sym-50\%          & Sym-80\%          \\ \hline

	CE					& 86.50.6   & 80.60.2   & 63.70.8   & 57.90.4       & 47.30.2      & 22.31.2      \\ 
	MixUp				    & 93.20.3   & 88.20.3   & 73.30.3   & 69.50.2       & 57.10.6      & 34.10.6      \\ 
	M-correction*       & 94.0           & 92.0           & 86.8           & 73.9               & 66.1              & 48.2              \\
	DivideMix*          & 95.2           & 94.2           & 93.0           & 75.2               & 72.8              & 58.3              \\
	DivideMix           & 95.60.1   & 94.60.1   & 92.90.3   & 75.30.1       & 72.70.6      & 56.40.3      \\ 
	ELR+                    & 94.90.2   & 93.60.1   & 90.40.2   & 75.50.2       & 71.00.2      & 50.40.8      \\ \hline
    Ours (Semi)         & \textbf{95.90.1}  & \textbf{95.10.2}   & \textbf{93.10.2} & \textbf{77.40.3} & \textbf{74.30.6}  & \textbf{61.60.6}      \\ 
\Xhline{2\arrayrulewidth}
\end{tabular}
}}
\label{tab:cifar_sym}
\end{table}




\begin{table}[!tp]
\fontsize{8.5}{10}\selectfont
\centering
\caption{Comparison with state-of-the-art methods with semi-supervised learning on CIFAR-10 and CIFAR-100 with instance-dependent and pairflip label noise from different levels. The mean and standard deviation are computed over three runs.}
\vspace{5pt}
\scalebox{1}{
\begin{tabular}{c | c | c | c | c | c| c} 
\Xhline{2\arrayrulewidth}
	Dataset          &  \multicolumn{3}{c |}{CIFAR-10}                  & \multicolumn{3}{c}{CIFAR-100}                              \\ \hline
	Methods / Noise  & Inst-20\%       & Inst-40\%      & Pair-45\%    & Inst-20\%        & Inst-40\%      & Pair-45\%       \\ \hline
	CE               & 87.50.5    & 78.90.7   & 74.91.7 & 56.80.4     & 48.20.5   & 38.50.6    \\
	MixUp            & 93.30.2    & 87.60.5   & 82.41.0 & 67.10.1     & 55.00.1   & 44.20.5    \\
	DivideMix        & 95.50.1    & 94.50.2   & 85.61.7 & 75.20.2     & 70.90.1   & 48.21.0    \\
	ELR+             & 94.90.1    & 94.30.2   & 86.11.2 & 75.80.1     & 74.30.3   & 65.31.3    \\ \hline
	Ours (Semi)      & \textbf{95.90.1}  & \textbf{95.30.1}   & \textbf{94.50.3}  & \textbf{77.60.3} & \textbf{76.10.4} & \textbf{73.61.7} \\
\Xhline{2\arrayrulewidth}
\end{tabular}
}
\label{tab:cifar_asym}
\end{table}




Table \ref{tab:cifar_sym} and Table \ref{tab:cifar_asym} present the mean accuracy and standard deviation for our method and all baselines on CIFAR-10 and CIFAR-100, respectively. From the results, we can get that the proposed method can outperform all baselines in all cases. For pairflip label noise, the advantages of our proposed method become more apparent, and it significantly outperforms state-of-the-art methods by over 8\% on both CIFAR-10 and CIFAR-100. These empirical results support our proposal that PES can improve the quality of selected confident examples, which helps improve performance and reduce the variance of the final classifier. \\


\begin{wrapfigure}{r}{0.3\textwidth}
\centering
\vspace{-20pt}
\subfloat[]{
{\includegraphics[width=0.3\textwidth]{images/ablation.png}}
\label{fig:ablation}
}
\\
\subfloat[]{
{\includegraphics[width=0.3\textwidth]{images/ablation2.png}}
\label{fig:ablation2}
}
\caption{Sensitivity analysis for different training iteration numbers:  and .} 
\vspace{-40pt}
\end{wrapfigure}


\noindent \textbf{Real-world dataset.} We evaluate the performance of the proposed method on a real-world dataset with Clothing-1M~\cite{Xiao2015Clothing} and select methods such as CE, Forward, Joint-Optim, DMI, and T-revision, which use a single network, and also methods such as DivideMix and ELR+, which adopt an ensemble model with two different networks, as baselines. We also report the results for the proposed PES with a single network as \emph{ours} and the results for PES, which ensembles two networks, as \emph{ours*}. The overall results are reported in Table \ref{tab:clothing}, from which we can observe that the proposed PES with a single network can outperform all baselines using a single network. And with an ensemble model, which contains two different networks, our method can outperform all the adopted baselines. These results clearly demonstrate that, by improving the performance of the initial classification network, our method is more flexible to handle such real-world noise problems.

\begin{table}[!tp]
\centering
\fontsize{8.5}{10}\selectfont
\label{tab:cifar100}
\caption{Compassion with state-of-the-art methods on Clothing-1M. Results of baseline methods are taken from the original papers. ours represent the results obtained by PES with a single network and ours* indicate the results obtained by PES with an ensemble model. }
\vspace{5pt}
\scalebox{1}{
{
\begin{tabular}{ c | c | c | c | c | c | c | c | c}
\Xhline{2\arrayrulewidth}
    CE       & Forward  &  Joint-Optim  &   DMI    &  T-revision & DivideMix* & ELR+*  &  Ours          &  Ours*         \\ \hline 
    69.21    & 69.84    &  72.16        & 72.46    &  74.18      & 74.76      & 74.81  & \textbf{74.56} & \textbf{74.85} \\
\Xhline{2\arrayrulewidth}
\end{tabular} 
}}
\label{tab:clothing}
\end{table}




\subsection{Sensitivity Analysis}
\label{sec:abl}
In this section, we investigate the parameter sensitivity for the training iteration number  and , respectively. We firstly analyze the training iteration number for the second DNN part by varying  from the range of . The results are illustrated in Figure \ref{fig:ablation}, from which can find that, with the increasing of , the performance of PES first increase and then decrease in all the cases except for 45\% Pairflip noise on the CIFAR-10 dataset. While the model achieves the best performance with  as  for all types of noisy labels.  Then we fix  as 7, and analyze the impact of  the third DNN part by varying  from the range of . The results are shown in Figure \ref{fig:ablation2}. Although the performance variance for different  is smaller than that for , we can still observe that the best performance can be obtained when  is set as . More importantly, from these two figures, we can get that both  and  are robust to the different types of noisy labels. 




\subsection{Training Time Comparison}
\label{subsec:time}
In this section, we compare the training time of our method and other state-of-the-art baselines. All the experiments are conducted on a server with a single Nvidia V100 GPU. The training times for all the methods are reported in Table \ref{tab:training_time}, from which we can get that our algorithm with cross-entropy loss achieves the fastest speed across all baselines, only about 1 hour. Our method combining with MixMatch \cite{Berthelot2019MixMatch} is also fast, only a little more than half of the training time of DivideMix. The time of ELR+ \cite{Liu2020ELR} shows superior, but ELR+ trains the network with fewer epochs, with 200 epochs compared with ours for 300 epochs. 



\begin{table}[!tp]
\fontsize{8.5}{10}\selectfont
\centering
\caption{Training time comparison for baselines on CIFAR-10 with 50\% Symmetric label noise.}
\label{tab:training_time}
\vspace{5pt}
\scalebox{1}{{
\begin{tabular}{c | c | c | c | c | c | c | c}
\Xhline{2\arrayrulewidth}
	CE    & Co-teaching   & CDR         & T-revision  & ELR+     & DivideMix & Ours  & Ours (Semi)  \\ \hline
	0.9h  & 1.5h          & 3.0h        & 3.5h        & 2.2h     & 5.5h      & 1.0h  & 3.1h         \\
\Xhline{2\arrayrulewidth}
\end{tabular} 
}}
\vspace{-10pt}
\end{table}

\section{Related work}
\label{sec:rel}
Learning with noisy data has been well studied \cite{liu2016reweighting, Goldberger17AdaptLayer, Ma18Dimensionality, Wang2019Symmetric, Ma2020NormalizedLoss, xia2021robust}. Current works can be mainly categorized into two groups: model-based and model-free methods. In this section, we briefly review some closely related works. 


The first type models the relationship between clean labels and noisy labels by estimating the noise transition matrix and build a loss function to correct the loss \cite{Patrini2017forward, xia2020parts, Yao2020Dual, wu2020class2simi}. \cite{Patrini2017forward} first combines algorithms for estimating the noise rates and loss correction techniques together and introduces two alternative procedures for loss correction. It also proves that both of the two procedures enjoy formal robustness guarantees \emph{w.r.t.} the clean data distribution. DMI \cite{Xu2019DMI} proposes an information-theoretic loss function, which utilizes Shannon’s mutual information and is robustness to different kinds of label noise. T-revision \cite{xia2019anchor} estimates the noise transition matrix without anchor points by adding a fine-tuned slack variables. Although these methods have made certain progress, they are usually fragile to estimate the noise transition matrix for heavy noisy data and are also hard to handle a large number of classes. Therefore, in this paper, we mainly focus on the model-free methods.

The second strand mainly counteracts noisy labels by exploiting the memorization effect that deep networks tend to first memorize and fit majority (clean) patterns and then overfit minority (noisy) patterns~\cite{Arpit2017Look}. To exploit this property,  Co-teaching \cite{han2018co} employs two networks with different initialization and uses \textit{small loss} to select confident examples.  M-correction \cite{Arazo2019UnsupervisedLabel} uses two Gaussian Mixture Models to identify confident examples, instead of using networks themselves. DivideMix \cite{Li2020DivideMix} extends Co-teaching \cite{han2018co} and employs two Beta Mixture Model to select confident examples. MixMatch \cite{Berthelot2019MixMatch} is then adopted to  leverage unconfident examples with a semi-supervised learning framework. All the above methods exploit the memorization effect by considering the adopted network as a whole. Recently, \cite{Jimmy2020GoodRepr} shows that networks training with noisy labels can produce good representations, if the structure of networks suits the targeted tasks. Our method further explains that noisy labels have different impacts for different layers in a DNN. And latter layers will receive earlier and more severe impact than their former counterparts. Therefore, by considering a DNN as a composition of several layers and training different layers with different epochs, our method is able to better exploit the memorization effect and achieve Superior performance. 



\section{Conclusion}
\label{sec:con}
In this work, we provide a progressive early stopping (PES) method to better exploit the memorization effect of deep neural networks (DNN) for noisy-label learning. We first find that the impact of noisy labels for former layers in a DNN is much less and later than that for latter DNN layers, and then build upon this insight to propose the PES method, which separates a DNN into different parts and progressively train each part to counteract the different impacts of noisy labels for different DNN layers. To show that PES can boost the performance of state-of-the-art methods, we conduct extensive experiments across multiple synthetic and real-world noisy datasets and demonstrate that the proposed PES can help to obtain substantial performance improvements compared to current state-of-the-art baselines. 
The main limitation of our method lies in that, by splitting a DNN into different parts, PES introduces several additional hyper-parameters that need to be tuned carefully. 
In the future, we will extend the work in the following aspects. First, we will study other mechanisms that distinguishing desired and undesired memorization rather than early stopping, e.g., the gradient ascent trick \cite{han2020SIGUA}. Second, we are interested in combining PES with interesting ideas from semi-supervised learning and unsupervised learning.




\newpage
\bibliographystyle{plainnat}
\bibliography{bib}


\newpage
\appendix

\section{Training details}
In this section, we first provide details about the adopted three kinds of noisy labels. Then, we elaborate on the data preprocessing and the hyperparameter settings in our experiments.

\subsection{Definition of noise}
According to different correlations between noisy labels and clean labels, there are three kinds of widely used label noise, namely symmetric class-dependent label noise, pairflip class-dependent label noise, and instance-dependent label noise~\cite{Patrini2017forward, han2018co, xia2020parts}. In the following, we first introduce one basic concept: transition matrix~\cite{Patrini2017forward}, and then provide the details for all the three kinds of label noise, respectively. 

\noindent \textbf{Transition matrix:}\ \  The \textit{transition matrix}  is used to explicitly model the generation process of label noise, where  is the flip rate between the true label  and noisy label on given
data .  is the variable of instances,  is the variable of clean labels, and  is the variable of noisy labels.  is the -th entry of the transition matrix , which denotes the probability of the instance  with clean label  being observed with a noisy label .

\noindent \textbf{Symmetric class-dependent label noise:}\ \ Symmetric class-dependent label noise is generated with symmetric class-dependent noise transition matrices. We set the flip rate . Random flipping labels may change to true labels, so the flip rate may include or exclude true labels. For the flip rate excluding true labels, the diagonal entries of symmetric transition matrix are  and the off-diagonal entries are . For the flip rate including true labels, the diagonal entries of symmetric transition matrix are  and the off-diagonal entries are .

\noindent \textbf{Pairflip class-dependent label noise:}\ \ The label noise is generated with pairflip class-dependent noise transition matrices, which is defined as follow. Let flip rate is . The diagonal entries of a pairflip transition matrix are  and their adjacent class is .

\noindent \textbf{Instance-dependent label noise:} We generate the instance-dependent label noise according to  Algorithm \ref{alg:noise}. More details about this algorithm can be found in \cite{xia2020parts}.

\begin{algorithm}[h!]
 {\bfseries Input}: Clean samples ; Noise rate .
 
	1: Sample instance flip rates  from the truncated normal distribution ;
	
	2: Independently sample  from the standard normal distribution ;
	
	3: For  do
	
    4:\quad ; \hfill// generate instance-dependent flip rates
    
    5:\quad  ; \hfill// control the diagonal entry of the instance-dependent transition matrix
    
    6:\quad  ; \hfill// make the sum of the off-diagonal entries of the -th row to be 
    
    7:\quad  ; \hfill// set the diagonal entry to be 
    
    8:\quad  Randomly choose a label from the label space according to possibilities  as noisy label ;
    
	9: End for.
	
{\bfseries Output}: Noisy samples 
\caption{Instance-dependent Label Noise Generation}
\label{alg:noise}
\end{algorithm}

\subsection{Data preprocessing and experimental settings}
\textbf{Data preprocessing:} For experiments on CIFAR-10/100 \cite{krizhevsky2009CIFAR} without semi-supervised learning, we use simple data augmentation techniques including random crop and horizontal flip. For experiments on CIFAR-10/100 with semi-supervised learning, except random cropping and horizontal flip, MixUp \cite{Zhang2017MixUp} is also employed, which is a critical component of MixMatch \cite{Berthelot2019MixMatch}. For Clothing-1M \cite{Xiao2015Clothing}, we first resize images to 256 × 256, and then random crop to 224 × 224, following a random horizontal flip. \\

\noindent \textbf{Hyper-parameters of semi-supervised learning}: We keep all the hyper-parameters fixed for different levels of noise, and only adjust  for different noisy settings, since the ratio of confident examples (labeled data) and unconfident examples (unlabeled data) can vary greatly for different noisy settings. Specifically, we set , , and  is chosen from .  begins with , and changes to  after 150th epoch. More details of hyper-parameters can be found in Table \ref{tab:hyper} and Table \ref{tab:lambda_u}. 

\begin{table}[!ht]
\centering
\caption{Training hyperparameters for CIFAR-10, CIFAR-100 \cite{krizhevsky2009CIFAR}, and Clothing-1M \cite{Xiao2015Clothing}}
\label{tab:hyper}
\scalebox{0.8}{
\begin{tabular}{c | c | c | c | c | c }
\Xhline{2\arrayrulewidth}
	                   & \multicolumn{2}{c |}{CIFAR-10} & \multicolumn{2}{c |}{CIFAR-100}        & Clothing-1M           \\ \hline
	architecture       & ResNet-18      & PreAct ResNet-18  & ResNet-34      & PreAct ResNet-18  & Pretrained Resnet-50  \\ 
	loss function      & CE             & MixMatch loss     & CE             & MixMatch loss     & CE                    \\   
	learning rate (lr) & 0.1            & 0.02              & 0.1            & 0.02              &     \\
	lr decay           & 100th \& 150th & Cosine Annealing  & 100th \& 150th & Cosine Annealing  & 3rd \& 4th            \\ 
	weight decay       &       & -                 &       & -                 &              \\ 
	batch size         & 128            & 128               & 128            & 128               & 128                   \\ 
	training examples  & 45,000         & 50,000            & 45,000         & 50,000            & 900,000               \\ 
	training epochs    & 200            & 300               & 200            & 300               & 6                     \\ 
	PES lr             &       &          &       &          &     \\ 
	              & 25             & 20                & 30             & 35                & 3                     \\ 
	              & 7              & 5                 & 7              & 5                 & 3                     \\ 
	              & 5              & -                 & 5              & -                 & -                     \\ 

\Xhline{2\arrayrulewidth}
\end{tabular} 
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Semi loss weight  for CIFAR-10 and CIFAR-100}
\label{tab:lambda_u}
\scalebox{0.8}{
\begin{tabular}{c | c | c | c | c | c | c }
\Xhline{2\arrayrulewidth}
	Datasets / Noise    & Sym-20\%  & Sym-50\%  & Sym-80\%  & Pairflip-45\%  & Inst-20\%  & Inst-40\%  \\ \hline
	CIFAR-10            & 5         & 15        & 25        & 5              & 5          & 15         \\ \hline
	CIFAR-100           & 50        & 75        & 100       & 50             & 50         & 50         \\
\Xhline{2\arrayrulewidth}
\end{tabular} 
}
\end{table}

\section{Additional experiments}
In this section, we provide more experimental results on CIFAR-100 and Fashion-MNIST to further verify the hypothesize that noisy labels may have more severe impacts on the latter layers. We also provide additional comparisons with baselines, which exploit ensemble networks.

In the first experiment, we adopt a dataset with more classes: CIFAR-100 and a deeper network: ResNet-34 \cite{He2016ResNet}. In addition, we adopt Fashion-MNIST \cite{Xiao2017Fashion}, including 60,000 training images with 28x28 size and LeNet \cite{Cun1989Handwritten}, which consists of two convolutional layers and three full-connected layers with ReLU activation. The learning procedure for CIFAR-100 and Fashion-MNIST is the same as that for CIFAR-10 in the paper. Specifically, we first train the whole network on noisy data with different training epochs. For the final layer, we directly report the overall classification performance. For other selected layers, we frozen the parameters for the selected layer and previous layers, and then reinitialize and optimize the rest layers with clean data, and the final classification performance is adopted to evaluate the impact of noisy labels. We do not use image augmentation techniques for Fashion-MNIST dataset.

\begin{figure}[!ht]
\vspace{-10pt}
\centering
\subfloat[Symmetric 50\%]{{\includegraphics[width=0.33\textwidth]{images/sym_cifar100.png}}}
\subfloat[Pairflip 45\%]{{\includegraphics[width=0.33\textwidth]{images/pair_cifar100.png}}}
\subfloat[Instance 40\%]{{\includegraphics[width=0.33\textwidth]{images/inst_cifar100.png}}}
\caption{We adopt ResNet-34 as the model on CIFAR-100 and evaluate the impact of noisy labels on the representations from the -th layer, the -th layer, and the final layer. The curves present the mean of five runs and the
best performances highlight with dotted vertical lines.}
\label{fig:4}
\vspace{-10pt}
\end{figure}

\begin{figure}[!ht]
\centering
\subfloat[Symmetric 50\%]{{\includegraphics[width=0.33\textwidth]{images/sym_fashion.png}}}
\subfloat[Pairflip 45\%]{{\includegraphics[width=0.33\textwidth]{images/pair_fashion.png}}}
\subfloat[Instance 40\%]{{\includegraphics[width=0.33\textwidth]{images/inst_fashion.png}}}
\caption{We adopt LeNet as the model on Fashion-MNIST and evaluate the impact of noisy labels on the representations from the 1-st layer, the -th layer, and the final layer. The curves present the mean of five runs and the best performances highlight with dotted vertical lines. Note that vertical lines are merged together for the 1-st layer and 4-th layer on Symmetric 50\%, and vertical lines of the 1-st layer and the final layer are merged together on Instance 40\%.}
\label{fig:5}
\vspace{-10px}
\end{figure}


Figure \ref{fig:4} and Figure \ref{fig:5} demonstrate the impacts of noisy labels on different layers on CIFAR-100 and Fashion-MNIST, respectively. From Figure \ref{fig:4}, we can see that the drop of the green line (the final layer) is the largest, the blue line (the 33-th layer) has a gradual decline, and the orange line (the 17-th layer) is relatively stable during the training process. These observations are similar to those for CIFAR-10. The performance of 17-th layer in ResNet-34 is affected by noisy labels later and less than that of the 9-th layer in ResNet-18. It is because there are more layers after the 17-th layer in ResNet-34 than the 9-th layer. Similar trends are observable in Figure \ref{fig:5}. The first layer is nearly unaffected by noisy labels, and the performance of the final layer has a larger decline compared with the 4-th layer. The learning speeds of different layers are unapparent in LeNet, since there are only three hidden layers in LeNet, and the gradient of losses transfers much easier compared with deeper networks. Another reason may be the simplicity of patterns in Fashion-MNIST without image augmentations, which leads the convolutional layers to learn fast.


\begin{table}[!ht]
\centering
\caption{Comparison with state-of-the-art methods using ensemble two networks and semi-supervised learning on CIFAR-10 and CIFAR-100 with symmetric label noise from different levels. Baseline results are taken from \cite{Li2020DivideMix} and \cite{Liu2020ELR}. The highest results are reported for all the methods.}
\vspace{5pt}
\scalebox{1}{
{
\begin{tabular}{c | c | c | c | c | c | c }
\Xhline{2\arrayrulewidth}
	Dataset             &  \multicolumn{3}{c |}{CIFAR-10}                  & \multicolumn{3}{c}{CIFAR-100}                              \\ \hline
	Methods / Noise     & Sym-20\%       & Sym-50\%       & Sym-80\%       & Sym-20\%           & Sym-50\%          & Sym-80\%          \\ \hline
	CE                  & 87.2           & 80.9           & 65.8           & 58.1               & 47.5              & 23.6              \\
	MixUp               & 93.5           & 88.4           & 73.6           & 69.7               & 57.9              & 34.69             \\
	DivideMix*          & \textbf{96.1}  & 94.6           & 93.2           & 77.3               & 74.6              & 60.2              \\
	ELR+                & 95.8           & 94.8           & \textbf{93.3}  & 77.6               & 73.6              & 60.8              \\ 
    Ours (Semi)         & \textbf{96.1}  & \textbf{95.3}  & \textbf{93.3}  & \textbf{77.7}      & \textbf{74.9}     & \textbf{62.3}     \\
\Xhline{2\arrayrulewidth}
\end{tabular}
}}
\label{tab:cifar_sym_highest}
\end{table}

In the paper, we compare our results with baselines evaluated with a single network. In this section, we compare our method with state-of-the-art methods with ensemble two networks taken from the original papers \cite{Li2020DivideMix, Liu2020ELR}. We also adopt cross-entropy and MixUp \cite{Zhang2017MixUp} with a single network as baselines. From Table \ref{tab:cifar_sym_highest}, we can observe our results with a single network are comparable to results of baselines with ensemble two networks. Specifically, on CIFAR-100, our method outperforms state-of-the-art methods across all settings.




\end{document}
