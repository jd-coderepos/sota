In this section, we present and explain our reasoning behind the chosen techniques for our search space, inspired by many recent design decisions, including ConvNeXt by \citet{LiuEtAl2022} and the re-study of ResNet by \citet{WigEtAl2021}, and extending those ideas with other innovations.

The search space is built with the PyTorch library \citep{PasEtAl2019} and \textit{timm} \citep{Wightman2019}. We apply SMAC \citep{LindauerEtAl2022} automated algorithm configurator to find a good configuration of improvements. 
Due to the considerable training time of a full network on ImageNet, we test the configurations with a reduced network, with four blocks in its main part instead of eight, and on a smaller dataset (\mbox{CIFAR-100} \citep{KrizEtAl2009})\footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}}, with only one downsampling block in the entry flow. 
This allows trials of as many configurations as possible within 3 days on a single RTX 3090 Ti. The search space containing the combinations of parameters has more than fifty thousand different possible configurations. We optimize the architecture to \mbox{maximize accuracy.}

Our search space consists of various kernel sizes (3, 5, 7, 9), stem types (convolutional stem or patchify stem), different pooling types (max pooling, convolutional downsampling layer or blur pool),  whether to implement bottleneck in the middle flow, or to add Squeeze-and-Excitation at the end of each block. Moreover, we experiment with various positions and types of activation functions (ReLU, GELU, ELU or CELU) and with different positions and types of normalization methods (batch normalization or layer normalization). 

We performed several preliminary experiments to find the optimal training procedure. However,
we find that existing ones perform better on the final models. Therefore, we use similar training procedures to the ones created by the authors of \citet{WigEtAl2021} for the ResNet network. For more details about training procedure parameters, see Table \ref{tab:training-procedure-parameters} and the comparison between Xception and NEXcepTion architectures in Tables \ref{tab:comparison:Xception_nexception-t} and \ref{tab:comparison:Xception_nexception-s}, all in the Appendix section.


\subsection{Training Procedures}

\textbf{Stochastic Depth.}
The original Xception network performs regularization by adding a Dropout layer before the classification layer.
Stochastic depth \citep{HuangSun2016} changes the network depth during training by randomly bypassing groups of blocks and using the entire network for inference. Consequently, training time is reduced substantially and accuracy is improved introducing regularization into the network.

\textbf{Optimizer.}
In our paper, we choose \textit{Layer-wise Adaptive Moments optimizer for Batch training} (LAMB) optimizer inspired by \citet{YouEtAl2019}. As stated by \citet{WigEtAl2021}, LAMB optimizer increases the efficiency and performance of the network, in comparison to other common optimizers, like AdamW in \citet{LiuEtAl2022}, LAMB performs more accurate updates of the learning rate.


\textbf{Data Augmentation.}
While the original Xception model was trained without any data augmentation methods, newer training procedures utilize multiple techniques, which improve generalization. In our NEXcepTion model, we apply Rand Augment \citep{CubEtAl2019} that performs a few random transformations, Mixup \citep{ZhaEtAl2018} and CutMix \citep{YunEtAl2019}, which merge images, see Table \ref{tab:training-procedure-parameters} for specific values.

\textbf{Learning Rate Decay.}
Similarly to recent models such as DeiT \citep{TouEtAl2020}, we adopt cosine annealing \citep{LosEtAl2018} with warmup epochs. 
This method initially sets a low learning rate value, which gradually increases during the warmup epochs. Then, the learning rate is gradually reduced using the cosine function to achieve rapid learning. 

\subsection{Structural changes}
\textbf{``Soft'' Patchify Stem.}
Patchify layers are characterized by large kernel sizes and non-overlapping convolutions (by setting the stride and the kernel size to the same value). Inspired by this design, we add a  patchify layer to the search space, which we consider a ``soft'' patch, different from the aggressive  solution proposed by \citet{DosEtAl2020} in the Transformer schema and the  from ConvNeXt \citep{LiuEtAl2022}.
We use the initial block with kernel  and stride 2 to match the original Xception network and to fit the output size. 
This stem is adapted to the reduced resolution of the input images, similarly to the efficient configuration introduced by \citet{CorEtAl2019}.

\textbf{Bottleneck.}
The idea of inverted bottleneck was introduced by \citet{SandlerZhu2018} and has been prevalent in modern attention-based architectures, significantly improving performance. 
The Xception architecture does not feature a bottleneck and has a constant number of channels through the middle flow of the network. In the NEXcepTion architecture, we introduce a bottleneck in the middle flow blocks, as proposed by \citet{LiuEtAl2022}, see Figure \ref{fig:NEXcepTion Block and Xception Block}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/both_blocks.pdf}
    \caption{NEXcepTion block (left) and Xception block (right).}
    \label{fig:NEXcepTion Block and Xception Block}
\end{figure}

\textbf{Larger Kernels.}
Inspired by \citet{LiuEtAl2022}, among others, we pick larger kernels for our experiments, and we achieve the best accuracy with their size set to 5. Combining this idea with bottleneck blocks and the reduced resolution allows using bigger kernels without excessive increase in the computational demand.

\newpage
\textbf{Squeeze-and-Excitation Block.}
Squeeze-and-Excitation block (SE block) from \citet{HuEtAl2017} improves channel interdependencies with an insignificant decrease in efficiency by recalibrating the feature responses channel-wise, creating superior feature maps. SE blocks provide significant performance improvement and are easy to include in existing networks as specified by \citet{HuEtAl2017}. 

\textbf{Fewer Activations and Normalizations.}

Similarly to \citet{LiuEtAl2022}, we employ less activation layers than in the original Xception network. Fewer activation layers is a distinctive property of the state-of-the-art Transformer blocks and, by replicating this concept, we can achieve higher accuracy.

Moreover, what is also inherent to Transformers architectures, is fewer normalization layers than in typical convolution-based solutions. It is important to mention that in the original Xception architecture, all convolutional layers are followed by batch normalization. 



\textbf{Activation Function.} 
Concerning neuron activations, GELU \citep{HenGim2016} is used in modern Transformer architectures like BERT \citep{DevEtAl2018} and recent convolutional-based architectures like ConvNeXt \citep{LiuEtAl2022}.
Despite ReLU's simplicity and efficiency, we decide to experiment with different activation functions, inspired by the survey performed by \citet{DubeyEtAl2021}. Based on our search, we achieve the best results with the GELU activation function.


\textbf{Standardizing the Input.}
The original Xception model uses an input size of . We found that standardizing the input size to , as in \citet{HeEtAl2015}, makes the training faster on Nvidia Tensor Cores. To compensate for the lower resolution, we make the network wider by adding more channels. 

\textbf{Blur Pooling.}
Inspired by the solution from \citet{Zhang2019}, we integrate a blurring procedure before subsampling the signal. By introducing this anti-aliasing technique, our network generalizes better and achieves higher accuracy.

