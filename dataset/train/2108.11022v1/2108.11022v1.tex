\begin{table*}[htbp!]
\footnotesize
\renewcommand{\arraystretch}{1}
\setlength{\extrarowheight}{1.25pt}
\setlength\tabcolsep{3pt}
\caption{Statistics of datasets.}
\vskip -2ex
\label{table-statist-dataset}
\centering
\begin{tabular}{p{15mm}ccccccc}
\hline
\multicolumn{2}{l}{\textbf{Networks}}          & \textbf{Nodes}     & \textbf{Edges}     & \textbf{Features}     & \textbf{Classes}     & \textbf{Train/Val/Test}  & \textbf{Type}     \\
\hline
\multirow{3}{*}{\textbf{Homophily}} &\multicolumn{1}{l}{Cora}& 2708 & 5429 & 1433 & 7      & \makecell{140/500/1000} &Citation network\\
& \multicolumn{1}{l}{Citeseer}& 3327 & 4732 & 3703 & 6 & 120/500/1000 &Citation network\\
& \multicolumn{1}{l}{Pubmed}& 19717 & 44338 & 500 & 3 & 60/500/1000 & Citation network\\
\hline
\multirow{4}{*}{\makecell{\textbf{Non-}\\\textbf{homophily}}} & \multicolumn{1}{l}{Cornell} & 183 & 295 & 1703 & 5 & 48\%/32\%/20\% & Webpage network\\
& \multicolumn{1}{l}{Texas} & 183 & 309 & 1703 & 5 & 48\%/32\%/20\% & Webpage network \\
& \multicolumn{1}{l}{Wisconsin} & 251 & 499 & 1703 & 5 & 48\%/32\%/20\% & Webpage network \\
& \multicolumn{1}{l}{Actor} & 7600 & 33544 & 931 & 5 & 48\%/32\%/20\% & Actor co-occurrence network \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[]\footnotesize
\renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{5pt}
\caption{Summary of semi-supervised classification accuracy (\%) $\pm$ stdev over Cora, Citeseer, and Pubmed datasets.}
\vskip -2ex
\label{table-semi}
\centering
\begin{tabular}{|l|cc|cc|cc|c|}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Cora}} & \multicolumn{2}{c|}{\textbf{Citeseer}} & \multicolumn{2}{c|}{\textbf{Pubmed}} & \multirow{2}{*}{\textbf{Avg. Rank}}  \\
\cline{2-7}
& Fixed & Random & Fixed & Random & Fixed & Random & \\
\hline
GCN & 81.50$\pm$0.79 (0-2) & 79.91$\pm$1.64 (0-2) & 71.42$\pm$0.48 (0-2) & 68.78$\pm$2.01 (0-2) & 79.12$\pm$0.46 (0-2) & 77.84$\pm$2.36 (0-2) & 7.17 \\
GAT & 83.10$\pm$0.40 (0-2) &  80.80$\pm$1.60 (0-2) &  70.80$\pm$0.50 (0-2) & 68.90$\pm$1.70 (0-2)  &  79.10$\pm$0.40 (0-2) & 77.80$\pm$2.10 (0-2)  &  7.00 \\
SGC & 82.63$\pm$0.01 (0-2) & 80.18$\pm$1.57 (0-2) & 72.10$\pm$0.14 (0-2) & 69.33$\pm$1.90 (0-2) & 79.12$\pm$0.10 (0-2) & 76.74$\pm$2.84 (0-2)& 6.83\\
APPNP & 83.34$\pm$0.56 (0-10) & 82.26$\pm$1.39 (0-10) & 72.22$\pm$0.50 (0-10) & 70.53$\pm$1.57 (0-10) & 80.14$\pm$0.24 (0-10) & 79.54$\pm$2.23 (0-10) & 3.83\\
DAGNN & 84.88$\pm$0.49 (0-10) & 83.47$\pm$1.18 (0-10) & 73.39$\pm$0.57 (0-9) & 70.87$\pm$1.44 (0-10) & \textbf{80.51$\pm$0.42 (0-20)} & 79.52$\pm$2.19 (0-20) & 2.33\\
GCNII* &  \textbf{85.57}$\pm$\textbf{0.45 (0-64)}& 82.58$\pm$1.68 (0-64) & 73.24$\pm$0.61 (0-32) &70.04$\pm$1.72 (0-10) & 80.00$\pm$0.48 (0-16)& 79.03$\pm$1.68 (0-16) & 3.83\\
TDGNN-s & 85.35$\pm$0.49 (0-4)  & \textbf{83.84}$\pm$\textbf{1.45 (0-6)} &    \textbf{73.78}$\pm$\textbf{0.60 (0-8)}  &  \textbf{71.27}$\pm$\textbf{1.71 (0-8)} & 80.20$\pm$0.33 (0-5)& \textbf{80.01}$\pm$\textbf{1.96 (0-5)} & \textbf{1.33} \\
TDGNN-w & 84.42$\pm$0.59 (0-4)  & 83.43$\pm$1.35 (0-6) & 72.14$\pm$0.49 (0-6) & 70.32$\pm$1.57 (0-6) & 80.12$\pm$0.44 (0-5) &79.77$\pm$2.04 (0-5) & 3.67\\
\hline
\end{tabular}
\vskip -1ex
\end{table*}

\section{Experiments}\label{sec-experiments}
In this section, we conduct extensive node classification experiments to evaluate the superiority of our proposed TDGNN model. We begin by introducing the datasets and experimental setup we employed. Then, we compare TDGNN with prior baselines and some state-of-the-art (SOTA) deep GNNs. 

\subsection{Experimental Settings}

\subsubsection{Datasets}
We evaluate the performance of our TDGNN model and baseline models with node classification on multiple real-world datasets. More specifically, we use the three standard citation network datasets Cora, Citeseer, and Pubmed~\cite{sen2008collective} for semi-supervised node classification~\cite{yang2016revisiting}, where nodes correspond to documents associated with the bag-of-words as the features and edges correspond to citations. For full-supervised node classification, in addition to the three citation networks we include three extra web network datasets, Cornell, Texas, and Wisconsion~\cite{geomgcn}, where nodes and edges represent web pages and hyperlinks, and one actor co-occurrence network dataset, Actor~\cite{geomgcn}, where nodes and edges represent actors and their co-occurrence in the same movie.
Table~\ref{table-statist-dataset} contains the basic network statistics for each of these datasets.



\subsubsection{Baselines}
To evaluate the effectiveness of TDGNN, we choose the following representative supervised node classification baselines including SOTA GNN models.
\begin{itemize}[leftmargin=0.6cm]

    \item \textbf{MLP}~\cite{murtagh1991multilayer}: 2-layer multilayer perceptron with dropout and ReLU non-linearity, which is empirically shown in other works to perform well on non-homophily network datasets~\cite{zhu2020beyond}.
    
    \item \textbf{GCN}~\cite{GCN}: GCN is one of the most popular graph convolutional models and our proposed model is modified based on it.
    
    \item \textbf{GAT}~\cite{GAT}: Graph attention network employs attention mechanism to pay different levels of attention to nodes within the neighorhood set, and is widely used as a GNN baseline. 
    
    \item \textbf{SGC}~\cite{SGC}: Simple graph convolution network removes nonlinearities and collapsing weight matrices between consecutive layers, which obtains the comparable accuracy and yields orders of magnitude speedup over GCN. We note that SGC collapses the traditional GNN aggregation tree such that the center node receives the features directly from the flattened neighborhood while being weighted according to the higher-order neighborhood information.
    
    \item \textbf{APPNP}~\cite{APPNP}: APPNP links GCN and PageRank to derive an improved propagation scheme based on personalized PageRank, which incorporates higher-order neighborhood information and meanwhile keeps the local information.
    
    \item \textbf{Geom-GCN}~\cite{geomgcn}: Geom-GCN explores to capture long-range dependencies in non-homophily networks. It uses the geometric relationships defined in the latent space to build structural neighorhoods for aggregation. Since Geom-GCN is mainly designed for non-homophily networks, we only report its performance in full-supervised node classification where three non-homophily networks are included.
    
    \item \textbf{DAGNN}~\cite{DAGNN}: Deep adaptive graph neural network first decouples the representation transformation from propagation so that large receptive fields can be applied without suffering from performance degradation. Then, it utilizes an adaptive adjustment mechanism, which adaptively balances the information from local and global neighborhoods for each node.
    
    \item \textbf{GCNII}~\cite{GCNII}: GCNII employs residual connection to retain part of the information from the previous layer and adds an identity mapping to ensure the non-decreasing performance as the GNN model goes deeper (i.e., successfully adds more layers).
\end{itemize}

For baselines that have multiple variants (Geom-GCN, GCNII), we only choose the best for each dataset and denote it as model*. 


\begin{table*}[t]\footnotesize
\renewcommand{\arraystretch}{1.1}
    \setlength\tabcolsep{3pt}
\caption{Summary of full-supervised classification accuracy (\%) $\pm$ stdev over 8 datasets.}
\vskip -2ex
\label{table-full}
\centering
\begin{tabular}{|l|ccccccc|c|}
\hline
\textbf{Method} & \textbf{Cora} & \textbf{Cite.} & \textbf{Pub.} & \textbf{Corn.}& \textbf{Tex.}& \textbf{Wisc.}& \textbf{Act.}& \textbf{Avg. Rank}\\
\hline
MLP &75.78$\pm$1.84 (0) & 73.81$\pm$ 1.74 (0)& 86.90$\pm$0.37 (0)& 80.97$\pm$6.33 (0) & 81.32$\pm$ 4.19 (0) & 85.38$\pm$3.95 (0) & 36.60$\pm$1.25 (0) & 5.57\\
GCN & 86.97$\pm$1.32 (0-2) & 76.37$\pm$1.47 (0-2) & 88.19$\pm$0.48 (0-2) & 58.57$\pm$3.57 (0-2) & 58.68$\pm$4.64 (0-2) &53.14$\pm$6.25 (0-2) & 28.65$\pm$1.38 (0-2)& 8.14\\
GAT & 87.30$\pm$1.01 (0-2) & 75.55$\pm$1.32 (0-2)& 85.33$\pm$0.48 (0-2)& 61.89$\pm$5.05 (0-2)& 58.38$\pm$6.63 (0-2)& 55.29$\pm$4.09 (0-2)& 28.45$\pm$0.89 (0-2)& 8.00\\
SGC & 87.07$\pm$1.20 (0-2) & 76.01$\pm$1.78 (0-2) & 85.11$\pm$0.52 (0-2) & 58.68$\pm$3.75 (0-2)&  60.43$\pm$5.11 (0-2)& 53.49$\pm$5.13 (0-2)& 27.46$\pm$1.46 (0-2)& 8.57\\
Geom-GCN* & 85.35$\pm$1.57 (0-2) & \textbf{78.02$\pm$1.15 (0-2)} & 89.95$\pm$0.47 (N/A) &  60.54$\pm$3.67 (0-2) &   66.76$\pm$2.72 (N/A)  &64.51$\pm$3.66 (N/A)& 31.63$\pm$1.15 (N/A)& 5.86\\
APPNP & 86.76$\pm$1.74 (0-10) & 77.08$\pm$1.56 (0-10)& 88.45$\pm$0.42 (0-10)& 74.59$\pm$5.11 (0-10)& 74.30$\pm$4.74 (0-10)&81.10$\pm$2.93 (0-10)& 34.36$\pm$1.09 (0-10)& 5.43\\
DAGNN &  87.26$\pm$1.42 (0-10) & 76.47$\pm$1.54 (0-10)& 87.49$\pm$0.63 (0-20) & 80.97$\pm$6.33 (0)& 81.32$\pm$4.19 (0) & 85.38$\pm$3.95 (0) & 36.60$\pm$1.25 (0) & 4.71\\
GCNII* &  \textbf{88.27}$\pm$\textbf{1.31 (0-64)} & 77.06$\pm$1.67 (0-64) & \textbf{90.26$\pm$0.41 (0-64)} & 76.70$\pm$5.40 (0-16) &77.08$\pm$5.84 (0-32) & 80.94$\pm$4.94 (0-16) & 35.18$\pm$1.30 (0-64) & 3.71\\
TDGNN-s & 88.26$\pm$1.32 (0-4) & 76.64$\pm$1.54 (0-8) & 89.13$\pm$0.39 (0-1) & 80.97$\pm$6.33 (0)& 82.95$\pm$4.59 (0, 4-5)& 85.47$\pm$3.88 (0, 4-5) & 36.70$\pm$1.28 (0, 3-4)& 2.86\\
TDGNN-w & 88.01$\pm$1.32 (0-5)& 76.58$\pm$1.40 (0-2)& 89.22$\pm$0.41 (0-1)& \textbf{82.92$\pm$6.61 (0, 2-6)} & \textbf{83.00$\pm$4.50 (0, 2)} & \textbf{85.57$\pm$3.78 (0, 3-5)}& \textbf{37.11$\pm$0.96 (0, 3-4)} & \textbf{2.14}\\
\hline
\end{tabular}
\begin{tablenotes}
      \small
      \centering
      \item \textbf{*} We reuse the results reported in~\cite{coin} for Geom-GCN. 'N/A' indicate the corresponding layers are not reported in the paper.
\end{tablenotes}
\vspace{-1.5em}
\end{table*}

\subsubsection{Parameter Settings}
We implement our proposed TDGNN and some necessary baselines using Pytorch~\cite{pytorch} and Pytorch Geometric~\cite{paszke2019pytorch}, a library for deep learning on graph-structured data built upon Pytorch. For DAGNN\footnote{\url{https://github.com/vthost/DAGNN}}, and GCNII\footnote{\url{https://github.com/chennnM/GCNII}}, we use the original code from the authors' github repository. We aim to provide a rigorous and fair comparison between different models on each dataset by tuning hyperparameters for all models individually. The number of hidden unit is searched from $\{16, 32, 64, 128\}$, the dropout rate is searched from $\{0, 0.5, 0.8\}$, the weight decay is searched from $[1e^{-4}, 2e^{-2}]$, the training epochs is searched from $\{300, 500, 1000, 1500, 3000, 4000\}$ and the learning rate is set to be 0.01. We find that some baselines even achieve better results than their original reports. Note that in this work, we do not treat the random seed as a hyperparamter and therefore, the random seed fixed in previous models for reproducing results, if any, is reset to be totally random to remove any potential bias and thus allow for more generalized comparison. For reproducibility, codes of all of our models and corresponding hyperparameter configurations for results in Table \ref{table-semi}-\ref{table-full} are publicly available \footnote{\url{https://github.com/YuWVandy/TDGNN}\label{github}}. 

\subsection{Semi-supervised Node Classification}
For the semi-supervised node classification task, we apply the fixed split following~\cite{yang2016revisiting} and random training/validation/testing split on Cora, Citeseer, and Pubmed, with 20 nodes per class for training, 500 nodes for validation and 1000 nodes for testing. For each model, we conduct 100 runs and report the mean classification accuracy with the standard deviation in both the fixed and random splitting cases. Table \ref{table-semi} reports the best mean accuracy with the standard deviation over different data splits where the best model per benchmark is highlighted in bold and the number in parentheses corresponds to layers of neighborhoods utilized at which the best performance is achieved. For example, (0-4) means the corresponding performance is achieved when we use neighborhood of layers up to 4 and $0$-layer neighborhoods correspond to using center nodes themselves.



We observe that TDGNN-s performs the best in terms of the average rank through all datasets and across both random and fixed splits, which suggests the comprehensive superiority of TDGNN-s to other baselines. Specifically, our TDGNN-s model outperforms the representative baselines including GCN, GAT, SGC, and APPNP across all datasets by significant margins. Compared with two recent deep GNN models, DAGNN and GCNII*, TDGNN-s can still achieve the comparable or even better performance. Especially when the data split is random, TDGNN-s outperforms all other models, which demonstrates the strong robustness of TDGNN-s (in terms of dataset splits). It is also worthwhile to note that our TDGNN model achieves the SOTA performance with relatively shallow layers compared with DAGNN and GCNII*. On Cora dataset, the best performance is achieved when layers are used up to 4 and 6 for our TDGNN-s model, respectively, in fixed and random data splitting, while DAGNN and GCNII require up to 10 and 64 layers to achieve the best, which demands heavy computation and thus are time inefficient. On Citeseer dataset, our model also utilizes up to the most shallow layers compared with DAGNN and GCNII* to achieve the SOTA performance. Surprisingly, the weighted version of our model, TDGNN-w, has poorer performance than TDGNN-s while still outperforms most of the baselines. This is because the weight coefficients $\{\theta_0, \theta_1, ..., \theta_L\}$ are only decided by training nodes and the suitable weights for combining aggregated features $\{\mathbf{H}_{0}, \mathbf{H}_{1}, ..., \mathbf{H}_{L}\}$ and getting good predictions on training and validation nodes might not be suitable for testing nodes, which inspires future work for a layer aggregation mechanism that enables node-adaptive layer combination.

\vspace{-0.75ex}
\subsection{Full-supervised Node Classification}
For the full-supervised node classification task, we evaluate our TDGNN model and existing GNNs using 7 datasets: Cora, Citeseer, Pubmed, Cornell, Texas, Wisconsin, and Actor. For each dataset, we use 10 random splits (48\%/32\%/20\% of nodes per class for training/validation/testing) from~\cite{geomgcn}\footnote{Note that although~\cite{geomgcn} reports that the ratios are 60\%/20\%/20\%, but this is different from the actual data splits shared on their GitHub~\cite{coin}.}. We conduct 100 runs with each split evaluated 10 times and report the mean accuracy with the standard deviation in Table \ref{table-full}. We note that here the numbers in the parentheses again correspond to layers of neighborhoods utilized (e.g., (0,3-5) means the corresponding performance is achieved when we use neighborhoods of layers 3 to 5 and $0$-layer neighborhoods corresponding to the center nodes themselves.) 

First, we observe from Table~\ref{table-full} that TDGNN-w has the best average rank across the two types of networks (i.e., homophily and heterophily) with TDGNN-s ranks second. Next, we observe that TDGNN-w significantly outperforms the baselines across the heterophily networks. However, both variants of TDGNN are slightly outperformed on the homophily networks in this full-supervised setting (whereas in most homophily networks under the semi-supervised setting TDGNN-s performs the best). Thus, to better understand the inner workings of TDGNN, we next perform a detailed parameter analysis.



\vspace{-0.75ex}
\subsection{Parameter Analysis}
Here we compare the performance of TDGNN with other baselines when utilizing neighborhoods in different layers. Furthermore, we perform a parameter analysis of TDGNN by varying the neighborhood layers ($L$) and the multi-hop dependencies ($K$).


First, to demonstrate the strength of the TDGNN-s model in shallow layers, we visualize the performance of each model using layers from up to 1 to up to 10 in Figure~\ref{fig-layer}. For the Cora and Citeseer datasets, our model achieves around 84\% and 73\% using only the first 2-layer neighborhoods and the first three-layer neighborhoods, respectively. Compared to two SOTA deep GNNs where DAGNN achieves the same level performance using 5 layers and 7 layers, and GCNII* achieves using 8 layers and at least 32 layers, our model can leverage less neighborhood information to achieve comparable performance, which clearly validates the importance of considering multi-hop dependency. This further to some extent raises the concern over whether we need deep GNNs to incorporate higher-layer neighborhood information in homophily networks, or if shallow feature information aggregated according to higher-order multi-hop dependencies provides sufficient information. Besides, the continued high-level performance as model depth increases demonstrates the higher resilience of TDGNN-s against over-smoothing.
\begin{figure}[t]
     \centering
     \hspace{-1.5ex}
     \begin{subfigure}[b]{0.235\textwidth}
         \centering
         \includegraphics[width=1.02\textwidth]{figure/layer_cora.png}
         \vskip -0.25ex
         \caption{Cora}
         \label{fig-layercora}
     \end{subfigure}
     \hspace{-1ex}
     \begin{subfigure}[b]{0.235\textwidth}
         \centering
         \includegraphics[width=1.02\textwidth]{figure/layer_citeseer.png}
         \vskip -0.25ex
         \caption{Citeseer}
         \label{fig-layerciteseer}
     \end{subfigure}
     \vskip -2ex
     \caption{Results of models with different layers.}
     \label{fig-layer}
     \vskip -3.5ex
\end{figure}


Second, we vary the maximum layer of neighborhoods and the multi-hop dependency to study their effect on the performance of the proposed two models: TDGNN-s on two representative homophily networks and TDGNN-w on two representative heterophily networks. Both of the maximum layer of the neighborhoods and the length of the multi-hop dependency are selected from $\{1, 2, 3 ,5, 10\}$ due to the small-world theory that two nodes will be connected through few series of intermediaries~\cite{milgram1967small}. Figure~\ref{fig-heath} visualizes the averaged accuracy across 10 runs for various layers and dependency configurations. For two homophily networks, including extra neighborhood layers significantly increase the model performance for lower-layers and such boosting effect becomes progressively weaker as more and more higher-layer neighborhood layers are included, e.g., the performance increases from $79.85$ to $84.00$ and from $71.50$ to $72.85$ for Cora and Citeseer when including the $2^\text{nd}$-layer neighborhood while only from $84.00$ to $85.06$ and from $72.85$ to $73.28$ when including the $3^\text{rd}$-layer. This weaker boost as the layer number increases is also in line with the decreasing homophily level as observed in Figure~\ref{fig-distcora}. In comparison, for heterophily networks, in Figures~\ref{fig-cornellheat} and~\ref{fig-texasheat} we can observe a more significant need for the decoupling of neighborhood layers since increasing the receptive field (i.e., increasing the maximum layer of neighborhoods) is not always advantageous. Similarly including deeper multi-hop dependencies is not always a clear advantage as seen in the homophily networks because lower-layer neighborhoods that have different labels or representations from their corresponding center nodes may contribute more to their center nodes' prediction through longer dependency. We note that these findings also align with our empirical analysis in Figure~\ref{fig-distcora}.
Therefore, we believe that the increased performance obtained by TDGNN over prior work is partially credited to its ability to separate the concept of graph convolutions in deeper GNNs with higher-layer neighborhoods into both multi-hop dependencies and decoupled neighborhood layers, which can allow any deep GNN model to be more flexibly customized via hyperparameter tuning on a wider variety of complex networks.



\begin{figure}[t]
     \centering
\begin{subfigure}[b]{0.245\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{figure/heatmap_cora.png}
         \vskip -1ex
         \caption{Cora (TDGNN-s)}
         \label{fig-coraheat}
     \end{subfigure}
     \hspace{-3ex}
     \begin{subfigure}[b]{0.245\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{figure/heatmap_citeseer.png}
         \vskip -1ex
         \caption{Citeseer (TDGNN-s)}
         \label{fig-citeheat}
     \end{subfigure}
     \begin{subfigure}[b]{0.245\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{figure/heatmap_cornell_w.png}
         \vskip -1ex
         \caption{Cornell (TDGNN-w)}
         \label{fig-cornellheat}
     \end{subfigure}
     \hspace{-3ex}
     \begin{subfigure}[b]{0.245\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{figure/heatmap_texas_w.png}
         \vskip -1ex
         \caption{Texas (TDGNN-w)}
         \label{fig-texasheat}
     \end{subfigure}
     \vskip -1.25ex
     \caption{Visualizing the effect of varying the maximum layer neighborhoods and the length of mutli-hop dependency on the performance of TDGNN.}
     \label{fig-heath}
     \vskip -4ex
\end{figure}

