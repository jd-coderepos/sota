\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{Images/diverse7.pdf}
\caption{\textbf{Comparison of generative models without and with anchor-based sampling.} Anchors and network parameters are jointly optimized. (a) Conventional generative model with only stochastic noise; (b) Generative model with deterministic anchor process: an anchor with Gaussian noise corresponds to a prediction; (c) Spatial-temporal compositional anchors: any pair of combined spatial and temporal anchors corresponds to a prediction; (d) Multi-level spatial-temporal anchors: anchors at different levels are combined for encoding multi-scale modes}
\label{fig:diverse}
\end{figure}

\section{Methodology}
\label{sec:method}

\noindent{\bf Problem Formulation.}
We denote the input motion sequence of length $T_h$ as $\mathbf X=[\mathbf x_1, \mathbf x_2,\ldots,\mathbf x_{T_h}]^T$,
where the 3D coordinates of $V$ joints are used to describe each pose $\mathbf x_i \in \mathbb{R}^{{V}\times C^{(0)}}$. 
Here, we have $C^{(0)} = 3$. 
And $K$ output sequences of length $T_p$ are denoted as $\mathbf{\widehat{Y}}_1, \mathbf{\widehat{Y}}_2, \ldots, \mathbf{\widehat{Y}}_K$. We have access to a {\em{single}} ground truth future motion of length $T_p$ as $\mathbf{Y}$.
{\it{Our objectives}} are: (1) one of the $K$ predictions is as close to the ground truth as possible; and (2) the $K$ sequences are as diverse as possible, yet represent realistic future motions.

In this section, we first briefly review deep generative models, describe how they draw samples to generate multiple futures, and discuss their limitations (Sec.~\ref{sec:shmp}). We then detail our insights on \oursanchor{} including anchor-based sampling and multi-level spatial-temporal anchors (Sec.~\ref{sec:shmp} and Fig.~\ref{fig:diverse}). To model human motion, we design an \oursmodel{} and incorporate our spatial-temporal anchors into it (Sec.~\ref{sec:network}), as illustrated in Fig.~\ref{fig:deterministic2}.

\subsection{Multi-Level Spatial-Temporal Anchor-Based Sampling}\label{sec:shmp}

\noindent{\bf Preliminaries: Deep Generative Models.}
There is a large body of work on the generation of multiple hypotheses with deep generative models, most of which learn a parametric probability distribution function explicitly or implicitly. Let $p(\mathbf{Y}|\mathbf{X})$ denote the distribution of the future human motion $\mathbf{Y}$ conditioned on the past sequence $\mathbf{X}$. With a latent variable $\mathbf z \in \mathcal{Z}$, the distribution can be reparameterized as $p(\mathbf{Y}|\mathbf{X}) = \int p(\mathbf{Y}|\mathbf{X},\mathbf z)p(\mathbf z) \mathrm{d} \mathbf z$, where $p(\mathbf z)$ is often a Gaussian prior distribution. To generate a future motion sequence $\mathbf{\widehat Y}$, $\mathbf{z}$ is drawn from the given distribution $p(\mathbf z)$, and then a deterministic generator $\mathcal{G}:\mathcal{Z}\times \mathcal{X} \rightarrow \mathcal{Y}$ is used for mapping, as illustrated in Fig.~\ref{fig:diverse}{\color{red}{(a)}}: 
\begin{align}
    \mathbf{z} \sim p(\mathbf{z}), \ \mathbf{\widehat Y} = \mathcal{G}(\mathbf{z}, \mathbf{X}),
\end{align}
where $\mathcal{G}$ is a deep neural network parameterized by $\theta$. The goal of generative modeling is to make the distribution $p_{\theta}(\mathbf{\widehat Y}|\mathbf{X})$ derived from the generator $\mathcal{G}$ close to the actual distribution $p(\mathbf{Y}|\mathbf{X})$. 

To generate $K$ diverse motion predictions, traditional approaches first independently sample a set of latent codes $Z = \{\mathbf{z}_1, \ldots, \mathbf{z}_K\}$ from a prior distribution $p(\mathbf{z})$.
Although in theory, generative models are capable of covering different modes, they are not guaranteed to locate all modes precisely, and mode collapse has been widely observed \cite{yuan2020dlow,Yuan2020Diverse}.

\noindent{\bf Anchor-Based Sampling.}
To address this problem, we propose a simple, yet effective sampling strategy. Our intuition is that the diversity in future motions could be characterized by: (1) deterministic component -- across different actions performed by different subjects, there exist correlated or shareable changes in velocity, direction, movement patterns, etc., which naturally emerge and can be directly learned from data; and (2) stochastic component -- given an action carried out by a subject, the magnitude of the changes exists, which is stochastic.

Therefore, we disentangle the code in the latent space of the generative model into a {\em{stochastic}} component sampled from $p(\mathbf{z})$, and a {\em{deterministic}} component represented by a set of $K$ {\em{learnable parameters}} called {\em anchors} $\mathcal{A} = \{\mathbf a_k\}_{k=1}^K$. Deterministic anchors are expected to identify as many modes as possible, which is achieved through a carefully designed optimization, while stochastic noise further specifies motion variation within certain modes. With this latent code disentanglement, we denote the new multi-modal distribution as
\begin{align}
    p_{\theta}(\mathbf{\widehat Y}|\mathbf{X}, \mathcal{A}) = \frac{1}{K}\sum_{k=1}^K \int p_{\theta}(\mathbf{\widehat Y}|\mathbf{X}, \mathbf z,\mathbf a_k)p(\mathbf z) \mathrm{d} \mathbf z.
\end{align}

Consequently, as illustrated in Fig.~\ref{fig:diverse}{\color{red}{(b)}}, suppose that we select the $k$-th learned anchor $\mathbf a_k \in \mathcal{A}$, along with the randomly sampled noise $\mathbf z \in Z$, we can generate the prediction $\mathbf{\widehat{Y}}_k$ as,
\begin{align}
    \mathbf{z} \sim p(\mathbf z),\ \mathbf{\widehat{Y}}_k = \mathcal{G}(\mathbf{a}_k, \mathbf{z}, \mathbf{X}).
\end{align}

We can produce a total of $K$ predictions if using each anchor once, though all anchors are not limited to being used or used only once. To incorporate anchors into the network, we find it effective to make simple additions between selected anchors and latent features, as shown in Fig.~\ref{fig:deterministic2}.


\noindent{\bf Spatial-Temporal Compositional Anchors.}\label{sec:mmst} We observe that the diversity of future motions can be roughly divided into two types, namely {\em{spatial variation}} and {\em{temporal variation}}, which are relatively independent. This sheds light on a feasible further decomposition of the $K$ anchors into two types of learnable codes: spatial anchors $\mathcal{A}_s = \{\mathbf a^s_i\}_{i=1}^{K_s}$ and temporal anchors $\mathcal{A}_t = \{\mathbf a^t_j\}_{j=1}^{K_t}$, where $K = K_s \times K_t$. 
With this decomposition, we still can yield a total of $K_s \times K_t$ compositional anchors through \textit{each pair of spatial-temporal anchors}.
Note that the temporal anchors here, in fact, control the frequency variation of future motion sequences, since our temporal features are in the frequency domain, as we will demonstrate in Sec.~\ref{sec:network}. 
To be more specific, conceptually, all spatial anchors are set to be identical in the temporal dimension but characterize the variation of motion in the spatial dimension, taking control of the movement trends and directions. Meanwhile, all temporal anchors remain unchanged in the spatial dimension but differ in the temporal dimension, producing disparities in frequency to affect the movement speed. 

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{Images/diverse8.pdf}
\caption{\textbf{Overview of our \ours{} framework.} We combine multi-level spatial-temporal anchors, the sampled noise, with the backbone \oursmodel. To generate one of the predictions given a past motion, we draw noise $\mathbf z_k$, and add the selected spatial-temporal anchors to the latent feature at each level}
\label{fig:deterministic2}
\end{figure}

To produce $\mathbf{\widehat{Y}}_k$, as depicted in Fig.~\ref{fig:diverse}{\color{red}{(c)}}, we sample $\mathbf z$ and select $i$-th spatial anchor $\mathbf a_i^s$ and $j$-th temporal anchor $\mathbf a_j^t$, 
\begin{align}
    \mathbf{z} \sim p(\mathbf z),\ \mathbf{\widehat{Y}}_k = \mathcal{G}(\mathbf a_i^s + \mathbf a_j^t, \mathbf{z}, \mathbf{X}),
\end{align}
where $\mathbf a_i^s + \mathbf a_j^t$ is a spatial-temporal compositional anchor corresponding to an original anchor $\mathbf a_k$. Furthermore, motion control over spatial and temporal variation can be customized through these spatial-temporal anchors. For example, we can produce future motions with similar trends by fixing the spatial anchors while varying or interpolating the temporal anchors, as shown in Sec.~\ref{sec:qual}. 

\noindent{\bf Multi-Level Spatial-Temporal Anchors.}
To further learn and capture multi-scale modes of future motions, we propose a multi-level mechanism to extend the spatial-temporal anchors.
As an illustration, Fig.~\ref{fig:diverse}{\color{red}{(d)}} shows a simple two-level case for this design. We introduce two different spatial-temporal anchor sets, $\{\mathcal{A}_t^{(1)},\mathcal{A}_s^{(1)}\}$ and $\{\mathcal{A}_t^{(2)},\mathcal{A}_s^{(2)}\}$, and assign them sequentially to different network parts $\mathcal{G}^{(1)},\mathcal{G}^{(2)}$. Suppose $(i, j)$ is a spatial-temporal index corresponding to the 1D index $k$, we can generate $\mathbf{\widehat{Y}}_k$ through a two-level process as
\begin{align}
    \mathbf{z} \sim p(\mathbf z),\ \mathbf{\widehat{Y}}_k = \mathcal{G}^{(2)}(\mathbf a_i^{s_{2}} + \mathbf a_j^{t_{2}}, \mathbf{z}, \mathcal{G}^{(1)}(\mathbf a_i^{s_{1}} + \mathbf a_j^{t_{1}}, \mathbf{X})),
\end{align}
where $a_i^{s_{1}} \in \mathcal{A}_s^{(1)}, a_j^{t_{1}} \in \mathcal{A}_t^{(1)}, a_i^{s_{2}} \in \mathcal{A}_s^{(2)}, a_j^{t_{2}} \in \mathcal{A}_t^{(2)}$. As a principled way, anchors can be applied at more levels to encode richer assumptions about future motions.
 
\noindent{\bf Training.}
During training, the model uses \textit{each spatial-temporal anchor} explicitly to generate $K$ future motions for each past motion sequence. The loss functions are mostly adopted as proposed in~\cite{mao2021generating}, which we summarize into three categories: (1) reconstruction losses that optimize \textit{the best predictions} under different definitions among $K$ generated motions, and thus optimize anchors to their own nearest modes; (2) a diversity-promoting loss that explicitly promotes pairwise distances in predictions, avoiding that anchors collapse to the same; and (3) motion constraint losses that encourage output movements to be realistic. All anchors are directly learned from the data via gradient descent.
In the forward pass, we explicitly take {\em every} anchor $\mathbf{a}_i \in \mathcal{A}=\{\mathbf{a}_k\}_{k=1}^K$ as an additional input to the network and produce a total of $K$ outputs. In the backward pass, each anchor is optimized \textit{separately} based on its corresponding outputs and losses, while the backbone network is updated based on the \textit{fused} losses from all outputs. This separate backward pass is automatically done via PyTorch~\cite{paszke2019pytorch}. Please refer to Sec.~\ref{sec:shmp_supp} of the supplementary material for more details.

\subsection{Interaction-Enhanced Spatial-Temporal Graph Convolutional Network}\label{sec:network}
In principle, our proposed anchor-based sampling permits flexible network architectures. Here, to incorporate our multi-level spatial-temporal anchors, we naturally represent motion sequences as spatial-temporal graphs (to be precise,  spatial-frequency graphs), instead of the widely used spatial graphs~\cite{wei2019motion,mao2021generating}.  Our approach builds upon the Discrete Cosine Transform (DCT)~\cite{wei2019motion,mao2021generating} to transform the motion into the frequency domain. Specifically, given a past motion $\mathbf X_{1:T_h} \in \mathbb{R}^{T_h \times V \times C^{(0)}}$, where each pose has $V$ joints, we first replicate the last pose $T_p$ times to get $\mathbf {X}_{1:T_h+T_p} = [\mathbf x_1, \mathbf x_2,\ldots,\mathbf x_{T_h}, \mathbf x_{T_h}, \ldots,\mathbf x_{T_h}]^T$. With the predefined $M$ basis $\mathbf{C} \in \mathbb{R}^{ M\times (T_h+T_p)}$ for DCT, the motion is transformed as
\begin{align}
    \widetilde{\mathbf {X}} = \mathbf{C}\mathbf {X}_{1:T_h+T_p}.
\end{align}

We formulate $\widetilde{\mathbf {X}} \in \mathbb{R}^{M \times V \times C^{(0)}}$ in the $0$-th layer and latent features in any $l$-th graph layer as spatial-temporal graphs $(\mathcal{V}^{(l)}, \mathcal{E}^{(l)})$ with $M \times V$ nodes. We specify the node $i$ by the 2D index $(f_i, v_i)$ for the joint $v_i$ with frequency $f_i$ component. The edge $(i, j) \in \mathcal{E}^{(l)}$ associated with the interaction between node $i$ and node $j$ is represented by $\mathbf {Adj}^{(l)}[i][j]$, where the adjacency matrix $\mathbf {Adj}^{(l)} \in \mathbb{R}^{M V\times M V}$ is learnable. We bottleneck spatial-temporal interactions as~\cite{Sofianos_2021_ICCV}, by factorizing the adjacency matrix into the product of low-rank spatial and temporal matrices $\mathbf {Adj}^{(l)} = \mathbf {Adj}^{(l)}_{s} \mathbf {Adj}^{(l)}_{f}$. The spatial adjacency matrix $\mathbf {Adj}^{(l)}_{s} \in \mathbb{R}^{M V\times M V}$ connects only nodes with the same frequency. And the frequency adjacency matrix $\mathbf {Adj}^{(l)}_{f} \in \mathbb{R}^{M V\times M V}$ is merely responsible for the interaction between the nodes that represent the same joint.



The spatial-temporal graph can be conveniently encoded by a graph convolutional network (GCN). Given a set of trainable weights $\mathbf W^{(l)} \in \mathbb{R}^{C^{(l)}\times C^{(l+1)}}$ and activation function $\sigma(\cdot)$, such as ReLU, a spatial-temporal graph convolutional layer projects the input from $C^{(l)}$ to $C^{(l+1)}$ dimensions by
\begin{align}\label{eq:1}
    \mathbf{H}^{(l+1)}_k = \sigma(\mathbf {Adj}^{(l)}\mathbf{H}^{(l)}_k\mathbf W^{(l)})=\sigma(\mathbf {Adj}_{s}^{(l)}\mathbf {Adj}_{f}^{(l)}\mathbf{H}^{(l)}_k\mathbf W^{(l)}),
\end{align}
where $\mathbf{H}^{(l)}_k \in \mathbb{R}^{M V \times C^{(l)}}$ denotes the latent feature of the prediction $\mathbf{\widehat{Y}}_k$ at $l$-th layer. 
The backbone consists of multiple graph convolutional layers. After generating predicted DCT coefficients $\widetilde{\mathbf {Y}}_k \in \mathbb{R}^{M \times V \times C^{(L)}}$ reshaped from $\mathbf{H}^{(L)}_k$, where $C^{(L)} = 3$, we recover $\mathbf{\widehat{Y}}_k$ via Inverse DCT (IDCT) as
\begin{align}
    \mathbf {\widehat Y}_k = (\mathbf{C^\mathsf{T}}\widetilde{\mathbf {Y}}_k)_{T_h+1:T_h+T_p},
\end{align}
where the last $T_p$ frames of the recovered sequence represent future poses.

Conceptually, interactions between spatial-temporal nodes should be relatively invariant across layers, and different interactions should not be equally important. For example, we expect constraints and dependencies between ``left arm'' and ``left forearm,'' while the movements of ``head'' and ``left forearm'' are relatively independent.
We consider it redundant to construct a {\em complete} spatial-temporal graph for each layer {\em independently}. Therefore, we introduce cross-layer interaction sharing to share parameters between graphs in different layers, and spatial interaction pruning to prune the complete graph.

\noindent{\bf Cross-Layer Interaction Sharing.} 
Much care has been taken to employ learnable interactions between spatial nodes across all graph layers~\cite{Sofianos_2021_ICCV,wei2019motion,yan2021dmsgcn}.
We consider the spatial relationship to be relatively unchanged. Empirically, we find that sharing the adjacency matrix at intervals of one layer is effective. As shown in Fig.~\ref{fig:deterministic2}, we set $\mathbf {Adj}_s^{(4)} = \mathbf {Adj}_s^{(6)} = \mathbf {Adj}_s^{(8)}$ and $\mathbf {Adj}_s^{(5)} = \mathbf {Adj}_s^{(7)}$.

\noindent{\bf Spatial Interaction Pruning.}
To emphasize the physical relationships and constraints between spatial joints, we prune the spatial connections $\mathbf {\widehat{Adj}}_s^{(l)} = \mathbf M_s \odot \mathbf {Adj}_s^{(l)}$ in each graph layer $l$ using a predefined mask $\mathbf M_s$, where $\odot$ is an element-wise product.
Inspired by~\cite{Liu_2021_ICCV}, we emphasize spatial locality based on skeletal connections and mirror symmetry tendencies. We denote our proposed predefined mask matrix as
\begin{align}
    \mathbf M_s[i][j] = \begin{cases} 1, & v_i \mbox{ and } v_j \mbox{ are physically connected, }f_i=f_j \\
    1, & v_i \mbox{ and } v_j \mbox{ are mirror-symmetric, }f_i=f_j  \\ 0, & \mbox{otherwise}. \end{cases}
\end{align}

Finally, our architecture consists of four original \textbf{STGCNs} without spatial pruning and four \textbf{Pruned STGCNs}, as illustrated in Fig.~\ref{fig:deterministic2}. Please refer to Sec.~\ref{sec:shmp_supp} of the supplementary material for more information on the architecture.
