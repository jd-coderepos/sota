\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{Images/diverse7.pdf}
\caption{\textbf{Comparison of generative models without and with anchor-based sampling.} Anchors and network parameters are jointly optimized. (a) Conventional generative model with only stochastic noise; (b) Generative model with deterministic anchor process: an anchor with Gaussian noise corresponds to a prediction; (c) Spatial-temporal compositional anchors: any pair of combined spatial and temporal anchors corresponds to a prediction; (d) Multi-level spatial-temporal anchors: anchors at different levels are combined for encoding multi-scale modes}
\label{fig:diverse}
\end{figure}

\section{Methodology}
\label{sec:method}

\noindent{\bf Problem Formulation.}
We denote the input motion sequence of length  as ,
where the 3D coordinates of  joints are used to describe each pose . 
Here, we have . 
And  output sequences of length  are denoted as . We have access to a {\em{single}} ground truth future motion of length  as .
{\it{Our objectives}} are: (1) one of the  predictions is as close to the ground truth as possible; and (2) the  sequences are as diverse as possible, yet represent realistic future motions.

In this section, we first briefly review deep generative models, describe how they draw samples to generate multiple futures, and discuss their limitations (Sec.~\ref{sec:shmp}). We then detail our insights on \oursanchor{} including anchor-based sampling and multi-level spatial-temporal anchors (Sec.~\ref{sec:shmp} and Fig.~\ref{fig:diverse}). To model human motion, we design an \oursmodel{} and incorporate our spatial-temporal anchors into it (Sec.~\ref{sec:network}), as illustrated in Fig.~\ref{fig:deterministic2}.

\subsection{Multi-Level Spatial-Temporal Anchor-Based Sampling}\label{sec:shmp}

\noindent{\bf Preliminaries: Deep Generative Models.}
There is a large body of work on the generation of multiple hypotheses with deep generative models, most of which learn a parametric probability distribution function explicitly or implicitly. Let  denote the distribution of the future human motion  conditioned on the past sequence . With a latent variable , the distribution can be reparameterized as , where  is often a Gaussian prior distribution. To generate a future motion sequence ,  is drawn from the given distribution , and then a deterministic generator  is used for mapping, as illustrated in Fig.~\ref{fig:diverse}{\color{red}{(a)}}: 

where  is a deep neural network parameterized by . The goal of generative modeling is to make the distribution  derived from the generator  close to the actual distribution . 

To generate  diverse motion predictions, traditional approaches first independently sample a set of latent codes  from a prior distribution .
Although in theory, generative models are capable of covering different modes, they are not guaranteed to locate all modes precisely, and mode collapse has been widely observed \cite{yuan2020dlow,Yuan2020Diverse}.

\noindent{\bf Anchor-Based Sampling.}
To address this problem, we propose a simple, yet effective sampling strategy. Our intuition is that the diversity in future motions could be characterized by: (1) deterministic component -- across different actions performed by different subjects, there exist correlated or shareable changes in velocity, direction, movement patterns, etc., which naturally emerge and can be directly learned from data; and (2) stochastic component -- given an action carried out by a subject, the magnitude of the changes exists, which is stochastic.

Therefore, we disentangle the code in the latent space of the generative model into a {\em{stochastic}} component sampled from , and a {\em{deterministic}} component represented by a set of  {\em{learnable parameters}} called {\em anchors} . Deterministic anchors are expected to identify as many modes as possible, which is achieved through a carefully designed optimization, while stochastic noise further specifies motion variation within certain modes. With this latent code disentanglement, we denote the new multi-modal distribution as


Consequently, as illustrated in Fig.~\ref{fig:diverse}{\color{red}{(b)}}, suppose that we select the -th learned anchor , along with the randomly sampled noise , we can generate the prediction  as,


We can produce a total of  predictions if using each anchor once, though all anchors are not limited to being used or used only once. To incorporate anchors into the network, we find it effective to make simple additions between selected anchors and latent features, as shown in Fig.~\ref{fig:deterministic2}.


\noindent{\bf Spatial-Temporal Compositional Anchors.}\label{sec:mmst} We observe that the diversity of future motions can be roughly divided into two types, namely {\em{spatial variation}} and {\em{temporal variation}}, which are relatively independent. This sheds light on a feasible further decomposition of the  anchors into two types of learnable codes: spatial anchors  and temporal anchors , where . 
With this decomposition, we still can yield a total of  compositional anchors through \textit{each pair of spatial-temporal anchors}.
Note that the temporal anchors here, in fact, control the frequency variation of future motion sequences, since our temporal features are in the frequency domain, as we will demonstrate in Sec.~\ref{sec:network}. 
To be more specific, conceptually, all spatial anchors are set to be identical in the temporal dimension but characterize the variation of motion in the spatial dimension, taking control of the movement trends and directions. Meanwhile, all temporal anchors remain unchanged in the spatial dimension but differ in the temporal dimension, producing disparities in frequency to affect the movement speed. 

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{Images/diverse8.pdf}
\caption{\textbf{Overview of our \ours{} framework.} We combine multi-level spatial-temporal anchors, the sampled noise, with the backbone \oursmodel. To generate one of the predictions given a past motion, we draw noise , and add the selected spatial-temporal anchors to the latent feature at each level}
\label{fig:deterministic2}
\end{figure}

To produce , as depicted in Fig.~\ref{fig:diverse}{\color{red}{(c)}}, we sample  and select -th spatial anchor  and -th temporal anchor , 

where  is a spatial-temporal compositional anchor corresponding to an original anchor . Furthermore, motion control over spatial and temporal variation can be customized through these spatial-temporal anchors. For example, we can produce future motions with similar trends by fixing the spatial anchors while varying or interpolating the temporal anchors, as shown in Sec.~\ref{sec:qual}. 

\noindent{\bf Multi-Level Spatial-Temporal Anchors.}
To further learn and capture multi-scale modes of future motions, we propose a multi-level mechanism to extend the spatial-temporal anchors.
As an illustration, Fig.~\ref{fig:diverse}{\color{red}{(d)}} shows a simple two-level case for this design. We introduce two different spatial-temporal anchor sets,  and , and assign them sequentially to different network parts . Suppose  is a spatial-temporal index corresponding to the 1D index , we can generate  through a two-level process as

where . As a principled way, anchors can be applied at more levels to encode richer assumptions about future motions.
 
\noindent{\bf Training.}
During training, the model uses \textit{each spatial-temporal anchor} explicitly to generate  future motions for each past motion sequence. The loss functions are mostly adopted as proposed in~\cite{mao2021generating}, which we summarize into three categories: (1) reconstruction losses that optimize \textit{the best predictions} under different definitions among  generated motions, and thus optimize anchors to their own nearest modes; (2) a diversity-promoting loss that explicitly promotes pairwise distances in predictions, avoiding that anchors collapse to the same; and (3) motion constraint losses that encourage output movements to be realistic. All anchors are directly learned from the data via gradient descent.
In the forward pass, we explicitly take {\em every} anchor  as an additional input to the network and produce a total of  outputs. In the backward pass, each anchor is optimized \textit{separately} based on its corresponding outputs and losses, while the backbone network is updated based on the \textit{fused} losses from all outputs. This separate backward pass is automatically done via PyTorch~\cite{paszke2019pytorch}. Please refer to Sec.~\ref{sec:shmp_supp} of the supplementary material for more details.

\subsection{Interaction-Enhanced Spatial-Temporal Graph Convolutional Network}\label{sec:network}
In principle, our proposed anchor-based sampling permits flexible network architectures. Here, to incorporate our multi-level spatial-temporal anchors, we naturally represent motion sequences as spatial-temporal graphs (to be precise,  spatial-frequency graphs), instead of the widely used spatial graphs~\cite{wei2019motion,mao2021generating}.  Our approach builds upon the Discrete Cosine Transform (DCT)~\cite{wei2019motion,mao2021generating} to transform the motion into the frequency domain. Specifically, given a past motion , where each pose has  joints, we first replicate the last pose  times to get . With the predefined  basis  for DCT, the motion is transformed as


We formulate  in the -th layer and latent features in any -th graph layer as spatial-temporal graphs  with  nodes. We specify the node  by the 2D index  for the joint  with frequency  component. The edge  associated with the interaction between node  and node  is represented by , where the adjacency matrix  is learnable. We bottleneck spatial-temporal interactions as~\cite{Sofianos_2021_ICCV}, by factorizing the adjacency matrix into the product of low-rank spatial and temporal matrices . The spatial adjacency matrix  connects only nodes with the same frequency. And the frequency adjacency matrix  is merely responsible for the interaction between the nodes that represent the same joint.



The spatial-temporal graph can be conveniently encoded by a graph convolutional network (GCN). Given a set of trainable weights  and activation function , such as ReLU, a spatial-temporal graph convolutional layer projects the input from  to  dimensions by

where  denotes the latent feature of the prediction  at -th layer. 
The backbone consists of multiple graph convolutional layers. After generating predicted DCT coefficients  reshaped from , where , we recover  via Inverse DCT (IDCT) as

where the last  frames of the recovered sequence represent future poses.

Conceptually, interactions between spatial-temporal nodes should be relatively invariant across layers, and different interactions should not be equally important. For example, we expect constraints and dependencies between ``left arm'' and ``left forearm,'' while the movements of ``head'' and ``left forearm'' are relatively independent.
We consider it redundant to construct a {\em complete} spatial-temporal graph for each layer {\em independently}. Therefore, we introduce cross-layer interaction sharing to share parameters between graphs in different layers, and spatial interaction pruning to prune the complete graph.

\noindent{\bf Cross-Layer Interaction Sharing.} 
Much care has been taken to employ learnable interactions between spatial nodes across all graph layers~\cite{Sofianos_2021_ICCV,wei2019motion,yan2021dmsgcn}.
We consider the spatial relationship to be relatively unchanged. Empirically, we find that sharing the adjacency matrix at intervals of one layer is effective. As shown in Fig.~\ref{fig:deterministic2}, we set  and .

\noindent{\bf Spatial Interaction Pruning.}
To emphasize the physical relationships and constraints between spatial joints, we prune the spatial connections  in each graph layer  using a predefined mask , where  is an element-wise product.
Inspired by~\cite{Liu_2021_ICCV}, we emphasize spatial locality based on skeletal connections and mirror symmetry tendencies. We denote our proposed predefined mask matrix as


Finally, our architecture consists of four original \textbf{STGCNs} without spatial pruning and four \textbf{Pruned STGCNs}, as illustrated in Fig.~\ref{fig:deterministic2}. Please refer to Sec.~\ref{sec:shmp_supp} of the supplementary material for more information on the architecture.
