\section{Experiments}
\label{sec:experiments}
As the key contributions of this work are on Bi-Interaction pooling in neural network modelling and the design of NFM for sparse data prediction, we conduct experiments to answer the following research questions:
\begin{itemize}
	\item[\textbf{RQ1}] Can Bi-Interaction pooling effectively capture the second-order feature interactions? How does dropout and batch normalization work for Bi-Interaction pooling? 
	\item[\textbf{RQ2}] Are hidden layers of NFM useful for capturing higher-order interactions between features and improving the expressiveness of FM? 
	\item[\textbf{RQ3}] How does NFM perform as compared to higher-order FM and the state-of-the-art deep learning methods Wide\&Deep and DeepCross?
\end{itemize}

In what follows, we first present the experimental settings, followed
by answering the above research questions one by one.

\subsection{Experimental Settings}
\subsubsection{\textbf{Datasets}} We experimented with two publicly accessible datasets: Frappe\footnote{\url{http://baltrunas.info/research-menu/frappe}} and MovieLens\footnote{\url{http://grouplens.org/datasets/movielens/latest}}: \\\vspace{-5pt}

\textbf{1. Frappe}.  Frapp\'e is a context-aware app discovery tool. This dataset is constructed by Baltrunas \etal~\cite{FrappeData}. It contains $96,203$ app usage logs of users under different contexts. Besides user ID and app ID, each log contains 8 context variables, including weather, city and daytime (\eg morning or afternoon). We converted each log (\ie user ID, app ID and all context variables) to a feature vector using one-hot encoding, resulting in $5,382$ features in total. A target value of 1 means the user has used the app under the context.  


\textbf{2. MovieLens}. This is the $Full$ version of the latest MovieLens data published by GroupLens~\cite{MovielensData}. As this work concerns higher-order interactions between features, we study the task of personalized tag recommendation rather than collaborative filtering~\cite{He:WWW2017} that considers the second-order interactions only. The tagging part of the data includes $668,953$ tag applications of $17,045$ users on $23,743$ items with $49,657$ distinct tags. We converted each tag application (\ie user ID, movie ID and tag) to a feature vector, resulting in $90,445$ features in total. A target value of 1 means the user has assigned the tag to the movie. \\\vspace{-5pt}

As both original datasets contain positive instances only (\ie all instances have target value 1), we sampled two negative instances to pair with one positive instance to ensure the generalization of the predictive model. 
For each log of Frappe, we randomly sampled two apps that the user has not used in the context; for each tag application of MovieLens, we randomly sampled two tags that the user has not assigned to the movie. 
Each negative instance is assigned to a target value of $-1$. Table~\ref{tab:dataset} summarizes the statistics of the final evaluation datasets. 

\begin{table}[t]
	\begin{center}
		\caption{\textbf{Statistics of the evaluation datasets.}}
		\vspace{-10pt}
		\label{tab:dataset}
		\begin{tabular}{ | l | c | c | c | c | }
			\hline
			\textbf{Dataset} & \textbf{Instance\#} & \textbf{Feature\#} & \textbf{User\#} & \textbf{Item\#} \\ \hline
			Frappe	& 288,609 & $5,382$ & 957 & 4,082\\ \hline
			MovieLens	& 2,006,859	& $90,445$ & 17,045 & 23,743 \\ \hline
		\end{tabular}
		\vspace{-15pt}
	\end{center}
\end{table}

\subsubsection{\textbf{Evaluation Protocols}} We randomly split the dataset into training (70\%), validation (20\%), and test (10\%) sets. The validation set was used for tuning hyper-parameters and the final performance comparison was conducted on the test set.
The study of NFM properties (\ie the answering of RQ1 and RQ2) was performed on the validation set, which can also reflect how we choose the optimal hyper-parameters. 
To evaluate the performance, we adopted \textit{root mean square error} (RMSE), where a lower RMSE score indicates a better performance. 
Note that RMSE has been widely used for evaluating regression tasks such as recommendation with explicit ratings~\cite{cao2017tois,fastFM} and click-through rate prediction~\cite{ImportanceFM}. 
We rounded up the prediction of each model to $1$ or $-1$ if it was out of the range. The one-sample paired t-test was performed to judge the statistical significance where necessary. 

\subsubsection{\textbf{Baselines}} 
We implemented NFM using TensorFlow\footnote{Codes are available at  \url{https://github.com/hexiangnan/neural_factorization_machine}}. We compared with the following competitive embedding-based models that are specifically designed for sparse data prediction:

- \textbf{LibFM}~\cite{libFM}. This is the official implementation\footnote{\url{http://www.libfm.org/}} of FM released by Rendle. It has shown strong performance for personalized tag recommendation and context-aware prediction~\cite{fastFM}. We used the SGD learner for a fair comparison with other methods which were all optimized with SGD (or its variants). 

- \textbf{HOFM}. This is the TensorFlow implementation\footnote{\url{https://github.com/geffy/tffm}} of higher-order FM, as described in \cite{FM}. We experimented with order size 3, since the MovieLens data concerns the ternary relationship between users, movies and tags.


- \textbf{Wide\&Deep}~\cite{WideDeep}. As introduced in Section~\ref{sec:dnn}, the deep part first concatenates feature embeddings, followed by a MLP to model feature interactions. As the structure of a DNN is difficult to be fully tuned, we used the same structure as reported in their paper, which has three layers with size $1024, 512$ and $256$, respectively. While the wide part (which is a linear regression model) is subjected to design to incorporate cross features, 
we used the raw features only for a fair comparison with FM and NFM. 

- \textbf{DeepCross}~\cite{DeepCross}. It applies a multi-layered residual network on the concatenation of feature embeddings for learning feature interactions. We used the same structure as reported in their paper, which stacks 5 residual units (each unit has two layers) with the hidden dimension $512, 512, 256, 128$ and $64$, respectively. 

\subsubsection{\textbf{Parameter Settings.}}
To fairly compare models' capability, we learned all models by optimizing the square loss (Equation (\ref{eq:squared_loss})). The learning rate was searched in $[0.005, 0.01, 0.02, 0.05]$ for all methods. 
To prevent overfitting, we tuned the $L_2$ regularization for linear models LibFM and HOFM in $[1e^{-6},5e^{-6},1e^{-5},...,1e^{-1}]$, and the dropout ratio for neural network models Wide\&Deep, DeepCross and NFM in $[0,0.1,0.2,...,0.9]$. 
Note that we found that dropout can well regularize the hidden layers of Wide\&Deep and NFM; however it did not work well for the residual units of DeepCross. 
Besides LibFM that optimized FM with the vanilla SGD, all other methods were optimized with mini-batch Adagrad~\cite{Adagrad}, where the batch size was set to 128 for Frappe and 4096 for MovieLens. Note that the batch size was selected by considering both training time and convergence rate, as a larger batch size usually led to faster training per epoch but slower convergence. 
For all methods, the early stopping strategy was performed, where we stopped training if the RMSE on validation set increased for 4 successive epochs. 
Without special mention, we show the results of embedding size 64, and more results of larger embedding sizes are shown in Section~\ref{ss:performance_comparison}.

\subsection{Study of Bi-Interaction Pooling (RQ1)}
We empirically study the Bi-Interaction pooling operation. 
To avoid other components (\eg hidden layers) affecting the analysis, we study the NFM-0 model that directly projects the output of Bi-Interaction pooling to prediction score with no hidden layer. As discussed in Section~\ref{ss:nfm_generalizes_fm}, NFM-0 is identical to FM as the trainable $\textbf{h}$ does not impact model's expressiveness. We first compare dropout with traditional $L_2$ regularization for preventing model overfitting, and then explore the impact of batch normalization. 

\subsubsection{\textbf{Dropout Improves Generalization}}
\label{ss:exper_dropout}
\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/dropout_reg_frappe.pdf}
		\vspace{-18pt}
\label{fig:dropout_reg_frappe}
	\end{subfigure} \hspace{-7pt}
	\begin{subfigure}[b]{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/dropout_reg_ml.pdf}
		\vspace{-18pt}
\label{fig:dropout_reg_ml}
	\end{subfigure} \hspace{-7pt}
	\caption{Validation error of NFM-0 \wrt dropout on the Bi-Interaction layer and $L_2$ regularization on embeddings.}
	\vspace{-15pt}
	\label{fig:dropout_reg}
\end{figure}

Figure~\ref{fig:dropout_reg} shows the validation error of NFM-0 \wrt dropout ratio on the Bi-Interaction layer and $L_2$ regularization on feature embeddings. The performance of linear regression (LR) is also shown for benchmarking the performance of prediction that does not consider feature interactions. 
First, LR leads to very poor performance, highlighting the importance of modelling interactions between sparse features for prediction. 
Second, we see that both $L_2$ regularization and dropout can well prevent overfitting and improve NFM-0's generalization to unseen data. 
Between the two strategies, dropout offers better performance. Specifically, on Frappe, using a dropout ratio of 0.3 leads to a lowest validation error of $0.3562$, which is significantly better than that of $L_2$ regularization $0.3799$.
One reason might be that enforcing $L_2$ regularization only suppresses the values of parameters in each update numerically, while using dropout can be seen as ensembling multiple sub-models~\cite{srivastava2014dropout}, which can be more effective. 
Considering the genericity of FM that subsumes many factorization models, we believe this is a new interesting finding, meaning that dropout can also be an effective strategy to address overfitting of linear latent-factor models. 

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/dropout_converge_frappe.pdf}
		\vspace{-20pt}
\label{fig:dropout_converge_frappe}
	\end{subfigure} 
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/dropout_converge_ml.pdf}
		\vspace{-20pt}
\label{fig:dropout_converge_ml}
	\end{subfigure} 
	\caption{Training and validation error of each epoch of NFM-0 with and without dropout on Bi-Interaction layer.}
	\vspace{-10pt}
	\label{fig:dropout_converge}
\end{figure}

To be more clear about the effect of dropout, we show the training and validation error of each epoch of NFM-0 with and without dropout in Figure \ref{fig:dropout_converge}. Both datasets show that with a dropout ratio of 0.3, although the training error is higher, the validation error becomes lower. This demonstrates the ability of dropout in preventing overfitting and as such, better generalization can be achieved. 

\subsubsection{\textbf{Batch Normalization Speeds up Training}} 
\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/batchnorm_frappe.pdf}
		\vspace{-24pt}
\label{fig:batchnorm_frappe}
	\end{subfigure} 
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/batchnorm_ml.pdf}
		\vspace{-24pt}
\label{fig:batchnorm_ml}
	\end{subfigure}
	\caption{Training and validation error of each epoch of NFM-0 with and without BN on the Bi-Interaction layer.}
	\vspace{-10pt}
	\label{fig:batchnorm}
\end{figure}

Figure~\ref{fig:batchnorm} shows the training and validation error of each epoch of NFM-0 with and without BN on the Bi-Interaction layer. The dropout is enabled with a ratio of 0.3, and the learning rate is set to 0.02. Focusing on the training error, we can see that BN leads to a faster convergence; on Frappe, when BN is applied, the training error of epoch 20 is even lower than that of epoch 60 without BN; and the validation error indicates that the lower training error is not overfitting --- in fact, \cite{cvpr16best,ioffe2015batch} showed that by addressing the internal covariate shift with BN, the model's generalization ability can be improved. Our result also verifies this point, where using BN leads to slight improvement (although the improvement is not statistically significant). 
Furthermore, we notice that BN makes the learning less stable, as evidenced by the larger performance fluctuation of blue lines. 
This is caused by our use of dropout and BN together, as randomly dropping neurons can change the input distribution normalized by BN. It is an interesting direction to effectively combine BN and dropout. 


\subsection{Impact of Hidden Layers (RQ2)}

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/layers_frappe.pdf}
		\vspace{-20pt}
\label{fig:layers_frappe}
	\end{subfigure} \hspace{-7pt}
	\begin{subfigure}[b]{0.24\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/layers_ml.pdf}
		\vspace{-20pt}
\label{fig:layers_ml}
	\end{subfigure} \hspace{-7pt}
	\caption{Validation error of LibFM, NFM-0 and NFM with different activation functions on the first hidden layer.}
	\vspace{-10pt}
	\label{fig:layers}
\end{figure}


The hidden layers of NFM play a pivotal role in capturing higher-order interactions between features. 
To explore the impact, we first add one hidden layer above the Bi-Interaction layer and slightly overuse the name NFM to indicate this specific model. 
To ensure the same model capability with NFM-0, we set the size of hidden layer the same as the embedding size.

Figure~\ref{fig:layers} shows the validation error of NFM \wrt different activation functions and dropout ratios for the hidden layer. The performance of LibFM and NFM-0 are also shown for benchmarking purposes. 
First and foremost, we observe that by using non-linear activations, NFM's performance is improved with a large margin --- compared to NFM-0 which has a similar performance with LibFM, the relative improvement is $11.3\%$ and $5.2\%$ for Frappe and MovieLens, respectively. 
This highlights the importance of modelling higher-order feature interactions for quality prediction. 
Among the different non-linear activation functions, there is no obvious winner. 
Second, when we use the identity function as the activation function, \ie the hidden layer performs a linear transformation, NFM does not perform that well. 
This provides evidence to the necessity of learning higher-order feature interactions with non-linear functions. 

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/pretrain_frappe.pdf}
		\vspace{-22pt}
\label{fig:pretrain_frappe}
	\end{subfigure} 
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/pretrain_ml.pdf}
		\vspace{-22pt}
\label{fig:pretrain_ml}
	\end{subfigure} 
	\caption{Training and validation error of each epoch of NFM-1 with and without pre-training.}
	\vspace{-12pt}
	\label{fig:pretrain}
\end{figure}

To see whether a deeper NFM can further improve the performance, we stack more ReLU layers above the Bi-Interaction layer. 
As it is computationally expensive to tune the size and dropout ratio for each hidden layer separately, we use the same setting for all layers and tune them the same way as NFM-1. 
As can be seen from Table~\ref{tab:layers}, when we stack more layers, the performance is not further improved, and best performance is when we use one hidden layer only. 
We have also explored other designs for hidden layers,
such as the tower structure and residual units, however, the performance is still not improved. 
We think the reason is because the Bi-Interaction layer has encoded informative second-order feature interactions, and based on which, a simple non-linear function is sufficient to capture higher-order interactions. 
To verify this, we replaced the Bi-Interaction layer with concatenation (which leads to the same architecture as Wide\&Deep), and found that the performance can be gradually improved with more hidden layers (up to three); however, the best performance achievable is still inferior to that of NFM-1.
This demonstrates the value of using a more informative operation for low-level layers, which can ease the burden of higher-level layers for learning meaningful information. As a result, a deep structure becomes not necessarily required.

\begin{table}[h]
	\vspace{-10pt}
	\begin{center}
		\caption{NFM \wrt different number of hidden layers.}
		\vspace{-10pt}
		\label{tab:layers}
		\begin{tabular}{| l | c | c |} \hline
			\textbf{Methods} & \textbf{Frappe} & \textbf{MovieLens} \\ \hline\hline
			NFM-0 	& 0.3562  & 0.4901 \\ \hline
			NFM-1 	& \textbf{0.3133}  & \textbf{0.4646} \\ \hline
			NFM-2 	& 0.3193  & 0.4681 \\ \hline
			NFM-3 	& 0.3219 & 0.4752 \\ \hline
			NFM-4 	& 0.3202  & 0.4703 \\ \hline
		\end{tabular}
	\end{center}
	\vspace{-10pt}
\end{table}



\begin{table*}[t]
	\small
	\begin{center}
		\caption{Test error and number of trainable parameters for different methods on latent factors 128 and 256. M denotes ``million''; $*$ and $**$ denote the statistical significance for $p<0.05$ and $p<0.01$, respectively, compared to the best baseline. }
		\vspace{-10pt}
		\label{tab:performance}
		\begin{tabular}{| l | c | c | c | c | c | c | c | c |} \hline
			& \multicolumn{4}{c|}{\textbf{Frappe}} & \multicolumn{4}{c|}{\textbf{MovieLens}} \\ \hline 
			& \multicolumn{2}{c|}{\textbf{Factors=128}} & \multicolumn{2}{c|}{\textbf{Factors=256}} & \multicolumn{2}{c|}{\textbf{Factors=128}} & \multicolumn{2}{c|}{\textbf{Factors=256}}\\ \hline 
			\textbf{Method} & \textbf{Param\#} & \textbf{RMSE} & \textbf{Param\#} & \textbf{RMSE} & \textbf{Param\#} & \textbf{RMSE} & \textbf{Param\#} & \textbf{RMSE} \\ \hline\hline
			LibFM~\cite{libFM} 	& 0.69M  & 0.3437 & 1.38M  & 0.3385 & 11.67M  & 0.4793 & 23.24M  & 0.4735 \\ \hline
			HOFM & 1.38M  & 0.3405 & 2.76M & 0.3331 & 23.24M & 0.4752
			& 46.40M & 0.4636 \\ \hline\hline
			Wide\&Deep~\cite{WideDeep} &  2.66M  & 0.3621 & 4.66M & 0.3661 & 12.72M & 0.5323 & 24.69M  & 0.5313 \\ \hline
			Wide\&Deep~(pre-train) &  2.66M  & 0.3311 & 4.66M & 0.3246  & 12.72M & 0.4595 & 24.69M  & 0.4512 \\ \hline \hline
			DeepCross~\cite{DeepCross}	& 4.47M  & 0.4025 & 8.93M & 0.4071  & 12.71M & 0.5885 & 25.42M  & 0.5907 \\ \hline
			DeepCross~(pre-train)	& 4.47M  & 0.3388 & 8.93M & 0.3548  & 12.71M & 0.5084 & 25.42M  & 0.5130  \\ \hline \hline
			\textbf{NFM} & \textbf{0.71M} & $\textbf{0.3127}^{**}$ & \textbf{1.45M}  & $\textbf{0.3095}^{**}$ & \textbf{11.68M} & $\textbf{0.4557}^*$ & \textbf{23.31M}  & $\textbf{0.4443}^*$\\ \hline
		\end{tabular}
	\end{center}
	\vspace{-8pt}
\end{table*}

\begin{figure*}[t]
	\centering
	\begin{subfigure}[b]{0.36\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/performance_frappe.pdf}
		\vspace{-15pt}
\label{fig:performance_frappe}
	\end{subfigure} \hspace{+55pt}
	\begin{subfigure}[b]{0.36\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/performance_ml.pdf}
		\vspace{-15pt}
\label{fig:performance_ml}
	\end{subfigure} \hspace{+7pt}
	\caption{Performance comparison on the test set \wrt different embedding sizes. LibFM, HOFM and HOFM are trained from random initialization; Wide\&Deep and DeepCross are pre-trained with FM feature embeddings.}
	\vspace{-10pt}
	\label{fig:performance}
\end{figure*}

\subsubsection{\textbf{Pre-training Speeds up Training}}
It is known that parameter initialization can greatly affect the convergence and performance of DNNs~\cite{erhan2010pretrain,He:WWW2017}, since gradient-based methods can only find local optima for DNNs. 
As have been shown in Section~\ref{ss:difficulty_dnn}, initializing with feature embeddings learned by FM can significantly enhance Wide\&Deep and DeepCross. Now the question arises, how does pre-training impact NFM? 

Figure~\ref{fig:pretrain} shows the state of each epoch of NFM-1 with and without pre-training.
First, we can see that by using FM embeddings as pre-training, NFM exhibits extremely fast convergence --- on both datasets, with 5 epochs only, the performance is on par with 40 epochs of NFM that is trained from scratch (with BN enabled).
Second, we find that pre-training does not improve NFM's final performance, and a random initialization can achieve a result that is slightly better than that with pre-training. 
This demonstrates the robustness of NFM, which is relatively insensitive to parameter initialization. 
In contrast to the huge impact of pre-training on Wide\&Deep and DeepCross (\cf Figure~\ref{fig:difficulty}) that improves both their convergence and final performance, we draw the conclusion that NFM is much easier to train and optimize, which is due largely to the informative and effective Bi-Interaction pooling operation. \vspace{-5pt}

\subsection{Performance Comparison (RQ3)}
\label{ss:performance_comparison}
We now compare with state-of-the-art methods. 
For NFM, we use one hidden layer with ReLU as the activation function, since the baselines DeepCross and Wide\&Deep also choose ReLU in their original papers. Note that the most important hyper-parameter for NFM is the dropout ratio, which we use $0.5$ for the Bi-Interaction layer and tune the value for the hidden layer. 

Figure~\ref{fig:performance} plots the test RMSE \wrt different number of latent factors (\ie embedding sizes), where Wide\&Deep and DeepCross are pre-trained with FM to better explore the two methods. 
Table~\ref{tab:performance} shows the concrete scores obtained on factors 128 and 256, and the number of model parameters of each method. The scores of Wide\&Deep and DeepCross without pre-training are also shown. We have the following three key observations. 

First and foremost, NFM consistently achieves the best performance on both datasets with the fewest model parameters besides FM. This demonstrates the effectiveness and rationality of NFM in modelling higher-order and non-linear feature interactions for prediction with sparse data. 
The performance is followed by Wide\&Deep, which uses a 3-layer MLP to learn feature interactions. We have also tried deeper layers for Wide\&Deep, however the performance has not been improved. This further verifies the utility of using the informative Bi-Interaction pooling in the low level. 

Second, we observe that HOFM shows slight improvement over FM with $1.45\%$ and $1.04\%$ average improvement on Frappe and MovieLens, respectively. This sheds light on the limitation of FM that models only the second-order feature interactions, and thus the usefulness of modelling higher-order interactions. Meanwhile, the large performance gap between HOFM and NFM reflects the value of modelling higher-order interactions in a non-linear way, since HOFM models higher-order interactions linearly and uses much more parameters than NFM. 

Lastly, the relatively weak performance of DeepCross reveals that deeper learnings are not always better, as DeepCross is the deepest method among all baselines that utilizes a 10-layer network. 
On Frappe, DeepCross only achieves a comparable performance with the shallow FM model, while it underperforms FM significantly on MovieLens. 
We believe that the reasons are due to optimization difficulties and overfitting (as evidenced by the worse performance on factors 128 and 256). \\\vspace{-8pt}

