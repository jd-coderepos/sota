\documentclass[a4paper,11pt,oneside,english,onecolumn]{article}
\usepackage[lmargin=1in,rmargin=1in,tmargin=1in,bmargin=1in]{geometry}

\usepackage{lineno}
\usepackage{authblk}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{breqn}
\usepackage[english]{babel}
\usepackage[english,vlined,ruled,boxed,commentsnumbered]{algorithm2e}
\usepackage{tikz}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{properties}{Properties}

\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{problem}{Problem}

\newcommand{\br}[1]{\left\{#1\right\}}                            \renewcommand{\Pr}[1]{\ensuremath{\mathbf{Pr}\left[#1\right]}}
\newcommand{\Ex}[1]{\ensuremath{\mathbf{E}\left[#1\right]}}
\newcommand{\indic}{\mathbf 1}
\newcommand{\dotProd}[2]{\ensuremath{\left\langle #1 , #2 \right\rangle }}

\newcommand{\dist}[0]{\ensuremath{\mathrm{dist}}}
\newcommand{\bigO}[1]{\ensuremath{O\left( #1 \right)}}
\newcommand{\norm}[1]{\ensuremath{\left\| #1\right\|_2}}
\newcommand{\normF}[1]{\ensuremath{\left\| #1\right\|_F}}
\newcommand{\normone}[1]{\ensuremath{\left\| #1\right\|_1}}
\DeclareMathOperator{\cost}{cost}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\Var}{Var}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}
\newcommand{\append}[1]{}


\renewcommand\Authfont{}
\newcommand{\REAL}{\ensuremath{\mathbb{R}}}
\renewcommand\Affilfont{\small}
\newcommand{\eps}{\ensuremath{\varepsilon}}

\sloppy

\title{Asymptotically exact streaming algorithms\thanks{Part of this work has been supported by Deutsche Forschungsgemeinschaft (DFG) within the Collaborative Research Center SFB 876 "Providing Information by Resource-Constrained Analysis",
project C4. Marc Heinrich acknowledges the support of \'Ecole Normale Sup\'erieure.}}
\author[1]{Marc Heinrich}
\affil[1]{D\'epartement d'Informatique,\authorcr
\'Ecole Normale Sup\'erieure, Paris, France\authorcr
		  \texttt{marc.heinrich@ens.fr}
}
\author[2]{Alexander Munteanu}
\author[2]{Christian Sohler}
\affil[2]{Department of Computer Science, TU Dortmund, Germany\authorcr
		  \texttt{\{alexander.munteanu,christian.sohler\}@tu-dortmund.de}
}

\setlength{\parindent}{0pt}

\begin{document}
\maketitle
\thispagestyle{empty}
\begin{abstract}
\noindent
We introduce a new computational model for data streams: asymptotically exact streaming algorithms. These algorithms have an approximation ratio that tends to one as the length of the stream goes to infinity while the memory used by the algorithm is restricted to  size. Thus, the output of the algorithm is optimal in the limit. We show positive results in our model for a series of important problems that have been discussed in the streaming literature. These include computing the frequency moments, clustering problems and least squares regression. Our results also include lower bounds for problems, which have streaming algorithms in the ordinary setting but do not allow for sublinear space algorithms in our model.\\

\noindent
\end{abstract}

\section{Introduction}
Streaming algorithms aim at solving problems in a setting where the input is given as a stream of items like numerical values, points in Euclidean space or edges of a graph and moreover it is not possible to store the entire input in the main memory. Usually a streaming algorithm is allowed only one pass over the data and its working memory is restricted to polylogarithmic size in the length of the stream \cite{Muthukrishnan05}. For most non-trivial problems, it is not possible to get an exact solution with these restrictions. However, we can focus on the design of efficient approximation algorithms. The seemingly best we can hope for in this situation is a -approximation. Such approximation algorithms have been developed for many interesting and important problems. Known results in this area cover a broad variety of computational problems, including -approximation algorithms for estimating the frequency moments of a stream of items \cite{ApproxFreqMom}, least squares regression, low-rank approximation \cite{LinAlgStream} and clustering \cite{ClusteringMotion}. These have many applications in machine learning, classification, data mining and other fields of research.

From an information theoretic as well as statistical perspective it seems natural to say that the more data is used in a learning task, the more precise our result will be. In this context one might think of the law of large numbers or central limit theorems. However, these arguments require the observed data to follow some fixed distribution and assume for example some underlying unknown mean value or covariance structure of the data.

In a recent work \cite{MichaelJordan2013} such arguments have been leveraged to say that more data can actually lead to more efficient learning. The basic idea supporting this hypothesis is that possibly non-convex learning tasks on a ground set  can be handled more efficiently by using convex relaxations  and even further relaxations  thereof. While this leads to considerably more efficient computations, the solution quality might decrease due to the relaxations. This is where the increase in the amount of data comes into play. Under certain distributional assumptions we can get a solution that satisfies the same bounds on the precision as approximating the initial problem without the relaxation. This is achieved by using an appropriate amount of additional data whose size depends on the complexity and dimension of the relaxed sets. The authors even claim a trade-off between computational cost and the amount of data used, but the technical part does not cover lower bounds to support this claim.

Our goal is to show that for many problems that have been studied in the streaming context we can also hope for improving error guarantees as we have access to more data. Firstly, our approach is distinguished from the above in that we do not impose or use statistical assumptions on the source of the data. In some cases we still have to put mild restrictions on the input stream but this is only due to the obvious fact that if some part of the result won't get enough data or only redundant data, i.e., there is no new information on that part, then we cannot hope to improve an error that we have already made. Secondly, we have lower bounds on the space complexity supporting that these assumptions are actually necessary, not only sufficient.

\paragraph*{Our model} The main purpose of the present paper is to introduce a novel model for the design of streaming algorithms. The above discussion raises the question whether we can develop streaming algorithms, which have a guarantee on the error that approaches zero as the length  of the stream tends to infinity, i.e., that have an approximation ratio of  for some , while the memory is still bounded by . As the space complexity of many problems in the streaming model is polynomial in  we might think of choosing . Given an algorithm in the usual streaming model, we could just fix  to such a value in advance. This means that we already have non-uniform approximation algorithms in the above sense. But this requires to know the length of the stream in advance. If otherwise, the length of the stream exceeds its pre-defined limit, the algorithm will fail to satisfy the desired approximation guarantee. Our intention is therefore to develop algorithms that are \emph{uniform} in terms of  and can deal with potentially infinite input streams. We will call such algorithms \emph{asymptotically exact streaming algorithms} according to the following definition.

\begin{definition}
A problem  with objective function  has an \emph{asymptotically exact streaming algorithm} if there exists a one-pass streaming algorithm that for an infinite input stream  and every  maintains a solution  that with probability at least  satisfies  where  is the optimal resp. exact solution to the substream of length  that has been read. The space complexity of the algorithm is bounded by .
\end{definition}
Asymptotically exact algorithms have been designed for several problems including bin packing \cite{BinPacking}, the traveling salesman problem \cite{TSP}, scheduling problems \cite{Scheduling} and pickup and delivery problems \cite{PDP}. However, to our knowledge, no such results exist in the streaming literature. Moreover, our approach is different, since we explicitly use the information from the input data to improve the approximation.

Our definition also gives rise to a stronger notion of approximation in the streaming context. Consider for example the problem of approximating the  frequency moments  of items that are read from an input stream. While we can give an asymptotically exact streaming algorithm for the case , our results show that no such algorithm can exist for the case . Note that both cases allow for  streaming algorithms for any fixed  \cite{ApproxFreqMom,Bar-YossefJKST02}.

\paragraph*{Our results}

We will study the following problems in the setting of asymptotically exact streaming algorithms.

\begin{problem}[\emph{ estimation}] Let  be a sequence of  integers from . The task is to compute a -estimate of the  moment 
is the number of elements in the sequence that are equal to .
\end{problem}

The task of estimating the frequency moments of a sequence arises in the analysis of network traffic and covers some applications on large databases and statistical data analysis. It has been shown in \cite{ImpFreqLowBound} that estimating the  frequency moment requires polynomial space for . However, for , the frequency moments can be estimated using only polylogarithmic space. According algorithms and negative results were given by Alon, Matias and Szegedy in their seminal paper \cite{ApproxFreqMom} on the space complexity of estimating the frequency moments.
Using their sketching techniques, we can give an asymptotically exact streaming algorithm for maintaining an estimate on the second frequency moment of an infinite data stream within  error, where . Our algorithm uses  memory words.
We also have negative results regarding the frequency moments. We are able to show that there exists no asymptotically exact streaming algorithm for estimating , since any such algorithm must use  space. We have similar results regarding any  if we allow for insertion and also deletion of elements.
The lower bounds on the space complexity are derived by reduction from the disjointness problem, which is known to have linear communication complexity.

Another interesting problem that has many applications in data analysis, compression and classification is the clustering problem.

\begin{problem}[\emph{clustering}] Given a set  of  points, and an integer , find a set  of  centers closest to the input set . More precisely, the task is to minimize one of the following quantities

where  is the minimum distance to a set of points, i.e., .
\end{problem}

The first coreset constructions for the -means and -median problems are described in \cite{FirstCoreset}, \cite{HPClustering} and \cite{SmallerCoresets}. These coresets can be seen as a small size representation of the original point set such that for any choice of centers their cost is approximated up to . Thus, solving the problem exactly on the small size coreset yields a solution that is within multiplicative -error to the optimal solution. More recently, a dynamic coreset construction was designed in \cite{DynamicCoreset}. \cite{PolyCoreset} gives the first coreset construction with a polynomial dependency on the dimension, which was further improved in \cite{ImproveCoreset1} and \cite{Feldman} resulting in coresets of size  for -median. Another construction described in \cite{TinyData} returns a coreset of size  for -means clustering. Note that the size of the coreset does not depend on  or . The coreset is built by first projecting the input points into a lower dimensional subspace and then applying a coreset construction from \cite{Feldman}.
Our results for clustering are asymptotically exact streaming algorithms for the -means as well as -median problems that are based on maintaining coresets and computing the centers only on these coresets. The results are generic with respect to the coreset construction that is used. Given any such coreset construction of size , our algorithm constructs a summary of size  and approximates the problem based on this small set to get a solution that is within  to the optimal. 
The analysis is conducted by showing that the obtained solution from the summary is close to the optimum. We need the additional assumption that the number of points that is associated with each center increases with the number of points from the stream. This assumption is necessary as we can show by a complementing lower bound. While under this mild assumption, we can give positive results for the -means and -median problems, we have further lower bounds showing that no asymptotically exact streaming algorithm can exist for the -center clustering problem even for  and in  dimensions.

Another problem that has been discussed extensively in the streaming literature is the least squares regression problem.
\begin{problem}[\emph{least squares regression}]
Let  be an  matrix and  be a column vector of size . Find a solution  such that

where  is an optimal solution.
\end{problem}

Regression is a very important problem used in machine learning and statistics to study the dependency between variables. The most efficient algorithms for solving regression problems with little time and space are due to the early works of Sarl\'os \cite{ImpLinAlg} as well as Clarkson and Woodruff \cite{LinAlgStream} and have been further optimized and generalized in the last years. Their approach is to apply space and time efficient versions of the well known Johnson-Lindenstrauss transform \cite{JLT} as a dimensionality reduction technique to reduce the space and time bounds of their algorithms.

Using the same kind of sketching technique based on random linear projections we are able to develop an asymptotically exact streaming algorithm for the least squares regression problem. On the technical part we give an asymptotically exact streaming algorithm for maintaining a sketch that allows for matrix multiplication as well as embedding a linear subspace in our setting. These problems serve as building blocks to derive the result on least squares regression as shown in \cite{ImpLinAlg,LinAlgStream}. Following the outline of these references we still have to add some additional arguments regarding the columnbasis given by the singular value decomposition (SVD) of the input matrix to account for the improving approximation guarantee. This finally enables us to derive the positive result if the input is given row-by-row and the smallest singular value of the data matrix diverges with growing . Note that while the original references give algorithms in the most general model of turnstile updates \cite{Muthukrishnan05}, our input stream is much more restricted. However, on the negative side we are able to modify the lower bound arguments from \cite{LinAlgStream} to show that no asymptotically exact streaming algorithm can exists when the input matrix is given in the turnstile model even if every entry is modified only once, and
even under the additional assumption on the divergence of the smallest singular value.\\

The rest of the paper is organized as follows. In Section \ref{prelim} we give some basic definitions and repeat results that we will use in our proofs. In Section \ref{section:F2}, we describe and analyze an asymptotically exact algorithm for estimating the second frequency moment. In Section \ref{section:Clustering}, we describe our clustering algorithm for the -means and -median problems with improving precision. In Section \ref{section:Regression}, we describe an asymptotically exact algorithm for regression. The analysis is adapted from \cite{LinAlgStream} to work in our setting. We also derive an algorithm with improving precision for matrix multiplication as a tool for solving the regression problem. Our lower bounds can be found in the corresponding sections and are mainly derived by reduction from communication complexity problems. We conclude our paper in Section \ref{conclusion}.




\section{Preliminaries}
\label{prelim}
Before we turn to our main results regarding the introduced problems, we state some preliminary definitions and tools that we will need in our proofs.

We will assume that the error parameter  and the failure probability  satisfy . For any integer  we will denote by  the set of all integers up to . For values  we will write  meaning that . For any two vectors  let  denote the inner product of  and . Throughout the paper we will consider two different matrix norms, the Frobenius norm and the operator norm.

\begin{definition}[matrix norms]
For a matrix  the Frobenius norm is defined by  and the spectral norm is given by 
\end{definition}
Note that in the special case of a vector , both matrix norms coincide with the Euclidean vector norm or length of , i.e., 

In this paper we will make use of a sketching method described in \cite{RandomProj} that has also been used in \cite{ImpLinAlg} and \cite{LinAlgStream}. This method is based on random linear maps and is an improvement of the so called Johnson-Lindenstrauss transform from \cite{JLT}, using matrices that only consist of appropriately rescaled random entries from .

\begin{theorem}
\label{th:sketching}
Fix . Let  be a  matrix whose entries are independent random variables, taking values  or  with probability  each. Let . Then for an arbitrary vector  we have with probability  that

The entries on the same row of  only need to be -wise independent.
\end{theorem}

Using a technique from \cite{ImpLinAlg}, which consists in putting a grid of appropriate size on the unit ball and embedding the grid points, it was shown that we can have an embedding of a whole linear subspace using small size sketches. The space complexity of this task has been settled later in \cite{LinAlgStream} and \cite{NelsonN14}.
\begin{theorem}
\label{th:embedding}
Let  be a  sketching  matrix as in Theorem \ref{th:sketching} but with . Let  be an arbitrary  matrix. Then with probability  we have that

\end{theorem}

For our lower bounds we will use some standard results from two-party one-way communication complexity. Alice and Bob are given input strings . Their goal is to compute some boolean function  by exchanging as little information as possible. In one-way protocols, Alice sends a message and then Bob must compute the output based on this message and on its own input string. Let  the minimum amount of communication for a randomized two-party one-way protocol that computes  with error probability at most .

We will mainly reduce from the following two problems. In the indexing () problem Alice is given a string  and Bob has an index . Bobs task is to compute  based on Alice's message. In the disjointness problem (). The bit strings  received by Alice and Bob are interpreted as subsets of  where the subset  contains the element  if and only if . Bob's task is to decide whether the two sets are disjoint. We have the following results regarding their communication complexity.

\begin{theorem}(\cite{Disjointness})
\label{th:disjointness}

\end{theorem}

\begin{theorem}(\cite{CommComplexity})
\label{th:indexing}

\end{theorem}



\section{Estimating the frequency moments}
\label{section:F2}

Our first problem is to maintain an estimate on the second frequency moment for an infinite stream  of integers from . Let  denote the canonical basis of . We can associate the vector  of this base with each entry  of the input sequence. Then the frequency vector of  can be expressed as  and its second frequency moment is given by . The second moment of the vector  is equal to the squared Euclidean norm of , which can be estimated within a fixed precision using the sketching matrices from Theorem \ref{th:sketching}. We will make use of this technique to develop an asymptotically exact streaming algorithm for estimating the second frequency moment.

In our first lemma we show that we will get a good estimate if we simply discard a small prefix of the input stream. More precisely, if the part that we discard is sufficiently smaller than the square root of the total size, then we will get a good estimate for the entire sequence.
\begin{lemma}
\label{lemma:F2}
Let  be an input sequence of size  composed as the concatenation of two parts  and  of size  and . Let  be the frequency vector of the subsequence . Let  be a sketching matrix according to Theorem \ref{th:sketching} with  rows for some absolute constant . Then with probability at least  we have that  where .
\end{lemma}

\begin{proof}
First observe that using the result of Theorem \ref{th:sketching} we have with probability  that  Thus, the upper bound of our claim follows immediately from . For the lower bound, let  denote the frequency vector of  and observe that we have

Now, note that  and similarly . This finally yields 

\end{proof}

We can already devise a quite simple algorithm from this lemma. Assume that we have already processed a first part . We can continue to read a number of elements that is large enough to form the second part, such that the contribution of  will be negligible. Then, we simply compute a sketch for  with a better precision and use it as an approximation for the whole sequence. After reading enough elements, Lemma \ref{lemma:F2} ensures that we can bound the error that we make in the process.

Now, we would have a problem if we reached the point to report an estimate before the appropriate number of new elements has been read. The old sketch for  would not meet the error bounds for the whole sequence because of the elements that have only been inserted into the second sketch and the sketch for  would not use enough elements for the first part to be negligible. So instead, we continue to update both the first sketch and the new sketch with the elements of the sequence. If we meet the point to report before the second sketch contains enough elements, we just ignore it and return the estimate of the first sketch. This gives the following theorem.

\begin{theorem}
\label{th:F2}
There exists an asymptotically exact streaming algorithm that with probability at least  maintains an estimate on the second frequency moment of a sequence within  relative error, where . The algorithm uses  memory words.
\end{theorem}

\begin{proof}
To prove the claim, we can choose , and . Then by applying Lemma \ref{lemma:F2}, we obtain an approximation of the second moment within  relative error where  because . Renaming  and folding the constant factor into the memory requirements gives the required result. 
As noted in \cite{ApproxFreqMom}, the columns of the sketching matrices only need to be -wise independent. Using this, each line of the matrix can be stored implicitly using only  bits of memory, i.e., one memory word. So, we only need  memory to store the sketching matrix at step . Since each of the entries of the sketched vectors have values smaller than , they can be stored in one memory word, so this is also the memory required to store the sketched vector. The memory used then follows from the fact that at any moment we only need to keep two sketching matrices.
\end{proof}

One drawback is that, in order to get a better estimate, we need to process a very large number of elements, namely more than . We can reduce that by computing several sketches in parallel, at the cost of using slightly more memory. The algorithm is described in Algorithm \ref{algo:F2}. We use the bound of Lemma \ref{lemma:F2} to determine which sketch has the best error guarantee. Also note that after a certain number of steps, some of the sketches are no longer needed because some other sketch that we have started later has already reached a better error bound. In that case we can simply discard the old one instead of continuing to update it.

\begin{algorithm}
\DontPrintSemicolon \KwIn{A sequence  of integers, }
\KwOut{An approximation of }
 \;
\While{not \emph{End of Stream}} {
Start a new sketch with precision \;
  Process the next  items for all sketches \;
   \;
}
\Return estimate with smallest error bound according to Lemma \ref{lemma:F2}\;
\caption{Improving algorithm for  estimation}
\label{algo:F2}
\end{algorithm}

Compared to the previous method, where we keep only two sketches at any moment, the precision increases more often. Indeed, if one sketch is started after  elements have been processed (these elements are dropped for this sketch), then it will require less than  more elements to become valid. The next sketch is started after  elements are processed, and it will require  more elements to become valid. So we get a better approximation every time the size of the input is multiplied by 8. However, we need to keep  sketches at any time, so the memory used by this algorithm is increased by a factor of at most .

\subsection{Lower bounds on estimating the frequency moments}
While we have a positive result on -estimation, we now show that there exists no asymptotically exact streaming algorithm for -estimation. This is particularly interesting since in the usual streaming setting both problems allow for -approximation algorithms.

\begin{theorem}
\label{lem:fzero}
Any asymptotically exact streaming algorithm for estimating  requires at least  memory.
\end{theorem}

\begin{proof}
We reduce from the disjointness problem, which has linear communication complexity \cite{Disjointness}. Assume that we have an improving algorithm  to compute an approximation of  using  memory. Then we create the following protocol for the disjointness problem. Alice and Bob interpret their input strings  as sets of integers , resp.  and they have to decide whether . Alice runs the algorithm on the integers from  and sends the memory of size at most  to Bob, as well as the number of elements  which can be coded in  bits. Bob continues the execution of the algorithm, first inserting his own input set  once, and then inserting any element from  a great number of times. He will eventually reach a point where the error on the estimate  returned by the algorithm satisfies , since the number of distinct elements is at most . So Bob knows the exact number of distinct elements from the sequence. By comparing this quantity to , Bob can decide whether the two sets are disjoint. This means that  implying .
\end{proof}

We can show a similar bound for  estimation in the dynamic setting. This means in particular that our above algorithm can not be extended to work under insertions \emph{and} deletions.

\begin{theorem}
\label{lem:fk}
Any asymptotically exact streaming algorithm for estimating ,  under insertions and deletions requires at least  memory.
\end{theorem}

\begin{proof}
The argument is very similar to the previous one. Again we reduce from the disjointness problem. Alice and Bob are given binary strings of size  that they interpret as subsets of . Then, Alice runs the algorithm on her input and communicates to Bob the memory of size  as well as , the number of elements in her set. Then Bob can continue the execution of the algorithm on his own input. Hereafter, he feeds a sequence consisting in repeatedly adding and removing the same element to the algorithm. Doing that a great number of times does not change the value of the frequency moments, but after some time, the error is small enough to determine  exactly. Its value will be equal to  if and only if the sets are disjoint. Thus, the number of bits exchanged by the two parties is at least .
\end{proof}

\section{Clustering}
\label{section:Clustering}

In this section we develop and analyze asymptotically exact streaming algorithms for the -means and -median clustering problems based on so called coresets. We also give lower bounds for these problems, as well as for the -center problem. Let  be an infinite sequence of points and as previously let  denote the first  points of the sequence that acts as the input stream to our algorithm. The task is to find a set  consisting of  points that minimizes one of the following cost functions

Here the distance of one point to a set of point is the minimal distance of this point to any point in the set, i.e.,  One way to solve the above problems approximately within a fixed precision is to use coreset constructions.
\begin{definition}[\cite{HPClustering}]
Let  be a set of points. A possibly weighted set of points  is an -coreset for  if for any set  of  centers we have 
\end{definition}
A coreset is often chosen to be a weighted subset of the original points and is typically smaller than . So, a coreset acts as a summary that has approximately the same behaviour as the original point set regarding the considered problem.
The idea of a clustering algorithm based on coresets is that if a coreset contains very few points, then it is easy to compute the optimal for the coreset and then use this optimal as an approximate solution for the original point set. The property of the coreset then ensures that we have found a -approximate solution to the original.
We now want to solve the -means and -median problems with a decreasing error using coresets. The algorithm is generic with regard to the actual coreset construction being used. Our negative results from Section \ref{section:negativeClustering} show that it is not possible to have asymptotically exact algorithms in the general case. However, we can still have an asymptotically exact algorithm if we impose some mild assumptions on our input point set.

In the following, we will focus on the -means problem and then explain how the analysis can be adapted to work also for -median. In order to simplify notations, the mean cost will be denoted by \emph{cost} omitting the subscript.

Algorithm \ref{algo:clustering} works as follows. It splits the input stream into blocks, where the block at step  is of size  and then applies a coreset construction to each of these blocks. Finally, as a summary for the whole sequence, the algorithm returns the union of the coresets obtained at each step. If the coreset construction that is used also has a failure probability, then we choose the size of the coreset in such a way that at step  this failure probability is at most  for a large enough constant . By the union bound, the probability that the algorithm succeeds for all steps is at least . We will denote by  the set of points considered at step , of size  and by  the coreset built from . Also, let  be the coreset for the whole point set  after reading  points and let  be the error parameter at step . 

\begin{algorithm}
\DontPrintSemicolon \KwIn{A sequence  of points, }
\KwOut{A summary  of these points}
\;
\While{not \emph{End of Stream}} {
  Compute a coreset  for the next  points with precision  \;
  \;
}
\Return \;
\caption{Algorithm for computing a coreset with improving precision}
\label{algo:clustering}
\end{algorithm}

We can apply the method of \cite{ExactKMeans} to get an optimal -means clustering on the coreset in time polynomial in the size of the coreset. Now we want to show that if we have the optimal set of centers for the summary , we get an approximate solution for the input point set with  relative error for some  decreasing to zero as .

Given an infinite stream  and an optimal solution  to the -means problem on , we denote by  the minimum number of points assigned to one of the centers. More formally, if  are the different clusters, then . We will prove the following theorem

\begin{theorem}
\label{th:improvingClustering}
Assume . If  is an optimal solution to -means for the coreset , then , where . The coreset  contains at most  points, where  denotes the size of a -coreset on  points.

\end{theorem}
\bigskip
Note, that since coresets are closed under union, we already know that  is an -coreset for the whole sequence. Thus we can assume without loss of generality that  is already a -approximation of the optimal. Before we move to the actual proof, we first use the assumption on  to show that  and  are \emph{close} in terms of distance as well as in terms of cost. We begin with a lemma, which shows that there is a point of  near any center of .
\begin{lemma}
\label{lemma:distanceToCenters}
We have the following inequality:

\end{lemma}
\begin{proof}
Let  and let  be the index for which this maximum is attained. We denote by  the number of points that are in the Vorono\"i cell corresponding to  and at a distance of at most  from . Since there are  points at a distance of at least  from  inside this cell, we have  This leads to a lower bound of 
Note that all these points are at a distance of at least  from . We thus have 
Now, using the fact that  is a -approximate solution for , this inequality can be rewritten as 
which implies our claim.
\end{proof}

We now show the following lemma, which bounds the difference of cost between two sets of centers depending on their distance
\begin{lemma}
\label{lemma:ineqCostMean}
Let  and  two set of centers and define . Then 
\end{lemma}

\begin{proof}
For any point , let  be the closest point of  from  and let  be the closest point of  to . Then, applying the triangle inequality and the definition of  we get that
 Thus, taking the square and summing over all points, we have

Note that the last term depends on the median cost instead of the mean cost.
Now, using Cauchy-Schwartz inequality the median cost satisfies
 Plugging this into the previous inequality concludes the proof.
\end{proof}

Now we have all the tools that we need and proceed with the proof of our theorem
\begin{proof}(of Theorem \ref{th:improvingClustering})
The idea is to separately analyze the points for which we have a small relative error and the others. To this end, we split the cost of the approximate into two parts. One part where we have sketches with a good enough precision and one part for which there are only few points. Then, we show for the first part, that the approximation is close enough from the optimum such that the total error that we make is small. Fix any  let  be the smallest index such that , i.e., . For , we have . Now we split the cost as explained above. 
Next, we bound the two terms separately. Recall that  and let . From  Lemma \ref{lemma:ineqCostMean} we have for the first term

where the last inequality follows from Lemma \ref{lemma:distanceToCenters}. For the second term, we can apply the coreset property to each block.

The last inequality is a consequence of the fact that  is a -approximation for  and  is a -coreset. Thus we have . Now we can leverage the fact that  is an optimal solution for the set  to bound the first term of the last line (\ref{proof:errorIneq**1}). Then we have

Plugging this into inequality (\ref{proof:errorIneq**1}) yields

Using Lemma \ref{lemma:distanceToCenters} on the middle term we get

Putting this together with inequalities (\ref{proof:errorIneq*}) and (\ref{proof:errorIneq**2}) we have
 for some absolute constant .
Now, since we have  if we assume that  satisfies , then we have that . It can easily be verified that there exists an  that satisfies the above condition. This concludes our proof by rescaling and renaming .
\end{proof}

\subsection{Modifications for -Median}
Now we show how the results for -means can be adapted to work for the -median problem. The calculations remain essentially the same up to some minor modifications. We have an inequality bounding the distance of the centers that can be derived similarly to Lemma \ref{lemma:distanceToCenters}.
\begin{lemma}
We have the following inequality: 
\end{lemma}


Our bound on the cost for the median case becomes stronger than Lemma \ref{lemma:ineqCostMean}, since we can remove one of the error terms, which was due to the squared distances.
\begin{lemma}
Let  and  two set of centers, and we note , then we have the following inequality 
\end{lemma}
Reusing the proof of Theorem \ref{th:improvingClustering} with the modified inequalities yields our result.

\begin{theorem}
Assume . If  is a -approximate solution to -means for the coreset , then 
where . The coreset  contains at most  points, where  denotes the size of a -coreset on  points.
\end{theorem}

Contrarily to the -means case, \cite{Bajaj} showed that it is not even possible to compute the -median of a set of points exactly in the usual model of computation. So instead, we can use a brute-force method similar to the one used for approximating -median in \cite{oneMedian} to build a grid of possible centers of size  and then enumerate all possible  tuples from this set to get a -approximation. By using the \emph{centroid sets} from \cite{HPClustering} one can even reduce number of possible centers to  and therefore reduce the running time of the exhaustive search.



\subsection{Lower bounds on clustering problems}
\label{section:negativeClustering}
We were able to give positive results for the -means and -median clustering problems under the mild assumptions that the number of points in each Vorono\"i cell goes to infinity. Here we show that there is no hope for such algorithms when we drop this assumption. Furthermore, we show that in contrast to the other objectives, no asymptotically exact algorithm can exist for the -center problem. We begin with the following lemma on computing the exact solution to the -median and -means problems.

\begin{lemma}
\label{lower:exact}
Any streaming algorithm solving -means or -median exactly in dimension  uses  memory
\end{lemma}
\begin{proof}
We reduce from the indexing problem. We denote by  the input string received by Alice. From , Alice produces  points  where  denote the  unit roots and  is a constant chosen to be small enough. Alice feeds these points to the algorithm and then communicates the memory of size  to Bob. Given an index , Bob will put points in the direction given by . He puts a large number of points in the position  and ; see Figure (\ref{figure:lowerBoundClustering}). Here the idea is that the position of one of the centers will only depend on the value , while the other will prevent the other bits from having any influence. If the number of points at  and  is large enough, then the optimal centers will move close to these points. Let  be the centers close to  and  respectively. Moreover, all  with  will be in the Vorono\"i cell of . Thus, the Vorono\"i cell of  contains all the points at  and also . Now, if  then  and otherwise  since . Thus, Bob can distinguish between the two cases and solve the indexing problem. Consequently .
\begin{figure}
\center
\begin{tikzpicture}[scale=3]
\tikzstyle{point} = [shape=circle,inner sep =0pt, minimum size = 5, fill, red]
\tikzstyle[every node] = [draw, shape=circle, fill=red]
\node[point, label=north:{A}] (a) at (0,1) {} ;

\node[point, label=south east:B] (b) at (0,0.71) {} ;

\node[point, minimum size = 2, black] at (0,0.9) {} ;

\node[label=center:\tiny ] at (0.05,0.95) {} ;

\node[point, label=north east:\small {}] (c) at (0.71,0.71)  {};

\node[point, label=north west:\small {}] (d) at (-0.71,0.71) {} ;


\draw[dashed] (c) -- (b) -- (d) ;
\draw (0,0.5) -- (b) -- (a) ;
\draw[dotted] (0.87,0.5) arc [start angle = 30, end angle = 150, radius = 1];
\end{tikzpicture}
\caption{Construction for the lower bound on clustering.
}
\label{figure:lowerBoundClustering}
\end{figure}
\end{proof}


While -means is trivial, in the median case, we even have the following stronger result.
\begin{lemma}
\label{lemma:1median}
Any streaming algorithm solving the -median problem exactly uses  memory.
\end{lemma}

\begin{proof}
Again, we reduce from the indexing problem. Let  be the string received by Alice. She produces the points 
and feeds them to the algorithm. Then, she sends the memory of size  to Bob. Now, we use the fact that in one dimension, if there is an odd number of points, then the median is the unique point from the input stream, such that there is an equal number of points to its right and to its left. So, by just adding the correct number of points located at  and , it is possible for Bob to select the desired point , and thus, to retrieve the value of . Consequently,  follows.
\end{proof}

Using these lemmas we can derive our negative result concerning asymptotically exact streaming algorithms.

\begin{theorem}
\label{lower:kmeanmed}
Any asymptotically exact streaming algorithm for -median for  resp. -means for  uses  memory.
\end{theorem}
\begin{proof}
The idea is given an input stream of points for which we want to compute a -clustering, give this stream to an asymptotically exact streaming algorithm for -clustering. Then choose one point at a large enough distance from any input point and repeatedly feed the algorithm with this point. By doing this, one of the centers will move to this distant point and the other  centers will provide a -clustering of the points from the initial stream that does not change any more. If we repeat the insertion a large enough number of times, we will be able to retrieve the position of the centers up to an arbitrary small error implying that the solution is optimal. Now, Lemma \ref{lower:exact} and \ref{lemma:1median} yield the linear lower bound.
\end{proof}

We close this section with the lower bound on -center clustering.

\begin{theorem}
\label{lower:kcenter}
Any asymptotically exact streaming algorithm for -center clustering uses  bits of space.
\end{theorem}
\begin{proof}
We show the claim already holds for the case . 
The proof is conducted by reduction from the indexing problem. Let  be Alice's input and  be the index of the bit that Bob is supposed to report. Let  be the vertices of a regular -polygon in clockwise order, centered at the origin and starting with . This construction does not depend on the choice of  and  but only on the size . Now, suppose there is an asymptotically exact streaming algorithm for 1-center that uses space . Alice inserts the vertices  for all bits  into the input stream. Then she communicates the memory of size  to Bob. Bob can then continue to simulate the algorithm and inserts the point . If , we know that the 1-center must be located at  and is therefore at unit distance from  and also from . In particular this implies that any -approximation is at distance at least  from . If on the other hand , the center will appear within  from  for some fixed . Bob's intention is to distinguish between these two cases. To this end, he 
 continues to insert a large number of points located at the origin. These points do not affect the optimal center in any of the cases but, since we have an asymptotically exact streaming algorithm, the approximation improves and at some point the error decreases to . Then we have that  This means that the possible regions for the -center depending on Alice's bit are disjoint and thus Bob can recover and report the correct solution to the indexing problem. The lower bound of  follows.
\end{proof}

\section{Regression}
\label{section:Regression}

Next, we develop an asymptotically exact algorithm for the regression problem. Let  and  the input matrix and target vector obtained by processing the first  items from the input stream. One way to solve this problem for a fixed precision using random sign matrices for sketching is as follows. Sketch both, the matrix  and the vector  with an appropriately rescaled sign matrix  and solve the regression problem in the sketch space. That is, let  be the optimal solution to . It has been shown in \cite{LinAlgStream} that it is sufficient to have  as the target dimension of the sketching matrix, such that with probability  we have .

Now we want to compute a sketch of the matrix  and the vector  such that the approximate solution will have a  relative error, with  decreasing to zero as the size of the input goes to infinity. We first describe how the sketching will be performed according to our algorithm and then analyze this method. In the following, we will assume that the matrix  and the vector  are given row-by-row. Given a matrix , we will denote by  the sub-matrix consisting of the first  rows of . The algorithm is given in Figure \ref{algo:sketching}. It splits the input matrix into blocks of  rows at step . Then it computes a sketch of size  for the block  using rescaled sign matrices for sketching. The resulting sketch will simply be the concatenation of the single sketches at each step.

\begin{algorithm}
\DontPrintSemicolon \KwIn{Matrix  given row-by-row}
\KwOut{A sketch  of }
\;
\While{not \emph{End of Stream}} {
Sketch the next  rows of A with a sketching matrix  with  rows\;
  \;
}
\Return concatenation of the sketches at each step\;
\caption{Improving sketching algorithm for regression}
\label{algo:sketching}
\end{algorithm}

The procedure described by the algorithm is equivalent to multiplying the input matrix  and the vector  by the block diagonal matrix  where  is the matrix used at step  for sketching the input block. The exact number of rows  for each of the  will be determined later. It will be parametrized by  and , the error bound and failure probability of . Now we want to prove that solving the problem for  and  will give a good approximate solution.
In the following, we will choose  and , where  is a constant chosen to be large enough, such that  holds. Following the outline of \cite{LinAlgStream} we begin with analyzing our sketches for some simpler problems, namely subspace approximation and matrix multiplication. We will use these results as tools or building blocks in the analysis for regression.

\subsection{Subspace approximation}
Consider the problem of subspace approximation.
\begin{problem}[\emph{Subspace approximation}]
Given a matrix  the task is to find a matrix  of smaller size such that for all  we have 
\end{problem}

Theorem \ref{th:embedding} shows that this problem can be solved using random sign matrices for any fixed precision. Namely, if  is a random sign matrix with an appropriate number of rows, then  is a solution for the subspace approximation problem with high probability. Now, it is natural to ask whether we have a  similar property for our block-diagonal matrix .

We denote by  the smallest singular value of the matrix .
\begin{lemma}
\label{lemma:improvedSubspaceEmbedding}
Assume that  as the size of the input goes to infinity. Let  for some absolute constant . Then  is a -sketch for subspace approximation with  as  goes to infinity. More precisely, if  for some positive, monotone function , then we have 
\end{lemma}
\begin{proof}
By linearity, it is enough to prove the inequality for all  such that . We only show the upper bound, the lower bound can be treated similarly. Applying Theorem \ref{th:embedding} to each of the blocks, we get

Now, since , and since we can assume that the entries of the matrix can be stored by a logarithmic number of bits, we have that . So applying Lemma \ref{lemma:series} from the appendix, we have that

In particular, using the assumption on the smallest singular value, we have that  and so there exists a constant  such that
.
\end{proof}

 Actually, for the proof of our main theorem regarding the regression problem, we will only need that  is a subspace embedding for  with a fixed error  (without improving precision). This is given by the following corollary. 
\begin{corollary}
\label{corollary:simpleSubspaceEmbedding}
Let  for some absolute constant . Then  is a -sketch for subspace approximation.
\end{corollary}
\begin{proof}
The claim follows immediately by applying Theorem \ref{th:embedding} block-wise as in the previous proof and bounding every  by .
\end{proof}

\subsection{Matrix multiplication}
Before we get back to the analysis for regression, we investigate a second problem that we will need in the analysis, namely the matrix multiplication problem. Given two matrices, we want to compute their product. In the streaming setting, we can again use random sign matrices to sketch the input matrices and get a matrix that approximates the product using little space. The following result is proved in \cite{LinAlgStream}.
\begin{theorem}
\label{th:matrixMultiplication}(\cite{LinAlgStream})
Let ,  be matrices of size  and  respectively. If  is a normalized random sign matrix of size  with  for some constant , then with probability  we have that 
\end{theorem}

Now we show that we can get an improving bound on the error using the same sketching method as we have used for the subspace approximation. We would like to show that we can get a  approximation for improving  by sketching the matrices  and  with our block diagonal sketching matrix . Before we prove the decreasing bound, we show the following lemma, which states that the total relative error is at most the maximum error on one of the blocks. That is, using the matrix , we can get an approximation within a fixed error.

\begin{lemma}
\label{lemma:simpleMatrixMultiplication}
Let  and  be two matrices of size  and  respectively. If  then with probability  we have 

\end{lemma}

\begin{proof}
Denote by  (resp. ) the columns of  and . By definition we have 

Now, by the parallelogram rule \cite{ArriagaV06}, applying the sketching matrix  on vectors  and , we have with probability 

So, taking the union bound over all terms of the sum yields the claim with probability  since

\end{proof}

Now, our result to get an improving bound for matrix multiplication will follow from this lemma. However, as in the case of subspace embedding, we have to impose some mild assumptions on the input.

\begin{lemma}
\label{lemma:ImprovedmatrixMultiplication}
Let  and  be a series of matrices of size  and  respectively such that

for any  and absolute constants . Now, for any constant  and some constant , put . Then we have that

where .
\end{lemma}

Here  is an additional parameter that allows to modify the memory used depending on the convergence rate we want to achieve. To get a -approximate solution for regression, we will only need to apply this lemma to get a -approximation for matrix multiplication, i.e., we only need  removing the square in the embedding complexity. See \cite{LinAlgStream} for details.

\begin{proof}
We fix an  to be determined later. Let  be the smallest index such that , i.e., . By our choice, for  we have . We denote by  (resp. ) the sub-matrix of  composed of the blocks with index smaller than  (resp. greater or equal than ). Thus, we can rewrite

where  and  contain  lines. Then by splitting the terms we have 

Applying Lemma \ref{lemma:simpleMatrixMultiplication} to each term separately, we have with probability  

since all the blocks in  have at least  lines.
Now, using the assumption, we have that


Using a similar argument as in the proof of Theorem \ref{th:improvingClustering}, we can bound the last expression by  where  for some constant . Plugging this into inequality (\ref{proof:improveMatrixMultiplication}) yields the claim since

\end{proof}

\paragraph{Back to least squares regression:}
We now have all the necessary tools to prove an improving error bound for regression. In the following let  and  be the input to the regression problem after the first  rows have been read from the stream and let

be the optimal solution to the sketched problem and to the original problem at that point. We have the following theorem.
\begin{theorem}
\label{th:improveRegression}
Assume that the smallest singular value  of  satisfies  for a positive monotonous function  with . If the blocks  of the sketching matrix  consist of  rows for some absolute constant , then with probability at least  it holds that 
where .
\end{theorem}

\begin{proof}
Following the proof of Theorem 3.2 from \cite{LinAlgStream}, our claim is basically the consequence of two results. We have a subspace embedding with constant distortion using . This is given by Corollary \ref{corollary:simpleSubspaceEmbedding}. Moreover, we have a result on matrix multiplication with improving precision in Lemma \ref{lemma:ImprovedmatrixMultiplication}. Let  be the singular value decomposition of the input matrix, then Lemma \ref{lemma:ImprovedmatrixMultiplication} is applied to  as one of the matrices. So it remains to justify that  satisfies the assumptions of this lemma, i.e., that the norm of  is not concentrated on the first  rows. 
We denote by  and  the columns of  and  respectively and by  the  singular value of , which also corresponds to the  diagonal coefficient of . We have that . So considering only the  first rows, we have 
The last inequality holds since we can assume that the entries of the matrix can be stored by a logarithmic number of bits. So we get  and we can rewrite 
Now, since , we have the required inequality and therefore all assumptions are satisfied to apply Lemma \ref{lemma:ImprovedmatrixMultiplication}.
\end{proof}

We can get the following corollary for unique updates. Here we do not enforce a row-by-row order of the streaming data, but only assume that we have at most one update per entry. Our assumptions on the smallest singular vector is still needed for the proof.

\begin{corollary}
There exists an asymptotically exact streaming algorithm to compute a -approximate solution to the least squares regression problem under unique updates assuming that the smallest singular value of the input matrix diverges.
\end{corollary}

The proof is almost identical to the previous theorem, so we will only describe the differences. Note that we do not get an explicit bound on the error , but only prove that it goes to zero. The input  is an  matrix and the number of columns  is fixed at the beginning of the algorithm. Given an index , we denote by  the largest , such that the  update modifies a coordinate above the  row. Since there can only be a finite number of such updates,  is finite.
The lemma on the improving matrix multiplication is modified in the following way.
\begin{lemma}
Assume that we have a series of matrices  and  that satisfy
.
Then we have 
with .
\end{lemma}
The proof of this lemma is similar to the proof of Lemma \ref{lemma:ImprovedmatrixMultiplication} and is therefore omitted. It remains to prove that the assumptions of this lemma are still satisfied. Again, this can be done similarly to our previous proof using  in inequality (\ref{proof:ineqhypmult}). The subsequent calculations are still correct up to immediate modifications and thus we have the required bound for the assumptions of the modified matrix multiplication lemma to hold. Corollary \ref{corollary:simpleSubspaceEmbedding} remains true without any modification.

Note, that since we do not get the entries of the matrix row-by-row any more, we now have to store the matrices . It was shown in \cite{LinAlgStream} that the entries in the rows of the sketching matrix need only limited independence and so, these matrices can be stored implicitly using only polylogarithmic space.

\subsection{Lower bounds on regression}

We also have negative results concerning regression. These results justify the assumptions that the input is given row-by-row and that the smallest singular value diverges. We prove the following theorem.
\begin{theorem}
\label{lemma:negativeRegression}

We have the following series of results for the regression problem:
\begin{itemize}
\item Any asymptotically exact streaming algorithm for regression working under turnstile updates uses  memory.
\item Any asymptotically exact streaming algorithm for regression working under unique turnstile updates uses  memory. Here, we are allowed to modify one coordinate of the input matrix only once.
\item Any asymptotically exact streaming algorithm for regression working under turnstile updates, with the additional assumption that the smallest singular value diverges, uses  memory.
\end{itemize}
\end{theorem}

In particular, the third point shows that assuming only that the smallest singular value diverges is not enough. So, the other assumption, that the input is given in row-wise order is necessary.

\begin{proof}
We adapt the proof of Theorem 3.14 from \cite{LinAlgStream}. In their proof, they perform a reduction from the indexing problem with strings of size  for . The protocol is the following. Alice feeds to the algorithm a matrix  built from the input string and sends the content of the memory to Bob. Then, Bob feeds the algorithm with some modifications on  and builds a vector .
He retrieves the approximate optimum to the regression problem  and with probability larger than , he can guess from this approximate optimum the required bit from Alice's input string.

Now, to prove our result, we slightly modify this protocol. We denote by  the matrix and by  the vectors defined as 

where  is just a scalar and  and  are given by the protocol from \cite{LinAlgStream}. Remark that the value of  does not modify the optimal solution, nor its value. So, for a given , Alice and Bob can apply the protocol normally and then Bob simply modifies  repeatedly. Since we have an improving algorithm, after some time Bob will be able to get a -estimate of the optimal solution, where . Using this optimum, Bob can retrieve the required entry from . Thus, it follows that the memory used by the algorithm is at least . Now, this lower bound holds for any . So in particular it holds for  equal to this value. This yields a lower bound of . The second point is obtained by just replacing  by a column vector. In particular, this construction proves that even under the assumption that the norm of  goes to infinity, we cannot get a sub-linear improving algorithm. However, in the improving algorithm we described earlier, we made the assumption that the smallest singular value of  goes to infinity. With the above constructions, this condition is not met.

To prove the third claim, we need to look a little more into the details of the protocol from \cite{LinAlgStream}. In the protocol, we have the matrix 
where  is an upper triangular matrix. Alice fills the entries of the matrix above the diagonal with  according to her input string. Then Bob fills the diagonal entries with either  or , where  is only required to be a large enough value. In particular, the diagonal entries of the matrix that contain the bit that Bob wants to retrieve are all set to . Thus, updating the matrix by simply increasing the number  repeatedly, the smallest singular value of  goes to infinity and we would get an estimate with an error decreasing to zero. The estimated solution would still satisfy the properties needed to retrieve the bit . So using the same argument as before, the algorithm uses  memory.
\end{proof}



\section{Conclusion}
\label{conclusion}
In this paper, we introduced the notion of \emph{asymptotically exact streaming algorithms}. These have an approximation ratio that tends to one as the length of the stream goes to infinity and are thus optimal in the limit. We have considered several problems from the streaming literature in this setting. Interestingly, estimating the frequency moments works in the case of  without making additional assumptions, whereas  does not allow for algorithms in our model. This is different in the ordinary streaming model, where both problems have -approximations. However, for clustering and regression, we had to make some assumptions on the input stream to have a decreasing error bound. These were imposed to ensure that the value of a solution does not depend too much on a small number of items and were shown to be necessary. In contrast to our positive results concerning -means and -median clustering, there is no asymptotically exact streaming algorithm for -center. It would be interesting to have similar algorithms also for other base problems like counting frequent items in a data stream and further explore the possibilities and limitations of our model. Another possible direction for future work is to extend our model to semi-streaming and graph problems.

\bibliography{bib}
\bibliographystyle{abbrv}




\section{Appendix}
\label{append}

\begin{lemma}
\label{lemma:series}
Let  be a series of positive terms and  for a positive function . We assume that ,  and  for some absolute constants . Then we have that  and, more precisely, .
\end{lemma}
\begin{proof}
Fix an  to be determined later and let  be the smallest index such that .

Then using the assumptions, we have


It remains to bound the first term. We have

If we can bound this quantity by , then the lemma follows. It can be verified that  for some constant  satisfies this condition.
\end{proof}


\end{document}