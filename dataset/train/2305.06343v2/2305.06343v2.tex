\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}





\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\ourmodel}{ORVit}
\newcommand{\vot}{video transformer}
\newcommand{\vsizeup}{\vspace{-0.2cm}}
\newcommand{\vsizedown}{\vspace{-0.2cm}}
\newcommand{\vssizeup}{\vspace{-0.2cm}}
\newcommand{\vssizedown}{\vspace{-0.2cm}}

\newcommand{\reprojection}{Reprojection}

\newcommand{\newterm}[1]{{\bf #1}}

\newcommand{\ignore}[1]{}

\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{color}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage{tablefootnote}
\usepackage[font=small]{caption} \usepackage{multirow} \usepackage{footnote}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{pifont}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{sidecap}
\usepackage{bbm}

\definecolor{ForestGreen}{RGB}{34,139,34}
\definecolor{Cerulean}{RGB}{42,82,190}
\definecolor{CornflowerBlue}{RGB}{100,149,237}
\definecolor{Turquoise}{RGB}{48,213,200}
\definecolor{ProcessBlue}{RGB}{0,136,208}
\usepackage{tabularx}

\definecolor{lightgray}{gray}{0.9}
\definecolor{lightblue}{rgb}{0.93,0.95,1.0}
\definecolor{darkgreen}{rgb}{0.0,0.6,0.0}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\definecolor{pinegreen}{rgb}{0.0, 0.47, 0.44}
\definecolor{deepmagenta}{rgb}{0.8, 0.0, 0.8}
\definecolor{amber}{rgb}{1.0, 0.49, 0.0}

\newcommand{\hmark}{\textcolor{pink}{\ding{119}}}
\newcommand{\cmark}{\textcolor{darkgreen}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\newcommand\comment[1]{\textcolor{gray}{\# #1}}
\newcommand{\as}[1]{\textcolor{pinegreen}{AS:#1}}


\newcommand{\reals}{\mathbb{R}}
\newcommand{\dotp}[1]{\texttt{dot}\,(#1)}
\newcommand{\reducesum}[1]{\texttt{reduce\_sum}\,(#1)}
\newcommand{\gru}[2]{\texttt{GRU}\,(#1, \ #2)}
\newcommand{\slots}[0]{\texttt{slots}}
\newcommand{\tubes}[0]{\texttt{tubes}}
\newcommand{\ntubes}[0]{\texttt{object\_tubes}}
\newcommand{\etubes}[0]{\texttt{edge\_tubes}}
\newcommand{\attn}[0]{\texttt{attn}}
\newcommand{\inp}[0]{\texttt{inputs}}
\newcommand{\mlp}[0]{\texttt{MLP}}
\newcommand{\tam}[0]{Tube Attention\xspace}
\newcommand{\w}[0]{\texttt{w}}
\newcommand{\nupdates}[0]{\texttt{U}}
\newcommand{\eupdates}[0]{\texttt{e\_updates}}
\newcommand{\layernorm}[0]{\texttt{LayerNorm}}
\newcommand{\layern}[0]{\texttt{LN}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\ee}{\mathop{\mathbb{E}}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\ww}{\boldsymbol{w}}
\newcommand{\zz}{\boldsymbol{z}}
\newcommand{\uu}{\boldsymbol{u}}
\newcommand{\ag}{A}
\newcommand{\gen}{G}
\newcommand{\phiv}{\boldsymbol{\phi}}
\newcommand{\psiv}{\boldsymbol{\psi}}
\newcommand{\ignorebig}[1]{}
\newcommand{\Ig}{I_G}

\newcommand\myeq{\mkern0.5mu{=}\mkern6mu}
\newcommand{\minisection}[1]{\noindent{\textbf{#1}.}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\eqrref}[1]{Equation~\ref{#1}}
\newcommand{\figgref}[1]{Figure~\ref{#1}}
\newcommand\tripleq[3]{\langle\emph{#1}, \emph{#2}, \emph{#3}\rangle}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\newcommand{\methodwithboxes}{}
\newcommand{\methodwithoutboxes}{\footnotemark[2]}
\newcommand{\reldif}[1]{}
\newcommand{\model}{Scene Graphs for Vision-Language Models}
\newcommand{\smodel}{SGVL}
\newcommand{\gcol}[1]{{\bf \fontsize{6.5}{42}\selectfont \color{citecolor!80}~(#1)}}
\newcommand{\rcol}[1]{{\bf \fontsize{6.5}{42}\selectfont \color{lightred!180}~(#1)}}
\newcommand{\bcol}[1]{{\fontsize{6.5}{42}\selectfont~(#1) }} \definecolor{citecolor}{RGB}{34,139,34}
\definecolor{lightred}{RGB}{241,140,142}
\definecolor{amber(sae/ece)}{rgb}{1.0, 0.49, 0.0}
\definecolor{battleshipgrey}{rgb}{0.52, 0.52, 0.51}
\definecolor{cadmiumorange}{rgb}{0.93, 0.53, 0.18}
\definecolor{applegreen}{rgb}{0.55, 0.71, 0.0}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{red}{rgb}{0.89, 0.0, 0.13}
\newcommand\orange[1]{\textcolor{amber}{\textbf{#1}}}
\newcommand\green[1]{\textcolor{forestgreen}{\textbf{#1}}}
\newcommand\red[1]{\textcolor{red}{\textbf{#1}}}
\newcommand\grey[1]{\textcolor{battleshipgrey}{\textbf{#1}}}
\newcommand\darkblue[1]{\textcolor{darkblue}{\textbf{#1}}}


\def\Secref#1{Section~\ref{#1}}

\newcommand\roei[1]{\textcolor{blue}{[RH: #1]}}
\newcommand\cmr[1]{{#1}}












\title{Incorporating Structured Representations into Pretrained Vision \& Language Models Using Scene Graphs}







\author{
    Roei Herzig\thanks{~~Equal contribution.}~~,
    Alon Mendelson,\\
    \textbf{Leonid Karlinsky},
    \textbf{Assaf Arbelle},
    \textbf{Rogerio Feris},
    \textbf{Trevor Darrell},
    \textbf{Amir Globerson}\\
    \\
\tt
    Tel-Aviv University,
    UC Berkeley,
    IBM Research,
\tt
    MIT-IBM Watson AI Lab
}



\begin{document}
\maketitle




\begin{figure*}
    \vspace{-2.0em}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/teaser_figure_Alon_v8.pdf}
    \vspace{-1.mm}
    \captionof{figure}{\textbf{Scene Graphs Improve Pretrained Vision-Language Models.} Vision and Language models (VLMs) are typically trained on large-scale image-text pairs (Left). We propose to improve pretrained VLMs by utilizing a small set of scene graph (SG) annotations (Right) from the Visual Genome dataset, that is richer and reflects structured visual and textual information. Specifically, we design a specialized model architecture and a new finetuning scheme when learning from SGs as follows: (i) Use the graph to generate fine-grained positive and negative captions that highlight different compositional aspects of the scene. (ii) Finetune the pretrained model using contrastive learning with the generated captions from the SGs, along with image-text pairs from LAION. (iii) Predict SG information (object, relations, and their coordinates) from an image by incorporating ``Adaptive Scene Graph Tokens'' into the image transformer encoder. During inference, these learned tokens, which capture structured information from the SG, interact with the patch tokens and CLS tokens to improve compositional scene understanding.}
    \vspace{-1mm}
\label{fig:teaser}
\end{figure*}












\begin{abstract}
Vision and language models (VLMs) have demonstrated remarkable zero-shot (ZS) performance in a variety of tasks. However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states. In contrast, obtaining structured annotations, such as scene graphs (SGs), that could improve these models is time-consuming and costly, and thus cannot be used on a large scale. Here we ask whether small SG datasets can provide sufficient information for enhancing structured understanding of pretrained VLMs. We show that it is indeed possible to improve VLMs when learning from SGs by integrating components that incorporate structured information into both visual and textual representations. For the visual side, we incorporate a special ``SG Component'' in the image transformer trained to predict SG information, while for the textual side, we utilize SGs to generate fine-grained captions that highlight different compositional aspects of the scene. Our method improves the performance of several popular VLMs on multiple VL datasets with only a mild degradation in ZS capabilities.




\end{abstract}











 












\section{Introduction}
\label{sec:intro}

In recent years, vision and language models (VLMs) such as  CLIP~\citep{radford2021learning} have demonstrated impressive results and extraordinary zero-shot capabilities when trained on massive datasets containing image-text pairs  \citep[e.g., LAION 400M][]{laion}. However, recent empirical studies~\cite{winoground,yuksekgonul2023when,Ma2022CREPE} have shown that even the strongest VLMs struggle to perform compositional scene understanding, including identifying object attributes and inter-object relations.








Understanding the structure of visual scenes is a fundamental problem in machine perception and has been explored extensively in many previous works~\cite{sg_generation_msg_pass,herzig2019canonical,Yang2022PanopticSG}. In particular, datasets with scene graph (SG) annotations, such as the Visual Genome (VG) dataset~\cite{krishna2017visual}, have been collected and used to improve scene understanding models. However, such datasets are expensive to collect at scale and relatively small compared to those used in training VLMs.\footnote{VG contains  image-SG pairs, which is  smaller than the large-scale VLMs pretraining datasets.} This raises the following questions: (1) Can small datasets containing SG annotations be utilized to finetune VLMs and improve compositional scene understanding? (2) How should the model and training be adapted to best use this data? Here we show that it is indeed possible to improve VLMs using image-SG pairs by integrating components that incorporate structure into both visual and textual representations.

























Our first step is to convert SGs into highly detailed captions. A naive approach would be to finetune VLMs on these image-text pairs, however, we have found that this approach does not sufficiently improve performance.\footnote{See our ablations in \secref{sec:ablations}.} This is also aligned with recent work~\cite{doveh2022teaching,yuksekgonul2023when}, showing that contrastive learning approaches allow the model to concentrate mainly on object labels disregarding other important aspects, such as relations and attributes. To alleviate this issue, we take inspiration from these works and use the SG to generate hard-negative captions that highlight structural aspects. For example, if an SG contains an edge ``dog-chasing-cat'', then we can reverse that edge into ``cat-chasing-dog'' and generate a corresponding negative caption. 


































Next, we turn to introduce structure into the visual representation. Inspired by prompt learning approaches~\cite{Jia2022VisualPT,Herzig2022PromptonomyViT,Zhou2022ConditionalPL}, we incorporate into the image transformer encoder a set of ``Adaptive Scene Graph Tokens'', which interact with the patch and CLS tokens via attention. By training these tokens to predict SG information, the encoder can capture better structured representations.
















However, the above task of predicting SG from the image transformer encoder is challenging, as it deviates from the initial VLM training objective. Toward this end, we designed a technique tailored to the SG tokens, which parameterized the SG tokens independently from the patch tokens. In particular, we modify the transformer layers throughout the network by decomposing the parameters into two distinct sets: SG and patch tokens. We have found that this allows better learning of the graph prediction task while still maintaining zero-shot performance.  We name our proposed approach {\smodel} (\emph{Scene Graphs for Vision-Language Models}). See Figure~\ref{fig:teaser} for an overview. 





























































































To summarize, our main contributions are as follows: (i) We propose to exploit a small set of SG annotations that is rich with visual and textual information for enhancing compositional scene understanding in pretrained VLMs; (ii) We introduce a new finetuning scheme that captures structure-related information using the visual and textual components when learning from SG labels. Specifically, for the visual side, we incorporate special ``Adaptive SG tokens'' in the image transformer encoder, and train these to predict SG information; (iii) Our method shows improved performance on CLIP, BLIP, and BLIP2 on several benchmarks: Winoground~\cite{winoground}, VL-CheckList~\cite{vlc}, ARO~\cite{yuksekgonul2023when}, and VSR~\cite{Liu2022VisualSR}, highlighting the effectiveness of our approach.
















\begin{figure*}[ht!]
    \vspace{-1.em}
    \centering
    \includegraphics[width=.95\linewidth]{Figures/model_figure_v10.pdf}
    \vspace{-1mm}
    \captionof{figure}{\textbf{Our {\model} (\smodel) Approach.} Our key goal is to capture structure-related information in both visual and textual components when learning from SGs. For the textual side, we generate captions and negative captions using the graph (Graph-Based Negatives and SG-to-text modules). For the visual side, we incorporate into the image transformer an additional set of learnable ``Adaptive Scene Graph Tokens'' to predict SG information. The transformer parameters are partitioned into two distinct sets, one for patch tokens and one for SG tokens (shown on the right).    These tokens predict objects (pink tokens \& arrows) and relationships (cyan tokens \& arrows) from the image, and these predictions are matched with the ground-truth SGs via bipartite matching. To allow open vocabulary prediction, we embed the SG categories using the text transformer encoder. Last, we use LoRA adapters for finetuning, keeping all other model parameters frozen.
    }
    \label{fig:arch}
    \vspace{-1mm}
\label{fig:model}
\end{figure*}


















    




\section{Related Work}
\label{sec:related}

\minisection{Vision and Language Models} In recent years, popular VLMs, such as CLIP~\cite{radford2021learning}, BLIP~\cite{blip}, BLIP2~\cite{li2023blip2}, and others, have shown impressive results and extraordinary zero-shot capabilities. These models are trained with image-text pairs to align the two modalities in a joint embedding space. However, recent empirical studies (e.g., VL-CheckList~\cite{vlc}, Winoground~\cite{winoground}, and ARO~\cite{yuksekgonul2023when}) have shown that these models do not perform well on tasks that require compositional understanding, including relations between objects, and their attributes. Specifically, it has been shown that VLMs tend to learn a ``bag of objects'' representation, leading them to be less structure-aware. Several recent works, such as NegCLIP~\cite{yuksekgonul2023when}, CountCLIP~\cite{Paiss2023TeachingCT}, and SVLC~\cite{doveh2022teaching}, haven shown that hard negative generation from image-text pairs using language augmentations could improve fine-grained understanding. Unlike these works, we utilize image-SG pairs to improve structured representations by predicting SG information from the image encoder.


































\minisection{Multi-task Prompt Tuning} 
The concept of prompt tuning for efficient finetuning of language models was introduced by~\citet{Lester2021ThePO}, and later explored in vision models~\cite{Jia2022VisualPT,wang2022learning} and VLMs~\cite{ju2022prompting,zhou2022cocoop}. Several recent works~\cite{asai2022attempt, sanh2022multitask,vu-etal-2022-spot}, have explored prompt tuning in the context of multi-task learning in NLP, followed by works in vision, and specifically in video domain~\cite{avraham2022svit,Herzig2022PromptonomyViT}, and VL~\cite{Shen2022MultitaskVP}. Unlike these works, we add multiple prompts (which we refer to as Adaptive SG tokens) to VLMs in order to learn compositional scene information via the SG prediction task. We designed a technique tailored to the SG tokens, which decomposes the parameters throughout the transformer layers into two distinct sets: SG and patch tokens. This allows better learning of the graph prediction task while still maintaining zero-shot capabilities.


















































\minisection{Learning Structured Representations} Structured representations have been shown to be beneficial in many applications: video understanding~\cite{herzig2019stag,herzig2022orvit,Wang_videogcnECCV2018}, relational reasoning~\cite{baradel2018object,battaglia2018relational,Jerbi2020LearningOD}, VL~\cite{Chen2020UNITERUI,li2020oscar,tan2019lxmert}, human-object interactions~\cite{Kato2018CompositionalLF,Xu2019LearningTD}, and even image \& video generation~\cite{2020ActionGraphs,herzig2019canonical}. In particular, SGs~\cite{johnson2015image,sg_generation_msg_pass,herzig2018mapping} have been extensively used to provide semantic representations in a wide range of applications~\cite{johnson2018image,raboh2020dsg,ERNIE2021,Cong2023LearningSB}. \cmr{Unlike these previous works, here we propose a novel architecture design that utilizes scene graph data, demonstrating that a small amount of scene-graph data can be used to improve pretrained VLMs via fine-tuning. Finally, SG data has been recently used to evaluate for compositional understanding in VLMs~\cite{Ma2022CREPE}. This works differs from our approach, as this work only proposed a benchmark, and not a new training method, while our approach proposes a method for fine-tuning pretrained VLMs using scene graphs.}





























\section{Scene Graphs for VL Models}
\label{sec:model}





We begin by describing the standard VL transformer architecture and the scene graph annotations (\Secref{sec:model:preliminaries}). We then introduce our structural components for both language (\Secref{sec:model:language}) and vision (\Secref{sec:model:visual}), and the training losses (\Secref{sec:model:training}). Our method is illustrated in~\figgref{fig:arch}. 



\subsection{Preliminaries}
\label{sec:model:preliminaries}


VLMs are typically trained with image-text pairs: . Each of these modalities is processed by a separate encoder, and the training objective is to map the embeddings using contrastive learning. Next, we briefly describe the encoders.
















\minisection{Language Transformer Encoder } The text encoder is a transformer~\cite{attneed2017shazeer} as described in CLIP and others, where a CLS token is appended to the beginning of the text, and the final CLS embedding is used as the text embedding.

\minisection{Vision Transformer Encoder } 
A typical vision transformer model~\cite{dosovitskiy2020vit} takes an image  as input, extracts  non-overlapping patches,\footnote{We refer to these patches as ``patch tokens''.} and projects them into a lower-dimension , followed by adding spatial position embeddings, resulting in a new embedding . This forms the input tokens to the vision transformer encoder:






where  is a learnable token. The input  is fed into a standard transformer, and the final representation of the CLS token is the image embedding. 


\minisection{Scene Graphs (SGs)} Our motivation is to improve VLMs through structured annotations from an SG dataset. Formally, an SG is a tuple  defined as follows: \textit{(i) Nodes } - A set of  objects. Every object in the SG contains a class label, a bounding box, and attributes. \textit{(ii) Edges } - A set of  edges. These relationships are triplets  where  and  are object nodes, and  is the category of the relation between objects  and . More details of the SG preprocessing are in~\Secref{supp:models:graph}. 



\minisection{Problem Setup} 
As mentioned above, we use image-SG pairs  from VG during finetuning to improve structure understanding along with standard LAION image-text pairs . Next, we describe the textual and visual components that capture structure in both modalities.







\subsection{Structural Language Component}
\label{sec:model:language}

We begin by describing how we transform an SG into text and then explain how to manipulate the SG with our Graph-Based Negatives to further capture the structure in the model. For a visualization, see~\figgref{supp:fig:negative} in the supplementary.


\minisection{Scene Graph-to-Text}
Given an image  and a corresponding SG, , we use  to generate a textual caption for the image. We iterate over the connected components of  one by one. For each component, we iterate over the edges, and for each edge between object nodes  and  with relation , we generate the text  (e.g., ``cat chasing dog''). If a node has an attribute, we prepend it to the node's object category. Last, we generate a single caption by concatenating the captions of the connected components separated by a period.






\minisection{Graph-Based Negatives (GN)} We have found that using the scene graph data solely as image-text pairs with contrastive loss is not enough to force the model to develop structural understanding. As shown in recent work~\cite{yuksekgonul2023when}, the commonly used contrastive learning allows the model to concentrate mainly on object labels disregarding other important aspects, such as relations and attributes. In order to provide more focus on such aspects, we exploit the SG structure and propose a set of predefined graph-based rules (See~\Secref{supp:models:negatives}) that modify SGs and make them semantically inconsistent with the image. Next, these SGs are transformed into negative textual captions, which are used with a specified loss to motivate the model to focus on structural aspects. 



















\subsection{Structural Visual Component}
\label{sec:model:visual}

\minisection{Scene Graphs Tokens} We propose to capture structure-related information in the image encoder by predicting SGs. Toward this end, we add a set of ``SG tokens'' that are learned prompts designed to predict objects and relationships. The SG tokens consist of two groups: (i) ``object tokens'' that represent objects, their locations and their attributes, and (ii) ``relationship tokens'' that represent relationships and their locations.

Formally, we define a fixed set of  learned object prompts and denote them by . Similarly, we define a fixed set of  relationships prompts and denote them by . We refer to these prompts as the learned SG tokens. 
The SG tokens are concatenated with the standard CLS and patch tokens to obtain the following inputs to the transformer:


Next, the transformer processes the input , resulting in a new representation for each token. We use the new SG tokens representations to predict object and relationship labels and localization, by passing them through two feed-forward networks (FFNs). These predictions are supervised with ground-truth SG annotations through a matching process. \figgref{fig:sg_visualization} visualizes the SG tokens learned by our model. More details are in \Secref{sec:model:training}. 

 \begin{figure}
\centering
    \includegraphics[width=0.5\textwidth]{Figures/sg_visualization.pdf}
    \vspace{-8mm}
    \captionof{figure}{
        \textbf{``Adaptive SG Tokens'' Visualization}. The predictions of object tokens (pink) and relationship tokens (cyan) are shown for images that are not in the VG training data.}
    \label{fig:sg_visualization}
    \vspace{-2mm}
\end{figure}








\minisection{Adaptive SG tokens}
We have found that the SG prediction task is challenging as it deviates from the initial VLM training objective. To alleviate this issue, we consider a modification of the image transformer tailored specifically to the SG tokens. Specifically, we decompose the parameters throughout the transformer layers into two distinct sets: SG and patch tokens. This allows better learning of the graph prediction task. 
In more detail, recall that transformer layers have matrices for mapping tokens to queries, keys, and values. We denote these existing matrices for the patch tokens by . We introduce a separate set of matrices  that is used with the SG tokens. Importantly, the attention is performed over {\em all} tokens (patch and SG). Similarly, for the MLP component in the transformer layer, we also have a different version for the patch tokens () and the SG tokens (). 






\minisection{Parameter Efficiency}
To perform efficient finetuning, we use LoRA adapters for the VLM. Specifically, for each trainable matrix  (e.g.,  or ), we let  denote its pretrained weights, and parameterize the learned matrix as:
 
where  and  are -rank matrices and  is a hyperparameter.\footnote{For the SG-token parameters, we set  to the value of the corresponding parameters of the patch tokens.} We note we use two distinct :  and  for weights associated with the patch and SG tokens (as described above), respectively. During training,  is kept frozen, while  are learned. Overall, our additional trainable parameters are only  of the model.



\minisection{Open Vocabulary SG Prediction} The SG tokens predict the annotated SGs category information for objects, relationships, and attributes. A naive implementation of this idea would require a prediction head for each category. However, the VG dataset contains approximately 70K object categories and 40K relationship categories, and thus poses a significant challenge. Previous work~\cite{sg_generation_msg_pass} introduced a split containing only 100 object labels and 50 relation labels, which limits the dataset. Rather than restricting our data in this way, we use an open vocabulary approach. Namely, we utilize the text encoder to embed the categories from the SG components. Next, we use these embeddings in training to supervise the SG tokens. For example, if node  has category ``dog'' and attribute ``black'', we train one of the object tokens to predict the embedding of the phrase ``black dog''. This also applies to the prediction of relationship tokens, e.g., a relationship token predicts the embedding of the phrase ``dog chasing cat''. More details are in the next section. 











\subsection{Training and Losses}
\label{sec:model:training}




During training our batch contains image-text pairs  along with image-SG pairs (). We use the latter to generate positive captions () and negative captions (). We finetune our model using these inputs while optimizing the losses below.






\begin{table*}[t!]
    \centering
	\tablestyle{0.3pt}{1.0}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|ccc|ccc|cc|c|c}
            \toprule
            &\multicolumn{3}{c|}{Winoground} & \multicolumn{3}{c|}{VL-Checklist} & \multicolumn{2}{c|}{ARO} & 
            \multicolumn{1}{c|}{VSR} &
            \multicolumn{1}{c}{ZS}
            \\
            \toprule
            &\multicolumn{3}{c|}{All Dataset}
            &\multicolumn{3}{c|}{All Datasets Avg}
            & Flickr30K
            & COCO
            & All Dataset
            & {21 Tasks}\\
            Model & Text & Image & group & Attribute & Object & Relation & Order & Order & Avg & Avg  \\


\midrule
            FLAVA  & 32.3 &20.0 & 14.5  & 54.6 & 70.6 & 46.6 & 12.9 & 3.9 & 54.1 & -\\
            ViLT  &34.8  &14.0 &9.3   &73.3  &85.0  &62.0  &22.4  &18.7  & - & -\\
            LLaVA  &24.8  &25.0 &13.0   &65.5  &83.1  &83.0  &98.1  &97.5  & - & -\\
            MiniGPT-4  & 23.3 &18.0 & 9.5  & 71.3 & 84.2 & 84.1 & 99.4 & 98.9 & - & -\\
            

            \midrule 
CLIP  & 30.7 &10.5 & 8.0  & 65.5 & 80.6 & 78.0 & 59.5 & 46.0 & - & 56.4\\
BLIP  &39.0 &19.2 & 15.0 & 75.2 & 82.2 & 81.5 & 27.9 & 24.9 & 56.5 &  49.0\\
            BLIP2 &42.0 &23.8 &19.0  &77.8  &84.9  &84.9  &33.9 &32.3 & 61.9 & 52.5 \\
\midrule 
            NegCLIP & 29.5 & 10.5 & 8.0  &68.0  &81.4  & 81.3 & 91.0  & 86.0  & - & 55.1 \\
            NegBLIP  &42.5 &24.0 &18.5  &78.2  &83.7  &81.9  & 85.0  & 84.7  & 57.9 & 48.2 \\
            NegBLIP2 & 41.5 & 26.0 & 20.5  & 79.0  &87.0  &88.2  &91.8   & 88.2 & 62.1  &51.7  \\
            \midrule
            CLIP-SGVL (ours) &32.0 \gcol{+1.3} &14.0 \gcol{+3.5} & 9.8 \gcol{+1.8}  & 72.0 \gcol{+6.5} & 82.6 \gcol{+2.0} & 82.0 \gcol{+4.0} & 82.0 \gcol{+22.5} & 78.2 \gcol{+32.2} & - &  54.3 \rcol{-2.1}\\
            BLIP-SGVL (ours) &42.8 \gcol{+3.8} &27.3 \gcol{+8.1} & 21.5 \gcol{+6.5} & 81.8 \gcol{+6.6} & 85.2 \gcol{+3.0} & 81.9 \gcol{+0.4} & 70.0 \gcol{+42.1 } & 71.0 \gcol{+46.1} & 62.4 \gcol{+5.9} &  48.0 \rcol{-1.0}\\
            BLIP2-SGVL (ours) &42.8 \gcol{+0.8} &28.5 \gcol{+4.5} &23.3 \gcol{+4.3}  & 81.2 \gcol{+3.4}  &88.4 \gcol{+3.5}  &88.8 \gcol{+3.9}  &77.0 \gcol{+43.1}  &77.0 \gcol{+44.7}  & 63.4 \gcol{+1.5} &51.4 \rcol{-1.1}  \\
            \bottomrule        
    \end{tabular}
    \end{small}
    \vspace{-1.0em}
    \caption{
    \textbf{Winoground, VL-Checklist, ARO, VSR, and Zero-Shot (ZS) Results}. \green{Gains} \& \red{losses} are relative to the base models.}
    \vspace{-1.5em}
    \label{tab:res_main}
\end{table*} 
\minisection{Image-Text Loss}
Our image-text loss is comprised of \textit{contrastive loss} and \textit{graph negatives loss}. 

\noindent \textit{Contrastive Loss:} We apply contrastive loss on image-text pairs, as in \citet{radford2021learning}. Hence, the loss is calculated as follows based on standard pairs and those generated from SGs:
 
where  =  and  = .




\noindent\textit{Graph-Based Negative Loss:} 
For each image-SG pair we apply a loss that drives the embedding of  to be more similar to that of  than :

where  is the cosine similarity between the image and text embeddings. Finally, the image-text loss is a weighted sum of both losses: 



and  is a hyperparameter. For more information, see~\Secref{supp:models:blip} in supplementary.






\minisection{Scene Graph Loss}
The graph  contains several annotations: the set of object categories , their set of bounding boxes, the set of relationship categories , and their bounding boxes. As we do not aim to predict the object and relationship categories directly, but rather use the embeddings from the VLM, we extract the category embeddings with the text encoder : , and . We note that  and  classes are due to ``no object'' and ``no relationship'' classes. These class embeddings together with the bounding boxes are the SG elements that we aim to predict from the SG tokens.

We next describe the prediction process. The image encoder outputs a set of object tokens and a set of relationship tokens. We apply two separate FFNs to predict bounding boxes and class embeddings. To calculate probabilities over  and  classes, we compute the cosine similarity of the predicted class embeddings with the GT class embeddings,   and  followed by a softmax.














Next, to determine which SG tokens correspond to which GT objects and relationships, we match the predictions of the SG tokens with the ground-truth SG. We follow the matching process and loss computation as in DETR~\cite{detr2020}, except that in our case, objects and relationships are matched separately. Our final SG loss  is the sum of the objects matching loss  and the relationships matching loss . The complete matching process and losses details are in~\Secref{supp:models:sg_loss}.




We optimize our final loss as the sum of the scene graph loss and the image-text loss:
 
where  is a hyperparameter.







































\begin{table*}[t!]
    \vspace{-1.0em}
    \centering
	\tablestyle{0.1pt}{1.}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|ccc|ccccc|cc|c}
            \toprule
            &\multicolumn{3}{c|}{Winoground} &\multicolumn{8}{c}{VL-Checklist}\\ 
            \toprule
            &\multicolumn{3}{c|}{NoTag} &\multicolumn{5}{c|}{Attribute} &\multicolumn{2}{c|}{Object}&\multicolumn{1}{c}{Relation}\\
            Model & Text & Image & Group & Action & Color & Material & Size & State & Location & Size & Action\\
            \midrule            
            CLIP &30.4 &11.1 & 8.2 & 68.1 &70.2 & 73.1 & 52.9 &63.3 &81.0 & 80.1 & 78.0\\
            BLIP & 44.8 & 23.8 & 19.2 & 79.5 &83.2 & 84.7 & 59.8 &68.8 &83.0 & 81.3 & 81.5\\
            BLIP2 & 50.0  & 31.9 & 26.7  &81.0  & 86.2 & 90.3  & 61.7  & 70.1 & 85.4  & 84.3 & 84.9 \\
            \midrule
            CLIP-SGVL(ours) &33.3 \gcol{+2.8} & 14.0 \gcol{+2.9} & 8.7 \gcol{+0.5}  & 76.6 \gcol{+8.5} & 78.7 \gcol{+8.5} & 81.3 \gcol{+8.2} & 59.7 \gcol{+6.8}  & 62.0 \rcol{-1.3} & 83.2 \gcol{+2.2} & 82.0 \gcol{+1.9} & 81.3 \gcol{+3.3}\\
            BLIP-SGVL(ours) &45.9 \gcol{+1.1} &~34.3 \gcol{+10.5} & 25.6 \gcol{+6.4}  & 79.2 \rcol{-0.3} &~94.5 \gcol{+11.3} & 91.9 \gcol{+7.2} & ~73.3 \gcol{+13.5} & 70.0 \gcol{+1.2} & 86.4 \gcol{+3.4} & 83.9 \gcol{+2.6} & 81.9 \gcol{+0.4}\\
            BLIP2-SGVL(ours) &51.7 \gcol{+1.7}  & 37.2 \gcol{+5.3} & 29.0 \gcol{+2.3}   & 82.4 \gcol{+1.4}  & 91.7 \gcol{+5.5} & 92.2 \gcol{+1.9}  & 70.1 \gcol{+8.4}  & 69.6 \rcol{-0.5}  &89.0 \gcol{+4.6}  &87.6 \gcol{+3.3}  & 88.8 \gcol{+3.9} \\
            \bottomrule        
    \end{tabular}
    \end{small}
    \vspace{-1.0em}
    \caption{
    \textbf{Winoground and VL-Checklist Results}. Results for SGVL and baselines on VL-Checklist subsets and the NoTag split of  Winoground. For VL-Checklist  we exclude the Visual Genome dataset.  
}
    \vspace{-1.5em}


\label{tab:res_winoground_vlc}
\end{table*} \begin{table*}[t!]
\centering
	\tablestyle{2.7pt}{1.}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|cccccccc}
            \toprule
            Model & Adjacency & Directional & Orientation & Projective & Proximity & Topological & Unallocated & Average\\
            \midrule
            BLIP  & 55.4 & 48.9 & 53.7 & 58.2 & 56.5 & 55.6 & 63.4 & 56.5\\
            BLIP2  &56.2  &47.9  &59.8  &62.5  &55.8  &66.7  &66.3  &61.9  \\
            \midrule
            BLIP-SGVL (ours) & 57.7 \gcol{+2.3} & 54.1 \gcol{+5.2} & 57.8 \gcol{+4.1} & 63.8 \gcol{+5.6}& 57.8 \gcol{+1.3}& 64.8 \gcol{+8.8} & 68.8 \gcol{+5.4} & 62.4 \gcol{+5.9}\\

            BLIP2-SGVL (ours) & 59.8 \gcol{+3.6}  & 56.9 \gcol{+9.0} & 58.5 \rcol{-1.3} & 61.5 \rcol{-1.0} & 59.7 \gcol{+3.9} &  70.0 \gcol{+3.3} & 66.8 \gcol{+0.5} & 63.4 \gcol{+1.5}\\
            \bottomrule
    \end{tabular}
    \end{small}
    \vspace{-1.0em}
    \caption{
    \textbf{VSR Results.} Results for SGVL and baselines on the VSR dataset.}
    \vspace{-1.5em}
\label{tab:res_vsr}
\end{table*} \section{Experiments and Results}
\label{sec:expr}



We apply our {\smodel} approach to three popular VLMs: CLIP, BLIP, and BLIP2.\footnote{More details of BLIP2 modifications are in~\Secref{supp:models:blip2}.} We evaluate on several VL compositional datasets following the standard protocol for each model. Additional results and ablations are in Supp.~\Secref{supp:expr}.










\subsection{Datasets}
\label{sec:expr:datasets}
We describe the training and evaluation datasets below. More details are in Supp.~\Secref{supp:impl}.

\minisection{Training} For training, we use image-SG data from Visual Genome (VG), along with a small subset (less than 1\%) of the LAION dataset as ``standard'' image-text pairs. \textbf{VG} is annotated with  images and corresponding SGs, and \textbf{LAION 400M} is a large-scale image-text pair dataset that was automatically curated from the Internet. 












\minisection{Evaluation} We evaluate our method on four main benchmarks for compositional scene understanding: VL-Checklist (VLC)~\cite{vlc}, Winoground~\cite{winoground}, ARO~\cite{yuksekgonul2023when}, VSR~\cite{Liu2022VisualSR}, and zero-shot classification. \textbf{(1) VLC} combines samples from the following datasets: VG~\cite{krishna2017visual}, SWiG~\cite{swig}, VAW~\cite{vaw}, and HAKE~\cite{hake}. For each image, two captions are given, a positive and a negative. The negative caption is constructed by modifying one word in the positive caption that corresponds to a structural visual aspect (e.g., attribute). We report results on a combined VLC dataset excluding VG. \textbf{(2) Winoground} probes compositionality in VLMs. 
Each sample is composed of two image-text pairs that have overlapping lexical content but are differentiated by swapping an object, a relation, or both. For each sample, two text-retrieval tasks (text score), and two image-retrieval tasks (image score) are defined. The group score represents the combined performance. A recent study~\cite{Diwan2022WhyIW} has shown that solving  Winoground requires not just compositionality but also other abilities. The study suggested a subset (NoTag) that solely probes compositionality. We report results on the full dataset and the NoTag split. \textbf{(3) ARO} 
proposes four tasks that test sensitivity to order and compositionality: VG Relation, VG Attribution, COCO \& Flickr30k Order. Since our approach is trained on VG, we report only the COCO and Flickr30k order tasks.
\textbf{(4) VSR} tests spatial understanding. The dataset consists of pairs of an image and a description of the spatial relations between two objects shown in the image. The VLM should classify the pairs as either true or false. We do not evaluate CLIP-based models here as CLIP does not allow such classification in a straightforward manner. 









\minisection{Zero-Shot (ZS) Classification} We evaluate 21 classification datasets following the protocol from ELEVATER~\cite{elevater}. The evaluation includes datasets such as ImageNet, CIFAR100, and others. We report the average results over the 21 tasks in \tabref{tab:res_main} and \tabref{tab:Adaptive} in Ablations.

\subsection{Implementation Details}
\label{sec:expr:impl}
We implemented {\smodel} using Pytorch. \cmr{For code and pretrained models, visit the project page at \url{https://alonmendelson.github.io/SGVL/}.} Our code and training procedures are based on CLIP, BLIP and BLIP2. For CLIP, we use the ML-Foundation Open-CLIP repository~\cite{openclip}, and for BLIP/BLIP2 we use the official implementations.\footnote{\url{https://github.com/salesforce/LAVIS}} We use the ViT/B-32 model architecture for CLIP, ViT/B-16 for BLIP and ViT-g for BLIP2. The models are initialized with original weights released by the respective authors. The training batch contains 256/32/32 image-text pairs and 8 image-SG pairs for CLIP/BLIP/BLIP2-SGVL respectively. For more info, please refer to Supp.~\Secref{supp:impl}. Last, we select the best checkpoint for each experiment using the validation set, which is composed of VG samples from VLC that are not in the test set (as mentioned above, we report VLC results excluding VG).


\subsection{Baselines}
\label{sec:expr:baselines} 
In the experiments, we compare our {\smodel} approach to three VLMs: CLIP~\cite{radford2021learning}, BLIP~\cite{blip}, and BLIP2~\cite{li2023blip2} to which {\smodel} was applied. Additionally, we compare with NegCLIP~\cite{yuksekgonul2023when}, which uses negative text augmentations and also demonstrated strong results on these datasets. For a fair comparison, we also implement BLIP/BLIP2 versions of the NegCLIP approach, which we refer to as NegBLIP/NegBLIP2. \cmr{Last, we also show comparison with the state-of-the-art VLMs: FLAVA~\cite{Singh2021FLAVAAF} and ViLT~\cite{vilt}, as well as newly introduced generative VLMs (GVLMs), such as LLaVA~\cite{liu2023llava} and MiniGPT-4~\cite{zhu2023minigpt}.}



























\begin{table*}[ht]
\begin{subtable}[t]{0.38\linewidth}
    \tablestyle{1.0pt}{1.0}
        \caption{{Scene Graph  Utilization}}
        

\begin{tabularx}
{1\linewidth}{@{}l c c c}
        \toprule
        Model & Text & Image & Group \\
        \midrule
        BLIP & 39.0 & 19.2  & 15.0  \\
        BLIP+Graph Text (GT) & 40.3 & 20.5  & 16.5  \\
BLIP+GT+Graph Neg. (GN) & 40.5 & 25.5  & 19.0  \\
        {BLIP+GT+GN+SG Tokens} & 42.8 & 27.3  & 21.5  \\
\bottomrule
\end{tabularx}
         \label{tab:graph_usage}
     \end{subtable}
\begin{subtable}[t]{0.35\linewidth}
    \tablestyle{1.0pt}{1.0}
        \caption{{Adaptive SG Tokens}}
        

\begin{tabularx}
{1\linewidth}{@{}l c c c}
        \toprule
        Model & WG & ZS & mAP\\
\midrule
        BLIP & 15.0 & 49.0  & -   \\
BLIP + SG Tokens & 20.0 & 47.5  & 16.1  \\
        BLIP + Adap. SG Tokens & 21.5 & 48.0  & 17.7  \\
        \bottomrule
\end{tabularx}
%
         \label{tab:Adaptive}
     \end{subtable}
\begin{subtable}[t]{0.25\linewidth}
    \tablestyle{1.0pt}{1.0}
        \caption{{Sparse Vs. Dense SGs}}
        

\begin{tabularx}
{1\linewidth}{@{}l c c c}
        \toprule
        Model & Text & Image & Group \\
        \midrule
        30\% of Graph & 33.2 & 26.5  & 18.0  \\
        70\% of Graph & 40.7 & 26.5  & 20.0  \\
        w/o Relations & 40.5 & 20.5  & 15.8  \\
Entire Graph & 42.8 & 27.3  & 21.5  \\
        \bottomrule
\end{tabularx}
         \label{tab:comprehensivness}
     \end{subtable}
     \vspace{-0.5em}
     \caption{\textbf{Ablations on the Winoground Dataset.} We show (a) The contribution of our proposed components, utilizing the SG information. (b) The benefits of our adaptive SG tokens. Reported metrics are: Winoground group score (WG), zero-shot (ZS) on ELEVATER, and mAP of SG prediction. (c) Importance of the image-SG comprehensiveness. More ablations are in~\Secref{supp:expr}.}
     \label{tab:ablations}
    \vspace{-1.0em}
\end{table*} \subsection{Results}
\label{sec:expr:results}




Results are shown in~\tabref{tab:res_main}, and demonstrate that CLIP/BLIP/BLIP2-{\smodel} outperforms the pretrained base models across several datasets. These improvements come at the price of a slight degradation in zero-shot performance. This may be due to several factors: (1) Finetuning with negative captions may harm the ZS performance, as observed in \citet{yuksekgonul2023when}; (2) Finetuning for SG prediction deviates from the original VLM training objective; (3) We use only 1\% of LAION image-text pairs for finetuning. Additionally, our method outperforms the Neg baselines on all datasets except ARO, which measures the sensitivity to text order. For this task, the text augmentations in \citet{yuksekgonul2023when} are more appropriate. Finally, we note that we do not evaluate CLIP-based models on VSR (See VSR in~\Secref{sec:expr:datasets}).

~\tabref{tab:res_winoground_vlc} and~\tabref{tab:res_vsr} show the performance of our method on fine-grained Winoground, VLC, VSR splits. For most splits, our method is significantly better or comparable to the pretrained models. \cmr{In~\figgref{fig:wino_vlc_vsr_visualizations}, we show where our model improves upon the baseline and where it still fails.}  For more results, please refer to~\Secref{supp:expr:more_results}.






























\subsection{Ablations}
\label{sec:ablations}


We perform a comprehensive ablation on the Winground dataset with our {\smodel} approach using the BLIP model\footnote{We chose BLIP due to its lower computational requirements (e.g., BLIP2 requires A100).} (see \tabref{tab:ablations}). More ablations are in \Secref{supp:expr:more_ablt}. We also provide ablations on \textit{all datasets} in \Secref{supp:expr:all_datasets}.















\minisection{Finetuning on VG without SGs} We compare our approach to naive finetuning with captions provided by the VG dataset. We finetuned BLIP on textual descriptions from VG, resulting in 40.0/20.5/16.0 for Winoground Text/Image/Group scores, while our BLIP-SGVL achieves 42.8/27.3/21.5, indicating the improvements are not solely due to the VG data.




\minisection{SG prediction without SG tokens} Our model predicts SG information from specialized SG tokens. A naive approach might have been to predict from the CLS token. Thus, we consider a variant that we refer to as \textit{BLIP MT (multi-task)}, which is a simple implementation of object and relation prediction. This variant does not include SG tokens and instead predicts the graph using object and relation MLPs on top of the CLS token. Using exactly the same training recipe and data as our BLIP-SGVL, this ablation achieves 38.5/22.5/17.5 for Winoground Text/Image/Group scores, while our BLIP-SGVL achieves 42.8/27.3/21.5, justifying our design choice of SG tokens.








\minisection{Scene graph utilization ablation} Our model consists of three parts: converting SG into captions, adding hard-negatives, and adding adaptive SG tokens. In~\tabref{tab:graph_usage}, we examine the contribution of each component. By finetuning the model with only positive captions generated from the SGs (\textit{BLIP+GT}), we obtain +1.3/+1.3/+1.5 for Winoground Text/Image/Group scores over the BLIP baseline. When the graph-based negatives (\textit{BLIP+GT+GN}) are added, an improvement of +1.5/+6.3/+3.2 is achieved compared to the baseline. Finally, when fully implementing our SGVL approach, i.e. adding the SG prediction task from the SG tokens, we achieve improved results over the baseline of +3.8/+8.1/6.5. This highlights the contribution of the model components. 







\minisection{Adaptive scene graph tokens} Our approach uses different transformer parameters for SG and patch tokens, unlike the standard transformer, which uses the same parameters for all tokens. Thus, we next examine what happens when SG and patch tokens use the same parameters. In~\tabref{tab:Adaptive}, we report the performance of two variants on three tasks: Winoground group score (WG), Zero-shot classification (ZS) on ELEVATER, and SG prediction (mAP metric). The first variant, \textit{BLIP + SG Tokens}, refers to the addition of tokens dedicated to predicting SGs with the same transformer parameters shared between them and other input tokens. The second variant, \textit{BLIP + Adaptive SG Tokens}, refers to our technique that introduces parameters specific to the SG tokens in every transformer layer (see \secref{sec:model:visual}). We can see that the second variant outperforms the SG token addition in all tasks. This demonstrates how our modification to the image transformer encoder improves SG prediction and VL performance without compromising ZS.



































































\begin{figure*}[ht]
\centering
    \includegraphics[width=0.95\textwidth]{Figures/neg_pos_winoground_vlc_vsr.pdf}
    \vspace{-3mm}
    \captionof{figure}{
        \cmr{\textbf{Visualization Predictions using our BLIP-SGVL model on Winoground, VL-Checklist, and VSR}. The left panel shows where our model succeeds and the baseline fails, and the right panel shows where our model still fails. For VL-Checklist examples true captions are in \green{green} and false captions in \red{red}. Our model outperforms the baseline in samples that require understanding relationships between objects, spatial relationships, binding attributes (color, material, size) to objects, and counting objects. The failure cases illustrate the complexity and ambiguity of the samples.}}
    \label{fig:wino_vlc_vsr_visualizations}
\end{figure*}



\minisection{Using sparse vs. dense SGs} In this ablation, we investigate whether the density of the SG in VG affects performance, since SGs contain denser and richer information than standard captions. Towards this end, we train SGVL with sparsified versions of the graph. Specifically, we train two variants where objects and relations from the graphs are randomly removed (30\% and 70\%) and a third variant in which all relations are removed. As can be seen in~\tabref{tab:comprehensivness}, our results show that our model performs better when the graphs are denser, richer, and describe the image more accurately, highlighting the motivation to utilize SGs for VLMs.    


\minisection{Training without LAION image-text pairs} In SGVL, we simultaneously train with the original image-text pairs from LAION and the image-SG pairs from VG. To test the effectiveness of simultaneous training, we train with only Image-SG pairs (without LAION) and obtain a degradation of -2.0/-1.2/-1.2 for Text/Image/Group scores compared to our BLIP-SGVL. In addition, we observe a degradation of 1.1\% in ZS performance when compared to our BLIP-SGVL model. The results indicate that simultaneous training is beneficial.





























\section{Conclusions}


Structured understanding of complex scenes is a key element of human perception, but its modeling still remains a challenge. In this work, we propose a new approach for incorporating structured information into pretrained VLMs from SG data to improve scene understanding. We demonstrate improved performance on four benchmarks probing compositional scene understanding with only a mild degradation in ZS performance. Our findings suggest that only a small amount of qualitative annotations may be sufficient to improve such models. \cmr{We hope these findings will encourage future work to generalize our approach beyond the VL regime or using other types of dense annotations, such as segmentation maps and depth maps.} 













\section{Limitations}


As mentioned above, our work proposes a specialized model architecture and a new finetuning scheme for learning from SGs. We demonstrate improved performance on several different models and datasets. Nevertheless, our work has some limitations. First, we leverage SG annotations since they are rich and reflect structured visual and textual information. However, the quality of the data is crucially important to our method, and thus poor data may result in lower performance. Finally, our improvements may be restricted by the fact that these annotations are rare and expensive to collect. 








\subsubsection*{Acknowledgements}
This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080). Prof. Darrells group was supported in part by DoD, including PTG and/or LwLL programs, as well as BAIR's industrial alliance programs. 







\bibliography{anthology,custom,egbib}
\bibliographystyle{acl_natbib}




\newpage
\appendix
\clearpage
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\setcounter{page}{1} \setcounter{section}{0} \section*{Supplementary Material for ``SGVL''}




Here we provide additional information about our experimental results, qualitative examples, implementation details, and datasets. Specifically, \Secref{supp:expr} provides more experiment results, \Secref{supp:method} provides more additional method details, \Secref{supp:qual} provides qualitative visualizations to illustrate our approach, and \Secref{supp:impl} provides additional implementation details.




\section{Additional Experiment Results}
\label{supp:expr}

We begin by presenting several additional ablations (\Secref{supp:expr:more_ablt}) that further demonstrate the benefits of our {\smodel} approach. Next, we present the BLIP model ablations for all datasets (\Secref{supp:expr:all_datasets}). Last, we present additional results (\Secref{supp:expr:more_results}). 






\subsection{Additional Ablations}
\label{supp:expr:more_ablt}

In what follows, we provide additional ablations that further illustrate the benefits of {\smodel}. For all ablations, we finetuned the BLIP model using the same recipe, in which only LoRA adapters are learned, and the training batch consists of VG image-SG pairs and LAION image-text pairs. 






\minisection{The importance of the SG data} To examine the significance of the information provided by scene graphs, we suggest learning SGs without any useful information. Thus, we run an experiment in which the SGs are completely random. This ablation obtains on Winoground 37.8/18.7/14.0 compared to our BLIP-SGVL 42.8/27.3/21.5 for Text/Image/Group scores (the BLIP baseline obtains 39.0/19.2/15.0). This illustrates that the approach we employ is not merely a regularization, but also provides important information about the SGs that can be used by pretrained VLMs.



\minisection{The effect of image-SG data size} In this experiment, we train our method using varying amounts of image-SG pairs of data (10\%, 40\%, 70\%, and 100\% of the dataset) in order to examine the effect of the data portion.~\figgref{supp:fig:sg_amount} shows the Winoground group score performance as a function of the image-SG pairs data portion. As can be seen, the positive slope suggests that adding image-SG data consistently improves results.






\minisection{Training with negatives from non-graph data} 
To demonstrate the importance of structured information in textual descriptions, we examine the performance of the model when only LAION captions are used. Specifically, we trained using generated negatives that were not derived from SG data but were generated in a manner that approximated our graph-based negatives. Since we do not have the graphs in this setup, we apply the following augmentations: (i) Swapping asymmetric relations - We swap the nouns that are relevant to the relation by using a standard parser. (ii) Relation falsification - The relation is replaced with one from a closed set of relations we manually annotated in order to obtain the wrong semantic meaning. (iii) Attributes swapping - We swap attributes from a closed set of attribute categories that we manually annotated (e.g., color, etc.). The Text/Image/Group scores compared to the BLIP baseline are +4.0/-0.7/-0.8 while using BLIP with our graph-based augmentations (without SG tokens) obtains +1.5/+6.3/+4.0 compared to the BLIP baseline. It can be seen that the generated negatives from LAION improve only the Text score while applying our graph-based negatives improves all the metrics. This indicates that the main reason for the improvement is the structured information contained in the descriptions generated from the scene graphs.


\begin{figure}[t!]
\centering
    \includegraphics[width=.5\textwidth]{Figures/scene_graph_amount.pdf}
    
\captionof{figure}{
        \textbf{Image-SG Pair Data Size}. We report the performance of our model on Winoground group score as a function of the amount of scene-graph data used during training (percentage of the available data).}
    \label{supp:fig:sg_amount}
\end{figure}




\minisection{Scene graph token representations} To analyze what the scene graph tokens learned, we can evaluate the ability of object and relationship tokens to be utilized explicitly for the auxiliary task as a simple SG predictor in images. This is accomplished by predicting the scene graphs on Visual Genome based on the learned SG tokens. We compared the learned SG tokens with a BLIP model extended with object and relationship heads, as explained in \textit{BLIP-MT} variant in the main paper (See~\Secref{sec:ablations}). Our model achieved an mAP of 17.7, while the \textit{BLIP MT} achieved an mAP of 14.4. These results indicate that the scene graph tokens learn meaningful and useful representations. 

\begin{figure*}[t!]
\centering
    \includegraphics[width=\textwidth]{Figures/tokens_specialization.pdf}
\captionof{figure}{
        \textbf{Tokens Specialization}. We visualize the box predictions of 10 random object tokens (left) and 7 relationship tokens (right) on all images from the COCO validation set. Each box is represented as a point with the normalized coordinates of its center. Colors indicate the predictions made by different tokens.}
    \label{supp:fig:token_specialization}
\end{figure*}

\begin{table*}[t!]
    \centering
	\tablestyle{3pt}{1.}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|ccc|ccc|cc|c}
            \toprule
            &\multicolumn{3}{c|}{Winoground} & \multicolumn{3}{c|}{VL-Checklist} & \multicolumn{2}{c|}{ARO} & 
            \multicolumn{1}{c}{VSR}
            \\
            \toprule
            &\multicolumn{3}{c|}{All Dataset}
            &\multicolumn{3}{c|}{All Datasets Avg}
            & Flickr30K
            & COCO
            & All Dataset\\
            Model & Text & Image & group & Attribute & Object & Relation & Order & Order & Avg\\
            \midrule 
            BLIP  &39.0 &19.2 & 15.0 & 75.2 & 82.2 & 81.5 & 27.9 & 24.9 & 56.5\\
            BLIP + Graph Text (GT)  & 40.3 &20.5 & 16.5  & 76.0 & 80.8 & 77.5 & 28.0 & 24.6 & 57.2 \\
            BLIP + GT + Graph Neg. (GN)  & 40.5 &25.5 & 19.0  & 80.0 & 84.0 & 81.2 & 69.6 & 70.0 & 61.4 \\
            BLIP + GT + GN + SG Tokens &42.8 &27.3 & 21.5 & 81.8  & 85.2 & 81.9 & 70.0 & 71.0& 62.4\\
            \bottomrule        
    \end{tabular}
    \end{small}
\caption{Scene graph utilization ablation results for \textit{all datasets}.}
\label{supp:tab:graph_usage_all_datasets}
\end{table*} \begin{table*}[t!]
    \centering
	\tablestyle{0.8pt}{1.}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|ccc|ccc|cc|c|c|c}
            \toprule
            &\multicolumn{3}{c|}{Winoground} & \multicolumn{3}{c|}{VL-Checklist} & \multicolumn{2}{c|}{ARO} & 
            \multicolumn{1}{c|}{VSR} & 
            \multicolumn{1}{c|}{Graph Pred.} & 
            \multicolumn{1}{c}{ZS}
            \\
            \toprule
            &\multicolumn{3}{c|}{All Dataset}
            &\multicolumn{3}{c|}{All Datasets Avg}
            & Flickr30K
            & COCO
            & All Dataset
            & mAP
            & 21 Tasks\\
            Model & Text & Image & group & Attribute & Object & Relation & Order & Order & Avg & Score & Avg\\
            \midrule 
            BLIP  &39.0 &19.2 & 15.0 & 75.2 & 82.2 & 81.5 & 27.9 & 24.9 & 56.5 & - & 49.0\\
            BLIP + SG Tokens  & 39.8 &26.5 & 20.0  & 81.0 & 84.5 & 81.2 & 67.6 & 67.0 & 61.9 & 16.1 & 47.5\\
            BLIP + Adaptive SG Tokens &42.8 &27.3 & 21.5  & 81.8  & 85.2  & 81.9  & 70.0 & 71.0 & 62.4  & 17.7 &  48.0 \\

            \bottomrule        
    \end{tabular}
    \end{small}
\caption{Adaptive SG tokens results for \textit{all datasets}.}
\label{supp:tab:sg_tokens_all_datasets}
\end{table*} 
\minisection{Token specialization} Our SGVL approach learns a different specialization for each scene graph token in~\figgref{supp:fig:token_specialization}. We visualize the bounding box center coordinates predicted by 10 different object tokens and 7 random relationship tokens for all images in the COCO val set. We observe that these tokens are specialized in different locations in the image, whereas the relationship tokens tend to be centered since their boxes are larger and spread over a greater area.






\subsection{Ablations on All Datasets}
\label{supp:expr:all_datasets}

The main paper only includes ablation results for Winoground in~\tabref{tab:graph_usage} and in~\tabref{tab:Adaptive}. Here, in~\tabref{supp:tab:graph_usage_all_datasets} and~\tabref{supp:tab:sg_tokens_all_datasets}, we include additional ablation results. Specifically, we have performed a comprehensive ablation using our SGVL approach with the BLIP model on \textit{all datasets}: Winoground, VL-Checklist, ARO, and VSR. It is evident from~\tabref{supp:tab:graph_usage_all_datasets} that the utilization of our approach for both visual and textual components is important. Last,~\tabref{supp:tab:sg_tokens_all_datasets} shows that the ``Adaptive SG Tokens'' improve SG prediction and VL performance for all datasets with only a mild zero-shot degradation.





\begin{table*}[t!]
    \centering
	\tablestyle{4pt}{1.}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|ccc|ccc|cc|c}
            \toprule
            &\multicolumn{3}{c|}{Winoground} & \multicolumn{3}{c|}{VL-Checklist} & \multicolumn{2}{c|}{ARO} & 
            \multicolumn{1}{c}{VSR}
            \\
            \toprule
            &\multicolumn{3}{c|}{All Dataset}
            &\multicolumn{3}{c|}{All Datasets Avg}
            & Flickr30K
            & COCO
            & All Dataset\\
            Model & Text & Image & group & Attribute & Object & Relation & Order & Order & Avg\\


\midrule
            CLIP-SGVL~(COCO)  & 32.8 &11.8 & 10.0  & 73.5 & 83.3 & 81.0 & 83.0 & 79.4 & -\\
            BLIP-SGVL~(COCO)  & 43.8 &26.0 & 21.3  & 81.9 & 85.5 & 82.0 & 70.7 & 71.2 & 61.7\\
            BLIP2-SGVL~(COCO)  & 46.0 &26.5 & 22.8  & 81.2 & 88.8 & 88.8 & 77.4 & 78.2 & 63.0\\
            \bottomrule
    
    \end{tabular}
    \end{small}
\caption{\textbf{Winoground, VL-Checklist, ARO, VSR Results} when finetuning on COCO image-text pairs instead of LAION}.
\label{tab:res_coco}
\end{table*} 

\subsection{Additional Results}
\label{supp:expr:more_results}

We start by presenting zero-shot classification results tested with linear probing. Next, we show additional results when finetuning image-text pairs from COCO in~\tabref{tab:res_coco}. Moreover, we show additional results on \textit{all splits} in Winoground in~\tabref{supp:tab:res_winoground_full}, as suggested in~\citet{Diwan2022WhyIW}. Last, we also present fine-grained results of NegCLIP, NegBLIP, and NegBLIP2 baselines on VL-Cheklist, and VSR datasets in~\tabref{supp:tab:res_vlc_full} and \tabref{supp:tab:res_vsr_neg}. For all tables, the difference between the baselines and our SGVL approach is denoted by gains \gcol{+X} and losses \rcol{-X}. 





\cmr{As discussed in our paper, we believe that the slight degradation in zero-shot classification is due to fine-tuning with negative captions and graph prediction tasks that deviate from the original VLM training objective. This phenomenon is also visible in recent works~\cite{doveh2022teaching,yuksekgonul2023when}. Here, we also report on the classification performance achieved with linear probing. For CLIP-SGVL, when using 5-shot, 10-shot linear probing, the difference on classification tasks with respect to CLIP (in the same setting) changes to -0.3\%, +0.1\% and with 20-shot to +0.5\%. This demonstrates that partial fine-tuning does not cause degregation as previously observed in other studies.}

\cmr{In order to demonstrate that our method can also be used with image-text pairs other than LAION, we report in~\tabref{tab:res_coco} results when used with image-text pairs from COCO~\cite{Lin2014MSCOCO}. It can be seen that the results are comparable to our original version, which indicates the flexibility of our approach.}





~\tabref{supp:tab:res_winoground_full} shows the performance on fine-grained splits on Winoground. It can be observed that our method generally outperforms or is comparable to pretrained models for most splits. Although we present all splits, we note that~\citet{Diwan2022WhyIW} suggested that only the samples from the NoTag split are actually probing compositionality, while the other splits are difficult to solve for various additional reasons. Thus, the NoTag is the most important split to evaluate.








~\tabref{supp:tab:res_vlc_full} shows the performance on fine-grained splits of the VL-Checklist dataset, including the ``Attribute'', ``Object'', and ``Relation'' splits. As can be seen, our method improves both the pretrained baselines CLIP, BLIP, and BLIP2, as well as the NegCLIP, NegBLIP, and NegBLIP2 baselines on those fine-grained splits. 


~\tabref{supp:tab:res_vsr_neg} shows the performance on fine-grained splits of the VSR dataset, which test spatial understanding in VLMs. Similarly, we find that our method improves on most splits and across models when compared with pretrained and Neg baselines.

Overall, our approach improves multiple architectures (CLIP, BLIP, and BLIP2) on a variety of VL datasets with only mild degradation in zero-shot performance.

\begin{table*}[t!]
\centering
	\tablestyle{1.0pt}{1.0}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|ccc|ccc|ccc|ccc|ccc|ccc|ccc}
            \toprule
            &\multicolumn{3}{c|}{Ambiguosly}
            &\multicolumn{3}{c|}{Unusual}
            &\multicolumn{3}{c|}{Unusual}
            &\multicolumn{3}{c|}{Non}
            &\multicolumn{3}{c|}{Visually} &\multicolumn{3}{c|}{Complex}
            &\multicolumn{3}{c}{No}\\
            &\multicolumn{3}{c|}{Correct}
            &\multicolumn{3}{c|}{Image}
            &\multicolumn{3}{c|}{Text}
            &\multicolumn{3}{c|}{Compositional}
            &\multicolumn{3}{c|}{Difficult} &\multicolumn{3}{c|}{Reasoning}
            &\multicolumn{3}{c}{Tag}
            \\
            Model& T & I & G & T & I & G & T & I & G & T & I & G & T & I & G & T & I & G & T & I & G\\
            \midrule            
            CLIP & 30.4 & 15.2 & 13.0 & 25.0 & 8.9 & 5.4 &30.0 &16.0 & 10.0 & 76.7 & 36.7 & 33.3 & 15.8 & 0.0 & 0.0 &24.4 &7.7 & 3.8 & 30.4 & 11.1 & 8.2\\
            BLIP  & 39.1 & 17.4 & 15.2 & 37.5 & 16.1 & 14.3 &30.0 &14.0 & 8.0 & 50.0 & 33.3 & 30.0 & 29.0 & 10.5 & 10.5 &24.4 &7.7 & 2.6 & 44.8 & 23.8 & 19.2\\
            BLIP2  &41.3  &28.3 &19.6  &30.4  &17.9  &14.3 &38.0 &14.0 &12.0  &53.3 &26.7  &26.7  &34.2 &13.1  &10.5 &20.5 &7.6 &2.5 &50.0  &32.0  &26.7\\
            \midrule
            NegCLIP  &28.2  &4.3  &4.3  &17.9  &7.2 &3.6 &36.0 &8.0  &8.0 &66.7  &30.0  &26.7 &10.5  &2.6 &2.6 &21.8 &7.7  &5.1  &33.2&12.2&8.7\\
            NegBLIP  &43.5  &21.7 &8.7  &42.9  &19.6  &14.3 &28.0 &18.0 &12.0  &56.7 &43.3  &33.3  &28.9 &15.8  &10.5 &32 &10.2&3.8 &47.0  &27.3  &22.6\\
            NegBLIP2  &41.3  &23.9 &19.6  &35.7  &17.8  &17.8 &36.0 &14.0 &14.0  &63.3 &33.3  &30.0  &23.7 &15.8  &13.1 &24.3 &9.0&6.4 &49.4  &34.3  &27.9\\
            \midrule
            CLIP-SGVL (ours)  & 37.0 & 13.0 & 8.7 & 28.6 & 10.7 & 7.1 &32.0 &10.0 & 8.0 & 70.0 & 40.0 & 30.0 & 18.4 & 5.3 & 5.3 &24.4 &16.7 & 12.8 & 33.2 & 14.0 & 8.7\\
            BLIP-SGVL (ours)  & 45.6 & 21.7 & 21.7 & 44.6 & 23.2 & 21.4 &38.0 &24.0 & 18.0 & 46.7 & 43.3 & 40.0 & 23.7 & 18.3 & 15.8 &33.3 &11.2 & 7.7 & 45.9 & 34.3 & 25.6\\
            BLIP2-SGVL (ours)  & 43.5  & 26.1  & 23.9 & 39.3  & 26.8 & 21.4 & 34.0 & 18.0 & 16.0  & 60.0 & 46.7 & 46.7 & 26.3 & 18.4  & 15.3  & 29.5 & 11.5 & 6.4 & 51.7  & 37.2  & 29.0 \\
            \bottomrule        
    \end{tabular}
    \end{small}
\caption{
    \textbf{Results on Winoground} for all the splits presented in \citet{Diwan2022WhyIW}. We report T, I, and G for Text retrieval, Image retrieval, and Group retrieval.}

\label{supp:tab:res_winoground_full}
\end{table*} \begin{table*}[t!]
\centering
	\tablestyle{0.8pt}{1.}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|ccccc|cc|c} 
            \toprule
             &\multicolumn{5}{c|}{Attribute} &\multicolumn{2}{c|}{Object}&\multicolumn{1}{c}{Relation}\\
            Model & Action & Color & Material & Size & State & Location & Size & Action\\
            \midrule            
            CLIP  & 68.1 &70.2 & 73.1 & 52.9 &63.3 &81.0 & 80.1 & 78.0\\
            BLIP  & 79.5 &83.2 & 84.7 & 59.8 &68.8 &83.0 & 81.3 & 81.5\\
            BLIP2   &81.0  & 86.2 & 90.3  & 61.7  & 70.1 & 85.4  & 84.3 & 84.9 \\
            \midrule
            NegCLIP &66.7  &74.9  &78.4  &54.8  & 63  & 81.9  &80.9  & 81.3\\
            NegBLIP &82.6  &88.3  &89.3  &60.8  &70.1  &81.0  &79.6  &83.0 \\
            NegBLIP2  &82.1  &88.7  &90.7  &63.0  &70.0  &87.2  &86.8  &88.2 \\
            \midrule
            CLIP-SGVL (ours) & 76.6 \gcol{+8.5} & 78.7 \gcol{+8.5} & 81.3 \gcol{+8.2} & 59.7 \gcol{+6.8}  & 62.0 \rcol{-1.3} & 83.2 \gcol{+2.2} & 82.0 \gcol{+1.9} & 81.3 \gcol{+3.3}\\
            BLIP-SGVL (ours)   & 79.2 \rcol{-0.3} & 94.5 \gcol{+11.3} & 91.9 \gcol{+7.2} & 73.3 \gcol{+13.5} & 70.0 \gcol{+1.2} & 86.4 \gcol{+3.4} & 83.9 \gcol{+2.6} & 81.9 \gcol{+0.4}\\
            BLIP2-SGVL (ours) &82.4 \gcol{+1.4}  & 91.7 \gcol{+5.5} & 92.2 \gcol{+1.9}  & 70.1 \gcol{+8.4}  & 69.6 \rcol{-0.5}  &89.0 \gcol{+4.6}  &87.6 \gcol{+3.3}  &88.8 \gcol{+3.9}\\
            \bottomrule        
    \end{tabular}
    \end{small}
\caption{
    \textbf{VL-Checklist Results}. We report VL-Checklist~\cite{vlc} on all splits of the Attribute, Object, and Relation tests, excluding the Visual Genome dataset. The difference between base models and SGVL is denoted by \gcol{+X}.}


\label{supp:tab:res_vlc_full}
\end{table*} \begin{table*}[t!]
\centering
	\tablestyle{2.7pt}{1.}
	\scriptsize
    \begin{small}
    \begin{tabular}{l|cccccccc}
            \toprule
            Model & Adjacency & Directional & Orientation & Projective & Proximity & Topological & Unallocated & Average\\
            \midrule
            BLIP  & 55.4 & 48.9 & 53.7 & 58.2 & 56.5 & 55.6 & 63.4 & 56.5\\
            BLIP2  &56.2  &47.9  &59.8  &62.5  &55.8  &66.7  &66.3  &61.9 \\
            \midrule
            NegBLIP &56.0  &48.0  &55.7  &59.0 & 55.6 &58.7 & 63.2 & 57.8 \\
            NegBLIP2 &56.3  &48.0  &57.4  &61.5 &55.8 &67.8&70.5& 61.2 \\
            \midrule
            BLIP-SGVL (ours) & 57.7 \gcol{+2.3} & 54.1 \gcol{+5.2} & 57.8 \gcol{+4.1} & 63.8 \gcol{+5.6}& 57.8 \gcol{+1.3}& 64.8 \gcol{+8.8} & 68.8 \gcol{+5.4} & 62.4 \gcol{+5.9}\\

            BLIP2-SGVL (ours) & 59.8 \gcol{+3.6}  & 56.9 \gcol{+9.0} & 58.5 \rcol{-1.3} & 61.5 \rcol{-1.0} & 59.7 \gcol{+3.9} &  70.0 \gcol{+3.3} & 66.8 \gcol{+0.5} & 63.4 \gcol{+1.5}\\
            \bottomrule
    \end{tabular}
    \end{small}
    \vspace{-1.0em}
    \caption{
    \textbf{VSR Results.} We report accuracy on all splits of the VSR~\cite{Liu2022VisualSR} dataset.}
    \vspace{-1.5em}
\label{supp:tab:res_vsr_neg}
\end{table*} 














\section{Additional Modeling Details}
\label{supp:method}
We begin by presenting additional model details regarding our graph preprocessing procedure (\Secref{supp:models:graph}). Next, we describe our method for creating graph-based negatives (\Secref{supp:models:negatives}), which is illustrated in~\figgref{supp:fig:negative}. We provide more details on our scene graph loss (\Secref{supp:models:sg_loss}) and some modifications made to our loss calculation when training BLIP~\cite{blip} and BLIP2~\cite{li2023blip2} (\Secref{supp:models:blip}). We conclude by describing more in detail our approach when using the BLIP2 model (\Secref{supp:models:blip2}).


\subsection{Graph Preprocessing}
\label{supp:models:graph}

We next describe how we process the image-SG pairs to create our training dataset. Our guiding principle is to create image-SG training pairs where the graphs are dense enough but not too large, in order to allow structured and short descriptions. To this end, given an image  and a corresponding graph , we extract the sub-graphs by taking a random walk on the graph. The random walk is initialized by randomly picking a relationship from the graph (edge  and nodes  such that ) and ends when a node that has no outgoing edges is reached, resulting in a sub-graph . Next, the image is cropped to the union of the bounding boxes of all objects () in the extracted sub-graph, resulting image . We finish the process by adding new nodes and relationships to  from the residual graph  that are visible in . We use  and  as a training sample only if the derived  contains at most 10 objects (i.e. ). This process creates SGs composed of connected components that are all DAGs with a single Hamiltonian path, which facilitates caption generation.

\subsection{Graph-Based Negatives} 
\label{supp:models:negatives}

In order to generate negative image captions, we propose a set of predefined rules that when applied to an image-scene graph pair, result in a negative scene graph that incorrectly describes the image. Our scene graph to caption scheme transforms negative scene graphs into negative captions that are semantically inconsistent with the images they accompany. For each training sample we randomly apply one of the following negative rules focused on object attributes and relationships in the graph: (i) \textit{asymmetric relations swapping}  - we call a relationship  asymmetric, if for two objects , . We manually annotated a relation as asymmetric out of the  most common VG relations. We use these to generate a negative scene graph by searching for an edge  representing an asymmetric relationship, and modify the graph by replacing  with an edge . For example, in the case of a graph describing the phrase ``dog chasing cat'', such a negative will result in the phrase ``cat chasing dog''. (ii) \textit{relation falsification} - we replace relations in the graph with false relations. For example, we turn ``cup on table'' to ``cup under table''. For negatives focused on object attributes, we first scan the dataset and split the attributes into the following categories: \emph{color}, \emph{material}, \emph{size}, \emph{state}. Next, we use this split to perform two types of negatives: (i) \textit{attributes falsification} - we replace attributes for objects in the graph with false attribute from the same category. For example, turning ``blue ball'' to ``red ball''. (ii) \textit{attributes swapping} - we search the graph for two objects that are annotated with attributes from the same category. Given that such a pair has been found we switch between the attributes, resulting for example, ``silver spoon and golden knife'' from ``golden spoon and silver knife''. 





\subsection{Scene Graph Loss} 
\label{supp:models:sg_loss}

This section provides a detailed explanation of our Scene Graph loss, as mentioned in the main paper in~\Secref{sec:model:training}.

As explained in the paper (See~\Secref{sec:model:training}), we incorporate SG tokens into the model, which are used to predict the SG that corresponds to the image. We next explain this process. The graph  contains several annotations: the set of object categories , the set of bounding boxes , and set of relationship categories . We also augment the relationships with a set of bounding boxes, that are constructed as the union of the boxes of the corresponding objects, and denote these by . 

As we use an open vocabulary approach, we do not aim to predict the object and relationship categories directly, but rather use the embeddings from the VL model. Thus, we extract the embeddings of these labels with the text encoder  to get class embeddings:  , and . We note that  and  classes are due to ``no object'' and ``no relationship'' categories (denoted as ), which are represented with learned embeddings.
















Thus far we described the elements of the SG that we aim to predict from the SG tokens. We next describe the prediction process, and the corresponding losses. The image encoder outputs a set of  object queries and  relationship queries. We apply two feed-forward networks,  and , on top of these queries to predict bounding boxes and label embeddings respectively. Specifically, given the final representations of  object token and  relationship token for image , which we denote  and  respectively, we predict:  


where  are bounding box predictions and  are class embeddings predictions. Next, we use the class embeddings matrices to predict probabilites over  and  classes:


where  and . Next, we need to match the predictions of the SG tokens with the ground-truth SG, in order to determine which SG tokens correspond to which ground-truth objects and relationships. We follow the matching approach as in DETR~\cite{detr2020}, except that in our case, objects and relationships are matched separately. We describe the object matching below.
Given a permutation  over the object tokens, we define the matching-cost between the permutation and the GT by:

\small

\normalsize


Namely, we check the compatibility between the GT and permuted objects both in terms of object category (i.e., the probability assigned by the query to the GT object ) and in terms of bounding boxes (i.e., how well the predicted box matches the GT one). Here  is from~\cite{giou_2018_CVPR}. 

The optimal matching  is found by optimizing this score: . Finally, we use the optimal matching  from above to calculate the following Objects loss:

\small

\normalsize
The relation matching and loss  is calculated in a similar manner, and the total scene graph loss is the sum of  and :






\subsection{BLIP Image-Text Loss Details} 
\label{supp:models:blip}
Besides image and text unimodal encoders trained using a contrastive loss,  BLIP~\cite{blip} and BLIP2~\cite{li2023blip2} also includes an image-grounded text encoder that uses additional image-text cross-attention layers. The encoder is equipped with a binary classification head (a linear layer) and is trained to predict whether an image-text pair is positive (matching) or negative (unmatching). In the training procedure described by the authors, the encoder uses a hard negative mining strategy to calculate an additional loss for all image-text pairs in the batch. When training our BLIP/BLIP2-SGVL models, we apply this loss as well. Additionally, we use this encoder to add another term to our graph-based negative loss (). Let  denote the positive score given by the encoder to some image-text pair (), then the following term is added to :

\small

\normalsize
with  denoting the VG images, positive and negative captions in the batch, respectively.



\subsection{BLIP2 Model Details}
\label{supp:models:blip2}
In this section, we describe how we incorporate our ``Adaptive SG Tokens'' into the BLIP2~\cite{li2023blip2} model. Recall that the BLIP2 model architecture is based on a Q-Former module that consists of two transformer sub-modules: (a) an image transformer that interacts with a frozen image encoder to produce the visual features used for contrastive learning; and (b) a text transformer that performs both the functions of a text encoder (producing the textual features required for contrastive learning) and a text decoder. The inputs to the image transformer sub-module are a set of learnable query embeddings. These queries interact with each other through self-attention layers, as well as with frozen image encoder features through cross-attention layers. When applying our SGVL approach to BLIP2, we add our SG tokens as an additional set of prompts in parallel to the learnable queries. Therefore, our SG tokens interact with the learnable queries and the frozen image encoder features. We apply our adaptation technique to these tokens as we do in BLIP and CLIP (see Adaptive SG Tokens in~\secref{sec:model:visual}) and use the exact same procedure for the graph prediction task.  




\begin{figure*}
\centering
    \includegraphics[width=.9\textwidth]{Figures/caption_negative_figure.pdf}
    \vspace{-3em}
    \captionof{figure}{
        \textbf{Visualization} of some of our Graph-based Negatives as well as the SG-to-Text module. We show the generation process of positive captions (green) and negative captions using the graph (red).}
    \label{supp:fig:negative}
\end{figure*}

\section{Qualitative Visualizations}

\label{supp:qual}




\figgref{supp:fig:negative} shows a visualization of the generation process of captions, including positive captions as well as negative captions, based on our Graph-based Negatives module. As shown in the figure, captions generated from scene graphs are much more focused on describing fine-grained details. Furthermore, we show in~\figgref{supp:fig:sg_visualization} visualizations of scene graph tokens predictions for images from Visual Genome, which the model was not trained on. It can be seen that although the model has not been trained on these images, the predictions are reasonable. Finally, we show in~\figgref{supp:fig:winoground_pos_neg} and~\figgref{supp:fig:vlchecklist_pos_neg} error analysis on Winoground and VL-Checklist to evaluate the success and errors of our method and the baselines. This illustrates which examples our BLIP-SGVL model is successful on, in contrast to the BLIP model.

















\section{Additional Implementation Details}
\label{supp:impl}

Our {\smodel} approach can be used on top of a variety of VL models. For our experiments, we choose the CLIP~\cite{radford2021learning}, BLIP~\cite{blip} and BLIP2~\cite{li2023blip2} models as they are among the most popular and easy-to-use methods. These models are implemented based on the Open-CLIP library~\cite{openclip} and the BLIP/BLIP2 code base (available at \url{https://github.com/salesforce/LAVIS}). We implement {\smodel} based on these repositories. As described above, our approach is trained using both the original image-text pairs from LAION and the image-SG pairs we curate from Visual Genome. In particular, for CLIP-SGVL we use 3M image-text pairs, while for BLIP/BLIP2-SGVL, we use 750K due to computational constrains. 

In our experiments, we trained our CLIP-SGVL on 4 V100 GPUs for 32 epochs with a batch comprised of 256 image-text pairs and 8 image-SG pairs. We use AdamW optimizer~\cite{kingma2014adam,Loshchilov2017AdamW} with , , and . We use  with cosine scheduler, and a weight decay of  for regularization. For BLIP-SGVL, we trained on 4 V100 GPUs and for BLIP2-SGVL on 4 A100 GPUs. We train both for 8 epochs with a batch comprised of 32 image-text pairs and 8 image-SG pairs. We use AdamW optimizer~\cite{kingma2014adam} with , , and . We use  with cosine scheduler, and a weight decay of  for regularization. 





\subsection{VL-Checklist}
\minisection{Dataset} VL-Checklist~\cite{vlc} is a new study that combines the following datasets: Visual Genome~\cite{krishna2017visual}, SWiG~\cite{swig}, VAW~\cite{vaw}, and HAKE~\cite{hake}. For each image, two captions are given, a positive and a negative. The positive caption is derived from the source dataset and is coherent with the visual structure of the image. the negative caption is constructed by modifying one word in the positive caption that corresponds to a structural aspect in the image. To correctly solve a sample the model needs to identify the caption faithfully describing the image. Specifically, VL-Checklist evaluates the following structured concepts: (1) Object: identifying whether objects mentioned in the text appear in the image invariantly to their spatial location and size, (2) Relation: spatial or action relation between two objects, and (3) Attribute: color, material, size, state, and action bounded to objects. We report results on a combined VL-Checklist dataset excluding VG.
\label{supp:impl:vl}








\minisection{Inference details} We use the official data and code released by the authors which is available at \url{https://github.com/om-ai-lab/VL-CheckList}.  A test sample consists of an image and two captions. For CLIP, we compute the cosine similarity between the image and the captions and report the positive caption as the one with the higher similarity. For BLIP/BLIP2 we use the ITM head, which predicts both a positive and negative score for each pair. We consider the caption with the higher positive score to be the correct one.




\subsection{Winoground}
\label{supp:impl:wino}

\minisection{Dataset} Winoground~\cite{winoground} is a new challenging dataset that evaluates the ability of VL models to capture compositionality in vision \& language. The dataset contains 1600 tests across 400 samples. Each sample is composed of two image-text pairs . The pairs have overlapping lexical content but are differentiated by a swapping of an object, a relation, or both. To correctly solve the sample the model needs to correctly solve two text retrieval and two image retrieval tasks. A recent study~\cite{Diwan2022WhyIW} has shown that solving  Winoground requires not just compositional understanding but also other abilities such as commonsense reasoning. The study proposed a new split to the dataset differentiating the samples by the source of their hardness. Specifically, the split of the samples into the following categories is as follows: \emph{Non Compositional} - There are 30 samples in this category that do not require compositional reasoning. \emph{Visually Difficult} - The model must be able to detect an item that is visually difficult to identify (small, blurry, in the background, etc.) in order to sort these samples correctly. This category includes 38 samples. \emph{Ambiguously Correct} - This category includes 46 samples where at least one caption accurately describes both images or doesn't quite describe any of the images. \emph{Unusual Text \& Unusual Image} - There are 106 samples in these categories, all of which contain unrealistic or awkward texts or images that make it difficult to solve them with a VL model. \emph{Complex Reasoning} - This category consists of 78 samples that require common sense reasoning or knowledge of the world around us. \emph{No Tag} - These are vanilla Winoground examples that solely probe compositional understanding.   



\minisection{Inference details} We use the official data and code released by the authors which is available at \url{https://huggingface.co/datasets/facebook/winoground}. For testing, the pairs are given, and a text score, an image score, and a group score for a sample is computed in the following way: The text score is 1 if and only if image  has a higher similarity to caption  than , and image  has a higher similarity to caption  than . Similarly the image score is 1 if and only if caption  has a higher similarity to image  than image  and  has a higher similarity to image  than image . The group score is 1 if and only if both text and image scores are 1. Thus, the random chances for both the image and text score, is  while for group score it is . Similarities between image-text pairs is computed as in \secref{supp:impl:vl}.


\subsection{ARO}
\label{supp:impl:aro}
\minisection{Dataset} ARO~\cite{yuksekgonul2023when} (Attribution, Relation, and Order) is a new benchmark that tests compositionality in VL models.
The authors propose four tasks that are sensitive to order and composition, namely Visual Genome Relation, Visual Genome Attribution, COCO\& Flickr30k Order. Since our approach is trained on Visual Genome, we report only the COCO and Flickr30k order task (PRC). For the order task, image-text pairs from the mentioned datasets are used. The words in the text are reordered in order to create false captions for the image, according to the following perturbations: \emph{nouns and adjectives shuffle}, \emph{everything but nouns and adjectives shuffle},  \emph{trigrams shuffle} and \emph{words within trigrams shuffle}.


\minisection{Inference details} We use the official data and code released by the authors which is available at \url{https://github.com/mertyg/vision-language-models-are-bows}. 
During inference, each sample consists of an image and five textual descriptions. The similarity of each text to the image is measured as in \secref{supp:impl:vl}, and the text with the highest similarity to the image is reported as the real caption.



\subsection{VSR}
\label{supp:impl:vsr}
\minisection{Dataset} VSR~\cite{Liu2022VisualSR} VSR (Visual Spatial Reasoning) is a new benchmark for measuring the spatial understanding of vision-language models. The VSR dataset consists of natural image-text pairs in English, each example contains an image and a natural language description of the spatial relationship between two objects shown in the image. The VL model needs to classify images and captions as either true or false, indicating whether a caption accurately describes the spatial relationship. The dataset has more than  image-text samples,  derived from 6,940 COCO images and covers 65 spatial relations. The dataset is split into a train, validation and test sets, however, since we evaluate in a zero-shot manner we test our model and baselines using all samples from the train, validation, and test splits. The spatial relations are divided into 7 meta-categories: \emph{Adjacency}, \emph{Directional}, \emph{Orientation}, \emph{Projctive}, \emph{Proximity}, \emph{Topological}, \emph{Unallocated}. We report results according to these categories, as well as the average over all spatial relations. 










\minisection{Inference details} We use the official data released by the authors which is available at \url{https://github.com/cambridgeltl/visual-spatial-reasoning} We do not evaluate CLIP on this task since the task requires assigning a true or false label to an image-text pair. CLIP, however, does not allow this to be done in a straightforward manner, and Therefore only the BLIP/BLIP2 models can be used. We use the ITM head to determine whether the sample is true or false.

\subsection{Finetuning Datasets}
\label{supp:datasets}

In our work, we use the LAION dataset as ``standard'' image-text pairs, along with image-SG data pair from Visual Genome~\cite{krishna2017visual} (VG). Visual Genome is annotated with  images accompanied by their corresponding scene graphs. On average, images have 35 entities, 21 relationships, and 26 attributes per image. Additionally, there are approximately 70K object categories and 40K relationship categories. In general, Visual Genome scene graphs can be viewed as dense knowledge representations for images, similar to the format used for knowledge bases in natural language processing.








\subsection{Licenses and Privacy}
\label{supp:datasets:Licenses}
The license, PII, and consent details of each dataset are in the respective papers. In addition, we wish to emphasize that the datasets we use do not contain any harmful or offensive content, as many other papers in the field also use them. Thus, we do not anticipate a specific negative impact, but, as with any Machine Learning method, we recommend to exercise caution. 













\begin{figure*}
\centering
    \includegraphics[width=.9\textwidth]{Figures/sg_visualizations_supp.pdf}
\captionof{figure}{
        \textbf{Scene Graph Prediction}. We show the predictions of the ``scene graph tokens'' on images from Visual Genome that were not trained by our model.}
    \label{supp:fig:sg_visualization}
\end{figure*}










\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{Figures/neg_pos_winoground.pdf}
\captionof{figure}{
        \textbf{Error Analysis on Winoground}. We demonstrate on the left in green where our BLIP-SGVL model succeeds, while the baseline BLIP model fails. On the right, in red, we can observe examples in which our BLIP-SGVL model fails. As visible, our model improves in samples that require understanding relations between objects, binding attributes to objects, and counting objects.}
    \label{supp:fig:winoground_pos_neg}
\end{figure*}



\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{Figures/neg_pos_vlchecklist.pdf}
\captionof{figure}{
        \textbf{Error Analysis on VL-Checklist}. We demonstrate on the left in green where our BLIP-SGVL model succeeds, while the baseline BLIP model fails. On the right, in red, we can observe examples in which our BLIP-SGVL model fails. True captions are in \green{green} and false captions in \red{red}. As visible, some of the samples on which our model fails are ambiguous or visually difficult to solve.}
    \label{supp:fig:vlchecklist_pos_neg}
\end{figure*}







%
 




\end{document}
