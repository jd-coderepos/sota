\documentclass{article}








\usepackage[preprint]{neurips_2022}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[hidelinks]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{isabelle,isabellesym}
\usepackage{xcolor}         \usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}

\usepackage{mdframed}
\usepackage{algorithm}
\usepackage{algpseudocode}


\newcommand{\SUM}[1]{{\color{blue}Summary of the section: #1}}
\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}
\newcommand{\piotrm}[1]{\textcolor{blue}{\small [pm: #1]}}
\newcommand{\wenda}[1]{\textcolor{brown}{\small [wenda: #1]}}
\newcommand{\aj}[1]{\textcolor{red}{\small [Albert: #1]}}

\title{Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers}




\author{Albert Q. Jiang \\
  University of Cambridge\\
  \texttt{qj213@cam.ac.uk} \\
  \And
  Wenda Li \\
  University of Cambridge\\
  \texttt{wl302@cam.ac.uk} \\
  \And
  Szymon Tworkowski \\
  University of Warsaw \\
  \texttt{szy.tworkowski@gmail.com} \\
  \And
  Konrad Czechowski \\
  University of Warsaw \\
  \texttt{konrad.czechowski@gmail.com} \\
  \And
  Tomasz Odrzygóźdź \\
  IDEAS NCBR \\
  \texttt{tomaszo@impan.pl} \\
  \And
  Piotr Miłoś \\
  Polish Academy of Sciences \\
  \texttt{pmilos@mimuw.edu.pl} \\
  \And
  Yuhuai Wu \\
  Google Research \& Stanford University \\
  \texttt{yuhuai@google.com} \\
  \And
  Mateja Jamnik \\
  University of Cambridge\\
  \texttt{mateja.jamnik@cl.cam.ac.uk} \\
}



\begin{document}


\maketitle

\begin{abstract}
In theorem proving, the task of selecting useful premises from a large library to unlock the proof of a given conjecture is crucially important. This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form. This paper introduces Thor, a framework integrating language models and automated theorem provers to overcome this difficulty. In Thor, a class of methods called hammers that leverage the power of automated theorem provers are used for premise selection, while all other tasks are designated to language models. Thor increases a language model's success rate on the PISA dataset from  to , while solving  of problems neither language models nor automated theorem provers are able to solve on their own. Furthermore, with a significantly smaller computational budget, Thor can achieve a success rate on the MiniF2F dataset that is on par with the best existing methods. Thor can be instantiated for the majority of popular interactive theorem provers via a straightforward protocol we provide.

\end{abstract}




\section{Introduction}
\label{sec: intro}







In theorem proving, premise selection is the task of identifying useful facts from a large library that enable finding the proof of a given conjecture. It is essential for the discovery of many proofs, and Automated Reasoning in Large Theories~(ARLT) depends on having apt methods for premise selection~\citep{arlt1, arlt2}. A group of proof methods have been developed inside interactive theorem provers~(ITPs) to deal with this task. They use external automated theorem provers~(ATPs) to reach the remaining goals, inspect the proofs produced, and pick out the premises involved in them. Such systems are called hammers~\citep{blanchette2016hammering}. Hammers are available in many ITPs~\citep{paulson2010three, holyhammer, hol4hammer, coqhammer} and are immensely popular within the theorem proving community. 


Language models have had some successful applications in the area of formal theorem proving~\citep{polu2020generative, han2021proof, jiang2021lisa, polu2022formal}.
However, we observe that language-model-based reasoning systems are inept at premise selection.
The difficulty of premise selection for language models is that they cannot effectively reason over thousands of available facts and their definitions in plain text form. In subsection~\ref{subsec: lmtp}, we elaborate on the scale of the problems language models need to deal with for premise selection and provide empirical evidence for this difficulty.
Seeing that hammers are very good at finding relevant facts, we propose in our framework to offload the premise selection task to them from language models.
The resulting system is Thor, a framework that organically integrates language models and ATPs via the use of hammers.

The methodology of Thor is simple and can be deployed in any hammer-enabled ITP:
we first use the hammer method to attempt to prove every proof state in the training problems, and mark the successful application steps.
Then we train the language model on the training problems, predicting a special token~(e.g., \texttt{<hammer>}) if the hammer can be applied at the state.
When doing evaluation, if the language model emits the special token, we invoke the hammer method. 
This methodology incurs very little extra computation compared to standard language model training while capitalising on the potential of a hybrid neuro-symbolic model.


To demonstrate the usefulness of Thor, we instantiate it with a language-model-based reasoning system for the ITP Isabelle and Sledgehammer~\citep{paulson2010three}, Isabelle's implementation of the hammer method.
We then investigate the performance of the instantiated Thor system on two datasets, PISA~\citep{jiang2021lisa} and MiniF2F~\citep{zheng2022minif2f}. On PISA we dramatically improve the success rate of a language-model-based reasoning system from  to  and solve  of problems that cannot be solved by either language models or Sledgehammer alone. On MiniF2F, \citet{polu2022formal} used expert iteration to improve on a language model and achieved the state-of-the-art 1-pass success rate of . With much less computation, Thor increases this rate to , slightly exceeding the previous result. It is worth noting that Thor and expert iteration can be used in tandem.


In this paper, we demonstrate that finding suitable sub-systems for premise selection can benefit the performance of the overall reasoning system. 
Given Thor's strong performance, computational efficiency, and applicability to many ITPs,
we believe it should become a strong baseline as often as possible when language models are used for theorem proving. 


\paragraph{Contributions}
\begin{enumerate}
    \item We created Thor, a theorem proving framework which integrates language models and automated theorem provers via using hammers.
    \item We raised the state-of-the-art success rate of language-model-based reasoning systems on PISA from  to . Thor proved  theorems which cannot be proved by either language models or Sledgehammer.
    \item We improved the state-of-the-art success rate on MiniF2F from  to , matching the language models trained with expert iteration, but with far less computation.
\end{enumerate} \section{Background}
\label{sec: back}

\subsection{Automated and Interactive Theorem Proving}
\label{subsec: aitp}
Mechanising theorem proving has been a grand challenge of artificial intelligence since the late 1950s~\citep{gelernter1959realization}.
A group of systems which we call automated theorem provers attempt to use automated procedures to determine the validity of conjectures~(e.g., the DPLL algorithm~\citep{davis1962machine} for SAT problems~\citep{tarski1969truth}). Popular examples of ATPs include \href{https://wwwlehre.dhbw-stuttgart.de/~sschulz/E/E.html}{E}, \href{http://www.spass-prover.org}{SPASS}, \href{https://github.com/Z3Prover/z3}{Z3}, \href{https://cvc4.github.io}{CVC4}, and \href{https://vprover.github.io}{Vampire}. Although SAT is known to be NP-complete~\citep{cook1971complexity}, modern ATPs can often solve problems with millions of symbols~\citep{ohrimenko2009propagation} and are useful practically.

ATPs are often based on fragments of first-order logic, which limits the type of theorems they can express.
Hence, projects such as the formalisation of complicated mathematical results~\citep{gonthier2008formal, avigad2007formally, gonthier2013machine, scholze2021liquid} and operating system kernel verification~\citep{klein2009sel4} are done in interactive theorem provers, often based on higher-order logic or dependent type theory. 
ITPs and ATPs have very different objectives: ITPs aim at making it easy to formalise a diverse set of problems in numerous mathematical domains in a sound manner; while ATPs focus on improving the efficiency and performance on very well-defined problems like SAT solving.
Prominent ITPs include \href{https://www.cl.cam.ac.uk/research/hvg/Isabelle/}{Isabelle}, \href{http://mizar.org}{Mizar}, \href{https://www.cl.cam.ac.uk/~jrh13/hol-light/}{HOL Light}, \href{https://hol-theorem-prover.org}{HOL4}, \href{https://leanprover.github.io}{Lean}, and \href{https://coq.inria.fr}{Coq}. 
Theorem proving in ITPs can be modelled as a sequential decision process: initially a theorem gets declared and the\texttt{ proof state }contains some goals; at each step, the user produces a\texttt{ proof step }that applies to and transforms the\texttt{ proof state}; when all the goals have been discharged, the theorem is considered proven. 
Large libraries of mathematical knowledge such as the Archive of Formal Proofs\footnote{\href{https://www.isa-afp.org}{https://www.isa-afp.org}} and the Mizar Mathematical Library\footnote{\href{http://mizar.org/library/}{http://mizar.org/library/}} have been built around these ITPs.
Because of the size of these mathematical libraries, to find useful premises in them is a difficult problem. 
In the next subsections we illustrate how two different approaches deal with premise selection.


\subsection{Language Models for Theorem Proving}
\label{subsec: lmtp}
Language models that automate theorem proving mostly follow the approach of the \textit{GPT-f} model~\citep{polu2020generative}: pre-trained causal language models are used to predict a\texttt{ proof step }that can be applied, given the current\texttt{ proof state} and some optional \texttt{context}. Concretely, a language model can take as input and output, two sequences of the following form:

At test time, the reasoning system receives the text representation of the current\texttt{ proof state}, samples a\texttt{ proof step }from the language model, applies it to the ITP, and repeats until the proof is finished or a computational budget has been reached.
A best-first strategy is often used for proof search: a queue of search nodes is maintained with the priority being the accumulated log likelihood of the generated\texttt{ proof steps}.

Language models treat all input and output information as text and they are usually limited to be a few thousands of characters long. 
To do premise selection well, the language model has to either memorise all the relevant premises during training, or be prompted with available premises in evaluation. 
It is difficult to do the former because a mathematical corpus can have too many facts for a language model to remember.
For example, the Archive of Formal Proofs has more than 200,000 theorems, plus the numerous definitions and their derivations to serve as premises. 
The latter is no easier because there may be too many premises to fit into the input. 
For instance, if we use the textual representation of 300 available premises~(a usual number used for premise selection with symbolic tools) and their definitions as the\texttt{ context }in the input-output format above, the input length can well exceed 10,000 characters and the limit of standard language models.
We observe that empirically  of the steps involving premise selection generated by the language model manage to advance the proof, while the number is  for steps having no premises. Hence, a good mechanism for premise selection could bring crucial benefits.




\subsection{Hammers}
\label{subsec: hammer}

\begin{figure}[t]
\centering
\begin{subfigure}{0.4\linewidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/hammer_proof.png}
  \caption{The proof sketch produced by the human user. The\texttt{ sledgehammer }command indicates that the human invokes the Sledgehammer method at that point.}
  \label{fig: hammer proof}
\end{subfigure}
\hfill
\begin{subfigure}{0.55\linewidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/normal_proof.png}
  \caption{The proof accepted by Isabelle. The steps containing \texttt{assume, obtain, have, show }are from the original human proof sketch. The steps containing \texttt{ metis, smt, fastforce, blast, auto, fastforce} are completed by Sledgehammer.}
  \label{fig: orig proof}
\end{subfigure}
\caption{A proof of , adapted from the original by~\citet{li2021isarstep} with consent.}
\label{fig: whole proof}
\end{figure}

\citet{blanchette2016hammering} define hammers as methods that ``automate reasoning over large libraries developed with formal proof assistants~(ITPs)''. Consider, for example, Sledgehammer (designed for Isabelle) which is the original and the most popular implementation of hammers.
Figure~\ref{fig: whole proof} presents a proof of  in Isabelle. The beauty of using Sledgehammer with Isabelle is that despite the complicated-looking proof, humans only need to sketch the proof in Figure~\ref{fig: hammer proof} and let Sledgehammer find all the necessary premises to complete every single proof step. The final accepted proof is presented in Figure~\ref{fig: orig proof}. The Sledgehammer proof steps use the internal proof methods\texttt{ metis, meson, smt, auto, simp, fastforce }and\texttt{ blast}. Conveniently, this tells us which steps in the corpus are generated by Sledgehammer. 
Note that a human user might also use the proof methods\texttt{ auto, simp, fastforce }and\texttt{ blast }as these do not contain additional premises. Only the methods\texttt{ metis, meson, smt} are exclusive to Sledgehammer.


We now describe how Sledgehammer performs premise selection: Sledgehammer makes it possible to leverage the advancement of ATP research while using ITPs, and can thus be seen as a bridge between the two~\citep{paulson2010three}. When invoked, Sledgehammer translates the current goal together with hundreds of possibly relevant premises into a format (e.g., SMT-LIB, TPTP) that external ATPs can understand~\citep{meng2008translating}. The ATPs are then executed to solve the current goal. Note that Isabelle follows a kernel philosophy (i.e., only a handful of axioms and inference rules are trusted), and external ATPs are used skeptically---should a proof be found by the ATPs, Sledgehammer picks out the useful premises, and reconstructs the proof within the Isabelle kernel (e.g., using the primitive inference rules). 
Here, external ATPs serve as relevance filters of premises rather than trusted oracles. Hammers implemented in other ITPs are largely similar.







 \section{Thor}
\label{sec: thor}
In this section we introduce Thor, a framework integrating language models and automated theorem provers via the use of hammers. Thor is motivated by the difficulty for language models to do premise selection and the excellent performance of hammers for it: we should be able to drastically improve automation in theorem proving if we can take the best from both worlds. 

Below we provide the protocol of adopting Thor for a hammer-enabled ITP. We first provide Thor's training data preprocessing procedure in Algorithm~\ref{alg: thor}, and then look at a concrete example to demonstrate its use.
\begin{algorithm}
\caption{Thor's training data preprocessing algorithm.}
\label{alg: thor}
\begin{algorithmic}
\Require Proof state \texttt{s}, hammer method \texttt{h}
\State \texttt{INPUT} = \texttt{s.input}
\If{\texttt{h(s)}} \Comment{Hammer can be applied to the proof state}
    \State \texttt{OUTPUT = <hammer> <EOS>}
\Else \Comment{Hammer fails at the proof state}
    \State \texttt{OUTPUT = s.output}
\EndIf
\State \Return \texttt{(INPUT, OUTPUT)}
\end{algorithmic}
\end{algorithm}

Now consider the situation in the proof of ~(Figure~\ref{fig: whole proof}) after the step\texttt{ then have "even a"}: without Thor, it should produce the following datapoint

With Thor's preprocessing, we apply the hammer method to the proof state and find out that it can be done successfully. Hence, we keep the input the same and change the output to:
\begin{center}
    \texttt{OUTPUT: \quad} \texttt{<hammer> <EOS>}
\end{center}
If the hammer method cannot be applied, we leave the datapoint unchanged. We iterate over every datapoint in the training data and apply this preprocessing algorithm.

We hypothesise that being exposed to training data in this format, the language model is capable of learning a heuristic for \emph{when} the hammer can be successfully invoked.
At evaluation time, whenever the language model outputs the sequence\texttt{ <hammer> <EOS>}, instead of applying it directly to the ITP, we call the hammer method. This effectively makes the hammer an invokable method for the language model.
This protocol is straightforward to implement for hammer-enabled ITPs.


The only extra cost of deploying Thor is in the data preprocessing step. 
Multiplying the hammer time limit by the average number of problems submitted to the Archive of Formal Proofs in one year, we estimate that  CPU hours per year are needed to preprocess one of the largest proof corpora available. This is a modest cost since the process only needs to be done once per dataset and the results can be shared.
Better still, for some ITPs, the hammer method leaves a trace, greatly reducing the time needed to figure out which steps can be solved by hammers. For the ITP Coq, all steps containing the keyword\texttt{ sauto }are generated by CoqHammer~\citep{coqhammer}. For Isabelle, all steps containing the keywords\texttt{ metis, meson, smt }are generated by Sledgehammer~(described in Section~\ref{subsec: hammer}). With these traces, deploying Thor on ITPs like Coq or Isabelle incurs little extra computational cost compared to training a standard language model. 





 \section{Experiment}
\label{sec: exp}

Our experiments are intended to answer the following research questions:
\begin{enumerate}
    \item Can Thor prove theorems that cannot be proved by language models or automated theorem provers individually? Does Thor improve premise selection for language models?
\item Does explicitly learning \emph{how} to select premises hurt the performance of language models?
    \item How important are the context information and the diversity of sequence generation?
    \item How does Thor compare with other methods at improving language models for theorem proving?
\end{enumerate}
To answer these questions, we create an instance of Thor for the ITP Isabelle. We choose Isabelle for two reasons: (1) Isabelle's Sledgehammer is one of the most mature hammer methods among major ITPs, and may thus showcase Thor's full potential; and (2) Isabelle's Archive of Formal Proofs is one of the world's largest formal mathematical libraries, suitable for data-hungry methods like language models. We make explicit the details of our experimental setup next.

\subsection{Experimental Setup}
\label{subsec: thor/isa}
\paragraph{Machine specification}
For pre-training, fine-tuning, and evaluation, we use a TPUVM with 8 cores from \href{https://cloud.google.com/tpu?hl=en}{Google Cloud Platform}.
The Isabelle process has access to up to 32 CPU cores. We estimate that reproducing all the experiments in this paper requires a total of  TPU hours.

\paragraph{Language model architecture}
We use a decoder-only transformer~\citep{vaswani2017attention} language model, adapting the setup, codebase, and hyperparameters from~\citep{gpt-j}.
The language model has M non-embedding parameters, with  layers,  attention heads, a hidden dimension of , and a GPT-2~\citep{radford2019language} tokenizer with a vocabulary size of . Rotary positional embeddings~\citep{rotary} are used.
The model is pre-trained on the GitHub + arXiv subsets of The Pile~\citep{thepile}, with a context length of 2048. We use a global batch size of  sequences which amounts to  tokens. For the first 3,000 steps, the learning rate linearly increases from  to , and then it follows a cosine schedule with a final value of  for 197,000 steps. We use a weight decay rate of  and no dropout for pre-training. Pre-training takes  TPU hours.
For fine-tuning, we use the procedure described in Section \ref{sec: thor} to prepare the PISA dataset. We use the most recent\texttt{ proof step }as the\texttt{ context }in each datapoint.
The same learning rate scheduling strategy is used, with a peak learning rate of  after 10,000 steps and a final learning rate of  after a further 90,000 steps. We use a dropout rate of  and a weight decay rate of . The global batch size is  sequences, or  tokens. We early-stop fine-tuning and take the checkpoint at 11,000 steps for evaluation as the validation loss reaches a minimum then. Fine-tuning takes  TPU hours.


\paragraph{Sledgehammer configuration}
To set up Sledgehammer, we mostly follow the default Isabelle2021 configuration. An important default parameter is that the Sledgehammer timeout limit is s. Our configuration uses the on-machine versions of the five default ATPs~(E, SPASS, Vampire, Z3, and CVC4) to prevent performance deviation caused by network issues.

\paragraph{Proof search}
To sample from the language model, we use temperature sampling with the temperature parameter .
To search for the proof of a theorem, we use the best-first search strategy described in \citep{polu2020generative}.
The queue is ordered by the accumulated log likelihoods of the generated\texttt{ proof steps}, with a maximum length of 32.
Each\texttt{ proof step }has a timeout limit of s.
The search is terminated if and only if one of the following scenarios happens: (1) a valid proof has been found for the theorem; (2) the language model is queried 300 times; (3) a wallclock timeout of s has been reached; (4) the queue is empty but the theorem is not proved. Empirically, it takes  TPU hours to evaluate  problems.

Our language model setup is different from Language models of ISAbelle proofs~\citep[LISA]{jiang2021lisa} in three aspects: (1) our language model has 700M instead of 163M non-embedding parameters (2) the most recent\texttt{ proof step }is included in the language model prompt (3) a higher sampling temperature~( instead of ) is used.

\subsection{Datasets and Environment}
\label{subsec: data}
We use two datasets. 
The first is the PISA dataset~\citep{jiang2021lisa}, which includes the Isabelle/HOL repository\footnote{\href{https://isabelle.in.tum.de/website-Isabelle2021/dist/library/HOL/index.html}{https://isabelle.in.tum.de/website-Isabelle2021/dist/library/HOL/index.html}} under a \href{https://www.cl.cam.ac.uk/research/hvg/Isabelle/dist/Isabelle2021-1/COPYRIGHT}{BSD-style license} and the Archive of Formal Proofs version 2021-10-22\footnote{\href{https://www.isa-afp.org/release/afp-2021-10-22.tar.gz}{https://www.isa-afp.org/release/afp-2021-10-22.tar.gz}}, whose various entries are under open-source licenses as described on its \href{https://www.isa-afp.org/about.html}{official page}.
PISA contains the core higher-order logic library of Isabelle, as well as a diverse library of proofs formalised with Isabelle, mostly concerning mathematics or verification of software and hardware.
The PISA dataset contains  million datapoints in total. The\texttt{ proof states }have an average length of  characters and the\texttt{ proof steps }have an average length of  characters.
All of the Isabelle/HOL theorems go into the training set as they are considered foundational and might be used by all other repositories. We make a  split of theorems from the AFP for the training/validation/test sets. We randomly select 3,000 theorems from the test set~(\emph{PISA/test}) for the evaluation of model performance.

The second is the Isabelle fraction of the MiniF2F dataset~\citep{zheng2022minif2f} under an \href{https://github.com/openai/miniF2F/blob/main/isabelle/LICENSE}{Apache license}. The dataset contains 488 high school mathematics competition problems split into a validation set and a test set, each with 244 problems.
These problems have been formalised in Lean, Metamath, and Isabelle to provide a benchmark of the same problems in different ITP languages. This allows us to contrast different approaches developed for different ITPs. Since we do not use the validation set for model selection, we do not actually distinguish between the two sets. Hence, we mainly compare with previous work on the test set as the final result.

We use the codebase by \citet{jiang2021lisa}, under a \href{https://github.com/albertqjiang/Portal-to-ISAbelle/blob/main/LICENSE}{BSD 3-clause license}, to interact with the Isabelle server and prove theorems from both datasets.

\subsection{Thor Against an Ensemble of a Language Model and Sledgehammer}

\begin{table}[t]
    \begin{minipage}{\linewidth}
      \centering
        \caption{Proof success rates on \emph{PISA/test}}
        \label{tab: pisa rate}
        \centering
        \begin{tabular}{lcc}
            \toprule
            Method   & Success rate (\%)\\ 
            \midrule
            LISA~\citep{jiang2021lisa} &  \\
            \midrule
            Sledgehammer &  \\
            Language model &   \\
            Language model  Sledgehammer & \\
            \midrule
            Thor & \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}

Because Thor has both a language model and Sledgehammer at its disposal, we wish to investigate how it fares against a simple ensemble of the two. We set out to evaluate the performance of Thor, as well as a language model of the same configuration, and Sledgehammer with a 120s timeout on \emph{PISA/test}.
It takes  TPU hours to evaluate Thor for  problems.
The proof success rates on \emph{PISA/test} are presented in the second column of Table~\ref{tab: pisa rate}.
We can see that the language model alone and Sledgehammer alone can prove  and  of the problems respectively. When we take the union of the problems they manage to solve individually, we get a  success rate. Thor manages to prove  of the problems. This implies that for  of the problems, Thor uses both the language model and Sledgehammer to complete the proofs, and it's not possible to achieve this with only the language model or only Sledgehammer. We perform 4 case studies on problems that only Thor can solve in Appendix~\ref{sec: appendix}.

Thor's motivation is to solve the premise selection problem for language models. To confirm that Thor helps premise selection, we collect the proofs generated by the language model and Thor respectively and count the number of premises in them. The results are presented in Figure~\ref{fig: success premise}: we can see that for proofs requiring  or  premises, Thor and the language model perform similarly. But for proofs requiring more premises, Thor performs much more robustly, finding several times more proofs than the language model. We also count the number of premises in the ground truth proofs~(written by humans) for theorems the language model and Thor can prove. The results are presented in Figure~\ref{fig: ground truth premise}: we see that whatever the number of premises the ground truth uses, Thor outperforms the language model in finding proofs, and the more premises the ground truth proof has, the more obvious is the effect. We conclude that Thor is indeed more capable of premise selection than language models.
\begin{figure}[t]
\begin{subfigure}{0.47\linewidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/premise_count_comparison.pdf}
    \caption{The number of premises in successful proofs found by the language model and Thor.}
    \label{fig: success premise}
\end{subfigure}
\hfill
\begin{subfigure}{0.47\linewidth}
  \centering
  \includegraphics[width=\linewidth]{imgs/ground_truth_premise_count_comparison.pdf}
    \caption{The number of premises in ground truth proofs for problems solved by the language model and Thor.}
    \label{fig: ground truth premise}
\end{subfigure}
\caption{Comparison of the number of premises in problems the language model and Thor can solve.}
\label{fig: premise}
\end{figure}



\subsection{The Effect of Learning How to Select Premises}
The procedure we described in Section~\ref{sec: thor} ensures that the language model learns \emph{when} to do premise selection, but not \emph{how} to do it, by replacing the premise selection steps with\texttt{ <hammer>}.
Here we investigate the effect of making the language model learn both \emph{when} and \emph{how}.
An easy way to achieve this is to create a variant of Thor: (i) at training time, use the original data; (ii) at evaluation time, when the language model outputs a sequence containing any of the Sledgehammer keywords, invoke Sledgehammer. This further simplifies data preparation and explicitly subjects the language model to perform premise selection.
To investigate the effect of this alternative approach, we evaluate a system trained in this way on \emph{PISA/test} and present its success rate in Table~\ref{tab: thor/isa variant}.
\begin{table}[t]
    \begin{minipage}{\linewidth}
      \centering
        \caption{Proof success rates on \emph{PISA/test}}
        \label{tab: thor/isa variant}
        \centering
        \begin{tabular}{lcc}
            \toprule
            Variants of Thor   & Success rate (\%)\\ 
            \midrule
            Base, sampling temperature  &  \\
            \midrule
            Learning \emph{how} to select premises &  \\
            No proof context &  \\
            Sampling temperature  &  \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}
We can see that it achieves a success rate of  on \emph{PISA/test},  lower than the base version of Thor, which suggests that explicitly learning \emph{how} to do premise selection marginally decreases its success rate. This result is expected: since finding \emph{how} to do premise selection is entrusted to the hammer method, the language model should focus on learning \emph{when} to invoke the hammer for optimal performance. Making the language model learn an irrelevant additional task only hurts Thor's performance.




\subsection{The Effect of the Proof Context}
Our language model setup differs from that of LISA~\citep{jiang2021lisa} in that we use the most recent\texttt{ proof step }as the\texttt{ context} in the input data, as introduced in Section~\ref{sec: thor}. This is based on the intuition that the most recent\texttt{ proof step }information is beneficial for the language model's reasoning ability.
In this subsection we perform an ablation study to confirm the effect of this\texttt{ context }on Thor.
Here a variant of Thor is trained without the\texttt{ context }information and evaluated on \emph{PISA/test}. The results are in Table~\ref{tab: thor/isa variant}.
We observe that this variant manages to prove  of theorems on \emph{PISA/test},  fewer than the base version of Thor.
The drop in success rate indicates that the\texttt{ context }information we use is crucial for the optimal performance of Thor.

\subsection{The Effect of the Sequence Sampling Diversity}
Our language model setup differs from LISA~\citep{jiang2021lisa} also in the sampling temperature.
Previous works on language models for theorem proving often use a temperature ~\citep{polu2020generative, jiang2021lisa} for sampling output sequences, while we use .
A higher temperature in the sampling procedure means that the generated sequences are more diverse~(having a higher entropy).
Here we perform an ablation study on the diversity of Thor-generated sequences.
We evaluate Thor with sampling temperature  on \emph{PISA/test} and the success rate is in Table~\ref{tab: thor/isa variant}.
We can see that the success rate with sampling temperature  is ,  lower than with .
This suggests a more diverse sampling strategy can improve Thor's performance, and that the optimal diversity in language model samples varies for different systems.


\subsection{Comparing Thor with Expert Iteration}
\begin{table}[t]
    \begin{minipage}{\linewidth}
      \centering
        \caption{Proof success rates on \emph{MiniF2F}.}
        \label{tab: mini rate}
        \centering
        \begin{tabular}{lcc}
            \toprule
            Method   & Valid~(\%) & Test~(\%)\\ 
            \midrule
            PACT~\citep{han2021proof} &  & \\
            Expert iteration~\citep{polu2022formal} &  &  \\
            \midrule
            Sledgehammer &  & \\
            Language model &  &  \\
            Language model  Sledgehammer &  &  \\
            \midrule
            Thor &  &  \\
            \bottomrule
        \end{tabular}
    \end{minipage}\end{table}

There exist other methods for improving language models for theorem proving like value function training~\citep{polu2020generative}, proof artifact co-training~\citep[PACT]{han2021proof}, and expert iteration~\citep{polu2022formal}. We wish to compare Thor with them. However, these methods operate in ITPs other than Isabelle and are thus hard to compare with directly. Thankfully, \citet{polu2022formal} used expert iteration~\citep{silver2017mastering} to improve PACT~\citep{han2021proof} and to achieve the state-of-the-art result on MiniF2F, a dataset containing multiple ITP formalisations of the same problems. Hence, we can fairly contrast expert iteration with Thor. We should emphasise that Thor and expert iteration are not incompatible methods: one can use Thor \emph{together with} expert iteration.

We start by evaluating Thor, a language model with the same configuration, and Sledgehammer on MiniF2F. The results are presented in Table~\ref{tab: mini rate}. We also include the success rates of the language model that \citet{polu2022formal} used~(PACT), as well as the language model after expert iteration in the same table. The success rates on the validation set are also included, but we use the rates on the test set as the final results, as the valid set can be used for model selection. We can see that the language model is able to prove  of the problems on MiniF2F, similar to PACT's . Thor increases the success rate of the language model by  to , while expert iteration increases the success rate of PACT by  to . Hence, the improvement in proof success rate brought upon the language model by Thor is comparable to that by expert iteration. 





An important factor in choosing a suitable method is its cost.
Expert iteration requires manually creating a set of ``curriculum'' problems, evaluating the language model on them, and training the language model on a growing training set for one epoch every iteration.
We estimate that to perform expert iteration at the same scale as \citet{polu2022formal} for Isabelle, it would cost  human hours to formalise  maths problems, and  TPU hours to evaluate and fine-tune the language model for  expert iterations.
Thor, on the other hand, incurs little extra computational cost compared with training a standard language model.
We conclude that while requiring a much smaller computational budget, Thor can improve language models' success rates to a similar degree as expert iteration.




 \section{Related Work}
\label{sec: related}






Language models were first applied to automate theorem proving by~\citet{polu2020generative}. 
Since then, there have been a few works~\citep{han2021proof, jiang2021lisa, polu2022formal} aiming to enhance the ability of language-model-based reasoning systems, or to enable these systems for interactive theorem provers that were not supported before. All of these works used the same framework laid down by~\citet{polu2020generative}, namely to iteratively sample from a language model and directly apply the output to the ITP. Thor, to the best of our knowledge, is the first system to explicitly hybridise language models and symbolic reasoning tools~(ATPs) for theorem proving.
Instead of relying on language models entirely, Thor uses hammers, a well-established tool, to solve premise selection.

With the growing bodies of formal mathematical libraries, premise selection has become one of the most crucial tasks of theorem proving.
The hammer method is one of the many ways that premise selection can be done.
We have described how the Isabelle implementation of the hammer method selects premises in Section~\ref{sec: back}.
HOL(y)Hammer~\citep{holyhammer} and CoqHammer~\citep{coqhammer} implement the hammer method for HOL Light and Coq respectively, making it possible for Thor to be instantiated for them.
Apart from hammers, SInE~\citep{hoder2011sine} and SRASS~\citep{sutcliffe2007srass} are both symbolic methods that take on the task of premise selection by ranking the available premises according to their relevance to the current conjecture, measured by syntactic and semantic distances respectively.
MaLARea~\citep{urban2007MaLAReaAM} pioneered having machine learning components in premise selection systems and its later version MaLARea SG1~\citep{urban2008sg1} combines machine learning and formal semantics for premise selection.
A few approaches~\citep{irving2016deepmath, wang2017premise, kaliszyk2017holstep} use deep learning in the premise selection task.
All these diverse methods may have quantitative or qualitative merits over the hammer approach, and thus have the potential to be integrated as the premise selection component for future versions of Thor.









 \section{Discussion}
\label{sec: discuss}


In this paper we introduced a simple approach to overcome language models' weakness in premise selection for theorem proving: we created Thor, a framework that integrates language models and automated theorem provers via the hammer proof method. We presented a straightforward protocol for deploying Thor on any hammer-enabled ITP. The instance of Thor with Isabelle dramatically increased the number of automatically proved theorems, suggesting that language models' deficiency at premise selection can be effectively compensated by utilising ATPs.
Furthermore, approaches like expert iteration~\citep{polu2022formal} or proof artifact co-training~\citep{han2021proof} have no contradictions and can be easily incorporated with Thor. Compared with these methods, Thor has the additional advantage of being computationally efficient.

One limitation of Thor is that it only admits automated theorem provers that directly generate valid proof steps in the ITP via the use of the hammer. In Section~\ref{sec: related}, we pointed out that there are other premise selection tools with approaches different from the hammer method that the current version of Thor cannot use. Also, there exist methods which assist premise selection but do not directly generate the proof steps. An example of this is SErAPIS~\citep{stathopoulos2020serapis}, which performs semantic search over the Isabelle mathematical library with the help of Wikipedia. Thor cannot use this class of methods either.
We leave to future work the task of broadening the options for the premise selection tool that Thor uses.
Here we only tested Thor on the ITP Isabelle due to the computational costs of experiments. Therefore another future direction is to instantiate Thor with other ITPs and see whether improvements brought by Thor are as significant for other ITPs as we show them here for Isabelle.

Thor demonstrates how a difficult problem for language models can be solved by borrowing tools from another research domain.
We are encouraged by its success and think that more problems like premise selection can be identified and solved similarly.
With its strong performance, computational efficiency, and convenient deployment, Thor gives scope to tool hybridisation, which shows promise to be impactful in the field of automated reasoning, and artificial intelligence in general.







 

\bibliographystyle{plainnat}
\bibliography{main}

























\newpage




\appendix


\section{Appendix}
\label{sec: appendix}



In this section, we present some lemmas solved by Thor only.


{\bf Case 1.} The lemma \isa{cols\_upt\_k
\_insert} is from the \emph{QR Decomposition} entry\footnote{\url{QR_Decomposition/Gram_Schmidt.thy}} in the AFP.
\begin{mdframed}
\begin{isabelle}
\isacommand{lemma}\isamarkupfalse \ cols{\isacharunderscore}{\kern0pt}upt{\isacharunderscore}{\kern0pt}k{\isacharunderscore}{\kern0pt}insert{\isacharcolon}{\kern0pt}\isanewline
\ \ \isakeyword{fixes}\ A{\isacharcolon}{\kern0pt}{\isacharcolon}{\kern0pt}{\isachardoublequoteopen}{\isacharprime}{\kern0pt}a{\isacharcircum}{\kern0pt}{\isacharprime}{\kern0pt}n{\isacharcolon}{\kern0pt}{\isacharcolon}{\kern0pt}{\isacharbraceleft}{\kern0pt}mod{\isacharunderscore}{\kern0pt}type{\isacharbraceright}{\kern0pt}{\isacharcircum}{\kern0pt}{\isacharprime}{\kern0pt}m{\isacharcolon}{\kern0pt}{\isacharcolon}{\kern0pt}{\isacharbraceleft}{\kern0pt}mod{\isacharunderscore}{\kern0pt}type{\isacharbraceright}{\kern0pt}{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{assumes}\ k{\isacharcolon}{\kern0pt}\ {\isachardoublequoteopen}{\isacharparenleft}{\kern0pt}Suc\ k{\isacharparenright}{\kern0pt}{\isacharless}{\kern0pt}ncols\ A{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{shows}\ {\isachardoublequoteopen}cols{\isacharunderscore}{\kern0pt}upt{\isacharunderscore}{\kern0pt}k\ A\ {\isacharparenleft}{\kern0pt}Suc\ k{\isacharparenright}{\kern0pt}\ {\isacharequal}{\kern0pt}\ {\isacharparenleft}{\kern0pt}insert\ {\isacharparenleft}{\kern0pt}column\ \isanewline
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ {\isacharparenleft}{\kern0pt}from{\isacharunderscore}{\kern0pt}nat\ {\isacharparenleft}{\kern0pt}Suc\ k{\isacharparenright}{\kern0pt}{\isacharparenright}{\kern0pt}\ A{\isacharparenright}{\kern0pt}\ {\isacharparenleft}{\kern0pt}cols{\isacharunderscore}{\kern0pt}upt{\isacharunderscore}{\kern0pt}k\ A\ k{\isacharparenright}{\kern0pt}{\isacharparenright}{\kern0pt}{\isachardoublequoteclose}\isanewline
\ \ \isacommand{unfolding}\isamarkupfalse \ cols{\isacharunderscore}{\kern0pt}upt{\isacharunderscore}{\kern0pt}k{\isacharunderscore}{\kern0pt}def\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}auto{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}metis\ Suc{\isacharunderscore}{\kern0pt}lessD\ from{\isacharunderscore}{\kern0pt}nat{\isacharunderscore}{\kern0pt}mono{\isacharprime}{\kern0pt}\ from{\isacharunderscore}{\kern0pt}nat{\isacharunderscore}{\kern0pt}to{\isacharunderscore}{\kern0pt}nat{\isacharunderscore}{\kern0pt}id\ k\ \isanewline
\ \ \ \ \ \ \ \ less{\isacharunderscore}{\kern0pt}Suc{\isacharunderscore}{\kern0pt}eq{\isacharunderscore}{\kern0pt}le\ less{\isacharunderscore}{\kern0pt}le\ ncols{\isacharunderscore}{\kern0pt}def\ to{\isacharunderscore}{\kern0pt}nat{\isacharunderscore}{\kern0pt}le{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{by}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}metis\ from{\isacharunderscore}{\kern0pt}nat{\isacharunderscore}{\kern0pt}mono{\isacharprime}{\kern0pt}\ k\ less{\isacharunderscore}{\kern0pt}imp{\isacharunderscore}{\kern0pt}triv\ \isanewline
\ \ \ \ \ \ \ \ less{\isacharunderscore}{\kern0pt}or{\isacharunderscore}{\kern0pt}eq{\isacharunderscore}{\kern0pt}imp{\isacharunderscore}{\kern0pt}le\ ncols{\isacharunderscore}{\kern0pt}def\ not{\isacharunderscore}{\kern0pt}less{\isacharunderscore}{\kern0pt}eq\ order{\isacharunderscore}{\kern0pt}trans{\isacharparenright}{\kern0pt}\end{isabelle}
\end{mdframed}
Here, \isa{cols\_upt\_k A (Suc k)} returns the set of columns in the matrix \isa{A} up to the natural number \isa{k+1}, while \isa{ncols A} counts the number of columns in the matrix \isa{A}. In short, this lemma claims that the set of columns (in a matrix ) up to column index  is equivalent to that of the same matrix up to column index  inserted with the \textsuperscript{th} column (of ). This will subject to the condition that  is less than the number of columns in . With Thor, the LM decided to unfold the goal with the definition of \isa{cols\_upt\_k}, which is followed by an \isa{auto} tactic to simplify the proof state. All remaining subgoals are then discharged by Sledgehammer.

{\bf Case 2.} The lemma \isa{size\_del\_max} is from the\emph{Weight-Balanced Trees} entry\footnote{\url{Weight_Balanced_Trees/Weight_Balanced_Trees.thy}} in the AFP.
\begin{mdframed}
\begin{isabelle}
\isacommand{lemma}\isamarkupfalse \ size{\isacharunderscore}{\kern0pt}del{\isacharunderscore}{\kern0pt}max{\isacharcolon}{\kern0pt}\ {\isachardoublequoteopen}t\ {\isasymnoteq}\ Leaf\ {\isasymLongrightarrow}\ size\ t\ {\isacharequal}{\kern0pt}\ Suc{\isacharparenleft}{\kern0pt}size{\isacharparenleft}{\kern0pt}snd{\isacharparenleft}{\kern0pt}del{\isacharunderscore}{\kern0pt}max\ t{\isacharparenright}{\kern0pt}{\isacharparenright}{\kern0pt}{\isacharparenright}{\kern0pt}{\isachardoublequoteclose}\isanewline
\ \ \isacommand{apply}\isamarkupfalse {\isacharparenleft}{\kern0pt}induction\ t\ rule{\isacharcolon}{\kern0pt}\ del{\isacharunderscore}{\kern0pt}max{\isachardot}{\kern0pt}induct{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ simp\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}clarsimp\ split{\isacharcolon}{\kern0pt}\ prod{\isachardot}{\kern0pt}splits{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}smt\ {\isacharparenleft}{\kern0pt}z{\isadigit{3}}{\isacharparenright}{\kern0pt}\ size{\isacharunderscore}{\kern0pt}rotateR\ size{\isacharunderscore}{\kern0pt}wbt{\isachardot}{\kern0pt}simps{\isacharparenleft}{\kern0pt}{\isadigit{1}}{\isacharparenright}{\kern0pt}{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{by}\isamarkupfalse \ simp\end{isabelle}
\end{mdframed}
In this lemma, \isa{t} is a weight-balanced tree, and the \isa{size} function measures its size (as the name suggests) and \isa{del\_max} deletes the maximum node from it. Essentially, this lemma claims that when a weight-balanced its size will be reduced by one if we remove the largest node from it. For the proof, Thor intelligently performs structural induction with the induction rule \isa{del\_max.induct} and then simplifies the proof state a few times, which includes splitting products with the rule \isa{prod.splits}. Finally, Thor concludes the remaining goals with Sledgehammer.

{\bf Case 3.} The lemma \isa{t\_list\_of\_B\_log\_bound} is from the AFP entry named as \emph{Priority Queues Based on Braun Trees}.\footnote{\url{Priority_Queue_Braun/Sorting_Braun.thy}}
\begin{mdframed}
\begin{isabelle}
\isacommand{lemma}\isamarkupfalse \ t{\isacharunderscore}{\kern0pt}list{\isacharunderscore}{\kern0pt}of{\isacharunderscore}{\kern0pt}B{\isacharunderscore}{\kern0pt}log{\isacharunderscore}{\kern0pt}bound{\isacharcolon}{\kern0pt}\isanewline
\ \ {\isachardoublequoteopen}braun\ t\ {\isasymLongrightarrow}\ t{\isacharunderscore}{\kern0pt}list{\isacharunderscore}{\kern0pt}of{\isacharunderscore}{\kern0pt}B\ t\ {\isasymle}\ {\isadigit{3}}\ {\isacharasterisk}{\kern0pt}\ {\isacharparenleft}{\kern0pt}nlog{\isadigit{2}}\ {\isacharparenleft}{\kern0pt}size\ t\ {\isacharplus}{\kern0pt}\ {\isadigit{1}}{\isacharparenright}{\kern0pt}\ {\isacharplus}{\kern0pt}\ {\isadigit{1}}{\isacharparenright}{\kern0pt}\ {\isacharasterisk}{\kern0pt}\ size\ t{\isachardoublequoteclose}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}induction\ t\ rule{\isacharcolon}{\kern0pt}\ measure{\isacharunderscore}{\kern0pt}induct{\isacharunderscore}{\kern0pt}rule{\isacharbrackleft}{\kern0pt}\isakeyword{where}\ f{\isacharequal}{\kern0pt}size{\isacharbrackright}{\kern0pt}{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}case{\isacharunderscore}{\kern0pt}tac\ x{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ simp\isanewline
\ \ \isacommand{using}\isamarkupfalse \ braun{\isachardot}{\kern0pt}simps{\isacharparenleft}{\kern0pt}{\isadigit{1}}{\isacharparenright}{\kern0pt}\ t{\isacharunderscore}{\kern0pt}list{\isacharunderscore}{\kern0pt}of{\isacharunderscore}{\kern0pt}B{\isacharunderscore}{\kern0pt}braun{\isacharunderscore}{\kern0pt}simps{\isacharparenleft}{\kern0pt}{\isadigit{1}}{\isacharparenright}{\kern0pt}\ \isacommand{apply}\isamarkupfalse \ blast\isanewline
\ \ \isacommand{by}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}metis\ acomplete{\isacharunderscore}{\kern0pt}if{\isacharunderscore}{\kern0pt}braun\ height{\isacharunderscore}{\kern0pt}acomplete\ order{\isacharunderscore}{\kern0pt}refl\ \isanewline
\ \ \ \ \ \ size{\isadigit{1}}{\isacharunderscore}{\kern0pt}size\ t{\isacharunderscore}{\kern0pt}list{\isacharunderscore}{\kern0pt}of{\isacharunderscore}{\kern0pt}B{\isacharunderscore}{\kern0pt}induct{\isacharparenright}{\kern0pt}\end{isabelle}
\end{mdframed}
Here, \isa{size} measures the size of a Braun tree; \isa{nlog2} stands for the function ; \isa{t\_list\_of\_B} is another measure of a Braun tree. Basically, this lemma describes the relationship between a normal tree size and a Braun-tree specific measure. The proof starts with an intelligent structural induction, progresses with case analysis, and is concluded with Sledgehammer on each of the remaining subgoals. 

{\bf Case 4.} The lemma \isa{inj\_imp\_Ker0} is from the AFP entry named as \emph{Matrices, Jordan Normal Forms, and Spectral Radius Theory}.\footnote{\url{Jordan_Normal_Form/Missing_VectorSpace.thy}}
\begin{mdframed}
\begin{isabelle}
\isacommand{lemma}\isamarkupfalse \ inj{\isacharunderscore}{\kern0pt}imp{\isacharunderscore}{\kern0pt}Ker{\isadigit{0}}{\isacharcolon}{\kern0pt}\isanewline
\ \ \isakeyword{assumes}\ {\isachardoublequoteopen}inj{\isacharunderscore}{\kern0pt}on\ T\ {\isacharparenleft}{\kern0pt}carrier\ V{\isacharparenright}{\kern0pt}{\isachardoublequoteclose}\isanewline
\ \ \isakeyword{shows}\ {\isachardoublequoteopen}carrier\ {\isacharparenleft}{\kern0pt}V{\isachardot}{\kern0pt}vs\ kerT{\isacharparenright}{\kern0pt}\ {\isacharequal}{\kern0pt}\ {\isacharbraceleft}{\kern0pt}{\isasymzero}\isactrlbsub V\isactrlesub {\isacharbraceright}{\kern0pt}{\isachardoublequoteclose}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}rule\ equalityI{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}rule\ subsetI{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{apply}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}unfold\ ker{\isacharunderscore}{\kern0pt}def{\isacharcomma}{\kern0pt}\ auto{\isacharparenright}{\kern0pt}\isanewline
\ \ \isacommand{by}\isamarkupfalse \ {\isacharparenleft}{\kern0pt}metis\ V{\isachardot}{\kern0pt}module{\isachardot}{\kern0pt}M{\isachardot}{\kern0pt}zero{\isacharunderscore}{\kern0pt}closed\ assms\ f{\isadigit{0}}{\isacharunderscore}{\kern0pt}is{\isacharunderscore}{\kern0pt}{\isadigit{0}}\ inj{\isacharunderscore}{\kern0pt}on{\isacharunderscore}{\kern0pt}contraD{\isacharparenright}{\kern0pt}\end{isabelle}
\end{mdframed}
Here, \isa{T} is a linear map between two vector spaces. The lemma claims that if the \isa{T} is injective on the carrier set of the space \isa{V}, the kernel of \isa{T} has to be a singleton set with the zero in \isa{V}. In this proof, Thor naturally performs a sequence of introduction steps by applying the lemma \isa{equalityI} and \isa{subsetI}, before unfolds the definition of a kernel (i.e., \isa{ker\_def}) and uses \isa{auto} to simplify the proof state. The final remaining goal is closed with Sledgehammer. 















\end{document}
