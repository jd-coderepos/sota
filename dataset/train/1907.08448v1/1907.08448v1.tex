








\documentclass[journal]{IEEEtran}























\ifCLASSINFOpdf
\else
\fi



















































\hyphenation{op-tical net-works semi-conduc-tor}


\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{tikz}
\usetikzlibrary{spy}

\usepackage{algorithm, algorithmic}
\usepackage{graphicx} \usepackage{amsmath} \usepackage{amssymb}  \usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{color}
\usepackage{bm}
\usepackage{hyperref}
\urlstyle{tt}

\usepackage{times,amsmath,epsfig}
\usepackage{cite}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{hhline}
\usepackage{mathtools}
\usepackage{multirow}

\newcommand{\nub}{\bm{\nu}}
\newcommand{\mub}{\bm{\mu}}
\newcommand{\Lambdab}{\bm{\Lambda}}

\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Lb}{\mathbf{L}}
\newcommand{\Ub}{\mathbf{U}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Wb}{\mathbf{W}}

\newcommand{\nb}{\mathbf{n}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\zb}{\mathbf{z}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\wb}{\mathbf{w}}

\newcommand{\fun}{\mathcal{F}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Ac}{\mathcal{T}}
\newcommand{\Ncb}{\bar{\mathcal{N}}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Sx}{\mathcal{S}}
\newcommand{\Acc}{\mathcal{\Ac_{\xb}}}
\newcommand{\I}{\mathcal{I}}

\newcommand{\iT}{\tilde{i}}
\newcommand{\jT}{\tilde{j}}

\newcommand{\ROT}{\mathrm{R}}
\newcommand{\Q}{\mathrm{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\N}{\mathds{N}}
\newcommand{\Ed}{\mathds{E}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\gmin}{g_{\mathrm{\tiny min}}}
\newcommand{\e}{\bm{\varepsilon}}
\newcommand{\gammas}{\gamma_2(\Ac,\lVert \cdot\rVert_2)}
\newcommand{\gammasb}{\gamma_2(\Acc,\lVert \cdot\rVert_2)}
\newcommand{\xc}{\hat{\mathbf{x}}}
\newcommand{\pii}{\pi(i)}
\newcommand{\pij}{\pi(j)}


\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax\,}}}
\newcommand{\Sup}[1]{\underset{#1}{\mathrm{sup\,}}}
\newcommand{\Max}[1]{\underset{#1}{\mathrm{max\,}}}
\newcommand{\Min}[1]{\underset{#1}{\mathrm{min\,}}}
\newcommand{\Lim}[1]{\underset{#1}{\mathrm{lim\,}}}
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin\,}}}




 
\begin{document}
\title{Deep Graph-Convolutional Image Denoising}

\author{Diego Valsesia,
        Giulia Fracastoro,
        Enrico Magli\thanks{The authors are with Politecnico di Torino -- Department of Electronics and Telecommunications, Italy. email: \{name.surname\}@polito.it. This research has been partially funded by the SmartData@PoliTO center for Big Data and Machine Learning technologies. We thank Nvidia for donating a Quadro P6000 GPU.} }


















\maketitle

\begin{abstract}
Non-local self-similarity is well-known to be an effective prior for the image denoising problem. However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information. In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields. The graph convolution operation generalizes the classic convolution to arbitrary graphs. In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns. We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution. Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.
\end{abstract}

\begin{IEEEkeywords}
Graph neural networks, image denoising, graph convolution
\end{IEEEkeywords}



\IEEEpeerreviewmaketitle



\section{Introduction}


Denoising is a staple among image processing problems and its importance cannot be overstated. Despite decades of work and countless methods, it still remains an active research topic because its purpose goes far beyond generating visually pleasing pictures. Denoising is fundamental to enhance the performance of higher-level computer vision tasks such as classification, segmentation or object recognition, and is a building block in the solution to various problems \cite{lukavs2006digital,valsesia2015compressed,romano2017little,sun2019block}. The recent successes achieved by convolutional neural networks (CNNs) extended to this problem as well and have brought a new generation of learning-based methods that is redefining the state of the art. However, it is important to learn the lessons of past research on the topic and integrate them with the new deep learning techniques. In particular, classic denoising methods such as BM3D \cite{dabov2007image} showed the importance of exploiting non-local self-similar patterns. However, the convolution operation underpinning all CNNs architectures \cite{zhang2017beyond,mao2016image,tai2017memnet,bae2017beyond} is unable to capture such patterns because of the locality of the convolution kernels. Only very recently, some works started addressing the integration of non-local information into CNNs \cite{cruz2018nonlocality,lefkimmiatis2018universal,plotz2018neural,liu2018non}.

This paper presents a denoising neural network, called GCDN, where the convolution operation is generalized by means of graph convolution, which is used to create layers with hidden neurons having non-local receptive fields that successfully capture self-similar information. Graph convolution is a generalization of the traditional convolution operation when the data are represented as sitting over the vertices of a graph. In this work, every pixel is a vertex and the edges in the graph are dynamically computed from the similarities in the feature space of the hidden layers of the network. This allows us to exploit the powerful representational features of neural networks to discover and use latent self-similarities. With respect to other CNNs integrating non-local information for the denoising task, the proposed approach has several advantages: i) it creates an adaptive receptive field for the pixels in the hidden layers by dynamically computing a nearest-neighbor graph from the latent features; ii) it creates dynamic non-local filters where feature vectors that may be spatially distant but close in a latent vector space are aggregated with weights that depend on the features themselves; iii) the aggregation weights are estimated by a fully-learned operation, implemented as a subnetwork, instead of a predefined parameterized operation, allowing more generality and adaptability. Starting from the Edge-Conditioned Convolution (ECC) definition of graph convolution, we propose several improvements to address stability, over-parameterization and vanishing gradient issues. Finally, we also propose a novel neural network architecture which draws from an analogy with an unrolled regularized optimization method.

A preliminary version of this work appeared in \cite{ValsesiaICIP19}. There are several differences with the work in this paper. The architecture of the network is improved by drawing an analogy with proximal gradient descent methods, and it is significantly deeper. Moreover, we propose several solutions to address the ECC overparameterization and computational issues. Finally, we also present an in-depth analysis of the network behavior and greatly extended experimental results.

This paper is structured as follows. Sec. \ref{sec:bkg} provides some background material on graph-convolutional neural networks and state-of-the-art denoising approaches. Sec. \ref{sec:method} describes the proposed method. Sec. \ref{sec:experiments} analyzes the characteristics of the proposed method and experimentally compares it with state-of-the-art approaches. Finally, Sec. \ref{sec:conclusions} draws some conclusions.


\section{Related work} \label{sec:bkg}

\subsection{Graph neural networks}
Inspired by the overwhelming success of deep neural networks in computer vision, a significant research effort has recently been made in order to develop deep learning methods for data that naturally lie on irregular domains. One case is when the data domain can be structured as a graph and the data are defined as vectors on the nodes of this graph. Extending CNNs from signals with a regular structure, such as images and video, to graph-structured signals is not straightforward, since even simple operations such as shifts are undefined over graphs. 

One of the major challenges in this field is defining a convolution-like operation for this kind of data. Convolution has a key role in classical CNNs, thanks to its properties of locality, stationarity, compositionality, which well match prior knowledge on many kinds of data and thus allow effective weight reuse. For this reason, defining an operation with similar characteristics for graph-structured data is of primary importance in order to obtain effective graph neural networks. The literature has identified two main classes of approaches to tackle this problem, namely spectral or spatial. In the former case \cite{henaff2015deep,defferrard2016convolutional,kipf2016semi}, the convolution is defined in the spectral domain through the graph Fourier transform \cite{shuman2013emerging}. Fast polynomial approximations \cite{defferrard2016convolutional} have been proposed in order to obtain an efficient implementation of this operation. Graph-convolutional neural networks (GCNN) with this convolution operator have been successfully applied in problems of semi-supervised node classification and link prediction \cite{kipf2016semi,schlichtkrull2018modeling}. The main drawback of these methods is that the graph is supposed to be fixed and it is not clear how to handle the cases where the structure varies. The latter class of approaches overcomes this issue by defining the convolution operator in the spatial domain \cite{simonovsky2017dynamic,wang2018dynamic,xu2018powerful,monti2017geometric,verma2018feastnet,valsesia2019learning}. In this case, the convolution is performed by local aggregations, i.e. a weighted combination of the signal values over neighboring nodes. Since in this case the operation is defined at a neighborhood level, the convolution remains well-defined even when the graph structure varies. Many of the spatial approaches present in the literature \cite{xu2018powerful,monti2017geometric,verma2018feastnet} perform local aggregations with scalar weights. Instead, \cite{simonovsky2017dynamic} proposes to weight the contributions of the neighbors using edge-dependent matrices. This makes the convolution a more general function, increasing its descriptive power. For this reason, in this paper we employ the convolution operator proposed in \cite{simonovsky2017dynamic}. However, in order to obtain an efficient operation, we introduce several approximations that reduce its computation complexity, memory occupation, and mitigate vanishing gradient issues that arise when trying to build very deep architectures.

\subsection{Image denoising}
The literature on image denoising is vast, as it is one of most classic problems in image processing. Focusing on the recent developments, we can broadly define two categories of methods: model-based approaches and learning-based approaches. 

Model-based approaches traditionally focused on defining hand-crafted priors to carefully capture the salient features of natural images. Early works in this category include total variation minimization \cite{rudin1992nonlinear}, and bilateral filtering \cite{tomasi1998bilateral}. Non-local means \cite{buades2005non} introduced the idea of non-local averaging according to the similarity of local neighborhood. The popular BM3D \cite{dabov2007image} expanded on the idea by collaborative filtering of the matched patches. WNNM \cite{gu2014weighted} used nuclear norm minimization to enforce a low-rank prior. Finally, some works recently introduced graph-based regularizers \cite{pang2017graph} to enforce a measure of smoothness of the signal across the edges of a graph of patch or pixel similiarities.   
Many of the most successful model-based approaches are non-local, i.e., they exploit the concept of self-similarity among structures in the image beyond the local neighborhood.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{net.pdf}
    \caption{GCDN architecture.}
    \label{fig:net}
\end{figure*}
Learning-based approaches use training data to learn a model for natural images. The popular K-SVD algorithm \cite{elad2006image} learns a dictionary in which natural patches have a sparse representation, and therefore casts image denoising as a sparse coding problem on this learned dictionary. The TNRD method \cite{chen2016trainable} uses a nonlinear reaction diffusion model with trainable filters. An early work with neural networks \cite{burger2012image} used a multilayer perceptron discriminatively trained on synthetic Gaussian noise and showed significant improvements over model-based methods. More recently, CNNs have achieved remarkable performance. Zhang et al. \cite{zhang2017beyond} showed that the residual structure and the use of batch normalization \cite{ioffe2015batchnorm} in their DnCNN greatly helps the denoising task. Following the DnCNN, many other architectures have been proposed, such as RED \cite{mao2016image}, MemNet \cite{tai2017memnet} and a CNN working on wavelet coefficients \cite{bae2017beyond}. However, those CNN-based methods are limited by the local nature of the convolution operation, which is unable to increase the receptive field of a neuron-pixel to model non-local image features. This means that CNNs are unable to exploit the self-similar patterns that were proven to be highly successful in model-based methods. Very recently, a few works started addressing this issue by trying to incorporate non-local information in a CNN. NN3D \cite{cruz2018nonlocality} uses a global post-processing stage based on a non-local filter after the output of a denoising CNN. This stage performs block matching and filtering over the whole image denoised by the CNN. This is clearly suboptimal as the non-local information does not contribute to the training of the CNN. 
UNLNet \cite{lefkimmiatis2018universal} introduces a trainable non-local layer which collaboratively filters image blocks. However, performance is limited by the selection of matching blocks from the noisy input image instead of the feature space, and ultimately UNLNet does not improve over the performance of the simpler DnCNN. NNet \cite{plotz2018neural} introduces a continuous nearest-neighbor relaxation to create a non-local layer. Finally, NLRN \cite{liu2018non} proposes a non-local module that uses the distances among hidden feature vectors of a search window around the pixel of interest to aggregate such vectors and return the output features of the pixel. However, there are significant differences with respect to the work in this paper. First, they use all the pixels in the search window instead of only a number of nearest neighbors, which means that their receptive field cannot dynamically adapt to the content of the image. Then, while in both works the feature aggregation weights are dynamically computed from the features themselves, NLRN uses an explicitly-parameterized function with learnable parameters, in contrast to this work where the function is fully learned as a dedicated sub-network. These choices increase the adaptivity of the proposed non-local operations, which result in better performance around edges.


\section{Proposed denoiser}\label{sec:method}


\subsection{Overview} \label{sec:overview}

An overview of the proposed graph-convolutional denoiser network (GCDN) can be seen in Fig. \ref{fig:net}. The structure will be explained more in detail in Sec. \ref{sec:analogy} where an analogy is drawn between unrolled proximal gradient descent with a graph total variation regularizer and the proposed network architecture. At a first glance, the network has a global input-output residual connection whereby the network learns to estimate the noise rather than successively clean the image. This has been shown \cite{zhang2017beyond} to improve training convergence for the denoising problem. 

The main feature of the proposed network is the use of graph-convolutional layers where the graphs are dynamically computed from the feature space. The graph-convolutional layer, described in Sec. \ref{sec:gconv_layer}, creates a non-local receptive field for each pixel-neuron, so that pixels that are spatially distant but similar in the feature space created by the network can be merged. 

An important block of the proposed network is the preprocessing stage at the input. It can be noticed that the first layers of the network are classic 2D convolutions rather than graph convolutions. This is done to create an embedding over a receptive field larger than a single pixel and stabilize the graph construction operation, which would otherwise be affected by the input noise. The preprocessing stage has three parallel branches that operate on multiple scales, in a fashion similar to the architectures in \cite{szegedy2015going} and \cite{divakar2017image}. The multiscale features are extracted by a sequence of three convolutional layers with filters of size , , and , depending on the branch. After a final graph-convolutional layer, the features are concatenated. 

The remaining network layers are grouped into an HPF block and multiple LPF blocks, named after the analogy with highpass and lowpass graph filters described in Sec. \ref{sec:analogy}. These blocks have an initial  convolutional layer followed by three graph-convolutional layers sharing the same graph constructed from the output of the convolutional layer. All layers are interleaved by Batch Normalization operations \cite{ioffe2015batchnorm} and leaky ReLU nonlinearities. Notice that the LPF blocks have themselves a residual connection to help backpropagation, as in ResNet architectures \cite{he2016deep}. The final layer is a graph-convolutional layer mapping from feature space to the image space.




\subsection{Graph-convolutional layer} \label{sec:gconv_layer}
\label{sec:graph_conv}
\begin{figure}
    \centering
    \includegraphics[width=0.23\textwidth]{gconv.pdf}
    \caption{Graph-convolutional layer. The operation has a receptive field with a local component ( 2D convolution) and a non-local component (pixels selected as nearest neighbors in the feature space).}
    \label{fig:gconv}
\end{figure}

The operation performed by the graph-convolutional layer is summarized in Fig. \ref{fig:gconv}. The two inputs to the graph-convolutional layer are the feature vectors  associated to the  image pixels at layer  and the adjacency matrix of a graph connecting image pixels. In this work, the graph is constructed as a -nearest neighbor graph in the feature space. For each pixel, the Euclidean distances between its feature vector and the feature vectors of pixels inside a search window are computed and an edge is drawn between the pixel and the  pixels with smallest distance. Using this method, we obtain a -regular graph , where  is the set of vertices with  and  is the set of edges. We also assume that the edges of  are labeled, i.e. there exists a function  that assigns a label to each edge. In this work, we define the edge labeling function as the difference between the two feature vectors, i.e. .  A classic  local convolution processes the local neighborhood to provide its estimate of the output feature vector for the current pixel, while the feature vectors of the non-local pixels connected by the graph are aggregated by means of the edge-conditioned convolution (ECC) \cite{simonovsky2017dynamic}. Notice that the 8 local neighbors of the pixel are excluded from graph construction as they are already used by the local convolution.
The non-local aggregation is computed as:

where  is a fully-connected network that takes as input the edge labels and outputs the corresponding weight matrix ,  are the weights parameterizing network , and  is the set of neighbors of node  in the graph . The scalar  is an edge-attention term computed as:

where  is a cross-validated hyper-parameter. This term is reminiscent of the edge attention mechanism from the graph neural network literature \cite{gong2019exploiting} and it serves the purpose of stabilizing training by underweighting the edges that connect nodes with distant feature vectors. Note that this term could, in principle, be learned by the  network but we found that decoupling it and making it explicitly dependent on feature distances in an exponential way, accelerated and stabilized training. Also notice that in Sec. \ref{sec:experiments} we show that this term alone, i.e. without weight matrices , is not powerful enough to reach good performance. Moreover, it is worth mentioning that the edge weights  and the edge-attention term  depend only on the edge labels. This means that two pairs of nodes with the same edge labels will have the same weights, resulting in a behaviour similar to weight sharing in classical CNNs.

Finally, we combine the feature vector estimated by the non-local aggregation with the one produced by the local convolution to provide the output features as follows

where  is the output of the  local convolution for the node  and  is the bias.

The advantages of the ECC with respect to other definitions of graph convolution are trifold: i) the edge weights depend on the edge label, ii) it allows to compute an affine transformation along every edge, and iii) the edge weight function is highly general since it does not have a predefined structure. By making the edge weights depend on the input features, the ECC implements an adaptive filter which can be more complex than the non-adaptive local filters. Moreover, the second advantage is due to the fact that  is an edge-dependent matrix, making the convolution operation more general than other non-local aggregation methods using scalar edge weights. Among such methods we can find GCN \cite{kipf2016semi}, GIN \cite{xu2018powerful}, MoNet \cite{monti2017geometric}, and FeastNet \cite{verma2018feastnet}.  Finally, the  function is a general function which can be learned to be the optimal one for the denoising task by the function approximation capability of the subnetwork implementing it. This is in contrast with other methods where the function predicting the edge weights is fixed with some learnable parameters. For example, FeastNet \cite{verma2018feastnet} employs scalar edge weights computed using the following function

where  and  are learnable parameters. Instead, MoNet \cite{monti2017geometric} employs a Gaussian kernel as follows

where  and  are learnable parameters. Also NLRN \cite{liu2018non} uses a Gaussian kernel to perform non-local aggregations. We can consider this operation as a graph convolution where each pixel is connected to all the other pixels in its search window and the edge weights are defined as follows

where  and  are learnable parameters.


\subsection{Lightweight Edge-Conditioned Convolution} \label{sec:ecc}
As seen in the previous section, the function  has a key role in the ECC because it defines the weights for the neighborhood aggregation. In the original definition of ECC \cite{simonovsky2017dynamic}, the function  is implemented as a two-layer fully connected network. This definition raises some relevant issues. In the following, we will describe in detail these issues and present two possible solutions.

\subsubsection{Circulant approximation of dense layer}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{circulant_dense.png}
    \caption{Circulant approximation of a fully-connected layer.}
    \label{fig:circ}
\end{figure}

The first issue is related to the risk of over-parameterization. The dimension of the input of the  network is , while the dimension of its output is . This means that the number of weights of the network depends cubically on the number of features. Therefore, the number of parameters quickly becomes excessively large, resulting in vanishing gradients or overfitting. 



To address the over-parameterization problem we propose to use a partially-structured matrix for the last layer, instead of an unstructured one. We impose that this matrix is composed of multiple stacked partial circulant matrices, i.e., matrices where only a few shifted versions of the first row are used instead of all the possible ones of the full square matrix. Fig. \ref{fig:circ} shows the structure of the approximated matrix. Using this approximation, the only free parameters are in the first row of each partial circulant matrix. If only  shifts per partial circulant matrix are allowed, we reduce the number of parameters by a factor . Thus, if the unstructured dense matrix has  parameters, with the proposed approximation the number of parameters drops to  . Similar approaches to approximate fully connected layers have already been studied in the literature \cite{cheng2015exploration,wu2016compression}. In particular, \cite{cheng2015exploration} shows that imposing a partial circulant structure does not significantly impact the final performance in a classification problem. Indeed, there are connections with results stating that random partial circulant matrices implement stable embeddings almost as well as fully random matrices \cite{hinrichs2011johnson,valsesia2017user,valsesia2017binary}.


\subsubsection{Low-rank node aggregation}

The second issue related to the  network regards memory occupation and computations. In order to perform the ECC operation, we have to compute a weight matrix  for each edge  of every neighborhood  of every image in the batch. If we consider a -regular graph and a batch of  images with  pixels each, the memory occupation needed to store all the matrices  as single-precision floating point tensors is equal to  bytes and this quantity can easily become unmanageable. To give an idea of the required amount of memory,  let us consider an example with , , , , then the memory required to store all the matrices  for only one graph-convolutional layer is around 2 GB.

\begin{figure}
    \centering
    \includegraphics[width=0.28\textwidth]{Fnet.pdf}
    \caption{ network. FC is a fully-connected layer followed by a leaky ReLU non-linearity. The FC, FC, FC do not have any output non-linearities.}
    \label{fig:fnet}
\end{figure}

In order to solve this issue, we propose to impose a low-rank approximation for . Let us consider the singular value decomposition of a matrix  

where  and  are the left and right singular vectors and  the singular values. We can obtain a low-rank approximation of rank  by keeping only the  largest singular values and setting the others to zero. Therefore, the approximation is reduced to a sum of  outer products. Inspired by this fact, we define  as follows

where , ,  and . Notice that the approximation in \eqref{eq:theta_approx} ensures that the rank is at most  rather than exactly enforcing a rank- structure, because we do not impose orthogonality between  and , even though random initialization makes them quasi-orthogonal. Using this approximation, we can redefine the  network in such a way that it outputs  for . In particular, we redefine the second layer of the  network: instead of having a single fully connected layer that outputs the entire matrix , we have three parallel fully connected layers that separately output ,  and , as shown in Fig. \ref{fig:fnet}. The advantage of this approximation is that we only need to store ,   and  instead of the entire matrix , drastically reducing the memory occupation to  bytes. If we consider the example presented above and set , the memory requirement drops from 2 GB to 700 MB. Another advantage of this approximation is that it also leads to a significant reduction of the computation burden, because we never have to actually compute all the matrices . In fact, the neighborhood aggregation can be reduced as follows

where the computational cost of the full operation on the first line is , instead the cost of the decoupled operation on the second line is .
Finally, this approximation also helps to reduce the number of parameters of the last layer of the  network since the output has size  instead of .

When we employ the new structure of the  network, we need to pay special attention to the weight initialization. In particular, we have to carefully define the variance of the random weight initialization of the three parallel layers to avoid scaling problems. We define  as the weight matrix of the first layer of the  network, and ,  and  as the weight matrices of the three parallel fully connected layers. Let us suppose that  has been normalized to be approximately a standard Gaussian, i.e.,  for , and that  has been initialized using Glorot initialization \cite{pmlr-v9-glorot10a}, i.e.,  with . Let us also assume that , , and . Then, we obtain

where . Finally, considering the aggregation formula in Eq. \eqref{eq:lowrank_aggr} leads to the following result:

with . In Eq. \eqref{eq:init}, we can observe that the variance of  depends on the fourth power of the number of features. This term can easily become extremely large, therefore it is important to set ,  and  in such a way that they can balance it. In this work, we set  and . This allows us to obtain  with .
\subsection{Analogy with unrolled graph smoothness optimization} \label{sec:analogy}

\begin{figure}
    \centering
    \subfigure[Linear inverse problem]{
    \includegraphics[width=0.232\textwidth]{update_linear.pdf}
    }
    \subfigure[Denoising]{
    \includegraphics[width=0.2\textwidth]{update_2.pdf}
    }
    \caption{Single iteration. LPF is graph lowpass filter, HPF is a graph highpass filter.}
    \label{fig:update_fig}
\end{figure}

The neural network architecture presented in Sec. \ref{sec:overview} can be seen as a generalization of few iterations of an unrolled proximal gradient descent optimization method, which is widely used to solve linear inverse problems in the form of

being  the clean image,  a forward model (e.g., a degradation such as blurring, downsampling, compressed sensing, etc.) and  a noise term. A well-known technique to recover  from  is to cast the problem as a least-squares minimization problem with a regularization term that models some prior knowledge about the image. One such regularizer is graph smoothness. Considering a graph with Laplacian matrix  where edges connect pixels that are deemed correlated according to some criterion, the graph smoothness  is the graph equivalent of the total variation measure, indicating how much  varies across the edges of the graph. Natural images where the graph connects the local neighborhood typically have lowpass behavior, resulting in a low graph smoothness value. Reconstruction is therefore cast as:

The functional in Eq. \eqref{eq:x_est} is in the form of a sum of two terms () and can be minimized by means of proximal gradient descent \cite{combettes2011proximal} which alternates a gradient descent step over  and a proximal mapping over :

Solving for the proximal mapping operator results in the following update equation:

In order to match the framework of residual networks, let us define the least-squares solution  and perform a change of variable whereby the optimization estimates the residual of the least squares solution, i.e., . Hence, we can rewrite Eq. \eqref{eq:update} as:

Finally, the following update equation can be derived:




This update can be visualized as in Fig. \ref{fig:update_fig}a and is composed of two major operations involving the signal prior:
\begin{enumerate}
    \item : the graph Laplacian can be seen as a graph highpass filter applied to ;
    \item : this term can be seen as a graph lowpass filter. In order to see this, let us use the matrix inversion lemma as , where  is the graph Fourier transform. The term  is a diagonal matrix whose entries are equal to  where  are the eigenvalues of the graph Laplacian, and the lowpass behavior is due to decreasing value of such entries for increasing .
\end{enumerate}
For the denoising problem, we can set  and obtain the update shown in Fig.\ref{fig:update_fig}b.
The network architecture proposed in Sec. \ref{sec:overview} draws from this derivation by unrolling a finite number of Eq. \eqref{eq:noise_est} iterations and generalizing the lowpass and highpass filters with learned graph filters interleaved by nonlinearities. In Sec. \ref{sec:experiments} we experimentally show that the learned filters actually show an approximate highpass and lowpass behavior.


\section{Experimental Results} \label{sec:experiments}

\subsection{Training details}
The training protocol follows the one used in \cite{zhang2017beyond}. The network is trained with patches of size  randomly extracted from 400 images from the train and test partitions of the Berkeley Segmentation Dataset (BSD) \cite{MartinFTM01}, withholding the 68 images in the validation set for testing purposes (BSD68 dataset). The loss function is the mean squared error (MSE) between the denoised patch output by the network and the ground truth. Each model is trained for approximately 800000 iterations with a batch size of 8. The Adam optimizer \cite{kingma2014adam} has been used with an exponentially decaying learning rate between  and . The behavior of the graph-convolutional layer is slightly different between training and testing for efficiency reasons. During training all pairwise distances are computed among the feature vectors corresponding to the pixels in the patch. On the other hand, testing is ``fully convolutional'', as every pixel has a search window centered around it and neighbors are identified as the closest pixels in such search window. The search window size is , roughly comparable to the patch size used in training. This procedure is slightly suboptimal as some pixels might suffer from border effects during training (their search windows are not centered around them) but it is advantageous in terms of speed and memory requirements. Reflection padding is used for all 2D convolutions to avoid border effects. The  parameter in the edge attention term in Eq. \eqref{eq:edge_attention} is set to a value equal to 10. The number of features used in all convolutional layers is 132, except for the three parallel branches of the preprocessing stage which have 44 features. The number of
circulant rows in the circulant approximation of dense layers in the  network is . The low-rank approximation uses  terms. During training, we noticed that the proposed lightweight ECC presented in Sec. \ref{sec:ecc} is extremely useful. In fact, without it, the network suffered from vanishing gradient problems even with a significantly lower number of layers.

\subsection{Feature analysis}
In this section we study the properties of the features in the hidden layers of the network.

\subsubsection{Adaptive receptive field}
We first analyze the characteristics of the receptive field of a single pixel. Since the proposed network employs graph-convolutional layers, the shape of the receptive field is not fixed as in classical CNNs, but it depends on the structure of the graph. In Fig. \ref{fig:recfield} we show two examples of the receptive field of a single pixel for the graph-convolutional layers in an LPF block with respect to the input of the block. Instead, in Fig. \ref{fig:recfield_long} we show the receptive field of a single pixel for the layers in the HPF and in the first LPF blocks with respect to the output of the preprocessing block. We can clearly see that the receptive field is adapted to the characteristics of the image: if we consider a pixel in a uniform area, its receptive field will mostly contain pixels that belong to similar regions; instead if we consider a pixel on an edge, its receptive field will be mainly composed of other edge pixels. This is beneficial to the denoising task as it allows to exploit self-similarity and it descends from the use of a nearest neighbor graph, connecting each pixel to other pixels with similar features. Notice that differently from algorithms performing block matching in the pixel space, we compute distances between feature vectors which can capture more complex image characteristics. This can be seen in Fig. \ref{fig:dir} where we compute the Euclidean distances between the feature vector of the central pixel and the feature vectors of the other pixels in the search window. We notice that the distances reflect the type of edge that includes the central pixel, e.g., a pixel sitting on a horizontal edge will detect as closest other pixels sitting on horizontal edges. This is due to the visual features learned by the network and would not happen in pixel-space matching.
Thanks to the adaptability of the receptive field, graph convolution can be interpreted as a generalization of the block matching operation performed in other non-local denoising methods, such as BM3D \cite{dabov2007image}.


\begin{figure}
\centering
\includegraphics[width=0.32\columnwidth]{rf_lpf_l1.pdf}
\includegraphics[width=0.32\columnwidth]{rf_lpf_l2.pdf}
\includegraphics[width=0.32\columnwidth]{rf_lpf_l3.pdf}
\6pt]
\includegraphics[width=0.32\columnwidth]{feat_hpf_1.pdf}
\includegraphics[width=0.32\columnwidth]{feat_hpf_2.pdf}
\includegraphics[width=0.32\columnwidth]{feat_hpf_3.pdf}
\caption{Log-magnitude of discrete Fourier transform of three feature maps at the output of the LPF block (top) and HPF block (bottom). Blue is lower magnitude.}
\label{fig:filters}
\end{figure}



\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{graph_embedding.pdf}
\caption{Accuracy of edge prediction from hidden layers.}
\label{fig:graph_embedding}
\end{figure}

\subsubsection{Filter analysis}
We also study the behavior of the LPF and HPF operators. In particular, we are interested in validating the analogy made in Sec. \ref{sec:analogy}. We compute the discrete Fourier transform (DFT) of the feature maps at the output of these operators. As an example, Fig. \ref{fig:filters} shows the log-magnitude of the coefficients of three feature maps at the output of the HPF block and of the first LPF block. The energy of the DFT coefficients of the LPF feature maps is concentrated in the low frequencies, thus showing a lowpass behavior. Instead, the coefficients of the HPF feature maps show a typical highpass behavior, having the energy concentrated along few directions.
This substantiates our claim that the learned convolutional layers actually approximate nonlinear highpass and lowpass operators.

\subsubsection{Edge prediction}
Lastly, we measure how much the true graph constructed by pixel or patch similarities on the noiseless image is successfully predicted by the graph constructed from the feature vectors in the hidden layers. In order to construct the true graph of the image, we first compute the average pixel value of a 55 window centered at the considered pixel, for every pixel in the image, and then we use the obtained values to compute a nearest neighbor graph with Euclidean distances. We then compare the true graph with the graph computed in the hidden layers of the network. Fig. \ref{fig:graph_embedding} shows the percentage of edges correctly identified as function of the number of neighbors considered for the true graph. We can notice that the accuracy of the prediction decreases in layers closer to the output. This is due to the fact that we use a residual network that estimates the noise instead of approximating the clean image. In fact, the network learns to successively remove the latent correlations in the feature space, and as a consequence, the graph becomes more random in the later layers.


\subsection{Ablation studies}
We study the impact of various design parameters on denoising performance. First, Table \ref{table:psnr_vs_nn} shows the PSNR on the Set12 testing set as function of the number of neighbors used by the graph convolution operation for several values of the noise standard deviation . Each model has been independently trained for the specified number of neighbors.  It can be noticed that increasing the number of neighbors improves the denoising performance up to a saturation point, and then the performance slightly decreases. This shows that there an optimal neighborhood size and that it is important to employ only a small number of neighbors, in order to select only pixels with similar characteristics. This is in contrast with the NLRN method which uses all the pixels in the search window.

Then, we study the relative impact on performance of the edge aggregation matrices  in Eq. \eqref{eq:ecc} with respect to using only the edge attention scalar . Table \ref{table:onlyscalar} reports the PSNR achieved on Set12 by the proposed method with the non-local aggregation performed as in Eq. \eqref{eq:ecc} and a variant where the aggregation is computed as:

Both methods use a non-local graph with 8 nearest neighbors.
We can notice that the edge attention term alone achieves a worse PSNR with respect to GCDN by approximately 0.2 dB, even though it improves over a model without non-local neighbors (see Table \ref{table:psnr_vs_nn} for the corresponding 0-NN value). This shows the advantage of using a trainable affine transformation, such as  in Eq. \eqref{eq:ecc}, instead of a scalar weight function with a predefined structure. 

Finally, we remark that we do not compare with respect to the full ECC without the approximations introduced in Sec. \ref{sec:ecc} because it suffers from vanishing gradient problems, rendering training unstable even for a much smaller number of layers, and it would be computationally prohibitive.

\begin{table} 
\centering
\caption{PSNR (dB) v. non-local neighborhood size (Set12)}
\begin{tabular}{ccccccc}
\hline
 & 0-NN & 4-NN & 8-NN & 12-NN & 16-NN & 20-NN \\ \hline
15       & 32.91 & 33.09 & 33.11 & 33.13 & 33.14 & 33.13 \\ \hline
25       & 30.50 & 30.70 & 30.74 & 30.75 & 30.78 & 30.78 \\ \hline
50       & 27.28 & 27.52 & 27.58 & 27.58 & 27.60 & 27.59 \\ \hline
\end{tabular}
\label{table:psnr_vs_nn}
\end{table}

\begin{table}
\centering
\caption{Edge attention v. ECC + edge attention (8-NN). PSNR (dB).}
\begin{tabular}{ccc}
\hline
 & Edge attention only & Proposed \\ \hline
25       & 30.53 & 30.74 \\ \hline
\end{tabular}
\label{table:onlyscalar}
\end{table}


\subsection{Comparison with state of the art}

\begin{table*}[t]
\centering
\caption{Natural image denoising results. Metrics are PNSR (dB) and SSIM.}
\begin{tabular}{ccccccccc}
\hline
Dataset                   & Noise  & BM3D           & WNNM           & TNRD           & DnCNN          & NNet  & NLRN                    & \textbf{GCDN}       \\ \hline
\multirow{3}{*}{Set12}    & 15             & 32.37 / 0.8952 & 32.70 / 0.8982 & 32.50 / 0.8958 & 32.86 / 0.9031 & - / -     & \textbf{33.16} / 0.9070 & 33.14 / \textbf{0.9072}         \\
                          & 25             & 29.97 / 0.8504 & 30.28 / 0.8557 & 30.06 / 0.8512 & 30.44 / 0.8622 & 30.55 / - & \textbf{30.80 / 0.8689} & 30.78 / 0.8687          \\
                          & 50             & 26.72 / 0.7676 & 27.05 / 0.7775 & 26.81 / 0.7680 & 27.18 / 0.7829 & 27.43 / - & \textbf{27.64 / 0.7980} & 27.60 / 0.7957          \\ \hline
\multirow{3}{*}{BSD68}    & 15             & 31.07 / 0.8717 & 31.37 / 0.8766 & 31.42 / 0.8769 & 31.73 / 0.8907 & - / -     & \textbf{31.88} / 0.8932 & 31.83 / \textbf{0.8933}          \\ 
                          & 25             & 28.57 / 0.8013 & 28.83 / 0.8087 & 28.92 / 0.8093 & 29.23 / 0.8278 & 29.30 / - & \textbf{29.41} / 0.8331 & 29.35 / \textbf{0.8332} \\ 
                          & 50             & 25.62 / 0.6864 & 25.87 / 0.6982 & 25.97 / 0.6994 & 26.23 / 0.7189 & 26.39 / - & \textbf{26.47} / 0.7298 & 26.38 / \textbf{0.7389}          \\ \hline
\multirow{3}{*}{Urban100} & 15             & 32.35 / 0.9220 & 32.97 / 0.9271 & 31.86 / 0.9031 & 32.68 / 0.9255 & - / -     & 33.42 / 0.9348          & \textbf{33.47 / 0.9358} \\ 
                          & 25             & 29.70 / 0.8777 & 30.39 / 0.8885 & 29.25 / 0.8473 & 29.97 / 0.8797 & 30.19 / - & 30.88 / 0.9003 & \textbf{30.95 / 0.9020}         \\ 
                          & 50             & 25.95 / 0.7791 & 26.83 / 0.8047 & 25.88 / 0.7563 & 26.28 / 0.7874 & 26.82 / - & 27.40 / \textbf{0.8244} & \textbf{27.41} / 0.8160          \\ \hline
\end{tabular}
\label{table:results}
\end{table*}

\begin{table*}[t]
\setlength{\tabcolsep}{4pt} \renewcommand{\arraystretch}{1} \centering
\caption{Depth map denoising results. Metrics are PNSR (dB) and SSIM.}
\begin{tabular}{ccccccccccc}
\hline
      & Method   & \textit{aloe}           & \textit{art}            & \textit{baby}           & \textit{cones}          & \textit{dolls}          & \textit{laundry}        & \textit{moebius}        & \textit{reindeer}               & Average                 \\ \hline
\multirow{3}{*}{15} & GCDN & 40.74 / \textbf{0.9873}          & 40.66 / \textbf{0.9886}          & 41.64 / \textbf{0.9917}          & 39.29 / \textbf{0.9832}          & \textbf{40.70 / 0.9830} & \textbf{41.97 / 0.9842} & \textbf{42.07 / 0.9877}   & \textbf{42.62 / 0.9915}           & \textbf{41.21 / 0.9872} \\ 
                    & NLRN     & 40.50 / 0.9844          & 40.48 / 0.9858          & \textbf{41.76} / 0.9899 & 39.50 / 0.9814          & 40.69 / 0.9800          & 41.96 / 0.9814          & 42.01 / 0.9848          & 42.44 / 0.9880           & 41.17 / 0.9845          \\
                    & OGLR     & \textbf{40.82} / 0.9801 & \textbf{40.77} / 0.9821 & 40.90 / 0.9806          & \textbf{39.65} / 0.9774 & 40.41 / 0.9756          & 41.32 / 0.9764          & 41.48 / 0.9793          & 41.72 / 0.9823                   & 40.88 / 0.9792          \\ \hline
\multirow{3}{*}{25} & GCDN & \textbf{37.12 / 0.9771} & \textbf{37.15 / 0.9788} & \textbf{37.50 / 0.9814} & 35.88 / \textbf{0.9697}          & \textbf{37.05 / 0.9705} & \textbf{38.62 / 0.9730} & \textbf{38.39 / 0.9786} & \textbf{38.80 / 0.9836}          & \textbf{37.56 / 0.9766} \\ 
                    & NLRN     & 37.08 / 0.9720          & 37.01 / 0.9734          & 37.37 / 0.9797          & \textbf{36.09} / 0.9661 & 37.01 / 0.9646          & 38.42 / 0.9679          & 38.33 / 0.9723          & 38.65 / 0.9786         & 37.50 / 0.9718          \\ 
                    & OGLR     & 36.67 / 0.9592          & 36.68 / 0.9649          & 36.29 / 0.9594          & 35.51 / 0.9545          & 36.41 / 0.9541          & 37.44 / 0.9541          & 37.17 / 0.9575          & 37.86 / 0.9655                & 36.75 / 0.9587          \\ \hline
\multirow{3}{*}{50} & GCDN & \textbf{33.37 / 0.9522} & \textbf{33.18 / 0.9536} & 32.23 / 0.9468          & \textbf{31.61 / 0.9379} & 32.37 / \textbf{0.9417}          & 34.07 / \textbf{0.9526}          & \textbf{33.73 / 0.9567} & 34.35 / \textbf{0.9672}                   & \textbf{33.11 / 0.9511} \\  
                    & NLRN     & 33.23 / 0.9444          & 32.86 / 0.9448          & \textbf{32.42 / 0.9534} & 31.53 / 0.9304          & \textbf{32.40} / 0.9347 & \textbf{34.15} / 0.9459 & 33.58 / 0.9475          & \textbf{34.37} / 0.9603  & 33.07 / 0.9452          \\ 
                    & OGLR     & 32.24 / 0.9121          & 31.92 / 0.9129          & 31.23 / 0.9027          & 30.21 / 0.8926          & 31.44 / 0.8999          & 32.85 / 0.9051          & 32.46 / 0.9093          & 32.99 / 0.9191               & 31.92 / 0.9067          \\ \hline
\end{tabular}
\label{table:depthmap}
\end{table*}

In this section we compare the proposed network with state-of-the-art models for the Gaussian denoising task of grayscale images. We train an independent model for each noise standard deviation, which is assumed to be known a priori for all methods. We fix the number of neighbors for the proposed method to 16. The reference methods can be classified into model-based algorithms such as BM3D \cite{dabov2007image}, WNNM \cite{gu2014weighted}, TNRD \cite{chen2016trainable} and recent deep-learning methods such as DnCNN \cite{zhang2017beyond}, NNet \cite{plotz2018neural} and NLRN \cite{liu2018non}. In particular, among the deep-learning methods, NNet and NLRN propose non-local approaches. All results have been obtained running the pretrained models provided by the authors, except for NNet at  which is unavailable. Table \ref{table:results} reports the PSNR and SSIM values obtained for the Set12, BSD68 and Urban100 standard test sets. It can be seen that the proposed method achieves state-of-the art performance and works especially well at low to medium levels of noise. This can be explained by a higher difficulty in constructing a meaningful graph from the noisy image at higher noise levels. We also notice that the proposed method achieves strong results on the Urban dataset. This dataset contains higher resolution images with respect to the other two and is mainly composed of photos of buildings and other regular structures where exploiting self-similarity is very important. In addition, it is also worth mentioning that the proposed method provides a better visual quality. In many cases, the proposed method has a higher SSIM score, even if NRLN has better performance in terms of PSNR. This can also be noticed in Fig. \ref{fig:natural_visual}, which shows a visual comparison on an image from the Urban100 dataset. In general, the images produced by the proposed algorithm present sharper edges and smoother content in uniform areas.
We can notice that many areas in the photos from Urban100 have approximately piecewise smooth characteristics. It is well known that image processing algorithms based on graphs are well suited for piecewise smooth content (see, e.g., \cite{Hu2015multiresolution,Fracastoro2017graph} in the context of compression and \cite{pang2017graph} for denoising). To further show this point, we study the performance of the proposed method for denoising of depth maps, e.g., generated by time-of-flight cameras. The OGLR algorithm \cite{pang2017graph} based on a graph smoothness regularizer achieved state-of-the-art results among model-based algorithms for this specific task where it is essential to preserve edge sharpness while simultaneously smoothing the flat areas. Table \ref{table:depthmap} reports the PSNR and SSIM results achieved on a standard set of depth maps\footnote{http://vision.middlebury.edu/stereo/data/.}. It can be seen that the proposed method outperforms both NLRN and OGLR, even at high levels of noise. Also, we can notice that OGLR displays competitive performance at low noise levels, but its visual quality significantly degrades when in presence of stronger noise. Fig. \ref{fig:depth_visual} shows a visual comparison where it can be seen that GCDN produces sharper edges while also providing a very smooth background.

\begin{figure*}
\vspace*{5cm}
\centering
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3.5,size=2cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{urban_clean.png}};
    {\spy[red!70!black] on (0.7,0.7) in node at (1.00,3.51);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3.5,size=2cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{urban_noisy.png}};
    {\spy[red!70!black] on (0.7,0.7) in node at (1.00,3.51);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{
\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3.5,size=2cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{urban_bm3d.png}};
    {\spy[red!70!black] on (0.7,0.7) in node at (1.00,3.51);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{
\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3.5,size=2cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{urban_dncnn.png}};
    {\spy[red!70!black] on (0.7,0.7) in node at (1.00,3.51);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{
\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3.5,size=2cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{urban_nlrn.png}};
    {\spy[red!70!black] on (0.7,0.7) in node at (1.00,3.51);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{
\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3.5,size=2cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{urban_gcdn.png}};
    {\spy[red!70!black] on (0.7,0.7) in node at (1.00,3.51);}
    \end{tikzpicture}
}
\caption{Extract from Urban100 scene 13, . Left to right: ground truth, noisy ( dB), BM3D ( dB), DnCNN ( dB), NLRN ( dB), GCDN (\textbf{31.53 dB}).}
\label{fig:natural_visual}
\end{figure*}


\begin{figure*}
\vspace*{3.2cm}
\centering
\parbox{0.195\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3,size=1cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.195\textwidth]{aloe_gt.png}};
    {\spy[red!70!black] on (1.70,1.47) in node at (3.08,0.51);}
\end{tikzpicture}
}
\parbox{0.195\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3,size=1cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.195\textwidth]{aloe_noisy50.png}};
    {\spy[red!70!black] on (1.70,1.47) in node at (3.08,0.51);}
\end{tikzpicture}
}
\parbox{0.195\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3,size=1cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.195\textwidth]{aloe_oglr50.png}};
    {\spy[red!70!black] on (1.70,1.47) in node at (3.08,0.51);}
\end{tikzpicture}
}
\parbox{0.195\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3,size=1cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.195\textwidth]{aloe_nlrn50.png}};
    {\spy[red!70!black] on (1.70,1.47) in node at (3.08,0.51);}
\end{tikzpicture}
}
\parbox{0.195\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=3,size=1cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.195\textwidth]{aloe_gcdn50.png}};
    {\spy[red!70!black] on (1.70,1.47) in node at (3.08,0.51);}
\end{tikzpicture}
}
\caption{\textit{aloe} depthmap denoising, . Left to right: ground truth, noisy ( dB), OGLR ( dB), NLRN ( dB), GCDN (\textbf{33.37 dB}).}
\label{fig:depth_visual}
\end{figure*}


\subsection{Real image denoising}
Real image noise is generally more challenging than synthetic Gaussian noise. There are multiple contributions such as quantization noise, shot noise, fixed-pattern noise \cite{holst1998ccd,lukavs2006digital}, dark current, etc. that make it overall signal-dependent. It has been observed \cite{plotz2017benchmarking,SIDD_2018_CVPR} that deep learning methods trained on synthetic Gaussian noise perform poorly in presence of real noise. However, suitable retraining with real data generally improves their performance. In this section, we study the behavior of the proposed network in a blind denoising setting with real noisy images acquired by smartphones. We retrain the proposed method, NLRN and DnCNN on the SIDD dataset \cite{SIDD_2018_CVPR} composed of 30000 high-resolution images acquired by smartphone cameras at varying illumination and ISO levels. The authors provide clean and carefully registered ground truths for all the available scenes, so that it is possible to perform a supervised training. We create training and testing subsets from the sRGB images in the SIDD dataset by selecting a range of noise levels. Our training set is composed of 3500 crops of size  whose RMSE with respect to the ground truth is below 15. The testing set is composed of 25 random crops of size  with noise in the same range as the training set. Table \ref{table:real_noise} reports the results for CBM3D \cite{dabov2007color}, DnCNN, NLRN and the proposed GCDN. Notice that CBM3D is not a blind method, so we provide an estimate of the noise standard deviation, as computed by a noise estimation algorithm \cite{Chen2015Efficient}.
We can notice that the proposed method achieves better results and this is confirmed by the visual comparison in Fig. \ref{fig:real_visual}.
\begin{figure*}
\vspace*{3cm}
\centering
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=2,width=1.9cm, height=2.3cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{real_gt.png}};
    {\spy[red!70!black] on (0.5,1.5) in node at (2,1.8);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=2,width=1.9cm, height=2.3cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{real_noisy.png}};
    {\spy[red!70!black] on (0.5,1.5) in node at (2,1.8);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=2,width=1.9cm, height=2.3cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{real_cbm3d.png}};
    {\spy[red!70!black] on (0.5,1.5) in node at (2,1.8);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=2,width=1.9cm, height=2.3cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{real_dncnn.png}};
    {\spy[red!70!black] on (0.5,1.5) in node at (2,1.8);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=2,width=1.9cm, height=2.3cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{real_nlrn.png}};
    {\spy[red!70!black] on (0.5,1.5) in node at (2,1.8);}
\end{tikzpicture}
}
\parbox{0.161\textwidth}{\begin{tikzpicture}[overlay,remember picture,spy using outlines={rectangle,magnification=2,width=1.9cm, height=2.3cm}]
    \node[inner sep=0pt, anchor=south west,outer sep=0pt] 
    {\pgfimage[width=0.161\textwidth]{real_gcdn.png}};
    {\spy[red!70!black] on (0.5,1.5) in node at (2,1.8);}
\end{tikzpicture}
}
\caption{Real image denoising. Left to right: ground truth, noisy (23.80 dB), CBM3D (34.84 dB), DnCNN (36.05 dB), NLRN (37.15 dB), GCDN (\textbf{37.33 dB}).}
\label{fig:real_visual}
\end{figure*}


\begin{table}[]
\centering
\caption{Real image denoising (SIDD dataset)}
\begin{tabular}{ccccc}
\hline
     & CBM3D    & DnCNN    & NLRN     & GCDN          \\ \hline
PSNR & 38.73 dB & 39.98 dB & 41.24 dB & \textbf{41.48 dB} \\ \hline
SSIM & 0.9587 & 0.9605   & 0.9652   & \textbf{0.9697}   \\ \hline
\end{tabular}
\label{table:real_noise}
\end{table}

\vspace{-0.2cm}
\section{Conclusions} \label{sec:conclusions}
\vspace{-0.2cm}
In this paper, we presented a graph-convolutional neural network targeted for image denoising. The proposed  graph-convolutional layer allows to exploit both local and non-local similarities, resulting in an adaptive receptive field. We showed that the proposed architecture can outperform state-of-the-art denoising methods, achieving very strong results on piecewise smooth images. Finally, we have also considered a real image denoising setting, showing that the proposed method can provide a significant performance gain.
Future work will focus on extending the proposed architecture to other inverse problems, such as super-resolution \cite{dong2014learning,bordone2019deepsum}.


\bibliographystyle{IEEEtran}

\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{lukavs2006digital}
J.~Luk{\'a}{\v{s}}, J.~Fridrich, and M.~Goljan, ``Digital camera identification
  from sensor pattern noise,'' \emph{IEEE Transactions on Information Forensics
  and Security}, vol.~1, no.~2, pp. 205--214, 2006.

\bibitem{valsesia2015compressed}
D.~{Valsesia}, G.~{Coluccia}, T.~{Bianchi}, and E.~{Magli}, ``Compressed
  fingerprint matching and camera identification via random projections,''
  \emph{IEEE Transactions on Information Forensics and Security}, vol.~10,
  no.~7, pp. 1472--1485, July 2015.

\bibitem{romano2017little}
Y.~Romano, M.~Elad, and P.~Milanfar, ``The little engine that could:
  Regularization by denoising (red),'' \emph{SIAM Journal on Imaging Sciences},
  vol.~10, no.~4, pp. 1804--1844, 2017.

\bibitem{sun2019block}
Y.~Sun, J.~Liu, and U.~S. Kamilov, ``Block coordinate regularization by
  denoising,'' \emph{arXiv preprint arXiv:1905.05113}, 2019.

\bibitem{dabov2007image}
K.~Dabov, A.~Foi, V.~Katkovnik, and K.~Egiazarian, ``{Image denoising by sparse
  3-D transform-domain collaborative filtering},'' \emph{{IEEE Transactions on
  Image Processing}}, vol.~16, no.~8, pp. 2080--2095, 2007.

\bibitem{zhang2017beyond}
K.~Zhang, W.~Zuo, Y.~Chen, D.~Meng, and L.~Zhang, ``{Beyond a Gaussian
  denoiser: residual learning of deep CNN for image denoising},'' \emph{IEEE
  Transactions on Image Processing}, vol.~26, no.~7, pp. 3142--3155, 2017.

\bibitem{mao2016image}
X.~Mao, C.~Shen, and Y.-B. Yang, ``Image restoration using very deep
  convolutional encoder-decoder networks with symmetric skip connections,'' in
  \emph{Advances in Neural Information Processing Systems 29}, D.~D. Lee,
  M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Curran Associates, Inc., 2016, pp. 2802--2810.

\bibitem{tai2017memnet}
Y.~Tai, J.~Yang, X.~Liu, and C.~Xu, ``Memnet: A persistent memory network for
  image restoration,'' in \emph{Proceedings of the IEEE international
  conference on computer vision}, 2017, pp. 4539--4547.

\bibitem{bae2017beyond}
W.~Bae, J.~J. Yoo, and J.~C. Ye, ``Beyond deep residual learning for image
  restoration: Persistent homology-guided manifold simplification,'' in
  \emph{IEEE Conference on Computer Vision and Pattern Recognition Workshops
  (CVPRW)}, July 2017, pp. 1141--1149.

\bibitem{cruz2018nonlocality}
C.~Cruz, A.~Foi, V.~Katkovnik, and K.~Egiazarian, ``Nonlocality-reinforced
  convolutional neural networks for image denoising,'' \emph{IEEE Signal
  Processing Letters}, vol.~25, no.~8, pp. 1216--1220, Aug 2018.

\bibitem{lefkimmiatis2018universal}
S.~Lefkimmiatis, ``{Universal denoising networks: a novel CNN architecture for
  image denoising},'' in \emph{Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition}, 2018, pp. 3204--3213.

\bibitem{plotz2018neural}
T.~Pl{\"o}tz and S.~Roth, ``Neural nearest neighbors networks,'' in
  \emph{Advances in Neural Information Processing Systems}, 2018, pp.
  1087--1098.

\bibitem{liu2018non}
D.~Liu, B.~Wen, Y.~Fan, C.~C. Loy, and T.~S. Huang, ``Non-local recurrent
  network for image restoration,'' in \emph{Advances in Neural Information
  Processing Systems}, 2018, pp. 1673--1682.

\bibitem{ValsesiaICIP19}
D.~Valsesia, G.~Fracastoro, and E.~Magli, ``Image denoising with
  graph-convolutional neural networks,'' in \emph{2019 26th IEEE International
  Conference on Image Processing (ICIP)}, 2019.

\bibitem{henaff2015deep}
M.~Henaff, J.~Bruna, and Y.~LeCun, ``Deep convolutional networks on
  graph-structured data,'' \emph{arXiv preprint arXiv:1506.05163}, 2015.

\bibitem{defferrard2016convolutional}
M.~Defferrard, X.~Bresson, and P.~Vandergheynst, ``Convolutional neural
  networks on graphs with fast localized spectral filtering,'' in
  \emph{Advances in Neural Information Processing Systems}, 2016, pp.
  3844--3852.

\bibitem{kipf2016semi}
T.~N. Kipf and M.~Welling, ``Semi-supervised classification with graph
  convolutional networks,'' in \emph{International Conference on Learning
  Representations (ICLR) 2017}, 2017.

\bibitem{shuman2013emerging}
D.~Shuman, S.~Narang, P.~Frossard, A.~Ortega, and P.~Vandergheynst, ``The
  emerging field of signal processing on graphs: Extending high-dimensional
  data analysis to networks and other irregular domains,'' \emph{IEEE Signal
  Processing Magazine}, vol.~3, no.~30, pp. 83--98, 2013.

\bibitem{schlichtkrull2018modeling}
M.~Schlichtkrull, T.~N. Kipf, P.~Bloem, R.~Van Den~Berg, I.~Titov, and
  M.~Welling, ``Modeling relational data with graph convolutional networks,''
  in \emph{European Semantic Web Conference}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2018, pp. 593--607.

\bibitem{simonovsky2017dynamic}
M.~Simonovsky and N.~Komodakis, ``Dynamic edge-conditioned filters in
  convolutional neural networks on graphs,'' in \emph{IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, July 2017, pp. 29--38.

\bibitem{wang2018dynamic}
Y.~Wang, Y.~Sun, Z.~Liu, S.~E. Sarma, M.~M. Bronstein, and J.~M. Solomon,
  ``Dynamic graph cnn for learning on point clouds,'' \emph{arXiv preprint
  arXiv:1801.07829}, 2018.

\bibitem{xu2018powerful}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka, ``How powerful are graph neural
  networks?'' in \emph{International Conference on Learning Representations
  (ICLR) 2019}, 2019.

\bibitem{monti2017geometric}
F.~Monti, D.~Boscaini, J.~Masci, E.~Rodola, J.~Svoboda, and M.~M. Bronstein,
  ``Geometric deep learning on graphs and manifolds using mixture model cnns,''
  in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2017, pp. 5115--5124.

\bibitem{verma2018feastnet}
N.~Verma, E.~Boyer, and J.~Verbeek, ``Feastnet: Feature-steered graph
  convolutions for 3d shape analysis,'' in \emph{Proceedings of the IEEE
  Conference on Computer Vision and Pattern Recognition}, 2018, pp. 2598--2606.

\bibitem{valsesia2019learning}
D.~Valsesia, G.~Fracastoro, and E.~Magli, ``Learning localized generative
  models for 3d point clouds via graph convolution,'' in \emph{International
  Conference on Learning Representations (ICLR) 2019}, 2019.

\bibitem{rudin1992nonlinear}
L.~I. Rudin, S.~Osher, and E.~Fatemi, ``Nonlinear total variation based noise
  removal algorithms,'' \emph{Physica D: nonlinear phenomena}, vol.~60, no.
  1-4, pp. 259--268, 1992.

\bibitem{tomasi1998bilateral}
C.~Tomasi and R.~Manduchi, ``Bilateral filtering for gray and color images.''
  in \emph{ICCV}, vol.~98, no.~1, 1998, p.~2.

\bibitem{buades2005non}
A.~Buades, B.~Coll, and J.-M. Morel, ``A non-local algorithm for image
  denoising,'' in \emph{2005 IEEE Computer Society Conference on Computer
  Vision and Pattern Recognition (CVPR'05)}, vol.~2.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2005, pp. 60--65.

\bibitem{gu2014weighted}
S.~Gu, L.~Zhang, W.~Zuo, and X.~Feng, ``Weighted nuclear norm minimization with
  application to image denoising,'' in \emph{Proceedings of the IEEE Conference
  on Computer Vision and Pattern Recognition}, 2014, pp. 2862--2869.

\bibitem{pang2017graph}
J.~Pang and G.~Cheung, ``{Graph Laplacian regularization for image denoising:
  analysis in the continuous domain},'' \emph{IEEE Transactions on Image
  Processing}, vol.~26, no.~4, pp. 1770--1785, 2017.

\bibitem{elad2006image}
M.~Elad and M.~Aharon, ``Image denoising via sparse and redundant
  representations over learned dictionaries,'' \emph{IEEE Transactions on Image
  Processing}, vol.~15, no.~12, pp. 3736--3745, Dec 2006.

\bibitem{chen2016trainable}
Y.~Chen and T.~Pock, ``Trainable nonlinear reaction diffusion: A flexible
  framework for fast and effective image restoration,'' \emph{IEEE transactions
  on pattern analysis and machine intelligence}, vol.~39, no.~6, pp.
  1256--1272, 2016.

\bibitem{burger2012image}
H.~C. Burger, C.~J. Schuler, and S.~Harmeling, ``{Image denoising: Can plain
  neural networks compete with BM3D?}'' in \emph{IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2012, pp. 2392--2399.

\bibitem{ioffe2015batchnorm}
\BIBentryALTinterwordspacing
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in \emph{Proceedings of the
  32Nd International Conference on International Conference on Machine Learning
  - Volume 37}, ser. ICML'15.\hskip 1em plus 0.5em minus 0.4em\relax JMLR.org,
  2015, pp. 448--456. [Online]. Available:
  \url{http://dl.acm.org/citation.cfm?id=3045118.3045167}
\BIBentrySTDinterwordspacing

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in
  \emph{2015 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, June 2015, pp. 1--9.

\bibitem{divakar2017image}
N.~Divakar and R.~V. Babu, ``{Image denoising via CNNs: an adversarial
  approach},'' in \emph{New Trends in Image Restoration and Enhancement, CVPR},
  2017.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{gong2019exploiting}
L.~Gong and Q.~Cheng, ``Exploiting edge features for graph neural networks,''
  in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2019, pp. 9211--9219.

\bibitem{cheng2015exploration}
Y.~Cheng, F.~X. Yu, R.~S. Feris, S.~Kumar, A.~Choudhary, and S.-F. Chang, ``An
  exploration of parameter redundancy in deep networks with circulant
  projections,'' in \emph{The IEEE International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{wu2016compression}
J.~Wu, ``Compression of fully-connected layer in neural network by kronecker
  product,'' in \emph{2016 Eighth International Conference on Advanced
  Computational Intelligence (ICACI)}, Feb 2016, pp. 173--179.

\bibitem{hinrichs2011johnson}
A.~Hinrichs and J.~Vyb{\'\i}ral, ``Johnson-lindenstrauss lemma for circulant
  matrices,'' \emph{Random Structures \& Algorithms}, vol.~39, no.~3, pp.
  391--398, 2011.

\bibitem{valsesia2017user}
D.~{Valsesia}, G.~{Coluccia}, T.~{Bianchi}, and E.~{Magli}, ``User
  authentication via prnu-based physical unclonable functions,'' \emph{IEEE
  Transactions on Information Forensics and Security}, vol.~12, no.~8, pp.
  1941--1956, Aug 2017.

\bibitem{valsesia2017binary}
D.~{Valsesia} and E.~{Magli}, ``Binary adaptive embeddings from order
  statistics of random projections,'' \emph{IEEE Signal Processing Letters},
  vol.~24, no.~1, pp. 111--115, Jan 2017.

\bibitem{pmlr-v9-glorot10a}
X.~Glorot and Y.~Bengio, ``Understanding the difficulty of training deep
  feedforward neural networks,'' in \emph{Proceedings of the Thirteenth
  International Conference on Artificial Intelligence and Statistics}, ser.
  Proceedings of Machine Learning Research, Y.~W. Teh and M.~Titterington,
  Eds., vol.~9.\hskip 1em plus 0.5em minus 0.4em\relax Chia Laguna Resort,
  Sardinia, Italy: PMLR, 13--15 May 2010, pp. 249--256.

\bibitem{combettes2011proximal}
P.~L. Combettes and J.-C. Pesquet, ``Proximal splitting methods in signal
  processing,'' in \emph{Fixed-point algorithms for inverse problems in science
  and engineering}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2011, pp.
  185--212.

\bibitem{MartinFTM01}
D.~Martin, C.~Fowlkes, D.~Tal, and J.~Malik, ``A database of human segmented
  natural images and its application to evaluating segmentation algorithms and
  measuring ecological statistics,'' in \emph{Proc. 8th Int'l Conf. Computer
  Vision}, vol.~2, July 2001, pp. 416--423.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{Hu2015multiresolution}
W.~{Hu}, G.~{Cheung}, A.~{Ortega}, and O.~C. {Au}, ``Multiresolution graph
  fourier transform for compression of piecewise smooth images,'' \emph{IEEE
  Transactions on Image Processing}, vol.~24, no.~1, pp. 419--433, Jan 2015.

\bibitem{Fracastoro2017graph}
G.~Fracastoro, D.~Thanou, and P.~Frossard, ``Graph-based transform coding with
  application to image compression,'' \emph{arXiv preprint arXiv:1712.06393},
  2017.

\bibitem{holst1998ccd}
G.~C. Holst, \emph{CCD arrays, cameras, and displays}.\hskip 1em plus 0.5em
  minus 0.4em\relax JCD Publishing SPIE Press, 1992.

\bibitem{plotz2017benchmarking}
T.~Plotz and S.~Roth, ``Benchmarking denoising algorithms with real
  photographs,'' in \emph{Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR)}, 2017, pp. 1586--1595.

\bibitem{SIDD_2018_CVPR}
A.~Abdelhamed, S.~Lin, and M.~S. Brown, ``A high-quality denoising dataset for
  smartphone cameras,'' in \emph{IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2018.

\bibitem{dabov2007color}
K.~Dabov, A.~Foi, V.~Katkovnik, and K.~O. Egiazarian, ``Color image denoising
  via sparse 3d collaborative filtering with grouping constraint in
  luminance-chrominance space.'' in \emph{ICIP (1)}, 2007, pp. 313--316.

\bibitem{Chen2015Efficient}
G.~{Chen}, F.~{Zhu}, and P.~A. {Heng}, ``An efficient statistical method for
  image noise level estimation,'' in \emph{2015 IEEE International Conference
  on Computer Vision (ICCV)}, Dec 2015, pp. 477--485.

\bibitem{dong2014learning}
C.~Dong, C.~C. Loy, K.~He, and X.~Tang, ``Learning a deep convolutional network
  for image super-resolution,'' in \emph{European conference on computer
  vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2014, pp. 184--199.

\bibitem{bordone2019deepsum}
A.~Bordone~Molini, D.~Valsesia, G.~Fracastoro, and E.~Magli, ``Deepsum: Deep
  neural network for super-resolution of unregistered multitemporal images,''
  \emph{arXiv preprint arXiv:1907.06490}, 2019.

\end{thebibliography}



\end{document}
