

\documentclass[11pt,letterpaper,oneside,english]{article}
\usepackage[margin=1.4in]{geometry}  

\usepackage{mdwlist}



\usepackage{amssymb,amsbsy,latexsym,amsthm}
\usepackage{amsmath}
\usepackage{graphics, subfigure, float}
\usepackage{fp, calc}
\usepackage{bm}


\usepackage[latin1]{inputenc}
\usepackage[american]{babel}
\usepackage[T1]{fontenc} 

\def\url#1{{\texttt #1}}

\usepackage{bm}



\usepackage{amscd} 









\usepackage{pst-all}
\usepackage{verbatim, comment}

\newtheoremstyle{theorem}{1em}{1em}{\slshape}{0pt}{\bfseries}{.}{ }{}
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{question}[theorem]{Question}
\newtheorem{claim}[theorem]{Claim}






\providecommand{\setN}{\mathbb{N}}
\providecommand{\setZ}{\mathbb{Z}}
\providecommand{\setQ}{\mathbb{Q}}
\providecommand{\setR}{\mathbb{R}}

\newcommand{\conv}{\textrm{conv}}




        \def\drawRect#1#2#3#4#5{
           \FPeval{\x2}{(#2) + #4} 
           \FPeval{\y2}{(#3) + #5} 
           \pspolygon[#1](#2,#3)(\x2,#3)(\x2,\y2)(#2,\y2)
        }

\usepackage[displaymath,textmath,graphics, subfigure, floats]{preview} \PreviewEnvironment{center} 
\PreviewEnvironment{pspicture} 
 \usepackage{datetime}

\makeatother

\usepackage{calrsfs} \DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}







\title{A -Approximation for Makespan Scheduling with Precedence Constraints using LP Hierarchies\footnote{The conference version of this work appeared in the 48th ACM Symposium on Theory of Computing (STOC 2016).}}
\date{} 

\author{Elaine Levey\thanks{University of Washington, Seattle, USA. Email: elevey@cs.washington.edu} \and Thomas Rothvo{ss}\thanks{University of Washington, Seattle, USA. Email: rothvoss@uw.edu. Supported by NSF grant 1420180 with title ``\emph{Limitations of convex relaxations in combinatorial optimization}'' and an Alfred P. Sloan Research Fellowship.}}

\begin{document}

\maketitle


\begin{abstract}
\noindent 
In a classical problem in scheduling, 
one has  unit size jobs with a \emph{precedence order} and the goal is to find 
a schedule of those jobs on   identical machines as to minimize the makespan. It is one of the remaining four open problems from the book of Garey \& Johnson whether or not this problem is -hard for . 
 
We prove that for any fixed  and , an LP-hierarchy lift of the time-indexed LP with a slightly super poly-logarithmic number of  rounds provides a -approximation. 
For example Sherali-Adams suffices as hierarchy. This implies an algorithm that yields a -approximation in time . 
The previously best approximation algorithms guarantee
a -approximation in polynomial time for  and  for . 
Our algorithm is based on a recursive scheduling approach where in each step we reduce the correlation in form of long chains. Our method adds to the rather short list of examples where hierarchies are actually useful to obtain better approximation algorithms. 
\end{abstract}



\section{Introduction}

One of the landmarks in the theory of scheduling is the paper of Graham~\cite{Graham66} from 1966,
dealing with the following problem: suppose we have a set  of  jobs, each one
with a running time  along with  identical parallel machines that we can use to
process the jobs. Moreover, the input contains a \emph{precedence order} 
on the jobs; we write  if job  has to be completed before job 
can be started. The goal is to schedule the jobs in a non-preemptive fashion so that the
\emph{makespan} is minimized. Here, the makespan gives the time that the last job is finished.
In the \emph{3-field notation}\footnote{In the 3-field notation, the first field specifies the available processors, the 2nd field the jobs and the last field the objective function. In our case,  means that we 
have  identical machines;  indicates that the jobs have unit length and precedence constraints and the last field  specifies that the objective function is to minimize the maximum completion time.}, this problem is abbreviated as . Graham showed that the following \emph{list schedule} gives a -approximation on the makespan: compute an arbitrary topological ordering of the jobs and whenever a machine becomes idle, select the first available job from the list. It had been known since the late 70's that it is -hard to approximate the problem better than within a factor of  due to Lenstra and Rinnooy Kan~\cite{ComplexityOfScheduling-Lenstra-RinnoyKan1978} and Schuurman and Woeginger~\cite{TenOpenProblems-SchuurmanWoeginger1991} prominently placed the quest for any improvement on their well known list of 10 open problems in scheduling. Finally in 2010, Svensson~\cite{HardnessPrecedenceScheduling-Svensson-STOC2010} showed that assuming a variant of the unique games conjecture~\cite{OptLongCodeTest-BansalKhot-FOCS09}, there is no -approximation algorithm for . 
However, for unit size jobs, Lam and Sethi~\cite{TwoSchedulingAlgorithms-Lam-Sethi-SICOMP77} analyzed an
algorithm of Coffman and Graham and showed that it provides a slighly better guarantee of   for . Later, Gangal and Ranade~\cite{PrecedenceConstrainedScheduling-GangalRanade2008} gave an algorithm with a  guarantee for . 
 

In a typical scheduling application, the number of jobs might be huge compared to the number of machines, 
which does justify to ask for the complexity status of such problems if the number  of machines
is a constant. Even under the additional restriction of unit size jobs, no better approximation result
is known. In fact, it is one of the remaining four open problems from the book of 
Garey and Johnson~\cite{GareyJohnson79}
whether  is even -hard. Also
Schuurman and Woeginger~\cite{TenOpenProblems-SchuurmanWoeginger1991} list under ``Open Problem 1'' the question whether there is a PTAS for this
problem (recall that for , the result of \cite{TwoSchedulingAlgorithms-Lam-Sethi-SICOMP77} gives an optimum schedule). 


To understand where the lack of progress is coming from, one has to go back to the 
list scheduling algorithm of Graham. If we schedule the jobs in a greedy manner, then one can argue 
that there is always a \emph{chain} of jobs  so that at any point in time
either all  machines are fully busy or a job from that chain was processed. Since both quantities, 
the \emph{load}  and the length of any chain are lower bounds on any
schedule, we can conclude that the schedule has length at most . 
One can shave off a factor of  even for general running times, by observing that the 
processing times of the 
jobs in the longest chain do not need to be again counted in the load bound. 
Also the papers \cite{TwoSchedulingAlgorithms-Lam-Sethi-SICOMP77} and \cite{PrecedenceConstrainedScheduling-GangalRanade2008} effectively rely on those two lower bounds. 
In fact, \cite{TechReportCharikar1995} showed that a large class of algorithms including the
ones of \cite{Graham66,PrecedenceConstrainedScheduling-GangalRanade2008} cannot beat 
a bound of ; moreover Graham's algorithm is indeed not better than
a -approximation for unit size jobs, see \cite{PrecedenceConstrainedScheduling-GangalRanade2008}.


Of course, one always has the option to study the strength of linear programs for an optimization problem.
The most natural one for  is certainly the following
\emph{time-indexed LP}: 
For a parameter  that denotes the length of the time horizon, we define a set  as the set of fractional solutions to:

Here  is a decision variable that is supposed to tell whether job  is scheduled in 
time slot , where . The constraints guarantee that in an integral solution each job is assigned
to one time slot; no time slot receives more than  jobs and for a pair of jobs , job 
has to be scheduled before . 


Unsurprisingly, this LP has a constant integrality gap as one can see from the following construction: 
take  blocks  of  jobs each and define the precedence order so
that all the jobs in  have to be finished before any job in  can be started. 
Any integral schedule needs two time units per block, hence . On the other hand, the LP
solution can schedule the  jobs of each block ``in parallel'', each at a rate of 
and finish the schedule after  time units in which each machine has always
been fully busy. This results in an integrality gap of at least .

It has been long known, that in principle one can take the linear program for any optimization problem and 
strengthen it automatically by applying an LP or SDP hierarchy lift.
We will provide formal definitions later, but basically these operators ensure that for any
set of at most  variables, the LP solution indeed lies in the convex hull of integral combinations.
Here,  is the number of \emph{levels} or \emph{rounds} and one typically needs
time  to solve an -level hierarchy.


Some known approximation results have been reinterpreted in hindsight in this framework, for 
example a constant number of Lasserre rounds applied to a basic LP suffices for the 
Goemans-Williamson algorithm for MaxCut~\cite{MaxCut-GoemansWilliamson-JACM95} and also a constant 
number of Lasserre rounds
implies the triangle inequalities in the -approximation algorithm by 
Arora, Rao and Vazirani~\cite{SparsestCut-AroraRaoVazirani-JACM09}. Moreover, the
subspace enumeration component in the subexponential time algorithm of Arora, Barak and
Steurer~\cite{UniqueGamesAlgo-AroraBarakSteurer-FOCS10} for Unique Games could be replaced with a Lasserre SDP.
However, there are relatively few results where hierarchies
have been genuinely useful (at least fewer than researchers have hoped for). 
For example Chlamt{\'a}{\v c}~\cite{ApproxAlgoViaSDP-Chlamtac-FOCS07}
used SDP hierarchies to find better colorings in 3-colorable graphs and Raghavendra and 
Tan~\cite{CSPs-with-card-constraints-RaghavendraTanSODA12} apply them to obtain approximation
algorithms for CSPs with cardinality constraints. An application to color
hypergraphs can be found in~\cite{HypergraphColoringChlamtacSinghAPPROX08}.
Hierarchies also turned out to be the
right approach for Sparsest Cut in bounded tree width graphs, see the paper by 
Chlamt{\'a}{\v c}, Krauthgamer and Raghavendra~\cite{SparsestCutInBoundedTreeWidthGraphs-CKR-APPROX2010} 
and the 2-approximation by Gupta, Talwar and Witmer~\cite{SparsestCut-GuptaTalwarWitmer-STOC2013}.
For an application of the Lasserre hierarchy in the context of scheduling, see the recent work 
of Bansal, Srinivasan and Svensson~\cite{LiftAndRound-BansalSrinivasanSvensson-STOC2016}.
Throughout this paper, logarithms will be with respect to base 2, that means .







\subsection{Our Contribution}

Our main result is that an LP lift with 

rounds closes the integrality gap of LP~\eqref{eq:LP} to at most . This implies:
\begin{theorem}
For the problem  one can compute a -approximate
solution in time  where . \end{theorem}
This gives a partial answer to one of the questions under ``Open Problem 1'' in \cite{TenOpenProblems-SchuurmanWoeginger1991}
which asked whether there is a PTAS for this problem. 
In a Dagstuhl workshop, Mathieu~\cite{DagstuhlOpenProblems2010} asked the more specific 
question whether the Sherali-Adams hierarchy gives a -approximation 
after  rounds. We also make progress on the question from the book of Garey and 
Johnson~\cite{GareyJohnson79}
by improving the -polynomial time approximation for ~\cite{TwoSchedulingAlgorithms-Lam-Sethi-SICOMP77}
to a  in slightly more than quasi-polynomial time. In particular, this implies that    is
not -hard, assuming that .








\section{An Explicit LP Hierarchy for Makespan Scheduling}

In principle, our result can be obtained by applying the well-known Sherali-Adams
hierarchy to the linear program in \eqref{eq:LP} --- of course the same still
holds true for even more powerful hierarchies such as the Lasserre SDP hierarchy. 
While this may be the preferable option for experts, we will work with 
an explicit strengthening of the above linear program that hopefully will be more
accessible to non-experts in LP hierarchies. For a set  we denote
 
as the \emph{convex cone} that is spanned by .

Let us fix a
parameter .
Let  be a \emph{partial assignment} that
assigns slots
only for a subset of jobs. All the jobs with  are unassigned.
Let  be the support of that partial
assignment. We denote  as the partial assignment that assigns
no job at all.
Moreover for a partial assignment  and  and ,
let  be the partial assignment augmented by
.


We say that a solution to  is a set of vectors 
,
where we define 
satisfying the following program:

In other words,  is a collection of  many vectors 
that each has dimension .
Note that if  is a non-zero vector, then it can be scaled to be a 
fractional solution in  that has all assignments of the partial assignment 
integral. 
Notice that we have a variable for each  with , 
so one can find a feasible solution of the program in  time. 

One can think of this system as basically being the Sherali-Adams system, just that 
we do include more redundant variables that will make it easy to prove the needed
properties. 
First, we claim that if there exists a valid schedule , then
. Here we can build a valid solution
by simply choosing  as the
characteristic vector of 
if  and  agree. We set  if there
is a job  so that .
We give the following useful properties:

\begin{lemma}\label{lem:HierarchyProperties}
Fix some . Let . Let\footnote{Here  is any fixed job. But note that this definition does not depend on
the choice of .} . Then the following holds
\begin{enumerate}
\item[a)]  If , then
.
\item[b)] If , then 
.
\item[c)] Let  and  so that .
Then taking  for each , one
has  and .
Moreover,   for all  and .
\end{enumerate}
\end{lemma}

\begin{proof}
We prove the following:
\begin{enumerate}
\item[a)] Follows from  and the definition of .
\item[b)] We can iteratively apply (I) to obtain 
 
By a),
 are  vectors.
\item[c)] From the definition we can see that  are just
inherited.  and 
follow from the scaling. The implication 
follows from .
\end{enumerate}
\end{proof}
If we have solution 
and variables  with , then \emph{conditioning on } means to replace the solution  with the solution 
 described in Lemma~\ref{lem:HierarchyProperties}.c.






\section{An Overview}\label{sec:Overview}

In this section, we will give an overview over the different steps in our algorithm; the detailed 
implementation of some of the steps will be given in Section~\ref{sec:ReducingDependence}, Section~\ref{sec:SchedulingTopJobs} and Section~\ref{sec:AccountingDiscardedJobs}. 
For a given time horizon , a \emph{feasible schedule} is an assignment 
with  for all  and for all  one has .
Formally, our main technical theorem is as follows: \begin{theorem} \label{thm:MainTechnicalTheorem}
For any solution  with 
,
one can find a feasible schedule  of the jobs in 
time  so that 

\end{theorem}
To obtain a -approximation, we can find the minimum value of  so that  
with binary search and then compute a solution 
. In particular, by virtue of being a relaxation, that value of  will satisfy , where 
 is the makespan of the optimum schedule. For the sake of a simpler notation, we will assume that 
is a power of 2 --- if  for some integer , then one can add  many dummy
jobs that all depend on each original job so that the algorithm will schedule the dummy jobs  at the
very end. Moreover we will assume that  as otherwise the bound is meaningless.


The main routine of our algorithm will schedule jobs only within the time horizon  of the LP-hierarchy solution, but we will allow it to \emph{discard} jobs. Formally this
means, we will find an assignment  that will not have assigned slots to jobs 
in . Such an assignment will still be called ``feasible'' if apart from the load bound, 
the condition  is satisfied for all . 
In particular dependencies with discarded jobs play no role in this definition. 

The reason for this definition is that one can easily insert the discarded jobs at the very end of the algorithm: 
\begin{lemma} \label{lem:InsertingDiscardedJobs}
Any feasible schedule 
 
can be modified in polynomial time 
to a feasible schedule 

which also includes the previously discarded jobs. 
\end{lemma}
\begin{proof} 
Select any job . Since  is a valid schedule which respects all precedence
constraints in , there must be a time  so that all jobs  have  and 
all jobs  with  have . Then we insert an extra time unit after time ; in
this extra time slot, we only process . We continue the procedure with inserting the next job from .
\end{proof}



Now, let us introduce some notation:
We can imagine the precedence order ``'' as a directed transitive graph  with the nodes
as jobs and edges . In that view, let  be the jobs 
depending on  and let   be the jobs on which  depends. 
Note that  and  are always distinct. We abbreviate  
as the jobs that have any dependency with . Finally, for a subset of jobs , 
let  be the 
maximum degree of a node in the subgraph induced by , counting also the node itself. 

We partition the time horizon  into a balanced binary family  of intervals of lengths . 
Let  be the \emph{binary laminar family of intervals} that
we obtain by repeatedly partitioning
intervals into two equally-sized subintervals.
Recall that each \emph{level}  contains 
many interval ; each one consisting
of  many time units. 
For each job  and each interval , we now define 
, which denotes how much 
of job  will be scheduled somewhere within 
that interval .


Our algorithm will schedule the jobs in  a recursive manner. The main claim is that for any interval ,  LP-hierarchy solution  and a set of jobs  with 
we can schedule almost all jobs from  within  while respecting all precedence constraints. 

We use parameters  where  is a large 
enough constant that we will choose 
in Section~\ref{sec:AccountingDiscardedJobs}, and .
To get some intuition for the parameters, considering  and  as fixed constants, 
one would have  and .
Formally, the main technical lemma is the following: \begin{lemma} \label{lem:MainRecursiveAlgorithm}
Fix . Let  be an interval from the balanced family of length .
Let  be an LP-hierarchy solution with 

Let
.
Then one can find a feasible assignment  
that discards only

many jobs.
\end{lemma}


Before we move on to explain the procedure behind Lemma~\ref{lem:MainRecursiveAlgorithm}, we 
want to argue that it implies our main result, Theorem~\ref{thm:MainTechnicalTheorem}: 
\begin{proof}
We set  and , then  is a valid choice as trivially 
for any job. To satisfy the requirement of Lemma~\ref{lem:MainRecursiveAlgorithm} we need

many levels of the hierarchy. Here we use that , 
hence  and 
(we want to point out that many of the lower order terms are absorbed into the -notation
of the exponent and we assume that ).
Then Lemma~\ref{lem:MainRecursiveAlgorithm} returns a valid assignment  
that discards only

many jobs. Inserting those discarded jobs via Lemma~\ref{lem:InsertingDiscardedJobs} then
results in a feasible schedule of makespan at most  .
\end{proof}

The rest of the manuscript will be devoted to proving Lemma~\ref{lem:MainRecursiveAlgorithm}. 
We fix a constant  as the target value for our approximation ratio
and denote  as the length of our interval.

Let us first argue how to handle the base case, which for us is if . 
In that case, we have at most  jobs. Hence, the 
LP-hierarchy lift has   many levels and one can repeatedly condition on events
 for  and  until one arrives at an LP hierarchy solution 
with  for all . This then represents a valid schedule of
jobs  in the interval  without the need to discard any jobs.


We now come to a high-level description of the algorithm. 
Let  be the family of
subintervals of , where  contains  intervals of length 
each, see Figure~\ref{fig:DissectionForIStar}. 
For a job , we define  as the level that \emph{owns} the job in the current LP-hierarchy solution. 
We also abbreviate  as 
all jobs owned by level . The algorithm is as follows:
\begin{itemize}
\item {\bf Step 1:} Starting with the LP-hierarchy solution , we can iteratively condition on events 
until we arrive at a solution 
that has the property that for any interval , the jobs owned by that interval have small dependence degree, that means , where .
If we then consider the set of jobs  
owned by the first  levels, the longest chain in  will contain at most  jobs.
We will show in Section~\ref{sec:ReducingDependence} that the number of required conditionings can be upperbounded by , 
which implies that . \item {\bf Step 2:} From now on, we work with the modified LP-hierarchy solution . We select a 
level index  and partition the jobs in  in three different groups: 
  \begin{itemize}
  \item The jobs on the top levels: 
  \item The jobs on the  middle levels: 
  \item The jobs on the bottom levels: 
  \end{itemize}
Then we discard all jobs in . In Section~\ref{sec:AccountingDiscardedJobs} we will describe how the
index  is chosen and in particular we will provide an upper bound on the number of 
discarded middle jobs. \item {\bf Step 3:} In this step, we will find a schedule for the bottom jobs. 
For this purpose, we call Lemma~\ref{lem:MainRecursiveAlgorithm} \emph{recursively} for each interval  
with a copy of the solution  and jobs . Here it is crucial that the intervals are disjoint but also 
the sets  are disjoint for different intervals .
Then Lemma~\ref{lem:MainRecursiveAlgorithm} returns a valid schedule of the form  for each interval .
Let  be the union of jobs that were discarded
in those calls. The partial schedules  satisfy  for  and 
for . We combine those schedules to a schedule  
 
From the disjointness of the intervals, it is clear that again   for all . 
Moreover, if
 and  for some interval , then by the inductive hypothesis . 
On the other hand, if  and  then we know by Lemma~\ref{lem:HierarchyProperties}.c that  had to come before  since . 
\item {\bf Step 4:} We continue working with the previously constructed schedule    that schedules 
the non-discarded bottom jobs. In this step, we will extend the schedule  and insert the jobs 
of  in the remaining free slots. We will prove in Section~\ref{sec:SchedulingTopJobs} that this can be done 
without changing the position of any scheduled bottom job and without violating any precedence constraints. 
Again, we allow that the procedure discards a small number of additional jobs 
from  that we will account for later. Eventually, the schedule  satisfies the claim 
for Lemma~\ref{lem:MainRecursiveAlgorithm}.
\end{itemize}



\begin{figure}
\begin{center}\psset{xunit=0.6cm,yunit=0.6cm}
\begin{pspicture}(-2,-2)(15,9.3)
\drawRect{linewidth=1.0pt,fillstyle=solid,fillcolor=lightgray}{0}{-1}{16}{10}
\drawRect{linewidth=1.5pt,fillstyle=solid,fillcolor=lightgray}{0}{6}{16}{3}
\drawRect{linewidth=1.5pt,fillstyle=vlines,fillcolor=lightgray,hatchcolor=darkgray}{0}{3}{16}{3}
\drawRect{linewidth=0.5pt,fillstyle=none,fillcolor=lightgray}{0}{7}{8}{1}
\drawRect{linewidth=0.5pt,fillstyle=none,fillcolor=lightgray}{8}{7}{8}{1}
\multido{\N=0+4}{4}{\drawRect{linewidth=0.5pt,fillstyle=none,fillcolor=lightgray}{\N}{6}{4}{1}}
\multido{\N=0+1}{16}{\drawRect{linewidth=0.5pt,linestyle=solid,fillstyle=none,fillcolor=lightgray}{\N}{2}{1}{1}}
\multido{\N=0+1}{9}{\drawRect{linewidth=0.5pt,fillstyle=none,fillcolor=lightgray}{0}{\N}{16}{1}}
\rput[c](8,4.75){}
\rput[c](-1.2,8.5){}
\rput[c](-1.2,7.5){}
\rput[c](8,1.75){}
\rput[c](-1.2,6.5){}
\rput[c](-1.2,5.5){}
\rput[c](-1.2,4.5){}
\rput[c](-1.2,3.5){}
\rput[c](-1.2,2.5){}
\rput[c](-1.2,1.5){}
\rput[c](-1.2,0.5){}
\rput[c](-1.2,-0.5){}
\psbrace[nodesepB=5pt,braceWidthInner=4pt,braceWidthOuter=4pt](17,0)(17,9){}
\psline{->}(0,-2)(18,-2) \multido{\N=0+1}{17}{\psline(\N,-1.8)(\N,-2.2)}
\rput[c](0.5,-1.5){}
\rput[c](1.5,-1.5){}
\rput[c](2.5,-1.5){}
\rput[c](15.5,-1.5){}
\rput[c](17.5,-1.5){time}
\psbrace[nodesepB=5pt,braceWidthInner=4pt,braceWidthOuter=4pt,rot=180,nodesepA=-18pt](-2.5,9)(-2.5,6){}
\psbrace[nodesepB=5pt,braceWidthInner=4pt,braceWidthOuter=4pt,rot=180,nodesepA=-29pt](-2.5,6)(-2.5,3){}
\psbrace[nodesepB=5pt,braceWidthInner=4pt,braceWidthOuter=4pt,rot=180,nodesepA=-32pt](-2.5,3)(-2.5,-2){}
\end{pspicture}
\end{center}
\caption{Binary dissection of the interval  used in the algorithm behind Lemma~\ref{lem:MainRecursiveAlgorithm}.\label{fig:DissectionForIStar}}
\end{figure}


The intuition behind the algorithm is as follows:  When we call the procedure recursively for intervals 
we cannot control where the jobs  will be scheduled within that interval . In particular the
decisions made in different intervals  will in general not be consistent.
But the discarding of the middle jobs creates a gap between the top 
jobs and the bottom jobs in the sense that the intervals of the top jobs are at least a factor 
longer than intervals of the bottom jobs. For a top job  we will be pessimistic and
assume that all the bottom jobs that  depends on will be scheduled just at the very end of their
interval. Still, as those intervals are very short, we will be able to argue that the loss in the
flexibility is limited and most of the top jobs can be processed. As a second crucial ingredient, the
conditioning had the implication that the top jobs do not contain any long chains any more.
This will imply that a greedy
schedule of the top jobs will leave little idle time, resulting in only few discarded top jobs.



A high-level pseudo-code description of the whole scheduling algorithm can be found in Figure~\ref{fig:MainAlgorithm}:
\begin{figure}
\begin{center}
\psframebox{
\begin{minipage}{14.3cm}
 \vspace{1mm} \hrule  \vspace{1mm}
{\bf Input:} Scheduling instance , parameters  and  \\
{\bf Output:} -approximate schedule  \vspace{1mm} \hrule  \vspace{1mm}
\begin{enumerate*}
\item[(1)] Compute a solution  with  and  minimal
\item[(2)] Call 
\item[(3)] Insert discarded jobs into schedule 
\end{enumerate*}
\vspace{1mm} \hrule \vspace{1mm} \hrule  \vspace{1mm}
 \vspace{1mm} \hrule  \vspace{1mm}
{\bf Input:} Jobs , LP lift , interval  with  for  \\
{\bf Output:} Schedule  with some jobs discarded \vspace{1mm} \hrule  \vspace{-2mm}
\begin{enumerate*}
\item[(1)] Build binary family of intervals 
\item[(2)] Call  
\item[(3)] Select partition into top, middle, bottom jobs. Pick . 
\item[(4)] Discard middle jobs.
\item[(5)] For each  set  and \\ call 
\item[(6)] Combine 's to one schedule 
\item[(7)] Call two-phased algorithm based on matching and EDF to insert top jobs into 
\end{enumerate*}
\end{minipage}}
\end{center}
\caption{High-level description of main algorithm\label{fig:MainAlgorithm}.}
\end{figure}





















\section{Step (1) --- Reducing Dependence} \label{sec:ReducingDependence}


In this section we will implement ``Step (1)'' and show that we can reduce the maximum dependence degrees of 
the jobs owned by the first  levels in order to bound the length of chains. 
We will do this by conditioning on up to
 many variables.
We are considering an interval  and 
a subset of jobs  that the vector  from the current LP-hierarchy solution  fully
schedules within . Recall that for one of the subintervals  
below , we write 
as the jobs owned by that particular interval.
\begin{lemma} \label{lem:BreakingChains}
Let .
Then one can find an induced solution 
with 
so that  for all
intervals .
\end{lemma}
\begin{proof}
We set initially . If there is any interval  with
, then we must have a job

that has either  or 
. We assume that 
 holds and omit the other case, which is symmetric.
Then we pick a time  with  and
 replace  by the LP-hierarchy solution conditioned on the
event ``''. Note that this
means that all jobs in  will be removed
from . In fact, each such job will be moved to  where 
is some subinterval. The conditioning can also change the owning interval of
other jobs, but for each job , the set  of times  such that 
can only \emph{shrink} if we condition on
any event, see Lemma~\ref{lem:HierarchyProperties}.c. 
Hence jobs only move from intervals to subintervals. 


Since in each iteration, at least  many
jobs ``move'' and each job moves at most 
 many
times out of an interval
in , we need to condition at most 

many times.
\end{proof}


The implication of Lemma~\ref{lem:BreakingChains} is that the set of jobs owned by
intervals  will not contain long chains,
simply because we have only few intervals and none of jobs owned
by a single interval contain long chains anymore.
\begin{lemma} \label{lem:BoundedChains}
After applying Lemma~\ref{lem:BreakingChains}, the longest chain within
jobs owned by intervals 
has length at most .
\end{lemma}
\begin{proof}
First, let us argue how many jobs a chain can have that are all assigned
to intervals of the same level
. From each interval , the chain can only include  many jobs.
Since , the total number of jobs from
level  is bounded by .
The claim follows from the pigeonhole principle and the fact that we have 
many levels in .
\end{proof}


We can summarize the algorithm from Lemma~\ref{lem:BreakingChains} as follows: 
\begin{center}
\psframebox{
\begin{minipage}{14.1cm}
{\sc Breaking long chains} \vspace{1mm} \hrule  \vspace{1mm}
{\bf Input:} Scheduling instance with jobs , a precedence order, an LP-hierarchy solution , an interval  \\
{\bf Output:} An LP-hierarchy solution  with maximum chain length  in    \vspace{1mm} \hrule \vspace{2mm}
\begin{enumerate*}
\item[(1)] Make a copy 
\item[(2)] WHILE  WITH  
 DO \begin{enumerate*}
      \item[(3)] Choose a job  with . 
      \item[(4)] If  THEN condition in  on  for some \\
ELSE condition on  for some . 
      \end{enumerate*} 
\end{enumerate*}
\end{minipage}}
\end{center}
Note that after each conditioning in step (6), the solution  will change and the 
set  will be updated. 




\section{Step (4) --- Scheduling Top Jobs}\label{sec:SchedulingTopJobs}


Consider the algorithm from Section~\ref{sec:Overview} and the state at the end of
Step 3. At this point, we have a schedule  that schedules most of the bottom jobs.
The main argument that remains to be shown is how to add in the top jobs
which are owned by intervals in .


This is done in two steps.
First, we use a \emph{matching-based} argument to show that most top jobs can be inserted 
in the existing schedule so that
the precedence constraints with the bottom jobs are respected. In this step, we will be 
discarding up to   many jobs. More crucially, the schedule
will not have satisfied precedence constraints within .
In a 2nd step, we temporarily remove the top jobs from the schedule and reinsert them 
with a variant of the \emph{Earliest Deadline First (EDF)} scheduling. As we will see later in Theorem~\ref{thm:EDF}, this results in at most 
  additionally discarded jobs. 


\subsection{A Preliminary Assignment of Top Jobs}









Let us recall what we did so far. In Step 3, we applied Lemma~\ref{lem:MainRecursiveAlgorithm} 
recursively on each interval  to schedule the bottom jobs. We already
argued that the resulting schedules could be combined to a schedule 
 that respects all precedence
constraints. 


Let the intervals in  be called , so that  the time horizon  is partitioned into
 equally sized
\emph{subintervals} with . After reindexing the time horizon, let us assume for the
sake of a simpler notation that . If we abbreviate  for ,
then the th interval contains the time periods .
Each time  has an available \emph{capacity} of  many machines, which is the number of machines not used by jobs in .
We abbreviate  as
the capacity of interval .

The available positions of jobs in  are constrained by the scheduled times of jobs in . As we had no further control over the exact position of the bottom jobs within their intervals , we want to define for each job   a \emph{release time}  and a \emph{deadline}  determined by the most pessimistic outcome of how  could have scheduled
the bottom jobs.  For all , we define

In particular, the release time will be the first time unit of an interval  and the deadline
will be the last time unit of an interval . 
Let  and  be the corresponding indices, so that the release time
is of the form  and the deadline is . Then our goal is to show that
most top jobs  can be scheduled somewhere in the time frame
. This would imply that at least all precedence constraints between bottom and top jobs are going to be satisfied. 





\begin{figure}
\begin{center}
\psset{xunit=0.80cm,yunit=0.7cm}
\begin{pspicture}(0,-0.8)(16,3.9)
\multido{\N=0+2}{8}{
\drawRect{fillstyle=solid,fillcolor=lightgray}{\N}{0}{2}{1}
}
\multido{\N=0.0+0.5}{33}{
\psline(\N,0)(\N,-5pt)
}
\drawRect{fillstyle=vlines}{4}{0}{6}{1}
\drawRect{fillstyle=vlines,hatchcolor=gray}{2}{0}{2}{1}
\drawRect{fillstyle=vlines,hatchcolor=gray}{10}{0}{2}{1}
\rput[c](-0.25,-10pt){}
\rput[c](0.25,-10pt){}
\rput[c](0.75,-10pt){}
\rput[c](1.75,-10pt){}
\rput[c](3.75,-10pt){}
\rput[c](9.75,-10pt){}
\rput[c](15.75,-10pt){} \rput[c](15.75,-20pt){}
\rput[c](1,1.5){}
\rput[c](5,1.5){}
\rput[c](9,1.5){}
\rput[c](15,1.5){}
\pnode(4.25,1){A} \pnode(4.25,2){B} \ncline{->}{B}{A} \nput{90}{B}{}
\pnode(9.75,1){A} \pnode(9.75,2){B} \ncline{->}{B}{A} \nput{90}{B}{}
\end{pspicture}
\caption{Visualization of interval  and possible release times and deadlines for a job . Note that  might schedule  over the whole hatched area, while our choice of  and  will force us to process  inside the black-hatched area (or to discard the job).\label{fig:ReleaseTimeDeadline}}
\end{center}
\end{figure}


Notice here that the existing fractional assignment that  provides for  might 
also be using the slots in the two intervals coming right before and right after  the range  (see Figure~\ref{fig:ReleaseTimeDeadline}). This is due to our rounding of release times and deadlines to interval beginnings and ends.
Let  be the bottom jobs 
scheduled by the recursive calls of the algorithm.

\begin{lemma} \label{lem:MakeRoomForJobs} A valid schedule  of bottom jobs can be extended to a schedule  
with 
that includes most of the top jobs. The schedule satisfies (i)  for ; (ii)   for all  and (iii) one discards at most  many top jobs. \end{lemma}


\begin{proof}
We want to use a matching-based argument.
For this sake, we consider the bipartite graph with jobs on one side and subintervals on the other. Formally, we define a graph  with ,  where the nodes  have capacity , and edges 

We say that a matching  is \emph{-perfect} if it covers every node in .
Then the neighborhood of each top job includes every interval in which it is fractionally scheduled in . Moreover, each of the bottom jobs  has been 
assigned by  to precisely that interval  with . Hence  gives a
-perfect fractional matching that respects the given capacities .
In bipartite graphs, the existence of a \emph{fractional} -perfect matching implies
the existence of an \emph{integral} -perfect matching, see e.g.~\cite{CombinatorialOptimizationBook-Schrijver-2003}. 

However, in order to assign the top jobs to slots within release times and deadlines
we are only allowed to use the smaller set of edges

For any , we let  be the neighborhood of  along edges in  and let  be its neighborhood along edges in . Let the magnitude of a neighborhood  be defined as the sum of capacities of the nodes it contains.
By \emph{Hall's Theorem}~\cite{CombinatorialOptimizationBook-Schrijver-2003}, the minimum number of \emph{exposed} -nodes in a maximum matching in  is precisely

Now, fix the set  attaining the maximum; then 
is the number of jobs that we have to discard. Since  allows for a 
-perfect matching, the reverse direction of Hall's Theorem gives that
. Thus .
Note that  is in general not a consecutive interval of .
We can upper bound the difference  by 
times the number of connected components of . 
This is the point where we take advantage of the ``gap'' between the levels of the top and bottom jobs: for each job  there is an interval  so that 
contains the midpoint of that interval. Due to the gap, there are at most  such intervals\footnote{It is possible that . Still  will contain a midpoint of a level  interval, hence we have accounted for those jobs as well. Note that such jobs would automatically get discarded.}. Hence the number of discarded jobs can be bounded by

Finally note that a corresponding maximum matching can be computed in polynomial time.
\end{proof}

\begin{comment}
\begin{quote}
\red{Remark (T.): There are some points in the proof where I don't see why they would work. 
For example,  is a node in . That means . But we also write 
, which would indicate that .
Moreover, if  and , then I don't think that in general
 is true. One has that either the release time of  or the deadline
is fairly close to the midpoint. But that does not seem to imply . 
}
\end{quote}
\begin{proof}[OLD PROOF]
\red{
To obtain a matching using , we have to give up some of the top jobs. For each interval  with top jobs  to be assigned, discard the following jobs, which we will call :
\begin{itemize}
\item The  jobs in  with  closest to the midpoint of .
\item The  jobs in  with  closest to the midpoint of .
\end{itemize}
Let the set of jobs which are discarded be .
We claim that  has a left-perfect matching from  to  using 
edges in , which implies that there is a valid assignment of jobs in  to 
spaces between their release times and deadlines. It suffices to show that \emph{Hall's condition} holds (see again~\cite{CombinatorialOptimizationBook-Schrijver-2003}). 
For this purpose, we fix any subset ; our goal is 
to show that . In fact, since the neighborhoods of nodes in  are intervals of , 
it suffices to show this if  is an interval as well (otherwise we can apply
the same arguments separately to each subinterval).


Notice that any job  for which the range  is empty must belong to . 
Such a job would have  equal to the midpoint of its assigned interval . 
For any interval , there can only be at most  jobs for which this holds, because they 
would all have been fully scheduled in the range  in the fractional solution, 
and that range has capacity at most . So, all these jobs would belong to  
by definition. Therefore,  is non-empty. 

Let  be a node contained in . We know that for any jobs  and , we have , because otherwise  would have been discarded. So, . 
Since  fully schedules every job in  in the intervals contained in , we must have that 
.
Then 


Assume without loss of generality that  is consecutive. If not, we can separately consider each subset of  with a consecutive neighborhood, because if the condition holds for each such subset then it holds for . So we have  because  only ``cuts off'' edges to one subinterval on each end of the neighborhood. Then   and so the claim holds.
 




We now account for the number of discarded jobs. We can take advantage of the ``gap'' between the levels of the top and bottom jobs, which ensures that top jobs are assigned to relatively few intervals compared to the bottom jobs. We can say that  contains  equally sized intervals, and by our choice of constants, . So by discarding  jobs for each of the  intervals , we discard only  jobs.}
\end{proof}
\end{comment}



\subsection{Reassigning the Top Jobs via EDF}\label{sec:ReassigningTopJobsViaEDF}

We have seen so far that we can schedule most of the bottom and top jobs so that 
all precedence constraints within the bottom jobs are satisfied and the
top jobs are correctly scheduled between the bottom jobs that they depend on. However, 
the schedule as it is now ignores the precedence constraints within the top jobs. 
In this section, we will remove the top jobs from the schedule and then reinsert them
using a variant of the \emph{Earliest Deadline First} (EDF) scheduling policy. 

For the remainder of Section~\ref{sec:ReassigningTopJobsViaEDF}, we will show a 
stand-alone theorem that we will use as a black box. Imagine a general setting where
we have  identical machines and  jobs , each one with integer release 
times  and deadlines  and a unit processing time. The EDF scheduling rule picks
at any time the available job with minimal  for processing.
It is a classical result in scheduling theory by Dertouzos~\cite{EDF-Dertouzos74} that for 
and unit size jobs, EDF is an \emph{optimal} policy. Here ``optimal'' means that if there
is any schedule that finishes all jobs before their deadline, then EDF does so, too.
The result extends to the case of arbitrary running times  if one allows \emph{preemption}.

Now, our setting is a little bit different. For each time , we have a certain number  of slots. Additionally, we have a precedence order that we need to respect. 
But we can use to our advantage that the precedence order has only short chains; moreover, the number of different release times and deadlines is limited. 


Formally we will prove the following:
\begin{theorem}\label{thm:EDF}
Let  be a set of jobs with release times , deadlines  and consistent
precedence constraints\footnote{Here ``consistent'' means that 
for a pair of dependent jobs  one has  and .} with maximum chain length .
Suppose that  is the time horizon, partitioned into
equally sized intervals 
and all release times/deadlines correspond to beginnings and ends of
those intervals. Let

be a capacity function and assume that there exists a 
schedule  assigning each job to slots between its release time and deadline that respects
capacities but does not necessarily respect precedence constraints within .


Then in polynomial time, one can find a schedule  that respects
capacities, release times, deadlines and precedence
constraints and discards   many jobs.
\end{theorem}

We use the following algorithm, which is a variant of Earliest Deadline First, where we
discard those jobs that we cannot process in time:
\begin{center}
\psframebox{
\begin{minipage}{14.0cm}
{\sc Earliest Deadline First} \vspace{1mm} \hrule  \vspace{1mm}
{\bf Input:} Jobs  with deadlines, release times, precedence constraints; capacity function  \\
{\bf Output:} Schedule  \vspace{1mm} \hrule \vspace{2mm} 
\begin{enumerate*}
\item[(1)] Set  for all  and

\item[(2)] Sort the jobs  so that 
\item[(3)] FOR  TO  DO \vspace{-1.0mm}
   \begin{enumerate*}
   \item[(4)] FOR  MANY TIMES DO
   \begin{enumerate*}
     \item[(5)] Select the lowest index  of a job with  that has not been scheduled or discarded
and that has all jobs in  already processed (or discarded).
     \item[(6)] Set  (if there was any such job)
   \end{enumerate*}
   \item[(7)] FOR each   with  and , add  to 
and set 
   \end{enumerate*}
\end{enumerate*}
\end{minipage}
}
\end{center}
At the end all jobs  will be either scheduled between  and 
(that means ) or they are
in .

We will say that a job  was \emph{discarded in the interval }
if  and
. We call a time  \emph{busy} if
 and \emph{non-busy} otherwise.
Let us make a useful observation:
\begin{lemma} \label{lem:dep-discarded-job-to-nonbusy-time}
Let  be part of one of the subintervals. Suppose that there is a non-busy time 
and a job  with  and . Then there is a job

with .
\end{lemma}
\begin{proof}
Consider any inclusion-wise maximal chain of jobs  that
ends in  and otherwise has only jobs .
First suppose that  and hence .
It is impossible that  because by assumption  and hence
EDF would have processed  already earlier at time  (by
maximality of the chain, there is
no job scheduled at times  on which 
might depend).
Hence   and by transitivity , which
gives the claim.

In the 2nd case, we have , hence there is no job that 
depends on scheduled between  and .
But we know that . Thus EDF would have processed  at time  or earlier.
\end{proof}



With this observation we can easily limit the number of non-busy times
per interval:

\begin{lemma} \label{lem:Bound-on-num-nonbusy-times}
Let  be part of one of the subintervals. Suppose that there is at least one job  with  and .
Then the number of non-busy times in  is bounded by .
\end{lemma}
\begin{proof}
By Lemma~\ref{lem:dep-discarded-job-to-nonbusy-time}, for the latest time 
with , there is at least one job  with . 
Then we can continue by induction, replacing  with  and replacing  by 
to build a chain of jobs ending with  that includes a job scheduled at each non-busy time.
Since no chains can be longer than , this gives the claim.
\end{proof}


Now we come to the main argument where we give an upper bound of the
number of discarded jobs:
\begin{lemma} 
One has .
\end{lemma}


\begin{proof}
Suppose that ; we will
then derive
a bound on .
By the pigeonhole principle, we can find an interval 
so that
at least  many jobs get discarded in .
Let us denote the lowest priority (i.e. the latest deadline) job
that gets discarded in  by . Now delete all those lower priority
jobs . Note that this does not affect how EDF schedules
 and still we would have at least  jobs discarded in
, including .
By Lemma~\ref{lem:Bound-on-num-nonbusy-times},
the number of non-busy periods in 
is bounded by . Now, choose a minimal index  so that in
each of the
intervals  one has at most  many non-busy
periods.
We abbreviate . Note that
by definition  has more than  many non-busy
periods\footnote{Admittedly it is possible that  in which
case one might imagine  as an interval in which all times are
non-busy and which does not contain any release times.}.
Define

By Lemma~\ref{lem:dep-discarded-job-to-nonbusy-time}, any job in  has
its release time in  or later, since otherwise we could not have

non-busy times in . Now, let us double count the number of
jobs in .
On the one hand, we have

On the other hand, we know that there is an assignment  of jobs
in  to slots in . That tells us that .
Comparing both bounds gives that .
\end{proof}










\section{Step (2) --- Accounting for Discarded Jobs}\label{sec:AccountingDiscardedJobs}

In this section, we need to argue that the level  can be chosen so that the total 
number of jobs that are discarded in Steps (1)-(4) are bounded by

as claimed. 
Let us summarize the three occasions in the algorithm where a job might get discarded: 
\begin{enumerate}
\item[(A)] In Step (3), in order to schedule the bottom jobs, 
we have  many recursive calls of Lemma~\ref{lem:MainRecursiveAlgorithm}
for intervals . The cumulative number of discarded jobs from all those calls
is bounded by

\item[(B)] As we have seen in Section~\ref{sec:SchedulingTopJobs}, the number of top jobs that 
need to be discarded in Step (4) can be bounded by 
Here we use that the length of the maximum chain within the top jobs is .
Moreover, we have substituted the parameters 
as well as   and  with a large enough constant .
\item[(C)] In Step (2), we discard all the middle jobs. In the remainder of this section we prove 
that there is an index  so that 

\end{enumerate}


Let us first assume that we can indeed find a proper index  so that the bound in (C)
is justified. Then the total number of jobs that the algorithm discards
is

which is the bound that we claimed in Lemma~\ref{lem:MainRecursiveAlgorithm}. 

It remains to justify the claim in (C). 
We abbreviate    for  . 
In words, each number  represents the number of jobs owned by  consecutive levels. 
We observe that if there is an index  so that 

then we can choose  and (C) will be satisfied. Here we use that for this particular choice of , 
we will have  and .


So, we assume for the sake of contradiction that no index  satisfies either  or  (or both). 
Then one can easily show that the 's have to grow exponentially. We show this in a small lemma:
\begin{lemma} \label{lem:GrowOfSequenceAlphaIs}
Let   and suppose we have a sequence of numbers  
satisfying  and  for all . 
Then . 
\end{lemma}
\begin{proof}
Group the indices into consecutive \emph{blocks} of  numbers, where  is 
block ,  is block 1 and so on. We want to prove by induction that
each  in the th block is at least . For , the claim follows from the 
assumption. For , we use that  is at least the sum of  numbers 
that by inductive hypothesis are all at least . The claim follows. 
\end{proof}

Applying Lemma~\ref{lem:GrowOfSequenceAlphaIs} with  and
 we obtain in particular that 

If we set  
 for some adequately large , then
 , which is a contradiction
since we only have  many jobs with . 


\section{Conclusion} 

For the proof of Lemma~\ref{lem:MainRecursiveAlgorithm} we already argued that the
number of discarded jobs satisfies the claimed bound and that all precedence constraints
will be satisfied. Regarding the number of rounds in the hierarchy, recall that
we started with a solution  with . Then we apply a round of conditionings in Lemma~\ref{lem:BreakingChains} 
to obtain  with .
We use copies of the solution  in our recursive application of Lemma~\ref{lem:MainRecursiveAlgorithm} to intervals of size . Since , the remaining 
number of LP-hierarchy rounds satisfies . Thus we still have enough LP-hierarchy rounds for the recursion.  

Another remark concerns why we may assume that the precedence constraints are consistent in 
Theorem~\ref{thm:EDF}. 
Suppose we have jobs  with  and consider the definition of release times and
deadlines in Eq.~\ref{eq:ReleaseTimeDeadlineDef}. By transitivity, any job  with  which limits the deadline
of  will also limit the deadline of , hence . Similarly one can argue
that .
This concludes the proof of Lemma~\ref{lem:MainRecursiveAlgorithm}
and our main result follows. 

\section{Follow-up work and open problems}

A natural question that arises is whether the number of 
rounds for constant  can be improved. In fact, after the conference version of this
paper appeared, Garg~\cite{QPTAS-Scheduling-GargArxiv2017} was able to reduce the number of rounds down 
to , hence providing an actual QPTAS. It remains open whether  many rounds
suffice as well. Another tantalizing question is whether a similar approach could give a -approximation for , where the processing times  are arbitrary. Note that the
difficulty in this setting comes from the issue that jobs have to be scheduled non-preemptively.

\begin{comment}
Note that the recursive applications 
It remains to put all pieces together and show the claim of our main result, which is Theorem~\ref{thm:MainTechnicalTheorem}. Recall from Section~\ref{sec:Overview} that it suffices to prove Lemma~\ref{lem:MainRecursiveAlgorithm}.

\begin{proof}[Proof of Lemma~\ref{lem:MainRecursiveAlgorithm}]

Set  as described in Appendix~\ref{sec:AccountingDiscardedJobs}, so that in each iteration of the recursive algorithm we are dealing with  levels of intervals. 

Now on Step 1, we let  so that the first  levels have no chains longer than  jobs after conditioning at most 

many times.

Notice that because we choose , we will run this step at most  times. Then in total we use at most 
 
levels of the hierarchy. 

Then we can pick a single  so that we start with an LP-hierarchy solution from the 

level of the hierarchy and have enough levels to use in our algorithm.


Then we partition and run the recursive step.

We schedule the top jobs as described in Appendix~\ref{sec:SchedulingTopJobs}, discarding at most  jobs. We apply Lemma~\ref{lem:MakeRoomForJobs} to ensure that there is enough room for the top jobs by certifying a valid schedule which ignores precedence constraints within top jobs. Then we can schedule top jobs using EDF to to get the result given in Theorem~\ref{thm:EDF}.

It follows as shown in Appendix~\ref{sec:AccountingDiscardedJobs} that


\end{proof}
\end{comment}

\bibliographystyle{alpha}
\bibliography{QPTASPrecScheduling}


\end{document}
