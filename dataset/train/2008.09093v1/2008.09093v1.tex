\documentclass[11pt,a4paper]{article}
\pdfoutput=1
\usepackage[nohyperref]{emnlp2020}
\usepackage{times}
\usepackage[misc]{ifsym}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{url}
\urlstyle{same}
\usepackage{arydshln}
\newcommand\bigthanks{\thanks{\quad This work was conducted while the author was an intern at the Max Planck Institute for Informatics.}}

\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\parade}[1]{PARADE}
\newcommand{\squishlist}{
 \begin{list}{}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{1pt}
     \setlength{\topsep}{1pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }
\newcommand{\squishend}{
  \end{list}  }
\newcommand{\sm}[1]{\textcolor{orange}{#1}}

\newcommand{\paragraphHdTop}[1] {\noindent\textbf{#1}} \newcommand{\paragraphHd}[1] {\vspace{3pt}\noindent\textbf{#1}}

\usepackage{microtype}

\aclfinalcopy 


\title{PARADE: Passage Representation Aggregation for Document Reranking}

\author{
Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, Yingfei Sun \\
	 University of Chinese Academy of Sciences, Beijing, China \hspace{0.3cm}\\ 
	 Max Planck Institute for Informatics, Saarbr\"ucken, Germany \\
	 Institute of Software, Chinese Academy of Sciences, Beijing, China\\
     IR Lab, Georgetown University, Washington, DC, USA\\
    \texttt{licanjia17@mails.ucas.ac.cn, ayates@mpi-inf.mpg.de}\\
    \texttt{sean@ir.cs.georgetown.edu, \{benhe, yfsun\}@ucas.ac.cn}
}


\begin{document}
\maketitle
\begin{abstract}
We present PARADE, an end-to-end Transformer-based model that considers document-level context for document reranking.
PARADE leverages passage-level relevance representations to predict a document relevance score, overcoming the limitations of previous approaches that perform inference on passages independently.
Experiments on two ad-hoc retrieval benchmarks demonstrate PARADE's effectiveness over such methods.
We conduct extensive analyses on PARADE's efficiency, highlighting several strategies for improving it.
When combined with knowledge distillation, a PARADE model with 72\% fewer parameters achieves effectiveness competitive with previous approaches using BERT-Base.
Our code is available at \url{https://github.com/canjiali/PARADE}.
\end{abstract}

\section{Introduction} \label{sec.introduction}
Pre-trained language models (PLMs), e.g., BERT~\cite{DBLP:conf/naacl/DevlinCLT19}, ELECTRA~\cite{DBLP:conf/iclr/ClarkLLM20} and T5~\cite{DBLP:journals/corr/abs-1910-10683}, have achieved state-of-the-art results on standard ad-hoc retrieval benchmarks and in many NLP tasks.
The success of PLMs mainly relies on learning contextualized representations of input sequences using the Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}.
The Transformer uses a self-attention mechanism whose computational complexity is quadratic with respect to the input sequence's length, so PLMs generally limit the sequence's length (e.g., to 512 tokens) to reduce computational costs.
Consequently, when applied to the ad-hoc ranking task, PLMs are commonly used to predict the relevance of passages or individual sentences.~\cite{DBLP:conf/sigir/DaiC19,DBLP:conf/emnlp/YilmazWYZL19}.
The max or -max passage scores (e.g., top 3) are then aggregated to produce a document relevance score.
Such approaches have achieved state-of-the-art results on a variety of ad-hoc retrieval benchmarks.


Documents are often much longer than a single passage, however, and intuitively there are many types of relevance signals that can only be observed in a full document.
For example, the {\it Verbosity Hypothesis}~\cite{DBLP:conf/sigir/RobertsonW94} states that relevant excerpts can appear at different positions in a document.
It is not necessarily possible to account for all such excerpts by considering only the top passages. Similarly, the ordering of passages itself may affect a document's relevance; a document with relevant information at the beginning is intuitively more useful than a document with the information at the end ~\cite{DBLP:conf/wsdm/HuiYBM18}. On the other hand, the amount of non-relevant information in a document can also be a signal, because relevant excerpts would make up a large fraction of an ideal document.
IR Axioms encode this idea in the first length normalization constraint (LNC1), which states that adding non-relevant information to a document should decrease its score~\cite{10.1145/1961209.1961210}.
Considering a full document as input has the potential to incorporate signals like these.
Furthermore, from the perspective of training a supervised ranking model, the common practice of applying document-level relevance labels to individual passages is undesirable, because it introduces unnecessary noise into the training process.

Empirical studies support the importance of full-document signals.
\citeauthor{DBLP:conf/sigir/WuML0M19} study how passage-level relevance labels correspond to document-level labels, finding that more relevant documents also contain a higher number of relevant passages~\cite{DBLP:conf/sigir/WuML0M19}.
Additionally, experiments in several works suggest that aggregating passage-level relevance scores to predict the document's relevance score outperforms the common practice of using the maximum passage's score ~\cite{DBLP:conf/ecir/BenderskyK08,DBLP:conf/sigir/FanGLXZC18,DBLP:conf/ecir/AiOC18}.



In this work, we study how PLMs like BERT can be applied to the ad-hoc document ranking task while preserving many document-level signals.
To this end, we propose PARADE, an end-to-end document reranking model.
PARADE predicts a document's relevance by learning passage-level relevance representations that are aggregated in a way that preserves document-level context.
These aggregation approaches include 1)~ a passage weighting method, 2)~a pooling technique, and 3)~using the Transformer in a hierarchical way. 
PARADE is optimized end-to-end at the document level, which eliminates the noise introduced by using the document relevance label as a proxy for passage relevance labels.
Since the utilization of full-text causes more memory usage, we investigate using knowledge distillation to create smaller, more efficient PARADE models that remain effective.

In the recent TREC-COVID challenge that studies the problem of identifying literature relevant to COVID-19 information needs, PARADE performed well and was among the top positions in the second round (as measured by nDCG@10).
The details of our TREC-COVID submissions can be found in Appendix~\ref{sec:covid}.

Our contributions are threefold: 
\squishlist
\item The proposal of the end-to-end PARADE method for predicting a document's relevance by aggregating passage representations, 
\item An evaluation on standard TREC ad-hoc benchmark collections confirming the effectiveness of our approach, and 
\item  Analyses of how PARADE's efficiency can be improved by decreasing the model size, and of how its effectiveness is influenced by the number of passages considered and by the initial ranking method.
\squishend

 \section{Related Work}
We review three lines of related research.



\paragraphHdTop{Contextualized Language Models for IR.}
Neural IR models like
DSSM~\cite{DBLP:conf/cikm/HuangHGDAH13}, 
DRMM~\cite{DBLP:conf/cikm/GuoFAC16}, (Co-)PACRR~\cite{DBLP:conf/emnlp/HuiYBM17,DBLP:conf/wsdm/HuiYBM18},
and
(Conv-)KNRM~\cite{DBLP:conf/sigir/XiongDCLP17,DBLP:conf/wsdm/DaiXC018}
have been proposed for the ad-hoc retrieval task.
However, their contextual capacity is limited by using pre-trained unigram embeddings. 
Benefiting from BERT's pre-trained contextual embeddings, BERT-based IR models have been shown to be superior to neural IR models.
Nogueira et al. first adopted BERT to passage reranking tasks~\cite{DBLP:journals/corr/abs-1901-04085} using BERT's \texttt{[CLS]} vector.
Birch~\cite{DBLP:conf/emnlp/YilmazWYZL19} and BERT-MaxP~\cite{DBLP:conf/sigir/DaiC19} explore the sentence-level and passage-level relevance signals using BERT for document reranking, respectively.
CEDR proposed a joint approach that combines BERTâ€™s outputs with existing neural IR models~\cite{DBLP:conf/sigir/MacAvaneyYCG19}.
Other researchers trade off PLM effectiveness for efficiency by utilizing the PLM to improve document indexing~\cite{DBLP:journals/corr/abs-1904-08375, DBLP:journals/corr/abs-1910-10687}, pre-computing intermediate Transformer representations~\cite{Khattab2020ColBERTEA,MacAvaney2020EfficientDR,Gao2020EARLST,DBLP:conf/iclr/HumeauSLW20}, using the PLM to build sparse representations~\cite{MacAvaney2020ExpansionVP}, or reducing the number of Transformer layers~\cite{DBLP:journals/corr/abs-2002-01854,DBLP:conf/sigir/HofstatterZMCH20}.

While several works have recently investigated approaches for improving the Transformer's efficiency by reducing the computational complexity of its attention module, none of these approaches have been shown to be effective for the document ranking task. 
The Sparse Transformer~\cite{DBLP:journals/corr/abs-1904-10509} and  Reformer~\cite{DBLP:conf/iclr/KitaevKL20} focus on text generation. 
We were unable to effectively use Transformer-XL~\cite{DBLP:conf/acl/DaiYYCLS19} in pilot experiments, 
while Longformer~\cite{DBLP:journals/corr/abs-2004-05150} is an interesting contemporaneous work. 
We note that PARADE could be used in conjunction with such models.


\paragraphHd{Passage-based Document Retrieval.}
Given the increasing lengths of documents in full-text collections, Callan first experimented with paragraph-based and window-based methods of defining passages~\cite{DBLP:conf/sigir/Callan94}.
Several works drive passage-based document retrieval in the language modeling context~\cite{DBLP:conf/cikm/LiuC02,DBLP:conf/ecir/BenderskyK08}, indexing context~\cite{DBLP:journals/bmcbi/Lin09}, and learning to rank context~\cite{DBLP:journals/ir/SheetritSK20}.
In the realm of neural networks, 
HiNT demonstrated that aggregating representations of passage level relevance can perform well in the context of pre-BERT models~\cite{DBLP:conf/sigir/FanGLXZC18}.
Others have investigated sophisticated evidence aggregation approaches~\cite{DBLP:conf/iclr/ZhaoXRSBT20,DBLP:conf/acl/ZhouHYLWLS19}.
Wu et al. explicitly modeled the importance of passages based on position decay, passage length, length with position decay, exact match, etc~\cite{DBLP:conf/sigir/WuML0M19}.
In a contemporaneous study, they proposed a model that considers passage-level representations of relevance in order to predict the passage-level cumulative gain of each passage~\cite{wuleveraging}.
In this approach the final passage's cumulative gain can be used as the document-level cumulative gain.
Our approaches share some similarities, but theirs differs in that they use passage-level labels to train their model and perform passage representation aggregation using a LSTM. 

\paragraphHd{Knowledge Distillation.}
Knowledge distillation is the process of transferring knowledge from a large model to a smaller student model~\cite{DBLP:conf/nips/BaC14,DBLP:journals/corr/HintonVD15}.
Ideally, the student model performs well while consisting of fewer parameters.
One line of research investigates the use of specific distilling objectives for intermediate layers in the BERT model~\cite{DBLP:journals/corr/abs-1909-10351,DBLP:conf/emnlp/SunCGL19}.
Turc et al. pre-train a family of compact BERT models and explore transferring task knowledge from large fine-tuned models~\cite{DBLP:journals/corr/abs-1908-08962}.
Tang et al. distill knowledge from the BERT model into Bi-LSTM~\cite{DBLP:journals/corr/abs-1903-12136}.
Tahami et al. propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model for fast retrieval~\cite{DBLP:journals/corr/abs-2004-11045}.
We demonstrate this approach can be applied to PARADE to improve efficiency without substantial reductions in effectiveness.


 \section{Method}
In this section, we present the proposed PARADE method for end-to-end document reranking.
Given a query  and a document , a ranking method aims to generate a relevance score  that estimates to what degree document  satisfies the query .
As described in the following sections, we perform this relevance estimation by aggregating passage-level relevance representations into a document-level representation, which is then used to produce a relevance score.

\paragraphHdTop{Representing a Document as Passages.}
As introduced in Section~\ref{sec.introduction}, a long document cannot be considered directly by the BERT model due to its fixed sequence length limitation.
Following~\cite{DBLP:conf/sigir/DaiC19,DBLP:conf/sigir/Callan94}, we split a document into passages that can be handled by BERT individually.
To do so, a sliding window of 150 words is applied to the document with a stride of 100 words, formally expressed as  where  is the number of passages.
Afterward, these passages are taken as input to the BERT model for relevance estimation.

\paragraphHd{Creating Passage Relevance Representations.}
Following prior work~\cite{DBLP:journals/corr/abs-1901-04085},
we concatenate a pair of query  and passage  with a \texttt{[SEP]} token in between and another \texttt{[SEP]} token at the end. 
The special \texttt{[CLS]} token is also prepended, in which the corresponding output in the last layer is parameterized as a relevance representation , denoted as follows:



\paragraphHd{Aggregating Passage Relevance Representations.}
Given the passage relevance representations , PARADE summarizes  into a single dense representation  in three different ways, coined as \parade{Max}, \parade{Attn},  and PARADE.

{\bf \parade{Max}} utilizes a robust max-pooling operation on the passage relevance features\footnote{Note that max pooling is performed on passage \textit{representations}, not over passage relevance scores as in prior work.}
in . As widely applied in Convolution Neural Network, max-pooling has been shown to be effective in obtaining position-invariant features~\cite{DBLP:conf/icann/SchererMB10}.
Herein, each element at index  in  is obtained by a element-wise max-pooling operation on the passage relevance representations over the same index.



{\bf \parade{Attn}} assumes that each passage contributes differently to the relevance of a document to the query.
A simple yet effective way to learn the importance of a passage is to apply a feed-forward network to predict passage weights:


where  is the normalization function and  is a learnable weight.
For completeness of study, we also introduce a {\bf \parade{Avg}} that simply averages the passage relevance representations.
This can be regarded as manually assigning equal weights to all passages (i.e., ).






{\bf \parade{Transformer}}, which as shorthand we simply call {\bf PARADE}, enables passage relevance representations to interact by adopting the Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} in a hierarchical way.
Specifically, BERT's \texttt{[CLS]}\footnote{In our pilot study, there is no difference among using \texttt{[CLS]}, \texttt{[SEP]}, and \texttt{[UNK]}  as the prepended token in . } token embedding and all  are concatenated, resulting in an input  that is consumed by Transformer layers to exploit the ordering of and dependencies among passages. That is,

where LayerNorm is the layer-wise normalization as introduced in~\cite{DBLP:journals/corr/BaKH16},
MultiHead is the multi-head self-attention~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}, and
FFN is a two-layer feed-forward network with a ReLu activation in between.


The \texttt{[CLS]} vector of the last Transformer output layer, regarded as a pooled representation of the relevance between query and the whole document, is taken as .
The sequence length of the Transformer layers in PARADE is equal to the number of passages used in a document, usually dozens, hence this approach adds only a small amount of computation compared with \parade{Attn} and \parade{Max}.

\paragraphHd{Generating the Relevance Score.}
For all three PARADE variants, after obtaining the final  embedding, a single-layer feed-forward network is adopted to generate a relevance score, as follows:

where  is a learnable weight. \section{Experiments}
\label{sec:experiment}

\subsection{Dataset} \label{sec:PARADE_dataset}

We experiment with two ad-hoc collections: Robust04\footnote{\url{https://trec.nist.gov/data/qa/T8_QAdata/disks4_5.html}} and GOV2\footnote{\url{http://ir.dcs.gla.ac.uk/test_collections/gov2-summary.htm}}.
Both are common TREC benchmarks.
Robust04 is a newswire collection used by the TREC 2004 Robust track.
GOV2 is a Web collection crawled from government Websites.
We consider both keyword (title) queries and description queries in our experiments.
The statistics of these two datasets are shown in Table~\ref{tab.stats}.
Note that the average document length is obtained only from the documents returned by BM25. 
Documents in GOV2 are much longer than Robust04, making it more challenging to train an end-to-end ranker.

\begin{table}[tb]
    \centering
        \resizebox{.45\textwidth}{!}{
    \begin{tabular}{cccc} \toprule
    Collection & \# Queries & \# Documents & \# tokens / doc \\ \hline
      Robust04 & 249 & 0.5M & 0.7k \\
      GOV2     & 149 & 25M  & 3.8k \\ \bottomrule
    \end{tabular}}
    \caption{Collection statistics.}
    \label{tab.stats}
\end{table}



\begin{table*}[tb]
    \centering
    \resizebox{\textwidth}{!}{
\begin{tabular}{lllllllll} \toprule
& \multicolumn{4}{c}{Robust04} & \multicolumn{4}{c}{GOV2}\\ 
& \multicolumn{2}{c}{Title} & \multicolumn{2}{c}{Description}
& \multicolumn{2}{c}{Title} & \multicolumn{2}{c}{Description} \\ \hline
Model                & P@20   & nDCG@20 & P@20   & nDCG@20 & P@20   & nDCG@20 & P@20   & nDCG@20 \\ \hline
BM25                 & 0.3631 & 0.4240  & 0.3345 & 0.4058  & 0.5362 & 0.4774  & 0.4705 & 0.4264  \\ 
BM25+RM3 & 0.3821 & 0.4407 & 0.3661 & 0.4255 & 0.5634 & 0.4851 & 0.4966 & 0.4212 \\ \hline
Birch (MS)           & 0.3616 & 0.4227  & 0.3341 & 0.4053  & 0.5352 & 0.4722  & 0.4701 & 0.4260  \\
Birch (MSMB)        & 0.4404 & 0.5137  & 0.4211 & 0.5069  & 0.6409 & 0.5608  & 0.5973 & 0.5307  \\
BERT-MaxP (MS)       & 0.4277 & 0.4931  & 0.4522 & 0.5453  & 0.6356 & 0.5600  & 0.6087 & 0.5506  \\ \hline
\parade{Avg}        & 0.4251 & 0.4917  & 0.4482 & 0.5324  & 0.6107 & 0.5362  & 0.5872 & 0.5288  \\
\parade{Max}        & 0.4432 & 0.5115  & 0.4657 & 0.5487  & 0.6319 & 0.5399  & 0.6148 & 0.5419  \\
\parade{Attn} & 0.4410 & 0.5134 & 0.4614 & 0.5517 & 0.6319 & 0.5554 & 0.6198 & 0.5513 \\
PARADE & {\bf 0.4486} & {\bf0.5252}  & {\bf0.4661} & {\bf0.5605}  & {\bf0.6530} & {\bf0.5750}  & {\bf0.6299} & {\bf0.5674} \\ \hline
PARADE (ELECTRA)    & {\bf0.4604} & {\bf0.5399} & {\bf0.4717} & {\bf0.5713} & {\bf0.6678} & {\bf0.5851} & {\bf0.6470} & {\bf0.5762} \\\bottomrule
\end{tabular} }
    \caption{Reranking effectiveness of different models on {\it Robust04} and {\it GOV2} dataset. 
    Best results are in {\bf bold}.
    Significant improvements over Birch (MS), Birch (MSMB) and BERT-MaxP (MS) are marked with ,  and , respectively. (, two-tailed paired t-test.)}
    \label{tab.main}
\end{table*}
\subsection{Baselines}
We compare PARADE against the following traditional and neural baselines:

{\bf BM25} 
is  an  unsupervised  ranking  model  based  on  IDF-weighted counting~\cite{DBLP:conf/trec/RobertsonWHGP95}.
The documents retrieved by BM25 also serve as the candidate documents used with reranking methods.

{\bf BM25+RM3} is a query expansion model based on RM3~\cite{DBLP:conf/sigir/LavrenkoC01}.
We used Anserini's~\cite{DBLP:journals/jdiq/YangFL18} implementations of BM25 and BM25+RM3.
Documents are indexed and retrieved with the default settings for keywords queries.
For description queries, we set  and changed the number of expansion terms to 20.

{\bf Birch (MS)} and {\bf Birch (MS MB)}
aggregate sentence-level evidence provided by BERT to rank documents~\cite{DBLP:conf/emnlp/YilmazWYZL19}. 
Birch (MS) is the fine-tuned BERT-Large model on the MSMARCO passage dataset while Birch (MSMB) is further fine-tuned on TREC MicroBlog datasets.
We use BM25 rather than BM25+RM3 as an initial ranking method for a fair comparison.

{\bf BERT-MaxP (MS)}
adopts the maximum score of passages within a document as an overall relevance score~\cite{DBLP:conf/sigir/DaiC19}.
However, rather than fine-tuning BERT-base on a Bing search log, we improve performance by fine-tuning on the MSMARCO passage ranking dataset.

\subsection{Training PARADE} \label{sec:training}
To prepare the BERT model for the ranking task, we first fine-tune BERT on the MSMARCO passage ranking dataset~\cite{msmarco}.
The fine-tuned BERT model is then used to initialize PARADE's BERT component.
Training of PARADE was performed on a single Google TPU v3-8 using a cross entropy loss where  in Equation~\ref{eq.pred} is the logits.
We train on the top 1,000 documents returned by BM25; documents that are labeled relevant in the ground-truth are taken as positive samples and all other documents as negative samples.
We train PARADE for 3 epochs with batches of 32 training instances.
Each instance comprises a query and all split passages in a document.
We use a learning rate of 3e-6 with warm-up over the first 10 proportions of training steps.
Training takes approximately 2.5 hours for each fold on the Robust04 collection.
Further details on fine-tuning and hyper-parameters are available in Appendix~\ref{sec.domain} and~\ref{sec:HPs}.


\subsection{Evaluation}
Following prior work~\cite{DBLP:conf/sigir/DaiC19, DBLP:conf/sigir/MacAvaneyYCG19}, we use 5-fold cross-validation.
We set the reranking threshold to 100 on the test fold as trade-off between latency and effectiveness.
The reported results are based on the average of all test folds.
Performance is measured in terms of the P@20 and nDCG@20 ranking metrics using \texttt{trec\_eval}\footnote{\url{https://trec.nist.gov/trec_eval}}.



\subsection{Results}
\label{sec:results}

The reranking effectiveness of PARADE is shown in Table~\ref{tab.main}.
It can be seen that the performance of \parade{Max} and \parade{Attn} is comparable, while nDCG@20 of \parade{Attn} can always surpass \parade{Max}.
\parade{Avg} underperforms other models by a large margin, which confirms that passages differ in their contributions to the overall relevance of a document.
PARADE consistently outperforms the other models across both datasets, suggesting that the multi-head self-attention mechanism in the Transformer is a superior method for passage-level relevance aggregation.

Compared with other baseline models,
Birch has two innate advantages: it uses the BERT-Large model with 3x more parameters than BERT-Base, and it is an ensemble model that additionally considers the original ranking scores.
Nevertheless, PARADE still outperform it, especially on description queries.\footnote{Note the Birch results presented here are lower than those in the original work, because we rerank 100 documents. PARADE continues to outperform Birch when reranking 1,000 documents in a comparable setting, as shown later in Table~\ref{tab.understand_reranking}.}
For BERT-MaxP, the reported results are better than those reported in~\cite{DBLP:conf/sigir/DaiC19} with approximately a 0.02 nDCG@20 increase on Robust04 title queries.
On the Robust04 collection with deeper judgments, PARADE outperforms BERT-MaxP significantly.

When applying PARADE to the more recent and efficiently trained LM model ELECTRA-Base~\cite{DBLP:conf/iclr/ClarkLLM20}, PARADE's performance increases substantially.
This model significantly improves over all baselines on nDCG@20 for the Robust04 collection and P@20 for both collections.
These results illustrate that as Transformer pre-training techniques advance, PARADE is able to take advantage of improved pre-trained models.
 \section{Analysis}
\label{sec:analysis}
\begin{table*}[ht]
    \centering
     \resizebox{\textwidth}{!}{
        \begin{tabular}{lllllllll} \toprule
  &                      &                             & \multicolumn{2}{c}{Robust04} & \multicolumn{2}{c}{Robust04 (Distilled)} &  Parameter & Inference Time\\
ID                   & Model                 & L / H                            & P@20           & nDCG@20      & P@20           & nDCG@20      & Count  & (ms / doc)   \\ \hline
1                    & BERT-Large           & 24 / 1024        & 0.4508         & 0.5243      &   \textbackslash    &   \textbackslash  & 360M                 & 15.93 \\
2                    & BERT-Base            & 12 / 768         & 0.4486         & 0.5252      &   \textbackslash    &   \textbackslash  & 123M                 & 4.93   \\ \hline
3                    &     \textbackslash   & 10 / 768         & 0.4420         & 0.5168      &   0.4494 &	0.5296         & 109M                 & 4.19\\
4                    &     \textbackslash   & 8 / 768          & 0.4428         & 0.5168      &   0.4490 &	0.5231         & 95M                  & 3.45\\
5                    & BERT-Medium          & 8 / 512          & 0.4303         & 0.5049      &   0.4388	&   0.5110       & 48M                  & 1.94\\
6                    & BERT-Small           & 4 / 512          & 0.4257         & 0.4983      &   0.4365	&   0.5098     & 35M                  & 1.14 \\
7                    & BERT-Mini            & 4 / 256          & 0.3922         & 0.4500      &   0.4046	&   0.4666     & 13M                  & 0.53  \\
8                    &     \textbackslash   & 2 / 512          & 0.4000         & 0.4673      &   0.4038	&   0.4729      & 28M                  & 0.74 \\
9                    & BERT-Tiny            & 2 / 128          & 0.3614         & 0.4216      &   0.3831	&   0.4410    & 5M                   & 0.18  \\ \bottomrule
    \end{tabular}}
    \caption{PARADE's effectiveness using BERT models of varying sizes on Robust04 title queries. Significant improvements of distilled over non-distilled models are marked with . (, two-tailed paired t-test.)} \label{tab:model_size}
\end{table*}
In this section, we consider the following research questions:
\squishlist
    \item {\bf RQ1:} How can BERT's efficiency be improved while maintaining its effectiveness?
    \item {\bf RQ2:} How does the number of document passages preserved influence effectiveness?
    \item {\bf RQ3:} Is it beneficial to rerank documents from a more effective initial ranking method? In particular, is reranking BM25+RM3 better than reranking BM25? 
\squishend
Additionally, we study the effectiveness of PARADE on the TREC-COVID Challenge in Appendix~\ref{sec:covid}, on the TREC 2019 DL document ranking task in Appendix~\ref{sec:trec_DL}, and the impact of fine-tuning on different domains in Appendix ~\ref{sec.domain}.

\subsection{Reranking Effectiveness vs. Efficiency} \label{sec.modelsize}
While BERT-based models are effective at producing high-quality ranked lists, they are computationally expensive.
However, the reranking task is sensitive to efficiency concerns, because documents must be reranked in real time after the user issues a query.
In this section we consider two strategies for improving PARADE's efficiency, which also answers  RQ1.

\paragraphHdTop{Using a Smaller BERT Variant.}
As smaller models require fewer computations, we study the reranking effectiveness of PARADE when using pre-trained BERT models of various sizes, providing an important guidance for deploying a retrieval system. 
Pre-trained BERT models of various sizes were provided by~\citep{DBLP:journals/corr/abs-1908-08962}.
From Table~\ref{tab:model_size}, it can be seen that as the size of models is reduced, their effectiveness decline monotonously.
The hidden layer size (\#6 vs \#7, \#8 vs \#9) plays a more critical role for performance than the number of layers (\#3 vs \#4, \#5 vs \#6).
An example is the comparison between models \#7 and \#8.
Model \#8 performs better; it has fewer layers but contains more parameters.

The number of parameters and inference time are also given in Table~\ref{tab:model_size} to facilitate the study of trade-offs between model  complexity and effectiveness.

\paragraphHd{Distilling Knowledge from a Large Model.}
To further explore the limits of smaller PARADE models, we apply knowledge distillation to leverage knowledge from a large teacher model.
We use PARADE trained with BERT-Base on the target collection as the teacher model.
Smaller student models then learn from the teacher at the output level.
We use mean squared error as the distilling objective, which has been shown to work effectively~\cite{DBLP:journals/corr/abs-2004-11045,DBLP:journals/corr/abs-1903-12136}.
The learning objective penalizes the student model based on both the ground-truth and the teacher model:

where  is the cross-entropy loss with regard to the logit of the student model and the ground truth, 
weights the importance of the learning objectives, and
 and  are logits from the teacher model and student model, respectively.


As shown in Table~\ref{tab:model_size}, the nDCG@20 of distilled models always increases.
The PARADE model using 8 layers (\#4) can achieve comparable results with the teacher model.
Moreover, the PARADE model using 10 layers (\#3) can  outperform the teacher model with 11\% fewer parameters.
The PARADE model trained with BERT-Small achieves a nDCG@20 above 0.5, which outperforms BERT-MaxP using BERT-Base, while
requiring only 1.14 ms to perform inference on one document.
The inference time for each query is only 0.114 second by reranking top 100 documents.



\subsection{Number of Passages Considered} \label{sec.numPassages}

\begin{figure}[tb]
    \centering
    \includegraphics[ height=2.2in]{figures/parameter_sensitivity_gov2_title_ndcg.png}
    \caption{Reranking effectiveness of PARADE when different number of passages are being used on {\it Gov2} title dataset. nDCG@20 is reported.}
    \label{fig.parameter_sensitivity}
\end{figure}

\begin{table}[tb]
    \centering
    \resizebox{.48\textwidth}{!}{
    \begin{tabular}{ccccc} \toprule
Train \textbackslash \space Eval 
& 8  & 16   & 32  & 64   \\ \hline
8   & {\it 0.5554} & 0.5648 & 0.5648 & 0.5680  \\
16  & 0.5621 & {\it 0.5685} & 0.5736 & 0.5733 \\
32  & 0.5610 & 0.5735 & {\it 0.5750} & 0.5802 \\
64  & 0.5577 & 0.5665 & 0.5760 & {\it 0.5815} \\ \bottomrule
    \end{tabular}}
    \caption{Reranking effectiveness of PARADE using various preserved data size on {\it GOV2} title dataset. 
    nDCG@20 is reported.
    The indexes of columns and rows are number of passages being used.}
    \label{tab:train_eval}
\end{table}

One hyper-parameter in PARADE is the maximum number of passages being used, i.e., preserved data size, which is studied to answer RQ2 in this section.
We consider title queries on the GOV2 dataset given that these documents are longer on average than in Robust04.
Figure~\ref{fig.parameter_sensitivity} depicts nDCG@20 of PARADE with the number of passages varying from 8 to 64.
Generally, larger preserved data size results in better performance for PARADE, which suggests that a document can be better understood from document-level context with more preservation of its content.
For \parade{Max} and \parade{Attn}, however, the performance degrades a little when using 64 passages.
Both max-pooling (Max) and simple attention mechanism (Attn) have limited capacity and are challenged when dealing with such longer documents.
PARADE is able to improve nDCG@20 as the number of passages increases, demonstrating its superiority in identifying relevant and non-relevant documents when documents become much longer.

However, considering more passages also increases the number of computations performed.
One advantage of the PARADE models is that the number of parameters remains constant as the number of passages in a document varies.
Thus, we consider the impact of varying the number of passages considered between training and inference.
As shown in Table~\ref{tab:train_eval}, rows indicate the number of passages considered at training time while columns indicate the number used to perform inference.
The diagonal indicates that preserving more of the passages in a document consistently improves nDCG.
Similarly, increasing the number of passages considered at inference time (columns) or at training time (rows) usually improves nDCG.
In conclusion, the number of passages considered plays a crucial role in PARADE's effectiveness.
When trading off efficiency for effectiveness, PARADE models' effectiveness can be improved by training on more passages than will be used at inference time. This generally yields a small nDCG increase.


\subsection{Understanding Reranking Behavior} \label{sec.understanding}



\begin{table*}[htb]
    \centering
    \resizebox{.8\textwidth}{!}{
    \begin{tabular}{lcccccc}     \toprule   
Model               & Recall@100 & Recall@1k & MAP@100 & MAP@1k & P@20   & nDCG@20 \\ \hline
BM25                & 0.4137     & 0.6989    & 0.2154  & 0.2531 & 0.3631 & 0.4240  \\
BM25+RM3            & 0.4517     & 0.7549    & 0.2451  & 0.2903 & 0.3821 & 0.4407  \\
PARADE (BM25)     & 0.4996     & 0.6989    & 0.2889  & 0.3280 & 0.4562 & 0.5291  \\
PARADE (BM25+RM3) & 0.5058     & 0.7549    & 0.2943  & 0.3407 & 0.4548 & 0.5303  \\ \hline
PARADE (BM25+RM3, Ensemble)    & 0.5347    & 0.7549 &0.3167 & 0.3635 & 0.4733 & 0.5411\\  \bottomrule
    \end{tabular}}
    \caption{(Re)ranking effectiveness of different models.}
    \label{tab.understand_reranking}
\end{table*}


\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=1.8in]{figures/bm25rm3.png}
        \caption{Ranking with BM25+RM3}
        \label{fig.understand_rm3}
    \end{subfigure}~ 
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=1.8in]{figures/docubert_bm25rm3.png}
        \caption{Reranking with PARADE (BM25+RM3)}
        \label{fig.understand_rm3_PARADE}
    \end{subfigure} ~
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=1.8in]{figures/bm25.png}
        \caption{Ranking with BM25}
        \label{fig.understand_bm25}
    \end{subfigure}~ 
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[height=1.8in]{figures/docubert_bm25.png}
        \caption{Reranking with PARADE (BM25)}
        \label{fig.understand_bm25_PARADE}
    \end{subfigure}
    \caption{(Re)ranking distributions by different models. The X-axis represents the ranking position bins while Y-axis represents the average number of relevant documents dropped in each bin.}
    \label{fig.understand_reranking}
\end{figure*}

Query expansion methods based on pseudo-relevance feedback, like RM3~\cite{DBLP:conf/sigir/LavrenkoC01} and NPRF~\cite{DBLP:conf/emnlp/LiSHWHYSX18}, have been shown to increase the effectiveness of a search system.
The use of PRF methods in prior work on BERT ranking models varies, however.
Thus, in this section we consider the question (i.e., RQ3) of whether reranking a stronger initial ranking method (e.g., RM3) improves retrieval results.
To do so, we compare the reranking effectiveness of PARADE on top of BM25 and BM25+RM3.
To simplify the analysis, we focus on the ranking distribution of relevant documents. 
On the Robust04 dataset with title queries, we examine the top 1,000 documents retrieved by BM25 and BM25+RM3.
We then divide all relevant documents retrieved into three partitions, ,  and , defined as follows:
\squishlist
    \item : the relevant documents retrieved by both BM25 and BM25+RM3 \item : the relevant documents retrieved by BM25 but not retrieved by BM25+RM3
    \item : the relevant documents retrieved by BM25+RM3 but not retrieved by BM25
\squishend
For all methods,  is the same; differences come from , , and non-relevant documents.
In total, Count() = 9863, Count() = 1538, and Count() = 409, which means that BM25 and BM25+RM3 share a large number of relevant documents.

Different from the previous setting, we set the reranking threshold to 1,000 to increase recall.
The most effective PARADE is adopted as a reranker.
The (re-)ranking effectiveness of these models is shown in Table~\ref{tab.understand_reranking}.
Replacing BM25 with BM25+RM3 increases Recall@1k by about 8\% and MAP@1k by about 4\%, which may be a result of the nearly 1,000 relevant documents introduced by RM3.
The differences for the other metrics are minor, with RM3 slightly reducing P@20.
These findings are in line with recent work demonstrating that there is little difference in effectiveness between reranking BM25 and reranking BM25+RM3~\cite{DBLP:journals/corr/abs-2003-06713}.

To investigate why there is little difference between reranking BM25 and BM25+RM3 for metrics considering top positions, we provide four sub-figures in Figure~\ref{fig.understand_reranking} that depict the number of relevant documents placed in different position bins (averaged by the number of queries).
Figures~\ref{fig.understand_rm3},~\ref{fig.understand_rm3_PARADE},~\ref{fig.understand_bm25},~\ref{fig.understand_bm25_PARADE} depict the ranking distribution of BM25+RM3, PARADE (reranking BM25+RM3), BM25, PARADE (reranking BM25), respectively.
Due to the change in bin size from 10 to 100, there is a steep increase in the bin 101-200 across all figures.
The distribution is mono-decreasing if the bin size is unchanged.
It can be seen that:
\squishlist

    \item From figures~\ref{fig.understand_rm3} and~\ref{fig.understand_bm25}, the documents from  and  are more likely to be ranked at the low positions (behind 100) by the initial ranking models, which suggests that both models are less confident in these documents.
    For BM25+RM3, it might be that the documents from  are mostly retrieved by the expanded terms;
    for BM25, it may be these documents are retrieved by terms with lower weights.
    \item Comparing Figure ~\ref{fig.understand_rm3} with~\ref{fig.understand_rm3_PARADE}, as well as Figure~\ref{fig.understand_bm25} with ~\ref{fig.understand_bm25_PARADE}, the documents from  and  can be boosted to higher positions by PARADE.
    Mostly, documents in  are retrieved using the expanded terms. PARADE can boost these documents without even knowing these terms, which confirms contextualization benefits by BERT.
    \item Comparing Figure~\ref{fig.understand_bm25} with ~\ref{fig.understand_bm25_PARADE}, it can be seen that a large amount of documents from , especially the documents behind position 100, are boosted to higher positions, which closes the large gap in MAP between BM25 and BM25+RM3 as shown in Table~\ref{tab.understand_reranking}.
\squishend
The advantage of using BM25+RM3 may be that its relevance scores are good source for model ensemble. 
As shown in Table~\ref{tab.understand_reranking}, an ensemble method that linearly interpolates the scores achieves the best results.
In conclusion, while BM25+RM3 does retrieve more relevant documents than BM25, these documents are not effectively utilized by the reranking methods.
BM25+RM3 is thus more of a reranking method than an initial ranking method.
 \section{Conclusion} \label{sec:conclusion}
We proposed the PARADE end-to-end document reranking model
and demonstrated its effectiveness on two TREC ad-hoc benchmark collections.
Our results indicate the importance of incorporating diverse relevance signals from the full text into ad-hoc ranking, rather than basing it on a single passage.
We additionally investigated how model size and the initial ranking method affect performance.
Knowledge distillation on PARADE boosts the performance of smaller PARADE models while substantially reducing their parameters. 
 
\section*{Acknowledgments}
This work was supported in part by Google Cloud and the TensorFlow Research Cloud.

\bibliography{8-Reference}
\bibliographystyle{acl_natbib}

\newpage 
\appendix
\section{Appendices}
\label{sec:appendix}

\subsection{Results on the TREC-COVID Challenge}
\label{sec:covid}
\begin{table}[tbh]
    \centering
    \resizebox{.5\textwidth}{!}{
\begin{tabular}{llllll}
\toprule
& runid              & nDCG@10 & P@5    & bpref  & MAP    \\ \hline
1 & {\bf mpiid5\_run3}       & 0.6893  & 0.8514 & 0.5679 & 0.3380 \\
2 & {\bf mpiid5\_run2}       & 0.6864  & 0.8057 & 0.4943 & 0.3185 \\
3 & SparseDenseSciBert & 0.6772  & 0.7600 & 0.5096 & 0.3115 \\
4 & {\bf mpiid5\_run1}       & 0.6677  & 0.7771 & 0.4609 & 0.2946 \\
5 & UIowaS\_Run3       & 0.6382  & 0.7657 & 0.4867 & 0.2845 \\
\bottomrule
\end{tabular}}
    \caption{Ranking  effectivenes of different retrieval systems in the TREC-COVID Round 2.}
    \label{tab:covid_r2}
\end{table}

\begin{table}[htb]
    \centering
        \resizebox{.5\textwidth}{!}{
\begin{tabular}{llllll}
\toprule
& runid              & nDCG@10 & P@5    & bpref  & MAP    \\ \hline
1 & covidex.r3.t5\_lr      & 0.7740  & 0.8600 & 0.5543 & 0.3333 \\
2 & BioInfo-run1           & 0.7715  & 0.8650 & 0.5560 & 0.3188 \\
3 & UIowaS\_Rd3Borda       & 0.7658  & 0.8900 & 0.5778 & 0.3207 \\
4 & udel\_fang\_lambdarank & 0.7567  & 0.8900 & 0.5764 & 0.3238 \\
\hdashline
11 & sparse-dense-SBrr-2    & 0.7272  & 0.8000 & 0.5419 & 0.3134  \\
13 & {\bf mpiid5\_run2}          & 0.7235  & 0.8300 & 0.5947 & 0.3193 \\
16 & {\bf mpiid5\_run1} (Fusion)          & 0.7060  & 0.7800 & 0.6084 & 0.3010 \\
43 & {\bf mpiid5\_run3} (Attn)           & 0.3583  & 0.4250 & 0.5935 & 0.2317 \\
\bottomrule
    \end{tabular}}
    \caption{Ranking  effectivenes of different retrieval systems in the TREC-COVID Round 3.}
    \label{tab:covid_r3}
\end{table}

\begin{table}[htb]
    \centering
    \resizebox{.5\textwidth}{!}{
\begin{tabular}{llllll}
\toprule
& runid                   & nDCG@20 & P@20   & bpref  & MAP    \\ \hline
1 & UPrrf38rrf3-r4          & 0.7843  & 0.8211 & 0.6801 & 0.4681 \\
2 & covidex.r4.duot5.lr     & 0.7745  & 0.7967 & 0.5825 & 0.3846 \\
3 & UPrrf38rrf3v2-r4 & 0.7706 & 0.7856 & 0.6514 & 0.4310 \\
4 & udel\_fang\_lambdarank  & 0.7534  & 0.7844 & 0.6161 & 0.3907 \\
5 & run2\_Crf\_A\_SciB\_MAP & 0.7470  & 0.7700 & 0.6292 & 0.4079 \\
6 & run1\_C\_A\_SciB & 0.7420 & 0.7633 & 0.6256 & 0.3992 \\
7 & {\bf mpiid5\_run1}            & 0.7391  & 0.7589 & 0.6132 & 0.3993 \\ \bottomrule
\end{tabular}}
    \caption{Ranking  effectiveness of different retrieval systems in the TREC-COVID Round 4.}
    \label{tab:covid_r4}
\end{table}
In response to the urgent demand for reliable and accurate retrieval of COVID-19 academic literature, TREC has been developing the TREC-COVID challenge to build a test collection during the pandemic~\cite{DBLP:journals/corr/abs-2005-04474}.
The challenge uses the CORD-19 data set~\cite{DBLP:journals/corr/abs-2004-10706}, which is a dynamic collection enlarged over time.
There are supposed to be 5 rounds for the researchers to iterate their systems.
TREC develops a set of COVID-19 related topics, including queries (key-word based), questions, and narratives.
A retrieval system is supposed to generate a ranking list corresponding to these queries.

We began submitting PARADE runs to TREC-COVID from Round 2.
The Round 5 results are not yet available at the time of writing.
By using PARADE, we are able to utilize the full-text of the COVID-19 academic papers.
We used the question topics since it works much better than other types of topics.
In all rounds, we employ the full \parade{} model.
In Round 3, we additionally tested \parade{Attn} and a combination of \parade{} and \parade{Attn} using reciprocal rank fusion~\cite{10.1145/1571941.1572114}.

Results from TREC-COVID Rounds 2-4 are shown in Table~\ref{tab:covid_r2}, Table~\ref{tab:covid_r3}, and Table~\ref{tab:covid_r4}, respectively.\footnote{Further details and system descriptions can be found at \url{https://ir.nist.gov/covidSubmit/archive.html}}
In Round 2, PARADE achieves the highest nDCG, further supporting its effectiveness.\footnote{To clarify, our run type is feedback, not manual.}
In Round 3, our runs are not as competitive as the previous round.
One possible reason is that the collection doubles from Round 2 to Round 3, which can introduce more inconsistencies between training and testing data as we trained PARADE on Round 2 data and tested on Round 3 data.
In particular, our run {\tt mpiid5\_run3} performed poorly.
We found that it tends to retrieve more documents that are not likely to be included in the judgment pool.
When considering the bpref metric that takes only the judged documents into account, its performance is comparable to that of the other variants.
As measured by nDCG, PARADE's performance improved in Round 4 (Table~\ref{tab:covid_r4}), but is again outperformed by other approaches.
It is worth noting that the PARADE runs were created by single models (excluding the fusion run from Round 3), whereas e.g. the {\tt UPrrf38rrf3-r4} run in Round 4 is an ensemble of more than 20 runs.


\subsection{Results on the TREC 2019 DL Document Ranking Task}
\label{sec:trec_DL}

The MSMARCO document ranking dataset\footnote{\url{https://microsoft.github.io/TREC-2019-Deep-Learning}} is a large-scale collection and is used in TREC 2019 Deep Learning track~\cite{DBLP:conf/trec/CraswellMYCV19}.
There are 367k, 5193, and 43 queries for training, development, and test set respectively.
To create document labels for the development and training sets, passage-level labels from the MSMARCO passage dataset are transferred to the corresponding source document that contained the passage.
In other words, a document is considered relevant as long as it contains a relevant passage, and each query can be satisfied by a single passage.

The results are shown in Table~\ref{tab.DL_Doc}.
We include comparisons with competitive runs from TREC:
{\tt ucas\_runid1}~\citep{DBLP:conf/trec/ChenLHS19} used BERT-MaxP~\citep{DBLP:conf/sigir/DaiC19} as the reranking method,
{\tt TUW19-d3-re}~\citep{DBLP:conf/trec/HofstatterZH19} is a Transformer-based non-BERT method, and
{\tt idst\_bert\_r1}~\citep{DBLP:conf/trec/YanLWBWXS19} utilizes structBERT~\cite{DBLP:conf/iclr/0225BYWXBPS20}, which is intended to strengthen the modeling of sentence relationships.
All PARADE variants outperform {\tt ucas\_runid1} and {\tt TUW19-d3-re} in terms of nDCG@10, but cannot outperform {\tt idst\_bert\_r1}.
Since this run's pre-trained structBERT model is not publicly available, we are not able to embed it into PARADE and make a fair comparison.
In contrast with the previous results, the other variants outperform \parade{} in this setting.

\begin{table}[tb]
    \centering
        \resizebox{.5\textwidth}{!}{
\begin{tabular}{llcc}\toprule
group  & runid                & MAP   & nDCG@10 \\ \hline
\multirow{4}{*}{TREC} 
&BM25                 & 0.237 & 0.517   \\
&ucas\_runid1~\cite{DBLP:conf/trec/ChenLHS19}         & 0.264 & 0.644   \\
&TUW19-d3-re~\cite{DBLP:conf/trec/HofstatterZH19}          & 0.271 & 0.644   \\
&idst\_bert\_r1~\cite{DBLP:conf/trec/YanLWBWXS19}       & 0.291 & 0.719   \\
 \hline
\multirow{3}{*}{Ours}
&\parade{Avg}       & 0.269 & 0.662   \\
&\parade{Max}        & 0.287 & 0.679   \\
&\parade{Attn}	      & 0.285 &	0.677   \\
&PARADE & 0.274 & 0.650   \\ \bottomrule
\end{tabular}}
    \caption{Ranking effectiveness on TREC 2019 DL Track document task test set.}
    \label{tab.DL_Doc}
\end{table}


\subsection{Effectiveness of Domain Adaptation } \label{sec.domain}

\begin{table}[tb]
    \centering
    \resizebox{.45\textwidth}{!}{
    \begin{tabular}{l c c} \toprule
Fine-tuned model     & P@20   & nDCG@20 \\ \hline
BERT-Base            & 0.4333 & 0.4970   \\
BERT-Base (Bing)     & 0.4223 & 0.4930   \\
BERT-Base (MSMARCO)  & 0.4486 & 0.5252  \\ \hline
BERT-Large           & 0.4408 & 0.5046  \\
BERT-Large (MSMARCO) & 0.4508 & 0.5243  \\ \bottomrule
    \end{tabular}}
    \caption{Rereanking effectiveness of PARADE using different fine-tuned BERT models on {\it Robust04} dataset with Title queries.}
    \label{tab.domain_adaptation}
\end{table}

As previously described, the BERT model used in PARADE is fine-tuned on the MSMARCO passage ranking dataset before being embedded into PARADE.
This training set consists of approximately 400M tuples of query, relevant passage, and nonrelevant passage.
The dev set and test set consist of approximately 6,900 and 6,800 queries, respectively.
For each passage, we use BERT's \texttt{[CLS]} vector as in Equation~\ref{eq.cls_vec} to a single-layer feed-forward network to obtain the probability of being relevant.
We follow the training setup in~\cite{DBLP:journals/corr/abs-1901-04085} and fine-tune the model with a batch size of 32 for 400k iterations.
After that, the fine-tuned model is used as weight initialization in the BERT layers of PARADE.

As mentioned in~\citep{DBLP:conf/emnlp/YilmazWYZL19}, fine-tuning BERT on different domains can result in different model effectiveness.
We verify the performance of PARADE using the BERT models fine-tuned on the above mentioned MSMARCO domain as well as Bing search log\footnote{\url{http://boston.lti.cs.cmu.edu/appendices/SIGIR2019-Zhuyun-Dai}}.
Results on Robust04 when using the original BERT model, a BERT model fine-tuned on Bing search logs, and using a BERT model fine-tuned on MSMARCO are shown in Table~\ref{tab.domain_adaptation}.
It can be seen that fine-tuning on MSMARCO outperforms the other approaches by a large margin.
Considering the model size, BERT-Base shows comparable ability to BERT-Large while requiring fewer parameters.

\subsection{Hyper-parameters}
\label{sec:HPs}
In our pilot study, we tune learning rates from \{1e-6, 3e-6, 5e-6, 1e-5, 5e-5\}, 
 for knowledge distillation from \{0.25, 0.5, 0.75\},
numbers of Transformer layers from 1 to 4, and the
numbers of training epochs from 1 to 10.
Then we fix the learning rate as 3e-6, 
the number of Transformer layers as 2, 
the number of training epochs as 3,
and  as 0.75 for all experiments.
For PARADE, the configuration of Transformer layers (e.g., number of attention heads, hidden size, etc.) is the same as the Transformer block being used in BERT.

Documents are split into passages. 
We set the maximum number of passages in each document as 16 and 32 for Robust04 and GOV2 respectively.
As we split the documents using a sliding window of 150 words with a stride of 100 words, a maximum number of 1650 words in each document are retained on the Robust04 collection while 3250 on GOV2.
The maximum sequence length in BERT is set as 256.
When running PARADE, documents with less number of required passages are padded and later masked out by passage level masks.
For documents longer than required, the first and last passages are always kept while the remaining are selected using a uniform sampling strategy as in~\cite{DBLP:conf/sigir/DaiC19}. 







\end{document}
