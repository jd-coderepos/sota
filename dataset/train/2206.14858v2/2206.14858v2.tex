\documentclass{article}

\usepackage{fullpage}
\usepackage{natbib}
\usepackage{authblk}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{amsmath}

\usepackage{xcolor}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{courier}
\usepackage{etoolbox,siunitx}
\robustify\bfseries
\sisetup{
  detect-weight=true,
  detect-inline-weight=math,
}

\definecolor{light-gray}{gray}{0.95}
\renewcommand\Authfont{\fontsize{11}{15}\selectfont}

\usepackage{listings}
\lstset{
basicstyle=\scriptsize\ttfamily,
frame=single,
backgroundcolor=\color{light-gray}
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
} \lstset{style=pythonstyle}

\usepackage{hyperref}
\definecolor{citecolor}{rgb}{.259,.659,1}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{urlcolor}{rgb}{0,.145,.698}
\definecolor{linkcolor}{rgb}{.71,0.21,0.01}
\hypersetup{
    colorlinks=true,
    breaklinks=true,
    bookmarksnumbered=true,
    linkcolor=linkcolor,
    citecolor=citecolor,
    filecolor=mydarkblue,
    urlcolor=urlcolor,
    pdftitle={Solving Quantitative Reasoning Problems With Language Models},
    pdfpagemode=FullScreen,
    pdfview=FitH
    }

\usepackage[most]{tcolorbox}

\newcommand{\ATTN}[1]{\textcolor{blue}{#1}}
\newcommand{\TODO}[1]{\textcolor{red}{#1}}
\newcommand{\CUT}[1]{\textcolor{gray}{#1}}
\newcommand{\IGNORE}[1]{}

\newcommand{\ourmodel}[0]{{Minerva~}}
\newcommand{\ourbenchmark}[0]{{OCWCourses~}}

\newcommand{\passk}[0]{{\texttt{pass@k} }}
\newcommand{\passone}[0]{{\texttt{pass@1} }}
\newcommand{\majk}[0]{{\texttt{maj1@k} }}

\gdef\Sepline{\par\noindent\makebox[\linewidth][l]{\hspace*{-\mdflength{innerleftmargin}}\tikz\draw[thick,dashed,gray!60] (0,0) --(\textwidth+\the\mdflength{innerleftmargin}+\the\mdflength{innerrightmargin},0);
  }\par\nobreak}

\newcommand{\pretrainedmodel}{{PaLM }} 
\title{\bf{Solving Quantitative Reasoning Problems with \\ Language Models}}


\author{Aitor Lewkowycz\thanks{Equal leadership and advising contribution}\;\,}
\author{Anders Andreassen\thanks{Equal contribution}\;\,}
\author{David Dohan}
\author{Ethan Dyer}
\author{Henryk Michalewski}
\author{\authorcr Vinay Ramasesh}
\author{Ambrose Slone}
\author{Cem Anil}
\author{Imanol Schlag}
\author{Theo Gutman-Solo}
\author{\authorcr Yuhuai Wu}
\author{Behnam Neyshabur}
\author{Guy Gur-Ari}
\author{Vedant Misra}

\affil{Google Research}





\date{}


\begin{document}

\setlength{\parindent}{0em}
\setlength{\parskip}{1ex}

\maketitle

\begin{abstract}
Language models have achieved remarkable performance on a wide range of
tasks that require natural language understanding.
Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level.
To help close this gap, we introduce \ourmodel\!, a large language model pretrained on general natural language data and further trained on technical content. 
The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. 
We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.
\end{abstract}


\section{Introduction}

\label{sec:introduction}
Artificial neural networks have seen remarkable success in a variety of domains including computer vision, speech recognition, audio and image generation, translation, game playing, and robotics. In particular, large language models have achieved excellent performance across a variety of natural language tasks including common-sense reasoning, question answering, and summarization \citep{raffel2019exploring,gpt3,rae2021scaling,smith2022using,palm}. However, these models have struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems \citep{MATH,gsm8k}.

Quantitative reasoning problems are an interesting domain of application for language models because they test the capability of models on several fronts. They require the solver to correctly parse a natural language input, potentially recall world knowledge that pertains to the problem, and apply an algorithm or series of computations to the information provided in order to arrive at a correct solution. They also require that the solver is able to correctly parse and generate precise sequences of mathematical tokens, as well as apply a computational procedure to tokens via symbolic and numerical manipulation. Finally, such problems are a proving ground for research toward robust quantitative reasoning solvers that are useful in supporting the work of humans in scientific and technical fields.

Previous research has shown that large language models achieve impressive performance on math and programming questions after training on domain specific datasets \citep{codex,austin2021program,ocw_codex}. 
In this work, we apply this approach to quantitative reasoning problems, in which the model is required to provide a complete and self-contained solution, without relying on external tools.
Such tasks include mathematics word problems, competition mathematics evaluations, and many problems in science and engineering.

\begin{figure}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
\begin{small}
\textbf{Question:}
A line parallel to  passes through . What is the -coordinate of the point where this line crosses the -axis?
\vspace{3mm}

\textbf{Target:}
 Since the line is parallel to , we know that its slope is 4.  Thus the equation of the line is  where  is the -intercept.  Plugging  into this gives  which is what we wanted.
\end{small}
\tcblower
\begin{small}
\textbf{Model output:}
 The line is parallel to , which means that it has the same slope as .  Since the line passes through , we can use the point-slope form of a line to find the equation of the line: 
Now that we have the equation of the line, we can find the -coordinate of the point where the line crosses the -axis by substituting  into the equation:

\end{small}
\end{tcolorbox}
\caption{\small
Question from the MATH dataset (Algebra 4) with a solution generated by \ourmodel 62B. The solution was selected by majority vote over 256 samples.
This and other MATH questions were prompted with the same 4-shot prompt. 
The model derives the solution by writing down a line equation, simplifying it, substituting , and solving for . The model outputs valid \LaTeX{} along with natural language explanations. The approach taken by the model is different from the one used in the ground truth target.
}
\label{fig:example_MATH}
\end{figure}

\setlength{\columnsep}{1cm}
\begin{figure}[t]
\begin{multicols}{2}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
\begin{small}
\textbf{Question:}
Assume that the variance of the first  natural numbers is 10, and the variance of the first  even natural numbers is . Compute .
\end{small}

\vspace{2.8mm}
\tcblower
\vspace{-0.5mm}

\begin{small}
\textbf{Model output:}
Let  be the variance of the first  even natural numbers, and  be the variance of the first  natural numbers. Then, we have  and . We solve the system of equations:  This translates into the system  Therefore,  and , so .
\vspace{2.8mm}
\end{small}
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
\begin{small}
\textbf{Question:}
For every ,  prove that

\end{small}

\vspace{-0.5mm}
\tcblower
\vspace{-0.5mm}

\begin{small}
\textbf{Model output:}



which is true, because the square of a real number is positive.

\vspace{2.6mm}

\end{small}
\end{tcolorbox}

\end{multicols}

\caption{\small
Example questions with solutions from \ourmodel 540B.  
\textbf{Left:}  Question from the Joint Entrance Examination Main Math 2020 exam taken each year by almost 2M Indian high-school students intending to study engineering and similar fields.
\textbf{Right:} Question from the National Math Exam in Poland (May 2022). The exam is taken by approximately 270K high-school students every year.
}
\label{fig:examples}
\end{figure}


\subsection{Our Contribution}

We present \ourmodel\!\!, a language model that achieves strong performance on many quantitative reasoning tasks. Our model is able to process 
scientific and mathematical questions formulated in natural language, and generate step-by-step solutions using correct \LaTeX{} notation. 
Figures~\ref{fig:example_MATH}, \ref{fig:examples} and \ref{fig:example_particle} 
show example responses from \ourmodel to mathematics and physics questions.

\ourmodel is based on the \pretrainedmodel general language models \cite{palm} that are 
further trained on a high-quality dataset containing scientific and mathematical data.
In particular, we start with 8B, 62B, and 540B
parameter pretrained models, and continue training them on our technical content dataset.
We achieve state-of-the-art performance on MATH \cite{MATH}, GSM8k \cite{gsm8k}, and a STEM subset of the MMLU \cite{MMLU} dataset, all of which include math and science questions formulated in natural language. Notably, our models  achieve robust performance in a few-shot setting without being explicitly trained on these evaluation datasets.

The main novelty of this paper is a large training dataset that juxtaposes natural language with the correct use of formal mathematical language, such as equations and diagrams. 
The data is collected from the arXiv preprint server and from web pages that we carefully process to minimise the loss of mathematical content.  In addition to establishing new state-of-the-art results, this work establishes a new baseline for the performance achievable on quantitative reasoning benchmarks by increasing data quality and model size.

To further increase the coverage of quantitative reasoning benchmarks, 
we build a dataset of over 200 undergraduate-level questions in science and mathematics from MIT's OpenCourseWare (OCW). This provides a measure of our model's quantitative reasoning abilities in a chain-of-thought context beyond a pure mathematical setting.

\begin{figure}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
\begin{small}
\textbf{Question:}
A uniform solid disk starts from rest and rolls down an inclined plane without slipping. After some time, what fraction of the diskâ€™s total kinetic energy is rotational kinetic energy?
What of the following is the right choice? Explain you answer.

(A) 1/4 (B) 1/3 (C) 1/2 (D) 2/3

\end{small}

\tcblower

\begin{small}
\textbf{Model output:}
The disk has both translational and rotational kinetic energy. Let  be the speed of the center of mass of the disk,  be the moment of inertia of the disk,  be the mass of the disk, and  be the radius of the disk.
The translational kinetic energy of the disk is given by

The rotational kinetic energy of the disk is given by

Therefore, the ratio of the rotational kinetic energy to the total kinetic energy is


Final Answer: The final answer is (B).
 
\end{small}
\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black]
\begin{small}
\textbf{Question:}
A particle moves so that it is at  at time   Find the speed of the particle, measured in unit of distance per unit of time.

\vspace{3mm}

\textbf{Target:}
 The parametric curve  describes a circle with radius 3.  Furthermore, it makes a full revolution at time 
\textit{(figure)}
Therefore, the speed of the particle is 
\end{small}

\tcblower

\begin{small}
\textbf{Model output:}
 The speed is the magnitude of the velocity vector.  Therefore, we must first find the velocity vector, which is the derivative of the position vector.  We have

Therefore, the magnitude of the velocity vector is

Therefore, the speed of the particle is .

\end{small}
\end{tcolorbox}

\caption{\small Physics questions with their Minerva solutions. \textbf{Top:} Example question from MMLU (College Physics) and the solution generated by \ourmodel 540B. The model introduces notation to describe the moving object's physical properties, writes down the relevant physical relationships, recalls the moment of inertia of a solid disk, and then correctly computes the desired energy ratio.
\textbf{Bottom:} 
Question from the MATH dataset (Precalculus 4). The model correctly takes the derivative of a position vector to compute the velocity. It then computes the norm of the resulting vector, and uses a trigonometric identity to obtain a final numerical answer. \ourmodel takes a completely different approach from the ground truth solution.
}
\label{fig:example_particle}
\end{figure}



\subsection{Related Works}

Solving quantitative reasoning problems expressed in natural language has been an active area of study \citep{parsing_algebraic,learning_to_solve_arithmetic}. 
Prompting language models using scratchpad \cite{scratchpad} or chain-of-thought~\cite{chain_of_thought} solutions can lead them to output step-by-step  solutions to unseen problems. The GSM8k work~\cite{gsm8k} showed that training verifiers to rerank model outputs can lead to improved performance. The original version of GSM8k included special syntax for algebraic calculations, which were processed by a calculator. In this work we focus on self-contained models without access to external tools.

The standard method for evaluating language models on generative tasks is to greedily sample one solution per problem.
Recent works \citep{codex,alphacode,lamda,majority_voting} have shown that it is advantageous to sample multiple solutions per problem, and then filter those down to a final answer.
We find that majority voting \citep{majority_voting} significantly improves performance over greedy decoding. 

The work \cite{ocw_codex} includes an evaluation of davinci-002, OpenAI's latest publicly available language model, on a subset of 90 problems from the MATH dataset. Due to the focus on a subset of questions, as well as changes made to the way questions are formatted, it is difficult to directly compare our results with those of \cite{ocw_codex}. In Section~\ref{sec:results}, we compare OpenAI davinci-002 with our models under the same experimental conditions.

\paragraph{Code generation.}
Applying code generating models to mathematical problems has been an active area of exploration. PaLM~\cite{palm} showed that a large language model with code in its training dataset can achieve good performance on a code version of GSM8k. Furthermore, the Codex model \citep{codex} can generate code solutions to MATH problems \cite{ocw_codex}. 
These solutions often rely on external libraries to perform mathematical operations such as solving equations or taking limits.
This is a complementary approach to ours, in which we directly probe the model's ability to arrive at an answer by relying only on its own reasoning capability.


\paragraph{Formal mathematics.}
Mathematics developed as a discipline based in natural language, but its axiomatic fundamentals make it possible to simulate mathematical thinking. This can be achieved using specialized programming languages that facilitate the simulation of logical and mathematical thinking using a computer, such as Coq \citep{coq}, Isabelle \citep{isabelle}, HOL4 \citep{hollight}, Lean \citep{lean}, Metamath \citep{metamath} and Mizar \citep{mizar}.
Work on automation of proof assistants and automated theorem provers such as E~\citep{eprover}, leanCoP~\citep{leancop}, and Vampire~\citep{vampire} has substantially benefited from integration with machine learning methods \citep{deepmath, enigma, isarstep, gptf, rlcop}.

\paragraph{Language models applied to formal and synthetic mathematical problems.} 
Previous work trained language models to predict mathematical expressions \cite{skip_trees, isarstep, gptf, lime2021, han2022pact,formal_curriculum,thor,autoform_llm}. In turn, such a predictive model can be used to guide a proof search, as done by \cite{gptf}. Large language models excel in modelling natural language, though in the case of formal languages, models that facilitate retaining information about the graph structure of a given mathematical formula, such as GNNs, are still very competitive.

\paragraph{Modelling mathematics as a discipline of natural language.} New benchmark datasets \citep{MATH, naturalproof} cover more advanced mathematical topics. In this domain language models are facing limited competition from other classes of models.

\section{Training and Evaluation}
\subsection{Mathematical Training Dataset}
\label{sec:pretraining_data}

Our models were trained on a dataset of 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server. In addition, the dataset includes general natural language data, which is the same dataset that was used for pretraining \pretrainedmodel\!\!. Our mathematical webpage dataset was constructed by collecting pages that contain mathematical expressions in MathJax format. The pages underwent a cleaning process that removes most HTML tags but preserves mathematical notation, including \LaTeX{} symbols and formatting. The result is that mathematical formulae like  or  are presented in full to the model during training.  This procedure makes it possible for the model to perform well on tasks that require calculation and symbolic manipulation.
Table \ref{table:dataset-mix} provides a breakdown of the training dataset.
See Appendix \ref{appendix:pretraining data} 
for more details.\looseness=-1

\begin{table}[!ht]
\caption{\small Proportion of data, and number of tokens, from each source in the technical training dataset. The General Natural Language dataset is a subset of the dataset used to pretrain the model.}
\label{table:dataset-mix}
\begin{center}
\begin{tabular}{ lccc } 
\toprule
Data source & Proportion of data & Tokens & Present during pretraining    \\ 
\midrule
Math Web Pages & 47.5\% & 17.5B & No\\
arXiv    & 47.5\% & 21.0B   & No      \\ 
General Natural Language Data & 5\% & >100B & Yes \\ 
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Models and Training Procedure}
\label{sec:pretraining}

Our approach is to start with the \pretrainedmodel pretrained decoder-only transformer language models \cite{palm}, and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table~\ref{table:model-hyperparams} contains the main model and training hyperparameters.
The largest model, with B parameters, was finetuned on B tokens. While this model is highly undertrained compared to the 8B and 62B models, it still achieves superior performance.
Additional details can be found in Appendix \ref{appendix:training_details}.


\begin{table}[ht!]  
\caption{\small Model architecture and continued training hyperparameters. Model training was resumed from the pretrained \pretrainedmodel\! models, and the number of steps quoted refers only to continued training on our technical dataset.}
\label{table:model-hyperparams}
\begin{center}
\begin{tabular}{ 
    l
    S[table-format=2]   S[table-format=2]   S[table-format=5]   S[table-format=4.2] S[table-format=3]   S[table-format=3]   } 
\toprule
Model   & {Layers}  & {Heads}   &  {} & {Parameters}  & {Steps}   & {Tokens} \\ 
\midrule
\ourmodel 8B      & 32        & 16        & 4096                  & 8.63\si{B}    & 624k      & 164\si{B} \\
\ourmodel 62B     & 64        & 32        & 8192                  & 62.50\si{B}   & 416k      & 109\si{B} \\
\ourmodel 540B    & 118       & 48        & 18432                 & 540.35\si{B}  & 399k      & 26\si{B} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Evaluation Datasets}
\label{sec:datasets}


We mainly focus on few shot evaluation, though see Appendix~\ref{appendix:finetuning} for a discussion of finetuned evaluation.
For evaluation, we truncate the inputs from the left to  tokens and we use the model to generate up to  tokens. When sampling once per problem, we sample greedily. When sampling multiple times per problem we use nucleus sampling \citep{nucleus_sampling} with temperature , .
For generative tasks, the model produces a chain-of-thought answer and demarcates a final answer. We evaluate a solution as correct if the final answer matches the ground truth solution, independent of the quality of the chain-of-thought preceding it. 
To evaluate correctness, we parse the final answers and compare them using the \texttt{SymPy} library \citep{sympy}. This is done in order to correctly identify answers that are mathematically equivalent such as  and . See Appendix \ref{appendix:evaluation_formatting} for further details.

The existing datasets on which we focus are:

\begin{itemize}
    \item MATH: a dataset of 12K middle school and high school mathematics problems \cite{MATH}. Problem statements are written in \LaTeX{}. We prompt the model with a fixed 4-shot prompt (listed in Appendix \ref{appendix:math_prompt}).
    This prompt includes four random examples from the training dataset whose ground truth targets are not too long.
    \item GSM8k: middle school math word problems \cite{gsm8k}. Models are evaluated using the chain-of-thought prompt from Wei et al. \cite{chain_of_thought}. Previous models evaluated on GSM8k made use of an external calculator. In this work, our model does not have access to any external tools. 
    \item MMLU-STEM: subset of the MMLU dataset \citep{MMLU} focused on science, technology, engineering, and mathematics (STEM). For the original version, we use the 5-shot prompt from the development set for each task. We also consider chain-of-thought prompting for this task, where we prompt the model with examples that include step-by-step solutions. We use a multiple-choice version of the MATH prompt for topics that involve mathematical reasoning, and add step-by-step solutions to the standard 5-shot prompts for the rest of the topics. See Appendix \ref{appendix:mmlu_prompt} for more details.
    
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[width=0.95\textwidth]{figures/math_and_mmlu_subtopics.pdf}
\end{center}

\caption{\small 
Performance on MATH and MMLU-STEM by subtopic. \ourmodel achieves state-of-the-art results on both datasets. \texttt{maj1@k} denotes evaluations where  samples were generated for each problem and only the most common answer was selected \citep{majority_voting}. For MATH,  for \ourmodel 8B and 62B, and  for 540B. For MMLU-STEM, .
davinci-002 is the latest publicly available language model from OpenAI.
}
\label{fig:math_subtask}
\end{figure}

\subsection{Undergraduate-Level STEM Problems}

To evaluate the scientific reasoning capabilities of \ourmodel\!, we harvested a set of STEM problems at the undergraduate level, most of which involve multi-step reasoning, which we refer to in this paper as \ourbenchmark\!.  Using publicly-available course materials offered by MIT (OpenCourseWare), we collected problems with automatically-verifiable solutions (either numeric or symbolically verifiable via \texttt{SymPy}) from courses including ``solid-state chemistry'', ``information and entropy'', ``differential equations'', and ``special relativity.''  These problems were processed by contractors to be self-contained and to have a clearly-delineated final answer. Problems asking for a proof or open-ended short answer were not included.  In total we curated 272 problems, 191 of which have numeric solutions and 81 have symbolic solutions.  In Appendix \ref{sec:ocw_details}, we detail the contributions from each course, and the process of converting these course materials into a format suitable for processing by language models. We also provide the text of all problems.  We plan to release these as part of an open-source dataset which will be detailed in an upcoming manuscript.


\subsection{Inference-Time Techniques}
\label{sec:advanced_inference}

We find that we can considerably outperform greedy decoding by sampling  solutions (with a non-zero temperature) and selecting one using majority voting~\cite{majority_voting}. This consists of grouping predictions with respect to their final answer and selecting the most common answer. We denote this as \texttt{maj1@k}, following \cite{alphacode}. A variation of this algorithm, denoted \texttt{majn@k}, involves selecting the  most common answers.
Intuitively, the reason majority voting improves performance is that while there are many ways to answer a question incorrectly, there are typically very few ways to answer correctly.

Contrast majority voting with \passk\!, where a task is considered solved if any single sample solves it out of  samples. See Section~\ref{sec:false_positives} for more details on \passk performance.
In Appendix \ref{appendix:perf_vs_k},
we report on how performance depends on  for different metrics. 
We find that while \passk continues to improve as  is increased, majority voting performance saturates faster:  of the large  accuracy is achieved at  for MATH and  for GSM8k. This is likely because majority voting selects the most common answer in the modeled distribution, and the error of this estimate decreases with increasing . This is in contrast to \passk where the performance improvement comes from the tail of the distribution, which can keep improving as  is increased.

Log-likelihood is another metric that can be used to rerank samples. We found that majority voting performs significantly better than log-likelihood reranking (see Appendix \ref{appendix:loglikelihood}).




\newcommand{\lowk}[1]{}
\newcommand{\fat}{\bfseries}
\begin{table}[t]
\caption{\small\textbf{Model performance on several quantitative reasoning datasets.} For majority voting we use  (64 for 540B) ~samples for MATH,  for \ourbenchmark\!,   (40 for 540B) ~for GSM8k and  for MMLU-STEM. The PaLM GSM8k results do not use a calculator and were reported in \citep{palm}. 
We evaluated datasets that did not have published results on recent models on OpenAI davinci-002.
 Despite MMLU-STEM being a multiple choice task, we can apply majority vote by prompting the model to generate a rationale prior to the final answer, sampling multiple times, and then using majority vote on the final answers.
Superscripts denote results that are quoted from previous work:
 GPT-2~\cite{MATH},
 PaLM 540B \texttt{maj1@40}~\cite{majority_voting}, and
 Chinchilla~\cite{chinchilla}.
}
\label{table2}
\begin{center}
\begin{tabular}{
    l
    S[table-format=4.1]
    S[table-format=4.1]
    S[table-format=4.1]
    S[table-format=4.1]
}
\toprule
& {MATH} & {\ourbenchmark} & {GSM8k}  & {MMLU-STEM}  \\ 
\midrule
\pretrainedmodel 8B             & 1.5\si{\%}            & 1.5\si{\%}        & 4.1\si{\%}            & 22.0\si{\%} \\
\ourmodel 8B                    & 14.1\si{\%}           & 7.7\si{\%}        & 16.2\si{\%}           & 35.6\si{\%} \\
\ourmodel 8B, \texttt{maj1@k}   & 25.4\si{\%}           & 12.5\si{\%}       & 28.4\si{\%}           & 43.4\si{\%} \\
\midrule
\pretrainedmodel 62B            & 4.4\si{\%}            & 5.9\si{\%}        & 33.0\si{\%}           & 39.1\si{\%} \\
\ourmodel 62B                   & 27.6\si{\%}           & 12.9\si{\%}       & 52.4\si{\%}           & 53.9\si{\%} \\
\ourmodel 62B, \texttt{maj1@k}  & 43.4\si{\%}      & 23.5\si{\%}       & 68.5\si{\%}           & 63.5\si{\%} \\
\midrule
\pretrainedmodel 540B           & 8.8\si{\%}            & 7.1\si{\%} & 56.5\si{\%}           & 58.7\si{\%} \\
\ourmodel 540B                  & 33.6\si{\%}           & 17.6\si{\%}       & 58.8\si{\%}           & 63.9\si{\%} \\
\ourmodel 540B, \texttt{maj1@k}& \fat 50.3\si{\%}  & \fat30.8\si{\%}   & \fat78.5\si{\%}  & \fat75.0\si{\%} \\
\midrule
OpenAI davinci-002              & 19.1\si{\%}           & 14.8\si{\%}       & {-}                   & {-} \\
Published SOTA                  & 6.9\si{\%}        & {-}               & 74.4\%            & 54.9\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\section{Results}
\label{sec:results}


Table \ref{table2} summarizes the results for \ourmodel models and other models, on the evaluation datasets described in Section \ref{sec:datasets}.
Figure \ref{fig:math_subtask} presents a breakdown of the MATH dataset results by subtopic. For MMLU evaluations, unless otherwise noted, performance is measured by using the standard 5-shot prompt per topic and picking the answer with the highest score. When evaluating MMLU with majority voting, we sample  model answers using a chain-of-thought prompt.

We present model output samples in Figures~\ref{fig:example_MATH}, \ref{fig:examples} and \ref{fig:example_particle}, and additional output samples are listed in the Appendix. In addition, we evaluated \ourmodel 62B 
on the National Math Exam in Poland and found that it achieves a score of , which happened to be the national average in 2021 \citep[p.~23]{cke_results}. The 540B model achieves .


We include results on the latest publicly available language model from OpenAI, davinci-002, evaluated using the OpenAI API with temperature set to the official recommendation (). 
The combination of training data, scale and inference techniques yields state of the art results on all the technical tasks that we considered. For all tasks (with the exception of GSM8k), the improvement with respect to previous results is considerable.

While our main focus is on few shot evaluation,  we also tried to finetune \ourmodel on MATH. While we did not observe any improvement, we found that finetuning \pretrainedmodel on MATH did give a significant  improvement, which suggests that the marginal utility of standard finetuning decreases as the quality and diversity of the unsupervised training dataset improves. Further details can be found in Appendix \ref{appendix:finetuning}. 

\subsection{Basic arithmetic}
In Appendix \ref{appendix:arithmetic}, we study the performance of \ourmodel 540B on simple arithmetic tasks. The model achieves over  accuracy on 10-digit addition and over  accuracy on 18-digit addition.  


\section{Performance Analysis}

\subsection{Model Mistakes}
\label{sec:mistake_exploration}

To better understand the types of mistakes our models make, we compare the performance of \ourmodel 8B and \ourmodel 62B on 216 problems with high confidence majority decisions of both models. Specifically, we selected examples where the top answer received at least  of votes, and that either \ourmodel 8B was correct and \ourmodel 62B was incorrect (15 samples), or vice versa (201 samples). The categories and examples for each category are described in Appendix \ref{appendix:model_mistakes}.

As shown in Table \ref{table:mistakes_8b}, the prevailing errors of the 8B model were related to incorrect reasoning or calculations. Many of the calculation errors were relatively benign arithmetic mistakes.
Solutions that were too short were relatively rare (in these cases, the model immediately produces an incorrect answer without any intermediate reasoning steps).
Finally, in a few cases, the model hallucinates an equation or mathematical fact that is not real.

In the samples where the 62B model was incorrect, the dominating failure modes were again incorrect reasoning and incorrect calculations.
In summary, we find that the 62B \ourmodel model retains most of the skills of the 8B model and improves upon both reasoning and calculation robustness. 

\begin{table}[ht!]  
\caption{\small Failure modes of the 8B \ourmodel model, out of 201 samples which the 62B model solved correctly and the 8B model did not.}
\label{table:mistakes_8b}
\begin{center}
\begin{tabular}{lc}
\toprule
Type of mistakes & Occurrences \\
\midrule
Incorrect reasoning & 82 \\
Incorrect calculation & 70 \\
Misunderstands question & 22 \\
Uses incorrect fact & 16 \\
Solution too short & 4 \\
Hallucinated math objects & 4 \\
Other mistakes & 3 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{False Positives}
\label{sec:false_positives}

In our approach to solving quantitative reasoning problems, we are able to automatically verify whether the final answer to a problem is correct, but we do not have an automatic way to verify the model's chain of reasoning. This leaves open the possibility of false positives: samples which have the correct final answer, but for which the reasoning is incomplete or incorrect.

We selected 100 random questions from MATH (20 per difficulty level), along with answers sampled at zero temperature from the 62B model. We then manually inspected the answers to determine the false positive rate, which is the ratio between number of false positive examples and number of examples for which the final answer is correct; see Table~\ref{table:62B_false_positive_rates}. We found that the overall false positive rate is low, though it does increase with difficulty level.

Our focus on \passone and majority voting as the primary evaluation metrics is due in part to the fact that they are less susceptible to false positives than \passk \citep{alphacode}. While the \texttt{pass@256} accuracy is  for the 62B model, false positives account for part of it.
We inspected the samples that failed in majority voting but passed on \texttt{pass@k} due to a single correct answer, and estimate the false positive rate for \texttt{pass@256} to be 30\% among samples selected in this way. After removing false positives, we estimate that the \texttt{pass@256} accuracy to be bigger than 68\%; see Appendix  \ref{appendix:false_positives} for details.

\begin{table}[ht!]  
\caption{\small Estimated false positive rates of the 62B model on the MATH dataset, by difficulty level. The average is the estimated false positive rate on the MATH dataset, given by the average of per-level false positive rates weighted by positive rates.}
\label{table:62B_false_positive_rates}
\begin{center}
\begin{tabular}{ l|ccccc|c } 
\toprule
 & \multicolumn{5}{c|}{Difficulty level} & \\
 & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \midrule
False positive rate &  &  &  &  &  &   \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\section{Memorization}
\label{sec:memorization}
A central question in interpreting \ourmodel\!\!'s solutions is whether performance reflects genuine analytic capability or instead rote memorization. This is especially relevant as there has been much prior work indicating that language models often memorize some fraction of their training data~\citep{trinh2018simple, radford2019language, carlini_memorization_22}.
When examining model solutions, we find that memorization of intermediate facts, such as numerical values of square roots or trigonometric identities, are crucial elements of model solutions.
Truly strong performance would combine recall of intermediate facts with genuine solution synthesis. 
We would like to investigate a strong form of memorization, where model performance is a result of memorizing the explicit problems and solutions in our evaluation set,
but also a weaker form, where the model has memorized alternate answers to the same questions.

In order to evaluate the degree to which our models solve problems by recalling information memorized from training data, we conduct three analyses on the MATH dataset.
First we directly search for problems and solutions in our training corpus.
Next, we generate modified versions of problems and evaluate our models' robustness to these changes.
Finally, we measure the degree of overlap between the ground truth solutions and solutions generated by our model and measure the effect of this similarity on model performance. Overall, we find little evidence that the model's performance can be attributed to memorization.

\subsection{Training and Evaluation Dataset Overlap}

We selected the problems for which our 62B parameter model produced a correct answer, and filtered them to the 100 problems with the highest majority vote score, expecting that problems with a high majority vote score are more likely to have been memorized. For each of these question-answer pairs, we compute the BLEU score across chunks of 500 characters in our Math Web Pages dataset (a histogram of the BLEU scores is shown in Appendix Figure~\ref{fig:MATH_trainds_BLEU}).
We then manually inspect the 250 documents with the highest BLEU scores. While many of the top matches were from homework help sites with math questions and solutions, none of the questions matched the questions in the subset of MATH under consideration.
We have included these 250 segments in Appendix~\ref{sec:data_overlap}.
We note that some problems from MATH can be found on the web.
Nevertheless, this analysis concludes that these problems did not make it through our data collection process.

\subsection{Performance on Modified MATH Problems}
To further investigate memorization, we randomly selected twenty problems which the 62B model answered correctly under majority voting. We manually modified each problem either by introducing minor changes to problem wording (framing) or by changing the numbers which appeared in the problem and modifying the solution accordingly.
We then compared the accuracy over sampled solutions before and after the modification. Results are shown in Figure~\ref{fig:memorization_correct_solutions}. In both cases the accuracy before and after modifications are correlated, with no clear bias in favor of the original formulation. This is suggestive of minimal memorization. The modified problems are listed in Appendix~\ref{sec:mem_def}.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.30\textwidth]{figures/MATH_memorization_comparison_1.pdf}
\includegraphics[width=0.30\textwidth]{figures/MATH_memorization_comparison_2.pdf}
\includegraphics[width=0.37\textwidth]{figures/acc_at_bleu.pdf}
\caption{\small
\textbf{Results indicating lack of memorization on MATH.}
\textbf{Left, Center:}
Accuracy of original questions from the MATH dataset and their modified versions. Each point represents a question. The  axis is accuracy on the original question, and the  axis is accuracy on the modified one.
\textbf{Right:} Majority vote accuracy, computed only on samples with BLEU score to the ground truth solution less than or equal to the -axis value.
}
\label{fig:memorization_correct_solutions}
\end{center}
\end{figure}



\subsection{BLEU Score Between Ground Truth and Generated Solutions}

We seek to detect memorization of solutions by computing BLEU score between ground truth answers and model generated answers. We use the 62B model and analyze  samples per problem in the MATH dataset.  First, we compute overlap statistics for all correct samples.  
We find that 160 out of 5,000 test questions have a sample with a BLEU score greater than or equal to 80 (see Appendix~\ref{sec:solution_overlap}). 
We note that they tend to be short solutions. 
To understand the effect of answer similarity on performance, we remove model samples above a certain BLEU score threshold, and recompute the majority vote accuracy. We find that majority vote performance is robust even down to relatively low similarities (see Figure~\ref{fig:memorization_correct_solutions}), indicating that performance cannot be attributed to model outputs that are very similar to ground truth answers.


\section{Conclusions and Discussion}
\label{sec:conclusion}
In this work, we take an approach to quantitative reasoning that relies on solving problems using mathematical reasoning expressed in natural language. 
We show that by training a large language model on a high quality mathematical dataset, we are able to achieve strong performance on tasks that require logical reasoning, numerical calculation, and symbolic manipulation. Our model does not make use of external tools, and at inference time relies exclusively on autoregressive sampling to achieve this performance.
Complementary approaches to quantitative reasoning include code-generating models and formal methods. These are all different routes toward a common goal: an agent that can reason about and solve quantitative problems. We believe that such an agent should combine useful elements from all of these approaches. 


\subsection{Limitations of Our Approach}
Our approach to quantitative reasoning has several limitations. First, we have no automatic way of verifying the correctness of the model's answers. This is in contrast to formal approaches, for which automatic verification is intrinsic. Second, our model has no access to external tools such as a calculator or a Python interpreter. It is therefore limited in its ability to perform quantitative reasoning tasks that require complicated numerical calculations. Third, because our model was trained on a large amount of data, we have little direct control over the specific capabilities that the model acquired.\looseness=-1

\subsection{Societal Impact}
Artificial neural networks capable of solving quantitative reasoning problems in a general setting have the potential of substantial societal impact. \ourmodel\!, while a step in this direction, is still far from achieving this goal, and its potential societal impact is therefore limited. The model's performance is still well below human performance, and furthermore, we do not have an automatic way of verifying the correctness of its outputs. If these issues could be solved, we expect the impacts of this model to be broadly positive. A direct application could be an accessible and affordable math tutor which could help improve educational inequalities.

\section{Acknowledgments}
We thank David Andre, Jacob Austin, Maarten Bosma, Aakanksha Chowdhery, Sergey Ioffe, Colin Raffel, Charles Sutton, and Christian Szegedy for helpful discussions.

\bibliographystyle{apalike}
\bibliography{references}

\newpage
\begin{center}
\textbf{\huge Appendix}
\end{center}
\appendix

\section{Detailed Contributions}

\textbf{Aitor} prepared the Mathematical web pages dataset and \textbf{Aitor} and \textbf{David} prepared the arXiv dataset used to train \ourmodel\!\!.

\textbf{Aitor} trained the \ourmodel models presented in the paper, and he, along with \textbf{David} and \textbf{Vedant}, conducted ablation studies.

\textbf{Aitor}, \textbf{Ambrose}, and \textbf{David} built the experimental infrastructure for training and evaluating \ourmodel\!. They, along with \textbf{Anders}, \textbf{Ethan}, \textbf{Henryk}, \textbf{Vinay}, and \textbf{Vedant} collected the evaluation datasets and conducted model evaluations.

\textbf{Aitor}, \textbf{Anders}, \textbf{Behnam}, \textbf{Ethan}, \textbf{Guy}, and \textbf{Vedant} conducted experiments and ablation studies on inference-time techniques.

\textbf{Vedant} and \textbf{Vinay} collected the OCWCourses dataset and supervised the contractors' work.

\textbf{Aitor}, \textbf{Ambrose}, \textbf{Anders}, \textbf{David}, \textbf{Ethan}, \textbf{Guy}, \textbf{Henryk}, \textbf{Theo}, \textbf{Vedant}, \textbf{Vinay}, and \textbf{Yuhuai} analyzed the models' results, including sample explorations to categorize model mistakes and identify false positives.

\textbf{Aitor}, \textbf{Anders}, and \textbf{Cem} conducted fine-tuning evaluation experiments.

\textbf{Ethan}, \textbf{Vedant}, and \textbf{Vinay} designed and conducted the memorization experiments.

\textbf{Aitor}, \textbf{Anders}, \textbf{Ethan}, \textbf{Guy}, \textbf{Henryk}, \textbf{Imanol}, \textbf{Vedant}, and \textbf{Yuhuai} wrote the paper.

\textbf{Aitor}, \textbf{Behnam}, \textbf{Guy}, and \textbf{Vedant} advised and led the project throughout its life cycle.


\section{Training Dataset Details}
\label{appendix:pretraining data}
\newcommand{\code}[1]{\texttt{#1}}

The two main data sources for our training dataset are arXiv papers and web pages that contain mathematics. Here we present additional details on how the data from each source was collected and processed.

\subsection{arXiv}

The arXiv dataset contains 2M arXiv papers up to February 2021, in \LaTeX{} format. If multiple \LaTeX{} files were present, they were concatenated. Comments were removed, and anything before the first section header or after an appendix/bibliography header was removed. The title and abstract of each paper were added to the document from the arXiv metadata. In order to retain high quality documents and maximize the information per token, papers were filtered out if they were longer than 75k tokens, had on average more than 0.6 tokens per character, had no \code{\textbackslash section} headers, or ended up being empty after processing. The final arXiv dataset after processing includes 1.2M papers totalling GB of data.

\subsection{Mathematical web pages}

We started with a collection of web pages that included the string \code{"<math"} or \code{"MathJax-Element-"} in the raw HTML, which we used as our filter for pages that that include mathematical content. We considered pages as of January 2022. We then used several heuristics to process the pages. We found empirically that these are sufficient to extract most of the available mathematical content in either \LaTeX{} format or ASCII-math format. The majority of the documents (about 80\% of documents) have one of these two formats:

\begin{enumerate}
\item
A majority of these HTML documents contain math in TeX or AsciiMath format inside tags of the form \code{<script type="math/latex">} or \code{<script type="math/asciimath">}.
\item
Another common appearance of \LaTeX{} happens with  \code{<annotation encoding="application/x-tex">} tags inside \code{<math>} MathML blocks. We extract the content of these \code{<annotation>} blocks but do not include other content from inside the \code{<math>} blocks. 
\end{enumerate}
The remaining documents (about 20\%) generally have math in MathML format, which we discarded. After extracting the content in any of the previous two forms, we removed all other content that was inside  \code{<math>} or \code{<span id=MathJax-Element-*>} blocks, because these blocks often encode the MathML version of TeX or AsciiMath content.
After filtering, processing,
and selecting only English documents, the final dataset size is GB.


\section{Model and Training Procedure Details}
\label{appendix:training_details}

We start with pretrained \pretrainedmodel models, and perform unsupervised finetuning on our technical dataset to obtain \ourmodel\!.
The models have context length . They are trained with batch size  (except for the B model which was trained with batch size ) and without dropout.

The learning rate schedule was reciprocal square-root decay, which continued the schedule of the pretrained models. 
The B model was pretrained for M steps and further trained for k additional unsupervised finetuning steps. The B model was pretrained for k steps and further trained for k additional unsupervised finetuning steps.
 The B model was pretrained for k steps and was further trained for k additional steps during unsupervised finetuning.

Finally, the learning rate was dropped x and all models were then trained for  additional steps.  We note that these models had a significantly larger batch size during pretraining.

We used the \texttt{t5x} framework \citep{t5x} and trained our models with v4 TPU on Google Cloud. The 8B model was trained for 14 days on a 
v4-128, the 62B model was trained for 17 days on a v4-512, and the 540B model was trained for 29 days on a v4-1024.

\section{MATH Evaluation Details}

\subsection{MATH Answer Normalization}
\label{appendix:evaluation_formatting}

Extracting and evaluating the correctness of answers to math questions is non-trivial because answers can often be presented in many different ways, both in terms of formatting (e.g. answers can be underlined, or surrounded by a box) and in terms of mathematical content (a large number can be equivalently represented as 1,000 or 1000, answers about currency potentially have the currency symbol attached to them, etc.). Here we describe how final answers are extracted and normalized. After normalization, answers are compared using SymPy (see below). Failing to normalize answers properly will typically lead to falsely identifying correct answers as incorrect (``false negatives''), and therefore to underestimate the model's accuracy.

We first extract the final answer from the full model response, which potentially includes chain-of-thought reasoning. In the few-shot prompt, we used the format \texttt{"Final Answer: The final answer is ANSWER. I hope it is correct."} for every final answer. We look for this pattern in the model output and extract \texttt{ANSWER}.

We then apply a normalization function to this answer, shown in Listing \ref{lst:format_solution}.
In order to develop it we manually inspected ground truth targets, samples from \ourmodel\!, and samples from OpenAI davinci-002. We were especially careful to avoid changes in the format of the ground truth target that might produce false positives.

\clearpage
\begin{lstlisting}[
language=python,
basicstyle=\scriptsize\ttfamily,
backgroundcolor=\color{light-gray},
caption={\small Python code used to normalize final answers.},
label={lst:format_solution}
]
SUBSTITUTIONS = [
    ('an ', ''), ('a ', ''), ('.'), ('.*?)(\\\3', '')
  
  # Normalize 100,000 -> 100000
  if final_answer.replace(',', '').isdigit():
    final_answer = final_answer.replace(',', '')
    
  return final_answer
\end{lstlisting}


After applying this normalization function, we checked whether the formatted target and prediction strings are \texttt{SymPy}-equivalent.  \texttt{SymPy} equivalence is determined by parsing the answers via \\
\texttt{sympy.parsing.latex.parse\_latex} and then checking whether substracting the two resulting \texttt{SymPy} objects and applying \texttt{sympy.simplify} gives zero. We set a timeout of s when calling \texttt{sympy.simplify}, and labeled strings as nonequivalent if this timeout was exceeded.

For MATH problems, \texttt{SymPy} equivalence improved overall accuracy by around . See Table \ref{table:sympy_comparison} for the accuracies in MATH with only exact string match vs. \texttt{SymPy} equivalence.
\begin{table}[ht!]
\caption{Comparing MATH accuracy when evaluating results with and without \texttt{SymPy} processing.}
\label{table:sympy_comparison}
\begin{center}
\begin{tabular}{lccc}
\toprule
  &  \multicolumn{2}{c}{MATH Accuracy} \\
\cmidrule(r){2-3}
& without \texttt{SymPy} & with \texttt{SymPy}  \\
\midrule
\ourmodel 8B            & 13.3  & 14.1  \\
\ourmodel 8B Majority   & 24.6  & 25.4 \\
\ourmodel 62B           & 26.5  & 27.6 \\
\ourmodel 62B Majority  & 42.2  & 43.4 \\
OpenAI davinci-002              & 18.7  & 19.1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\newpage
\subsection{MATH Prompt}
\label{appendix:math_prompt}

Listing \ref{lst:math_prompt} shows the 4-shot prompt used when sampling answers to MATH questions. We picked it by choosing  random examples from MATH and selecting examples which did not include Asymptote plotting commands. We chose four examples so that most problems fit within a context length of , to enable comparisons with a wide range of models.

\begin{lstlisting}[
basicstyle=\scriptsize\ttfamily,
backgroundcolor=\color{light-gray},
frame=single,
caption={\small 4-shot prompt used for MATH problems.},
label=lst:math_prompt]
Problem:
Find the domain of the expression  .}

Solution:
The expressions inside each square root must be non-negative. Therefore, 
, so , and , so . Also, the denominator 
cannot be equal to zero, so , which gives . Therefore, the domain of
the expression is .
Final Answer: The final answer is . I hope it is correct.

Problem:
If  and  then find 


Solution:
We have that 
Final Answer: The final answer is . I hope it is correct.

Problem:
Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound 
weights instead, how many times must Terrell lift them in order to lift the
same total weight?

Solution:
If Terrell lifts two 20-pound weights 12 times, he lifts a total of 
 pounds of weight.  If he lifts two 15-pound 
weights instead for  times, he will lift a total of  
pounds of weight.  Equating this to 480 pounds, we can solve for : 

Final Answer: The final answer is . I hope it is correct.

Problem:
If the system of equations

has a solution  where  and  are both nonzero,
find  assuming  is nonzero.

Solution:
If we multiply the first equation by , we obtain

Since we also know that , we have


Final Answer: The final answer is . I hope it is correct.
\end{lstlisting}



\section{Additional Evaluation Experiments}

\subsection{Dependence of performance on number of generated samples}
\label{appendix:perf_vs_k}
We study the dependence of performance on the number of generated samples per question on MATH and GSM8k.
Table~\ref{tab:higher_metrics} shows results for maj and  maj, and Figure~\ref{fig:math_62B_majority_topk} shows the dependence on  for \passk and majority voting. 
We observe that while \passk continues to improve, majority voting saturates quickly.


\begin{table}[ht!]
\caption{Performance on MATH () and GSM8k () when generating  samples per task.}
\label{tab:higher_metrics}
\begin{center}
\begin{tabular}{lcccc}
\toprule
  & MATH  & GSM8k  \\ 
\midrule
\ourmodel 8B, maj       & 25.4\%                  & 28.4\% \\
\ourmodel 8B, maj       & 47.6\%                   & 56.8\%  \\
\ourmodel 62B, maj      & 43.4\%                  & 67.5\% \\
\ourmodel 62B, maj      & 64.9\%         & 89.0\% \\
\midrule
Published SOTA                      & 6.9\%   & 74.5\%  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.49\textwidth]{figures/math_test_at_k.pdf}
\includegraphics[width=0.49\textwidth]{figures/gsm8k_test_at_k.pdf}
\caption{Accuracy as a function of , the number of samples per task. Majority voting performance saturates quickly while \passk seems to continue improving slowly. Accuracies were computed using exact string match (without \texttt{SymPy} processing).}
\label{fig:math_62B_majority_topk}
\end{center}
\end{figure}


\subsection{Log-Likelihood Reranking}
\label{appendix:loglikelihood}
Table \ref{adv_inference_table} compares majority voting with reranking based on the log-likelihood that the model assigns to each response. We observe that majority voting is significantly better.

\begin{table}[ht!]
\caption{A comparison of the majority voting results presented in the  main text with log-likelihood reranking. We do not use \texttt{SymPy} processing here.
}
\label{adv_inference_table}
\begin{center}
\begin{tabular}{lcc}
\toprule
  & MATH  \\
\midrule
\ourmodel 62B, \texttt{pass1}   & 26.5\% \\ 
\ourmodel 62B, Majority Voting 1\text{@}k & 42.0\%  \\
\ourmodel 62B, \texttt{pass1}   & 21.8\% \\ 
\ourmodel 62B, Log-likelihood  & 23.8\% \\ 
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Finetuning on MATH}
\label{appendix:finetuning}

Most of our results involve few-shot prompting \ourmodel on MATH and other datasets on which the model was not explicitly trained. In this section we discuss finetuning our models on the training split of the MATH dataset, and then evaluating on the test split as before. We finetune both the \pretrainedmodel and \ourmodel 8B for  steps with 2048 tokens per batch with batch size  and dropout of 0.1. Similar to \cite{alphacode}, we found that the accuracy for \pretrainedmodel kept improving despite the test loss increasing. We picked the model with the best test accuracy after  training steps.

We finetuned using a few different prompts: A 0-shot prompt, our custom 4-shot prompt, and a prompt containing 4 random examples. 
Each model was evaluated using the same prompt as was used during finetuning, except for the random prompt model, with was evaluated using the fixed 4-shot prompt that we used for the non-finetuned models. 

The results can be found in Table \ref{tbl:finetuning}. Standard finetuning does not seem to improve the performance of \ourmodel\!. On the other hand, it does lead to measurable improvements in \pretrainedmodel\!, though this performance still lagged behind \ourmodel\!. These results suggest that the marginal utility of supervised finetuning decreases as one improves the quality and diversity of the unsupervised pretraining or unsupervised finetuning dataset. 


\begin{table}[ht!]
\caption{We finetune \pretrainedmodel and \ourmodel using different finetuning methods. We find that while finetuning helps considerably for the \pretrainedmodel\!, it does not help for \ourmodel\!.}
\label{tbl:finetuning}
\begin{center}
\begin{tabular}{lcc}
\toprule
  &  \multicolumn{2}{c}{MATH Accuracy} \\
\cmidrule(r){2-3}
  &  \pretrainedmodel 8B & \ourmodel 8B    \\
\midrule
Few Shot & 1.5\% & 14.1\% \\
Custom prompt finetuning & 5.6 \% & 13.4\% \\
Random prompt finetuning & 4.4 \% & 12.9\% \\
No prompt finetuning  & 5.6 \% & 13.0\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Majority Voting Thresholds}
\label{appendix:majority_thresholds}

From Figure~\ref{fig:math_62B_majority_topk}, we see how majority voting saturates rather quickly at some , while \passk keeps improving. Here we analyze the asymptotic behavior of majority voting at large .


Let  denote the sorted number of counts for answer  when we sample  times and let there be a total for  answers. In other words,  . We expect that when sampling  samples, we can model the sampling distribution as a multinomial distribution with probabilities . This approximation will have the error of attributing  to any answer which doesn't appear in  draws, so we can't really resolve probabilities smaller than . This issue will not matter for our purposes as long as the maximum probability  is significantly higher than .

If we draw  samples from this multinomial distribution, we expect to \emph{not} be able to identify the majority answer with  confidence as long as 

For , this bound implies that the resolution for  is , but this is a very rough estimate. However, this exercise quantifies why and how majority voting saturates even if \passk doesn't.

Another point is that in order to obtain the majority solution with  confidence, we need 

for , we can probe up to .


\section{\ourbenchmark Evaluation Dataset Details}
\label{sec:ocw_details}

\subsection{Breakdown of courses}

Table \ref{table:stem-lite-problems-per-course} shows the breakdown of problems in our dataset by course. See Table~\ref{table:stem-lite-answer-types} for a breakdown of problems by solution type.

\begin{table}[h!]  
\caption{\small Problems of the \ourbenchmark dataset broken down by course.}
\label{table:stem-lite-problems-per-course}
\begin{center}
\begin{tabular}{ lc } 
\toprule
Course & No. problems \\
\midrule
Solid State Chemistry &  \\
Introduction to Astronomy &  \\
Differential equations &  \\
Dynamics and Control &  \\
Principles of Microeconomics &  \\
Special Relativity &  \\
Physical Chemistry &  \\
Ecology &  \\
Information and Entropy &  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\begin{table}[t!]  
\caption{\small Answer types in \ourbenchmark}
\label{table:stem-lite-answer-types}
\begin{center}
\begin{tabular}{ lc } 
\toprule
Answer Type & No. Problems\\
\midrule
Numeric &   \\
Symbolic &  \\
\midrule
Total & 272 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\subsection{Contractor instructions}

Figure \ref{fig:contactor_instructions} shows the instructions provided to our contractor workforce.

\begin{figure}[ht]
\noindent\fbox{
    \parbox{\textwidth}{
We would like to build a dataset of clean self-contained STEM problems and solutions written in clean and correct LaTeX code.

This dataset should have the following properties:

\begin{itemize}
\item \textbf{Self-contained problems with no external references}: A human should be able to solve each problem and understand the given solution without having to reference any other sources. For example, some problems reference lecture notes or a textbook. These problems should be rewritten to include the referenced information. If it takes you more than roughly five minutes to find the referenced material, please delete the problem; do not include it in the final submission.

\item \textbf{No extraneous material}: The raw dataset contains extraneous data, such as headers, footers, problem numbers, and point values for problems. All of this data should be removed, so that each problem/solution pair contains only the content of the problem.

\item \textbf{Clearly marked final answers}: For some problems, the solution ends in a specific value that constitutes the final answer. For example, a problem might ask the student to compute the value of an integral. In this case, the steps for computing the integral are part of the solution, but the expression that represents the antiderivative is the final answer (or in the case of a definite integral, the numerical value). When a problem has such a final answer, we ask that you annotate it using a special annotation. If such a final answer is not available, we ask that you try to define one yourself that represents the solution to the problem (though in some cases this will not be possible).

\item \textbf{Including images and annotating non-essential images} If there are images in the problem, please include them with a single-line 
includegraphics command in the same way that they appear in the raw input files.  To make the image render nicely, you can add a [scale=...] modifier; just make sure the command is on one line. 
\end{itemize}
}
}
\caption{\small Instructions provided to contractors who worked on \ourbenchmark\!.}
\label{fig:contactor_instructions}
\end{figure}

\clearpage


\subsection{\ourbenchmark Prompt}

\begin{lstlisting}[
basicstyle=\scriptsize\ttfamily,
backgroundcolor=\color{light-gray},
frame=single,
caption={\small Prompt used for OCWCourses.}]
Problem:                                                                                
Subproblem 0: What is the net charge of arginine in a solution of ? 
Please format your answer as +n or -n.                           
Solution:
The answer is +2.                                                     
Final answer: The final answer is +2. I hope it is correct.

Problem:
Subproblem 0: Let . Find  that satisfy the equation 
. Express your answer as the ordered pair .         
Solution:
 has argument  and radius 16 , so it's equal to . 
Thus , and our answer is .
Final answer: The final answer is (-8, -8\sqrt{3}). I hope it is correct.

Problem:
Preamble: For each Laplace Transform , find the function :
Subproblem 0: 

Solution:
We can simplify with partial fractions:
\nfind the constants 
 and  by setting  and 

therefore

By looking up the inverse Laplace Transform of , we find the total 
solution 
.
Final answer: The final answer is . 
I hope it is correct.

Problem:
Preamble: The following subproblems refer to the differential equation 
.
Subproblem 0: What is the characteristic polynomial  of 
?
Solution:
The characteristic polynomial is .
Final answer: The final answer is . I hope it is correct.
\end{lstlisting}

\subsection{Problems in \ourbenchmark}

We provide the problems in \ourbenchmark as a separate file.




\subsection{\ourbenchmark evaluation}

As with the MATH dataset, special care must be taken in order to correctly extract answers and evaluate them for correctness. Here we describe how final answers are extracted and normalized. See Listing \ref{lst:ocw_format_solution} for the code. During dataset creation, contractors annotated all automatically-verifiable solutions as belonging to one of several types:  \texttt{symbolicexpression}, \texttt{symbolicequation}, or \texttt{numeric}. For \texttt{symbolicexpression} and \texttt{symbolicequation} answers, our approach is to convert the answer strings into \texttt{SymPy} quantities, and check equality programmatically.  For numeric quantities, we first remove any units from the answer string, then convert the answer string to a float.  If either numeric quantity is close to zero, our equality condition is that the absolute value of their difference is less than a threshold (0.01) of their mean; otherwise, we use the \texttt{numpy.isclose()} comparison.

As with MATH, we first extract the final answer from the full model response, which potentially includes chain-of-thought reasoning. In the few-shot prompt, we used the format \texttt{Final Answer: The final answer is ANSWER. I hope it is correct.} for every final answer. We look for this pattern in the model output and extract \texttt{ANSWER}.


\newpage
\begin{lstlisting}[
language=python,
basicstyle=\scriptsize\ttfamily,
backgroundcolor=\color{light-gray},
caption={\small Python code used to normalize final answers.},
label={lst:ocw_format_solution}
]
import numpy as np

def get_answer(s: str) -> str:
  end_str = "I hope it is correct"
  start_str = "Final answer: "
  replacement_str = "The final answer is "

  scrub_periods = lambda x: x.strip().rstrip('.').strip()

  try:
    ans = s.split(end_str)[0].split(start_str)[1].strip().replace(replacement_str, "") 
    ans = scrub_periods(ans)
    return ans 
  except:
    print("Answer extraction failed")
    return None

def grade_question(question: dict) -> bool:
  """Grades a question."""
  formatting_fns = {'automatic:symbolicexpression': normalize_symbolic_expression,
                    'automatic:symbolicequation': normalize_symbolic_equation,
                    'automatic:numeric': normalize_numeric,
                    }
  question_type = question['type']
  formatting_fn = formatting_fns[question_type]

  # Get ground truth answer
  ground_truth_answer = formatting_fn(question['target'])
  if ground_truth_answer is None:
    raise ValueError("Could not parse question target answer")

  # Get model's answer
  model_answer = formatting_fn(get_answer(question['model_outputs'][0]))

  # Perform comparison
  grading_fns = {
                 'automatic:symbolicexpression': symbolic_equality,
                 'automatic:symbolicequation': lambda x,y: x == y,
                 'automatic:numeric': numeric_equality,
                 }

  return grading_fns[question_type](model_answer, ground_truth_answer)
  
def normalize_numeric(s):
  if s is None:
    return None
  for unit in ['eV',
               ' \\mathrm{~kg} \\cdot \\mathrm{m} / \\mathrm{s}',
               ' kg m/s', 'kg*m/s', 'kg', 'm/s', 'm / s', 'm s^{-1}',
               '\\text{ m/s}',
               ' \\mathrm{m/s}',
               ' \\text{ m/s}',
               'g/mole','g/mol',
               '\\mathrm{~g}',
               '\\mathrm{~g} / \\mathrm{mol}',
               'W',
               'erg/s',
               'years',
               'year',
               'cm']:
    s = s.replace(unit, '')
    s = s.strip()
  for maybe_unit in ['m', 's', 'cm']:
    s = s.replace('\\mathrm{'+maybe_unit+'}','')
    s = s.replace('\\mathrm{~'+maybe_unit+'}','')
    s = s.strip()
  s = s.strip('') or s.endswith('')
  try:
    maybe_expression = parse_latex(s)
    if not isinstance(maybe_expression, sympy.core.relational.Equality):
      # we have equation, not expression
      return None
    else:
      return maybe_expression
  except:
    return None

def normalize_symbolic_expression(s: Optional[str]):
  if not isinstance(s, str):
    return None
  if s.startswith('\'):
    s = s[:-2]
  s = s.replace('\\left(', '(')
  s = s.replace('\\right)', ')')
  s = s.replace('\\\\', '\\')
  if s.startswith(''):
    s = s.strip('\inABCDk=16\frac{\sqrt{x-2}}{\sqrt{5-x}}x-2 \ge 0x\ge25 - x \ge 0x \le 55-x>0x<5\boxed{[2,5)}\det \mathbf{A} = 2\det \mathbf{B} = 12,\det (\mathbf{A} \mathbf{B}).\det (\mathbf{A} \mathbf{B}) = (\det \mathbf{A})(\det \mathbf{B}) = (2)(12) = \boxed{24}.2\cdot 12\cdot20=480n2\cdot15\cdot n=30nn(x, y)xy\frac{a}{b},b-\frac{2}{3}\frac{2}{3}\frac{1}{3}\frac{4}{9}-\frac{3}{2}6y-9x=b12 + 344621 * 67140798 - 178184763 + 2307910784284763 + 23079107842313091 * 278453457{x} {op} {y}M = \begin{pmatrix} 1 & 2 \\ -1 & 5 \end{pmatrix}M|M-\lambda I|I|M-\lambda I| = 
\begin{vmatrix} 1-\lambda & 2 \\ -1 & 5-\lambda \end{vmatrix} = (1-\lambda)(5-\lambda)+2 = \lambda^2-6\lambda+7\lambda^2-6\lambda+7=03+\sqrt{2}3-\sqrt{2}f(0)+f(1)+f(2)0 \leq 1f(0)=0-1=-11 \le 1f(1)=1-1=02>1f(2)=2^3+2(2)-1=11f(0)+f(1)+f(2)=-1+0+11=\boxed{10}f(0)=0-1=-1f(1)=1-1=0f(2)=2^3+2\cdot2-1=8+4-1=11f(0)+f(1)+f(2)=-1+0+11=\boxed{10}6353!=643!=64343333213!=623+6+6+3+3+6+1=\boxed{28}636+3-1=88(x+y)^8x^6y^2(x+y)^8x^6y^2x^6y^2\binom{8}{2}x^6y^2\binom{8}{2}2828\sqrt{400}=\sqrt{81}+\sqrt{n}n20=9+\sqrt{n}.\sqrt{n}=11n=\boxed{121}.121\sqrt{400}=\sqrt{81}+\sqrt{n}400 = 81 + n319 = n319\sqrt{400}=\sqrt{81}+\sqrt{n}\Rightarrow\sqrt{16\cdot25}=\sqrt{9\cdot9}+\sqrt{n}\sqrt{16\cdot25}=\sqrt{16} \cdot \sqrt{25}=4\cdot5=20\sqrt{9\cdot9}=\sqrt{9} \cdot \sqrt{9}=3\cdot3=920=9+\sqrt{n}11=\sqrt{n}11^2=nn=\boxed{121}121y = 8 - 5x +4x^2yx=-2y=8-5x +4x^2 = 8-5(-2) +4(-2)^2 = 8+10 + 4(4) = 8+10 + 16 = \boxed{34}34y=8-5x+4x^2x=-2y=8-5(-2)+4(-2)^2y=8+10-16=44y = 8 - 5x +4x^2x=-234\frac{1}{2}(48)=24\boxed{3}3\frac{48}{2}=2424n48i(i-1)30.1_2-0.01_2+0.001_2-0.0001_2+0.00001_2\ldots2^{-1}-2^{-2}+2^{-3}\ldots=\frac{1}{2}-\frac{1}{4}+\frac{1}{8}\ldots-\frac{1}{2}\frac{a}{1-r}ar\boxed{\frac{1}{3}}\frac{1}{3}\frac{0.1_2}{1-0.1_2}=\frac{1/2}{1-1/2}=\frac{1/2}{1/2}=\boxed{1}.10.1_2=\frac{1}{2}0.01_2=\frac{1}{4}0.001_2=\frac{1}{8}0.0001_2=\frac{1}{16}0.00001_2=\frac{1}{32}\frac{1}{3}(1,4)(3,5)(-6,0)(-4,-2)?(1,4)\sqrt{(1-0)^2+(4-0)^2}=\sqrt{17}(3,5)\sqrt{(3-0)^2+(5-0)^2}=\sqrt{34}(-6,0)\sqrt{(-6-0)^2+(0-0)^2}=\sqrt{36}(-4,-2)\sqrt{(-4-0)^2+(-2-0)^2}=\sqrt{20}\boxed{(-6,0)}(-6,0)(x_1,y_1)(x_2,y_2)(1,4)(3,5)(3,5)(-6,0)(-6,0)(-4,-2)2\sqrt{2}(-4,-2)(-4,-2)(x,y)d=\sqrt{x^2+y^2}dx^2+y^2x^2+y^2(-6,0)(-6,0)jdddjj(d-31)+d=53d=42\boxed{42}42xx+31x+31+x=5311xyx=y-31.x+y=53.y-31x42f(x)g(x)2f(x) + 4g(x)f(x) = a_3 x^3 + a_2 x^2 + a_1 x + a_0g(x) = b_5 x^5 + b_4 x^4 + b_3 x^3 + b_2 x^2 + b_1 x + b_02f(x) + 4g(x)\boxed{5}5f(x)g(x)2f(x) + 4g(x)f(x)g(x)2f(x) + 4g(x)5y=(3a+2)x-22y=(a-4)x+2a3a+2=\frac{a}{2}-2a=\boxed{-\frac{8}{5}}-\frac{8}{5}m_1=(3a+2)m_2=(a-4)m_1=m_23a+2=a-42a=-6a=-3-3y=(3a+2)x-22y=(a-4)x+2-\frac{8}{5}yy + 612y12 - (y + 6) = 6 - yy - 126 - y = y - 12yy = \boxed{9}912 - (y + 6) = y - 6y + 6y + 6 = a_1 = 12y = 66d12y+6d=12-(y+6)=6-yyd=y-126-y=y-12y=99\sqrt[3]{4^5 + 4^5 + 4^5 + 4^5}4^5+4^5+4^5+4^5=4\cdot 4^5=4^64^64^{6/3}=4^2=\boxed{16}16\sqrt[3]{4^5 + 4^5 + 4^5 + 4^5}\sqrt[3]{4^5}\cdot\sqrt[3]{1}4\cdot\sqrt[3]{1}=\boxed{4}.416729=3^6\frac{\sqrt{3}}{3}\frac{1}{3}\frac{\sqrt{3}}{3}k=2561\text{@}2562702530\%84\%-43\%=41\%1\text{@}k41\% \cdot 0.7 = 29\%8\%43\% \cdot 0.92+ 29\% = 65\%190\%x = \arcsin 0.5y = \arcsin 0.4,\frac{9}{100}\sin( \arcsin 0.4 + \arcsin 0.5) \cdot \sin( \arcsin 0.5 - \arcsin 0.4) = \sin(\arcsin(\frac{4}{10} + \frac{5}{10})) \cdot \sin(\arcsin(\frac{5}{10} - \frac{4}{10})) = \sin(\arcsin(\frac{9}{10})) \cdot \sin(\arcsin(\frac{1}{10})) = \frac{9}{10} \cdot \frac{1}{10} = \boxed{\frac{9}{100}}.\frac{9}{100}10 \cdot \frac{4}{3} \cdot \frac{6}{4} \cdot \frac{8}{5} \cdot \frac{10}{6} \cdot \frac{12}{7} \cdot \frac{14}{8} \cdot \frac{16}{9} \cdot \frac{18}{10}51210 \cdot \frac{4}{3} \cdot \frac{6}{4} \cdot \frac{8}{5} \cdot \frac{10}{6} \cdot \frac{12}{7} \cdot \frac{14}{8} \cdot \frac{16}{9} \cdot \frac{18}{10} = 2\cdot 2\cdot 2\cdot 2\cdot 2\cdot 2\cdot 2\cdot 2\cdot 2=2^{9}=\boxed{512}.5121530^{\mathrm{th}}15515^{\mathrm{th}}67815^{\mathrm{th}}2^{\mathrm{nd}}\boxed{8}.81516^{\mathrm{th}}617^{\mathrm{th}}618^{\mathrm{th}}619^{\mathrm{th}}620^{\mathrm{th}}621^{\mathrm{st}}722^{\mathrm{nd}}723^{\mathrm{rd}}724^{\mathrm{th}}725^{\mathrm{th}}726^{\mathrm{th}}727^{\mathrm{th}}828^{\mathrm{th}}829^{\mathrm{th}}830^{\mathrm{th}}88270075ab2700,a+b = 540075a+bab450ab\frac{a+b}{2}=2700\frac{2ab}{a+b}=75\frac{2ab}{a+b}=7545025,26,27,\dots,250250-25+1 = 22625, 26, \ldots, 2503^3,\ldots,6^3226-4=\boxed{222}22225^3=15625250^3=156250001562515625000\log_{10}15625=4\log_{10}15625000=77-4+1=41562515625000250-25+1=226226-4=222222\text{lcm}(9,60)=180\boxed{\text{Sunday}}\text{Sunday}9\cdot 2\cdot 5=9090-7=8312\boxed{\text{Sunday}}\text{Sunday}yy^\circy^{\circ}90^{\circ}360^{\circ}y^{\circ}+90^{\circ}=360^{\circ}y=360-90=\boxed{270}270360^\circ - 90^\circ = 270^\circ180^\circ - 90^\circ - 90^\circ = 0^\circ270^\circ + 0^\circ = \boxed{270^\circ}270^\circ(2 \sqrt{8}-3 \sqrt{2})^{2}212614(2 \sqrt{8}-3 \sqrt{2})^{2} = (2 \sqrt{2}\cdot 2 \sqrt{2}-3 \sqrt{2})^{2} = (4 \sqrt{2}-3 \sqrt{2})^{2} = (\sqrt{2})^{2} = \boxed{2}2xy2x = 3y\frac{x^{2}+y^{2}}{x \cdot y}2x = 3yx = \frac{3}{2}y\frac{13}{6}10\%x0.9x0.9(0.9x) = 0.81x0.81x = 78732x = 78732/0.81 = 9720097200a_na_n > 09\cdot a_5 = 4 \cdot a_3a_{n+1} / a_n\frac{2}{3}\frac{3}{2}\frac{2}{9}\frac{9}{2}a_n = a_1\cdot r^{n-1}a_5 = a_1\cdot r^4a_3 = a_1\cdot r^29\cdot a_5 = 4 \cdot a_39\cdot a_1\cdot r^4 = 4\cdot a_1\cdot r^2a_1r^29\cdot r^2 = 4r^2 = \frac{4}{9}r = \pm \frac{2}{3}a_n > 0r = \frac{2}{3}a_{n+1} / a_n = \frac{a_1\cdot r^{n}}{a_1\cdot r^{n-1}} = r = \boxed{\frac{2}{3}}\frac{2}{3}610120^{\circ}30 \sqrt{3}3060 \sqrt{3}60610 \sin 60^{\circ} = 10 \sqrt{3}/2 = 5 \sqrt{3}6 \cdot 5 \sqrt{3} = \boxed{30 \sqrt{3}}30 \sqrt{3}9 \cdot 8 \cdot 7 \cdot 29 \cdot 10 \cdot 10 \cdot 19 \cdot 10 \cdot 10 \cdot 29 \cdot 9 \cdot 8 \cdot 159 \cdot 10 \cdot 10 \cdot 2109 \cdot 10 \cdot 10 \cdot 15109 \cdot 10 \cdot 10 \cdot 2 - 9 \cdot 10 \cdot 10 \cdot 1 = 9 \cdot 10 \cdot 10 \cdot 1 = \boxed{900}900a_na_{1}=-1a_{4}=8a_n = a_1 + (n-1)dda_4 = a_1 + 3d8 = -1 + 3dd = 3a_n = -1 + 3(n-1) = 3n-4\sum_{n=1}^{100} a_n = \sum_{n=1}^{100} (3n-4) = 3\sum_{n=1}^{100} n - 4\sum_{n=1}^{100} 1 = 3\frac{100\cdot101}{2} - 400 = 150\cdot101 - 400 = \boxed{15150}15150a_{1}=-1a_{4}=8dd = \frac{a_{4}-a_{1}}{4-1}=\frac{9}{3}=3a_{100} = a_1+(100-1)d = -1+99\cdot 3 = 296147501, 2, 3, 4, 5, 6, 7, 8, 9nmn \times m = 249^2 = 81n \times m = 24(6, 4)\frac{1}{81}\frac{1}{81}n \times m = 24n=3m=8n=4m=6n=6m=4n=8m=39 \times 9 = 81\boxed{\frac{4}{81}}\frac{4}{81}1, 2, 3, 4, 5, 6, 7, 8, 9nmn \times m = 249^2 = 81n \times m = 24(6, 4)\frac{1}{81}\frac{1}{81}n \times m = 24n=3m=8n=4m=6n=6m=4n=8m=39 \times 9 = 81\boxed{\frac{4}{81}}\frac{4}{81}A=(-2,6)B=(3, b)0,0b9-9-44\frac{b-6}{3+2} = \frac{b-6}{5}\frac{b-6}{5} = \frac{0-0}{0+0} = 0b-6=0b=\boxed{6}6AB\frac{b - 6}{3 - (-2)}AB\frac{6}{-2}\frac{b - 6}{5} = -3b = \boxed{-9}-94.625\frac{39}{8}\frac{37}{8}\frac{185}{4}\frac{17}{4}4.6254.625 = 4 + \frac{625}{1000} = 4 + \frac{5}{8} = \frac{32}{8} + \frac{5}{8} = \frac{37}{8}\boxed{\frac{37}{8}}\frac{37}{8}p2x=y-py=\frac{3p}{2}x=\frac{p}{4}y=\frac{3p}{2}200<m\le102810<m\le20m=2020+28=484020<m\le30m=3048+40=882030<m\le40m=4088+20=1081240<m\le50m=50108+12=120907x4xxy450xnx^{2k}\binom{2n}{2k}-\binom{2n}{2k-2}S=612n+1=61n=\boxed{30}30x\left| \frac{a}{b} \right| = \frac{|a|}{|b|} = \frac{6}{4} = \boxed{\frac{3}{2}}.\left| \frac{a}{b} \right| = \frac{|a|}{|b|} = \frac{6}{4} = \boxed{\frac{3}{2}}.\frac{3}{\sqrt{27}}=\frac{3\sqrt{3}}{\sqrt{81}}=\frac{3\sqrt{3}}{9}=\boxed{\frac{\sqrt{3}}{3}}$
\end{footnotesize}
\end{tcolorbox}

\end{document}