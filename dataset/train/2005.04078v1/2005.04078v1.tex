



\documentclass[a4paper, 10pt, conference]{ieeeconf}      

\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      





\usepackage{graphics}
\usepackage{amsmath,amssymb}
\usepackage[table]{xcolor}            \usepackage{subcaption}               \captionsetup[figure]{size=footnotesize,
                        skip=\abovecaptionskip,
                        labelsep=period}
  \captionsetup[table]{labelfont=small,
                       textfont={footnotesize,sc},
                       labelsep=newline}
\usepackage{siunitx}                  \sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
  \sisetup{range-phrase=~--~}
  \sisetup{binary-units=true}
  \sisetup{detect-weight=true,detect-inline-weight=math}
\usepackage[textsize=tiny]{todonotes} \usepackage{lipsum}                   \usepackage{dblfloatfix}              \usepackage{tabu}                     

\usepackage{url}
\def\UrlBreaks{\do\/\do-}

\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage{hyperref}

\hyphenation{U-Net}
\hyphenation{uNetXST}

\newcommand{\fig}[1]{Fig.~\ref{#1}}
\newcommand{\Eq}[1]{Equation~(\ref{#1})}
\newcommand{\eq}[1]{(\ref{#1})}
\newcommand{\tab}[1]{Table~\ref{#1}}
\newcommand{\chap}[1]{Chapter~\ref{#1}}
\newcommand{\sect}[1]{Section~\ref{#1}}
\newcommand{\app}[1]{Appendix~\ref{#1}}
\newcommand{\lst}[1]{Listing~\ref{#1}}

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}

\clubpenalty10000
\widowpenalty10000
\displaywidowpenalty=10000

\newcommand\copyrighttext{\footnotesize \copyright{ }2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}
\newcommand\copyrightnotice{\begin{tikzpicture}[remember picture,overlay]
    \node[anchor=south,yshift=10pt,xshift=7pt] at (current page.south) {\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{\copyrighttext}};
    \end{tikzpicture}}


\title{\LARGE \bf
  A Sim2Real Deep Learning Approach for the\\
  Transformation of Images from Multiple Vehicle-Mounted Cameras\\
  to a Semantically Segmented Image in Bird's Eye View*
}

\author{Lennart Reiher and Bastian Lampe, Lutz Eckstein
  \thanks{*This research is accomplished within the project "UNICAR\textit{agil}"~(FKZ~16EMO0289). We acknowledge the financial support for the project by the Federal Ministry of	Education and Research of Germany (BMBF).}
  \thanks{The authors contributed equally to this work. They are with the Institute for Automotive Engineering~(ika), 
  RWTH Aachen	University, 52074 Aachen, Germany.
  {\tt\small \{firstname.lastname\}@ika.rwth-aachen.de}}
  \thanks{Lutz Eckstein is head of the Institute for Automotive Engineering~(ika),
  RWTH Aachen	University, 52074 Aachen, Germany.
	{\tt\small lutz.eckstein@ika.rwth-aachen.de}}
}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}
\copyrightnotice


\begin{abstract}
  Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge. Distances can be more easily estimated when the camera perspective is transformed to a bird's eye view (BEV). For flat surfaces, \textit{Inverse Perspective Mapping} (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360 BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such a way that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. Source code and datasets are available at \url{https://github.com/ika-rwth-aachen/Cam2BEV}.
\end{abstract}


\section{Introduction}

In recent years, the development of automated vehicles~(AVs) has received substantial attention from both research and industry. One of the key elements of automated driving is the accurate perception of an AV's environment. It is essential for planning safe and efficient behavior.

Different types of environment representations can be computed, e.g.\ object lists or occupancy grids. Both require information on the world coordinates of elements in the environment. Among the different types of sensors commonly used to achieve an understanding of the environment, cameras are popular due to low cost and well-established computer vision techniques. Since monocular cameras can only provide information on locations in the image plane, a perspective transformation can be applied to images that results in a top-down or bird's eye view~(BEV). It is an approximation of the same scene as seen from a perspective in which the image plane aligns with the ground plane in front of the camera. The method used for transforming camera images to BEV is commonly referred to as \textit{Inverse Perspective Mapping}~(IPM)~\cite{MallotEtAl_InversePerspectiveMapping_1991}.

IPM assumes the world to be flat. Any three-dimensional object and changing road elevations violate this assumption. Mapping all pixels to a flat plane thus results in strong visual distortions of such objects. This impedes our goal of accurately locating objects such as other vehicles and vulnerable road users in the vehicle's environment. For this reason, images transformed through IPM often only serve as input to algorithms for lane detection or free space computation, for which the flat world assumption is often reasonable~\cite{BarHillelEtAl_RecentProgressRoad_2014}.

Even if errors introduced by IPM could be corrected, we are left with the task of detecting objects in the BEV. Deep learning approaches have proven to be powerful for tasks like semantic segmentation of images but usually require vast amounts of manually labeled data. Simulations can provide BEV images and their corresponding labels but suffer from the so-called reality gap: BEV images computed by a virtual camera in a simulated environment are rather dissimilar to e.g.\ a drone image captured above a vehicle in the real world, mostly due to unrealistic textures in the simulation. The generalization from a complex task learned in a simulation to the real world has therefore proven to be difficult so far. In order to reduce the reality gap, many approaches thus aim at making simulated data more realistic, e.g.~\cite{ZhaoEtAl_MultiSourceDomain_2019}. 

\begin{figure}[t]
  \captionsetup[subfigure]{skip=0pt}
  \begin{subfigure}[t]{0.24\linewidth}
    \captionsetup{belowskip=6pt}
    \caption*{Front camera}
    \includegraphics[width=\textwidth, height=0.5\textwidth]{figures/teaser/front_cc}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.24\linewidth}
    \captionsetup{belowskip=6pt}
    \caption*{Rear camera}
    \includegraphics[width=\textwidth, height=0.5\textwidth]{figures/teaser/rear_cc}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.24\linewidth}
    \captionsetup{belowskip=6pt}
    \caption*{Left camera}
    \includegraphics[width=\textwidth, height=0.5\textwidth]{figures/teaser/left_cc}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.24\linewidth}
    \captionsetup{belowskip=6pt}
    \caption*{Right camera}
    \includegraphics[width=\textwidth, height=0.5\textwidth]{figures/teaser/right_cc}
  \end{subfigure}

  \vspace{0.25\baselineskip}
  \captionsetup[subfigure]{skip=2pt}
  \begin{subfigure}[b]{0.4925\linewidth}
    \includegraphics[width=\textwidth, height=0.5\textwidth]{figures/teaser/homo_cc}
    \caption*{Homography}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.4925\linewidth}
    \includegraphics[width=\textwidth, height=0.5\textwidth]{figures/teaser/drone_cc_occl}
    \caption*{Ground Truth BEV}
  \end{subfigure}\caption{A homography can be applied to the four semantically segmented images from vehicle-mounted cameras to transform them to BEV. Our approach involves learning to compute an accurate BEV image without visual distortions.}
  \label{fig:TeaserImage}
\end{figure}

In this paper, we propose a methodology to obtain BEV images that are not subject to the errors introduced by the flatness-assumption underlying IPM. Instead of trying to make simulated images look more realistic, we remove mostly unnecessary texture from real-world data by computing semantically segmented camera images. We show how their use as input to our algorithm allows us to train a neural network on synthetic data only, while still being able to successfully perform the desired task on real-world data. With semantically segmented input, the algorithm has access to class information and is thus able to incorporate these into the correction of images produced by IPM. The output is a semantically segmented BEV of the input scene. Since the object shapes are preserved, the output can not only be used for determining free space but also to locate dynamic objects. In addition, the semantically segmented BEV images contain a color-coding for unknown areas, which are occluded in the original camera image. The image obtained through IPM and the desired ground truth BEV image are displayed in~\fig{fig:TeaserImage}.

The main contributions of this work are as follows:
\begin{itemize}
  \item We propose a methodology capable of transforming the images of multiple vehicle-mounted cameras to semantically segmented images in BEV.
  \item We design and compare two variations of our methodology using different neural network architectures, one of which we specifically design for the task.
  \item We design the process in such a way that no manual labeling of BEV images is required for training our neural network-based models.
  \item We show a successful real-world application of the trained models.
\end{itemize}


\section{Related Work}

Numerous works of literature address the perspective transformation to BEV. In the automotive context, both~\cite{SungEtAl_DevelopmentImageSynthesis_2012} and~\cite{ZhangEtAl_SurroundViewCamera_2014} deal with the synthesized transformation of multiple camera images to a top-down surround view. Most works are geometry-based and focus on an accurate depiction of the ground level.

Only few works combine the transformation to BEV with the task of scene understanding. However, object detection can give clues on an object's geometry, from which the transformation could benefit. Recently, the deep learning approaches presented below have shown how complex neural networks can aid in improving the classical IPM technique and contribute to environment perception.

The focus of~\cite{BrulsEtAl_RightAngledPerspective_2019} and~\cite{ZhuEtAl_GenerativeAdversarialFrontal_2019} is to correct the errors introduced by the IPM approach. Dynamic and three-dimensional objects are sought to be removed in the transformed BEV achieved by~\cite{BrulsEtAl_RightAngledPerspective_2019} to improve road scene understanding. In contrast, the method proposed in~\cite{ZhuEtAl_GenerativeAdversarialFrontal_2019} aims to synthesize an accurate BEV representation of an entire road scene as seen through a front-facing camera, including dynamic objects. Due to the generative nature of the underlying task, both methods employ Generative Adversarial Networks~\cite{Schmidhuber_MakingWorldDifferentiable_1990, GoodfellowEtAl_GenerativeAdversarialNets_2014}.

Palazzi~et~al.~\cite{PalazziEtAl_LearningMapVehicles_2017} present the prediction of vehicle bounding boxes in BEV from the images of a front-facing camera. Roddick~et~al.~\cite{RoddickEtAl_OrthographicFeatureTransform_2019} demonstrate advanced object detection in computing three-dimensional bounding boxes by using an in-network orthographic feature transform to a three-dimensional discretization of space.

A semantic road understanding in a top-down frame leading to a coarse and static semantic map is achieved in~\cite{SenguptaEtAl_AutomaticDenseVisual_2012}. Similar to~\cite{BrulsEtAl_RightAngledPerspective_2019}, this approach tries to remove dynamic traffic participants.

To the best of our knowledge, the only source pursuing the idea of directly transforming multiple semantically segmented images to BEV is a blog article~\cite{Dziubinski_SemanticSegmentationSemantic_2019}. It lacks detailed testing and an application to real-world data though. The designed neural network is a fully-convolutional autoencoder and has multiple weaknesses, e.g.\ the range of an accurate object detection is relatively low.


\section{Methodology}\label{sec:Methodology}

We base our methodology on the use of a Convolutional Neural Network~(CNN), a class of deep neural networks commonly used for image analysis. Most popular CNNs process only one input image. In order to fuse images from multiple cameras mounted on a vehicle, a single-input network could take as input multiple images concatenated along their channel dimension. However, for the task at hand, this would result in spatial inconsistency between input and output images. Convolutional layers operate locally, i.e.\ information in particular parts of the input are mapped to approximately the same part of the output. An end-to-end learning approach for the presented problem however needs to be able to handle images from multiple viewpoints. This suggests the need for an additional mechanism.

IPM certainly introduces errors, but the technique is capable of producing an image at least similar to a ground truth BEV image. Due to this similarity, it seems reasonable to incorporate IPM as a mechanism to provide better spatial consistency between input and output images. The image resulting from IPM is also used as an intermediate guiding view in~\cite{BrulsEtAl_RightAngledPerspective_2019} and~\cite{ZhuEtAl_GenerativeAdversarialFrontal_2019}. In the following, we present two variations of our neural network-based methodology that both include the application of IPM. Before introducing the two neural network architectures, the applied data preprocessing techniques are explained in detail.

\subsection{Dealing with Occlusions}

When only considering the input domain and the desired output for this task, one difficulty immediately becomes apparent: traffic participants and static obstacles may occlude parts of the environment making predictions for those areas in a BEV image mostly impossible. As an example, such occlusions would occur when driving behind a truck: what is happening in front of the truck cannot reliably be determined only from vehicle-mounted camera images.

In order to formulate a well-posed problem, an additional semantic class needs to be introduced for areas in BEV, which are occluded in the camera perspectives. This class is introduced to the ground truth label images in a preprocessing step. For each vehicle camera, virtual rays are cast from its mount position to the edges of the semantically segmented ground truth BEV image. The rays are only cast to edge pixels that lie within the specific camera's field of view. All pixels along these rays are processed to determine their occlusion state according to the following rules:
\begin{itemize}
  \item some semantic classes always block sight \ \vect{x}_w \in \mathbb{R}^4  \vect{x}_i \in \mathbb{R}^3  \mat{P} \in \mathbb{R}^{3 \times 4} 
  \vect{x}_i = \mat{P} \vect{x}_w \,.
  \label{eq:World2ImageProjection}
 \mat{K}  \mat{R}  \vect{t} 
  \mat{P} = \mat{K} \left[ \mat{R} \vert \vect{t} \right] \,.
  \label{eq:ProjectionMatrix}
 \mat{M} \in \mathbb{R}^{4 \times 3}  \vect{x}_r \in \mathbb{R}^3 
  \vect{x}_w = \mat{M} \vect{x}_r \,,
  \label{eq:Road2WorldProjection}

  \vect{x}_r = \left( \mat{P} \mat{M} \right)^{-1} \vect{x}_i \,.
 \mat{M}  \left( \mat{P} \mat{M} \right)  \mat{P}  \vartheta  \vert\vert  \vartheta  \vert\vert -1em]
      uNetXST & \textbf{\num{98.10}} & \num{93.36} & \textbf{\num{13.56}} & \textbf{\num{80.90}} & \num{65.82} & \num{62.10} & \textbf{\num{32.43}} & \num{88.99} & \textbf{\num{97.27}} & \textbf{\num{86.62}}\\
      DL Xception & \num{98.06} & \textbf{\num{94.02}} & \num{6.93} & \num{80.21} & \textbf{\num{65.94}} & \textbf{\num{65.98}} & \num{30.80} & \textbf{\num{89.05}} & \num{97.09} & \num{85.42}\\
      DL MobileNetV2 & \num{96.93} & \num{91.51} & \num{0.00} & \num{76.05} & \num{60.33} & \num{64.92} & \num{16.79} & \num{85.83} & \num{96.28} & \num{77.31}\\
      DL Xception* & \num{96.60} & \num{88.81} & \num{0.20} & \num{68.18} & \num{53.63} & \num{32.80} & \num{2.74} & \num{84.84} & \num{95.85} & \num{77.61}\\
      DL MobileNetV2* & \num{94.68} & \num{84.12} & \num{0.00} & \num{59.09} & \num{43.91} & \num{22.39} & \num{3.75} & \num{79.75} & \num{94.35} & \num{68.83}\\
      uNetX* & \num{89.80} & \num{77.15} & \num{0.00} & \num{42.36} & \num{24.27} & \num{13.59} & \num{0.00} & \num{75.43} & \num{91.16} & \num{45.70}\\
      Homography & \num{77.32} & \num{75.78} & \num{0.07} & \num{4.27} & \num{8.56} & \num{8.55} & \num{0.38} & \num{37.06} & \num{89.74} & \num{0.00}\\
    \end{tabu}
  \end{center}
  \label{tab:IoU}
\end{table*}


\section{Experimental Setup}

In order to evaluate the methodology presented before, we train the neural networks entirely on simulated data. In the following, we present the synthetic dataset and the training setup.

\subsection{Data Acquisition}

The data used to train and assess our proposed methodology is created in the simulation environment \textit{Virtual Test Drive~(VTD)}~\cite{Neumann-CoselEtAl_VirtualTestDrive_2009}. A recording toolchain allows the generation of potentially arbitrarily many sample images including their corresponding label.

In the simulation, the ego vehicle is equipped with four identical virtual wide-angle cameras covering a full \ang{360} surround view. Ground truth data is provided by a virtual drone camera. The BEV ground truth image is centered above the ego vehicle and has an approximate field of view of . 

Both input and ground truth images are recorded at a resolution of . All virtual cameras produce both realistic and semantically segmented images. For semantic segmentation, nine different semantic classes are considered for the visible areas (\textit{road}, \textit{sidewalk}, \textit{person}, \textit{car}, \textit{truck}, \textit{bus}, \textit{bike}, \textit{obstacle}, \textit{vegetation}).

As a trade-off between keeping simulation time low and maximizing data variety, images are recorded at \SI{2}{Hz}. In total, the dataset contains approximately \num{33000} samples for training and \num{3700} samples for validation, where each sample is a set of multiple input images and one ground truth label. As we only require our method to operate in specified spatial areas, the static elements in the simulated world (i.e.\ \textit{roads}, \textit{buildings}, etc.) remain the same between training and validation data.

In order to later test a real-world application of our methods, a second synthetic dataset is recorded for usage with a single front camera. In this scenario, only three classes are considered for visible areas (\textit{road}, \textit{vehicle}, \textit{occupied space}) and only the area in front of the vehicle is of interest. For this reason, the ground truth images are left-aligned with the ego vehicle. The second dataset contains approximately \num{32000} samples for training and \num{3200} samples for validation.

\subsection{Training Setup}

To keep training and inference time relatively short, network input images and target labels are center-cropped to an aspect ratio of 2:1 and resized to a resolution of . The input images are converted to a one-hot representation. In order to counter class imbalance in the dataset, the loss function is modified to weigh semantic classes according to the logarithm of their relative occurrence. During training, the Adam optimizer with a learning rate of 
\num{1e-4} and parameters  and  is applied to batches of size \num{5}.

\subsection{Evaluation Metrics}

The \textit{Intersection-over-Union}~(IoU) score is used as the main metric for model performance on the task of predicting a certain semantic class. Class IoU scores are averaged into a single \textit{Mean Intersection-over-Union}~(MIoU) score.


\section{Results and Discussion}\label{sect:ResultsAndDiscussion}

In this section, we compare the performance of our method variations to each other and discuss the overall improvements of our methodology compared to the classical IPM technique. The standard homography image obtained by IPM is used as the baseline for our evaluation.

We present results for the two single-input models \textit{DeepLab Xception} and \textit{DeepLab MobileNetV2} as well as the multi-input model \textit{uNetXST}. In order to quantify the benefit of incorporating homographies into our approach, we also present results for alternative model versions without IPM. For the model of our first method variation, \textit{DeepLab}, this means to simply concatenate the multiple input images along their channel dimension, as explained in the beginning of~\sect{sec:Methodology}. For the \textit{uNetXST} model, this means to ablate the \textit{Spatial Transformer} units. In the following, these simplified models are denoted by an asterisk~(*).

Additionally, we qualitatively test the hypothesis that the proposed methodology can generalize from simulated to real-world data.

\subsection{Results on Synthetic Data}

\setcounter{table}{0}
\begin{table}[t]
  \caption{MIoU Scores (\%) on the Validation Set}
  \begin{center}
    \begin{tabular}{l c}
      \textbf{Model} & \textbf{MIoU}\\
      \hline
      \^{\circ} multi-camera setup, which will require good semantic segmentation performance not only on front camera images.




\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}