\documentclass[submission,copyright,creativecommons]{eptcs}
\providecommand{\event}{TERMGRAPH 2011} 


\usepackage{graphicx}
\usepackage{multirow}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amstext}
\usepackage{mathpartir} \usepackage{graphicx}
\usepackage{wasysym}
\usepackage{float}
\usepackage{enumerate}
\usepackage[arrow, matrix, curve]{xy}
\usepackage[justification=centering,singlelinecheck=off]{caption}
\usepackage{color}
\usepackage{url}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{sidecap}
\usepackage{comment}
\usepackage{MnSymbol}
\usepackage{trfsigns}
\usepackage{fancyvrb}

\usepackage[utf8x]{inputenx}
\usepackage{tipa}

\usepackage{breakurl}              
\newtheorem{theorem}{Theorem} \newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}





\newcommand{\lambdaletrec}{\ensuremath{\lambda_\textit{letrec}}}
\newcommand\lambdamu{\ensuremath{\lambda\mu}}

\newcommand{\sletrec}{\texttt{letrec}}
\newcommand{\letrec}{\texttt{letrec}}
\newcommand{\beIn}{~\texttt{in}~}

\newcommand{\ametavar}{X}
\newcommand{\bmetavar}{Y}
\newcommand{\cmetavar}{Z}


\newcommand\listing[1]{\begin{quotation}\noindent\includegraphics{listings/#1}\end{quotation}}
\newcommand\haskell[1]{\ensuremath{\mathit{#1}}}

\newcommand{\slabs}{{\lambda}}
\newcommand{\slapp}{{@}}
\newcommand{\lbind}[1]{{\slabs{#1}}}
\newcommand{\labs}[2]{\lbind{#1}.\hspace*{0.5pt}{#2}}
\newcommand{\lapp}[2]{{#1}{#2}}

\newcommand{\scons}{{:}}
\newcommand{\nil}{{\mit nil}}

\newcommand{\cxthole}{[\hspace*{0.5pt}]}
\newcommand{\icxthole}[1]{\cxthole_{#1}}

\newcommand{\avar}{x}
\newcommand{\bvar}{y}
\newcommand{\cvar}{z}

\newcommand{\afunvar}{f}
\newcommand{\bfunvar}{g}
\newcommand{\cfunvar}{h}




\newcommand{\alter}{M}
\newcommand{\blter}{N}
\newcommand{\clter}{P}

\newcommand{\allter}{t}
\newcommand{\bllter}{s}
\newcommand{\cllter}{u}
\newcommand{\dllter}{v}

\newcommand{\slet}{\texttt{let}}
\newcommand{\sletin}{\texttt{let\_in}}
\newcommand{\sleti}[1]{\slet_{#1}}
\newcommand{\sletini}[1]{\slet_{#1}\texttt{\_in}}
\newcommand{\leti}[2]{\sleti{#1}{\hspace*{2pt}}{#2}}
\newcommand{\letibeIn}[3]{\sleti{#1}{\hspace*{2pt}}{#2}{~\texttt{in}~}{#3}}
\newcommand{\letini}[2]{\sletini{#1}{\hspace*{2pt}}{#2}}
\newcommand{\letbeIn}[2]{\slet{\hspace*{2pt}}{#1}{\hspace*{3pt}\texttt{in}\hspace*{5pt}}{#2}}

\newcommand{\seq}{=}


\newcommand{\sortTer}{\text{`term'}}\newcommand{\sortDef}{\text{`definition'}}

\newcommand{\typetrm}{\textsf{trm}}
\newcommand{\typedef}{\textsf{def}}

\newcommand{\atype}{\tau}



\newcommand{\Vars}{\mathit{Var}}
\newcommand{\BaseTypes}{\mathit{BTypes}}
\newcommand{\Types}{\mathit{Types}}



\newcommand{\sred}{\rightarrow}
\newcommand{\red}{\mathrel{\sred}}
\newcommand{\sinvred}{\leftarrow}
\newcommand{\invred}{\mathrel{\sinvred}}

\newcommand{\indap}[2]{#1_{#2}}
\newcommand{\sredi}{\indap{\rightarrow}}
\newcommand{\redi}[1]{\mathrel{\sredi{#1}}}
\newcommand{\sinvredi}{\indap{\leftarrow}}
\newcommand{\invredi}[1]{\mathrel{\sinvredi{#1}}}

\newcommand{\smred}{\twoheadrightarrow}
\newcommand{\mred}{\mathrel{\smred}}
\newcommand{\sminvred}{\twoheadleftarrow}
\newcommand{\minvred}{\mathrel{\sminvred}}

\newcommand{\smredi}{\indap{\twoheadrightarrow}}
\newcommand{\mredi}[1]{\mathrel{\smredi{#1}}}
\newcommand{\sminvredi}{\indap{\twoheadleftarrow}}
\newcommand{\minvredi}[1]{\mathrel{\sminvredi{#1}}}

\newcommand{\thsp}{-1.74ex}
\newcommand{\threeheadrightarrow}{{\twoheadrightarrow\hspace*\thsp\twoheadrightarrow}}
\newcommand{\threeheadleftarrow}{{\twoheadleftarrow\hspace*\thsp\twoheadleftarrow}}

\newcommand{\sinfred}{\threeheadrightarrow}
\newcommand{\infred}{\mathrel{\sinfred}}
\newcommand{\sinvinfred}{\threeheadleftarrow}
\newcommand{\invinfred}{\mathrel{\sinvinfred}}

\newcommand{\sinfredi}{\indap{\threeheadrightarrow}}
\newcommand{\infredi}[1]{\mathrel{\sinfredi{#1}}}
\newcommand{\sinvinfredi}{\indap{\threeheadleftarrow}}
\newcommand{\invinfredi}[1]{\mathrel{\sinvinfredi{#1}}}

\newcommand{\sbetared}{\sredi{\beta}}
\newcommand{\betared}{\mathrel{\sbetared}}
\newcommand{\sinvbetared}{\sinvredi{\beta}}
\newcommand{\invbetared}{\mathrel{\sinvbetared}}

\newcommand{\smbetared}{\smredi{\beta}}
\newcommand{\mredbeta}{\mathrel{\smbetared}}
\newcommand{\sminvbetared}{\sminvredi{\beta}}
\newcommand{\minvbetared}{\mathrel{\sminvbetared}}

\newcommand{\sinfbetared}{\sinfredi{\beta}}
\newcommand{\infbetared}{\mathrel{\sinfbetared}}
\newcommand{\sinvinfbetared}{\sinvinfredi{\beta}}
\newcommand{\invinfbetared}{\mathrel{\sinvinfbetared}}

\newcommand{\genbetaredsubscript}{g\hspace*{0pt}\beta}
\newcommand{\sgenbetared}{\sredi{\genbetaredsubscript}}
\newcommand{\genbetared}{\mathrel{\sgenbetared}}
\newcommand{\sinvgenbetared}{\sinvredi{g\hspace*{0pt}\beta}}
\newcommand{\invgenbetared}{\mathrel{\sinvgenbetared}}

\newcommand{\smgenbetared}{\smredi{g\hspace*{0pt}\beta}}
\newcommand{\mbetared}{\mathrel{\smgenbetared}}
\newcommand{\sminvgenbetared}{\sminvredi{g\hspace*{0pt}\beta}}
\newcommand{\minvgenbetared}{\mathrel{\sminvgenbetared}}

\newcommand{\sinfgenbetared}{\sinfredi{g\hspace*{0pt}\beta}}
\newcommand{\infgenbetared}{\mathrel{\sinfgenbetared}}
\newcommand{\sinvinfgenbetared}{\sinvinfredi{g\hspace*{0pt}\beta}}
\newcommand{\invinfgenbetared}{\mathrel{\sinvinfgenbetared}}

\newcommand{\setared}{\sredi{\eta}}
\newcommand{\etared}{\mathrel{\setared}}
\newcommand{\sinvetared}{\sinvredi{\eta}}
\newcommand{\invetared}{\mathrel{\sinvetared}}

\newcommand{\smetared}{\smredi{\eta}}
\newcommand{\mredeta}{\mathrel{\smetared}}
\newcommand{\sminvetared}{\sminvredi{\eta}}
\newcommand{\minvetared}{\mathrel{\sminvetared}}

\newcommand{\sinfetared}{\sinfredi{\eta}}
\newcommand{\infetared}{\mathrel{\sinfetared}}
\newcommand{\sinvinfetared}{\sinvinfredi{\eta}}
\newcommand{\invinfetared}{\mathrel{\sinvinfetared}}


\newcommand{\veceta}{\vec{\eta}}
\newcommand{\svecetared}{\sredi{\veceta}}
\newcommand{\vecetared}{\mathrel{\svecetared}}
\newcommand{\sinvvecetared}{\sinvredi{\veceta}}
\newcommand{\invvecetared}{\mathrel{\sinvvecetared}}

\newcommand{\smvecetared}{\smredi{\veceta}}
\newcommand{\mvecredeta}{\mathrel{\smvecetared}}
\newcommand{\sminvvecetared}{\sminvredi{\veceta}}
\newcommand{\minvvecetared}{\mathrel{\sminvvecetared}}

\newcommand{\sinfvecetared}{\sinfredi{\veceta}}
\newcommand{\infvecetared}{\mathrel{\sinfvecetared}}
\newcommand{\sinvinfvecetared}{\sinvinfredi{\veceta}}
\newcommand{\invinfvecetared}{\mathrel{\sinvinfvecetared}}


\newcommand{\vecetazero}{\vec{\eta}_0}
\newcommand{\svecetazerored}{\sredi{\vecetazero}}
\newcommand{\vecetazerored}{\mathrel{\svecetazerored}}
\newcommand{\sinvvecetazerored}{\sinvredi{\vecetazero}}
\newcommand{\invvecetazerored}{\mathrel{\sinvvecetazerored}}

\newcommand{\smvecetazerored}{\smredi{\vecetazero}}
\newcommand{\mvecredetazero}{\mathrel{\smvecetazerored}}
\newcommand{\sminvvecetazerored}{\sminvredi{\vecetazero}}
\newcommand{\minvvecetazerored}{\mathrel{\sminvvecetazerored}}

\newcommand{\sinfvecetazerored}{\sinfredi{\vecetazero}}
\newcommand{\infvecetazerored}{\mathrel{\sinfvecetazerored}}
\newcommand{\sinvinfvecetazerored}{\sinvinfredi{\vecetazero}}
\newcommand{\invinfvecetazerored}{\mathrel{\sinvinfvecetazerored}}


\newcommand{\vecetazeroperm}{\vec{\eta}_{0}^{\text{per}}}
\newcommand{\svecetazeropermred}{\sredi{{\vecetazeroperm}}}
\newcommand{\vecetazeropermred}{\mathrel{\svecetazeropermred}}
\newcommand{\sinvvecetazeropermred}{\sinvredi{\vecetazeroperm}}
\newcommand{\invvecetazeropermred}{\mathrel{\sinvvecetazeropermred}}

\newcommand{\smvecetazeropermred}{\smredi{{\vecetazeroperm}}}
\newcommand{\mvecetazeropermred}{\mathrel{\smvecetazeropermred}}
\newcommand{\sminvvecetazeropermred}{\sminvredi{{\vecetazeroperm}}}
\newcommand{\minvvecetazeropermred}{\mathrel{\sminvvecetazeropermred}}

\newcommand{\sinfvecetazeropermred}{\sinfredi{{\vecetazeroperm}}}
\newcommand{\infvecetazeropermred}{\mathrel{\sinfvecetazeropermred}}
\newcommand{\sinvinfvecetazeropermred}{\sinvinfredi{{\vecetazeroperm}}}
\newcommand{\invinfvecetazeropermred}{\mathrel{\sinvinfvecetazeropermred}}


\newcommand{\smured}{\sredi{\mu}}
\newcommand{\mured}{\mathrel{\smured}}
\newcommand{\sinvmured}{\sinvredi{\mu}}
\newcommand{\invmured}{\mathrel{\sinvmured}}

\newcommand{\smmured}{\smredi{\mu}}
\newcommand{\mredmu}{\mathrel{\smmured}}
\newcommand{\sminvmured}{\sminvredi{\mu}}
\newcommand{\minvmured}{\mathrel{\sminvmured}}

\newcommand{\sinfmured}{\sinfredi{\mu}}
\newcommand{\infmured}{\mathrel{\sinfmured}}
\newcommand{\sinvinfmured}{\sinvinfredi{\mu}}
\newcommand{\invinfmured}{\mathrel{\sinvinfmured}}

\newcommand{\letrecredsubscript}{\texttt{let}}
\newcommand{\sletrecred}{\sredi{\letrecredsubscript}}
\newcommand{\letrecred}{\mathrel{\sletrecred}}
\newcommand{\sinvletrecred}{\sinvredi{\letrecredsubscript}}
\newcommand{\invletrecred}{\mathrel{\sinvletrecred}}

\newcommand{\smletrecred}{\smredi{\letrecredsubscript}}
\newcommand{\mletrecred}{\mathrel{\smletrecred}}
\newcommand{\sminvletrecred}{\sminvredi{\letrecredsubscript}}
\newcommand{\minvletrecred}{\mathrel{\sminvletrecred}}

\newcommand{\sinfletrecred}{\sinfredi{\letrecredsubscript}}
\newcommand{\infletrecred}{\mathrel{\sinfletrecred}}
\newcommand{\sinvinfletrecred}{\sinvinfredi{\letrecredsubscript}}
\newcommand{\invinfletrecred}{\mathrel{\sinvinfletrecred}}


\newcommand{\unfoldredsubscript}{\medtriangledown}
\newcommand{\sunfoldred}{\sredi{\unfoldredsubscript}}
\newcommand{\unfoldred}{\mathrel{\sunfoldred}}
\newcommand{\sinvunfoldred}{\sinvredi{\unfoldredsubscript}}
\newcommand{\invunfoldred}{\mathrel{\sinvunfoldred}}

\newcommand{\smunfoldred}{\smredi{\unfoldredsubscript}}
\newcommand{\munfoldred}{\mathrel{\smunfoldred}}
\newcommand{\sminvunfoldred}{\sminvredi{\unfoldredsubscript}}
\newcommand{\minvunfoldred}{\mathrel{\sminvunfoldred}}

\newcommand{\sinfunfoldred}{\sinfredi{\unfoldredsubscript}}
\newcommand{\infunfoldred}{\mathrel{\sinfunfoldred}}
\newcommand{\sinvinfunfoldred}{\sinvinfredi{\unfoldredsubscript}}
\newcommand{\invinfunfoldred}{\mathrel{\sinvinfunfoldred}}


\newcommand{\betaletrecredsubscript}{\beta,\letrecredsubscript}
\newcommand{\sbetaletrecred}{\sredi{\betaletrecredsubscript}}
\newcommand{\betaletrecred}{\mathrel{\sbetaletrecred}}
\newcommand{\sinvbetaletrecred}{\sinvredi{\betaletrecredsubscript}}
\newcommand{\invbetaletrecred}{\mathrel{\sinvbetaletrecred}}

\newcommand{\smbetaletrecred}{\smredi{\betaletrecredsubscript}}
\newcommand{\mredbetaletrec}{\mathrel{\smbetaletrecred}}
\newcommand{\sminvbetaletrecred}{\sminvredi{\betaletrecredsubscript}}
\newcommand{\minvbetaletrecred}{\mathrel{\sminvbetaletrecred}}

\newcommand{\sinfbetaletrecred}{\sinfredi{\betaletrecredsubscript}}
\newcommand{\infbetaletrecred}{\mathrel{\sinfbetaletrecred}}
\newcommand{\sinvinfbetaletrecred}{\sinvinfredi{\betaletrecredsubscript}}
\newcommand{\invinfbetaletrecred}{\mathrel{\sinvinfbetaletrecred}}


\newcommand{\genbetaletrecredsubscript}{\letrecredsubscript,\genbetaredsubscript}
\newcommand{\sgenbetaletrecred}{\sredi{\genbetaletrecredsubscript}}
\newcommand{\genbetaletrecred}{\mathrel{\sgenbetaletrecred}}
\newcommand{\sinvgenbetaletrecred}{\sinvredi{\genbetaletrecredsubscript}}
\newcommand{\invgenbetaletrecred}{\mathrel{\sinvgenbetaletrecred}}

\newcommand{\smgenbetaletrecred}{\smredi{\genbetaletrecredsubscript}}
\newcommand{\mredgenbetaletrec}{\mathrel{\smgenbetaletrecred}}
\newcommand{\sminvgenbetaletrecred}{\sminvredi{\genbetaletrecredsubscript}}
\newcommand{\minvgenbetaletrecred}{\mathrel{\sminvgenbetaletrecred}}

\newcommand{\sinfgenbetaletrecred}{\sinfredi{\genbetaletrecredsubscript}}
\newcommand{\infgenbetaletrecred}{\mathrel{\sinfgenbetaletrecred}}
\newcommand{\sinvinfgenbetaletrecred}{\sinvinfredi{\genbetaletrecredsubscript}}
\newcommand{\invinfgenbetaletrecred}{\mathrel{\sinvinfgenbetaletrecred}}


\newcommand{\betaunfoldredsubscript}{\unfoldredsubscript\hspace*{-1pt},\hspace*{0.75pt}\beta}
\newcommand{\sbetaunfoldred}{\sredi{\betaunfoldredsubscript}}
\newcommand{\betaunfoldred}{\mathrel{\sbetaunfoldred}}
\newcommand{\sinvbetaunfoldred}{\sinvredi{\betaunfoldredsubscript}}
\newcommand{\invbetaunfoldred}{\mathrel{\sinvbetaunfoldred}}

\newcommand{\smbetaunfoldred}{\smredi{\betaunfoldredsubscript}}
\newcommand{\mbetaunfoldred}{\mathrel{\smbetaunfoldred}}
\newcommand{\sminvbetaunfoldred}{\sminvredi{\betaunfoldredsubscript}}
\newcommand{\minvbetaunfoldred}{\mathrel{\sminvbetaunfoldred}}

\newcommand{\sinfbetaunfoldred}{\sinfredi{\betaunfoldredsubscript}}
\newcommand{\infbetaunfoldred}{\mathrel{\sinfbetaunfoldred}}
\newcommand{\sinvinfbetaunfoldred}{\sinvinfredi{\betaunfoldredsubscript}}
\newcommand{\invinfbetaunfoldred}{\mathrel{\sinvinfbetaunfoldred}}


\newcommand{\genbetaunfoldredsubscript}{\unfoldredsubscript\hspace*{-1pt},\hspace*{0.75pt}\genbetaredsubscript}\newcommand{\sgenbetaunfoldred}{\sredi{\genbetaunfoldredsubscript}}
\newcommand{\genbetaunfoldred}{\mathrel{\sgenbetaunfoldred}}
\newcommand{\sinvgenbetaunfoldred}{\sinvredi{\genbetaunfoldredsubscript}}
\newcommand{\invgenbetaunfoldred}{\mathrel{\sinvgenbetaunfoldred}}

\newcommand{\smgenbetaunfoldred}{\smredi{\genbetaunfoldredsubscript}}
\newcommand{\mgenbetaunfoldred}{\mathrel{\smgenbetaunfoldred}}
\newcommand{\sminvgenbetaunfoldred}{\sminvredi{\genbetaunfoldredsubscript}}
\newcommand{\minvgenbetaunfoldred}{\mathrel{\sminvgenbetaunfoldred}}

\newcommand{\sinfgenbetaunfoldred}{\sinfredi{\genbetaunfoldredsubscript}}
\newcommand{\infgenbetaunfoldred}{\mathrel{\sinfgenbetaunfoldred}}
\newcommand{\sinvinfgenbetaunfoldred}{\sinvinfredi{\genbetaunfoldredsubscript}}
\newcommand{\invinfgenbetaunfoldred}{\mathrel{\sinvinfgenbetaunfoldred}}

\newcommand\binds{\mathrel{\scriptsize\fourier\hspace{-0.3em}}}
\newcommand\blackhole{\bullet}




\newcommand{\sopeq}{=^{\infty}_{\unfoldredsubscript,\genbetaredsubscript}}\newcommand{\opeq}{\mathrel{\sopeq}}
\newcommand{\sappbisim}{{\sim^{B}}}
\newcommand{\appbisim}{\mathrel{\sappbisim}}





\newcommand{\sredeq}{{\to^{=}}}
\newcommand{\redeq}{\mathrel{\sredeq}}
\newcommand{\sinvredeq}{{\leftarrow^{=}}}
\newcommand{\invredeq}{\mathrel{\sinvredeq}}


\newcommand{\stcred}{\rightarrow^+}
\newcommand{\tcred}{\mathrel{\stcred}}






\newcommand{\funap}[2]{{#1}({#2})}
\newcommand{\bfunap}[3]{{#1}({#2,#3})}

\newcommand{\nbd}{\nobreakdash}  
\newcommand{\nbde}{\nobreakdash-\hspace*{0pt}}

\newcommand{\figpath}{figs}

\newcommand{\sdefdby}{{:=}}
\newcommand{\defdby}{\mathrel{\sdefdby}}

\newcommand\sep[1]{&#1&} 

\renewcommand\;{\,}

\newcommand{\nats}{\mathbb{N}}



\renewcommand\textbeta{\ensuremath{\beta}}
\renewcommand\textlambda{\ensuremath{\lambda}}
\renewcommand\textmu{\ensuremath\mu}
\newcommand\textalpha{}
\newcommand\texteta{}
\renewcommand\textgamma{\ensuremath{\gamma}}
\newcommand\textGamma{\ensuremath{\Gamma}}
\newcommand\textDelta{\ensuremath{\Delta}}
\newcommand\Tau{T}
\newcommand\Rho{R}

\newcommand\T{\ensuremath{\vdash}} 


\newcommand{\saperm}{\pi}
\newcommand{\aperm}{\funap{\saperm}}


\newcommand{\sdiredge}{{\rightarrowtail}}
\newcommand{\diredge}{\mathrel{\sdiredge}}

\newcommand{\srtcdiredge}{{\rightarrowtail^*}}
\newcommand{\rtcdiredge}{\mathrel{\srtcdiredge}\hspace*{-2pt}}

\newcommand{\snotrtcdiredge}{{{\not\rightarrowtail}^*}}
\newcommand{\notrtcdiredge}{\mathrel{\snotrtcdiredge}\hspace*{-2pt}}

\newcommand{\stcdiredge}{{\rightarrowtail^+}}
\newcommand{\tcdiredge}{\mathrel{\stcdiredge}\hspace*{-2pt}}

\newcommand{\snottcdiredge}{{{\not\rightarrowtail}^+}}
\newcommand{\nottcdiredge}{\mathrel{\snottcdiredge}\hspace*{-2pt}}



\newcommand{\sdom}[1]{{{\mit dom}_{#1}}}
\newcommand{\dom}[3]{\bfunap{\sdom{#1}}{#2}{#3}}

\newcommand{\sstrongdom}[1]{{{\mit sdom}_{#1}}}
\newcommand{\strongdom}[3]{\bfunap{\sstrongdom{#1}}{#2}{#3}}

\newcommand{\adigraph}{G}
\newcommand{\verts}{V}



\newcommand{\pair}[2]{\langle {#1},\hspace*{0.5pt} {#2} \rangle}

\newcommand{\aDeriv}{{\cal D}}
\newcommand{\aDerivtilde}{\tilde{\aDeriv}}
 
\raggedbottom

\title{Repetitive Reduction Patterns\\ in Lambda Calculus with {\tt letrec}\\
       ({\it Work in Progress})\thanks{Funded by the NWO-project \emph{Realising Optimal Sharing}}}

\author{
  Jan Rochel
    \institute{Utrecht University\\ Utrecht, The Netherlands}
    \institute{Department of Information and Computing Sciences\\
               Information and Software Systems}
    \email{J.Rochel@cs.uu.nl}
\and
  Clemens Grabmayer
    \institute{Utrecht University\\Utrecht, The Netherlands}
    \institute{Department of Philosophy\\
               Theoretical Philosophy}
    \email{Clemens.Grabmayer@phil.uu.nl}
}
\def\titlerunning{Repetitive Reduction Patterns in Lambda Calculus with {\tt letrec}}
\def\authorrunning{Rochel, Grabmayer}

\begin{document}
\maketitle

\begin{abstract}
For the λ-calculus with \texttt{letrec} we develop an optimisation, which
is based on the contraction of a certain class of `future' (also: \emph{virtual})
redexes.

In the implementation of functional programming languages it is common practice
to perform β\nbd-reductions at compile time whenever possible in order to produce
code that requires fewer reductions at run-time. This is, however, in principle
limited to redexes and created redexes that are `visible' (in the sense that
they can be contracted without the need for unsharing), and cannot generally be
extended to redexes that are concealed by sharing constructs such as
\texttt{letrec}. 
In the case of recursion, concealed redexes become visible only after
unwindings during evaluation, and then have to be contracted time and again.

We observe that in some cases such redexes exhibit a certain form of repetitive
behaviour at run time.
We describe an analysis for identifying binders that give rise to such
repetitive reduction patterns, and eliminate them by a sort of predictive
contraction.
Thereby these binders are lifted out of recursive positions or eliminated altogether,
as a result alleviating the amount of β\nbd-reductions required for each recursive iteration. 

Both our analysis and simplification are suitable to be integrated into existing
compilers for functional programming languages as an additional optimisation phase.
With this work we hope to contribute to increasing the efficiency of executing
programs written in such languages.
\end{abstract}

In this extended abstract we report on work in progress carried out within the
framework of the NWO project \emph{Realising Optimal Sharing}. Instead of
discussing optimal reduction in the λ\nbd-calculus, however, here we are concerned
with a static analysis of -terms which aims at
removing \nbd-redexes that are concealed by recursion constructs and
cause cyclic migration of arguments during evaluation. We have to stress that
our research on this particular topic is still in an early phase. 



\section{Introduction}\label{sec:intro}


In this work we study terms in , i.e.\ in \nbde{}calculus with an explicit \nbde{}construct for recursive definitions,
that exhibit a form of repetitive reduction pattern when evaluated.
We try to identify a class of such terms
for which this behaviour can be avoided by a transformation into a term with,
in some sense, the same semantic denotation.
Even though the presented optimisation can be described directly for \nbd-terms, 
and hence is applicable in all functional languages of which  is a meaningful abstraction, 
we will use Haskell to denote examples of such terms and their optimised equivalents.
Additionally, we depict terms as λ-graphs with explicit
sharing-nodes (\textit{multiplexers}) as used, for example, in
\cite{AspertiGuerriniOptImpl}.



A function well-known to Haskell programmers is the \haskell{repeat} function that
generates an infinite, constant stream of the supplied argument. A definition
is easily found, namely: \listing{repeat}
An experienced Haskell programmer, however, would spot a `space leak',
which refers to an  memory consumption for generating  stream elements
while  is possible, due to lazy evaluation. Therefore in the Haskell
standard libraries that function is defined as: \listing{repeat_eff}
The exact reasons for this difference in efficiency involve the characteristics
of the deployed Haskell compiler and run-time system. A more direct and
theoretical explanation can be attempted within the formal framework of :
The improved variant of \haskell{repeat} does not require any β-reductions to
produce further stream elements. That becomes apparent by the λ-graphs and
their infinite unfoldings (Fig. \ref{repeat_graphs}). We use a rewriting
relation arrow indexed by a triangle () to mark unfolding,
regardless of whether sharing is expressed by a multiplexer or a
\letrec-binding.

\begin{figure}[htp]
\begin{center}
\raisebox{-0.5\height}{\includegraphics{figs/repeat.pdf}}
\hspace{1mm}\hspace{1mm}
\raisebox{-0.5\height}{\includegraphics{figs/pstricks/repeat_unfold.pdf}}
\hspace{8mm}\hspace{8mm}
\raisebox{-0.5\height}{\includegraphics{figs/pstricks/repeat_eff_unfold.pdf}}
\hspace{1mm}\hspace{1mm}
\raisebox{-0.5\height}{\includegraphics{figs/repeat_eff.pdf}}
\end{center}
\caption{\label{repeat_graphs}Term graph representation of the two \haskell{repeat} implementations and their unfoldings}
\end{figure}

From a software engineering perspective it is unsatisfactory that the
programmer has to recognise and mitigate such cases. One might even consider
the unoptimised version superior with respect to code
clarity. Therefore we propose an analysis and transformation method to automate
the optimisation, which then can be integrated into the compiler pipeline of
existing functional language implementations.
In the following sections we will work with simple examples to develop this
method and successively generalise it for wider applicability.






\section{Preliminaries}\label{sec:prelims}


The method we describe applies to the \lambdaletrec-calculus, which is a
higher-order rewrite system. Still, in this work-in-progress report we
primarily intend to motivate our research and outline the approaches developed
so far. Hence, for the moment we give a largely informal description, and
resort to first-order formulations, λ-graphs, or Haskell, whichever seems more suited.

\begin{definition}[First-order representation of ]\normalfont Let  be a set of variables. Then a \lambdaletrec-term is defined as follows:

\end{definition}


But ultimately only a higher-order formulation can be formally
satisfactory, thus we propose the following representation as a higher-order
rewrite system (HRS) \cite{terese:2003} for \lambdaletrec.

\begin{definition}[Higher-order representation of ]\normalfont
  \label{def:lambdaletrec:terms}
Let  be a set of variables, and  a set of base types that induce the set  of simple types.
  The \emph{terms} of  are simply-typed higher-order terms over the HRS\nbd-signature
  that for all , and all types  contains a symbol  of type:

Product types are only used for better readability.
We will use the symbols , ,  for terms in . 
\end{definition}  
  
Based on this notion of terms, the \nbd-calculus 
consists of the rewrite relations: -reduction, \nbd-reduction, and \nbd-unfolding.
Additionally, we use the concept of generalised \nbd-reduction \cite{kama:nede:1995}.


For example, \nbd-unfolding on the informal \nbd-terms according to the grammar above
could be described by the following rewrite rules:
0.5ex]
  \begin{aligned} 
    &
    \letbeIn{\underbrace{\afunvar_1 = \funap{\bllter_1}{\vec{\afunvar}}, \ldots, \afunvar_n = \funap{\bllter_n}{\vec{\afunvar}}}_{\mathit{Defs}}}
            {\funap{\allter}{\vec{\afunvar}}}  
    \;\;\;\sunfoldred\;\;\;
\allter\bigl(\letbeIn{\mathit{Defs}}
                    {\funap{\bllter_1}{\vec{\afunvar}}}, \ldots,  \letbeIn{\mathit{Defs}}
                   {\funap{\bllter_n}{\vec{\afunvar}}}\bigl)
\end{aligned}

\letini{n}{\labs{\afunvar_1\ldots\afunvar_n}{(\bmetavar,\funap{\cmetavar_1}{\vec{\afunvar}},\ldots,\funap{\cmetavar_n}{\vec{\afunvar}})}}
    \;\;\;\sunfoldred\;\;\;
  \bmetavar
  \      
      
  
\begin{notation}[Rewrite relations in ]\normalfont
  On \nbd-terms we consider the following rewrite relations:  
  \nbd-reduction denoted by ; generalised \nbd-reduction denoted by ; \nbd-reduction denoted by ; \nbd-unfolding denoted by .

  For each of these rewrite relations~,  
  the \emph{many-step rewrite relation} with respect to  will be written as ,
  and the (strongly convergent) \emph{infinite rewrite relation} as .
\end{notation}


With the transformation from Section~\ref{sec:intro}, which is further developed in
the next sections, we aim to convert a given term  with repetitive
reduction patterns into a term  that does not require these reductions
to be performed any more, but such that  and 
are `operationally equivalent', in a sense that guarantees that important properties observable during evaluation are preserved.

One candidate for a precisely defined notion of operational equivalence is
the extension to \nbd-terms of `applicative bisimulation' on \nbd-terms
due to Abramsky \cite{abra:1990}.
Two \nbd-terms  and  are called \emph{applicative bisimilar} (symbolically: )
if  and   behave in the same way under all possible series  of `experiments' of the following kind:
on a starting term  the first experiment  consists in finding out whether or not  reduces to an abstraction (a weak head normal form);
if the outcome  of the previous experiment  is indeed an abstraction ,
then for experiment  an arbitrary term  is chosen, and it is determined whether
or not the redex  reduces to an abstraction.

While applicative bisimulation has been frequently used to justify optimising transformations
for functional programming languages, there may be a host of other interesting notions of operational equivalence.
Since, for the moment, we do not want to commit ourselves to a particular notion of operational equivalence, 
we will use a syntactically defined notion of equivalence between terms instead.
In fact, we will define this  syntactic notion as the convertibility relation with respect to 
rewrite relations that we use for motivating and justifying the optimising transformation
in, for example, Fig.~\ref{repeat_graphs} and Fig~\ref{replicate_graphs}.  
There, we use, in addition to infinite convertibility with respect to  and ,
a restricted form of `vector \nbde{}reduction' that is defined by the following rewrite rule:

The induced rewrite relation  extends \nbde{}reduction, but can be mimicked with \nbd-steps,
and therefore has the same many-step relation.
However, neither for \nbde{}reduction nor for vector \nbde{}reduction
it holds in generality that the source and the target of a step are applicative bisimilar:
for example, in the \nbde{}reduction step 
the source is an abstraction, but the target is not.

Since we want to obtain a syntactically defined notion of operational equivalence that is stronger than applicative bisimilarity,
we define a restriction  of , and a variant  of ,
both of which serve our purposes and, importantly, only allow steps between applicative bisimilar terms.
The converse rewrite relation  of  
performs a copying operation for \nbde{}abstraction prefixes in terms;
and the converse rewrite relation  of 
both copies a \nbde{}abstraction prefix and carries out a permutation in it.

\begin{definition}[Restriction and variant of ]
  \normalfont\label{def:vecetazeropermred}
The restricted version~ of the rewrite relation  on \nbd-terms is defined by the rule:

And the extension   of  with respect to permuting variable names in abstraction prefixes
  is defined by the rewrite rule: 


\end{definition}
It is easy to verify that left- and right-hand sides of these rules are applicative bisimilar. 

Now we define the syntactic notion of equivalence on which we base our transformation. 

\begin{definition}[Equivalence relation ]\normalfont\label{def:opeq}
\emph{Infinite convertibility with respect to}  and , extended by finitely many \nbde{}reduction steps,
  is the following relation on \nbd-terms: 

\end{definition}

Since source and targets of each of the rewrite relations , , and  
are applicative bisimilar, and since applicative bisimilarity is a contextual congruence \cite{abra:1990}, 
the following proposition can be proved, which states that the syntactical equivalence 
from Definition~\ref{def:opeq} is at least as strong as, i.e.\ is contained in, applicative bisimulation. 

\begin{proposition}
For all \nbd-terms  it holds: {} 
    .    
\end{proposition}













\section{Further Examples}\label{sec:examples}


The  function shows that for some cases it is possible to lift
parameters out of recursive positions and thereby improve run-time efficiency.
That raises the question of when this is possible. What is the pattern that
allows for such an optimisation?

What strikes the eye are the occurrences of the syntactic element
\haskell{repeat~x} on both the left-hand and the right-hand sides of the
function definition. That suggests a sort of common subexpression elimination
that takes into account both sides of the equation.
This formulation, however, cannot cover the following example, which differs from \haskell{repeat}
essentially only by an additional parameter \haskell{n} on the left-hand side,
and the argument \haskell{n-1} on the right. \listing{replicate}
As it was the case for , again it is possible to lift the
parameter  out of the recursion. \listing{replicate_eff}
Again we regard both variants and their infinite unfolding (Fig.
\ref{replicate_graphs}) to understand the transformation. For reasons of
clarity, however, we completely leave out the scrutinisation of
parameter  and the subsequent case discrimination and concentrate on the
recursive pattern.

\begin{figure}[htp]
\begin{center}
\raisebox{-0.5\height}{\includegraphics[scale=0.9]{figs/replicate.pdf}}
\raisebox{-0.5\height}{\includegraphics[scale=0.9]{figs/replicate_eta.pdf}}
\raisebox{-0.5\height}{\includegraphics[scale=0.9]{figs/pstricks/replicate_eta_unfold.pdf}}

\raisebox{-0.5\height}{\includegraphics[scale=0.9]{figs/pstricks/replicate_eff_unfold.pdf}}
\raisebox{-0.5\height}{\includegraphics[scale=0.9]{figs/replicate_eff.pdf}}
\end{center}
\caption{\label{replicate_graphs}Operational equivalence of the two \haskell{replicate} variants depicted in graph notation}
\end{figure}

In comparison with the previous example we observe two differences. First, note
the `header'  attached on top of the second graph, which we
obtain by two η-expansions. This allows us to produce an optimised term that does
not comprise a duplicated function body. Furthermore, instead of relying
on ordinary β-reduction we have to add {\it generalised beta-reductions} ({\it
gβ}-reduction) \cite{kama:nede:1995} to our arsenal.



The key pattern shared by the two presented examples that permits optimisation
is a parameter  that is being passed through
unchanged in the recursive application. Consequently, once the function is
called from the outside with some argument  in 's position, while
recursively evaluating that call,  can never again be bound to another value
than  or a descendant of . In that sense one might call  a `constant
parameter'. It suggests itself, that components that are `constant' to a
recursive construct can be lifted out of the recursion.

\section{A rewrite rule for simple recursive patterns}
\label{sec:rules}


The examples in Section~\ref{sec:intro} and Section~\ref{sec:examples} 
suggest that there are many similar situations
in which optimisations of the kind as described can be carried out. 
A first attempt to obtain general formulations of such simplification steps
would be to use schemata, and in effect, rewrite rules on \lambdaletrec\nbd-terms.    

As an example, let us consider the recursive definition of a function  in
which the th parameter  is passed on to all recursive calls of  as
the th argument.
In this case the transformation that eliminates the recurrent parameter 
can be described by the following first-order rewrite rule on \lambdaletrec
\begin{samepage}
\begin{flushleft}
\vspace*{-0.5ex}
  \hspace*{6ex}\includegraphics{listings/transform1_lhs}
  \nopagebreak[4]\0.5ex]
  \hspace*{6ex}\includegraphics{listings/transform1_rhs}
\end{flushleft}
\end{samepage}
\vspace{-0.5ex}
where  is a \lambdaletrec\nbd-context with possibly more than one occurrence of the context-hole ,
 and  do not occur in , and  does not get bound during hole-filling.
Here the recurrent parameter  in the recursive definition of  is lifted out
of the recursion, and the number of arguments in the recursive call of the function in
the {\tt let}\nbd-construct has decreased by one after the transformation.
Note, that context  might start off with initial lambdas, and therefore
also covers the case in which  is followed by further parameters.

To cover situations with multiple recursive calls to  with (possibly)
varying arguments, we can generalise the rewrite rule as follows, as long as
the argument in question, , is the same.
\begin{flushleft}
\vspace*{-0.5ex}
  \hspace*{6ex}\includegraphics{listings/transform2_lhs}
  \0.5ex]
  \hspace*{6ex}\includegraphics{listings/transform2_rhs}
\end{flushleft}
 is a context with m sort of holes , \ldots, , 
in which holes of each sort may occur more than once,
where  and  do not occur in , and
the parameter  does not get bound during hole-filling.

In order to enhance its application to a bigger class of \lambdaletrec\nbd-terms,
the second rule can be further generalised
to cover situations in which  is only one amongst many functions defined in
a \letrec\nbd-construct, or there
are also other recursive calls to  that are not of the `good' form. 
Namely, if a definition like: \listing{formula_lhs} occurs somewhere in a
let-binding, then it can be substituted by: \listing{formula_rhs}
There might be calls of  in  that are of `bad' shape. These remain unchanged. 
On \nbd-terms, this more general transformation can be expressed by the rewrite rule:
\begin{flushleft}
\vspace*{-0.5ex}
  \hspace*{6ex}\includegraphics{listings/formula1_lhs}
  \0.5ex]
  \hspace*{6ex}\includegraphics{listings/formula1_rhs}
\end{flushleft}
where  is a context of the form 
consisting of definitions , \ldots, , , \ldots,  and a single hole . 












\subsection{Rewriting the Haskell Prelude}


To demonstrate the above rewriting rule, we apply it to straightforward
implementations of some well-known functions from the Haskell Prelude.

\listing{map}
\listing{until}
\listing{append}

For the optimised counterparts the amount of β-reduction steps saved per
recursive call amounts to one for \haskell{map} and \haskell{(++)}, and to two
for \haskell{until}.

\listing{map_eff}
\listing{until_eff}
\listing{append_eff}

In practise, however, the amount of β\nbd-reductions is only one of many
factors for the run time that is necessary to evaluate a piece of code.
Therefore it is to be expected that when executed with a system like the
Glasgow Haskell Compiler (GHC) the obtained functions would not necessarily
lead to better performance. Depending on the compiler version and the flags
provided in the invocation of GHC, simple benchmarks yield mixed results, but
never resulting in severe degradation (more than an increase of 5\% in run
time) and with one of the functions (\haskell{until}) consistently reaching a
speed-up of over 200\%.

Let us conclude that before integrating the transformation into a compiler for
practical purposes an analysis on how it interacts with other optimisations
remains yet to be done. 

\subsection{Limitations of the Rewrite Rules}


While the most general rewrite rule above has proven to be applicable in a number of situations
that occur in practice, it also has some severe limitations: 
First and foremost it applies only to patterns with immediate recursion, thus
it fails to capture the repetitive reduction pattern in the evaluation of
the term in Fig. \ref{mutual}.
\begin{figure}[h]
\listing{mutual}
\caption{\label{mutual}Schematic term involving mutual recursion}
\end{figure}
If \haskell{f} and \haskell{g} are not used at further positions, once in the
recursion initiated by  both  and  will never be bound to a
different value than . The property leading to this behaviour is the
relation between the parameters of  and . In , if  is called, then  is
passed as an argument and thereby bound to . Conversely,  is bound to 
in the call of  in . Thus, we observe a relation between parameters that
is cyclic. Also for all of the previous example such a {\it parameter cycle}
exists, however comprising only a single parameter. In the following section we
shall elaborate further on the idea of parameter cycles.

\section{Binding Analysis}\label{sec:binding:analysis}


In this section we shall develop an analysis that statically recognises
repetitive reduction patterns indicated by parameter cycles, and show how this
allows us to eliminate those binders that are part of such a cycle. Parameter
cycles describe the possibility of a parameter being passed on from function to
function unchanged, finally arriving at its original position. To detect such
cycles we need to analyse which subterm might be bound to which variable during
the evaluation of a term.





\subsection{Binding Graph}

To this end we introduce a \textit{binding relation}  on variables  and subterms  of a term. It is a conservative approximation
of which bindings might occur during the evaluation of the term and does not
distinguish between different descendants of its components.

Since we need to take a global vantage point, i.e.\ to uniquely identify the
term's syntactic elements we assume globally unique variable naming. To this
end we could also use positional information, but only along with additional
technicalities. Therefore, without loss of generality we henceforth assume that
each abstraction binds a distinct variable (variables in the term are `distinctly bound'), 
and no variable name has both a free and a bound occurrence (the term observes `Barendregt's Variable Convention' \cite[2.1.13, p.26]{bare:1984}). 
That allows us to unmistakably address a specific
binding  by the variable  it binds.\footnote{Even if that forbids
distinguishing between different occurrences of the term , it turns out not
to be necessary for our needs.}

For example for some context  in the reduction of term 
the β-redex  might be contracted and by consequence a
descendant of  be bound to an instance of . Therefore  implies . This principle carries over to gβ-reduction, and more importantly,
to sharing.

In order to obtain the binding relation for a term  one has to identify all
terms in argument position that might ever match up with  in its reduction.
(More precisely: terms whose {\it descendants} might ever match up with {\it
descendants} of .) This could be naively accomplished by a search that
starts at each abstraction  and from there travels upwards the spine
segment of . When an applicator with  as an argument is encountered that
matches  according to the semantics of gβ-reduction  is noted
down. When a multiplexer is encountered the search is pursued for each of the
incoming edges.

The use of higher-order functions makes it impossible to enumerate the binding
relation completely. Thus, if during the search
the end of the spine is reached one cannot make a safe assumptions on
`future' arguments for that branch. Therefore we employ an additional
`artificial' {\it blackhole} node  that represents an unknown term
on the right hand side of the binding relation. According to this, the
occurrence of  implies . Note, that the
blackhole node is not needed to identify parameter cycles, but is however
required for the domination property introduced later on.

Let us revisit the examples presented so far and enlist their binding
relation. In the λ-graph of \haskell{repeat} (Fig. \ref{repeat_graphs}) the
search starting from the only abstraction  branches at the multiplexer
above. The left branch yields a matching application with  in argument
position, so we obtain . At right branch the spine immediately
ends yielding . The directed graph that is induced by
this relation features a single-node parameter cycle.

\begin{figure}[ht]
\raisebox{-0.5\height}{\includegraphics{figs/pstricks/repeat_bg.pdf}}
\hspace{2cm}
\raisebox{-0.5\height}{\includegraphics{figs/pstricks/replicate_bg.pdf}}
\hspace{2cm}
\raisebox{-0.5\height}{\includegraphics{figs/pstricks/mutual_bg.pdf}}
\caption{Binding graphs of \haskell{repeat}, \haskell{replicate}, and the term from Fig. \ref{mutual}}
\end{figure}

The binding-graph of \haskell{replicate} is very similar featuring the same
kind of parameter cycle, only does it involve an additional parameter. Also we
have to apply the idea of gβ-reduction when searching upwards from . First
we encounter another abstraction , which according to the notion of
gβ-reduction requires us to leave out the next application node. In the left
branch of the multiplexer this is the one with  in argument position.
Therefore we end up with  as the argument of the matching application node,
by which we obtain . Parameter cycles of greater length would occur
for scenarios involving mutual recursion such as the term in Fig. \ref{mutual}.



\subsection{Inference Rules}

To properly define the binding relation we formulate it using inference rules.
Since it follows the structure of typing rules, we first give rules for a
simply-typed -calculus (Fig. \ref{typing_rules}) and then
decorate these typing rules to also yield a term's binding graph, by which we
hope to provide an easier access for those who are already familiar with typing
rules for the λ-calculus. Type variables are denoted by Greek~letters.

\begin{figure}[htp]
\begin{mathpar}
\inferrule[Var]{x:\tau \in Γ}{Γ ⇒ x:\tau}

\inferrule[Abs]{Γ∪\{x:\tau\} ⇒ e:\sigma}{Γ ⇒ λx.e:\tau→\sigma}

\inferrule[Letrec]{\forall i \in \{0,\dots,n\}: Γ ∪ \{f_1:\tau_1,\dots,f_n:\tau_n\} ⇒ e_i:\tau_i}
                  {Γ ⇒ \letrec ~ f_1 = e_1 ~ \dots ~ f_n = e_n ~ \beIn ~ e_0~:~\tau_0}

\inferrule[App]{Γ ⇒ e_1:\tau→\sigma \\ Γ ⇒ e_2:\rho \\ \tau=\rho}
               {Γ ⇒ e_1~e_2:\sigma}
\end{mathpar}
\caption{\label{typing_rules}Typing rules for the simply-typed -calculus}
\end{figure}

In order to infer the binding relation (Fig. \ref{annotated}), at any application
 one has to determine to which variable  might be bound to in
. That we accomplish by annotating the types of terms with the names of
the variables that are bound by their abstractions. The annotations are in
superscript position, so for some annotated types , ,
, the annotated type of  is . We use  as an annotation to indicate that
we cannot determine the associated variable (due to the use of higher-order
functions).

The binding relation is denoted on the left-hand side of the turnstile symbol.
Both upper-case and lower-case Greek letters denote annotated type variables,
but the latter do not include the outermost annotation (which are then added
explicitly).

\begin{figure}[h]
\begin{mathpar}
\inferrule[Var]{x:\Tau \in Γ}{\emptyset \T Γ ⇒ x:\Tau}

\inferrule[Abs]{B \T Γ ∪ \{x:\tau^\epsilon\} ⇒ e:\Sigma}{B \T Γ ⇒ λx.e:(\tau^\epsilon→\Sigma)^x}

\inferrule[Letrec]{\forall i \in \{0,\dots,n\}: B_i \T Γ∪\{f_1:\Tau_1, \dots, f_n:\Tau_n\} ⇒ e_i:\Tau_i}
                  {B_0 ∪ \dots ∪ B_n \T Γ ⇒ \letrec ~ f_1 = e_1 ~ \dots ~ f_n = e_n ~ \beIn ~ e_0~:~\Tau_0}

\inferrule[App]{B_1 \T Γ ⇒ e_1:(\Tau→\Sigma)^x \\ B_2 \T Γ ⇒ e_2:\Rho \\ bare(\Tau)=bare(\Rho)}
                {B_1 ∪ B_2 ∪ bind(x,e_2) ∪ blackholes(\Rho) \T Γ ⇒ e_1~e_2:\Sigma}

blackholes(\Tau) =
\begin{cases}
blackholes(\Rho) ∪ \{x\binds\blackhole\} & \text{if } \\
blackholes(\Rho)                         & \text{if } \\
\emptyset                                & \text{otherwise}
\end{cases}

bind(x,e_2) =
\begin{cases}
\emptyset & \text{if }\\
\{x\binds e_2\} & \text{otherwise}
\end{cases}

\end{mathpar}
\caption{\label{annotated}Inductive definition of the binding relation based on annotated types}
\end{figure}

In the {\sc Abs} rule the abstraction puts a new variable , which is
annotated by an  because we do not make assumptions on the variables
exposed by higher-order functions. The type of  is annotated by ,
which is exposed.
To compare types {\sc App} employs a function , which maps
annotated types  to bare types  by removing all annotations. Two further
functions,  and , are used to enumerate the elements to be
added to the binding relation. In case the argument  is a higher-order
function the expressions cannot be determined that are bound to the variables
it exposes, therefore  binds a blackhole to each of them.





\begin{proposition}
Every derivation  in the type system in Fig.~\ref{typing_rules}
  with conclusion ,
  which justifies the assignment of type  to the \nbde{}term ,
  can be decorated effectively with as result
  a derivation  in the proof system in Fig.~\ref{annotated}
  with conclusion ,
  by which the binding graph  for  is obtained and justified.  
\end{proposition}

The proof of this proposition consists in describing an effective algorithm
that, given a derivation  in the type system in Fig.~\ref{typing_rules}, proceeds as follows:
First it constructs, in a step by step manner, variable decorations for the types in  
(by concentrating on `spine loops' of  that correspond to cyclic threads in ,
and starting with the decorations at formulas on such threads where the types have minimal length)
in such a way that the rule instances are correct for the system in Fig.~\ref{annotated}
when the leading binding graph annotations of the form  are neglected.
Second, after the first step is concluded, the algorithm constructs 
the binding graph annotations according to the rules in Fig.~\ref{annotated}
in a top-down manner.
 


\section{Transformation}
Using the binding graph we will now develop a more general version of the
optimisation previously formulated as rewriting rules restricted to directly
recursive functions.

An edge  in the binding graph of a term  indicates a (possibly
infinite) class of gβ-redexes on the infinite unfolding  of . While all
these redexes could be at once contracted on  this does not automatically
carry over to its finite representation .\footnote{In fact, the reduct of an
unfolded -term does not have to be expressible as a finite term
in  in general.} Let us for the present restrict our attention
to cases where  has no further incoming edges.

\begin{proposition}\normalfont
Let  be a -term with infinite unfolding  and a binding
graph featuring a node  with a {\it single incoming} edge .
If we label the transition that contracts all gβ-redexes with involving a
-abstraction , and the substitution of all occurrences of  by 
and the subsequent elimination of all vacuous -abstractions , then
the following diagram commutes.

\end{proposition}
This follows from the following considerations. Since we assume unique variable
naming, the infinitely unfolding  without any renaming is semantics
preserving. \cite{endr:grab:klop:oost:2010}\footnote{Even if this has only been
proven for μ-unfolding but it is assumed that it also holds for
. The unfolding does {\it not} preserve unique naming.} Every
occurrence of  in  gives rise to one ore more gβ-redexes in  each
having  with  as an argument. This follows from  being the
sole incoming edge of . By both paths from  to  one finds  to
have the same shape: There are no occurrences of neither  nor .
Evidently this not a very strong argument and we hope to improve on it by means
of higher-order reasoning.

The restriction above to only consider nodes with a single predecessor prevents
us from dealing with any of the examples shown so far since they involve cyclic
binding graphs. Fortunately, it can be relaxed to a much less restrictive
property.



\begin{definition}[Domination, and strong domination]\normalfont\label{def:domination}
Let a  be a directed graph,
  and  and  be vertices of .
  We say that  \emph{dominates}  ( is a \emph{dominator} for , symbolically: ) if either  or
   and for every path  in  that leads to  but does not contain 
  it holds that the start vertex of  is reachable from ;
  more formally, if:\footnote{The condition \ref{eq1:def:domination} could be simplified by taking it to be just the
      subformula starting with the universal quantification
      (that subformula is true if ); the longer condition is used here to increase readability.}

Note that  holds if  dominates . 
  
  And we say that  \emph{strongly dominates}  
  ( is a \emph{strong dominator} for , symbolically: )  if  and for every path  in  that leads to  but does not contain 
  it holds that the start vertex  of  is reachable from , but does not reside on a common cycle with ,
  more formally, if:

\end{definition}

\begin{remark}\normalfont
The standard definition of a ` dominates ' for control-flow graphs (see e.g.~\cite{hech:ullm:1974}) 
  requires that each path from the start node to  has to pass through .
  The definition above is a generalisation to directed graphs that does not depend
  on the existence of a designated start node. 
Our definition of `~strongly dominates ' excludes self-domination (i.e.\ makes the relation irreflexive),
  and adds the restriction 
  that for all paths from  to  that do not repeatedly pass through 
  it holds no vertex except the starting vertex  resides on a common cycle with~.
\end{remark}


The following proposition suggests an alternative, co-recursive definition of strong domination
between vertices, which proceeds stepwisely by examining predecessors of the strongly dominated vertex.

\begin{proposition}\normalfont
Let  be a directed graph.
  Then for all  it holds:

\end{proposition}

\vspace{1ex}
For the definition of the optimising transformation, 
we choose a formulation different from the one using rewrite rules described in Section~\ref{sec:rules},
one that is particularly easy to express.
Such as β-reduction can be decomposed into substitution of individual
occurrences of variables (local β-reduction) and the elimination of vacuous
bindings (AT-removal) as described in \cite{brui:1987}, for the transformation
we have the possibility of expressing the transformation with an arbitrary
level of granularity. The following rule encompasses the substitution of all
occurrences of one dominated variable. It could have been formulated more
fine-grained by substituting only individual occurrences, or less fine-grained
by including the elimination of the bindings that have become vacuous.


\begin{proposition}[Main proposition]
In a -term  occurrences of a variable that is dominated by an
expression  in 's binding graph can be substituted by .

\end{proposition}



\section{Advanced Examples}

There are interesting examples to which the presented transformation cannot be
directly applied or does not lead to satisfactory results. Only in combination
with a number well-directed unfoldings the desired effect can be obtained. Let
us consider two schematic examples: \listing{intricate} The first one exhibits
dominated parameters only after a single let-unfolding (Fig. \ref{fx-fy}). Also
it features repetitive reduction pattern, without having a parameter cycle.
This because the argument in question is bound outside of the recursion. Still,
after the unfolding the presented optimisation does cure the term.

\begin{figure}[ht]
\begin{center}
\raisebox{-0.5\height}{\includegraphics{figs/pstricks/fx-fy_bg.pdf}}
\hspace{1cm}
\raisebox{-0.5\height}{\includegraphics{figs/fx-fy.pdf}}
\hspace{5mm}\hspace{5mm}
\raisebox{-0.5\height}{\includegraphics{figs/fx-fy_unfold.pdf}}
\hspace{1cm}
\raisebox{-0.5\height}{\includegraphics{figs/pstricks/fx-fy_unfold_bg.pdf}}
\caption{\label{fx-fy}A term, which after one unfolding exhibits a dominated parameter}
\end{center}
\end{figure}

The second of the two term is particularly intricate since it can be unfolded
and then transformed in many different ways with considerable differences in
the amount of concealed gβ-redexes. Most solutions involve more than one
recursive call with more than two arguments. The ideal transformation with respect to
the number of concealed redexes of both terms is depicted in Fig.
\ref{intricate}.

\begin{figure}[ht]
\raisebox{-0.5\height}{\includegraphics{figs/fx-fy.pdf}}

\raisebox{-0.5\height}{\includegraphics{figs/fx-fy_eff.pdf}}
\hfill
\raisebox{-0.5\height}{\includegraphics{figs/fxb-fay.pdf}}

\raisebox{-0.5\height}{\includegraphics{figs/fxb-fay_eff.pdf}}
\caption{\label{intricate}Optimisation of two terms that are not covered by the presented rewriting rules}
\end{figure}







\section{Status quo}


Presently this work consists mostly of an extended problem description, which
is to a some extent rather informal. Ideas for resolution have been presented,
but yet lack both precision and genericity. Therefore, outstanding issues to
continue the investigation suggest themselves. Currently we are working on the
following problems:
\begin{itemize}
  \item Properly formalising the concepts introduced in this report. This
  involves putting grammars and rewrite rules into the higher-order setting of HRSs. 
  \item Investigate whether there are other interesting notions 
  of `operational equivalence', apart from applicative bisimulation,
  with respect to which we could show correctness of our optimising transformation.
  (We think of notions that are in line with the semantics of
   functional programming languages, but nevertheless are language independent,
   and also of theoretical interest.)
\item Expressing the presented transformation as a higher-order rewrite
  system and proving its correctness with respect to that equivalence relation.
\end{itemize}

Once these fundamental issues are resolved, we intend to tackle the following questions: \begin{itemize}
  \item We have seen that unfolding a \lambdaletrec-term in order to facilitate
  the optimisation can be effected in different ways, leading to terms of
  different quality. To obtain the most efficient terms one has to provide a
  procedure to select the most suitable unfolding for each situation.
  \item This requires an adequate measure for efficiency.
  \item We would like to learn more about the rewrite properties of the transformation:
    is it possible to find a formulation that guarantees confluence and normalisation?
\item In some of the easy examples we studied, our transformation seemed to be
    closely connected with the concept of `lambda-dropping' \cite{danv:schu:1997,danv:1999},
    and hence also with its converse, `lambda-lifting' \cite{john:1985,peyt:jone:1987,danv:schu:2002}.
    We want to understand that relationship in detail. 
\item Once the analysis has been optimised in terms of genericity, i.e.\ that
  it recognises as many cases as possible for which the transformation is
  correct, it would be interesting to assess the frequency in which these cases
  occur in existing systems, such as functional programming libraries or
  intermediate code generated by compilers.
  \item The remaining question is, how the optimisation actually affects the
  run-time efficiency of real-world systems like Haskell programs.
\end{itemize}









\paragraph{Acknowledgment.}
The incentive to investigate the presented optimisation was provided by
  Doaitse Swierstra. We thank him and Vincent van Oostrom for many insightful
  discussions and hints.

\bibliographystyle{eptcs}
\bibliography{termgraph-2011}
\end{document}
