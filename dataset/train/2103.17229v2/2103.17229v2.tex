\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/pipeline.pdf}
    \caption{Overview of our algorithm. Given an image with 2D key points, we infer the corresponding image-specific 3D points in terms of a deformation of 3D universe points. The universe 3D points are learned during training for a given class of objects, while the deformations are predicted per image. We create edges and find a matching between the two graphs using a graph matching network. Since the matchings are between universe points and images, our matchings are intrinsically cycle consistent.}
    \label{fig:pipeline}
\end{figure*}

\section*{Proposed Method}
Our learning framework consists of four main components. The first two components have the purpose to obtain
 3D universe points, along with a deformation of these 3D points representing the underlying 3D structure of the 2D key points in the -th image.
The purpose of the other two components is to predict the matching between the 2D points of  and the 3D points of . 
Thus, rather than learning  pairwise matchings between  and , we utilise an object-to-universe matching formulation. 
Therefore, the underlying 3D structure and cycle-consistent multi-matchings  are both attained by our method. 
The whole pipeline is illustrated in Fig.~\ref{fig:pipeline} and comprises the following four main components: 
\begin{enumerate}
    \item \textbf{Learnable 3D Universe Points}: the 2D key points  of all images in the collection are used to reconstruct the 3D universe points  by incorporating a reconstruction loss that approximates Eq.~\eqref{eq:recon_3D}.
    \item \textbf{Deformation Module}: the retrieved universe points  are static and therefore they cannot accurately model the geometric variability present in
     different instances of an object from the same category (e.g.~different bicycles).
    To address this, the universe points are non-linearly deformed by the
    deformation module that takes the 2D points and the (learned) 3D universe points as input. 

    \item \textbf{Assignment Graph Generation}: by connecting the 2D and universe points, respectively, the 2D graph and the 3D universe graph are constructed.
    The assignment graph is then constructed as the product of these two graphs.
    \item \textbf{Graph Matching Network}: a graph matching network performs graph convolutions on the assignment graph, and eventually performs a binary node classification on the assignment graph representing the matching between the 2D graph and the universe graph.

\end{enumerate}


\noindent\subsection{Learnable 3D Universe Points}
As discussed above,
the universe points can be retrieved by minimising~\eqref{eq:recon_3D}.
This problem, however, is generally under-determined, since  and  in~\eqref{eq:recon_3D} are generally unknown in most practical settings.
Additionally, although all objects
share a similar 3D geometry, the nonlinear deformations between different instances are disregarded in \eqref{eq:recon_3D}. Thus, instead of an exact solution we settle for an approximation that we later refine in our pipeline. 
To this end, we assume a weak perspective projection model,~i.e.~all universe points are assumed to have the same distance from the camera.
With this condition, the diagonal of  is constant and can be absorbed into 
.
This leads to the least-squares problem

which can be solved in an end-to-end manner during network training based on `backpropagable' pseudo-inverse implementations.
The variable  can be expressed as , where  is the right pseudo-inverse that satisfies .
Therefore, we solve the following problem


\subsection{Deformation Module}
The universe points retrieved in the previous step can only reflect 
the coarse geometric structure of the underlying 3D object, but cannot represent finer-scale variations between different instances within a particular object category. 
Thus, we introduce the deformation module to model an additional nonlinear deformation.

This module takes the universe points  and the 2D points  as input.
As shown in the bottom left of Fig.~\ref{fig:pipeline},  is passed to a 2D Point Encoder.
The encoder first performs a nonlinear feature transform of all input points based on multi-layer perceptron (MLP), and then performs a max pooling to get a global feature representing the input object.
As can be seen in the top left in Fig.~\ref{fig:pipeline}, an MLP is utilised  to perform a nonlinear feature transform for each of the 3D points in .
Each 3D point feature is then concatenated with the same global feature from the 2D Point Encoder.
The concatenated per 3D point features are fed into an MLP 
to compute the deformation of each point.
The output is a set of per-point offsets  that
are added to  to generate the deformed 3D universe points. 
The computation of the per-point offsets is summarised as 

where  represents the concatenation operation.

 We enforce that the projection of the deformed universe points onto the image plane should be close to the observed 2D points, similar to the reconstruction loss in Eq. ~\eqref{eq:reconstructionLoss}.
Since the static 3D universe points should reflect the rough geometry of the underlying 3D object, the offset  should be small.
Therefore, we introduce the deformed reconstruction loss and the offset regulariser as
    


\subsection{Assignment Graph Generation}\label{sec:assignment}
To obtain graphs from the 2D points and the deformed 3D universe points, respectively, we utilise the Delaunay algorithm \cite{botsch2010polygon} to generate edges, see Fig.~\ref{fig:pipeline}. Moreover, we define the attribute of each edge as the concatenation of the  coordinates of the respective adjacent points. Note that other edge generation methods and attributes can be utilised as well. 

Once the 3D universe graph  and the 2D graph  are generated, we construct the assignment graph  as the product graph of  and  following~\citet{leordeanu2005spectral}.
To be more specific, the nodes in  are defined as the product of the two node sets  (of )  and   (of ), respectively,~i.e.~.
The edges in  are built between nodes ,  if and only if there is an edge between  and  in , as well as between  and  in .
The attribute of each node and edge in  is again the concatenation of the attribute of corresponding nodes and edges in  and , respectively.

\subsection{Graph Matching Network}
The graph matching problem is converted to a binary classification problem on the assignment graph . 
For example, an assignment graph is shown on the top right of Fig.~\ref{fig:pipeline}.
Classifying nodes  as positive equals to matching point  to ,  to  and  to , where numeric nodes correspond to the 2D graph, and alphabetic nodes correspond the 3D universe graph.

The assignment graph is then passed to the graph matching network~\cite{wang2020combinatorial}. A latent representation is achieved by alternatingly applying edge convolutions and node convolutions.
The edge convolution assembles the attributes of the connected nodes, while the node convolution aggregates the information from its adjacent edges and updates the attributes of each node. The overall architecture is based on the graph network from~\citet{battaglia2018relational}.



\begin{figure*}[t]
  \centering
  \footnotesize
  \newcommand{\mywidth}{0.12\textwidth} 
  \newcolumntype{C}{ >{\centering\arraybackslash} m{0.02\textwidth} }
  \newcolumntype{X}{ >{\centering\arraybackslash} m{\mywidth} }
  \setlength\tabcolsep{0.5pt} 
  \def\arraystretch{1.0} 
  \begin{tabular}[t]{XXXXXXXX}
  \multicolumn{4}{c}{Car (Willow Dataset)} & 
  \multicolumn{4}{c}{Duck (Willow Dataset)}\\
  \includegraphics[width=\mywidth]{images/car1.png} &
  \includegraphics[width=\mywidth]{images/car2.png} &
  \includegraphics[width=\mywidth]{images/car3.png} &
  \includegraphics[width=\mywidth]{images/car4.png} &
  \includegraphics[width=\mywidth]{images/duck1.png} &
  \includegraphics[width=\mywidth]{images/duck2.png} &
  \includegraphics[width=\mywidth]{images/duck3.png} &
  \includegraphics[width=\mywidth]{images/duck4.png} \\
  \multicolumn{4}{c}{Bicycle (Pascal VOC Dataset)} & 
  \multicolumn{4}{c}{Cow (Pascal VOC Dataset)} \\
  \includegraphics[width=\mywidth]{images/bicycle1.png} &
  \includegraphics[width=\mywidth]{images/bicycle2.png} &
  \includegraphics[width=\mywidth]{images/bicycle3.png} &
  \includegraphics[width=\mywidth]{images/bicycle4.png} &
  \includegraphics[width=\mywidth]{images/cow1.png} &
  \includegraphics[width=\mywidth]{images/cow2.png} &
  \includegraphics[width=\mywidth]{images/cow3.png} &
  \includegraphics[width=\mywidth]{images/cow4.png}
  \end{tabular} 
  \caption{Qualitative results of our method on the Willow and Pascal VOC Dataset. We achieve accurate results for non-deformable objects of different types (car, bike) and reasonable results for instances of articulated objects (duck, cow).}
  \label{fig:qualitative_res}
\end{figure*}
 

\subsection{Loss Function}
Similarly as existing deep graph matching approaches, we train
our network in a supervised way based on the ground-truth matching matrix  between  and . To this end, we use the matching loss

Furthermore, 
similarly as in previous work \cite{wang2018abpf, wang2020combinatorial}, we adopt a one-to-one matching prior in terms of a soft constraint.
To this end, we first convert the predicted permutation matrix  to a binary node label matrix  that we define as 

Here,  is the vectorisation of .
We can compute the corresponding index vector  defined as

By leveraging the auxiliary matrix   and the ground-truth permutation matrix ~\cite{wang2018abpf}, the one-to-one matching regularisation is 

The total loss that we minimise during training is 




\subsection{Training}
We train a single network that is able to handle multiple object categories at the same time. To this end, we learn separate 3D universe points for each category, and in addition  we introduce a separate learnable linear operator for each category that is applied to the global feature obtained by the 2D Point Encoder. The linear operator aims to transform the global feature to a category-specific representation, and also helps in resolving ambiguities between categories with objects that are somewhat similar (e.g.~cat and dog).

In practice, we apply a warm start to learn the universe points , which are randomly initialised for each category. 
After retrieving , we start training the neural network on the total loss with , ,  and  (in all our experiments).
The batch size  is 16 and the number of iterations after warm start is 150k. 
The learning rate is  and scheduled to decrease exponentially by 0.98 after each 3k iterations.