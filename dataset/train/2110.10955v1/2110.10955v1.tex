\label{experiments}
In this section, we will experimentally demonstrate the insights discussed in the previous sections. We will mainly utilize the fully annotated MS-COCO dataset \cite{lin2014microsoft} to validate and demonstrate the effectiveness of our approach by simulating partial annotation under specific case studies. The evaluation metric used in the experiments is the mean average precision (mAP). Training details are provided in appendix \ref{appendix:training_details}.


\subsection{Impact of Annotation Schemes}
\label{sec:annotation_schemes}
As aforementioned in section \ref{sec:how_to_treat}, the scheme used for annotating the dataset can substantially induce the learning process. Specifically, the choice of how to treat the un-annotated labels is highly influenced by the annotation scheme. To demonstrate that, we simulate two partial annotation schemes on the original fully annotated MS-COCO dataset \cite{lin2014microsoft}. 
MS-COCO includes 80 classes, 82,081 training samples, and 40,137 validation samples, following the 2014 split. The two simulated annotation schemes are detailed as follows:



\noindent\textbf{Fixed per class (FPC).} 
For each class, we randomly sample a fixed number of positive annotations, denoted by $N_s$, and the same number of negative annotations. The rest of the annotations are dropped. 


\noindent\textbf{Random per annotation (RPA).}
We omit each annotation with probability $p$. Note that this simulation preserves the true class distribution of the data. 

In Figure \ref{fig:impact_modes}, we show results obtained using each one of the simulation schemes for each primary mode (\textit{Ignore} and \textit{Negative}) while varying $N_s$ and $p$ values. As can be seen, while in RPA (Figure \ref{fig:impact_modes}(a)), the \textit{Ignore} mode consistently shows better results, in FPC (Figure \ref{fig:impact_modes}(b)), the \textit{Negative} mode is superior. 
Note that as we keep more of the annotated labels (by either increasing $N_s$ or decreasing $p$), the gap between the two training modes is reduced, catching the maximal result.
The phenomenons observed in the two case studies we simulated are also related in real practical procedures for partially annotating multi-label datasets. While in the FPC simulation, the class distribution is completely vanished and cannot be inferred by the number of positive annotations ($N_s$ for  $c=1,..., C$), the RPA scheme preserves the class distribution. 


\begin{figure}[t!]
\begin{subfigure}[a]{.23\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/coco_rps.eps}
  \caption{}
\end{subfigure}\begin{subfigure}[h]{.23\textwidth }
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/coco_fpc.eps}
  \caption{}
\end{subfigure}
\caption{\textbf{Impact of annotation schemes.} mAP results obtained using the RPA (a) and the FPC (b) simulation schemes for each primary mode. While in RPA, \textit{Ignore} mode consistently shows better results, in FPC, the \textit{Negative} mode is superior.}
\label{fig:impact_modes}
\vspace{-3mm}
\end{figure}

\begin{figure} [t!]
  \vspace{-0.2cm}
  \centering
  \includegraphics[scale=.38]{Figures/coco_spearman.eps}
  \vspace{-0.1cm}
\caption{\textbf{Spearman correlation between the true class distribution and the estimated distribution.} 
  Unlike the \textit{Negative} mode, training a model using \textit{Ignore} mode is well suited for estimating the class distribution.}
  \label{fig:coco_spearman}
  \vspace{-0.1cm}
\end{figure}

\subsection{Estimating the Label Prior}
\label{sec:results_estimate_class_prior}
To demonstrate the estimation quality of the class distribution obtained by the approach proposed in section \ref{sec: estimate_class_distribution}, we follow the FPC simulation scheme applied on the MS-COCO dataset (as described in section \ref{sec:annotation_schemes}), where a constant number of 1,000 annotations remained for each class. Because MS-COCO is a fully annotated dataset, we can compare the estimated class distribution (i.e. the label prior) to the true class distribution inferred by the original number of annotations. In particular, we measure the similarity between the original class frequencies and the estimated ones using the Spearman correlation test. In figure \ref{fig:coco_spearman}, we show the Spearman correlation scores while varying the number of top-ranked classes. We also show the results obtained with \textit{Negative} mode as a reference. Specifically, the Spearman correlation computed over all the 80 classes, with the estimator obtained using the \textit{Ignore} mode is $0.81$, demonstrating the estimator's effectiveness. In the next section, we will show how it benefits the overall classification results. Also, in appendix \ref{appendix:b} we present the top frequent classes measured by our estimator and compare them to those obtained by the original class frequencies in MS-COCO.







































