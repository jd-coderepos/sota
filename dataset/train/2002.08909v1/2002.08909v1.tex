

\documentclass{article}

\usepackage{header}





\usepackage[accepted,nohyperref]{icml2020}






\begin{document}

\twocolumn[
\icmltitle{REALM: Retrieval-Augmented Language Model Pre-Training}




\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kelvin Guu}{equal,google}
\icmlauthor{Kenton Lee}{equal,google}
\icmlauthor{Zora Tung}{google}
\icmlauthor{Panupong Pasupat}{google}
\icmlauthor{Ming-Wei Chang}{google}
\end{icmlauthorlist}

\icmlaffiliation{google}{Google Research}
\icmlcorrespondingauthor{Kelvin Guu}{kguu@google.com}
\icmlcorrespondingauthor{Kenton Lee}{kentonl@google.com}
\icmlcorrespondingauthor{Zora Tung}{gatoatigrado@google.com}
\icmlcorrespondingauthor{Panupong Pasupat}{ppasupat@google.com}
\icmlcorrespondingauthor{Ming-Wei Chang}{mingweichang@google.com}

\icmlkeywords{Machine Learning, ICML, Neural Networks, Information Retrieval, Question Answering, Natural Language Processing}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.
To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent {\em knowledge retriever}, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.
We demonstrate the effectiveness of \thefullname pre-training (\thename) by fine-tuning on the challenging task of Open-domain Question Answering (\openqa). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular \openqa benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity. \end{abstract}
\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/intro_end_to_end.eps}
\caption{\label{fig:intro} \ \thename augments language model pre-training with a {\bf neural knowledge retriever}
that retrieves knowledge from a {\bf textual knowledge corpus},  (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in ---a significant computational challenge that we address.}
\end{figure}

Recent advances in language model pre-training have shown that models such as BERT~\cite{bert}, RoBERTa~\cite{roberta} and T5~\cite{t5} store a surprising amount of world knowledge,
acquired from the massive text corpora they are trained on~\cite{lm_as_kb}.
For example, BERT is able to correctly predict the missing word in the following sentence:
\nl{The \blank\xspace is the currency of the United Kingdom} (answer: \nl{pound}).

In these language models, the learned world knowledge is stored {\em implicitly} in the parameters of the underlying neural network. This makes it difficult to determine what knowledge is stored in the network and where.
Furthermore, storage space is limited by the size of the network---to capture more world knowledge, one must train ever-larger networks, which can be prohibitively slow or expensive. 

To capture knowledge in a more interpretable and modular way, we propose a novel framework, \thefullname (\thename) pre-training, which augments language model pre-training algorithms with a learned {\em textual knowledge retriever}.
In contrast to models that store knowledge in their parameters, this approach \emph{explicitly} exposes the role of world knowledge by asking the model to decide what knowledge to retrieve and use during inference.
Before making each prediction, the language model uses the retriever to retrieve documents\footnotemark~from a large corpus such as Wikipedia,
and then attends over those documents to help inform its prediction.
Learning this model end-to-end requires backpropagating through a retrieval step that considers an entire corpus of textual knowledge, as shown in Figure~\ref{fig:intro}.

The key intuition of \thename is to
train the retriever using a {\em performance-based} signal from unsupervised text:
a retrieval that {\em improves} the language model's perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized. 
For example, in Figure~\ref{fig:intro}, if the model needs to fill the blank in \nl{the~\blank~at the top of the pyramid}, the retriever should be rewarded for selecting a document containing \nl{The pyramidion on top allows for less material higher up the pyramid}. We achieve this behavior by modeling our {\em retrieve-then-predict} approach as a latent variable language model and optimizing the marginal likelihood.

Incorporating a large-scale neural retrieval module
during pre-training constitutes a significant computational challenge, since the retriever must consider millions of candidate documents for each pre-training step, and we must backpropagate through its decisions. To address this, we structure the retriever such that the computation performed for each document can be cached and asynchronously updated, and selection of the best documents can be formulated as Maximum Inner Product Search (MIPS).

Numerous prior works have
demonstrated the benefit of adding a discrete retrieval step to neural networks~\cite{key_value_memorynetwork, drqa}, but did not apply the framework
to language model pre-training and employed non-learned retrievers to handle large-scale document collections.
In the language modeling literature, the -Nearest Neighbor Language Model~\cite{knnlm} (NN-LM) retrieves similar LM examples to improve memorization. However, NN-LM was not fine-tuned for downstream tasks, perhaps because it is unclear how to adapt the retrieval mechanism: a NN can only use examples labeled for the target task---during fine-tuning, this precludes LM examples, which contain the desired world knowledge. In contrast, \thename's retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.

We evaluate our approach by fine-tuning the models pre-trained with \thename on the task of Open-domain Question Answering (\openqa), one of the most knowledge-intensive tasks in natural language processing.
We evaluate on three popular \openqa benchmarks (\nq, \wq, and \trec) and compare to state-of-the-art \openqa models, including both extremely large models that store knowledge implicitly (such as T5) as well as previous approaches that also use a knowledge retriever to access external knowledge, but implement retrieval in a more heuristic fashion~\cite{orqa,openqa_hardem,rrp_salesforce}. \thename achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16\% absolute accuracy. We also demonstrate qualitative benefits of \thename, including interpretability and modularity.

\footnotetext{We use the term ``document'' loosely to refer to a passage from the knowledge corpus, not necessarily a whole article.} \section{Background}
\label{sec:background}
\paragraph{Language model pre-training}

The goal of language model pre-training is to learn useful representations of language,
usually from unlabeled text corpora.
The resulting pre-trained model can then be further trained (\emph{fine-tuned}) for a downstream task of primary interest (in our case, \openqa),
often leading to better generalization than training from scratch \cite{dai_finetune,gpt2}.

We focus on the {\em masked language model}\footnote{Strictly speaking, MLM is not a standard language model, since it does not define a distribution over the entire sequence of tokens. In the paper we sometimes abuse the term ``language model'' slightly to make the phrase shorter.}
(MLM) variant of pre-training popularized by BERT~\cite{bert}.
In its basic form, an MLM is trained to predict the missing tokens in an input text passage.
Given an unlabeled pre-training corpus  (e.g., Wikipedia text),
a training example  can be generated by randomly masking tokens in a sampled piece of text
(e.g.,  \nl{The \mask is the currency \mask the UK};  (\nl{pound}, \nl{of})).
The model uses its representation of the masked input  to predict
the token that should go in each mask.
A good MLM must learn to encode syntactic and semantic information (e.g., to predict \nl{of})
as well as some world knowledge (e.g., to predict \nl{pound}).

\paragraph{Open-domain question answering (\openqa)}
To measure a model's ability to incorporate world knowledge, we need a downstream task where world knowledge is critical.
Perhaps one of the most knowledge-intensive tasks in natural language processing is open-domain question answering (\openqa):
given a question  such as ``\texttt{\small What is the currency of the UK?}'', a model must output the correct answer string , ``\texttt{{\small pound}}''.
The ``open'' part of \openqa refers to the fact that the model does {\em not} receive a pre-identified document that is known to contain the answer, unlike traditional reading comprehension (RC) tasks such as SQuAD \cite{squad, squad2}. While RC models comprehend a single document, \openqa models must retain knowledge from millions of documents, since a question could be about any of them.

We focus on \openqa systems that utilize a \emph{textual knowledge corpus}  as the knowledge source.
Many of these systems employ a \emph{retrieval-based} approach:
given a question , retrieve potentially relevant documents  from the corpus ,
and then extract an answer  from the documents~\cite{askmsr,drqa,orqa}.
Our approach, \thename, is inspired by this paradigm and extends it to language model pre-training.
Alternatively, some recent work has proposed \emph{generation-based} systems that
apply a sequence-to-sequence model on  to directly generate  token-by-token
\cite{bart_not_bert,t5}.
We will compare against state-of-the-art systems from both paradigms in our experiments. \section{Approach} \label{sec:approach}
\begin{figure*}[t!]
\centering
\includegraphics[width=.9\textwidth]{figures/pretrain_finetune.eps}
\caption{The overall framework of \thename. {\bf Left:} {\em Unsupervised pre-training.} The knowledge retriever and knowledge-augmented encoder are jointly pre-trained on the unsupervised language modeling task. 
{\bf Right:} {\em Supervised fine-tuning.} After the parameters of the retriever () and encoder () have been pre-trained, they are then fine-tuned on a task of primary interest, using supervised examples.}
\label{fig:approach}
\end{figure*}

We start by formalizing \thename's pre-training and fine-tuning tasks as a {\em retrieve-then-predict} generative process in Section~\ref{sec:generative_process}. Then in Section~\ref{sec:model_architecture}, we describe the model architectures for each component of that process. In Section~\ref{sec:training}, we show how to implement \thename pre-training and fine-tuning by maximizing the likelihood of \thename's generative process. En route, we address important computational challenges, explain why training works, and also discuss strategies for injecting useful inductive biases. The overall framework is illustrated in Figure~\ref{fig:approach}.


\subsection{\thename's generative process}
\label{sec:generative_process}
For both pre-training and fine-tuning, \thename takes some input  and learns a distribution  over possible outputs . For pre-training, the task is masked language modeling:  is a sentence from a pre-training corpus  with some tokens masked out, and the model must predict the value of those missing tokens, . For fine-tuning, the task is \openqa:  is a question, and  is the answer.

\thename decomposes  into two steps: {\em retrieve}, then {\em predict}. Given an input , we first retrieve possibly helpful documents  from a knowledge corpus . We model this as a sample from the distribution . Then, we condition on both the retrieved  and the original input  to generate the output ---modeled as . To obtain the overall likelihood of generating , we treat  as a latent variable and marginalize over all possible documents , yielding 

\subsection{Model architecture}
\label{sec:model_architecture}
We now describe the two key components: the \textbf{neural knowledge retriever}, which models , and the \textbf{knowledge-augmented encoder}, which models .

\paragraph{Knowledge Retriever}
The retriever is defined using a dense inner product model:

where  and  are embedding
functions that map  and  respectively to -dimensional vectors.
The \emph{relevance score}  between  and  is defined as the inner product of the vector embeddings.
The retrieval distribution is the softmax over all relevance scores.

We implement the embedding functions using BERT-style Transformers \cite{bert}. Following standard practices, we join spans of text by applying wordpiece tokenization, separating them with  tokens, prefixing a  token, and appending a final  token.

As in \citet{bert}, we pass this into a Transformer, which produces one vector for each token, including the vector corresponding to  which is used as a ``pooled'' representation of the sequence (denoted ). Finally, we perform a linear projection to reduce the dimensionality of the vector, denoted as a projection matrix :
where  is the document's title and  is its body.
We let  denote all parameters associated with the retriever, which include the Transformer and projection matrices.

\paragraph{Knowledge-Augmented Encoder}
Given an input  and a retrieved document , the knowledge-augmented encoder defines . We join  and  into a single sequence that we feed into a Transformer (distinct from the one used in the retriever). This allows us to perform rich cross-attention between  and  before predicting . See Figure~\ref{fig:intro} for a concrete example.

At this stage, the architectures for pre-training and fine-tuning differ slightly. For the masked language model pre-training task, we must predict the original value of each \mask token in . To do so, we use the same masked language modeling (MLM) loss as in \citet{bert}:

where  denotes the Transformer output vector corresponding to the  masked token,  is the total number of \mask tokens in , and  is a learned word embedding for token .

For \openqa fine-tuning, we wish to produce the answer string . Following previous reading comprehension work \cite{squad,bidaf,rasor,bidaf_plusplus}, we will assume that the answer  can be found as a contiguous sequence of tokens in some document .
Let  be the set of spans matching  in . Then we can define  as:

where  and  denote the Transformer output vectors corresponding to the start and end tokens of span , respectively, while  denotes a feed-forward neural network. We will let  denote all parameters associated with the knowledge-augmented encoder.

\subsection{Training}
\label{sec:training}

For both pre-training and fine-tuning, we train by maximizing the log-likelihood  of the correct output . Since both the knowledge retriever and knowledge-augmented encoder are differentiable neural networks, we can compute the gradient of  (defined in Equation~\ref{eqn:marginal}) with respect to the model parameters  and , and optimize using stochastic gradient descent.

The key computational challenge is that the marginal probability

involves a summation over all documents  in the knowledge corpus . We approximate this by instead summing over the top  documents with highest probability under ---this is reasonable if most documents have near zero probability.

Even with this approximation, we still need an efficient way to find the top  documents. Note that the ordering of documents under  is the same as under the relevance score , which is an inner product. Thus, we can employ Maximum Inner Product Search (MIPS) algorithms to find the approximate top  documents, using running time and storage space that scale sub-linearly with the number of documents \cite{mips_cone,mips_alsh,mips_binary}.

To employ MIPS, we must pre-compute  for every   and construct an efficient search index over these embeddings. However, this data structure will no longer be consistent with  if the parameters  of  are later updated. Hence, the search index goes ``stale'' after every gradient update on .

Our solution is to ``refresh'' the index by asynchronously re-embedding and re-indexing all documents every several hundred training steps. The MIPS index is slightly stale between refreshes, but note that it is {\em only} used to select the top  documents. We recompute  and its gradient, using the fresh , for these top  documents after retrieving them.
In Section~\ref{sec:ablation}, we empirically demonstrate that this procedure results in stable optimization, provided that refreshes happen at a sufficiently frequent rate.

\paragraph{Implementing asynchronous MIPS refreshes}
We asynchronously refresh the MIPS index by running two jobs in parallel: a primary {\em trainer} job, which performs gradient updates on the parameters, and a secondary {\em index builder} job, which embeds and indexes the documents. As shown below, the trainer sends the index builder a snapshot of its parameters, . The trainer then continues to train while the index builder uses  to construct a new index in the background. As soon as the index builder is done, it sends the new index back to the trainer, and the process repeats.

{
\begin{figure}
\begin{center}
\includegraphics[width=.8\columnwidth]{figures/trainer_generator.eps}
\caption{\thename pre-training with asynchronous MIPS refreshes.}
\end{center}
\end{figure}
}

While asynchronous refreshes can be used for both pre-training and fine-tuning, in our experiments we only use it for pre-training. For fine-tuning, we just build the MIPS index once (using the pre-trained ) for simplicity and do not update .\footnote{This works because pre-training already yields a good  function. However, it is possible that refreshing the index would further improve performance.} Note that we still fine-tune , so the retrieval function is still updated from the query side.

\paragraph{What does the retriever learn?}
Since the knowledge retrieval of \thename is latent, it is not obvious how the training objective encourages meaningful retrievals. Here, we show how it rewards retrievals that improve prediction accuracy.

For a given query  and document , recall that 
is the ``relevance score'' that the knowledge retriever assigns to document .
We can see how a single step of gradient descent during \thename pre-training alters this score by analyzing
the gradient with respect to the parameters of the knowledge retriever, :

For each document , the gradient encourages
the retriever to change the score  by  --- 
increasing if  is positive, and decreasing if negative.
The multiplier 
is positive if and only if .
The term  is the probability of predicting
the correct output  when using document .
The term  is the expected value of  when randomly sampling a document from . Hence, document  receives a positive update whenever it performs better than expected.


\subsection{Injecting inductive biases into pre-training} \label{sec:inductive-bias}
In the process of developing \thename, we discovered several additional strategies that further guide the model towards meaningful retrievals, described below.

\paragraph{Salient span masking}
During \thename pre-training, we want to focus on examples  that require world knowledge to predict the masked tokens. As explained in Section~\ref{sec:background}, some MLM spans only require local context. To focus on problems that require world knowledge, we mask \emph{salient spans} such as \nl{United Kingdom} or \nl{July 1969}. We use a BERT-based tagger trained on CoNLL-2003 data \cite{conll_ner} to identify named entities, and a regular expression to identify dates. We select and mask one of these salient spans within a sentence for the masked language modeling task. We show that this significantly outperforms other masking strategies in Section~\ref{sec:ablation}.

\paragraph{Null document}
Even with salient span masking, not all masked tokens require world knowledge to predict. We model this by adding an empty \emph{null document} ~to the top  retrieved documents, allowing appropriate credit to be assigned to a consistent sink when no retrieval is necessary.

\paragraph{Prohibiting trivial retrievals}
If the pre-training corpus  and the knowledge corpus  are the same, there exists a trivial retrieval candidate  that is \emph{too} informative: if the masked sentence  comes from document , the knowledge augmented encoder can trivially predict  by looking at the unmasked version of  in . This results in a large positive gradient for . If this occurs too often, the knowledge retriever ends up learning to look for exact string matches between  and , which does not capture other forms of relevance. For this reason, we exclude this trivial candidate during pre-training.

\paragraph{Initialization}
At the beginning of training, if the retriever does not have good embeddings for  and , the retrieved documents  will likely be unrelated to . This causes the knowledge augmented encoder to learn to ignore the retrieved documents. Once this occurs, the knowledge retriever does not receive a meaningful gradient and cannot improve, creating a vicious cycle. To avoid this cold-start problem, we warm-start  and  using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from. We defer to ~\citet{orqa} for details. For the knowledge-augmented encoder, we warm-start it with BERT pre-training---specifically, the uncased BERT-base model (12 layers, 768 hidden units, 12 attention heads).
 \section{Experiments} \label{sec:experiments}


We now evaluate our approach on the \openqa task.
In this section, we describe in detail the benchmarks used and the different approaches to which we compare empirically.

\subsection{\openqa Benchmarks}
A number of benchmarks have been proposed for \openqa. In this work, we focus on datasets where the question writers did not already know the answer. This yields questions that reflect more realistic information-seeking needs, and also avoids artifacts that can arise if the question is formulated with a particular answer in mind. A deeper justification is given in ~\citet{orqa}.
In all cases, the predicted answer is evaluated via exact match with any reference answer, following previous \openqa work~\cite{drqa}.

\paragraph{NaturalQuestions-Open} The NaturalQuestions dataset \cite{naturalquestions} consists of naturally occurring Google queries and their answers. Each answer also comes with an ``answer type'': following \citet{orqa}, we only keep questions that are categorized as ``short answer type'' with at most five tokens. The dataset also provides a suggested Wikipedia document to retrieve; like all models we compare against, we do not provide this to our model.



\paragraph{WebQuestions} The WebQuestions dataset \cite{webquestions} was collected from the Google Suggest API, using one seed question and expanding the set to related questions. We follow the setting defined by \citet{drqa}.

\paragraph{CuratedTrec}
The CuratedTrec dataset is a collection of question-answer pairs drawn from real user queries issued on sites such as MSNSearch and AskJeeves. To account for multiple correct answers or different spelling variations, the answers in this dataset are defined as regular expressions that match all correct answers. It is unclear how to train generation-based models with this type of supervision, so we do not evaluate them on this dataset.

\subsection{Approaches compared}
\label{sec:comparisons}
\paragraph{Retrieval-based \openqa}
Most existing \openqa systems answer the input question by first retrieving potentially relevant documents from a knowledge corpus, and then using a reading comprehension system to extract an answer from the documents. In this paradigm, the knowledge is stored \emph{explicitly} in the corpus.
We wish to compare different methods for implementing retrieval.

\hyphenation{HardEM}
Many approaches use non-learned heuristic retrieval such as sparse bag-of-words matching~\cite{bm25} or entity linking on the question to select a small set of relevant documents (e.g., 20). These documents are typically then re-ranked using a learned model, but coverage may be limited by the initial heuristic retrieval step. Approaches such as DrQA~\cite{drqa}, HardEM~\cite{openqa_hardem}, GraphRetriever~\cite{GraphRetriever}, and PathRetriever~\cite{rrp_salesforce} in Table~\ref{tab:main_results} are in this category.

Some recent approaches have proposed to implement learnable retrieval using a MIPS index. ORQA~\cite{orqa} formulates \openqa using a similar latent variable model as \thename, and also trains by maximizing the marginal likelihood. However, \thename adds a novel language model pre-training step, and backpropagates into the MIPS index, rather than using a fixed index. In Table~\ref{tab:main_results}, we directly compare the two. It is also important to note that the retrievers for both \thename pretraining and ORQA are initialized using the Inverse Cloze Task, described in Section~\ref{sec:inductive-bias}.





\paragraph{Generation-based \openqa}
An emerging alternative approach to \openqa is to model it as a sequence prediction task:
simply encode the question, and then decode the answer token-by-token based on the encoding.
While it was initially unclear how large amounts of knowledge could be injected into the model,
GPT-2~\cite{gpt2} hinted at the possibility of directly generating answers without using any given context via sequence-to-sequence. However, their performance was not competitive possibly due to the lack of fine-tuning. Orthogonally, T5~\cite{t5} showed that directly generating answers without explicit extraction from the given context is viable approach, but they only experimented on the reading comprehension task, where a context document is provided.




For the most competitive and comparable generation-based baseline, we compare to concurrent work which fine-tunes T5 for \openqa~\cite{t5_openqa}.\footnote{We initially conducted our own T5 experiments using the code from {\tiny\url{https://tinyurl.com/t5-openqa-colab}}~\cite{t5}. We now report results from the concurrent work of \citet{t5_openqa}, which has an improved fine-tuning procedure.}
We compare against the Base, Large, and even larger 11-billion parameter model to measure the effect of model size.

\begin{table*}[t!]
\caption{Test results on \openqa benchmarks. The number of train/test examples are shown in paretheses below each benchmark. Predictions are evaluated with exact match against any reference answer. Sparse retrieval denotes methods that use sparse features such as TF-IDF and BM25. Our model, \thename, outperforms all existing systems. }
\vspace{.1in}
\centering
\footnotesize
\begin{tabular}{@{}lllcccr@{}}
\toprule
\textbf{Name} & \textbf{Architectures}
& \makecell[l]{\textbf{Pre-training}}
& \makecell{\textbf{NQ}\3k/2k)} & \makecell{\textbf{CT}\
\nabla\log p(y\mid x) &= p(y\mid x)^{-1}\nabla p(y\mid x) \\
 &= p(y\mid x)^{-1}\sum_{z}p(y\mid z,x)\nabla p(z\mid x) \\
 &= p(y\mid x)^{-1}\sum_{z}p(y\mid z,x)p(z\mid x)\nabla\log p(z\mid x) \\
 & =\sum_{z}p(z\mid y,x)\nabla\log p(z\mid x),

\nabla\log p(z\mid x) &= \nabla\log\frac{\exp f(x,z)}{\sum_{z'}\exp f(x,z')} \\
 &= \nabla\left[f(x,z)-\log\sum_{z'}\exp f(x,z')\right] \\
 &= \nabla f(x,z)-\sum_{z'}p(z'\mid x)\nabla f(x,z')

\nabla\log p\left(y\mid x\right) & =\sum_{z}p\left(z\mid y,x\right)\left[\nabla f(x,z)-\sum_{z'}p\left(z'\mid x\right)\nabla f(x,z')\right]\\
 & =\sum_{z}p\left(z\mid y,x\right)\nabla f(x,z)-\sum_{z'}p\left(z'\mid x\right)\nabla f(x,z')\\
 & =\sum_{z}\left[p\left(z\mid y,x\right)-p\left(z\mid x\right)\right]\nabla f(x,z)\\
 & =\sum_{z}\left[\frac{p\left(y\mid z,x\right)p\left(z\mid x\right)}{p\left(y\mid x\right)}-p\left(z\mid x\right)\right]\nabla f(x,z)\\
 & =\sum_{z}\left[\frac{p\left(y\mid z,x\right)}{p\left(y\mid x\right)}-1\right]p\left(z\mid x\right)\nabla f(x,z).

\nabla\log p\left(y\mid x\right)=\sum_{z}\left[p\left(z\mid y,x\right)-p\left(z\mid x\right)\right]\nabla f(x,z).

\nabla\log p\left(y\mid x\right) & =\nabla f\left(x,z^{*}\right)-\sum_{z}p\left(z\mid x\right)\nabla f(x,z)\\
 & =\nabla\log p\left(z^{*}\mid x\right).

\text{RU}(z \mid x) = \log p(y \mid z, x) - \log p(y \mid \znull, x). 
\label{eq:retrieval_utility}

A negative RU shows that  is less useful for predicting  than the null document. This could mean that  is irrelevant to , but could also mean that the masked tokens in  do not require world knowledge to predict, or that the world knowledge is sufficiently commonplace it has been baked into the model's parameters. In practice, we find that RU increases steadily over the course of pre-training, and is more predictive of good performance on the downstream task of \openqa than even the overall log-likelihood. An example of how RU behaves over time and across different settings is in Figure~\ref{fig:retrieval_utility}.
\begin{figure*}[ptb]
\centering
\begin{tikzpicture}
 \begin{axis}[
   width=1.2\columnwidth,
   height=0.8\columnwidth,
   legend cell align=left,
   legend style={at={(1, 1)},anchor=north east,font=\scriptsize},
   mark options={mark size=3},
   font=\scriptsize,
   xmin=0, xmax=200,
   ymin=0, ymax=3.2,
   xtick={0, 50, 100, 150, 200},
   ymajorgrids=true,
   xmajorgrids=true,
   xlabel style={yshift=0.5ex,},
   xlabel=Pre-training Steps (Thousands) ,
   ylabel=Retrieval Utility,
   ylabel style={yshift=-0.5ex,}]
    \addplot[mark=triangle,g-blue] plot coordinates {
(0, 0.4870335553570302)
(10, 1.6283367644777194)
(20, 1.933776036871063)
(30, 2.0041376407247413)
(40, 1.9967591197965078)
(50, 1.967421851377687)
(60, 1.9783062606785702)
(70, 1.9747018990660743)
(80, 1.9228911003560645)
(90, 2.036878828395492)
(100, 1.9200309391254438)
(110, 1.914781704899911)
(120, 1.9980497854538093)
(130, 1.8706547102603057)
(140, 1.9837496104468106)
(150, 1.973855775378632)
(160, 1.9895911115697824)
(170, 1.9696919231070622)
(180, 2.028405946627168)
(190, 1.9590725785261562)
(200, 2.086580118497257)
    };
    \addlegendentry{Salient span masking}
    \addplot[mark=x,g-red] plot coordinates {
(0, 0.23141121073211263)
(10, 0.8205503256147296)
(20, 0.854025527215577)
(30, 1.003707903515242)
(40, 1.0150897896138955)
(50, 0.9640269530993363)
(60, 0.9427491107832932)
(70, 0.9697737769948522)
(80, 1.0363693585282714)
(90, 1.0821630555977242)
(100, 0.9739717430562631)
(110, 1.0695464290750996)
(120, 1.02078362435653)
(130, 1.0538118199206679)
(140, 1.0220123031814914)
(150, 0.9783359728089053)
(160, 1.0108277345016037)
(170, 1.1087646700114135)
(180, 1.0097750809231012)
(190, 1.017119983666107)
(200, 1.0468622987085805)
    };
    \addlegendentry{Random span masking}
    \addplot[mark=o,g-orange] plot coordinates {
(0, 0.24583594144307278)
(10, 0.3688633470909383)
(20, 0.36925738236742056)
(30, 0.416591803198644)
(40, 0.4619641520176722)
(50, 0.4627946554077122)
(60, 0.5223345536397)
(70, 0.4888092925400017)
(80, 0.5354503663593189)
(90, 0.5411899211868447)
(100, 0.5492647498842971)
(110, 0.5604785949541389)
(120, 0.556891506009893)
(130, 0.5717637951899287)
(140, 0.5974641177394396)
(150, 0.6012307466956258)
(160, 0.6146574663791348)
(170, 0.6005905914097749)
(180, 0.5742741544145618)
(190, 0.6144357218130096)
(200, 0.6187538774662582)
};
\addlegendentry{Random uniform masking}
\end{axis}
\end{tikzpicture}
\caption{The Retrieval Utility (RU, described in Eq.~\ref{eq:retrieval_utility}) vs the number of pre-training steps. RU roughly estimates the ``usefulness''
of retrieval. RU is impacted by the choice of masking and the number of pre-training steps. }
\label{fig:retrieval_utility}
\end{figure*}   \bibliographystyle{icml2020}
\end{document}