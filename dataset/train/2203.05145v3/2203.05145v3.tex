\documentclass{bmvc2k}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=cyan,
}


\title{Cascaded Sparse Feature Propagation Network for Interactive Segmentation}

\addauthor{Chuyu Zhang*}{zhangchy2@shanghaitech.edu.cn}{1,3}
\addauthor{Chuanyang Hu*}{huchy3@shanghaitech.edu.cn}{1}
\addauthor{Hui Ren}{renhui@shanghaitech.edu.cn}{1}
\addauthor{Yongfei Liu}{liuyf3@shanghaitech.edu.cn}{1}
\addauthor{Xuming He}{hexm@shanghaitech.edu.cn}{1,2}

\addinstitution{
ShanghaiTech University \\
Shanghai, China
}
\addinstitution{
Shanghai Engineering \\
Research Center of \\
Intelligent Vision and Imaging \\
Shanghai, China
}

\addinstitution{
Lingang Laboratory \\
Shanghai, China
}

\runninghead{Zhang et al.}{Cascaded Sparse Feature Propagation Network}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\begin{document}

\maketitle
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work.}
\begin{abstract}
We aim to tackle the problem of point-based interactive segmentation, in which the key challenge is to propagate the user-provided annotations to unlabeled regions efficiently. Existing methods tackle this challenge by utilizing computationally expensive fully connected graphs or transformer architectures that sacrifice important fine-grained information required for accurate segmentation. To overcome these limitations, we propose a cascade sparse feature propagation network that learns a click-augmented feature representation for propagating user-provided information to unlabeled regions. The sparse design of our network enables efficient information propagation on high-resolution features, resulting in more detailed object segmentation. We validate the effectiveness of our method through comprehensive experiments on various benchmarks, and the results demonstrate the superior performance of our approach. Code is available at \href{https://github.com/kleinzcy/CSFPN}{https://github.com/kleinzcy/CSFPN}.
\end{abstract}


%
 
\section{Introduction}
Interactive image segmentation plays a vital role in a broad range of human-in-the-loop vision tasks, such as image editing~\cite{cheng2010repfinder}, medical image analysis~\cite{wang2018deepigeos} and dense image annotation~\cite{sofiiuk2020f}. There has been a long history of interactive segmentation in vision literature, in which a variety of interaction strategies have been explored, including points~\cite{xu2016deep,sofiiuk2020f}, scribbles~\cite{bai2014error,agustsson2019interactive}, and bounding boxes~\cite{rother2004grabcut}. In this work, we mainly focus on the point-based interactive segmentation that only provides point clicks to indicate foreground or background on an image, which typically requires less effort from human annotators.  
Unlike semantic segmentation, which only takes the input image, point-based interactive segmentation takes both image and extra human point annotation based on the segmentation map as input. Therefore, the main challenge of interactive segmentation is to propagate human-provided feedback information to unlabeled pixels effectively.

Most methods only utilize point clicks in the network input, where they encode human input as a Gaussian map and then utilize stacked convolutions to propagate user annotation in an implicit manner~\cite{xu2016deep,lin2020interactive}. This strategy has a limited capacity to capture long-range dependency, often leading to incomplete foreground masks. 
To tackle this, CDNet~\cite{chen2021conditional} adopt fully-connected graph networks~\cite{wang2018non} to facilitate information propagation at both global and local levels. Nonetheless, the fully-connected graph network can only cope with relatively low-resolution feature maps due to its high-computation complexity, which can result in inaccurate boundaries. Recently, ViT~\cite{dosovitskiy2020image} has been introduced into interactive segmentation, such as SAM~\cite{kirillov2023segment}. Although SAM can capture long-range dependency, it is limited by the resolution of ViT and hard to handle fine-grained segmentation. 



To capture long-range dependency and handle fine-grained segmentation, we propose a novel cascaded sparse feature propagation network for interactive image segmentation. Our main idea is to \textit{learn a click-augmented feature representation} based on a cascaded sparse graph neural network (GNN), which allows efficient long-range information propagation at a high spatial resolution, thus enabling us to generate more accurate segmentation. To this end, we introduce a new sparse graph network to propagate the user click information to the unlabeled region in a non-local yet efficient manner. 

Specifically, we first select a set of high-level feature representations corresponding to human-provided points as the source information, which is clean and informative, and propagate the selected source information to the unlabeled region by a sparse graph neural network. Such a sparse graph design is effective in propagating human-provided information but is unable to preserve high-resolution information, which is vitally important for interactive segmentation. Therefore, to preserve more detailed information, we propose another high-resolution sparse feature propagation network, which fuses high and low-level information and simultaneously propagates the fused information to the target region. Thanks to the sparsity of our design, our graph neural network can propagate information on feature maps of 1/2 image height and width, which is infeasible for fully connected graph networks. Such a cascaded network of two sparse GNNs is capable of computing a set of click-augmented feature representations at high spatial resolution in linear complexity, achieving high efficiency in propagating information and preserving more detailed information for foreground mask prediction. What's more, we adopt a global-to-local strategy to zoom-into human-interested regions for more accurate and efficient segmentation.

We conduct extensive experiments on six datasets, including GrabCut, Berkeley, DAVIS, COCO, and SBD, with a detailed ablation study. The results demonstrate the effectiveness of our method, which achieves the state-of-the-art performance.
Our contributions are summarized as the following:
\begin{itemize}
\item We propose a cascaded sparse feature propagation network for interactive image segmentation, which is capable of capturing long-range dependency on both low- and high-resolution feature maps and generating more accurate object boundaries. 
\item We develop a global-to-local strategy, which dynamically zooms into human-interested regions and provides more high-resolution information.
\item Our method achieves state-of-the-art results on most public benchmarks, demonstrating the effectiveness of our design. 
\end{itemize} \section{Related works}
\paragraph*{Interactive segmentation:}
Interactive image segmentation has attracted much attention in computer vision research, and a variety of interaction strategies have been developed based on bounding boxes, scribbles, or points. While the bounding-box-based methods~\cite{rother2004grabcut,zhang2020interactive,wu2014milcut} can localize the target object quickly, and the scribble-based methods~\cite{grady2006random,bai2014error,gulshan2010geodesic,agustsson2019interactive} provide richer user-input cues, they often involve more user interactions. By contrast, the point-based, where a user provides points to indicate foregrounds or backgrounds on the image, requires less effort from human annotators~\cite{xu2016deep,sofiiuk2020f,sofiiuk2021reviving,chen2021conditional,benenson2019large,Hao_2021_ICCV}. Consequently, we mainly focus on the point-based methods in the discussion below.

Many point-based works have emerged since \cite{xu2016deep} first proposed a CNN-based method, and they can be largely grouped into two categories. While one trend focuses on annotating object boundaries~\cite{castrejon2017annotating,le2018interactive,acuna2018efficient,ling2019fast}, most deep learning methods perform region-based segmentation, aiming to leverage user clicks in each interactive step more efficiently. In particular, \cite{liew2017regional} attempts to refine local regions based on pairs of positive and negative clicks.  \cite{majumder2019content} generates a content-aware guidance map for exploiting the hierarchical structural information in the image. \cite{lin2020interactive} argues that the first click is more important than others and designs a first-click attention mechanism. \cite{Hao_2021_ICCV} improves the usage of interactive information from user clicks with the edge-guided flow. To better preserve detailed information, \cite{Chen_2022_CVPR,Lin_2022_CVPR} propose a local refinement strategy. To better adapt to test cases, \cite{jang2019interactive,sofiiuk2020f} develop a backpropagating refinement scheme to correct the mislabeled user clicks in the test time. 

In spite of their promising performances, existing approaches in the field typically rely on human-provided information in the input space. These methods encode positive and negative points provided by humans as Gaussian maps and expect the convolutional neural network (CNN) to propagate this information to unlabeled points. Additionally, several recent approaches~\cite{kirillov2023segment,liu2022simpleclick} have introduced transformer-based methods that effectively propagate user-provided information. However, these methods are limited in their resolution due to the constraints of the vision transformer architecture.

In contrast to these approaches, we propose a cascaded sparse feature propagation network that leverages human-provided information on both low- and high-resolution feature maps. This allows for more effective utilization of the human-provided points. Furthermore, our approach adopts a global-to-local strategy, which aims to exclude the distraction from background regions and accurately localize the regions of interest identified by humans. Our strategy is simple and complementary to previous methods that focused on local region refinement.



\paragraph{Graph neural network:}
Graph neural networks ~\cite{gori2005new,scarselli2008graph} have been widely adopted to capture long-range dependencies, and there exists a large body of literature on this research topic~\cite{wu2020comprehensive}. However, only a few works utilize GNNs in the task of interactive segmentation. For instance, based on the non-local networks~\cite{wang2018non}, CDNet~\cite{chen2021conditional} proposes a conditional diffusion network for interactive segmentation, which performs non-local feature propagation on the global convolutional features and local-level pixels using color similarity.
This mixed strategy tends to suffer from inaccurate foreground boundaries due to the low-resolution deep feature map and/or the noisy graph affinity estimated based on previous foreground masks or color similarity. In contrast, we propose a simple sparse attention-based non-local graph network to propagate the click information,  which can be applied to a high-resolution feature map and generate more accurate masks. \section{Method}
\begin{figure*}[!t]
	\centering
	\includegraphics[scale=0.55]{imgs/BMVC.pdf}
	\caption{The overall architecture of cascaded sparse feature propagation network (CSFP). We take as input an image, the previous step probability map and a set of human clicks. In the feature space, to fully utilize human-provided information, we select human-provided click information and propagate that information by a cascaded of sparse graphs.
}
	\label{fig:Overview}
\end{figure*}
\subsection{Method Overview}\label{overview}
Interactive segmentation aims to correctly infer the region of the user's interest and segment the target object with as few clicks as possible. This sequential estimation task is typically converted into a series of foreground segmentation problems, each of which aims to output a foreground mask as accurately as possible.


In this work, we focus on the key aspects of click-guided foreground mask prediction to efficiently propagate the click labels to unlabeled regions. To this end, we propose a novel cascaded sparse feature propagation module, which performs information propagation on both low- and high-resolution feature maps as shown in Fig.\ref{fig:Overview}. Our model employs the newly designed sparse GNNs to facilitate the information propagation of the user's inputs. In addition, we develop a global-to-local strategy to zoom into high-resolution human-interested regions for more efficient segmentation. Below, we introduce the details of our model design.



\subsection{Cascaded Sparse Feature Propagation}\label{prop}
We aim to propagate the user-provided information to unlabeled regions in the input images. Due to the sparsity of user clicks, this typically requires modeling long-range feature relations across the image plane. To achieve efficient information propagation, we introduce a new graph neural network module, which augments a base CNN segmentation network for predicting the foreground mask. In contrast to previous non-local design~\cite{wang2018non,chen2021conditional}, our graph network module is built on a sparse graph topology, which enables us to compute a user-input-aware representation on a high-resolution feature map and hence produces detailed foreground segmentation with accurate boundaries.   


Specifically, we first use a CNN-based segmentation network to compute a stack of feature maps from which a higher-level and a lower-level map are selected. The higher-level feature map, denoted as , typically encodes more semantics but has a low resolution, while the lower-level map, denoted as , has a high resolution and preserves more object boundary cues. The size of  and  are 1/16 and 1/4 of the original image size. 
In order to better exploit both the higher- and lower-level features, we employ a cascaded design: a sparse graph network first augments the higher-level feature map with the user-click features, which is further integrated with the lower-level features in a high-res sparse graph network at the second stage. 
Below we describe the details of those two graph networks in turn.  

\paragraph{Sparse Graph sub-Module (SGM):} 
Our sparse graph submodule performs feature propagation on the higher-level feature maps to disseminate the user-click information to all features. 
Formally, we represent the higher-level feature map  as , where  and  are the height and width of the low-res feature map respectively. At each location,  is a -channel feature vector, and  is the spatial location index. 
To build the sparse graph, we select the feature vectors at the location of user clicks in , denoted as , and connect them to each location on the feature map.  

Given the graph, we perform feature augmentation by passing messages from the click nodes  to each feature location as follows (See Fig.~\ref{fig:labelpropagation} for illustration):

where  is the updated feature,  is a weight matrix for feature transform, and   denotes an attention function in which  and  are linear transforms and  is the normalization factor. 
Intuitively, the augmentation moves all the features towards the click-annotated representations, which reduces in-class variation and improves foreground prediction. However, these augmented features, denoted as , are built on the low-res feature map , which tends to produce coarse segmentation masks. To remedy this, we introduce a second graph network to refine them as below. 

\begin{figure*}[!t]
	\centering
	\includegraphics[scale=0.35]{imgs/CPM.pdf}
	\caption{The structure of sparse graph sub-module(SGM) and high-resolution sparse graph sub-module(HSGM).  and  represent the height and width of the feature map.  and  represent the channel of higher-level and lower-level feature maps, respectively.  is the number of user clicks. For simplicity, we ignore the reshape operation.}
	\label{fig:labelpropagation}
\end{figure*}

\paragraph{High-res Sparse Graph sub-Module (HSGM):}
The second graph network generates a click-augmented feature representation with a high spatial resolution. To this end, we integrate the first-stage output  with the lower-level feature map , followed by another pass of click-to-feature propagation. 
Specifically, we denote the lower-level feature map as 
 where  and  are the height and width of the high-res feature map, respectively. Similar to the SGM, we select the click-annotated feature, represented as , and link them to every feature location on .

Given the high-res graph, we first upsample the previous output  so that it has the same spatial dimension as the lower-level feature map . We then perform feature integration and click-aware augmentation by a message passing as shown in Fig.~\ref{fig:labelpropagation}. Formally, denoting the upsampled feature as , we update the high-res feature representation as follows,

where  denotes the high-res augmented feature,  indicates feature concatenation,  is a transformation function and  is a weight matrix for feature transform. Note that we use the higher-level features  to compute the attention weights so that the information propagation is less susceptible to variations in the lower-level feature . Moreover, our sparse graphs only perform message passing from  selected nodes to  feature nodes, which can be computed efficiently with a complexity  where  in the interaction. 

Given the high-res augmented features , we finally generate the foreground probability map by applying a two-layer conv-block followed by the Sigmoid function. 

\subsection{Model Training}\label{train}
To train our deep network for interactive segmentation, we first utilize a simulation process to generate a set of image-click pairs from a foreground segmentation dataset as in~\cite{sofiiuk2021reviving}, which assumes the user clicks on the center of maximum error regions. Given the training dataset, 
following \cite{sofiiuk2020f}, we employ the Normalized Focal Loss (NFL)~\cite{sofiiuk2019adaptis} as training objectives. 

\subsection{Global to local strategy}
In interactive segmentation, users have specific areas of interest within an image. Performing segmentation on the entire image is typically less effective than segmenting the user-interested region due to the limited resolution. To achieve more accurate segmentation results in areas of user interest, we propose a global-to-local strategy in the interactive process.

Our global-to-local strategy consists of three steps for each foreground segmentation task. We first perform object segmentation on the entire image by utilizing the segmentation map from the previous interaction round and the human correction click in the current interaction. Subsequently, based on the segmentation map obtained in the first
step, we determine the bounding box of the target and then expand it by a margin, such as  of its original size. Finally, we focus on the region defined by the expanded box and perform target segmentation within this zoomed-in area. By adopting this global-to-local strategy, our model can efficiently segment the areas that users care about and enhance the segmentation quality.



%
 \vspace{-3mm}
\section{Experiments}

In this section, we first describe the experiment setting and implementation details, 
then compare our model with existing works, followed by the ablation study to validate each component. 
Finally, we show some qualitative results to demonstrate the model's efficacy.



\vspace{-2mm}
\subsection{Evaluation and Implementation Details}

\paragraph{Datasets:} We evaluate our method on a wide range of datasets, including GrabCut, Berkeley, DAVIS, COCO and SBD, with the standard evaluation protocol. COCO is split into COCO and COCO according to whether their object classes are in PASCAL VOC or not.














\vspace{-2mm}
\paragraph{\textbf{Metric:}} To mimic the real user clicks in evaluation, we follow \cite{xu2016deep} to click the center of the maximum error region to correct the output mask in each interaction. The interaction process will terminate when the IoU between prediction and ground truth mask exceeds threshold , or reaches the maximum number of clicks. 
In this paper, we typically set  as 85\% or 90\%, and set a maximum number of clicks to 20 as in previous works.
The number of clicks (NoC) and the number of failures (NoF) to meet the termination conditions is used as the evaluation metric. For example, NoC@90 means the average number of clicks for the test set is needed to reach 90\% IoU under 20 maximum clicks, and NoF@90 means the number of failures case that does not reach 90\% IoU with 20 maximum clicks.

\vspace{-2mm}
\paragraph{\textbf{Implementation Details:}} We adopt DeeplabV3+\cite{Chen_2018_ECCV} as our base network.
Besides, we follow \cite{sofiiuk2021reviving} to utilize Conv1S
to fuse click maps and foreground estimation, then sum the fused feature with image features at the output of the first convolutional block.
During training, we follow the same iterative sampling strategy in \cite{sofiiuk2021reviving} to 
generate positive and negative clicks to alleviate the gap between the training and inference stages.
For clicks encoding, we adopt the disk encoding strategy proposed in \cite{benenson2019large} with a fixed radius of 5.
Our network is trained for 120 epochs using the SBD training set and COCO+LVIS dataset. We apply common data augmentation techniques, such as random cropping and scaling. The Adam optimizer is used for optimization with a learning rate of 5e-4. The learning rate is reduced by a factor of 10 at the 100th and 115th epochs. We use a batch size of 28, and the augmented images are resized to .



    





\begin{table*}[t]
    \centering
\caption{\small{Comparison with SOTA. The bold means the best results across different backbones, and the underline means the best results under the same backbone. * means that we implement the results.  denotes the model is trained on COCO+LVIS, while others are trained on the SBD dataset. - denotes the results are not available. The lower is the better.}}
    \label{sota_Table}
    \renewcommand\arraystretch{1.3} 
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{ll|cccccc}
    \toprule[1pt]
    \multicolumn{2}{c|}{\textbf{Method}}        &\textbf{GrabCut} &\textbf{Berkeley}  &\textbf{COCO} &\textbf{COCO} & \textbf{DAVIS} & \textbf{SBD} \\ 
    &                                           &\textbf{NoC@90}  & \textbf{NoC@90}  & \textbf{NoC@85} & \textbf{NoC@85} & \textbf{NoC@85 / 90}  & \textbf{NoC@85 / 90} \\
    \midrule\midrule



    
    CDNet\cite{chen2021conditional}  \scriptsize{\texttt{ICCV21}}                          & ResNet-50       & 2.64 & 3.69   & -    & -   & 5.17~/~6.66 &  4.37~/~7.87  \\
FocusCut\cite{Lin_2022_CVPR} \scriptsize{\texttt{CVPR22}}  & ResNet-50 & \underline{1.78} & 3.44 & - & - & 5.00~/~6.38 & 3.62~/~5.66 \\
    Ours                                                     & ResNet-50       & 2.31 & \underline{3.35}   & \underline{2.46} & \underline{3.69}  & \underline{4.52}~/~\underline{5.83}  & \underline{3.08}~/~\underline{4.98}  \\ 
    \midrule

    IS+SA\cite{zhang2020interactive}  \scriptsize{\texttt{ECCV20}}                    & ResNet-101      & 3.07 & 4.94      & 4.08 & 5.01 & 5.16~/~-    &    -~/~-   \\
    FCA\cite{lin2020interactive} \scriptsize{\texttt{CVPR20}}                              & ResNet-101      & 2.14 & 4.19   & 4.45 & 5.33 &  -~/~7.90   &    -~/~-   \\
f-BRS-B\cite{sofiiuk2020f}  \scriptsize{\texttt{CVPR20}}                              & ResNet-101      & 2.72 & 4.57   & -    & -    & 5.04~/~7.41 & 4.81~/~7.73  \\ 
    RITM*\cite{sofiiuk2021reviving} \scriptsize{\texttt{ARXIV}}                       & ResNet-101     & 2.31 & 3.50   & 2.54 & 3.61 & 5.09~/~6.78 &   3.33~/~5.33 \\
    FocusCut\cite{Lin_2022_CVPR} \scriptsize{\texttt{CVPR22}} & ResNet-101 & \underline{1.64} & \underline{3.01} & - & - & 4.85~/~6.22 & 3.40~/~5.31 \\
    \textbf{Ours}                                                     & ResNet-101      & 2.15 & 3.20 & \underline{2.27} & \underline{3.50} & \underline{4.51} ~/~ \underline{5.80} &  \underline{\textbf{2.98}}~/~\underline{\textbf{4.83}}  \\ \midrule
    
    RITM\cite{sofiiuk2021reviving} \scriptsize{\texttt{ARXIV}}                  & HRNet-18        & 2.04 & 3.22   & 2.40    & 3.61   & 4.94~/~6.71 &  3.39~/~5.43  \\
    PseudoClick\cite{liu2022pseudoclick} \scriptsize{\texttt{ECCV22}} & HRNet-18 & 2.04 & 3.23 & - & - & 4.81~/~6.57 & -~/~5.40 \\
    \textbf{Ours}                                                & HRNet-18        & \underline{1.75} & \underline{2.86} & \underline{2.23} & \underline{3.00} & \underline{4.68}~/~ \underline{6.01} &  \underline{3.33}~/~\underline{5.25} \\ \midrule
    SAM\cite{kirillov2023segment} \scriptsize{\texttt{ARXIV}}                 & ViT-H        & 1.84 & \textbf{2.09}   & \multicolumn{2}{c}{5.16}    & 4.21~/~5.32 & 5.23~/~8.50  \\ \midrule
    FocalClick\cite{Chen_2022_CVPR} \scriptsize{\texttt{CVPR22}} & HRNet-32 & 1.80 & 2.36 & - & - & \underline{\textbf{4.01}}~/~5.39 & 4.24~/~6.51 \\
    RITM\cite{sofiiuk2021reviving} \scriptsize{\texttt{ARXIV}}                 & HRNet-18        & \underline{\textbf{1.54}} & 2.26   & -    & -    & 4.36~/~5.74 &  3.80~/~6.06  \\
    \textbf{Ours}                                                 & HRNet-18        & 1.68 & \underline{2.12} & \multicolumn{2}{c}{\textbf{\underline{2.17}}} & 4.03~/~ \underline{\textbf{5.22}} &  \underline{3.67}~/~\underline{5.91} \\    
    \bottomrule[1pt]
    \end{tabular}}

\end{table*}



\subsection{Quantitative Results}



\paragraph{\textbf{Main results:}} 
As shown in Tab.\ref{sota_Table}, we compare our method with previous approaches over a wide range of benchmarks. When trained on the SBD dataset, our method achieves the strongest performance over different datasets, outperforming all existing approaches under the same backbone. We achieve the best results under HRNet-18 backbone, which only requires average \textbf{2.86} clicks to reach 90\% IoU on Berkeley, \textbf{2.23} clicks to reach 85\% IoU on COCO datasets. The result on COCO shows that our framework can generalize to unseen classes greatly. Specifically, for the challenging SBD and fine-grained DAVIS dataset, we achieve average \textbf{5.8} and \textbf{4.83} NoC@90 respectively on the ResNet-101 backbone, improve \textbf{0.42} and \textbf{0.48} compared with FocusCut. 



Moreover, we train our model on the COCO+LVIS dataset. The results in the last three lines show that we still achieve a great improvement on DAVIS. Meanwhile, like RITM, we also observe a drop in SBD datasets compared to those trained on SBD datasets. Furthermore, our method achieves similar or better results than SAM which is trained on massive data. Those results show the superiority of our method.



\vspace{-1mm}
\paragraph{\textbf{IOU@k Analysis:}} We analyse the performance varying over the number of clicks k. As shown in the Fig.\ref{fig:foobar}, our method is consistently better than RITM and about 2\% higher in mIOU for the first five clicks. Moreover, our method outperforms SAM in the first three clicks, and achieves similar results with more than 14 clicks. The results validate the effectiveness of our method.





\paragraph{\textbf{Results with maximum 100 clicks:}} 
We report NoC with the maximum number of clicks limited to 100 on the DAVIS dataset with ResNet-50 backbone. Tab.\ref{tb:a100c} shows that our method improves significantly on NoC@90. Moreover, we improve 0.91 on NoC@90 metric compared with CDNet. This is mainly because our sparse graph can propagate information on high-resolution feature maps which preserve more fine-grained feature information, while CDNet lacks such modeling capacity.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/output_DAVIS.pdf}
\caption{\small{The improvement of mIOU across different numbers of clicks k on DAVIS.}}
    \label{fig:foobar}
\end{figure}










\begin{table}[t]
\centering
\begin{minipage}[t]{0.48\textwidth}
        \centering
             \caption{\small{Analysis of 100 clicks.}}
     \label{tb:a100c}
        \renewcommand\arraystretch{1.2}
        \resizebox{0.9\textwidth}{!}{
        \begin{tabular}{c|ccc}
            \toprule[1pt]
            \textbf{Method}   & \textbf{NoF@90} & \textbf{NoC@90} & \textbf{NoF@90} \\ \midrule\midrule
            f-BRS\cite{sofiiuk2020f}              & 78            & 20.70          & 50             \\
            CDNet\cite{chen2021conditional}              & 65            & 18.59          & 48             \\
            FocusCut\cite{Lin_2022_CVPR}              & 57            & \textbf{17.42}          & \textbf{43}             \\
Ours               & \textbf{56}            & 17.68          & 46       \\ \bottomrule[1pt]       
        \end{tabular}}
\end{minipage} \hfill
\begin{minipage}[t]{0.5\textwidth}
    \centering
         \caption{Component Analysis.}
 \label{CSFP-verification}
    \renewcommand\arraystretch{1.2}
    \resizebox{0.9\textwidth}{!}{	
    \begin{tabular}{c|cccc|cc}
        \toprule[1pt]
        \textbf{\#} & \textbf{BS}  & \textbf{SGM}  & \textbf{HSGM} & \textbf{G2L} & \textbf{DAVIS} & \textbf{SBD} \\ \midrule
        1          &   & -             & -   & -          & 6.85           & 5.67          \\
        2          &   &   & -   & -          & 6.62           & 5.57          \\
        3          &   &   &  & -  & 6.39  & 5.36 \\ 
        4          &   &   &  &   & \textbf{6.05}      & \textbf{5.13} \\\bottomrule[1pt]
    \end{tabular}
        }
\end{minipage}
\vspace{-1em}
\end{table}










\subsection{Ablation study}


\paragraph{\textbf{Effectiveness of each component:}}  We conduct incremental ablation experiments to study the effectiveness 
of each component. As shown in Tab.\ref{CSFP-verification}, the introduction of \textit{Sparse Graph sub-Module (SGM)} brings 0.23 and 0.1 NoC@90 improvements on DAVIS and SBD, respectively. And the introduction of HSGM improves performance further, which means that feature propagation on high-resolution does help to preserve more precise boundary information. Moreover, our results boost significantly with global to local strategy (G2L).
\begin{table*}[!tp]
    \centering
     \caption{\small{Graph design analysis on DAVIS. 
 Low resolution denotes 1/4 feature map while high resolution represents 1/2 scale feature map.}}
 \label{graph-compare}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{c|cc|ccc}
        \toprule[1pt]
        \textbf{Method} & \textbf{Params(M)} & \textbf{Flops(G)}  & \textbf{NoC} & \textbf{NoC} \\ \midrule\midrule
        Baseline*       &  31.4         & 508.72                & 6.60           & 8.42         \\
        Baseline* + FDM~\cite{chen2021conditional}   & 31.42      & 513.82                & 5.40           & 7.64         \\
        Baseline* + FDM in both low \& high res.     & 31.44        & 1510.16                  & \XSolidBrush    & \XSolidBrush \\
        Baseline* + CSFP      & 31.5       & 531.42                  & \textbf{5.05}           & \textbf{7.17}        \\ \bottomrule[1pt]
        \end{tabular}}

\end{table*}

\begin{figure*}[!t]
    \centering
    \includegraphics[scale=0.3]{imgs/cvpr-final-final.pdf}
    \caption{\small{\textbf{Visualization analysis:} The odd and even columns show the prediction result of the baseline and our method. 
Row (a) indicates that our method preserves more accurate boundary information, and Row (b) indicates that our method captures more 
    reliable long-range dependencies}}
    \label{vis-fig:2}
\end{figure*}

\paragraph{\textbf{Sparse Graph Analysis:}}
To validate the efficacy of CSFP design, we conduct more experiments to compare with feature diffusion graph module (FDM) in CDNet~\cite{chen2021conditional}, which is a fully-connected graph network. 
Since our method is easy to implement comparing to FDM, we immigrate our CSFP to CDNet’s baseline (named as Baseline*) and adopt the same training setup as CDNet. As in Tab~\ref{graph-compare}, we can observe that our `Baseline* + CSFP' achieves significant improvement in both metrics. It is worth noting that our advanced sparse graph design makes it feasible to conduct message propagation in both low \& high-resolution feature maps. 
In addition, when applying FDM in both low \& large scale feature maps, we find that the overall network consumes an amount of computation (1510.16 GFLOPs \textit{v.s.} 531.42 GFLOPs), which is unacceptable in real systems.







\vspace{-2mm}
\subsection{\textbf{Visualization analysis}}
\vspace{-1mm}
In Fig.\ref{vis-fig:2}(a), we observe that our method can obtain more precise boundaries even with fewer user clicks. This shows the effectiveness of the CSFP module and global-to-local strategy, which helps us preserve more accurate target region information. In Fig.\ref{vis-fig:2}(b), the results indicate that our sparse graph neural network can capture more reliable long-range dependencies and make the coverage of its prediction more complete. 





 \section{Conclusion}
In this paper, we have developed a novel cascaded sparse feature propagation network for interactive segmentation. Our method is capable of effectively propagating user-provided sparse annotations to the entire input image. To achieve this, we introduce a cascaded sparse feature propagation module that selects user-provided information in the feature space and propagates the provided information to the whole image in high resolution. In addition, we propose a global-to-local strategy in the evaluation, which can accurately locate human-interested regions and zoom into this region, enabling our method to preserve more detailed information. Finally, we evaluated our method on several benchmarks, in which our approach outperforms the prior works by a sizable margin.
\section*{Acknowledgement}
This work was supported by Shanghai Science and Technology Program 21010502700, Shanghai Frontiers Science Center of Human-centered Artificial Intelligence and the MoE Key Lab of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University). 

\bibliography{bmvc_final}

\end{document}