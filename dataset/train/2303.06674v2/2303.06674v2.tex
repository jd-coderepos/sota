

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{773} \def\confName{CVPR}
\def\confYear{2023}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs} \usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\jiangyi}[1]{\textcolor[RGB]{255,0,0}{jiangyi: #1}}
\newcommand{\rankfirst}[1]{\textbf{#1}}
\newcommand{\ranksecond}[1]{\underline{#1}}
\newcommand{\mj}{}
\newcommand{\mf}{}
\newcommand{\mjf}{}
\newcommand{\smj}{}
\newcommand{\smf}{}
\newcommand{\smjf}{}

\newcommand{\mjs}{}
\newcommand{\mfs}{}
\newcommand{\mju}{}
\newcommand{\mfu}{}
\newcommand{\mg}{}

\usepackage{bm}
\def\pt{p_\textrm{t}}
\def\at{\alpha_\textrm{t}}
\newcommand{\eqnnm}[2]{\ignorespaces}
\usepackage{bbm,nicefrac}
\usepackage{colortbl}


\begin{document}

\title{Universal Instance Perception as Object Discovery and Retrieval}



\author{Bin Yan\thanks{This work was performed while Bin Yan worked as an intern at
ByteDance. Email: \href{mailto:yan_bin@mail.dlut.edu.cn}{\color{black}{yan\_bin@mail.dlut.edu.cn}.}  Corresponding authors: \href{mailto:jiangyi.enjoy@bytedance.com}{\color{black}{jiangyi.enjoy@bytedance.com}}, \href{mailto:wdice@dlut.edu.cn}{\color{black}{wdice@dlut.edu.cn}}.},
Yi Jiang,
Jiannan Wu, 
Dong Wang, \\
Ping Luo,
Zehuan Yuan,
Huchuan Lu\\
 School of Information and Communication Engineering, Dalian University of
Technology, China \\
 ByteDance  The University of Hong Kong  Peng Cheng Laboratory
}
\maketitle

\begin{abstract}
All instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent sub-tasks.
In this work, we present a \textbf{un}iversal \textbf{in}stance perception model of the \textbf{next} generation, termed \textbf{UNINEXT}.
UNINEXT reformulates diverse instance perception tasks into a unified object discovery and retrieval paradigm and can flexibly perceive different types of objects by simply changing the input prompts. This unified formulation brings the following benefits: (1) enormous data from different tasks and label vocabularies can be exploited for jointly training general instance-level representations, which is especially beneficial for tasks lacking in training data. (2)  the unified model is parameter-efficient and can save redundant computation when handling multiple tasks simultaneously.  
UNINEXT shows superior performance on 20 challenging benchmarks from 10 instance-level tasks including classical image-level tasks (object detection and instance segmentation), vision-and-language tasks (referring expression comprehension and segmentation), and six video-level object tracking tasks. 
Code is available at \href{https://github.com/MasterBin-IIAU/UNINEXT}{https://github.com/MasterBin-IIAU/UNINEXT}.

\end{abstract}


\section{Introduction}
\label{sec:intro}




Object-centric understanding is one of the most essential and challenging problems in computer vision. Over the years, the diversity of this field increases substantially. In this work, we mainly discuss 10 sub-tasks, distributed on the vertices of the cube shown in Figure~\ref{fig-tasks}. As the most fundamental tasks, object detection~\cite{DPM,FasterRCNN,FPN,YOLO,FCOS,CascadeRCNN, DETR} and instance segmentation~\cite{MaskRCNN,SOLO,PANet,YOLACT,CondInst} require finding all objects of specific categories by boxes and masks respectively. Extending inputs from static images to dynamic videos, Multiple Object Tracking (MOT)~\cite{MOT17,Tracktor,CenterTrack,FairMOT}, Multi-Object Tracking and Segmentation (MOTS)~\cite{MOTS,PointTrackV2,PCAN}, and Video Instance Segmentation (VIS)~\cite{VIS, VISTR, SeqFormer,IFC} require finding all object trajectories of specific categories in videos. Except for \textbf{category names}, some tasks provide other reference information. For example, Referring Expression Comprehension (REC)~\cite{RefCOCO&plus,FAOA,SeqTR}, Referring Expression Segmentation (RES)~\cite{RefCOCO&plus,CMSA,LAVT}, and Referring Video Object Segmentation (R-VOS)~\cite{URVOS,MTTR,ReferFormer} aim at finding objects matched with the given \textbf{language expressions} like ``The fourth person from the left''. Besides, Single Object Tracking (SOT)~\cite{OTB2015,SiameseFC,SiameseRPN} and Video Object Segmentation (VOS)~\cite{YoutubeVOS,STM,STCN} take the \textbf{target annotations} (boxes or masks) given in the first frame as the reference, requiring to predict the trajectories of the tracked objects in the subsequent frames. Since all the above tasks aim to perceive instances of certain properties, we refer to them collectively as \textit{instance perception}.



\begin{figure}[!t]
  \begin{center}
\includegraphics[width=1.0\linewidth]{figs/Tasks-Summary_V3.pdf}
  \end{center}
  \vspace{-5mm}
\caption{Task distribution on the Format-Time-Reference space. Better view on screen with zoom-in.} \label{fig-tasks}
\vspace{-3mm}
\end{figure}

Although bringing convenience to specific applications, such diverse task definitions split the whole field into fragmented pieces. As the result, most current instance perception methods are developed for only a single or a part of sub-tasks and trained on data from specific domains. Such fragmented design philosophy brings the following drawbacks: (1) Independent designs hinder models from learning and sharing generic knowledge between different tasks and domains, causing redundant parameters.  (2) The possibility of mutual collaboration between different tasks is overlooked. For example, object detection data enables models to recognize common objects, which can naturally improve the performance of REC and RES. (3) Restricted by fixed-size classifiers, traditional object detectors are hard to jointly train on multiple datasets with different label vocabularies~\cite{COCO,LVIS,Objects365} and to dynamically change object categories to detect during inference~\cite{COCO,VIS,OVIS,BDD100K,MOT17,TAO}. Since \textit{essentially all instance perception tasks aim at finding certain objects according to some queries}, it leads to a natural question: could we design a unified model to solve all mainstream instance perception tasks once and for all?

To answer this question, we propose UNINEXT, a universal instance perception model of the next generation. We first reorganize 10 instance perception tasks into three types according to the different input prompts: (1) \textbf{category names as prompts} (Object Detection, Instance Segmentation, VIS, MOT, MOTS). (2) \textbf{language expressions as prompts} (REC, RES, R-VOS). (3) \textbf{reference annotations as prompts} (SOT, VOS). Then we propose a unified prompt-guided object discovery and retrieval formulation to solve all the above tasks. Specifically, \textbf{UNINEXT first discovers  object proposals under the guidance of the prompts, then retrieves the final instances from the proposals according to the instance-prompt matching scores}. Based on this new formulation, UNINEXT can flexibly perceive different instances by simply changing the input prompts. To deal with different prompt modalities, we adopt a prompt generation module, which consists of a reference text encoder and a reference visual encoder. Then an early fusion module is used to enhance the raw visual features of the current image and the prompt embeddings. This operation enables deep information exchange and provides highly discriminative representations for the later instance prediction step. Considering the flexible query-to-instance fashion, we choose a Transformer-based object detector~\cite{DeformableDETR} as the instance decoder. Specifically, the decoder first generates  instance proposals, then the prompt is used to retrieve matched objects from these proposals. This flexible retrieval mechanism overcomes the disadvantages of traditional fixed-size classifiers and enables joint training on data from different tasks and domains.

With the unified model architecture, UNINEXT can learn strong generic representations on massive data from various tasks and solve 10 instance-level perception tasks using a single model with the same model parameters. Extensive experiments demonstrate that UNINEXT achieves superior performance on 20 challenging benchmarks. The contributions of our work can be summarized as follows. 
\begin{itemize}
	\vspace{-1mm}
	\item{We propose a unified prompt-guided formulation for universal instance perception, reuniting previously fragmented instance-level sub-tasks into a whole.}
	\vspace{-1mm}
	\item{
	Benefiting from the flexible object discovery and retrieval paradigm, UNINEXT can train on different tasks and domains, in no need of task-specific heads.
	}
	\vspace{-1mm}
	\item{UNINEXT achieves superior performance on 20 challenging benchmarks from 10 instance perception tasks using a single model with the same model parameters.}
\end{itemize}


\section{Related Work}
\textbf{Instance Perception.} The goals and typical methods of 10 instance perception tasks are introduced as follows. 


\textit{Retrieval by Category Names}. Object detection and instance segmentation aim at finding all objects of specific classes on the images in the format of boxes or masks. Early object detectors can be mainly divided into two-stage methods~\cite{FasterRCNN,CascadeRCNN,HTC} and one-stage methods~\cite{YOLO,RetinaNet,FCOS,ATSS,YOLOX} according to whether to use RoI-level operations~\cite{FastRCNN,MaskRCNN}. Recently, Transformer-based detectors~\cite{DETR,DeformableDETR,DN-DETR} have drawn great attention for their conceptually simple and flexible frameworks. Besides, instance segmentation approaches can also be divided into detector-based~\cite{MaskRCNN,CascadeRCNN,HTC,PointRend,CondInst} and detector-free~\cite{SOLO,Mask2Former} fashions according to whether box-level detectors are needed. Object detection and instance segmentation play critical roles and are foundations for all other instance perception tasks. 
For example, MOT, MOTS, and VIS extend image-level detection and segmentation to videos, requiring finding all object trajectories of specific classes in videos. Mainstream algorithms~\cite{DeepSORT,JDE,QDTrack,bytetrack,PCAN,Unicorn} of MOT and MOTS follow an online "detection-then-association" paradigm. However, due to the intrinsic difference in benchmarks of MOTS~\cite{MOTS,BDD100K} (high-resolution long videos) and VIS~\cite{VIS} (low-resolution short videos), most recent VIS methods~\cite{VISTR,IFC,ProposeReduce,SeqFormer} adopt an offline fashion. This strategy performs well on relatively simple VIS2019~\cite{VIS}, but the performance drops drastically on challenging OVIS~\cite{OVIS} benchmark. Recently, IDOL~\cite{IDOL} bridges the performance gap between online fashion and its offline counterparts by discriminative instance embeddings, showing the potential of the online paradigm in unifying MOT, MOTS, and VIS. 

\textit{Retrieval by Language Expressions}. REC, RES, and R-VOS aim at finding one specific target referred by a language expression using boxes or masks on the given images or videos. Similar to object detection, REC methods can be categorized into three paradigms: two-stage~\cite{CM-Att-Erase,DGA,RvG-Tree,NMTree}, one-stage~\cite{FAOA,RCCF,MCN,RESC}, and Transformer-based~\cite{TransVG,TRAR,MDETR} ones. Different from REC, RES approaches~\cite{CMSA,STEP,BRINet,EFN,CGAN,LTS,VLT} focus more on designing diverse attention mechanisms to achieve vision-language alignment. Recently, SeqTR~\cite{SeqTR} unifies REC and RES as a point prediction problem and obtains promising results. Finally, R-VOS can be seen as a natural extension of RES from images to videos. Current state-of-the-art methods~\cite{MTTR,ReferFormer} are Transformer-based and process the whole video in an offline fashion. However, the offline paradigm hinders the applications in the real world such as long videos and ongoing videos (e.g. autonomous driving).

\textit{Retrieval by Reference Annotations}. SOT and VOS first specify tracked objects on the first frame of a video using boxes or masks, then require algorithms to predict the trajectories of the tracked objects in boxes or masks respectively. The core problems of these two tasks include (1) How to extract informative target features? (2) How to fuse the target information with representations of the current frame? For the first question, most SOT methods~\cite{SiameseFC,SiameseRPN,SiamRPNplusplus,TransT,STARK} encode target information by passing a template to a siamese backbone. While VOS approaches~\cite{STM,CFBI,STCN} usually pass multiple previous frames together with corresponding mask results to a memory encoder for extracting fine-grained target information. For the second question, correlations are widely adopted by early SOT algorithms~\cite{SiameseFC,SiameseRPN,AlphaRefine}. However, these simple linear operations may cause serious information loss. To alleviate this problem, later works~\cite{TransT,STARK,MixFormer,OSTrack} resort to Transformer for more discriminative representations. Besides, feature fusion in VOS is almost dominated by space-time memory networks~\cite{STM,CFBI,STCN,XMem}.

\textbf{Unified Vision Models.} 
Recently, unified vision models~\cite{MuST,INTERN,OFA-Ali,Gato,MaskRCNN,Mask2Former,Unicorn,Pix2SeqV2,GLIP,Uni-Perceiver,Unified-IO} have drawn great attention and achieved significant progress due to their strong generalizability and flexibility. Unified vision models attempt to solve multiple vision or multi-modal tasks by a single model. Existing works can be categorized into unified learning paradigms and unified model architectures.


\textit{Unified Learning Paradigms.} These works~\cite{MuST,INTERN,OFA-Ali,Gato,Pathways,Uni-Perceiver,Unified-IO} usually present a universal learning paradigm for covering as many tasks and modalities as possible. For example, MuST~\cite{MuST} presents a multi-task self-training approach for 6 vision tasks. INTERN~\cite{INTERN} introduces a continuous learning scheme, showing strong generalization ability on 26 popular benchmarks. Unified-IO~\cite{Unified-IO} and OFA~\cite{OFA-Ali} proposes a unified sequence-to-sequence framework that can handle a variety of vision, language, and multi-modal tasks. Although these works can perform many tasks, the commonality and inner relationship among different tasks are less explored and exploited.





\textit{Unified Model Architectures}. These works~\cite{MaskRCNN,Mask2Former,GLIP,Pix2SeqV2,Unicorn} usually design a unified formulation or model architecture for a group of closely related tasks. For example, Mask R-CNN~\cite{MaskRCNN} proposes a unified network to perform object detection and instance segmentation simultaneously. Mask2Former~\cite{Mask2Former} presents a universal architecture capable of handling panoptic, instance, and semantic segmentation. Pix2SeqV2~\cite{Pix2SeqV2} designs a unified pixel-to-sequence interface for four vision tasks, namely object detection, instance segmentation, keypoint detection, and image captioning. GLIP~\cite{GLIP} cleverly reformulates object detection as phrase grounding by replacing classical classification with word-region alignment. This new formulation allows joint training on both detection and grounding data, showing strong transferability to various object-level recognition tasks. However, GLIP~\cite{GLIP} supports neither prompts in other modalities such as images \& annotations nor video-level tracking tasks. In terms of object tracking, Unicorn~\cite{Unicorn} proposes a unified solution for SOT, VOS, MOT, and MOTS, achieving superior performance on 8 benchmarks with the same model weights. However, it is still difficult for Unicorn to handle diverse label vocabularies~\cite{COCO,MOT17,BDD100K,TAO,VIS,OVIS} during training and inference. In this work, we propose a universal prompt-guided architecture for 10 instance perception tasks, conquering the drawbacks of GLIP~\cite{GLIP} and Unicorn~\cite{Unicorn} simultaneously. 


\section{Approach}
Before introducing detailed methods, we first categorize existing instance perception tasks into three classes. 
\begin{itemize}
    \vspace{-1mm}
    \item{
    Object detection, instance segmentation, MOT, MOTS, and VIS take category names as prompts to find all instances of specific classes.} 
    \vspace{-1mm}
    \item{
    REC, RES, and R-VOS exploit an expression as the prompt to localize a certain target.} 
    \vspace{-1mm}
    \item{
    SOT and VOS use the annotation given in the first frame as the prompt for predicting the trajectories of the tracked target.}
\end{itemize}

Essentially, all the above tasks aim to find objects specified by some prompts. This commonality motivates us to reformulate all instance perception tasks into a prompt-guided object discovery and retrieval problem and solve it by a unified model architecture and learning paradigm. As demonstrated in Figure~\ref{fig-framework}, UNINEXT consists of three main components: (1) prompt generation (2) image-prompt feature fusion (3) object discovery and retrieval. 

\begin{figure*}[!t]
  \begin{center}
\includegraphics[width=1.0\linewidth]{figs/FrameworkV7.pdf}
  \end{center}
  \vspace{-5mm}
\caption{Framework of UNINEXT. The whole pipeline is shown on the left side. The schematic diagram of object retrieval is shown on the right side. The instance head predicts both boxes and masks of the objects. Better view in color on screen.} \label{fig-framework}
\vspace{-3mm}
\end{figure*}

\subsection{Prompt Generation}
First, a prompt generation module is adopted to transform the original diverse prompt inputs into a unified form. According to different modalities, we introduce the corresponding strategies in the next two paragraphs respectively. 

To deal with language-related prompts, a language encoder~\cite{BERT}  is adopted. To be specific, for category-guided tasks, we concatenate class names that appeared in the current dataset~\cite{COCO,VIS,OVIS,BDD100K} as the language expression. Take COCO~\cite{COCO} as an example, the expression can be written as ``person. bicycle. ... . toothbrush". Then for both category-guided and expression-guided tasks, the language expression is passed into , getting a prompt embedding  with a sequence length of . 

For the annotation-guided tasks, to extract fine-grained visual features and fully exploit the target annotations, an additional reference visual encoder  is introduced. Specifically, first a template with  times the target box area is cropped centered on the target location on the reference frame. Then the template is resized to a fixed size of . To introduce more precise target information, an extra channel named the target prior is concatenated to the template image, forming a 4-channel input. In more detail, the value of the target prior is  on the target region otherwise . Then the template image together with the target prior is passed to the reference visual encoder , obtaining a hierarchical feature pyramid . The corresponding spatial sizes are , , , and . To keep fine target information and get the prompt embedding in the same format as other tasks, a merging module is applied. Namely, all levels of features are first upsampled to  then added, and flattened as the final prompt embedding .

The prompt generation process can be formulated as 



\subsection{Image-Prompt Feature Fusion}
In parallel with the prompt generation, the whole current image is passed through another visual encoder , obtaining hierarchical visual features . To enhance the original prompt embedding by the image contexts and to make the original visual features prompt-aware, an early fusion module is adopted. To be specific, first a bi-directional cross-attention module (Bi-XAtt) is used to retrieve information from different inputs, and then the retrieved representations are added to the original features. This process can be formulated as  

Different from GLIP~\cite{GLIP}, which adopts 6 vision-language fusion layers and 6 additional BERT layers for feature enhancement, our early fusion module is much more efficient.

\subsection{Object Discovery and Retrieval}

With discriminative visual and prompt representations, the next crucial step is to transform input features into instances for various perception tasks. UNINEXT adopts the encoder-decoder architecture proposed by Deformable DETR~\cite{DeformableDETR} for its flexible query-to-instance fashion. We introduce the detailed architectures as follows.

The Transformer encoder takes hierarchical prompt-aware visual features as the inputs. With the help of efficient Multi-scale Deformable Self-Attention~\cite{DeformableDETR}, target information from different scales can be fully exchanged, bringing stronger instance features for the subsequent instance decoding. Besides, as performed in two-stage Deformable DETR~\cite{DeformableDETR}, an auxiliary prediction head is appended at the end of the encoder, generating  initial reference points with the highest scores as the inputs of the decoder.

The Transformer decoder takes the enhanced multi-scale features,  reference points from the encoder, as well as  object queries as the inputs. As shown in previous works ~\cite{ReferFormer,Trackformer,VISTR,DINO}, object queries play a critical role in instance perception tasks. In this work, we attempt two query generation strategies: (1) static queries which do not change with images or prompts. (2) dynamic queries conditioned on the prompts. The first strategy can be easily implemented with \texttt{nn.Embedding(N,d)}. The second one can be performed by first pooling the enhanced prompt features  along the sequence dimension, getting a global representation, then repeating it by  times. The above two methods are compared in Sec~\ref{sec:ablation} and we find that static queries usually perform better than dynamic queries. The potential reason could be that static queries contain richer information and possess better training stability than dynamic queries. With the help of the deformable attention, the object queries can efficiently retrieve prompt-aware visual features and learn strong instance embedding .

At the end of the decoder, a group of prediction heads is exploited to obtain the final instance predictions. Specifically, an instance head produces both boxes and masks of the targets. Besides, an embedding head~\cite{IDOL} is introduced for associating the current detected results with previous trajectories in MOT, MOTS, and VIS. Until now, we have mined  potential instance proposals, which are represented with gray masks in Figure~\ref{fig-framework}. However, not all proposals are what the prompts really refer to. Therefore, we need to further retrieve truly matched objects from these proposals according to the prompt embeddings as demonstrated in the right half of Figure~\ref{fig-framework}. Specifically, given the prompt embeddings  after early fusion, for category-guided tasks, we take the embedding of each category name as a weight matrix . Besides, for expression-guided and annotation-guided tasks, the weight matrix  is obtained by aggregating the prompt embedding  using global average pooling (GAP) along the sequence dimension. 



Finally, the instance-prompt matching scores  can be computed as the matrix multiplication of the target features and the transposed weight matrix. . Following previous work~\cite{GLIP}, the matching scores can be supervised by Focal Loss~\cite{RetinaNet}. Different from previous fixed-size classifiers~\cite{DeformableDETR}, the proposed retrieval head selects objects by the prompt-instance matching mechanism. This flexible design enables UNINEXT to jointly train on enormous datasets with diverse label vocabularies from different tasks, learning universal instance representations.





\subsection{Training and Inference}
\label{sub-sec-train-infer}
\textbf{Training.} The whole training process consists of three consecutive stages: (1) general perception pretraining (2) image-level joint training (3) video-level joint training. 

In the first stage, we pretrain UNINEXT on the large-scale object detection dataset Objects365~\cite{Objects365} for learning universal knowledge about objects. Since Objects365 does not have mask annotations, we introduce two auxiliary losses proposed by BoxInst~\cite{BoxInst} for training the mask branch. The loss function can be formulated as 

Then based on the pretrained weights of the first stage, we finetune UNINEXT jointly on image datasets, namely COCO~\cite{COCO} and the mixed dataset of RefCOCO~\cite{RefCOCO&plus}, RefCOCO+~\cite{RefCOCO&plus}, and RefCOCOg~\cite{RefCOCOg-umd}. With manually labeled mask annotations, the traditional loss functions like Dice Loss~\cite{DiceLoss} and Focal Loss~\cite{RetinaNet} can be used for the mask learning. After this step, UNINEXT can achieve superior performance on object detection, instance segmentation, REC, and RES. 

Finally, we further finetune UNINEXT on video-level datasets for various downstream object tracking tasks and benchmarks. In this stage, the model is trained on two frames randomly chosen from the original videos. Besides, to avoid the model forgetting previously learned knowledge on image-level tasks, we also transform image-level datasets to pseudo videos for joint training with other video datasets. In summary, the training data in the third stage includes pseudo videos generated from COCO~\cite{COCO}, RefCOCO/g/+~\cite{RefCOCO&plus,RefCOCOg-umd,RefCOCO&plus}, SOT\&VOS datasets (GOT-10K~\cite{GOT10K}, LaSOT~\cite{LaSOT}, TrackingNet~\cite{trackingnet}, and Youtube-VOS~\cite{YoutubeVOS}), MOT\&VIS datasets (BDD100K~\cite{BDD100K}, VIS19~\cite{VIS}, OVIS~\cite{OVIS}), and R-VOS dataset Ref-Youtube-VOS~\cite{URVOS}. Meanwhile, a reference visual encoder for SOT\&VOS and an extra embedding head for association are introduced and optimized in this period. 

\textbf{Inference.} For category-guided tasks, UNINEXT predicts instances of different categories and associates them with previous trajectories. The association proceeds in an online fashion and is purely based on the learned instance embedding following~\cite{QDTrack,IDOL}. For expression-guided and annotation-guided tasks, we directly pick the object with the highest matching score with the given prompt as the final result. Different from previous works~\cite{ReferFormer,SiamRCNN} restricted by the offline fashion or complex post-processing, our method is simple, online, and post-processing free. 


\section{Experiments}
\subsection{Implementation Details}
We attempt three different backbones, ResNet-50~\cite{ResNet}, ConvNeXt-Large~\cite{ConvNeXt}, and ViT-Huge~\cite{ViT} as the visual encoder. We adopt BERT~\cite{BERT} as the text encoder and its parameters are trained in the first and second training stages while being frozen in the last training stage. The Transformer encoder-decoder architecture follows ~\cite{DeformableDETR} with  encoder layers and  decoder layers. The number of object queries  is set to . The optimizer is AdamW~\cite{AdamW} with weight decay of . The model is trained on  and  A100 GPUs for Objects365 pretraining and other stages respectively. More details can be found in the appendix.










\subsection{Evaluations on 10 Tasks}

We compare UNINEXT with task-specific counterparts in 20 datasets. In each benchmark, the best two results are indicated in \rankfirst{bold} and with \ranksecond{underline}. UNINEXT in all benchmarks uses the same model parameters.

\begin{table}[!t]
\caption{State-of-the-art comparison on object detection.}
\label{tab-od}
\vspace{-3mm}
\begin{center}
{\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|cccccc}
\toprule
Model & Backbone & AP & AP & AP & AP & AP & AP\\
\midrule
Faster R-CNN~\cite{FasterRCNN} & \multirow{7}{*}{ResNet-50} &  &  &  &  &  &  \\
DETR~\cite{DETR} & &  &  &  &  &  &  \\
Sparse R-CNN~\cite{sparsercnn} & &  &  &  &  &  &  \\
Cascade Mask-RCNN~\cite{CascadeRCNN} &  &  &  &  & - & - & - \\
Deformable-DETR~\cite{DeformableDETR}  &  &  &  &  &  &   &  \\
DN-Deformable-DETR~\cite{DN-DETR}  & & \ranksecond{} & \ranksecond{} & \ranksecond{} & \ranksecond{} & \ranksecond{} & \ranksecond{} \\ 
\textbf{UNINEXT} & & \rankfirst{51.3} & \rankfirst{68.4} & \rankfirst{56.2} & \rankfirst{32.6} & \rankfirst{55.7} & \rankfirst{66.5} \\
\hline
HTC++~\cite{HTC} & \multirow{2}{*}{Swin-L} & 58.0 & - & - & - & - & - \\
DyHead~\cite{DyHead} &  & \ranksecond{60.3} & - & - & - & - & - \\
\hline
Cascade Mask R-CNN~\cite{CascadeRCNN} & \multirow{2}{*}{ConvNeXt-L} & 54.8 & 73.8 & 59.8 & - & - & - \\
\textbf{UNINEXT} &  & 58.1 & \ranksecond{74.9} & \ranksecond{63.7} & \ranksecond{40.7} & \ranksecond{62.5} & \ranksecond{73.6} \\
\hline
ViTDet-H~\cite{ViTDet} & \multirow{2}{*}{ViT-H} & 58.7 & - & - & - & - & - \\
\textbf{UNINEXT} &  & \rankfirst{60.6} & \rankfirst{77.5} & \rankfirst{66.7} & \rankfirst{45.1} & \rankfirst{64.8} & \rankfirst{75.3} \\
\bottomrule
\end{tabular}
} }
\end{center}
\end{table}

\begin{table}[!t]
\vspace{-3mm}
\caption{State-of-the-art comparison on instance segmentation. Methods marked with  are evaluated on the  split.}
\label{tab-is}
\vspace{-3mm}
\begin{center}
{\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|cccccc}
\toprule
Model & Backbone & AP & AP & AP & AP & AP & AP\\
\midrule
CondInst~\cite{CondInst} & \multirow{6}{*}{ResNet-50} &  &  &  &  &  &  \\
Cascade Mask R-CNN~\cite{CascadeRCNN} & &  &  &  &  &  &  \\
SOLOv2~\cite{SOLOv2} & &  &  &  &  &  & \ranksecond{} \\
HTC~\cite{HTC} & &  &  &  &  &  &  \\
QueryInst~\cite{QueryInst} &  & \ranksecond{40.6} & \ranksecond{63.0} & \ranksecond{44.0} & \ranksecond{23.4} &  \ranksecond{42.5} & 52.8 \\
\textbf{UNINEXT} & & \rankfirst{44.9} & \rankfirst{67.0} & \rankfirst{48.9} & \rankfirst{26.3} & \rankfirst{48.5} & \rankfirst{59.0} \\
\hline
QueryInst~\cite{QueryInst} & \multirow{2}{*}{Swin-L} & 49.1 & \ranksecond{74.2} & 53.8 & \ranksecond{31.5} & 51.8 & 63.2 \\
Mask2Former~\cite{Mask2Former} &  & 50.1 & - & - & 29.9 & \ranksecond{53.9} & \rankfirst{72.1} \\
\hline
Cascade Mask R-CNN~\cite{CascadeRCNN} & \multirow{2}{*}{ConvNeXt-L} & 47.6 & 71.3 & 51.7 & - & - & - \\
\textbf{UNINEXT} & & 49.6 & 73.4 & \ranksecond{54.3} & 30.4 & 53.6 & 65.7 \\
\hline
ViTDet-H~\cite{ViTDet} & \multirow{2}{*}{ViT-H} & \ranksecond{50.9} & - & - & - & - & - \\
\textbf{UNINEXT} &  & \rankfirst{51.8} & \rankfirst{76.2} & \rankfirst{56.7} & \rankfirst{33.3} & \rankfirst{55.9} & \ranksecond{67.5} \\
\bottomrule
\end{tabular}
} }
\end{center}
\vspace{-5mm}
\end{table}

\textbf{Object Detection and Instance Segmentation.} We compare UNINEXT with state-of-the-art object detection and instance segmentation methods on COCO  ( images) and - split ( images) respectively. As shown in Table~\ref{tab-od}, UNINEXT surpasses state-of-the-art query-based detector DN-Deformable DETR~\cite{DN-DETR} by  box AP. By replacing ResNet-50~\cite{ResNet} with stronger ConvNeXt-Large~\cite{ConvNeXt} and ViT-Huge~\cite{ViT} backbones, UNINEXT achieves a box AP of  and , surpassing competitive rivals Cascade Mask-RCNN~\cite{CascadeRCNN} and ViTDet-H~\cite{ViTDet} by  and  respectively. Besides, the results of instance segmentation are shown in Table~\ref{tab-is}. With the same ResNet-50 backbone, UNINEXT outperforms state-of-the-art QueryInst by  AP and  AP. When using ConvNeXt-Large as the backbone, UNINEXT achieves a mask AP of , surpassing Cascade Mask R-CNN~\cite{CascadeRCNN} by . With ViT-Huge as the backbone, UNINEXT achieves state-of-the-art mask AP of .




\textbf{REC and RES.} RefCOCO~\cite{RefCOCO&plus}, RefCOCO+~\cite{RefCOCO&plus}, and RefCOCOg~\cite{RefCOCOg-g} are three representative benchmarks for REC and RES proposed by different institutions. Following previous literature, we adopt Precision@0.5 and overall IoU (oIoU) as the evaluation metrics for REC and RES respectively and results are rounded to two decimal places. As shown in Table~\ref{tab-rec} and Table~\ref{tab-res}, our method with ResNet-50 backbone surpasses all previous approaches on all splits. Furthermore, when using ConvNeXt-Large and ViT-Huge backbones, UNINEXT obtains new state-of-the-art results, exceeding the previous best method by a large margin. Especially on RES, UNINEXT-H outperforms LAVT~\cite{LAVT} by  on average.

\begin{table}[!t]
\caption{State-of-the-art comparison on REC.}
\label{tab-rec}
\vspace{-3mm}
\begin{center}
{\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccc|ccc|cc}
\hline\noalign{\smallskip}
\multirow{2}{*}{Method}& \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{2}{c}{RefCOCOg}\\
& val & testA & testB & val & testA & testB & val-u & test-u\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
~\cite{Uniter}& 81.41 & 87.04 & 74.17 & 75.90 & 81.45 & 66.70 & 74.86 & 75.77 \\
~\cite{VILLA}& 82.39 & 87.48 & 74.84 & 76.17 & 81.54 & 66.84 & 76.18 & 76.71 \\
MDETR~\cite{MDETR}& 86.75 & 89.58 & 81.41 & 79.52 & 84.09 & 70.62 & 81.64 & 80.89 \\
RefTR~\cite{RefTR}& 85.65 & 88.73 & 81.16 & 77.55 & 82.26 & 68.99 & 79.25 & 80.01 \\
SeqTR~\cite{SeqTR}& 87.00 & 90.15 & 83.59 & 78.69 & 84.51 & 71.87 & 82.69 & 83.37 \\
\textbf{UNINEXT-R50}&89.72&91.52& 86.93 & 79.76 & 85.23 & 72.78 & 83.95 & 84.31 \\
\textbf{UNINEXT-L}&\ranksecond{91.43}& \ranksecond{93.73} & \ranksecond{88.93} & \ranksecond{83.09} & \ranksecond{87.90} & \ranksecond{76.15} & \ranksecond{86.91} &\ranksecond{87.48}\\
\textbf{UNINEXT-H}&\rankfirst{92.64}& \rankfirst{94.33} & \rankfirst{91.46} & \rankfirst{85.24} & \rankfirst{89.63} & \rankfirst{79.79} & \rankfirst{88.73} &\rankfirst{89.37}\\
\hline
\end{tabular}
} }
\end{center}
\end{table}


\begin{table}[!t]
\vspace{-3mm}
\caption{State-of-the-art comparison on RES.}
\label{tab-res}
\vspace{-3mm}
\begin{center}
{\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccc|ccc|cc}
\hline\noalign{\smallskip}
\multirow{2}{*}{Method}& \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{2}{c}{RefCOCOg} \\
& val & testA & testB & val & testA & testB & val-u & test-u \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
CMSA~\cite{CMSA} & 58.32 & 60.61 & 55.09 & 43.76 & 47.60 & 37.89 & - & - \\
BRINet~\cite{BRINet} &60.98 & 62.99 & 59.21 & 48.17 & 52.32 & 42.11 & - & - \\
CMPC+~\cite{CMPC+} & 62.47 & 65.08 & 60.82 & 50.25 & 54.04 & 43.47 & - & - \\
MCN~\cite{MCN} & 62.44 & 64.20 & 59.71 & 50.62 & 54.99 & 44.69 & 49.22 & 49.40 \\
EFN~\cite{EFN} & 62.76 & 65.69 & 59.67 & 51.50 & 55.24 & 43.01 & - & - \\
VLT~\cite{VLT} & 65.65 & 68.29 & 62.73 & 55.50 & 59.20 & 49.36 & 52.99 & 56.65 \\
SeqTR~\cite{SeqTR} &71.70 & 73.31 & 69.82 & 63.04 & 66.73 & 58.97 & 64.69 & 65.74 \\
LAVT~\cite{LAVT} & 72.73 & 75.82 & 68.79 & 62.14 & 68.38 & 55.10 & 61.24 & 62.09 \\
\textbf{UNINEXT-R50} & 77.90 & 79.68 & 75.77 & 66.20 & 71.22 & 59.01 & 70.04 & 70.52 \\
\textbf{UNINEXT-L} & \ranksecond{80.32} & \ranksecond{82.61} & \ranksecond{77.76} & \ranksecond{70.04} & \ranksecond{74.91} & \ranksecond{62.57} & \ranksecond{73.41} & \ranksecond{73.68} \\
\textbf{UNINEXT-H} & \rankfirst{82.19} & \rankfirst{83.44} & \rankfirst{81.33} & \rankfirst{72.47} & \rankfirst{76.42} & \rankfirst{66.22} & \rankfirst{74.67} & \rankfirst{76.37} \\
\noalign{\smallskip}
\hline
\end{tabular}
} }
\end{center}
\vspace{-3mm}
\end{table}



\textbf{SOT.} We compare UNINEXT with state-of-the-art SOT methods on four large-scale benchmarks: LaSOT~\cite{LaSOT}, LaSOT-ext~\cite{lasot_ext}, TrackingNet~\cite{trackingnet}, and TNL-2K~\cite{TNL-2K}. These benchmarks adopt the area under the success curve (AUC), normalized precision (P), and precision (P) as the evaluation metrics and include , , , and  videos in the test set respectively. As shown in Table~\ref{tab-sot}, UNINEXT achieves the best results in terms of AUC and P among all trackers with ResNet-50 backbone. Especially on TNL-2K, UNINEXT outperforms the second best method TransT~\cite{TransT} by  AUC and  P respectively. Besides, UNINEXT with stronger backbones obtains the best AUC on all four benchmarks, exceeding Unicorn~\cite{Unicorn} with the same backbone by  on LaSOT.

\begin{table*}[!t]
\caption{State-of-the-art comparison on SOT.}
\label{tab-sot}
\vspace{-3mm}
\begin{center}
{\begin{center}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|c|ccc|ccc|ccc|cc}
\hline
\multirow{2}{*}{Method} &
  \multirow{2}{*}{Backbone} &
  \multicolumn{3}{c|}{LaSOT~\cite{LaSOT}} &
  \multicolumn{3}{c|}{LaSOT~\cite{lasot_ext}} &
  \multicolumn{3}{c|}{TrackingNet~\cite{trackingnet}} &
  \multicolumn{2}{c}{TNL-2K~\cite{TNL-2K}} \\ \cline{3-13} 
                                      &         & AUC  & P & P    & AUC  & P & P    & AUC  & P & P    & AUC   & P \\ \hline
PrDiMP~\cite{PrDiMP}                   & \multirow{5}{*}{ResNet-50} & 59.8 & 68.8 & 60.8 & -    & -          & -    & 75.8 & 81.6 & 70.4 &  47.0  & 45.9       \\
LTMU~\cite{ltmu}                  &  & 57.2 & -          & 57.2    & 41.4    & \ranksecond{49.9}          & \ranksecond{47.3}    & - & -       & - & 48.5 & 47.3 \\
TransT~\cite{TransT}                    &  & 64.9 & 73.8       & 69.0 & -    & -          & -    & \ranksecond{81.4} & \ranksecond{86.7}       & \ranksecond{80.3} & \ranksecond{50.7} & \ranksecond{51.7}        \\
KeepTrack~\cite{keeptrack}             &  & \ranksecond{67.1} & \rankfirst{77.2}       & \ranksecond{70.2} & \ranksecond{48.2} & -          & -    & -    & -          & -    & -    & -           \\
\textbf{UNINEXT}  & &\rankfirst{69.2}&\ranksecond{77.1}&\rankfirst{75.5}&\rankfirst{51.2}&\rankfirst{58.1}&\rankfirst{58.1}&\rankfirst{83.2}&\rankfirst{86.9}&\rankfirst{83.3}&\rankfirst{56.0}&\rankfirst{57.5}\\
\hline
SimTrack~\cite{SimTrack}  &\multirow{3}{*}{ViT-B}&69.3&78.5&-&-&-&-&82.3&-&\rankfirst{86.5}&54.8&53.8\\
OSTrack~\cite{OSTrack} &  &
  71.1 &
  \rankfirst{81.1} &
  77.6 &
  50.5 &
  61.3 &
  57.6 &
  83.9 &
  88.5 &
  83.2 &
  55.9 &
  - \\ 
SeqTrack~\cite{SeqTrack} & &
71.5 & 
\rankfirst{81.1} &
77.8 &
50.5 &
61.6 &
57.5 &
83.9 &
\ranksecond{88.8} &
83.6 &
57.8 &
- \\
\hline
Unicorn~\cite{Unicorn}  &\multirow{2}{*}{ConvNeXt-L}&68.5&76.6&74.1&-&-&-&83.0&86.4&82.2&-&-\\
\textbf{UNINEXT}  & &\rankfirst{72.4}&\ranksecond{80.7}&\ranksecond{78.9}&\ranksecond{54.4}&\ranksecond{61.8}&\ranksecond{61.4}&\ranksecond{85.1}&88.2&84.7&\ranksecond{58.1}&\ranksecond{60.7}\\
\hline
\textbf{UNINEXT}  & ViT-H&\ranksecond{72.2}&\ranksecond{80.7}&\rankfirst{79.4}&\rankfirst{56.2}&\rankfirst{63.8}&\rankfirst{63.8}&\rankfirst{85.4}&\rankfirst{89.0}&\ranksecond{86.4}&\rankfirst{59.3}&\rankfirst{62.8}\\
\hline
 \end{tabular}}
\end{center}



%
 }
\end{center}
\end{table*}

\textbf{VOS.} The comparisons between UNINEXT with previous semi-supervised VOS methods are demonstrated in Table~\ref{tab:vos}. DAVIS-2017~\cite{DAVIS17} adopts region similarity \mj, contour accuracy \mf, and the averaged score \mjf\ as the metrics. Similarly, Youtube-VOS ~\cite{YoutubeVOS} reports \mj\ and \mf\ for both seen and unseen categories, and the averaged overall score \mg. UNINEXT achieves the best results among all non-memory-based methods, largely bridging the performance gap between non-memory-based approaches and memory-based ones. Furthermore, compared with traditional memory-based methods~\cite{STM, STCN}, UNINEXT does not rely on the intermediate mask predictions. This leads to constant memory consumption, enabling UNINEXT to handle long sequences of any length.  

\begin{table}[!t]
\vspace{-5mm}
\caption{State-of-the-art comparison on VOS.}
\centering{}{\footnotesize{}}{\resizebox{1.0\linewidth}{!}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{llcccccccc}
\toprule 
\multirow{2}{*}{\ \ \ \ }&\multirow{2}{*}{Method} & \multicolumn{5}{c}{YT-VOS 2018 val~\cite{YoutubeVOS}}  & \multicolumn{3}{c}{DAVIS 2017 val~\cite{DAVIS17}} \\
\cmidrule(lr){3-7} \cmidrule(lr){8-10}
& & \mg & \mjs & \mfs & \mju & \mfu & \mjf & \mj & \mf\\

\midrule

\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Memory}}}
&STM~\cite{STM}&79.4&79.7&84.2&72.8&80.9&81.8&79.2&84.3\tabularnewline
&CFBI~\cite{CFBI}&81.4&81.1&85.8&75.3&83.4&81.9&79.1&84.6\tabularnewline
&STCN~\cite{STCN}&83.0&81.9&86.5&77.9&85.7&85.4&82.2&88.6\tabularnewline
&XMem~\cite{XMem}&86.1&85.1&89.8&80.3&89.2&87.7&84.0&91.4\tabularnewline
\midrule

\parbox{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{Non-Memory}}}
&SiamMask~\cite{SiamMask}&52.8&60.2&58.2&45.1&47.7&56.4&54.3&58.5\tabularnewline
&Unicorn~\cite{Unicorn}&-&-&-&-&-&69.2&65.2&73.2\tabularnewline
&Siam R-CNN~\cite{SiamRCNN}&73.2&73.5&-&66.2&-&70.6&66.1&75.0\tabularnewline
&TVOS~\cite{TVOS}&67.8&67.1&69.4&63.0&71.6&72.3&69.9&74.7\tabularnewline
&FRTM~\cite{FRTM}&72.1&72.3&76.2&65.9&74.1&76.7&\ranksecond{73.9}&79.6\tabularnewline
&\textbf{UNINEXT-R50}& 77.0 & 76.8 & 81.0 & \ranksecond{70.8} & \rankfirst{79.4} & 74.5 & 71.3 & 77.6 \tabularnewline
&\textbf{UNINEXT-L}& \ranksecond{78.1} & \ranksecond{79.1} & \ranksecond{83.5} & \rankfirst{71.0} & 78.9 & \ranksecond{77.2} & 73.2 & \ranksecond{81.2} \tabularnewline
&\textbf{UNINEXT-H}& \rankfirst{78.6} & \rankfirst{79.9} & \rankfirst{84.9} & 70.6 & \ranksecond{79.2} & \rankfirst{81.8} & \rankfirst{77.7} & \rankfirst{85.8} \tabularnewline
\bottomrule
\end{tabular}} }
\label{tab:vos}
\vspace{-3mm}
\end{table}

\textbf{MOT.} We compare UNINEXT with state-of-the-art MOT methods on BDD100K~\cite{BDD100K}, which requires tracking  classes of instances in the autonomous driving scenario. Except for classical evaluation metrics Multiple-Object Tracking Accuracy (MOTA), Identity F1 Score (IDF1), and Identity Switches (IDS), BDD100K additionally introduces mMOTA, and mIDF1 to evaluate the average performance across  classes. As shown in Table~\ref{tab:bdd}, UNINEXT surpasses Unicorn~\cite{Unicorn} by  mMOTA and  mIDF1 respectively. 


\textbf{MOTS.} Similar to MOT, BDD100K MOTS Challenge~\cite{BDD100K} evaluates the performance on multi-class tracking by mMOTSA, mMOTSP, mIDF1, and ID Sw. This benchmark contains  sequences with mask annotations in the validation set. As shown in Table~\ref{tab:mots}, UNINEXT achieves state-of-the-art performance, surpassing the previous best method Unicorn~\cite{Unicorn} by  mMOTSA.

\textbf{VIS.} We compare UNINEXT against state-of-the-art VIS methods on Youtube-VIS ~\cite{VIS} and OVIS~\cite{OVIS} validation sets. Specifically, Youtube-VIS  and OVIS have  and  object categories, containing  and  videos respectively in the validation set. Both benchmarks take AP as the main metric. As shown in Table~\ref{tab:vis}, when using the same ResNet-50 backbone,  
UNINEXT obtains the best results on both datasets. Especially on more challenging OVIS, UNINEXT exceeds the previous best method IDOL~\cite{IDOL} by  AP. When using stronger ViT-Huge backbone, UNINEXT achieves state-of-the-art AP of  on Youtube-VIS  and  on OVIS respectively, surpassing previous methods by a large margin. 



\begin{table}[t]
\vspace{-5mm}
    \caption{State-of-the-art comparison on MOT.}
    \centering
    {    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule
            Method                  & Split & mMOTA & mIDF1 & MOTA & IDF1 & ID Sw. \\
            \midrule
            Yu~\etal~\cite{BDD100K} & val   & 25.9             & 44.5             & 56.9            & 66.8          & \ranksecond{8315}           \\
            QDTrack~\cite{QDTrack}                    & val   & 36.6    & 50.8    & 63.5   & \rankfirst{71.5}   & \rankfirst{6262} \\
Unicorn~\cite{Unicorn} & val &41.2&54.0&\ranksecond{66.6}&\ranksecond{71.3}&10876\\
            \textbf{UNINEXT-L} & val &\ranksecond{41.8}&\ranksecond{54.9}&64.6&68.7&9134\\
            \textbf{UNINEXT-H} & val &\rankfirst{44.2}&\rankfirst{56.7}&\rankfirst{67.1}&69.9&10222\\
\bottomrule
        \end{tabular}
    }

%
 }
    \label{tab:bdd}
\end{table}

\begin{table}[t]
    \caption{State-of-the-art comparison on MOTS.}
    \centering
    {    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule
            Method & Online & mMOTSA & mMOTSP & mIDF1 & ID Sw. \\
            \midrule
            SortIoU&\ding{51}&10.3&59.9&21.8&15951\\
            MaskTrackRCNN~\cite{VIS}&\ding{51}&12.3&59.9&26.2&9116\\
            STEm-Seg~\cite{stemseg}&\ding{55}&12.2&58.2&25.4&8732\\
            QDTrack-mots~\cite{QDTrack}&\ding{51}&22.5&59.6&40.8&1340\\
PCAN~\cite{PCAN}&\ding{51}&27.4&66.7&45.1&\ranksecond{876}\\
            VMT~\cite{VMT}&\ding{55}&28.7&67.3&\ranksecond{45.7}&\rankfirst{825}\\
            Unicorn~\cite{Unicorn}&\ding{51}&29.6&\ranksecond{67.7}&44.2&1731\\
\textbf{UNINEXT-L}&\ding{51}&\ranksecond{32.0}&60.2&45.4&1634\\
            \textbf{UNINEXT-H}&\ding{51}&\rankfirst{35.7}&\rankfirst{68.1}&\rankfirst{48.5}&1776\\
            \bottomrule
        \end{tabular}
    } }
    \label{tab:mots}
\vspace{-3mm}
\end{table}

\textbf{R-VOS.} Ref-Youtube-VOS~\cite{URVOS} and Ref-DAVIS17~\cite{ref-davis} are two popular R-VOS benchmarks, which are constructed by introducing language expressions for the objects in the original Youtube-VOS~\cite{YoutubeVOS} and DAVIS17~\cite{DAVIS17} datasets. As same as semi-supervised VOS, region similarity \mj , contour accuracy \mf, and the averaged score \mjf\ are adopted as the metrics. As demonstrated in Table~\ref{tab:rvos}, UNINEXT outperforms all previous R-VOS approaches by a large margin, when using the same ResNet-50 backbone. Especially on Ref-DAVIS17, UNINEXT exceeds previous best ReferFormer~\cite{ReferFormer} by  \mjf. Furthermore, when adopting stronger ViT-Huge backbone, UNINEXT achieves new state-of-the-art \mjf of  on Ref-Youtube-VOS and  on Ref-DAVIS17. Besides, different from offline RefFormer, UNINEXT works in a flexible online fashion, making it applicable to ongoing videos in the real world.


\begin{table}[t]
    \caption{State-of-the-art comparison on VIS.}
    \centering
    {\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{l|c|c|ccc|ccc}
\hline
\multirow{2}{*}{Method}   & \multirow{2}{*}{Backbone} &\multirow{2}{*}{Online} &\multicolumn{3}{c}{VIS2019 val}&\multicolumn{3}{c}{OVIS val} \\
\arrayrulecolor{white}\cline{4-9}
\arrayrulecolor{black}\cline{4-9}
\arrayrulecolor{black}\cline{4-9}
\arrayrulecolor{black}\cline{4-9}
\arrayrulecolor{white}\cline{4-9}
&&&    &  & &    &  &\\
\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline


VisTR~\cite{VISTR}   & \multirow{7}{*}{ResNet-50}      &\ding{55} &36.2 &59.8  &36.9 & - & - & -\\
MaskProp~\cite{MaskProp}  &    &\ding{55} &40.0 &-&42.9 & - & - & -\\  
IFC~\cite{IFC}  &   &\ding{55} &42.8 &65.8 &46.8&13.1&27.8&11.6 \\  
{SeqFormer}~\cite{SeqFormer}   &    &\ding{55} &47.4 &69.8 &51.8&15.1&31.9&13.8\\
IDOL~\cite{IDOL}  &  &\ding{51} & 49.5 &\ranksecond{74.0} &52.9&\ranksecond{30.2}&\ranksecond{51.3}&\ranksecond{30.0}\\
 VITA~\cite{VITA}&&\ding{55}&\ranksecond{49.8}&72.6&\ranksecond{54.5}&19.6&41.2&17.4\\
\textbf{UNINEXT}  &  &\ding{51} &\rankfirst{53.0}& \rankfirst{75.2}& \rankfirst{59.1} & \rankfirst{34.0} & \rankfirst{55.5} & \rankfirst{35.6}\\  

\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline
 
  SeqFormer~\cite{SeqFormer}  &  \multirow{4}{*}{Swin-L} &\ding{55} &{59.3}  &{82.1}  &{66.4} & - & - & -\\ 
  VMT~\cite{VMT}&&\ding{55}&59.7&-&66.7&19.8&39.6&17.2\\
VITA~\cite{VITA}&&\ding{55}&63.0&86.9&67.9&-&-&-\\
   IDOL~\cite{IDOL}  &  &\ding{51} &\ranksecond{64.3} &\rankfirst{87.5} &71.0&\ranksecond{42.6}&65.7&\ranksecond{45.2}\\  
\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline
\textbf{UNINEXT}  & ConvNeXt-L &\ding{51} &\ranksecond{64.3}&\ranksecond{87.2}&\ranksecond{71.7}&41.1&\ranksecond{65.8}&42.0\\  
   \textbf{UNINEXT}  & ViT-H &\ding{51} &\rankfirst{66.9}&\rankfirst{87.5}&\rankfirst{75.1}&\rankfirst{49.0}&\rankfirst{72.5}&\rankfirst{52.2}\\  
\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline
\end{tabular}
} }
    \label{tab:vis}
\end{table}

\begin{table}[t]
    \caption{State-of-the-art comparison on R-VOS.}
    \centering
    {\resizebox{1.0\linewidth}{!}{
\begin{tabular}{l | c | c c c | c c c}

\toprule

\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multicolumn{3}{c}{Ref-Youtube-VOS} & \multicolumn{3}{c}{Ref-DAVIS17} \\

\arrayrulecolor{white}\cline{3-8}
\arrayrulecolor{black}\cline{3-8}
\arrayrulecolor{black}\cline{3-8}
\arrayrulecolor{black}\cline{3-8}
\arrayrulecolor{white}\cline{3-8}

 & &  &  &  & 
      &  &  \\

\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline

CMSA~\cite{CMSA}  &\multirow{5}{*}{ResNet-50}& 36.4 & 34.8 & 38.1 & 40.2 & 36.9 & 43.5 \\ 
URVOS~\cite{URVOS} &  & 47.2 & 45.3 & 49.2 & 51.5 & 47.3 & 56.0 \\
YOFO~\cite{YOFO}& &48.6&47.5&49.7&54.4&50.1&58.7\\
ReferFormer~\cite{ReferFormer} &  & \ranksecond{58.7} & \ranksecond{57.4} & \ranksecond{60.1} & \ranksecond{58.5} & \ranksecond{55.8} & \ranksecond{61.3} \\
\textbf{UNINEXT}& & \rankfirst{61.2} & \rankfirst{59.3}& \rankfirst{63.0} & \rankfirst{63.9} & \rankfirst{59.6} & \rankfirst{68.1} \\

\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline

PMINet + CFBI ~\cite{PMINet} & \multirow{2}{*}{Ensemble} & 54.2 & 53.0 & 55.5 & - & - & - \\
CITD ~\cite{CITD} &  & 61.4 & 60.0 & 62.7 & - & - & - \\
\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline
MTTR~\cite{MTTR} & \multirow{2}{*}{Video-Swin-T} & 55.3 & 54.0 & 56.6 & - & - & - \\
ReferFormer~\cite{ReferFormer} &  & 64.9 & 62.8 & 67.0 & 61.1 & 58.1 & 64.1 \\
\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline
\textbf{UNINEXT}&ConvNext-L&\ranksecond{66.2}&\ranksecond{64.0}&\ranksecond{68.4}&\ranksecond{66.7}&\ranksecond{62.3}&\ranksecond{71.1}\\
\textbf{UNINEXT}&ViT-H&\rankfirst{70.1} & \rankfirst{67.6} & \rankfirst{72.7}&\rankfirst{72.5}&\rankfirst{68.2}&\rankfirst{76.8}\\
\arrayrulecolor{white}\hline
\arrayrulecolor{black}\hline
\arrayrulecolor{white}\hline



\end{tabular}
} }
    \label{tab:rvos}
\vspace{-3mm}
\end{table}


\subsection{Ablations and Other Analysis}
\label{sec:ablation}

In this section, we conduct component-wise analysis for better understanding our method. All models take ResNet-50 as the backbone. The methods are evaluated on five benchmarks (COCO~\cite{COCO}, RefCOCO~\cite{RefCOCO&plus}, Youtube-VOS~\cite{YoutubeVOS}, Ref-Youtube-VOS~\cite{URVOS}, and Youtube-VIS ~\cite{VIS}) from five tasks (object detection, REC, VOS, R-VOS, and VIS). The results are shown in Table~\ref{tab:ablation}. 

\textbf{Fusion.} To study the effect of feature fusion between visual features and prompt embeddings, we implement a variant without any early fusion. In this version, prompt embeddings do not have an influence on proposal generation but are only used in the final object retrieval process. Experiments show that early fusion has the greatest impact on VOS, the performance on VOS drops drastically by  \mjf\ without feature fusion. This is mainly caused by the following reasons (1) Without the guidance of prompt embeddings, the network can hardly find rare referred targets like trees and sinks. (2) Without early fusion, the network cannot fully exploit fine mask annotations in the first frame, causing degradation of the mask quality. Besides, the removal of feature fusion also causes performance drop of  P@0.5 and  \mjf on REC and RVOS respectively, showing the importance of early fusion in expression-guided tasks. Finally, feature fusion has minimum influence on object detection and VIS. This can be understood because both two tasks aim to find all objects as completely as possible rather than locating one specific target referred by the prompt. 

\textbf{Queries.} We compare two different query generation strategies: static queries by \texttt{nn.Embedding(N, d)} and dynamic queries conditioned on the prompt embeddings. Experiments show that dynamic queries perform slightly better than static queries on the first four tasks. However, static queries outperform dynamic ones by 2.8 AP on the VIS task, obtaining higher overall performance. A potential reason is that  different object queries can encode richer inner relationship among different targets than simply copying the pooled prompt by  times as queries. This is especially important for VIS because targets need to be associated according to their affinity in appearance and space.


\textbf{Unification.} We also compare two different model design philosophies, one unified model or multiple task-specific models. Except for the unified model, we also retrain five task-specific models only on data from corresponding tasks. Experiments show that the unified model achieves significantly better performance than its task-specific counterparts on five tasks, demonstrating the superiority of the unified formulation and joint training on all instance perception tasks. 
Finally, the unified model can save tons of parameters, being much more parameter-efficient. 

\begin{table}[t]
    \caption{Ablations. The settings in our final model is underlined.}
    \centering
    {\resizebox{1.0\linewidth}{!}{
\begin{tabular}{clccccc}
\toprule
\multirow{3}{*}{Experiment} & \multirow{3}{*}{Method} & \multicolumn{1}{c}{\underline{OD}} & 
\multicolumn{1}{c}{\underline{REC}} & 
\multicolumn{1}{c}{\underline{VOS}} & 
\multicolumn{1}{c}{\underline{RVOS}} & 
\multicolumn{1}{c}{\underline{VIS}} \\
& &COCO&RefCOCO&YTBVOS&R-YTBVOS&VIS19  \\
& &(AP)&(P@0.5)&()&()&(AP)\\ 
\midrule 
\midrule
\multirow{3}{*}{Fusion}  & \underline{Early Fusion} &51.3&89.7&77.0&61.2&53.0  \\
                            & W/o Fusion&51.1&87.4& 55.6 & 58.4 &51.0 \\ 
                            & & (+0.2) & (+2.3) & (+21.4) & (+2.8) & (+2.0) \\ \midrule
\multirow{3}{*}{Queries}      
    &\underline{Static}&51.3&89.7&77.0&61.2&53.0  \\
    & Dynamic&51.9&89.8&77.4&61.6&50.2 \\ 
    & & (-0.6) & (-0.1) & (-0.4) & (-0.4) & (+2.8) \\
                            \midrule
\multirow{3}{*}{Model}    
&\underline{Unified}&51.3&89.7&77.0&61.2&53.0  \\
&Task-specific& 50.8 & 87.6 & 74.2 & 57.2 & 50.1 \\
& & (+0.5) & (+2.1) & (+2.8) & (+4.0) & (+2.9) \\
\midrule
\end{tabular}
} }
    \label{tab:ablation}
    \vspace{-3mm}
\end{table}

\section{Conclusions}
We propose UNINEXT, a universal instance perception model of the next generation. For the first time, UNINEXT unifies 10 instance perception tasks with a prompt-guided object discovery and retrieval paradigm. Extensive experiments demonstrate that UNINEXT achieves superior performance on 20 challenging benchmarks with a single model with the same model parameters. We hope that UNINEXT can serve as a solid baseline for the research of instance perception in the future.

\noindent{\textbf{Acknowledgement.}
We would like to thank the reviewers for their insightful comments. The paper is supported in part by the National Key R\&D Program of China under Grant No. 2018AAA0102001, 2022ZD0161000 and National Natural Science Foundation of China under grant No. 62293542, U1903215, 62022021 and the Fundamental Research Funds for the Central Universities No.DUT22ZD210.}
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix

\section{Appendix}

\begin{table*}[t]
    \caption{Details in training. Step is the time to reduce the learning rate.}
    \centering
    {\resizebox{1.0\linewidth}{!}{
\begin{tabular}{c|c|ccccc|cccc}

\toprule
Stage & Task & Dataset & Sampling Weight & Batch Size & Short & Long & Num GPU & Lr & Max Iter & Step \\

\midrule
\multirow{1}{*} {\uppercase\expandafter{\romannumeral1}}& \multirow{1}{*} {OD\&IS} & Objects365~\cite{Objects365} & 1 &  2 &  & 1333 & 32 & 0.0002 & 340741 & 312346 \\
\midrule
\multirow{2}{*}{\uppercase\expandafter{\romannumeral2}} & OD\&IS & COCO~\cite{COCO} & 1 & 2 &  & 1333 & \multirow{2}{*}{16} & \multirow{2}{*}{0.0002} & \multirow{2}{*}{91990} & \multirow{2}{*}{76658} \\
& REC\&RES & RefCOCO/g/+~\cite{RefCOCO&plus,RefCOCOg-umd} & 1 & 2 &  & 1333 & & & & \\
\midrule
\multirow{14}{*}{\uppercase\expandafter{\romannumeral3}} & \multirow{5}{*}{SOT\&VOS} & LaSOT~\cite{LaSOT} & 0.20 & 2 &  & 1333 & \multirow{14}{*}{16} & \multirow{14}{*}{0.0001} & \multirow{14}{*}{180000} & \multirow{14}{*}{150000} \\
& & GOT10K~\cite{GOT10K} & 0.20 & 2 &  & 1333 & &&&\\
& & TrackingNet~\cite{trackingnet} & 0.20 & 2 &  & 1333 & &&&\\
& & Youtube-VOS~\cite{YoutubeVOS} & 0.20 & 2 &  & 768 &&&&\\
& & COCO~\cite{COCO} & 0.20 & 2 &  & 1333 & &&&\\
\cline{2-7}
& \multirow{4}{*}{MOT\&MOTS} & BDD-obj-det~\cite{BDD100K} & 0.18 & 2 &  & 1333  &&&&\\
& & BDD-box-track~\cite{BDD100K} & 0.72 & 2 &  & 1333  &&&&\\
& & BDD-inst-seg~\cite{BDD100K} & 0.02 & 2 &  & 1333  &&&&\\
& & BDD-seg-track~\cite{BDD100K} & 0.08 & 2 &  & 1333  &&&&\\
\cline{2-7}
& \multirow{3}{*}{VIS} & Youtube-VIS-19~\cite{VIS} & 0.34 & 4 &  & 768  &&&&\\
& & OVIS~\cite{OVIS} & 0.17 & 2 &  & 1333  &&&&\\
& & COCO~\cite{COCO} & 0.51 & 2 &  & 1333  &&&&\\
\cline{2-7}
& \multirow{2}{*}{R-VOS} & Ref-Youtube-VOS~\cite{URVOS} & 0.33 & 2 &  & 768  &&&&\\
& & RefCOCO/g/+~\cite{RefCOCO&plus,RefCOCOg-umd} & 0.67 & 2 &  & 1333  &&&&\\
\bottomrule
\end{tabular}}
    
%
 }
    \label{tab:detail}
\end{table*}

In this appendix, we present more details about the training process and loss functions in ~\ref{sec-train-detail} and ~\ref{loss-func}, network architecture in ~\ref{sec-network}, as well as more analysis and visualizations for better understanding in ~\ref{sec-qualitative}.

\subsection{Training Process}
\label{sec-train-detail}
The detailed hyperparameters during training are shown in Tab~\ref{tab:detail}. The whole training process consists of three stages. In each stage, the \texttt{StepLR} learning rate scheduler is adopted. The learning rate drops by a factor of 10 after the given steps. For multi-dataset training, we follow the implementation of Detic~\cite{Detic}, which randomly samples data from different tasks and then computes them on different GPUs in one iteration. Besides, the multi-scale training technique is used across all datasets in all stages. Take the pre-training on Objects365~\cite{Objects365} as an example, the original images are resized such that the shortest side is at least 480 and at most 800 pixels while the longest side is at most 1333. We use this as the default setting except on Youtube-VOS~\cite{YoutubeVOS}, Youtube-VIS-2019~\cite{VIS}, and Ref-Youtube-VOS~\cite{URVOS}. A lower resolution with the shortest side ranging from 320 to 640 and the longest side not exceeding 768 is applied to these datasets~\cite{YoutubeVOS,VIS,URVOS}, following previous works~\cite{IDOL,ReferFormer,STCN}.

Specifically, in the first stage, the model is pretrained on Objects365~\cite{Objects365} for about 340K iterations (12 epochs) and the learning rate drops on the 11th epoch. In the second stage, we finetune UNINEXT on COCO~\cite{COCO} and RefCOCO/g/+~\cite{RefCOCO&plus,RefCOCOg-umd} jointly for 12 epochs. In the third stage, UNINEXT is further finetuned for diverse video-level tasks. To guarantee balanced performance on various benchmarks, we set the data sampling ratios as (SOT\&VOS):(MOT\&MOTS):VIS:R-VOS = 1:1:1:1. For each task, 45K iterations are allocated, thus bringing 180K iterations in total for the third stage. Besides, to avoid forgetting previously learned knowledge on image-level tasks, we also generate pseudo videos from COCO~\cite{COCO} and RefCOCO/g/+~\cite{RefCOCO&plus, RefCOCOg-umd} and mix them with training data of VIS~\cite{VIS,OVIS} and R-VOS~\cite{URVOS} respectively.

\subsection{Loss Functions}
\label{loss-func}
We present detailed loss functions described in Sec.~\ref{sub-sec-train-infer} for better readability. First,  and  are used across all three stages. Second, to learn mask representations from coarse boxes~\cite{Objects365} and fine mask annotations~\cite{COCO,RefCOCO&plus,YoutubeVOS,VIS,URVOS}, UNINEXT uses  in the first stage and  in the next two stages respectively. Finally, to associate instances on different frames~\cite{BDD100K,VIS,OVIS}, UNINEXT additionally adopts  in the last stage.


\underline{\bm{}}. Given the raw instance-prompt matching score , the normalized matching probability  is computed as , where  is sigmoid function. Then  can be written as the form of Focal loss~\cite{RetinaNet}.
 \eqnnm{flalpha}{\mathcal{L}_\mathrm{retrieve}(\pt) = - \at (1 - \pt)^\gamma \log (\pt).}
\eqnnm{pt}{\pt=\begin{cases} p &\text{if matched}\\ 1 - p &\text{otherwise.}\end{cases}}
 and  are 2 and 0.25 respectively.

\underline{\bm{}}. Following DETR-like methods~\cite{DETR,DeformableDETR},  consists of two terms, GIoU Loss~\cite{GIoULoss} and  loss:
 \eqnnm{loss_box}{\mathcal{L}_\mathrm{box}(b,\hat{b}) = \lambda_{giou}\mathcal{L}_\mathrm{giou}(b,\hat{b})+\lambda_{L_1}\Vert b-\hat{b} \Vert.}
\eqnnm{loss_giou}{\mathcal{L}_\mathrm{giou}(b,\hat{b})=1-IoU(b,\hat{b})+\frac{A^c(b,\hat{b})-U(b,\hat{b})}{A^c(b,\hat{b})},}
where  is the area of the smallest box containing  and .  is the area of the union of  and .

\underline{\bm{}}. For datasets with mask annotations~\cite{COCO,RefCOCO&plus,YoutubeVOS,VIS,URVOS}, Focal Loss~\cite{RetinaNet} and Dice Loss~\cite{DiceLoss} are adopted. 
\eqnnm{loss_mask}{\mathcal{L}_\mathrm{mask}(m,\hat{m}) = \lambda_{focal}\mathcal{L}_\mathrm{focal}(m,\hat{m})+\lambda_{dice}\mathcal{L}_\mathrm{dice}(m,\hat{m}).}
\eqnnm{loss_dice}{\mathcal{L}_\mathrm{dice}(m,\hat{m})=1-\frac{2m\hat{m}+1}{\hat{m}+m+1},}
where  and  are binary GT masks and predicted masks after sigmoid activation respectively.

\underline{\bm{}}. For Objects365~\cite{Objects365} without mask annotations, UNINEXT uses Projection Loss and Pairwise Affinity Loss like BoxInst~\cite{BoxInst}, which can learn mask prediction only based on box-level annotations.
\eqnnm{loss_boxinst}{\mathcal{L}^\mathrm{boxinst}_\mathrm{mask}(b,\hat{m}) = \mathcal{L}_\mathrm{proj}(b,\hat{m})+\mathcal{L}_\mathrm{pairwise}(b,\hat{m}).}
\eqnnm{loss_proj}{
\begin{split}
\mathcal{L}_\mathrm{proj}(b,\hat{m})=&\mathcal{L}_\mathrm{dice}(\mathrm{proj_x}(b),\mathrm{proj_x}(\hat{m}))+\\
&\mathcal{L}_\mathrm{dice}(\mathrm{proj_y}(b),\mathrm{proj_y}(\hat{m})).
\end{split}
}
\eqnnm{loss_pairwise}{\mathcal{L}_\mathrm{pairwise} = -\frac{1}{N}\sum_{e \in E_{in}}\mathbbm{1}_{\{S_e \geq \tau\}}\log P(y_e = 1).}
\eqnnm{y_e}{
     P(y_e = 1) = \hat{m}_{i, j}
     \cdot
     \hat{m}_{k, l} + (1 - \hat{m}_{i, j})
\cdot
     (1 - \hat{m}_{k, l}).}
\eqnnm{S_e}{S_e = S(c_{i, j}, c_{l, k}) = \exp\left(-\frac{||c_{i, j} - c_{l, k}||}{\theta}\right),}
where  means the two pixels have the same ground-truth label.  is the color similarity of the edge e.  and  are respectively the LAB color vectors of the two pixels  and  linked by the edge.  is 2 in this work.

\underline{\bm{}}. UNINEXT uses contrastive loss~\cite{IDOL} to train discriminative embeddings for associating instances on different frames.
\eqnnm{loss_embed}{\mathcal{L}_\mathrm{embed} = \log [1+\sum_{\mathbf{k}^+}\sum_{\mathbf{k}^-}\exp(\mathbf{v} \cdot \mathbf{k}^-  - \mathbf{v} \cdot \mathbf{k}^+) ],}
where  and  are positive and negative feature embeddings from the reference frame. For each instance in the key frame,  is the feature embedding with the lowest cost.

\begin{figure}[!t]
  \begin{center}
\includegraphics[width=1.0\linewidth]{figs/Radar_v3.pdf}
  \end{center}
  \vspace{-5mm}
\caption{Better view in color on screen.} \label{fig-radar}
\vspace{-3mm}
\end{figure}


\begin{figure*}[!t]
  \begin{center}
\includegraphics[width=1.0\linewidth]{figs/Det-Visual.pdf}
  \end{center}
  \vspace{-5mm}
\caption{Illustration of \textbf{retrieval by category names}. UNINEXT can flexibly perceive objects of different categories by changing the input prompts. Better view in color on screen.} 
  \label{fig-det-visual}
\vspace{-3mm}
\end{figure*}

\subsection{Network Architecture}
\label{sec-network}
To transform the enhanced visual features  and prompt features  into the final instance predictions, an encoder-decoder Transformer architecture is adopted. Based on the original architecture in two-stage Deformable DETR~\cite{DeformableDETR}, UNINEXT makes the following improvements:
\begin{itemize}
\item{\textbf{Introducing a mask head for segmentation.}} To predict high-quality masks, UNINEXT introduces a mask head~\cite{CondInst} based on dynamic convolutions. Specifically, first an MLP is used to transform instance embeddings into a group of parameters . Then these parameters are used to perform three-layer  convolutions with feature maps, obtaining masks of instances.
\item{\textbf{Replacing one-to-one Hungarian matching with one-to-many SimOTA~\cite{YOLOX}.}} Traditional Hungarian matching forces one GT to be only assigned to one query, leaving most of the queries negative. UNINEXT uses SimOTA~\cite{YOLOX}, which enables multiple queries to be matched with one GT. This strategy can provide more positive samples and speed up convergence. During inference, UNINEXT uses NMS to remove duplicated predictions. 
\item{\textbf{Adding an IoU branch.}} UNINEXT adds an IoU branch to reflect the quality of the predicted boxes. During training, IoU does not affect the label assignment. During inference, the final scores are the geometric mean of the instance-prompt matching scores (after sigmoid) and the IoU scores.
\item{\textbf{Adding some techniques in DINO~\cite{DINO}.}} To further improve the performance, UNINEXT introduces some techniques~\cite{DINO}, including contrastive DN, mixed query selection, and look forward twice. 
\end{itemize}



\subsection{Analysis and Visualizations}
\label{sec-qualitative}

\textbf{Analysis}. We compare UNINEXT with other competitive counterparts, which can handle multiple instance-level perception tasks. The opponents include Cascade Mask R-CNN~\cite{CascadeRCNN} for object detection and instance segmentation, SeqTR~\cite{SeqTR} for REC and RES, VMT~\cite{VMT} for MOTS and VIS, and Unicorn~\cite{Unicorn} for SOT, VOS, MOT, and MOTS. As shown in Figure~\ref{fig-radar}, UNINEXT outperforms them and achieve state-of-the-art performance on all 10 tasks. 

\textbf{Retrieval by Category Names}. As shown in Figure~\ref{fig-det-visual}, UNINEXT can flexibly detect and segment objects of different categories by taking the corresponding category names as the prompts. For example, when taking ``dining table. wine glass. cake. knife'' as the prompts, UNINEXT would only perceive dining tables, wine glasses, cakes, and knives. Furthermore, benefiting from the flexible retrieval formulation, UNINEXT also has the potential for zero-shot (open-vocabulary) object detection. However, open-vocabulary object detection is beyond the scope of our paper and we leave it for future works.

\textbf{Retrieval by Language Expressions}. We provide some visualizations for retrieval by language expressions in Figure~\ref{fig-rec-res}. UNINEXT can accurately locate the target referred by the given language expression when there are many similar distractors. This demonstrates that our method can not only perceive objects but also understand their relationships in positions (left, middle, right, etc) and sizes (taller, etc). 

\textbf{Retrieval by Target Annotations}. Our method supports annotations in formats of both boxes (SOT) and masks (VOS). Although there is only box-level annotation for SOT, we obtain the target prior by filling the region within the given box with  and leaving other regions as . As shown in Figure~\ref{fig-vos}, UNINEXT can precisely track and segment the targets in complex scenarios, given the annotation in the first frame.


\begin{figure*}[!t]
  \begin{center}
\includegraphics[width=1.0\linewidth]{figs/REC_RES.pdf}
  \end{center}
  \vspace{-5mm}
  \caption{Illustration of \textbf{retrieval by language expressions}. Better view in color on screen.}
  \label{fig-rec-res}
\vspace{-3mm}
\end{figure*}

\begin{figure*}[!t]
  \begin{center}
\includegraphics[width=0.9\linewidth]{figs/SOT_VOS.pdf}
  \end{center}
  \vspace{-5mm}
  \caption{Illustration of \textbf{retrieval by target annotations}. UNINEXT can flexibly perceive different objects according to the box or mask annotations given in the first frame. Better view in color on screen.} \label{fig-vos}
\vspace{-3mm}
\end{figure*}



\end{document}
