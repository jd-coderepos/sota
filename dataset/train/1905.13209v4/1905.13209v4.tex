
\documentclass{article} \usepackage{iclr2020_conference,times}
\usepackage{graphicx}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{epsfig}
\usepackage{graphicx}
\usepackage[margin=0pt]{caption}

\addtolength{\floatsep}{-2mm}
\addtolength{\textfloatsep}{-1mm}
\renewcommand{\baselinestretch}{.99}


\title{AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures}



\author{Michael S. Ryoo, AJ Piergiovanni, Mingxing Tan \& Anelia Angelova\\
Robotics at Google\\
Google Research\\
\texttt{\{mryoo,ajpiergi,tanmingxing,anelia\}@google.com}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle


\begin{abstract}
Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding.
This is done by evolving a population of overly-connected architectures guided by connection weight learning. 
Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6\% mAP on Charades and 34.27\% accuracy on Moments-in-Time.
\end{abstract}


\section{Introduction}


Learning to represent videos is a challenging problem. Because a video contains spatio-temporal data, its representation is required to abstract both appearance and motion information. This is particularly important for tasks such as activity recognition, as understanding detailed semantic contents of the video is needed. Previously, researchers approached this challenge by designing a two-stream model for appearance and motion information respectively, combining them by late or intermediate fusion to obtain successful results: \cite{simonyan2014two,feichtenhofer2016convolutional,feichtenhofer2016tsres,feichtenhofer2017tsmult,feichtenhofer2018slowfast}. However, combining appearance and motion information is an open problem and 
the study on how and where different modalities should interchange representations and what temporal aspect/resolution each stream (or module) should focus on has been very limited.




In this paper, we investigate how to learn feature representations across spatial and motion visual clues. We propose a new multi-stream neural architecture search algorithm with \emph{connection learning guided evolution}, which focuses on finding higher-level connectivity between network blocks taking multiple input streams at different temporal resolutions. Each block itself is composed of multiple residual modules with space-time convolutional layers, learning spatio-temporal representations. 
Our architecture learning not only considers the connectivity between such multi-stream, multi-resolution blocks, but also merges and splits network blocks to find better multi-stream video CNN architectures. Our objective is to address two main questions in video representation learning: (1) what feature representations are needed at each intermediate stage of the network and at which resolution and (2) how to combine or exchange such intermediate representations (i.e., connectivity learning). 
Unlike previous neural architecture search methods for images that focus on finding a good `module' of convolutional layers to be repeated in a single-stream networks (\citealp{zoph2018nas,real2019amoeba}), our objective is to search for higher-level connections between multiple sequential or concurrent blocks to form multi-stream architectures.













We propose the concept of AssembleNet, a new method of fusing different sub-networks with different input modalities and temporal resolutions. AssembleNet is a general formulation that allows representing various forms of multi-stream CNNs as directed graphs, coupled with an efficient evolutionary algorithm to explore the network connectivity. Specifically, this is done by utilizing the learned connection weights to guide  evolution, in addition to randomly combining, splitting, or connecting sub-network blocks. 
AssembleNet is a `family' of learnable architectures; they provide a generic approach to learn connectivity among feature representations across input modalities, while being optimized for the target task.
We believe this is the first work to (i) conduct research on automated architecture search with multi-stream connections for video understanding, and (ii) introduce the new connection-learning-guided evolutionary algorithm for neural architecture search. 


Figure~\ref{fig:assemblenet} shows an example learned AssembleNet.
The proposed algorithm for learning video architectures is very effective: it outperforms all prior work and baselines on two very challenging benchmark datasets, and establishes a new state-of-the-art. AssembleNet models use equivalent number of parameters to standard two-stream (2+1)D ResNet models.



\begin{figure}
  \centering
\includegraphics[width=0.55\linewidth]{images/AssembleNet-full-re.pdf}\caption{AssembleNet with multiple intermediate streams. Example learned architecture. Darker colors of connections indicate stronger connections. At each convolutional block, multiple 2D and (2+1)D residual modules are repeated alternatingly. Our network has 4 block levels (+ the stem level connected to raw data). Each convolutional block has its own output channel size (i.e., the number of filters)  and the temporal resolution  controlling the 1D temporal convolutional layers in it.
}
  \label{fig:assemblenet}
\end{figure}




\section{Previous work}


A video is a spatio-temporal data (i.e., image frames concatenated along time axis), and its representation must abstract both spatial and temporal information. Full 3D space-time (i.e., XYT) convolutional layers as well as (2+1)D convolutional layers have been popularly used to represent videos (\citealp{tran2014c3d,carreira2017quo,tran2018closer,xie2018rethinking}). Researchers studied replacing 2D convolutional layers in standard image-based CNNs such as Inception~(\citealp{szegedy2016rethinking}) and ResNet~(\citealp{he2016deep}), so that it can be directly used for video classification.



Two-stream network designs, which combine motion and appearance inputs, are commonly used (e.g., \citealp{simonyan2014two,feichtenhofer2016tsres,feichtenhofer2017tsmult,feichtenhofer2016convolutional}). Combining appearance information at two different temporal resolutions (e.g., 24 vs. 3 frames per second) with intermediate connections has been proposed by \cite{feichtenhofer2018slowfast}. Late fusion of the two-stream representations or architectures with more intermediate connections (\citealp{diba2019holistic}), have also been explored. However, these video CNN architectures are the result of careful manual designs by human experts.

Neural Architecture Search (NAS), the concept of automatically finding better architectures based on data, is becoming increasingly popular (\citealp{zoph2017neural,zoph2018nas,liu2018progressive}). Rather than relying on human expert knowledge to design a CNN model, neural architecture search allows the machines to generate better performing models optimized for the data. The use of reinforcement learning controllers (\citealp{zoph2017neural,zoph2018nas})  as well as evolutionary algorithms (\citealp{real2019amoeba}) have been studied, and they meaningfully outperform handcrafted architectures. Most of these works focus on learning architectures of modules (i.e., groupings of layers and their connections) to be repeated within a fixed single-stream meta-architecture (e.g., ResNet) for image-based object classification. 
One-shot architecture search to learn differentiable connections (\citealp{bender2018understanding,liu2019darts}) has also been successful for images. However, it is very challenging to directly extend such work to find multi-stream models for videos, as it requires preparing all possible layers and interactions the final architecture may consider using. In multi-stream video CNNs, there are many possible convolutional blocks with different resolutions, and fully connecting them requires a significant amount of memory and training data, which makes it infeasible. 






Our work is also related to \cite{ahmed2017connectivity} which used learnable gating to connect multiple residual module branches, and to the RandWire network~(\citealp{xie2019exploring}), which showed that randomly connecting a sufficient number of convolutional layers creates performant architectures. However, similar to previous NAS work, the latter focuses only on generating connections between the layers within a block. The meta-architecture is fixed as a single stream model with a single input modality. In this work, our objective is to learn high-level connectivity between multi-stream blocks for video understanding driven by data. We confirm experimentally that in the multi-stream video CNNs, where multiple types of input modalities need to be considered at various resolutions, randomly connecting blocks is insufficient and the proposed architecture learning strategy is necessary.







\section{AssembleNet}

We propose a new principled way to find better neural architectures for video representation learning. We first expand a video CNN to a multi-resolution, multi-stream model composed of multiple sequential and concurrent neural blocks, and introduce a novel algorithm to search for the optimal connectivity between the blocks for a given task.




We model a video CNN architecture as a collection of convolutional blocks (i.e., sub-networks) connected to each other.
Each block is composed of a residual module of space-time convolutional layers repeated multiple times, while having its own temporal resolution. 
The objective of our video architecture search is to automatically (1) decide the number of parallel blocks (i.e., how many streams to have) at each level of the network, (2) choose their temporal resolutions, and (3) find the optimal connectivity between such multi-stream neural blocks across various levels. 
The highly interconnected convolutional blocks allow learning of the video representations combining multiple input modalities at various temporal resolutions. We introduce the concept of connection-learning-guided architecture evolution to enable multi-stream architecture search.



















We name our final architecture as an `AssembleNet', since it is formulated by assembling (i.e., merging, splitting, and connecting) multiple building blocks. 







\subsection{Graph formulation}

In order to make our neural architecture evolution consider multiple different streams with different modalities at different temporal resolutions, we formulate the multi-stream model as a directed acyclic graph. Each node in the graph corresponds to a sub-network composed of multiple convolutional layers (i.e., a block), and the edges specify the connections between such sub-networks. Each architecture is denoted as  where  is the set of nodes and  is the set of edges defining their connectivity.

\vspace{-5pt}
\paragraph{Nodes.}

A node in our graph representation is a ResNet block composed of a fixed number of interleaved 2D and (2+1)D residual modules. 
A `2D module' is composed of a 1x1 conv. layer, one 2D conv. layer with filter size 3x3, and one 1x1 convolutional layer.
A `(2+1)D module' consists of a temporal 1D convolutional layer (with filter size 3), a 2D conv. layer, and a 1x1 conv. layer. In each block, we repeat a regular 2D residual module followed by the (2+1)D residual module  times.

Each node has its own block level, which naturally decides the directions of the edges connected to it. Similar to the standard ResNet models, we made the nodes have a total of four block levels (+ the stem level). Having multiple nodes of the same level means the architecture has multiple parallel `streams'. Figure~\ref{fig:assemblenet} illustrates an example. Each level has a different  value: 1.5, 2, 3, and 1.5.  means that there is one 2D module, one (2+1)D module, and one more 2D module. As a result, the depth of our network is 50 conv. layers. We also have a batch normalization layer followed by a ReLU after every conv. layer.

There are two special types of nodes with different layer configurations: source nodes and sink nodes. A source node in the graph directly takes the input and applies a small number of convolutional/pooling layers (it is often referred as the `stem' of a CNN model). In video CNNs, the input is a 4D tensor (XYT + channel) obtained by concatenating either RGB frames or optical flow images along the time axis.  Source nodes are treated as level-0 nodes.
The source node is composed of one 2D conv. layer of filter size 7x7, one 1D temporal conv. layer of filter size 5, and one spatial max pooling layer. The 1D conv. is omitted in optical flow stems. A sink node generates the final output of the model, and it is composed of one pooling, one fully connected, and one softmax layer. The sink node is also responsible for combining the outputs of multiple nodes at the highest level, by concatenating them after the pooling. More details are provided in Appendix.

Each node in the graph also has two attributes controlling the convolutional block: its temporal resolution and the number of channels. We use temporally dilated 1D convolution to dynamically change the resolution of the temporal convolutional layers in different blocks, which are discussed more below. The channel size (i.e., the number of filters) of a node could take arbitrary values, but we constrain the sum of the channels of all nodes in the same block level to be a constant so that the capacity of an AssembleNet model is equivalent to a ResNet model with the same number of layers.

\vspace{-5pt}
\paragraph{Temporally Dilated 1D Convolution.}



One of the objectives is to allow the video architectures to look at multiple possible temporal resolutions. This could be done by preparing actual videos with different temporal resolutions as in \cite{feichtenhofer2018slowfast} or by using temporally `dilatedâ€™ convolutions as we introduce here. Having dilated filters allow temporal 1D conv. layers to focus on different temporal resolution without losing temporal granularity.  This essentially is a 1D temporal version of standard 2D dilated convolutions used in \cite{chen2018deeplab} or \cite{yu2016dilated}:

Let  be a temporal filter (i.e., a vector) with size . The dilated convolution operator  is similar to regular convolution but has different steps for the summation, described as:

where , , and  are time indexes.  indicates the temporal resolution (or the amount of dilation), and the standard 1D temporal convolution is a special case where . In the actual implementation, this is done by inserting  number of zeros between each element of  to generate , and then convolving such zero-inflated filters with the input: . Importantly, the use of the dilated convolution allows different intermediate sub-network blocks (i.e., not just input stems) to focus on very different temporal resolutions at different levels of the convolutional architecture.




Note that our temporally dilated convolution is different from the one used in \cite{lea2017temporal}, which designed a specific layer to combine representations from different frames with various step sizes. Our layers dilate the temporal filters themselves. Our dilated convolution can be viewed as a direct temporal version of the standard dilated convolutions used in \cite{chen2018deeplab,yu2016dilated}.


\vspace{-5pt}
\paragraph{Edges.}

Each directed edge specifies the connection between two sub-network blocks, and it describes how a representation is transferred from one block to another block. We constrain the direction of each edge so that it is connected from a lower level block to a higher level block to avoid forming a cycle and allow parallel streams. A node may receive inputs from any number of lower-level nodes (including skip connections) and provide its output to any number of higher-level nodes.

Our architectures use a (learnable) weighted summation to aggregate inputs given from multiple connected nodes.  That is, an input to a node is computed as , where  are output tensors (i.e., representations) of the nodes connected to the node and  are their corresponding weights. Importantly, each  is considered as a variable that has to be learned from training data through back propagation. This has two key advantages compared to conventional feature map concatenation: (i) The input tensor size is consistent regardless of the number of connections. (ii) We use learned connection weights to `guide' our architecture evolution algorithm in a preferable way, which we discuss more in Section \ref{subsec:evolution}.



If the inputs from different nodes differ in their spatial size, we add spatial max pooling and striding to match their spatial size. If the inputs have different channel sizes, we add a 1x1 conv. layer to match the bigger channel size. Temporal sizes of the representations is always consistent in our graphs, as there is no temporal striding in our formulation and the layers in the nodes are fully convolutional.












\subsection{Evolution}
\label{subsec:evolution}




We design an evolutionary algorithm with discrete mutation operators that modify nodes and edges in architectures over iterations. The algorithm maintains a population of  different architectures, , where each architecture  is represented with a set of nodes and their edges as described above.

The initial population is formed by preparing a fixed number of randomly connected architectures (e.g., ). Specifically, we (1) prepare a fixed number of stems and nodes at each level (e.g., two per level), (2) apply a number of node split/merge mutation operators which we discuss more below, and (3) randomly connect nodes with the probability  while discarding architectures with graph depth  4. As mentioned above, edges are constrained so that there is no directed edge reversing the level ordering. Essentially, a set of overly-connected architectures are used as a starting point. Temporal resolutions are randomly assigned to the nodes.

\begin{figure}
  \centering
\includegraphics[width=1.0\linewidth]{images/evolution-example-re.pdf}\caption{An example showing a sequence of architecture evolution. These architectures have  actual parent-child relationships. The fitness of the third model was worse than the second model (due to random mutations), but it was high enough to survive in the population pool and eventually evolve into a better model.}
  \label{fig:evolution-example}
\end{figure}




We use the tournament selection algorithm~(\citealp{goldberg91acomparative}) as the main evolution framework: At each evolution round, the algorithm updates the population by selecting a `parent' architecture and mutating (i.e., modifying) it to generate a new `child' architecture. The parent is selected by randomly sampling a subset of the entire population , and then computing the architecture with the highest `fitness':  where  is the fitness function. Our fitness is defined as a video classification accuracy of the model, measured by training the model with a certain number of initial iterations and then evaluating it on the validation set as its proxy task. More specifically, we use top-1 accuracy + top-5 accuracy as the fitness function. The child is added into the population, and the model with the least fitness is discarded from the population.

A child is evolved from the parent by following two steps. First, it changes the block connectivity (i.e., edges) based on their learned weights: `connection-learning-guided evolution'. Next, it applies a random number of mutation operators to further modify the node configuration. The mutation operators include (1) a random modification of the temporal resolution of a convolutional block (i.e., a node) as well as (2) a merge or split of a block. 
When splitting a node into two nodes, we make their input/output connections identical while making the number of channels in their convolutional layers half that of the node before the split (i.e.,  where  is the channel size of the parent). More details are found in Appendix. As a result, we maintain the total number of parameters, since splitting or merging does not change the number of parameters of the convolutional blocks.

\vspace{-5pt}
\paragraph{Connection-Learning-Guided Mutation.} Instead of randomly adding, removing or modifying block connections to generate the child architecture, we take advantage of the learned connection weights from its parent architecture. Let  be the set of edges of the parent architecture. Then the edges of the child architecture  are inherited from , by only maintaining high-weight connections while replacing the low-weight connections with new random ones. Specifically, :

where  and  is the set of all possible edges.
 corresponds to the edges the child architecture inherits from the parent architecture, decided based on the learned weight of the edge . This is possible because our fitness measure involves initial proxy training of each model, providing the learned connection weight values  of the parent.



, which controls whether or not to keep an edge from the parent architecture, could either be a constant threshold or a random variable following a uniform distribution:  or .  corresponds to the new randomly added edges which were not in the parent architecture. We enumerate through each possible new edge, and randomly add it with the probably of . This makes the expected total number of added edges to be , maintaining the size of .
Figure~\ref{fig:evolution-example} shows an example of the evolution process and Figure~\ref{fig:architecture-examples} shows final architectures.
















\vspace{-7pt}
\paragraph{Evolution Implementation Details.}

Initial architectures are formed by randomly preparing either \{2 or 4\} stems, two nodes per level at levels 1 to 3, and one node at level 4.
We then apply 15 random number of node split operators so that each initial architecture has a different number of nodes. 
Each node is initialized with a random temporal resolution of 1, 2, 4, or 8. As mentioned, each possible connection is then added with the probability of .

\vspace{-3pt}
At each evolution round, the best-performing parent architecture is selected from a random subset of 5 from the population. The child architecture is generated by modifying the connections from the parent architecture (Section \ref{subsec:evolution}). A random number of node split, merge, or temporal resolution change mutation operators (04) are then applied. Evaluation of each architecture (i.e., measuring the fitness) is done by training the model for 10K iterations and then measuring its top-1 + top-5 accuracy on the validation subset. The Moments-in-Time dataset, described in the next section, is used as the proxy dataset to measure fitness. 
The evolution was run for 200 rounds, although a good performing architecture was found within only 40 rounds (e.g., Figure~\ref{fig:architecture-examples}-right). Figure~\ref{fig:assemblenet} shows the model found at the 165th round. 10K training iterations of each model during evolution took 35 hours; with our setting, evolving a model for 40 rounds took less than a day with 10 parallel workers.



\begin{figure}
  \centering
\includegraphics[width=1.0\linewidth]{images/AssembleNet-examples-re.pdf}\caption{More AssembleNet examples. Similarly good performing diverse architectures, all with higher-than-50\% mean-average precision on Charades. For instance, even our simpler two-stem AssembleNet-50 (left) got 51.4\% mAP on Charades. Darker edges mean higher weights.}
  \label{fig:architecture-examples}
\end{figure}

\section{Experiments}
\vspace{-5pt}
\subsection{Datasets}



\vspace{-5pt}
\paragraph{Charades Dataset.}
We first test on the popular Charades dataset~(\citealp{sigurdsson2016hollywood}) which is unique in the activity recognition domain as it contains long sequences. It is one of the largest public datasets with continuous action videos, containing 9848 videos of 157 classes (7985 training and 1863 testing videos). Each video is 30 seconds. It is a challenging dataset due to the duration and variety of the activities. Activities may temporally overlap in a Charades video, requiring the model to predict multiple class labels per video. We used the standard `Charades\_v1\_classify' setting for the evaluation. To comply with prior work (e.g. \citealp{feichtenhofer2018slowfast}), we also report results when pre-training on Kinetics~(\citealp{carreira2017quo}), which is another large-scale dataset. We note that Kinetics is shrinking in size (15\% videos removed from the original Kinetics-400) and the previous versions are no longer available from the official site.


\vspace{-5pt}
\paragraph{Moments in Time (MiT) Dataset.}

The Moments in Time (MiT) dataset (\citealp{monfort2018moments}) is a large-scale video classification dataset with more than 800K videos (3 seconds per video). It is a very challenging dataset with the state-of-the-art models obtaining less than 30\% accuracy. We use this dataset for the architecture evolution, and train/test the evolved models. We chose the MiT dataset because it provides a sufficient amount of training data for video CNN models and allows stable comparison against previous models. We used its standard classification evaluation setting.


















\begin{table}[t]
    \setlength\tabcolsep{2pt}
    \caption{Reported state-of-the-art action classification performances (vs. AssembleNet) on Charades. `2-stream (2+1)D ResNet-50' is the two-stream model with connection learning for level-4 fusion.}
    \label{tab:charades}
\centering
    \begin{tabular}{lccc}
    \toprule
       Method & pre-train & modality & mAP \\
    \midrule
    2-stream (\citealp{simonyan2014two})  & UCF101 & RGB+Flow & 18.6 \\
    Asyn-TF (\citealp{sigurdsson2016asynchronous}) & UCF101 & RGB+Flow & 22.4 \\
    CoViAR (\citealp{wu2018compressed}) & ImageNet & Compressed & 21.9 \\
    MultiScale TRN (\citealp{zhou2018temporal}) & ImageNet &  RGB & 25.2 \\
    I3D (\citealp{carreira2017quo}) & Kinetics & RGB & 32.9 \\
    I3D (from \citealp{wang2018non}) & Kinetics & RGB & 35.5 \\
    I3D-NL (\citealp{wang2018non}) & Kinetics & RGB & 37.5 \\
STRG (\citealp{wang2018videos}) & Kinetics & RGB & 39.7 \\
    LFB-101 (\citealp{wu2018long}) & Kinetics & RGB & 42.5 \\
    SlowFast-101 (\citealp{feichtenhofer2018slowfast}) & Kinetics & RGB+RGB & 45.2 \\
\hline
    2-stream (2+1)D ResNet-50 (ours) & None & RGB+Flow & 39.9 \\
    2-stream (2+1)D ResNet-50 (ours) & MiT & RGB+Flow & 48.7 \\
2-stream (2+1)D ResNet-50 (ours) & Kinetics & RGB+Flow & 50.4 \\
    2-stream (2+1)D ResNet-101 (ours) & Kinetics & RGB+Flow & 50.6 \\
    \hline
    AssembleNet-50 (ours) & None & RGB+Flow & 47.0 \\
    AssembleNet-50 (ours) & MiT & RGB+Flow & 53.0 \\
AssembleNet-50 (ours) & Kinetics & RGB+Flow & 56.6 \\
    AssembleNet-101 (ours) & Kinetics & RGB+Flow & \textbf{58.6} \\
    \bottomrule
    \end{tabular}
\end{table}


\begin{figure}
\begin{minipage}{0.55\textwidth}
    \small
    \captionsetup{type=table}
    \setlength\tabcolsep{3pt}
    \caption{State-of-the-art action classification accuracies on Moments in Time~(\citealp{monfort2018moments}).}
    \label{tab:mit}
    \begin{tabular}{lccc}
    \toprule
       Method & modality & Top-1 & Top-5 \\
    \midrule
    ResNet50-ImageNet & RGB & 27.16 & 51.68 \\
    TSN~(\citealp{wang2016temporal})  & RGB & 24.11 & 49.10 \\
    \cite{ioffee2015batch} & Flow & 11.60 & 27.40 \\
TSN-Flow~(\citealp{wang2016temporal}) & Flow & 15.71 & 34.65 \\
    TSN-2Stream~(\citealp{wang2016temporal}) & RGB+F & 25.32 & 50.10 \\
    TRN-Multi~(\citealp{zhou2018temporal}) & RGB+F & 28.27 & 53.87 \\
    Two-stream (2+1)D ResNet-50 & RGB+F & 28.97 & 55.55 \\
    I3D~(\citealp{carreira2017quo}) & RGB+F & 29.51 & 56.06 \\
    \hline
AssembleNet-50 & RGB+F & 31.41 & 58.33 \\
    AssembleNet-50 (with Kinetics) & RGB+F & 33.91 & 60.86 \\
AssembleNet-101 (with Kinetics) & RGB+F & \textbf{34.27} & \textbf{62.71} \\
\bottomrule
    \end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.4\textwidth}
      \centering
       \includegraphics[width=1.0\linewidth]{images/evol-figure.pdf}\caption{Comparison of different search methods. }
\label{fig:evolution-graph}
\end{minipage}
\end{figure}

\vspace{-5pt}
\subsection{Results}
\vspace{-5pt}

Tables~\ref{tab:charades} and \ref{tab:mit} compare the performance of AssembleNet against the state-of-the-art models. We denote AssembleNet more specifically as AssembleNet-50, since its depth is 50 layers and has an equivalent number of parameters to ResNet-50. AssembleNet-101 is its 101 layer version having equivalent number of parameters to ResNet-101. 
AssembleNet is outperforming prior works on both datasets, setting new state-of-the-art results for them. Its performance on MiT is the first above 34\%. We also note that the performances on Charades is even more impressive at  whereas previous known best results are  and . For these experiments, the architecture search was done on the MiT dataset, and then the found models are trained and tested on both datasets, which demonstrates that the found architectures are useful across datasets.

In addition, we compare the proposed connection-learning-guided evolution with random architecture search and the standard evolutionary algorithm with random connection mutations. We made the standard evolutionary algorithm randomly modify 1/3 of the total connections at each round, as that is roughly the number of edges the connection-learning-guided evolution modifies. 
Figure~\ref{fig:evolution-graph} shows the results, visualizing the average fitness score of the three top-performing models in each pool. We observe that the connection-learning-guided evolution is able to find better architectures, and it is able to do so more quickly. The standard evolution performs similarly to random search and is not as effective. We believe this is due to the large search space the approach is required to handle, which is exponential to the number of possible connections. For instance, if there are  nodes, the search space complexity is  just for the connectivity search. Note that the initial 30 rounds are always used for random initialization of the model population, regardless of the search method.





\vspace{-5pt}
\subsection{Ablation studies}
\vspace{-5pt}

We conduct an ablation study comparing the evolved AssembleNet to multiple (2+1)D two-stream (or multi-stream) architectures which are designed to match the abilities of Assemblenet but without evolution. 
We note that these include very strong architectures that have not been explored before, such as the four-stream model with dense intermediate connectivity. 
We design competitive networks having various connections between streams, where the connection weights are also learned (see the supplementary material for detailed descriptions and visualizations). Note that all these models have equivalent capacity (i.e., number of parameters). The performance difference is due to network structure.
Table~\ref{tab:ablation} shows the results, demonstrating that these architectures with learnable inter-connectivity are very powerful themselves and evolution is further beneficial. The Moments in Time models were trained from scratch, and the Charades models were pre-trained on MiT. In particular, we evaluated an architecture with intermediate connectivity from the flow stream to RGB, inspired by~\cite{feichtenhofer2016convolutional,feichtenhofer2018slowfast} (+ connection weight learning). It gave \% accuracy on MiT and \% on Charades, which are not as accurate as AssembleNet.
Randomly generated models (from 50 rounds of search) are also evaluated, confirming that such architectures do not perform well. 


Further, we conduct another ablation to confirm the effectiveness of our search space. Table~\ref{tab:ablation2} compares the models found with our full search space vs. more constrained search spaces, such as only using two stems and not using temporal dilation (i.e., fixed temporal resolution).





\begin{figure}
\begin{minipage}{0.5\textwidth}
    \centering
    \captionsetup{type=table}
    \setlength\tabcolsep{2pt}
    \caption{Comparison between AssembleNet and architectures without evolution, but with connection weight learning. Four-stream models are reported here for the first time, and are very effective. All these models have a similar number of parameters.}
    \label{tab:ablation}
    \begin{tabular}{lccc}
    \toprule
       Architecture & MiT & Charades\\
    \midrule
    Two-stream (late fusion)  & 28.97 & 46.5\\
    Two-stream (fusion at lv. 4) & 30.00 & 48.7\\
    Two-stream (flowRGB inter.) & 30.21 & 49.5\\
    Two-stream (fully, fuse at 4) & 29.87 & 50.5\\
Four-stream (fully, fuse at 4) & 29.98 & 50.7\\
    \hline
    Random (+ connection learning) & 29.91 & 50.1\\
    \hline
    AssembleNet-50 & 31.41 & 53.0\\
    \bottomrule
    \end{tabular}
\end{minipage}
\hspace{0.02\textwidth}
\begin{minipage}{0.47\textwidth}
    \centering
    \captionsetup{type=table}
    \setlength\tabcolsep{3pt}
    \caption{Ablation comparing different AssembleNet architectures found with full vs. constrained search spaces. The models are trained from scratch.}
    \label{tab:ablation2}
    \begin{tabular}{lccc}
    \toprule
       Architecture & MiT\\
    \midrule
    Baseline (random + conn. learning) & 29.91\\
    \hline
    No mutation & 30.26\\
    RGB-only & 30.30\\
    Without temporal dilation & 30.49\\
    Two-stem only & 30.75\\
    \hline
    Full AssembleNet-50 & 31.41\\
    \bottomrule
    \end{tabular}
\end{minipage}
\end{figure}

\vspace{-5pt}
\subsection{General findings}
\vspace{-5pt}

As the result of connection-learning-guided architecture evolution, non-obvious and non-intuitive connections are found (Figure~\ref{fig:architecture-examples}). As expected, more than one possible ``connectivity'' solution can yield similarly good results. Simultaneously, models with random connectivity perform poorly compared to the found AssembleNet. Our observations also include: (1) The models prefer to have only one block at the highest level, although we allow the search to consider having more than one block at that level. (2)  The final block prefers simple connections gathering all outputs of the blocks in the 2nd to last level. (3) Many models use multiple blocks with different temporal resolutions at the same level, justifying the necessity of the multi-stream architectures. (4) Often, there are 1 or 2 blocks heavily connected to many other blocks. (5) Architectures prefer using more than 2 streams, usually using 4 at many levels.




\section{Conclusion}

We present AssembleNet, a new approach for neural architecture search using connection-learning-guided architecture evolution. AssembleNet finds multi-stream architectures with better connectivity and temporal resolutions for video representation learning. Our experiments confirm that the learned models significantly outperform previous models on two challenging benchmarks. 

\bibliographystyle{iclr2020_conference}
\bibliography{egbib}

\newpage
\appendix
\section{Appendix}


\subsection{Super-graph visualization of the connectivity search space}

Figure~\ref{fig:super-graph} visualizes all possible connections and channel/temporal resolution options our architecture evolution is able to consider. The objective of our evolutionary algorithm could be interpreted as finding the optimal sub-graph (of this super-graph) that maximizes the performance while maintaining the number of total parameters. Trying to directly fit such entire super-graph into the memory was infeasible in our experiments.

\begin{figure}[h]
  \centering
   \includegraphics[width=1.0\linewidth]{images/assemblenet-super-graph.pdf}
  \caption{Visualization of the super-graph corresponding to our video architecture search space.}
  \label{fig:super-graph}
\end{figure}

\subsection{Channel sizes of the layers and node split/merge mutations}

As we described in the paper, each node (i.e., a convolutional block) has a parameter  controlling the number of filters of the convolutional layers in the block. When splitting or merging blocks, the number of filters are split or combined respectively. Figure~\ref{fig:node-split} provides a visualization of a block with number of filter specified to the right and a split operation. While many designs are possible, we design the blocks and splitting as follows. The size of 1x1 convolutional layers and 1D temporal convolutional layers are strictly governed by , having the channel size of  (some ). On the other hand, the number of 2D convolutional layer is fixed per level as a constant  where  is the level of the block. , , , and . The layers in the stems have 64 channels if there are only two stems and 32 if there are four stems.

When a node is split into two nodes, we update the resulting two nodes' channel sizes to be half of their original node. This enables us to maintain the total number of model parameters before and after the node split to be identical. The first 1x1 convolutional layer will have half the parameters after the split, since its output channel size is now 1/2. The 2D convolutional layer will also have exactly half the parameters, since its input channel size is 1/2 while the output channel size is staying fixed. The next 1x1 convolutional layer will have the fixed input channel size while the output channel size becomes 1/2: thus the total number of parameters would be 1/2 of the original parameters. 

Merging of the nodes is done in an inverse of the way we split. When merging two nodes into one, the merged node inherits all input/output connections from the two nodes: we take a union of all the connections. The channel size of the merged node is the sum of the channel sizes of the two nodes being merged. The temporal dilation rate of the merged node is randomly chosen between the two nodes before the merge.




\begin{figure}
  \centering
   \includegraphics[width=0.7\linewidth]{images/node-split-mutation.pdf}\caption{An illustration of the node split mutation operator, used for both evolution and initial architecture population generation.}
  \label{fig:node-split}
\end{figure}


\subsection{Hand-designed models used in the ablation study}

Figure~\ref{fig:hand-examples} illustrates the actual architectures of the hand-designed (2+1)D CNN models used in our ablation study. We also show the final learned weights of the connections, illustrating which connections the model ended up using or not using. We note that these architectures are also very enlightening as the connectivity within them are learned in the process. We observe that stronger connections tend to be formed later for 2-stream architectures. For 4-stream architectures, stronger connections do form early, and, not surprisingly, a connection to at least one node of a different modality is established, i.e. a node stemming from RGB will connect to at least one flow node at the next level and vice versa.

Below is a more detailed description of the networks used in the paper:
``Two-stream (late fusion)'' means that the model has two separate streams at every level including the level 4, and the outputs of such two level 4 nodes are combined for the final classification. ``Fusion at lv. 4'' is the model that only has one level 4 node to combine the outputs of the two level 3 nodes using a weighted summation. ``Two-stream (fully)'' means that the model has two nodes at each level 1-3 and one node at level 4, and each node is always connected to every node in the immediate next level. ``FlowRGB'' means that only the RGB stream nodes combine outputs from both RGB and flow stream nodes of the immediate lower level.

\begin{figure}
  \centering
   \includegraphics[width=1.0\linewidth]{images/handdesigned-examples-re.pdf}
  \caption{Illustration of hand-designed baseline (2+1)D CNN models used in our ablation study. Their connections weights are learned.}
  \label{fig:hand-examples}
\end{figure}









\subsection{AssembleNet model/layer details}

We also provide the final AssembleNet model in table form in Table~\ref{tab:model}. In particular, the 2nd element of each block description shows the list of where the input to that block is coming from (i.e., the connections). As already mentioned, 2D and (2+1)D residual modules are repeated in each block. The number of repetitions  are 1.5, 2, 3, and 1.5 at each level.  means that we have one 2D residual module, one (2+1)D module, and one more 2D module. This makes the number of convolutional layers of each block at levels 1-4 to be 9, 12, 18, and 9. In addition, a stem has at most 2 convolutional layers. The total depth of our network is 50, similar to a conventional (2+1)D ResNet-50. For AssembleNet-101, we use  1.5, 2, 11.5, and 1.5 at each level.

If a block has a spatial stride of 2, the striding happens at the first 2D convolutional layer of the block. In the stem which has the spatial stride of 4, the striding of size 2 happens twice, once at the 2D convolutional layer and at the max pooling layer. As mentioned, the model has a batch normalization layer followed by ReLU after every convolutional layer regardless of its type (i.e., 2D, 1D, and 1x1). 2D conv. filter sizes are 3x3, and 1D conv. filter sizes are 3.

\begin{table}
    \centering
    \setlength\tabcolsep{5pt}
    \caption{The table form of the AssembleNet model with detailed parameters. This model corresponds to Figure 1. The parameters correspond to \{node\_level, input\_node\_list, , , and spatial stride\}}
    \label{tab:model}
    \begin{tabular}{|l|c|}
    \hline
      Index & Block parameters\\
    \hline
    0 & 0, [RGB], 32, 4, 4 \\ \hline
    1 & 0, [RGB], 32, 4, 4 \\ \hline
    2 & 0, [Flow], 32, 1, 4 \\ \hline
    3 & 0, [Flow], 32, 1, 4 \\ \hline
    4 & 1, [1], 32, 1, 1 \\ \hline
    5 & 1, [0], 32, 4, 1 \\ \hline
    6 & 1, [0,1,2,3], 32, 1, 1 \\ \hline
    7 & 1, [2,3], 32, 2, 1 \\ \hline
    8 & 2, [0, 4, 5, 6, 7], 64, 2, 2 \\ \hline
    9 & 2, [0, 2, 4, 7], 64, 1, 2 \\ \hline 
    10 & 2, [0, 5, 7], 64, 4, 2 \\ \hline
    11 & 2, [0, 5], 64, 1, 2 \\ \hline
    12 & 3, [4, 8, 10, 11], 256, 1, 2 \\ \hline
    13 & 3, [8, 9], 256, 4, 2 \\ \hline
    14 & 4, [12, 13], 512, 2, 2 \\
    \hline
    \end{tabular}
\end{table}


\subsection{Sink node details}

When each evolved or baseline (2+1)D model is applied to a video, it generates a 5D (BTYXC) tensor after the final convolutional layer, where B is the size of the batch and C is the number of channels. The sink node is responsible for mapping this into the output vector, whose dimensionality is identical to the number of video classes in the dataset. The sink node first applies a spatial average pooling to generate a 3D (BTC) tensor. If there are multiple level 4 nodes (which rarely is the case), the sink node combines them into a single tensor by averaging/concatenating them. Averaging or concatenating does not make much difference empirically. Next, temporal average/max pooling is applied to make the representation a 2D (BC) tensor (average pooling was used for the MiT dataset and max pooling was used for Charades), and the final fully connected layer and the soft max layer is applied to generate the final output.


\subsection{Training details}

For the Moments in Time (MiT) dataset training, 8 videos are provided per TPU core (with 16GB memory): the total batch size (for each gradient update) is 512 with 32 frames per video. The batch size used for Charades is 128 with 128 frames per video. The base framerate we used is 12.5 fps for MiT and 6 fps for Charades. The spatial input resolution is 224x224 during training. We used the standard Momentum Optimizer in TensorFlow. We used a learning rate of 3.2 (for MiT) and 25.6 (for Charades), 12k warmup iterations, and cosine decay. No dropout is used, weight decay is set to 1e-4 and label smoothing set to 0.2. 

Training a model for 10k iterations (during evolution) took 35 hours and fully training the model (for 50k iterations) took 24 hours per dataset.

We used the TV-L1 optical flow extraction algorithm (\citealp{zach2007duality}) implemented with tensor operations by \cite{piergiovanni2018representation} to obtain flow input.


\subsection{Evaluation details}

When evaluating a model on the MiT dataset, we provide 36 frames per video. The duration of each MiT video is 3 seconds, making 36 frames roughly correspond to the entire video. For the Charades dataset where each video duration is roughly 30 seconds, the final class labels are obtained by applying the model to five random 128 frame crops (i.e., segments) of each video. The output multi-class labels are max-pooled to get the final label, and is compared to the ground truth to measure the average precision scores. The spatial resolution used for the testing is 256x256.





\end{document}
