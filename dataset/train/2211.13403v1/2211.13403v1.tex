
\documentclass[letterpaper]{article} \usepackage{fullpage}



\usepackage{hyperref}
\usepackage{url}
\usepackage[round]{natbib}
\usepackage{fullpage}


\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} \usepackage{amsmath,amscd,amssymb,amsthm,cleveref}
\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{graphicx}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{dcolumn}
\usepackage{multicol}
\newcolumntype{d}{D{.}{.}{-1}}
\usepackage{wrapfig}
\usepackage{todonotes}

\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\newcommand{\N}{{\mathbb N}}
\def\cA{\mathcal{A}}
\def\cD{\mathcal{D}}
\def\cO{\mathcal{O}}
\def\cR{\mathcal{R}}
\def\cS{\mathcal{S}}
\def\cY{\mathcal{Y}}
\def\PrivacyLoss{\textrm{PrivacyLoss}}
\renewcommand{\Pr}{\mathop{\mathbf{Pr}}}
\def\calC{\mathcal{C}}
\def\calN{\mathcal{N}}
\def\calL{\mathcal{L}}

\def\cardD{n}



\newcommand{\snote}[1]{\todo[color=black!10]{SS: #1}}
\newcommand{\snoteinline}[1]{\todo[color=black!10,inline]{SS: #1}}

\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}[lem]{Theorem}
\newtheorem{cor}[lem]{Corollary}
\newtheorem{problem}[lem]{Problem}
\newtheorem{defn}[lem]{Definition}
\newtheorem{fact}[lem]{Fact}
\newtheorem{assumption}[lem]{Assumption}
\newtheorem{claim}[lem]{Claim}
\newtheorem{definition}[lem]{Definition}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\ltwo}[1]{\left\|#1\right\|_2}
\newcommand{\alexnote}[1]{\todo[color=black!10]{Alex: #1}}
\newcommand{\harshnote}[1]{\todo[color=red!10]{Harsh: #1}}

\newcommand{\anote}[1]{\todo[color=blue!30]{\tiny AT: #1}}
\newcommand{\anoteinline}[1]{\todo[color=blue!30,inline]{AT: #1}}


\title{Differentially Private Image Classification from Features}





\usepackage{authblk}

\author[1]{Harsh Mehta}
\author[1]{Walid Krichene}
\author[1]{Abhradeep Thakurta}
\author[1]{Alexey Kurakin}
\author[2]{Ashok Cutkosky}

\affil[1]{Google Research \thanks{}}
\affil[2]{Boston University \thanks{}}

\date{}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
In deep learning, leveraging transfer learning has recently been shown to be an effective strategy for training large high performance models with Differential Privacy (DP). Moreover, somewhat surprisingly, recent works have found that privately training just the last layer of a pre-trained model provides the best utility with DP. While past studies largely rely on using first-order differentially private training algorithms like DP-SGD for training large models, in the specific case of privately learning from features, we observe that computational burden is often low enough to allow for more sophisticated optimization schemes, including second-order methods. To that end, we systematically explore the effect of design parameters such as loss function and optimization algorithm. We find that, while commonly used logistic regression performs better than linear regression in the non-private setting, the situation is reversed in the private setting. We find that least-squares linear regression is much more effective than logistic regression from both privacy and computational standpoint, especially at stricter epsilon values (). On the optimization side, we also explore using Newton's method, and find that second-order information is quite helpful even with privacy, although the benefit significantly diminishes with stricter privacy guarantees. While both methods use second-order information, least squares is more effective at lower epsilon values while Newton's method is more effective at larger epsilon values. To combine the benefits of both methods, we propose a novel optimization algorithm called DP-FC, which leverages feature covariance instead of the Hessian of the logistic regression loss and performs well across all  values we tried. With this, we obtain new SOTA results on ImageNet-1k, CIFAR-100 and CIFAR-10 across all values of  typically considered. Most remarkably, on ImageNet-1K, we obtain top-1 accuracy of 88\% under DP guarantee of (8, ) and 84.3\% under (0.1, ).
\end{abstract}

\section{Introduction}
Despite impressive performance, large machine learning models are susceptible to attacks. Previous work has demonstrated successful membership attacks where the goal is to extract exact instances of training examples that the model was trained on \citep{shokri2017membership,carlini2019secret, carlini2020extracting,choquettechoo2020labelonly,liu2021mldoctor,balle2022reconstructing} and models of larger size are known to be more likely to memorize training data. These attacks can be quite egregious if the model was trained on sensitive data like personal photos or emails. One approach to mitigate this risk is to train such models with privacy guarantees. In particular, Differential Privacy (DP) has become the gold standard in quantifying the risk and providing formal guarantees against membership attacks \citep{adv_inst2021}.

Informally, Differentially Privacy implies that an adversary learns almost the same thing about an individual data point independent of their presence or absence in the training data set. More formally, DP is defined as follows:
\begin{definition}
[Differential Privacy \citep{dwork2006dp,ODO}] A randomized algorithm  is -differentially private if, for any pair of datasets  and  differing in at most one example (called {\em neighboring datasets}), and for all events  in the output range of , we have 

where the probability is over the randomness of .
\label{def:dp}
\end{definition}

In the field of deep learning, Differentially Private Stochastic Gradient Descent (DP-SGD) \citep{song2013stochastic,Bassily_2014,abadi2016dpsgd} is the most commonly used method for training models with DP guarantees. While DP-SGD is a fairly general algorithm, a naive application can suffer from several computational challenges. To make matters worse, the gap between performance of a model with and without privacy typically widens as the model is made larger. This stands as a significant obstacle to a wider adoption and deployment of large practical models with privacy guarantees.

For the problem of Image Classification, several works have shown that transfer learning can be a very effective strategy in order to improve privacy-utility trade-off when formal privacy guarantees are required \citep{kurakin2022training,mehta2022large,dm_transfer_2022}. In this setting, a model is first pre-trained with ``non-sensitive" data without privacy guarantees, then fine-tuned on a ``sensitive" dataset over which a formal privacy guarantee is required. Similar to previous works, we simulate several publicly available image classification benchmarks (like ImageNet) as ``sensitive" datasets.

Interestingly, \cite{mehta2022large,dm_transfer_2022} observe that privately fine-tuning just the last layer of a pre-trained model (using DP-SGD) leads to state of the art results in the ImageNet-1k dataset. This is quite fortuitous, since privately fine-tuning the full model typically introduces significant computational challenges. We build on this observation, and perform a comprehensive exploration of various design parameters, including the choice of loss function and optimization algorithm, beyond simple DP-SGD. In this restricted setting of learning a single layer privately using features extracted from a pre-trained model, more sophisticated methods, such as second-order methods, are computationally viable. Our main contributions are as follows:

\begin{itemize}
    \item Somewhat surprisingly, we find that linear regression solved using DP Least Squares performs much better than logistic regression solved using DP-SGD, especially at lower epsilons.
    \item Postulating that the benefits largely stem from the use of second-order information in the least squares solution, we further explore using Newton's method to solve logistic regression. While Newton's method outperforms linear regression in the non-private setting, we find that it still performs worse with privacy constraints, largely because sanitizing the Hessian with logistic regression requires adding far more noise than in linear regression, where part of the Hessian can be shared across all classes.
    \item To combine the benefits of both, we introduce a method which we call Differentially Private SGD with Feature Covariance (abbreviated as DP-FC) where we simply replace the Hessian in Newton's method with sanitized Feature Covariance. Using Feature Covariance instead of Hessian allows us to make use of second-order information in the training procedure while sharing it across classes and iterations, which greatly reduces the amount of noise that needs be added to sanitize it. This allows us to continue using logistic regression, which performs better in non-private setting, while benefiting from improved privacy-utility trade-off as seen with linear regression in the private setting.
    \item With DP-FC, we surpass previous state of the art results considerably on 3 image classification benchmarks, namely ImageNet-1k, CIFAR-10 and CIFAR-100, just by performing DP fine-tuning on features extracted from a pre-trained model, see Table \ref{tab:method_comparison} for a summary. Consistent with previous works, we also find that performance increases as the pre-training dataset and the model are made larger.
\end{itemize}

\begin{table}[]
\centering
\small
\begin{tabular}{lcccccc}
    \toprule
        \addlinespace[0.1cm]
Dataset &                           Epsilon & Previous SOTA & Accuracy & Method & Epochs ( Steps) & Pretraining DS  \\
\midrule
\multirow{9}{*}{CIFAR-10}
&         0.01  &       & 97.4 & DP-LS & 1 & JFT \\
&         0.05  &       & 98.2 & DP-FC & 10 & JFT \\
&         0.1    &     & 98.4 & DP-FC & 10 & JFT \\
&         0.5    &     & 98.8 & DP-FC & 10 & JFT \\
&        1.0     &  96.7   & 98.8 & DP-FC & 10 & JFT\\
&       2.0     &  97.1  & 98.9 & DP-FC & 10 & JFT\\
&      4.0      & 97.2   & 98.9 & DP-FC & 10 & JFT\\
&     8.0       & 97.4  & 98.9 & DP-FC & 10 & JFT\\
&       &    & 98.9 & LS & 1 & JFT\\

                                  \addlinespace[0.1cm]
                                         \hline
                                          \addlinespace[0.1cm]

\multirow{9}{*}{CIFAR-100} 
&         0.01  &       & 77.2 & DP-LS & 1 & I21K \\
&         0.05  &       & 80.3 & DP-LS & 1 & JFT \\
&         0.1   &      & 82.5 & DP-LS & 1 & JFT \\
&         0.5   &      & 86.2 & DP-FC & 10 & JFT \\
&        1.0    &  83.0   & 88.1 & DP-FC & 10 & JFT\\
&       2.0     &  86.2  & 89.0 & DP-FC & 10 & JFT\\
&      4.0      &  87.7  & 90.0 & DP-FC & 10 & JFT\\
&     8.0       &  88.4   & 90.1 & DP-FC & 10 & JFT\\
&       &   & 90.6 & LS & 1 & JFT\\

                                  \addlinespace[0.1cm]
                                         \hline
                                          \addlinespace[0.1cm]

\multirow{9}{*}{ImageNet-1K} 
&         0.01&         & 82.4 & DP-LS & 1 & JFT \\
&         0.05&         & 83.8 & DP-LS & 1 & JFT \\
&         0.1 &        & 84.3 & DP-FC & 1 & JFT \\
&         0.5 &        & 86.1 & DP-FC & 10 & JFT \\
&        1.0  &   84.4    & 86.8 & DP-FC & 10 & JFT\\
&       2.0   &  85.6    & 87.4 & DP-FC & 10 & JFT\\
&      4.0    &  86.0   & 87.7 & DP-FC & 10 & JFT\\
&     8.0     &  86.7  & 88.0 & DP-FC & 10 & JFT\\
&     &   & 88.9 & Newton & 10 & JFT\\

\bottomrule
    \end{tabular}
        \caption{Compilation of our best private Top-1 test accuracies. All number are SOTA across all epsilons to the best of our knowledge. Previous state of the art for CIFAR-10 and CIFAR-100 were reported form \cite{bu2022scalable_oldsota} and for ImageNet-1K from \cite{dm_transfer_2022}. We denote  to be  for non-private setting where we turn off all sanitization steps including clipping. We set  to  for ImageNet-1k, and  for CIFAR-10 and CIFAR-100. Interestingly, in the non-private setting, most previous works (including \cite{zhai2021scaling}) use Linear Regression when finetuning from features but we found that Logistic Regression (solved using Newton's method) performs much better and leads to an impressive 88.9\% accuracy when finetuning just the last layer. To put this in perspective, this is only 1.1\% less than the current state of the art non-private accuracy of 91\% on ImageNet-1k \citep{yu2022coca}. We report extensive hyperparameter details in the appendix for reproducibility of our results.}
            \label{tab:method_comparison}
\end{table}







    



\section{Private Learning from Features}
\label{sec:method}
In this section, we describe the details of optimization strategies we considered, and state the privacy guarantees for each of them.

Given a data set , we optimize the function  defined as follows



where  is the number of examples,  is the number of classes,  is the weight matrix to be learned,  is the feature vector for example ,  is the label of example  and class , and  is a convex loss function. We assume that  for all . Additionally, we also use the short hand   where helpful in order to simplify the notation.

In the case of learning from features extracted from a pre-trained model, the feature vectors  are last layer features. Further, unless otherwise specified, we will assume  to be the logistic loss, i.e.  where . Finally, we will also assume that each step of optimization considers the whole batch, which greatly simplifies both the privacy analysis of algorithms and the experiments. Rest of this section includes descriptions of several iterative solvers of this minimization problem, both in non-private and private settings. Some of these methods rely on the fact that we are only interested in fine-tuning just the last layer, while others are more general. In the privacy analysis, we use zCDP (zero - Concentrated Differential Privacy) \cite{bun2016concentrated}, but we state our empirical results always with final privacy guarantee in -DP terms, as done in previous works. 


\subsection{DP-SGD}
Arguably the most popular approach to solving the above minimization problem in the non-private setting is Stochastic Gradient Descent (SGD).
In the full-batch setting, at every iteration, SGD performs the update:

where  denotes the learning rate used for for iteration .

DP-SGD is a private variant of this algorithm and the baseline in all our experiments. Computationally, in order to bound the sensitivity of each training example, \cite{abadi2016dpsgd} suggest computing a gradient for each example separately and clipping each to a maximum norm of  (a user-specified hyper-parameter):

where .

After summing the clipped example gradients, a noise vector sampled from a Gaussian distribution with standard deviation  is added, where  is a parameter that determines the privacy guarantee via the Gaussian mechanism. 


As shown in Algorithm \ref{alg:dpsgd}, once the gradient has been sanitized, we are free to use it to accumulate statistics (e.g. first or second moment estimates) which are typically useful with the optimization process. Algorithm \ref{alg:dpsgd} presents a generalized version of DP-SGD where the gradients get processed in the traditional DP-SGD fashion, and are then passed to a first-order optimizer as an input. This lets us instantiate DP versions of well-known first-order optimizers like SGD, Momentum and Adam. We employ DP-Adam in all our experiments. Finally, we omit the privacy analysis for the DP-SGD baseline since it is standard, but we do include in the appendix the details of the implementation we used for translation from privacy constraints to noise scale.

\begin{algorithm}[ht]
\caption{Generalized First Order Differentially Private Algorithm}
\begin{algorithmic}[1]
\REQUIRE Data set  with , loss function: , a first order optimizer , clipping norm: , 
number of iterations: , noise multiplier: 
\STATE Randomly initialize .
\FOR{}
{\STATE {}, where {}.\label{step:clip}}
\STATE 
{\STATE  {single step of first order optimization with gradient }\label{step:noiseDPSGD}}
\ENDFOR
{\STATE {\bf return}  or \label{eq:lastDPSGD}.}
\end{algorithmic}
\label{alg:dpsgd}
\end{algorithm}

\subsection{DP-Newton}
Since the optimization problem under consideration is a relatively simple convex problem, second-order DP algorithms can be viable. We first consider a privatized version of the popular Newton's method and denote it as DP-Newton. To control the sensitivity of each training example, one naive approach is to compute per-example Hessians and clip their norm, in a similar fashion to example gradient clipping in DP-SGD. But even in our last-layer fine-tuning setting, this can be prohibitively expensive. For instance, training on features extracted from ViT-G for ImageNet-1k fine-tuning, Hessian tensor (in a block diagonal form) is of size [1000, 1664, 1664] with approximately 2.8B entries. In order to avoid instantiating the Hessian for every training example, we instead choose to clip the feature vectors , then translate the clipping threshold into bounds on the Hessian and gradient norms. This is summarized in Algorithm \ref{alg:dpnewton}.

\begin{algorithm}[ht]
\caption{Differentially Private Newton's Method}
\begin{algorithmic}[1]

\REQUIRE Data set  with , loss function: , regularization coefficient , learning rate , clipping norm: , 
number of iterations: , noise multiplier: , bound on the second derivative of the loss: 
\STATE Clip all features:  for all  where {}.
\STATE Randomly initialize .
\FOR{}
\FOR{}
{\STATE {}}.
{\STATE {}}
{\STATE  where  indicates a -dimensional vector each of whose coordinates is an i.i.d. Gaussian with standard deviation .\label{line:gradN}}
{\STATE  where  indicates a  matrix each of whose coordinates is an i.i.d. Gaussian with standard deviation .\label{line:hessN}}
{\STATE  \label{line:newton-solution}}
\ENDFOR
\ENDFOR
{\STATE {\bf return} \label{eq:lastDPSGD}.}
\end{algorithmic}
\label{alg:dpnewton}
\end{algorithm}



\begin{thm}[Privacy guarantee for Algorithm \ref{alg:dpnewton}]
Suppose  is twice differentiable, and that for all ,  and . Then, Algorithm \ref{alg:dpnewton} satisfies -zCDP.
\end{thm}

Here  and  denote the first and second derivatives of  with respect to its first argument. We choose the bound on the first derivative to be  without loss of generality (via scaling of ). Note that the assumptions of the theorem are satisfied for the squared loss  with , as well as for the sigmoid cross-entropy loss (i.e. logistic regression) with . Indeed, , and  where . The first expression is bounded by  (since ) and the second expression is bounded by .

\begin{proof}
The crux of the proof is to ensure that Lines \ref{line:gradN} and \ref{line:hessN} individually satisfy -zCDP. The rest of the proof is just simple composition of zCDP \cite{bun2016concentrated} across  classes and  iterations. To see this, first let  and  be neighboring datasets and let  be the differing datapoint between  and . 

Now, notice that in the computation of , we have the following: . Thus, since , and :

From \eqref{eq:sens1_newton} it immediately follows that the computation of  for each  and each  satisfies -zCDP. Now, moving on to the sensitivity of  in Line \ref{line:hessN}, we have . Then, since  and , we have:

and \eqref{eq:sensHess} immediately implies that the computation of  for each  and  satisfies -zCDP. This completes the proof.
\end{proof}

\subsection{DP-LS}
One drawback of DP-SGD and DP-Newton is that each training example  contributes to the gradients (resp. Hessians) of all classes , so the sensitivity (and hence the scale of required noise) increases with the number of classes. This is visible in Algorithm \ref{alg:dpnewton} where the amount of noise added for privacy protection scales with  (Lines \ref{line:gradN} and \ref{line:hessN}). When the number of classes is large, this reduces signal-to-noise ratio and can hurt quality. In this section, we develop a method that aims to address this issue. We take inspiration from the matrix factorization literature, in which one can separate the loss function into the contribution of positive and negative classes. We assume that labels are binary (), and we consider the following quadratic loss function:

in other words, we take . The first term fits the positive labels (notice that this term vanishes when ), while the second term fits negative labels, and  is hyper-parameter that trades-off the two terms. This formulation was studied by \cite{hu2008ials} and enjoys remarkable empirical success \cite{koren2015collaborative}. It turns out that this method is also well-suited for privacy, as we shall discuss below.

By expanding the quadratic terms, we can write the loss as

where for all 

The exact solution is then given by

Notice that the solution  for class  depends on class-specific statistics (the matrix  and the vector ), as well as the global quantity . Algorithm \ref{alg:dpls} computes a private estimate of the solution by adding Gaussian noise to each of  (Lines \ref{line:ls-gram}, \ref{line:ls-lhs}, and \ref{line:ls-rhs} respectively), then solving the linear system using the noised statistics (Line \ref{line:ls-solution}). This is a variant of the popular sufficient statistics perturbation algorithm for DP linear regression. One crucial observation is that the noisy version of  is only computed once (Line \ref{line:ls-gram}), and reused for all classes. This allows to control the sensitivity of the solution w.r.t. each example; indeed, suppose example  only has one positive class , then that example only contributes to , , and . In particular, the sensitivity of the solution (and hence the amount of noise we need to add) does not scale with the total number of classes, only with the number of \emph{positive} classes per example. This is represented by the parameter  in Algorithm \ref{alg:dpls} ( is equal to  for single-class classification tasks, and even in multi-class tasks, we typically have ). We now give the formal privacy guarantee:

\begin{thm}[Privacy guarantee for Algorithm \ref{alg:dpls}]
Algorithm \ref{alg:dpls} satisfies -zCDP.
\end{thm}
\begin{proof}
First, let  be neighboring data sets that differ in the data point , and let  be the global statistic in \eqref{eq:dpls_stats} computed on data set . Then , and since  (due to clipping), , thus the computation of  (Line \ref{line:ls-gram}) is -zCDP. Now moving to the computation of : let  be the concatenation of all the class-specific statistics, i.e. . Notice that  if  and  otherwise. Since by assumption, the number of positive classes per example is bounded by , we have that , which is bounded by  due to clipping. Therefore the computation of all  combined (Line \ref{line:ls-lhs}) is -zCDP. Finally, by a similar argument, we have that , and the computation of all  combined (Line \ref{line:ls-rhs}) is -zCDP.

By simple composition of zCDP \cite{bun2016concentrated}, the algorithm is -zCDP.
\end{proof}

\begin{algorithm}[ht]
\caption{Differentially Private Least Squares}
\begin{algorithmic}[1]
\REQUIRE Data set  with , weight coefficient , regularization coefficient , maximum number of positive classes per example: , clipping norm: , noise multiplier: .
\STATE Clip all features:  for all .
{\STATE {}}, where  indicates a -matrix, each of whose coordinates is an i.i.d. Gaussian with standard deviation . \label{line:ls-gram}
\FOR{}
{\STATE {} \label{line:ls-lhs}}
\STATE . \label{line:ls-rhs}
{\STATE \label{line:ls-solution}}
\ENDFOR{}
{\STATE {\bf return} \label{eq:lastDPLS}.}
\end{algorithmic}
\label{alg:dpls}
\end{algorithm}

\subsection{DP-FC}
From our early experiments, we found that Newton's method with logistic regression performs better than least squares linear regression in non-private setting. But in the private setting, DP-LS outperforms DP-Newton, especially for lower values of epsilons. Notice that both are second-order methods, and both rely on estimating and inverting the Hessian (Line \ref{line:newton-solution} in Algorithm \ref{alg:dpnewton} and Line \ref{line:ls-solution} in Algorithm \ref{alg:dpls}); the main advantage of DP-LS is that the private hessian computation does not have to composed over classes or iterations. In this section, in order to mitigate the trade-off between the two methods, we introduce a method called Differentially Private SGD with Feature Covariance (DP-FC) which leverages covariance of features to make use of second-order information without paying the cost of composition over classes or iterations. Indeed, since feature covariance neither depends on the model parameters nor the prediction, it can be shared across both classes and iterations. This is described in Algorithm \ref{alg:dpfc}. The method can be interpreted as DP-SGD with preconditioning (where the approximate feature covariance  is used as preconditioner). We empirically observe that this leads to greatly reduced sensitivity compared to DP-Newton and significant improvements in overall metrics across all values of epsilons we tried.


\begin{algorithm}[ht]
\caption{Differentially Private SGD with Feature Covariance (DP-FC) Method}
\begin{algorithmic}[1]
\REQUIRE Data set  with , loss function: , learning rate , clipping norms:  and , number of iterations: , noise multiplier: 
\STATE Clip features for covariance computation:  for all  where .
\STATE Compute the feature covariance: .
\STATE  \label{line:Sigmafc}
\STATE Randomly initialize .
\FOR{}
{\STATE {} where {}.}
\STATE . \label{line:gradNfc}
{\STATE }
\ENDFOR
{\STATE {\bf return} \label{eq:lastDPSGD}.}
\end{algorithmic}
\label{alg:dpfc}
\end{algorithm}

\begin{thm}[Privacy guarantee for Algorithm \ref{alg:dpfc}]
Algorithm \ref{alg:dpfc} satisfies -zCDP.
\end{thm}

\begin{proof}
The crux of the proof is to ensure that the computation of  (Line \ref{line:Sigmafc}) and  (Line \ref{line:gradNfc}) individually satisfy -zCDP. The rest of the proof is by simple composition of zCDP \cite{bun2016concentrated} (once for the  computation, and  times for the  computation).

Let  and  be neighboring datasets and let  be the differing datapoint between  and . 

In the computation of , we have the following: . It immediately follows that the computation of  for each  satisfies -zCDP. Now, moving on to the sensitivity of . We have , since . It immediately follows that  satisfies -zCDP, which completes the proof.
\end{proof}


\section{Empirical Results}
In this section, we present private fine-tuning results on several Image Classification datasets using the optimization schemes described in Section \ref{sec:method}.  We start with details about the datasets we used, model variants and other fine-tuning hyperparameters. We also include much more details about pre-training and hyperparameter tuning in the appendix.

\begin{table*}[!h]
	\centering
	\small
	\begin{tabular}{lllc|cccccccc}
		\toprule
		\addlinespace[0.1cm]
		& & & & \multicolumn{8}{c}{Epsilon} \\
		\addlinespace[0.1cm]
		\hline
		\addlinespace[0.1cm]
		Pretraining & Method                      & Epochs & NP            & 0.01 & 0.05 & 0.1  & 0.5  & 1.0  & 2.0  & 4.0  & 8.0  \\
		\midrule
		      
		\multirow{8}{*}{ImageNet-21K} 
		&  \multirow{ 1}{*}{DP-LS}
		                                          & 1 &      75.5          & 71.2 & 71.5 & 71.5 & 73.0 & 73.6 & 74.1 & 74.6 & 75.0 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-Newton} & 1      & 72.7          & 70.2 & 71.3 & 71.3 & 71.4 & 71.6 & 71.7 & 72.0 & 72.2 \\
		            &                             & 10     & 78.2          & 66.5 & 71.0 & 71.4 & 71.9 & 71.9 & 72.3 & 73.0 & 74.2 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-FC}     & 1      & 72.9          & \textbf{71.9} & \textbf{72.1} & \textbf{72.5} & 72.9 & 72.9 & 72.9 & 72.9 & 72.9 \\
		            &                             & 10     & 77.1          & 71.8 & 71.9 & 72.1 & \textbf{75.4} & \textbf{76.3} & \textbf{76.8} & \textbf{77.0} & \textbf{77.1} \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 3}{*}{DP-Adam}   & 1      & 73.8          & -   & -   & -   & -   & 39.1 & 54.2 & 63.3 & 68.3 \\
		            &                             & 10     & 76.1          & -   & -   & 22.1 & 52.9 & 62.3 & 66.6 & 69.5 & 71.0 \\
		            &                             & 100    & \textbf{78.3} & -   & -   & -   & 47.2 & 59.1 & 65.8 & 68.8 & 70.4 \\


		\addlinespace[0.2cm]
		\hline
		\addlinespace[0.2cm]
		    
		\multirow{8}{*}{JFT} & \multirow{ 1}{*}{DP-LS}
		                                          & 1      & 87.5          & \textbf{82.4} & \textbf{83.8} & 84.1 & 85.8 & 86.2 & 86.4 & 86.6 & 86.7 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-Newton} & 1      & 85.9          & 77.8 & 80.3 & 81.0 & 82.9 & 83.6 & 84.0 & 84.5 & 84.9 \\
		            &                             & 10     & \textbf{88.9} & 76.0 & 79.7 & 80.1 & 81.8 & 82.9 & 83.1 & 84.7 & 85.3 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-FC}     & 1      & 85.8          & 82.1 & 83.8 & \textbf{84.3} & 85.1 & 85.4 & 85.5 & 85.6 & 85.6 \\
		            &                             & 10     & 88.4          & 81.0 & 83.1 & 83.7 & \textbf{86.1} & \textbf{86.8} & \textbf{87.4} & \textbf{87.8} & \textbf{88.0} \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 3}{*}{DP-Adam}   & 1      & 84.8          & -   & 68.7 & 74.0 & 82.1 & 83.7 & 84.4 & 84.8 & 84.8 \\
		            &                             & 10     & 87.3          & -   & 70.5 & 75.6 & 83.4 & 84.9 & 85.6 & 86.3 & 86.7 \\
		            &                             & 100    & 88.6          & -   & 49.7 & 64.4 & 78.3 & 81.5 & 83.9 & 85.4 & 86.3 \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of Top-1 test accuracies when privately fine-tuning on Imagenet-1K. We denote accuracy  20\% with the symbol `-'. When pre-trained with JFT, we observe that DP-FC performs best for epsilon values ranging from [0.1, 8.0] whereas DP-LS is best for even lower epsilons. In the case of pre-training with ImageNet-21k, we find that DP-FC (10 epochs) outperforms all other methods across the board.}
	\label{tab:imagenet}
\end{table*}

\begin{table*}[!h]
	\centering
	\small
	\begin{tabular}{llll|cccccccc}
		\toprule
		\addlinespace[0.1cm]
		& & & & \multicolumn{8}{c}{Epsilon} \\
		\addlinespace[0.1cm]
		\hline
		\addlinespace[0.1cm]
		Pretraining          & Method                      & Epochs & Non-Private & 0.01 & 0.05 & 0.1  & 0.5  & 1.0  & 2.0  & 4.0  & 8.0  \\
		\midrule
		        
		\multirow{8}{*}{ImageNet-1K}   &  \multirow{ 1}{*}{DP-LS}
		                                                   & 1      & 91.3        & 81.1 & 83.7 & 84.3 & 86.3 & 87.7 & 88.7 & 89.6 & 90.3 \\ \addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 2}{*}{DP-Newton} & 1      & 91.0        & 77.4 & 79.6 & 80.5 & 84.2 & 85.7 & 87.1 & 88.4 & 89.2 \\
		                     &                             & 10     & \textbf{91.6}        & 79.7 & 81.4 & 82.7 & 86.1 & 87.5 & 89.0 & 88.6 & 90.2 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 2}{*}{DP-FC}     & 1      & 91.0        & 79.0 & 81.5 & 83.1 & 86.6 & 88.0 & 88.9 & 89.7 & 90.3 \\
		                     &                             & 10     & 91.1        & \textbf{81.2} & \textbf{84.7} & \textbf{86.5} & \textbf{89.5} & \textbf{90.4} & \textbf{91.0} & \textbf{91.1} & \textbf{91.1} \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 3}{*}{DP-Adam}   & 1      & 77.9        & 52.6 & 74.3 & 76.7 & 78.2 & 78.2 & 77.8 & 77.6 & 77.8 \\
		                     &                             & 10     & 82.3        & 56.4 & 78.3 & 76.7 & 82.0 & 82.6 & 82.4 & 82.2 & 82.7 \\ &                             & 100    & 87.5        & 40.9 & 63.8 & 72.3 & 83.6 & 86.8 & 87.4 & 87.4 & 87.6 \\ 

		                
		\addlinespace[0.2cm]
		\hline
		\addlinespace[0.2cm]
		        
		\multirow{8}{*}{ImageNet-21K} &    \multirow{ 1}{*}{DP-LS}
		                                                    & 1     & 96.5        & 94.5 & 95.2 & 95.4 & 95.8 & 96.0 & 95.5 & 96.2 & 96.3 \\ \addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 2}{*}{DP-Newton} & 1      & 96.5        & 94.3 & 94.9 & 95.2 & 95.6 & 95.7 & 96.0 & 96.1 & 96.2 \\
		                     &                             & 10     & 96.5        & 94.8 & 95.2 & 95.4 & 95.6 & 95.9 & 96.0 & 96.2 & 96.2 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 2}{*}{DP-FC}     & 1      & 96.6        & 94.6 & 95.2 & 95.5 & 95.9 & 96.0 & 96.2 & 96.3 & 96.3 \\
		                     &                             & 10     & \textbf{96.6}        & \textbf{94.8} & \textbf{95.6} & \textbf{95.8} & \textbf{96.1} & \textbf{96.3} & \textbf{96.5} & \textbf{96.5} & \textbf{96.5} \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 3}{*}{DP-Adam}   & 1      & 95.2        & 90.0 & 94.7 & 95.1 & 95.1 & 95.1 & 95.1 & 95.1 & 95.2 \\
		                     &                             & 10     & 96.1        & 83.8 & 94.9 & 95.5 & 95.8 & 95.8 & 95.8 & 95.9 & 96.0 \\ &                             & 100    & 96.5        & 64.0 & 90.0 & 93.3 & 95.5 & 95.7 & 95.9 & 96.1 & 96.2 \\ 

		\addlinespace[0.2cm]
		\hline
		\addlinespace[0.2cm]
		        
		\multirow{8}{*}{JFT} & \multirow{ 1}{*}{DP-LS}     & 1      & 98.9        & \textbf{97.4} & 98.2 & 98.4 & 98.4 & 98.6 & 98.8 & 98.8 & 98.9 \\ \addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 2}{*}{DP-Newton} & 1      & 98.9        & 94.1 & 96.5 & 97.2 & 98.1 & 98.3 & 98.5 & 98.7 & 98.8 \\
		                     &                             & 10     & 98.9        & 95.9 & 97.5 & 97.9 & 98.2 & 98.5 & 98.6 & 98.4 & 98.8 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 2}{*}{DP-FC}     & 1      & \textbf{98.9}        & 95.2 & 97.6 & 97.9 & 98.5 & 98.6 & 98.8 & 98.8 & \textbf{98.9} \\
		                     &                             & 10     & 98.9        & 97.3 & \textbf{98.2} & \textbf{98.4} & \textbf{98.8} & \textbf{98.8} & \textbf{98.9} & \textbf{98.9} & 98.9 \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		                     & \multirow{ 3}{*}{DP-Adam}   & 1      & 97.5        & 93.5 & 97.0 & 97.5 & 97.5 & 97.6 & 97.6 & 97.6 & 97.6 \\
		                     &                             & 10     & 98.7        & 87.6 & 97.7 & 98.1 & 98.5 & 98.6 & 98.7 & 98.7 & 98.7 \\ &                             & 100    & 98.9        & 79.2 & 93.2 & 96.3 & 98.3 & 98.6 & 98.6 & 98.8 & 98.8 \\ \bottomrule
	\end{tabular}
	\caption{Comparison of Top-1 test accuracies when private finetuning on CIFAR-10. We denote accuracy  20\% with the symbol `-'. Similar to other datasets, DP-FC (10 epochs) outperform all other methods almost across the board with a single exception of epsilon of 0.01 when pre-training with JFT were DP-LS performs slightly better.}
	\label{tab:cifar10}
\end{table*}

\begin{table*}[!h]
	\centering
	\small
	\begin{tabular}{llll|cccccccc}
		\toprule
		\addlinespace[0.1cm]
		& & & & \multicolumn{8}{c}{Epsilon} \\
		\addlinespace[0.1cm]
		\hline
		\addlinespace[0.1cm]
		Pretraining & Method                      & Epochs & Non-Private & 0.01 & 0.05 & 0.1  & 0.5  & 1.0  & 2.0  & 4.0  & 8.0                     \\
		\midrule
		\multirow{8}{*}{ImageNet-1K} &      \multirow{ 1}{*}{DP-LS}
		& 1 & 71.9 & \textbf{49.2} & \textbf{51.8} & \textbf{53.9} & 57.6 & 60.0 & 62.5 & 65.4 & 67.5 \\ \addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-Newton} & 1      & 68.8        & 44.6 & 48.6 & 49.3 & 50.2 & 51.2 & 54.4 & 57.2 & 59.5                    \\
		            &                             & 10     & \textbf{72.1}        & 36.4 & 48.2 & 49.8 & 50.7 & 51.8 & 53.5 & 56.1 & 59.5                    \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-FC}     & 1      & 68.7        & 49.2 & 50.2 & 52.1 & 58.3 & 60.8 & 62.8 & 64.6 & 66.1                    \\
		            &                             & 10     & 71.4        & 48.9 & 49.7 & 53.7 & \textbf{61.4} & \textbf{64.9} & \textbf{68.2} & \textbf{69.8} & \textbf{70.4}                    \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 3}{*}{DP-Adam}   & 1      & 52.1        & -   & 26.0 & 34.6 & 47.6 & 51.2 & 51.7 & 51.7 & 52.2                    \\
		            &                             & 10     & 57.3        & -   & 20.0 & 33.5 & 51.2 & 54.4 & 55.6 & 57.5 & 56.3                    \\
		            &                             & 100    & 67.9        & -   & -   & -   & 36.3 & 40.2 & 51.5 & 58.4 & 63.7                    \\
		\addlinespace[0.2cm]
		\hline
		\addlinespace[0.2cm]
		        
		        
		\multirow{8}{*}{ImageNet-21K} &        \multirow{ 1}{*}{DP-LS}
		                                            & 1 & 83.9 & \textbf{77.2} & \textbf{77.7} & 78.1 & 79.8 & 80.7 & 81.4 & 81.9 & 82.4 \\ \addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-Newton} & 1      & 83.0        & 76.5 & 77.2 & 77.2 & 77.6 & 78.3 & 79.0 & 79.5 & 80.5                    \\
		            &                             & 10     & 83.0        & 73.6 & 77.4 & 77.8 & 78.3 & 78.9 & 79.6 & 80.4 & 81.4                    \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-FC}     & 1      & 83.0        & 77.1 & 77.5 & 78.2 & 80.0 & 80.9 & 81.6 & 81.9 & 82.4                    \\
		            &                             & 10     & 84.3        & 77.1 & 77.1 & \textbf{78.5} & \textbf{81.6} & \textbf{82.7} & \textbf{83.3} & \textbf{83.8} & \textbf{83.9}                    \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 3}{*}{DP-Adam}   & 1      & 79.9        & 27.6 & 67.0 & 72.7 & 78.3 & 79.5 & 79.7 & 79.7 & 79.7                    \\
		            &                             & 10     & 82.0        & -   & 55.7 & 67.5 & 78.6 & 80.7 & 81.5 & 81.5 & 81.6                    \\
		            &                             & 100    & \textbf{84.6}        & -   & 29.7 & 45.2 & 71.3 & 76.2 & 79.5 & 81.1 & 82.2                    \\
		        
		\addlinespace[0.2cm]
		\hline
		\addlinespace[0.2cm]
		        
		\multirow{8}{*}{JFT} & \multirow{ 1}{*}{DP-LS}
		                                        & 1 & \textbf{90.6} & \textbf{74.9} & \textbf{80.3} & \textbf{82.5} & 85.5 & 86.4 & 87.7 & 88.4 & 88.9 \\ \addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-Newton} & 1      & 89.9        & 73.1 & 72.6 & 73.4 & 78.5 & 80.9 & 82.8 & 84.6 & 85.9                    \\
		            &                             & 10     & 89.9        & 69.6 & 75.7 & 76.7 & 77.4 & 77.6 & 80.0 & 82.9 & 85.4                    \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 2}{*}{DP-FC}     & 1      & 89.9        & 73.5 & 78.6 & 81.0 & 85.2 & 86.7 & 87.7 & 88.3 & 88.6                    \\
		            &                             & 10     & 90.1        & 72.1 & 75.9 & 79.0 & \textbf{86.2} & \textbf{88.1} & \textbf{89.0} & \textbf{90.0} & \textbf{90.1}                    \\
		\addlinespace[0.1cm]
\addlinespace[0.1cm]
		            & \multirow{ 3}{*}{DP-Adam}   & 1      & 83.5        & 27.9 & 61.8 & 71.3 & 79.7 & 82.1 & 83.4 & 83.5 & 83.5                    \\
		            &                             & 10     & 88.2        & 21.9 & 60.2 & 69.7 & 81.9 & 83.9 & 86.2 & 86.8 & 87.8                    \\
		            &                             & 100    & 90.0        & -   & 29.4 & 50.5 & 73.7 & 78.7 & 83.1 & 86.1 & 88.0                    \\
\bottomrule
	\end{tabular}
	\caption{Comparison of Top-1 test accuracies when private finetuning on CIFAR-100. We denote accuracy  20\% with the symbol `-'.  Similar to other datasets, DP-FC outperforms all other methods for moderate privacy budgets whereas DP-LS performs slightly better for very strict privacy guarantees depending on the pre-training dataset.}
	\label{tab:cifar100}
\end{table*}
\textbf{Datasets.} 
We use 3 datasets for private finetuning, namely 1) ILSVRC-2012 ImageNet dataset \citep{deng2009imagenet} with 1k classes and 1.3M images (we refer to it as ImageNet in what follows) 2) CIFAR-10 and 3) CIFAR-100. We also refer to these as the private dataset for which we want a privacy guarantee. For pre-training, we rely on JFT-3B, ImageNet-21k and ImageNet-1K (as done in \cite{zhai2021scaling}). For JFT, we intentionally chose a slightly smaller version of the dataset i.e. JFT-3B instead of JFT-4B, enabling us to exactly follow \cite{zhai2021scaling} and thus lowering the risk of the project. Also note that, as done in recent works, none of our finetuning datasets in reality are sensitive datasets: we are only simulating a public/private dataset split only for demonstration purposes \citep{kurakin2022training, mehta2022large, dm_transfer_2022}. The JFT datasets are not publicly available but have been used extensively as a pre-training dataset in the non-private setting to obtain state-of-the-art results \citep{dosovitskiy2021an_vit,brock2021characterizing,tolstikhin2021mlpmixer,zhai2021scaling}. Similar to \cite{mehta2022large,dm_transfer_2022}, to make sure that our simulated ``public'' and ``private'' datasets capture a practical scenario, we carefully de-duplicate our pre-training datasets w.r.t. \textbf{all} splits of our finetuning datasets \citep{bit-paper,dosovitskiy2021an_vit}. More details about this process can be found in the appendix.

\textbf{Model variants.} We evaluate the transfer learning capabilities of the Vision Transformer (ViT) \citep{dosovitskiy2021an_vit} model family in our study. We follow the standard notation to indicate the model size and the input patch size, for example, ViT-B/32 means the ``Base" variant with 32x32 input patch size. Note that for ViT, compute requirements scales up as we reduce the patch size. We obtained features from ViT-G/14 model pre-trained on JFT-3B \citep{zhai2021scaling}, and ViT-B/16 pre-trained on ImageNet-21k and ImageNet-1k. \citep{augreg_steiner2021train}.

\textbf{Training details.} For our private fine-tuning experiments, to limit other confounding factors we always train in full batch setting. Also, similar to \cite{mehta2022large}, we initialize the last layer weights to zero (or a small value) for all our experiments.

Next, we present our main set of private fine-tuning results and core observations on all 3 datasets, namely ImageNet-1k (Table \ref{tab:imagenet}), CIFAR-10 (Table \ref{tab:cifar10}) and CIFAR-100 (Table \ref{tab:cifar100}).

\begin{figure}[h!]
  \centering
      \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{imgs/cifar10_more.pdf}
    \caption{CIFAR-10}
  \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{imgs/cifar100_more.pdf}
    \caption{CIFAR-100}
  \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{imgs/I1K_more.pdf}
    \caption{ImageNet-1K}
    \end{subfigure}
  \caption{Comparison of top-1 accuracies with private fine-tuning using DP-FC method on all 3 datasets across a range of epsilons. We observe that better pre-training helps even more for lower values of epsilon (stricter privacy guarantee).}
  \label{fig:better_pretraining}
\end{figure}
\subsection{Better pre-training continues to improve private fine-tuning performance}
We evaluate private fine-tuning performance on features extracted from pre-trained models of 2 sizes i.e. ViT-G/14 and ViT-B/16, pre-trained on 3 different datasets, namely JFT-3B, ImageNet-21K and ImageNet-1K. We do this to quantify the extent to which the representation quality is improved by increasing pre-training dataset in combination of model size. As shown in Figure \ref{fig:better_pretraining}, as the model size and pre-training dataset size is increased, we continue to see improvement in downstream private fine-tuning performance for all 3 datasets we consider. In addition, we make following observations from our results.

First, \textbf{better pre-training can help even more at stricter privacy budgets}. As shown in Figure \ref{fig:better_pretraining}, for both CIFAR-10 and CIFAR-100, when comparing features extracted from ViT-B/16 pre-trained with ImageNet-1K and features from ViT-G/14 pre-trained with JFT, the improvement in performance at  is larger than . We see a similar trend at even lower epsilons in Table \ref{tab:cifar10} and Table \ref{tab:cifar100}.

Second, \textbf{features extracted from off-the-shelf pre-trained models can suffice for DP}. Except for the fact that we deduplicate all splits of our pre-training dataset with our fine-tuning datasets, we use the exact same procedure used to pre-train large vision models. This suggests that, in practice, a recipe where features extracted from large off-the-shelf vision model used to privately fine-tune a classifier can be quite effective for DP performance. Since there is no need for a special pre-trained model for use in DP, this considerably reduces the cost of training private image classifiers.

Lastly, \textbf{private fine-tuning of high quality features closes the gap between private and non-private performance considerably}. In the non-private setting, \cite{zhai2021scaling} obtain an impressive 90.45\% top-1 accuracy by fine-tuning the whole ViT-G/14 model on ImageNet-1K dataset. We observe that even by fine-tuning just the last layer, we can obtain as much as 88.9\% top-1 accuracy on ImageNet-1K from the same sized model. Thus the marginal benefit of fine-tuning the whole model is 2\% even in the non-private case. In the private case, fine-tuning of pre-extracted features with DP-FC at  leads to state of the art 88\% top-1 accuracy. On ViT-G/14, this represents  difference between best non-private and private performance. This is also just 3\% below the best non-private accuracy of 91\% on ImageNet-1K \cite{yu2022coca}.



\subsection{Better optimizers improve privacy-utility trade-off}






We observe that the choice of optimizer can have a significant impact on the privacy-utility trade-off.

First, \textbf{optimizers that work well in the large batch regime are better for private training}. Even in the non-private setting, it is known that batch size is intimately tied to the optimization procedure and can lead to suboptimal use of resources if increased beyond a certain point while keeping number of epochs constant \cite{goyal2017largebatch,you2017largebatch,lamb}. The maximum batch size which can be used without jeopardizing the utility-compute trade-off is also heavily dependent on the choice of optimizer \cite{zhang2019algorithmic}. Though in the private setting, several works have observed that increasing the batch size leads to improved privacy-utility trade-off \cite{doorman_notallnoise_2021,li_large_batch,hoory_2021_llm}. It is also empirically observed that the utility in large batch regime can be further improved by leveraging optimizers which work well in large batch regime, such as DP-Adam or DP-LAMB \cite{anil21_dpbert,mehta2022large,bu2022scalable_oldsota}. Second-order methods such as LS or Newton are also well-suited to the large-batch regime (they were developed and are typically used in the full-batch setting).


Second, \textbf{optimizers with faster convergence rates are advantageous in the private setting}, because they can reduce the number of epochs required for convergence. Indeed, in typical privacy analysis of iterative methods, the analysis works by composition over iterations, which means that the privacy penalty scales with the number of visitations of the data. Thus, in addition to the computational benefit, any improvement in convergence rate directly helps training with privacy because it requires less noise to be added under the same privacy constraints. We empirically observe that this reduction in noise in the case of DP-FC, in combination with sharing of the privatized Feature Covariance across classes and iterates, allow us to obtain better results with 10 epochs compared to even when using 100 epochs with DP-SGD.

\section{Related Work}


Differential privacy \cite{dwork2006dp} is a popular method to guarantee privacy in a quantifiable way in many data-driven applications. To achieve differential privacy in machine learning, tasks practitioners commonly train models with privatized variation of gradient descent, called DP-SGD \citep{song2013stochastic,Bassily_2014,abadi2016dpsgd}. 

Despite theoretical guarantees, differentially private training has two major drawbacks which limits its wide adoption. First of all, differentially private training is slower compared to regular SGD. Training with DP-SGD is notably different from non-private training where the forward pass can be vectorized and only a single pre-accumulated gradient need be calculated and used per mini-batch. If implemented naively, this step alone increases the computational cost of DP training proportional to the batch size for a single step and the dimensionality of the model. Indeed, to address this, in several deep learning architectures, either its feasible to vectorize the computation \citep{subramani2020fastdpsgd} or it is sometimes possible to bound the sensitivity of each example without calculating the gradient for every example separately, leading to a dramatic cost-reduction both in terms of memory and compute \citep{Goodfellow2015EfficientPG,li2022llmdp, bu2022scalable_oldsota,Bu2022DifferentiallyPO}. Although, in our work this cost is minimal since we only train the last layer. We do note that computational burden can still be an issue for optimization schemes like DP-Newton in our work where we had to resort to feature clipping in order to produce bounds on sensitivity of the gradient and the hessian.

In addition to the computational cost, model trained with differentially privacy usually suffer from so-called ``utility loss'', which means that accuracy (or any other quality metric) is worse (and sometimes significantly worse) compared to accuracy of non-private model \citep{doorman_notallnoise_2021,klause2022differentially}. Over the years, several lines of improvements have been proposed including adaptive clipping \citep{pichapati2019adaclip, thakkar2019differentially,Bu2022AutomaticCD,Golatkar2022}, param-efficient finetuning \cite{Yu2022DifferentiallyPF,mehta2022large,Bu2022DifferentiallyPB,Cattan2022FineTuningWD,Li2022WhenDD} and even leveraging intermediate checkpoints \citep{dm_transfer_2022,Shejwalkar2022RecyclingSI}. One of the recent trends to improve utility of private models significantly involves various ideas related to transfer learning where previous works demonstrate improved performance in the setting where we have access to a large public or non-sensitive dataset of the same modality as the private data \cite{kurakin2022training,dm_transfer_2022,mehta2022large,tramer2021dpfeatures,Yu2022DifferentiallyPF,li2022llmdp,kurakin2022training,hoory_2021_llm}. Our work also leverages large pre-trained models in order to obtain high-quality features for private finetuning. In addition, similar to the works that focus on studying and reducing dimensionality of the model in the context of DP \citep{Li2022WhenDD,Golatkar2022,yu_YZCL21,zhang2021wide,zhou2020bypassing}, we focus solely on learning just the last layer privately, which can be seen as an implicit way to reduce dimensionality. 

In the context of differential privacy, several recent papers also advocate the use of large batch sizes in order to improve the privacy-utility tradeoff \citep{mehta2022large,McMahan2018learning,anil21_dpbert,doorman_notallnoise_2021,hoory_2021_llm,liu2021mldoctor,kurakin2022training}. Even though this work explicitly does not explore the affect changing the batch size, the fact that we are able to obtain state of the art results in the full batch setting may point to the effectiveness of large batch sizes in the context of DP.

Further, our work also zooms in on differentially private linear and logistic regression. Several existing works have studied differential private convex optimization \citep{chaudhuri2011differentially,kifer2012private,song2013stochastic,Bassily_2014, wu2016bolton, mcmahan2017learning,bassily2019private,iyengar2019towards, feldman2020hiding,bassily2020stability,song2020characterizing,andrew2021differentially}. There is also a growing interest in the special case of linear regression \citep{smith2017interaction,pmlr-v98-sheffet19a,liu2021differential,cai2021cost,pmlr-v178-varshney22a} and even second order methods in the context of differential privacy \cite{avellamedina2021differentially,chien2021private}. In this work, we illustrate empirically the extent to which second order methods can help in DP. It would be quite interesting to see how other popular second order methods like Shampoo \cite{gupta2018shampoo,anil2020scalable} would fare in the context of DP.







 
\section{Limitations}
This work leverages a large properietary dataset called JFT-3B to pre-train ViT-G/14 model in order to illustrate the benefits of scale on differential privacy with transfer learning. In order to make our work more generalizable and reproducible, we also include results with models pre-trained with ImageNet-21k and ImageNet-1k. 

Another limitation of our work may be the fact that our pre-training dataset is largely in-distribution with the private fine-tuning datasets. We would like to argue that, in practice, this is still valuable since it illustrates the effectiveness of the approach, and helps estimate the utility of gathering a public dataset to pre-train on, given a sensitive dataset that one wants privacy guarantee over. Finally, out of distribution performance is an interesting research question even in the non-private setting and its exploration in the context of privacy can be a direction of very valuable future work. 


In terms of societal impact, the biggest cost of this work is training the largest ViT-G model on a large dataset and its energy impact. However, we argue that our results ultimately point towards amortizing and increasingly leveraging already trained models for high-performance DP training, and thus potentially reducing the overall energy consumption.

\section{Conclusion}
In this work, we focus on private finetuning of image classification datasets using features extracted from a pre-trained model. Given that, with privacy, finetuning just on the features is significantly cheaper than finetuning the full model, we systematically explore optimization schemes which are perceived to be expensive in very high-dimensional settings and its effect on private finetuning performance. As illustrated on 3 finetuning datasets i.e. ImageNet-1k, CIFAR-10 and CIFAR-100, we find that DP-LS (Least Squares) outperforms DP-SGD with logistic regression, especially for lower values of epsilons. Given the intuition that 2nd order information may be the reason for superior performance of DP-LS, we also explore Newton's method with Logistic Loss. Noticing that the amount of noise required by Newton's method scales with the number of classes and iterations, we introduce an optimization scheme called DP-FC which replaces the hessian by the feature covariance matrix, that can be shared across classes and iterations. Using this insight, we demonstrate that it is indeed possible to get state of the art results by just finetuning the last layer of a pre-trained model with privacy constraints. Most remarkably, we obtain top-1 accuracy of 88\% on ImageNet-1K under DP guarantee of (8, ) and 84.3\% under (0.1, ). All of our results rely on leveraging well-understood procedures for transfer learning on standard architectures. We hope that our work significantly reduces the barrier in training private models.


\bibliography{main}
\bibliographystyle{plainnat}

\appendix
\section{Appendix}
\section{Algorithmic details}



\subsection{Privacy Analysis details for DP-SGD}
The privacy parameters  are functions of , , , , and the total number of iterations . DP-SGD algorithm involves setting the right clipping norm  and the noise multiplier  given a privacy budget, batch and dataset size. The  guarantee is computed by analysis of the Gaussian Mechanism with privacy amplification by subsampling and composition across across iterations \citep{KLNRS,Bassily_2014,abadi2016dpsgd,mironov2017renyi,mcmahan2017learning, mironov2019rnyi,Erlingsson_amplification_2019,zhu2019poission,feldman2020hiding,wang_subsampled_2020}.  Our implementation relies on Tensorflow Privacy \footnote{\url{https://github.com/tensorflow/privacy}} codebase for conversion of  and clipping norm  to/from noise multiplier .  We rely on the default R\'enyi accountant implementation already open-sourced as part of Tensorflow Privacy library.

To put the epsilon-delta values in context, privacy guarantee for let's say  on ImageNet-1K satisfies a much stronger property of zCDP  1 (0.154 for ) which is by now an industry standard.

\section{Pre-training Details}
We conduct all our experiments in Jax \citep{jax2018github,XLA} is framework that leverages just-in-time compilation using XLA\footnote{https://www.tensorflow.org/xla} and does auto-vectorization of the backward pass. We leverage this functionality throughout our experiments. Finally, we conduct our experiments on TPUv4 architecture.

\subsection{Pre-training with JFT-3B}
\textbf{Dataset.}
Contrary to \cite{mehta2022large,dm_transfer_2022}, we use a smaller version of JFT, namely JFT-3B (instead of JFT-4B) for our pre-training. We do this to lower the risk of the project and follow \cite{zhai2021scaling} exactly for pre-training ViT-G/14 model. JFT-3B dataset consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semiautomatic pipeline. As done previously, we ignore the hierarchical aspect of the labels and use only the assigned labels as targets for multi-label classification via a sigmoid cross-entropy loss.

\textbf{Deduplication.} In order to both not inflate our results and break privacy guarantee offered by fine-tuning privately on ImageNet, we extend the deduplication process proposed by \cite{bit-paper} and deduplicate both JFT-3B with respect to all splits of ImageNet. We use a model based deduplication system which removes both exact and near-duplicates across common image transformation like crop, shift, resize etc.

\textbf{Hyperparameters.}
At the pre-training stage, we follow \cite{zhai2021scaling} exactly and stick with the common practice of employing Adafactor optimizer with  and , with a batch size of 32768, dropout rate of 0.0, clip global norm of 1, and a high weight decay of 3.0 for the ``head" and 0.03 for the ``body". In addition, we remove the additional  token to save memory. Finally, all the models are pre-trained at resolution [224, 224], with inception crop followed by random horizontal flip pre-process. We also use reciprocal square-root schedule with a linear learning rate warmup of 10k steps. Finally, ViT-G/14 model was pre-trained using 2048 TPUv3 chips.

\subsection{Pre-training with ImageNet21k and ImageNet1k}
\label{sec:i21k}

\textbf{Datasets.} ImageNet-21k is a superset of ImageNet-k with 21k classes and 14M images \citep{deng2009imagenet}. Similar to before, in order to both not inflate our results and break privacy guarantee, we extend the deduplication process proposed by \cite{bit-paper} and deduplicate ImageNet-21k with respect to all splits of ImageNet-1k, CIFAR-10 and CIFAR-100. Similarly, we deduplicate ImageNet-1k with respect to all splits of CIFAR-10 and CIFAR-100.

\textbf{Hyperparameters.}
At the pre-training stage, we stick with the common practice of employing Adam optimizer (even for ResNet) with  0.9 and  0.999, with a batch size of 4096. Unlike pre-training with JFT dataset, we follow recommendations from \cite{augreg_steiner2021train} to use AugReg strategy where we lower the weight decay to 0.1 (which gets multiplied by the learning rate) and don't use dropout but instead use data augmentation strategy called \textbf{medium1} which combines Mixup with  \citep{zhang2017mixup} and RandAugment with  and  \citep{randugment2020}. We also use linear learning rate warmup until 10k steps and linearly decay it until the end. Our model is pre-trained with 224x224-sized images.

\begin{table}[H]
    \centering
\begin{tabular}{c|c|c|c|c}
    \toprule
        Model & Dataset & Epochs & Base  & TPU v4 hours\\
        \midrule
      ViT-B/16 & ImageNet-21k & 300 &  & 2.7k \\
      ViT-B/16 & Imagenet-1K & 300 &  & 0.4k \\
         \bottomrule
         \addlinespace[0.3cm]
    \end{tabular}
        \caption{Pre-training hyperparams. We used batch size of 4096, learning rate warmup of 10k steps and then linear decay. Additionally, we set dropout rate to 0.0, clip global norm to 1 and weight decay to 0.0001. We use images of resolution 224x224. Note that we intentionally keep the model size the same to illustrate the effect of larger pre-training dataset and its effect on private fine-tuning.}
            \label{tab:learning_rate}
\end{table}

\section{Finetuning Details}

\subsection{Datasets}

\textbf{ImageNet-1k} We fine-tune on ImageNet train split and present the Top-1 accuracies we obtain from the official test split. Following \cite{mehta2022large}, we used images of input resolution 256x256 which is central cropped from a resolution of 384x384. Note that this is slightly lower resolution and without Inception Crop \citep{Szegedy2015GoingDW_inception} which is typically done in non-private setting. Finally, for training with DP, we fixed  to be 8e-7.

\textbf{CIFAR-10 and CIFAR-100} Similar to above, we fine-tune on train split and present the Top-1 accuracies we obtain from the official test split. We also changed the input resolution to 256x256 which is central cropped from an image of resolution 384x384. Again, this may look a little unusual at first for CIFAR-10 and CIFAR-100 since the original resolution of the images is 32x32. But we first upsample them to 384x384 and then central crop them. We found that using higher resolution images made a big difference in performance (even in non-private setting), especially when using features from a pre-trained model. Finally, for training with DP, we fixed  to be 1e-5.

\subsection{DP-Adam hyperparameters}

\begin{table}[H]
    \centering
\label{tab:hparams_lamb}
    \begin{tabular}{cccccc}
    \toprule
        Model & Pre-training DS & Fine-tuning DS &  &  & DP Clipping Norm ()  \\
        \midrule
      ViT-G/14 & JFT-3B & ImageNet-1k &  &  & 1.0 \\
           ViT-G/14 & JFT-3B & CIFAR-10 &  &  & 0.005 \\
      ViT-G/14 & JFT-3B & CIFAR-100 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & ImageNet-1k &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-100 &  &  & 0.005 \\
           ViT-B/16 & ImageNet-1k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-1k & CIFAR-100 &  &  & 0.005 \\
         \bottomrule
         \addlinespace[0.3cm]
    \end{tabular}
        \caption{Fine-tuning hyperparams for DP-Adam. All models are trained in full-batch setting with a constant learning rate and no dropout. When training the models with DP, we replace the global clipping with per example clipping norm as specified in the table. Following \cite{mehta2022large}, we set initial weights to 0.0, bias to -10.0 and train with sigmoid cross-entropy loss. Note that we employed a Bayesian optimization package called Vizier \citep{vizier, oss_vizier} and used a total of 200 trials for jointly tuning both the learning rate and weight decay as specified in the table.}
\end{table}

\subsection{DP-LS hyperparameters}

\begin{table}[H]
    \centering
\label{tab:hparams_lamb}
    \begin{tabular}{cccccc}
    \toprule
        Model & Pre-training DS & Fine-tuning DS &  &  & DP Clipping Norm ()   \\
        \midrule
      ViT-G/14 & JFT-3B & ImageNet-1k &  &  & 1.0 \\
           ViT-G/14 & JFT-3B & CIFAR-10 &  &  & 0.005 \\
      ViT-G/14 & JFT-3B & CIFAR-100 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & ImageNet-1k &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-100 &  &  & 0.005 \\
           ViT-B/16 & ImageNet-1k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-1k & CIFAR-100 &  &  & 0.005 \\
         \bottomrule
         \addlinespace[0.3cm]
    \end{tabular}
        \caption{Fine-tuning hyperparams for DP-LS. All models are trained in full-batch setting. When training the models with DP, we replace the global clipping with per example clipping norm as specified in the table. Specific to DP-LS, we clip the RHS with  and the gramians with . Interestingly, least squares is invariant to the starting weights which takes an important confounding factor away from the private training procedure. Similar to DP-Adam, we employed a Bayesian optimization package called Vizier \citep{vizier, oss_vizier} and used a total of 200 trials for jointly tuning both the  and weight decay as specified in the table.}
\end{table}


\subsection{DP-Newton hyperparameters}

\begin{table}[H]
    \centering
\label{tab:hparams_lamb}
    \begin{tabular}{cccccc}
    \toprule
        Model & Pre-training DS & Fine-tuning DS &  &  & DP Clipping Norm ()  \\
        \midrule
      ViT-G/14 & JFT-3B & ImageNet-1k &  &  & 1.0 \\
      ViT-G/14 & JFT-3B & CIFAR-10 &  &  & 0.005 \\
      ViT-G/14 & JFT-3B & CIFAR-100 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & ImageNet-1k &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-100 &  &  & 0.005 \\
           ViT-B/16 & ImageNet-1k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-1k & CIFAR-100 &  &  & 0.005 \\
         \bottomrule
         \addlinespace[0.3cm]
    \end{tabular}
        \caption{Fine-tuning hyperparams for DP-Newton. All models are trained in full-batch setting with a constant learning rate and no dropout. When training the models with DP, we replace the global clipping with per example clipping norm as specified in the table. Specific to DP-Newton, we clip the features instead of the gradient and the Hessian. We do this save on computational cost since now we don't need to explicitly compute per-example gradient and the Hessian. For the privacy analysis, we use the clipping norm  to sanitize the gradient and  for the Hessian. Following \cite{mehta2022large}, we set initial weights to 0.0 and train with sigmoid cross-entropy loss. We employed a Bayesian optimization package called Vizier \citep{vizier, oss_vizier} and used a total of 300 trials for jointly tuning both the learning rate and weight decay as specified in the table.}
\end{table}

\subsection{DP-FC hyperparameters}

\begin{table}[H]
    \centering
\label{tab:hparams_lamb}
    \begin{tabular}{cccccc}
    \toprule
        Model & Pretraining DS & Finetuning DS &  &  & DP Clipping Norm ()  \\
        \midrule
      ViT-G/14 & JFT-3B & ImageNet-1k &  &  & 1.0 \\
           ViT-G/14 & JFT-3B & CIFAR-10 &  &  & 0.005 \\
      ViT-G/14 & JFT-3B & CIFAR-100 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & ImageNet-1k &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-21k & CIFAR-100 &  &  & 0.005 \\
     ViT-B/16 & ImageNet-1k & CIFAR-10 &  &  & 0.005 \\
      ViT-B/16 & ImageNet-1k & CIFAR-100 &  &  & 0.005 \\
         \bottomrule
         \addlinespace[0.3cm]
    \end{tabular}
        \caption{Fine-tuning hyperparams for DP-FC. All models are trained in full-batch setting with a constant learning rate and no dropout. When training the models with DP, we replace the global clipping with per example clipping norm as specified in the table. Specific to DP-FC, we clip the per-example gradients with clipping norm  and  for the gramian which is shared across classes and iterations. Following \cite{mehta2022large}, we set initial weights to 0.0, set initial bias to -10.0 and train with sigmoid cross-entropy loss. We employed a Bayesian optimization package called Vizier \citep{vizier, oss_vizier} and used a total of 200 trials for jointly tuning both the learning rate and weight decay as specified in the table.}
\end{table}









\end{document}
