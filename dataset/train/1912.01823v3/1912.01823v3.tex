\documentclass{article}

\newif\ifarxiv
\arxivtrue

\ifarxiv
   \usepackage[accepted]{icml2020}
\else
   \usepackage{icml2020}
\fi

\usepackage{times}

\usepackage[utf8]{inputenc}   \usepackage[T1]{fontenc}      \usepackage[
   colorlinks,
   citecolor=teal]{hyperref}  \usepackage{url}              \usepackage{booktabs}         \usepackage{amsfonts}         \usepackage{nicefrac}         \usepackage{microtype}        \usepackage{amsmath}
\usepackage{pifont}
\usepackage{amsthm}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{arydshln} 
\newcommand{\gb}[1]{\boldsymbol{#1}}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\one}[1]{\left[\!\left[ #1 \right]\!\right]}


\newcommand{\FF}{\mathcal{F}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\GG}{\mathcal{G}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{cor}{Corollary}
\newtheorem{rmk}{Remark}
\newtheorem{conj}{Conjecture}


\newcommand{\st}{\text{ s.t.} }
\newcommand{\Th}{\text{th} }
\newcommand{\ie}{\textit{i.e.,} }
\newcommand{\eg}{\textit{e.g.,} }

\newcommand{\rank}{\operatorname{rank}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\ceil}[1]{\lceil #1\rceil}
\newcommand{\floor}[1]{\lfloor #1\rfloor}
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\abs}[1]{\left\lvert{#1}\right\rvert}
\newcommand{\vnorm}[1]{\left\lVert{#1}\right\rVert} \newcommand{\mnorm}[1]{\left\lVert{#1}\right\rVert} \newcommand{\ip}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\EE}{\ensuremath{\mathbb{E}}}
\newcommand{\iter}[1]{\left[{#1}\right]} \newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand*\Laplace{\mathop{}\!\mathbin\bigtriangleup}

\newcommand{\pnote}[1]{{\color{magenta}[Pedro: #1]}}
\newcommand{\dnote}[1]{{\color{blue}[David: #1]}}
\newcommand{\note}[1]{{\color{red}[Michael: #1]}}
\newcommand{\snote}[1]{{\color{green}[Sudarshan: #1]}}

\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\exmref}[1]{Example~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\sthmref}[1]{Thm.~\ref{#1}}
\newcommand{\defref}[1]{Definition~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\claimref}[1]{Claim~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\algoref}[1]{Algorithm~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\renewcommand{\eqref}[1]{(\ref{#1})}

\newcommand{\currw}{w_t}
\newcommand{\nextw}{w_{t+1}}
\newcommand{\optw}{w^\star}
\newcommand{\wi}{w_i}
\newcommand{\currwi}{w_{t,i}}
\newcommand{\w}{w}

\newcommand{\fs}{f_s}
\newcommand{\fst}{f_{s_t}}

\newcommand{\prevg}{g_{t-1}}
\newcommand{\currg}{g_t}
\newcommand{\currgi}{g_{t,i}}

\newcommand{\prevm}{m_{t-1}}
\newcommand{\currm}{m_t}

\newcommand{\prevv}{v_{t-1}}
\newcommand{\currv}{v_t}
\newcommand{\prevvi}{v_{t-1,i}}
\newcommand{\currvi}{v_{t,i}}

\newcommand{\preve}{\eta_{t-1}}
\newcommand{\curre}{\eta_t}
\newcommand{\currei}{\eta_{t,i}}
\newcommand{\prevei}{\eta_{t-1,i}}

\newcommand{\preva}{\alpha_{t-1}}
\newcommand{\curra}{\alpha_t}

\newcommand{\normed}[1]{\left\lVert {#1} \right\rVert}
\newcommand{\dotp}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\btwo}{\beta_2}
\newcommand{\bone}{\beta_1}
\newcommand{\btwot}{\beta_{2,t}}
\newcommand{\bonet}{\beta_{1,t}}
\newcommand{\reg}{R_T}
\newcommand{\avgreg}{\frac1T R_T}

\newcommand{\fdist}{D}
\newcommand{\smooth}{M}
\newcommand{\lowinf}{L}
\newcommand{\lowinft}{L_{t}}
\newcommand{\highinf}{H}
\newcommand{\highinft}{H_{t}}
\newcommand{\gradb}{G_\infty}
\newcommand{\gradbtwo}{G_2}

\newcommand{\loss}{\ell}

\newcommand{\supp}{\mathcal S}

\newcommand{\expec}[2]{\mathbb E_{#1} \left[ {#2} \right]}
\newcommand{\var}[2]{\mathrm{Var}_{#1} \left[ {#2} \right]}

\newcommand{\algrand}{\substack{S \sim \dist^T \\ t \sim \mathcal P(t|S)}}

\newcommand{\dist}{\mathcal D}
\newcommand{\cov}{\mathrm{Cov}}

\newcommand{\vones}{\vec 1}

\newcommand{\nrt}[2]{\sqrt[\leftroot{-3}\uproot{3}{#1}]{{#2}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
 \icmltitlerunning{Domain-independent Dominance of Adaptive Methods} 
\begin{document}
\twocolumn[
\icmltitle{Domain-independent Dominance of Adaptive Methods}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pedro Savarese}{tti}
\icmlauthor{David McAllester}{tti}
\icmlauthor{Sudarshan Babu}{tti}
\icmlauthor{Michael Maire}{uc}
\end{icmlauthorlist}

\icmlaffiliation{tti}{TTI-Chicago}
\icmlaffiliation{uc}{University of Chicago}

\icmlcorrespondingauthor{Pedro Savarese}{savarese@ttic.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{} \begin{abstract}
From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. \end{abstract}
\section{Introduction}
\label{sec:intro}

Deep network architectures are becoming increasingly complex, often containing
parameters that can be grouped according to multiple functionalities, such as
gating, attention, convolution, and generation.  Such parameter groups should
arguably be treated differently during training, as their gradient statistics
might be highly distinct.  Adaptive gradient methods designate parameter-wise
learning rates based on gradient histories, treating such parameters groups
differently and, in principle, promise to be better suited for training complex
neural network architectures.

Nonetheless, advances in neural architectures have not been matched by progress
in adaptive gradient descent algorithms.  SGD is still prevalent, in
spite of the development of seemingly more sophisticated adaptive alternatives,
such as RMSProp \citep{rmsprop} and Adam \citep{adam}.  Such adaptive methods
have been observed to yield poor generalization compared to SGD in
classification tasks \citep{marginal}, and hence have been mostly adopted for
training complex models \citep{transformers, wgan}.  For relatively simple
architectures, such as ResNets \citep{resnet1} and DenseNets \citep{densenet},
SGD is still the dominant choice.

At a theoretical level, concerns have also emerged about the current crop of
adaptive methods.  Recently, \citet{amsgrad} has identified cases, even in the
stochastic convex setting, where Adam \citep{adam} fails to converge.
Modifications to Adam that provide convergence guarantees have been formulated,
but have shortcomings.  AMSGrad \citep{amsgrad} requires non-increasing
learning rates, while AdamNC \citep{amsgrad} and AdaBound \citep{adabound}
require that adaptivity be gradually eliminated during training.  Moreover,
while most of the recently proposed variants do not provide formal guarantees
for non-convex problems, the few current convergence rate analyses in the literature \citep{yogi,adamlike} do not match SGD's.  Section~\ref{sec:related} fully
details the convergence rates of the most popular Adam variants, along with
their shortcomings.

Our contribution is marked improvements to adaptive optimizers, from both
theoretical and practical perspectives.  At the theoretical level, we focus on
convergence guarantees, deriving new algorithms:

\vspace{-5.0pt}
\begin{itemize}[leftmargin=0.2in]
   \item{
      \textbf{Delayed Adam.} Inspired by \citet{yogi}'s analysis of Adam,
      Section~\ref{sec:dadam} proposes a simple modification for adaptive
      gradient methods which yields a provable convergence rate of
       in the stochastic non-convex setting -- the same as SGD.
      Our modification can be implemented by \emph{swapping two lines of code}
      and preserves adaptivity without incurring extra memory costs.  To
      illustrate these results, we present a non-convex problem where Adam
      fails to converge to a stationary point, while Delayed Adam -- Adam with
      our proposed modification -- provably converges with a rate of
      .
   }
   \item{
      \textbf{AvaGrad.}
      Inspecting the convergence rate of Delayed Adam, we show that it would
      improve with an adaptive global learning rate, which self-regulates based
      on global statistics of the gradient second moments.  Following this
      insight, Section~\ref{sec:ava} proposes a new adaptive method, AvaGrad,
      whose hyperparameters decouple learning rate and adaptability.
   }
\end{itemize}

\pagebreak

Through extensive experiments, Section~\ref{sec:exp} demonstrates that AvaGrad
is not merely a theoretical exercise.  AvaGrad performs as well as both SGD and
Adam in their respectively favored usage scenarios.  Along this experimental
journey, we happen to disprove some conventional wisdom, finding adaptive
optimizers, including Adam, to be superior to SGD for training CNNs.  The
caveat is that, excepting AvaGrad, these methods need extensive grid search to outperform SGD, often requiring unconventional hyperparameter values to even yield competitive performance.

AvaGrad is a uniquely attractive adaptive optimizer, as it decouples the learning rate and its adaptability parameter, making hyperparameter search significantly faster. In particular, given a computational budget similar to SGD's, AvaGrad yields near best results over a wide range of tasks. 
 \section{Preliminaries}
\label{sec:prelim}

\subsection{Notation}

For vectors , we use the
following notation:
    for element-wise division
      (),
    for element-wise square root
      (),
    for element-wise addition
      (),
    for element-wise multiplication
      ().
Moreover,  is used to denote the -norm: other norms will be
specified whenever used (\eg ).

For subscripts and vector indexing, we adopt the following convention: the
subscript  is used to denote an object related to the -th iteration of an
algorithm (\eg  denotes the iterate at time step ); the
subscript  is used for indexing:  denotes the -th coordinate
of .  When used together,  precedes : 
denotes the -th coordinate of .

\subsection{Stochastic Non-Convex Optimization}

In the stochastic non-convex setting, we are concerned with the optimization
problem:

where  is a probability distribution over a set  of
``data points''.  We also assume that  is -smooth in , as is
typically done in non-convex optimization:


Methods for stochastic non-convex optimization are evaluated in terms of number
of iterations or gradient evaluations required to achieve small loss gradients.
This differs from the stochastic convex setting where convergence is measured
w.r.t.~suboptimality .  We assume that the
algorithm takes a sequence of data points  from which
it deterministically computes a sequence of parameter settings
 together with a distribution  over
.  We say an algorithm has a convergence rate of  if
 where, as defined above, .

We also assume that the functions  have bounded gradients: there exists
some  such that
   
for all  and .  Throughout the paper, we also let
 denote an upper bound on .
 \section{Related Work}
\label{sec:related}

Here we present a brief overview of optimization methods commonly used for
training neural networks, along with their convergence rate guarantees for
stochastic smooth non-convex problems.  We consider methods which, at each
iteration , receive or compute a gradient estimate:

and perform an update of the form:

where  is the \textbf{global learning rate},
 are the \textbf{parameter-wise learning rates}, and
 is the update direction, typically defined as:

Non-momentum methods such as SGD, AdaGrad, and RMSProp \citep{rmsprop, adagrad}
have  (\ie ), while momentum SGD and Adam
\citep{adam} have .  Note that while  can always be
absorbed into , representing the update in this form will be convenient
throughout the paper.

SGD uses the same learning rate for all parameters, \ie .
Although SGD is simple and offers no adaptation, it has a convergence rate of
 with either constant, increasing, or decreasing learning rates
\citep{nonconvex}, and is widely used when training deep networks, especially
CNNs \citep{resnet1, densenet}.  At the heart of its convergence proof is the
fact that
   .

Popular adaptive methods such as RMSProp \citep{rmsprop}, AdaGrad
\citep{adagrad}, and Adam \citep{adam} have
   ,
where  is given by:

As  is an estimate of the second moments of the gradients, the
optimizer designates smaller learning rates for parameters with larger
uncertainty in their stochastic gradients.  However, in this setting 
and  are no longer independent, hence
   .
This ``bias'' can cause RMSProp and Adam to present convergence issues, even in
the stochastic convex setting \citep{amsgrad}.

Recently, \citet{yogi} showed that, with a constant learning rate, RMSProp and Adam have a convergence rate of
, where
   ,
hence their result does not generally guarantee convergence. 

\citet{adamlike} showed that
AdaGrad and AMSGrad enjoy a convergence rate of  when a decaying learning rate is used. Note
that both methods constrain  in some form, the former with
    (adaptability diminishes with ),
and the latter explicitly enforces
    for all  ( is point-wise non-increasing).
In both cases, the method is less adaptive than Adam, and the rates above are worse than SGD's . \section{SGD-like Convergence without Constrained Rates}
\label{sec:dadam}



We first take a step back to note the following: to show that Adam might not
converge in the stochastic convex setting, \citet{amsgrad} provide a
stochastic linear problem where Adam fails to converge w.r.t. suboptimality.
Since non-convex optimization is evaluated w.r.t. norm of the gradients, a
different instance is required to characterize Adam's behavior in this setting.

The following result shows that even for a quadratic problem, Adam indeed does
not converge to a stationary point:

\begin{thm}
   For any  and constant , there is a
   stochastic convex optimization problem for which Adam does not converge to
   a stationary point.
   \label{thm:adamdiv}
\end{thm}
\begin{proof}
   The full proof is given in Appendix \ref{sec:proof1} \footnote{See suplementary material for Appendices}.  The argument follows
   closely from \citet{amsgrad}, where we explicitly present a stochastic
   optimization problem:
   
   We show that, for large enough  (as a function of
   ), Adam will move towards
    where , and that the constraint
    does not make  a stationary point.
\end{proof}

This result, like the one in \citet{amsgrad}, relies on the fact that 
and  are correlated: upon a draw of the rare sample , the
learning rate  decreases significantly and Adam takes a small step in
the correct direction.  On the other hand, a sequence of common samples
increases  and Adam moves faster towards .

Instead of enforcing  to be point-wise non-increasing in 
\citep{amsgrad}, which forces the optimizer to take small steps even for a long
sequence of common samples, we propose to simply have  be independent
of .  As an extra motivation for this approach, note that successful
proof strategies \citep{yogi} to analyzing adaptive methods include the
following step:

where bounding , seen
as a form of bias, is a key part of recent convergence analyses.  Replacing
 by  in the update equation of Adam removes this bias and can
be implemented by simply swapping lines of code (updating  \textit{after}
), yielding a simple convergence analysis without hindering the adaptability of the method in any way.

Algorithm~\ref{alg:dadam} provides pseudo-code when applying this modification to Adam, yielding Delayed Adam.  The following Theorem shows that this
modification is enough to guarantee a SGD-like convergence rate of
 in the stochastic non-convex setting for general adaptive
gradient methods.

\begin{thm}
   \label{thm:dadamconv}
   Consider any optimization method which updates parameters as follows:
   
   where , , and
    are independent of . Assume that ,
    is -smooth, and
    for all
   .
   Moreover, let .

   For ,
   if  for all , then:
   
   where  assigns probabilities
      .
\end{thm}
\begin{proof}
   The full proof is given in Appendix \ref{sec:proof2}, along with analysis
   for the case with momentum  in
   Appendix~\ref{sec:proof2-momentum}, and in particular
   , which yields a similar rate.
\end{proof}

The convergence rate depends on  and , which are
random variables for Adam-like algorithms.  However, if there are constants
 and  such that

for all  and , then a
rate of  is guaranteed.

This is the case for Delayed Adam,
where  for all  and
.  \thmref{thm:dadamconv} also requires that  and  are
independent of , which can be assured to hold by applying a ``delay'' to
their respective computations, if necessary (\ie replacing  by
, as in Delayed Adam).

Additionally, the assumption that , meaning that a single
sample should not affect the distribution of
, is required since  is
conditioned on the samples  (unlike in standard analysis, where
 and  is deterministic), and is expected to
hold as .

Practitioners typically use the last iterate  or
perform early-stopping: in this case, whether the assumption holds or not does
not affect the behavior of the algorithm.  Nonetheless, we also show in
Appendix~\ref{sec:proof2-unconditional} a similar rate that does not require
this assumption to hold, which also yields a  convergence rate
taken that the parameter-wise learning rates are bounded from above and below.
 \section{AvaGrad: An Adaptive Method with Adaptive Variance}
\label{sec:ava}
\begin{algorithm}[t]
   \caption{\textsc{Delayed Adam}}
   \label{alg:dadam}
   \textbf{Input:}
      , , 
   \begin{algorithmic}[1]
      \State Set 
      \For{ \textbf{to} }
         \State Draw 
         \State Compute 
         \State 
         \State 
         \State 
         \State 
      \EndFor
   \end{algorithmic}
\end{algorithm}
 
\begin{algorithm}[t]
   \caption{\textsc{AvaGrad}}
   \label{alg:avagrad}
   \textbf{Input:}
      , , 
   \begin{algorithmic}[1]
      \State Set 
      \For{ \textbf{to} }
         \State Draw 
         \State Compute 
         \State 
         \State 
         \State 
         \State 
      \EndFor
   \end{algorithmic}
\end{algorithm}
 
Now, we consider the implications of \thmref{thm:dadamconv} for Delayed Adam,
where , and hence

for all  and .

For a fixed , chosen a-priori (that is, without knowledge of
the realization of ), we can optimize  to minimize
the worst-case rate using  and
.  This yields
, and a convergence rate linear in ,
suggesting that, at least in the worst case,  should be chosen to be
as large as possible, and the learning rate  should scale linearly with
.

What if we allow  to vary in each time step?  For example, choosing
 yields a convergence rate of

Using  we see
that in the worst-case this is also linear in . However, this dependence differs
from the one with fixed  in a few aspects.  Most notably, if
we consider different scalings of 
(\eg small  and scaling ), the convergence rate with fixed
 can get arbitrarily worse: in \eqref{eq:convrate}, we get that the numerator grows quadratically with the scaling, while the denominator only grows linearly.

On the other hand, for
 the convergence rate remains unchanged since in \eqref{eq:convrate2}, both  and  grow linearly with the scaling and hence its effect is cancelled out.

For the particular case , we have
 in \eqref{eq:convrate2}, yielding the exact same convergence rate as SGD \emph{including constant factors}. However, a constant
 results in a dependence on the scale of , which
again can be large if the second moment estimate is either large or small.  Lastly, normalizing
the learning rate by  removes its dependence on
 in the worst-case setting, making the two hyperparameters
more separable.

Motivated by the above observations, the choice of , where  is added for normalization effects, yields a method which we name AvaGrad --
\textbf{A}daptive \textbf{VA}riance \textbf{Grad}ients, presented as
pseudo-code in Algorithm \ref{alg:avagrad}.  We call it an
``adaptive variance'' method since, if we scale up or down the variance of the
gradients, the convergence guarantee in
\thmref{thm:dadamconv} does not change, while for global learning rates that are independent of  (as in Adam and other adaptive methods), it can be arbitrarily bad.
 \section{Experiments}
\label{sec:exp}

\begin{figure*}[!bt]
   \centering
   \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/mean_iters.pdf}
   \end{subfigure}\hfill
   \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/mean_grads.pdf}
   \end{subfigure}
   \caption{
      Plots of Adam, AMSGrad, and Delayed Adam trained on the synthetic
      example in Equation ~\ref{eq:synth}, with a stationary point at
      .
      \textbf{Left:}
         The expected iterate sampled uniformly from ,
         for each iteration . As predicted by our theoretical results, Adam
         moves towards  with , while Delayed
         Adam converges to .
      \textbf{Right:}
         The expected norm squared of the gradient, for  randomly sampled
         from .  Delayed Adam converges significantly
         faster than AMSGrad, while Adam fails to converge.
   }
   \label{fig:synth}
\end{figure*}
 
\subsection{Synthetic Data}

To illustrate empirically the implications of \thmref{thm:adamdiv} and
\thmref{thm:dadamconv}, we set up a synthetic stochastic optimization problem
with the same form as the one used in the proof of \thmref{thm:adamdiv}:

This function has a stationary point
, and it satisfies
\thmref{thm:adamdiv} for .
We proceed to perform stochastic optimization with Adam, AMSGrad, and Delayed
Adam, with constant learning rate .  For simplicity, we let
 be uniform over , since  is constant.

Figure~\ref{fig:synth} shows the progress of 
and  for each iteration
: as expected, Adam fails to converge to the stationary point ,
while both AMSGrad and Delayed Adam converge.  Note that Delayed Adam converges
significantly faster, likely because it has no constraint on the learning
rates.

\subsection{Image Classification on CIFAR}
\label{sec:cifar}
\begin{figure*}[bt!]
   \centering
   \begin{subfigure}{.5\textwidth}
      \centering
      \scriptsize{\textsf{Adam}}\\
      \includegraphics[width=1.0\linewidth]{figures/adam_heatmap.png}
   \end{subfigure}\hfill
   \begin{subfigure}{.5\textwidth}
      \centering
      \scriptsize{\textsf{AvaGrad}}\\
      \includegraphics[width=1.0\linewidth]{figures/avagrad2_heatmap.png}
   \end{subfigure}
   \caption{
      Validation error of a Wide ResNet 28-4 trained on the CIFAR-10
      dataset with Adam (\textbf{left}) and AvaGrad (\textbf{right}), for different values of the learning rate  and
      parameter , where larger  yields less
      adaptability. Best performance is achieved with small adaptability ().
   }
   \label{fig:cnns}
\end{figure*}
 
Our theory suggests that, in the worst case,  should be chosen as large as possible, at which point the learning rate  should scale linearly with it. As a first experiment to assess this hypothesis, we analyze the interaction between  and  when training a Wide ResNet 28-4 \citep{wide}
on the CIFAR dataset \citep{cifar}. The CIFAR-10 and CIFAR-100 datasets consist of 60,000 RGB images with  pixels and comes with a standard train/test split of 50,000 and 10,000 images, respectively.

Following \citet{wide}, we pre-process the dataset by performing channel-wise normalization using statistics computed from the training set. We also flip each image horizontally with  probability and perform random cropping by first padding 4 black pixels to each image and then extracting a random  crop.

We use a validation set of 5,000 images to evaluate the performance of SGD and different adaptive gradient methods:
Adam, AMSGrad, AdaBound \citep{adabound, adabound2}, AdaShift \citep{adashift}, and our proposed algorithm, AvaGrad. We also
assess whether performing weight decay as proposed in \citet{adamw} instead
of standard  regularization positively impacts the performance of adaptive
methods: we do this by evaluating AdamW and AvaGradW.

The learning rate is decayed by a factor of 5 at epochs 60,
120 and 160, and the model is trained for a total of 200 epochs with a weight
decay of . We use a mini-batch size of 128, and each model is trained on a single GPU. For SGD, we use a momentum of , while for adaptive methods we use the default  and . For AdaBound, we use the default final learning rate  and  for the bound functions. Finally, for AdaShift, we use the default stack size .

We run each adaptive method with different powers of  for , multiplied by  and , from  up to , a value large enough such that adaptability can be effectively ignored. We also vary the learning rate 
of each method with different powers of , multiplied by  and , from  up to . In total, we evaluate  different hyperparameter settings for each adaptive method.

Figure \ref{fig:cnns} shows the results for
Adam and AvaGrad.  Our main findings are twofold:

\vspace{-5.0pt}
\begin{itemize}[leftmargin=0.2in]
   \item{
      The optimal  for every adaptive method is considerably larger
      than the values typically used in practice, ranging from  (Adam,
      AMSGrad, AvaGradW) to  (AvaGrad, AdamW).  For Adam and AMSGrad, the
      optimal learning rate is , a value  times
      larger than the default.
   }
   \item{
      All adaptive methods, except for AdaBound, outperform SGD in terms of
      validation performance.  Note that for SGD the optimal learning rate is
      , matching the value used in work such as
      \citet{resnet1, wide, resnext}, which presented state-of-the-art results
      at time of publication.
   }
\end{itemize}

\begin{figure*}[bt!]
   \centering
   \begin{subfigure}{.5\textwidth}
      \centering
      \scriptsize{\textsf{Adam}}\\
      \includegraphics[width=\linewidth]{figures/adam_ptb_heatmap.png}
   \end{subfigure}\hfill
   \begin{subfigure}{.5\textwidth}
      \centering
      \scriptsize{\textsf{AvaGrad}}\\
      \includegraphics[width=\linewidth]{figures/avagrad2_ptb_heatmap.png}
   \end{subfigure}
   \caption{
      Validation bits-per-character (\emph{lower is better}) of a 3-layer LSTM
      with 300 hidden units, trained on the Penn Treebank dataset with Adam (\textbf{left}) and AvaGrad (\textbf{right}), for
      different values of the learning rate  and
      parameter , where larger  yields less adaptability. Best performance is achieved with high adaptability ().
   }
   \label{fig:rnns}
\end{figure*}
 
\begin{table*}
   \caption{
      Test performance of SGD and popular adaptive methods in benchmark tasks. Red indicates results with the recommended optimizer, following the paper that proposed each model, and any improved performance is given in blue. The best result for each task is in bold, and numbers in parentheses present standard deviations of 3 runs for CIFAR.
   }
   \label{tab:results}
   \begin{center}
   \begin{tabular}{|c|c|c|c|c|}
\hline
Method      & \begin{tabular}[x]{@{}c@{}}CIFAR-10\Test Err \%)\end{tabular} &
              \begin{tabular}[x]{@{}c@{}}ImageNet\Test Bits per Character)\end{tabular}\\
\hline
SGD         & {\color{red} 3.86 (0.08)} & {\color{red} 19.05 (0.24)} & {\color{red} 24.01} & 1.238   \\ \hdashline
Adam        & {\color{blue} \textbf{3.64 (0.06)}} & {\color{blue}18.96 (0.21)} & \textbf{{\color{blue} 23.45}} & {\color{red} 1.182} \\
AMSGrad     & 3.90 (0.17) & {\color{blue} 18.97 (0.09)} & {\color{blue} 23.46} & 1.187 \\
AdaBound    & 5.40 (0.24) & 22.76 (0.17) & 27.99 & 2.863 \\
AdaShift    & 4.08 (0.11) & {\color{blue} 18.88 (0.06)} & N/A & 1.274 \\
AdamW    	& 4.11 (0.17) & 20.13 (0.22) & 27.10 & 1.230 \\  \hline
AvaGrad     & {\color{blue} 3.80 (0.02)} & {\color{blue}\textbf{18.76 (0.20)}} & {\color{blue} 23.58} & {\color{blue} 1.179} \\
AvaGradW     & 3.97 (0.02) & {\color{blue} 19.04 (0.37)} & {\color{blue} 23.49} & {\color{blue} \textbf{1.175}} \\
\hline
\end{tabular}
   \end{center}
\end{table*}
 
However, the fact that adaptive methods outperform SGD in this setting is not
conclusive, since they are executed with more hyperparameter settings (varying
 as well as ).  Moreover, the main motivation for adaptive
methods is to be less sensitive to hyperparameter values; performing an
extensive grid search defeats their purpose.

Aiming for a fair comparison between SGD and adaptive methods, we also train a Wide ResNet 28-10 on both
CIFAR-10 and CIFAR-100, evaluating the test performance of each adaptive method with its
optimal values for  and  found in the previous experiment. For SGD, we confirmed that the learning  rate  still yielded the best validation performance with the new architecture, hence the fact that we transfer hyperparameters from the Wide ResNet 28-4 runs does not unfairly advantage adaptive methods in the comparison with SGD. With a larger network and a different task (CIFAR-100), this experiment should
also capture how hyperparameters of adaptive methods transfer between tasks and
models.

On CIFAR-10, SGD achieves  test error (reported as  in
\citet{wide}) and is outperformed by both
      Adam () and
   AvaGrad ().
On CIFAR-100, SGD () is outperformed by
    Adam (),
 AMSGrad (),
 AdaShift (),
 AvaGrad (), and
AvaGradW ().
We believe these results are surprising, as they show that adaptive methods
can yield state-of-the-art performance when training CNNs as long as their
adaptability is correctly controlled with .

\subsection{Image Classification on ImageNet}

As a final evaluation of the role of adaptability when training convolutional
networks, we repeat the previous experiment on the ImageNet dataset
\citep{imagenet}, a challenging benchmark composed of 1.2M training and 50,000 validation RGB images sampled from a total of 1,000 classes. We follow \citet{gross} for data augmentation (scale and color transformations) and use 224x224 single-crops to compute the top-1 accuracy on the validation set.

We train a ResNet-50 \citep{resnet2} with SGD and different
adaptive methods, transferring the hyperparameters from our original CIFAR-10
results.  The network is trained for 100 epochs
with a batch size of  on 4 GPUs (batch size of  per GPU), the learning rate is decayed by a factor of 10 at
epochs 30, 60 and 90, and a weight decay of  is applied. 

SGD yields
 top-1 validation error, underperforming
    Adam (),
 AMSGrad (),
 AvaGrad () and
AvaGradW () -- a total of 4 out of the 6 adaptive methods evaluated on the dataset. Table~\ref{tab:results} summarizes the results.

In contrast to numerous papers that surpassed the state-of-the-art on ImageNet by training networks with SGD \citep{vgg, googlenet, resnet1, resnet2, wide, resnext}, our results show that adaptive methods can yield superior results in terms of generalization performance.

Most strikingly, we observed that Adam outperformed more sophisticated methods such as AMSGrad, AdaBound, and AdamW.  Note that the hyperparameter values we used for SGD match the ones in
\citet{resnet1}, \citet{resnet2} and \citet{gross}: an initial learning rate of
 with a momentum of . 


\subsection{Language Modelling with RNNs}

It is perhaps not very surprising that to perform optimally in the image
classification tasks studied previously, adaptive gradient methods required
large values of , and hence were \emph{barely} adaptive.

Here, we
consider a task where state-of-the-art results are not typically achieved by SGD, but by
adaptive methods with low values for : language modelling with
recurrent networks. In particular, we perform character-level language
modelling on the Penn Treebank dataset \citep{ptb,ptbc}, which consists of 5.01M/393k/442k training/validation/test tokens, respectively, and a vocabulary size of 10,000.

Following \citet{awd}, we train 3-layer LSTMs \citep{lstm} with a character embedding size of 200 and varying size for the LSTM cells. The model is trained for a total of 500 epochs,
and the learning rate is decayed by  at epochs 300 and 400.  We use a batch size
of 128, a BPTT length of 150, and weight decay of .

As in \citet{awd}, we apply weight dropout with  to the LSTM's hidden-to-hidden matrix, variational dropout \citep{variationaldropout} with  for the input/output layers,  for the LSTM layers, and  to the columns of the embedding matrix (embedding dropout).

We first evaluate the validation performance of SGD, Adam, AMSGrad, AdaShift, AdaBound, AdamW,
AvaGrad and AvaGradW with varying learning rate  and adaptability
parameter , when training a 3-layer LSTM with 300 hidden
units in each layer. We vary the learning rate  in powers of  multiplied by 2: from  up to ; for the adaptability parameter , we vary the values in powers of , multiplied by  and , from  up to .


Figure \ref{fig:rnns} shows that, in
this task, smaller values for  are indeed optimal: Adam, AMSGrad and
AvaGrad performed best with . The optimal learning rates for both Adam and AMSGrad, , agree with the value used in \citet{awd}. Both AvaGrad and AvaGradW performed best with : the former with , the latter with .

Next, we train a larger model: a 3-layer
LSTM with 1000 hidden units per layer (the same model used in \citet{awd},
where it was trained with Adam),
choosing values for  which yielded the best validation performance in
the previous experiment. For SGD, we again confirmed that a learning rate of  performed best on the validation set. 

Table~\ref{tab:results} (right column) reports all
results. In this setting, AvaGrad and AvaGradW outperform Adam, achieving
bit-per-characters of  and  compared to . The poor performance of AdaBound could be caused by convergence issues or due to the default values for its hyperparameters: \citet{adabound2} showed that that the bound functions strongly affect the optimizer's behavior and might require careful tuning.

When combined with the previous results, we see that adaptive methods actually \emph{dominate} SGD across tasks of different domains. In particular, both Adam and AvaGrad outperformed SGD in all 4 considered tasks.


\subsection{Hyperparameter Separability and Domain-Independence}

We observed that, given enough budget for hyperparameter tuning, Adam can actually outperform SGD in tasks such as image classification with CNNs, where adaptive methods have traditionally found little success. But can we decrease the cost of hyperparameter search?

One of the main motivations behind AvaGrad is that it removes the dependence
between the learning rate  and the adaptability parameter ,
at least in the worst-case rate of \thmref{thm:dadamconv}.  Observing the
heatmaps in Figure \ref{fig:cnns} and
\ref{fig:rnns}, we can see that indeed AvaGrad offers great
separability between  and , unlike Adam. 

In particular, for values larger than ,  has little to no interaction with the
learning rate , as opposed to Adam where the optimal  increases
linearly with . For language modelling on Penn Treebank, the optimal
learning rate for AvaGrad was  \emph{for every choice of
}, while for image classification on CIFAR-10, we had 
for all except two values of .  This
shows that AvaGrad enables a grid search over  and  (naively, with
quadratic complexity) to be broken into two line searches over  and
 separately (linear complexity). In the context of Section \ref{sec:cifar}, this leads to a decrease from  to  total trials for hyperparameter search, only twice as many as SGD's budget. The cost of optimizing both SGD and Adam \emph{with a fixed } is the same as fully optimizing AvaGrad. Therefore, unless  is chosen optimally a-priori, AvaGrad dominates both SGD and Adam given the same budget and coarseness for hyperparameter tuning.




 \section{Conclusion}
\label{sec:conclusion}

As neural architectures become more complex, with parameters having highly
heterogeneous roles, parameter-wise learning rates are often necessary for
training.  However, adaptive methods have both theoretical and empirical gaps,
with SGD outperforming them in some tasks and having stronger theoretical convergence guarantees.  In this paper, we close
this gap, by first providing a convergence rate guarantee that matches SGD's, and by showing that, with proper hyperparameter tuning, adaptive methods can dominate in both computer vision and natural language processing tasks. Key to our finding is AvaGrad, our proposed optimizer whose adaptability is decoupled from its learning rate.

Our experimental results show that proper tuning of the learning rate together with the adaptability of the method is necessary to achieve optimal results in different domains, where distinct neural network architectures are used across tasks. By enabling this tuning to be performed in linear time, AvaGrad takes a leap towards efficient domain-agnostic training of general neural architectures. 
\bibliographystyle{icml2020}
\ifarxiv
   \bibliography{avagrad_arxiv}
\else
   \bibliography{avagrad}
\fi

\onecolumn
\newpage
\part*{Appendix}
\appendix

\section{Proof of \thmref{thm:adamdiv}}
\label{sec:proof1}

\begin{proof}
Consider the following stochastic optimization problem:

where .  Note
that , and  is minimized at
.

The proof follows closely from \citet{amsgrad}'s linear example for convergence in
suboptimality.  We assume w.l.o.g. that .  Consider:


where the expectation is over all the randomness in the algorithm up to time
, as all expectations to follow in the proof.  Note that  for
. For  we bound  as follows:


Hence,
.

As for , we have, from Jensen's inequality:


Now, remember that ,
hence:

and thus:


Plugging in the bounds for  and  in Equation \ref{eq:onlinebound}:


Hence, for large enough , and ,
 while the above quantity becomes
non-negative, and hence .  In other words, Adam
will, in expectation, drift away from the stationary point, towards ,
at which point .  For example, 
implies that
. To see that  is not a stationary point due to the feasibility constraints, check that : that is, the negative gradient points \textit{towards} the feasible region.
\end{proof}

\section{Proof of \thmref{thm:dadamconv}}
\label{sec:proof2}

\begin{proof}

Throughout the proof we use the following notation for clarity: 

We start from the fact that  is -smooth:

and use the update :

where in the first step we used the fact that
,
in the second we used
,
in the third we used Cauchy-Schwarz,
and in the fourth we used , along with
.

Now, taking the expectation over , and using the fact that , and that ,  are both independent of :

where in the second step we used
 and
.

Re-arranging, we get:


Now, defining
   , where , dividing by  and summing over :


Now, taking the conditional expectation over all samples  given :

where in the second step we used  which follows from the assumption that , and the third step follows from a telescoping sum, along with the fact that .  Now, using
:


Then, taking the expectation over :


Now, let :


where we used the fact that .

Now, recall that :


Setting  and checking that :


Recalling that  proves the claim.

\end{proof}

\subsection{The case with first-order momentum}
\label{sec:proof2-momentum}
For the case , assume that  in Equation \ref{eq-proof2inter1}:


where in the last step we used .

Similarly to the guarantee in Equation~\ref{eq-proof2final}, we can show a  convergence rate if we further assume that there exist constants  and  such that
 for all  and  (\ie the parameter-wise learning rates are bounded away from zero and also from above), and that  is bounded similarly. For example, having  yields:


where in the first step we used , , and .

Note that for any constant  the above is .

\subsection{The case with unconditional distribution over iterates}
\label{sec:proof2-unconditional}

To show a similar bound without the assumption that , we can alternatively bound  and  using the worst case over possible samples . From \eqref{eq:proof-inter} we have, with :

Now, define  with  instead. As long as  does not depend on ,  is no longer a random variable. Following the same steps as above leads to the following:


In particular, if there are constants  and  such that
 for all  and , can bound  and , yielding:


Hence a  follows as long as  can be upper and lower bounded accordingly by constants. 
\end{document}
 