\label{sec:method}




\subsection{Overview}
Without loss of generality, we explain our motivation with a simple case in which there is only one person. As shown in Fig.~\ref{fig:int} (A), the input to our approach is a 3D feature volume  which is constructed by back-projecting the 2D pose heatmaps in multiple cameras to the 3D voxel space \cite{voxelpose}. The 2D pose heatmaps are extracted from the images using an off-the-shelf pose estimation model ~\cite{sun2019deep}.  represents the number of voxels that are used to discretize the space and  represents the number of joint types. The volume approximately encodes the per-voxel likelihood of body joints. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{Figures/fig_int.pdf}
  \caption{\textbf{Problem Decomposition.} (A): Considering a single person, we re-project its feature volume to the coordinate planes with orthographic projection. The partial coordinates can be estimated by 2D CNN and assembled to 3D estimation. (B): Multi-person brings the extra challenge of ambiguity and occlusion. Nonetheless, people can be easily isolated from the bird's-eye view of the aggregated feature volume. Based on the intuitive ideas, we develop the lightweight \textit{Joint Localization Networks} and \textit{Human Detection Networks} respectively. }
  \label{fig:int}
\end{figure*}


In Fig.~\ref{fig:int} (A), we show a 3D joint of interest, \eg a shoulder joint, as . In general, the corresponding feature volume should have a distinctive pattern around  so that it can be localized by expensive 3D-CNNs~\cite{voxelpose}. To reduce the computation cost, we re-project the volume to the three coordinate planes (\ie the , ,  planes), respectively, resulting in three 2D feature maps. We can imagine that there are also distinctive patterns at the corresponding locations of each 2D feature map, \eg  at the  plane, which can be similarly detected by 2D-CNNs. Then the 3D position of  can be assembled from the estimated coordinates in the three planes. 

However, when we apply the idea to the multi-person scenario, we are confronted with new challenges. The features of different people may be mixed together after being projected to the coordinate planes even when they are far away from each other in the 3D space. This may corrupt the pose estimation accuracy. Inspired by top-down 2D pose estimation~\cite{fang2017rmpe}, the problem can be alleviated by ``cropping'' the person from the overall 3D space and only projecting features belonging to the person to the planes. So the remaining task is to detect each person in the 3D space efficiently. We utilize the prior that people barely overlap along the  axis, therefore they can be easily detected in the bird's-eye view as shown in Fig.~\ref{fig:int} (B).

We take a two-phase approach to address the challenges. In the first phase, we present \textit{Human Detection Networks} (Section \ref{sec:hdn}) which efficiently detects all people from the bird's-eye view by 3D bounding boxes, ensuring that only the person-of-interest features are passed to the next phase.  The second phase conducts fine-grained pose estimation for each person with \textit{Joint Localization Networks} (Section \ref{sec:JLN}), which is greatly eased since occlusion and distraction are mostly eliminated in the first phase. Importantly, all the operators in the networks are on 2D and 1D features, which boosts the speed. 





















\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{Figures/fig_HDN.pdf}
  \caption{\textbf{Human Detection Networks.} We first construct the feature volume  from the multi-view images. It is then projected to the  plane to obtain the feature map  (bird's-eye view). A Multi-branch 2D CNN estimates three feature maps encoding each person's center position, bounding box size, and center offset, respectively. We then select the 1D columns feature  from the positions with high confidence values on . Then a 1D CNN estimates the heatmap  of the vertical position of the 3D box center. Finally, HDN outputs the combined 3D bounding box.
  }
  \label{fig:hdn}
\end{figure*}

\subsection{Human Detection Networks}
\label{sec:hdn}
We first apply HRNet\cite{sun2019deep} to estimate 2D pose heatmaps from the multiview images, and construct an aggregated feature volume  by back-projecting the heatmaps to the 3D voxel space. Since people are usually on the ground plane and it is less probable that one person is right on top of another, it inspires us to construct a 2D bird's-eye view representation from the feature volume for efficiently detecting people.

\paragraph{Detection in  Plane}
We re-project the aggregated feature volume to the ground plane () by performing max-pooling along the  direction and obtain . Then we feed  to a 2D fully convolutional network to detect the locations of people in the  plane. The positions of all people in the plane are encoded by a 2D confidence map  whose value  represents the likelihood of human presence at the location . For training supervision, we generate the ground-truth (GT) 2D confidence map . Its values are computed by the distance between the GT center point and each grid point using a Gaussian kernel. Specifically, the confidence value of grid point  is computed by:

where  denotes the number of persons and  represents the corresponding GT position for person . We just keep the largest scores in the presence of multiple people. The mean squared error (MSE) loss is computed by:



We further estimate a 2D box size for each person instead of assuming a loose constant size as in the previous work~\cite{voxelpose}. The height of the box is simply set to be mm. This is critical to isolate the interference of multiple people, especially in crowded scenes. Our model generates a box size embedding at all grid points, denoted as . But only those at the locations with large confidences are meaningful. We compute a ground-truth size embedding  based on box annotations. 






During training, we only compute losses on the grid points which are adjacent to the ground-truth box centers. Specifically, for a 2D GT box center , we only add supervision on the discretized grid points , where  represents the length of a single voxel and  denotes the width. Let  denote the set of the neighboring points mentioned above and suppose  is the number of persons in the image. We compute an  loss at each center point in :


In addition, to reduce the quantization error, we estimate the local offset for each root joint on the horizontal plane. Similar to size estimation, the model outputs an offset prediction at each grid point, denoted as . We generate a GT offset prediction  and use an  loss on the neighboring points:


Inspired by \cite{zhou2019objects}, we use a simple network structure with three parallel branches to estimate the heatmap, offset and size respectively. As shown in Fig.~\ref{fig:hdn}, the 2D bird's-eye features are passed through a fully-convolutional backbone network and then fed into three separate branches with identical designs, which consist of a 3  3 convolution, ReLU, and another 1  1 convolution. 

\paragraph{Detection in  Axis}
The remaining task is to estimate the center height for each proposal. Firstly, we obtain the proposals with  largest confidences on the 2D heatmap  after applying non-maximum suppression (NMS). We set  in our experiments. Subsequently, we extract the corresponding 1D ``columns'' for each proposal from the aggregated feature volume , denoted as , which is then fed into a 1D fully convolutional network to regress the height. Similar to 2D detection, our model generates 1D heatmap estimation , indicating the likelihood of human presence at every possible height. We compute a GT 1D heatmap   for each proposal based on its center height using the Gaussian distribution. Likewise, we use an MSE loss here:


Finally, we select the height with maximum confidence and by combining it with the 2D box center, offset and size, we can obtain the 3D bounding box. The overall confidence score for each box is computed by multiplying the scores of the 2D heatmap and 1D outputs. According to the exponential property of the Gaussian function, it can be regarded as an approximate of the 3D Gaussian distribution. We set a threshold for confidence scores to select the valid proposals. To sum up, the overall training objective is as follows:

where we set ,  and .

\subsection{Joint Localization Networks}
\label{sec:JLN}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{Figures/fig_JLN.pdf}
  \caption{\textbf{Joint Localization Networks.} For each person, we first construct its local feature volume . The person-specific feature volume  is obtained by masking  with the detected 3D box. We re-project  to three orthogonal coordinate planes to get the 2D feature maps . A shared 2D pose estimator regresses the joint locations  for each plane, and a confidence network computes the corresponding weights . Finally, the 3D pose  is computed by weighting  with  in a pairwise manner. } 
  \label{fig:jln}
\end{figure*}

\paragraph{Person-specific Feature Volume.} With the bounding box of each person, we construct its fine-grained feature volume to predict the final 3D pose. We first crop a smaller feature volume  from  centered at the box center with a fixed size (\ie 2m  2m  2m). It suffices to cover arbitrary poses and maintains the relative scale of the motion space. The space is then divided into  voxels. Now the key step is to \textbf{zero out} the features outside the estimated bounding box to get the person-specific feature volume . This masking mechanism reduces the distraction of other people and enables safe volume re-projection in the following stage. 

\paragraph{Joint Localization.} To reduce the computational cost, we re-project  onto three orthogonal 2D planes, \ie the \textit{xy} plane, \textit{xz} plane and \textit{yz} planes in the world coordinate systems. Let ,  and  denote the re-projected feature maps corresponding to the three planes, respectively. Again, we use max-pooling for feature projection. 

Subsequently, they are concatenated as a batch and fed to a 2D CNN for joint localization, as shown in Fig.~\ref{fig:jln}. Note that we set the same granularity of voxels on different axes to enable parallel estimation, \ie . The 2D CNN produces a joint-wise heatmap estimation for each re-projection plane, denoted as  in the same shape of . To reduce the quantization error, we compute the center of mass of  instead of taking the maximum responses. Specifically, the estimated positions  are computed by:





We supervise the estimations with the ground-truth 2D location  on each plane. An  loss is computed by:



\paragraph{Adaptive Weighted Fusion.} The quality of  and the difficulty of pose estimation naturally vary with the re-projection plane and human pose, thus we hope the model could learn to discriminate and balance the estimations from different planes automatically. To achieve this, we introduce a lightweight confidence regression network. We assume that the pattern of  could reflect the quality of 2D pose estimation. Therefore, the estimated heatmaps  are fed into a shared confidence regression network. Inspired by \cite{zhang2021adafuse}, we adopt a simple design for the confidence regression network, consisting of a convolutional layer, a global average pooling layer and one fully-connected layer. 

The network generates joint-wise fusion weight for each plane, denoted as . We then use the Softmax function for normalization in a pair-wise manner and obtain the final 3D prediction . Specifically, for the joint , the final estimations can be computed by: 

where  denotes taking the first component of the 2D estimated coordinates of , namely the component on the \textit{x}-axis, and the other notations have similar interpretations. Let  denote the GT 3D pose, we use an  loss to train the confidence regression network:


Now we get the overall training objective of JLN as follows. In our experiments, we set .

