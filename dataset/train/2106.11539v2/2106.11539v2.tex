\section{Experiments}
\label{section:experiments}



For all experiments, we fine-tune on the training set and report numbers on the corresponding test/validation dataset. No dataset specific hyper-parameter tuning was done. We treat this as a plus and our reported numbers could be higher if dataset specific fine-tuning was done. For all downstream tasks, we use the official provided annotations unless otherwise stated. A common theme amongst these datasets is the relatively small amount of training data (most with 1000 samples). We posit that pre-training is much more helpful in such scenarios and helps measure the generalization capability of \papertitle. 

\textbf{Notations}: Tables \ref{table:funsd}, \ref{table:rvl_cdip}, \ref{table:cord}, \ref{table:nda}, use the following notation. T: Text features, S: spatial features. I: image features. \textbf{Bold} indicates SOTA. \underline{Underline} indicates second best.  denotes the use of Encoder + Decoder transformer layers.  signifies approximate estimation.

\textbf{Implementation details}: We summarize details for pre-training and fine-tuning in Table 1 in supplemental. 
We emphasize the importance of warm-up steps and learning rate scale. 
We found that these settings have a non-trivial impact on pre-training result as well as downstream task performance. 
We used Pytorch \cite{paszke2019pytorch} and the Huggingface library \cite{wolf2019huggingface}.

\textbf{Models}: We employ the commonly used terminology for transformer encoder models - \textit{base} with 12 transformer layers (768 hidden state and 12 attention heads) and large with 24 transformer layers (1024 hidden state and 16 attention heads). We show that \papertitle-base gets SOTA for three of the 4 tasks beating even large models and for the 4th task is close to a large model. In addition to the multi-modal \papertitle, we also present a text and spatial \papertitle by pre-training \papertitle multi-modally but fine-tuning with only text and spatial features. We do this to show the flexibility of our model and show that during pre-training visual features were infused into \papertitle leading it to do better than pure text and spatial models.

\subsection{Sequence Labeling Task}
FUNSD \cite{jaume2019} dataset is a form understanding task. It contains 199 noisy documents (149 train, 50 test) which are scanned and annotated. We focus on the semantic entity-labeling task (i.e., group tokens which belong to the same class). 
We measure entity-level performance using F1 score shown in Table \ref{table:funsd}. \papertitle-base  achieves 83.34\% F1 score which is better than comparable models: LayoutLMv2-base \textcolor{forestgreen}{(+0.58)}, BROS \textcolor{forestgreen}{(+2.13)}, LayoutLMv1-base \textcolor{forestgreen}{(+4.07)}. Story repeats for \papertitle-large inspite of it trained only with 5M pages.

\begin{table}[htbp]
	\centering
	\scalebox{0.8}{
	\begin{tabular}{l|c|c|c|c}
		Model & \#param (M) & Precision & Recall & F1 \\ 
		\Xhline{2.5\arrayrulewidth}


		\multicolumn{5}{l}{\textit{methods based on only text / (text + spatial) features:}} \\ \midrule
		BERT-base \cite{devlin2018bert} & 109 & 54.69 & 61.71 & 60.26  \\
		RoBERTa-base \cite{liu2019roberta} & 125 & 63.49 & 69.75 & 66.48 \\
		UniLMv2-base  \cite{bao2020unilmv2} & 125 & 63.49 & 69.75 & 66.48  \\ 
		LayoutLMv1-base \cite{xu2020layoutlm} & 113 & 76.12 & 81.55 & 78.66  \\
		BROS-base \cite{bros2020hong} & 139 & 80.56 & 81.88 & 81.21  \\ 
		\midrule
		BERT-large \cite{devlin2018bert} & 340 & 61.13 & 70.85 & 65.63 \\
		RoBERTa-large \cite{liu2019roberta} & 355 & 67.80 & 73.91 & 70.72 \\
		UniLMv2-large  \cite{bao2020unilmv2} & 355 & 67.80 & 73.91 & 70.72  \\ 
		LayoutLMv1-large \cite{xu2020layoutlm} & 343 & 75.36 & 80.61 & 77.89  \\
		\midrule
		\multicolumn{5}{l}{\textit{methods based on image + text + spatial features:}} \\ \midrule
		LayoutLMv1-base  \cite{xu2020layoutlm} & 160 & 76.77 & 81.95 & 79.27  \\
		LayoutLMv2-base \cite{xu2020layoutlmv2}  & 200 & 80.29 & 85.39 & 82.76  \\ 
		LayoutLMv2-large \cite{xu2020layoutlmv2}  & 426 & 83.24 & 85.19 & \uline{84.20}  \\ 
\midrule
		DocFormer-base (T+S) & 149 & 77.63 & 83.69 & 80.54 \\ 
		DocFormer-base (I+T+S) & 183 & 80.76 & 86.09 & 83.34 \\ DocFormer-large (T+S) & 536 & 81.33 & 85.44 & 83.33 \\
		\textbf{DocFormer-large (I+T+S)} & 536 & 82.29 & 86.94 & \textbf{84.55} \\
		
\end{tabular}
	}
	\caption{\textbf{FUNSD comparison}: \papertitle does better than  models its size and compares well with even larger models }
	\label{table:funsd}
	\vspace{-2.5ex}
\end{table}

\textbf{FUNSD performance vs Pre-training samples}: We also measure the performance of \papertitle-base with increasing number of pre-training samples. As seen in Figure \ref{fig:docformer_funsd_epochs}, our base model achieves state-of-the-art performance of 83.34 F1-score in-spite of being pre-trained with only 5M documents. Previous SOTA needed more than 2x pre-training documents (11M) to achieve (82.76). Also \papertitle converges faster. 

\textbf{\papertitle performance without images}: Please note \papertitle-base T+S model which was pre-trained with (I+T+S) but was fine-tuned on FUNSD without Images gives F1 of 80.54 which is \textcolor{forestgreen}{+1.88\%} higher than a LayoutLMv1 (78.66\%) which was purely pre-trained and fine-tuned on T+S. We hypothesize that \papertitle was infused with visual features during pre-training and is better than text-only pre-trained models. 

\begin{SCfigure}[10]
  \centering
  \includegraphics[scale=0.55]{images/docformer_funsd.png}
  \caption{\textbf{Amount of Pre-training matters}: -axis is the number of pre-training samples needed. -axis is the F1-score on FUNSD task. \papertitle-base gets 83.34 after pre-training on only 5M pages and outperforms current SOTA LayoutLMv2-base's 82.76 which was pretrained on more than 2x more data.}
  \label{fig:docformer_funsd_epochs}
  \vspace{-3ex}
\end{SCfigure}





\subsection{Document Classification Task}
For this task we use pooled features to predict a classification label for a document. The RVL-CDIP \cite{harley2015icdar} dataset consists of 400,000 grayscale images in 16 classes, with 25,000 images per class. Overall there are 320,000 training images, 40,000 validation images, and 40,000 test images. We report performance on test and eval metric is the overall classification accuracy. In line with prior art \cite{xu2020layoutlmv2,bros2020hong} text and layout information is extracted using Textract OCR. \papertitle-base achieves state-of-the-art performance of 96.17\%. \papertitle gives superior performance to all existing base and large transformer variants. Some models greater than 4x in number of parameters (TILT-large, 780M parameters gives 94.02\% (\textcolor{red}{-2.15}\% gap). 

\begin{table}[htbp]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{l|c|c}
			Model & \#param (M) & Accuracy (\%) \\ 
			\Xhline{2.5\arrayrulewidth}
			
			\multicolumn{3}{l}{\textit{methods based on only images:}} \\ \midrule
			CNN ensemble \cite{harley2015icdar} & *60 & 89.80 \\
			VGG-16 \cite{afzal2017cutting} & 138 & 88.33 \\
			AlexNet \cite{tensmeyer2017analysis} & 61 & 90.94 \\
			GoogLeNet \cite{csurka2016right} & 13 & 90.70 \\ 
			Single Vision model \cite{das2018document} & *140 & 91.11 \\
			Ensemble \cite{das2018document} & - & 92.21 \\ 
			InceptionResNetV2 \cite{szegedy2017inception} & 56 & 92.63 \\
			LadderNet \cite{sarkhel2019deterministic} & - & 92.77 \\
			\midrule
\multicolumn{3}{l}{\textit{methods based on text / (text + spatial) features:}} \\ 
			\midrule
			BERT-base \cite{devlin2018bert} & 110 & 89.81  \\
			UniLMv2-base  \cite{bao2020unilmv2} & 125 & 90.06 \\ 
			LayoutLMv1-base \cite{xu2020layoutlm} & 113 & 91.78	 \\
			
			BROS-base   \cite{bros2020hong} & 139 & 95.58  \\ 
			\midrule
			BERT-large \cite{devlin2018bert} & 340 & 89.92 \\
			UniLMv2-large  \cite{bao2020unilmv2} & 355 & 90.20 \\ 
			LayoutLMv1-large \cite{xu2020layoutlm} & 343 & 91.90  \\
			
			\midrule
			\multicolumn{3}{l}{\textit{methods based on image + text + spatial features:}} \\ 
			\midrule
			Single Modal \cite{dauphinee2019modular} & - & 93.03  \\
			Ensemble \cite{dauphinee2019modular} & - & 93.07 \\
			TILT-base  \cite{powalski2021going} & 230 & 93.50 \\
			LayoutLMv1-base \cite{xu2020layoutlm} & 160 & 94.42	 \\
			LayoutLMv2-base \cite{xu2020layoutlmv2} & 200 & 95.25  \\ 
			\midrule
			LayoutLMv1-large \cite{xu2020layoutlm} & 390 & 94.43	 \\
			TILT-large  \cite{powalski2021going}  & 780 & 94.02 \\
			LayoutLMv2-large \cite{xu2020layoutlmv2} & 426 & \underline{95.65}  \\ 
\midrule
\textbf{DocFormer-base }(I+T+S) & 183 & \textbf{96.17}  \\
			DocFormer-large (I+T+S) & 536 & 95.50
			
		\end{tabular}
	}
	\caption{\textbf{RVL-CDIP dataset} \cite{harley2015icdar} \textbf{comparison}: We report classification accuracy on the test set. \papertitle gets the highest classification accuracy and outperforms TILT-large by \textcolor{forestgreen}{+2.15} which is almost 4x its size.}
	\label{table:rvl_cdip}
\end{table}


\begin{table}[htbp]
	\scalebox{0.8}{
		\begin{tabular}{l|c|c|c|c}
			Model & \#param (M) & Precision & Recall & F1 \\ 
			\Xhline{2.5\arrayrulewidth}
			
			\multicolumn{5}{l}{\textit{methods based on only text / (text + spatial) features:}} \\ \midrule
			BERT-base \cite{devlin2018bert} & 109 & 88.33 & 91.07 & 89.68  \\
			UniLMv2-base  \cite{bao2020unilmv2} & 125 & 89.87 & 91.98 & 90.92  \\ 
			SPADE \cite{hwang2020spatial} & - & - & - & 91.50 \\
			LayoutLMv1-base \cite{xu2020layoutlm} & 113 & 94.37 & 95.08 & 94.72  \\
			BROS-base  \cite{bros2020hong}   & 139 & 95.58 & 95.14 & 95.36  \\ 
			\midrule
			BERT-large \cite{devlin2018bert} & 340 & 88.86 & 91.68 & 90.25 \\
			UniLMv2-large  \cite{bao2020unilmv2} & 355 & 91.23 & 92.89 & 92.05 \\ 
			LayoutLMv1-large \cite{xu2020layoutlm} & 343 & 94.32 & 95.54 & 94.93 \\
		
			\midrule
			\multicolumn{5}{l}{\textit{methods based on image + text + spatial features:}} \\ \midrule
			LayoutLMv2-base \cite{xu2020layoutlmv2}  & 200 & 94.53 & 95.39 & 94.95  \\ 
			TILT-base  \cite{powalski2021going} & 230 & - & - & 95.11 \\
			LayoutLMv2-large \cite{xu2020layoutlmv2} & 426 & 95.65 & 96.37 & 96.01 \\
			TILT-large  \cite{powalski2021going} & 780 & - & - & \uline{96.33} \\
\midrule
			DocFormer-base (T+S) & 149 & 94.82 & 95.07 & 94.95\\ 
			DocFormer-base (I+T+S) & 183 & 96.52 & 96.14 & \uline{96.33} \\
			DocFormer-large (T+S) & 502 & 96.46 & 96.14 & 96.30\\ 
			\textbf{DocFormer-large} (I+T+S) & 536 & 97.25 & 96.74 & \textbf{96.99} \\
		\end{tabular}
	}
	\caption{\textbf{CORD dataset} \cite{park2019cord} \textbf{comparison}. We present entity-level Precision, Recall, F1 on test set.}
	\label{table:cord}
\end{table}

\vspace{-1em}
\subsection{Entity Extraction Task}
We report performance on two different entity extraction datasets: 

\textbf{CORD Dataset}~\cite{park2019cord}: consists of receipts. It defines 30 fields under 4 categories. The task is to label each word to the right field. The evaluation metric is entity-level F1. We use the provided OCR annotations and bounding boxes for fine-tuning (Table \ref{table:cord}). \papertitle-base achieves 96.33\% F1 on this dataset besting all prior *-base and virtually all *-large variants tying with TILT-large~\cite{powalski2021going} which has higher number of parameters. \papertitle-large achieves 96.99\% besting all other *-large variants achieving SOTA.

\textbf{Kleister-NDA}~\cite{gralinski2020kleister}: dataset consists of legal NDA documents. The task with Kleister-NDA data is to extract the values of four fixed labels. The approach needs to learn to ignore unrelated text. This dataset is challenging since it has some “decoy” text, for which no label should be given. Also, there might be more than one value given for a given label and all values need to be extracted. In line with prior-art we measure F1-score on validation data (since ground truth is not provided for test data). Also we extract OCR and apply heuristics to create train/validation ground-truth on OCR (Table \ref{table:nda}).

\begin{table}[htbp]
	\centering
	\scalebox{0.8}{
		\begin{tabular}{l|c|c}
			Model & \#param (M) & F1 \\ 
			\Xhline{2.5\arrayrulewidth}
			
			\multicolumn{3}{l}{\textit{methods based on only text / (text + spatial) features:}} \\ \midrule
			LAMBERT \cite{garncarek2020lambert} & - & 75.7 \\
			RoBERTa \cite{liu2019roberta} & 125 & 76.7 \\
			BERT-base \cite{devlin2018bert} & 110 & 77.9 \\
			UniLMv2-base  \cite{bao2020unilmv2} & 125 & 79.5 \\ 
			LayoutLMv1-base \cite{xu2020layoutlm} & 113 & 82.7 \\
			\midrule
			BERT-large \cite{devlin2018bert} & 340 & 79.1 \\
			UniLMv2-large  \cite{bao2020unilmv2} & 355 & 81.8 \\ 
			LayoutLMv1-large \cite{xu2020layoutlm} & 343 & 83.4 \\

			\midrule
			\multicolumn{3}{l}{\textit{methods based on image + text + spatial features:}} \\ \midrule
			LayoutLMv2-base \cite{xu2020layoutlmv2}  & 200 & 83.3  \\ 
LayoutLMv2-large \cite{xu2020layoutlmv2} & 426 & \uline{85.2} \\
\midrule
			\papertitle-base (T+S) & 149 & 82.1 \\ 
			\textbf{\papertitle-base }(I+T+S) & 183 & \textbf{85.8} \\
		\end{tabular}
	}
	\caption{\textbf{Kleister-NDA dataset} \cite{gralinski2020kleister} \textbf{comparison}: We present entity-level Precision, Recall, F1 on validation set. \papertitle gives the best performance, out-performing other *-large models trained with 2.5x the learning capacity.}
	\label{table:nda}
\end{table}
















\subsection{More Experiments}
We conduct further analysis on the behavior of \papertitle pertaining to pre-training tasks, network structure and spatial embedding weight sharing.

\textbf{\textit{Shared or Independent Spatial embeddings?}} One of the benefits of our proposed \papertitle multi-modal self-attention architecture (Fig. \ref{fig:encoder_methodologies} and Eq. \ref{eqn:visual_self_attention},\ref{eqn:lang_self_attention}) is that sharing spatial embeddings across vision and language makes it easier for the model to learn feature-correlation across modalities. We see ablation on this aspect in Table \ref{table:w_wo_shared_spatial}. 
\begin{table}[H]
	\begin{center}
			\scalebox{0.75}{
		\begin{tabular}{l|c|c|c}
			Configuration & Num Params & FUNSD (F1) & CORD (F1) \\
			\Xhline{2.5\arrayrulewidth}
			w. shared spatial Eq. \ref{eqn:visual_self_attention},\ref{eqn:lang_self_attention} & 183 M & 76.9 & 93.36 \\
			w/o shared spatial & 198 M  & 75.58 \textcolor{red}{(-1.32)} & 92.51 \textcolor{red}{(-0.85)} \\
\end{tabular}
	}
	\end{center}
	\vspace{-3ex}
	\caption{\textbf{Spatial Weight Sharing}: 
In w/o shared spatial, vision and language get their own spatial weights .}
	\label{table:w_wo_shared_spatial}
\end{table}

\textbf{\textit{Do our pre-training tasks help?}}  Pretraining is essential for  \textit{low-to-medium} data regimes (FUNSD and CORD). but even for downstream tasks with a lot of training samples (RVL-CDIP) it helps to improve performance and convergence (Table \ref{table:with_and_without_pretrain}). 

\begin{table}[H]
	\begin{center}
		\scalebox{0.75}{
\begin{tabular}{l|c|c|c}
				\thead{Dataset} & \thead{Train\\ samples} &\thead{with pre-train \\ then 100 epochs (F1)} & \thead{w/o pre-train \\ 100 epochs (F1)} \\
				\Xhline{2.5\arrayrulewidth}
				FUNSD \cite{jaume2019} & 149 & 83.34 & 4.18 \\ CORD \cite{park2019cord} & 800 & 96.33 & 0.54 \\ RVL-CDIP \cite{harley2015icdar} & 320,000 & 96.17 & 93.95 \\
\end{tabular}
		}
	\end{center}
	\vspace{-3ex}
	\caption{\textbf{Effect of Pre-training} }
	\label{table:with_and_without_pretrain}
\end{table}






\begin{figure*}
	\centering










	\subcaptionbox{Ground Truth}[.288\linewidth][c]{\includegraphics[width=\linewidth]{images/funsd_left_viz.png}}\quad
	\subcaptionbox{Text + Spatial model \cite{xu2020layoutlm}}[.294\linewidth][c]{\includegraphics[width=\linewidth]{images/funsd_mid_viz.png}}\quad
	\subcaptionbox{\textbf{\papertitle} multi-modal}[.295\linewidth][c]{\includegraphics[width=\linewidth]{images/funsd_right_viz.png}}

    \vspace{-0.8em}
	\caption{ \textbf{\papertitle Qualitative Examples}: From \papertitle on FUNSD test-set \papertitle 83.34 F1 vs LayoutLMv1 78.66 F1. \textbf{Legend}:  \textcolor{red}{\textbf{Red}}: \textbf{Header}-label, \textcolor{blue}{\textbf{Blue}}: \textbf{Question}, \textcolor{green}{\textbf{Green}}: \textbf{Answer}. \textbf{Row 1}: ``TARGET'' is a \textbf{Header}-label which is very visual in nature. \papertitle correctly classifies it whereas a text + spatial model misses such visual cues.
\textbf{Row 2}: This is a challenging scenario. Notice the word ``Research'' behind the signature. Text + spatial model gets confused and mis-classifies ``Research'' as \textbf{Header}, whereas \papertitle figured out that ``Research'' is part of ``Marketing Research Director'' in spite of visual occlusions. \textbf{Row 3}: Notice ``Approvals'' is partially hidden behind DATE. In spite of that \papertitle correctly labelled ``APPROVALS'' as \textbf{Question}, where as text+spatial model incorrectly labels it as \textbf{Header}. Best viewed in color and digitally. Snippets are from FUNSD file 86079776\_9777, 89856243, and 87125460.}
	\label{fig:qualitative_examples}
\end{figure*}


\textbf{\textit{Does a deeper projection head help?}} So far we used a single linear layer for downstream evaluation as  is common practice \cite{he_cvpr2020_moco,chen_arxiv2020_mocov2,chen_icml2020_simclr,chen_arxiv2020_simclrv2,appalaraju2020towards} to compare against prior art. Recent publications \cite{chen_arxiv2020_simclrv2,appalaraju2020towards} in self-supervision show that a deeper projection head with ReLU activation acts as a one-way filter to enrich the representation space. We adapt this practice and see if a deeper projection head (fc  ReLU  LayerNorm  fc) can improve downstream performance. Table \ref{table:analysis_deeperhead} shows that in the \textit{low-to-medium} data regime adding a more powerful projection head is harmful and could lead to over-fitting. For the \textit{medium-to-large} downstream task data regime, adding a deeper projection head is beneficial.

\begin{table}[H]
	\begin{center}
			\scalebox{0.75}{
		\begin{tabular}{l|c|c|c}
			Dataset & Train samples & Linear head (F1) & Deeper head (F1) \\
			\Xhline{2.5\arrayrulewidth}
			FUNSD \cite{jaume2019} & 149 & 83.34 & 82.93 \textcolor{red}{(-0.41)}\\
			CORD \cite{park2019cord} & 800 & 96.33 & 96.87 \textcolor{forestgreen}{(+0.54)} \\
			RVL-CDIP \cite{harley2015icdar} & 320,000 & 96.17 & 96.85 \textcolor{forestgreen}{(+0.68)} \\
\end{tabular}
	}
	\end{center}
	\vspace{-3ex}
	\caption{\textbf{Deeper Projection Head}
}
	\label{table:analysis_deeperhead}
\end{table}





