\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{latex/acl}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{stfloats}
\usepackage{color}
\usepackage{subfigure}
\usepackage{balance}
\newcommand{\xz}[1]{\textcolor{red}{\bf [#1 --xz]}} 
\definecolor{midnightgreen}{rgb}{0.0, 0.29, 0.33}


\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{multirow}
\usepackage{graphicx}


\title{{\bf \textsc{INTERVENOR}\includegraphics[width=1.1em]{figures/image/title.jpg}}: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing}



\author{Hanbin Wang$^{1}$, Zhenghao Liu$^{1}$\thanks{ \ \ indicates corresponding author.}, Shuo Wang$^{2}$, Ganqu Cui$^{2}$, Ning Ding$^{2}$, Zhiyuan Liu$^{2}$ and Ge Yu$^{1}$ \\ 
$^1$Department of Computer Science and Technology, Northeastern University, China \\
$^2$Department of Computer Science and Technology, Institute for AI, Tsinghua University, China \\
Beijing National Research Center for Information Science and Technology, China \\
}


\begin{document}
\maketitle
\begin{abstract}
This paper proposes \textbf{INTER}acti\textbf{VE} chai\textbf{N} \textbf{O}f \textbf{R}epairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, \textit{Code Learner} and \textit{Code Teacher}, to play different roles in code repairing and work interactively to repair the generated codes. The \textit{Code Learner} is asked to generate and repair code according to the instructions from the \textit{Code Teacher}. The \textit{Code Teacher} rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for \textit{Code Learner}. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13\% and 4.5\% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and solution plans via natural language. Thanks to the feedback of code compilers, INTERVENOR can accurately identify the syntax errors and assertion errors in the code and provide precise instructions to repair codes, making LLMs achieve the plateau performance with only three repairing turns. All data and codes are available at \url{https://github.com/NEUIR/INTERVENOR}.
 \end{abstract}



\section{Introduction}
Large Language Models (LLMs), such as ChatGPT~\cite{chatgpt}, have demonstrated extremely strong emergency ability and benefited lots of tasks, especially for the code generation~\cite{openai2023gpt4,roziere2023code,wang2023codet5+}. LLMs greatly stimulate the efficiency and productivity of coding and software development~\cite{qian2023communicative}. Existing code based models focus on pretraining language models on code corpus~\cite{muennighoff2023octopack,luo2023wizardcoder,li2023starcoder,zheng2023codegeex} and using Chain-of-Thought (CoT) to prompt the coding ability of LLMs~\cite{wei2023chainofthought,huang2023codecot,li2023structured}. Nevertheless, forcing LLMs to directly generate the totally correct coding results is challenging even for skilled programmers in real-world scenarios~\cite{chen2023teaching}.

\begin{figure}[t] \centering
    \includegraphics[width=0.48\textwidth]{figures/image/cope_intro.pdf}
    \caption{The Illustration of INTERVENOR. There are two agents in INTERVENOR, the teacher and student, whom collaborate to conduct the interactive chain-of-repairing (CoR) to fix the code. The error messages are utilized as a kind of INTERVENORs\includegraphics[width=1.1em]{figures/image/title.jpg} to alleviate the Degeneration-of-Thought (DoT) problem.} \label{fig:intro}
\end{figure} Recently, researchers focus on self-improving code generation methods using Self-Repair methods~\cite{Olausson2023IsSA,chen2023teaching}, which employ LLMs to execute codes and repair codes by themselves for improving the quality of generated codes. Moreover, multi-agent collaborative coding approaches~\cite{qian2023communicative,dong2023selfcollaboration} have also proven their effectiveness in handling difficult code tasks by prompting LLMs to play different roles, such as developers and testers. However, lots of bugs are difficult to find due to the cognitive inertia~\cite{mcguire1960cognitive}--overlooking the buggy codes that may not conform to their pre-existing coding thinking of LLMs. Thus, these agent
based code refine methods~\cite{dong2023selfcollaboration,qian2023communicative} are highly dependent on the self-evaluation ability of LLMs and may face the Degeneration-of-Thought (DoT) problem~\cite{liang2023encouraging,shinn2023reflexion}. 



This paper proposes \textbf{INTER}acti\textbf{VE} chai\textbf{N} \textbf{O}f \textbf{R}epairing (INTERVENOR) to intervene the DoT problem in code repairing by incorporating the feedback from code compilers to facility the bug detection. However, there is a significant gap between compiler errors and task instructions~\cite{wang2022self} for code repair. Thus, INTERVENOR proposes an interactive Chain-of-Repairing (CoR) mechanism to bridge the gap from compiler feedback and repair instruction. Follow~\citet{dong2023selfcollaboration}, we design two agents \texttt{Code Learner} and \texttt{Code Teacher} to repair code. As shown in Figure~\ref{fig:intro}, the \texttt{Code Learner} is responsible for both initial code generation and iterative code repair, while the \texttt{Code Teacher} iteratively generates the CoR to guide the code repairing process for \texttt{Code Learner}. Different from Self-Debug~\cite{chen2023teaching}, \texttt{Code Teacher} leverages the bug report from compilers and the program generated by \texttt{Code Learner} to rethink and generate insightful error explanations and comprehensive planning on how to address the errors. This interactive code repair process will continue until the code learner successfully fixes all the code errors or reaches the predetermined maximum number of repair attempts. Besides, we collect the buggy codes that are generated from GPT-3.5 and build the CodeError dataset to facilitate the code repairing related tasks.



Our experiments demonstrate the effectiveness of INTERVENOR by outperforming the state-of-the-art code generation and code translation models. Notably, INTERVENOR also achieves about 13\% and 4.5\% improvements over the GPT-3.5 model~\cite{chatgpt} in code generation and code translation tasks, showing its ability to improve the quality of generated codes via iterative code repairing. Our further analyses illustrate that INTERVENOR is effective in leveraging the bug messages from code compilers, recognizing the reasons for code errors, and also providing correction planning in natural language. Thanks to the intervention of code compilers, INTERVENOR avoids thinking by LLMs themselves and can accurately diagnose the buggy codes and correct the assertion errors and name/syntax errors even in more difficult code generation scenarios. Our chain-of-repairing mechanism shows its effectiveness in rethinking code errors according to the feedback of code compilers. It gives LLMs the ability to avoid designing complex prompts for generating codes and achieve the best performance via only three-turn code repairing. INTERVENOR shows the potential in leveraging the feedback from environments or rule systems to evolve LLMs and achieve better coding quality~\cite{Olausson2023IsSA}.

 


\section{Related Work}
The code generation tasks~\cite{chen2021evaluating,austin2021program,zheng2023codegeex} aim to generate correct and executable code based on the given natural language description, which has drawn lots of attention from researchers. LLMs such as ChatGPT~\cite{chatgpt} and GPT-4~\cite{openai2023gpt4} have shown strong effectiveness in generating code of higher quality. To enhance the coding ability of LLMs, existing work focuses on code-specific pretraining and performs exceptionally well in code generation tasks~\cite{roziere2023code,li2023starcoder,luo2023wizardcoder,wang2023codet5+}.

Some models focus on utilizing some prompting techniques to enhance the coding capabilities of LLMs. CodeCoT~\cite{huang2023codecot} is inspired by Chain-of-Thought~\cite{wei2023chainofthought} and prompts the quality of generated codes using Code-CoT and the Self-exam CodeCoT methods. LLMs are asked to craft the code and design a set of test cases to generate and polish the codes. Structured Chain-of-Thought (SCoT)~\cite{li2023structured} further considers the program structure, such as sequences, branches, and loops, and prompts LLMs to generate intermediate reasoning steps with program structures. Nevertheless, forcing LLMs to directly generate the correct program results is challenging~\cite{chen2023teaching}.

To better polish the generated codes, existing work focuses on Self-Refine~\cite{Olausson2023IsSA} and Self-Repair~\cite{chen2023teaching} methods. The Self-Refine models improve the quality of generated codes LLMs by decoding multiple samples and then selecting the most suitable one based on certain criteria. To build tailored criteria, one approach is to execute the generated codes and select the most optimal one based on the execution results~\cite{ni2023lever,zhang2022coder,shi2022natural,Li_2022}. Another approach is to rerank multiple code solutions to select the final code~\cite{shi2022natural,zhang2022coder,chen2022codet,inala2022faultaware}. However, these methods need to consume a lot of computing resources to generate code candidates, which is inefficient~\cite{zhang2023selfedit}.

The other research direction is utilizing an iterative code repair approach to further improve the quality of generated codes~\cite{zhang2023selfedit,welleck2022generating,madaan2023selfrefine,shinn2023reflexion}. Self-Debug~\cite{chen2023teaching} utilizes the explanations generated by LLMs to fix self-generated code, whereas Self-Repair~\cite{Olausson2023IsSA} employs human-provided feedback for improvement. Self-Edit~\cite{zhang2023selfedit} uses error messages to refine the generated code, but it requires training an additional fault-aware editor, which involves additional complex tasks such as model design and construction of training data. Instead of directly incorporating feedback for code repair, INTERVENOR designs an additional agent to rethink the reasons for coder errors and generate the chain-of-repairing to teach the other agent to repair codes via natural language. 

Moreover, existing work~\cite{dong2023selfcollaboration,qian2023communicative} also designs a multi-agent collaborative approach to simulate the software development process and improve the efficiency of code generation. Nevertheless, these methods are highly dependent on the self-evaluation ability of LLMs and may face the Degeneration-of-Thought (DoT) problem~\cite{liang2023encouraging,shinn2023reflexion}. 
Unlike them, INTERVENOR focuses on the bug-fixing process and proposes a simple but effective solution, which utilizes external tools~\cite{qian2023creator}, such as the Python interpreter, to execute the code~\cite{xu2023lemur,qian2023creator} and regard the accurate bug report as the INTERVENOR to facility the agent collaboration during the interactive code repairing process.


 


\begin{figure*}[t] \centering
    \includegraphics[width=1\textwidth]{figures/image/interrepair_model.pdf}
    \caption{Illustration of Our  Interactive Chain of Repairing Model (INTERVENOR {\includegraphics[width=1.2em] {figures/image/title.jpg}}).} \label{fig:model}
\end{figure*} \section{Methodology}

In this section, we introduce INTERVENOR, which conducts an interactive program repairing by using multi-agent collaboration to enhance the coding ability of LLMs. We first describe the preliminary of code repairing (Sec.~\ref{model:pre}) and then introduce our multi-agent based interactive code repairing method (Sec.~\ref{model:codeagent}).

\subsection{Preliminary of Code Repairing}\label{model:pre}
The code repairing models mainly focus on \textit{self-repair}~\cite{Olausson2023IsSA,chen2023teaching}. These models usually consist of three steps, including code generation, code execution, and code explanation. \textit{Self-repair} aims to use LLM itself to conduct the self-debug and self-execution process and then iteratively repair the buggy codes. Nevertheless, programmers usually fail to recognize the code errors because of cognitive inertia, making the code execution more difficult by LLM itself.

Different from these self-repair models, INTERVENOR follows previous work~\cite{zhang2023selfedit,wang2022compilable} to incorporate the feedback from compilers to prompt the code generation ability of LLMs. Instead of directly feeding the code bug message to LLMs, we design the \textit{interactive chain of repairing mechanism}, which builds two agents to rethink and repair the code errors. The compiler serves as an INTERVENOR to avoid the Degeneration-of-Thought (DoT) problem during the interactive repairing process.

\subsection{Interactive Chain-of-Repairing (CoR)} \label{model:codeagent}
As shown in Figure \ref{fig:model}, given the coding task, INTERVENOR aims to mimic the human bug repairing behavior by iteratively acquiring feedback from compilers, then generating bug reasons and solving plans in natural language, and finally fixing the program. Specifically, INTERVENOR employs two LLM based agents to play different roles in code repairing, including code learner and coder teacher (Sec.~\ref{sec:agent}). Then conduct an interactive code repairing process with these agents (Sec.~\ref{sec:workflow}). 

\subsubsection{Agent Building}~\label{sec:agent}
INTERVENOR involves the integration of two agents, \textit{Code Learner} and \textit{Code Teacher}, who work interactively to repair the generated codes. The \textit{Code Learner} is asked to generate codes and repair codes according to the instruction from \textit{Code Teacher}. The \textit{Code Teacher} focuses more on rethinking and illuminating the code errors for students.

\textbf{Code Learner.} The code learner conducts two coding behaviors, including first time code generation and interactive code repairing.

In the first time code generation process, the code learner aims to produce the initial code version according to the requirements of the given coding task. Then we change the agent role for code repairing and use the instruction ``You are a student assistant with excellent code repair capabilities'' to activate the code repairing ability of LLMs. Such an instruction aims to stimulate the ability of LLMs to conduct execution by themselves, rethink the code errors according to the code repair instruction from the code teacher, and fix the bugs in the code.

\textbf{Code Teacher.} The code teacher plays a critical role in interactive code repairing. It aims to generate the code error explanation and bug fix planning for the code learner to help it repair codes.

Specifically, we use the prompt ``You are an experienced and insightful programming instructor, and you need to identify the bugs in the given code based on the error messages.'' to activate the bug tracing ability and code diagnosis ability. Subsequently, the teacher will offer detailed repair suggestions and guidance to help the student better comprehend and resolve the errors in the code.



\subsubsection{Interactive Code Repairing Workflow}~\label{sec:workflow}
INTERVENOR conducts an interactive code repairing process and we will introduce the communication among agents and code compiler.

At the initial step, we ask the code learner to generate codes for the given task (\textbf{Step A$^0$}, where $0$ denotes the initial turn). Then the code learner executes codes using the code compiler, which aims to evaluate the code correctness (\textbf{Step B}). Next, the code teacher generates the code repairing instruction according to the bug report and its corresponding codes. The instruction illuminates the correct direction, \textit{e.g.} ``modified to handle the case where
the input decimal is 0'', and the code correction planning \textit{e.g.} ``we can simply return "db0db". Such an instruction helps to form the \textit{chain of code repairing} in natural language to better guide the code learner (\textbf{Step C}). Finally, the code learner follows the \textit{chain of code repairing} to repair codes and resubmit the repaired ones to the compiler for executing in the next turn (\textbf{Step A$^i$}, where $i>=1$ and denotes the code repairing process). The A$^i$, B, and C steps are iterated in sequential order until the code passes the estimation of the compiler or the turn number reaches the max turn.

 





\section{Experimental Methodology}
In this section, we describe the datasets, evaluation metrics, baselines, and implementation details in our experiments.



\begin{table}[]
\small
\centering
\caption{Statistics of the data in our experiments. \#Tests represents the average number of test cases contained in each sample.}
\label{tab:data_statistic}
\begin{tabular}{llcc}
\hline
\textbf{Benchmark}           & \textbf{Language} & \textbf{Problems} & \textbf{\#Tests} \\ \hline
HumanEval                    & Python            & 164               & 7.8                \\ \hline
MBPP                         & Python            & 500               & 3.1                \\ \hline
\multirow{3}{*}{HumanEval-X} & C++               & 164               & 7.8                \\
                             & Java              & 164               & 7.8                \\
                             & JavaScript        & 164               & 7.8                \\ \hline
CodeError                    & Python            & 327               & 4.0                \\ \hline
\end{tabular}
\end{table}

%
 \textbf{Dataset.} We evaluate the code generation and translation effectiveness on three datasets, including \texttt{HumanEval}, \texttt{MBPP} and \texttt{HumanEval-X}. Then we build a new benchmark \texttt{CodeError} to further test the code repairing ability of INTERVENOR.

\texttt{HumanEval}~\cite{chen2021evaluating} serves as a benchmark for evaluating the functional correctness of synthesized programs generated from docstrings. It comprises 164 hand-written Python programming problems, which consist of function signatures, docstrings, bodies, and multiple unit tests. \texttt{MBPP}~\cite{austin2021program} is a benchmark that includes 974 introductory-level Python programming problems. Each problem comprises a problem statement, a code solution, and three automated test cases, and the task IDs that range from 11 to 510 are used for evaluation.
\texttt{HumanEval-X} ~\cite{zheng2023codegeex} is a benchmark that is used to assess a model's multi-programming language generation and translation capability. It consists of 820 human-crafted data instances, covering C++, Java, JavaScript, and Go. All data statistics are shown in Table~\ref{tab:data_statistic}. 

Then we build the \texttt{CodeError} benchmark to further evaluate the code repairing effectiveness of INTERVENOR. We collect data from \texttt{HumanEval} and \texttt{MBPP} and reserve the instances, which conduct code errors in the code generation experiment. Specifically, for these code generation tasks, we utilize GPT-3.5 to generate codes according to the task instructions. These generated codes usually contain various bugs and the code errors include syntax errors, logical errors, and others. In this paper, we use the Python interpreter to test the generated code and reserve the buggy codes that do not pass the Python interpreter. These buggy codes and error messages are used to construct the \texttt{CodeError} benchmark. We provide more details about the \texttt{CodeError} benchmark in Appendix ~\ref{app:codeerror}.





\textbf{Evaluation Metrics. } We use $Pass@k$~\cite{chen2021evaluating} to evaluate the code generation and translation effectiveness of different models, which is the same as the previous work~\cite{chen2021evaluating,zheng2023codegeex,li2023structured,chen2022codet,nijkamp2022codegen}.


\textbf{Baselines.} We first compare INTERVENOR with several code-oriented large language pretrained models, such as Incoder~\cite{fried2022incoder}, CodeGen~\cite{nijkamp2022codegen}, CodeGeeX~\cite{zheng2023codegeex}, CodeT5~\cite{wang2023codet5+}, StarCoder~\cite{li2023starcoder}, WizardCoder~\cite{luo2023wizardcoder}, and Llama based models~\cite{touvron2023llama,roziere2023code}. These models are pretrained on large-scale code corpora, demonstrating strong code generation capabilities. Additionally, we also compare INTERVENOR with some closed-source and high-performance large language models, \textit{e.g.} Claude\footnote{\url{https://www.anthropic.com/index/introducing-claude}}, GPT-3.5~\cite{chatgpt}, and GPT-4~\cite{openai2023gpt4}, which show strong emergent abilities, especially for the code generation tasks. In our experiments, GPT-3.5 is our main baseline model. Besides, we also compare Self-Debug~\cite{chen2023teaching} and the multi-agent collaborative method, Self-Collaboration~\cite{dong2023selfcollaboration} to show the code repairing ability of INTERVENOR.


\textbf{Implementation Details.} In our experiments, we use GPT-3.5 as the foundation model to build different agents in our INTERVENOR model. We set the temperature to 0.2 and the maximum generation length to 512 tokens. The maximum number of interactive code repairs is set to 5.



 






\section{Evaluation Results}
In this section, we evaluate the overall performance of INTERVENOR in code generation and translation tasks. Then we conduct ablation studies to investigate the effectiveness of different modules and also show the effectiveness of our Interactive Chain of Repairing mechanism in different testing scenarios. Finally, case studies are presented.

\begin{table*}[h]
\centering
\small
\caption{Overall performance of Different Models. We evaluate model effectiveness on code generation and code translation tasks using the $Pass@1$ evaluation metric. The baseline results are borrowed from corresponding papers.}
\label{tab:overall_code_gen}
\begin{tabular}{l|ccccc|cccc}
\hline
\multirow{3}{*}{\textbf{Model}} & \multicolumn{5}{c|}{\textbf{Code Generation}}                                                                                & \multicolumn{4}{c}{\textbf{Code Translation}}                 \\ \cline{2-10} 
                                & \multicolumn{1}{c|}{\textbf{HumanEval}} & \multicolumn{1}{c|}{\textbf{MBPP}} & \multicolumn{3}{c|}{\textbf{HumanEval-X}}     & \multicolumn{4}{c}{\textbf{Target Language}}                  \\
                                & \multicolumn{1}{c|}{Python}             & \multicolumn{1}{c|}{Python}        & C++           & Java          & JS            & Python        & C++           & Java          & JS            \\ \hline
InCoder~\cite{fried2022incoder}                         & \multicolumn{1}{c|}{15.2}               & \multicolumn{1}{c|}{19.4}          & 9.5           & 9.1           & 13.0          & -             & -             & -             & -             \\
CodeGen~\cite{nijkamp2022codegen}                         & \multicolumn{1}{c|}{18.3}               & \multicolumn{1}{c|}{20.9}          & 18.1          & 14.9          & 18.4          & 40.7          & 37.6          & 35.4          & 51.8         \\
CodeGeeX~\cite{zheng2023codegeex}                        & \multicolumn{1}{c|}{22.9}               & \multicolumn{1}{c|}{24.4}          & 17.1          & 20.0          & 17.6          & 68.5 & 43.6          &56.8          &45.2         \\
CodeT5+~\cite{wang2023codet5+}                         & \multicolumn{1}{c|}{30.9}               & \multicolumn{1}{c|}{-}             & -             & -             & -             & -             & -             & -             & -             \\
InstructCodeT5+~\cite{wang2023codet5+}                 & \multicolumn{1}{c|}{35.0}               & \multicolumn{1}{c|}{-}             & -             & -             & -             & -             & -             & -             & -             \\
PaLM-Coder~\cite{chowdhery2022palm}                      & \multicolumn{1}{c|}{35.9}               & \multicolumn{1}{c|}{47.0}          & -             & -             & -             & -             & -             & -             & -             \\
StarCoder~\cite{li2023starcoder}                       & \multicolumn{1}{c|}{40.8}               & \multicolumn{1}{c|}{49.5}          & -             & -             & -             & -             & -             & -             & -             \\
WizardCoder~\cite{luo2023wizardcoder}                     & \multicolumn{1}{c|}{57.3}               & \multicolumn{1}{c|}{51.8}          & -             & -             & -             & -             & -             & -             & -             \\
LLama2~\cite{touvron2023llama}                          & \multicolumn{1}{c|}{30.5}               & \multicolumn{1}{c|}{45.4}          & -             & -             & -             & -             & -             & -             & -             \\
CodeLLama~\cite{roziere2023code}                       & \multicolumn{1}{c|}{62.2}               & \multicolumn{1}{c|}{61.2}          & -             & -             & -             & -             & -             & -             & -             \\
PanGu-Coder2~\cite{shen2023pangucoder2}                    & \multicolumn{1}{c|}{61.6}               & \multicolumn{1}{c|}{-}             & -             & -             & -             & -             & -             & -             & -             \\
Claude~\cite{Claude}                          & \multicolumn{1}{c|}{47.6}               & \multicolumn{1}{c|}{-}             & -             & -             & -             & -             & -             & -             & -             \\
GPT-4~\cite{openai2023gpt4}                           & \multicolumn{1}{c|}{67.0}               & \multicolumn{1}{c|}{-}             & -             & -             & -             & -             & -             & -             & -             \\
Self-Debug~\cite{chen2023teaching}                      & \multicolumn{1}{c|}{73.8}               & \multicolumn{1}{c|}{-}             & -             & -             & -             & -             & -             & -             & -             \\
Self-Collaboration~\cite{dong2023selfcollaboration}              & \multicolumn{1}{c|}{74.4}               & \multicolumn{1}{c|}{68.2}          & -             & -             & -             & -             & -             & -             & -             \\ \hline
GPT-3.5~\cite{chatgpt}                 & \multicolumn{1}{c|}{60.3}               & \multicolumn{1}{c|}{39.8}          & 52.4          & 50.6          & 54.3          & 84.3          & 71.5          & 81.7          & 84.6          \\
INTERVENOR                      & \multicolumn{1}{c|}{\textbf{75.6}}      & \multicolumn{1}{c|}{\textbf{69.8}} & \textbf{67.1} & \textbf{68.3} & \textbf{67.1} & \textbf{89.8}          & \textbf{75.6} & \textbf{85.4} & \textbf{88.3} \\ \hline
\end{tabular}
\end{table*}



%
 \begin{table}[t]
\centering
\small

\caption{Code Translation Performance. We evaluate the code translation effectiveness among different program languages, including Python, C++, Java, and JavaScript (JS). We report the results of INTERVENOR, which only repair code with a single turn. All evaluation results are evaluated with $Pass@1$.}
\label{tab:overall_code_trans}
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{l|l|cccc}
\hline
\multirow{2}{*}{\textbf{Source}}                    & \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Target}}                          \\
                                     &                                 & \textbf{Python} & \textbf{C++}  & \textbf{Java} & \textbf{JS} \\ \hline
\multirow{4}{*}{\textbf{Python}}     & CodeGen               & -               & 35.9          & 29.3          & 43.4               \\
                                     & CodeGeeX                 & -               & 34.2          & 42.0          & 34.8                \\
                                     & GPT-3.5               & -               & 62.8          & 70.7          & 82.3                \\
                                     & INTERVENOR                            & -               & \textbf{67.7} & \textbf{75.1} & \textbf{87.8}       \\ \hline
\multirow{4}{*}{\textbf{C++}}        & CodeGen               & 33.8            & -             & 43.2          & 54.5                \\
                                     & CodeGeeX                 & 62.8            & -             & 71.7          & 50.8               \\
                                     & GPT-3.5               & 81.1            & -             & 89.6          & 82.3                \\
                                     & INTERVENOR                            & \textbf{90.2}   & -             & \textbf{92.1} & \textbf{86.1}       \\ \hline
\multirow{4}{*}{\textbf{Java}}       & CodeGen               & 52.7           & 41.4          & -             & 57.7                \\
                                     & CodeGeeX               &75.0   & 49.7          & -             & 50.0                \\
                                     & GPT-3.5               & 89.6            & 75.6          & -             & 89.1                \\
                                     & INTERVENOR                            & \textbf{92.1}            & \textbf{79.3} & -             & \textbf{90.9}       \\ \hline
\multirow{4}{*}{\textbf{JS}} & CodeGen               & 35.5            & 35.4          & 33.8           & -                   \\
                                     & CodeGeeX                 & 67.7   & 46.9          & 56.6          & -                   \\
                                     & GPT-3.5               & 82.3            & 76.2          & 84.8          & -                   \\
                                     & INTERVENOR                            & \textbf{87.2}            & \textbf{79.9} & \textbf{89.1} & -                   \\ \hline
\end{tabular}}
\end{table} 
\subsection{Overall Performance}
The overall performance of INTERVENOR in code generation and translation tasks is shown in Table~\ref{tab:overall_code_gen}. 

Overall, INTERVENOR outperforms all baselines in all tasks by achieving more than 1\% improvements, showing its effectiveness. Compared to our main baseline model GPT-3.5, INTERVENOR achieved about 13\% and  4.5\% improvements in code generation and code translation tasks, respectively. It illustrates that INTERVENOR has the ability to prompt the coding ability of LLMs by mimicking the human code repairing behavior--\textit{iteratively judging, rethinking, and repairing}. Notably, INTERVENOR also surpasses Self-Debug and Self-Collaboration models, demonstrating its ability to successfully intervene in the code generation/translation process and guide LLMs to better repair the codes using our chain of repairing mechanism. All these experimental results highlight the generalization ability of INTERVENOR in improving LLMs' coding ability in different languages. 


Furthermore, we present more detailed code translation results in Table \ref{tab:overall_code_trans}. In general, INTERVENOR shows the best performance across all twelve cross-language code translation tasks, demonstrating its ability to understand and translate codes. Compared with GPT-3.5, INTERVENOR improves the translation in all tasks, especially in the tasks that translate other languages into Python. This indicates that INTERVENOR is more effective in generating an effective code repair chain according to the bug reports from the Python interpreter. Such a phenomenon aligns with human intuition during the debugging process, namely that bugs in Python code are more easily resolved.



\begin{table*}[]
\centering
\small
\caption{Ablation Studies. We compare different prompting techniques to evaluate the effectiveness of our chain-of-repairing mechanism. All models are built based on GPT-3.5 and evaluated with $Pass@1$.}
\label{tab:prompt_tec}
\begin{tabular}{l|l|c|c|ccc|c|c}
\hline
\multirow{2}{*}{\textbf{Code Repair}} & \multirow{2}{*}{\textbf{Prompt Methods}} & \textbf{HumanEval} & \textbf{MBPP} & \multicolumn{3}{c|}{\textbf{HumanEval-X}} & \multicolumn{1}{l|}{\textbf{CodeError}} & \multirow{2}{*}{\textbf{Avg.}} \\
                          &       & Python             & Python        & C++        & Java       & JS      & Python                                  &                                   \\ \hline
\multirow{4}{*}{No Repair} & Zero-Shot            & 60.3               & 39.8          & 52.4       & 50.6       & 54.3            & -                                    & -                            \\
& Zero-Shot CoT          & 51.8               & 35.2          & 48.2       & 45.7       & 45.4            & -                                     & -                                 \\
&Few-Shot               & 62.2               & 45.4          & 53.1       & 62.2       & 43.3            & -                                     & -                             \\
& Few-Shot CoT                    & 60.4               & 45.4          & 48.2       & 63.4       & 57.9            & -                                     & -                              \\ \hline
\multirow{6}{*}{Single Turn} & Zero-Shot &62.2&41.6&54.3&65.2&60.4&4.9&48.1\\
 & Few-Shot &65.2&40.6&57.9&65.2&62.8&9.8&50.3\\
& CoT &66.5&48.8&56.7&62.2&60.4&9.8&50.7\\


& Self-Refine       & 65.2                  & 48.8             & 57.3          & 64.0          & 60.4              & 4.6                                     & 50.1                                 \\
& Error Msgs        & 67.1               & 51.8          & 57.3       & 59.8       & 62.8            & 9.8                                     & 51.4                              \\
& INTERVENOR (CoR)     & 69.5                  & 51.0             & 59.8          & 65.2          & 60.4              & 14.4                                     & 53.4                                 \\\hline
Multi-Turns & INTERVENOR  (CoR)                     & \textbf{75.6}               & \textbf{69.8}          & \textbf{67.1}       & \textbf{68.3}       & \textbf{67.1}            & \textbf{25.4}                                      & \textbf{62.2}                              \\ 
\hline
\end{tabular}
\end{table*}





%
 \subsection{Ablation Studies}
The ablation studies are conducted to show the effectiveness of the interactive CoR mechanism.

As shown in Table~\ref{tab:prompt_tec}, we conduct different methods to enhance the code generation ability of LLMs by prompting code generation and code repairing. We compare CoT~\cite{kojima2023large}, Few-Shot~\cite{chen2021evaluating}, and Few-Shot CoT~\cite{wei2023chainofthought} models in experiments, which prompt LLMs to better generate codes. These models try to generate natural language as the chain of coding thought (CoT) or provide some instances to demonstrate the coding task (Few-Shot). Then we compare different methods to generate the code repair instruction, including Self-Refine~\cite{madaan2023selfrefine}, Error Msgs, and Chain-of-Repairing (CoR). Self-Refine asks the to LLMs rethink the errors by themselves, while Error Msgs and CoR incorporate the code error messages from compilers. Msgs directly uses the error messages as instructions to guide code repair.


The experimental results show that code repairing is more effective than directly prompting LLMs to generate codes. Even though we conduct different methods to directly prompt LLMs to generate better codes, we achieve only 3.5\% improvements. On the contrary, the code repairing methods improve the quality of generated codes by achieving more than 9.7\% improvements with only single-turn repairing. Then we compare different code repairing methods. Both Error Msgs and CoR thrive on the feedback from code compilers and achieve more than 1.3\% improvements than Self-Refine, demonstrating that compilers can provide valuable signals to help LLMs better recognize the code bugs. Notably, CoR achieves the best performance among all code repairing models, which illustrates its effectiveness in teaching LLMs for code repairing. The CoR mechanism uses the error messages from code compilers to prompt LLMs, aiming to rethink the reasons for making errors and generate the instructions for repairing.



\subsection{Effectiveness of INTERVENOR in Different Testing Scenarios}
In this subsection, we delve deeper into exploring the effectiveness of INTERVENOR in two testing scenarios: 1) validating the impact of different code repairing turns, and 2) evaluating the code repairing effectiveness on different error types.

\begin{figure}[t] \centering
    \includegraphics[width=0.48\textwidth]{figures/image/Iterations.pdf}
    \caption{The Impact of Different Code Repairing Turns. HumanEval, MBPP, and HumanEval-X (HEX) are used to evaluate our INTERVENOR model.} \label{fig:iterations}
\end{figure} 





\begin{figure}[t]
    \centering
    \subfigure[AssertionError.] { \label{subfig:error_analysis_1} 
    \includegraphics[width=0.48\linewidth]{figures/image/repair_type_1.pdf}}
    \subfigure[Other Error Types.] { \label{subfig:error_analysis_2} 
    \includegraphics[width=0.48\linewidth]{figures/image/repair_type_2.pdf}}
    \caption{Code Repairing Performance on the CodeError Dataset. We repair the error codes with one single turn using different prompting methods. The codes are divided into two groups to show the effectiveness of code repairing, including Assertion Errors and Others (AttributeError, NameError, RecursionError, SyntacError and TypeError).}
    \label{fig:error_analysis}
\end{figure} As shown in Figure~\ref{fig:iterations}, the code generation performance is significantly improved during the iteratively repairing. After three turns, the INTERVENOR achieves almost the best performance on HumanEval and HumanEval-X datasets, showing the efficiency of our interactive chain-of-repairing mechanism. For the more difficult dataset MBPP, INTERVENOR achieves 30\% improvements and still has some room to achieve further improvement, demonstrating its effectiveness in dealing with more difficult and realistic coding tasks.


\begin{figure*}[h] \centering
    \includegraphics[scale=0.6]{figures/image/case_v2.pdf}
    \caption{Case Studies. We provide two cases that showcase the effectiveness of the Chain of Repairing generated by INTERVENOR when fixing AttributeError and AssertionError, respectively.} \label{fig:case}
\end{figure*}
 
Then we show the code repairing effectiveness on different error types in Figure \ref{fig:error_analysis}. We use different prompt methods to stimulate LLMs to repair code errors and show their effectiveness on different types of code errors. Overall, INTERVENOR doubles the number of corrected code examples of the baseline model on both AssertionError and other error types, showing its effectiveness in repairing the code errors. On the one hand, INTERVENOR significantly outperforms other methods on the AssertionError task.  It illustrates that INTERVENOR can provide more precise guidance and identify errors with the help of the bug report derived from the failed testing case. On the other hand, INTERVENOR also shows a strong ability to correct other code errors. Our chain-of-repairing method also thrives on the error messages of the compiler, breaks the cognitive inertia of LLMs, and identifies the specific code error for repairing. All these phenomena show that the quality of code execution feedback is critical in repairing codes.




\subsection{Case Studies}
Finally, we select two case studies in Figure \ref{fig:case} to demonstrate the effectiveness of INTERVENOR. We also compare INTERVENOR with Self-Refine~\cite{madaan2023selfrefine}, which prompts LLMs to execute codes, recognize bugs, and generate instructions by themselves.

Overall, the feedback from compilers indeed helps to improve the accuracy of code repairing by providing more valuable instruction. In the first case, Self-Refine fails to fix the AttributeError and adds the ``all()'' function in codes. On the contrary, INTERVENOR successfully fixed the code, showing its effectiveness. It accurately analyzes the reason for bugs ``call the islower() method on an integer object, which is not possible'' and also provides a solution by suggesting to ``check if the key is a string before calling the islower() method''. In the second case, the instruction provided by Self-Refine thinks the code seems to be correct and does not offer definite solutions, showing the cognitive inertia in debug--\textit{It is hard to debug the code written by ourselves}. INTERVENOR shows its effectiveness in directly generating the reason of bug ``the function is not handling the case where the input list has only one element correctly''. More cases are shown in Appendix~\ref{app:more_cases}.


 


\section{Conclusion}
This paper proposes INTERVENOR, which aims to generate the chain of repairing to prompt the coding ability of LLMs. INTERVENOR regards the feedback of compilers as the  INTERVENOR to facilitate the agent collaboration of rethinking and repairing. Our experiments show that INTERVENOR outperforms the state-of-the-art code generation and translation models by accurately diagnosing the buggy codes. 






\balance
\bibliography{custom}
\clearpage
\newpage
\appendix

\section{Appendix}
\subsection{More Details of the CodeError Dataset}\label{app:codeerror}
In this section, we show some data information of CodeError. Each sample in CodeError primarily consists of buggy code, error messages, and corresponding test cases to assess the functionality of the modified code, and the statistics are shown in Table ~\ref{tab:data_statistics}. As shown in Figure ~\ref{fig:error_distribution}, CodeError covers nine types of code error, and we provide detailed descriptions for each error type in Figure ~\ref{fig:error_description}.
\begin{table}[h]
\centering
\small
\caption{Data statistics of CodeError. We compute the average number of lines in buggy code, the average number of test cases, and the average length of error messages.}
\label{tab:data_statistics}
\begin{tabular}{l|ccc}
\hline
\textbf{Data Field}    & \textbf{Min} & \textbf{Max} & \textbf{Average} \\ \hline
\textbf{Buggy Code}    & 4            & 61           & 20           \\
\textbf{Test Cases}    & 1            & 26           & 4            \\
\textbf{Error Message} & 25           & 1928         & 76           \\ \hline
\end{tabular}
\end{table} 
\begin{figure*}[t] \centering
    \includegraphics[scale=0.56]{figures/image/error_distribution.pdf}
    \caption{The distribution of error types in CodeError. CodeError covers a total of 9 code error types.} \label{fig:error_distribution}
\end{figure*}
 \begin{figure*}[t] \centering
    \includegraphics[scale=0.60]{figures/image/error_description.pdf}
    \caption{Detailed descriptions of each error type in CodeError. We provide an overview of the causes behind each error.} \label{fig:error_description}
\end{figure*}
 

\begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/more_cases_3.pdf}
    \caption{Case Studies. We present two cases from CodeError. The chain of repairing clearly identifies the errors in the buggy code and provides the correct modification methods, which are highlighted in \textcolor{red}{\textbf{red}}.} \label{fig:more_cases3}
\end{figure*}

 \subsection{Case Studies}\label{app:more_cases}
In Figure ~\ref{fig:more_cases3}, we show two cases from CodeError to demonstrate the effectiveness of the Chain of repairing (CoR) mechanism.

In the first case, there is a simple NameError in the buggy code, which indicates that `hashlib' is not defined. We can see that the INTERVENOR recognizes hashlib as a Python package and provides a solution: ``import the `hashlib' module at the beginning of the code.'' In the second case, the error in the code is more subtle, involving an operation between `int' and `list'. The CoR explicitly states that this is a ``valid operation'' and provides a solution: ``check if the current element in the list is a list or not before adding it to the sum.'' This ultimately successfully corrects the code. These two cases demonstrate the effectiveness of the chain of repairing mechanism and highlight how the CoR effectively identifies errors and provides appropriate solutions for code repair.


\begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/prompt_1.pdf}
    \caption{Role Instructions in INTERVENOR. Within INTERVENOR, there are two LLM-based agents \textit{Code Teacher} and \textit{Code Learner}. We utilize specific instructions to ensure that they play the correct roles and carry out the intended tasks.} \label{fig:prompt}
\end{figure*}
 \begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/prompt_2.pdf}
    \caption{Zero-Shot, Zero-Shot CoT, and Few-Shot prompts for code generation tasks.} \label{fig:prompt_2}
\end{figure*}

\begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/prompt_3.pdf}
    \caption{Few-Shot CoT prompts for code generation tasks.} \label{fig:prompt_3}
\end{figure*}

\begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/prompt_4.pdf}
    \caption{Zero-Shot and Few-Shot prompts for code repairing. Task IDs 1, 2, 3 in HumanEval and HumanEval-X and task IDs 2, 3, 4 in MBPP are used to construct prompts.} \label{fig:prompt_4}
\end{figure*}

\begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/prompt_5.pdf}
    \caption{Few-Shot CoT prompts for code repairing. Task IDs 2, 3 in HumanEval and HumanEval-X and task IDs 2, 3, 4 in MBPP are used to construct prompts.} \label{fig:prompt_5}
\end{figure*}

\begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/prompt_selfrefine_1.pdf}
    \caption{Repair method generation and code repair prompts in Self-Refine.} \label{fig:prompt_6}
\end{figure*}

\begin{figure*}[h] \centering
    \includegraphics[scale=0.61]{figures/image/prompt_selfrefine_2.pdf}
    \caption{Prompt for code repair using only error messages.} \label{fig:prompt_7}
\end{figure*}




 \subsection{Prompts}\label{app:prompt}
In this section, we first show the role instructions used by \textit{Code Learner} and \textit{Code Teacher} in Figure ~\ref{fig:prompt}. Then, we provide prompts used in the ablation studies in Figures~\ref{fig:prompt_2} to~\ref{fig:prompt_7}.






 
\end{document}
