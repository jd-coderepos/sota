\section{Experiments}
\label{sec:exp}

We have implemented our method in PyTorch~\cite{paszke2019pytorch}. The ImageNet-pretrained ResNet-50 backbone (encoder block 1$\sim$4) is loaded from torchvision. The batch size is 16. The learning rate is initialized as $10^{-4}$ and changed to $10^{-5}$ after 200 epochs. We use random flipping, resizing and cropping for data augmentation. Our model is trained with the  AdamW~\cite{loshchilov2017decoupled} optimizer for 300 epochs, which takes around 35 hours on an NVIDIA RTX A6000 GPU with 48GB of RAM.


\subsection{Evaluation Metrics}

We adopt standard segmentation metrics: mean absolute error (MAE), intersection over union (IOU), maximum F-measure ($F_\beta$) and balanced error rate (BER). These four metrics are commonly used in previous glass segmentation papers~\cite{mei2020don,lin2021rich,He_2021_ICCV}, whereas only MAE is valid for evaluating images without glass (\ie, GT mask is black). To assess our performance on those images without glass in our new dataset, together with MAE, we use inverse intersection over union (IOU$^\ast$) and false positive rate (FPR). IOU$^\ast$ takes inverse masks and is defined as $\text{IOU}(1-\text{result}, 1-\text{GT})$. FPR is calculated as the ratio of the number of false positives (\ie, glass are wrongly detected) to the total number of images without glass. Because  MAE is commonly used, we use it to evaluate on all images.


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figures/qualitative_arrow_t.jpg}
\caption{Qualitative comparison of our method and 5 state-of-the-art methods (HDFNet~\cite{pang2020hierarchical}, ESANet~\cite{seichter2021efficient}, CLNet~\cite{zhang2021rgb}, SPNet~\cite{zhou2021specificity}, and VST~\cite{liu2021visual}). Results of our RGB-only and thermal-only variants are also displayed. For better visualization, we set the image border of each mask to black. The superiority of our method can be clearly validated at various places, as highlighted by the red arrows.}
\label{fig:qualitative}
\end{figure*}

\begin{table}[t]
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{5pt}
\centering
\caption{Quantitative evaluations on RGB-only glass segmentation dataset GDD~\cite{mei2020don}. Evaluation results except ours are directly copied from~\cite{He_2021_ICCV}. The colors \textcolor{blue}{blue} and \textcolor{cyan}{cyan} represent the best and the second best methods, respectively.}
\label{tab:GDD}
\begin{tabular}{lccc}
\toprule
Method          & MAE$\downarrow$   & IOU$\uparrow$  & BER$\downarrow$  \\ 
\midrule
MirrorNet~\cite{yang2019my}       & 0.094 & 81.3 & 8.98 \\ 
GDNet~\cite{mei2020don}           & 0.088 & 82.6 & 8.42 \\ 
Translab~\cite{xie2020segmenting}        & 0.081 & 85.1 & 7.43 \\ 
EBLNet~\cite{He_2021_ICCV}          & \textcolor{cyan}{0.074} & \textcolor{cyan}{86.0} & \textcolor{blue}{6.90} \\ \hline
Ours (RGB-only) & \textcolor{blue}{0.072} & \textcolor{blue}{86.6} & \textcolor{cyan}{7.35} \\ 
\bottomrule
\end{tabular}
\end{table}


\subsection{Quantitative Evaluations}
\label{sec:quan}
RGB-T image pairs are new cues for glass segmentation. We compare three variants of our method with 24 state-of-the-art methods from other related areas, which include ShapeConv~\cite{cao2021shapeconv}, ESANet~\cite{seichter2021efficient} and CMX~\cite{liu2022cmx} for RGB-D semantic segmentation, RTFNet~\cite{sun2019rtfnet} for RGB-T semantic segmentation, Segformer~\cite{xie2021segformer} and Segmenter~\cite{strudel2021segmenter} for RGB-only semantic segmentation, MCNet~\cite{xiong2021mcnet} for thermal-only semantic segmentation, DPANet~\cite{chen2020dpanet}, HAINet~\cite{li2021hierarchical}, SSF~\cite{zhang2020select}, UCNet~\cite{zhang2020uc}, CoNet~\cite{ji2020accurate}, ATSA~\cite{zhang2020asymmetric}, DANet~\cite{zhao2020single}, HDFNet~\cite{pang2020hierarchical}, FRDT~\cite{zhang2020feature}, RD3D~\cite{chen2021rgb}, DCFNet~\cite{ji2021calibrated}, UTA~\cite{zhao2021rgb}, EBS~\cite{zhang2021learning}, VST~\cite{liu2021visual}, CLNet~\cite{zhang2021rgb} and SPNet~\cite{zhou2021specificity} for RGB-D salient object detection, and Zhang~\etal~\cite{zhang2020revisiting} for RGB-T saliency detection. All of these competing methods are retrained with our dataset using RGB-T pairs as input. We also compare with a state-of-the-art RGB-only glass segmentation method EBLNet\footnote{Other recent glass segmentation methods~\cite{yang2019my, mei2020don,lin2021rich} did not provide training codes for their models.}~\cite{He_2021_ICCV}, which is retrained with the RGB images in our dataset. 


As shown in Table~\ref{tab:comp}, the last three rows give the ablation results of our full RGB-T method and its two variants that use either thermal or RGB data only. Albeit the thermal-only variant is the worst, it can significantly boost the performance when it is combined with RGB images. For example, the MAE of our RGB-T method on glass images is $86\%$ and $52\%$ better than the two variants, which demonstrates the effectiveness of  our RGB-T fusion idea for glass segmentation. Please refer to the Section~\ref{sec:ablation} for more ablation studies.

Our approach consistently outperforms previous methods on most metrics by a large margin. Similar to our method, CMX, EBS and Segformer utilize a combination of convolution and transformer (hybrid) in their architectures, which achieve superior performance in the evaluations of RGB-T and RGB-only, respectively. We attribute it to the hybrid architecture~\cite{carion2020end, xu2021line}, while pure convolution (\eg, ESANet, CLNet, SPNet, EBLNet) or transformer (\eg, VST, Segmenter) architectures obtain inferior results. 

The performance of our thermal-only variant and MCNet is much worse than that of other RGB-only and RGB-T methods. We believe that there are two reasons. First, the resolution of thermal images is lower than that of RGB images. Hence, the segmentation results of thermal images are poor. Second, even though the glass is opaque to the thermal light, we need to compare it with the RGB images to see the contrast. Hence, it is difficult, if not impossible, to distinguish glass and other opaque objects using a single thermal image. 




\begin{table}[t]
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{5pt}
\centering
\caption{Quantitative evaluations on RGB-T SOD dataset VT5000~\cite{tu2022rgbtbench} and VT1000~\cite{tu2019rgb}.  The colors \textcolor{blue}{blue} and \textcolor{cyan}{cyan} represent the best and the second best methods, respectively.}
\label{tab:rgbt_sod}
\begin{tabular}{lcccccc} 
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{VT5000~\cite{tu2022rgbtbench}}                           & \multicolumn{3}{c}{VT1000~\cite{tu2019rgb}}                            \\ 
\cmidrule[0.25pt](lr){2-4} \cmidrule[0.25pt](lr){5-7}
                        & MAE $\downarrow$ & S$_m\uparrow$ & F$_\beta\uparrow$ & MAE $\downarrow$ & S$_m\uparrow$ & F$_\beta\uparrow$  \\ 
\midrule
MTMR~\cite{wang2018rgb}                    & 0.114            & 0.680         & 0.662             & 0.119            & 0.706         & 0.755               \\ 

M3S-NIR~\cite{tu2019m3s}                 & 0.188            & 0.631         & 0.607             & 0.151            & 0.717         & 0.734               \\ 

SGDL~\cite{tu2019rgb}                    & 0.089            & 0.750         & 0.738             & 0.090            & 0.787         & 0.807               \\ 

ADF~\cite{tu2022rgbtbench}                     & 0.048            & 0.864         & 0.864             & 0.034            & 0.910         & 0.923               \\ 

MIDD~\cite{tu2021multi}                    & 0.043            & 0.868         & 0.872             & 0.027            & 0.915         & 0.926               \\ 

APNet~\cite{zhou2021apnet}                   & \textcolor{blue}{0.035}            & 0.876         & 0.875             & \textcolor{cyan}{0.021}            & 0.921         & 0.930               \\ 

ECFFNet~\cite{zhou2021ecffnet}                 & 0.038            & 0.874         & 0.872             & \textcolor{cyan}{0.021}            & 0.923         & 0.930               \\ 

MIA-DPD~\cite{liang2022multi}                 & 0.040            & \textcolor{cyan}{0.879}         & \textcolor{cyan}{0.880}             & 0.025            & 0.924         & \textcolor{cyan}{0.935}               \\ 

DCNet~\cite{tu2022weakly}                   & \textcolor{blue}{0.035}            & 0.872         & 0.870             & \textcolor{cyan}{0.021}            & 0.922         & 0.928               \\ 
\hline
Ours                    & \textcolor{cyan}{0.036}            & \textcolor{blue}{0.881}         & \textcolor{blue}{0.881}             & \textcolor{blue}{0.020}            & \textcolor{blue}{0.929}         & \textcolor{blue}{0.941}               \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Qualitative Evaluations}
Figure~\ref{fig:qualitative} shows the qualitative comparison results. Our method is able to accurately segment the glass regions in various challenging scenes, while previous methods and the RGB-only variant produce a plethora of noticeable errors: (1) blurry segmentation results and fuzzy boundaries, (2) under-segmentation masks due to the influence by the background behind glass, and (3) over-segmentation results where cabinet or door openings are wrongly identified as glass.
The last two rows show two extreme cases where the scenes are completely covered by glass. 
The human bodies in the thermal images are reflections of the photographer, which are invisible in the RGB images, further validating the different transmission models of visual and thermal light through glass. Our thermal-only variant tends to recognize the distinct boundaries which may mislead the segmentation. We provide more visual results in the supplementary materials.




\begin{table*}[ht]
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{5pt}
\centering
\caption{Ablation studies on input. The colors \textcolor{blue}{blue} and \textcolor{cyan}{cyan} represent the best and the second best methods, respectively.}
\label{tab:ab_input}             
\begin{tabular}{lcccccccc} 
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Images with glass} & \multicolumn{3}{c}{Images without glass} & All images        \\ 
\cmidrule[0.25pt](lr){2-5} \cmidrule[0.25pt](lr){6-8} \cmidrule[0.25pt](lr){9-9} 
& MAE $\downarrow$                      & IOU $\uparrow$                        & F$_\beta\uparrow$                     & BER $\downarrow$                      & MAE $\downarrow$                      & IOU$^\ast\uparrow$                    & FPR $\downarrow$                     & MAE $\downarrow$  \\ 
\midrule
     
Thermal-only & 0.189 & 68.63 & 0.781 & 19.315 & 0.136 & 86.68 & 0.55 & 0.018\\ 
Dual-Thermal & 0.189 & 68.47 & 0.783 & 19.120 & 0.120 & 88.34 & 0.50 & 0.018\\ 
RGB-only & \multicolumn{1}{c}{0.056} & \multicolumn{1}{c}{88.94} & \multicolumn{1}{c}{0.929} & 6.618 & \multicolumn{1}{c}{\textcolor{cyan}{0.016}} & \multicolumn{1}{c}{\textcolor{cyan}{98.42}} & 0.11 & 0.052\\ 
Dual-RGB & \multicolumn{1}{c}{\textcolor{cyan}{0.055}} & \multicolumn{1}{c}{\textcolor{cyan}{89.21}} & \multicolumn{1}{c}{\textcolor{cyan}{0.932}} & \textcolor{cyan}{6.250} & \multicolumn{1}{c}{0.018} & \multicolumn{1}{c}{98.25} & \textcolor{cyan}{0.10} & \textcolor{cyan}{0.050}\\ \hline

RGB-T (Ours) & \multicolumn{1}{c}{\textcolor{blue}{0.027}} & \multicolumn{1}{c}{\textcolor{blue}{93.80}} & \multicolumn{1}{c}{\textcolor{blue}{0.965}} & \textcolor{blue}{4.078} & \multicolumn{1}{c}{\textcolor{blue}{0.003}} & \multicolumn{1}{c}{\textcolor{blue}{99.73}} & \textcolor{blue}{0.07} & \textcolor{blue}{0.024}\\ 
\bottomrule
\end{tabular}
\end{table*}




\subsection{Evaluations on GDD dataset~\cite{mei2020don}}
\label{sec:GDD}
Table~\ref{tab:GDD} compares our RGB-only variant with four state-of-the-art glass segmentation methods: MirrorNet~\cite{yang2019my}, GDNet~\cite{mei2020don}, Translab~\cite{xie2020segmenting} and EBLNet~\cite{He_2021_ICCV}. Both our variant and compared methods are trained and tested on the RGB-only glass segmentation dataset GDD~\cite{mei2020don}. Our RGB-only variant removes the thermal branch from our RGB-T architecture and the resulted RGB-only architecture is a combination of convolution and transformer, while the four existing methods adopt convolution only. 
Our RGB-only variant achieves the best in IOU and MAE and the second best in BER, echoing recent other recognition methods (\eg, object detection~\cite{carion2020end}, wireframe parsing~\cite{xu2021line}) which also demonstrate the effectiveness of the hybrid structure of convolution and transformer.



\subsection{Evaluations on RGB-T SOD datasets}
To show the versatility of our method on other RGB-T tasks, we retrain our model on the 2500 training images from VT5000 dataset~\cite{tu2022rgbtbench} for RGB-T salient object detection (SOD), then evaluate on the 2500 testing images from VT5000 dataset and 1000 images from VT1000 dataset~\cite{tu2019rgb}. Nine state-of-the-art RGB-T SOD methods are compared including three conventional graph-based methods (MTMR~\cite{wang2018rgb}, M3S-NIR~\cite{tu2019m3s}, SGDL~\cite{tu2019rgb}) and six deep learning based methods (ADF~\cite{tu2022rgbtbench}, MIDD~\cite{tu2021multi}, APNet~\cite{zhou2021apnet}, ECFFNet~\cite{zhou2021ecffnet}, MIA-DPD~\cite{liang2022multi}, DCNet~\cite{tu2022weakly}). We take the mean absolute error (MAE), S-measure ($S_m$)~\cite{fan2017structure}, and maximum F-measure ($F_\beta$) to evaluate the SOD results. As shown in Table~\ref{tab:rgbt_sod}, our method is comparable with others specifically designed for RGB-T SOD.



\begin{table*}[ht]
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{5pt}
\centering
\caption{Ablation studies on MFM. The colors \textcolor{blue}{blue} and \textcolor{cyan}{cyan} represent the best and the second best methods, respectively.}
\label{tab:ab_mfm}
\begin{tabular}{lcccccccc} 
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Images with glass} & \multicolumn{3}{c}{Images without glass} & All images        \\ 
\cmidrule[0.25pt](lr){2-5} \cmidrule[0.25pt](lr){6-8} \cmidrule[0.25pt](lr){9-9} 
& MAE $\downarrow$                      & IOU $\uparrow$                        & F$_\beta\uparrow$                     & BER $\downarrow$                      & MAE $\downarrow$                      & IOU$^\ast\uparrow$                    & FPR $\downarrow$                     & MAE $\downarrow$  \\ 
\midrule
SFS & \multicolumn{1}{c}{0.058} & \multicolumn{1}{c}{89.88} & \multicolumn{1}{c}{0.946} & 5.865 & \multicolumn{1}{c}{0.041} & \multicolumn{1}{c}{96.08} & 0.29 & 0.045\\ 
SFC & \multicolumn{1}{c}{0.052} & \multicolumn{1}{c}{90.47} & \multicolumn{1}{c}{0.945} & 5.588 & \multicolumn{1}{c}{0.025} & \multicolumn{1}{c}{98.15} & 0.36 & 0.043\\ 
PAF~\cite{kalra2020deep} & \multicolumn{1}{c}{0.038} & \multicolumn{1}{c}{91.73} & \multicolumn{1}{c}{0.954} & 5.188 & \multicolumn{1}{c}{0.010} & \multicolumn{1}{c}{99.04} & \textcolor{blue}{0.05} & 0.035\\ 
AT~\cite{li2020rgb} & \multicolumn{1}{c}{0.038} & \multicolumn{1}{c}{91.96} & \multicolumn{1}{c}{0.953} & 4.885 & \multicolumn{1}{c}{0.016} & \multicolumn{1}{c}{98.48} & 0.20 & 0.033\\ 
MFM-EGFNet~\cite{zhou2022edge} & \multicolumn{1}{c}{\textcolor{cyan}{0.030}} & \multicolumn{1}{c}{92.82} & \multicolumn{1}{c}{\textcolor{cyan}{0.960}} & 5.328 & \multicolumn{1}{c}{0.006} & \multicolumn{1}{c}{99.42} & 0.10 & 0.028\\
MFM-DS & \multicolumn{1}{c}{0.031} & \multicolumn{1}{c}{\textcolor{cyan}{93.06}} & \multicolumn{1}{c}{0.959} & \textcolor{cyan}{4.392} & \multicolumn{1}{c}{\textcolor{cyan}{0.004}} & \multicolumn{1}{c}{99.59} & \textcolor{blue}{0.05} & \textcolor{cyan}{0.027}\\ 
MFM-DC & \multicolumn{1}{c}{0.033} & \multicolumn{1}{c}{92.75} & \multicolumn{1}{c}{0.958} & 4.565 & \multicolumn{1}{c}{\textcolor{blue}{0.003}} & \multicolumn{1}{c}{\textcolor{cyan}{99.69}} & \textcolor{cyan}{0.06} & 0.029\\ \hline


MFM (Ours) & \multicolumn{1}{c}{\textcolor{blue}{0.027}} & \multicolumn{1}{c}{\textcolor{blue}{93.80}} & \multicolumn{1}{c}{\textcolor{blue}{0.965}} & \textcolor{blue}{4.078} & \multicolumn{1}{c}{\textcolor{blue}{0.003}} & \multicolumn{1}{c}{\textcolor{blue}{99.73}} & 0.07 &\textcolor{blue}{0.024}\\ 
\bottomrule
\end{tabular}
\end{table*}







\begin{table*}[ht]
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{5pt}
\centering
\caption{Ablation studies on backbones. The colors \textcolor{blue}{blue} and \textcolor{cyan}{cyan} represent the best and the second best methods, respectively.} 
\label{tab:ab_backbone}
\begin{tabular}{lcccccccc} 
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Images with glass} & \multicolumn{3}{c}{Images without glass} & All images        \\ 
\cmidrule[0.25pt](lr){2-5} \cmidrule[0.25pt](lr){6-8} \cmidrule[0.25pt](lr){9-9} 
& MAE $\downarrow$                      & IOU $\uparrow$                        & F$_\beta\uparrow$                     & BER $\downarrow$                      & MAE $\downarrow$                      & IOU$^\ast\uparrow$                    & FPR $\downarrow$                     & MAE $\downarrow$  \\ 
\midrule
ResNet-18  & 0.033 & 92.34 & 0.956 & 5.388 & \textcolor{cyan}{0.006} & \textcolor{cyan}{99.48} & \textcolor{cyan}{0.09} & 0.031\\
ResNet-34 & 0.031 & 92.96 & 0.959 & 5.049 & 0.011 & 98.90 & 0.12 & 0.030\\
ResNet-101 & \multicolumn{1}{c}{\textcolor{cyan}{0.030}} & \multicolumn{1}{c}{\textcolor{cyan}{93.14}} & \multicolumn{1}{c}{\textcolor{cyan}{0.961}} & \textcolor{cyan}{4.982} & \multicolumn{1}{c}{0.010} & \multicolumn{1}{c}{99.04} & \textcolor{blue}{0.07} & \textcolor{cyan}{0.029}\\
ResNet-50 w/o pretraining  & \multicolumn{1}{c}{0.057} & \multicolumn{1}{c}{87.29} & \multicolumn{1}{c}{0.924} & 8.063 & \multicolumn{1}{c}{0.064} & \multicolumn{1}{c}{94.02} & 0.50 & 0.057\\ \hline
ResNet-50 (Ours) & \multicolumn{1}{c}{\textcolor{blue}{0.027}} & \multicolumn{1}{c}{\textcolor{blue}{93.80}} & \multicolumn{1}{c}{\textcolor{blue}{0.965}} & \textcolor{blue}{4.078} & \multicolumn{1}{c}{\textcolor{blue}{0.003}} & \multicolumn{1}{c}{\textcolor{blue}{99.73}} & \textcolor{blue}{0.07} &\textcolor{blue}{0.024}\\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[ht]
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{5pt}
\centering
\caption{Ablation studies on decoder. The colors \textcolor{blue}{blue} and \textcolor{cyan}{cyan} represent the best and the second best methods, respectively.} 
\label{tab:ab_decoder}
\begin{tabular}{lcccccccc} 
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Images with glass} & \multicolumn{3}{c}{Images without glass} & All images        \\ 
\cmidrule[0.25pt](lr){2-5} \cmidrule[0.25pt](lr){6-8} \cmidrule[0.25pt](lr){9-9} 
& MAE $\downarrow$                      & IOU $\uparrow$                        & F$_\beta\uparrow$                     & BER $\downarrow$                      & MAE $\downarrow$                      & IOU$^\ast\uparrow$                    & FPR $\downarrow$                     & MAE $\downarrow$  \\ 
\midrule

Decoder-DS & \multicolumn{1}{c}{\textcolor{cyan}{0.033}} & \multicolumn{1}{c}{\textcolor{cyan}{92.87}} & \multicolumn{1}{c}{\textcolor{cyan}{0.960}} & \textcolor{cyan}{4.460} & \multicolumn{1}{c}{\textcolor{cyan}{0.007}} & \multicolumn{1}{c}{\textcolor{cyan}{99.30}} & 0.11 & \textcolor{cyan}{0.030}\\ 
Decoder-DC & \multicolumn{1}{c}{0.034} & \multicolumn{1}{c}{92.42} & \multicolumn{1}{c}{0.956} & 4.907 & \multicolumn{1}{c}{0.008} & \multicolumn{1}{c}{99.24} & \textcolor{cyan}{0.10} & 0.031\\ \hline

Decoder (Ours) & \multicolumn{1}{c}{\textcolor{blue}{0.027}} & \multicolumn{1}{c}{\textcolor{blue}{93.80}} & \multicolumn{1}{c}{\textcolor{blue}{0.965}} & \textcolor{blue}{4.078} & \multicolumn{1}{c}{\textcolor{blue}{0.003}} & \multicolumn{1}{c}{\textcolor{blue}{99.73}} & \textcolor{blue}{0.07} &\textcolor{blue}{0.024}\\ 
\bottomrule
\end{tabular}
\end{table*}



\begin{table*}[ht]
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{5pt}
\centering
\caption{Ablation studies on images without glass. The colors \textcolor{blue}{blue} and \textcolor{cyan}{cyan} represent the best and the second best methods, respectively.} 
\label{tab:ab_glass}
\begin{tabular}{lcccccccc} 
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Images with glass} & \multicolumn{3}{c}{Images without glass} & All images        \\ 
\cmidrule[0.25pt](lr){2-5} \cmidrule[0.25pt](lr){6-8} \cmidrule[0.25pt](lr){9-9} 
& MAE $\downarrow$                      & IOU $\uparrow$                        & F$_\beta\uparrow$                     & BER $\downarrow$                      & MAE $\downarrow$                      & IOU$^\ast\uparrow$                    & FPR $\downarrow$                     & MAE $\downarrow$  \\ 
\midrule


Training w/ glass images only & \multicolumn{1}{c}{\textcolor{cyan}{0.029}} & \multicolumn{1}{c}{\textcolor{cyan}{93.03}} & \multicolumn{1}{c}{\textcolor{cyan}{0.960}} & \textcolor{cyan}{5.116} & \multicolumn{1}{c}{\textcolor{cyan}{0.006}} & \multicolumn{1}{c}{\textcolor{cyan}{99.36}} & \textcolor{cyan}{0.09} & \textcolor{cyan}{0.028}\\ \hline
Training w/ all images (Ours) & \multicolumn{1}{c}{\textcolor{blue}{0.027}} & \multicolumn{1}{c}{\textcolor{blue}{93.80}} & \multicolumn{1}{c}{\textcolor{blue}{0.965}} & \textcolor{blue}{4.078} & \multicolumn{1}{c}{\textcolor{blue}{0.003}} & \multicolumn{1}{c}{\textcolor{blue}{99.73}} & \textcolor{blue}{0.07} &\textcolor{blue}{0.024}\\ 
\bottomrule
\end{tabular}
\end{table*}




\subsection{Ablation Studies and Limitations}\label{sec:ablation}
Below we evaluate the contributions of different components of our architecture, followed by limitation analysis.

\mysubsubsection{Impact of inputs} In Table~\ref{tab:comp}, we obtain our RGB-only or thermal-only variant by removing the corresponding encoders from our architecture. To verify that the performance decrease is not due to the architectural change, we additionally train another two models by inputting the same RGB/thermal images to both encoders (dual-RGB/thermal). Table~\ref{tab:ab_input} shows that the differences between RGB/thermal-only and dual-RGB/thermal are negligible, validating that the accuracy gap is because of using single modality rather than architectural differences. 


\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/failure2.pdf}
\caption{Two typical failure examples. The red arrow highlights small glass regions.}
\label{fig:fail}
\end{figure}

\mysubsubsection{Effectiveness of MFM} We compare our method with four simple baselines by replacing the MFM with the simple feature summation (SFS), simple feature concatenation (SFC), the pixel-wise attention fusion (PAF)~\cite{kalra2020deep} and affine transform (AT)~\cite{li2020rgb}. The multi-modal fusion module from EGFNet~\cite{zhou2022edge} for RGB-T fusion are also utilized to replace our MFM. We also modify our MFM by replacing the weighted summation with direct summation (DS) or direct concatenation (DC), where the attention weights are discarded in the two variants. As shown in Table~\ref{tab:ab_mfm}, our variants using the MFM (the last row) achieves the best performance. 

\mysubsubsection{Selection of backbones} Following other related methods~\cite{chen2020dpanet, li2021hierarchical, zhang2020select, zhang2020uc, ji2020accurate}, we exploit a pretrained ResNet backbone. In Table~\ref{tab:ab_backbone}, we evaluate different pretrained backbones and the ResNet-50 backbone without pretraining. We can see that  pretraining is critical for the performance and the ResNet-50 yields the best results. We believe that the performance decrease of ResNet-101 is due to  over-parameterization which makes it hard to train.

\mysubsubsection{Variants of our decoder} Similar to the evaluation of the weighted summation in MFM, we also replace the weighted summation in decoder blocks with direct summation (DS) or direct concatenation (DC). As shown in Table~\ref{tab:ab_decoder}, our decoder that uses weighted summation outperforms the other two variants.



\mysubsubsection{Images without glass} To reduce the false positive segmentation, our training data includes 370 RGB-thermal images pairs without any glass. As shown in Table~\ref{tab:ab_glass}, the model trained on only glass images has an obvious performance decrease on both images with and without glass, which demonstrates the necessity of such samples.




\mysubsubsection{Failure cases} The first row in Fig.~\ref{fig:fail} shows that our method fails when the visual appearances of glass and non-glass regions are highly similar in both RGB and thermal images, which is also a difficult, if not impossible, task for the human eye to differentiate without looking at the GT mask beforehand. Our method also fails to detect small glass regions, as highlighted by the red arrow in the second row.






