\documentclass{article}

\usepackage[numbers]{natbib}
\usepackage[preprint]{neurips_2022}

\usepackage[dvipsnames]{xcolor}         \definecolor{linkColor}{rgb}{0.18,0.39,0.62}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks=true,linkcolor=linkColor,citecolor=linkColor,filecolor=linkColor,urlcolor=linkColor]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{capt-of}
\usepackage{tabularx}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{scalerel}
\usepackage[inline]{enumitem}
\usepackage{listings}
\usepackage{varwidth}
\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\newcommand{\crefrangeconjunction}{--}
\usepackage{stmaryrd}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{pifont}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\tx}[1]{``\textit{#1}''}
\newcommand{\sptk}[1]{\texttt{[#1]}}




\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{officeblue}{RGB}{0,102,204}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{mybrickred}{RGB}{182,50,28}
\newcommand\mybox[2][]{\tikz[overlay]\node[inner sep=1pt, anchor=text, rectangle, rounded corners=0mm,#1] {#2};\phantom{#2}}
\definecolor{fillcolor}{RGB}{216,217,252}
\newcommand\bg[1]{\mybox[fill=blue!20]{#1}}
\newcommand\graybox[1]{\mybox[fill=gray!20]{#1}}



\newcommand*\AlgCommentInLine[1]{{\color{deepblue}{ \textit{#1}}}}
 

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 



\usepackage{pifont}\newcommand{\cmark}{{\ding{51}}}\newcommand{\xmark}{{\ding{55}}}



\newcommand\our{\textsc{BEiT}}

\title{\our{}: BERT Pre-Training of Image Transformers}

\author{Hangbo Bao\thanks{Contribution during internship at Microsoft. Correspondence to: Li Dong\textless{}\href{mailto:lidong1@microsoft.com}{lidong1@microsoft.com}\textgreater{}, Furu Wei\textless{}\href{mailto:fuwei@microsoft.com}{fuwei@microsoft.com}\textgreater{}},~~Li Dong,~~Songhao Piao,~~Furu Wei \\
~Harbin Institute of Technology \\
~Microsoft Research \\
\url{https://aka.ms/beit} \\
}

\begin{document}

\maketitle

\begin{abstract}
We introduce a self-supervised vision representation model \textbf{\our{}}, which stands for \textbf{B}idirectional \textbf{E}ncoder representation from \textbf{I}mage \textbf{T}ransformers. Following BERT~\citep{bert} developed in the natural language processing area, we propose a \textit{masked image modeling} task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as  pixels), and visual tokens (i.e., discrete tokens). We first ``tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training \our{}, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Transformer~\citep{transformer} has achieved promising performance in computer vision~\citep{vit,deit}.
However, empirical studies show that vision Transformers require more training data than convolutional neural networks.
In order to solve the data-hungry issue~\citep{vit:small:data}, self-supervised pre-training is a promising solution to leverage large-scale image data.
Several strands of methods have been explored for vision Transformers, such as contrastive learning~\citep{mocov3,swin:ssl}, and self-distillation~\citep{dino}.

Concurrently, BERT~\citep{bert} has achieved great success in natural language processing.
Its masked language modeling task first randomly masks some proportion of tokens within a text, and then recovers the masked tokens based on the Transformer encoding results of the corrupted text.
Motivated by BERT, we turn to the denoising auto-encoding idea to pretrain vision Transformers, which has not been well studied by the vision community.
It is challenging to directly apply BERT-style pre-training for image data.
First of all, there is no pre-exist vocabulary for vision Transformer's input unit, i.e., image patches.
So we cannot simply employ a softmax classifier to predict over all possible candidates for masked patches.
In contrast, the language vocabulary, such as words and BPE~\citep{bpe}, is well-defined and eases auto-encoding prediction.
A straightforward alternative is regarding the task as a regression problem, which predicts the raw pixels of masked patches.
However, such pixel-level recovery task tends to waste modeling capability on pre-training short-range dependencies and high-frequency details~\citep{dalle}.
Our goal is to overcome the above issues for pre-training of vision Transformers.


\begin{figure}[t]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=1\textwidth]{overview.pdf}
\end{tabular}
\end{center}
\caption{Overview of \our{} pre-training.
Before pre-training, we learn an ``image tokenizer'' via autoencoding-style reconstruction, where an image is tokenized into discrete visual tokens according to the learned vocabulary.
During pre-training, each image has two views, i.e., image patches, and visual tokens.
We randomly mask some proportion of image patches (\graybox{gray} patches in the figure) and replace them with a special mask embedding \sptk{M}.
Then the patches are fed to a backbone vision Transformer.
The pre-training task aims at predicting the visual tokens of the \emph{original} image based on the encoding vectors of the \emph{corrupted} image.
}
\label{fig:overview}
\end{figure}


In this work, we introduce a self-supervised vision representation model \textbf{\our{}}, which stands for \textbf{B}idirectional \textbf{E}ncoder representation from \textbf{I}mage \textbf{T}ransformers.
Inspired by BERT, we propose a pre-training task, namely, masked image modeling (MIM).
As shown in Figure~\ref{fig:overview}, MIM uses two views for each images, i.e., image patches, and visual tokens.
We split the image into a grid of patches that are the input representation of backbone Transformer.
Moreover, we ``tokenize'' the image to discrete visual tokens, which is obtained by the latent codes of discrete VAE~\citep{dalle}.
During pre-training, we randomly mask some proportion of image patches, and feed the corrupted input to Transformer.
The model learns to recover the visual tokens of the original image, instead of the raw pixels of masked patches.

We perform self-supervised learning and then fine-tune the pretrained \our{} on two downstream tasks, i.e., image classification, and semantic segmentation.
Experimental results indicate that \our{} outperforms both from-scratch training and previous strong self-supervised models.
Moreover, \our{} is complementary to supervised pre-training. Performance of \our{} can be further improved by intermediate fine-tuning with ImageNet labels.
Ablation studies show that our proposed techniques are critical to the effectiveness of BERT-style pre-training for image data.
Apart from performance, the improvements of convergence speed and stability of fine-tuning reduce training costs on end tasks.
In addition, we demonstrate that self-supervised \our{} can learn reasonable semantic regions via pre-training, unleashing the rich supervision signals contained in images.

Our contributions are summarized as follows:
\begin{itemize}[leftmargin=1.5em]
\item We propose a masked image modeling task to pretrain vision Transformers in a self-supervised manner. We also provide a theoretical explanation from the perspective of variational autoencoder.
\item We pretrain \our{} and conduct extensive fine-tuning experiments on downstream tasks, such as image classification, and semantic segmentation.
\item We present that the self-attention mechanism of self-supervised \our{} learns to distinguish semantic regions and object boundaries, although without using any human annotation.
\end{itemize}


\section{Methods}
\label{sec:methods}


Given an input image , \our{} encodes it to contextualized vector representations.
As shown in Figure~\ref{fig:overview}, \our{} is pretrained by the masked image modeling (MIM) task in a self-supervised learning manner.
MIM aims at recovering the masked image patches based on encoding vectors.
For downstream tasks (such as image classification, and semantic segmentation), we append task layers upon pretrained \our{} and fine-tune the parameters on the specific datasets.


\subsection{Image Representations}
\label{sec:image:repr}

The images have two views of representations in our method, namely, \textit{image patch}, and \textit{visual tokens}.
The two types serve as input and output representations during pre-training, respectively.

\subsubsection{Image Patch}
\label{sec:image:patch}

The 2D image is split into a sequence of patches~\citep{vit}, so that a standard Transformer can directly accept image data.
Formally, we reshape the image  into  patches , where  is the number of channels,  is the input image resolution, and  is the resolution of each patch.
The image patches  are flattened into vectors and are linearly projected, which is similar to word embeddings in BERT~\citep{bert}.
Image patches preserve raw pixels and are used as input features in \our{}.

In our experiments, we split each  image into a  grid of image patches, where each patch is .

\subsubsection{Visual Token}
\label{sec:visual:token}

Similar to natural language, we represent the image as a sequence of discrete tokens obtained by an ``image tokenizer'', instead of raw pixels.
Specifically, we tokenize the image  into , where the vocabulary  contains discrete token indices.

Following~\citep{dalle}, we use the image tokenizer learned by discrete variational autoencoder (dVAE).
There are two modules during visual token learning, namely, \textit{tokenizer} and \textit{decoder}.
The tokenizer  maps image pixels  into discrete tokens  according to a visual codebook (i.e., vocabulary).
The decoder  learns to reconstruct the input image  based on the visual tokens .
The reconstruction objective can be written as .
Because the latent visual tokens are discrete, the model training is non-differentiable.
Gumbel-softmax relaxation~\citep{gumbel:Jang,gumbel:Maddison} is employed to train the model parameters.
Moreover, a uniform prior is put on  during dVAE training.
Refer to \citep{dalle} for more training details of the image tokenizer.

We tokenize each image to a  grid of visual tokens.
Notice the number of visual tokens and the number of image patches for one image are the same.
The vocabulary size is set to .
In our work, we directly use the publicly available\footnote{\url{https://github.com/openai/DALL-E}} image tokenizer described in \citep{dalle}.
We also compare it with a re-implemented tokenizer in Appendix~\ref{app:tokenizer}.


\subsection{Backbone Network: Image Transformer}
\label{sec:backbone}

Following ViT~\citep{vit}, we use the standard Transformer~\citep{transformer} as the backbone network.
So the results can be directly compared with previous work in terms of the network architecture.

The input of Transformer is a sequence of image patches .
The patches are then linearly projected to obtain patch embeddings , where .
Moreover, we prepend a special token \sptk{S} to the input sequence.
We also add standard learnable 1D position embeddings  to patch embeddings.
The input vectors  is fed into Transformer.
The encoder contains  layers of Transformer blocks , where .
The output vectors of the last layer  are used as the encoded representations for the image patches, where  is the vector of the -th image patch.

\subsection{Pre-Training \our{}: Masked Image Modeling}

We propose a \textit{masked image modeling} (MIM) task.
We randomly mask some percentage of image patches, and then predict the visual tokens that are corresponding to the masked patches.

Figure~\ref{fig:overview} shows the overview of our method.
As presented in Section~\ref{sec:image:repr}, given an input image , we split it into  image patches (), and tokenize it to  visual tokens ().
We randomly mask approximately  image patches, where the masked positions are denoted as .
Next we replace the masked patches with a learnable embedding .
The corrupted image patches  are then fed into the -layer Transformer as described in Section~\ref{sec:backbone}.
The final hidden vectors  are regarded as encoded representations of the input patches.
For each masked position , we use a softmax classifier to predict the corresponding visual tokens , where  is the corrupted image,  , and .
The pre-training objective is to maximize the log-likelihood of the correct visual tokens  given the corrupted image:

where  is the training corpus,  represents randomly masked positions, and  is the corrupted image that is masked according to .

\begin{wrapfigure}{R}{0.56\textwidth}
\vspace{-24pt}
\begin{minipage}{0.56\textwidth}
\begin{algorithm}[H]
\small
\caption{Blockwise Masking}
\label{alg:mask}
\begin{algorithmic}
\STATE {\bfseries Input:}  image patches \\
\STATE {\bfseries Output:} Masked positions 
\STATE {}
\REPEAT{}
\STATE{ \textsf{Rand(}\textsf{)}}
\hfill\AlgCommentInLine{Block size}
\STATE{ \textsf{Rand(, \textsf{)}}} \hfill\AlgCommentInLine{Aspect ratio of block}
\STATE{}
\STATE{ \textsf{Rand(, \textsf{)}}  \textsf{Rand(, \textsf{)}}}
\STATE{}
\UNTIL{}\hfill\AlgCommentInLine{Masking ratio is }
\STATE{\textbf{return} }
\end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-14pt}
\end{wrapfigure}


Rather than randomly choosing patches for the masked positions , we employ blockwise masking in our work.
As summarized in Algorithm~\ref{alg:mask}, a block of image patches is masked each time.
For each block, we set the minimum number of patches to .
Then we randomly choose an aspect ratio for the masking block.
We repeat the above two steps until obtaining enough masked patches, i.e., , where  is the total number of image patches, and  is masking ratio.

The MIM task is greatly inspired by masked language modeling~\citep{bert}, which is one of the most successful pre-training objective in natural language processing.
Moreover, blockwise (or n-gram) masking is also widely applied in BERT-like models~\citep{spanbert,unilm2,t5}.
However, directly using pixel-level auto-encoding (i.e., recovering the pixels of masked patches) for vision pre-training pushes the model to focus on short-range dependencies and high-frequency details~\citep{dalle}.
\our{} overcomes the above issue by predicting discrete visual tokens, which summarizes the details to high-level abstractions.
Ablation studies in Section~\ref{sec:ablation} show that our proposed method significantly outperforms pixel-level auto-encoding.


\subsection{From the Perspective of Variational Autoencoder}
\label{sec:vae}

The \our{} pre-training can be viewed as variational autoencoder~\citep{vae} training.
Let  denote the original image,  the masked image, and  the visual tokens.
Considering the evidence lower bound (ELBO) of the log-likelihood , i.e., recovering the original image from its corrupted version:

where (1)  denotes the image tokenizer that obtains visual tokens; (2)  decodes the original image given input visual tokens; (3)  recovers the visual tokens based on the masked image, which is our MIM pre-training task.

We learn the model following a two-stage procedure similar to~\citep{vqvae,vqvae2}.
In the first stage, we obtain the image tokenizer as a discrete variational autoencoder~\citep{dalle}.
Specifically, the first stage minimizes the reconstruction loss  with an uniform prior as described in Equation~(\ref{eq:elbo}).
In the second stage, we learn the prior  while keeping  and  fixed.
We simplify  to a one-point distribution with the most likely visual tokens .
Then Equation~(\ref{eq:elbo}) can be rewritten as:

where the second term is our \our{} pre-training objective.


\subsection{Pre-Training Setup}
\label{sec:pt:setup}

The network architecture of \our{} follows that of ViT-Base~\citep{vit} for a fair comparison.
We use a -layer Transformer with  hidden size, and  attention heads. The intermediate size of feed-forward networks is .
We employ the default  input patch size.
We directly borrow the image tokenizer trained by~\citep{dalle}. The vocabulary size of visual tokens is .


We pretrain \our{} on the training set of ImageNet-1K~\citep{imagenet}, which contains about M images.
Our augmentation policy includes random resized cropping, horizontal flipping, color jittering~\citep{wu2018unsupervised}.
Notice that we do not use the labels for self-supervised learning.
We use the  resolution in our experiments.
So the input is split to  image patches, and the same amount of visual tokens.
We randomly mask at most  patches (i.e., roughly  of total image patches).

The pre-training runs for about k steps (i.e.,  epochs) with k batch size.
Adam~\citep{adamw} with  is employed for optimization. The learning rate is set to 1.5e-3, with a warmup of  epochs, and cosine learning rate decay.
The weight decay is .
We employ stochastic depth~\citep{stochastic:depth} with a  rate, and disable dropout.
The k training steps take about five days using  Nvidia Telsa V100 32GB GPU cards.

We find that proper initialization is important to stabilize Transformer, especially for large-scale pre-training.
We first randomly initialize all the parameters within a small range, such as .
Then, for the -th Transformer layer, we rescale the output matrices (i.e., the last linear projection within each sub-layer) of the self-attention module and the feed-forward network by .


\subsection{Fine-Tuning \our{} on Downstream Vision Tasks}
\label{sec:ft}

After pre-training \our{}, we append a task layer upon the Transformer, and fine-tune the parameters on downstream tasks, like BERT.
We take image classification and semantic segmentation as examples in our work.
It is straightforward to leverage the pre-training-then-fine-tuning paradigm on other vision tasks with \our{}.

\paragraph{Image classification.}
For image classification tasks, we directly employ a simple linear classifier as the task layer.
Specifically, we use average pooling to aggregate the representations, and feed the global to a softmax classifier.
The category probabilities are computed as , where  is the final encoding vector of the -th image patch,  is a parameter matrix, and  is the number of labels.
We maximize the likelihood of labeled data by updating the parameters of \our{} and the softmax classifier.

\paragraph{Semantic segmentation.}
For semantic segmentation, we follow the task layer used in SETR-PUP~\citep{setr}.
To be specific, we use pretrained \our{} as a backbone encoder, and incorporate several deconvolution layers as decoder to produce segmentation.
The model is also end-to-end fine-tuned similar to image classification.

\paragraph{Intermediate fine-tuning.}
After self-supervised pre-training, we can further train \our{} on a data-rich intermediate dataset (i.e., ImageNet-1K in our work), and then finetune the model on the target downstream tasks.
Such intermediate fine-tuning is the common practice of BERT fine-tuning in NLP~\citep{intermediate:ft}. We directly follow the method for \our{}.


\section{Experiments}
\label{sec:exp}

We conduct full fine-tuning experiments on image classification and semantic segmentation.
Moreover, we present various ablation studies for pre-training and analyze the representations learned by \our{}. 
We also report linear probes on ImageNet in Appendix~\ref{app:linear_prob}.

\subsection{Image Classification}
\label{sec:image:cls}

The image classification task classifies input images to various categories.
We evaluate \our{} on the ILSVRC-2012 ImageNet dataset~\citep{imagenet} with 1k classes and M images.
We directly follow the most of hyperparameters of DeiT~\citep{deit} in our fine-tuning experiments for a fair comparison.
We reduce fine-tuning epochs compared with training from scratch, as \our{} has been pre-trained.
Accordingly, we use a larger learning rate with layer-wise decay.
The detailed hyperparameters are summarized in Appendix~\ref{app:finetune:cls}.

Table~\ref{tbl:results:cls:imagenet} reports top-1 accuracy on image classification.
We compare \our{} with vision Transformers trained by random initialization, supervised pre-training, and previous self-supervised learning methods.
All the compared models are base-size, except iGPT has 1.36B parameters.
Pre-training is conducted on ImageNet for the comparison purpose, except ViT-JFT300M is pretrained on Google's in-house M images.

Compared with the models trained by random initialization, we find that pre-trained \our{} significantly improves performance on both datasets.
\our{} improves the performance on ImageNet, which shows the effectiveness under the rich-resource setting.

Moreover, we compare \our{} with previous state-of-the-art self-supervised methods for Transformer, such as DINO~\citep{dino}, and MoCo v3~\citep{mocov3}.
Our proposed method outperforms previous models on ImageNet fine-tuning.
Among them, iGPT-1.36B~\citep{igpt} uses much more parameters (i.e., B vs M), and ViT-JFT300M~\citep{vit} is pretrained on larger corpus (i.e., M vs M), while others pretrain ViT-Base on ImageNet-1K.
iGPT-1.36B and ViT-JFT300M are the most comparable methods, which also follows auto-encoding pre-training for vision Transformer.
Specifically, iGPT uses clustered image tokens as both input and output for image GPT or image BERT.
In contrast, we use image patches as input to preserve raw pixels, and employ discrete visual tokens as a prediction bottleneck.
ViT-JFT300 predicts the mean, 3-bit color of each masked patch, rather than visual tokens learned by discrete VAE.
We also pretrain the self-supervised tasks of \our{} and DINO in a multi-task learning manner, which is presented in Appendix~\ref{app:multitask:dino}.

In addition, we evaluate our proposed method with intermediate fine-tuning.
In other words, we first pretrain \our{} in a self-supervised manner, and then fine-tune the pretrained model on ImageNet with labeled data.
The results show that \our{} is complementary to supervised pre-training, achieving additional gain after intermediate fine-tuning on ImageNet.


\begin{table}[t]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\bf Models & \bf Model Size & \bf Resolution & \bf ImageNet \\
\midrule
\multicolumn{3}{l}{\textit{Training from scratch (i.e., random initialization)}} \\
ViT-B~\citep{vit} & 86M &  & 77.9 \\
ViT-L~\citep{vit} & 307M &  & 76.5 \\
DeiT-B~\citep{deit}           & 86M &  &  81.8 \\
DeiT-B~\citep{deit}           & 86M &  &  83.1 \\
\midrule
\multicolumn{3}{l}{\textit{Supervised Pre-Training on ImageNet-22K (using labeled data)}} \\
ViT-B~\citep{vit} & 86M &   &  84.0 \\
ViT-L~\citep{vit}  & 307M &  & 85.2 \\
\midrule
\multicolumn{3}{l}{\textit{Self-Supervised Pre-Training on ImageNet-1K (without labeled data)}} \\
iGPT-1.36B~\citep{igpt} & 1.36B &  & 66.5 \\
ViT-B-JFT300M~\citep{vit} & 86M &  & 79.9 \\
MoCo v3-B~\citep{mocov3}       & 86M    &  &  83.2 \\
MoCo v3-L~\citep{mocov3}                              & 307M  &  & 84.1 \\
DINO-B~\citep{dino}       & 86M    &  &  82.8 \\
\our{}-B (ours)                                    & 86M &  & 83.2 \\
\our{}-B (ours)                                  & 86M  &  & 84.6 \\
\our{}-L (ours)                              & 307M  &  & 85.2 \\
\our{}-L (ours)                         & 307M  &  & \bf 86.3 \\
\bottomrule
\end{tabular}
\caption{Top-1 accuracy on ImageNet-1K.
We evaluate base- (``-B'') and large-size (``-L'') models at resolutions  and .
: iGPT-1.36B contains  billion parameters, while others are base-size models.
: ViT-B-JFT300M is pretrained with the ``masked patch prediction'' task on Google's in-house M images, while others use ImageNet.}
\label{tbl:results:cls:imagenet}
\end{table}


\paragraph{Fine-tuning to  resolution.}
After fine-tuning with resolution , we additionally fine-tune the model on  images by  more epochs.
We follow the standard higher-resolution setting of DeiT~\citep{deit}, except using fewer epochs.
Notice that we keep patch size the same for both  and  images. So the input sequence length of Transformers becomes longer for higher resolutions.
Table~\ref{tbl:results:cls:imagenet} shows that higher resolution improves the \our{} results by  points on ImageNet.
More importantly, \our{} pretrained on ImageNet-1K even outperforms supervised pre-training ViT that uses ImageNet-22K, when they use the same input resolution.


\paragraph{Scaling up to larger size.}
We further scale up \our{} to the large size (same as ViT-L).
As shown in Table~\ref{tbl:results:cls:imagenet}, ViT-L is worse than ViT on ImageNet, when training from scratch. The results verifies the data-hungry issue of vision Transformers.
Supervised pre-training on ImageNet-22K partially relieves the issue, where ViT-L finally outperforms ViT by .
In comparison, \our{}-L is better than \our{} by , and \our{}-L outperforms \our{} by .
In other words, the benefits of scaling up \our{} from base to large are greater than supervised pre-training with ImageNet-22K.
More importantly, comparing between \our{} with ViT that conducts supervised pre-training on ImageNet-22K, the improvements of \our{} become greater along with scaling the size from base (i.e., ) to large (i.e., ).
The results suggest that \our{} tends to help more for extremely larger models (such as 1B, or 10B), especially when labeled data are insufficient\footnote{\citep{scaling:vit} report that supervised pre-training of a 1.8B-size vision Transformer requires billions of labeled images.} to conduct supervised pre-training\footnote{Appendix~\ref{app:cmp:supervised:pt} shows that \our{} fine-tuned on ImageNet-22K (14M) can match the performance of supervised pre-training on Google's in-house JFT-3B~\citep{scaling:vit}, while using 214x less labels. We also demonstrate that large-size \our{} fine-tuned on 70M labeled images can achieve  top-1 accuracy on ImageNet and  mIoU on ADE20K, creating new state-of-the-art results for large-size vision Transformers.} for such large models.



\begin{table*}[t]
\centering
\small
\begin{minipage}{2.4in}
\centering
\begin{tabular}{c}
\includegraphics[width=0.8\textwidth]{convergence_curve.pdf}
\end{tabular}
\caption{Convergence curves of training DeiT from scratch and fine-tuning \our{} on ImageNet-1K.}
\label{fig:convergence}
\end{minipage}
\hfill
\begin{minipage}{2.8in}
\centering
\begin{tabular}{@{}lc@{}}
\toprule
\bf Models & \bf ADE20K \\
\midrule
Supervised Pre-Training on ImageNet & 45.3 \\
\midrule
DINO~\citep{dino} & 44.1 \\
\our{} (ours) & 45.6 \\
\our{} + Intermediate Fine-Tuning (ours) & \bf 47.7 \\
\bottomrule
\end{tabular}
\caption{Results of semantic segmentation on ADE20K.
We use SETR-PUP~\citep{setr} as the task layer and report results of single-scale inference.
}
\label{tbl:results:seg}
\end{minipage}
\end{table*}




\paragraph{Convergence curves.}
Figure~\ref{fig:convergence} compares the convergence curves of the training-from-scratch and pre-training-then-fine-tuning paradigms.
We find that fine-tuning \our{} not only achieves better performance, but also converging much faster than training DeiT from scratch.
Moreover, fine-tuning \our{} can reach reasonable numbers within very few epochs.


\subsection{Semantic Segmentation}
\label{sec:results:seg}

Semantic segmentation aims to predict a corresponding class for each pixel of the input image.
We evaluate \our{} on the ADE20K benchmark~\citep{ade20k} with K images and  semantic categories.
We report the metric of mean Intersection of Union (mIoU) averaged over all semantic categories.
As presented in Section~\ref{sec:ft}, we directly follow the task layer and the most of hyperparameters described in SETR-PUP~\citep{setr}.
On ADE20K, we use Adam~\citep{adamw} as the optimizer. The learning rate is set to 1e-3 with layer-wise decay similar to image classification.
We conduct fine-tuning for 160K steps. The batch size is .
The detailed hyperparameters are described in Appendix~\ref{app:finetune:seg}.

As shown in Table~\ref{tbl:results:seg}, we compare \our{} with supervised pre-training that relies on labeled data of ImageNet.
We find that our proposed method achieves better performance than supervised pre-training, although \our{} does not require manual annotations for pre-training.
Moreover, we employ intermediate fine-tuning for \our{} on ImageNet, i.e., we first fine-tune pretrained \our{} on ImageNet, and then fine-tune the model on ADE20K.
The results indicate that intermediate fine-tuning further improves \our{} on semantic segmentation.


\subsection{Ablation Studies}
\label{sec:ablation}

We conduct ablation studies to analyze the contributions of each component in \our{}.
The models are evaluated on image classification (i.e., ImageNet) and semantic segmentation (i.e., ADE20K).
We set the default pre-training steps to  epochs for the ablation studies, which is \% of the total steps used in the previous experiments.


\begin{table}[t]
\centering
\begin{tabular}{l c c}
\toprule
\bf Models & \bf ImageNet & \bf ADE20K \\
\midrule
\our{} (300 Epochs)                               & 82.86 & 44.65 \\
\midrule
~~ Blockwise masking                           & 82.77 & 42.93 \\
~~ Visual tokens (i.e., recover masked pixels) & 81.04 & 41.38 \\
~~ Visual tokens  Blockwise masking         & 80.50 & 37.09 \\
~~ Recover 100 visual tokens               & 82.59 & 40.93 \\
~~ Masking  Recover 100 visual tokens   & 81.67 & 36.73 \\
\midrule
Pretrain longer ( epochs)                    & 83.19 & 45.58 \\
\bottomrule
\end{tabular}
\caption{Ablation studies for \our{} pre-training on image classification and semantic segmentation.}
\label{tbl:ablation}
\end{table}


Table~\ref{tbl:ablation} reports the results of various model variants.
First, we ablate blockwise masking by randomly sample masked positions.
We find that blockwise masking is beneficial on both tasks, especially on semantic segmentation.
Second, we ablate the usage of visual tokens by predicting the raw pixels of masked patches, i.e., the pre-training task becomes a pixel regression problem to recover masked patches.
Our proposed masked image modeling task significantly outperforms naive pixel-level auto-encoding.
Compared with the results in Table~\ref{tbl:results:cls:imagenet}, the ablation result is worse than training vision Transformer from scratch on two tasks.
The results indicate that the prediction of visual tokens is the key ingredient of \our{}.
Third, we ablate the usage of visual tokens and blockwise masking together.
We find that blockwise masking is even more helpful for pixel-level auto-encoding, which relieves the suffering of short-distance dependency.
Forth, recovering all the visual tokens harms performance on downstream tasks.
Fifth, we compare \our{} with different training steps. Pre-training the model longer can further improve performance on downstream tasks.


\subsection{Analysis of Self-Attention Map}
\label{sec:analysis}

We show that the self-attention mechanism in \our{} can separate objects, even though our pre-training does not rely on any manual annotation at all. Similar properties are also observed by \citep{dino}.
The probing images are taken from the MS COCO~\citep{mscoco} corpus to avoid appearing in the pre-training data.


\begin{figure}[t]
\centering
\setlength{\tabcolsep}{3pt}
\includegraphics[width=1.0\linewidth]{sa_map_ref.pdf}
\caption{
Self-attention map for different reference points.
The self-attention mechanism in \our{} is able to separate objects, although self-supervised pre-training does not use manual annotations.
}
\label{fig:attmap:ref}
\end{figure}


As shown in Figure~\ref{fig:attmap:ref}, we plot the self-attention map for different reference points within an image.
The visualizations are produced by attention scores computed via query-key product in the last layer.
For each reference point, we use the corresponding patch as query, and show which patch it attends to.
After pre-training, \our{} learns to distinguish semantic regions using self-attention heads, without any task-specific supervision.
The property partially indicates the reason why \our{} is able to help downstream tasks.
Such knowledge acquired by \our{} potentially improves the generalization ability of fine-tuned models, especially on small-scale datasets.


\section{Related Work}
\label{sec:related:work}

\paragraph{Self-supervised visual representation learning.}
Various methods have been introduced over the years to pretrain vision models in a self-supervised manner.
Pioneering works design clever pretext tasks, such as predicting the patch orderings~\citep{jigsaw}, colorization~\citep{colorization}, and predicting rotation angles~\citep{rotation}.
In addition, \citep{selfie} propose to mask some patches within an image, and classify whether the masked patches are real or fake for each masked position. The method is similar to the masked version of Jigsaw pre-training~\citep{jigsaw}.
The recent strand of research follows contrastive paradigm~\citep{wu2018unsupervised,cpc,dim,amdim,moco,simclr,mocov2}. The models typically regard various data augmentations as different views of an image, and then make the representations of positive pairs similar while pushing negative pairs away.
In order to obtain enough informative negative samples in contrastive learning, the methods usually rely on large memory banks~\citep{wu2018unsupervised,moco} or large batch size~\citep{simclr}.
BYOL~\citep{byol} and SimSiam~\citep{simsiam} further eliminate the requirement of negative samples, using various techniques to avoid representation collapse.
Another strand of methods use clustering to organize image examples~\citep{deepcluster,sela,swav,li2021prototypical}.


\paragraph{Self-supervised vision Transformers.}
Pre-training vision Transformers has received significant attention recently due to the data-hungry issue.
iGPT~\citep{igpt} first creates a 9-bit color palette by k-means clustering RGB pixels, and then uses the clustered tokens to represent images. Next iGPT uses the tasks of BERT and GPT to pretrain Transformers.
In comparison, our proposed method uses image patches as input without losing pixel-level information. Moreover, our visual tokens are obtained by discrete VAE instead of clustering.
ViT~\citep{vit} conducts a preliminary exploration with the masked patch prediction task, which predicts the 3-bit mean color of the masked patches.
\citep{vit} also report that pixel-level auto-encoding performs worse, although it is the most straightforward translation of BERT from NLP to CV.
Rather than using heuristically designed pre-training tasks, our proposed model leverages visual tokens learned by discrete VAE, which not only achieves better performance but also is better theoretically motivated.
Apart from masked auto-encoding, other mainstream research works use contrastive learning~\citep{mocov3,swin:ssl}, and self-distillation~\citep{dino}.
In comparison, \our{} can achieve several times of improvement in terms of pre-training throughput (Appendix~\ref{app:multitask:dino}), and memory consumption.
The advantages make \our{} appealing to scale up vision Transformers.





\section{Conclusion}

We introduce a self-supervised pre-training framework for vision Transformers, achieving strong fine-tuning results on downstream tasks, such as image classification, and semantic segmentation.
We show that the proposed method is critical to make BERT-like pre-training (i.e., auto-encoding with masked input) work well for image Transformers.
We also present the intriguing property of automatically acquired knowledge about semantic regions, without using any human-annotated data.
In the future, we would like to scale up \our{} pre-training in terms of data size and model size.
Moreover, we will conduct multimodal pre-training in a more unified way, using the similar objectives and the shared architecture for texts and images.


\paragraph{Acknowledgement}
We would like to acknowledge Yue Cao, Han Hu, Hang Hua, Jingdong Wang, Zheng Zhang for the helpful discussions, and Yaru Hao for some analysis experiments using~\citep{hao2020self}.


\bibliography{beit}
\bibliographystyle{alpha}


\newpage
\appendix


\section{Architecture Variants of Vision Transformer}
\label{app:arch:variants}

We use the standard vision Transformer (ViT) in the experiments for fair comparisons.
In addition, we find that LayerScale~\citep{cait} and relative position bias~\citep{unilm2,t5} improve ViTs on downstream tasks.
We employ the same setting as in Section~\ref{sec:ablation} for ablation studies, which pretrains base-size models for  epochs on ImageNet-1K.

As shown in Table~\ref{tbl:arch_ablation}, both LayerScale and relative position bias improve performance on ImageNet classification and ADE20K semantic segmentation.
We denote the improved architecture as \our{} and use it for the experiments in Appendix~\ref{app:cmp:supervised:pt}.
We empirically notice that vanilla Transformer is the most stable when scaling up the model to billions of parameters, so we do not use LayerScale for extra-large models.


\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Architecture} & \bf ImageNet & \bf ADE20K \\ \midrule
ViT (used in this paper) & 82.86 & 44.86 \\ 
ViTLayerScale & 83.00 & 45.43 \\
ViTLayerScaleRelative Position Bias & 83.22 & 45.70 \\
\bottomrule
\end{tabular}
\caption{
Ablation studies of architecture variants on image classification and semantic segmentation.
For ADE20K, we use UperNet~\citep{upernet} as the task layer, and report mIoU scores of single-scale inference.
}
\label{tbl:arch_ablation}
\end{table}


\section{Comparison with Large-Scale Supervised Pre-Training}
\label{app:cmp:supervised:pt}

We compare with state-of-the-art supervised pre-training at scale.
In addition to using ImageNet-1K for fair comparisons with previous work, we pretrain \our{} on ImageNet-22K to boost performance.
We employ the architecture improvements (i.e., LayerScale, and relative position bias) as described in Appendix~\ref{app:arch:variants}, which is denoted as \our{} in Table~\ref{tbl:supervised:pt:cls} and Table~\ref{tbl:supervised:pt:ade20k}.
We follow the same pre-training setup as in Section~\ref{sec:pt:setup}, except we pretrain  epochs on ImageNet-22K.
After self-supervised pre-training, we conduct intermediate fine-tuning on ImageNet-22K for  epochs.
Moreover, we use an in-house dataset that has about 70M labeled images as a drop-in replacement of ImageNet-22K.


\begin{table}[H]
\centering
\begin{tabular}{@{}lcccc}
\toprule
\multirow{2}{*}{\bf Models} & \multirowcell{2}{\bf Model \\ \bf Size} & \multirowcell{2}{\bf Labeled \\ \bf Data Size} &  \multicolumn{2}{c}{\bf ImageNet} \\
 & & &  &  \\ 
\midrule
\multicolumn{5}{l}{\textit{Supervised Pre-Training on ImageNet-22K (using labeled data)}} \\
ViT-B~\citep{vit} & 86M & 14M & 84.0 & - \\
ViT-L~\citep{vit} & 307M & 14M & 85.2 & 85.30 \\
ViT-H~\citep{vit} & 632M & 14M & 85.1 & - \\
\midrule
\multicolumn{5}{l}{\textit{Supervised Pre-Training on Google JFT-300M (using labeled data)}} \\
ViT-B~\citep{vit} & 86M & 300M & 84.2 & - \\
ViT-L~\citep{vit} & 307M & 300M & 87.1 & 87.76 \\
ViT-H~\citep{vit} & 632M & 300M & 88.0 & 88.55 \\
\midrule
\multicolumn{5}{l}{\textit{Supervised Pre-Training on Google JFT-3B (using labeled data)}} \\
ViT-B~\citep{scaling:vit} & 86M & 3000M & 86.6 & - \\
ViT-L~\citep{scaling:vit} & 307M & 3000M & 88.5 & - \\
\midrule
\multicolumn{5}{l}{\textit{Self-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-22K}} \\
\our{}-B (ours) & 86M & 14M & 86.8 & - \\
\our{}-L (ours) & 307M & 14M & 88.4 & 88.6 \\
\midrule
\multicolumn{5}{l}{\textit{Self-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M}} \\
\our{}-L (ours) & 307M & 70M & \bf 89.3 & \bf 89.5 \\
\bottomrule
\end{tabular}
\caption{Top-1 accuracy on ImageNet-1K fine-tuning.
We evaluate models at resolutions  and .
}
\label{tbl:supervised:pt:cls}
\end{table}


Table~\ref{tbl:supervised:pt:cls} compares \our{} with previous state-of-the-art supervised pre-training~\citep{vit,scaling:vit} on ImageNet fine-tuning.
Rather than heavily relying on extremely large-size labeled data (such as Google's in-house JFT-300M and JFT-3B), we demonstrate that \our{} pre-training can catch up with only ImageNet-22k (14M).
Specifically, \our{}-L fine-tuned on ImageNet-22K achieves comparable performance with ViT-L trained on Google JFT-3B.
Moreover, \our{}-L obtains  top-1 accuracy on ImageNet after intermediate fine-tuning on an in-house 70M dataset.
The results indicate that \our{} pre-training greatly reduces the required labeling efforts and advances the new state of the art for large-size vision Transformers.

As shown in Table~\ref{tbl:supervised:pt:ade20k}, we report the fine-tuning results on the ADE20K semantic segmentation benchmark.
Following Swin~\citep{swin}, we use the same task layer (i.e., UperNet) and evaluate the models at the resolution .
The \our{}-L model obtains state-of-the-art performance on ADE20K.


\begin{table}[H]
\centering
\begin{tabular}{@{}lcc}
\toprule
\bf Models & \bf mIoU~()  & \bf Multi-Scale mIoU~() \\
\midrule
\multicolumn{3}{l}{\textit{Supervised Pre-Training on ImageNet-22K (using labeled data)}} \\
Swin-B~\citep{swin} & 50.0 & 51.7 \\
Swin-L~\citep{swin} & 52.1 & 53.5 \\
\midrule
\multicolumn{3}{l}{\textit{Self-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-22K}} \\
\our{}-B (ours)                         & 53.6 & 54.2 \\
\our{}-L (ours)                         & 56.7 & 57.0 \\
\midrule
\multicolumn{3}{l}{\textit{Self-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M}} \\
\our{}-L (ours)                         & \bf 57.9 & \bf 58.4 \\
\bottomrule
\end{tabular}
\caption{
Performance comparison on the ADE20K semantic segmentation.
We follow Swin-L~\citep{swin} to use UperNet~\citep{upernet} as the task layer and evaluate at resolution .
}
\label{tbl:supervised:pt:ade20k}
\end{table}


\section{Ablation Studies of Image Tokenizer}
\label{app:tokenizer}

For comparison, we re-train the image tokenizer on ImageNet-1K.
The reimplementation is based on \url{https://github.com/lucidrains/DALLE-pytorch}.
We use the same codebook size 8K as in DALL-E~\citep{dalle}.
Then we plug the tokenizer into our pre-training process.
We follow the same experimental setup of ablation studies as in Section~\ref{sec:ablation}.
Table~\ref{tbl:image_tokenizer_ablation} shows that our reimplemented tokenizer obtains comparable reconstruction loss and ImageNet fine-tuning performance compared with the off-the-shelf DALL-E tokenizer.


\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\bf Image Tokenizer & \bf Reconstruction Error & \bf ImageNet\\
\midrule
DALL-E Tokenizer~\citep{dalle} & \bf 0.0856 & \bf 82.86 \\
Our reimplementation & 0.0880 & 82.70 \\
\bottomrule
\end{tabular}
\caption{
Top-1 accuracy on ImageNet-1K using different image tokenizers during pre-training. For image reconstruction, we report mean absolute error of normalized RGB values. The reimplemented image tokenizer is trained on ImageNet-1K without labels.
}
\label{tbl:image_tokenizer_ablation}
\end{table}


\section{Linear Probes on ImageNet}
\label{app:linear_prob}

We evaluate linear probes on ImageNet for various pretrained vision Transformers.
We compare \our{} with two main strands of work, namely \textit{discriminative} and \textit{generative} self-supervised learning.
The first one applies discriminative learning for pre-training, such as contrastive learning~\citep{mocov3}, and self distillation~\citep{dino}.
The above methods typically learn to aggregate the image-level features into a global vector, which is relatively suitable for linear probing.
In contrast, the second strand of methods, such as iGPT~\citep{igpt} and ours, usually do not pretrain such global feature aggregation, which tends to make linear probes difficult.

Following iGPT~\citep{igpt}, we use average pooling to aggregate the hidden states of each image patches, and add the probing layer at the middle layer of Transformer instead of always at the final layer.
Similarly, we find that the best layer lies in -th layer for \our{}-B, and -th layer for \our{}-L.
To be specific, we use AdamW~\citep{adamw} to update the linear probe layer for  epochs. The learning rate is 4e-3 with cosine decay. The batch size is 1024. The weight decay is set to 1e-4.
We follow data augmentation used in DINO~\citep{dino}, which uses random resize crops and horizontal flips augmentation during training and evaluates on central crops.


\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\bf Models & \bf Model Size & \bf Accuracy \\ \midrule
\multicolumn{3}{l}{~~\textit{Discriminative self-supervised learning}} \\
DINO-B~\citep{dino} & 86M & 78.2 \\
MoCo v3-B~\citep{mocov3} & 86M & 76.7 \\
MoCo v3-L~\citep{mocov3} & 307M & 77.6 \\
\midrule
\multicolumn{3}{l}{~~\textit{Generative self-supervised learning}} \\
iGPT-L~\citep{igpt} & 1362M & 65.2 \\
iGPT-XL~\citep{igpt} & 6801M & 68.7 \\
iGPT-XL~\citep{igpt} & 6801M & 72.0 \\
\our{}-B (ours) & 86M & 56.7 \\
\our{}-L (ours) & 307M & 73.5 \\
\bottomrule
\end{tabular}
\caption{
Linear probing accuracy on ImageNet. ``'' denotes that iGPT-XL uses concatenation of five layers for linear probing, while others use the features of single layer.
}
\label{tbl:linear_probe}
\end{table}


As shown in Table~\ref{tbl:linear_probe}, we evaluate linear probes on ImageNet-1K for self-supervised learning.
Overall, discriminative methods perform better than generative pre-training on linear probing.
Linear probes keep the Transformer parameters fixed and only update the linear layer. So the pre-training of global aggregation of image-level features is beneficial to linear probing in DINO and MoCo v3, although full fine-tuning eliminates the gap.
Moreover, the results indicate that increasing the model size from base (86M) to large (304M) significantly improves accuracy for our proposed method.
In contrast, the gap between base- and large-size MoCo v3 is smaller.
We also find that \our{} outperforms iGPT by a large margin even using much fewer parameters.


\section{Multi-Task Pre-Training with DINO}
\label{app:multitask:dino}

We train the pre-training tasks of \our{} and DINO~\citep{dino} together in a multi-task manner.
As shown in Table~\ref{tbl:multitask:dino}, augmenting masked image modeling with DINO improves semantic segmentation on ADE20K, and obtains comparable results on ImageNet classification.
Moreover, \our{} is more efficient in terms of pre-training speed, as DINO has two copies of Transformer parameters for self-distillation and multi-crop augmentation~\citep{swav}.
For the throughput comparisons between \our{} and \our{}+DINO, we set batch size to the same.
Because \our{} is also more memory-efficient, we can use larger batch size to fully utilize GPU cards, which obtains greater speedup in practice than the reported numbers.


\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\bf 
Models & \bf ImageNet & \bf ADE20K & \bf Pre-Training Throughput \\ \midrule
DINO (400 Epochs)           & 82.8 & 44.08 & - \\
\our{} (300 Epochs)             & \textbf{82.9}    &  44.65 & \textbf{4.2x} \\
\our{} + DINO (300 Epochs) & \textbf{82.9} & \textbf{46.85} & 1.0x \\
\bottomrule
\end{tabular}
\caption{We train the pre-training tasks of \our{} and DINO~\citep{dino} in the way of multi-task learning. We report the performance by fine-tuning on ImageNet-1K image classification and ADE20K semantic segmentation. For ADE20K, we use SETR-PUP~\citep{setr} as the task layer and report the mIoU score of single-scale inference.
The pre-training throughput measures the speed, where larger numbers indicate faster pre-training.}
\label{tbl:multitask:dino}
\end{table}


\section{Image Classification on CIFAR-100}
\label{app:cifar100}

In addition to ImageNet classification, we conduct fine-tuning experiments on the CIFAR-100~\citep{cifar100} benchmark with 100 classes and 60k images. The experimental setup is the same as in Section~\ref{sec:image:cls}.

Table~\ref{tbl:results:cifar} reports the top-1 accuracy on CIFAR-100.
Notably, on the smaller CIFAR-100 dataset, ViT trained from scratch only reaches  accuracy~\citep{mocov3}. In comparison, \our{} achieves  with the help of pre-training.
The results indicate that \our{} can greatly reduce the requirement of annotation efforts.
\our{} also outperforms MoCo v3.
Moreover, intermediate fine-tuning on ImageNet-1K further improves the results on CIFAR-100.

\begin{table}[h]
\centering
\begin{tabular}{@{}lc@{}}
\toprule
\bf Models & \bf CIFAR-100 \\
\midrule
\multicolumn{2}{l}{\textit{Training from scratch (i.e., random initialization)}} \\
ViT~\citep{vit} & 48.5* \\
\midrule
\multicolumn{2}{l}{\textit{Supervised Pre-Training on ImageNet-1K (using labeled data)}} \\
ViT~\citep{vit} & 87.1 \\
DeiT~\citep{deit}           & 90.8 \\
\midrule
\multicolumn{2}{l}{\textit{Self-Supervised Pre-Training on ImageNet-1K (without labeled data)}} \\
DINO~\citep{dino}           & 91.7 \\
MoCo v3~\citep{mocov3}           & 87.1 \\
\our{} (ours)                                    & 90.1 \\
\midrule
\multicolumn{2}{l}{\textit{Self-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-1K}} \\
\our{} (ours)                                     &  \bf 91.8 \\
\bottomrule
\end{tabular}
\caption{Top-1 accuracy of image classification on CIFAR-100.
The models are at resolution , except ViT uses .
The results, unless otherwise indicated, are all obtained by base-size models.
*: result is taken from \citep{mocov3}.}
\label{tbl:results:cifar}
\end{table}


\section{Hyperparameters for Pre-Training}
\label{app:pretrain}

\begin{table}[H]
\centering
\small
\scalebox{0.98}{
\begin{tabular}{l|cc}
\toprule
\bf Hyperparameters & \bf Base Size & \bf Large Size \\
\midrule
Layers & 12 & 24 \\
Hidden size & 768 & 1024 \\
FFN inner hidden size & 3072 & 4096 \\
Attention heads & 12 & 16 \\
Attention head size & \multicolumn{2}{c}{64} \\
Patch size & \multicolumn{2}{c}{} \\
\midrule
Training epochs & \multicolumn{2}{c}{800} \\
Batch size & \multicolumn{2}{c}{2048} \\
Adam  & \multicolumn{2}{c}{1e-8} \\
Adam  & \multicolumn{2}{c}{(0.9, 0.999)} \\
Peak learning rate & \multicolumn{2}{c}{1.5e-3} \\
Minimal learning rate & \multicolumn{2}{c}{1e-5} \\
Learning rate schedule & \multicolumn{2}{c}{Cosine} \\
Warmup epochs & \multicolumn{2}{c}{10} \\
\midrule
Gradient clipping & 3.0 & 1.0 \\
Dropout & \multicolumn{2}{c}{\xmark} \\
Stoch. depth & \multicolumn{2}{c}{0.1} \\
Weight decay & \multicolumn{2}{c}{0.05} \\
\midrule
Data Augment & \multicolumn{2}{c}{RandomResizeAndCrop} \\
Input resolution & \multicolumn{2}{c}{} \\
Color jitter & \multicolumn{2}{c}{0.4} \\
\bottomrule
\end{tabular}
}

\caption{
Hyperparameters for pre-training \our{} on ImageNet-1K.
}
\label{tbl:pretrain:hyperparams}
\end{table}


\section{Hyperparameters for Image Classification Fine-Tuning}
\label{app:finetune:cls}

\begin{table}[H]
\centering
\begin{tabular}{l|ccc}
\toprule
\multirow{2}{*}{\bf Hyperparameters} & \bf CIFAR-100 & \multicolumn{2}{c}{\bf ImageNet-1K} \\
& Base Size & Base Size & Large Size \\
\toprule
Peak learning rate & 
\multicolumn{3}{c}{\{2e-3, 3e-3, 4e-3, 5e-3\}} \\
Fine-tuning epochs & 150 & 100 & 50 \\
Batch size & 512 & 1024 & 1024 \\
Warmup epochs & 20 & 20 & 5 \\
Layer-wise learning rate decay & 0.65 & 0.65 & 0.75 \\
Adam  & \multicolumn{3}{c}{1e-8}  \\
Adam  & \multicolumn{3}{c}{(0.9, 0.999)} \\
Minimal learning rate & \multicolumn{3}{c}{1e-6} \\
Learning rate schedule & \multicolumn{3}{c}{Cosine} \\
\midrule
Repeated Aug & \cmark & \cmark & \xmark \\
Weight decay & 0.3 & 0.05 & 0.05 \\
Label smoothing  & \multicolumn{3}{c}{0.1}     \\
Stoch. depth & \multicolumn{3}{c}{0.1} \\
Dropout & \multicolumn{3}{c}{\xmark} \\
Gradient clipping & \multicolumn{3}{c}{\xmark} \\
\midrule
Erasing prob.  & \xmark & 0.25 & 0.25 \\
Input resolution & \multicolumn{3}{c}{} \\
Rand Augment  & \multicolumn{3}{c}{9/0.5} \\
Mixup prob.  & \multicolumn{3}{c}{0.8}     \\
Cutmix prob.   & \multicolumn{3}{c}{1.0}    \\
\bottomrule
\end{tabular}
\caption{
Hyperparameters for fine-tuning \our{} on ImageNet-1K and CIFAR-100.
}
\label{tbl:ft:imagenet:hyperparams}
\end{table}

\section{Hyperparameters for ADE20K Semantic Segmentation Fine-Tuning}
\label{app:finetune:seg}

\begin{table}[H]
\centering
\begin{tabular}{l|c}
\toprule
\bf Hyperparameters & \bf Base Size \\
\midrule
Peak learning rate & 1e-3 \\
Fine-tuning steps & 160K \\
Batch size & 16 \\
Adam  & 1e-8  \\
Adam  & (0.9, 0.999) \\
Layer-wise learning rate decay & 0.65 \\
Minimal learning rate & 0 \\
Learning rate schedule & Linear \\
Warmup steps & 1500 \\
\midrule
Dropout & \xmark \\
Stoch. depth & 0.1 \\
Weight decay & 0.05 \\
\midrule
Input resolution &  \\
Position embedding interpolate & bilinear \\
\bottomrule
\end{tabular}
\caption{
Hyperparameters for fine-tuning \our{} on ADE20K.
}
\label{tbl:ft:ade20k:hyperparams}
\end{table}

\end{document}