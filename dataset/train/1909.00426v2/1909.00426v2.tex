\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subcaption}
\usepackage{makecell}

\renewcommand{\UrlFont}{\ttfamily\small}

\aclfinalcopy 

\setlength\titlebox{5cm}


\title{Global Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities}

\author{
    \begin{minipage}{8em}
        \begin{center}
			Ikuya Yamada\\
			{\tt \fontsize{7.5pt}{0pt}\selectfont ikuya@ousia.jp}
		\end{center}
	\end{minipage}
    \begin{minipage}{11em}
        \begin{center}
			Koki Washio\\
			{\tt \fontsize{7.5pt}{0pt}\selectfont kwashio@g.ecc.u-tokyo.ac.jp}
		\end{center}
	\end{minipage}
	\begin{minipage}{8.5em}
        \begin{center}
			Hiroyuki Shindo\\
			{\tt \fontsize{7.5pt}{0pt}\selectfont shindo@is.naist.jp}
		\end{center}
	\end{minipage}
	\begin{minipage}{8em}
        \begin{center}
			Yuji Matsumoto\\
			{\tt \fontsize{7.5pt}{0pt}\selectfont yuji.matsumoto@riken.jp}
		\end{center}
	\end{minipage}
	\\
	\\
  \fontsize{12pt}{0pt}\selectfont
  Studio Ousia, Tokyo, Japan
  \,\,\,
  The University of Tokyo, Tokyo, Japan\\
  \fontsize{12pt}{0pt}\selectfont
  Nara Institute of Science and Technology, Nara, Japan
  \,\,\,
  RIKEN AIP, Tokyo, Japan
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  We propose a new global entity disambiguation (ED) model based on contextualized embeddings of words and entities.
  Our model is based on a bidirectional transformer encoder (i.e., BERT) and produces contextualized embeddings for words and entities in the input text.
  The model is trained using a new \textit{masked entity prediction} task that aims to train the model by predicting randomly masked entities in entity-annotated texts obtained from Wikipedia.
  We further extend the model by solving ED as a sequential decision task to capture global contextual information.
  We evaluate our model using six standard ED datasets and achieve new state-of-the-art results on all but one dataset.
\end{abstract}

\section{Introduction}
Entity disambiguation (ED) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base (KB).
This task is challenging because of the ambiguity between entity names (e.g., ``World Cup'') and the entities they refer to (e.g., \texttt{FIFA World Cup} or \texttt{Rugby World Cup}).
Recent ED models typically rely on two types of contextual information: \textit{local} information based on words that co-occur with the mention, and \textit{global} information based on document-level coherence of the disambiguation decisions.
A key to improve the performance of ED is to combine both local and global information as observed in most recent ED models.

In this study, we propose a novel ED model based on contextualized embeddings of words and entities.
The proposed model is based on BERT \cite{devlin2018bert}.
Our model takes words and entities in the input document, and produces a contextualized embedding for each word and entity.
Inspired by the masked language model (MLM) adopted in BERT, we propose \textit{masked entity prediction} (MEP), a novel task that aims to train the model by predicting randomly masked entities based on words and non-masked entities.
We train the model using texts and their entity annotations retrieved from Wikipedia.

Furthermore, we introduce a simple extension to the inference step of the model to capture global contextual information.
Specifically, similar to the approach used in past work \cite{Fang:2019:JEL:3308558.3313517,yang2019learning}, we address ED as a sequential decision task that disambiguates mentions one by one, and uses words and already disambiguated entities to disambiguate new mentions.

We evaluate the proposed model using six standard ED datasets and achieve new state-of-the-art results on all but one dataset.
Furthermore, we will publicize our code and trained embeddings.

\section{Background and Related Work}
Neural network-based approaches have recently achieved strong results on ED \cite{ganea-hofmann:2017:EMNLP2017,TACL1065,le-titov-2018-improving,cao-EtAl:2018:C18-1,le-titov-2019-boosting,yang2019learning}.
These approaches are typically based on embeddings of words and entities trained using a large KB (e.g., Wikipedia).
Such embeddings enable us to design ED models that capture the contextual information required to address ED.
These embeddings are typically based on conventional word embedding models (e.g., skip-gram \cite{Mikolov2013}) that assign a fixed embedding to each word and entity \cite{Yamada2016,cao-EtAl:2017:Long1,ganea-hofmann:2017:EMNLP2017}.

\newcite{Shahbazi2019Entity-awareDisambiguation} and \newcite{Broscheit2019InvestigatingLinking} proposed ED models based on contextualized word embeddings, namely, ELMo \cite{peters-etal-2018-deep} and BERT, respectively.
These models predict the referent entity of a mention using the contextualized embeddings of the constituent or surrounding words of the mention.
However, unlike our proposed model, these models address the task based only on local contextual information.

\section{Contextualized Embeddings of Words and Entities for ED}
\label{sec:embedding-model}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{architecture-crop.pdf}
  \caption{Architecture of the proposed contextualized embeddings of words and entities.}
  \label{fig:architecture}
\end{figure}

Figure \ref{fig:architecture} illustrates the architecture of our contextualized embeddings of words and entities.
Our model adopts a multi-layer bidirectional transformer encoder \cite{NIPS2017_7181}.

Given a document, the model first constructs a sequence of tokens consisting of words in the document and entities appearing in the document.
Then, the model represents the sequence as a sequence of input embeddings, one for each token, and generates a contextualized output embedding for each token.
Both the input and output embeddings have  dimensions.
Hereafter, we denote the number of words and that of entities in the vocabulary of our model by  and , respectively.

\subsection{Input Representation}
Similar to the approach adopted in BERT \cite{devlin2018bert}, the input representation of a given token (word or entity) is constructed by summing the following three embeddings of  dimensions:
\begin{itemize}[leftmargin=10pt,topsep=5pt,itemsep=0pt]
  \item \textbf{Token embedding} is the embedding of the corresponding token.
        The matrices of the word and entity token embeddings are represented as  and , respectively.
  \item \textbf{Token type embedding} represents the type of token, namely, word type (denoted by ) or entity type (denoted by ).
  \item \textbf{Position embedding} represents the position of the token in a word sequence.
        A word and an entity appearing at the -th position in the sequence are represented as  and , respectively.
        If an entity name contains multiple words, its position embedding is computed by averaging the embeddings of the corresponding positions.
\end{itemize}

Following BERT \cite{devlin2018bert}, we insert special tokens \texttt{[CLS]} and \texttt{[SEP]} to the word sequence as the first and last words, respectively.

\subsection{Masked Entity Prediction}
\label{subsec:mep}
To train the model, we propose \textit{masked entity prediction} (MEP), a novel task based on MLM.
In particular, some percentage of the input entities are masked at random; then, the model learns to predict masked entities based on words and non-masked entities.
We represent masked entities using the special \texttt{[MASK]} entity token.

We adopt a model equivalent to the one used to predict words in MLM.
Specifically, we predict the original entity corresponding to a masked entity by applying the softmax function over all entities in our vocabulary:

where  is the output bias, and  is derived as

where  is the output embedding corresponding to the masked entity,  is the weight matrix,  is the bias,  is the gelu activation function \cite{hendrycks2016gaussian}, and  is the layer normalization function \cite{lei2016layer}.

\subsection{Training}
\label{subsec:training}
We used the same transformer architecture adopted in the BERT model \cite{devlin2018bert}.
We initialized the parameters of our model that were common with BERT (i.e., parameters in the transformer encoder and the embeddings for words) using the uncased version of the pretrained BERT model.\footnote{We initialized  using BERT's segment embedding for sentence A.}
Other parameters, namely, the parameters in the MEP and the embeddings for entities, were initialized randomly.

The model was trained via iterations over Wikipedia pages in a random order for seven epochs.
We treated the hyperlinks as entity annotations, and masked 30\% of all entities at random.
The input text was tokenized to words using the BERT's tokenizer with its vocabulary consisting of  words.
Similar to \newcite{ganea-hofmann:2017:EMNLP2017}, we built an entity vocabulary consisting of  entities, which were contained in the entity candidates in the datasets used in our experiments.
We optimized the model by maximizing the log likelihood of MEP's predictions using Adam \cite{kingma2014adam}.
Further details are provided in Appendix A.

\section{Our ED Model}
We describe our ED model in this section.

\subsection{Local ED Model}
Given an input document with  mentions and their  entity candidates, our local ED model first creates an input sequence consisting of words in the document, and  masked entity tokens corresponding to the mentions in the document.
Then, the model computes the embedding  for each mention using Eq. \eqref{eq:feature}, and predicts the entity for each mention using the softmax function over its  entity candidates:

where  and  consist of the entity token embeddings and the output bias values corresponding to the entity candidates, respectively.
Note that  and  are the subsets of  and , respectively.
This model is denoted as \textbf{local} in the remainder of the paper.

\subsection{Global ED Model}
\label{subsec:global-ed}

\SetAlFnt{\footnotesize}
\SetAlCapFnt{\footnotesize}
\SetAlCapNameFnt{\footnotesize}
\begin{algorithm}[t]
  \SetKwFor{RepTimes}{repeat}{times}{end}
  \SetAlgoLined
  \SetArgSty{}
  \SetKwInput{KwInit}{Initialize}
  \DontPrintSemicolon
  \KwIn{Words and mentions  in the input document}
  \KwInit{\;}
  \RepTimes{}{
    For all mentions, obtain entity predictions  using Eq.\eqref{eq:feature} and Eq.\eqref{eq:ed-prediction} using words and entities , ...,  as inputs\;
    Select a mention  that has the most confident prediction in all unresolved mentions\;
    
  }
  \Return{\{}
  \caption{Algorithm of our global ED model.}
  \label{alg:global-ed}
\end{algorithm}

Our global model addresses ED by resolving mentions sequentially for  steps.
The model is described in Algorithm \ref{alg:global-ed}.
First, the model initializes the entity of each mention using the \texttt{[MASK]} token.
Then, for each step, the model predicts an entity for each mention, selects a mention with the highest probability produced by the softmax function in Eq.\eqref{eq:ed-prediction} in all unresolved mentions, and resolves the selected mention by assigning the predicted entity to the mention.
This model is denoted as \textbf{confidence-order} in the remainder of the paper.
Furthermore, we test a baseline model that selects a mention by its order of appearance in the document and denote it by \textbf{natural-order}.

\section{Experiments}
\label{subsec:experimental-setup}

We test the proposed ED models using six standard ED datasets: AIDA-CoNLL\footnote{We used the \textit{test\_b} set of the CoNLL dataset.} (CoNLL) \cite{Hoffart2011}, MSNBC (MSB), AQUAINT (AQ), ACE2004 (ACE), WNED-CWEB (CW), and WNED-WIKI (WW) \cite{guo2018robust}.
We consider only the mentions that refer to valid entities in Wikipedia.
For all datasets, we use the \textit{KB+YAGO} entity candidates and their associated  \cite{ganea-hofmann:2017:EMNLP2017}, and use the top 30 candidates based on .
We split a document if it is longer than 512 words, which is the maximum word length of the BERT model.
We report the in-KB accuracy for the CoNLL dataset, and the micro F1 score (averaged per mention) for the other datasets.

Furthermore, we optionally fine-tune the model by maximizing the log likelihood of the ED predictions () using the training set of the CoNLL dataset.
We mask 90\% of the mentions and fix the entity token embeddings ( and ) and the output bias ( and ).
The model is trained for two epochs using Adam.
Additional details are provided in Appendix \ref{sec:fine-tuning-details}.

\subsection{Results and Analysis}

\begin{table}[t]
  \centering
  \scalebox{0.69}{
    \begin{tabular}{l|c|c}
      \hline
      Name                                             & Train      & Accuracy                \\
      \hline
      \newcite{Yamada2016}                             & \checkmark & 91.5                    \\
      \newcite{ganea-hofmann:2017:EMNLP2017}           & \checkmark & 92.220.14          \\
      \newcite{yang-etal-2018-collective}              & \checkmark & 93.0                    \\
      \newcite{le-titov-2018-improving}                & \checkmark & 93.070.27          \\
      \newcite{cao-EtAl:2018:C18-1}                    &            & 80                      \\
      \newcite{Fang:2019:JEL:3308558.3313517}          & \checkmark & 94.3                    \\
      \newcite{Shahbazi2019Entity-awareDisambiguation} & \checkmark & 93.460.14          \\
      \newcite{le-titov-2019-boosting}                 &            & 89.660.16          \\
      \newcite{Broscheit2019InvestigatingLinking}      & \checkmark & 87.9                    \\
      \newcite{yang2019learning} (DCA-SL)              & \checkmark & 94.640.2           \\
      \newcite{yang2019learning} (DCA-RL)              & \checkmark & 93.730.2           \\
      \hline
      \textbf{Our (confidence-order)}                  & \checkmark & \textbf{95.040.24} \\
      \textbf{Our (natural-order)}                     & \checkmark & 94.760.26          \\
      \textbf{Our (local)}                             & \checkmark & 94.490.22          \\
      \hline
      \textbf{Our (confidence-order)}                  &            & 92.42                   \\
      \textbf{Our (natural-order)}                     &            & 91.68                   \\
      \textbf{Our (local)}                             &            & 90.80                   \\
      \hline
    \end{tabular}
  }
  \caption{In-KB accuracy on the CoNLL dataset. The 95\% confidence intervals over five runs are also reported if available. \textbf{Train:} whether the model is trained using the training set of the CoNLL dataset.}
  \label{tb:conll-results}
\end{table}

\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{2pt}
  \scalebox{0.69}{
    \begin{tabular}{l|c|c|c|c|c|c|c}
      \hline
      Name                                             & Train      & MSB           & AQ            & ACE           & CW            & WW            & Avg.          \\
      \hline
      \newcite{ganea-hofmann:2017:EMNLP2017}           & \checkmark & 93.7          & 88.5          & 88.5          & 77.9          & 77.5          & 85.2          \\
      \newcite{yang-etal-2018-collective}              & \checkmark & 92.6          & 89.9          & 88.5          & \textbf{81.8} & 79.2          & 86.4          \\
      \newcite{le-titov-2018-improving}                & \checkmark & 93.9          & 88.3          & 89.9          & 77.5          & 78.0          & 85.5          \\

      \newcite{cao-EtAl:2018:C18-1}                    &            & -             & 87            & 88            & -             & 86            & -             \\
      \newcite{Fang:2019:JEL:3308558.3313517}          & \checkmark & 92.8          & 87.5          & 91.2          & 78.5          & 82.8          & 86.6          \\
      \newcite{Shahbazi2019Entity-awareDisambiguation} & \checkmark & 92.3          & 90.1          & 88.7          & 78.4          & 79.8          & 85.9          \\
      \newcite{le-titov-2019-boosting}                 &            & 92.2          & 90.7          & 88.1          & 78.2          & 81.7          & 86.2          \\
      \newcite{yang2019learning} (DCA-SL)              & \checkmark & 94.6          & 87.4          & 89.4          & 73.5          & 78.2          & 84.6          \\
      \newcite{yang2019learning} (DCA-RL)              & \checkmark & 93.8          & 88.3          & 90.1          & 75.6          & 78.8          & 85.3          \\
      \hline
      \textbf{Our (confidence-order)}                  &            & \textbf{96.3} & \textbf{93.5} & \textbf{91.9} & 78.9          & 89.1          & \textbf{89.9} \\
      \textbf{Our (natural-order)}                     &            & 96.1          & 92.9          & \textbf{91.9} & 78.4          & \textbf{89.2} & 89.6          \\
      \textbf{Our (local)}                             &            & 96.1          & 91.9          & \textbf{91.9} & 78.4          & 88.8          & 89.3          \\
      \hline
      \textbf{Our (confidence-order)}                  & \checkmark & 94.1          & 91.5          & 90.7          & 78.3          & 87.6          & 88.4          \\
      \textbf{Our (natural-order)}                     & \checkmark & 94.1          & 90.9          & 90.7          & 78.3          & 87.4          & 88.3          \\
      \textbf{Our (local)}                             & \checkmark & 93.9          & 90.8          & 90.7          & 78.2          & 87.2          & 88.2          \\
      \hline
    \end{tabular}
  }
  \caption{Micro F1 scores on the five ED datasets. \textbf{Train:} whether the model is trained using the training set of the CoNLL dataset.}
  \label{tb:five-ed-results}
\end{table}

Table \ref{tb:conll-results} presents the results of the CoNLL dataset.
Our global models successfully outperformed all the recent strong models, including models based on ELMo \cite{Shahbazi2019Entity-awareDisambiguation} and BERT  \cite{Broscheit2019InvestigatingLinking}.
Furthermore, our confidence-order model trained only on our Wikipedia-based annotations outperformed two recent models trained on the in-domain training set of the CoNLL dataset, namely, \newcite{Yamada2016} and \newcite{ganea-hofmann:2017:EMNLP2017}.

Table \ref{tb:five-ed-results} presents the results of the datasets other than the CoNLL dataset.
Our models trained only on our Wikipedia-based annotations outperformed recent strong models on the MSB, AQ, ACE, and WW datasets.
Additionally, we also tested the performance of our models fine-tuned on the CoNLL dataset, and found that fine-tuning generally degraded the performance on these five datasets.

Furthermore, our local model performed equally or worse in comparison with our global models on all datasets.
This clearly demonstrates the effectiveness of using global contextual information even if the local contextual information is modeled based on expressive contextualized embeddings.
Moreover, the natural-order model performed worse than the confidence-order model on most datasets.

Additionally, our models performed relatively worse on the CW dataset.
We consider that our model failed to capture important contextual information because this dataset is significantly longer on average than other datasets, i.e., approximately 1,700 words per document on average, which is more than thrice longer than the maximum word length of our model (i.e., 512 words).
We also consider that \newcite{yang-etal-2018-collective} achieved excellent performance on this specific dataset because their model is based on various hand-engineered features capturing document-level contextual information.

\begin{table}[tb]
  \centering
  \setlength{\tabcolsep}{4pt}
  \scalebox{0.69}{
    \begin{tabular}{c|c|c|c|c}
      \hline
      \# annotations & confidence-order & natural-order & local & \makecell{G\&H2017} \\
      \hline
      0              & 1.0              & 1.0           & 1.0   & 0.8                 \\
      1--10          & 95.55            & 95.55         & 95.55 & 91.93               \\
      11--50         & 96.98            & 96.70         & 96.43 & 92.44               \\
      51       & 96.64            & 96.38         & 95.80 & 94.21               \\
      \hline
    \end{tabular}
  }
  \caption{In-KB accuracy on the CoNLL dataset split by the frequency in Wikipedia entity annotations. Our models were fine-tuned using the CoNLL dataset. \textbf{G\&H2017}: The results of \newcite{ganea-hofmann:2017:EMNLP2017}.}
  \label{tb:analysis}
\end{table}

To investigate how the global contextual information helped our model to improve performance, we manually analyzed the difference between the predictions of the local, natural-order, and confidence-order models.
The CoNLL dataset was used to fine-tune and test the models.

The local model often failed to resolve mentions of common names referring to specific entities (e.g., ``New York'' referring to the basketball team \textit{New York Knicks}).
Global models were generally better to resolve such mentions because of the presence of strong global contextual information (e.g., mentions referring to basketball teams).

Furthermore, we found that the CoNLL dataset contains mentions that require a highly detailed context to resolve.
For example, a mention ``Matthew Burke'' can refer to two different former Australian rugby players.
Although the local and natural-order models incorrectly resolved this mention to the player who has the larger number of occurrences in our Wikipedia-based annotations, the confidence-order model successfully resolved this mention by disambiguating its contextual mentions, including his colleague players, in advance.
We provide detailed inference of the corresponding document in Appendix \ref{sec:inference-example}.

Next, we examined if our model learned effective embeddings for rare entities using the CoNLL dataset.
Following \newcite{ganea-hofmann:2017:EMNLP2017}, we used the mentions of which entity candidates contain their gold entities, and measured the performance by dividing the mentions based on the frequency of their entities in the Wikipedia annotations used to train the embeddings.
As presented in Table \ref{tb:analysis}, our models achieved enhanced performance to predict rare entities.

\section{Conclusions}
We proposed a global ED model based on contextualized embeddings trained using Wikipedia.
Our experimental results demonstrate the effectiveness of our model across a wide range of ED datasets.

\bibliographystyle{acl_natbib}
\bibliography{references}

\clearpage

\appendix

\section{Details of Training of Contextualized Embeddings}
\label{sec:pretraining-details}
As the input corpus for training our contextualized embeddings, we used the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations.
We generated input sequences by splitting the content of each page into sequences comprising  words and their entity annotations (i.e., hyperlinks).

To stabilize the training, we updated only those parameters that were randomly initialized (i.e., fixed the parameters initialized using BERT) at the first epoch, and updated all the parameters in the remaining six epochs.
We implemented the model using PyTorch, and the training took approximately ten days using eight Tesla V100 GPUs.

The hyper-parameters used in the training are detailed in Table  \ref{tb:pretraining-config}.

\begin{table}[tbhp]
  \centering
  \begin{tabular}{l|c}
    \hline
    Name                            & Value  \\
    \hline
    number of hidden layers         & 24     \\
    hidden size                     & 1024   \\
    attention heads                 & 16     \\
    attention head size             & 64     \\
    activation function             & gelu   \\
    maximum word length             & 512    \\
    batch size                      & 2048   \\
    learning rate (1st epoch)       & 5e-4   \\
    learning rate decay (1st epoch) & none   \\
    warmup steps (1st epoch)        & 1000   \\
    learning rate                   & 5e-5   \\
    learning rate decay             & linear \\
    warmup steps                    & 1000   \\
    dropout                         & 0.1    \\
    weight decay                    & 0.01   \\
    gradient clipping               & 1.0    \\
    adam                   & 0.9    \\
    adam                   & 0.999  \\
    adam                  & 1e-6   \\
    \hline
  \end{tabular}
  \caption{Hyper-parameters used for training our contextualized embeddings.}
  \label{tb:pretraining-config}
\end{table}

\section{Details of Fine-tuning on CoNLL Dataset}
\label{sec:fine-tuning-details}
The hyper-parameters used in the fine-tuning on the CoNLL dataset are detailed in Table \ref{tb:fine-tuning-config}.
We selected these hyper-parameters from the search space described in \newcite{devlin2018bert} based on the accuracy on the development set of the CoNLL dataset.

\begin{table}[tbhp]
  \centering
  \begin{tabular}{l|c}
    \hline
    Name                & Value  \\
    \hline
    maximum word length & 512    \\
    number of epochs    & 2      \\
    batch size          & 16     \\
    learning rate       & 2e-5   \\
    learning rate decay & linear \\
    warmup proportion   & 0.1    \\
    dropout             & 0.1    \\
    weight decay        & 0.01   \\
    gradient clipping   & 1.0    \\
    adam       & 0.9    \\
    adam       & 0.999  \\
    adam      & 1e-6   \\
    \hline
  \end{tabular}
  \caption{Hyper-parameters during fine-tuning on the CoNLL dataset.}
  \label{tb:fine-tuning-config}
\end{table}

\section{Example of Inference by Confidence-order Model}
\label{sec:inference-example}

\begin{figure*}[tbhp]
  \centering
  \fbox{
    \includegraphics[width=\textwidth]{inference-example-crop.pdf}
  }
  \caption{An illustrative example showing the inference performed by our fine-tuned confidence-order model on a document in the CoNLL dataset. Mentions are shown as underlined. Numbers in bold face represent the selection order of the confidence-order model.}
  \label{fig:inference-example}
\end{figure*}

Figure \ref{fig:inference-example} shows an example of the inference performed by our confidence-order model fine-tuned on the CoNLL dataset.
The document was obtained from the test set of the CoNLL dataset.
As shown in the figure, the model started with unambiguous player names to recognize the topic of the document, and subsequently resolved the mentions that were challenging to resolve.

Notably, the model correctly resolved the mention ``Nigel Walker'' to the corresponding former rugby player instead of a football player, and the mention ``Matthew Burke'' to the correct former Australian rugby player born in 1973 instead of the former Australian rugby player born in 1964, by resolving other contextual mentions, including their colleague players in advance.
These two mentions are denoted in red in the figure.
Note that our local model failed to resolve both mentions, and our natural-order model failed to resolve ``Mattew Burke.''
\end{document}
