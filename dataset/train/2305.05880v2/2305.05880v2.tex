 
As the raw data is quite diverse with varied annotation quality, automated data cleaning is necessary to remove videos which are either with low-quaity annotations or lacking meaningful visual content for a broad audience. With manual screening on the raw data, we empirically find that unwanted videos can be largely attributed to the following four categories, \ie \emph{empty-title}, \emph{face-only}, \textit{text-heavy}, and \emph{content-less}. To that end, we develop as follows a multimodal method to identify videos of the four categories step-by-step, and remove them accordingly. 


\subsubsection{Empty-title video removal}

We consider a video title empty if it has no verb-noun phrase (VNP) and thus tells little about the video content. In order to determine the presence of VNPs, we parse the given title with HanLP \cite{hanlp}, an open-source Chinese NLP toolbox\footnote{\url{https://github.com/hankcs/HanLP}}, to obtain a syntactical representation of the title. Accordingly, syntax pattern matching is performed to find verb-object structures, nominal phrases with modifier-head constructions, and subject-verb-object constructions within the representation. 

\begin{CJK*}{UTF8}{gbsn}
Note that due to the Chinese video-sharing culture, many titles are mainly  comprised of slang terms such as ``名场面'' (iconic scene), ``打卡挑战'' (daily attendance), and ``跟着UP主创作吧'' (follow the uploader to create it). These terms are so frequently used that they tell little about the video content, and thus form the basis of our stopword list. We further expand the list to cover mental verbs such as ``觉得'' (think), ``知道'' (know) and ``建议'' (suggest) and non-Chinese characters such as punctuation, emoticons and Japanese / Korean characters. The title of a given video is filtered with the stopword list, followed by syntax pattern matching. If no VNP is found, the given video will be removed.
\end{CJK*}

\subsubsection{Face-only video removal}

Face-only videos show mostly faces, with little background or other visual elements. More specifically, we observe two major patterns, \ie \emph{talking head} and \emph{face mosaic}. A talking-head video shows only a person's head (and shoulders), while a face-mosaic video contains frames showing a collage of many (small) faces. We therefore resort to frame-wise face detection. We adopt InsightFace\footnote{\url{https://insightface.ai/}}, an open-source deep face analysis library, with its default detection model (SCRFD-10GF). For a given video, we sample its frames uniformly. Face detection is performed per frame. A frame is classified as talking-head if a detected face region is over 50\% of the image area. Accordingly, the given video talking-head is treated as talking-head if more than 75\% of its frames are talking-head. To determine if the video is face-mosaic, we count the maximum number of faces detected per frame. If the number exceeds a given threshold (which is empirically set to 8), the video is labeled as face-mosaic. As such, we remove face-only videos.


















\subsubsection{Text-heavy video removal}

Video with many texts on their frames typically lack visual elements of common interest. In order to filter out such text-heavy videos, we conduct Optical Character Recognition (OCR) on frames. In particular, we adopt PaddleOCR\footnote{\url{https://github.com/PaddlePaddle/PaddleOCR}}, a public and leading OCT tool that recognizes multilingual texts from images. A frame is considered as text-heavy, if the number of OCR-detected characters exceeds 50. Similarly, we consider a video text-heavy if more than 75\% of its frames are text-heavy.



\subsubsection{Content-less video removal}

We consider a given video content-less if it lacks recognizable object, action or scene. To that end, we employ existing visual recognition models to estimate if there is any object / action / scene present in the given video. For scene recognition, we employ a ResNet-152 network trained on the Places-365 image dataset \cite{places365} (ResNet-P365). As the input of ResNet-P365 shall be an image, we simply take the middle frame of the given video. For action recognition, we utilize a Video Swin Transformer (Swin-B as its backbone) \cite{k400-swint} trained on the Kinetics-400 video dataset \cite{k400}, which we term SwinB-K400. Note that the 365 scene / 400 action classes defined in Places365 / Kinetics-400 are insufficient to cover the rich content of the Chinese videos. ResNet-P365 tends to incorrectly categorize videos of dogs or cats as ``veterinarians office'', while SwinB-K400 tends to mistakenly label videos of cooking as ``cooking chicken''. Despite such biases, their predictions remain instructive to filter out content-less videos.

For object recognition, we curate a set of 3,841 object labels by merging object classes from  MSCOCO \cite{mscoco}, VisualGenome \cite{visualgenome2017}, Objects365 \cite{Objects365}, LVIS \cite{LVIS2019} and OpenImages \cite{OpenImages}. In order to predict the relevance of these objects \wrt the video, we use a pre-trained CLIP model (ViT-B/32) \cite{clip} for zero-shot tagging on the middle frame. With the English labels manually translated to Chinese, we further employ CN-CLIP \cite{cnclip}, a Chinese version of CLIP, to tag the video with the Chinese object labels. 


Object-wise, we consider a video not content-less if the video is predicted with at least one highly confident label (cutoff at the 75th percentile) or two moderately confident labels (cutoff at the 50th percentile). Action and scenes labels are postprocessed in a similar manner. A video is treated as content-less if no label is emitted.



With the video cleaning process described above, we obtain a cleaned set of 50k videos, termed ChinaOpen-50k. Video duration is between 3 seconds to 608 seconds, with a mean value of 29.8 and median of 27. File size per video is between 81.9KB and 20.9MB, with a mean value of 1.4MB and a median value of 1.2MB. ChinaOpen-50k has 431.2 hours and 69.1 GB of videos in total. With 19.2 characters on average, video titles are longer than those in the raw data (16.4 characters on average). More importantly, compared to the raw data, ChinaOpen-50k provides a better starting point for both multimodal learning and fine-grained manual annotation. 


In addition to the user-generated titles, we enrich the annotations of ChinaOpen-50k by auto captioning.  We adopt an existing model  \cite{DongCCHW021}, trained on a joint set of  MSR-VTT \cite{msrvtt}, VaTeX \cite{vatex}, TGIF \cite{tgif2015} and Action-GIF \cite{autogif}.  As the generated captions are in English, we use machine translation\footnote{\url{https://fanyi-api.baidu.com/}} to convert them to Chinese. 




















