\section{Experiment}
\label{sec:result}

\subsection{Experiment Setup}
\label{sec:setup}
\textbf{Datasets.} Following our predecessors, we train and test all models on the public and widely used AIDA CoNLL-YAGO dataset \cite{hoffart2011robust}. The target knowledge base is Wikipedia. The corpus consists of 946 documents for training, 216 documents for development and 231 documents for testing (AIDA-train/A/B respectively). To evaluate the generalization ability of each model, we apply cross-domain experiments following the same setting in~\cite{ganea2017deep, le2018improving, yang2018collective}. Models are trained on AIDA-train, and evaluated on five popular public datasets: AQUAINT~\cite{milne2008learning}, MSNBC~\cite{cucerzan2007large}, ACE2004~\cite{ratinov2011local}, CWEB~\cite{guo2016robust} and WIKI \cite{guo2016robust}. The statistics of these datasets are available in Table~\ref{tab:dataset}. In the candidate generation step, we directly use the candidates provided by the Ment-Norm system \cite{le2018improving}\footnote{https://github.com/lephong/mulrel-nel}, and their quality is also listed in Table \ref{tab:dataset}.\\

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{lc}
  \toprule[2pt]
  \textbf{System}  & \textbf{\begin{tabular}[x]{@{}c@{}} In-KB acc. (\%)\end{tabular}} \\
\toprule[2pt]
  AIDA-light~\cite{nguyen2014aida} & 84.8 \\
  WNED~\cite{guo2016robust} & 89.0 \\
  Global-RNN~\cite{nguyen2016joint} & 90.7  \\
  MulFocal-Att~\cite{globerson2016collective} & 91.0 \\
  Deep-ED~\cite{ganea2017deep} & 92.22\\
  Ment-Norm~\cite{le2018improving} & 93.07 \\
\midrule
  Prior ()~\cite{ganea2017deep} &  71.51 \\
  \midrule
  \emph{Berkeley-CNN (Sec.~\ref{sec:base})}  & 84.21 \\
  \emph{Berkeley-CNN~+~DCA-SL} & 92.72  0.3\\
  \emph{Berkeley-CNN~+~DCA-RL} & 92.37  0.1 \\
\emph{ETHZ-Attn (Sec.~\ref{sec:base})} & 90.88 \\
  \emph{ETHZ-Attn~+~DCA-SL} &  \textbf{{94.64}}  0.2 \\
  \emph{ETHZ-Attn~+~DCA-RL} & {93.73}  0.2 \\
\bottomrule[2pt]
\end{tabular}}
\caption{\textbf{In-domain Performance Comparison on the AIDA-B Dataset.} For our method we show 95\% confidence intervals obtained over 5 runs. DCA-based models achieve the best reported scores on this benchmark.
}
\label{tab:conll}
\end{table}




\begin{table*}[t]
\centering
\scalebox{1}{
\resizebox{\textwidth}{!}{\begin{tabular}{lccccc}
  \toprule[2pt]
  \textbf{System} & \textbf{MSBNC} & \textbf{AQUAINT} & \textbf{ACE2004} & \textbf{CWEB} &\textbf{WIKI} \\
  \toprule[2pt]
AIDA~\cite{hoffart2011robust} & 79 & 56 & 80 & 58.6 & 63 \\
  GLOW~\cite{ratinov2011local} & 75 & 83 & 82 & 56.2 & 67.2 \\
  RI~\cite{cheng2013relational} & 90 & \textbf{90} & 86 & 67.5 & 73.4 \\ 
  WNED~\cite{guo2016robust} & 92 & 87 & 88 & 77 & \textbf{84.5} \\
  Deep-ED~\cite{ganea2017deep} &  93.7 & 88.5 & 88.5 & \textbf{77.9} & 77.5 \\
  Ment-Norm~\cite{le2018improving} & 93.9 & 88.3 & 89.9 & 77.5 & 78.0 \\
  \midrule
  Prior ()~\cite{ganea2017deep} & 89.3 & 83.2 & 84.4 & 69.8 & 64.2 \\
\midrule
  \emph{Berkeley-CNN (Section \ref{sec:base})} & 89.05 & 80.55 & 87.32 & 67.97 & 60.27 \\
  \emph{Berkeley-CNN~+~DCA-SL} & 93.38  0.2 & 85.63  0.3 & 88.73  0.3 & 71.01  0.1 & 72.55   0.2 \\
  \emph{Berkeley-CNN~+~DCA-RL} & 93.65  0.2 & 88.53  0.3 & 89.73  0.4 & 72.66  0.4 & 73.98  0.2 \\
\emph{ETHZ-Attn (Section  \ref{sec:base})} & 91.97 & 84.06 & 86.92 & 70.07 & 74.37  \\
  \emph{ETHZ-Attn~+~DCA-SL} &  \textbf{94.57}  0.2 & 87.38  0.5 & 89.44  0.4 & 73.47  0.1 & 78.16  0.1 \\
  \emph{ETHZ-Attn~+~DCA-RL} & 93.80  0.0 & 88.25  0.4 & \textbf{90.14}  0.0 & 75.59  0.3 & 78.84  0.2 \\
\bottomrule[2pt]
\end{tabular}}
}
\caption{\textbf{Performance Comparison on Cross-domain Datasets using F1 score (\%)}. The best results are in bold. Note that our own results all retain two decimal places. Other results with uncertain amount of decimal places are directly retrieved from their original paper.}
\label{tab:soa-cross}
\end{table*}



\medskip
\noindent
\textbf{Compared Methods.}
We compare our methods with following existing systems that report state-of-the-art results on the test datasets: \textbf{AIDA-light}~\cite{nguyen2014aida} uses a kind of two-stage collective mapping algorithm and designs several domain or category related coherence features.
\textbf{WNED}~\cite{guo2016robust} applies random walks on carefully built disambiguation graphs and uses a greedy, iterative and global disambiguation algorithm based on Information Theory.
\textbf{Global-RNN}~\cite{nguyen2016joint} develops a framework based on convolutional neural networks and recurrent neural networks to simultaneously model the local and global features.
\textbf{MulFocal-Att}~\cite{globerson2016collective} adopts a coherence model with a multi-focal attention mechanism.
\textbf{Deep-ED}~\cite{ganea2017deep} leverages learned neural representations, and uses a deep learning model combined with a neural attention mechanism and graphical models.
\textbf{Ment-Norm}~\cite{le2018improving} improving the \textbf{Deep-ED} model by modeling latent relations between mentions.


For a fair comparison with prior work, we use the same input as the \textbf{WNED}, \textbf{Deep-ED} and \textbf{Ment-Norm} (models proposed after 2016), and report the performance of our model with both Supervised Learning (\textbf{DCA-SL}) and Reinforcement Learning (\textbf{DCA-RL}). We won't compare our models with the \textbf{RLEL}~\cite{fang2019joint} which is a deep reinforcement learning based LSTM model. There are two reasons: 1) \textbf{RLEL} uses optimized candidate sets with smaller candidate size and higher gold recall than ours and the listed baselines. 2) \textbf{RLEL} uses additional training set from Wikipedia data. \cite{fang2019joint} doesn't release either their candidate sets or updated training corpus, so the comparison with their work would be unfair for us.

\medskip
\noindent\textbf{Hyper-parameter Setting.} We coarsely tune the hyper-parameters according to model performance on AIDA-A. We set the dimensions of word embedding and entity embedding to 300, where the word embedding and entity embedding are publicly released by \cite{pennington2014glove} and  \cite{ganea2017deep} respectively. Hyper-parameters of the best validated model are: , , , and the probability of dropout is set to 0.2. Besides, the rank margin  and the discount factor . We also regularize the \emph{Agent} model as adopted in \cite{ganea2017deep} by constraining the sum of squares of all weights in the linear layer with . When training the model,  we use Adam~\cite{kingma2014adam} with learning rate of 2e-4 until validation accuracy exceeds 92.8\%, afterwards setting it to 5e-5. \\

\begin{table}[]
\centering
\scalebox{0.88}{
\resizebox{\columnwidth}{!}{\begin{tabular}{c|cc}
\toprule[2pt]
    \multirow{2}{*}{\textbf{System}} & \multicolumn{2}{c}{\textbf{In-KB acc. (\%)}} \\ \cline{2-3} 
                                     & \textbf{SL}           & \textbf{RL}           \\ \hline
    ETHZ-Attn (Section \ref{sec:base})                   & 90.88                 & -                     \\
    ETHZ-Attn + 1-hop DCA           & 93.69                 & 93.20                 \\
    ETHZ-Attn + 2-hop DCA           & 94.47                 & 93.76                 \\ 
    \bottomrule[2pt] 
\end{tabular}}
}
\caption{\textbf{Ablation Study on Neighbor Entities.} We compare the performance of DCA with or without neighbor entities (i.e., 2-hop vs. 1-hop).}
\label{tab:abl_hop}
\end{table}
