\documentclass[a4paper,notitlepage,onecolumn]{article}

\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{subfig}
\usepackage[left=3cm,top=2.5cm,right=3cm,bottom=3.3cm]{geometry}



\def\1{\mathbf{1}}
\def\0{\mathbf{0}}

\def\Z{\mathds{Z}}
\def\N{\mathds{N}}
\def\R{\mathds{R}}
\def\C{\mathds{C}}
\def\P{\mathds{P}}
\def\T{\mathds{T}}

\def\I{{\cal I}}
\def\K{{\cal K}}
\def\F{{\cal F}}
\def\X{{\cal X}}

\def\m{{\bf m}}
\def\a{{\bf a}}
\def\b{{\bf b}}
\def\x{{\bf x}}
\def\u{{\bf u}}
\def\k{{\bf k}}
\def\h{{\bf h}}
\def\e{{\bf e}}
\def\f{{\bf f}}
\def\b{{\bf b}}
\def\v{{\bf v}}
\def\w{{\bf w}}
\def\q{{\bf q}}
\def\t{{\bf t}}
\def\y{{\bf y}}
\def\K{{\bf K}}
\def\H{{\bf H}}
\def\I{{\bf I}}
\def\P{{\bf P}}
\def\Q{{\bf Q}}
\def\A{{\bf A}}
\def\B{{\bf B}}
\def\Rb{{\bf R}}
\def\S{{\bf S}}
\def\J{{\bf J}}
\def\T{{\bf T}}
\def\G{{\bf G}}
\def\z{{\bf z}}
\def\Rb{{\bf R}}

\def\lambdab{\boldsymbol{\lambda}}
\def\phib{\boldsymbol{\phi}}
\def\psib{\boldsymbol{\psi}}
\def\varphib{\boldsymbol{\varphi}}
\def\Gammab{\boldsymbol{\Gamma}}
\def\epsilonb{\boldsymbol{\epsilon}}
\def\xib{\boldsymbol{\xi}}
\def\betab{\boldsymbol{\beta}}
\def\rhob{\boldsymbol{\rho}}
\def\mub{\boldsymbol{\mu}}
\def\upsilonb{\boldsymbol{\upsilon}}
\def\zetab{\boldsymbol{\zeta}}

\def\diag{\text{diag}}
\def\card{\text{card}}
\def\trace{{\rm tr}\,}
\def\E{\,\mathds{E}\,}
\def\sgn{\text{sgn}}
\def\MSE{\text{MSE}}
\def\SNR{\text{SNR}}
\def\barSNR{\overline{\text{SNR}}}




\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\numberwithin{equation}{section}



\graphicspath{{figures/}}



\begin{document}

\title{Distributed Estimation over Wireless Sensor Networks\\ with Packet Losses
\thanks{C.~Fischione and A.~Sangiovanni-Vincentelli wish to acknowledge the
support of the NSF ITR CHESS and the GSRC. The work by A.~Speranzon
was partially supported by the European Commission through the Marie
Curie Transfer of Knowledge project BRIDGET (MKTD-CD 2005 029961).
C.~Fischione and A.~Speranzon acknowledge the support of the San
Francisco Italian Institute of Culture by the Science \&
Technology Attach\'e T.~Scapolla. The work by K. H. Johansson was partially funded by the Swedish Foundation for Strategic Research and by the Swedish Research Council.}}
\author{C.~Fischione\thanks{C.~Fischione and A.~Sangiovanni-Vincentelli are with University of California at Berkeley, CA. E-mail:
\texttt{\{fischion,alberto\}@eecs.berkeley.edu}} \and A.~Speranzon\thanks{A.~Speranzon is
with Unilever R\&D Port Sunlight, CH63 3JW, United Kingdom. E- mail:
\texttt{alberto.speranzon@gmail.com}.} \and K.~H.~Johansson\thanks{K.~H.~Johansson is with the
ACCESS Linnaeus Center, Electrical Engineering, Royal Institute of Technology, 100-44 Stockholm, Sweden. E-mail: \texttt{kallej@ee.kth.se}}\and A. Sangiovanni-Vincentelli}

\maketitle
\begin{abstract}
A distributed adaptive algorithm to estimate a time-varying
signal, measured by a wireless sensor network, is designed and
analyzed. One of the major features of the algorithm is that no
central coordination among the nodes needs to be assumed. The
measurements taken by the nodes of the network are affected by
noise, and the communication among the nodes is subject to packet
losses. Nodes exchange local estimates and measurements with
neighboring nodes. Each node of the network locally computes
adaptive weights that minimize the estimation error variance.
Decentralized conditions on the weights, needed for the
convergence of the estimation error throughout the overall
network, are presented. A Lipschitz optimization problem is posed
to guarantee stability and the minimization of the variance. An
efficient strategy to distribute the computation of the optimal
solution is investigated. A theoretical performance analysis of
the distributed algorithm is carried out both in the presence of
perfect and lossy links. Numerical simulations illustrate
performance for various network topologies and packet loss
probabilities.
\end{abstract}


\begin{center}
\emph{Keywords}: Distributed Estimation; Wireless Sensor Networks; Parallel and Distributed Computation; Convex Optimization; Lipschitz Optimization.
\end{center}



\section{Introduction}
\label{sec:introduction}

Monitoring physical variables is a typical task performed by
wireless sensor networks (WSNs). Accurate estimation of these
variables is a major need for many applications, spanning from
traffic control, industrial manufacturing automation, environment
monitoring, to security
systems~\cite{EGH99}--\nocite{GK03}\cite{SIDSP06}. However, nodes
of WSNs have limitations, such as scarcity of energy supply, lightweight
processing and communication functionalities, with the
consequence that sensed data are affected by bias and noise, and
transmission is subject to interference, which results in
corrupted data (packet loss). Estimation algorithms must be
designed to cope with these adverse conditions, while offering
high accuracy.

There are two main estimation strategies for WSNs. A traditional
approach consists in letting nodes sense the environment and then
report data to a central unit, which extracts the desired physical
variable and sends the estimate to each local node for local
action. However, this approach has strong limitations: large
amount of communication resources (radio power, bandwidth,
routing, etc.) have to be managed for the transmission of
information from nodes to the central unit and vice versa, which
reduces the nodes' lifetime. An alternative approach, which we
investigate in this paper, enables each node to locally produce
accurate estimates taking advantage of data exchanged with only
neighboring nodes. Indeed, wireless communication makes it natural
to exploit cooperative strategies, as it has been already used for
coding and transmission~\cite{SIDSP06,JSAC06}. The challenge of
distributed estimation is that local processing must be carefully
designed to avoid heavy computations and spreading of local errors
throughout the network.

In this paper we consider the design and analysis of a distributed
estimation algorithm. Specifically, a time-varying signal is
jointly tracked by the nodes of a WSN, in which each node computes
an estimate as a weighted sum of its own and its neighbors'
measurements and estimates. The distributed estimator features
three particular characteristics: it is robust to packet losses,
it does not rely on a model of the signal to track, and it uses
filter coefficients that adapt to the changing network topology
caused by packet losses. We show that the estimation problem has a
distributed implementation. It is argued that the estimator
exhibits high accuracy, in term of estimation error variance, even
in the presence of severe packet losses, if the signal to track is
varying slowly.

\subsection{Related Work}

The estimator presented in this paper is related to recent
contributions on low-pass filtering by diffusion mechanisms,
e.g.,~\cite{paperCDC06}--\nocite{paperICC07,Morse03-1,OM04,ConsensusAlberto,XiaoBoydLall,SOSM-CDC05}\cite{Olfati05},
where each node of the network obtains the average of the initial
samples collected by nodes. In~\cite{XiaoBoydKim,XiaoBoyd} the
authors study a distributed average computation of a time-varying
signal, when the signal is affected by a zero-mean noise.
Distributed filtering using model-based approaches is studied in
various wireless network contexts,
e.g.,~\cite{SOSM-CDC05},~\cite{shang}--\nocite{GK-CDC06,OlfatiCDC07}\cite{Alriksson+07}.
In particular, distributed Kalman filters and more recently a
combination of the diffusion mechanism with distributed Kalman
filtering have been proposed, e.g.,~\cite{CarliChiuso+}.

In~\cite{JSAC07}, we have presented a distributed estimator to
track a time-varying signal without relaying on a model of the
signal to track, in contrast to model-based approaches,
e.g.,~\cite{SOSM-CDC05, GK-CDC06}. The approach is novel,
since~\cite{OM04}--\nocite{ConsensusAlberto}\cite{XiaoBoydLall}
are limited to averaging initial samples. Compared
to~\cite{XiaoBoydLall}--\cite{XiaoBoyd}
and~\cite{OlfatiCDC07}--\cite{CarliChiuso+}, we do not rely on the
Laplacian matrix associated to the communication graph. Our filter
parameters are computed through distributed algorithms which adapt
to the network topology and packet losses, whereas for
example~\cite{XiaoBoydKim} and~\cite{XiaoBoyd} rely on centralized
algorithms for designing the filters. The distributed estimator
proposed in this paper features better estimates when compared to
similar distributed algorithms presented in the literature, but at
the cost of a slightly increased computational complexity. With
respect to our earlier work~\cite{JSAC07}, here we provide a major
extension because we take into account lossy wireless
communications. Packet losses require a substantial redesign and
performance characterization of the filter proposed
in~\cite{JSAC07}. In this paper, we explicitly consider the effect
of packet losses (both i.i.d. and non-identical) in the design of
the adaptive weights so that the estimation error is guaranteed to
converge for any packet loss realization. The distributed minimum
variance estimator uses a Lipschitz optimization problem to
distribute the centralized stability constrains, whose
characterization is completely new. We devise a new algorithm to
distribute efficiently the computation of the solution of the
Lipschitz problem. An original analysis of the bounds on the
estimation error variance as function of the packet loss
probability and number of nodes is also discussed. We introduce
some examples to show how such bounds can be refined significantly
when the network topology is modelled by a graph in the class of
finite Cayley graphs.

The remainder of the paper is organized as follows: In
Section~\ref{sec:problem_formulation} we review and extend the
problem posed in~\cite{JSAC07} considering packet losses. In
Section~\ref{sec:distrib_min_var_estim} we deal with the design of
the optimal adaptive weights that minimize the estimation error
variance. In Section~\ref{sec:computation_of_psi} we show how to
compute efficiently some thresholds needed to bound the norm of
the estimation error. In Section~\ref{sec:performance_analysis} we
determine bounds on the estimation error variance achieved by the
proposed algorithm. Monte Carlo simulations are reported in
Section~\ref{sec:simulations} to illustrate the performance of the
proposed algorithm. Conclusions are drawn in
Section~\ref{sec:conclusions}.





\subsection{Notation}
\label{subsection:notation} Given a stochastic variable ,  denotes its expected value. With  we mean that the
expected value is taken with respect to the probability density
function of .  We keep
explicit the time dependence to remind the reader that the
realization is given at time . With  we denote the
-norm of a vector or the spectral norm of a matrix. Given
a matrix ,  and  denote the
minimum and maximum eigenvalue (with respect to the absolute value
of their real part), respectively, and its largest singular value
is denoted by . Given the matrix , 
is the Hadamard (element-wise) product between  and . With
 and  denote the element-wise
inequalities. With  and  we denote the identity matrix and
the vector , respectively, whose dimensions are
clear from the context. Let . To keep the
notation lighter, the time dependence of the variables and
parameters is not explicitly indicated, when this does not create
misunderstandings.

\section{Problem Formulation}
\label{sec:problem_formulation}

Consider a WSN with  sensor nodes. At every time instant,
each sensor in the network takes a noisy measure  of a
scalar signal , namely , for 
and for all . We assume that , for all~,
are normally distributed with zero mean and variance 
and that  for all .

We model the network as a weighted graph. In particular we consider
a graph, , where
 is the vertex set and
 is the edge set.


The set of neighbors of node  plus node  is
denoted as

Namely  is the set containing the maximum number of
neighbors a node~ can have, including itself.


Every node broadcasts data packets, so that these packets can be
received by any other node in the communication range. Packets may
be dropped because of bad channel conditions or radio
interference. Let , with , be a binary
random variable associated to the packet losses from node  to
 at time ~\cite{Stuber}. This random variable is defined on
the probability space , where
,  is a -algebra of subsets
of  and  a probability measure. For , we
assume that the random variables  are independent
with probability mass function:

where  denotes the successful packet reception
probability. Clearly that , since information locally
available is not subject to packet losses. Note also that
 if the packet sent from node  to  collided at
node  due to too much wireless interferences, or if the
wireless channel of the link from  to  is under deep fading,
or if  is too far from node  to receive packets from node
. The packet reception probabilities are assumed to be
independent among links, and independent from past packet losses.
These assumptions are natural when the coherence time of the
wireless channel is small if compared to the typical communication
rate of data packets over WSNs~\cite{Stuber,ieee802154}.


We assume each node~ computes an estimate  of
 by taking a linear combination of its own and of its
neighbors' estimates and measurements. Define  and similarly , then each node computes

with , and where

with , in which the -th element
is the weight coefficient used by node  for information coming
from node  at time , and  
denotes the vector of the packet reception process as seen from
node  with respect to all nodes of the network. Specifically,
the th element of , with , be
. Let  denotes a
realization of the process  at time . Notice that
at a given time instant, the -th component of 
is zero if no data packets are received from node . Let
, namely a such set collects the nodes communicating with
node~ at time . The number of nodes in the set is
.

The vector  and  are constructed from the elements , similarly to .



\section{Distributed Minimum Variance Estimator}
\label{sec:distrib_min_var_estim} In this section we
describe how each node computes adaptive weights to
minimize its estimation error variance.

\subsection{Estimation Error}

Define the estimation error at node~ as .
Introduce , then the expected error with respect to the measurement noise  is given by

where we set .

Typically one is interested in designing an unbiased estimator.
Notice, however that in~\eqref{eq:err_dyn} the expected error
depends on both unknowns  and .  The following
condition eliminates the dependence from :

for any possible realization of the packet loss process
. Note that~\eqref{eq:cond_1} holds both in the
presence of packet i.i.d. losses , and in the presence of
non-identical losses. The term  can be removed by
imposing that

for any possible realization of the packet loss process
. By imposing constraints~\eqref{eq:cond_1}
and~\eqref{eq:cond_2}, the unknown terms would disappear from the
expected error equation, so the minimum variance estimator would
be such that  and , where
 is the number of neighbors, including
node~, that are successfully communicating with node~ at
time~. We will show in the next sections that by imposing only
constraint~\eqref{eq:cond_1} we are able to design an estimator
that has lower variance than one that also obey~\eqref{eq:cond_2}.
The price paid for better performance is a biased estimator.
However, assuming that  is slowly varying (or that the
sampling frequency is high enough with resect to the variation of
the signal), the bias is negligible and the proposed estimator
outperforms the unbiased one in terms of the estimation error
variance. This can also be understood from an intuitive point of
view: having  means that nodes are
disregarding previous estimates and are just using current
measurements, which are typically corrupted by high noise. Having
a term that also weights previous estimates allows us to increment
the total available information at each node, obtaining a much
lower estimation error variance.

Notice that although~\eqref{eq:cond_1} holds, and thus  is
eliminated from~\eqref{eq:err_dyn}, we need further conditions on
vector  in order to guarantee that the
expected error asymptotically decreases.

\subsection{Convergence of Estimation Error}

In this subsection we derive conditions on the weights that ensure
that the estimation error decreasing over time, regardless the
measurement noise and the packet loss processes that affect the
system. In particular, we want to determine conditions on the
weights so that  as
.

Assume that constraint~\eqref{eq:cond_1} holds, and consider the
expected value with respect to the packet loss process
of~\eqref{eq:err_dyn}, then

where we have used the fact that the packet losses at time  are
independent from the preceding time instants, so that . It is clear that the evolution of
 depends on the overall error vector
, namely, the error at the local node depends
on the estimation error of neighboring nodes. We thus need a set
of other  equations to describe the estimation
error of all nodes in . Obviously, each new
equation will depend on the estimation error of nodes that are two
hops from node~, and so on. The full network will be considered
in this process of adding equations. Let  be the estimation error of the overall network, then

where  is the matrix whose rows are the vectors , ,
and  is the matrix whose rows are the vectors , . Let  be a realization of  at time , namely . The following result holds:
\begin{proposition}
\label{prop:error-stability} Consider the
system~\eqref{eq:error1} and assume that
\begin{itemize}
\item[(i)]   for all  and
    for each and every packet loss realization 
    of .
\item[(ii)]  for all .
\end{itemize}
Then, considering independent packet losses, we have that

\end{proposition}
\begin{proof}
For the sake of notational simplicity, define
. The dynamics of
 are thus given by a deterministic time-varying linear
system. Consider the function
. Simple algebra gives that

Now, consider that

where  is the probability of the packet reception realization  at time . The expectation is given by the sum of a finite number of combinations  of possible packet loss realizations, since the network has a finite number of links, and in each link a packet can be either successfully received or dropped, so that . It follows

because obviously , and, from assumption (i), . Therefore

from where, taking the limit , the proposition follows.
\end{proof}

\begin{remark}
Notice that the expected error converges to a neighborhood of the
origin exponentially fast, and more precisely with rate
.
\end{remark}




Proposition~\ref{prop:error-stability} provides us with conditions
for the convergence of the estimation error of the entire network
to a neighborhood of the origin. It follows that the estimation
error of the entire network is subject to a cumulative bias. It is
clear that such a bias depends on 
and . If the signal  is slowly varying, namely
, and  small, then the
bias will be small.




Notice that, in order to ensure that the estimation error
decreases at each node, a condition at network level is
required, namely it must hold that
. The constant 
can be chosen by fixing a maximum cumulative estimation
error and solving~\eqref{eq:error_bound} for
, as we show in Section~\ref{sec:simulations}.

We will show in the next subsection how to choose the
weights  and  locally at each node so that the estimation
error variance is minimized.

\subsection{Distributed Computation of Filter Coefficients}

To design a minimum variance distributed estimator, we
need to consider how the error variance evolves over time. The
estimation error variance dynamic at node  is given by

where

We assume that , and

because an initial (rough) estimate of  can be computed as
arithmetic average of the 
measurements received from neighboring nodes.

The estimation error variance~\eqref{eq:error_cov} can be
rewritten as

where we used the fact that  and .

The optimal weights  and  are chosen so that at
each time instant, for any given realization of the packet loss
process  of , the variance  is minimized, under the constrain~\eqref{eq:cond_1} and that .
As already mentioned, the second constraint is global, since 
depends on all , . However, it is possible to determine local conditions so that the global constraint is satisfied. We show next how this can be done.


For , we define the set
, which is the collection of
communicating nodes located at two hops distance from
node~ plus communicating neighbors of~, at time~.
The following result holds:
\begin{proposition}
    Suppose there exists , such that
    
    If , , then
    .
    \label{prop:gershorin-like}
\end{proposition}
\begin{proof}
The proof is similar to the proof of Proposition~III.1 in~\cite{JSAC07}.
\end{proof}
Using this proposition, the global constraint  can be replaced by the constraint , where  satisfies the set of nonlinear
inequalities~\eqref{eq:part-proof}. Therefore, each node needs to
solve the following optimization problem

We will discuss in Section~\ref{sec:computation_of_psi} how to
compute the values of , which are needed to state
problem~\eqref{eq:local-optimiz-probl1}. Observe that the
optimization problem~\eqref{eq:local-optimiz-probl1} is a
Quadratically Constrained Quadratic
Problem~\cite[pag.~653]{boyd2}. It admits a strict interior point
solution, corresponding to  and . Thus Slater's condition is satisfied and
strong duality holds~\cite[pag.~226]{boyd2}. The problem, however,
does not have a closed form solution and thus we need to rely on
numerical algorithms to derive the optimal  and
. We have the following proposition.
\begin{proposition}
    \label{prop:optimal-values}
    For a given covariance matrix  and a realization  of , the values of  and  that
    minimizes~\eqref{eq:local-optimiz-probl1} are
    
    with the optimal Lagrange multiplier .
\end{proposition}
\begin{proof}
The proof is similar to the proof of Proposition~III.2
in~\cite{JSAC07}.
\end{proof}
\begin{remark}
Modeling the packet loss by the Hadamard product allows us to
obtain weights having a similar form to those we obtained in the
case of no packet loss~\cite{JSAC07}. However, this result is not
a straightforward application of~\cite{JSAC07}
because~\eqref{eq:gainsk} and~\eqref{eq:gains} are obtained by
exploitation of the Hadamard product and the Moore-Penrose
pseudo-inverse in the computation of the Lagrange dual function
and the KKT conditions. Therefore, the previous proposition
generalizes our earlier result for any given realization of the
packet loss process. In the special case when , namely when there are no packet losses, we reobtain the
result in~\cite{JSAC07}.
\end{remark}

Previous proposition provides us with an interval within which the
optimal  is located. Simple search algorithms can be
considered to solve numerically  for
, such as, for example, the bisection algorithm.

It is worth noting that , and similarly ,
in~\eqref{eq:gains} are zero if node~ does not communicate with
node~ because of a lost packet. In such a case the -th
row and column of the matrix  is
zero. The pseudo-inverse  maintains the zeros in the
same position as those in the matrix
.

\subsection{Error Covariance Matrix}

Proposition~\ref{prop:optimal-values} provides us with the optimal
weights that minimize the estimation error variance at each time
step. The optimal weights  and  depend
indirectly on the thresholds , through the Lagrangian
multiplier , and directly on the error covariance
matrix . We will discuss in the next section how it is
possible to compute such thresholds  in a distributed
way, whereas we dedicate the rest of this subsection on discussing
how to locally compute the error covariance matrix .
More precisely, because of the packet loss process, node~
requires only the elements of  corresponding to its
neighbors, namely the matrix .

Each node can estimate from data the error covariance matrix,
which we denote with , as discussed
in~\cite{JSAC07}. However, here we need to extend the approach to
the case of packet losses, because the design of the estimator of
the covariance matrix is tricky when packets are lost. If a node
 exchanges data with its neighboring node~, after an outage
period, node~ needs to re-initialize the -th row and column
of  reasonably in order to take advantage of
the new acquired neighbor. We consider the following
re-initialization of elements of the error covariance matrix
.

If at time~ a new neighbor of a node is exchanging data, then
the diagonal element of the estimate of the error covariance
matrix at time , corresponding to such a neighbor, is
initialized to the maximum element in the diagonal of the error
covariance matrix. More precisely, let   and
assume that for , , and that . Then

and for ,

This heuristic is motivated by the fact that all nodes are
collaborating to build and estimate of , and they are using
the same algorithm. Thus the maximum variance of the estimation
error that a neighbor of a node is affected by must not be larger
than the worst variance of the estimation error of other
neighbors. Obviously, chances are that the heuristic might
overestimate the variance associated to a new neighbor. However,
from simulations in Section~\ref{sec:simulations} we see that this
strategy works well in practice, even in the presence of high packet loss
probabilities.


\section{Computation of the Thresholds}
\label{sec:computation_of_psi}

From Proposition~\ref{prop:gershorin-like}, we notice that
thresholds 's need to be upper bounded to guarantee
convergence of the estimation error. It holds that the larger the
value of  the lower the error variance. Indeed, after
some algebra, it follows that

From this inequality we have that the estimation error variance at
the node~ decreases as  decreases. From
Proposition~\ref{prop:optimal-values} we see that if  is
large, then the Lagrangian multiplier  is small, since
.

According to the arguments above, we are interested in determining
the largest solution of the nonlinear equations in
Proposition~\ref{prop:gershorin-like}. Therefore, we consider the
following optimization problem:

where  and

The solution of previous problem can be computed easily via
standard centralized approaches, but in our setup the computation
of the solution must be obtained in a decentralized fashion. The
distributed computation of the solution could be performed through
message passing, as in~\cite{JohannsonXiao06}. However, the
converge speed is prohibitive. Hence, we consider an alternative
approach. The fact that in~\eqref{eq:constraint_psi} only
information from two-hop neighboring nodes is required, and not of
the entire network, allows us to develop a decentralized algorithm
to compute the optimal solution. This is obtained in two steps.
First we show that the optimal solution satisfies the inequality
constraints~\eqref{eq:constraint_psi} with equality. Second, we build on this to distribute
the computation among nodes to obtain the optimal solution. We
provide details in the sequel.


\subsection{Equality constraints}

In this section, we show that there is a global optimal solution
of~\eqref{optp:maxpsi} that satisfies the inequality
constraints~\eqref{eq:constraint_psi} with equality. In particular
we have the following important result.

\begin{theorem} \label{theo:OP}
Problem~\eqref{optp:maxpsi} admits a global optimum , which is the solution of the following set of nonlinear equations:

where .
\end{theorem}

To prove this theorem, we need some intermediate technical results:

\begin{lemma}
\label{lemma:psi-bound}
There exists a feasible solution  of~\eqref{optp:maxpsi}, where

\end{lemma}
\begin{proof}
The -th element of , , is constructed by considering the th constraint, and imposing that the other variables , for , assume
the largest value, which is~:

By solving the equation for  we obtain

The same procedure can be repeated . The  obtained are collected into a vector . Since

 is a feasible solution.
\end{proof}
This lemma is useful, because it allows us to establish the
existence of an optimal solution:
\begin{lemma} \label{theo:OPsolutions}
Problem~\eqref{optp:maxpsi} admits an optimal solution , which is the solution of the following set of nonlinear equations:

\end{lemma}
\begin{proof}
The proof is based on a useful rewriting of the optimization
problem and by a {\it reductio ad absurdum} argument.

Let  for . Then, the optimization problem~\eqref{optp:maxpsi} can be rewritten as follows

where  and

with  being any positive scalar. This problem
and~\eqref{optp:maxpsi} are obviously equivalent: for all
,  if and only if . Let  be an optimal solution of~\eqref{optp:maxy}, then
. Problem~\eqref{optp:maxy} admits optimal
solutions, since from Proposition~\ref{lemma:psi-bound} the
problem is feasible. We show next that the optimal solutions
satisfy the constraints at the equality.

Let  be an optimal solution. Suppose by contradiction that
there is constraint  that is satisfied at a strict inequality,
namely , while suppose 
for . In the following, we show that from  we can
construct a feasible solution  such that , so that it is not possible that  be an
optimal solution.

Since  is arbitrary, we can select a convenient value. Let

This choice of  makes  being an increasing
function of , and a decreasing function of , for . Indeed

Let  such that . We have

because the third order derivatives are zero. Then, we chose a small positive scalar  so that  be an augmented vector of , with ,  for , , and . The last inequality is allowed by the fact that  is an increasing function of . From~\eqref{eq:taylor} it follows

By using  and , , we can define
a vector  such that ,
 if , and
 otherwise. Notice that  since
. The solution  is feasible for
problem~\eqref{optp:maxy}, namely , because
,
 if  and  if  and . Now, observe that

The last right-hand side of previous equation is always positive, provided that one chooses

This implies that , namely that  is a feasible solution of~\eqref{optp:maxy} with higher cost function than , which is a contradiction because  was assumed to be an optimal solution. It follows that optimal solutions must satisfy all the constraints at the equality.
\end{proof}
The previous lemma guarantees that there are optimal solutions
satisfying the constraints at the equality. However, we do not
know yet if there is a global optimal solution. If there were
multiple optimal solutions, we would have to chose the most fair
for all nodes. Recall that a small  means smaller
estimation quality. To establish the uniqueness of the optimal
solution, we need the following lemma, which will be used for the
proof of Theorem~\ref{theo:OP}:

\begin{lemma} \label{lemma:Jacob}
Let  be the Jacobian of . Then  is a nonsingular matrix.
\end{lemma}
\begin{proof}
The diagonal elements of the Jacobian are

whereas the off-diagonal elements  are either
zero if , or

By applying the Gershgorin theorem, we have that the eigenvalues of the Jacobian lie in the region

from which it follows that the real part of the minimum eigenvalue is such that

Therefore,  has no zero eigenvalues, namely it is non-singular.
\end{proof}

We are now in the position of proving Theorem~\ref{theo:OP}. From
Lemma~\ref{theo:OPsolutions}, we know that there is an optimal
solution satisfying the constraints at the equality. We show next
that such a solution is unique, thus proving
Theorem~\ref{theo:OP}.
\begin{proof}[Proof of Theorem~\ref{theo:OP}]
The proof of the uniqueness of the optimal solution is based on
the use of the Lagrange dual theory. First, observe that from
Lemma~\ref{theo:OPsolutions} the optimization problem admits
optimal solutions. The optimization problem is non-convex, since
the constraints~\eqref{eq:constraint_psi} are not convex. The
Lagrange dual theory for non-convex non-linear optimization
problems can be applied. A qualification constraint
from~\cite[pag. 25]{Horst+} states that strong duality holds if
the optimization problem is feasible and the Jacobian of
 is non-singular, which we know from
Lemma~\ref{lemma:psi-bound} and Lemma~\ref{lemma:Jacob},
respectively. Therefore, the optimal solution of the problem can
be investigated via the Lagrange dual function , where  is the
Lagrangian multiplier. From the KKT conditions it follows that
. We see that previous equality trivially
holds also for the optimal solution , namely
. Since from Lemma~\ref{lemma:Jacob}
the Jacobian is non-singular, it follows that there is a unique
solution to the previous system of equations, namely
, and since strong duality holds, we
conclude that the optimal solution given by~\eqref{eq:eqconstr} is
unique.
\end{proof}

\begin{corollary} \label{cor:lower-bound-opt-sol}
Let  be the solution of~\ref{eq:eqconstr}. Then,
, where  is given
by~\eqref{eq:ai}.
\end{corollary}
\begin{proof}
The simple proof is by contradiction. Suppose that  is not a lower bound on the optimal solution , namely there is some  for which . By observing that that , it follows

which is a contradiction, because the optimal solution must satisfy the constraints of~\ref{optp:maxpsi} at the equality.
\end{proof}

We use Theorem~\ref{theo:OP} and
Corollary~\ref{cor:lower-bound-opt-sol} in the next sections to
develop a strategy for the distributed computation of the optimal
solution.

\subsection{Distribution of the Computation}

From the previous section, we compute the thresholds to use
in~\eqref{eq:local-optimiz-probl1} by the system of nonlinear
equations~\eqref{eq:eqconstr}. Unfortunately, an explicit solution
for such a system is not available. Numerical techniques have to
be used. In the following, we present a quick decentralized
algorithm with certified convergence.

We define the class of functions parameterized in the scalar


where .
When  is contractive, then it is easy to show that the fixed point of the mapping is the solution of~\eqref{eq:eqconstr}~\cite[Pag.191]{tsi}.  Furthermore, the convergence speed can be tuned at a local node  by the parameter . We have the following result
\begin{proposition} \label{prop:fastcontrmapping}
Let

Then  is a contraction mapping having the largest convergence
speed among the mappings~\eqref{eq:mapping2}.
\end{proposition}

\begin{proof}
Proposition~ in~\cite[Pag.193]{tsi} gives a sufficient
condition to establish that~\eqref{eq:mapping2} is a contraction
mapping. If

where  is partial derivative operator with respect to
, then~\eqref{eq:mapping2} is contractive. The scalar
 determines the converge speed of the mapping, so
that the lower is  the faster is the convergence.

Suppose that

Then,  is minimized if

Suppose that~\eqref{eq:rho} does not hold, than  is minimized if . By putting together these cases, the proposition follows.
\end{proof}
From previous proposition, the overall mapping ,
where , is a contraction mapping. The component solution method~\cite[Pag.187]{tsi} can be applied.
The solution of~\eqref{optp:maxpsi} is given by the algorithm

Using the  given by Proposition~\ref{prop:fastcontrmapping}, the mapping converges quickly. From Monte Carlo simulations, we see that the algorithm converges in less than 10 iterations on average.



\subsection{Algorithm for the Computation of the Thresholds}

The distributed computation of the thresholds~ requires that the
neighboring nodes communicate the instantaneous values of the
local threshold, until~\eqref{eq:iterative_algo3} converges.
Clearly, the thresholds are over the same wireless channel used
for broadcasting estimates and measurements, and thus they are
subject to packet losses. These losses may happen during the phase
between the beginning of the iterations~\eqref{eq:iterative_algo3}
and the convergence. As a result, no convergence may be reached.
In the following, we develop a strategy to cope with this problem.



First, notice that the optimization problem is not sensitive to
perturbations of the constraints. In other words, if 
is the solution of the system of non linear
equations~\eqref{eq:eqconstr}, then  is not
significantly perturbed by packet losses. We can see this from the
proof of Theorem~\ref{theo:OP}, form where we know that the
optimal solution is such that , with
 being the Jacobian of the constraints and 
the Lagrange multipliers associated to the dual problem
of~\eqref{optp:maxpsi}. Specifically, the -th equation of
 is given by

This system of equations has positive coefficients, and
. Since , for strong duality holds, it follows that 
for . Then,  implies that the optimal
solution is not sensitive to perturbations of the
constraints~\cite[pag. 249]{boyd2}.

Since a change in the number of two-hops neighbors of a node,
caused by packet losses, can be regarded as a perturbation of the
constraints, we conclude that the optimal solution of the
problem~\eqref{optp:maxpsi} is not much sensitive to the packet
losses. By this argument, we can compute just once the optimal
solution. In particular, we assume that the nodes compute the
optimal thresholds before the estimation algorithm starts by
considering the maximum number of neighbors. This is accomplished
by using high transmission radio powers and a retransmission
protocol that guarantee a successful packet reception. Such a
preliminary phase is very short, since from
Proposition~\ref{prop:fastcontrmapping} the computation of the
thresholds according to~\eqref{eq:iterative_algo3} requires few
iterations to converge.
During the estimation phase, if the packet loss probability is
very high, the perturbation might be large, resulting in a
significant change of the optimum. However, simulations reported
in Section~\ref{sec:simulations} show that the solution we adopt
for the threshold computation is robust to rather intense packet
losses.




\section{Performance Analysis}
\label{sec:performance_analysis}

In this section we characterize the performance of our estimator
by investigating the variance of the estimation error. We have the
following results:
\begin{proposition}
    \label{cor:bound-variance-node-i}
    For any packet loss realization  of , the optimal value of  and  are such that the error variance at node~ satisfies
    
\end{proposition}

\begin{proof}
From~\eqref{eq:optim_value_var} the error variance is upper-bounded
by

where the inequality comes from the fact that , and recalling that .
\end{proof}
Notice that previous proposition guarantees that the
estimation error at each time , and in each node, is
always upper-bounded by the variance of the estimator that
just takes the averages of the received .

\begin{proposition}
    \label{prop:bound-lM-Gamma}
    For any packet loss realization  of ,
    
\end{proposition}

\begin{proof}
It holds that .
Recalling that  it follows
 
where previous inequality comes from Corollary~\ref{cor:lower-bound-opt-sol}, observing that , and that . Furthermore,  can be upper-bounded using the Ger\v{s}hgorin theorem, and recalling that the diagonal elements of  are less than  from Proposition~\ref{cor:bound-variance-node-i}, and that each diagonal element of a covariance matrix assumes the largest value along its row. Hence, it follows that .
Putting together previous inequality, and the upper bound on , the proposition follows.

\end{proof}
\begin{lemma} \label{lemma:boundexpt}

where

and the function  is a permutation.
Namely the -th coefficient of the polynomial is the sum of
 terms in which there are 
factors  and  factors 
with .
\end{lemma}
\begin{proof}
The random variable  is given by the
sum of  independent Bernoulli random
variables having different parameter. Then, we have~\cite{Chao72}

where  is the probability generating function of :

where the last equality is achieved by developing the product of
terms  in a polynomial in the general form. After
tedious manipulations, we see that the coefficients of the
polynomial are given by~\eqref{eq:poly_coeff}.
By using  in the integral~\eqref{eq:momentgen}, we obtain
the result.
\end{proof}
\begin{proposition}
For any packet loss realization  of ,
it holds

\end{proposition}
\begin{proof}
    The variance at node~ is bounded as in~\eqref{eq:optim_value_var}. Following the same steps as in the proof of
    Proposition~\ref{cor:bound-variance-node-i}, we have
    
    This inequality is based on the fact that the argument of the statistical expectation is always positive and the expectation is taken over a positive distribution, thus the sign of the argument is maintained~\cite[pag.392]{Horn85}.
    From Proposition~\ref{prop:bound-lM-Gamma}, it
    follows
    
    By using previous inequality in \eqref{eq:EPhiEvei2b}, we have
    
    The proposition follows by invoking Lemma~\ref{lemma:boundexpt}.
\end{proof}

Observe that the estimation error variance given by the previous
proposition depends on the packet loss probabilities , on
the maximum number of neighbors for each node ,
the total number of nodes in the networks , and the largest
singular value of the matrix . In
Figure~\ref{fig:e_phi_e_v_q} we have plotted the first factor of
the coefficient of~\eqref{eq:expected_value_var-slightly-bounded}.
It turns out that it is always less than 1. The smallest values
are achieved when  is large and~ small. The
second factor in~\eqref{eq:expected_value_var-slightly-bounded}
clearly depends on the value attained by the various . We
consider here the simple case when  for all~, which
allows us writing the equations in closed form:

In Figure~\ref{fig:coeff1} we have plotted such a function for
various values of~ and . The function
decreases very fast as the maximum number of neighbors of a node
increases, for all values of  (notice that we have considered
that  at most, namely a packet loss probability of 30\%).
This is rather intuitive, since as the number of neighbors
increases packet losses have less impact on the estimation and
thus better performance are achieved. Notice also that the value
of the function~\eqref{eq:coeff1_unif} for  is
. Thus in presence of non-identical packet
loss probabilities the degradation in performance is not remarked.
In particular even when the first factor
of~\eqref{eq:expected_value_var-slightly-bounded} is very close to
1, if the number of neighbors is greater than 2, with a packet
loss of  we have that the product of the two coefficient
does not exceed 0.65 and it is only a 30\% higher than the case
when no packet losses are present.
\begin{figure}
    \centering
    \psfrag{x}[][]{}
    \psfrag{b}[][]{}
    \psfrag{t}[][]{}
    \psfrag{y}[b][]{}
    \includegraphics[width=0.55\hsize]{figures/coeff.eps}
\caption{First factor of~\eqref{eq:expected_value_var-slightly-bounded} as function of  for increasing values of  ranging from 2 to 100.
    The factor is always less than 1 for all the values of the parameters. The smallest values are achieved when  is large and  is small.
}\label{fig:e_phi_e_v_q}
\end{figure}
\begin{corollary} \label{cor:benchest}
Consider as benchmark the estimator computing the estimates by the
instantaneous average of the available measurements, namely the
estimator for which the weights are chosen to be  and
, for all ,
and . Then,  and the variance is

\end{corollary}
\begin{figure}
    \centering
    \psfrag{q}[][]{}
    \psfrag{Ni}[l][]{}
    \psfrag{t}[][]{}
    \psfrag{y}[b][]{}
    \includegraphics[width=0.50\hsize]{figures/coeff1.eps}
    \caption{Second factor of~\eqref{eq:expected_value_var-slightly-bounded} as function of  for increasing values of 
    ranging from 1 to 20. The factor is always less than 1. The smallest values are achieved when
     is small and  is large. This is explained by the fact that the packet loss probability has a decreasing negative effect
    when the number of neighbors of a node increases, which translates into a smaller value of the coefficient.}\label{fig:coeff1}
\end{figure}


From this corollary we see that the difference in the expected
performance between the proposed estimator, given
by~\eqref{eq:expected_value_var-slightly-bounded}, and the
unbiased estimator that does an arithmetic average, given
by~\eqref{eq:average_performance}, is on the first coefficient
of~\eqref{eq:expected_value_var-slightly-bounded}. Clearly, the
proposed estimator outperforms the latter as the factor in~\eqref{eq:expected_value_var-slightly-bounded} is always
less than one, as shown in Figure~\ref{fig:e_phi_e_v_q}.

However, the bound~\eqref{eq:expected_value_var-slightly-bounded}
has been derived by
Proposition~\ref{eq:expected_value_var-slightly-bounded}, where
the cardinality of the set  is bounded by
. Obviously, this is in general a very conservative bound. The
set  depends on the network topology, and no
tight bound can be derived unless some assumptions are given on
the network topology itself. We will show next that when we assume
information on the network topology, we are able to bound
 more accurately. This further underlines
the improvement of the proposed estimator provides with respect to
the benchmark estimator of Corollary~\ref{cor:benchest}.

\begin{example}
Consider a simple line-graph. Let~ be a node at the extreme of
the line-graph, then we have that .
Let~ be a node of the line-graph different from the extremes,
then we have that . Thus

By assuming that , we see that the
coefficient in~\eqref{eq:expected_value_var-slightly-bounded} is
at most  for the border nodes and  for those in the
middle, regardless the packet losses. Thus we have a significant
improvement with respect to the estimator that takes the average
of the measurements.



\end{example}

\begin{example}
Consider the family of finite Cayley graphs defined on a finite
additive Abelian group , with
, namely the elements of the
group can be regarded as the labels of the nodes. The operator~
is considered as addition modulo~. Let us consider , such that  and  is closed under the inverse, namely
if , then . Two nodes, ~and~, communicate
if and only if . Thus if  and , then we
have a graph in which node~ has as neighboring nodes those with
label ,  and . In
Figure~\ref{fig:example2a} a Cayley graph is shown, where .

\begin{figure}
    \centering
    \subfloat[]{\label{fig:example2a}\includegraphics[width=0.3\hsize]{figures/cayley.eps}}\hspace*{2cm}
    \subfloat[]{\psfrag{x}[][]{}
    \psfrag{b}[][]{}
    \psfrag{y}[b][]{}
    \label{fig:example2b}\includegraphics[width=0.5\hsize]{figures/nu_gamma.eps}}
    \caption{On the left an example of Cayley graph 
    defined on the group  and . On
    the right a plot that shows the first coefficient
    of~\eqref{eq:expected_value_var-slightly-bounded} as function of  and  when the network topology is described by a Cayley graph.
    Furthermore,  and . As it can be seen, the coefficient is always less then one, and,
    compared to Figure~\ref{fig:e_phi_e_v_q}, it depends only on~, namely the degree of a node and not on the network size .}
    \label{fig:example2}
\end{figure}


We have that each node communicates with  nodes. In
other words, two distinct nodes have in common at most~
nodes. This implies that . Thus

The first coefficient
of~\eqref{eq:expected_value_var-slightly-bounded} as function of
 and  is shown in Figure~\ref{fig:example2b}.
Notice that the function is similar in shape to that in
Figure~\ref{fig:e_phi_e_v_q}, however in this case the dependence
is on , and not on the total number of nodes in the network
. Albeit the network might have a total number of nodes many
orders of magnitude larger than , the coefficient stays well
below the one in~\eqref{eq:expected_value_var-slightly-bounded},
which has been obtained when no information about the network is
known.

Since the coefficient
of~\eqref{eq:expected_value_var-slightly-bounded} is always much
less than one, the designed estimator outperforms significantly
the estimator that takes the average of the measurements.
\end{example}






\section{Simulations and Numerical Results} \label{sec:simulations}

Numerical simulations have been carried out to compare the
estimator proposed in this paper with some related estimators
available from the literature.

We consider the following five estimators:
\begin{description}
  \item[:]  where  is instantaneous Laplacian matrix
  associated to the graph . Clearly the graph changes when packets are dropped, so that arcs disappear from the graph.
  \item[:]  and  with
       if node  and
       communicate, and  otherwise. Thus, the
      updated estimate is the  average of the measurements
      (this is the estimator of Corollary~\ref{cor:benchest}).
  \item[:] , where ,
 if node  and  communicate,  otherwise, whereas 
  with , and  elsewhere. This is the average of the old estimates and node's single measurement.
  \item[:]  with
 if node  and 
communicate, and  otherwise. The updated estimate is the
average of the old estimates and all local new measurements.
   \item[:] The estimator proposed in this paper.
\end{description}
The estimators  are based on various heuristics.
They are related to proposals in the literature, e.g., 
uses filter coefficients given by the Laplacian matrix,
cf.,~\cite{XiaoBoydLall}--\cite{Olfati05} and  and  are
considered here as benchmark. Observe that the weights based on
Laplacian do not ensure the minimization of the variance of the
estimation error.

Figure~\ref{fig:dsig} shows a set of test signals
 that has been used to assess the various
estimators. Note that the signals differ only in their frequency
content. The test signals are highly nonlinear and generated so
that the signal presents intervals in which is very slowly varying
(low absolute value of the derivative) and intervals in which the
derivative is higher.
\begin{figure}
    \centering
    \psfrag{d1}[][]{}
    \psfrag{d2}[][]{}
    \psfrag{d3}[][]{}
    \psfrag{d4}[][]{}
    \psfrag{d5}[][]{}
    \includegraphics[width=0.8\hsize]{figures/d_sig.eps}
    \caption{Test signals used in the simulations. Signal  are obtained from  by changing the frequency. The test
    signals are highly nonlinear and are generated so that the signal shows intervals in which the derivative (in absolute value) is
    small and intervals in which the derivative is higher.}
    \label{fig:dsig}
\end{figure}
The choice of the parameter  is based on the
maximum cumulative bias, defined in
Equation~\eqref{eq:error_bound}: Let  denote the desired
power of the cumulative biases of the estimates. Since there are
 nodes, we consider the average power of the bias of each node
as . Assume that we want the estimator to
guarantee that the power of the right-hand side of
Equation~\eqref{eq:error_bound} is equal to . This is
equivalent to 
In the simulations we have chosen dB, which is a rather low value, and the noise variance is , which is quite a large value if compared to the amplitude of the signal to track. We also assume that he value of  is not known precisely but with an upper bound of about \% from its real value. Therefore, these choices allow us to test the proposed estimator in the worst conditions.

We have considered 30 geometric random graphs with  nodes
uniformly distributed in a square of side length equal to .
Two nodes are connected if their Euclidean distance is less than
. The average neighborhood size of all the considered
networks is  nodes with a maximum and minimum neighborhood
size of  and , respectively. The estimation of the test
signals  has been performed under four
different packet loss probabilities. More precisely we have
consider the case in which , ,
 and . We also considered
different measurement noise realizations.
\def\MSE{\text{MSE}}
We take the mean square error of the estimates of each node  as
performance measure. Each estimator has an initial transition
phase, during which the mean square error may not be significant.
Hence, it has been computed after  steps. Afterwards, the mean
square error has been averaged over all nodes of the network. The
average is denoted by . We define the improvement factor of
our estimator compared to the estimators  as 
Figure~\ref{fig:MSE_comparison} shows the  for all
the five different estimators as function of the packet loss
probability. In the simulation we assume the nodes know the
threshold value before the estimation process starts. This is
typically achieved after 10 time steps and thus the network experiences a short
initialization phase. Recall also that in all the simulations
related to the proposed estimator , the error covariance
matrix is estimated at each node and reset as described in the end
of Section~\ref{sec:distrib_min_var_estim}.
\begin{figure}
  \centering
  \psfrag{X}[t][]{Packet loss probability }
  \psfrag{Y}[b][]{MSE}
  \subfloat[]{\psfrag{T}[][]{MSE for estimation of }\includegraphics[width=0.45\hsize]{figures/signal05.eps}}
  \subfloat[]{\psfrag{T}[][]{MSE for estimation of }\includegraphics[width=0.45\hsize]{figures/signal10.eps}}\\
  \subfloat[]{ \psfrag{T}[][]{MSE for estimation of }\includegraphics[width=0.45\hsize]{figures/signal15.eps}}
  \subfloat[]{  \psfrag{T}[][]{MSE for estimation of }\includegraphics[width=0.45\hsize]{figures/signal35.eps}}\\
  \subfloat[]{\psfrag{T}[][]{MSE for estimation of }\includegraphics[width=0.45\hsize]{figures/signalconst.eps}}
  \caption{Mean Square Error (MSE) performance comparison among estimators for various packet loss probabilities . Each plot is associated to one of the five test signals , see Figure~\ref{fig:dsig}. The marker  refers to ,  refers to ,  refers to ,  refers to  and  refers to the proposed estimator . The vertical bars represent the variance of the MSE computed for the 30 simulations. Notice that the proposed estimator  has a very low variance, thus showing its high robustness to packet loss and measurement noise.}
  \label{fig:MSE_comparison}
\end{figure}
Notice that  outperforms all other estimators for any
considered packet loss probability. The two estimators  and
 have performance closer to . In particular 
performs better when the signal is slow, whereas  has better
performance when the signal is faster. This is motivated by the
fact that the estimator  uses only one measurement and it is
affected by a bias that depends on the derivative of signal 
(see Figure~\ref{fig:curves}). The  performance improves as
the packet loss probability increases. Indeed, the single local
measurement is weighted more when packet losses occurs, i.e.,
previous estimates as lost, and thus the overall estimate becomes
less biased. The other two heuristic estimators,  and ,
offer poor performance clearly in all the situations we have
considered.
\begin{figure}
    \centering
    \psfrag{M}[][]{Measurements}
    \psfrag{E1}[][]{}
    \psfrag{E2}[][]{}
    \psfrag{E3}[][]{}
    \psfrag{E4}[][]{}
    \psfrag{Ep}[][]{}
    \includegraphics[width=0.8\hsize]{figures/curves.eps}
    \caption{Realizations of the signal to be tracked, , as it is measured by all the  nodes, with a packet loss probability . Notice that the proposed estimator , visibly outperforms all the other estimators in term of variance. Notice also that the proposed estimator presents a small bias when the signal changes more rapidly.}
    \label{fig:curves}
\end{figure}
Figure~\ref{fig:svdk} shows that the local computation of the weights  with~\eqref{eq:gainsk} yields a stable . In particular, higher values of  are experienced when the signal is slower, because  is small. Viceversa, lower values of  are obtained when  is large. This is explained by considering that when the signal is slow, then it is better to weight more previous estimates (which means larger  and hence larger ) to achieve small variances of the estimation error. By the same argument, when the signal is fast, then it is better to weight less previous estimates.
\begin{figure}
    \centering
    \psfrag{Y}[][]{}
    \psfrag{svdk1}[][]{ for }
    \psfrag{svdk2}[][]{ for }
    \psfrag{svdk3}[][]{ for }
    \psfrag{svdk4}[][]{ for }
    \psfrag{time step}[][]{Time}
    \psfrag{a}[r][]{\small{From  to }}
    \includegraphics[width=0.95\hsize]{figures/svdk.eps}
    \caption{Largest value of the singular value of the matrix  in the 30 Monte Carlo simulation generated. The rows of  are computed individually by each node using~\eqref{eq:gainsk}. The curves correspond to the five test signals , and they are ordered from the slowest signal () to the fastest one ().}
    \label{fig:svdk}
\end{figure}


\section{Conclusions and Future Work}
\label{sec:conclusions} In this paper, we presented a
decentralized cooperative estimation algorithm for tracking a
time-varying signal using a wireless sensor network with lossy
communication. A mathematical framework was proposed to design a
filter, which run locally in each node of the network. Performance
analysis was carried out for the distributed estimation algorithm
in time varying communication networks with packet loss
probabilities both non-identical and identical among the links.
losses. We investigated how the estimation quality depends on
packet loss probability, network size and average number of
neighboring nodes. The theoretical analysis showed that the filter
is stable, and the variance of the estimation error is bounded
even in the presence of large packet loss probabilities. Numerical
results illustrated the validity of our approach, which
outperforms other estimators available from the literature.

Future studies will be devoted to the extension of our design methodology to the case when models of the signal to track are available. Lossy communication links with memory will also be included. Furthermore, we plan to implement our distributed filter on real wireless sensor networks, thus experimentally checking the validity of our theoretical predictions.

\section{Acknowledgments}
We would like to thanks the anonymous reviewers for the very useful comments that allowed us to improve and strengthen the paper.


\bibliographystyle{IEEEtran}
\bibliography{ref}





























\end{document}
