

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{xurl}

\usepackage[accsupp]{axessibility}  




\usepackage{booktabs}

\usepackage{epsfig}
\usepackage{graphics}
\usepackage{wrapfig}
\usepackage{float}

\def \E {\mathcal{E}}
\def \G {\mathcal{G}}
\def \V {\mathcal{V}}
\def \N {\mathcal{N}}
\def \P {\mathcal{P}}
\def \L {\mathcal{L}}
\def \T {\mathcal{T}}

\def \R {\mathbb{R}}

\def \bg {\mathbf{G}}
\def \bt {\mathbf{T}}
\def \A {\mathbf{A}}
\def \U {\mathbf{U}}
\def \I {\mathbf{I}}
\def \J {\mathbf{J}}
\def \F {\mathbf{F}}
\def \X {\mathbf{X}}
\def \Y {\mathbf{Y}}
\def \H {\mathbf{H}}
\def \h {\mathbf{h}}
\def \W {\mathbf{W}}
\def \S {\mathcal{S}}
\def \Q {\mathbf{Q}}
\def \h {\mathbf{h}}
\def \x {\mathbf{x}}
\def \tx {\mathbf{\tilde{x}}}
\def \a {\mathbf{a}}
\def \b {\mathbf{b}}
\def \q {\mathbf{q}}
\def \f {\mathbf{f}}
\def \be {\mathbf{E}}
\def \e {\mathbf{e}}
\def \g {\mathbf{g}}
\def \r {\mathbf{r}}

\newcommand{\CW}[1]{\textbf{\small\sffamily{\textcolor{red}{[#1 -- Chenyan]}}}}


\newcommand{\yandong}[1]{\textbf{\small\sffamily{\textcolor{blue}{[#1 -- Yandong]}}}}

\newcommand{\rgcn}{{R-GCN}\xspace}
\newcommand{\ours}{{\textsf{MUGNN}}\xspace}
\newcommand{\sota}{{state-of-the-art}\xspace}
\newcommand{\Sota}{{State-of-the-art}\xspace}


\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{}  

\title{MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose} 

\begin{comment}
\end{comment}


\titlerunning{MUG}
\author{Chenyan Wu\inst{1} \and
Yandong Li\inst{2} \and
Xianfeng Tang\inst{3} \and
James Z. Wang\inst{1}}
\authorrunning{Wu et al.}
\institute{
~The Pennsylvania State University \quad ~Google \quad ~Amazon \\
\email{\{czw390,jwang\}@psu.edu} \\ \email{lyndon.leeseu@outlook.com} \quad \email{xianft@amazon.com}
}
\maketitle

\begin{abstract}
 Reconstructing multi-human body mesh from a single monocular image is an important but challenging computer vision problem. 
 In addition to the individual body mesh models, we need to estimate relative 3D positions among subjects to generate a coherent representation. 
 In this work, through a single graph neural network, named MUG (Multi-hUman Graph network), we construct coherent multi-human meshes using only multi-human 2D pose as input.
Compared with existing methods, which adopt a detection-style pipeline ({\it i.e.}, extracting image features and then locating human instances and recovering body meshes from that) and suffer from the significant domain gap between lab-collected training datasets and in-the-wild testing datasets, our method benefits from the 2D pose which has a relatively consistent geometric property across datasets.
Our method works like the following:
 \textit{First}, to model the multi-human environment, it processes multi-human 2D poses and builds a novel heterogeneous graph, where nodes across people and within one person are connected to capture inter-human interactions and draw the body geometry ({\it i.e.}, skeleton and mesh structure).
 \textit{Second}, it employs a dual-branch graph neural network structure -- one for predicting inter-human depth relation and the other one for predicting root-joint-relative mesh coordinates.  
 \textit{Finally}, the entire multi-human 3D meshes are constructed by combining the output from both branches. 
 Extensive experiments demonstrate that MUG outperforms previous multi-human mesh estimation methods on standard 3D human benchmarks -- Panoptic, MuPoTS-3D and 3DPW.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Deep learning methods leveraging large-scale datasets have shown tremendous progress on tasks in 3D human mesh recovery, which is often an important initial step in computer vision and robotics applications on human behaviors, such as activity recognition~\cite{ke2013review} and bodily expressed emotion understanding~\cite{wang2020panel}. For single-human mesh~\cite{kanazawa2018end,kolotouros2019convolutional,kolotouros2019learning,choi2020pose2mesh,moon2020i2l} estimation, recent works achieve admirable results.
Naturally, it is desirable to extend the success to multi-human scenarios to provide a holistic understanding of a scene. 

\begin{figure}[t!]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=0.85\columnwidth]{images/compeccv2.pdf}
  \vspace{-8pt}
\caption{Previous multi-human mesh networks commonly adopt a detection-style pipeline using images as input. 
In our pipeline, the proposed Multi-hUman Graph network (MUG) takes the 2D pose as input. The 2D pose can be obtained by 2D human pose estimation networks.}
  \label{fig:comparsion}
 \vspace{-12pt} 
\end{figure}

Multi-human mesh estimation requires generating body mesh models and providing comprehensive 3D positions for each instance. Most recent methods~\cite{jiang2020coherent,zhang2021body,sun2021monocular} adopt a detection-style pipeline with whole images as input. 
As illustrated in Figure~\ref{fig:comparsion}, they first feed the images into a backbone to extract features.
Then one branch locates human instances. The other branch (or branches) will generate body meshes on top of the image feature and instances' locations. 
Particularly,~\cite{jiang2020coherent} leverages the region proposal network (RPN)~\cite{ren2015faster} to propose human body boxes 
and \cite{zhang2021body,sun2021monocular} locate body center points in their locate branches.

However, this pipeline, using images as input, suffers from the domain gap between in-the-wild and lab-collected datasets, which is a widely known problem in 3D-related works~\cite{Zhou_2017_ICCV,wang2019generalizing,choi2020pose2mesh,gong2021poseaug,martinez2017simple,Wu_2020_CVPR}. The primary contradiction is that lab-collected datasets~\cite{ionescu2013human3,sigal2010humaneva} are crucial for providing accurate 3D labels in \textbf{training}; While, these images have very different appearance from the in-the-wild \textbf{testing} images. 
For example, backgrounds and lighting conditions of the images collected in the monotonous laboratory environment are distinct from the images captured from in-the-wild environment. 
In addition, \textbf{\textit{the domain gap can be more prominent in multi-human mesh estimation.}} Single-human mesh networks commonly take cropped-and-centered human images as input~\cite{kolotouros2019learning,kolotouros2019convolutional,moon2020i2l}. 
Human bodies tend to take up most of the image area and they share more similarities across lab-collected and in-the-wild datasets, whereas there are dramatic differences between the surrounding backgrounds of humans in these two kinds of datasets. 
In contrast, the multi-human mesh networks take whole images as input~\cite{zhang2021body,jiang2020coherent,sun2021monocular}, where the backgrounds occupy much more areas. 
That means taking whole images as input may lead to a more significant domain gap, thus preventing multi-human mesh methods from fully utilizing the training data.

To tackle the aforementioned domain gap issue, we hope to build up a new pipeline with the 2D pose as input. 
Compared with the image features, the 2D pose has relatively consistent geometric properties across different datasets. 
No matter how environmental characteristics (e.g., lighting and weather) change, the 2D pose remains the body joint coordinates in the images. 
Also, accurate 2D poses can be obtained through cutting-edge keypoint-detection networks\cite{sun2019deep,cao2017realtime,he2017mask} trained on large-scale in-the-wild datasets.

To reconstruct multi-human meshes from the 2D pose, we need to design a network to accomplish the following two goals (1) \textit{accurately predict body meshes from the 2D pose,} and (2) \textit{estimate comprehensive relative 3D body positions from the 2D pose.} 
Regarding the first goal, each body mesh vertex needs to get the coordinate information from surrounding joints and mesh vertices within the human.
Therefore, we need to take advantage of the intra-human skeleton and mesh topology.
For the second goal, capturing the inter-human dependencies is crucial to obtain the correct relative body positions.
In short, we need to model intra- and inter-human relations simultaneously.

In this work, we propose a simple yet effective heterogeneous graph convolutional network MUG (\textbf{M}ulti-h\textbf{U}man \textbf{G}raph network). First, we construct a novel heterogeneous graph to represent the holistic multi-human environment. 
Body joints and mesh vertices are represented as two different types of nodes. 
Within one human, we connect joint and mesh nodes to form intra-human edges according to the body skeleton structure and mesh topology. 
To represent inter-human relations, we pick and then connect appropriate joint nodes across humans as edges. 
Through different kinds of edges, the coordinate information can be transmitted among nodes within a human or across humans.
Next, we design a dual branch graph convolutional network structure to simultaneously output root-camera depth and root-relative body mesh coordinates. 
After feeding the heterogeneous graph and 2D pose input into the neural network, we can construct multi-human meshes by combining the output of the two branches. 
The entire framework is illustrated in Figure~\ref{fig:framework}.

Extensive experiments show that our method significantly outperforms the previous SOTA~\cite{jiang2020coherent,zhang2021body,sun2021monocular} on multi-person 3D human datasets--Panoptic~\cite{joo2015panoptic}, MuPoTS-3D~\cite{mehta2018single} and 3DPW~\cite{von2018recovering}, where MuPoTS-3D and 3DPW are both in-the-wild datasets. 
Then we conduct ablation studies to help analyze the effectiveness of the graph construction and the influence of different 2D pose input. 
We also provide qualitative results.

Our {\bf contributions} are summarized as follows.
\begin{itemize}
\itemsep0em 
\item We design a new pipeline for multi-human reconstruction. 
With the 2D pose as input, we use one single graph network to output multi-human meshes and 3D locations simultaneously.
The consistent geometric property of the 2D pose makes our method more robust to the domain gap.
\item We propose a novel graph neural network, named MUG, that leverages the heterogeneous graph, including joint nodes, vertex nodes, and different types of edges, to represent the intra- and inter-human relations.
\item Our method significantly outperforms previous approaches on standard multi-person 3D human datasets.
\end{itemize}

\section{Related Work}
\subsection{Single-human mesh estimation}
Single-human mesh estimation aims at reconstructing body mesh in the cropped human image. The body mesh can be represented by the 3D vertex coordinates or the human body models (such as SMPL~\cite{loper2015smpl}). 
According to the method's output is body mesh model parameters or 3D vertex coordinates, human mesh estimation methods can be divided into two types: model-based and model-free. 

Bogo {\it et al.} pioneered a model-based method and demonstrated how to fit SMPL parameters by minimizing the error between predicting and input pose~\cite{bogo2016keep}.
Then the following works mainly adopt an end-to-end neural network to train images and regress SMPL parameters.
HMR~\cite{kanazawa2018end} utilized adversarial loss for its regression network.  
Pavlakos {\it et al.}~\cite{pavlakos2018learning} exploited 2D joint heatmaps and silhouettes as a cue. 
SPIN~\cite{kolotouros2019learning} introduced optimization methods into the regression network.

Despite the success of these model-based approaches, recent works argue that SMPL parameters, which are in the form of 3D rotation, might not be accessible to learn~\cite{kolotouros2019convolutional}. As such, more and more recent researches focus on model-free methods. 
GraphCMR~\cite{kolotouros2019convolutional} used ResNet to extract global features and GNN to regress 3D vertex coordinates. 
Inspired by the heatmap representation in 2D pose estimation, Moon {\it et al.}~\cite{moon2020i2l} developed a new representation, called lixel, to help regress 3D coordinates. 
Lin {\it et al.}~\cite{lin2021end} built up a transformer network.
Choi {\it et al.}~\cite{choi2020pose2mesh} took the 2D pose as input and exploited 3D pose as intermediate features, then used GNN to estimate body mesh coordinates in a coarse-to-fine manner~\cite{yu2019gradual}.
Our work also uses 2D pose as input and adopts GNN. However, the single-human problem does not need to estimate the human depth and analyze the human iteration. \cite{choi2020pose2mesh} only outputs root-relative coordinates. When reprojecting body meshes to the original image, \cite{choi2020pose2mesh} cannot distinguish human body orders.


\subsection{Multi-human mesh estimation}
Multi-human mesh estimation is a relatively new field~\cite{zanfir2018monocular,zanfir2018deep,jiang2020coherent,zhang2021body,sun2021monocular}. Zanfir {\it et al.}~\cite{zanfir2018monocular} published a pioneering work in 2018 in which they implemented a top-down approach. First, a multi-task single-human network was used to estimate 2D pose, 3D mesh, and other scene constraints. Then these outputs were jointly optimized to generate the final prediction.
In another work in 2018, Zanfir {\it et al.}~\cite{zanfir2018deep} proposed a bottom-up method that first performed 3D pose estimation then used an unsupervised auto-encoder to estimate SMPL parameters. 
Recent works adopt detection-style pipelines to solve this task and achieve stunning performance. 
Jiang {\it et al.}~\cite{jiang2020coherent} follows the two-stage detection framework Faster RCNN~\cite{ren2015faster} to use RPN to propose human body boxes. Then it uses SMPL-head to regress SMPL parameters according to proposed boxes. Zhang {\it et al.}~\cite{zhang2021body} and ROMP~\cite{sun2021monocular} adopt an one-stage solution -- using one head to locate the center point of the human, and the other head outputs SMPL parameters according to the center point. As aforementioned, these methods suffer from the domain shift problem.


\subsection{Graph neural networks}
Graph neural networks (GNNs) learn node embeddings that preserve the graph structure and the original node features \cite{kipf2016semi}. 
Recently, GNNs have achieved SOTA performance on various computer vision tasks \cite{li2019actional,choi2020pose2mesh,yang2020distilling,huang2023fsd}. According to the type of graphs, GNNs can be divided into two categories: homogeneous GNNs \cite{kipf2016semi,velivckovic2017graph,hamilton2017inductive} and heterogeneous GNNs \cite{schlichtkrull2018modeling,wang2019heterogeneous}. The homogeneous GNNs assume all nodes and edges in the graph are of the same type, where most existing work focus on modeling such relations via different approaches, such as graph convolution \cite{kipf2016semi}, attention network \cite{velivckovic2017graph}, recurrent neural networks \cite{hamilton2017inductive}, etc.
On the other hand, heterogeneous GNNs are proposed for graphs that contain different types of nodes and edges. The diversity of node types and edge types require GNNs to capture the characteristics of each node and edge type. For example, \cite{schlichtkrull2018modeling} proposes R-GCN where every type of node and edge is modeled by an individual graph convolutional network \cite{kipf2016semi}.

\section{Method}
In this section, we describe our technical approach. The entire framework is illustrated in Figure~\ref{fig:framework}. We describe our proposed heterogeneous multi-human graph in Subsection~\ref{subsection:graph}. Then we demonstrate the node feature construction in Subsection~\ref{subsection:feature}. The neural network structure is illustrated in Subsection~\ref{subsection:network}. In Subsection~\ref{subsection:Depth}, we analyze the root-camera depth computing. Finally, Subsection~\ref{subsection:mesh} introduces how to reconstruct multiple humans with network outputs.

\begin{figure*}[t!]
  \centering
  {\includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/frameworkeccv.pdf}}
  \vspace{-12pt}
  \caption{Illustration of the framework. 2D multi-human pose is the input. To explicitly model the inter- and intra-human relation, we build a heterogeneous graph containing joint nodes (blue circles) and mesh nodes (green circles). On top of the graph, we construct node features using 2D pose. The graph network adopts a dual-branch structure. Joint nodes with joint features go through the upper branch (blue blocks), meanwhile mesh nodes with mesh features go through the lower branch (green blocks). One GCN block (orange block) connects these two branches. Finally, the upper branch outputs root-camera depth and the lower branch outputs root-relative mesh coordinates. By combining the outcomes, we can recover the full multi-human meshes. The upper branch also outputs joint coordinates, which plays a supporting role in learning.
  }
  \label{fig:framework}
\vspace{-12pt} 
\end{figure*}
\subsection{Graph construction}\label{subsection:graph}

\begin{figure*}[t!]
  \centering
  {\includegraphics[trim=0 5 0 0,clip,width=\textwidth]{images/grapheccv2.pdf}}
  \vspace{-20pt}
\caption{Structure of the graph. The graph consists of two kinds of nodes, joint nodes  (blue circles) and mesh nodes . Each human corresponds to one subgraph of the whole graph. Within one human, joint nodes are connected to get edge  (green lines) according to body skeleton structure; following body mesh topology, mesh nodes are connected to get edge  (orange lines); each mesh node connects to its two nearest joint nodes, generating edge  (red lines). To represent inter-human relation, we connect joint nodes across humans to get edge  (purple lines). (b-c) shows the graph structure for image (a). For better display, we do not draw mesh nodes and only draw part of edge .} 
\vspace{-12pt} 
 \label{fig:graph}
\end{figure*}

This subsection introduces how to construct the graph of MUG, Figure~\ref{fig:graph} illustrates the graph structure.
Assuming there are  humans in total, we use  to denote the entire multi-human graph and  to denote the subgraph of each human. Each subgraph contains two types of nodes, one is joint node , the other is mesh node .  Joint nodes represent human body keypoints like hips and shoulders (totally  joint nodes within a human), which have 2D coordinate ground truth in training datasets and can be accurately predicted by 2D pose detection networks, such as~\cite{he2017mask,sun2019deep}. Mesh nodes represent the vertices in the body mesh (totally  mesh nodes within a human), our target is to obtain their absolute 3D coordinates. 

Let us dive into each subgraph. To represent the body geometric topology, we connect joint nodes according to the body skeleton structure, and we link mesh nodes based on the body mesh topology. Furthermore, we add edges between these two kinds of nodes according to their distance pre-defined by the SMPL body mesh template~\cite{loper2015smpl}. Each mesh node connects to its two nearest joint nodes. These edges are directional -- only from joint nodes to mesh nodes. The reason is that the input of MUG only has 2D coordinates of joints. Therefore, coordinates information should be transmitted strictly from joint nodes to mesh nodes.
Formally, we have each subgraph , where node set ; edge set ; and node features . The node features construction is introduced in Subsection~\ref{subsection:feature}.

To model the spatial correlation across humans, we further integrate all human subgraphs into a multi-layer graph . 
Each layer contains a human subgraph , we have . 
We construct edges  across different subgraphs to represent the inter-human relation. There are two types of : 
(1) We set a length threshold , any two joint nodes from different humans will be connected to form an edge if these two joints' 2D distance in the image is less than . 
By selecting appropriate , pairs of joints in small 2D distances will be connected.
The idea is inspired by the first law of geography~\cite{tobler1970computer} that points close in space share similar features.
A small 3D distance always leads to a small 2D distance. 
Joints pairs in small 2D distances are relatively more likely to have interaction (\textit{e.g.} overlap), thus need more accurate prediction.
 We connect these joints pairs as edges, inducing the GNN to produce better predictions on these joints and around mesh nodes.
(2) Any two humans' root-joint nodes must be connected. The root-joint is usually used to determine the depth of a human from the camera. Relative depth information for each human can be passed through root-to-root edges. More analysis of  can be found in the ablation study (Subsection~\ref{subsection:ablation}).

In summary, we have intra-human edges in each human subgraph (including , , ) and  among the subgraphs to connect individuals. 
The set of all these edges can be described as  .

\subsection{The node feature construction}\label{subsection:feature}


We define the node features as . 
The  human's feature  includes  for joint nodes and  for mesh nodes, where  is the feature dimension. 

For joint nodes, we denote the feature for the  joint of the  human as . The main information that can be used to construct features is the 2D pose input . Following previous work~\cite{wandt2019repnet,choi2020pose2mesh}, we first apply standard normalization to . Specifically, we subtract the mean of  and divide by the standard deviation in order to obtain . Then  is assigned to the  joint. 
The normalization~\cite{wandt2019repnet} allows for easier convergence of the GCN, but it turns each human's 2D pose into relative coordinates regardless of this human's body size and position.
To analyze relative 3D human positions, we need to compare the absolute joint coordinates across humans in the images.
Thus the joint coordinates over the entire image range are necessary to pass to joint nodes.
In the meantime, to facilitate training, we need to convert different images into a uniform size.
We resize the raw image's long edge as  pixels and then center it in the  pixels square.
 will be accordingly transformed into . Then  is assigned to the  joint. To fully utilize , we also pass whole  to .

In addition, we leverage 11 SMPL body mesh models, containing the SMPL template model as well as 10 randomly selected from Human3.6M dataset. Mesh vertices coordinates can be computed by these models. For simplicity, we just denote mesh coordinates from one model as . We can compute the 3D coordinates of joints by multiplying  with the joint regression matrix , where  is defined in SMPL body mesh model~\cite{loper2015smpl}. Then we pass  to . Although these body mesh coordinates are fixed, the graph neural network can learn some rigid geometry information by remembering these examples. Finally, the joint feature can be obtained by concatenating above features,


For mesh nodes, we denote the feature for the  mesh vertex in  human as . Different from joint nodes, we can not know the  position information of each mesh vertex by the 2D pose input. We directly use  and  to initialize the mesh features, where  represents the nearest joint with the  mesh node in the SMPL body mesh template. Following a similar process with joint nodes, we also feed  and  (the  vertex in ) to . Concatenate these features we have,


\subsection{Network structure}\label{subsection:network}

Then we introduce the model structure, which is illustrated in Figure~\ref{fig:framework}. Considering the two kinds of nodes, we design a dual-branch graph network. Joint nodes with joint features go through one branch, meanwhile mesh nodes with mesh features go through the other. The two branches are connected by a GCN block, namely the connection block. 
For the branch of joint nodes, they first go through two GCN blocks. Then the output has three destinations. One is passed into the mesh branch via the connection block. Another will step into one GCN block and finally reduce its feature dimension into 1. We pick the value of the root-joint node as the depth measure for its human instance. The last one will reduce the dimension into 3 to output 3D joint coordinates through one GCN block. Although these 3D joint coordinates are not used in 3D mesh reconstruction, they can guide the branch to learn depth representation. The other branch consists of four GCN blocks. After passing one GCN block, the mesh nodes will add their features with the output of the connection block. Then, going through three more GCN blocks and reducing their feature dimension into 3, mesh nodes finally output root-related body mesh coordinates. The GCN blocks are the GCN residual block which is inspired by ResNet Basic blocks~\cite{he2016deep}, or just the cascade of three graph convolutional layers. The amount of parameters of the entire network is not large, only 2.3M. The detailed structure and configuration are discussed in the supplementary. 

\subsection{Estimating depth}\label{subsection:Depth}
For this monocular setting, it is not meaningful to predict an accurate root-camera depth. Here we aim at providing a comprehensive depth estimation to reflect relative 3D position across humans. Skeletons of different humans usually have similar lengths. Through the pine-hole camera model~\cite{fusiello2006elements}, the ratio between the root-camera depth and focal length largely decides the size of the skeletons in the image. Conversely, we attempt to use 2D human pose to infer that ratio. For this, we construct the depth measure as ,
where  is the depth and  is the camera focal length. 
As mentioned in Subsection~\ref{subsection:feature} 
we resize the raw image to make its long edge width as 1000 pixels,
this is equivalent to adjusting the focal length while holding the depth. For the training convenience, we also add a constant coefficient  to make the the measure in the range of [0,1]. Finally, we get the depth measure,

where  is the long edge width of the raw image and we set  as 200 in practice.
In the training process, we utilize the  loss to supervise  and adopt a relative depth loss to get more accurate human relative depth,

where  represents the ground truth and  represent different human instances. 
In training, for the dataset containing root-camera depth and focal length, we can directly compute ground truth . 
If the dataset doesn't provide these labels, we assume  as .  
Then we use SMPLify~\cite{pavlakos2019expressive} to generate pseudo root-camera depth, such that pseudo  can be obtained.
The assumption of  is sufficient to give the comprehensive depth. Because  is the same for all humans within one image, relative 3D body positions are consistent regardless of the change of .
In inference, we can recover the  with ,  and predicted .

\subsection{Reconstruct multiple humans}\label{subsection:mesh}
Besides the depth, the graph network also outputs root-relative mesh vertex coordinates  and 3D joint coordinates  for each human, where  is what our need to recover the body mesh and  play a supporting role in training. Also, we can get the 3D joint coordinates from  by multiply the joint regression matrix .
In the training process, we use the  loss function to supervise ,  and ,

where  represents the ground truth. If the dataset doesn't provide 3D labels for joints or mesh coordinates, we use SMPLify~\cite{pavlakos2019expressive} to generate pseudo 3D coordinates ground truth.

Following \cite{wang2018pixel2mesh}, we also use the mesh normal vector loss  and mesh edge  vector loss  to make the mesh result more smooth.
These losses and above losses for depth are then summed together to get the total loss,


By optimizing these losses, we obtain accurate estimated  and  for each human. Because we can directly get the root-joint 2D image coordinates  from the 2D pose input, the 3D coordinate for the root-joint  can be computed by the camera intrinsic equation,

where  is the camera intrinsic matrix. If we do not have the camera parameters, we construct the intrinsic matrix by setting focal length as  and the center point of the image as the principal point. Finally, we can construct multiple humans by calculating the absolute body mesh coordinates for them. 


\section{Experiments}
We conducted extensive experiments to validate the effectiveness of MUG. We answer the following questions:(1) Can MUG outperform existing multi-human 3D mesh reconstruction approaches on various benchmark datasets?
(2) How do the inter-human edges contribute to MUG?
(3) How does the 2D pose input affect MUG?
We first describe the datasets used in our experiments in Subsection~\ref{subsection:dataset}. Then we introduce our implementation details in Subsection~\ref{subsection:details}. Next, we introduce comparisons with SOTA methods in Subsection~\ref{subsection:SOTA}. Finally, we conduct the ablation study in Subsection~\ref{subsection:ablation}




\subsection{Datasets}\label{subsection:dataset}
\noindent\textbf{Human3.6M}~\cite{ionescu2013human3} is a large-scale single-human 3D pose dataset, where the ground truth 3D pose is obtained through motion capture. Because this dataset has high-quality 3D annotations, we use it for both training and testing. Human3.6M does not provide body mesh ground truth so we adopt the pseudo-ground-truth provided by \cite{moon2020i2l}, where SMPLify~\cite{pavlakos2019expressive} is used to generate pseudo-ground-truth. We follow the evaluation method from \cite{kanazawa2018end}.

\noindent\textbf{MuPoTS-3D} is an outdoor multi-human 3D pose dataset proposed by \cite{mehta2018single}, where ground truth 3D pose is obtained by a multi-view marker-less motixon capture system. We use this dataset to evaluate. We follow the standard evaluation method from \cite{jiang2020coherent}.

\noindent\textbf{MuCo-3DHP} is a multi-human 3D pose dataset proposed by \cite{mehta2018single}. It is composited from the existing MPI-INF-3DHP 3D single-human pose estimation dataset~\cite{mehta2017monocular}. We only use this dataset for the training. We adopt the pseudo-ground-truth provided by \cite{choi2020pose2mesh}.

\noindent\textbf{Panoptic}~\cite{joo2015panoptic} is a multi-human dataset. It provides video clips collected from a panoptic environment. It has ground-truth 3D pose annotation in the multi-camera view angle. We follow the evaluation procedure of~\cite{zanfir2018monocular}. Haggling, Mafia, Ultimatum, and Pizza are four activities used to test, and we just use No.16 and No.30 HD cameras.

\noindent\textbf{3DPW}~\cite{von2018recovering} is a multi-human in-the-wild dataset with full-body mesh annotations. We use its test set for evaluation, following the same protocol as \cite{kocabas2020vibe}.

\noindent\textbf{COCO}~\cite{lin2014microsoft} is a large-scale in-the-wild multi-human dataset, providing 2D pose growth truth. Although we don’t have 3D pose ground truth, to make the training more diverse, we use COCO to train. We use pseudo human-mesh ground truth provided by \cite{moon2020i2l}, where SMPLify~\cite{pavlakos2019expressive} is used to generate pseudo-ground-truth. 
\subsection{Implementation details}\label{subsection:details}
Our implementation is built with PyTorch~\cite{pytorch}. The GCN network uses the DGL library~\cite{dgl}. 
In MUG we adopt a coarse body mesh model (431 vertices). 
According to~\cite{ranjan2018generating}, by multiplying the up-sample matrix provided by~\cite{ranjan2018generating}, the coarse mesh can be recovered into the full body mesh SMPL model (6890 vertices). 
In the training process, we use flip as the data augmentation strategy. 
Also, we add synthetic error to the 2D pose ground truth input in training following~\cite{moon2019posefix,choi2020pose2mesh}, which effectively improves the network robustness to wrong poses.
For our implementation, we set the batch size to 1 and  Adam as the optimizer. We train for 15 epochs in total. The initial learning rate is 0.001, which will be reduced by a factor of 10 after the 10th epoch. In the experiment of evaluating Human3.6, we only use Human3.6 in training. But for other experiments, we jointly train on MuCo-3DHP, COCO, and Human3.6M together. We set the coefficients in equation~\ref{equ:lamb} as . In inference, we conveniently adopt 2D pose prediction result of Human3.6M provided by \cite{choi2020pose2mesh}. For other testing datasets (MuPoTS-3D, Panoptic and 3DPW), we first use Mask-RCNN~\cite{he2017mask} to detect multi-humans and then use HRNet~\cite{sun2019deep} (enhanced by Darkpose~\cite{zhang2020distribution}) to estimate 2D pose.

\begin{table}[t]
\begin{center}
\caption{Performance comparison for \textbf{Human3.6M}. The evaluation metrics are MPJPE and PA MPJPE (mean per joint position error without and with Procrustes alignment). The unit is mm.  means using ground truth 2D pose as input.}\label{tab:result_on_hm36}
\vspace{-6pt} 
\begin{tabular}{c|c|c|c}
\specialrule{.8pt}{0.8pt}{0.8pt}
Method & Multi-human & MPJPE & PA MPJPE \\
\hline
GraphCMR~\cite{kolotouros2019convolutional} & - & 88.0& 56.8\\
Pose2Mesh~\cite{choi2020pose2mesh} & - & 64.9 & 46.3 \\
\hline
Jiang~\cite{jiang2020coherent} & \checkmark & - & 52.7 \\
BMP~\cite{zhang2021body} &  \checkmark & - & 51.3 \\
MUG & \checkmark &\textbf{61.9} & \textbf{48.5} \\
\hline
MUG & \checkmark & 50.3 & 38.5 \\
\specialrule{.8pt}{0.8pt}{0.8pt}
\end{tabular}
\end{center}
\vspace{-18pt} 
\end{table}

\subsection{Comparison with the state of the art}\label{subsection:SOTA}
First, we conduct experiments on Human3.6M. Although Human3.6M is a single-human dataset and our goal is to solve the multi-human problem, its experiments can still reflect our method's ability to recover body meshes. 
The result is reported in Table~\ref{tab:result_on_hm36}. Compared with previous multi-human mesh methods, the MPJPE drops by  (51.3mm vs 48.5mm). Part of this improvement comes from (1) the superb generalization ability of 2D pose (2) our carefully designed graph architecture which can model the intra-human relations. Concretely, we also compare our method with two single-human mesh methods GraphCMR~\cite{kolotouros2019convolutional} and Pose2Mesh~\cite{choi2020pose2mesh}, because they also apply graph neural network and use mesh coordinates as output. MUG achieves much better performance over GraphCMR and comparable performance with Pose2Mesh (better in MPJPE and worse in PA MPJPE). It is worth noting that our model parameters are significantly less than Pose2Mesh (2.3M vs 8.8M), we can still get a competitive result. The last row gives the performance with the ground truth 2D pose as input, which is the upper bounds of our method on Human3.6M.

Then we evaluate our method on different multi-human benchmarks to get comparisons with the SOTA works, containing~\cite{jiang2020coherent,zhang2021body,sun2021monocular}. It is worth noting that we only use Human3.6, MuCo-3DHP, and COCO in training for all the multi-human evaluation experiments. \cite{jiang2020coherent,zhang2021body} adopt more 2D pose datasets to enhance the performance (e.g., MPII~\cite{andriluka20142d}). ROMP~\cite{sun2021monocular} has two settings–-its primary setting utilizes seven datasets (Human36M, COCO, MPII, etc.) in training, and its advanced setting utilizes 11 datasets in training. Therefore, we compare with its primary setting for a fair comparison.
Finally, we achieve significantly better performance against these baselines on different benchmarks, demonstrating the superb generalization ability of the 2D pose. Below is the detailed comparison.

We examine performance on the Panoptic dataset, a standard benchmark for multi-human 3D human analysis.
Table~\ref{tab:result_on_pantic} shows the comparison results with \cite{zanfir2018monocular,zanfir2018deep,jiang2020coherent,zhang2021body,sun2021monocular}.
Our method has better performance in three activities. 
Compared with ROMP~\cite{sun2021monocular}, the mean MPJPE of all the activities is decreased by  (from 134.6mm to 127.8mm). 
This improvement comes from three places: 
(1) the good generalization ability of 2D pose 
(2) our graph neural network's ability to model inter-and intra- human relations
(3) well-performing human pose network HRNet~\cite{sun2019deep} is used to predict 2D pose. 

\begin{table}[t]
\begin{center}
\caption{Performance comparison for \textbf{Panoptic}. The evaluation metric is MPJPE.}\label{tab:result_on_pantic}
\vspace{-6pt} 
\begin{tabular}{c|c|c|c|c|c}
\specialrule{.8pt}{0.8pt}{0.8pt}
Method & Haggling & Mafia & Ultim. & Pizza & Mean \\
\hline
Zanfir~\cite{zanfir2018monocular} & 140.0 & 165.9 & 150.7 & 156.0 & 153.4\\
Zanfir~\cite{zanfir2018deep} & 141.1 & 152.3 & 145.0 & 162.5 & 150.3\\
Jiang~\cite{jiang2020coherent} & 129.6 & 133.5 & 153.0 & 156.7& 143.2\\
BMP~\cite{zhang2021body} & 120.4 &  132.7 &  140.9 & 147.5 & 135.4 \\
ROMP~\cite{sun2021monocular} & \textbf{111.8} &  129.0 &  148.5 & 149.1 & 134.6 \\
MUG  & {113.7}  & \textbf{120.8}  & \textbf{137.5} &\textbf{139.2} & \textbf{127.8} \\
\specialrule{.8pt}{0.8pt}{0.8pt}
\end{tabular}
\end{center}
\vspace{-8pt}
\end{table}

\begin{table}[!t]
  \begin{minipage}{.43\linewidth}
    \caption{3DPCK comparison on \textbf{MuPoTS-3D}. The column All and Matched represents the accuracy for all predictions and the predictions matched with annotations.} \label{table:mupo_all}
    \centering
      \begin{tabular}{c|c|c}
\specialrule{.8pt}{0.8pt}{0.8pt}
Method & All & Matched \\
\hline
          SMPLify-X~\cite{pavlakos2019expressive} & 62.84 & 68.04\\
          HMR~\cite{kanazawa2018end}  & 66.09 & 70.90\\
          Jiang~\cite{jiang2020coherent} & 69.12 & 72.22  \\
          BMP~\cite{zhang2021body} & 73.83 & 75.34  \\
          ROMP~\cite{sun2021monocular} & 69.90& 74.60  \\
          MUG & \textbf{76.27} & \textbf{77.53} \\
          \specialrule{.8pt}{0.8pt}{0.8pt}
      \end{tabular}
  \end{minipage}\hspace{5pt}
  \begin{minipage}{.5\linewidth}
    \centering
      \caption{Performance comparison for \textbf{3DPW}. The evaluation metric is MPJPE, PA MPJPE and MPVPE. The units are mm. MPVPE represents mean per vertex position error.  means using ground truth 2D pose as input.} \label{table:3DPW}
      \begin{tabular}{c|c|c|c}
        \specialrule{.8pt}{0.8pt}{0.8pt}
Method & MPJPE & PA MPJPE & MPVPE\\
\hline
        Jiang~\cite{jiang2020coherent} & 105.3 & 62.3 & 122.2\\
        BMP~\cite{zhang2021body} & 104.1 & 63.8 & 119.3\\
        ROMP~\cite{sun2021monocular} & 91.3 & \textbf{54.9} & 108.3\\
MUG & \textbf{87.0} & 60.5 & \textbf{106.2}\\
        \hline
        MUG & 60.9 & 37.4 & 70.1\\
        \specialrule{.8pt}{0.8pt}{0.8pt}
      \end{tabular}
  \end{minipage} 
  \vspace{-8pt} 
\end{table}

Also, we compare our method with SOTA 3D multi-human mesh methods on another popular 3D human benchmark MuPoTS-3D dataset in Table~\ref{table:mupo_all}. 
The evaluation metric is percentage of correct 3D keypoints (3DPCK).
Following the evaluation method described by \cite{mehta2018single}, MUG outperforms other SOTA methods by 2.44 PCK for all cases and 2.19 PCK for matched cases. 
Results of two baselines - the combination of OpenPose~\cite{cao2017realtime} and single-human mesh method SMPLify-X~\cite{pavlakos2019expressive} and HMR~\cite{kanazawa2018end} are also include in Table~\ref{table:mupo_all}.

Last, we conduct experiments on the challenging in-the-wild 3DPW test set. 
It has ground truth mesh coordinates, the error of the mesh vertices (MPVPE) can be reported. 
We show the result in Table~\ref{table:3DPW}. 
Our method significantly outperforms previous multi-human mesh methods on MPJPE and MPVPE, demonstrating that our method can achieve good results on in-the-wild datasets. 
We also report the result when we use 2D pose ground truth as input to show the upper bound of our method. 

\subsection{Ablation study}\label{subsection:ablation}
\begin{figure*}[t!]
  \centering
  {\includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/edge.pdf}
  \vspace{-12pt} 
  \caption{Illustration of inter-human edges with  pixels from left to right, where  means no root-to-root edges. Green points represent body joints. Orange and green lines represent root-to-root edges and -controlled edges separately.} 
  }
  \label{fig:edge}
  \end{figure*}

  
  \begin{table}[!t]
    \begin{minipage}{.5\linewidth}
      \caption{Ablation for inter-human edges on the 3DPW (MPVPE) and MuPoTS ().  means no root-to-root edges.}\label{tab:ablation_edge}
      \centering
        \begin{tabular}{c|ccccc}
\specialrule{.8pt}{0.8pt}{0.8pt}
 & 0 & 0 & 100 & 200 & 300 \\
\hline
            MPVPE & 113.4 & 109.2 & 107.0 & \textbf{106.2}& 115.1 \\
            \hline
             & 78.6 & 89.6 & {93.2} & \textbf{94.7} & 90.9    \\
            \specialrule{.8pt}{0.8pt}{0.8pt}
        \end{tabular}
    \end{minipage}\hspace{5pt}
    \begin{minipage}{.45\linewidth}
      \centering
        \caption{Performance of 3DPW with different 2D pose input. P1-3 represent HRNet-32, HRNet-48, and HRNet-48 enhanced by DarkPose separately.} \label{table:ablation_pose}
        \begin{tabular}{c|cccc}
          \specialrule{.8pt}{0.8pt}{0.8pt}
2D Pose & P1 & P2 & P3 & GT\\
\hline
          MPVPE & 110.3 & 108.8 & 106.2 & 70.1\\
          \specialrule{.8pt}{0.8pt}{0.8pt}
        \end{tabular}
    \end{minipage} 
  \vspace{-8pt} 
  \end{table}
\textbf{Inter-human edges:} 
Our method aims at multi-human cases. 
The core component of our proposed graph structure is the inter-human edges , connecting different human subgraphs. 
We study how inter-human edges affect the performance. 
As mentioned in Subsection~\ref{subsection:graph}, root-joints are connected to form root-to-root edges. 
Also, joint pairs are connected if the pair's 2D distance in the image is less than , where the image is preprocessed (resizing the raw image's long edge as 1000 pixels).
A larger  leads to more dense inter-human edges. 
In this ablation study, we set  pixels separately. 
With ,  only has root-to-root edges. 
When we remove the root-to-root edges, the whole graph is just the combination of separate subgraphs, which we denote as .
Figure~\ref{fig:edge} gives an example to show the inter-human edges with different .
Table~\ref{tab:ablation_edge} reports the experiment result. 
Row two shows the experiments on 3DPW test set with MPVPE as the metric.
 gets 113.4mm MPVPE. 
With more edges, the performance increases. 
However, when  is over 200, connections become too dense, which will hurt the performance.
Moreover, we follow~\cite{moon2019camera} to conduct experiments on the MuPoTS-3D dataset to evaluate the ordinal depth relations of all human instance pairs (row three). 
The number is the percentage of correct depth relations (), reflecting the performance of the relative 3D position. 
If we do not use inter-human edges, the performance is limited, just , because the information cannot transmit across humans to analyze the relative 3D positions.
When we add the root-to-root edges, the accuracy jumps from  to , demonstrating the effectiveness of root-to-root edges in model inter-human relations. 
With larger , the performance can be better. 
 produces the best result.

\noindent\textbf{2D pose input:}
Although we add synthetic error~\cite{moon2019posefix,choi2020pose2mesh} to the 2D pose input in training, making MUG more robust to wrong 2D poses. 
Input pose with different accuracy can still cause different performances in inference. 
This ablation experiment explores how various pose inputs affect MUG. 
We evaluate on 3DPW test set with MPVPE as the metric.
We employ three variants of HRNet to produce pose input -- HRNet-32, HRNet-48, and HRNet-48 enhanced by DarkPose. 
The three variants get better 2D pose predictions one by one.
Table~\ref{table:ablation_pose} shows that we can get better human mesh reconstruction results with better pose estimation results as input.  
Then we compare them with the ground truth input.
We observe a significant performance gap boost -- the ground truth pose input results in the  MPJPE drop (from 106.2mm to 70.1mm). 
The performance of MUG still has much room to increase if we can get a more accurate 2D pose input in testing.

  \begin{figure}[t!]
    \centering
    {\includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/example3.pdf}

    \vspace{-12pt} 
  \caption{{Qualitative results}. We show the multi-human mesh reconstruction of MUG on MuPoTS and 3DPW dataset. The second to the forth column are the front, top, side view respectively.} }
    \label{fig:examples}
  \vspace{-12pt} 
  \end{figure}

\section{Conclusions and Future Work}
We studied multi-human mesh reconstruction using 2D pose as input. 
We designed a multi-layer heterogeneous graph to represent the intra-human and inter-human relations. 
We further proposed a dual-branch graph neural network that leverages the heterogeneous graph to learn human relations and accurately recovers the 3D mesh coordinates. 
In the future, we will extend our pipeline for other 3D reconstruction problems, such as animal mesh reconstruction.
We hope that this work might spark further use of graph neural networks across various fields within artificial intelligence, such as neural network compression~\cite{Xu_2019_CVPR}.

\section*{Acknowledgments}

This research was supported by generous gifts from the Amazon Research Awards program. The project also used computational resources from the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant No. ACI-1548562. The authors appreciate the support and encouragement from Yelin Kim and Adam Fineberg.


\bibliographystyle{splncs04}
\bibliography{egbib}

\clearpage
\appendix
\setcounter{figure}{0} 
\setcounter{table}{0} 
\setcounter{equation}{0} 
\begin{figure*}[h!]
  \centering
  {\includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/supp_structure.pdf}}
\caption{Detailed network structure. The number inside each graph convolution layer represents the feature dimension of the layer output.} 
\label{fig:detail}
\end{figure*}



{\large\bf Supplementary Material
}
\section{The node feature dimension}
MUG applies the joint set from Human3.6M~\cite{ionescu2013human3} for the experiment on Human3.6M, where . The feature dimension of each joint node is 

where  correspond to  in the main text respectively. Following the similar calculation, we can also get the feature dimension of each mesh node is . Considering  humans in total and , the shape of the joint features is  and the shape of the mesh features is . 

MUG applies COCO~\cite{lin2014microsoft} joint set for the experiments on Panoptic~\cite{joo2015panoptic}, MuPoTS-3D~\cite{mehta2017monocular} and 3DPW~\cite{von2018recovering}. Following~\cite{choi2020pose2mesh}, we add root-joint and neck-joint by computing the middle point of left and right hips, and left and right shoulders respectively. We have  for COCO joint set. The feature dimension of each joint or mesh node is 

Considering  humans in total and , the shape of the joint features is  and the shape of the mesh features is .

\section{Detailed network structure}
The detailed network structure is illustrated in Figure~\ref{fig:detail}. The number inside each graph convolution layer represents the feature dimension of the layer output. We adopt the GCN residual block, which is inspired by \cite{he2016deep}. Our residual block is similar to the basic residual block of \cite{he2016deep}. However, we use Chebyshev spectral graph convolution layers~\cite{defferrard2016convolutional} as the convolutional layers and group normalization as the normalization layers.

\section{Loss functions}
Following \cite{wang2018pixel2mesh}, the equations for the mesh normal vector loss  and mesh edge  vector loss  are,


where  and  denote a triangle face in the human mesh and a ground truth unit normal vector of , respectively.  and  denote the  and  ground truth vertices in , respectively.  represents the ground truth.

\section{More qualitative results on MuPoTS}

More examples on the MuPoTS dataset are displayed in Figure~\ref{fig:mupo}.

\begin{figure}[t!]
  \centering
  {\includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/supp_example.pdf}}
 \vspace{9pt}
\caption{More qualitative results on MuPoTS. The second to the forth column are the front, top, side view respectively.} 
\label{fig:mupo}
\end{figure}

\section{More qualitative results on 3DPW}

More examples on the 3DPW dataset are displayed in Figure~\ref{fig:3dpw}.

\begin{figure}[t!]
  \centering
  {\includegraphics[trim=0 0 0 0,clip,width=\textwidth]{images/supp_example2.pdf}}
  \vspace{9pt}
\caption{More qualitative results on 3DPW. The second to the forth column are the front, top, side view respectively.} 
\label{fig:3dpw}
\end{figure}
\end{document}
