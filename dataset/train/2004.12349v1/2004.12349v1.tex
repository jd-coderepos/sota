








\documentclass[10pt,journal,compsoc]{IEEEtran}
















\ifCLASSOPTIONcompsoc	
\usepackage[nocompress]{cite}	
\else	
\usepackage{cite}	
\fi







\ifCLASSINFOpdf
\else
\fi














































\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{xcolor}
\usepackage{url}
\newcommand{\bftab}{\fontseries{b}\selectfont}
\definecolor{darkgreen}{RGB}{23,168,23}
\usepackage{dblfloatfix}





\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{When CNNs Meet Random RNNs: \\Towards Multi-Level Analysis for \\RGB-D Object and Scene Recognition}


\author{Ali~Caglayan, Nevrez~Imamoglu, Ahmet Burak~Can, and~Ryosuke~Nakamura\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem A. Caglayan, N. Imamoglu, and R. Nakamura are with the Artificial Intelligence Research Center (AIRC),
National Institute of Advanced Industrial Science and Technologhy (AIST), Tokyo, Japan.\protect\\
E-mail: (\{ali.caglayan, nevrez.imamoglu, r.nakamura\}@aist.go.jp)\IEEEcompsocthanksitem A. B. Can is with the Department of Computer Engineering, Hacettepe University, Ankara, Turkey.\protect\\
E-mail: (abc@cs.hacettepe.edu.tr).}

}




















\IEEEtitleabstractindextext{

\begin{abstract}
Recognizing objects and scenes are two challenging but essential tasks in image understanding. In particular, the use of RGB-D sensors in handling these tasks has emerged as an important area of focus for better visual understanding. Meanwhile, deep neural networks, specifically convolutional neural networks (CNNs), have become widespread and have been applied to many visual tasks by replacing hand-crafted features with effective deep features. However, it is an open problem how to exploit deep features from a multi-layer CNN model effectively. In this paper, we propose a novel two-stage framework that extracts discriminative feature representations from multi-modal RGB-D images for object and scene recognition tasks. In the first stage, a pretrained CNN model has been employed as a backbone to extract visual features at multiple levels. The second stage maps these features into high level representations with a fully randomized structure of recursive neural networks (RNNs) efficiently. In order to cope with the high dimensionality of CNN activations, a random weighted pooling scheme has been proposed by extending the idea of randomness in RNNs. Multi-modal fusion has been performed through a soft voting approach by computing weights based on individual recognition confidences (i.e. SVM scores) of RGB and depth streams separately. This produces consistent class label estimation in final RGB-D classification performance. Extensive experiments verify that fully randomized structure in RNN stage encodes CNN activations to discriminative solid features successfully. Comparative experimental results on the popular Washington RGB-D Object and SUN RGB-D Scene datasets show that the proposed approach significantly outperforms state-of-the-art methods both in object and scene recognition tasks.
\end{abstract}

\begin{IEEEkeywords}
Convolutional neural networks, recursive neural networks, randomized neural networks, transfer learning, RGB-D object recognition, RGB-D scene recognition.
\end{IEEEkeywords}

}


\maketitle


\IEEEdisplaynontitleabstractindextext




\IEEEpeerreviewmaketitle

 
\section{Introduction}
\label{sec:intro}
\IEEEPARstart{C}{onvolutional} neural networks (CNNs) have attracted researchers to handle many visual recognition tasks since their breakthrough emergence. However, building an effective model can be quite challenging due to the lack of labeled training data, limited time and computational resources, and the need for well defined hyperparameter settings for a good generalization capability. Especially in many real-world tasks, it is not preferable to train a model from scratch. Luckily, CNNs offer highly efficient solutions with their transferable off-the-shelf features. Consequently, many approaches take advantage of these features to propose new solutions for object recognition (e.g.\cite{Razavian_CVPRW_2014, Schwarz_ICRA_2015}), scene recognition (e.g. \cite{Liao_ICRA_2016, Song_TIP_2019}), object detection (e.g. \cite{Girshick_CVPR_2014, Sermanet_ICLR_2014}), and semantic segmentation (e.g. \cite{Girshick_CVPR_2014, Farabet_TPAMI_2013}) due to their high representation ability and capability of generalization among different tasks when trained with large scale datasets. The most common and straightforward strategy among these methods is to utilize the features obtained from final layers which provide semantically rich information with smaller dimensions comparing to the earlier layers \cite{Razavian_CVPRW_2014, Schwarz_ICRA_2015, Girshick_CVPR_2014, Sermanet_ICLR_2014, Farabet_TPAMI_2013}. However, one of the concerns about this semantics is the fact that as features evolve towards the final layers, they are increasingly dependent on the chosen dataset and task \cite{Yosinski_NIPS_2014}, which might diminish the generalization capabilities of these features when transferred. Moreover, this strategy ignores the locally activated distinctive information of the earlier layers which is less sensitive to semantics \cite{Hariharan_CVPR_2015, Zaki_ICRA_2016}. One of the main challenges in earlier layers of deep CNNs is the high dimensionality of extracted features. In addition, when these features are used as is, it makes the feature space untraceable. Eventually, while features are transformed from low-level general to high-level specific representations throughout the network, the relational information is distributed across the network at different levels \cite{Yosinski_NIPS_2014, Hariharan_CVPR_2015}. However, it remains unclear how to exploit the information effectively.
\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.85\textwidth]{overview.pdf}
	\caption{General overview of the proposed framework. The framework accepts RGB and depth images and it first colorizes depth inputs. In the CNN-Stage activations at different levels of a pretrained model are extracted. In the RNN-Stage, first, CNN activations are converted to reasonable dimensions and appropriate input requirements for RNNs by preprocessing operations. Then, multiple random RNNs are applied to map these inputs into high level representations. Finally, multiple level fusion and classification steps are deployed for recognition tasks.}
	\label{fig:FrameworkStructure}
\end{figure*}

In this paper, we aim to present an effective deep feature extraction framework to derive powerful image representations through transfer learning. The proposed pipeline relies on two key insights. The first one is to employ a pre-trained CNN as the backbone model and exploit activations at different layers of the network to cover the predominant information of the underlying localities. The second one is to implement multiple random recursive neural networks (RNNs) on top of CNNs to encode the CNN activations into a robust representation with reduced dimensionality and sufficient descriptiveness. 

In developing our framework, we particularly deal with the RGB-D object and scene recognition problems, which are challenging yet crucial tasks especially with the todayâ€™s wider application of robotics technologies. Moreover, the multi-modality of the RGB-D sensors arises additional difficulties in representation of input data such as handling different modalities and devising solutions that captures complementary information from both RGB and depth data effectively. Besides these challenges, alleviating limitations on time and memory consumption is another challenge to deal with. To address these challenges, we propose a novel framework that gathers feature representations at different levels in a compact and representative feature vector for both of RGB and depth data. After obtaining CNN activations, we first apply a preprocessing operation to the activation maps of each level through reshaping or randomized pooling. This not only provides a generic structure for each level by fixing an RNN tree but also it allows us to improve recognition accuracy through multi-level fusion. We then give the outputs of these operations to multiple random RNNs \cite{Socher_NIPS_2012} to acquire higher level compact feature representations. Incorporating multiple fixed RNNs together with the pre-trained CNN models allows feature transition at different levels to preserve both semantic and spatial structure of objects. In order to transfer learning from a pre-trained CNN model for depth modality, we embed depth data into the RGB domain with a highly efficient depth colorization technique based on surface normals. As for the multi-modal fusion of RGB and depth modalities, we explore different fusion techniques. Moreover, we present an approach that provides a decisive fusion of RGB and depth modalities based on the modality importance through a weighting scheme (see Sec. \ref{sec:fusionClassification}).  Our implementation is in Python using PyTorch\footnote{\url{https://github.com/pytorch/pytorch}} and numpy\footnote{\url{https://github.com/numpy/numpy}} libraries. All the source codes together with system requirements and documentations will be opened to the community on Github.

The proposed framework is evaluated with exhaustive experiments on two popular public datasets (i) Washington RGB-D Object dataset \cite{Lai_ICRA_2011} for RGB-D object recognition task and (ii) Sun RGB-D Scene dataset \cite{Song_CVPR_2015} for RGB-D scene recognition task. The experimental results demonstrate the effectiveness of our approach in terms of accuracy by achieving superior performance over the current state-of-the-art methods. A preliminary version of this work appeared in \cite{Caglayan_ECCVW_2018} for RGB-D object recognition. In this work, we present an extended and enhanced version of our work in \cite{Caglayan_ECCVW_2018} with a novel framework and contribute to the task of RGB-D object and scene recognition tasks as follows:
\begin{itemize}
	\item We present a novel framework for deep features with two-stage organization where information at different levels is encoded by incorporation of multiple random RNNs with a pre-trained CNN model for RGB-D object and scene recognition (see Sect. \ref{sec:method}). The framework is applicable to a variety of pre-trained CNN models including AlexNet \cite{Krizhevsky_NIPS_2012}, VGGNet \cite{Simonyan_ICLR_2015}, ResNet \cite{He_CVPR_2016}, and DenseNet \cite{Huang_CVPR_2017}. The overall structure has been designed in a modular and extendable way through a unified CNN and RNN process. Thus, it offers easy and flexible use. These also can easily be extended with new capabilities and combined with different setups and other models for implementing new ideas. In fact, our preliminary approach has been already successfully applied to another challenging robotics task in a SLAM system \cite{Guclu_CVPRW_2019}.
	
	\item We extend the idea of randomness in RNNs as a novel pooling strategy to cope with the high dimensionality of CNN activations from different levels (see Sec. \ref{sec:randomPooling}). This strategy has been applied as a preprocessing stage before RNNs and it allows us to evaluate and utilize multiple level information in deep models such as ResNet \cite{He_CVPR_2016} and DenseNet \cite{Huang_CVPR_2017} models. In addition, we give the experimental results of different pooling strategies in terms of accuracy and show the effectiveness of our pooling strategy over other pooling methods (see Sec. \ref{sec.exp.ma.poolingPerformances}). 
	
	
	\item We study several aspects of transfer learning through an empirical investigation including comparative profiling results of different baselines (see Sec. \ref{sec.exp.ma.profiling}), level-wise analysis of different baselines (see Sec. \ref{sec.exp.ma.levelPerformances}), the effects of finetuning over fixed pretrained CNN models (see Sec. \ref{sec.exp.ma.finetuning}), and different approaches to multi-level and multi-modality data fusion (see Sec. \ref{sec:exp.ma.fusionPerformance}). In regard to multi-model fusion, unlike our previous work using concatenation of features, we propose a soft voting approach based on individual SVM confidences of RGB and depth streams (see Sec. \ref{sec:fusionClassification}) and show the strength of our approach experimentally (see Sec. \ref{sec:exp.ma.fusionPerformance}). We also give; (\textit{i}) empirical evaluation of the randomness to see if random RNNs are stable enough (see Sec. \ref{sec.exp.ma.randomness}), (\textit{ii}) experimental analysis of multi-level RNNs (see Sec. \ref{sec.exp.ma.multilevelRNN}), and (\textit{iii}) comparative results of different pooling strategies over the proposed random pooling (see Sec. \ref{sec.exp.ma.poolingPerformances}). Finally, we provide experimental results demonstrating that our approach improves the state-of-the-art results on two the most comprehensive and challenging real-world public datasets; Washington RGB-D Object dataset for RGB-D object recognition (see Sec. \ref{sec:exp.objectRecognition}) and SUN RGB-D scene dataset for RGB-D scene recognition (see Sec. \ref{sec:exp.sceneRecognition}).
\end{itemize} 


\section{Related Work}
\label{sec:relatedwork}
The proposed work can be related with different areas, such as multi-modal CNN based approaches, transfer learning based approaches, and random recursive neural networks. In this section, we narrow our focus to RGB-D based recognition and give a brief review of the relevant approaches with stating the current work in the literature.

\subsection{Multi-Modal CNN based Approaches}
Following their success in computer vision, CNN-based solutions have replaced conventional methods such as the works in \cite{DepthKernel2011}, \cite{HKD2011}, and \cite{HONV2012} in the field of RGB-D object recognition, as in many other areas. For instance, Wang \textit{et al.} \cite{Wang_2015_ICCV, Wang_2015_IEEE_ToM} present CNN-based multi-modal learning systems motivated by the intuition of common patterns shared between RGB and depth modalities. They enforce their systems to correlate features of the two modalities in a multi-modal fusion layer with a pretrained model \cite{Wang_2015_ICCV} and their custom network \cite{Wang_2015_IEEE_ToM} respectively. Li \textit{et al.} \cite{Li_AAAI_2018df} extends the idea of considering multi-modal intrinsic relationship with intra-class and inter-class similarities for indoor scene classification by providing a two-stage training approach. In \cite{Rahman_ICME_2017}, a three-streams multi-modal CNN architecture has been proposed in which depth images are represented with two different encoding methods in two-streams and the remaining stream is used for RGB images. Despite the extra burden, this naturally has increased the depth accuracy in particular. Similar multi-representational approach has been proposed by Zia \textit{et al.} in \cite{Zia_ICCVW_2017} where a hybrid 2D/3D CNN model initialized with pre-trained 2D CNNs is employed together with 3D CNNs for depth images. Cheng \textit{et al.} \cite{Cheng_3DV_2015} propose convolutional fisher kernel (CFK) method which integrates a single CNN layer with fisher kernel encoding and utilizes Gaussian mixture models for feature distribution. The drawback of their approach is the very high dimensional of the feature space.

\subsection{Transfer Learning based Approaches}
Deep learning algorithms require a significant amount of annotated training data and obtaining such data can be difficult and expensive. Therefore, it is important to leverage transfer learning for enhancing high-performance learner on a target domain and the task at hand. Especially, applying a trained deep network and then fine-tuning the parameters can speed up the learning process or improve the classification performance \cite{Wang_TIP_2017}. Furthermore, many works show that a pre-trained CNN on a large-scale dataset can generate good generic representations that can effectively be used for other visual recognition tasks as well \cite{Razavian_CVPRW_2014, Yosinski_NIPS_2014, Oquab_CVPR_2014, Azizpour_CVPRW_2015, Azizpour_TPAMI_2015}. This is particularly important in vision tasks on RGB-D datasets, which is hard to collect with labeled data and generally amount of data is much less than that of the labeled images in RGB datasets.

There are many successful approaches that use transfer learning in the field of RGB-D object recognition. Schwarz \textit{et al.} use the activations of two fully connected layers, a.k.a. \textit{fc7} and \textit{fc8}, extracted from the pre-trained AlexNet \cite{Krizhevsky_NIPS_2012} for RGB-D object recognition and pose estimation. Gupta \textit{et al.} \cite{Gupta_ECCV_2014} study the problem of object detection and segmentation on RGB-D data and present a depth encoding approach referred as HHA to utilize a pre-trained CNN model on RGB datasets. Asif \textit{et al.} introduce a cascaded architecture of random forests together with the use of the \textit{fc7} features of the pre-trained models of \cite{Chatfield_BMVC_2014} and \cite{Simonyan_ICLR_2015} to encode the appearance and structural information of objects in their works of \cite{Asif_ICRA_2015} and \cite{Asif_ToR_2017}, respectively. Carlucci \textit{et al.} \cite{Carlucci_RAS_2018} propose a colorization network architecture and use a pre-trained model as feature extractor after fine-tuning it. They also make use of the final fully-connected layer in their approach. So, these above-mentioned studies mainly focus on the outputs of the fully-connected layers. 

On the other hand, many studies \cite{Liu_CVPR_2015, Zaki_ICRA_2016, Zaki_RAS_2017, Song_IJCAI_2017, Caglayan_ECCVW_2018} have concluded that using fully connected layers from pre-trained or fine-tuned networks might not be the optimum approach to capture discriminating properties in visual recognition tasks. Moreover, combining the activations obtained in different levels of the same modal enhances recognition performance further, especially for multi-modal representations, where earlier layers capture modality-specific patterns \cite{Yang_ICCV_2015, Song_IJCAI_2017, Caglayan_ECCVW_2018}. Hence, utilizing information at different levels in the works of \cite{Yang_ICCV_2015, Zaki_ICRA_2016, Zaki_RAS_2017, Song_IJCAI_2017, Caglayan_ECCVW_2018, Zaki_AuotRobots_2019} yields better performances. More recent approach of Loghmani \textit{et al.} \cite{Loghmani_RAL_2019} utilizes the pre-trained model of residual networks \cite{He_CVPR_2016} to extract features from multiple layers and combines them through a recurrent neural network. Their experimental results also verify that multi-level feature fusion provide better performance than single-level features. While their approach is based on a gated recurrent unit (GRU) \cite{Cho_EMNLP_2014} with a number of memory neurons, our approach employs multiple random neural networks with no necessarily need for training. A different related approach is proposed by Asif \textit{et al.} in \cite{Asif_TPAMI_2018}. They handle the classification task by dividing it into image-level and pixel-level branches and fusing through a Fisher encoding branch. Eitel \textit{et al.} \cite{Eitel_IROS_2015} and Tang \textit{et al.} \cite{Tang_TCDS_2019} employ two-stream CNNs, one for each modality of RGB and depth channels and each stream uses the pre-trained model of \cite{Krizhevsky_NIPS_2012} on the ImageNet. In both works \cite{Eitel_IROS_2015, Tang_TCDS_2019}, the two-streams are finally connected by a fully-connected fusion layer and a canonical correlation analysis (CCA) module, respectively. While feature fusion approaches (e.g. concatenation) may provide good accuracy for the visual recognition task, feature fusion may not be the only solution for multi-level decision process since increased feature space may not be good for recognition with small number of data. We experiment and show that voting on the SVM confidence scores for selected levels can also provide reliable and improved performance. Moreover, this also enables us to use confidence score based importance to RGB and depth domains in multi-modal fusion.

\subsection{Random Recursive Neural Networks}
Randomization in neural networks has been researched for a long time in various studies \cite{Schmidt_ICPR_1992, Pao_Computer_1992, Pao_Neurocomp_1994, Igelnik_Pao_TNN_1995, Huang_TNN_2006, Rahimi_NIPS_2008, Socher_NIPS_2012} due to its benefits, such as simplicity and computationally cheapness over optimization \cite{Rahimi_NIPS_2009}. Since a complete overview of these variations is beyond the scope of this paper, we give an overview specifically with the focus of random recursive neural networks \cite{Socher_NIPS_2012}. Recursive neural networks (RNNs) \cite{Pollack_AI_1990, Hinton_AI_1990, Socher_ICML_2011} are graphs that process a given input into recursive tree structures to make a high-level reasoning possible in a part-whole hierarchy by repeating the same process over the trees. RNNs have been employed for various research purposes in computer vision including image super-resolution \cite{Kim_CVPR_2016}, semantic segmentation \cite{Socher_ICML_2011, Sharma_NIPS_2014}, and RGB-D object recognition \cite{Socher_NIPS_2012, Bai_Neurocomp_2015, Cheng_CVIU_2015}. In \cite{Socher_NIPS_2012}, Socher \textit{et al.} have introduced a two-stage RGB-D object recognition architecture where the first stage is a single CNN layer using a set of k-means centroids as the convolution filters and the second stage is multiple random recursive neural networks to process outputs of the first stage. Bai \textit{et al.} \cite{Bai_Neurocomp_2015} propose a subset based approach of the pioneer work in \cite{Socher_NIPS_2012} where they use a sparse auto-encoder instead of the k-means clustering for convolution filters. Cheng \textit{et al.} \cite{Cheng_CVIU_2015} employ the same architecture of Socher \textit{et al.} \cite{Socher_NIPS_2012} for a semi-supervised learning system with a modification by adding a spatial pyramid pooling to prevent a potential performance degradation during resizing input images. Bui \textit{et al.} \cite{Bui_Access_2016} have replaced the single CNN layer in \cite{Socher_NIPS_2012} with a pre-trained CNN model for RGB object recognition and achieved impressive results. Following their success, in our preliminary work \cite{Caglayan_ECCVW_2018}, we propose an approach that aims to improve on this idea by gathering feature representations at different levels in a compact and representative feature vector for both of RGB and depth data. To this end, we reshape CNN activations in each layer that provides a generic structure for each layer by fixing the tree structure without hurting performance and it allows us to improve recognition accuracy by combining feature vectors at different levels. In this work,  we propose a pooling strategy to handle large dimensional CNN activations by extending the idea of randomness in RNNs. This can be related with the stochastic pooling by Zeiler and Fergus in \cite{Zeiler_ICLR_2013}, which picks the normalized activations of a region according to a multinomial distribution by computing the probabilities within the region. Instead of using probabilities, our pooling approach here is a form of averaging based on uniform distributed random weights. 

\section{Proposed Approach}
\label{sec:method}
The proposed pipeline has two main stages. In the first stage, a pre-trained CNN model has been employed as the underlying feature extractor. In this work, we have examined several models in this stage. The second stage transforms convolutional features through a randomized recursive neural network based structure that aims to acquire more compact representations. In order to cope with the high dimensionality of CNN activations, a pooling strategy based on random weights has been proposed. The final representative outcomes have been passed through a linear SVM classifier for categorization of objects and scenes. The overall pipeline can be related as a deeper analogy to \cite{Jarrett_ICCV_2009} where a proper architecture with random weights for object recognition task has been explored. In the following, we describe each stage of our approach in detail.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.80\textwidth]{models.pdf}
	\caption{Schematic overview of CNN models and their level-wise extraction points based structures. Each level of schematic view shows name of the level, operations performed in the level with the number of them if exist (for ResNet \cite{He_CVPR_2016} and DenseNet \cite{Huang_CVPR_2017} models), and dimensions of the activation output.}
	\label{fig:Models}
\end{figure*}

\subsection{Data Preparation} \label{sec:dataPreparation}
In order to use pre-trained CNN models, it is important to process input images appropriately. To this end, following common practices for preprocessing, we resize RGB images to 256x256 dimensions according to bilinear transformation and apply center cropping to get 224x224 dimensional images. Then, we apply commonly used z-score standardization on the input data by using mean and standard-deviation of the ImageNet \cite{Deng_Imagenet_CVPR_2009}. We do not perform any other practices such as data augmentation.

As for the depth domain, we first need appropriate RGB-like representation of depth data to leverage the power of pre-trained CNN models over the large-scale RGB dataset of the ImageNet. To do so, there are several ways to represent depth data as RGB-like images such as HHA method of Gupta \textit{et al.} \cite{Gupta_ECCV_2014} (i.e. using horizontal and vertical observation values and angle of the normal to common surface), ColorJet work by Eitel et al. \cite{Eitel_IROS_2015} (i.e. mapping depth values to different RGB color values), or commonly used surface normal based colorization as in \cite{Bo_IROS_2011, Caglayan_ECCVW_2018}. In this work, we prefer to use the colorization technique based on surface normals, as it confirms its effectiveness in our previous work \cite{Caglayan_ECCVW_2018}. However, unlike surface normal estimation from depth maps without camera parameters in \cite{Caglayan_ECCVW_2018}, we improve this in a more accurate way by estimating surface normals on 3D point clouds that has been computed using depth maps and camera intrinsic values. To address the issue of missing depth values, we first apply a fast vectorized depth interpolation by applying a median filter through a  neighborhood to reconstruct missing values in noisy depth inputs. Then, 3D point cloud estimation by using camera intrinsic constant values and surface normal calculation on point clouds are followed, respectively. After this, the common approach is scaling surface normals to map values to the  range to fit RGB image processing. However, since such an approach of mapping from floating point to integer values leads to a loss of information, we use these normal vectors as is without performing further quantization or scaling. Furthermore, unlike in RGB input processing, we apply resizing operation on these RGB-like depth data using the nearest neighborhood based interpolation rather than bilinear interpolation. Because the latter may lead to more distortion in geometric structure of a scene. Moreover, nearest neighbor interpolation is more suitable to the characteristics of depth data by providing a better separability between foreground and background in a scene. When applying z-score standardization to depth domain, we use the standard-deviation of the ImageNet as in RGB domain. However, we use zero-mean instead of the ImageNet mean as normal vectors are in the range of  without the need for zero-mean shifting.

\subsection{CNN-Stage}
The backbone of our approach is a pre-trained CNN model. Since size of available RGB-D datasets are much smaller than that of RGB's, it is important to make use of an efficient knowledge transfer from pre-trained models on large RGB datasets. In addition, it saves time by eliminating the need for training from scratch. In the previous work \cite{Caglayan_ECCVW_2018}, the available pre-trained CNN model of \cite{Chatfield_BMVC_2014}, named VGG\_f, in MatConvNet toolbox \cite{Vedaldi_Matconvnet_ICM_2015} has been used. In this work, we employ several available pre-trained models of PyTorch including AlexNet \cite{Krizhevsky_NIPS_2012}, VGGNet \cite{Simonyan_ICLR_2015} (specifically VGGNet-16 model with batch normalization), ResNet \cite{He_CVPR_2016} (specifically ResNet-50 and ResNet-101 models), and DenseNet \cite{Huang_CVPR_2017}. We extract features from seven different levels of CNN models. The models investigated in this study with the feature extraction levels are shown in Fig. \ref{fig:Models}. For AlexNet, outputs of the five successive convolutional layers and the following two fully-connected (FC) layers have been considered, while for VGGNet, the first two FC layers  are taken into account together with the outputs of each convolution block that includes several convolutions and a final max pooling operations. Unlike AlexNet and VGGNet, ResNet and DenseNet models consist of blocks such as residual, dense or transition blocks where there are multiple layers. While ResNet extends the sequential behaviour of AlexNet and VGGNet with the introduction of the skip-connections, DenseNet takes one step further by concatenating the incoming activations rather than summing up them. The ResNet models consist of five stages and a following average pooling and an FC layer. Therefore, each output of the five successive stages and the output of the final average pool have been considered for the six of the seven extraction points. As for the remaining extraction level for these models (ResNet-50 and ResNet-101), the middle point of the third block (which is the largest block) has been taken. Similarly, for DenseNet model, the output of all the four dense blocks (for the last dense block, the output of normalization that follows the dense block has been taken) and the transition blocks between them have been considered as the extraction points. Since common and straightforward model of AlexNet has a minimum depth with a seven layer stack-ups, the above-mentioned CNN extraction points for each model are selected to evaluate and compare level-wise model performances. In addition, these levels are also related to the CNN model in the previous work \cite{Caglayan_ECCVW_2018} that we improve on by considering their intrinsic reasoning behind the use of blocks and the approximate distance differences.

\subsection{RNN-Stage}
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.65\columnwidth, keepaspectratio]{rnn.pdf}
	\caption{Graphical representation of a single recursive neural network (RNN). The same random weights have been applied to compute each node and level.}
	\label{fig:RNN}
\end{figure}
Random recursive neural networks offer a feasible solution by randomly fixing the network connections and eliminate the need for selection in the parameter space. Motivated by this, we employ multiple random RNNs, whose inputs are the activation maps of a pre-trained CNN model. RNNs map a given 3D matrix input into a vector of higher level representations of it by applying the same operations recursively in a tree structure. In each layer, adjacent blocks are merged into a parent vector with tied weights where the objective is to map inputs   into a lower dimensional space   in the end through multiple levels. Then, the output of a parent vector is passed through a nonlinear function. A typical choice for this purpose is the  function. In our previous work \cite{Caglayan_ECCVW_2018}, we give the comparative results of different activation functions in terms of accuracy success and show hyperbolic functions work well. Therefore, in this work, we employ  activation function as in \cite{Socher_NIPS_2012, Caglayan_ECCVW_2018}. Fig. \ref{fig:RNN} shows a graphical representation of a pooled CNN output with the size  and an RNN structure with 3 levels and blocks of  child nodes (Note that this figure is inspired by the RNN graphical representation of \cite{Socher_NIPS_2012}).

In our case, inputs of RNNs are activation maps obtained from different levels of the underlying CNN model. Let  be an input image that pass through  a given CNN model, where  are the extraction levels and , where the output convolution maps are either a 3D matrix  for  convolutional layers or a 1D vector of  for  FC layers/global average pooling. Since RNN requires a 3D input of , we first process the convolution maps at each level to ensure the required form. Moreover, by applying this step, we ensure that RNNs are able to handle inputs fast and effectively by reducing the receptive field area and/or the number of activation maps of high-dimensional feature levels (e.g. the outputs of early levels for models such as VGGNet \cite{Simonyan_ICLR_2015}, ResNet \cite{He_CVPR_2016}, DenseNet \cite{Huang_CVPR_2017} etc). In addition, we apply preprocessing to obtain similar output structures with the previous work \cite{Caglayan_ECCVW_2018}. However, it was enough to apply only reshaping in the previous work due to less dimensional size of layers in VGG\_f model. In this work, we introduce random weighted pooling that copes with high dimensionality of layers in the underlying deeper models such as ResNet \cite{He_CVPR_2016} and DenseNet \cite{Huang_CVPR_2017}. Our pooling mechanism can downsample CNN activations in both number and spatial dimension of maps. After applying the preprocessing step to obtain suitable forms for RNNs, we compute parent vector as

where  for each CNN extraction level ,  is a nonlinearity function which is  in this study,  is block size of an RNN. Instead of a multi-level structured RNN, an RNN in this study is of one-level with a single parent vector. In fact, our experiments have shown that the single-level structure provides better or comparable results over the multi-level structure in terms of accuracy (see Sec. \ref{sec.exp.ma.multilevelRNN}). Moreover,  the single-level is more efficient with less computational burden. Thus,  block size is actually the receptive field size in an RNN. In Eq. \ref{eq:rnnParent}, the parameter weight matrix is  and it is randomly generated from a predefined distribution that satisfies the following probability density function

where  is a predefined distribution and  and  are boundaries of the distribution. In our case, the weights are set to be uniform random values in , which have been assigned by following our previous work \cite{Caglayan_ECCVW_2018} and specifically with the assumption of preventing possible explosion of tensor values due to our aggregating pooling strategy. Keeping in mind that in order to obtain sufficient descriptive power from the randomness, we need to generate enough samples from the range. In \cite{Socher_NIPS_2012}, it has been demonstrated experimentally that increasing the number of random RNNs up to  improves performance and gives the best result with  RNNs. In \cite{Caglayan_ECCVW_2018}, it has also been verified that  number of RNN weights can be generated for feature encoding with high performance in classification on both of RGB and depth data. Therefore, as a standard usage in this work, we do feature encoding on CNN features using  random RNNs with  channel representations, leading us to  dimensional feature vector at each level in a model.

The reason why random weights work well for object recognition tasks seems to lie in the fact that particular convolutional pooling architectures can naturally produce frequency selective and translational invariant features \cite{Saxe_ICML_2011}. As stated before, in analogy to the convolutional-pooling architecture in \cite{Jarrett_ICCV_2009}, our approach intuitively incorporates both selectivity due to the CNN stage and translational invariance due to the RNN stage. Moreover, we have to point out that there is biological plausibility lies in the use of randomness as well. In \cite{Rigotti_Frontiers_2010}, Rigotti et al. have shown that random connections between inter-layer neurons are needed to implement mixed selectivity for optimal performance during complex cognitive tasks. Before concluding this section, we give details of our random pooling approach, where we extend the idea of random RNN as a downsampling mechanism.

\subsubsection{Random Weighted Pooling} \label{sec:randomPooling}
\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth, keepaspectratio]{pooling.pdf}
	\caption{Illustration of random weighted pooling over number of maps (top) and size of maps (below).}
	\label{fig:Pooling}
\end{figure}
In our previous work \cite{Caglayan_ECCVW_2018}, we give CNN outputs to RNNs after a reshaping process. However, due to the high dimensional output size of the models used in this study, it is necessary to process CNN activations further. In this work, we propose a random pooling strategy to reduce the dimension in either size of the activation maps ( block size or receptive field area of an RNN) or number of maps () at CNN levels where reshaping is insufficient. In our random weighted pooling approach, we aggregate the CNN activation maps by sampling from a uniform distribution as in Eq. \ref{eq:weightPdf} from each pooling area. More precisely, for  extraction level, the pooling reduces  activations by mapping into  region as  where  and  in Eq. \ref{eq:randomPool}.

where  is pooling region,  convolutional activations,  is the index of each element within the pooling, and  is random weights.  and  when pooling is over number of maps whereas  and  when pooling is over size of maps. Fig. \ref{fig:Pooling} illustrates proposed random weighted pooling for both of downsampling in number of maps and size of maps. In this work, by extending the randomness in RNNs along the pipeline with the proposed pooling strategy, we aim to show that randomness can actually work quite effectively. In fact, as we can see in the comparative results (see Sec. \ref{sec.exp.ma.poolingPerformances}), this randomness in our approach works generally better comparing to the other common pooling methods such as max pooling and average pooling.

\subsection{Fusion and Classification} \label{sec:fusionClassification}
After obtaining encoded features from the RNN-Stage, we investigate multi-level fusions to capture more distinctive information at different levels for further recognition performance. In order to minimize the cross entropy error between output predictions and the target values, we could give multi-level outputs to fully connected layers and back-propagate through them. However, following the success in our previous study \cite{Caglayan_ECCVW_2018}, we perform classification by employing linear SVM with the scikit-learn\footnote{\url{https://github.com/scikit-learn/scikit-learn}} \cite{Pedregosa_JMLR_2011_scikit} implementation. To this end, in our previous work \cite{Caglayan_ECCVW_2018}, we have performed the straightforward feature concatenation on various combinations of the best mid-level representations. In this work, in addition to the feature concatenation, we also apply soft voting by averaging SVM confidence scores on these best trio of levels. Finally, RGB and depth features are fused to evaluate combined RGB-D accuracy performance. Shiny, transparent, or thin surfaces may cause corruption in depth information since depth sensors do not properly handle reflections from such surfaces, resulting better performance in favor of RGB in such cases. On the other hand, depth sensors work well in a certain range and are insensitive to changes in lighting conditions. Therefore, to take full advantage of both modalities in a complementary way, a compact multi-modal combination based on the success of input type is important in devising the best performing fusion. To this end, we present a decision mechanism using weighted soft voting based on the confidence scores obtained from RGB and depth streams. Modality weighting in this way is used to compensate imbalance and complement decision in different data modalities. Once the modality-specific branches proceed, we combine the predictions through the weighted SVM as follows. Let  represents SVM confidence scores of each category class , where  is number of classes, and  indicates RGB and depth modalities. Then, weights  are computed as in Eq. \ref{eq:fusionWeights}. 

where  is normalized squared magnitudes for each modality and defined as:


Finally, multi-modal RGB-D predictions are estimated as follows, in Eq. \ref{eq:weightedFusion}:

where  is a category class. Concretely, if RGB and depth results are balanced in confidence scores, then the final soft voting decision is based on equal contribution from each stream similar to averaging. 

\section{Experimental Evaluation}
\label{sec:experiments}
The proposed framework has been evaluated on two challenging benchmarks (Sec. \ref{sec:exp.datasets}) for two tasks: (i) RGB-D object recognition (Sec. \ref{sec:exp.objectRecognition}) using Washington RGB-D object dataset \cite{Lai_ICRA_2011} and (ii) RGB-D scene recognition (Sec. \ref{sec:exp.sceneRecognition}) using SUN RGB-D scene dataset \cite{Song_CVPR_2015}. In order to evaluate effects of various model parameters and setup properties in our framework, we carry out extensive experiments (Sec. \ref{sec:exp.modelAblation}) on the challenging Washington RGB-D object dataset, which is a larger-scale RGB-D dataset comparing to other RGB-D benchmarks. Finally, we compare our results with state-of-the-art results for both benchmarks. Results of other methods are taken from
the original papers.

\subsection{Dataset and Setup} \label{sec:exp.datasets}
\subsubsection{Washington RGB-D Object Dataset}
Washington RGB-D object dataset includes a total of  images for each modality under  object categories and  category instances. Categories are commonly used household objects such as cups, camera, keyboards, vegetables, fruits, etc. Each instance of a category has images taken from ,  and  elevation angles. The dataset provides  train/test splits where in each split, one instance for each category is used for testing and the remaining instances are for training. Thus, for a single split run, a total of  category instances (roughly  images) are used at testing and the remaining  instances (roughly  images) are used at training phase. We evaluate the proposed work on the provided cropped images with the same setup in \cite{Lai_ICRA_2011} for the 10 splits and average accuracy results are reported for the comparison to the related works.
\subsubsection{SUN RGB-D Scene Dataset}
SUN RGB-D scene dataset is the largest real-world RGB-D scene understanding benchmark to the date and  contains RGB-D images of indoor scenes. Following the publicly available configuration of the dataset, we choose  scene categories with a total of  images for training and  images for testing. We use the same train/test split of Song \textit{et al.} \cite{Song_CVPR_2015} to evaluate the proposed work for scene recognition.

\subsection{Model Ablation} \label{sec:exp.modelAblation}
We first have analysed and validated the proposed framework with extensive experiments with a variety of architectural configurations on the popular benchmark of Washington RGB-D dataset. In this section, the analysis and evaluations of the model ablative investigations are presented. The developmental experiments are carried out on two splits of Washington RGB-D Object dataset for both modalities in order to evaluate on more stable results. The average results are analysed. However, in some experiments, more runs have been carried out, which are clearly stated in the related sections. Then, the best performing models are compared with the state-of-the-art methods with the exact provided evaluation setups. We assess the proposed framework on a desktop PC with AMD Ryzen 9 3900X 12-Core Processor, 3.8 GHz Base, 128 GB DDR4 RAM 2666 MHz, and NVIDIA GeForce GTX 1080 Ti graphics card with 11 GB memory.

\begin{figure*}[!ht]
	\centering
	 \subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{randomnessRGB2.eps}}\subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{randomnessDepth2.eps}}\caption{Effect of randomness on the accuracy results for each level (L1 to L7). Values indicate standard deviations.}
	\label{fig:randomness}
\end{figure*}

\begin{figure*}[!b]
	\centering
	 \subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{layerwisePerformancesRGB.eps}}\subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{layerwisePerformancesDepth.eps}}\caption{Level-wise average accuracy performance of different baseline models on all the 10-splits of Washington RGB-D dataset.}
	\label{fig:levelwisePerformances}
\end{figure*}
\subsubsection{Computation Time and Memory Profiling on Different Models} \label{sec.exp.ma.profiling}

\begin{table}[!b]
	\caption{Average computational time and memory overhead for overall data processing and model learning on two splits of Washington RGB-D dataset. Results cover both of train and test phases together.}  
	\label{table:cnnProfiling}    
	\centering
	\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
	\begin{adjustbox}{width=\columnwidth}
		\begin{tabular}{l|c|c|c|c|c|c|c|}
			\cline{2-8}
														& \multicolumn{3}{c|}{Time \textit{(hh:mm:ss)}}                                                                                                                                                                & \multicolumn{4}{c|}{Memory}                                                                                                                                                                                                  \\ \hline
			\multicolumn{1}{|l|}{\multirow{2}{*}{Model}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Feature Extraction\\ (CNN-RNN Stages)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Classification\\ (SVMs)\end{tabular}} & \multirow{2}{*}{Overall} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CNN-Stage\\ (GPU)\end{tabular}} & \multicolumn{2}{c|}{RNN-Stage (CPU)}             & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Overall\\ (CPU)\end{tabular}}} \\ \cline{6-7}
			\multicolumn{1}{|l|}{}                       &                                                                                                &                                                                                  &                          &                                                                            & \multicolumn{1}{c|}{Pool Weights} & RNN Weights & \multicolumn{1}{l|}{}                                                                         \\ \hline
			\multicolumn{1}{|l|}{AlexNet}                & 00:07:41                                                                                       & 00:28:33                                                                         & 00:36:14                 & 1115 MB                                                                    & \multicolumn{1}{c|}{772.1 kB}     & 4.2 GB      & 12.6 GB                                                                                       \\ \hline
			\multicolumn{1}{|l|}{VGGNet-16}              & 00:21:21                                                                                       & 00:36:42                                                                         & 00:58:03                 & 9259 MB                                                                    & \multicolumn{1}{c|}{8.6 MB}       & 4.8 GB      & 11.8 GB                                                                                       \\ \hline
			\multicolumn{1}{|l|}{ResNet-50}              & 00:16:23                                                                                       & 00:38:36                                                                         & 00:54:59                 & 6067 MB                                                                    & \multicolumn{1}{c|}{9.6 MB}       & 5.1 GB      & 10.8 GB                                                                                       \\ \hline
			\multicolumn{1}{|l|}{ResNet-101}             & 00:19:08                                                                                       & 00:40:33                                                                         & 00:59:41                 & 8795 MB                                                                    & \multicolumn{1}{c|}{9.6 MB}       & 5.1 GB      & 11.8 GB                                                                                       \\ \hline
			\multicolumn{1}{|l|}{DenseNet-121}           & 00:17:02                                                                                       & 00:26:47                                                                         & 00:43:49                 & 8821 MB                                                                    & \multicolumn{1}{c|}{8.3 MB}       & 5.4 GB      & 13.3 GB                                                                                       \\ \hline
		\end{tabular}
	\end{adjustbox}
\end{table}

We first evaluate different baseline CNN models within our framework in terms of computational time and memory requirements. We evaluate the proposed framework in two parts: (\textit{i}) Feature extraction containing CNN-RNN stages and (\textit{ii}) Classification where a model based on the extracted features is learnt to distinguish the different classes. The batch size is set to 64 for all the models. Table \ref{table:cnnProfiling} reports computational times and memory workspaces for the whole data processing ( images) on Washington RGB-D dataset. The results here are the average results of two splits on RGB images. There is additional cost for depth data processing as it is required to colorize them. The results on this table cover the overall processing and classification of all  level features. Moreover, it should be noted that classification time covers both training and testing processes, in which training takes the main computational burden. Therefore, the main cost in terms of processing time comes from training SVM models that works on CPU for  times. The process for only a single optimum level would reduce the computational time to a ratio of seven approximately. Hence, using a single optimum level or fusion of selected levels can be efficient enough in terms of time and memory requirements while presenting sufficient representations.

\subsubsection{Empirical Evaluation of the Effect of Randomness} \label{sec.exp.ma.randomness}

The use of random weights both in pooling and RNN structures leads to the question of how stable are the results. Thus, we experimentally investigate to see whether there is a decisive difference between different runs that generate and use new random weights. We run the pipeline with different random weights on two splits, 5 times for each. Fig. \ref{fig:randomness} reports average results with their standard deviations for each level. The figure clearly shows that randomness does not cause any instability in the model and produces similar results with very small deviations.

\subsubsection{Level-wise Performance of Different Models} \label{sec.exp.ma.levelPerformances}

Fig. \ref{fig:levelwisePerformances} shows level-wise average accuracy performances of all the baseline models for both of RGB and depth modalities on all the 10 evaluation splits. The graphs show a similar performance trend line with a clear upward at the beginning and a downward at the end. Although the levels at which optimum performance is obtained vary according to the model, what is common to all models in general is that instead of final level representations, intermediate level representations present the optimal results. These experiments also verify that while deep models transform attributes from general to specific through  the network eventually \cite{Razavian_CVPRW_2014, Zeiler_ECCV_2014}, intermediate layers present the optimal representations. This makes sense because while early layers response to low-level raw features such as corners and edges, late layers extract more object-specific features of the trained datasets. This is more clear on the depth plot in Fig. \ref{fig:levelwisePerformances}, where the dataset difference is obvious due to the domain difference. We should state that RNN encoding on features extracted from FC layers with less than  dimension might not be efficient since they are already compact enough. Therefore, encoding outputs of these layers to a larger feature space through RNNs might lead to redundancy in representations. This might be another reason why there is a drop in accuracy of these layers (e.g. see L7 in Fig. \ref{fig:levelwisePerformances}). In addition, depth plot contains more fluctuations and irregularities comparing to the RGB plot, since the pretrained models of the RGB ImageNet are used as fixed extractors without finetuning. As for the different baseline model comparison, ResNet-101 and DenseNet-121 models perform similarly in terms of accuracy and are better than others.

\subsubsection{Comparative Results of Random Weighted Pooling} \label{sec.exp.ma.poolingPerformances}
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth, keepaspectratio]{poolingComparison.eps}
	\caption{Average accuracy performance of different pooling methods on RGB and depth data for the baseline model of DenseNet-121 on two splits of Washington RGB-D dataset.}
	\label{fig:poolingComparison}
\end{figure}
In our approach, we extend the idea of randomness into a pooling strategy to cope with the high dimensionality of CNN activations. We particularly employ random pooling to confirm that randomness works greatly in overall RNN-Stage even in such a pooling strategy together with random RNNs. To this end, we investigate the comparative accuracy performances of random pooling together with average pooling and max pooling. We use the DenseNet-121 model, where pooling is used extensively on each level (except in level 4), and we conduct experiments using the same RNN weights for fair comparison. Fig. \ref{fig:poolingComparison} shows average accuracy results of two splits for each pooling on both RGB and depth data. As seen from the figure, random weighted pooling generally performs similar to average pooling, while it performs better than max pooling. Moreover, it is seen that random pooling acquires better results especially in middle/late levels(L4-L7), which presents more stable and meaningful representations comparing to the early levels.
\subsubsection{Effect of Multi-Level RNN Structure} \label{sec.exp.ma.multilevelRNN}
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth, keepaspectratio]{multilevelRNN.eps}
	\caption{Comparison of single-level and multi-level RNNs on two different CNN activations (L6 and L7) of AlexNet. The horizontal axis shows average accuracy performances (\%) on two splits of Washington RGB-D dataset.}
	\label{fig:multilevelRNN}
\end{figure}
An RNN in this study is of one-level structure with a single parent computation, which is obviously computationally fast comparing to the multi-level structural RNNs. Furthermore, in this way, it provides an ease of use with no need of further processing for fixing the required input forms. However, in order to testify the performance of single-level RNNs over multiple-level RNNs, we analyze the comparative accuracy performances of 1-level RNNs together with 3-levels RNNs (see Fig. \ref{fig:RNN}). To this end, we conduct experiments on two CNN activation levels with highest semantic information (L6 and L7) of the baseline model of AlexNet. The average results of two splits for both of RGB and depth data are shown in Fig. \ref{fig:multilevelRNN}. The results show that RNN with 1-level performs better than RNN with 3-levels on RGB data, while 3-levels of RNN is better than 1-level of RNN on depth data. The better performance of RNN with 3-levels on depth data might be due to the use of a pretrained CNN model based on the RGB data of ImageNet. Hence, further processing might provide more representative information for depth data in that way. Therefore, this difference might be diminished or turn in favor of 1-level RNNs in the use of finetuned CNNs for depth modality as well. Overall, considering both RGB and depth data together, RNNs with 1-level are better in terms of accuracy performance as well.

\subsubsection{Contribution of Fine-tuning} \label{sec.exp.ma.finetuning}
\begin{figure*}
	\centering
	 \subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{finetuningRGB.eps}}\subfloat{\includegraphics[width=\columnwidth, keepaspectratio]{finetuningDepth.eps}}\caption{Level-wise average accuracy performance of finetuned CNN models together with fixed models on all the 10-splits of Washington RGB-D dataset.}
	\label{fig:finetuning}
\end{figure*}
We have not used any training or fine-tuning in our approach to feature extraction in the experiments so far. Although impressive results are obtained on RGB data, the same success is not achieved on depth data. The reason for this difference is that the baseline CNN models are pretrained models on RGB dataset of the ImageNet. Therefore, as the next step, we analyze the changes in accuracy performance of RGB and depth data modalities by fine-tuning the baseline CNN models in our approach. To this end, we first carry out a systematic inquiry to find optimal fine-tuning hyper-parameters on a predefined set of values using only one split of Washington RGB-D dataset as a validation set for AlexNet and DenseNet-121 models. Then, fine-tuning of the models are performed by stochastic gradient descent (SGD) with momentum. The hyper-parameters of momentum, learning rate, batch size, learning rate decay factor and decay step size, and number of epochs, respectively are used as following;  and  are used for AlexNet on RGB and depth data, respectively, whereas  and  are used for DenseNet-121. Apart from these two models, we also perform fine-tuning on the ResNet-101 model. We use the same fine-tuning hyperparameters of DenseNet-121 for ResNet-101, since they are in a similar architectural structure. Fig. \ref{fig:finetuning} shows average accuracy performance of finetuned CNN models together with fixed models on all the 10 evaluation splits of Washington RGB-D object dataset. The plot shows a clear upward in performance on depth data as expected. However, there is a loss of accuracy in general, when fine-tuning is performed on RGB data. Washington RGB-D object dataset contains a subset of the categories in ImageNet. Accordingly, pretrained models of ImageNet are already satisfy highly correlated distribution on RGB data. Therefore, there is no need for fine-tuning on RGB data. In contrast, in order to ensure coherence and relevance, fine-tuning is required for depth data due to domain difference of the inputs with the pretrained models.

\subsubsection{Empirical Performance of Different Fusion Strategies} \label{sec:exp.ma.fusionPerformance}
\begin{table}[!b]
\caption{Average accuracy performance of different fusion combinations on the best three levels using Washington RGB-D dataset (\%).}  
	\label{table:levelFusions}    
	\centering
	\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
	\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{cl|ll|ll|ll|}
\cline{3-8}
\multicolumn{1}{l}{}    &       & \multicolumn{2}{c|}{AlexNet}     & \multicolumn{2}{c|}{DenseNet-121} & \multicolumn{2}{c|}{ResNet-101} \\ \cline{3-8} 
                        &     & RGB & \multicolumn{1}{l|}{Depth} & RGB                & Depth   & RGB                & Depth           \\ \hline

\multicolumn{1}{|c|}{\multirow{3}{*}{Single}}         & LB1             &  81.4  1.8	&83.5  2.2	&89.7  1.0	&85.0  2.1	&89.2  1.3	&85.5  2.2   \\
\multicolumn{1}{|c|}{}                                & LB2             &  81.1  2.1	&83.3  2.2	&91.0  1.2	&86.2  2.3	&91.1  1.0	&87.1  2.7   \\
\multicolumn{1}{|c|}{}                                & LB3             &  79.2  2.4	&83.2  2.3	&89.5  1.5	&86.8  2.1	&90.5  1.6	&86.9  2.6   \\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{Concats}}        & LB1 + LB2       &  \textbf{83.0  1.9}	&84.0  2.4	&90.4  1.0	&85.5  2.1	&91.1  1.1	&87.1  2.7   \\
\multicolumn{1}{|c|}{}                                & LB1 + LB3       &  82.2  2.0	&83.8  2.4	&90.0  1.5	&\textbf{86.9  2.1}	&91.1  1.4	&86.9  2.6   \\
\multicolumn{1}{|c|}{}                                & LB2 + LB3       &  81.0  2.0	&83.4  2.3	&89.6  1.5	&86.8  2.1	&91.1  1.5	&87.0  2.7   \\
\multicolumn{1}{|c|}{}                                & LB1 + LB2 + LB3 &  82.5  2.0	&83.8  2.3	&90.0  1.5	&86.9  2.1	&91.5  1.3	&87.0  2.7   \\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{SVM Avg Voting}} & LB1 + LB2       &  82.8  1.9	&\textbf{84.1  2.3}	&91.2  1.0	&86.0  2.2	&91.0  1.1	&87.0  2.5   \\
\multicolumn{1}{|c|}{}                                & LB1 + LB3       &  82.5  2.0	&84.1  2.4	&91.2  1.0	&86.8  2.2	&91.8  1.2	&87.1  2.5   \\
\multicolumn{1}{|c|}{}                                & LB2 + LB3       &  81.1  2.0	&83.4  2.3	&91.3  1.3	&86.8  2.2	&92.2  1.0	&87.0  2.7   \\
\multicolumn{1}{|c|}{}                                & LB1 + LB2 + LB3 &  82.7  2.1	&84.0  2.4	&\textbf{91.5  1.1}	&86.7  2.2	&\textbf{92.3  1.0}	&\textbf{87.2  2.5}   \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}
We have shown that a fixed pretrained CNN model together with random RNN already achieves impressive results on a single level. Likewise, when such pretrained models are fine-tuned on depth data, the results are boosted greatly. The best single levels for RGB and depth data, respectively, are L4, L5 for AlexNet; L5, L6 for ResNet-101; and  L6, L7 for DenseNet-121. Next, to further improve accuracy performances, we investigate empirical accuracy analysis of multi-level fusions using fixed pretrained CNN models on RGB data and fine-tuned CNN models on depth data. In this work, in addition to the feature concatenation as in our previous work \cite{Caglayan_ECCVW_2018}, we also apply average voting based on SVM confidence scores on the best performing levels. Table \ref{table:levelFusions} reports the average accuracy on the all 10 train/test splits of Washington RGB-D dataset for AlexNet, DenseNet-121, and ResNet-101. The table shows the top three level results (best levels) for each modality and their fusion combinations. The best level triples (LB1, LB2, LB3) for both of AlexNet and ResNet-101 are (L4, L5, L6) on RGB data and (L5, L6, L7) on depth data, while for DenseNet-121 these levels are (L5, L6, L7) on both RGB and depth data. As can be seen from the table, one single level has already produced very good results. Since both model structures and data modality characteristics are different, the best results for each column generally vary depending on the data type and the used model. Nevertheless, in general, average voting on SVM confidence scores gives better results comparing to feature concatenation. 

Finally, we provide RGB-D combined results for all three models as shown in Table \ref{table:rgbdFusions} based on the SVM confidences. The table reports average results for fusion of the best levels of RGB and depth, and the best trio levels. We evaluate two variants of soft voting, our proposed weighted vote and average vote. The proposed weighted vote increases accuracy comparing to average vote for all the models both on the multi-modal fusion of the best single and best trio levels of RGB and depth streams. The results also confirm the strength of our multi-modal voting approach that combines RGB and depth modalities effectively.
\begin{table}[!h]
\caption{Average accuracy performance of RGB-D (RGB + Depth) with different fusion combinations on Washington RGB-D dataset (\%).}  
	\label{table:rgbdFusions}    
	\centering
	\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
	\begin{adjustbox}{width=\columnwidth}
		\begin{tabular}{llccc}
		\hline
						&								       																				& AlexNet 						  & DenseNet-121 					& ResNet-101 				\\ \hline \hline
		Avg Vote 		&           		& 90.2  1.3        	  & 92.9  1.4           	& 92.7  1.6         \\
		Weighted Vote 	&      			   	& 90.2  1.2        	  & 93.5  1.0          	& 93.8  1.1          \\
		Avg Vote 		&   & 90.6  1.6        	  & 92.6  1.4          	& 93.0  1.3           \\
		Weighted Vote 	&   & \textbf{90.9  1.3}    & \textbf{93.5  1.0}    & \textbf{94.1  1.0} 	 \\ \hline
		\end{tabular}
	\end{adjustbox}
\end{table}


\subsection{Object Recognition Performance} \label{sec:exp.objectRecognition}
\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=\textwidth, keepaspectratio]{wrgbdIndividualResultsV2.eps}
	\end{center}
	\caption{Per-category average accuracy performances of ResNet101-RNN on Washington RGB-D Object dataset.}
	\label{fig:wrgbdIndividualResults}
\end{figure*}

\begin{table}\caption{Average accuracy comparison of our approach with the related methods on Washington RGB-D Object dataset (\%). \textcolor{red}{Red:} Best result, \textcolor{blue}{Blue:} Second best result, \textcolor{darkgreen}{Green:} Third best result.}
	\begin{center}
		\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
		\begin{adjustbox}{width=\columnwidth}
		\begin{tabular}{ lccc }
			\hline
			Method 											& RGB 							& Depth 						& RGB-D \\ \hline \hline
			Kernel SVM \cite{Lai_ICRA_2011}    				& 74.5  3.1 			& 64.7  2.2 	 		& 83.9  3.5			\\ KDES \cite{Bo_IROS_2011}         				& 77.7  1.9 			& 78.8  2.7 			& 86.2  2.1			\\ CNN-RNN \cite{Socher_NIPS_2012}    				& 80.8  4.2 			& 78.9  3.8			& 86.8  3.3			\\ CaRFs \cite{Asif_ICRA_2015}         			& - 			                & - 			                & 88.1  2.4			\\ MMDL \cite{Wang_2015_IEEE_ToM}         			& 74.6  2.9			& 75.5  2.7			& 86.9  2.6			\\ Subset-RNN \cite{Bai_Neurocomp_2015}  			& 82.8  3.4 			& 81.8  2.6 	 		& 88.5  3.1			\\ CNN Features \cite{Schwarz_ICRA_2015}  	        & 83.1  2.0 			& -								& 89.4  1.3			\\ CNN-SPM-RNN \cite{Cheng_CVIU_2015}        		& 85.2  1.2		 	& 83.6  2.3 			& 90.7  1.1			\\ CFK \cite{Cheng_3DV_2015}  						& 86.8  2.7 			& 85.8  2.3	        & 91.2  1.4			\\ Fus-CNN \cite{Eitel_IROS_2015}  	        	& 84.1  2.7 			& 83.8  2.7			& 91.3  1.4			\\ AlexNet-RNN \cite{Bui_Access_2016}  			& 89.7  1.7 			& -								& -								\\ Fusion 2D/3D CNNs \cite{Zia_ICCVW_2017}         & 89.0  2.1 			& 78.4  2.4			& 91.8  0.9			\\ STEM-CaRFs \cite{Asif_ToR_2017}  			    & 88.8  2.0 			& 80.8  2.1			& 92.2  1.3			\\
			MM-LRF-ELM \cite{Liu_Neurocomp_2018}        	& 84.3  3.2		 	& 82.9  2.5 			& 89.6  2.5			\\ VGG\_f-RNN \cite{Caglayan_ECCVW_2018}     		& \bftab\textcolor{darkgreen}{89.9  1.6} 	        & 84.0  1.8			& 92.5  1.2	        \\
			DECO \cite{Carlucci_RAS_2018}     		        & 89.5  1.6 	        & 84.0  2.3			& \bftab\textcolor{darkgreen}{93.6  0.9}	        \\
			MDSI-CNN \cite{Asif_TPAMI_2018}  			    & \bftab\textcolor{darkgreen}{89.9  1.8} 			& 84.9  1.7			& 92.8  1.2			\\
			HP-CNN \cite{Zaki_AuotRobots_2019}    		    & 87.6  2.2 			& 85.0  2.1			& 91.1  1.4			\\ RCFusion \cite{Loghmani_RAL_2019}  			    & 89.6  2.2 			& \bftab\textcolor{darkgreen}{85.9  2.7}			& \bftab\textcolor{red}{94.4  1.4}			\\ \hline
			\bftab{This work - AlexNet-RNN}     			& 83.0  1.9 	        & 84.1  2.3			& 90.9  1.3	            \\
            \bftab{This work - DenseNet121-RNN}     		& \bftab\textcolor{blue}{91.5  1.1} 	        & \bftab\textcolor{blue}{86.9  2.1}			& 93.5  1.0	            \\
            \bftab{This work - ResNet101-RNN}     			& \bftab\textcolor{red}{92.3  1.0} 	       & \bftab\textcolor{red}{87.2  2.5}			& \bftab\textcolor{blue}{94.1  1.0}	            \\
			\hline
		\end{tabular}
		\end{adjustbox}
		\label{table:wrgbdResults}
	\end{center}
\end{table}

Table \ref{table:wrgbdResults} shows average accuracy performance of our approach along with the state-of-the-art methods for object recognition on Washington RGB-D object benchmark. Our approach greatly improves the previous state-of-the-art results for both of RGB and depth modalities with a margin of  and , respectively. As for the combined RGB-D results, our approach surpasses all the other methods except that of \cite{Loghmani_RAL_2019}, which is slightly better than ours (). These results emphasize the importance of deep features in a unified framework based on the incorporation of CNNs and random RNNs. 


We also present average accuracy performance of individual object categories on the 10 evaluation splits of Washinton RGB-D Object dataset using the best-performing structure, ResNet101-RNN. As shown in Fig. \ref{fig:wrgbdIndividualResults}, our approach is highly accurate in recognition of the most of the object categories. Categories with lower accuray results are \textit{mushroom}, \textit{peach}, and \textit{pitcher}. The common reason that leads to the lower performance in these categories seems to be due to their less number of instances. In particular, these categories have only  instances, which is the minimum number for any category in the dataset. Considering the other categories with up to  instances, this imbalance of the data may have biased the learning to favor of categories with more examples. Moreover, the accuracy of our combined RGB and depth based on weighted confidences of modalities reflects that the fusion of RGB and depth data in this way can provide strong discrimination capability for object categories.

\subsection{Scene Recognition Performance} \label{sec:exp.sceneRecognition}
To test the generalization ability of our approach, we also carry out comparative analysis of our best-performing model, namely ResNet101-RNN, on SUN RGB-D Scene \cite{Song_CVPR_2015} dataset for scene recognition as a more challenging task of scene understanding. To this end, we first apply ResNet101 pretrained model without finetuning, namely Fixed ResNet101-RNN, for both of RGB and depth modalities. Then, we finetune the pretrained CNN model on SUN RGB-D Scene dataset using the same hyper-parameters of object recognition task (see Sec. \ref{sec.exp.ma.finetuning}). The results of these experiments together with the-state-of-the-art results on this dataset are reported in Table \ref{table:sunrgbdResults}. Our best system outperforms the-state-of-the-art methods for all of the data types with impressive improvement of , , and  for RGB, depth, and RGB-D, respectively, over the previous best performing results. It is worth mentioning that we use the pretrained CNN model on object-centric dataset of ImageNet \cite{Deng_Imagenet_CVPR_2009}, which is less commonly used for scene recognition task than the pretrained models on scene-centric datasets such as Places \cite{Zhou_NIPS_2014}. Nevertheless, our approach outperforms existing state-of-the-art methods for RGB-D scene recognition task. Moreoever, it is interesting that our system even with fixed pretrained CNN model is already discriminative enough and achieves impressive accuracy performances. Contrary to our findings on Washington RGB-D Object dataset, finetuning provides much better results not only for depth domain but also for the RGB domain as well. This is what we expect as scene recognition is a cross-domain task for our approach that has the pretrained CNN model of the object-centric ImageNet as the backbone. Specifically, finetuning on depth data boosts the accuracy greatly by providing both domain and modality adaptation.

\begin{table}\caption{Accuracy comparison of our approach with the related methods on SUN RGB-D Scene dataset (\%). \textcolor{red}{Red:} Best result, \textcolor{blue}{Blue:} Second best result, \textcolor{darkgreen}{Green:} Third best result.}
	\begin{center}
		\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
		\begin{adjustbox}{width=\columnwidth}
		\begin{tabular}{ lccc }
			\hline
			Method 											& RGB 				& Depth 			& RGB-D \\ \hline \hline
			Places CNN-Lin SVM \cite{Zhou_NIPS_2014}    	& 35.6 				& 25.5 	 			& 37.2 		\\ Places CNN-RBF SVM \cite{Zhou_NIPS_2014}    	& 38.1 				& 27.7 	 			& 39.0 		\\ SS-CNN-R6 \cite{Liao_ICRA_2016}    				& 36.1 				& - 	 			& 41.3 		\\ DMFF \cite{Zhu_CVPR_2016}    					& 37.0  			& - 				& 41.5 		\\ Places CNN-RCNN \cite{Wang_CVPR_2016}         	& 40.4 	           	& 36.3 	        	& 48.1 		\\ MSMM \cite{Song_IJCAI_2017}         			& 41.5 	           	& 40.1 	        	& 52.3 		\\ RGB-D-CNN \cite{Song_AAAI_2017}         		& 42.7 	           	& \bftab\textcolor{darkgreen}{42.4} 	        	& 52.4 		\\ MDSI-CNN \cite{Asif_TPAMI_2018}         		& 39.6 	           	& 35.2 	        	& 45.2 		\\ DF\textsuperscript{2}Net \cite{Li_AAAI_2018df}  & - 	           	& - 	        	& \bftab\textcolor{darkgreen}{54.6} 		\\ HP-CNN-T \cite{Zaki_AuotRobots_2019}         	& 38.8 	           	& 28.5 	        	& 42.2 		\\ RGB-D-OB \cite{Song_TIP_2019}         			& - 	           	& \bftab\textcolor{darkgreen}{42.4} 	        	& 53.8 		\\ G-L-SOOR \cite{Song_TIP_2020}         			& \bftab\textcolor{darkgreen}{50.5} 	           	& \bftab\textcolor{blue}{44.1} 	        	& \bftab\textcolor{blue}{55.5} 		\\ \hline
			\bftab{This work - Fix ResNet101-RNN}           & \bftab\textcolor{blue}{50.8} 				& 38.6 				& 53.1 		\\ \bftab{This work - Finetuned ResNet101-RNN}  	& \bftab\textcolor{red}{58.5}  			& \bftab\textcolor{red}{50.1}  			& \bftab\textcolor{red}{60.7} 	\\ \hline
		\end{tabular}
		\end{adjustbox}
		\label{table:sunrgbdResults}
	\end{center}
\end{table}

\begin{figure}[!b]
	\begin{center}
		\includegraphics[width=\columnwidth, keepaspectratio]{sunrgbd_confusion_matrix.eps}
	\end{center}
	\caption{RGB-D confusion matrix of ResNet101-RNN on SUN RGB-D Scene dataset (best viewed with magnification).}
	\label{fig:sunrgbdConfusionMatrix}
\end{figure}

Fig. \ref{fig:sunrgbdConfusionMatrix} shows the confusion matrix of our approach with fine-tuning over the  categories of SUN RGB-D Scene dataset for RGB-D. The matrix demonstrates the degree of confusion between pairs of scene categories and implies the similarity between scenes on this dataset. The largest misclassification errors happen to be between extremely similar scene categories such as \textit{computer room} - \textit{office}, \textit{conference room}-\textit{classroom}, \textit{discussion area}-\textit{rest space}, \textit{lecture theatre}-\textit{classroom}, \textit{study space}-\textit{classroom}, \textit{lab}-\textit{office}, etc. In addition to the inter-class similarity, other reasons for poor performance might be intra-class variations of the scenes and lack of getting enough representative knowledge transfer from the ImageNet models. 

\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=0.95\columnwidth, keepaspectratio]{confusedScenes.pdf}
	\end{center}
	\caption{Top-5 RGB-D predictions of our system using sample test images of frequently confused scene categories on SUN RGB-D Scene dataset.}
	\label{fig:sunrgbdConfusedSamples}
\end{figure}

\begin{table}[!h]
	\caption{Scene recognition accuracy of top-1, top-3, and top-5 on SUN RGB-D Scene dataset (\%).}
	\begin{center}
		\setlength{\tabcolsep}{0.9em} \def\arraystretch{1.2}
		\begin{adjustbox}{width=0.65\columnwidth}
		\begin{tabular}{ lccc }
			\hline
			Accuracy			& RGB 				& Depth 			& RGB-D \\ \hline \hline
			top-1    			& 58.5 				& 50.1 	 			& 60.7 		\\ top-3    			& 81.0 				& 71.5 	 			& 83.6 		\\ top-5    			& 88.5 				& 80.9 	 			& 89.9 		\\ \hline
		\end{tabular}
		\end{adjustbox}
		\label{table:top135sceneResults}
	\end{center}
\end{table}

To further analyse the performance of our system, we give top-3 and top-5 classification accuracy together with top-1 results as in Table \ref{table:top135sceneResults}. While the top-1 accuracy shows the percentage of test images that exactly matches with the predicted classes, the top-3 and top-5 indicates the percentage of test images that are among the top ranked 3 and 5 predictions, respectively. The top-3 and top-5 results demonstrate the effectiveness of our system more closely by overcoming ambiguity among scene categories greatly. Fig. \ref{fig:sunrgbdConfusedSamples} depicts some test examples of scene categories confused with each other frequently on SUN RGB-D Scene dataset. As shown in the figure, these scene categories have similar appearances that make them hard to distinguish even for a human expert without sufficient context knowledge in the evaluation. Nevertheless, our approach is able to identify scene category labels among the top-3 and top-5 predictions with high accuracy.


\subsection{Discussion} \label{sec:exp.discussion}
Our framework presents an effective solution for deep feature extraction in an efficient way by integrating a pretrained CNN model with random weights based RNNs. Randomization throughout our RNN-Stage raises the question of whether the results are stable enough. The carefully implemented experiments in Sec. \ref{sec.exp.ma.randomness} are an empirical justification for the stability of random weights. On the other hand, our multi-level analysis shows that the optimum performance gain from a single level always comes from an intermediate level for all the models with/without finetuning for both of RGB and depth modalities. The only exception is in the use of finetuned DenseNet-121 model on depth data. This is an interesting finding, because one expects better representation capabilities of final layers, especially in the use of finetuned models. Yet, as expected, performance generally increases from the first level to the last level throughout the networks when the underlying CNN models are finetuned. Since Washington RGB-D Object \cite{Lai_ICRA_2011} dataset includes a subset of object categories in the ImageNet \cite{Deng_Imagenet_CVPR_2009}, finetuning does not improve accuracy success on RGB data. In contrast, accuracy gain is significant due to the need for domain adaptation in depth data. This also shows that using an appropriate technique to handle depth data as in our approach (Sec. \ref{sec:dataPreparation}), leads impressive performance improvement by knowledge transfer between modalities. 

In this study, although we have explored different techniques to fuse representations of multiple levels to further increase the classification success, a single optimum level may actually be sufficient enough for many tasks. In this way, especially for tasks where computational time is more critical, results can be obtained much faster without sacrificing accuracy success. Another point of interest is that the data imbalance in Washington RGB-D Object dataset results in poor performance for the individual categories with less instances and consequently leads to a drop in the overall success of the system. Hence, this imbalance might be overcome by applying data augmentation on the categories with less instances. However, it is worth to note that we do not perform any data augmentation in this study for both tasks.

The success of our approach for RGB-D scene recognition confirms the generalization ability of the proposed framework. Unlike object recognition, when the underlying CNN models are finetuned, success in both RGB and depth modalities increases significantly in scene recognition task. This is due to the need for cross-domain task adaptation of object-centric based pretrained models. Therefore, similar findings in object recognition could be observed if scene-centric based pretrained models are employed for scene recognition (e. g. Places \cite{Zhou_NIPS_2014}). Moreover, such pretrained models could improve the results further with our framework. Another potential that could improve the success for scene recognition is embedding contextual knowledge by jointly employing attention mechanism such as \cite{Fukui_2019_CVPR} in our structure.

 


\section{Conclusion}
\label{sec:conclusion}
In this paper, we have presented a framework that incorporates pretrained CNN models together with multiple random recursive neural networks. The proposed approach greatly improves RGB-D object and scene recognition performances over the-state-of-the-art results in the literature on the widely used Washington RGB-D Object and SUN RGB-D Scene datasets. The proposed randomized pooling schema allows us to deal with high-dimensional activations of CNN models effectively. The extensive experimental analysis of various parameters and setup properties show that the incorporation of multiple random RNNs with a pretrained CNN model provides a robust and effective general solution for both of RGB-D object and scene recognition tasks. Utilizing depth data by mapping it into RGB-like image domain allows knowledge transfer from RGB pretrained CNN models effectively. The generic design and the generalization capability of the proposed framework allow to utilize it for other visual recognition tasks. Thus, we will open our code along with models to the community in order to help future studies. 












\section*{Acknowledgment}

This paper is based on the results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO). 

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



\begin{thebibliography}{10}

\bibitem{Razavian_CVPRW_2014}
A.~Sharif~Razavian, H.~Azizpour, J.~Sullivan, and S.~Carlsson, ``Cnn features
  off-the-shelf: an astounding baseline for recognition,'' in \emph{Proceedings
  of the IEEE conference on computer vision and pattern recognition workshops},
  2014, pp. 806--813.

\bibitem{Schwarz_ICRA_2015}
M.~Schwarz, H.~Schulz, and S.~Behnke, ``Rgb-d object recognition and pose
  estimation based on pre-trained convolutional neural network features,'' in
  \emph{Robotics and Automation (ICRA), 2015 IEEE International Conference
  on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2015, pp. 1329--1335.

\bibitem{Liao_ICRA_2016}
Y.~Liao, S.~Kodagoda, Y.~Wang, L.~Shi, and Y.~Liu, ``Understand scene
  categories by objects: A semantic regularized scene classifier using
  convolutional neural networks,'' in \emph{2016 IEEE international conference
  on robotics and automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2016, pp. 2318--2325.

\bibitem{Song_TIP_2019}
X.~Song, S.~Jiang, L.~Herranz, and C.~Chen, ``Learning effective rgb-d
  representations for scene recognition,'' \emph{IEEE Transactions on Image
  Processing}, vol.~28, no.~2, pp. 980--993, 2019.

\bibitem{Girshick_CVPR_2014}
R.~Girshick, J.~Donahue, T.~Darrell, and J.~Malik, ``Rich feature hierarchies
  for accurate object detection and semantic segmentation,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2014, pp. 580--587.

\bibitem{Sermanet_ICLR_2014}
P.~Sermanet, D.~Eigen, X.~Zhang, M.~Mathieu, R.~Fergus, and Y.~LeCun,
  ``Overfeat: Integrated recognition, localization and detection using
  convolutional networks,'' in \emph{International Conference on Learning
  Representations (ICLR)}, 2014.

\bibitem{Farabet_TPAMI_2013}
C.~Farabet, C.~Couprie, L.~Najman, and Y.~LeCun, ``Learning hierarchical
  features for scene labeling,'' \emph{IEEE transactions on pattern analysis
  and machine intelligence}, vol.~35, no.~8, pp. 1915--1929, 2013.

\bibitem{Yosinski_NIPS_2014}
J.~Yosinski, J.~Clune, Y.~Bengio, and H.~Lipson, ``How transferable are
  features in deep neural networks?'' in \emph{Advances in neural information
  processing systems}, 2014, pp. 3320--3328.

\bibitem{Hariharan_CVPR_2015}
B.~Hariharan, P.~Arbel{\'a}ez, R.~Girshick, and J.~Malik, ``Hypercolumns for
  object segmentation and fine-grained localization,'' in \emph{Proceedings of
  the IEEE conference on computer vision and pattern recognition}, 2015, pp.
  447--456.

\bibitem{Zaki_ICRA_2016}
H.~F. Zaki, F.~Shafait, and A.~Mian, ``Convolutional hypercube pyramid for
  accurate rgb-d object category and instance recognition,'' in \emph{Robotics
  and Automation (ICRA), 2016 IEEE International Conference on}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2016, pp. 1685--1692.

\bibitem{Socher_NIPS_2012}
R.~Socher, B.~Huval, B.~Bath, C.~D. Manning, and A.~Y. Ng,
  ``Convolutional-recursive deep learning for 3d object classification,'' in
  \emph{Advances in neural information processing systems}, 2012, pp. 656--664.

\bibitem{Lai_ICRA_2011}
K.~Lai, L.~Bo, X.~Ren, and D.~Fox, ``A large-scale hierarchical multi-view
  rgb-d object dataset,'' in \emph{Robotics and Automation (ICRA), 2011 IEEE
  International Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2011, pp. 1817--1824.

\bibitem{Song_CVPR_2015}
S.~Song, S.~P. Lichtenberg, and J.~Xiao, ``Sun rgb-d: A rgb-d scene
  understanding benchmark suite,'' in \emph{The IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, June 2015.

\bibitem{Caglayan_ECCVW_2018}
A.~Caglayan and A.~Burak~Can, ``Exploiting multi-layer features using a cnn-rnn
  approach for rgb-d object recognition,'' in \emph{The European Conference on
  Computer Vision (ECCV) Workshops}, September 2018.

\bibitem{Krizhevsky_NIPS_2012}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012, pp. 1097--1105.

\bibitem{Simonyan_ICLR_2015}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' in \emph{International Conference on
  Learning Representations (ICLR)}, 2015.

\bibitem{He_CVPR_2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2016.

\bibitem{Huang_CVPR_2017}
G.~Huang, Z.~Liu, K.~Q. Weinberger, and L.~van~der Maaten, ``Densely connected
  convolutional networks,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, vol.~1, no.~2, 2017, p.~3.

\bibitem{Guclu_CVPRW_2019}
O.~Guclu, A.~Caglayan, and A.~Burak~Can, ``Rgb-d indoor mapping using deep
  features,'' in \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops}, June 2019.

\bibitem{DepthKernel2011}
L.~Bo, X.~Ren, and D.~Fox, ``Depth kernel descriptors for object recognition,''
  in \emph{2011 IEEE/RSJ International Conference on Intelligent Robots and
  Systems}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 821--826.

\bibitem{HKD2011}
L.~Bo, K.~Lai, X.~Ren, and D.~Fox, ``Object recognition with hierarchical
  kernel descriptors,'' in \emph{Computer Vision and Pattern Recognition
  (CVPR), 2011 IEEE Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2011, pp. 1729--1736.

\bibitem{HONV2012}
S.~Tang, X.~Wang, X.~Lv, T.~X. Han, J.~Keller, Z.~He, M.~Skubic, and S.~Lao,
  ``Histogram of oriented normal vectors for object recognition with a depth
  sensor,'' in \emph{Asian conference on computer vision}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2012, pp. 525--538.

\bibitem{Wang_2015_ICCV}
A.~Wang, J.~Cai, J.~Lu, and T.-J. Cham, ``Mmss: Multi-modal sharable and
  specific feature learning for rgb-d object recognition,'' in \emph{The IEEE
  International Conference on Computer Vision (ICCV)}, December 2015.

\bibitem{Wang_2015_IEEE_ToM}
A.~{Wang}, J.~{Lu}, J.~{Cai}, T.~{Cham}, and G.~{Wang}, ``Large-margin
  multi-modal deep learning for rgb-d object recognition,'' \emph{IEEE
  Transactions on Multimedia}, vol.~17, no.~11, pp. 1887--1898, Nov 2015.

\bibitem{Li_AAAI_2018df}
Y.~Li, J.~Zhang, Y.~Cheng, K.~Huang, and T.~Tan, ``Df2net: Discriminative
  feature learning and fusion network for rgb-d indoor scene classification,''
  in \emph{Thirty-Second AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{Rahman_ICME_2017}
M.~M. Rahman, Y.~Tan, J.~Xue, and K.~Lu, ``Rgb-d object recognition with
  multimodal deep convolutional neural networks,'' in \emph{2017 IEEE
  International Conference on Multimedia and Expo (ICME)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2017, pp. 991--996.

\bibitem{Zia_ICCVW_2017}
S.~Zia, B.~Yuksel, D.~Yuret, and Y.~Yemez, ``Rgb-d object recognition using
  deep convolutional neural networks,'' in \emph{2017 IEEE International
  Conference on Computer Vision Workshop (ICCVW)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. 887--894.

\bibitem{Cheng_3DV_2015}
Y.~Cheng, R.~Cai, X.~Zhao, and K.~Huang, ``Convolutional fisher kernels for
  rgb-d object recognition,'' in \emph{3D Vision (3DV), 2015 International
  Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2015, pp.
  135--143.

\bibitem{Wang_TIP_2017}
L.~Wang, S.~Guo, W.~Huang, Y.~Xiong, and Y.~Qiao, ``Knowledge guided
  disambiguation for large-scale scene classification with multi-resolution
  cnns,'' \emph{IEEE Transactions on Image Processing}, vol.~26, no.~4, pp.
  2055--2068, 2017.

\bibitem{Oquab_CVPR_2014}
M.~Oquab, L.~Bottou, I.~Laptev, and J.~Sivic, ``Learning and transferring
  mid-level image representations using convolutional neural networks,'' in
  \emph{Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference
  on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2014, pp. 1717--1724.

\bibitem{Azizpour_CVPRW_2015}
H.~Azizpour, A.~Sharif~Razavian, J.~Sullivan, A.~Maki, and S.~Carlsson, ``From
  generic to specific deep representations for visual recognition,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition workshops}, 2015, pp. 36--45.

\bibitem{Azizpour_TPAMI_2015}
H.~Azizpour, A.~S. Razavian, J.~Sullivan, A.~Maki, and S.~Carlsson, ``Factors
  of transferability for a generic convnet representation,'' \emph{IEEE
  transactions on pattern analysis and machine intelligence}, vol.~38, no.~9,
  pp. 1790--1802, 2015.

\bibitem{Gupta_ECCV_2014}
S.~Gupta, R.~Girshick, P.~Arbel{\'a}ez, and J.~Malik, ``Learning rich features
  from rgb-d images for object detection and segmentation,'' in \emph{European
  Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2014, pp. 345--360.

\bibitem{Chatfield_BMVC_2014}
K.~Chatfield, K.~Simonyan, A.~Vedaldi, and A.~Zisserman, ``Return of the devil
  in the details: Delving deep into convolutional nets,'' in \emph{British
  Machine Vision Conference (BMVC)}, 2014.

\bibitem{Asif_ICRA_2015}
U.~Asif, M.~Bennamoun, and F.~Sohel, ``Efficient rgb-d object categorization
  using cascaded ensembles of randomized decision trees,'' in \emph{2015 IEEE
  International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2015, pp. 1295--1302.

\bibitem{Asif_ToR_2017}
U.~Asif, M.~Bennamoun, and F.~A. Sohel, ``Rgb-d object recognition and grasp
  detection using hierarchical cascaded forests,'' \emph{IEEE Transactions on
  Robotics}, vol.~33, no.~3, pp. 547--564, 2017.

\bibitem{Carlucci_RAS_2018}
F.~M. Carlucci, P.~Russo, and B.~Caputo, ``(de)co: Deep depth
  colorization,'' \emph{IEEE Robotics and Automation Letters}, vol.~3, no.~3,
  pp. 2386--2393, 2018.

\bibitem{Liu_CVPR_2015}
L.~Liu, C.~Shen, and A.~van~den Hengel, ``The treasure beneath convolutional
  layers: Cross-convolutional-layer pooling for image classification,'' in
  \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2015, pp. 4749--4757.

\bibitem{Zaki_RAS_2017}
H.~F. Zaki, F.~Shafait, and A.~Mian, ``Learning a deeply supervised multi-modal
  rgb-d embedding for semantic scene and object category recognition,''
  \emph{Robotics and Autonomous Systems}, vol.~92, pp. 41--52, 2017.

\bibitem{Song_IJCAI_2017}
X.~Song, S.~Jiang, and L.~Herranz, ``Combining models from multiple sources for
  rgb-d scene recognition,'' in \emph{Proceedings of the Twenty-Sixth
  International Joint Conference on Artificial Intelligence, (IJCAI-17)}, 2017,
  pp. 4523--4529. [Online]. Available:
  \url{https://doi.org/10.24963/ijcai.2017/631}

\bibitem{Yang_ICCV_2015}
S.~Yang and D.~Ramanan, ``Multi-scale recognition with dag-cnns,'' in
  \emph{Proceedings of the IEEE international conference on computer vision},
  2015, pp. 1215--1223.

\bibitem{Zaki_AuotRobots_2019}
H.~F. Zaki, F.~Shafait, and A.~Mian, ``Viewpoint invariant semantic object and
  scene categorization with rgb-d sensors,'' \emph{Autonomous Robots}, vol.~43,
  no.~4, pp. 1005--1022, 2019.

\bibitem{Loghmani_RAL_2019}
M.~R. Loghmani, M.~Planamente, B.~Caputo, and M.~Vincze, ``Recurrent
  convolutional fusion for rgb-d object recognition,'' \emph{IEEE Robotics and
  Automation Letters}, vol.~4, no.~3, pp. 2878--2885, 2019.

\bibitem{Cho_EMNLP_2014}
K.~Cho, B.~Van~Merri{\"e}nboer, C.~Gulcehre, D.~Bahdanau, F.~Bougares,
  H.~Schwenk, and Y.~Bengio, ``Learning phrase representations using rnn
  encoder-decoder for statistical machine translation,'' in \emph{Proceedings
  of the 2014 Conference on Empirical Methods in Natural Language Processing
  (EMNLP)}, 2014, pp. 1724--1734.

\bibitem{Asif_TPAMI_2018}
U.~Asif, M.~Bennamoun, and F.~A. Sohel, ``A multi-modal, discriminative and
  spatially invariant cnn for rgb-d object labeling,'' \emph{IEEE Transactions
  on Pattern Analysis and Machine Intelligence}, vol.~40, no.~9, pp.
  2051--2065, 2018.

\bibitem{Eitel_IROS_2015}
A.~Eitel, J.~T. Springenberg, L.~Spinello, M.~Riedmiller, and W.~Burgard,
  ``Multimodal deep learning for robust rgb-d object recognition,'' in
  \emph{Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International
  Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2015, pp.
  681--687.

\bibitem{Tang_TCDS_2019}
L.~Tang, Z.-X. Yang, and K.~Jia, ``Canonical correlation analysis
  regularization: An effective deep multiview learning baseline for rgb-d
  object recognition,'' \emph{IEEE Transactions on Cognitive and Developmental
  Systems}, vol.~11, no.~1, pp. 107--118, 2019.

\bibitem{Schmidt_ICPR_1992}
W.~F. Schmidt, M.~A. Kraaijveld, and R.~P. Duin, ``Feed forward neural networks
  with random weights,'' in \emph{Proceedings of the 11th IAPR International
  Conference on Pattern Recognition}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 1992, pp. 1--4.

\bibitem{Pao_Computer_1992}
Y.-H. Pao and Y.~Takefuji, ``Functional-link net computing: theory, system
  architecture, and functionalities,'' \emph{Computer}, vol.~25, no.~5, pp.
  76--79, 1992.

\bibitem{Pao_Neurocomp_1994}
Y.-H. Pao, G.-H. Park, and D.~J. Sobajic, ``Learning and generalization
  characteristics of the random vector functional-link net,''
  \emph{Neurocomputing}, vol.~6, no.~2, pp. 163--180, 1994.

\bibitem{Igelnik_Pao_TNN_1995}
B.~Igelnik and Y.-H. Pao, ``Stochastic choice of basis functions in adaptive
  function approximation and the functional-link net,'' \emph{IEEE Transactions
  on Neural Networks}, vol.~6, no.~6, pp. 1320--1329, 1995.

\bibitem{Huang_TNN_2006}
G.-B. Huang, L.~Chen, C.~K. Siew \emph{et~al.}, ``Universal approximation using
  incremental constructive feedforward networks with random hidden nodes,''
  \emph{IEEE Trans. Neural Networks}, vol.~17, no.~4, pp. 879--892, 2006.

\bibitem{Rahimi_NIPS_2008}
A.~Rahimi and B.~Recht, ``Random features for large-scale kernel machines,'' in
  \emph{Advances in neural information processing systems}, 2008, pp.
  1177--1184.

\bibitem{Rahimi_NIPS_2009}
A.~Rahimi and B.~Recht, ``Weighted sums of random kitchen sinks: Replacing
  minimization with randomization in learning,'' in \emph{Advances in neural
  information processing systems}, 2009, pp. 1313--1320.

\bibitem{Pollack_AI_1990}
J.~B. Pollack, ``Recursive distributed representations,'' \emph{Artificial
  Intelligence}, vol.~46, no. 1-2, pp. 77--105, 1990.

\bibitem{Hinton_AI_1990}
G.~E. Hinton, ``Mapping part-whole hierarchies into connectionist networks,''
  \emph{Artificial Intelligence}, vol.~46, no. 1-2, pp. 47--75, 1990.

\bibitem{Socher_ICML_2011}
R.~Socher, C.~C. Lin, C.~Manning, and A.~Y. Ng, ``Parsing natural scenes and
  natural language with recursive neural networks,'' in \emph{Proceedings of
  the 28th international conference on machine learning (ICML-11)}, 2011, pp.
  129--136.

\bibitem{Kim_CVPR_2016}
J.~Kim, J.~Kwon~Lee, and K.~Mu~Lee, ``Deeply-recursive convolutional network
  for image super-resolution,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, 2016, pp. 1637--1645.

\bibitem{Sharma_NIPS_2014}
A.~Sharma, O.~Tuzel, and M.-Y. Liu, ``Recursive context propagation network for
  semantic scene labeling,'' in \emph{Advances in Neural Information Processing
  Systems}, 2014, pp. 2447--2455.

\bibitem{Bai_Neurocomp_2015}
J.~Bai, Y.~Wu, J.~Zhang, and F.~Chen, ``Subset based deep learning for rgb-d
  object recognition,'' \emph{Neurocomputing}, vol. 165, pp. 280--292, 2015.

\bibitem{Cheng_CVIU_2015}
Y.~Cheng, X.~Zhao, K.~Huang, and T.~Tan, ``Semi-supervised learning and feature
  evaluation for rgb-d object recognition,'' \emph{Computer Vision and Image
  Understanding}, vol. 139, pp. 149--160, 2015.

\bibitem{Bui_Access_2016}
H.~M. Bui, M.~Lech, E.~Cheng, K.~Neville, and I.~S. Burnett, ``Object
  recognition using deep convolutional features transformed by a recursive
  network structure,'' \emph{IEEE Access}, vol.~4, pp. 10\,059--10\,066, 2016.

\bibitem{Zeiler_ICLR_2013}
M.~D. Zeiler and R.~Fergus, ``Stochastic pooling for regularization of deep
  convolutional neural networks,'' in \emph{International Conference on
  Learning Representations (ICLR)}, 2013.

\bibitem{Jarrett_ICCV_2009}
K.~Jarrett, K.~Kavukcuoglu, M.~Ranzato, and Y.~LeCun, ``What is the best
  multi-stage architecture for object recognition?'' in \emph{2009 IEEE 12th
  international conference on computer vision}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2009, pp. 2146--2153.

\bibitem{Deng_Imagenet_CVPR_2009}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{Computer Vision and
  Pattern Recognition, 2009. CVPR 2009. IEEE Conference on}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2009, pp. 248--255.

\bibitem{Bo_IROS_2011}
L.~Bo, X.~Ren, and D.~Fox, ``Depth kernel descriptors for object recognition,''
  in \emph{2011 IEEE/RSJ International Conference on Intelligent Robots and
  Systems}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 821--826.

\bibitem{Vedaldi_Matconvnet_ICM_2015}
A.~Vedaldi and K.~Lenc, ``Matconvnet: Convolutional neural networks for
  matlab,'' in \emph{Proceedings of the 23rd ACM international conference on
  Multimedia}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2015, pp. 689--692.

\bibitem{Saxe_ICML_2011}
A.~M. Saxe, P.~W. Koh, Z.~Chen, M.~Bhand, B.~Suresh, and A.~Y. Ng, ``On random
  weights and unsupervised feature learning.'' in \emph{ICML}, vol.~2, no.~3,
  2011, p.~6.

\bibitem{Rigotti_Frontiers_2010}
M.~Rigotti, D.~D. Ben Dayan~Rubin, X.-J. Wang, and S.~Fusi, ``Internal
  representation of task rules by recurrent dynamics: the importance of the
  diversity of neural responses,'' \emph{Frontiers in computational
  neuroscience}, vol.~4, p.~24, 2010.

\bibitem{Pedregosa_JMLR_2011_scikit}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg \emph{et~al.},
  ``Scikit-learn: Machine learning in python,'' \emph{Journal of machine
  learning research}, vol.~12, no. Oct, pp. 2825--2830, 2011.

\bibitem{Zeiler_ECCV_2014}
M.~D. Zeiler and R.~Fergus, ``Visualizing and understanding convolutional
  networks,'' in \emph{European conference on computer vision}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2014, pp. 818--833.

\bibitem{Liu_Neurocomp_2018}
H.~Liu, F.~Li, X.~Xu, and F.~Sun, ``Multi-modal local receptive field extreme
  learning machine for object recognition,'' \emph{Neurocomputing}, vol. 277,
  pp. 4--11, 2018.

\bibitem{Zhou_NIPS_2014}
B.~Zhou, A.~Lapedriza, J.~Xiao, A.~Torralba, and A.~Oliva, ``Learning deep
  features for scene recognition using places database,'' in \emph{Advances in
  neural information processing systems}, 2014, pp. 487--495.

\bibitem{Zhu_CVPR_2016}
H.~Zhu, J.-B. Weibel, and S.~Lu, ``Discriminative multi-modal feature fusion
  for rgbd indoor scene recognition,'' in \emph{The IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, June 2016.

\bibitem{Wang_CVPR_2016}
A.~Wang, J.~Cai, J.~Lu, and T.-J. Cham, ``Modality and component aware feature
  fusion for rgb-d scene classification,'' in \emph{The IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, June 2016.

\bibitem{Song_AAAI_2017}
X.~Song, L.~Herranz, and S.~Jiang, ``Depth cnns for rgb-d scene recognition:
  Learning from scratch better than transferring from rgb-cnns,'' in
  \emph{Thirty-First AAAI Conference on Artificial Intelligence}, 2017.

\bibitem{Song_TIP_2020}
X.~Song, S.~Jiang, B.~Wang, C.~Chen, and G.~Chen, ``Image representations with
  spatial object-to-object relations for rgb-d scene recognition,'' \emph{IEEE
  Transactions on Image Processing}, vol.~29, pp. 525--537, 2020.

\bibitem{Fukui_2019_CVPR}
H.~Fukui, T.~Hirakawa, T.~Yamashita, and H.~Fujiyoshi, ``Attention branch
  network: Learning of attention mechanism for visual explanation,'' in
  \emph{The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  June 2019.

\end{thebibliography}


\begin{IEEEbiographynophoto}{Ali Caglayan}
is a research scientist at Artificial Intelligence Research Center of AIST in Tokyo, Japan. He received the B.Sc. and Ph.D. degrees from the Department of Computer Engineering, Hacettepe University in 2009 and 2018, respectively. His research is in the area of computer vision and machine learning, with a particular focus on 2D/3D scene analysis and understanding.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Nevrez Imamoglu}
is currently employed as a Senior Researcher at Artificial Intelligence Research Center (AIRC), AIST, Tokyo, Japan since April 2016. Before AIST, he was with RIKEN Brain Science Institute as a Researcher \& JSPS Foreign Postdoctoral Fellow. He received the PhD degree from Chiba University (2015), Japan, with a Japanese government scholarship (MEXT). He was also with School of Computer Eng., NTU, Singapore as Research Associate in 2010-2011. His research interests include applications of computer vision and machine learning, recently with special interests on deep learning for multi-modal or multi-temporal signal/image analysis, visual attention (saliency detection), classification/object detection/segmentation on various sensory data (3D, optical, multispectral, hyperspectral).
\end{IEEEbiographynophoto}



\begin{IEEEbiographynophoto}{Ahmet Burak Can}
received the B.Sc. and M.Sc. degrees from the Department of Computer Engineering, Hacettepe University, in 1998 and 2001, respectively, and the Ph.D. degree from the Department of Computer Science, Purdue University, in 2007. He has been with Hacettepe University since 1998. His primary research interests include computer vision, information security, and distributed systems.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Ryosuke Nakamura}
received the PhD degree from Kobe University in 1996 and worked at Japan Aerospace Exploration Agency (JAXA) to develop space-borne sensors and the ground-based data analysis system. He is currently at AIRC, AIST as the Team Leader of the Geoinformation Science Research Team. His current field of interests are Satellite Remote Sensing, Virtual 3D/4D models and their applications for smart city or smart farms. His team works on and develops intelligent and effective analysis engines to handle rapidly growing geoinformation, such as satellite imagery and aerial photo, to meet scientific and social demand.
\end{IEEEbiographynophoto}
 









\end{document}
