\documentclass[11pt,draftcls,onecolumn]{IEEEtran}

\usepackage{amsbsy,epsfig,epsf,url,citesort}

\usepackage{graphicx}
\usepackage{amssymb,amsmath,dsfont,amsfonts}
\usepackage{epstopdf}
\usepackage{cite}
\usepackage{color}
\usepackage{boxedminipage}
\usepackage{listings}
\usepackage{subfigure}
\usepackage{caption2}
\usepackage{float}
\usepackage{cases}


\title{Technical Report: Observability \\with Random Observations}

\author{Borhan M. Sanandaj Michael B. Waki and Tyrone L. Vincen\thanks{\textsuperscript{}B. M. Sanandaji is with the Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley, CA 94720, USA. Email: sanandaji@eecs.berkeley.edu.~\textsuperscript{}M. B. Wakin and T. L. Vincent are with the Department of Electrical Engineering and Computer Science at the Colorado School of Mines, Golden, CO 80401, USA. Email: \{mwakin, tvincent\}@mines.edu. Preliminary versions of portions of this work appeared in~\cite{wakin2010observability}. This work was partially supported by AFOSR Grant FA9550-09-1-0465, NSF Grant CCF-0830320, DARPA Grant HR0011-08-1-0078, and NSF Grant CNS-0931748.}
}

\def\real    { \mathbb{R} }
\def\sphere    { \mathbb{S} }


\newcommand{\qed}{{\unskip\nobreak\hfil\penalty50\hskip2em\vadjust{}
           \nobreak\hfil\parfillskip=0pt\finalhyphendemerits=0\par}}


\newtheorem{thm}{Theorem}[section] \newtheorem{prop}{Proposition}[section]
\newtheorem{remark}{Remark}[section]
\renewcommand{\b}{\mathbf b}

\newtheorem{theorem}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\newcommand{\eps}{\epsilon}
\newcommand{\bitem}{\begin{itemize}}
\newcommand{\eitem}{\end{itemize}}
\newcommand{\goto}{\rightarrow}
\newcommand{\poiss}{\mathrm{Poisson}}
\newcommand{\mexp}{\mathrm{Exp}}
\newcommand{\margmax}{\mathrm{argmax}}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\mmax}{\mathrm{max}}
\newcommand{\mmin}{\mathrm{min}}
\newcommand{\msup}{\mathrm{sup}}
\newcommand{\minf}{\mathrm{inf}}
\newcommand{\sgn}{\mbox{sgn}}
\newcommand{\beqn}{}
\newcommand{\balign}{}
\newcommand{\lam}{\lambda}

\def\ss{\vspace*{-3mm}}



\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\nullspace}[1]{\mathcal{N}( #1 )}

\newcommand{\Prob}[1]{{\bf P}\left\{#1\right\}}
\newcommand{\E}[1]{{\bf E}\left[#1\right]}
\newcommand{\var}[1]{\,\mbox{var}\left(#1\right)}
\newcommand{\cov}[1]{\,\mbox{cov}\left(#1\right)}

\def \tnot {{T_0}}
\def \tnotc {{T_0^\mathrm{c}}}
\def \tc {{T^\mathrm{c}}}

\def \ok {{\mathcal{O}_{\Omega}}}
\def \ck {{\mathcal{C}_{\Omega}}}
\def \ak {{\mathcal{A}_{\Omega}}}
\def \km {{\widetilde{M}}}
\def \kn {{\widetilde{N}}}
\def \mr {{\widetilde{\mathcal{O}_{\widetilde{m}}}}}
\def \mrp {{\widetilde{\mathcal{O}^{\prime}_{\widetilde{m}}}}}
\def \ckp {{\mathcal{O}_{\Omega}^{\prime}}}
\def \ckpp {{\mathcal{C}_{\Omega}^{\prime}}}

\newcommand{\prf}[1]{{\bf Proof} \, #1 \hfill }
\newcommand{\new}[1]{{\color{red} #1}}
\newcommand{\newBorhan}[1]{{\color{blue} #1}}
\newcommand{\cut}[1]{}

\newcommand{\note}[1]{{\bf\color{black} [{\em Note:} #1]}}
\newcommand{\todo}[1]{{\bf\color{red} [{\em To do:} #1]}}
\newcommand{\edit}[1]{{\bf\color{blue} [{\em Revised:} #1]}}


\newcommand{\bld}[1]{\textbf{#1}}
\newcommand{\vc}[1]{\boldsymbol{#1}}


\def\real    { \mathbb{R} }
\def\comp    { \mathbb{C} }

\usepackage{acronym}
\acrodef{i.i.d.}{independent and identically distributed}
\acrodef{LTI}{Linear Time-Invariant}
\acrodef{RIP}{Restricted Isometry Property}
\acrodef{SVD}{Singular Value Decomposition}
\acrodef{CS}{Compressive Sensing}
\acrodef{DSP}{Digital Signal Processing}

\acrodef{FIR}{Finite Impulse Response}
\acrodef{LTI}{linear time-invariant}
\acrodef{DFT}{Discrete Fourier Transform}
\acrodef{JL}{Johnson-Lindenstrauss}
\acrodef{ROC}{Receiver Operating Curve}
\acrodef{NP}{Neyman-Pearson}
\acrodef{CoM}{Concentration of Measure}
\acrodef{CSI}{Compressive System Identification}
\acrodef{CBD}{Compressive Binary Detection}



\begin{document}

\maketitle

\begin{abstract}

Recovery of the initial state of a high-dimensional system can require a large number of measurements. In this paper, we explain how this burden can be significantly reduced when randomized measurement operators are employed. Our work builds upon recent results from Compressive Sensing (CS). In particular, we make the connection to CS analysis for random
block diagonal matrices.
By deriving Concentration of Measure (CoM) inequalities, we show that the observability matrix satisfies the Restricted Isometry Property (RIP) (a sufficient condition for stable recovery of sparse vectors) under certain conditions on the state transition matrix. For example, we show that if the state transition matrix is unitary, and if independent, randomly-populated measurement matrices are employed, then it is possible to uniquely recover a sparse high-dimensional initial state when the total number of measurements scales \emph{linearly} in the sparsity level (the number of non-zero entries) of the initial state and logarithmically in the state dimension. \cut{This is in fact a significant reduction in the sufficient total number of measurement for correct initial state recovery as compared to traditional observability theory.}We further extend our RIP analysis for scaled unitary and symmetric state transition matrices. We support our analysis with a case study of a two-dimensional diffusion process.
\end{abstract}



\begin{keywords}
Observability, Restricted Isometry Property, Concentration of Measure Inequalities, Block Diagonal Matrices, Compressive Sensing
\end{keywords}




\section{Introduction}
\label{sec:intro}

In this paper, we consider the problem of recovering the initial state of a high-dimensional system from
compressive measurements~(i.e., we take fewer measurements than the system dimension).



\subsection{Measurement Burdens in Observability Theory}

Consider an -dimensional discrete-time linear dynamical system described by the state equation\footnote{The results of this paper also apply directly to systems described by a state equation of the form
1mm]
\vc{y}_k &=& C_k \vc{x}_k+ D\vc{u}_k,
\end{array}

\begin{array}{rcl}
\vc{x}_k &=& A \vc{x}_{k-1} \
where  represents the state vector at time ,  represents the state transition matrix,  represents a set of measurements (or ``observations'') of the state at time , and  represents the measurement matrix at time . (Observe that the number of measurements at each sample time is .)
For any finite set , define the {\em generalized observability matrix} as

where  contains  observation times. Note that this definition extends the traditional definition of the observability matrix by allowing arbitrary time samples in (\ref{eq:ok}) and matches the traditional definition when . The primary use of observability theory is in ensuring that a state (say, an initial state ) can be recovered from a collection of measurements . In particular, defining ,
we have



Although we will consider situations where  changes with each , we first discuss the classical case where  ( is assumed to have full row rank) for all  and  (the observation times are consecutive). In this setting, an important and classical result~\cite{Chen99} states that a system described by the state equation (\ref{eq:sys}) is observable if and only if  has rank  (full column rank) where . One challenge in exploiting this fact is that for some systems,  can be quite large. For example, distributed systems evolving on a spatial domain can have a large state space even after taking a spatially-discretized approximation.
In such settings, we might therefore require a very large total number of measurements ( with ) to identify an initial state, and moreover, inverting the matrix  could be very computationally demanding.

This raises an interesting question: under what circumstances might we be able to infer the initial state of a system when ?
We might imagine, for example, that the measurement burden could be alleviated in cases
when there is a model for the state  that we wish to recover. Alternatively, we may have cases where, rather than needing to recover  from , we desire only to solve a much simpler inference problem such as a binary detection or a classification problem. In this paper, inspired by the emerging theory of Compressive Sensing (CS)~\cite{CompSenDon,CompSampCand,candes2008people}, we explain how such assumptions can indeed reduce the measurement burden and, in some cases, even allow recovery of the initial state when  and the system of equations (\ref{eq:matvec1}) is guaranteed to be underdetermined. More broadly, exploiting CS concepts in the analysis of sparse dynamical systems from limited information has gained much attention over the last few years in applications such as system identification~\cite{ohlsson2010segmentation,toth2011csi,sanandaji2012thesis,shah2012linear}, control feedback design~\cite{zhao2012stability}, and interconnected networks~\cite{sanandaji2011cti,pan2012reconstruction}.


\subsection{Compressive Sensing and Randomized Measurements}

The CS theory states that it is possible to solve certain rank-deficient sets of linear equations by imposing a model assumption on the signal to be recovered. In particular, suppose  where  is an  matrix with . Suppose also that  is -sparse, meaning that only  out of its  entries are non-zero.\footnote{This is easily extended to the case where  is sparse in some transform domain.}
If  satisfies a condition called the~\ac{RIP} of order  for a sufficiently small isometry constant , then it is possible to {\emph{uniquely}} recover any -sparse signal  from the measurements  using a tractable convex optimization program known as -minimization~\cite{CompSenDon,CompSampCand}. The RIP also ensures that the recovery process is robust to noise and stable in cases where  is not precisely sparse~\cite{CandesRIP}.\cut{Similar statements can be made for recovery using various iterative greedy algorithms~\cite{tropp2007signal,needell2009uniform,needell2009cosamp,dai2009subspace}.
}
In the following, we provide the definition of the \ac{RIP}.
\begin{definition}
A matrix  () is said to satisfy the \ac{RIP} of order  with isometry constant  if

holds for all -sparse vectors .
\label{def:rip}
\end{definition}

Observe that the \ac{RIP} is a property of a matrix and has a deterministic definition. However, checking whether the \ac{RIP} holds for a given matrix  is computationally expensive and is almost impossible when  is large.
A common way to establish the \ac{RIP} for  is to populate  with random entries.
If  is populated with \ac{i.i.d.} Gaussian random variables having zero mean and variance , for example, then  will satisfy the \ac{RIP} of order  with isometry constant  with very high probability when  is 
proportional to ~\cite{candes2008people,baraniuk2008simple,davenport2010thesis}.
This result is significant because it indicates that the number of measurements sufficient for correct recovery scales \emph{linearly} in the sparsity level  and only \emph{logarithmically} in the ambient dimension . Other random distributions may also be considered, including matrices with uniform entries of random signs.
Consequently, a number of new sensing hardware architectures, from analog-to-digital converters to digital cameras, are being developed to take advantage of the benefits of random measurements~\cite{duarte2008single,healySPmag,wakin2012non,yoo2012compressed}.

A simple way~\cite{baraniuk2008simple,DeVoreL1IO} of proving the \ac{RIP} for a randomized construction of  involves first showing that the matrix satisfies a \ac{CoM} inequality akin to the following.
\begin{definition}
A random matrix (a matrix whose entries are drawn from a particular probability distribution)  is said to satisfy the
Concentration of Measure (CoM) inequality if for any fixed signal  (not necessarily sparse) and any ,

where  is a positive constant that depends on the isometry constant , and  is some maximum value of the isometry constant for which the CoM inequality holds.
\label{def:concgauss}
\end{definition}

Note that the failure probability in~(\ref{eq:CoM_def}) decays exponentially fast in the number of measurements  times some constant  that depends on the isometry constant . For most interesting random matrices, including matrices populated with \ac{i.i.d.} Gaussian random variables,  is quadratic in  as .

Baraniuk et al.~\cite{baraniuk2008simple} and Mendelson et al.~\cite{mendelson2008uniform} showed that a \ac{CoM} inequality of the form~(\ref{eq:CoM_def}) can be used to prove the \ac{RIP} for random compressive matrices.
This result is rephrased by Davenport~\cite{davenport2010thesis}.
\begin{lemma} {\em \cite{davenport2010thesis}}
Let  denote an -dimensional subspace in .
Let  denote a distortion factor and  denote a failure probability, and suppose  is an  random matrix that satisfies the \ac{CoM} inequality~(\ref{eq:CoM_def}) with 
.
Then with probability at least , for all ,

\label{lem:RIP_based_CoM}
\end{lemma}

Through a union bound argument (see, for example, Theorem~5.2 
in~\cite{baraniuk2008simple}) and by applying Lemma~\ref{lem:RIP_based_CoM} for all  -dimensional subspaces that define the space of -sparse signals in , one can show that  satisfies the \ac{RIP} (of order  and with isometry constant ) with high probability when  scales \emph{linearly} in  and \emph{logarithmically} in .
\cut{
Aside from connections to the \ac{RIP}, concentration inequalities such as the above can also be useful when solving other types of inference problems from compressive measurements. For example, rather than recovering a signal , one may wish only to solve a binary detection problem and determine whether a set of measurements  correspond only to noise (the null hypothesis ) or to signal plus noise (). When  is random, the performance of a compressive detector (and of other multi-signal classifiers) can be studied using concentration inequalities~\cite{davenport2010signal}, and in these settings it is not necessary to assume that  is sparse.
}
\subsection{Observability from Random, Compressive Measurements}

In order to exploit CS concepts in observability analysis, we consider scenarios where the measurement matrices  are populated with random entries. Physically, such randomized measurements may be taken using the types of CS protocols and hardware mentioned above. Our analysis is therefore appropriate in cases where one has some control over the sensing process.

As is apparent from (\ref{eq:ok}), even with randomness in matrices , the observability matrix  contains some structure and cannot be simply modeled as a matrix populated with \ac{i.i.d.} Gaussian random variables and thus, existing results cannot be directly applied.
Our work builds on a recent paper by Park et al.\ in which \ac{CoM} inequalities are derived for random block diagonal matrices~\cite{park2011block}.
Our concentration results cover a large class of systems (not necessarily unitary) and initial states (not necessarily sparse), and apart from guaranteeing recovery of sparse initial states, other inference problems concerning , such as detection or classification of more general initial states and systems, can also be solved using random, compressive measurements, and the performance of such techniques can be studied using the \ac{CoM} bounds that we provide~\cite{davenport2010signal}.

Our \ac{CoM} results have important implications for establishing the \ac{RIP}.~Such \ac{RIP} results provide a sufficient number of measurements for exact initial state recovery when the initial state is known to be sparse a priori. The results of this paper show that under certain conditions on 
(e.g., for unitary, scaled unitary, and certain symmetric matrices ), the observability matrix  will satisfy the
\ac{RIP} with high probability when the total number of
measurements  scales linearly in  and logarithmically in .
Before going into the details of the derivations and proofs, we first state in Section~\ref{sec:rip} our main results on establishing the \ac{RIP} for the observability matrix.
We then present in Section~\ref{sec:com} the \ac{CoM} results upon which the conclusions for establishing the RIP are based.
Finally, in Section~\ref{sec:expt} we support our results with a case study involving a diffusion process starting from a sparse initial state.
\cut{As an example, one could imagine a situation where a few drops of poison have been introduced into particular (i.e., sparse) locations in a lake of water. From the available measurements at later times, we would like to estimate the source of the contamination.
}


\subsection{Related Work}

Questions involving observability in compressive measurement settings have also been raised in a recent paper~\cite{wangCPF} concerned with tracking the state of a system from nonlinear observations.
Due to the intrinsic nature of the problems in that paper, however, the observability issues raised are quite different.
For example, one argument appears to assume that , a requirement that we do not have.
In a recent technical report~\cite{dai2012observability}, Dai et al.\ have also considered a similar sparse initial state recovery problem. However, their approach is different and the results are only applicable in noiseless and perfectly sparse initial state recovery problems. In this paper, we establish the \ac{RIP} for the observability matrix, which implies not only that perfectly sparse initial states can be recovered exactly when the measurements are noiseless but also that the recovery process is robust with respect to noise and that nearly-sparse initial states can be recovered with high accuracy~\cite{CandesRIP}.
Finally, we note that a matrix vaguely similar to the observability matrix has been studied by Yap et al.\ in the context of quantifying the memory capacity of echo state networks~\cite{yap2012echo}.
The recovery results and guarantees presented in this paper are substantially different from the above mentioned papers as we derive \ac{CoM} inequalities for the observability matrix and then establish the \ac{RIP} based on these \ac{CoM} inequalities. 
\cut{Moreover, our results guarantee are stable recovery (recovery in presence of noise and for compressible (not perfectly sparse) initial states). 
}


\section{Restricted Isometry Property and the Observability Matrix}
\label{sec:rip}

When the observability matrix  satisfies the RIP of order  with isometry constant , an initial state with  or fewer non-zero elements can be stably recovered by solving an -minimization problem~\cite{CandesRIP}. (Similar statements can be made for recovery using various iterative greedy algorithms~\cite{tropp2007signal,needell2009uniform,needell2009cosamp,dai2009subspace}.) In this section, we present cases where the total number of measurements sufficient for establishing the RIP scales linearly in  and only logarithmically in the state dimension .
As in standard observability theory, the state transition matrix  plays a crucial role in the analysis. Because the analysis is somewhat complex, results for completely general  are difficult to obtain. However, we present results here for unitary, scaled unitary, and certain symmetric matrices, and we believe that these can give interesting insight into the essential issues driving the initial state recovery problem.

To assist in interpreting the \ac{RIP} results, let us point out that we actually state our \ac{RIP} bounds in terms of the scaled observability matrix  where  is defined below and is chosen to ensure that
the measurements are properly normalized to be compatible with~(\ref{eq:rip}). In noiseless recovery, this scaling is unimportant. When noise occurs, however, the scaling enters into the effective signal to noise ratio for the problem, as the measurements will be more sensitive to noise when  is small.

Our first result, stated in Theorem~\ref{theo:main_RIP_CoM_result_1}, applies to a system with dynamics represented by a scaled unitary matrix when measurements are taken at the first  sample times, starting at zero.


\begin{theorem}
Assume . Suppose that  can be represented as  where  () and  is unitary. Define . Assume each of the measurement matrices  is populated with \ac{i.i.d.} Gaussian random entries with mean zero and variance . Assume all matrices  are generated independently of each other.
Suppose that , , and  are given.
Then with probability exceeding ,  satisfies the~\ac{RIP} of order  with isometry constant  whenever
\begin{numcases}{MK \geq}
\frac{512\left((1-a^2)K+a^2\right)\left(S(\log(\frac{42}{\delta_S})+ 1 + \log(\frac{N}{S})) + \log(\frac{2}{\nu})\right)}{\delta_S^2}, &  \label{eq:res1a}\\
\frac{512\left((1-a^{-2})K+a^{-2}\right)\left(S(\log(\frac{42}{\delta_S})+ 1 + \log(\frac{N}{S})) + \log(\frac{2}{\nu})\right)}{\delta_S^2}, & . \label{eq:res1b}
\end{numcases}
\label{theo:main_RIP_CoM_result_1}
\end{theorem}
{\textbf{Proof}} See Section~\ref{sec:RIP_based_CoM}. \hfill 

One should note that when  (), the
results of Theorem~\ref{theo:main_RIP_CoM_result_1} have a dependency on  (the number of sampling times).
This dependency is not desired in general. When  (i.e.,  is unitary), a result (Theorem~\ref{theo:main_RIP_CoM_result_3}) can be obtained in which the total number of measurements  scales linearly in  and with no dependency on . Our general results for  also indicate that when  is close to the origin (i.e., ), and by symmetry when , worse recovery performance is expected as compared to the case when . When , as an example, the effect of the initial state will be highly attenuated as we take measurements at later times. A similar intuition holds when .
When  (i.e., unitary ), we can relax the consecutive sample times assumption in Theorem~\ref{theo:main_RIP_CoM_result_1} (i.e., ). We have the following \ac{RIP} result when  arbitrarily-chosen samples are taken.

\begin{theorem}
Assume . Suppose that  is unitary.
Assume each of the measurement matrices  is populated with \ac{i.i.d.} Gaussian random entries with mean zero and variance . Assume all matrices  are generated independently of each other.
Suppose that , , and  are given.
Then with probability exceeding ,  satisfies the~\ac{RIP} of order  with isometry constant  whenever

\label{theo:main_RIP_CoM_result_3}
\end{theorem}
{\textbf{Proof}} See Section~\ref{sec:RIP_based_CoM}. \hfill 

Theorem~\ref{theo:main_RIP_CoM_result_3} states that under the assumed conditions,  satisfies the \ac{RIP} of order  with isometry constant  with high probability when the total number of measurements  scales linearly in the sparsity level  and logarithmically in the state ambient dimension .
Consequently under these assumptions, \emph{unique} recovery of any -sparse initial state  is possible from  by solving the -minimization problem or using various iterative greedy algorithms~\cite{tropp2007signal,needell2009cosamp} whenever  is proportional to . This is in fact a significant reduction in the sufficient total number of measurements for correct initial state recovery as compared to traditional observability theory.

We further extend our analysis and establish the \ac{RIP} for certain symmetric matrices . We believe this analysis has important consequences in analyzing problems of practical interest such as 
diffusion (see, for example, Section~\ref{sec:expt}). Suppose  is a positive semidefinite matrix with the eigendecomposition

where  is unitary,  is a diagonal matrix with non-negative entries, , , , and .
The submatrix  contains the  largest eigenvalues of .
The value for  can be chosen as desired; our results below give the strongest bounds when all eigenvalues in  are large compared to all eigenvalues in .
Let  denote the smallest entry of ,  denote the largest entry of ,
and  denote the largest entry of .

In the following, we show that in the special case where the matrix  () happens to itself satisfy the \ac{RIP} (up to a scaling), then  satisfies the \ac{RIP} (up to a scaling). Although there are many state transition matrices  that do not have a collection of eigenvectors  with this special property, we do note that if  is a circulant matrix, its eigenvectors will be the \ac{DFT} basis vectors, and it is known that a randomly selected set of \ac{DFT} basis vectors will satisfy the \ac{RIP} with high probability~\cite{rudelson2008sparse}.

\begin{theorem}
Assume .
Assume  has the eigendecomposition given in~(\ref{eq:decomp1}) and  () satisfies a scaled version\footnote{The 
scaling in~(\ref{eq:rip_U_1^T}) is to account for the unit-norm rows of .} of the \ac{RIP} of order  with isometry constant . Formally, assume for  that

holds for all -sparse .
Assume each of the measurement matrices  is populated with \ac{i.i.d.} Gaussian random entries with mean zero and variance . Assume all matrices  are generated independently of each other.
Let  denote a failure probability and  denote a distortion factor. Then with probability exceeding ,

for all -sparse  whenever

where

and

\label{theo:rip_ok_symmetric_A}
\end{theorem}
{\textbf{Proof} See Appendix A.} \hfill 

The result of Theorem~\ref{theo:rip_ok_symmetric_A} is particularly interesting in applications where the largest eigenvalues of  all cluster around each other and the rest of the eigenvalues cluster around zero. Put formally, we are interested in applications where

The following corollary of Theorem~\ref{theo:rip_ok_symmetric_A} considers an extreme case when  and .
\begin{cor}
Assume .
Assume each of the measurement matrices  is populated with \ac{i.i.d.} Gaussian random entries with mean zero and variance . Assume all matrices  are generated independently of each other.
Suppose  has the eigendecomposition given in~(\ref{eq:decomp1}) and  () satisfies a scaled version of the \ac{RIP} of order  with isometry constant  as given in~(\ref{eq:rip_U_1^T}). Assume  () and .
Let  denote a failure probability and  denote a distortion factor. Define  and .
Then with probability exceeding ,

for all -sparse  whenever
\begin{numcases}{MK \geq}
 \frac{512(1+\delta_S)^2\lambda^{-4(k_{K-1}-k_0)}\left(S\left(\log(\frac{42}{\delta})+ 1 + \log(\frac{N}{S})\right) + \log(\frac{2}{\nu})\right)}{(1-\delta_S)^2\delta^2}, & \\
 \frac{512(1+\delta_S)^2\lambda^{4(k_{K-1}-k_0)}\left(S\left(\log(\frac{42}{\delta})+ 1 + \log(\frac{N}{S})\right) + \log(\frac{2}{\nu})\right)}{(1-\delta_S)^2\delta^2}, & .
\end{numcases}
\label{cor:rip_ok_symmetric_A_extreme}
\end{cor}
{\textbf{Proof}} See Appendix B.\hfill 

While the result of Corollary~\ref{cor:rip_ok_symmetric_A_extreme} is generic and valid for any , an important \ac{RIP} result can be obtained when . The following corollary states the result.

\begin{cor}
Suppose the same notation and assumptions as in Corollary~\ref{cor:rip_ok_symmetric_A_extreme} and additionally assume . Then with probability exceeding ,

for all -sparse  whenever

\label{cor:rip_ok_symmetric_A_extreme_1}
\end{cor}
{\textbf{Proof}} See Appendix B.\hfill 

These results essentially indicate that the more  deviates from one, the more total measurements  are required to ensure unique recovery of any -sparse initial state .
The bounds on  (which we state in Appendix B to derive Corollaries~\ref{cor:rip_ok_symmetric_A_extreme} and \ref{cor:rip_ok_symmetric_A_extreme_1} from Theorem~\ref{theo:rip_ok_symmetric_A}) also indicate that when , the smallest number of measurements are required when the sample times are {\em consecutive} (i.e., when ).
Similar to what we mentioned earlier in our analysis for a scaled unitary , when  the effect of the initial state will be highly attenuated as we take measurements at later times (i.e., when ) which results in a larger total number of measurements  sufficient for exact recovery.


\section{Concentration of Measure Inequalities and the Observability Matrix}
\label{sec:com}
In this section, we derive \ac{CoM} inequalities for the observability matrix when the measurement matrices  are populated with \ac{i.i.d.} Gaussian random entries.
These inequalities are the foundation for establishing the \ac{RIP} presented in the previous section, via Lemma~\ref{lem:RIP_based_CoM}. However, they are also of independent interest for other types of problems involving the states of dynamical systems, such as detection and classification\cite{davenport2010signal,sanandaji2010toeplitz,sanandaji2013com}.
As mentioned earlier, we make a connection to the analysis for block diagonal matrices from~\ac{CS}.
To begin, note that when  we can write

where  and . In this decomposition,  is a block diagonal matrix whose diagonal blocks are the measurements matrices, . We derive \ac{CoM} inequalities for two cases. We first consider the case where all measurement matrices  are generated independently of each other. We then consider the case where all measurement matrices  are the same.
\subsection{Independent Random Measurement Matrices}
\label{sec:indep}

In this section, we assume all matrices  are generated independently of each other.
Focusing just on  for the moment, we have the following bound on its concentration behavior.\footnote{All results in Section~\ref{sec:indep} may be extended to the case where the matrices  are populated with sub-Gaussian random variables, as in~\cite{park2011block}.}

\begin{theorem} {\em \cite{park2011block}}
Assume each of the measurement matrices  is populated with \ac{i.i.d.} Gaussian random entries with mean zero and variance . Assume all matrices  are generated independently of each other.
Let  and define

Then
\begin{numcases}{\Prob{\bigg{|}\|\ck \vc{v}\|_{2}^{2}-\|\vc{v}\|_2^2\bigg{|} > \epsilon\|\vc{v}\|_{2}^{2}} \leq}
 2\exp\{-\frac{M\epsilon^{2}\|\vc{\gamma}\|_{1}^{2}}{256\|\vc{\gamma}\|_{2}^{2}}\}, &   \label{eq:CoM_lower_eps}\\
2\exp\{-\frac{M\epsilon\|\vc{\gamma}\|_{1}}{16\|\vc{\gamma}\|_{\infty}}\}, & ,
\label{eq:CoM_higher_eps}
\end{numcases}
where

\label{thm:block1}
\end{theorem}

As we will be frequently concerned with applications where
 is small, consider the first of the cases given in the right-hand side of the above bound. (It can be shown~\cite{park2011block} that this case always permits any value of  between  and .) Define

and note that for any , . This simply follows from the standard relation that  for all .
The case  is quite favorable because the failure probability will decay exponentially fast in the total number of measurements .
A simple comparison between this result and the \ac{CoM} inequality for a
{\em dense} Gaussian matrix stated in Definition~\ref{def:concgauss} reveals that we get the same degree of concentration from the  block diagonal matrix  as we would get from a dense  matrix populated with \ac{i.i.d.} Gaussian random variables. This event happens if and only if the components  have equal energy, i.e., if and only if

On the other hand, the case   is quite unfavorable and implies that we get the same degree of concentration from the  block diagonal matrix  as we would get from a dense Gaussian matrix having size only . This event happens if and only if  for all  but one . Thus, more uniformity in the values of the  ensures a higher probability of concentration.


We now note that, when applying the observability matrix to an initial state, we will have

This leads us to the following corollary of Theorem~\ref{thm:block1}.
\begin{cor} Suppose the same notation and assumptions as in Theorem~\ref{thm:block1}. Then for any fixed initial state  and for any ,

\label{cor:obs_indep}
\end{cor}

There are two important phenomena to consider in this result, and both are impacted by the interaction of  with .
First, on the left-hand side of (\ref{eq:conc2}), we see that the point of concentration of  is around , where

For a concentration bound of the same form as Definition~\ref{def:concgauss}, however,  should concentrate around some constant multiple of . In general, for different initial states  and transition matrices , we may see widely varying ratios . However, further analysis is possible in scenarios where this ratio is predictable and fixed.
Second, on the right-hand side of (\ref{eq:conc2}), we see that the exponent of the concentration failure probability scales with

As mentioned earlier, . The case  is quite favorable and happens when ; this occurs when the state ``energy'' is preserved over time. The case  is quite unfavorable and happens when  and  for .




\subsubsection{Unitary and Scaled Unitary System Matrices}
\label{sec:unitary1}

In the special case where  is unitary (i.e.,  for all  and for any power ), we can draw a particularly strong conclusion. Because a unitary  guarantees both that  and that , we have the following result.

\begin{cor} Suppose the same notation and assumptions as in Theorem~\ref{thm:block1}. Assume .
Suppose that  is a unitary operator. Then for any fixed initial state  and for any ,\footnote{The observant reader may note that Corollary~\ref{cor:obs_indep} requires  to be less than . This restriction on  appears so that we can focus on the upper \ac{CoM} inequality~(\ref{eq:CoM_lower_eps}) and ignore the lower one~(\ref{eq:CoM_higher_eps}). 
However, for most of the problems considered in this paper (i.e., unitary, scaled unitary, and certain symmetric matrices ), we can actually apply~(\ref{eq:CoM_lower_eps}) for a much broader range of  (up 
to  and even higher). In fact, we can show that in these settings,

for all . Consequently, we allow  in Corollaries~\ref{cor:unitary1} and \ref{cor:scaled-unitary1}. We have omitted these details for the sake of space.} 

\label{cor:unitary1}
\end{cor}

What this means is that we get the same degree of concentration from the  matrix  as we would get from a
fully dense  matrix populated with \ac{i.i.d.} Gaussian random variables.
Observe that this concentration result is valid for any
 (not necessarily sparse) and can be used, for example, to prove that finite point clouds~\cite{indyk1998approximate} and low-dimensional manifolds~\cite{mbwFocm} in  can have stable, approximate distance-preserving embeddings under the matrix . In each of these cases we may be able to solve very powerful signal inference and recovery problems with .

When  (consecutive sample times), one can further derive \ac{CoM} inequalities when  is a scaled unitary matrix (i.e., when  where  () and  is unitary).

\begin{cor} Suppose the same notation and assumptions as in Theorem~\ref{thm:block1}. Assume . Suppose that  () and  is unitary. Define .
Then for any fixed initial state  and for any ,
\begin{numcases}{\Prob{\bigg{|}\|\frac{1}{\sqrt{b}}\ok \vc{x}_0 \|_{2}^{2}- \|\vc{x}_0\|_2^2\bigg{|} > \epsilon  \|\vc{x}_0\|_{2}^{2}}
\leq}
2\exp\left\{-\frac{M K \epsilon^{2}}{256\left(\left(1-a^2\right)K+a^2\right)}\right\}, &  \label{eq:scaled-conc3}\\
2\exp\left\{-\frac{M K \epsilon^{2}}{256\left(\left(1-a^{-2}\right)K+a^{-2}\right)}\right\}, & . \label{eq:scaled-conc4}
\end{numcases}
\label{cor:scaled-unitary1}
\end{cor}

{\textbf{Proof of Corollary \ref{cor:scaled-unitary1}}}
First note that when  ( is unitary) and  then . From~(\ref{eq:gamma1}) when ,

Also observe\footnote{In order to prove~(\ref{eq:geo_sum_bnd_1}), for a given , let  be a constant such that for all  ( only takes positive integer values), . By this assumption, .
Observe that for a given ,  is a decreasing function of  and its maximum is achieved when . Choosing  completes the proof of~(\ref{eq:geo_sum_bnd_1}).}
that when ,

Thus, from~(\ref{eq:bnd_Gamma_a_small_1}) and (\ref{eq:geo_sum_bnd_1}) and noting that  when ,

Similarly, one can show that when ,

and consequently,

With the appropriate scaling of  by , the \ac{CoM} inequalities follow from Corollary~\ref{cor:obs_indep}.
\hfill 



\subsubsection{Implications for the \ac{RIP}}
\label{sec:RIP_based_CoM}
As mentioned earlier, our \ac{CoM} inequalities have immediate implications in establishing the \ac{RIP} for the observability matrix. Based on Definition~\ref{def:concgauss} and Lemma~\ref{lem:RIP_based_CoM}, in this section we prove Theorems~\ref{theo:main_RIP_CoM_result_1}
and \ref{theo:main_RIP_CoM_result_3}.

{\textbf{Proof of Theorem~\ref{theo:main_RIP_CoM_result_1}}} In order to establish the \ac{RIP} based on Lemma~\ref{lem:RIP_based_CoM}, we simply need to evaluate  in our \ac{CoM} result derived in Corollary~\ref{cor:scaled-unitary1}. One can easily verify that
\begin{numcases}{f(\epsilon) = }
\frac{\epsilon^2}{256\left((1-a^2)K+a^2\right)}, & \\
\frac{\epsilon^2}{256\left((1-a^{-2})K+a^{-2}\right)}, & .
\end{numcases}
Through a union bound argument and by applying Lemma~\ref{lem:RIP_based_CoM} for all  -dimensional subspaces in , the \ac{RIP} result follows.
\hfill 

{\textbf{Proof of Theorem~\ref{theo:main_RIP_CoM_result_3}}} In order to establish the \ac{RIP} based on Lemma~\ref{lem:RIP_based_CoM}, we simply need to evaluate  in our \ac{CoM} result derived in Corollary~\ref{cor:unitary1}. In this case,

Through a union bound argument and by applying Lemma~\ref{lem:RIP_based_CoM} for all  -dimensional subspaces in , the \ac{RIP} result follows.
\hfill 



\subsection{Identical Random Measurement Matrices}
\label{sec:same}

In this section, we consider the case where all matrices  are identical and equal to some  matrix  which is populated with \ac{i.i.d.} Gaussian entries having zero mean and variance . Once again note that we can write

where this time

and  is as defined in~(\ref{eq:ckak}). The matrix  is block diagonal with equal blocks on its main diagonal, and we have the following bound on its concentration behavior.

\begin{theorem} {\em \cite{park2011block}} Assume each of the measurement matrices  is populated with \ac{i.i.d.} Gaussian random entries with mean zero and variance . Assume all matrices  are the same (i.e., ). Let  and define

Then,

where

and  are the first (non-zero) eigenvalues of the  matrix , where

\label{thm:block2}
\end{theorem}

Consider the first of the cases given in the right-hand side of the above bound. (This case permits any value of  between  and .) Define

and note that for any , . Moving forward, we will assume for simplicity that , although this assumption can be removed.
The case  is quite favorable and implies that we get the same degree of concentration from the 
block diagonal matrix  as we would get from a dense  matrix populated with \ac{i.i.d.} Gaussian random variables. This event happens if and only if , which happens if and only if

and  for all  with .
On the other hand, the case   is quite unfavorable and implies that we get the same degree of concentration from the  block diagonal matrix  as we would get from a dense Gaussian matrix having only  rows.
This event happens if and only if the dimension of  equals 1. Thus, comparing to Section~\ref{sec:indep}, uniformity in the norms of the vectors  is no longer sufficient for a high probability of concentration; in addition to this we must have diversity in the directions of the .

The following corollary of Theorem~\ref{thm:block2} derives
a \ac{CoM} inequality for the observability matrix. Recall that  where  is a block diagonal matrix whose diagonal blocks are repeated.

\begin{cor} Suppose the same notation and assumptions as in Theorem~\ref{thm:block2} and suppose . Then for any fixed initial state  and for any ,

\end{cor}

Once again, there are two important phenomena to consider in this result, and both are impacted by the interaction of  with .
First, on the left hand side of (\ref{eq:conc4}), we see that the point of concentration of  is around . Second, on the right-hand side of (\ref{eq:conc4}), we see that the exponent of the concentration failure probability scales with , which is determined by the eigenvalues of the  Gram matrix , where

As mentioned earlier, . The case  is quite favorable and happens when  and  for all  with . The case  is quite unfavorable and happens if the dimension of  equals 1.

In the special case where  is unitary, we know that . However, a unitary system matrix does not guarantee a favorable value for . Indeed, if  we obtain the worst case value . If, on the other hand,  acts as a rotation that takes a state into an orthogonal subspace, we will have a stronger result.

\begin{cor} Suppose the same notation and assumptions as in Theorem~\ref{thm:block2} and suppose . Suppose that  is a unitary operator. Suppose also that  for all  with . Then for any fixed initial state  and for any ,

\label{cor:indep_unitary_rotate}
\end{cor}

This result requires a particular relationship between  and , namely that  for all  with . Thus, given a particular system matrix , it is possible that it might hold for some  and not others. One must therefore be cautious in using this concentration result for CS applications (such as proving the RIP) that involve applying the concentration bound to a prescribed collection of vectors~\cite{baraniuk2008simple}; one must ensure that the ``orthogonal rotation'' property holds for each vector in the prescribed set.
\section{Case Study: Estimating the Initial State in a Diffusion Process}
\label{sec:expt}

So far we have provided theorems that provide a sufficient number of measurements for stable recovery of a sparse initial state under certain conditions on the state transition matrix and under the assumption that the measurement matrices are independent and populated with random entries. In this section, we use a case study to illustrate some of the phenomena raised in the previous sections.
\subsection{System Model}

We consider the problem of estimating the initial state of a system governed by the diffusion equation

where  is the concentration, or density, at position  at time , and  is the diffusion coefficient at position . If  is independent of position, then this simplifies to

The boundary conditions can vary according to the surroundings of the domain . If  is bounded by an impermeable surface (e.g., a lake surrounded by the shore), then the boundary conditions are , where  is the normal to  at .
We will work with an approximate model discretized in time and in space. For simplicity, we explain a one-dimensional (one spatial dimension) diffusion process here but a similar approach of discretization can be taken for a diffusion process with two or three spatial dimensions.
Let  be a vector of equally-spaced locations with spacing , and let . Then a first difference approximation in space gives the model

where  represents the discrete Laplacian. We have

where  is the Laplacian matrix associated with a path (one spatial dimension).
This discrete Laplacian  has eigenvalues  for .

To obtain a discrete-time model, we choose a sampling time  and let the vector
 be the concentration at positions  at sampling time .  Using a first difference approximation in time, we have

where .
For a diffusion process with two spatial dimensions, a similar analysis would follow, except one would use the Laplacian matrix of a grid (instead of the Laplacian matrix of a one-dimensional path) in .
For all simulations in this section we take , , , and . An example simulation of a one-dimensional diffusion is shown in Figure~\ref{fig:diffsim1}, where we have initialized the system with a sparse initial state  containing unit impulses at  randomly chosen locations.

\begin{figure}
\begin{center}
\includegraphics[width=3in]{diffsim1}
\caption{One-dimensional diffusion process. At time zero, the concentration (the state) is non-zero only at a few locations of the path graph of  nodes.}
\label{fig:diffsim1}
\ss
\end{center}
\end{figure}

In Section~\ref{sec:exprecovery}, we provide several simulations which demonstrate that recovery of a sparse initial state is possible from compressive measurements.

\subsection{Diffusion and its Connections to Theorem~\ref{theo:rip_ok_symmetric_A}}

Before presenting the recovery results from compressive measurements, we would like to mention that our analysis in Theorem~\ref{theo:rip_ok_symmetric_A} gives some insight into (but is not precisely applicable to) the diffusion problem. In particular, the discrete Laplacian matrix  and the corresponding state transition matrix  (see below) are almost circulant, and so their eigenvectors will closely resemble the \ac{DFT} basis vectors. The largest eigenvalues correspond to the lowest frequencies, and so the  matrix corresponding to  or  will resemble a basis of the lowest frequency \ac{DFT} vectors. While such a matrix does not technically satisfy the \ac{RIP}, matrices formed from random sets of \ac{DFT} vectors do satisfy the \ac{RIP} with high probability~\cite{rudelson2008sparse}. Thus, even though we cannot apply Theorem~\ref{theo:rip_ok_symmetric_A} directly to the diffusion problem, it does provide some intuition that sparse recovery should be possible in the diffusion setting.




\subsection{State Recovery From Compressive Measurements}
\label{sec:exprecovery}

In this section, we consider a two-dimensional diffusion process. As mentioned earlier, the state transition matrix  associated with this process is of the form , where  is the sampling time and  is the Laplacian matrix of a grid. In these simulations, we consider a grid of size  with .

\begin{figure}[tb]
\centering
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{Dense_Meas}
   \label{fig:Dense_Meas}
 }
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{Line_Meas}
   \label{fig:Line_Meas}
 }
\caption{
Dense Measurements versus Line Measurements.
The color of a node indicates the corresponding weight of that node. The darker the node color, the higher the weight. These weights are the entries of each row of each . (a) Dense Measurements. The weights are drawn from a Gaussian distribution with mean zero and variance . These values are random and change for each measurement.
(b) Line Measurements. The weights are generated as a function of the perpendicular distances of all nodes of the grid to the line. The slope and the intercept of the line are random and change for each measurement.}
\label{fig:Dense_Line_Meas}
\end{figure}

We also consider two types of measuring processes. We first look at random measurement matrices  where the entries of each matrix are \ac{i.i.d.} Gaussian random variables with mean zero and variance .
Note that this type of measurement matrix falls within the assumptions of our theorems in Sections~\ref{sec:rip} and \ref{sec:indep}. In this measuring scenario, all of the nodes of the grid (i.e., all of the states) will be measured at each sample time. Formally, at each observation time we record a random linear combination of all nodes.
In the following, we refer to such measurements as ``Dense Measurements.'' Figure~\ref{fig:Dense_Meas} illustrates an example of how the random weights are spread over the grid. The weights (the entries of each row of each ) are shown using grayscale. The darker the node color, the higher the corresponding weight.
We also consider a more practical measuring process in which at each sample time the operator measures the nodes of the grid occurring along a line with random slope and random intercept. Formally, 
where  is the perpendicular
distance of node  () to the th () line with random slope and random intercept and  is an absolute constant that determines how fast the node weights decrease as their distances increase from the line. Figure~\ref{fig:Line_Meas} illustrates an example of how the weights are spread over the grid in this scenario. Observe that the nodes that are closer to the line are darker, indicating higher weights for those nodes.\cut{
The slope and the intercept of the line is random for each measurement.
Put formally, the entries of each row of each measurement matrix  are generated as a function of the perpendicular distances of all nodes of the grid to a random line. A random line in this context is a line with a random slope and a random intercept that passes through the field. In other words, in order to take a measurement in this scenario, we first generate a random line and then calculate the perpendicular distances of all the points of the grid to this line. The entries of the corresponding row of  are functions of these distances. We use an exponential function for this regard.} We refer to such measurements as ``Line Measurements.''

To address the problem of recovering the initial state , let us first consider the situation where we collect measurements only of  itself. We fix the sparsity level of  to .
For various values of , we construct measurement matrices  according to the two models explained above.
At each trial, we collect the measurements  and attempt to recover  given  and  using the canonical -minimization problem from CS:

with . (In the next paragraph, we repeat this experiment for different .)
In order to imitate what might happen in reality (e.g., a drop of poison being introduced to a lake of water at ), we assume the initial contaminant appears in a cluster of nodes on the associated diffusion grid. In our simulations, we assume the  non-zero entries of the initial state correspond to a 
square-neighborhood of nodes on the grid. For each , we repeat the recovery problem for  trials; in each trial we generate a random sparse initial state  (an initial state with a random location of the  square and random values of the  non-zero entries) and a measurement matrix  as explained above.

Figure~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_0} depicts, as a function of , the percent of trials (with  and  randomly chosen in each trial) in which the initial state is recovered perfectly, i.e., . Naturally, we see that as we take more measurements, the recovery rate increases.
When Line Measurements are taken, with almost  measurements we recover every sparse initial state of dimension  with sparsity level . When Dense Measurements are employed, however, we observe a slightly weaker recovery performance at  as almost  measurements are required to see exact recovery.
In order to see how the diffusion phenomenon affects the recovery, we repeat the same experiment at . In other words, we collect the measurements  and attempt to recover  given  and  using the canonical -minimization problem~(\ref{eq:ell1k}). As shown in Fig.~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_10}, the recovery performance is improved when Line and Dense Measurements are employed (with almost  measurements exact recovery is possible).
Qualitatively, this suggests that due to diffusion, at , the initial contaminant is now propagating and consequently a larger surface of the lake (corresponding to more nodes of the grid) is contaminated. In this situation, a higher number of contaminated nodes will be measured by Line Measurements which potentially can improve the recovery performance of the initial state.


\begin{figure}[tb]
\centering
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_0}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_0}
 }
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_10}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_10}
 }
\caption{
Signal recovery from compressive measurements of a diffusion process which has initiated from a sparse initial state of dimension  and sparsity level . The plots show the percent of trials (out of  trials in total) with perfect recovery of the initial state  versus the number of measurements . (a) Recovery from compressive measurements at time . (b) Recovery from compressive measurements at time .
}
\label{fig:TwoDimDiffRecoveryTest2_Tvec_0_1}
\end{figure}



In order to see how the recovery performance would change as we take measurements at different times, we repeat the previous example for . The results are shown in Fig.~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Random_all} and Fig.~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Line_all} for Dense and Line Measurements, respectively. In both cases, the recovery performance starts to improve as we take measurements at later times.
However, in both measuring scenarios, the recovery performance tends to decrease if we wait too long to take measurements. For example, as shown in Fig.~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Random_all}, the recovery performance is significantly decreased at time  when Dense Measurements are employed. A more dramatic decrease in the recovery performance can be observed when Line Measurements are employed in Fig.~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Line_all}.
Again this behavior is as expected and can be interpreted with the diffusion phenomenon. If we wait too long to take measurements from the field of study (e.g., the lake of water), the effect of the initial contaminant starts to disappear in the field (due to diffusion) and consequently measurements
at later times contain less information. In summary, one could conclude from these observations that taking compressive measurements of a diffusion process at times that are too early or too late might decrease the recovery performance.
\begin{figure}[tb]
\centering
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Random_all}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Random_all}
 }
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Line_all}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_Line_all}
 }
\caption{
Signal recovery from compressive measurements of a diffusion process which has initiated from a sparse initial state of dimension  and sparsity level . The plots show the percent of trials (out of  trials in total) with perfect recovery of the initial state  versus the number of measurements  taken at observation times . (a) Recovery from compressive Dense Measurements. (b) Recovery from compressive Line Measurements.
}
\label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2_Tvec_all}
\end{figure}




In another example, we fix , consider the same model for the sparse initial states with  as in the previous examples, introduce white noise in the measurements with standard deviation 0.05, use a noise-aware version of the  recovery algorithm~\cite{CandesRIP}, and plot a histogram of the recovery errors .
We perform this experiment at  and . As can be seen in Fig.~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2RunNEW_Noise_Tvec_2}, at time  the Dense Measurements have lower recovery errors (almost half) compared to the Line Measurements. However, if we take measurements at time , the recovery error of both measurement processes tends to be similar, as depicted in Fig.~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest2RunNEW_Noise_Tvec_10}.

\begin{figure}[tb]
\centering
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest2RunNEW_Noise_Tvec_2}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2RunNEW_Noise_Tvec_2}
 }
\subfigure[]{
   \includegraphics[width = .45\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest2RunNEW_Noise_Tvec_10}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2RunNEW_Noise_Tvec_10}
 }
\caption{
Signal recovery from  compressive measurements of a diffusion process which has initiated from a sparse initial state of dimension  and sparsity level . The plots show the
recovery error of the initial state  over  trials.
(a) Recovery from compressive measurements at time . (b) Recovery from compressive measurements at time .
}
\label{fig:NEW_MEAS_TwoDimDiffRecoveryTest2RunNEW_Noise}
\end{figure}

Of course, it is not necessary to take all of the measurements only at one observation time. What may not be obvious a priori is how spreading the measurements over time may impact the initial state recovery. To this end, we perform the signal recovery experiments when a total of  measurements are spread over  observation times (at each observation time we take  measurements). In order to see how different observation times affect the recovery performance, we repeat the experiment for different sample sets, . We consider  sample sets as , , , , , , , , , and .
Figure~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest3RunNEW_M8} illustrates the results. For both of the measuring scenarios, the overall recovery performance improves when we take measurements at later times. As mentioned earlier, however, if we wait too long to take measurements the recovery performance drops.
For sample sets  through , we have perfect recovery of the initial state only from  total measurements, either using Dense or Line Measurements. The overall recovery performance is not much different compared to, say, taking  measurements at a single instant and so there is no significant penalty that one pays by slightly spreading out the measurement collection process in time, as long as a different random measurement matrix is used at each sample time. We repeat the same experiment when the measurements are noisy. We introduce white noise in the measurements with standard deviation 0.05 and use a noise-aware version of the -minimization problem to recover the true solution.
Figure~\ref{fig:NEW_MEAS_TwoDimDiffRecoveryTest3RunNEW_Noise_Tvec_all_M8_eps_20} depicts a histogram of the recovery errors  when  measurements are spread over  sample times .

\begin{figure}[tb]
\centering
\subfigure[]{
   \includegraphics[width = 0.45\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest3RunNEW_M8}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest3RunNEW_M8}
 }
\subfigure[]{
   \includegraphics[width = 0.39\columnwidth]{NEW_MEAS_TwoDimDiffRecoveryTest3RunNEW_Noise_Tvec_all_M8_eps_20}
   \label{fig:NEW_MEAS_TwoDimDiffRecoveryTest3RunNEW_Noise_Tvec_all_M8_eps_20}
 }
\caption{
Signal recovery from compressive measurements of a diffusion process which has initiated from a sparse initial state of dimension  and sparsity level . A total of  measurements are spread over  observation times while at each time,  measurements are taken. (a) Percent of trials (out of  trials in total) with perfect recovery of the initial state  are shown for different sample sets, . (b) Recovery error of the initial state  over  trials for set .
}
\end{figure}




\section*{ACKNOWLEDGMENTS}
The authors gratefully acknowledge Alejandro Weinstein, Armin Eftekhari, Han Lun Yap, Chris Rozell, Kevin Moore, and Kameshwar Poolla for insightful comments and valuable discussions.

\appendix






\subsection{Proof of Theorem~\ref{theo:rip_ok_symmetric_A}}
We start the analysis by showing that  lies within a small neighborhood around  for any -sparse . To this end, we derive the following lemma.

\begin{lemma}
Assume . Assume  has the eigendecomposition given in~(\ref{eq:decomp1}) and  () satisfies a scaled version of the \ac{RIP} of order  with isometry constant  as given in~(\ref{eq:rip_U_1^T}). Then, for ,

holds for all -sparse .
\label{lem:deterministic_bnd_ak}
\end{lemma} 

{\textbf{Proof of Lemma~\ref{lem:deterministic_bnd_ak}}}
If  is of the form given in~(\ref{eq:decomp1}), we have
, and consequently,

On the other hand,

Thus,

If  satisfies the scaled \ac{RIP}, then from~(\ref{eq:rip_U_1^T}) and~(\ref{eq:upper_lower_bnd_A})
for ,

holds for all -sparse . 
Similarly, one can show that for ,

and consequently, for ,

holds for all -sparse . Consequently using~(\ref{eq:ak_x_0_sum}), for ,

holds for all -sparse . 
\hfill 

Lemma~\ref{lem:deterministic_bnd_ak} provides deterministic bounds on the ratio  for all -sparse  when  satisfies the scaled \ac{RIP}. Using this deterministic result, we can now state the proof of Theorem~\ref{theo:rip_ok_symmetric_A} where we show that a scaled version of  satisfies the \ac{RIP} with high probability. 


First observe that when all matrices  are independent and populated with \ac{i.i.d.} Gaussian random entries, from Corollary~\ref{cor:obs_indep} we have the following \ac{CoM} inequality for 
. For any fixed -sparse , let . Then for any ,

As can be seen, the right-hand side of (\ref{eq:CoM_signal_dependent}) is signal dependent. However, we need a universal failure probability bound (that is independent of ) in order to prove the \ac{RIP} based a \ac{CoM} inequality. Define

Therefore from (\ref{eq:CoM_signal_dependent}) and (\ref{eq:def_min_Gamma}), for any fixed -sparse  and for any ,

where , , and . Let  denote a failure probability and  denote a distortion factor.
Through a union bound argument and by applying Lemma~\ref{lem:RIP_based_CoM} for all  -dimensional subspaces in , whenever  satisfies the \ac{CoM} inequality~(\ref{eq:CoM_ck_sparse}) with

then with probability exceeding ,

for all -sparse . Consequently using the deterministic bound on  derived in~(\ref{eq:deterministic_ak}), with probability exceeding ,

for all -sparse . 
\hfill 






\subsection{Proof of Corollary~\ref{cor:rip_ok_symmetric_A_extreme} and Corollary~\ref{cor:rip_ok_symmetric_A_extreme_1}}


We simply need to derive a lower bound on  as an evaluation of . Recall~(\ref{eq:gamma1}) and define

If all the entries of  lie within some bounds as  for all , then one can show that

Using the deterministic bound derived in~(\ref{eq:deterministic_bnd_A_k_i}) on  for all , one can show that when  ( and ),  and , and thus,

Similarly one can show that when , 

and when ,

Using these lower bounds on  (recall that  is defined in (\ref{eq:def_min_Gamma}) as the infimum of  over all -sparse ) in the result of Theorem~\ref{theo:rip_ok_symmetric_A} completes the proof.
We also note that when  and , the upper bound given in~(\ref{eq:CoM_ck_sparse}) can be used to bound the left-hand side failure probability even when . In fact, we can show that~(\ref{eq:CoM_ck_sparse}) holds for 
any . The \ac{RIP} results of Corollaries~\ref{cor:rip_ok_symmetric_A_extreme} and
\ref{cor:rip_ok_symmetric_A_extreme_1} follow based on this \ac{CoM} inequality. We have omitted these details for the sake of space.













\footnotesize
\bibliographystyle{IEEEbib}
\bibliography{referfile}


\end{document}
