

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage[dvipsnames]{xcolor}
\usepackage{caption}
\usepackage{url}
\usepackage{amsmath, amsthm, amssymb, dsfont}
\usepackage[noend]{algpseudocode}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}
\newcommand{\bw}{{\textbf{w}}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{eq. ~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sH{{\mathbb{H}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\bell}{\boldsymbol{\ell}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
 


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand\ourmethod{pFedHN}
\newcommand\as[1]{\textcolor{orange}{[AS: #1]}}
\newcommand\an[1]{\textcolor{magenta}{[AN: #1]}}
\newcommand\gal[1]{\textcolor{blue}{[GC: #1]}}
\newcommand\ef[1]{\textcolor{red}{[EF: #1]}}
\newcommand\galch[1]{{#1}}

\usepackage[accepted, nohyperref]{icml2021}



\icmltitlerunning{Personalized Federated Learning using Hypernetworks}

\begin{document}

\twocolumn[
\icmltitle{Personalized Federated Learning using Hypernetworks}



\begin{icmlauthorlist}
\icmlauthor{Aviv Shamsian}{biu}
\icmlauthor{Aviv Navon}{biu}
\icmlauthor{Ethan Fetaya}{biu}
\icmlauthor{Gal Chechik}{biu,nvidia}
\end{icmlauthorlist}
\icmlaffiliation{biu}{Bar-Ilan University, Ramat Gan, Israel}
\icmlaffiliation{nvidia}{Nvidia, Tel-Aviv, Israel}

\icmlcorrespondingauthor{Aviv Shamsian}{aviv.shamsian@live.biu.ac.il}
\icmlcorrespondingauthor{Aviv Navon}{aviv.navon@biu.ac.il}



\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}







\begin{abstract}
Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models in a collaborative way while accounting for data disparities across clients and reducing communication costs.

We propose a novel approach to this problem using hypernetworks, termed \textit{\ourmethod{}} for \textit{personalized Federated HyperNetworks}. In this approach, a central hypernetwork model is trained to generate a set of models, one model for each client. This architecture provides effective parameter sharing across clients, while maintaining the capacity to generate unique and diverse personal models. Furthermore, since hypernetwork parameters are never transmitted, this approach decouples the communication cost from the trainable model size. We test \ourmethod{} empirically in several personalized federated learning challenges and find that it outperforms previous methods. Finally, since hypernetworks share information across clients we show that \ourmethod{} can generalize better to new clients whose distributions differ from any client observed during training.



\end{abstract}

\section{Introduction}
\label{intro}
Federated learning (FL) is the task of learning a model over multiple disjoint local datasets \cite{McMahan2017CommunicationEfficientLO, Yang2019FederatedML}. It is particularly useful when local data cannot be shared due to privacy, storage, or communication constraints. This is the case, for instance, in IoT applications that create large amounts of data at edge devices, or with medical data that cannot be shared due to privacy \citep{Wu2020PersonalizedFL}. In federated learning, all clients collectively train a shared model without sharing data and while trying to minimize communication.
Unfortunately, learning a single global model may fail when the data distribution varies across clients. For example, user data may come from different devices or geographical locales and is potentially heterogeneous. In the extreme, each client may be required to solve a different task. To handle such heterogeneity across clients, \textit{Personalized Federated Learning} (PFL) \cite{smith2017federated} allows each client to use a \textit{personalized} model instead of a shared global model. The key challenge in PFL is to benefit from joint training while allowing each client to keep its own unique model and at the same time limit the communication cost. While several approaches were recently proposed for this challenge, these problems are far from being resolved.  

In this work, we describe a new approach that aims to resolve these concerns. Our approach, which we name, \textit{\ourmethod{}} for \textit{personalized Federated HyperNetwork} addresses this by using hypernetworks \cite{Ha2017HyperNetworks}, a model that for each input produces parameters for a neural network. Using a single joint hypernetwork to generate all separate models allows us to perform smart parameter sharing. Each client has a unique embedding vector, which is passed as input to the hypernetwork to produce its personalized model weights. As the vast majority of parameters belong to the hypernetwork, most parameters are shared across clients. Despite that, by using a hypernetwork, we can achieve great flexibility and diversity between the models of each client. Intuitively, as the hypernetwork maps between the embedding space and the personal networks' parameter space, its image can be viewed as a low-dimensional manifold in that space. Thus, we can think of the hypernetwork as the coordinate map of this manifold. Each unique client's model is restricted to lay on this manifold and is parametrized by the embedding vector.

Another benefit of using hypernetworks is that the trained parameter vector of the hypernetwork, which is generally much larger than the parameter vectors of the clients that it produces, is never transmitted. Each client only needs to receive its own network parameters to make predictions and compute gradients. Furthermore, the hypernetwork only needs to receive the gradient or update direction to optimize its own parameters.  As a result, we can train a large hypernetwork with the same communication costs as in previous models. Compared to previous parameter sharing schemes, e.g., \citet{Dinh2020PersonalizedFL, McMahan2017CommunicationEfficientLO}, hypernetworks open new options that were not directly possible before. Consider the case where each client uses a cell phone or a wearable device, each one with different computational resources. 
The hypernetwork can produce several networks per input, each with a different computational capacity, allowing each client to select its appropriate network.

This paper makes the following contributions: (1) A new approach for personalized federated learning based on hypernetworks. 
(2) This approach generalizes better (a) to novel clients that differ from the ones seen during training; and (b) to clients with different computational resources, allowing clients to have different model sizes.   
(3) A new set of state-of-the-art results for the standard benchmarks in the field CIFAR10, CIFAR100, and Omniglot.


The paper is organized as follows. Section \ref{sec:model} describes our model in detail. Section \ref{sec:theory} establishes some theoretical results to provide insight into our model. Section \ref{sec:experiments} shows experimentally that \ourmethod{} achieves state-of-the-art results on several datasets and learning setups. We make our source code publicly available at: \textcolor{magenta}{\url{https://github.com/AvivSham/pFedHN}}. 



\begin{figure*}[t]
\centering
    \begin{subfigure}[]{
    \includegraphics[width=0.57\linewidth]{figures/arch.png}
    }
    \label{fig:global_arch}
    \end{subfigure}
    \hfill
    \begin{subfigure}[]{
    \includegraphics[width=0.37\linewidth]{figures/arch_local.png}
    }
    \label{fig:arch_local}
     \end{subfigure}
    \caption{The Federated hypernetwork framework. \textbf{(a)} An HN is located on the server and communicate personal model for each clients. In turn, the clients send back the update direction ;
    \textbf{(b)} The HN acts on the client embedding  to produce model weights . The client performs several local optimization steps to obtain , and sends back the update direction .}
    \label{fig:arch}
\end{figure*}
 

\section{Related Work}


\label{related}
\subsection{Federated Learning}
 Federated learning (FL)~\citep{McMahan2017CommunicationEfficientLO, kairouz2019advances, mothukuri2021survey, Li2019FederatedLS, li2020federated} is a learning setup in machine learning in which multiple clients collaborate to solve a learning task while maintaining privacy and communication efficiency. Recently, numerous methods have been introduced for solving the various FL challenges. \citet{duchi2014privacy, mcmahan2017learning, agarwal2018cpsgd, zhu2019federated} proposed new methods for preserving privacy, and \citet{reisizadeh2020fedpaq, dai2019hyper, basu2020qsparse, li2020acceleration, stich2018local} focused on reducing communication cost. 
While some methods assume a homogeneous setup, in which all clients share a common data distribution~\citep{wang2018cooperative, lin2018don}, others tackle the more challenging heterogeneous setup in which each client is equipped with its own data distribution~\citep{zhou2017convergence, hanzely2020federated, zhao2018federated, sahu2018convergence, karimireddy2019scaffold, haddadpour2019convergence,Hsu2019MeasuringTE}.

Perhaps the most known and commonly used FL algorithm is FedAvg~\citep{McMahan2017CommunicationEfficientLO}. It learns a global model by aggregating local models trained on IID data. However, the above methods learn a shared global model for all clients instead of personalized per-client solutions.

\subsection{Personalized Federated Learning}

The federated learning setup presents numerous challenges including data heterogeneity (differences in data distribution), device heterogeneity (in terms of computation capabilities, network connection, etc.), and communication efficiency~\citep{kairouz2019advances}.
Especially data heterogeneity makes it hard to learn a single shared global model that applies to all clients. To overcome these issues, Personalized Federated Learning (PFL) aims to personalize the global model for each client in the federation~\citep{Kulkarni2020SurveyOP}.
Many papers proposed a decentralized version of the model agnostic meta-learning (MAML) problem 
~\citep{Fallah2020PersonalizedFL, li2017meta, behl2019alpha, zhou2019efficient, fallah2020convergence}. Since MAML approach relies on the Hessian matrix, which is computationally costly, several works attempted to approximate the Hessian \citep{finn2017model, nichol2018first}. Another approach to PFL is model mixing where the clients learn a mixture of the global and local models \citep{deng2020adaptive, arivazhagan2019federated}.
\citet{hanzely2020federated} introduced a new neural network architecture that is divided into base and personalized layers. The central model trains the base layers by FedAvg and the personalized layers (also called top layers) are trained locally.
\citet{liang2020think} presented LG-FedAvg a mixing model where each client obtains local feature extractor and global output layers. This is an opposite approach to the conventional mixing model that enables lower communication costs as the global model requires fewer parameters. Other approaches to train the global and local models under different regularization \citep{Huang2020PersonalizedCF}. \citet{Dinh2020PersonalizedFL} introduced pFedMe, a method that uses Moreau envelops as the client regularized loss. This regularization helps to decouple the personalized and global model optimizations.  
Alternatively, clustering methods for federated learning assume that the local data of each client is partitioned by nature \citep{Mansour2020ThreeAF}. Their goal is to group together similar clients and train a centralized model per group. In case of heterogeneous setup, some clients are "closer" than others in terms of data distribution. Based on this assumption and inspired by FedAvg, \citet{zhang2020personalized} proposed pFedFOMO, an aggregation method where each client only federates with a subset of relevant clients.

\subsection{Hypernetworks}



Hypernetworks (HNs)~\citep{Klein2015ADC, Riegler2015ConditionedRM, Ha2017HyperNetworks} are deep neural networks that output the weights of another target network, that performs the learning task. The idea is that the output weights vary depending on the input to the hypernetwork.

HNs are widely used in various machine learning domains, including language modeling~\citep{suarez2017character}, computer vision~\citep{Ha2017HyperNetworks, klocek2019hypernetwork}, continual learning~\citep{von2019continual}, hyperparameter optimization~\citep{Lorraine2018StochasticHO, MacKay2019SelfTuningNB, Bae2020DeltaSTNEB}, multi-objective optimization~\citep{navon2021learning}, and decoding block codes~\citep{DBLP:conf/nips/NachmaniW19}. 

HNs are naturally suitable for learning a diverse set of personalized models, as HNs dynamically generate target networks conditioned on the input. 
 
 
 
 \section{Method}\label{sec:model}
 


 
 In this section, we first formalize the personalized federated learning (PFL) problem, then we present our \textit{personalized Federated HyperNetworks} (\ourmethod{}) approach.
 
\subsection{Problem Formulation}
Personalized federated learning (PFL) aims to collaboratively train personalized models for a set of  clients, each with its own personal private data. Unlike conventional FL, each client  is equipped with its own data
distribution  on . Assume each client has access to  IID samples from , . Let  denote the loss function corresponds to client , and  the average loss over the personal training data . Here  denotes the personal model of client . The PFL goal is to optimize 

and the training objective is given by 

where  denotes the collection of all personal model parameters .

\subsection{Federated Hypernetworks}
\label{fhn}



In this section,  we describe our proposed \textit{personalized Federated Hypernetworks} (\ourmethod{}), a novel method for solving the PFL problem (eq. \ref{pfl_problem}) using hypernetworks.
Hypernetworks are deep neural networks that output the weights of another network, conditioning on its input. Intuitively, HNs simultaneously learn a family of target networks.
Let  denote the hypernetwork parametrized by  and  the target network parametrized by . 
The hypernetwork is located at the server and acts on a client descriptor  (see Figure~\ref{fig:arch}). The descriptor can be a trainable embedding vector for the client or fixed, provided that a good client representation is known a-priori. Given  the HN outputs the weights for the  client . Hence, the HN  learns a family of personalized models . \ourmethod{} provides a natural way for sharing information across clients while maintaining the flexibility of personalized models, by sharing the parameters .

We adjust the PFL objective (eq. \ref{pfl_problem}) according to the above setup to obtain


One crucial and attractive property of \ourmethod{} is that it decouples the size of  and the communication cost. The amount of data transferred is determined by the size of the target network during the forward and backward communications, and does not depend on the size of . Consequently, the hypernetwork can be arbitrarily large without impairing communication efficiency. Indeed, using the chain rule we have  so the client only needs to communicate  back to the hypernetwork, which has the same size as the \textit{personal} network parameters . 

In our work, we used a more general update rule  where  is the change in the local model parameters after several local update steps. As the main limitation is the communication cost, we found it beneficial to perform several local update steps, on the client side per communication round. This aligns with prior work that highlighted the benefits of local optimization steps in terms of both convergence speed (hence communication cost) and final accuracy~\cite{McMahan2017CommunicationEfficientLO,huo2020faster}.
Given the current personalized parameters , we perform several  local optimization steps on the personal data to obtain . We then return the personal model update direction  therefore, the update for  is given by . This update rule is inspired by  \citet{zhang2019lookahead}. Intuitively, suppose we have access to the optimal solution of the personal problem , then our update rule becomes the gradient of an approximation to the surrogate loss  by replacing  with . In Appendix B (Figure \ref{fig:local_opt}), 
we compare the results for a different number of local update steps and show considerable improvement over using the gradient, i.e., using a single step.



\begin{algorithm}[t]
    \caption{Personalized Federated Hypernetwork}\small\label{alg:fhn}
    \begin{algorithmic}[H]
    \State \textbf{input:}  --- number of rounds,  --- number of local rounds,  --- learning rate,  --- client learning rate
    \For{}
    \State sample client 
    \State set  and  
    \For{}
    \State sample mini-batch  
    \State 
    \EndFor
    \State 
    \State 
    \State 
    \EndFor
    \State \textbf{return:} 
    \end{algorithmic}
\end{algorithm}

\subsection{Personal Classifier}

In some cases, it is undesirable to learn the entire network end-to-end with a single hypernetwork. As an illustrative example, consider a case where clients differ only by the label ordering in their output vectors. In this case, having to learn the right label ordering per client adds another unnecessary  difficulty if they were to learn the classification layer as well using the hypernetwork.



As another example, consider the case where each client solves an entirely separate task, similar to multitask learning, where the number of classes may differ between clients. It makes little sense to have the hypernetwork produce each unique task classification layer. 

In these cases, it would be preferable for the hypernetwork to produce the feature extraction part of the target network, which contains most of the trainable parameters, while learning a local output layer for each client. Formally, let  denote the personal classifier parameters of client . We modify the optimization problem (\eqref{fhn_problem}) to obtain,

where we define the feature extractor , as before. The parameters  are updated according to Alg.~\ref{alg:fhn}, while the personal parameters  are updated locally using 



\section{Analysis}\label{sec:theory}

In this section, we theoretically analyze \ourmethod{}. First, we provide an insight regarding the solution for the  \ourmethod{} (Eq.~\ref{fhn_problem}), using a simple linear version of our hypernetwork. Next, we describe the generalization bounds of our framework.



\subsection{A Linear Model}

Consider a linear version of the hypernetwork, where both the target model and the hypernetwork are linear models,  with  and  is the  clients embedding. Let  denote the  matrix whose columns are the clients embedding vectors . We note that even for convex loss functions  the objective  might not be convex in  but block multi-convex. In one setting, however, we get a nice analytical solution.

\begin{prop}
Let  be the data for client  and let the loss for linear regressor  be . Furthermore assume for all , . Define the empirical risk minimization (ERM) solution for client  as  . The optimal  minimizing  are given by PCA on , where  is the top  principle components and  is the coefficients for  in these components.
\end{prop}

We provide the proof in Section A of the Appendix. The linear version of our \ourmethod{} performs dimensionality reduction by PCA, but unlike classical dimensionality reduction which is unaware of the learning task, \ourmethod{} uses multiple clients for reducing the dimensionality while preserving the optimal model as best as possible. This allows us to get solutions between the two extremes: A single shared model up to scaling () and each client training locally (). We note that optimal reconstruction of the local models () is generally suboptimal in terms of generalization performance, as no information is shared across clients.

This dimensionality reduction can also be viewed as a denoising process. Assume a linear regression with Gaussian noise model, i.e., for all clients   and that each client solves a maximum likelihood objective. From the central limit theorem for maximum likelihood estimators \cite{CLT_for_ML} we get that\footnote{Note that the Fisher matrix is the identity from our assumption that .}  where  is the maximum likelihood solution. This means that approximately  with  , i.e., our local solutions  are a noisy version of the optimal model  with isotropic Gaussian noise.  

We can now view the linear hypernetworks as performing denoising on , by PCA. PCA is a classic approach to denoising \cite{PCAdenoising} and is well suited for reducing isotropic noise when the energy of the original points is concentrated on a small dimensional subspace. Intuitively we think of our standard hypernetwork as a nonlinear extension of this approach, which has a similar effect by forcing the models to lay on a low-dimensional manifold.

\setlength{\tabcolsep}{5pt}
\begin{table*}[t]
\scriptsize
    \centering
    \caption{\textit{Heterogeneous data}. Test accuracy over  clients on the CIFAR10, CIFAR100, and Omniglot datasets.}
    \vskip 0.11in
    \begin{tabular}{l c c c c c c c c c c}
    \toprule
    & \multicolumn{3}{c}{CIFAR10} & & \multicolumn{3}{c}{CIFAR100} && Omniglot\\
     \cmidrule{2-4}\cmidrule{6-8}\cmidrule{10-10}\\
     \# clients & 10 & 50 & 100  & &  10 & 50 & 100 && 50\\
    \midrule
    Local &  &  &  &&  &  &  &&  \\
    FedAvg &  &  &  &&  &  &  &&  \\
        \hline
    Per-FedAvg &  &  &  &&  &  &  && \\
    FedPer &  &   &  &&  &  &  &&  \\
    pFedMe &  &   &  &&  &  &  &&  \\
    LG-FedAvg &  &  &  &&  &  &  && \\
    \midrule
    \ourmethod{} (ours) &  &  &  &&   &  &  &&  
     \\
    \ourmethod{}-PC (ours) &  &  &  &&  &  &  &&  
     \\
    \bottomrule
    \end{tabular}
    \label{tab:hetro}
\end{table*}



\subsection{Generalization}

We now investigate how \ourmethod{} generalizes using the approach of \citet{baxter2000model}. The common approach for multi-task learning with neural networks is to have a common feature extractor shared by all tasks and a per-task head operating on these features. This case was analyzed by \citet{baxter2000model}.
Conversely, here the per-task parameters are the inputs to the hypernetwork. Next, we provide the generalization guarantee under this setting and discuss its implications.





Let  be the training set for the  client, generated by a distribution . We denote by  the empirical loss of the hypernetwork  and by  the expected loss  . 

We assume weights of the hypernetwork and the embeddings are bounded in a ball of radius , in which the following three Lipschitz conditions hold:
\begin{enumerate}
    \item 
    \item 
    \item . \quad
\end{enumerate}

\begin{theorem}\label{theory_gen}
 Let the hypernetwork parameter space be of dimension  and the embedding space be of dimension . Under previously stated assumptions, there exists  such that if the number of samples per client  is greater than , we have with probability at least  for all  that 
\end{theorem}


Theorem~\ref{theory_gen} provides insights on the parameter-sharing effect of \ourmethod{}. The first term for the number of required samples  depends on the dimension of the embedding vectors; as each client corresponds to its unique embedding vector (i.e. not being shared between clients), this part is independent of the number of clients . 
However, the second term depends on the size of the hypernetwork , is reduced by a factor , as the hypernetwork's weights are shared.



Additionally, the generalization is affected by the Lipschitz constant of the hypernetwork,  (along with other Lipschitz constants), as it can affect the effective space we can reach with our embedding. In essence, this characterizes the price that we pay, in terms of generalization, for the hypernetworks flexibility. It might also open new directions to improve performance. However, our initial investigation into bounding the Lipschitz constant by adding spectral normalization \cite{spectral_normalization} did not show any significant improvement, see Appendix~C.





\section{Experiments}\label{sec:experiments}



We evaluate \ourmethod{} in several learning setups using three common image classification datasets:  CIFAR10, CIFAR100, and Omniglot \citep{cifar, lake2015human} Unless stated otherwise, we report the Federated Accuracy, defined as   , averaged over three seeds.
The experiments show that \ourmethod{} outperforms classical FL approaches and leading PFL models. 


\paragraph{Compared Methods:} We evaluate and compare the following approaches: \textbf{(1)} \textbf{\ourmethod{}}, Our proposed Federated HyperNetworks  \textbf{(2)} \textbf{\ourmethod{}-PC}, \ourmethod{} with a personalized classifier per client ; 
\textbf{(3)} \textbf{Local}, Local training on each client, with no collaboration between clients;
\textbf{(4)} \textbf{FedAvg}~\cite{McMahan2017CommunicationEfficientLO}, one of the first and perhaps the most widely used FL algorithm;
\textbf{(5)} \textbf{Per-FedAvg}~\cite{Fallah2020PersonalizedFL} a meta-learning based PFL algorithm.
 \textbf{(6) pFedMe}~\cite{Dinh2020PersonalizedFL}, a PFL approach which adds a Moreau-envelopes loss term; \textbf{(7) LG-FedAvg}~\cite{liang2020think} PFL method with local feature extractor and global output layers; \textbf{(8) FedPer}~\cite{arivazhagan2019federated} a PFL approach that learns per-client personal classifier on top of a shared feature extractor.

\paragraph{Training Strategies:} In all experiments, our target network shares the same architecture as the baseline models. Our hypernetwork is a simple fully-connected neural network, with three hidden layers and multiple linear heads per target weight tensor. We limit the training process to at-most  server-client communication steps for most methods. One exception is LG-FedAvg which utilizes a pretrained FedAvg model, hence it is trained with additional  communication steps. The \textit{Local} baseline is trained for  optimization steps on each client. For \ourmethod{}, we set the number of local steps to , and the embedding dimension to , where  is the number of clients. We provide an extensive ablation study on design choices in Appendix~\ref{app:additional_exp}.
We tune the hyperparameters of all methods using a pre-allocated held-out validation set. Full experimental details are provided in Appendix~\ref{app:exp_details}.

\subsection{Heterogeneous Data}\label{sec:hetro}



We evaluate the different approaches on a challenging heterogeneous setup. We adopt the learning setup and the evaluation protocol described  in~\citet{Dinh2020PersonalizedFL} for generating heterogeneous clients in terms of classes and size of local training data.  First, we sample two/ten classes for each client for CIFAR10/CIFAR100; Next, for each client  and selected class , we sample , and assign it with  of the samples for this class. We repeat the above using  and  clients. This procedure produces clients with different number of samples and classes.
For the target network, we use a LeNet-based~\cite{lecun1998gradient} network with two convolution and two fully connected layers.


We also evaluate all methods using the Omniglot dataset \cite{lake2015human}.
Omniglot contains 1623 different grayscale handwritten characters (with 20 samples each), from 50 different alphabets. Each alphabet obtains a varying number of characters. In this setup, we use 50 clients and assign an alphabet to each client. Therefore, clients receives  different numbers of samples and the distribution of labels is disjoint across clients. We use a LeNet-based model with four convolution and two fully connected layers.

The results are presented in Table~\ref{tab:hetro}. 
The two simple baselines, local and FedAvg, that do not use personalized federated learning perform quite poorly on most tasks\footnote{In the 10-client split, each client sees on average 10\% of the train set. It is sufficient for training a model locally.}, showing the importance of personalized federated learning. \ourmethod{} achieves large improvements of 2\%-10\% over all competing approaches. Furthermore, on the Omniglot dataset, where each client is allocated with a completely different learning task (different alphabet), we show significant improvement using \ourmethod{}-PC. We present additional results on the MNIST dataset in Appendix~C.

\subsection{Computational Budget}\label{sec:comp}

\begin{table*}[t]
    \small
\centering
    \caption{\textit{Computational budget}. Test accuracy for CIFAR10/100 with 75 clients and varying computational capacities.}
    \vskip 0.11in
    \begin{tabular}{l c c c c c c c c}
    \toprule
    & \multicolumn{3}{c}{CIFAR10} && \multicolumn{3}{c}{CIFAR100} \\
     \cmidrule{2-4} \cmidrule{6-8}\\
    Local model size & S  & M & L && S  & M & L\\
    \midrule
    FedAvg &  &  &  &&  &  & \\
    Per-FedAvg &  &  &  &&  &  &  \\
    pFedMe &  &  &    &&  &  &  \\
    LG-FedAvg &  &  &  &&  &  & \\
    \midrule
    \ourmethod{} (ours) &  &  &  &&  &  & \\
\bottomrule
    \end{tabular}
    \label{tab:comp}
\end{table*}



We discussed above how the challenges of heterogeneous data can be handled using \ourmethod{}. Another major challenge presented by personalized FL is that the communication, storage, and computational resources of clients may differ significantly. These capacities may even change in time due to varying network and power conditions.
In such a setup, the server should adjust to the communication and computational policies of each client. Unfortunately, previous works do not address this resource heterogeneity. \ourmethod{} can naturally adapt to this challenging learning setup by producing target networks of different sizes. 

In this section, we evaluate the capacity of \ourmethod{} to handle clients that differ in their computational and communication resource budget. We use the same split described in Section~\ref{sec:hetro} with a total of  clients divided into the three equal-sized groups named \textit{S} (small), \textit{M} (medium), and \textit{L} (large). The Models of clients within each group share the same architecture. The three architectures (of the three groups) have a different number of parameters. 

We train a single \ourmethod{} to output target client models of different sizes. Importantly, this allows \ourmethod{} to share parameters between all clients even if those have different local model sizes. 

\noindent\textbf{Baselines:} For quantitative comparisons, and since the existing baseline methods cannot easily extend to this setup, we train three independent per-group models. Each group is trained for  server-client communication steps. 
See details in Appendix~C for further details.

The results are presented in Table~\ref{tab:comp}, showing that \ourmethod{} achieves  improvement over all competing methods. The results demonstrate the flexibility of our approach, which is capable of adjusting to different client settings while maintaining high accuracy.


\subsection{Generalization to Novel Clients}



Next, we study an important learning setup where
new clients join, and a new model has to be trained for their data. In the general case of sharing models across clients, this would require retraining (or finetuning) the shared model. While PFL methods like pFedME~\cite{Dinh2020PersonalizedFL} and Per-FedAvg~\cite{Fallah2020PersonalizedFL} can adapt to this setting by finetuning the global model locally, \ourmethod{} architecture offers a significant benefit. Since the shared model learns a meta-model over the distribution of clients, it can in principle generalize to new clients without retraining. 
With \ourmethod{}, once the shared model  has been trained on a set of clients, extending to a new set of novel clients requires little effort. We freeze the hypernetwork weights  and optimize an embedding vector .
Since only a small number of parameters are being optimized, training is less prone to overfitting compared to other approaches. The success of this process depends on the capacity of the hypernetwork to learn the distribution over clients and generalize to clients that have different data distributions. 

To evaluate \ourmethod{} in this setting, we use the CIFAR10 dataset, with a total of 100 clients, of which 90 are used for training and 10 are held out novel clients. To allocate data samples, for each client  we first draw a sample from a Dirichlet distribution with parameter , . Next we normalize the 's so that  for all , to obtain the vector . We now allocate samples according to the 's. For the training clients, we choose , whereas for the novel clients we vary . To estimate the ``distance'' between a novel client and the training clients, we use the total variation (TV) distance between the novel client and its nearest neighbor in the training set. The TV is computed over the empirical distributions . Figure~\ref{fig:gen} presents the accuracy generalization gap as a function of the total variation distance. \ourmethod{} achieves the best generalization performance for all levels of TV (corresponds to the different values for ).






\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\linewidth]{figures/generalization_tv_new.png}
    \caption{\textit{Generalization to novel clients}. The accuracy generalization gap between training and novel clients, defined as , where  denotes the average accuracy.}
    \label{fig:gen}
\end{figure}

\subsection{Heterogeneity of personalized classifiers}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/pesonalization.png}
    \caption{\textit{Model personalization}. Rows correspond to clients, each with their trained in a binary classification task and keeping their personalized classifier . Columns correspond to the feature extractor  of another client. The diagonal corresponds to the stanard training with  and . For better visualization, values denote accuracy normalized per row:  .}
    \label{fig:personalized_fe}
\end{figure}



We further investigate the flexibility of \ourmethod{}-PC in terms of personalizing different networks for different clients. 
Potentially, since clients have their own personalized classifier , the feature extractor component  generated by the HN may in principle become strongly similar across clients, making the HN redundant. The empirical results show that this is not the case because \ourmethod{}-PC out-performs FedPer~\cite{arivazhagan2019federated}. However this raises a fundamental question about the interplay between local and shared personalized components. We provide additional insight to this topic by answering the question: do the feature extraction layers generated by the \ourmethod{}-PC's hypernetwork significantly differ from each other?




To investigate the level of personalization in  achieved by \ourmethod{}-PC, we first train it on CIFAR10 dataset split among ten clients, with two classes assigned to each client. Next, for each client, we replace its feature extractor  with that of another client  while keeping its personal classifier  unaltered. 
Figure~\ref{fig:personalized_fe} depicts the normalized accuracy in this mix-and-match experiment. Rows correspond to a client, and columns correspond to the feature extractor of another client. 

Several effects in Figure~\ref{fig:personalized_fe} are of interest. First, \ourmethod{}-PC produces personalized feature extractors for each client since the accuracy achieved when crossing classifiers and feature extractors varies significantly. Second, some pairs of clients can be crossed without hurting the accuracy. Specifically, we had two clients learning to discriminate \textit{horse} vs. \textit{dog}. Interestingly, the client with \textit{ship} and \textit{airplane} classes performs quite well when presented with \textit{truck} and \textit{bird}, presumably because both of their feature extractors learned to detect sky. 


\subsection{Learned Client Representation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.\linewidth]{figures/cifar100_embedding.png}
    \caption{t-SNE visualization of the learned client representation  for the CIFAR100 dataset. Clients are tasked with classifying classes that belong to the same coarse class. Clients marked with the same color correspond to the same coarse-class, see text for details. \ourmethod{} clustered together clients from the same group.}
    \label{fig:embedding}
\end{figure}
In our experiments, we learn to represent each client using a trainable embedding vector . These embedding vectors therefore learn a continuous semantic representation over the set of clients. The smooth nature of this representation gives the HN the power to share information across clients. 
We now wish to study the structure of that embedding space.  

To examine how the learned embedding vectors reflect a meaningful representation over the client space, 
we utilize the hierarchy in CIFAR100 for generating clients with similar data distribution of semantically similar labels. Concretely, we split the CIFAR100 into 100 clients, where each client is assigned with data from one out of the twenty coarse classes uniformly (i.e., each coarse class is assigned to five clients). 

In Figure~\ref{fig:embedding} we project the learned embedding vectors into  using the t-SNE algorithm~\cite{Maaten2008VisualizingDU}. A clear structure is presented, in which clients from the same group (in terms of coarse labels) are clustered together. 

\section{Conclusion}

In this work, we present a novel approach for personalized federated learning. Our method trains a central hypernetwork to output a unique personal model for each client.
We show through extensive experiments significant improvement in accuracy on all datasets and learning setups.

Sharing across clients through a central hypernetwork has several benefits compared to previous architectures. First, since it learns a unified model over the distribution of clients, the model generalizes better to novel clients, without the need to retrain the central model. Second, it naturally extends to handle clients with different compute power, by generating client models of different sizes. 
Finally, this architecture decouples the problem of training complexity from communication complexity, since local models that are transmitted to clients can be significantly more compact than the central model.

We expect that the current framework can be further extended in several important ways. First, the architecture opens questions about the best way of allocating learning capacity to a central model vs distributed components that are trained locally. Second, the question of generalization to clients with new distribution awaits further analysis. 











\nocite{langley00}

\section*{Acknowledgements}
This study was funded by a grant to GC from the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). AS and AN were funded by a grant from the Israeli Innovation Authority, through the AVATAR consortium.


\bibliography{ref}
\bibliographystyle{icml2021}

\clearpage
\twocolumn[
\icmltitle{Supplementary Material for Personalized Federated Learning by Hypernetworks}]

\appendix



\section{Proof of Results}\label{app:theory}

\paragraph{Proof for Proposition 1.} Let  denote the optimal solution at client , then . Denote , we have 

Thus, our optimization problem becomes . 

WLOG, we can optimize  over the set of all matrices with orthonormal columns, i.e. . Since for each solution  we can obtain the same loss for , and select a  that performs Gram-Schmidt on the columns of . In case of fixed  the optimal solution for  is given by . Hence, our optimization problem becomes,

which is equivalent to PCA on .
\qed


\paragraph{Proof for Theorem 1.}
We note a  factor missing in the statement of Theorem 1 in the paper, the correct statement should use .

Using Theorem 4 from \cite{baxter2000model} and the notation used in that paper, we get that  where  is the covering number for . In our case each element of  is parametrized by  and the distance is given by 
{\small

}

From the triangle inequality and our Lipshitz assumptions we get

Now if we select a covering of the parameter space such that each  has a point  that is  away and each embedding  has an embedding  at the same distance we get an -covering in the  metric. From here we see that .\qed

\section{Experimental Details}\label{app:exp_details}

For all experiments presented in the main text, we use a fully-connected hypernetwork with  hidden layers of  hidden units each. For all relevant baselines, we aggregate over  clients at each round. We set  ,i.e.,  local steps, for the pFedMe algorithm, as it was reported to work well in the original paper~\citep{Dinh2020PersonalizedFL}.

\paragraph{Heterogeneous Data (Section 5.1).} 
For the CIFAR experiments, we pre-allocate  training examples for validation. For the Omniglot dataset, we use a 70\%/15\%/15\% split for train/validation/test sets. 
The validation sets are used for hyperparameter tuning and early stopping. We search over learning-rate , and personal learning-rate  for PFL methods using  clients. For the CIFAR datasets, the selected hyperparameters are used across all number of clients (i.e. ). 

\paragraph{Computational Budget (Section 5.2)} We use the same hyperparameters selected in Section 5.1. To align with previous works \cite{Dinh2020PersonalizedFL, liang2020think, Fallah2020PersonalizedFL}, we use a LeNet-based (target) network with two convolution layers, where the second layer has twice the number of filters in comparison to the first. Following these layers are two fully connected layers that output logits vector. In this learning setup, we use three different sized target networks with different numbers of filters for the first convolution layer. Specifically, for  sized networks, the first convolution layer consists of  filters, respectively. \ourmethod{}'s HN produces weights vector with size equal to the sum of the weights of the three sized networks combined. Then it sends the relevant weights according to the target network size of the client.





\section{Additional Experiments}\label{app:additional_exp}

\subsection{MNIST} \label{app:mnist}

We provide additional experiment over MNIST dataset. We follow the same data partition procedure as in the CIFAR10/CIFAR100 heterogeneity experiment, described in Section 5.1.

For this experiment we use a single hidden layer fully-connected (FC) hypernetwork. The main network (or target network in the case of \ourmethod{}) is a single hidden layer FC NN.

All FL/PFL methods achieve high classification accuracy on this dataset, which makes it difficult to attain meaningful comparisons. The results are presented in Table~\ref{tab:mnist_hetro}. \ourmethod{} achieves similar results to pFedMe.

\begin{table}[th]
    \vskip 0.15in
\tiny
    \centering
    \caption{Comparison on the MNIST dataset.}
    \begin{tabular}{l c c c }
    \toprule
    & \multicolumn{3}{c}{MNIST}\\
     \cmidrule{2-4}\\
     & 10  & 50 & 100\\
    \midrule
    FedAvg &  &  & \\
    Per-FedAvg &  &  & \\
    pFedMe &  &  & \\
    \midrule
    \ourmethod{} (ours) &  &  & \\
    \bottomrule
    \end{tabular}
    \label{tab:mnist_hetro}
\end{table}


\subsection{Exploring Design Choices}


In this section we return to the experimental setup of Section~5.1, 
and evaluate \ourmethod{} using CIFAR10 dataset with 50 clients. First, we examine the effect of the local optimization steps. Next, we vary the capacity of the HN and observe the change in classification accuracy. Finally we vary the dimension of the client representation (embedding).

\subsubsection{Effect of Local Optimization}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.\linewidth]{figures/la_steps_ablation.png}
    \caption{Effect of the number of local optimization steps on the test accuracy for the CIFAR10 dataset.}
    \label{fig:local_opt}
\end{figure}

First, we examine the effect of performing local optimization step and transmitting  back to the hypernetwork. Figure~\ref{fig:local_opt} shows the test accuracy throughout the training process. 
It compares training using the standard chain rule () with the case of training locally for  steps, .
Using our proposed update rule, i.e., making multiple local update steps, yields large improvements in both convergence speed and final accuracy, compared to using the standard chain rule (i.e., ). The results show that \ourmethod{} is relatively robust to the choice of local local optimization steps. As stated in the main text we set  for all experiments.

\subsubsection{Client Embedding Dimension}
Next, we investigate the effect of embedding vector dimension on \ourmethod{} performance. Specifically, we run an ablation study on set of different embedding dimensions . The results are presented in Figure~\ref{fig:ablation} (a). We show \ourmethod{} robustness to the dimension of the client embedding vector; hence we fix the embedding dimension through all experiments to , where  is the number of client.




\begin{figure*}[t]
\centering
    \begin{subfigure}[]{
    \includegraphics[width=0.45\linewidth]{figures/ablation_embedding.png}
    }
    \end{subfigure}
    \hfill
    \begin{subfigure}[]{
    \includegraphics[width=0.45\linewidth]{figures/ablation_hidden.png}
    }
     \end{subfigure}
    \caption{Test results on CIFAR10 showing the effect of (a) the dimension of the the client embedding vector, and; (b) the number of hypernetwork's hidden layers.}
    \label{fig:ablation}
\end{figure*}


\subsubsection{Hypernetwork Capacity}
Here we inspect the effect of the HN's capacity on the local networks performance. We conducted an experiment in which we change the depth of the HN by stacking fully connected layers.

We evaluate \ourmethod{} on CIFAR10 dataset using  hidden layers. Figure~\ref{fig:ablation} (b) presents the final test accuracy. \ourmethod{} achieves optimal performance with  and  hidden layers, with accuracies  and  respectively. We use a three hidden layers HN for all experiments in the main text.


\subsection{Spectral Normalization}\label{app:specnorm}

\renewcommand{\tabcolsep}{3pt}
\begin{table}[h]
    \vskip 0.15in
    \small
\centering
\caption{\ourmethod{} with spectral-normalization.}
    \begin{tabular}{l c c c }
    \toprule
    & \multicolumn{3}{c}{CIFAR10}\\
     \cmidrule{2-4}\\
     & 10  & 50 & 100 \\
     \midrule 
    \ourmethod{} (ours) &  &  & \\
    \bottomrule
    \end{tabular}
\label{tab:specnorm}
\end{table}

 We show in Theorem 1 that the generalization is affected by the hypernetworks Lipschitz constant . This theoretical result suggests that we can benefit from bounding this constant. Here we empirically test this by applying spectral normalization~\cite{spectral_normalization} for all layers of the HN. The results are presented in Table~\ref{tab:specnorm}. We do not observe any significant improvement compared to the results without spectral normalization (presented in Table 1 of the main text).






\subsection{Generalization to Novel Clients}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.\linewidth]{figures/ours_gen_scatter_k=1.png}
    \caption{Accuracy for novel clients on the CIFAR10 test set. Each point represents a different client. Total variation is computed w.r.t the nearest training set client.}
    \label{fig:gen_raw}
\end{figure}

Here we provide additional results on the generalization performance for novel clients, studied in Section 5.3 of the main text. Figure~\ref{fig:gen_raw} shows the accuracy of individual clients as a function of the total variation distance. Each point represents a different client, where the total variation distance is calculated w.r.t to the nearest training set client. As expected, the results show (on average) that the test accuracy decreases with the increase in the total variation distance.

\subsection{Fixed Client Representation}

We wish to compare the performance of \ourmethod{} when trained with a fixed vs trainable client embedding vectors. We use CIFAR10 with the data split described in Section 5.3 of the main text and  clients. We use a client embedding dimension of . We set the fixed embedding vector for client  to the vector of class proportions, , described in Section 5.3. \ourmethod{} achieves similar performance with both the trainable and fixed client embedding,  and  respectively.
 
\end{document}
