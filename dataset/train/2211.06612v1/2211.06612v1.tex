\documentclass{article}


\PassOptionsToPackage{numbers}{natbib}









\usepackage[final]{neurips_2022}




\usepackage{natbib}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{caption}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{xspace}
\usepackage[subrefformat=parens]{subfig}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{float}
\usepackage{dblfloatfix}
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}
\usepackage{authblk}
\usepackage{xspace}
\usepackage{multirow}  
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
urlcolor=blue,
}



\usepackage{amsmath,amsfonts,bm}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{xspace}
\usepackage{amsthm}
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}

\ifx\BlackBox\undefined
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  \fi

\ifx\QED\undefined
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}
\fi

\ifx\proof\undefined
\newenvironment{proof}{\par\noindent{\em Proof:\ }}{\hfill\BlackBox\
    \tau_c \geq \frac{L_h r}{4}+ \frac{1}{2}.
\epsilon_{\mathcal{D}_T}(h) \leq  
\left(
  \mathbb{P}_{\mathcal{D}_T}[h(x) \neq h_{pl}(x)] 
 - \epsilon_{\mathcal{D}_S}(h_{pl}) + q
\right)
\frac{\mathcal{R}_{\mathcal{D}_T}(h)(1+\gamma)}{\gamma \cdot \min{\{q,\gamma\}}} + \max_{i \in [C]}\{d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_{S_i},\mathcal{D}_{O_i})\}+\lambda,
    \mathcal{L} =  \mathcal{L}_{con} + \alpha \mathcal{L}_{self}  + \beta \mathcal{L}_{\textit{EMMD}}.
\label{pl}
\tilde{\mathcal{Y}}=\{
    \tilde{y}_i| \tilde{y}_i = \argmax_k \cos{(\phi(x_i),\textbf{c}_k)}, x_i \in \mathcal{D}_T \},
\label{eq:ce}
    \mathcal{L}_{self} = -\mathbb{E}_{x_i\in \mathcal{B}_T}  \left[\sum_{c=1}^C {\hat{y}^c_i}\log({p}_i^w[c])+{\hat{y}^c_i}\log( {p}_i^s[c]) \right]+\sum_{c=1}^C\text{KL}(\bar{p}_c||\frac{1}{C})+\omega H(p_i^w),
\label{eq:div_sl2}
    \tilde{\mathcal{D}}_{S}^k= \{x_i | \max_c (p_i^w[c]) \geq \tau_c, \argmax_c (p_i^w[c]) = k, x_i \in \mathcal{D}_T \},

\label{eq:contrastive}
    \mathcal{L}_{con}= -\mathbb{E}_{x_i\in \mathcal{B}_T} \log \frac{\exp(\vf_i \cdot \vk^+/\tau)}
    { \exp{(\vf_i \cdot \vk^+ /\tau)} + \sum_{j= 1}^{C+n_o-1} \exp{(\vf_i \cdot \vk^-_j/\tau)} },

    \tilde{\mathcal{D}}^c_{S} =\underset{|{\mathcal{X}}|=N,\hat{\mathcal{X}}\subseteq\mathcal{D}_t}{\arg \max} \sum_{ x_i\in {\mathcal{X}}} p^w_i[c],
    \label{eq:div_sl1}
\label{eq:w_update}
    \vw_c = \frac{1}{|\tilde{\mathcal{D}}_{S}^c|} \sum_{x_i \in \mathcal{D}_{S}^c} \vz_i. 
\label{eq:mmd}
    d_{\textit{MMD}}({\mathcal{S}},{\mathcal{T}}) =
\frac{1}{m}\sum_{i=1}^m  \vs_i \left( \frac{1}{m}\sum_{i'=1}^m \vs_{i'} 
- 
 \frac{1}{n} \sum_{j'=1}^n \vt_{j'}  \right)
+
\frac{1}{n}\sum_{i=1}^n \vt_i  \left( \frac{1}{n}\sum_{j'=1}^n \vt_{j'}
-
 \frac{1}{m} \sum_{i'=1}^m \vs_{i'}  \right).

\label{eq:da1}
    \mathcal{L}_{\textit{LMMD}} = \mathbb{E}_{x_i\in \mathcal{B}_T} \vf_i(\vq^-_i-\vq^+_i),

\max{ \{ 0, \vf\vq^- - \vf\vq^+\}}  = \max{\{ \vf\vq^+,\vf\vq^-\}}-\vf\vq^+ 
         \leq \log\left(\exp(\vf\vq^+)+\exp(\vf\vq^-)\right) - \vf\vq^+ ,
\label{eq:emmd}
    \mathcal{L}_{\textit{EMMD}} = -\mathbb{E}_{x_i\in \mathcal{B}_T}  \log \frac{\exp(\vf_i\vq^+_i/\tau)}{\exp(\vf_i\vq^+_i/\tau) + \exp(\vf_i\vq^-_i/\tau)}.
\mathcal{B}(x) = \{x': \exists A\in \mathcal{A} \ s.t. \ ||(x'-A(x)||<r \},\mathcal{R}_{\mathcal{D}_T}(h) = \mathbb{E}_{x\sim \mathcal{D}_T}[\mathbbm{1}(\exists x' \in \mathcal{B}(x) \ s.t. \ h(x')\neq h(x))].
    \mathcal{D}_S = \{x_i| \max_c \bar{h}(x_i) \geq \tau , x_i \in \mathcal{D}_T\},

    \tau \geq \frac{L_h r}{4}+ \frac{1}{2}.

    L_h||x-x'||\geq||\bar{h}(x) - \bar{h}(x')||\geq|\bar{h}(x)_{[i]}-\bar{h}(x')_{[j]}|+|\bar{h}(x)_{[j]}-\bar{h}(x')_{[i]}|\geq 4\tau-2\geq L_h r,
\label{eq:main}
    \epsilon_{\mathcal{D}_T}(h) \leq  
\left(
  \mathbb{P}_{\mathcal{D}_T}[h(x) \neq h_{pl}(x)] 
 - \epsilon_{\mathcal{D}_S}(h_{pl}) + q
\right)
\frac{\mathcal{R}_{\mathcal{D}_T}(h)(1+\gamma)}{\gamma \cdot \min{\{q,\gamma\}}} + \max_{i \in [C]}\{d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_{S_i},\mathcal{D}_{O_i})\}+\lambda, 
\label{lemma:src0}
    \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}_1^i \cup \mathcal{M}_2^i] =\mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}_1^i] + \mathbb{P}_{\mathcal{D}_{S_i}}  [\mathcal{M}_2^i] \leq q
\sum_{i\in [C]\setminus I}\mathbb{P}_{\mathcal{D}_S}[\mathcal{D}_{S_i}] \leq \frac{1}{\gamma}(\mathbb{P}_{x\in\mathcal{D}_{S}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S}}(h_{pl})+q)\label{lemma:popI_help1}
    \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_2]+ \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_3]+ \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i(h_{pl})\cap \overline{\mathcal{M}^i(h)}]
    \leq 
    \mathbb{P}_{x\sim \mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)]
\label{lemma:popI_help2}
\begin{aligned}
    \epsilon_{\mathcal{D}_{S_i}}(h_{pl})-\mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}_1^i] &\leq \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i(h_{pl})\setminus \mathcal{M}^i_1] \\
    & \leq \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_2]+ \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i(h_{pl})\cap \overline{\mathcal{M}^i(h)}]\\
    \text{by Eqn. \ref{lemma:popI_help1}}& \leq \mathbb{P}_{x\sim \mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)] - \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_3]
\end{aligned}
\label{lemma:popI_help3}
    \begin{aligned}
        \mathbb{P}_{x\sim \mathcal{D}_{S}}[h(x)\neq h_{pl}(x)] &= \sum_{i\in I} \mathbb{P}_{x\sim \mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)] \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}] \\
        &+\sum_{i\in [C]\setminus I} \mathbb{P}_{x\sim \mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)] \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}]\\
        \text{by Eqn. \ref{lemma:popI_help2}}&\geq 
        \sum_{i\in I} \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}](\epsilon_{\mathcal{D}_{S_i}}(h_{pl})-\mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}_1^i])
         \\
        &+\sum_{i\in [C]\setminus I}
        \mathbb{P}_{x\sim \mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)] \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}] \\
        \text{by Eqn. \ref{lemma:src0} and Definition of }&> 
        \sum_{i\in I} \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}](\epsilon_{\mathcal{D}_{S_i}}(h_{pl})-q)
         \\
        &+\sum_{i\in [C]\setminus I}
        (\epsilon_{\mathcal{D}_{S_i}}(h_{pl}) + \gamma) \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}]\\
        &\geq \epsilon_{\mathcal{D}_{S_i}}(h_{pl}) -q + \gamma \sum_{i\in [C]\setminus I} \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}]
    \end{aligned}
\epsilon_{\mathcal{D}_{S_i}}(h)\leq \mathbb{P}_{x\sim\mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)]- \epsilon_{\mathcal{D}_{S_i}}(h_{pl}) + 2q
    \begin{aligned}
        \epsilon_{\mathcal{D}_{S_i}}(h) &= \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_2]+ \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_3] +\mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_1]\\
        \text{by Eqn. \ref{lemma:popI_help2}}&\leq
        \mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_2]  +2\mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_1] + \mathbb{P}_{x\sim\mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S_i}}(h_{pl})\\
        \leq&2(\mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_2]  +\mathbb{P}_{\mathcal{D}_{S_i}}[\mathcal{M}^i_1]) + \mathbb{P}_{x\sim\mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S_i}}(h_{pl})\\
        \text{by Eqn. \ref{lemma:src0}}&\leq
        2q  + \mathbb{P}_{x\sim\mathcal{D}_{S_i}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S_i}}(h_{pl})
    \end{aligned}
\label{thm1:help0}
    \epsilon_{\mathcal{D}_T}(h) = \sum_{i=1}^C \mathbb{P}_{\mathcal{D}_{T}}[\mathcal{D}_{T_i}] \epsilon_{\mathcal{D}_{T_i}}(h)
    =\sum_{i\in G_1} \mathbb{P}_{\mathcal{D}_{T}}[\mathcal{D}_{T_i}] \epsilon_{\mathcal{D}_{T_i}}(h)+\sum_{i\in G_2} \mathbb{P}_{\mathcal{D}_{T}}[\mathcal{D}_{T_i}] \epsilon_{\mathcal{D}_{T_i}}(h)
\label{thm1:help1}
    \epsilon_{\mathcal{D}_{T_i}}(h)\leq q,
\label{thm1:help2}
    \sum_{i \in G_2} \mathbb{P}_{\mathcal{D}_{T}}[\mathcal{D}_{T_i}] \leq \frac{\mathcal{R}_{\mathcal{D}_T}(h)}{\min{\{q,\gamma\}}},
\label{thm1:help3}
\begin{aligned}
    \epsilon_{\mathcal{D}_{T_i}}(h) &= \mathbb{P}_{\mathcal{D}_{T_i}}[\mathcal{D}_{S_i}] \epsilon_{\mathcal{D}_{S_i}}(h) + \mathbb{P}_{\mathcal{D}_{T_i}}[\mathcal{D}_{O_i}]\epsilon_{\mathcal{D}_{O_i}}(h)\\
    &\leq 
    \mathbb{P}_{\mathcal{D}_{T_i}}[\mathcal{D}_{S_i}] \epsilon_{\mathcal{D}_{S_i}}(h)
    +\mathbb{P}_{\mathcal{D}_{T_i}}[\mathcal{D}_{O_i}]
    \left(
    \epsilon_{\mathcal{D}_{S_i}}(h)+ d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_{S_i},\mathcal{D}_{O_i})
    +\lambda_i
    \right) \\
    &\leq \epsilon_{\mathcal{D}_{S_i}}(h) +d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_{S_i},\mathcal{D}_{O_i})
    +\lambda_i,
\end{aligned}
\label{eq:help1}
    \begin{aligned}
        \epsilon_{\mathcal{D}_T}&\leq \sum_{i \in G_1} \mathbb{P}_{\mathcal{D}_{T}}[\mathcal{D}_{T_i}] q + \sum_{i\in G_2} \mathbb{P}_{\mathcal{D}_{T}}[\mathcal{D}_{T_i}] \epsilon_{\mathcal{D}_{S_i}}(h) + \max_i \{d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_{S_i},\mathcal{D}_{O_i})\} + {\lambda}' \\
        &\leq q + \sum_{i\in G_2} \mathbb{P}_{\mathcal{D}_{T}}[\mathcal{D}_{T_i}] \epsilon_{\mathcal{D}_{S_i}}(h) + \max_i \{d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_{S_i},\mathcal{D}_{O_i})\} + {\lambda}' \\
         (\text{by Eqn. \ref{thm1:help2}})& \leq \frac{\mathcal{R}_{\mathcal{D}_T}(h)}{\min{\{q,\gamma\}}} \epsilon_{\mathcal{D}_{S}}(h)
         +\max_i \{d_{\mathcal{H}\Delta\mathcal{H}}(\mathcal{D}_{S_i},\mathcal{D}_{O_i})\} + {\lambda}'+q\\
    \end{aligned}
\label{eq:help2}
\begin{aligned}
    \epsilon_{\mathcal{D}_{S}}(h)
    &=\sum_{i\in I}   \epsilon_{\mathcal{D}_{S_i}}(h)
    \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}]
    +
    \sum_{i\in [c] \setminus I}  \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}]\epsilon_{\mathcal{D}_{S_i}}(h) \\
    \text{(by Lemma \ref{lemma:popI})}
    &\leq \frac{1}{\gamma}(\mathbb{P}_{x\sim\mathcal{D}_{S}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S}}(h_{pl})+q) + \sum_{i\in [c] \setminus I}  \mathbb{P}_{\mathcal{D}_{S}}[\mathcal{D}_{S_i}]\epsilon_{\mathcal{D}_{S_i}}(h) \\
    \text{(by Lemma \ref{lemma:errI})}
    & \leq \frac{1}{\gamma}(\mathbb{P}_{x\sim\mathcal{D}_{S}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S}}(h_{pl})+q) \\
    &+ \mathbb{P}_{x\in\mathcal{D}_{S}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S}}(h_{pl})+2q\\
    & \leq \frac{1+\gamma}{\gamma}(\mathbb{P}_{x\sim\mathcal{D}_{T}}[h(x)\neq h_{pl}(x)]-\epsilon_{\mathcal{D}_{S}}(h_{pl})+q)+q.
\end{aligned}

Combining the results of Eqn. \ref{eq:help1} and Eqn. \ref{eq:help2}, we prove the result of Theorem \ref{thm:main1}. Specifically, in Eqn. \ref{eq:main}
, the  is a constant  the expansion constant  and task risk of ideal optimal model.
\end{proof}

\section{Additional Experimental Details}\label{supple_details}
\subsection{Implementation Details}
We train our model on four Nvidia Geforce GTX 1080Ti graphic cards, using SGD with a momentum of 0.9, and a weight decay of 0.0005. We conduct experiments on VisDA, Office-Home and DomainNet and set the batch size to 64 for all benchmarks. The initial learning rate is set as 5e-4 for VisDA, 2e-2 for Office-Home, and 1e-2 for DomainNet. The total epoch is set as 60 for VisDA, 30 for Office-Home and DomainNet. 
We apply the learning rate scheduler  following~\cite{shot}, where training process  changes from 0 to 1, and we further reduce the learning rate by a factor of 10 after 40 epochs on Visda, 15 epochs on Office-Home and DomainNet. We find that most hyperparameters of \textit{DaC} do not require to be heavily tuned. As can be seen in Table \ref{tab:sensitive}, the performance is not sensitive to the choice of , and we set the confidence threshold  as 0.95 for all experiments following \cite{fixmatch,li2021semiuda}.
We adopt a set of hyperparameters  for the large scale benchmarks VisDA and DomainNet, and  for most transfer senarios of Office-Home. 
\begin{table}[!htbp]
    \vspace{-3mm}
    \centering
    \caption{Sensitive analysis of .}
    \label{tab:sensitive}
    \begin{tabular}{ccccccc}
					\toprule
					 & 0.91 &	0.93 & 0.95 & 0.97 & 	0.98 &	target-supervised   \\
					\midrule
					\multicolumn{1}{c}{Avg. (\%)} & 87.06 &	87.27 &	87.34 & 87.39 &	87.19 &	89.6\\
					\bottomrule
		\end{tabular}

    \vspace{-3mm}
\end{table}

\subsection{Baseline Methods on DomainNet}
We compare \textit{DaC} with source-present and source-free domain adaptation methods.
DomainNet is widely used in multi-source domain adaptation tasks, and its subset is used as one of the benchmarks of single-source domain adaptation benchmark by \cite{saito2019semidomainnet}. The results of MME~\cite{saito2019semidomainnet} and CDAN~\cite{long2018conditional} are copied from \cite{saito2019semidomainnet}. The rest source-present and source-free methods are implemented by their official codes. We choose the learning rate for all baselines by five-fold cross-validation, and apply the training scheduler of their own. 

\section{Algorithm for DaC}\label{supple_Algo}
As shown in Algorithm \ref{alg:dac}, our method consists of self-training by pseudo-labeling, adaptive contrastive learning, and distribution alignment.
After self-training to achieve preliminary class-wise adaptation, we divide target data as source-like and target-specific to conduct representation learning. The adaptive contrastive learning framework exploits local and global information and improves feature discriminability. Distribution alignment reduces the mismatching between source-like and target-specific samples.   




\begin{algorithm}
\caption{Training of DaC}\label{alg:dac}
\begin{algorithmic}
\Require unlabeled target data , source model:, augmentation set , threshold , batch size .
\State \textbf{Initialization:} target model , memory bank by forward computation: , source-like and target specific features by Eqn. \ref{eq:div_sl1}.
\While{}
\State Obtain the pseudo labels  based on Eqn. \ref{pl}.
\For{}
\State From , draw a mini-batch .\Comment{batch-training}
\For{}
\State ;
\State ;
\State  ;
\Comment{update memory bank}
\State update the source-like and target-specific sample based on Eqn. \ref{eq:div_sl2}.
\Comment{divide}
\EndFor
\State Compute  using by Eqn. \ref{eq:ce} \Comment{self-training};
\State Generate prototypes  from memory bank;
\State Compute  by Eqn. \ref{eq:contrastive}; \Comment{contrast}
\State Compute  by Eqn. \ref{eq:emmd}; \Comment{distribution alignment}
\State ;
\State .
\EndFor
\EndWhile
\State\textbf{Output} 
\end{algorithmic}
\end{algorithm}


\end{document}