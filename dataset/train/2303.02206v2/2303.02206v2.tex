\pdfoutput=1



\documentclass[11pt]{article}

\usepackage{EMNLP2023}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}




\title{Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models}



\author{Navid Madani \and Kenneth Joseph \and Rohini K. Srihari  \\
        University at Buffalo \\ smadani,kjoseph,rohini@buffalo.edu}


\begin{document}
\maketitle
\begin{abstract}

Answering questions over domain-specific graphs requires a tailored approach due to the limited number of relations and the specific nature of the domain. Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task. By representing the questions as Prolog queries, which are readable and near close to natural language in representation, we facilitate the generation of programmatically derived answers. To validate the effectiveness of our approach, we evaluate it using a well-known benchmark dataset, MetaQA. Our experimental results demonstrate that our method achieves accurate identification of correct answer entities for all test questions, even when trained on a small fraction of annotated data. Overall, our work presents a promising approach to addressing question answering over domain-specific graphs, offering an explainable and robust solution by incorporating logical programming languages.
\end{abstract}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{qa_pipeline.png}
  \caption{The complete inference pipeline of our proposed method. Note that the inference tree on the right side is a subset of the answer drawn here to clarify the schema of the model's output.}
  \label{fig:qa_pip}
\end{figure*}

\section{Introduction}

Question Answering over Knowledge Graphs (KGQA) poses significant challenges in the field of Natural Language Processing (NLP). As structured knowledge graphs capturing rich semantic information become prevalent, there is a pressing need for intelligent systems that can reason effectively and provide accurate answers to intricate questions within specific domains. The primary focus of KGQA is to bridge the gap between human language and structured knowledge representations. When presented with a question in natural language, KGQA systems aim to traverse the knowledge graph consisting of entities and their relationships, extracting relevant information to generate precise answers. This task demands not only language comprehension but also the ability to perform logical reasoning across the edges of the graph to derive meaningful insights. Although large language models (LLMs) powered by deep learning have shown remarkable capabilities in natural language understanding and generation, they may not be specifically trained on a particular knowledge source or possess a deep understanding of domain-specific facts. However, LLMs can serve as a valuable tool to represent questions within a domain, extracting query or question meanings. Leveraging logical programming approaches, these representations can be processed to handle reasoning and knowledge representation. One notable advantage of this approach is that it empowers users of the system to manage knowledge dynamically. They can modify, delete, or add new entries into the knowledge graph without requiring changes to the system itself. By integrating logical programming techniques with LLMs, KGQA systems gain the flexibility to adapt to evolving knowledge requirements while maintaining their functionality.

In this paper, we address the challenges of domain-specific KGQA by combining the strengths of large language models and logical programming. We propose an approach that utilizes LLMs to represent questions within a specific domain, extracting their meanings, while employing logical programming techniques for reasoning and knowledge representation. Our objective is to demonstrate how this integration enables robust and adaptable KGQA systems that can navigate domain-specific knowledge graphs and provide accurate answers to complex questions. To evaluate the effectiveness of our proposed approach, we conduct experiments using the MetaQA dataset \cite{zhang2017variational}, a widely adopted benchmark in KGQA research. By comparing our method against state-of-the-art approaches, we demonstrate its capability to accurately identify the correct answer entities for a range of questions. Notably, our experiments show promising results even when our model is trained on only a small fraction of the available training data, indicating its efficiency and generalization ability.

The contributions of this paper are two-fold: 
\begin{itemize}
    \item We propose a novel approach to equip LLMs with logical programming languages for domain-specific KGQA, enhancing their reasoning capabilities and providing explainable solutions. \footnote{\href{https://github.com/navidmdn/logic_based_qa}{https://github.com/navidmdn/logic\_based\_qa}}
    \item  We demonstrate the effectiveness of our approach through comprehensive experiments on the MetaQA dataset, showcasing its ability to accurately represent questions given only a small set of annotated data.
\end{itemize}


\section{Related Work}
\label{gen_inst}

A variety of approaches have been taken to address the problem of multi-hop question answering. A number of prior works have used graph embedding models to encode entities and relations in a knowledge graph and then score the triples in a KG and construct a scoring function so that the score for a correct triple is higher than the score of an incorrect one  \citep{Nickel2011ATM,Yang2014EmbeddingEA,Balazevic2019TuckERTF,Dettmers2017Convolutional2K,Vashishth2019InteractEIC}. Others have approached the problem by constructing a function that maps the question embedding along with an embedding of the graph or a subgraph around the question entity to the answer entity's embedding in knowledge graph \citep{Sun2018OpenDQ,Saxena2020ImprovingMQ,Sun2019PullNetOD,He2021ImprovingMK}. Still others adapt a slightly different method, training a teacher model to learn intermediate signals and a student model to answer the questions \citep{He2021ImprovingMK}. There also has been efforts by \citep{Xie2022ASF} that pushed the performance of these models on 2 and 3 hop splits to the limits. They propose a sequential reasoning self-attention mechanism which is guided by a GRU-inspired Flow Control (GFC) and their work is inspired by \citep{shi-etal-2021-transfernet}. Finally, most relevant to our work, \citep{Yang2014JointRE} and \citep{Yang2015KnowledgebasedQA} try to learn the logical form of the natural language questions by building a semantic embedding space. However, our work differs from theirs in that we use LLMs to represent the question in logical form instead of manually building a semantic mapping space. The present work is thus the first to use large language models to represent questions in logical form and equip LLMs with logical programming tools to answer questions.


\section{Dataset}

The MetaQA dataset is a widely used benchmark dataset for question answering over knowledge graphs (KGQA). The MetaQA dataset consists of questions that require reasoning over a given knowledge graph. The knowledge graph represents a structured database of 134,741 facts and 9 relations, providing a rich source of information for answering domain specific questions. Appendix \ref{ap:metaqa} describes the dataset in detail.

\section{Approach}
\label{sec:approach}


\subsection{Question to Logical Form}
\label{sec:q2l}

To facilitate the translation of questions into their corresponding logical forms, we begin by developing a question comprehension module. To accomplish this task, we harness the power of encoder-decoder transformer models, known for their exceptional potential in sequence to sequence transformation ability \citep{Vaswani2017AttentionIA}. To collect the dataset necessary for fine-tuning a sequence-to-sequence transformer model, we leverage the multi-hop path information provided by the MetaQA dataset. We will go over the implementation details of annotating each question with a prolog query using it's intermediate steps in appendix \ref{ap:annotation}. From the total pool of 329,282 multi-hop training examples in MetaQA, we randomly sample and annotate subsets consisting of 100, 250, 500, and 1000 samples with each subset consisting of equal number of examples from each of the 1, 2 and 3 hop samples. These subsets are respectively labeled as \emph{s100, s250, s500} and \emph{s1000}. 
To transform the question into an intermediate query representation, we employ a T5-small sequence-to-sequence transformer model \citep{Raffel2019ExploringTL}. This model effectively learns to generate accurate representations of the questions, serving as a bridge between natural language input and logical query output. For the training process, we fine-tune the model using each of the annotated training sets, iterating through 5000 training steps. The best-performing model is selected based on the exact match score obtained from the development dataset.To optimize the model's performance, we utilize the AdamW optimizer with an initial learning rate of 5e-5. Additionally, a linear learning rate scheduler is employed. The training is conducted using a batch size of 8, making efficient use of a single A100 GPU for computational acceleration.

\subsection{Question Answering}

The question answering process in our proposed model is illustrated in Figure \ref{fig:qa_pip}. To begin, we transform each triple in the knowledge base of the MetaQA dataset into a first-order logic predicate. For instance, given the triple \emph{(Innocence | written\_by | Hilary Brougher)}, we construct the corresponding predicate \textbf{written\_by(Innocence, Hilary Brougher)}.

When processing a specific question, we generate its logical form using the transformer model. The logical form provides a structured representation of the question's meaning. Subsequently, we replace the \emph{ENT} token in the logical form with the corresponding entity ID from the knowledge graph. This substitution results in the final Prolog query. Finally, we execute the Prolog query, which involves querying the knowledge graph. By executing the query, we retrieve both the answers to the question and the logical path that connects the question entity to the answer entities. This path provides valuable insights into the reasoning process and the information flow within the knowledge graph.

\begin{table*}

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline

Models                & MetaQA-1hop & MetaQA-2hop & MetaQA-3hop \\
\hline\hline
GraftNet \citep{Sun2018OpenDQ}           & 97.0     & 94.8     & 77.7     \\
PullNet \citep{Sun2019PullNetOD}            & 97.0     & 99.9     & 91.4     \\
EmbedKGQA \citep{Saxena2020ImprovingMQ}            & 97.5     & 98.8     & 94.8     \\
NSM \citep{He2021ImprovingMK}               & 97.1     & 99.9     & 98.9     \\
TransferNet \citep{shi-etal-2021-transfernet}       & 97.5     & 100.0     & 100.0     \\
GFC \citep{Xie2022ASF}               & 97.7     & 100.0     & 100.0     \\
\hline
T5-small+prolog+250 samples  & 98.67    & 97.77    & \textbf{100.0}    \\
T5-small+prolog+500 samples  & \textbf{100.0}    & 99.33    & \textbf{100.0}         \\
T5-small+prolog+1000 samples & \textbf{100.0}    & \textbf{100.0}    & \textbf{100.0}    \\    
\hline

\end{tabular}
\caption{Comparison of hit@1 score of previous methods compared to our method over multi-hop test
datasets. The scores for the best model among 5 iterations of sampling is reported for our proposed method.}
\label{tab:results}

\end{center}
\end{table*}
\section{Experiments and Results}
MetaQA questions mostly come with multiple answers. Prior methods have used hit@1 as a metric to measure the performance of their model. This means that they measure if the highest ranked entity given by their model exists in the answer set. Our approach produces the exact solution path inside the knowledge graph and consequently it outputs all of the answers to the question instead of producing a score distribution over graph entities (as depicted in Figure \ref{fig:qa_pip}). For the sake of comparison, we also measure the hit@1 metric for our model over multi hop test datasets. In other words, we randomly pick one of the answer entities and assume it is the rank 1 answer of the model and consequently we calculate the hit@1 score. Table \ref{tab:results} compares our method with prior work. 

In order to get robust results we repeat the process of sampling training data and annotating it 5 times and each time we sampled 100, 250, 500 and 1000 samples. The sampling process was straightforward; each time we sample randomly and equally from each of the 1-hop, 2-hop and 3-hop training datasets. We also annotated 3000 samples from the validation and test set of the MetaQA dataset. Figure \ref{fig:conf} shows the variance of performance on each of these datasets. Since the variance was high on the test set with 100 samples we only reported s250, s500 and s1000 in table \ref{tab:results}. According to these results the 3-hop test set's representation is the easiest to learn since it doesn't come with many variations of natural language to describe. On the other hand the 2-hop dataset is the hardest to learn. However, all of these samples are collected randomly. But with a manual and careful sample collection, we can see that even 500 samples are enough to learn the whole dynamics of this dataset and learn to represent questions in logical form. We conclude that our model is capable of correctly answering all questions in the test dataset with only 1000 annotated examples.




\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\linewidth]{conf.png}
  \caption{The variance of hit@1 of each model based on the number of training examples available in each training dataset}
  \label{fig:conf}
\end{figure}


\subsection{Robustness of the method}
In order to get robust results we repeat the process of sampling training data and annotating it 5 times and each time we sampled 100, 250, 500 and 1000 samples. The sampling process was straightforward; each time we sample randomly and equally from each of the 1-hop, 2-hop and 3-hop training datasets. We also annotated 3000 samples from the validation and test set of the MetaQA dataset. Figure \ref{fig:conf} shows the variance of performance on each of these datasets. Since the variance was high on the test set with 100 samples we only reported s250, s500 and s1000 in table \ref{tab:results}. According to these results the 3-hop test set's representation is the easiest to learn since it doesn't come with many variations of natural language to describe. On the other hand the 2-hop dataset is the hardest to learn.

However, all of these samples are collected randomly. But with a manual and careful sample collection, we can see that even 500 samples are enough to learn the whole dynamics of this dataset and learn to represent questions in logical form.



\section{Conclusion}

In this work, we have presented a framework that leverages logical programming languages as a powerful tool for large language models (LLMs) for domain specific question answering over knowledge graphs. By utilizing logical programming languages such as Prolog which benefits from the inherent similarity between the representations of meaning in logical programming languages and natural language, we have showcased the ability to bridge the gap between natural language understanding and logical reasoning. We evaluated our model on a relatively small dataset and showed that it is able to fully answer questions given a small subset of annotated representations due to the pre-trained knowledge encoded even in relatively small LLMs.

\section{Limitations}

 MetaQA dataset is a synthesized dataset focused on the movie domain. Although it provides a comprehensive evaluation environment for a domain-specific question answering over knowledge graphs, it may not capture the full complexity and diversity of real-world scenarios. For instance, it does not encompass a wide range of relations found in open-domain datasets like WebQuestions, which are based on Freebase and cover a broader domain. To mitigate this limitation, future research could explore approaches such as relation and entity matching. By incorporating techniques to match entities and relations in a more flexible and adaptive manner, our model could potentially be extended to handle datasets like WebQuestions and address a broader range of real-world KGQA scenarios.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Details of MetaQA dataset}
\label{ap:metaqa}

Each question in the MetaQA dataset is associated with the provided knowledge graph, comprising entities, relations, and their connections. The dataset incorporates a diverse range of questions, covering various domains and types of queries. These questions often involve multiple hops or intermediate steps to reach the correct answer. These multi-hop paths guide the reasoning process required to answer the questions accurately. By traversing these paths, the model must navigate through different entities and relations to arrive at the correct answer. The dataset also provides the intermediate steps that leads us from question entity to the answer . This is one of the important reasons that we chose MetaQA dataset. Table \ref{tab:metaqastats} briefly describes the statistics of this dataset.

In order to capture reverse relations we replicate each relation in the knowledge graph with \emph{\_reverse} suffix resulting in 9 more relations. For instance, if we have a triple \emph{(Chopper | starred\_actors | Eric Bana)} we will also include the triple \emph{(Eric Bana | starred\_actors\_reveresed | Chopper)} in the knowledge graph.

\begin{table}
\centering
\begin{tabular}{l|l l l}
\hline
\textbf{MetaQA} & \textbf{train} & \textbf{dev} & \textbf{test}\\
\hline
1-hop & 96106 & 9992 & 9947 \\
2-hop & 118980 & 14872 & 14872 \\
3-hop & 114196 & 14274 & 14274 \\
\hline
\end{tabular}

\caption{Statistics for MetaQA dataset}
\label{tab:metaqastats}
\end{table}

\section{Question to Logical Form Annotation}
\label{ap:annotation}

Each question in MetaQA dataset comes with the inference path inside the knowledge graph. For example, for the 2-hop question \emph{”the movies
written by [Hilary Brougher] were directed by who?”} there exists an inference path of \emph{writer\_movie\_director} which shows the sequence of relations we need to traverse in the graph to reach the answer entity from the question entity \emph{Hilary Brougher}. We use this inference path and annotate the question with the correct prolog query. To do so, we first break down the inference path into pairs. For the example above we would get \emph{writer\_movie} and \emph{movie\_director} pairs. Then we map each of the pairs to their corresponding predicate. Table \ref{tab:mapping} provides a list of all mappings that are available in the dataset. To ensure that the model focuses solely on the representation of the question itself, we employ a substitution strategy. Specifically, we replace the question entity with a designated string placeholder denoted as \emph{ENT}. For example, if we have the question "Which movies directed by [ENT] were written by whom?" we construct the corresponding Prolog query as \textbf{directed\_by\_reverse(ENT, X), written\_by(X, Y)}. This query captures the essence of the original question while preserving its logical structure

\begin{table}[H]

\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Inference Pair}             & \textbf{Predicate}                        \\ \hline
actor\_movie                        & starred\_actors\_reverse                  \\ \hline
director\_movie                     & directed\_by\_reverse                     \\ \hline
movie\_actor                        & starred\_actors                           \\ \hline
movie\_director                     & directed\_by                              \\ \hline
movie\_genre                        & has\_genre                                \\ \hline
movie\_imdbrating                   & has\_imdb\_rating                         \\ \hline
movie\_imdbvotes                    & has\_imdb\_votes                          \\ \hline
movie\_language                     & in\_language                              \\ \hline
movie\_tags                         & has\_tags                                 \\ \hline
movie\_writer                       & written\_by                               \\ \hline
movie\_year                         & release\_year                             \\ \hline
{tag\_movie}    &  {has\_tags\_reverse}   \\ \hline
{writer\_movie} & {written\_by\_reverse} \\ \hline
\end{tabular}
\end{center}
\caption{Mapping between different inference pairs and Prolog predicates}
\label{tab:mapping}

\end{table}

\end{document}
