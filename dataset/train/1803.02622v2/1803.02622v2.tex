\section{EXPERIMENTS - POSE ESTIMATION}

\subsection{Datasets for training}
\tabref{tab:train_sets} shows that the proposed \textit{PoseNet3D} already reaches good results on the evaluation split of both datasets when trained only on \textit{MKV-t}. Training a network only on \textit{CAP-t} leads to inferior performance, which is due to starkly limited variation in the training split of the Captury dataset, which only contains a single actor and scene. Training jointly on both sets performs roughly on par with training exclusively on \textit{MKV-t}. Therefore we use \textit{MKV-t} as default training set for our networks and evaluate on \textit{CAP-e} for following experiments. Furthermore, we confirm generalization of our \textit{MKV-t} trained approach on the \textit{InOutDoor} Dataset \cite{mees2016choosing}. Because the dataset does not contain pose annotations we present qualitative results in the supplemental video.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
 Training set & \textit{CAP-e} full & \textit{CAP-e} subset & \textit{MKV-e} \\
\hline\hline
\textit{MKV-t}                      & $0.627$  & $0.618$  & $0.793$\\
\textit{CAP-t}                      & $0.603$  & $0.588$  & $0.665$\\
\textit{CAP-t} \& \textit{MKV-t}    & $0.633$  & $0.625$  & $0.794$\\
\hline
\end{tabular}
\caption{Performance measured as area under the curve (AUC) for different training sets of \textit{VoxelPoseNet}. \textit{CAP-t} does not generalize to \textit{MKV-e}, whereas \textit{MKV-t} provides sufficient variation to generalize to \textit{CAP-e}. Training jointly on \textit{CAP-t} and \textit{MKV-t} doesn't improve results much anymore. }\label{tab:train_sets}
\end{center}
\end{table}

\subsection{Comparison to literature}
In \tabref{tab:epe_results} we compare our approach with common baseline methods. 
The first baseline is the Skeleton Tracker integrated in Microsofts Software Development Kit\footnote{https://www.microsoft.com/en-us/download/details.aspx?id=44561} (Kinect SDK). We show that its performance heavily drops on the more challenging subset and therefore argue that it is unsuitable for many robotics applications. Furthermore, \figref{fig:pck_over_dist} shows that the Kinect SDK is unable to predict keypoints farther away than a certain distance. The qualitative examples in \figref{fig:qualitative_results} reveal that the SDK is led astray by objects and is unable to distinguish if a person is facing towards or away from the camera, which expresses itself in mixing up left and right side.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
 & Captury full & Captury subset & Multi Kinect \\
\hline\hline
Kinect SDK & $13.5$ & $16.4$ & $8.9$\\
Naive Lifting & $14.7$ & $15.2$ & $8.8$\\
Tome \etal\cite{tome_lifting_2017} & $22.7$ & $21.9$ & $15.1$\\

Proposed & $\mathbf{11.2}$ & $\mathbf{11.6}$ & $\mathbf{6.1}$\\
\hline
\end{tabular}
\caption{Average mean end point error per keypoint of the predicted 3D pose for different approaches in \SI{}{cm}. For the Captury dataset we additionally report results on the subset of non-frontal scenes and with object interaction.}\label{tab:epe_results}
\end{center}
\end{table}

The baseline named Naive Lifting uses the same Keypoint detector for color images as our proposed approach and simply picks the corresponding depth value from the depth map. It chooses the depth value as median value of the $3$ closest neighbors. The approach shows reasonable performance, but is prone to pick bad depth values from the noisy depth map. Also any kind of occlusion results into an error, which is seen in \figref{fig:qualitative_results}.

Tome \etal\cite{tome_lifting_2017} predicts scale and translation normalized poses. So in order to compare the results to the other approaches we provide the algorithm with ground truth scale and translation. For every prediction we seek scale and translation in order to minimize the reconstruction error between ground truth and prediction. \tabref{tab:epe_results} shows that the approach reaches competitive results, but performs worst in our comparison, which is reasonable given the lack of depth information. In \figref{fig:auc_curves_cap} the approach stays far behind, which partly lies in the fact that the approach misses to provide predictions in $8.7 \%$ of the frames of \textit{CAP-e}, which compares to $12.4 \%$ for Kinect SDK and $~0 \%$ for Naive Lifting and our approach.

\textit{VoxelPoseNet} outperforms its baseline methods, because it exploits both modalities. On the one hand, color information helps to disambiguate left and right side, which is infeasible from depth alone. On the other hand, the depth map provides valuable information to exactly infer the 3D keypoint. Furthermore, the network learns a prior about possible body part configurations, which makes it possible to infer 3D locations even for completely occluded keypoints (see \figref{fig:qualitative_results}).

\begin{figure}
    \centering  
        \includegraphics[width=\columnwidth, height=.6\columnwidth]{./figures/pose/auc_curve_new2.tikz}
\caption{Performance of different algorithms on \textit{CAP-e} measured as percentage of correct keypoints (PCK) on the more challenging subset of non-frontal poses and object interaction.}\label{fig:auc_curves_cap}
\end{figure}

\begin{figure}
    \centering  
        \includegraphics[width=\columnwidth, height=.6\columnwidth]{./figures/pose/auc_pck_dist_new2.tikz}
\caption{Percentage of correct keypoints (PCK) over their distance to the camera. Most approaches are only mildly affected by the keypoint distance to the camera, but the Kinect SDK can only provide predictions in a limited range.}\label{fig:pck_over_dist}
\end{figure}


\begin{figure*}
\centering
    \includegraphics[]{./figures/pose/quali_tikz_new.tikz}
\caption{Typical failure cases of the algorithms evaluated for samples from \textit{CAP-e}. The first row shows the scene and the other two rows depict the ground truth skeleton in dashed blue and the prediction in solid green and red. Green color indicates the persons right side. Predictions of our proposed approach are shown in the last row, whereas the middle row shows predictions by other algorithms. The first two columns correspond to predictions of the Kinect SDK, the next two are by the Naive Lifting approach and the last two by the approach presented by Tome \etal\cite{tome_lifting_2017}. Typical failures for the SDK are caused by objects and or people that face away from the camera. Naive Lifting fails when any sort of keypoint occlusion is present. }
\label{fig:qualitative_results}
\end{figure*}


\subsection{HandNormalNet}
We use the annotated samples of \textit{MKV} to evaluate the accuracy of the normal estimation we achieve with the adopted network from \cite{zb2017hand}. For the $129$ samples we get an average angular error of $60.3$ degree, which is sufficient for the task learning application as is shown in the next section.



\section{EXPERIMENTS - ACTION LEARNING}
\label{exp_action_learning}
We evaluate our recently proposed graph-based approach~\cite{twelsche17iros} for learning a mobile manipulation task from human demonstrations on data acquired with the approach for 3D human pose estimation presented in this work. We evaluate the methods on the same four tasks as in our previous work~\cite{twelsche17iros}: one task of opening and moving through a room door and three tasks of opening small furniture pieces. The tasks will be referred to as room door, swivel door, drawer, and sliding door. Each consists of three parts. First a specific part of the object is grasped, \ie, a handle or a knob, then the object is manipulated according to its geometry, and lastly released. The demonstrations were recorded with a Kinect v2 at $\SI{10}{Hz}$. As we need to track both, the manipulated object and the human teacher, the actions were recorded from a perspective that show the human from the side or back making pose estimation challenging. For an example of the setup see \figref{fig:robotaction}.   

\subsection{Adapting Human Demonstrations to Robot Requirements}
First we evaluate adaption of the recorded demonstrations towards the robot capabilities. Specifically we compare the optimization for all aforementioned tasks for two different teacher pose estimation methods. The first relies on detecting markers attached to the teachers hand and torso and was conducted for our previous work~\cite{twelsche17iros}. The second follows the approach presented in this work. In Table~\ref{T:grasp_comparison} we summarize the numerical evaluation for both recording methods. The table shows that the offset between a valid robot grasp and the demonstrated grasp pose is higher for the 3D human pose estimation than for the estimation with markers for all tasks. The highest difference occurs for the room door task, because the hand is occluded in many frames resulting in fewer predictions. Nevertheless our graph-based optimization is still able to shift the human hand trajectory to reproduce the intended grasp, see \figref{fig:GraspAdaption}. This is reflected in higher distances, both Euclidean and angular, between gripper and recorded hand poses after the optimization. Next we compare the standard deviation on the transformations between the object and the gripper, respectively the object and hand in the manipulation segment. These transformations correspond to the robot and human grasps. We see that for both the translational and the rotational part we have comparable values for the two pose estimation methods. This indicates that, although not being as accurate as using markers, we still have a high robustness in the pose estimation, meaning that the error is systematic and the relative measurements are consistent with little deviation. After the optimization we obtain low standard deviations for both the human and the robot grasp, which corresponds, as desired, to a fixed grasp during manipulation. On the one hand the results show that our graph optimization approach is able and stringently necessary to adapt noisy human teacher demonstrations to robot friendly trajectories. On the other hand they also demonstrate that our approach for pose estimation without markers is sufficiently accurate for action learning.



\setlength{\tabcolsep}{5pt}
\begin{table*}
\centering
\begin{tabular}{l | c c | c c | c c | c c |}
&\multicolumn{2}{c|}{Room Door} &\multicolumn{2}{c|}{Swivel Door} &\multicolumn{2}{c|}{Drawer}    &\multicolumn{2}{c|}{Sliding Door}\\
& Before Opt.		& After Opt.			& Before Opt.		& After Opt.			& Before Opt.		& After Opt.			& Before Opt.	& After Opt.\\
\hline
& \multicolumn{8}{c|}{Human Pose Estimation with AR-Marker}\\ 
&\multicolumn{2}{c|}{10 demos, 1529 poses}  &\multicolumn{2}{c|}{4 demos, 419 poses}    &\multicolumn{2}{c|}{6 demos, 656 poses} &\multicolumn{2}{c|}{10 demos, 1482 poses}\\
\hline
 Euclidean distance gripper-grasp	& $\SI{2.82}{cm}$	& $\SI{0.55}{cm}$	& $\SI{2.36}{cm}$	& $\SI{0.49}{cm}$	& $\SI{6.33}{cm}$	& $\SI{0.37}{cm}$	& $\SI{3.23}{cm}$	& $\SI{0.60}{cm}$\\
 Angular distance gripper-grasp		& $18.3\degree$		& $8.0\degree$		& $5.3 \degree$		& $0.7 \degree$		& $5.4\degree$		& $1.6\degree$ 		& $6.5\degree$	& $0.5\degree$\\
 Euclidean distance gripper-hand		& $-$				& $\SI{2.2}{cm}$		& $-$				& $\SI{2.68}{cm}$	& $-$				& $\SI{5.54}{cm}$ 	& $-$	& $\SI{3.13}{cm}$\\
 Angular distance gripper-hand		& $-$				& $13.5 \degree$		& $-$				& $3.0 \degree$		& $-$				& $2.8\degree$ 		& $-$ &$5.8\degree$\\
 Std dev on gripper-object trans.	& $\SI{1.7}{cm}$		& $\SI{0.53}{cm}$	& $\SI{2.35}{cm}$	& $\SI{0.21}{cm}$	& $\SI{2.66}{cm}$		& $\SI{0.18}{cm}$	& $\SI{0.51}{cm}$ 	& $\SI{0.12}{cm}$\\
 Std dev on gripper-object rot.		& $20.5\degree$		& $2.4 \degree$		& $19.3\degree$		& $1.6\degree$		& $0.88\degree$		& $0.21\degree$		& $3.4\degree$ & $0.34\degree$\\
 Std dev on hand-object trans.		& $\SI{1.7}{cm}$		& $\SI{0.5}{cm}$		& $\SI{2.35}{cm}$	& $\SI{0.16}{cm}$	& $\SI{2.66}{cm}$	& $\SI{0.28}{cm}$ 	& $\SI{0.51}{cm}$	& $\SI{0.16}{cm}$\\
 Std dev on hand-object rot.			& $20.5\degree$		& $4.6\degree$		& $19.3\degree$		& $0.9\degree$		& $0.88\degree$		& $0.3\degree$ 		& $3.4\degree$	& $0.6\degree$\\ 
 Map collision free poses			& $\SI{89.2}{\%}$	& $\SI{99.74}{\%}$	& $-$				& $-$				& $-$				& $-$ 				& $-$	& $-$\\
 Kinematically achievable			& $\SI{69.8}{\%}$	& $\SI{96.86}{\%}$	& $\SI{85.9}{\%}$	& $\SI{99.52}{\%}$	& $\SI{87.3}{\%}$	& $\SI{100}{\%}$ 	& $\SI{63.2}{\%}$	& $\SI{99.93}{\%}$\\
 \hline
 & \multicolumn{8}{c|}{3D Human Pose Estimation from RGBD}\\ 
 & \multicolumn{2}{c|}{10 demos, 1215 poses} & \multicolumn{2}{c|}{5 demos, 330 poses} & \multicolumn{2}{c|}{5 demos, 370 poses} &\multicolumn{2}{c|}{5 demos, 451 poses}\\
 \hline
 Euclidean distance gripper-grasp	& $\SI{31.17}{cm}$	& $\SI{0.40}{cm}$	& $\SI{9.77}{cm}$	& $\SI{0.53}{cm}$	& $\SI{16.18}{cm}$	& $\SI{0.32}{cm}$	& $\SI{5.64}{cm}$	& $\SI{0.19}{cm}$\\
 Angular distance gripper-grasp		& $130.27\degree$		& $0.24\degree$		& $102.9\degree$		& $0.4\degree$		& $108.89\degree$		& $0.06\degree$ 		& $149.30\degree$	& $0.07\degree$\\
 Euclidean distance gripper-hand		& $-$				& $\SI{32.78}{cm}$		& $-$				& $\SI{14.19}{cm}$	& $-$				& $\SI{24.18}{cm}$ 	& $-$	& $\SI{19.26}{cm}$\\
 Angular distance gripper-hand		& $-$				& $63.94\degree$		& $-$				& $92.87\degree$		& $-$				& $103.39\degree$ 		& $-$ &$121.69\degree$\\
 Std dev on gripper-object trans.	& $\SI{17.40}{cm}$		& $\SI{0.25}{cm}$	& $\SI{1.75}{cm}$	& $\SI{0.15}{cm}$	& $\SI{1.08}{cm}$		& $\SI{0.12}{cm}$	& $\SI{1.03}{cm}$ 	& $\SI{0.10}{cm}$\\
 Std dev on gripper-object rot.		& $34.31\degree$		& $0.14\degree$		& $23.39\degree$		& $0.76\degree$		& $14.28\degree$		& $0.06\degree$		& $15.86\degree$ & $0.03\degree$\\
 Std dev on hand-object trans.		& $\SI{17.40}{cm}$		& $\SI{8.01}{cm}$		& $\SI{1.75}{cm}$	& $\SI{1.18}{cm}$	& $\SI{1.08}{cm}$	& $\SI{0.83}{cm}$ 	& $\SI{1.03}{cm}$	& $\SI{0.73}{cm}$\\
 Std dev on hand-object rot.			& $34.31\degree$		& $0.50\degree$		& $23.39\degree$		& $0.89\degree$		& $14.28\degree$		& $0.18\degree$ 		& $15.86\degree$	& $0.27\degree$\\ 
 Map collision free poses			& $\SI{89.14}{\%}$	& $\SI{99.51}{\%}$	& $-$				& $-$				& $-$				& $-$ 				& $-$	& $-$\\
 Kinematically achievable			& $\SI{38.10}{\%}$	& $\SI{96.05}{\%}$	& $\SI{80.0}{\%}$	& $\SI{97.27}{\%}$	& $\SI{60.81}{\%}$	& $\SI{98.92}{\%}$ 	& $\SI{63.64}{\%}$	& $\SI{95.12}{\%}$\\
 
\end{tabular}

\caption{Results for the optimization for all four trained tasks. The upper half of the table summarizes the results from experiments conducted in~\cite{twelsche17iros}. There the human pose estimation was obtained using markers. The lower half shows the results of the experiments carried out with the human pose estimation presented in this work. The total number of recorded poses refers to the length after interpolating missing ones. The shown distance between gripper and grasp poses is a mean over the endpoints of the reaching segments of the demonstrations. For the distance between gripper and hand as well as the collisions and the kinematic feasibility all pose tuples are considered. Kinematic feasibility expresses the lookup in the inverse reachability map. For the relation between object and robot gripper respectively human hand a mean over all poses in the manipulation segments is calculated. Since gripper poses are initialized with the measured hand poses no meaningful distance before optimization can be given. For the three furniture operating tasks no collisions with the map are considered.
}
\label{T:grasp_comparison}
\end{table*}
\setlength{\tabcolsep}{6pt}

\setlength{\tabcolsep}{1pt}
\begin{figure}[t]
	\centering
	\begin{tabular}{cc}  		
  	\includegraphics[width=0.239\textwidth ,trim={10.0cm 2.5cm 5.5cm 3.5cm},clip]{figures/robotics/graspAdaptionDrawer4} & 
  	\includegraphics[width=0.239\textwidth ,trim={17.0cm 8.5cm 9.5cm 1.5cm},clip]{figures/robotics/graspAdaptionKallaxDoor1}
	\end{tabular}
	\caption{Adaption of the recorded human teacher trajectory to the robot grasping capabilities for grasping the handle of the drawer (left) and the swivel door (right). The gripper poses (magenta dots) are shifted towards the handle of the drawer, respectively the door, leading to a successful robot grasp. By just imitating the human hand motion (orange dots) the grasps would fail.}
  	\label{fig:GraspAdaption}
\end{figure}
\setlength{\tabcolsep}{6pt}


\subsection{Action Imitation by the Robot}
In a follow-up experiment we used the adapted demonstrations from our pose estimation approach shown in Table~\ref{T:grasp_comparison} to learn action models that our PR2 robot can use to imitate the demonstrated actions in real world settings. These time-driven models are learned as in our previous work~\cite{twelsche17iros} using mixtures of Gaussians~\cite{Calinon12Hum}. We learn combined action models for robot gripper and base in Cartesian space. The models are used to generate trajectories for the robot in the frame of the manipulated object.   
With the learned models we reproduced each action five times. For opening the swivel door we had one failure due to localization problems during the grasping. For the drawer and the room door all trials of grasping and manipulating were successful. The sliding door was always grasped successfully but due to the small door knob and the tension resulting from the combined gripper and base motion, the knob was accidentally released during the manipulation process. We ran five successful trials of opening the sliding door by keeping the robot base steady. A visualization of the teaching process and the robot reproducing the action demonstration can be seen in \figref{fig:robotaction}.


\setlength{\tabcolsep}{1pt}
\begin{figure}[t]
	\centering
	\begin{tabular}{cc}
  		\includegraphics[width=0.239\textwidth]{figures/robotics/OpenKallaxTraj-1} &
  		\includegraphics[width=0.239\textwidth]{figures/robotics/PR2_OpenKallax-1} 
	\end{tabular}
	\caption{On the left image the teacher demonstrates the task of opening the swivel door. Superimposed on the image we see the recorded trajectories for hand (orange), torso (green) and manipulated object (blue) which serve as the input for the action learning. The right image shows the robot reproducing the action using a model learned from the teacher demonstration.}
  	\label{fig:robotaction}
\end{figure}
\setlength{\tabcolsep}{6pt}