\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{threeparttable}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{utfsym}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework\\
}
\author{
\IEEEauthorblockN{1\textsuperscript{st} Shanglin Lei\textsuperscript{12}, 1\textsuperscript{st} Guanting Dong\textsuperscript{3}, 2\textsuperscript{nd} Xiaoping Wang\textsuperscript{12*} , 3\textsuperscript{rd} keheng wang\textsuperscript{4}, 4\textsuperscript{th} sirui wang\textsuperscript{5}}


\IEEEauthorblockA{
\textsuperscript{1}\textit{School of Artificial Intelligence and Automation}, \textit{Huazhong University of Science and Technology} , Wuhan 430074, China \\
\textsuperscript{2}\textit{Key Laboratory of Image Processing and Intelligent Control of Education Ministry of China}, Wuhan 430074, China\\
\textsuperscript{3}\textit{School of Artificial Intelligence, Beijing University of Posts and Telecommunications}, Beijing, China\\
\textsuperscript{4}\textit{Sino-French Engineer School, Beihang University}, Beijing, China \\
\textsuperscript{5}\textit{Meituan Inc.}, Beijing, China \\
\{lawson, wangxiaoping\}@hust.edu.cn, dongguanting@bupt.edu.cn,wangsirui@meituan.com, wkh9575638@buaa.edu.cn
}
}



\maketitle

\begin{abstract}
The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns.
In this study, we propose a novel approach, namely InstructERC, to reformulate the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has four significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based plug-and-play plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Additionally, we have undertaken the task of unifying label mapping and modeling across three ERC datasets for the first time, showcasing the LLM's robust generalization capabilities. Extensive analysis of parameter-efficient, data-scaling and data mixing experiments provide empirical guidance for applying InstructERC in practical scenarios. Our code has been released in Github.
\end{abstract}


\section{Introduction}
``The question is not whether intelligent machines can have emotions, but whether machines without emotions can achieve intelligence'', as pointed out by the pioneer of artificial intelligence, Minsky, in his book ``Society of Mind'' \cite{minsky1988society}.
Empowering machines with the ability to understand emotions in various scenarios has always been the unwavering direction of researchers. In recent years, the task of dialogue emotion recognition has become a hot research topic in the field of natural language processing (NLP) due to its enormous potential application scenarios in human-computer interaction \cite{mackenzie2012human} and dialogue systems \cite{ma2020survey}.


In contrast to conventional binary sentiment analysis tasks \cite{pontiki2016semeval} , which only rely on text with explicit attitude tendencies, the emotion recognition in conversation (ERC) task aims to identify more fine-grained emotional tendencies in each sentence of a conversation. 
Specifically, for a given complete dialogue sequence input and a set of emotional labels, the model is required to accurately assign an emotional label to each sentence. Intuitively, the recognition of emotional tendencies in the target sentence is heavily influenced by its historical utterances \cite{yingjian2023emotionic}, and there is significant variation in how different speakers perceive and express emotions \cite{shen2021directed}. Consequently, it becomes crucial to intricately model both the speakers and the context of the dialogue.


\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{./intro_1.pdf} \caption{The illustration of different paradigms for ERC}
\label{fig1}
\end{figure}

Currently, the ERC field is hindered by four significant development challenges: 
\begin{itemize}


\item \textbf{Highly ERC-specific Approaches leading to overfitting}:
A series of previous works in Emotion Recognition in Conversation (ERC) have focused on different ERC-specific design paradigms, such as transformer-based, GNN-based, and recurrent-based models, as shown in Figure \ref{fig1}. These different ERC model architectures have each achieved state-of-the-art (SOTA) on various ERC datasets, as indicated in Table \ref{tab2}, with each model belonging to a different architecture. This indicates a potential overfitting issue to specific datasets, underscoring the imperative to investigate more generalized modeling methods.


\item \textbf{Lack of contextual information leads to insufficient emotional state modeling}: only a minority of models adapt an end-to-end approach, such as \cite{shen2021dialogxl} or \cite{kim2021emoberta}, leveraging corresponding small-scale pre-trained language models for ERC emotion recognition directly. However, the limitation lies in their maximum input token capacity, which is restricted no more than 512. This limitation frequently makes these models unable to handle dialogues exceeding four sentences, greatly hindering their effectiveness in ERC tasks where detailed encoding of prior utterances is essential for understanding the emotional context at the sentence level.




\item \textbf{Unified Generative Modeling Need}: Lastly, while \cite{hu-etal-2022-unimse} has pioneered the use of a generative method to unify the modeling of both MSA and ERC tasks. 
In real application scenarios, unified modeling across datasets can bring more powerful multi-scenario adaptation capabilities to the model. However, Conducting unified label mapping and modeling for multiple ERC datasets is a blank field, which is worth exploring.


\end{itemize}








Fortunately, the recent successful application and emergence capabilities of pre-trained large language models (LLMs) have demonstrated remarkable performance in natural language reasoning tasks. By using a generative architecture and larger input token window, LLMs unify the output and input of different tasks and have shown significant performance improvements in all NLP tasks. Despite their powerful capabilities, enabling these abilities for specific sub-tasks requires high-quality prompts \cite{wei2021finetuned,chung2022scaling} and designs to fill the reasoning gap. 
Therefore, how to use LLMs framework to reconstruct ERC while considering context modeling, speaker modeling, and capturing conversation relationships poses a significant challenge in pushing this framework towards a real dialogue system.

In this work, we reformulate the ERC task using LLMs. Specifically, we design a simple but efficient retrieval template module, which consists of instruction, historical utterance, label statements, and emotional domain retrieval to explicitly integrate multi-granularity dialogue supervision information during reasoning. In addition, we separately design two auxiliary tasks for the ERC task: speaker identification task and emotion prediction task. The speaker identification task assists LLMs in modeling dialogue role relationships by predicting the speaker of each sentence, while the emotion prediction task models future emotional tendencies in conversations. Finally, and most importantly, for the first time in the ERC field, we align labels for three conversational emotion recognition datasets and conducted unified dataset modeling. We further explored data mixing strategies and data scaling.

In conclusion, our work can be outlined as follows:
\begin{itemize}
\item To the best of our knowledge, we are the first to reformulate the ERC task as a unified Seq2Seq paradigm based on LLMs and present a effective instruction template which can adapt to different dialog scenarios.
\item We propose two novel emotional auxiliary tasks to implicitly model the dialogue role relationships and future emotional tendencies in conversations.
\item Our InstructERC significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Further analysis provides empirical guidance for finetuning ways in LoRA and All-parameters.\item We conducted, for the first time, a unified label mapping for three common ERC datasets, and unified data scaling as well as exploration of different data mixing strategies. We discovered a phenomenon of low-resource gains and high-resource containment, providing empirical guidance for industrial practical applications.
\end{itemize}

\section{Related Work}
\subsection{Large Language Models}
The emergence of large-scale language models (LLMs) have brought revolutionary transformation to the field of natural language processing (NLP) \cite{shen2023hugginggpt}. LLMs, such as GPT-3 \cite{brown2020language}, LLaMA \cite{touvron2023llama} and GPT-4 \cite{openai2023gpt4}, have demonstrated impressive abilities on various tasks, as well as the use of external techniques such as reinforcement learning from human feedback (RLHF) \cite{ouyang2022training}. LLMs based on generative framework even reformulate the multi modal perspective \cite{10.1145/3447548.3467206,zhang-etal-2023-pay}. More recently, the NLP community has been exploring various application directions for LLMs. For instance, chain-of-thought prompting and RFT \cite{wei2023chainofthought,yuan2023scaling} enables LLMs to generate problem-solving processes step-by-step, significantly enhancing the model's reasoning ability. Researchers have utilized the interactive capabilities of LLMs to generate commands that invoke external tools for handling of downstream tasks\cite{shen2023hugginggpt}. 
Other researchers have proposed parameter-efficient fine-tuining (PEFT) to address the issue of excessive computational resource without sacrificing performance \cite{hu2021lora}. 

\subsection{Emotion Recognition in Conversation}










After more than a decade of development, the field of Emotion Recognition in Conversation (ERC) has seen many outstanding works. These can be broadly classified into four categories: transformer-based, GNN-based, recurrent-based, and PLM-based.

Specifically, transformer-based works \cite{li2020multi,song2022supervised,liu2023hierarchical,yingjian2023emotionic,chudasama2022m2fnet} attempt to establish long-range emotional correlations in conversational scenarios by directly adopting or modifying the original transformer block. These efforts have made significant contributions in this direction.

GNN-based works \cite{ghosal2019dialoguegcn,ishiwatari2020relation,shen2021directed,li2023graphcfc} extensively use graphs and edges to model interactions between people in conversational scenarios and the influences between different modalities. They employ various forms of multi-layer graph neural networks to fit potential conversational relations, effectively exploring this direction.

Recurrent-based works \cite{hu2023supervised,lei2023watch,majumder2019dialoguernn,hazarika2018icon,poria2017context} utilize various forms of RNNs, like LSTM and GRU, to model individual emotional states and global emotional impacts separately. They incorporate attention mechanisms or direct vector concatenation to represent personal and global emotional states collectively, marking effective exploration in this area.

Differing from the above three kinds of approaches, which use a two-stage training process, PLM-based works \cite{shen2021dialogxl,hu-etal-2022-unimse,kim2021emoberta} rely on pre-trained models for end-to-end ERC modeling. However, they are limited by the maximum input tokens and typically do not exceed five sentences of conversational context. Compared to the aforementioned discriminative methods, PLM-based approaches are more concise.

In all four methodologies, some models\cite{freudenthaler2022ki,ghosal2020cosmic,zhong2019knowledge,zhu2021kat,li2021past} have incorporated common sense knowledge, injecting this into the models to enhance their performance in ERC tasks.



\subsection{Data scaling exploration}

The remarkable capabilities of Large Language Models stem from scaling up model sizes, data volumes, and computational resources substantially. Investigating their effectiveness across different scales is crucial. There has been significant research in the LLM field focusing on scaling laws in areas such as pre-training\cite{anil2023palm}, transfer learning\cite{hoffmann2022training}, and preference modeling\cite{chronopoulou2019embarrassingly}. However, given the smaller size of the ERC dataset and the relatively unexplored domain of using LLMs for ERC, this work pioneers the exploration of the relationship between data scaling and model performance specifically for ERC datasets. This provides fresh perspectives on how data scale impacts the efficiency of language models.

\section{Methodology}
In this section, we present a comprehensive overview of the proposed InstructERC framework shown as Figure \ref{fig3}. Firstly, we provide a brief introduction to the task definition of ERC. Next, we discuss the framework
of InstructERC, which consists of two major parts:
retrieval template module and emotional alignment tasks.
Finally, we introduce training and inference process of our framework.




\subsection{Problem Definition}
Assuming a dialogue text  of length  is given, which includes  speakers/parties  () in the dialogue, and each utterance  spoken by the corresponding speaker . Function  is employed to establish a mapping between each utterance and its corresponding speaker.

In the discriminative framework, researchers first fine-tune an Pretrained Language Model with the context-free utterance, extract the feature vector at the CLS position as the input for the downstream ERC model. The task of ERC in this case is to map the feature vector of the given utterance to a scalar between 1 and , where  represents the number of emotional labels in the dataset.

In the generative framework based on LLMs, for a given utterance, we process it into formatted text according to the pre-designed template and input it into LLMs. The objective of ERC in this case is to enable LLMs generate the most reasonable text emotional label, which must belong to the predefined text emotional label set .  is the number of emotoinal categories.




\subsection{Retrieval Template Module}
To better transfer and utilize the inference ability of pre-trained large language models, we reconstruct the ERC task to the seq2seq form and solve it through fine-tuning LLMs. Therefore, we construct a efficient retrieval template module to bridge the gap when applying LLMs to specific NLP subtasks. As shown in Figure \ref{fig2}, for ERC task, each input consists of four parts: instructions, historical content, label statement, and demonstration-retrieve.



\textbf{Instruction.} The instructions serve to provide the model with a well-defined role, precise details of the ERC task, and a standardized format for the input dialogue text. For the primary ERC task, our instruction  is shown in Figure \ref{fig2}. 




\textbf{Historical Content.} The ERC task is heavily reliant on contextual information, yet in daily conversations, the affective state of a speaker in the present moment is impervious to the emotional influence of future utterances. Therefore, the historical content that is included in the model's input is limited to those utterances that precede the current recognized utterance. We employ a hyperparameter, the historical window (denoted as ), to indicate the specific rounds of historical dialogue along with the corresponding speaker information. For the emotion recognition of the target utterance , its historical content  is shown in Figure \ref{fig2}.
\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{./template.pdf} \caption{The Schematic of Retrieval Template Module.}
\vspace{-0.1cm}
\label{fig2}
\end{figure}

\textbf{Label Statement.} To confine the model's output within a finite range of labels, facilitate statistical analysis of the model's output, and enable the model to focus on the current utterance being recognized, our label statement  is shown in Figure \ref{fig2}. 


\textbf{Demonstration Retrieval.}
In order to further integrate emotional information to assist reasoning, we have developed a domain demonstration recall module based on semantic similarity. In detail, we construct a domain base  from the training dataset that removes speaker identity information and balances the number of emotion labels, which ensures that the demonstrations is not influenced by the distribution of speakers or emotion labels in the dataset. For a given utterance  to be identified, we retrieve the most relevant ERC example from  as the demonstration. To perform the retrieval, we use a bidirectional encoder SBERT \cite{reimers2019sentence} to find the most semantically similar ERC example . SBERT generates independent CLS embeddings for the target utterance  and each element  in . After sorting all target-demonstration pairs by cosine similarity, we select the pair with the highest score as the most relevant element . An abstract mathematical description of this process is as follows:

The textual input  for the demonstration retrieval part is shown in Figure \ref{fig2}.
In summary, after constructing the Retrieval template, the simplified input  for the main task is as follows:

where [;] means the textual concatenation, , , , and  indicate Instructions, Historical content, Label statement, demonstration retrieval for a given utterance . 


\subsection{Emotional alignment tasks}
To better capture the dialogue role relationships and future emotional tendencies in conversations, we have incorporated two auxiliary tasks, namely speaker identification and emotion impact prediction, which constitute the fine-grained subtasks of the InstructERC framework. The model is jointly trained with these auxiliary tasks to improve its overall performance, which is illustrated in Figure \ref{fig3}

\begin{figure*}[t]
\centering
\includegraphics[width=2.0\columnwidth]{./overview.pdf} \vspace{-0.1cm}
\caption{The overview of InstructERC framework}
\label{fig3}
\vspace{-0.122cm}
\end{figure*}

\textbf{Speaker Identification task.}
Emotions are expressed differently among different speakers. Previous models have used techniques such as speaker-based masked attention modules or multiple GRUs to capture the emotional expression features of different characters. This modeling of emotional expression in the task can also be transformed into a generative task using our InstructERC. while MTL \cite{li2020multi} once
introduced a similar task, they acknowledged their model's limitation in not recognizing the speakers of utterances, which can be crucial for accurately matching the speaking characteristics of different individuals in real-world applications.
To enable the LLM to capture the speaking styles of different individuals, the model is trained to really identify the relevant speaker for a given utterance, without considering the historical context. For a given dataset, a predefined set of speaker labels is provided. Consistent with the main task, the Instruction text input  for this task is constructed as follows:
\begin{quote}
\textit{``Now you are an expert of sentiment and emotional analysis. Please select the Speaker label of the utterance \textless Speaker:\textgreater\quad from \quad\textless,...,\textgreater''}
\end{quote}
The loss function for the Speaker Identification is as follows:

Here,  represents the token of the corresponding speaker label for the given speaker identification task input sample .
Unless otherwise specified,  stands for the total number of utterances in the dataset, while  represents the parameters of the LLM.

\textbf{Emotion Impact Prediction task.}
In the daily conversations, the intricate relationships between individuals can have a significant impact on the emotional states of subsequent dialog. Prior research has attempted to address this issue by constructing a dialogue relationship graph and utilizing a complex graph neural network to model the emotional impacts of these relationships. 
However, these methods are often associated with a highly intricate data preprocessing pipeline and are susceptible to overfitting on certain datasets. To address these issues, we propose a generative framework for the emotion impact prediction task, which implicitly captures the interplay between dialogues and emotional impacts.

To be specific, we maintain the instruction part  of the input  of the main task and modify the historical content  from . For the Emotion Impact Prediction task,  will not include the target statement .
The corresponding label statement  is modified as follows:
\begin{quote}
\textit{``Based on the above historical utterances, the next utterance is spoken by \textless  \textgreater, please predict the emotion states of \textless  \textgreater  from \textless , , ...,  \textgreater:''}
\end{quote}
Hence, the overall input for emotion impact prediction is:

The loss calculation for the emotion impact prediction task is as follows:

Here,  represents the emotional label token of the text label  corresponding to the formatted input utterance . 
\subsection{Overview of InstructERC}

To sum up the instruction based generative framework for ERC, given an input utterance  after concatenating the retrieval template  and a LLM, the model returns the logits  and the generated text  for the entire sentence, including both input and output tokens. This is represented by the following equation:


Here,  is the same as mentioned. The LLM predicts the conditional probability  of generating each token  of the generated text  until the end symbol \textless eos\textgreater is outputted. 
As for logits ,
where  and  denote the length of the entire sentence and the size of the vocabulary used by the LLM, respectively.

In accordance with the original training method of LLMs, we adopt the next token prediction loss to measure the model's output error. Therefore, the loss calculation of the main task, denoted as , is defined as follows:






\textbf{Training and Inference.}
During training and inference, our retrieval process, emotional alignment tasks and main tasks in InstructERC can be divided into two stages:




In the first stage of joint training, the characteristics of the speaker intuitively form the basis of emotional expression. Therefore, we use the speaker identification task for LLM pre-training to fine-tune speaker characteristics, which aims to preheat parameters for subsequent ERC tasks.. 

In the second stage, we fine-tune LLM using both the ERC main task and the emotion influence prediction task to improve overall performance. The training loss at this stage is ,  where  is a hyperparameter The parameter  is used to adjust the weight of the emotion influence prediction task loss in the second overall joint training loss.


The difference of demonstration retrieval on training and inference stage is shown in figure \ref{fig2}, we limit the retrieved examples to those with the same emotion label as the current recognized speech, namely Same label pairing ,in order to provide more diverse emotional understanding while avoiding excessive noise during training. During inference, there are no restrictions on the retrieved demonstrations due to the labels are unknown, namely all labels pairing. The retrieval results, simply referred as , are specialized as   and  in training and inference stage, respectively.

\section{Experiments and Results}

\subsection{Dataset}
We evaluate the efficacy of InstructERC on three standard benchmark datasets: IEMOCAP, MELD, and EmoryNLP.







\textbf{IEMOCAP} \cite{busso2008iemocap} is a dataset recorded as dyadic conversational video clips with eight speaker participating in the training set while two speaker in testing set. Emotional tags in IEMOCAP are \textit{happy, sad, neutral, angry, excited,} and \textit{frustrated.} 

\textbf{MELD} \cite{poria2018meld} is a multimodal dataset that has been expanded from the EmotionLines dataset. MELD is obtained from the popular TV show \textit{Friends} and comprises over 1400 dialogues and 13000 utterances, each of which is labeled with emotion and sentiment classes. The emotion classes include \textit{(i.e., happy/joy, anger, fear, disgust, sadness, surprise,} and \textit{neutral)}, while the sentiment classes consist of \textit{positive, negative, or neutral}. 

\textbf{EmoryNLP} \cite{zahiri2017emotion} is a dataset also collected from the TV series \textit{Friends}. The dataset comprises utterances that are categorized into seven distinct emotional classes, namely \textit{neutral, joyful, peaceful, powerful, scared, mad,} and \textit{sad}, while the sentiment classes consist of \textit{positive, negative, or neutral}.

\textbf{Evaluation metrics.} To maintain consistency with previous methods, we use accuracy (Acc) and weighted-F1 (W-avg F1) as the evaluation metrics. Due to the severe class imbalance in the EmoryNLP and MELD datasets, as illustratesd in Figure \ref{fig:dataset_short}, the weighted-F1 metric, as opposed to accuracy (Acc), is more reflective of the true performance of the model.
For each method, we conduct tests with five random seeds and report the average results from the test sets. Specifically, for the ablation experiments, we conduct significance tests comparing the results of each ablated version with those of the complete model.

This study exclusively focuses on the emotional classes and the text modality in these datasets. Moreover, we ensure consistency with COSMIC regarding the train/val/test splits. The specifics of the datasets are outlined in Table \ref{tab:datasets_statics}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{./dataset_visual_single_short.pdf}
    \caption{Statistical Distribution of Classes Across Datasets}
    \label{fig:dataset_short}
\end{figure}


\subsection{Baselines}
For discriminative ERC models, we selected a \textbf{SOTA} baseline for each method. 

\textbf{Transformer-based}: 
\begin{itemize}
\item \textbf{KET} \cite{zhong2019knowledge} introduces Knowledge-Enriched Transformers that incorporate commonsense knowledge (CSK) using a hierarchical self-attention mechanism and a context-aware graph attention process.

\item \textbf{TODKAT} \cite{zhu2021kat} is a language model enhanced with a specialized layer for topic detection. This layer, combined with commonsense statements from a knowledge base tailored to the dialogue context, augments the model's conversational capabilities by providing deeper contextual understanding.
\item \textbf{MTL} \cite{li2020multi} leverages Speaker Identification (SI) as an auxiliary task (but does not really recognize each speakers) to improve the representation of utterances within conversations.
\item \textbf{CoG-BART} \cite{li2021contrast} employs the pretrained encoder-decoder model BART as its foundational architecture. An auxiliary task of response generation is added to augment the model's proficiency in understanding context information.
\item \textbf{M2FNet} \cite{chudasama2022m2fnet} proposes Multi-modal Fusion Network , leveraging a novel feature extraction and multi-head attention-based fusion mechanism to capture emotion-relevant features from visual, audio, and text data.
\item \textbf{SPCL+CL} \cite{song2022supervised} integrates a prompt-based BERT framework with supervised prototypical contrastive learning, as outlined in works by  and , complemented by curriculum learning concepts from .
\item \textbf{Hidialog} \cite{liu2023hierarchical} uses special tokens and turn-level attention to create hierarchical turn embeddings, and then applying a heterogeneous graph module to enhance embeddings for more accurate dialogue interpretation.
\end{itemize}


\textbf{Recurrent-based}:
\begin{itemize}
\item \textbf{SACL-LSTM} \cite{hu2023supervised} extracts structured representations using contrast-aware adversarial training and joint class-spread contrastive learning, an additional adversarial strategy is added to enhance context robustness. 
\item \textbf{HCAN} \cite{lei2023watch} integrates a hybrid recurrent and attention-based module for global emotion tracking and introduces Emotional Attribution Encoding for detailed emotion analysis in conversations. 
\item \textbf{EmotionIC} \cite{yingjian2023emotionic} features three key components: Identity Masked Multi-Head Attention (IMMHA) for global context, Dialogue-based Gated Recurrent Unit (DiaGRU) for local context, and Skip-chain Conditional Random Field (SkipCRF) for detailed emotion flow detection, combining attention and recurrence methods for a comprehensive approach.
\item \textbf{CauAIN} \cite{zhao2022cauain} involves extracting causal indicators rooted in commonsense knowledge, which assists in the traceback of causal utterances. Importantly, this process encompasses both the retrieval and traceback stages, taking into account the dynamics of interactions within and between speakers concurrently.
\item \textbf{COIN} \cite{zhang2021coin} introduces a conversational model that leverages state interactions and a hierarchical global interaction module for enhanced emotion detection, also utilizing adversarial examples to improve robustness and generalization in multimodal contexts.
\item \textbf{ICON} \cite{hazarika2018icon} hierarchically processes emotional influences at both individual and inter-speaker levels, integrating these insights into global memories, facilitating the generation of detailed contextual summaries.
\item \textbf{DialogueRNN} \cite{majumder2019dialoguernn} employs a recurrent neural network based approach that meticulously tracks the states of individual participants throughout a conversation. 
\item \textbf{DialogueCRN} \cite{DBLP:conf/acl/HuWH20} is designed with multi-turn reasoning modules that extract and integrate emotional clues. These modules perform an iterative process of intuitive retrieval and conscious reasoning, 
\item \textbf{COSMIC} \cite{ghosal2020cosmic} is a conversational model that integrates commonsense knowledge to enhance its performance, which injects commonsense knowledge into GRUs to capture features related to the internal state, external state, and intent state.
\end{itemize}


\textbf{GNN-based}: 
\begin{itemize}
\item \textbf{DialogueGCN} \cite{ghosal2019dialoguegcn} creates a graph modeling speakers' interactions to mimic the structure of a dialogue. It employs a Graph Convolutional Network for information propagation.
\item \textbf{RGAT} \cite{ishiwatari2020relation} incorporates position encodings into the RGAT framework to account for speaker and sequential dependencies in its analysis.
\item \textbf{GraphCFC} \cite{li2023graphcfc} is a module that efficiently models contextual and interactive information for ERC task. It uses multiple extractors and PairCC strategy to address the heterogeneity gap in multimodal fusion.
\item \textbf{DAG-ERC} \cite{shen2021directed} views the internal structure of dialogue as a directed acyclic graph, encoding utterances to intuitively model the flow of conversation context.
\item \textbf{SKAIG} \cite{li2021past} uses a connected graph to enhance the targeted utterance with information from the past and future context, and utilizes CommonSense Knowledge (CSK) to enrich edges with knowledge representations.
\end{itemize}

\textbf{PLM-based}:
\begin{itemize}
\item \textbf{DialogXL} \cite{shen2021dialogxl} adapts the recurrence mechanism of XLNet to accommodate longer historical contexts. It also integrates dialogue-aware self-attention to effectively handle the complexities of multi-party structures in conversations.
\item \textbf{EmoBERTa} \cite{kim2021emoberta} leverages RoBERTa for ERC by prepending speaker names and inserting separation tokens between utterances, which models emotional influnence based on both intra- and inter-speaker context in an end-to-end fashion.
\item \textbf{UniMSE} \cite{lu-etal-2022-unified} is a framework that unifies multimodal sentiment analysis and emotion recognition in conversation tasks. This framework performs modality fusion at both the syntactic and semantic levels.
\item \textbf{KI-NET} \cite{freudenthaler2022ki} uses commonsense knowledge and sentiment lexicons. It features a self-matching module and an auxiliary task for Sentiment Polarity Intensity Prediction, aiding in ERC.
\end{itemize}

\textbf{LLM-based}:
\begin{itemize}
\item \textbf{ChatGLM-6B \& ChatGLM2-6B}: ChatGLM-6B is an open-source conversational language model \cite{du2022glm} for Chinese and English. It has 6.2 billion parameters and is optimized for Chinese QA. It has been trained on 1 trillion Chinese and English identifiers and further improved through various techniques. ChatGLM2-6B is the second generation of the model, pre-trained on 1.4 trillion Chinese and English identifiers with human preference alignment training. It extends the context window to 32K and speeds up inference with Multi-Query Attention.
\item \textbf{Llama-7B \&  Llama2-7B}: Llama-7B is the 7B parameters' version of the a collection of foundation language models \cite{touvron2023llama} ranging from 7B to 65B parameters, which is trained on trillions of tokens. 
Llama2-7B pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1. Its fine-tuned models have been trained on over 1 million human annotations.
\end{itemize}



\begin{table*}[htbp]
\caption{The statistics of datasets.  denotes the average number of utterances in a conversation. } 
	\centering
\scalebox{1.1}{
	\footnotesize
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}	
	\toprule
	\multirow{2}{*}{Datasets} & \multicolumn{3}{c|}{Conversations}  & \multicolumn{3}{c|}{Utterances} & \multirow{2}{*}{classes} & \multirow{2}{*}{type} & \multirow{2}{*}{avg\_utt} & \multirow{2}{*}{Evaluation} \\ 
		& Train & Val & Test & Train & Val & Test & & & & \\
		\midrule
		IEMOCAP     & 108   & 12  & 31  & 5163  & 647   & 1623 & 6 & two-person   & 47  & Weighted-F1 / Acc \\ 	
		MELD        & 1038  & 114 & 280 & 9989  & 1109  & 2610 & 7 & multi-party  & 9   & Weighted-F1 \\ 
		EmoryNLP    &  713  & 99  & 85  & 9934  & 1344  & 1328 & 7 & multi-party  & 11  & Weighted-F1 \\ 
		\bottomrule
	\end{tabular}
} 
\label{tab:datasets_statics}	
\end{table*}


\begin{table*}[htbp]
\caption{The main results on three benchmarks.}
\centering
\scalebox{1.1}{
\begin{tabular}{c|c|cc|c|c|c|c|c}
\toprule
 Dataset            & Parameters   & \multicolumn{2}{c|}{IEMOCAP}           & MELD         & EmoryNLP  & Average   &  Extra      & Model type      \\
{Models}            &              & {W-avg F1} & ACC   & {W-avg F1}       & {W-avg F1}   &    {W-avg F1}         &  Knowledge  & \\
\midrule 
 \multicolumn{9}{c}{Small-scale Discriminant ERC-specific Model }       \\
\midrule 

{KET}           & 2.6M      & 59.56   & 61.33            & 58.18                      & 34.39              & 50.17                          & ConceptNet & transformer-based  \\      
{TODKAT}  & 330M      & 61.33   & 62.35            & 65.47                      & 38.69              & 55.16                          & COMET & transformer-based  \\
{MTL}           & 1.2M      & -----   & 62.45            & 61.90                      & 35.92              & -----                          & \usym{2717} & transformer-based  \\
{CoG-BART}      & 415.1M    & 64.87   & 64.95            & 63.82                      & 37.33              & 55.34                          & \usym{2717} & transformer-based  \\
{M2FNet}        & -----     & 69.86   & 69.69            & 66.71                      & -----              & -----                          & \usym{2717} & transformer-based  \\
{SPCL}    & 356.7M    & 68.42   & 69.21            & 66.13    & {\color{orange} \textit{40.25}}      & 58.26                  & \usym{2717} & transformer-based  \\
{Hidialog}      & -----     & -----   & -----            & {\color{orange}{66.96}}     & -----              & -----                          & \usym{2717} & transformer-based  \\
{SACL-LSTM}          & 2.6M      & 69.22   & 69.08            & 66.45                      & 39.65              & 58.44                          & \usym{2717} & recurrent-based    \\
{HCAN}          & 3.5M      & 69.21   & 69.13            & 66.24                      & 39.67              & 58.37                          & \usym{2717} & recurrent-based    \\
{ICON}          & 0.5M      & 63.50   & 64.00            & -----                      & -----              & -----                          & \usym{2717} & recurrent-based  \\
{DialogueRNN}   & 9.9M      & 64.65 & 64.85               & 65.30                      & 37.54              & 55.83                          & \usym{2717} & recurrent-based  \\
{DialogueCRN}   & 3.3M      & 67.53 & 67.39              & 65.77                      & 38.79              & 57.36                          & \usym{2717} & recurrent-based  \\
{EmotionIC}     & -----         & 69.50  &  {\color{orange}69.44}          & 66.40          & 40.01  & {\color{orange} \textit{58.63}} & \usym{2717}  & recurrent-based    \\
{CauAIN}        & 6.1M      & 65.01  & 65.08                  & 64.89                      & 37.87              & 55.92                          & ATOMIC & recurrent-based  \\
{COIN}          & 0.5M      & 65.37  & 66.05                   & -----                      & -----              & -----                          & \usym{2717} & recurrent-based  \\
{COSMIC}        & 11.9M     & 65.03 & 63.43                     & 63.43                      & 38.49              & 55.65                          & COMET & recurrent-based  \\
{DialogueGCN}   & 2.1M      & 62.11 & 62.49                    & 62.68                      & 36.43              & 53.14                          & \usym{2717} & GNN-based  \\
{RGAT}          & 13M       & 65.22 & -----                     & 60.91                      & 34.42              & 53.52                          & \usym{2717} & GNN-based  \\
{SKAIG}         & -----     & 66.96 & -----                       & 65.18                      & 38.88              & 57.01                          & COMET & GNN-based  \\
{DAG-ERC} & 9.5M      & 66.54 & 66.53               & 63.36                      & 38.29              & 56.06                          & \usym{2717} & GNN-based  \\
{GraphCFC}      & -----     & 68.91 & 69.13              & 58.86                      & -----              & -----                          & \usym{2717} & GNN-based\\
\midrule
 \multicolumn{9}{c}{Small-scale Pretrained Language Model }       \\
\midrule
{KI-NET}        & 500M      & 67.00 & -----                     & 63.24                      & -----              & -----                          & ConceptNet & Encoder-Decoder \\
{DialogueXL}    & 510M      & 65.94 & -----                     & 62.41                      & 34.73              & 54.36                          & \usym{2717} & Encoder-Decoder\\
{EmoBERTa}      & 355M      & 68.57 & -----                       & 65.61                      & -----              & -----                          & \usym{2717} & Encoder \\
{UniMSE}        & 220M      & {\color{orange} \textbf{\textit{ 70.66}}} & ----- & 65.51      & -----              & -----                          & \usym{2717} & Encoder-Decoder   \\
\midrule
\multicolumn{9}{c}{Zero-shot + InstructERC}       \\
\midrule
 ChatGLM   & 6B & \textbf{\textit{38.6}} & 39.72 & \textbf{\textit{38.8}}  & 19.6 & \textbf{\textit{32.33}} & \usym{2717} & LLM-based \\
ChatGLM2   & 6B & 21.1 & 23.7 & 21.8  & \textbf{\textit{24.4}} & 22.43  & \usym{2717} & LLM-based \\
Llama      & 7B & 0.753  & 1.32 & 9.12  & 5.31 & 5.06 & \usym{2717} & LLM-based\\
Llama2     & 7B & 2.774 & 3.54 & 16.28 & 8.36 & 9.46 & \usym{2717} & LLM-based\\
\midrule
 \multicolumn{9}{c}{LoRA + Backbone}       \\
\midrule 

ChatGLM           & 6B     & 18.94  & 17.98 & {40.54} & {25.71}  & 28.07 & \usym{2717} & LLM-based \\
ChatGLM2         & 6B     & 52.88  & 54.13   & {64.85}  & {37.69} & 51.80 & \usym{2717} & LLM-based \\
Llama            & 7B     & 55.81  & 57.27   & \textbf{\textit{66.15}}  & {37.98} & 53.21 & \usym{2717} & LLM-based \\
Llama2           & 7B     & \textbf{\textit{55.96}} & 55.53   & {65.84} & \textbf{\textit{38.21}} & \textbf{\textit{53.33}} & \usym{2717} & LLM-based \\
\midrule
\multicolumn{9}{c}{LoRA + InstructERC}       \\
\midrule 
 
ChatGLM          & 6B     & 36.04 & 38.36                        & 46.41                 & 30.86             & 37.77  & \usym{2717} & LLM-based\\
ChatGLM2         & 6B     & 67.54 & 66.91                            & 65.58              & 39.09             & 57.40  & \usym{2717} & LLM-based\\
Llama            & 7B     & 64.17 & 64.72                              & 67.62              & 39.34             & 57.04  & \usym{2717} & LLM-based\\
Llama2           & 7B     & {\color{red} \textbf{71.39}} & {\color{red} \textbf{71.68}}   & {\color{red}\textbf{69.15}}   & {\color{red}\textbf{41.37}}    & {\color{red}\textbf{60.64}}  & \usym{2717} & LLM-based \\
\bottomrule
\end{tabular}
}
\begin{tablenotes}
    
\item[a] NOTE: The best-performing results of other models are highlighted in gold font, while SOTA results across all models are emphasized in red font. Models annotated with an * indicate results sourced from the model's paper, and a () denotes results from reproductions conducted by the authors.
\end{tablenotes}
\label{tab2}
\end{table*}


\subsection{Implementation Details}
We use ChatGLM and Llama as our backbone model. 
Considering the efficiency and effectiveness of Parameter-Efficient-Fine-Tuning (PEFT),
we adopt LoRA \cite{hu2021lora} and insert low-rank adapters after self-attention layers. We set the dimension of adapters to 16 a nd the learning rate to 2e-4. The learning rate is set to 2e-5 for all parameters' finetune. The histoical window is set to 1, 5, 12, 20 for iemocap, meld and EmoryNLP respectively for all experiments. The retrieval parameter “TopK” is set to Top1 emprically. 
The hypermeter  is set to 0.1 during training.
Greedy search is used during inference if not specified. Moreover, our experiments are conducted by taking the average of three runs with no hyperparameter searching. We train with FP16 precision on 4  80G Nvidia A100 GPUs. 


\subsection{Main Results}
Table \ref{tab2} illustrates the results of comparing our InstructERC model with other models and backbones from different perspectives. Based on this, We make the following observations: (1) Our methods achieves significant improvements over the SOTA of discriminative models on all benchmarks. Specifically, we outperform UniMSE, SACL-LSTM, and EmotionIC by 0.73\%, 2.70\%, and 1.36\% on iemocap, meld, and EmoryNLP, respectively. Notably, we completely outperformed multimodal models on two benchmarks using only single-text modality data, demonstrating the extreme utilization of our method for textual data.

(2) To gain an insight into LLM models under different supervision scenarios for ERC task, we conduct experiments on Zero-shot + InstructERC and LoRA + InstructERC settings. It can be observed that even with carefully designed primary task instructions, LLMs still struggle in zero-shot scenarios, which further confirms the existence of a significant reasoning gap in their application to ERC sub-task.
Furthermore, by utilizing the LoRA + InstructERC, the performance of the four LLMs has significantly improved, especially on the IEMOCAP dataset. This fully demonstrates the effectiveness and generalization ability of our InstructERC framework, which greatly enhances the emotion recognition capability of LLM in long texts.

(3) InstructionERC is a plug-and-play method that can be adapted to multiple generative frameworks, such as prefix decoder or causal decoder. Our unified alignment task and demonstration construction strategy are not tailored to any specific dataset design, highlighting the strong transferability and generalization capability of our approach.


\subsection{Ablution study}


We conduct an ablation study to investigate the characteristics of the main components in InstructERC. Table \ref{tab3} shows the ablation results, and “w/o" denotes the model performance without a specific module. We have following observations: 1) The performance of InstructERC drops when removing any one component, which suggests that every part of the design is necessary 2) Removing any one Emotional alignment task results in great performance degradation. This is consistent with our conjecture since speaker identification and emotion impact prediction provide relatively orthogonal semantic information from two perspectives. Missing each part will make the semantic space more chaotic and make the emotion recognition effect worse. 3) Taking away the domain retrieval module resulted in a steady decline on all three datasets, demonstrating the important role of domain information in dialogue modeling. 4) Removing joint alignment task tasks causes obvious performance degradation compared with removing one of them, which indicates that jointly pre-training objectives have a mutually reinforcing effect. 5) Replacing LoRA with full-parameter fine-tuning results in a significant drop in performance, which indicates that the parameter-efficient approach is effective in preventing overfitting of LLMs on the ERC task. For detailed analysis, please refer to the ``All Parameters vs Parameter Efficiency'' section.

\begin{table}[htbp]
\centering
\caption{The ablation results of Llama2 on three benchmarks.}
\begin{tabular}{cccc}
\toprule
 Dataset          & IEMOCAP                    & MELD                       & EmoryNLP        \\ 
{Models}                   & {W-F1}       & {W-F1}       & {W-F1} \\ 
\midrule
\multicolumn{4}{c}{LoRA + InstructERC}       \\
\midrule 
Llama2             & \textbf{71.39} & \textbf{69.15} & \textbf{41.37}  \\
w/o  & 70.50 & 68.97 & 40.78 \\
w/o   & 70.70 & 68.76 & 40.59 \\ 
w/o     & 69.71 & 68.39 & 39.56 \\
w/o         & 70.91 & 68.62 & 40.54  \\
w/o         & 70.30 & 64.80 & 40.05 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
    
\item[a] Results with standard deviation and significance testing between w/o* and LLama2 (*p0.05, **p0.01.)
\end{tablenotes}
\vspace{-0.2cm}
\label{tab3}
\end{table}

\begin{table}[htbp]
\centering
\caption{The historical window exploration of Llama2 on three benchmarks.}
\begin{tabular}{cccc}
\toprule
 Dataset          & IEMOCAP                    & MELD                       & EmoryNLP        \\ 
histoical window                  & {W-F1}       & {W-F1}       & {W-F1} \\ 
\midrule
\multicolumn{4}{c}{LoRA + LLaMA2  + InstructERC}       \\
\midrule 
1             & 56.12 & {65.91} & {38.32}  \\
5 & 68.65 & 66.97 & 40.48 \\
12 & \textbf{71.39} & \textbf{69.15} & \textbf{41.37}  \\
20 & 71.01 & 68.75 & 40.56 \\
\bottomrule
\end{tabular}


\vspace{-0.2cm}
\label{tab4}
\end{table}


In the historical window exploration study, we examine how different sizes of historical windows affect emotion recognition tasks. Due to token limitations, we set the upper limit for conversational turns to 20. This is an upgrade from earlier, smaller Pretrained Language Models (PLMs), which only support up to 5 turns. We find that a window of 12 turns is optimal for capturing the necessary historical context. In general, expanding the count of historical turns aids in enhancing the accuracy of emotion detection, a trend that is readily observable in the IEMOCAP dataset featured long-term turns. However, there's a point where adding more historical turns doesn't lead to better results and might even harm performance, especially for datasets like MELD and EmoryNLP, which have an average length of 6 to 7 turns. However, these insights are beyond the reach of smaller PLMs that top out at 5 turns.


\subsection{All Parameters vs Parameter Efficiency }










In order to investigate the effect of different parameter fine-tuning methods on the ERC task, we conducted comparative experiments in Table \ref{tab5}. We have the following observations:

(1) The all parameter fine-tuning performs weaker than LoRA's fine-tuning on all backbones on average performance (especially ChatGLM with a 9.32 \% improvement). It is worth noting that the best performance of the full parameter method is often achieved in the first 1-3 epochs in the experiment. These findings demonstrate that parameter-efficient methods are more suitable for LLMs in ERC tasks.
\begin{table}[htbp]
\caption{The comparison results of different parameter fine-tuning settings on three benchmarks.}
\centering
\begin{tabular}{ccccc}
\toprule
 Dataset          & IEMOCAP                    & MELD                       & EmoryNLP                   & Average \\
{Models}                   & {W-F1}       & {W-F1}       & {W-F1}       & {W-F1} \\
\midrule
 \multicolumn{5}{c}{All parameters + InstructERC}       \\
\midrule 

ChatGLM                       & {33.94} & {37.96} & {13.25} & 28.38 \\
ChatGLM2                      & {70.05} & {63.24}  &\textbf{\textit{38.77}} & 57.35 \\
Llama                         & \textbf{\textit{69.38}} & \textbf{\textit{66.01}}  & {40.21} & \textbf{\textit{58.53}} \\
Llama2                        & {70.30} & {64.80} & {40.05} & 58.38 \\
\midrule
\multicolumn{5}{c}{LoRA + InstructERC}       \\
\midrule 
 
ChatGLM                               & 36.04                   & 46.41                 & 30.86             & 37.77 \\
ChatGLM2                               & 67.54                     & 65.58              & 39.09             & 57.40 \\
Llama                                  & 69.71                     & 68.89              & 39.90             & 59.50 \\
Llama2                                 & \textbf{71.39}         & \textbf{69.15}        & \textbf{41.37}    & \textbf{60.64} \\
\bottomrule
\end{tabular}
\label{tab5}
\vspace{-0.2cm}
\end{table}
\begin{figure*}[t]
\centering
\includegraphics[width=1.8\columnwidth]{./low_source.pdf} \caption{The scaling relationship of data and performance for different parameter fine-tuning settings (LoRA \& All Parameters)}
\label{fig4}
\end{figure*}
(2) From the perspective of model structure, the average performance of full parameter ChatGLM even decreases compared to the zero-shot results in Table \ref{tab2} (from 32.33\% to 28.38\%), while replacing it with LoRA brings a significant improvement (from 32.33\% to 37.77\%). Other decoder-only backbones do not show such drastic performance fluctuations, which further indicates that the prefix-decoder paradigm is unstable in ERC tasks compared to the casual decoder, and parameter-efficient frameworks can effectively alleviate this problem.

(3) From the perspective of datasets, compared to full parameter fine-tuning, the performance gain of the LoRA method in MELD and EmoryNLP is significantly greater than that in IEMOCAP. We believe that this is related to the characteristics of thees datasets: IEMOCAP has long dialogue texts and multiple conversation rounds, these strong supervision signals lead to good performance in both settings. However, MELD and Emory have fewer dialogue rounds, diverse speakers, and imbalanced categories. Low-parameter methods can effectively prevent LLMs from overfitting to certain semantic patterns of dialogues format and speaker's habits, thereby enhancing the generalization ability of emotion recognition in conversation.










\subsection{Scaling Analysis in Low-source Scenario}








In this section, we gain an insight into the scaling relationship of data and performance for different parameter fine-tuning settings (LoRA \& All Parameter), as shown in Figure \ref{fig4}.



\textbf{Parameter-efficient Scaling Analysis}: On the IEMOCAP dataset, our scaling curve initially increases (from 1/16 to 1/4) and then stabilizes. This may be because the dataset has long dialogue texts and multiple dialogue rounds, leading to increased diversity with the addition of early data. However, as the supervision signal strengthens, the performance gain gradually weakens. For datasets with fewer dialogue rounds and imbalanced categories, such as MELD and EmoryNLP, our method only yields a small gain in extremely low-resource scenarios (from 1/16 to 1/4) and achieves a relatively stable performance improvement with the increase of data (from 1/2 to 1). This finding supports the idea that when a unit-scaling of data only provides weak supervision signals, the data size needs to exceed a certain threshold (1/4 - 1/2) to achieve significant improvement. 

\textbf{Full-Parameter Scaling Analysis}: 
The scaling curves of full-parameter settings on the IEMOCAP and EmoryNLP datasets showed significant fluctuations and performance degradation in two intervals (from 1/16 to 1/8, 1/4 to 1/2) compared to LoRA. Fine-tuning large models with all parameters may cause redundant parameters to overfit the patterns in the current dialogue, which hinders the model's ability to generalize new supervised signals as data volume increases. The MELD dataset also exhibited performance degradation with data augmentation (from 1/4 to 1). These findings demonstrate the stability and robustness of parameter-efficient fine-tuning in the ERC task, providing empirical guidance for large models in industrial interfaces with ERC tasks of varying data characteristics.






























































\section{unified Dataset Experiments}


To further substantiate the efficacy and robustness of our framework, 
we conduct a compelling experiment involving a unified dataset.
Within the settings of this experiment, all emotional labels across the datasets are standardized, and all speaker labels are also consolidated.
Subsequently, we conduct data scaling experiments on the processed unified dataset.
The evaluation method employed in the experimental results, utilizing the weighted F1 score, aligned with the evalution method delineated in Section Experiments.



\begin{figure*}
    \centering
    \includegraphics[width=2.0\columnwidth]{./Unified_label_mapping_V3.pdf}
    \caption{Unified Label Mapping Across three Open-source Benchmarks. The Feeling Wheel is proposed by \cite{willcox1982feeling}}
    \label{fig:Unified_mapping}
\end{figure*}

\subsection{Unified dataset labeling}


We continue to use the previous datasets IEMOCAP, MELD, and EmoryNLP.
According to The Feeling Wheel \cite{willcox1982feeling} proposed in 1982, as shown in subfigure of Figure \ref{fig:Unified_mapping}, we align all emotional labels from three datasets with this standard, the details of which are shown in Tabel \ref{tab7}.
After completion of label mapping, there are a total of 9 types of emotional labels, which are \textit{ joyful, sad, neutral, mad, excited, powerful, fear, peaceful and disgust}. 
Furthermore, due to the uniqueness of character labels in each dataset, we have renumbered them using a One-hot encoding approach, as demonstrated in the "One-hot Speaker Label Mapping" subfigure of Figure \ref{fig:Unified_mapping}.



\subsection{Unified dataset Experiment}
We still utilize the LoRA method in PEFT to train InstructERC on the unified dataset, and the training results are evaluated on the three datasets respectively.
As mentioned above, these datasets have significant variations in sample size and class imbalance within each dataset. To explore the impact of different sampling methods on the final performance, two data scaling approaches were experimented with: total mix and ratio mix.

In the total mix approach, all datasets are combined for uniform sampling. Conversely, in the ratio mix approach, datasets are sampled separately and then combined. Both approaches maintain the same quantity of training data, but due to the larger absolute number of training samples in MELD and EmoryNLP, the total mix approach results in a higher proportion of samples from these two datasets when varying data scaling is applied.
On the basis of the following, we further explore the impact of data sampling ratio on the model's performance.The details of results are shown in Table \ref{tab6}, and a more intuitive presentation is shown in Figure \ref{fig:scaling2}.




\subsubsection{The robustness of InstructERC} 


As depicted in the first row of Table \ref{tab6}, upon fine-tuning InstructERC using the unified dataset, there is a slight decrease in the performance of the three benchmarks compared to the SOTA under single dataset training.
However, a relatively high Weighted F1 score (W-F1) can still be maintained simultaneously on these three benchmarks, particularly the performance of MELD, which continues to surpass the SOTA level of all small models.
Consequently, it is evident that our approach to dataset processing is simple, yet efficient.
Furthermore, InstructERC, grounded on the Llama2-7B large language model base, exhibits exceptional robustness, capable of concurrently acquiring emotional paradigms from a multitude of distinct distributions, a feat previously unattainable by small models.


\subsubsection{The data scaling exploration}

Large language models possess formidable learning capabilities, thus validating the data scaling relationship is a crucial part of our framework.
We conduct data scaling experiments on the unified dataset from 1 to 1/64.
As the scale of trainig data exponentially decreases from 1 to 1/32 within the range, the performance of the model on the three benchmarks exhibits a slight fluctuation in linear decline. This is consistent with the findings of some existing explorations in large models \cite{dong2023abilities}.
\begin{figure*}
    \centering
    \includegraphics[width=2.0\columnwidth]{./data_scaling.pdf}
    \caption{The data scaling law demonstrated on three benchmarks using different data mixing strategies}
    \label{fig:scaling2}
\end{figure*}

\subsubsection{The Low resource mutual gain} 

We also surprised to discover that during the final stage of training data reduction from 1/32 to 1/64, the Total Mix and Ratio Mix strategies continue to exhibit a linear performance decline.
However, the performance of the model trained under the single method experiences a drastic drop, as depicted in Figure \ref{fig:scaling2}.
We posit that data from different scenarios endows the model with the capability to comprehend emotions from diverse perspectives.
This, in turn, allows the model to achieve robust enhancements under various data conditions.
Such mutual gain is particularly pronounced in low resource scenarios (1/64).

\subsubsection{The exploring of different mixing strategies}  
\begin{table*}[htbp]
\caption{The Unified Dataset Experiments of Llama2 on three benchmarks}
\scalebox{1.2}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{\textbf{Data Precent}} & \multicolumn{3}{c|}{\textbf{IEMOCAP  W-F1}} & \multicolumn{3}{c|}{\textbf{MELD W-F1}}      & \multicolumn{3}{c}{\textbf{EmoryNLP W-F1}}   \\ \cline{2-10} 
                                       & Total Mix   & Ratio Mix       & Single          & Total Mix      & Ratio Mix      & Single         & Total Mix      & Ratio Mix      & Single         \\ \hline
1                                      & 68.99       & 68.99           & \textbf{71.39}  & 68.07          & 68.07          & \textbf{69.15} & 40.27          & 40.27          & \textbf{41.37} \\
1/2                                    & 67.95       & 68.96           & \textbf{69.13}  & 66.50          & 66.42          & \textbf{67.54} & 39.18          & 39.33          & \textbf{39.65} \\
1/4                                    & 63.02       & 64.46           & \textbf{67.54}  & 66.41          & 65.85          & \textbf{66.42} & 38.26          & 37.29          & \textbf{38.33} \\
1/8                                    & 58.48       & 60.06           & \textbf{64.13}  & 64.57          & 62.94          & \textbf{65.14} & 38.27          & \textbf{39.24} & 38.24 \\
1/16                                   & 57.77       & 53.40           & \textbf{60.42}  & 61.15          & 58.42          & \textbf{62.89} & 37.19          & \textbf{37.60} & 36.83          \\
1/32                                   & 45.89       & 48.50           & \textbf{54.76}  & 57.38          & \textbf{57.76} & {57.72} & \textbf{37.09} & 36.09          & 34.03          \\
1/64                                   & 38.42       & \textbf{43.07}  & 30.34           & \textbf{54.26} & 53.29          & 45.48          & \textbf{35.19} & 34.65          & 26.10          \\ \hline
\end{tabular}
}
\label{tab6}
\end{table*}
We have further investigated the impact of different mixing strategies on data scaling.
These two strategies maintain consistency in the number of training data.
In various sampling strategies, IEMOCAP, with the smallest sample proportion, exhibits inferior performance in ratio mixed sampling compared to mixed-ratio sampling, while MELD, having the largest sample proportion, shows the reverse trend. This can be explained by two key factors:

Data Representativeness: In total mix sampling, where each dataset's samples are equally likely to be selected, the unique traits of smaller datasets like IEMOCAP may be obscured by larger ones like MELD. In contrast, ratio mix sampling, which represents each dataset proportionally to its original sample size, may better highlight the characteristics and influence of smaller datasets.

Effect of Class Imbalance: In smaller datasets with internal class imbalances, total mix sampling could exacerbate these imbalances. For instance, if IEMOCAP has a relatively smaller number of samples in a certain category, total mix sampling might further intensify this imbalance during model training. Ratio mix sampling, however, better preserves the original class proportions of the datasets, potentially mitigating class imbalance impacts to a degree.

These insights indicate the importance of considering dataset size and class distribution when selecting sampling strategies to ensure optimal model performance and generalizability.







\begin{table}[]
\caption{Unified Label Mapping}
\scalebox{1.0}{
\begin{tabular}{ccccc}
\hline
Number                 & IEMOCAP            & MELD               & EmoryNLP                                & Final Emotion \\ \hline
\multicolumn{1}{c|}{1} & happy              & joyful             & \multicolumn{1}{l|}{joyful}             & joyful        \\
\multicolumn{1}{c|}{2} & sad                & sad                & \multicolumn{1}{l|}{sad}                & sad           \\
\multicolumn{1}{c|}{3} & neutral            & neutral            & \multicolumn{1}{l|}{neutral}            & neutral       \\
\multicolumn{1}{c|}{4} & angry              & angry              & \multicolumn{1}{l|}{mad}                & mad           \\
\multicolumn{1}{c|}{5} & excited            & N\textbackslash{}A & \multicolumn{1}{l|}{N\textbackslash{}A} & excited       \\
\multicolumn{1}{c|}{6} & N\textbackslash{}A & surprise           & \multicolumn{1}{l|}{powerful}           & powerful      \\
\multicolumn{1}{c|}{7} & scared             & fear               & \multicolumn{1}{l|}{frustrated}         & fear          \\   
\multicolumn{1}{c|}{8} & N\textbackslash{}A & N\textbackslash{}A & \multicolumn{1}{l|}{peaceful}           & peaceful      \\
\multicolumn{1}{c|}{9} & N\textbackslash{}A & disgust            & \multicolumn{1}{l|}{N\textbackslash{}A} & disgust       \\ \hline
\end{tabular}
}
\label{tab7}
\end{table}
















\section{Conclusion}
In conclusion, our study introduces InstructERC, a transformative approach that redefines the ERC task within a generative framework utilizing Large Language Models (LLMs). InstructERC incorporates a unique retrieval template with an emotional domain retrieval module, enabling adaptation to varying conversation lengths and offering highly relevant emotional recognition demonstrations. The historical window exploration experiment illustrates what is the optimal number of conversational turns for context modeling, which is unreachable for previous works due to the token limitation. Additionally, it integrates two novel tasks: speaker identification and emotion prediction, which effectively model complex conversational dynamics and speaker relationships. This approach allows for more nuanced ERC information integration. Significantly, our LLM-based plug-in framework surpasses all prior models, setting new benchmarks on three ERC datasets. We also pioneer in unifying label mapping and modeling across these datasets, demonstrating the LLM's robust generalization capabilities. Futhermore, the low resource mutual gain phenomenon is discovered in data scaling exploration experiments. Our extensive analysis provides practical insights for implementing InstructERC in real-world scenarios, highlighting its efficiency and effectiveness in emotion recognition within dialogues.




















































































\section{Acknowledgments}
This research received funding from the National Natural Science Foundation of China under grants 62236005, 61936004, and U1913602. Additionally, sincere gratitude is extended to Meituan Inc. for providing support in the form of an Nvidia A100 computing cluster for experimental purposes.
\clearpage
\bibliographystyle{IEEEtran}
\bibliography{IEEEexample.bib}











\end{document}
