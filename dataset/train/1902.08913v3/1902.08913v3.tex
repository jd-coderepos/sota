\begin{figure*}[ht!]
    \centering
        \includegraphics[width=0.98\textwidth]{figures/arch_multimodal/ArchitectureRedone.pdf}
	    \vspace{-0.8em}   
    \caption{Overview of our architecture consisting of four single-shot detector branches with deep feature exchange and adaptive fusion of lidar, RGB camera, gated camera, and radar. All sensory data is projected into the camera coordinate system following Sec.~\ref{sec:datarep}. To steer fusion in-between sensors, the model relies on sensor entropy, which is provided to each feature exchange block \textit{(red)}. The deep feature exchange blocks \textit{(white)}  interchange information \textit{(blue)} with parallel feature extraction blocks. The fused feature maps are analyzed by SSD blocks \textit{(orange)}.}\label{fig:arch}
     \vspace{-1.5em}   
\end{figure*}
In this section, we describe the proposed adaptive deep fusion architecture that allows for multimodal fusion in the presence of unseen asymmetric sensor distortions. We devise our architecture under real-time processing constraints required for self-driving vehicles and autonomous drones. Specifically, we propose an efficient single-shot fusion architecture.
\subsection{Adaptive Multimodal Single-Shot Fusion}
The proposed network architecture is shown in Figure~\ref{fig:arch}. It consists of multiple single-shot detection branches, each analyzing one sensor modality.

\vspace{-0.5em}
\paragraph{Data Representation}\label{sec:datarep} The camera branch uses conventional three-plane RGB inputs, while for the lidar and radar branch, we depart from recent bird's eye-view (BeV) projection~\cite{ku2018joint} schemes or raw point-cloud representations~\cite{xu2017pointfusion}. BeV projection or point-cloud inputs do not allow for deep early fusion as the feature representations in the early layers are inherently different from the camera features. Hence, existing BeV fusion methods can only fuse features in a lifted space, after matching region proposals, but not earlier. Figure~\ref{fig:arch} visualizes the proposed input data encoding, which aids deep multimodal fusion. 
Instead of using a naive depth-only input encoding, we provide depth, height, and pulse intensity as input to the lidar network. 
For the radar network, we assume that the radar is scanning in a 2D-plane orthogonal to the image plane and parallel to the horizontal image dimension. Hence, we consider radar invariant along the vertical image axis and replicate the scan along vertical axis. Gated images are transformed into the image plane of the RGB camera using a homography mapping, see supplemental material. The proposed input encoding allows for a position and intensity-dependent fusion with pixel-wise correspondences between different streams. We encode missing measurement samples with zero value.

\vspace{-0.5em}
\paragraph{Feature Extraction} As feature extraction stack in each stream, we use a modified VGG~\cite{simonyan2014very} backbone. Similar to \cite{ku2018joint,chen2017multi}, we reduce the number of channels by half and cut the network at the conv4 layer. Inspired by \cite{SSDLiu2015,featurePyramid}, we use six feature layers from conv4-10 as input to SSD detection layers. The feature maps decrease in size\footnote{\tiny{We use a feature map pyramid $\left[(24, 78), (24, 78), (12, 39), (12, 39), (6, 20), (3, 10)\right]$}}, implementing a feature pyramid for detections at different scales. As shown in Figure~\ref{fig:arch}, the activations of different feature extraction stacks are exchanged. To steer fusion towards the most reliable information, we provide the sensor entropy to each feature exchange block. We first convolve the entropy, apply a sigmoid, multiply with the concatenated input features from all sensors, and finally concatenate the input entropy. The folding of entropy and application of the sigmoid generates a multiplication matrix in the interval [0,1]. This scales the concatenated features for each sensor individually based on the available information. Regions with low entropy can be attenuated, while entropy rich regions can be amplified in the feature extraction. Doing so allows us to \emph{adaptively fuse features} in the feature extraction stack itself, which we motivate in depth in the next section. 

\begin{figure*}[t]
	\hspace{2pt}\begin{tikzpicture}
	\node[black, anchor=west] at (120pt,135 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_16-18-27_00020_gated_entropy.jpg}}};
	\node[mark size=3pt,color=dai_ligth_grey40K] at (130pt,145 pt) {\pgfuseplotmark{*}};
	\node[black, anchor=west] at (120pt,100 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_16-18-27_00020_image_entropy.jpg}}};
	\node[mark size=3pt,color=dai_deepred] at (127pt,110 pt) {\pgfuseplotmark{triangle*}};
	\node[black, anchor=west] at (120pt,65 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_16-18-27_00020_lidar_entropy.jpg}}};
	\node[mark size=3pt,color=dai_petrol] at (130pt,75 pt) {\pgfuseplotmark{square*}};
	\node[black, anchor=west] at (120pt,30 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_16-18-27_00020_radar_entropy.jpg}}};
	\node[mark size=3pt,color=magenta] at (130pt,40 pt) {\pgfuseplotmark{square*}};
	\node[black, anchor=west] at (185pt,135 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_16-18-27_00020_gated_entropy.jpg}}};
	\node[very thick, mark size=3pt,color=dai_ligth_grey40K] at (195pt,145 pt) {\pgfuseplotmark{o}};
	\node[black, anchor=west] at (185pt,100 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_16-18-27_00020_image_entropy.jpg}}};
	\node[very thick, mark size=3pt,color=dai_deepred] at (192pt,110 pt) {\pgfuseplotmark{triangle}};
	\node[black, anchor=west] at (185pt,65 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_16-18-27_00020_lidar_entropy.jpg}}};
	\node[very thick, mark size=3pt,color=dai_petrol] at (195pt,75 pt) {\pgfuseplotmark{square}};
	\node[black, anchor=west] at (185pt,30 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_16-18-27_00020_radar_entropy.jpg}}};
	\node[very thick, mark size=3pt,color=magenta] at (195pt,40 pt) {\pgfuseplotmark{square}};
	\draw [
	    thick,
	    decoration={
	        brace,
},
	    decorate
	] (122pt, 155pt) -- (250pt, 155pt);
\draw[line width=0.5pt, densely dotted, -to]    (186pt,157pt) .. controls (186pt,170pt) and (215pt,157pt)  .. (215pt,184pt);
	\node[black, anchor=west] at (-10pt,135 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_17-24-20_00030_gated_entropy.jpg}}};
	\node[black, anchor=west] at (-10pt,100 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_17-24-20_00030_image_entropy.jpg}}};
	\node[black, anchor=west] at (-10pt,65 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_17-24-20_00030_lidar_entropy.jpg}}};
	\node[black, anchor=west] at (-10pt,30 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/2017-12-05_17-24-20_00030_radar_entropy.jpg}}};
	\node[black, anchor=west] at (55pt,135 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_17-24-20_00030_gated_entropy.jpg}}};
	\node[black, anchor=west] at (55pt,100 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_17-24-20_00030_image_entropy.jpg}}};
	\node[black, anchor=west] at (55pt,65 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_17-24-20_00030_lidar_entropy.jpg}}};
	\node[black, anchor=west] at (55pt,30 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/NormalizedScnearioA1/images/input_2017-12-05_17-24-20_00030_radar_entropy.jpg}}};
	\node[very thick, mark size=3pt,color=dai_ligth_grey40K] at (65pt,145 pt) {\pgfuseplotmark{o}};
	\node[very thick, mark size=3pt,color=dai_deepred] at (62pt,110 pt) {\pgfuseplotmark{triangle}};
	\node[very thick, mark size=3pt,color=dai_petrol] at (65pt,75 pt) {\pgfuseplotmark{square}};
	\node[very thick, mark size=3pt,color=magenta] at (65pt,40 pt) {\pgfuseplotmark{square}};
	\node[mark size=3pt,color=dai_ligth_grey40K] at (0pt,145 pt) {\pgfuseplotmark{*}};
	\node[mark size=3pt,color=dai_deepred] at (-3pt,110 pt) {\pgfuseplotmark{triangle*}};
	\node[mark size=3pt,color=dai_petrol] at (0pt,75 pt) {\pgfuseplotmark{square*}};
	\node[mark size=3pt,color=magenta] at (0pt,40 pt) {\pgfuseplotmark{square*}};
	\draw [
	    thick,
	    decoration={
	        brace,
},
	    decorate
	] (-8pt, 155pt) -- (120pt, 155pt);
\draw[line width=0.5pt, densely dotted, -to]  (56pt,157pt) .. controls (56pt,170pt) and (23pt,157pt)  .. (23pt,184pt);
\node[anchor=west] (plot1) at (60pt,-5pt) {\begin{tikzpicture}
		    \begin{customlegend}[legend columns=5,legend style={draw=none,column sep=1ex},
	    	legend entries={Projected Entropies:, Gated, Image, Lidar, Radar, Projected Measurements:, Gated, Image, Lidar, Radar}]
		\addlegendimage{very thick, densely dashed, white}
		\addlegendimage{very thick, dai_ligth_grey40K, mark=*}
		\addlegendimage{very thick, dai_deepred, mark=triangle*}
		\addlegendimage{very thick, dai_petrol, mark=square*}
		\addlegendimage{very thick, magenta, mark=square*}
		\addlegendimage{very thick, densely dashed, white}
		\addlegendimage{only marks,very thick,dai_ligth_grey40K, mark=o}
		\addlegendimage{only marks,very thick,dai_deepred, mark=triangle}
		\addlegendimage{only marks,very thick,dai_petrol, mark=square}
		\addlegendimage{only marks,very thick, magenta, mark=square}
		    \end{customlegend}
	\end{tikzpicture}};
\node[anchor=west] (plot1) at (-10pt,210pt) {	\begin{tikzpicture}
	\begin{axis}[
	axis background/.style={fill=white},
	xlabel=Fog Visiblity $m$,
    xmin=20,
	xmax=85,
	ymin=-0.05,
	ymax=1.2,    
grid=major,
	width=1.1\columnwidth,
	height=0.5\columnwidth,
	x tick label style={rotate=45, anchor=east, align=center, font=\small},
	xtick={20,30,40,50,60,70,80},
	xticklabels={\unit[20]{m},\unit[30]{m},\unit[40]{m},\unit[50]{m},\scriptsize{$\cdots$},\scriptsize{$\cdots$}, \unit[$\infty$]{m}},
	]

	\addplot+ [very thick, solid, color=magenta,
	mark=square*,
	mark size={2}, 
	mark options={solid,magenta},  
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,magenta}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/NormalizedScnearioA1/radar_entropy.txt};
		
	\addplot+ [very thick, solid, dai_ligth_grey40K,mark=*,
	mark size={2},
	mark options={solid,dai_ligth_grey40K}, 
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,dai_ligth_grey40K}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/NormalizedScnearioA1/gated_entropy.txt};
	
	\addplot+ [very thick, solid, color=dai_deepred,
	mark=triangle*,
	mark size={2}, 
	mark options={solid,dai_deepred}, 
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,dai_deepred}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/NormalizedScnearioA1/image_entropy.txt};
\addplot+ [very thick, solid, color=dai_petrol,
	mark=square*,
	mark size={2}, 
	mark options={solid,dai_petrol},  
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,dai_petrol}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/NormalizedScnearioA1/lidar_entropy.txt};
	\end{axis}
	\end{tikzpicture} };
\node[black, anchor=west, rotate=90] at (-10pt,175pt) {{\footnotesize{Normalized Entropy [\%]}}};
	\node[black, anchor=west, rotate=90] at (260pt,175pt) {{\footnotesize{Normalized Entropy [\%]}}};
\node[black, anchor=west] at (30pt,272pt) {{Asymetric Sensor Performance: Clear $\rightarrow$ Fog}};
	\node[black, anchor=west] at (285pt,272pt) {{Asymetric Sensor Performance: Day $\rightarrow$ Night}};

	\node[black, anchor=west] at (280pt,135 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_17-21-45__ScenarioBuilding2_00020_gated_entropy.jpg}}};
	\node[black, anchor=west] at (280pt,100 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_17-21-45__ScenarioBuilding2_00020_image_entropy.jpg}}};
	\node[black, anchor=west] at (280pt,65 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_17-21-45__ScenarioBuilding2_00020_lidar_entropy.jpg}}};
	\node[black, anchor=west] at (280pt,30 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/input_2018-10-20_17-21-45__ScenarioBuilding2_00020_image_entropy.jpg}}};
	\node[mark size=3pt,color=dai_ligth_grey40K] at (290pt,145 pt) {\pgfuseplotmark{*}};
	\node[mark size=3pt,color=dai_deepred] at (287pt,110 pt) {\pgfuseplotmark{triangle*}};
	\node[mark size=3pt,color=dai_petrol] at (290pt,75 pt) {\pgfuseplotmark{square*}};
	\node[very thick, mark size=3pt,color=dai_deepred] at (287pt,40pt) {\pgfuseplotmark{triangle}};
		\draw [
		    thick,
		    decoration={
		        brace,
},
		    decorate
		] (281pt, 155pt) -- (344pt, 155pt);
\draw[line width=0.5pt, densely dotted, -to]   (312.5pt,157pt) .. controls (312.5pt,170pt) and (285pt,157pt)  .. (285pt,184pt);
	\node[black, anchor=west] at (345pt,135 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_18-17-27__ScenarioBuilding5_00020_gated_entropy.jpg}}};
	\node[black, anchor=west] at (345pt,100 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_18-17-27__ScenarioBuilding5_00020_image_entropy.jpg}}};
	\node[black, anchor=west] at (345pt,65 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_18-17-27__ScenarioBuilding5_00020_lidar_entropy.jpg}}};
	\node[black, anchor=west] at (345pt,30 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/input_2018-10-20_18-17-27__ScenarioBuilding5_00020_image_entropy.jpg}}};
	\node[mark size=3pt,color=dai_ligth_grey40K] at (355pt,145 pt) {\pgfuseplotmark{*}};
	\node[mark size=3pt,color=dai_deepred] at (352pt,110 pt) {\pgfuseplotmark{triangle*}};
	\node[mark size=3pt,color=dai_petrol] at (355pt,75 pt) {\pgfuseplotmark{square*}};
	\node[very thick, mark size=3pt,color=dai_deepred] at (352pt,40pt) {\pgfuseplotmark{triangle}};
		\draw [
		    thick,
		    decoration={
		        brace,
		    },
		    decorate
		] (346pt, 155pt) -- (409pt, 155pt);
	\draw[line width=0.5pt, densely dotted, -to]   (377.5pt,157pt) .. controls (377.5pt,170pt) and (348pt,157pt)  .. (348pt,184pt);
	\node[black, anchor=west] at (410pt,135 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_18-54-02__ScenarioBuilding7_00020_gated_entropy.jpg}}};
	\node[black, anchor=west] at (410pt,100 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_18-54-02__ScenarioBuilding7_00020_image_entropy.jpg}}};
	\node[black, anchor=west] at (410pt,65 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/2018-10-20_18-54-02__ScenarioBuilding7_00020_lidar_entropy.jpg}}};
	\node[black, anchor=west] at (410pt,30 pt) {{\includegraphics[trim=0 0 0 0, clip, width=0.25\columnwidth]{tikz/data_entropy_correlation/DaytimeToNight/images/input_2018-10-20_18-54-02__ScenarioBuilding7_00020_image_entropy.jpg}}};
	\node[mark size=3pt,color=dai_ligth_grey40K] at (420pt,145 pt) {\pgfuseplotmark{*}};
	\node[mark size=3pt,color=dai_deepred] at (417pt,110 pt) {\pgfuseplotmark{triangle*}};
	\node[mark size=3pt,color=dai_petrol] at (420pt,75 pt) {\pgfuseplotmark{square*}};
	\node[very thick, mark size=3pt,color=dai_deepred] at (417pt,40pt) {\pgfuseplotmark{triangle}};
		\draw [
		    thick,
		    decoration={
		        brace,
		        mirror,
},
		    decorate
		] (474pt, 155pt) -- (411pt, 155pt);
\draw[line width=0.5pt, densely dotted, -to]   (442.5pt,157pt) .. controls (442.5pt,170pt) and (411.5pt,157pt)  .. (411.5pt,184pt);
\node[anchor=west] (plot2) at (260pt,210pt) {	\begin{tikzpicture}
	\begin{axis}[
	axis background/.style={fill=white},
	xlabel=\qquad Time $h$,
xmin=1,
	xmax=7,
	ymin=-0.05,
	ymax=1.2,  
	grid=major,
width=\columnwidth,
	height=0.5\columnwidth,
	x tick label style={rotate=45, anchor=east, align=center, font=\small},
xtick={0,1,2,3,4,5,6,7,8},
	xticklabels={\scriptsize{17:05},\scriptsize{17:21},\scriptsize{17:22},\scriptsize{18:17},\scriptsize{18:33},\scriptsize{18:54},\scriptsize{19:03}, \scriptsize{19:10}, \scriptsize{19:21}},
	]
	


	\addplot+ [very thick, solid, color=magenta,
	mark=square*,
	mark size={2}, 
	mark options={solid,magenta},  
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,magenta}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/DaytimeToNight/radar_entropy_day_night2.txt};
		
	\addplot+ [very thick, solid, dai_ligth_grey40K,mark=*,
	mark size={2},
	mark options={solid,dai_ligth_grey40K}, 
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,dai_ligth_grey40K}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/DaytimeToNight/gated_entropy_day_night2.txt};
	
	\addplot+ [very thick, solid, color=dai_deepred,
	mark=triangle*,
	mark size={2}, 
	mark options={solid,dai_deepred}, 
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,dai_deepred}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/DaytimeToNight/image_entropy_day_night2.txt};
\addplot+ [very thick, solid, color=dai_petrol,
	mark=square*,
	mark size={2}, 
	mark options={solid,dai_petrol},  
	mark repeat={2},
	error bars/.cd,
	y dir=both,
	y explicit, 
	error mark options={rotate=90,dai_petrol}
	]
	table[col sep=comma, x={x}, y={mean}, y error = {std}]{tikz/data_entropy_correlation/DaytimeToNight/lidar_entropy_day_night2.txt};




	\end{axis}
	\end{tikzpicture} };
	\end{tikzpicture}
	\vspace{-2.5em}
	\caption{Normalized entropy with respect to the clear reference recording for a gated camera, RGB camera, radar, and lidar in varying fog visibilities (left) and changing illumination (right). The entropy has been calculated based on a dynamic scenario within a controlled fog chamber illustrated in Figure~\ref{fig:sensor_performance} (left) and a static scenario with changing natural illumination settings (right). Quantitative numbers have been calculated following Eq.~\eqref{eq:entropy}. Note the asymmetric sensor failure for different sensor technologies. Qualitative results are given below and are connected via arrows to their corresponding fog density/daytime.}\vspace{-1.5em}
	\label{fig:entropycorrelation}
\end{figure*}

\subsection{Entropy-steered Fusion}\label{sec:entropy}
To steer the deep fusion towards redundant and reliable information, we introduce an entropy channel in each sensor stream, instead of directly inferring the adverse weather type and strength as in \cite{fogest2014,tarel2010improved}. We estimate local measurement entropy,
\vspace{-14pt}
\begin{equation}\label{eq:entropy}
\begin{aligned}
\rho & = \sum_{m,n}^{w,h}\sum_{i=0}^{255} p_i^{mn} \log\left(p_i^{mn}\right) \;\;\; \text{,with} \\\vspace{-0.3em}
p_i^{mn} & =\frac{1}{MN}\sum_{j,k}^{M,N} \delta\left(I(m+j,n+k)-i\right).
\end{aligned}
\vspace{-1.3em}
\end{equation}
The entropy is calculated for each \unit[8]{bit} binarized stream $I$ with pixel values $i\ \epsilon\left[0,255\right]$ in the proposed image-space data representation. Each stream is split into patches of size $M \times N = \unit[16]{px} \times \unit[16]{px}$ resulting in a $w \times h = \unit[1920]{px}\times\unit[1024]{px}$ entropy map. The multimodal entropy maps for two different scenarios are shown in Figure~\ref{fig:entropycorrelation}: the left scenario shows a scene containing a vehicle, cyclist, and pedestrians in a controlled fog chamber. The passive RGB camera and lidar suffer from backscatter and attenuation with decreasing fog visibilities, while the gated camera suppresses backscatter through gating. Radar measurements are also not substantially degraded in fog. The right scenario in Figure~\ref{fig:entropycorrelation} shows a static outdoor scene under varying ambient lighting. In this scenario, active lidar and radar are not affected by changes in ambient illumination. For the gated camera, the ambient illumination disappears, leaving only the actively illuminated areas, while the passive RGB camera degenerates with decreasing ambient light. 

The steering process is learned purely on clean weather data, which contains different illumination settings present in day to night-time conditions. No real adverse weather patterns are presented during training. Further, we drop sensor streams randomly with probability 0.5 and set the entropy to a constant zero value. 

\subsection{Loss Functions and Training Details}\label{sec:training}
The number of anchor boxes in different feature layers and their sizes play an important role during training and are given in the supplemental material. In total, each anchor box with class label $y_i$ and probability $p_i$ is trained using the cross entropy loss with softmax, 
\vspace{-5pt}
\begin{equation}
H(p)=\sum_i (y_i\log(p_i)+(1 - y_i)\log(1 - p_i)).
\end{equation}
The loss is split up for positive and negative anchor boxes with a matching threshold of $0.5$. For each positive anchor box, the bounding box coordinates $x$ are regressed using a Huber loss $H(x)$ given by,
\vspace{-10pt}
\begin{equation}
H(x)=\left\lbrace
\begin{array}{cc}
x^2 / 2, &  \mathrm{if} \left|x\right| < 1\\
\left|x\right| - 0.5, &    \mathrm{if} \left|x\right| > 1
\end{array}\right.\vspace{-4pt}
\end{equation}
The total number of negative anchors is restricted to $5 \times$ the number of positive examples using hard example mining \cite{SSDLiu2015,Shrivastava16}. All networks are trained from scratch with a constant learning rate and L2 weight decay of 0.0005. 