\section{\YasoName}
\label{sec:annotation_results}

\begin{figure*}[ht]

\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=160mm]
    {figures/Picture_AnnotationProcess.pdf} 
\caption{The process for creating \YasoName, the new TSA evaluation dataset. 
An input sentence is passed through two phases of annotation (in orange), followed by four post-processing steps (in green).
}
\label{img:annotation_process}
\end{figure*} 
Next, we detail the process of creating \YasoName.
An input sentence was first passed through two phases of annotation, followed by several post-processing steps. 
\figureRef{img:annotation_process} depicts an overview of that process, as context to the details given below.
 
\subsection{Annotation}
\label{sec:annotation_scheme}

\paragraph{Target candidates annotation} 
Each input sentence was tokenized (using \texttt{spaCy} by \citet{spacy2}) and shown to $5$ annotators who were asked to mark target candidates by selecting corresponding token sequences within the sentence. 
Then, they were instructed to identify the sentiment expressed towards the candidate -- positive, negative, or mixed (\figureRef{img:comprehensive_ui}).

This step is recall-oriented, without strict quality control, and some candidates may be detected by only one or two annotators.
In such cases, sentiment labels based on annotations from this step alone may be incorrect
(see \sectionRef{section:annotation_analysis} for further analysis).


Selecting multiple non-overlapping target candidates in one sentence was allowed, each with its own sentiment. To avoid clutter and maintain a reasonable number of detected candidates, the selection of overlapping spans was prohibited. 

\paragraph{Sentiment annotation}
To verify the correctness of the target candidates and their sentiments, each candidate was highlighted within its containing sentence, and presented to $7$ to $10$ annotators who were asked to determine its sentiment 
(without being shown the sentiment chosen in the first phase).
For cases in which an annotator believes a candidate was wrongly identified and has no sentiment expressed towards it, 
a "none" option
was added to the original labels (\figureRef{img:verification_ui}).

To control the quality of the annotation in this step, 
test questions with an a priori known answer were 
interleaved 
between
the regular questions. 
A per-annotator accuracy was computed on these questions, and under-performers were excluded.
Initially, a random sample of targets was labeled by two of the authors, and cases in which they agreed were used as test questions in the first annotation batch.
Later batches also included test questions formed from unanimously answered questions in previously completed batches.

All annotations were done using the \href{www.appen.com}{Appen} platform.\footnote{\rurl{www.appen.com}}
Overall, $20$ annotators took part in the target candidates annotation phase, and $45$ annotators worked on the sentiment annotation phase. The guidelines for each phase are given in \supp \ref{appendix:annotation_guidelines}.

\subsection{Post-processing}

The 
sentiment label 
of a candidate 
was determined by majority vote from its sentiment annotation answers, and the percentage of annotators who chose that majority label is the annotation \emph{confidence}.
A threshold $t$ defined on these confidence values (set to 0.7 based on an analysis detailed below) separated the annotations between high-confidence targets (with confidence $\geq t$) and low-confidence targets (with confidence $<t$). 

A target candidate was considered as \emph{valid} when annotated with high-confidence with a particular sentiment (i.e., its majority sentiment label was not "none"). 
The valid targets were clustered by considering overlapping spans as being in the same cluster. Note 
that non-overlapping targets may be clustered together, for example, if $t_1,t_2,t_3$ are valid targets, $t_1$ overlaps $t_2$ and $t_2$ overlaps $t_3$, then all three are in one cluster, regardless of whether $t_1$ and $t_3$ overlap. The sentiment of a cluster was set to the majority sentiment of its members.

The clustering is needed for handling overlapping labels when computing recall. For example, given 
the input \sentenceQuote{The food was great}, and the annotated (positive) targets \targetTermExample{The food} and \targetTermExample{food}, a system which outputs only one of these targets should be evaluated as achieving full recall. 
Representing both labels as one cluster allows that (see details in \sectionRef{section:benchmark_results}).
An alternative to our approach is considering any prediction that overlaps a label as correct. In this case, continuing the above example, an output of \targetTermExample{food} or \targetTermExample{The food} alone will have the desired recall of $1$. 
Obviously, this alternative comes with the disadvantage of evaluating outputs with an inaccurate span as correct, e.g., an output of \targetTermExample{food was great} will not be evaluated as an error.



\subsection{Results}

\paragraph{Confidence} The per-dataset distribution of the confidence in the annotations is depicted in \figureRef{fig:confidence_all_historgam}.
For each confidence bin, one of the authors manually annotated a random sample of $30$ target candidates for their sentiments, and computed a per-bin annotation error rate (see \tableRef{tab:labeling_conf_analysis}). 
Based on this analysis, the 
confidence threshold for valid targets was set to \minConfidenceThreshold, since under this value the estimated annotation error rate was high. 
Overall, around \percentage{15}-\percentage{25} of all annotations were considered as low-confidence (light red in \figureRef{fig:confidence_all_historgam}). 

\newcommand{\figureWidth}[0]{0.46}
\newcommand{\topTrim}[0]{0cm}
\begin{figure*}[t]
    \centering
    \subfloat[Annotation confidence distribution]
    {
        \includegraphics[width=\figureWidth\linewidth, trim={0cm, 0cm, 0cm, \topTrim}, clip]{figures/distributions/confidenceAll_histogram.pdf}
        \label{fig:confidence_all_historgam}
    }
    \qquad
    \subfloat[Sentiment labels distribution]
    {
        \includegraphics[width=\figureWidth\linewidth, trim={0cm, 0cm, 0cm, \topTrim}, clip]{figures/distributions/answerHighConfidence_histogram.pdf}
        \label{fig:high_confidence_answers_histograms}
    }
    
    \subfloat[Cluster size distribution]
    {
        \includegraphics[width=\figureWidth\linewidth, trim={0cm, 0cm, 0cm, \topTrim}, clip]{figures/distributions/group_size_HighConfidence_histogram.pdf}
        \label{fig:group_size_high_confidence_histogram}
    }
    \qquad
    \subfloat[Clusters per sentence distribution]
    {
        \includegraphics[width=\figureWidth\linewidth, trim={0cm, 0cm, 0cm, \topTrim}, clip]{figures/distributions/targets_per_sentence_HighConfidence_histogram.pdf}
        \label{fig:targets_per_sentence}
    }
    \caption{Per-dataset statistics 
showing the distributions of: \protect\subref{fig:confidence_all_historgam} The confidence in the sentiment annotation of each target candidate;
    \protect\subref{fig:high_confidence_answers_histograms} The sentiment labels of targets annotated with high-confidence (HC); 
    \protect\subref{fig:group_size_high_confidence_histogram} The number of valid targets within each cluster;
    \protect\subref{fig:targets_per_sentence} The number of clusters in each annotated sentence. The datasets are marked as: \SeLapName (L),  \SeResName (R), \YelpName (Y), \AmazonName (A), \SstName (S) and \OpinosisName (O).
    }
     \label{fig:dataset_histograms}
\end{figure*}
 
\newcommand{\confidenceBinColumnTitle}[0]{\columnTitle{Confidence Bin}}
\newcommand{\matchColumnTitle}[0]{\columnTitle{Y}}
\newcommand{\mismatchColumnTitle}[0]{\columnTitle{N}}

\begin{table}[ht]
\tabcolsep=0.1cm
\begin{center}
\begin{tabular}{lcccc}
\toprule
\columnTitle{Bin} &  $\left[ 0.0, 0.7 \right)$ & $\left[ 0.7, 0.8 \right)$ & $\left[ 0.8, 0.9 \right)$ & $\left[ 0.9, 1.0 \right]$ \\
\midrule
\columnTitle{Error} & \percentage{33.3} & \percentage{10} & \percentage{3.3} & \percentage{3.3} \\
\bottomrule
\end{tabular}
\end{center}
\caption{The annotation error rate per confidence-bin. }
\label{tab:labeling_conf_analysis} 
\end{table} 
\paragraph{Sentiment labels} Observing the distribution of sentiment labels annotated with high-confidence (\figureRef{fig:high_confidence_answers_histograms}), hardly any targets were annotated as \mixedLabel, and in all datasets (except \AmazonName) there were more positive labels than negative ones. 
As many as \percentage{40} of the target candidates may be labeled as not having a sentiment in this phase (grey in \figureRef{fig:high_confidence_answers_histograms}), 
demonstrating the need for the second annotation phase. 

\paragraph{Clusters} While a cluster may include targets of different sentiments,
in practice, cluster members were always annotated with the same sentiment, further supporting the quality of the sentiment annotation. 
Thus, the sentiment of a cluster is simply the sentiment of its targets.

The distribution of the number of valid targets in each cluster is depicted in \figureRef{fig:group_size_high_confidence_histogram}.
As can be seen, the majority of clusters contain a single target.
Out of the $31\%$ of clusters that contain two targets, $70\%$ follow the pattern 
\sentenceQuote{the/this/a/their <T>} for some term \emph{T}, e.g., \emph{color} and \emph{the color}.
The larger clusters of $4$ or more targets ($2\%$ of all clusters), mostly stem from conjunctions or lists of targets (see examples in \supp \ref{appendix:annotation_examples}).

The distribution of the number of clusters identified in each sentence is depicted in \figureRef{fig:targets_per_sentence}. Around \percentage{40} of the sentences have one cluster identified within, and as many as \percentage{40} have two or more clusters (for \OpinosisName).
Between \percentage{20} to \percentage{35} of the sentences contain no clusters, i.e. no term with a sentiment expressed towards it was detected. 
Exploring the connection between the number of identified clusters and properties of the annotated sentences (e.g.,  length) is an interesting direction for future work.

\paragraph{Summary} \tableRef{tab:labeling_stats} summarizes the statistics of the collected data.
It also shows the average pairwise inter-annotator agreement, computed with Cohen's Kappa \citep{cohenKappa}, which was in the range considered as moderate agreement (substantial for \SeResName) by \citet{landis1977measurement}.
\newcommand{\kappaColumnTitle}[0]{\columnTitle{K}}
\newcommand{\numSentencesColumnTitle}[0]{\columnTitle{\#S}}
\newcommand{\numTargetsColumnTitle}[0]{\columnTitle{\#TC}}
\newcommand{\numHighConfidenceColumnTitle}[0]{\columnTitle{\#HC}}
\newcommand{\numValidTargetsColumnTitle}[0]{\columnTitle{\#VT}}
\newcommand{\numValidTargetGroupsColumnTitle}[0]{\columnTitle{\#TC}}

\begin{table}[t]
\tabcolsep=0.108cm
\begin{center}
\begin{tabular}{lcccccc}
\toprule
\bf Dataset &
\numSentencesColumnTitle &
\numTargetsColumnTitle & 
\numHighConfidenceColumnTitle  &
\numValidTargetsColumnTitle &
\numValidTargetGroupsColumnTitle &
\kappaColumnTitle 
\\
\midrule
\bf \SeLapName & \numSentencesSeLapAll & \numAnnotatedTargetsSeLapAll & \numAnnotatedTargetsSeLapHighConfidence & \numValidTargetsSeLapHighConfidence & \numTargetGroupsSeLapHighConfidence & \kappaSeLapAll
\\
\bf \SeResName & \numSentencesSeResAll & \numAnnotatedTargetsSeResAll & \numAnnotatedTargetsSeResHighConfidence & \numValidTargetsSeResHighConfidence & \numTargetGroupsSeResHighConfidence & \kappaSeResAll
\\
\bf \YelpName & \numSentencesYelpAll & \numAnnotatedTargetsYelpAll & \numAnnotatedTargetsYelpHighConfidence & \numValidTargetsYelpHighConfidence & \numTargetGroupsYelpHighConfidence & \kappaYelpAll
\\
\bf \AmazonName & \numSentencesAmazonAll & \numAnnotatedTargetsAmazonAll & \numAnnotatedTargetsAmazonHighConfidence & \numValidTargetsAmazonHighConfidence & \numTargetGroupsAmazonHighConfidence & \kappaAmazonAll
\\
\bf \SstName & \numSentencesSstAll & \numAnnotatedTargetsSstAll & \numAnnotatedTargetsSstHighConfidence & \numValidTargetsSstHighConfidence & \numTargetGroupsSstHighConfidence & \kappaSstAll
\\
\bf \OpinosisName & \numSentencesOpinosisAll & \numAnnotatedTargetsOpinosisAll & \numAnnotatedTargetsOpinosisHighConfidence & \numValidTargetsOpinosisHighConfidence & \numTargetGroupsOpinosisHighConfidence & \kappaOpinosisAll
\\
\bottomrule
\bf Total &
\numSentencesTotalAll & 
\numAnnotatedTargetsTotalAll &
\numAnnotatedTargetsTotalHighConfidence &
\numValidTargetsTotalHighConfidence &
\numTargetGroupsTotalHighConfidence &
- 
\end{tabular}
\end{center}
\caption{Per-dataset annotation statistics: The number of annotated sentences (\numSentencesColumnTitle) and target candidates annotated within those sentences (\numTargetsColumnTitle); 
The number of targets annotated with high confidence (\numHighConfidenceColumnTitle), and as valid targets; (\numValidTargetsColumnTitle); 
The number of clusters formed from the valid targets (\numValidTargetGroupsColumnTitle);
The average pairwise inter-annotator agreement (\kappaColumnTitle). See \sectionRef{sec:annotation_results}.}
\label{tab:labeling_stats} 
\end{table} 
Overall, the \YasoName dataset contains  \numSentencesTotalAll sentences and \numAnnotatedTargetsTotalAll annotated target candidates. 
Several annotated sentences are exemplified in  \supp \ref{appendix:annotation_examples}. 
To enable further analysis, the dataset includes \textbf{all} candidate targets, not just valid ones, each marked with its confidence, sentiment label (including raw annotation counts), and span. 
\YasoName 
is released along with code for performing the post-processing steps described above, and computing the evaluation metrics presented in \sectionRef{section:benchmark_results}.
