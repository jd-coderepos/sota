We used our collected dataset to study body gestures and self-adaptors. In this section, we demonstrate two different methods to analyze the body modality within the context of psychological distress. As a first step, we extract the most common audio-visual features. Then we describe a set of generic statistical body features that we extract to analyze general body gesture movement. To look specifically for self-adaptors, we then present an automatic approach to extract self-adaptors and fidgeting behavior in our dataset. We then perform a feature-based statistical analysis on the extracted body features - both generic and fidgeting features to understand what features are generally correlated with distress classification. Lastly, we move on to propose a multimodal approach to demonstrate further the effectiveness of body modality, where we incorporate and analyze the co-occurrence of multiple modalities to make predictions.







\subsection{Audiovisual Feature Extraction}\label{sec:meta_feature_extraction}
\subsubsection{Visual Features}
\label{sec:visual_features}
For each video, we used state-of-the-art tools, OpenPose \cite{cao2018openpose} and OpenFace 2.2 \cite{baltrusaitis2018openface}, to extract body pose features, facial Action Units (AUs), and gaze directions.

However, OpenPose and OpenFace do not take into account the consistency of the keypoints across time, causing the keypoints to usually fluctuate highly in many parts, introducing noise to the real continuous face and body motion. 
Besides, there are some frames where OpenPose or OpenFace fail to extract all pose points or gaze features, respectively.
To overcome these problems, we infer the missing data via Cubic Spline Interpolation across the whole sequence.
We then smooth the data using a Savitzky-Golay filter \cite{schafer2011savitzky} (window length is 11 and the order of the polynomial is 3). 

\subsubsection{Audio Features}
Speaker diarization involves partitioning an audio stream into homogeneous segments according to the speaker's identity. In order to distinguish the speech of the interviewer and the participant, we use the open-source Speaker-Diarization project~\cite{speakerdiarization} which utilizes an Unbounded Interleaved-State Recurrent Neural Network (UIS-RNN) \cite{zhang2019fully}, to extract speaker identities with respect to the time axis. We then conduct a manual check to assign correct diarization labels to the participant and the interviewer.
We also use pyAudioAnalysis \cite{giannakopoulos2015pyaudioanalysis} to extract MFCCs.


\subsection{Generic Body Features} \label{sec:meta_gesture_feature_extraction}


To explore the body modality, we extract and analyze the set of generic statistical features that describe the body movements.
\subsubsection{Feature Extraction}
Two kinds of statistical features are computed and extracted: global features and localized features.
In the global features, we care about the overall statistics of motion, while in the localized features (features that are within specific body parts, such as head, hands, and legs), we are interested in the statistics of the motion within the body parts, which we refer to as ``localization''. Our notation is summarized in Table \ref{table:results-feature-notation-mapping}.

We define a ``gesture'' as a period of sustained movement within a body localization. For example, waving hands is a gesture within ``\texttt{Hn} (hand)'' localization, and shaking legs continuously will register a gesture in ``\texttt{L} (Legs)'' localization.

To detect gestures within a localization, we scan the video using a moving window method.


First, the per-frame absolute movement ( distance) is calculated for each pose point.
The value is then averaged by the number of pose points in the localization.
Formally,

where  is the position
vector of pose point  at time , and  is the averaged per-frame movement across all points.  are the collection of pose points in this localization.

Second, a moving window is applied such that a small number of frames do not have a
disproportionate effect on the detection. This process can be expressed by:

where  is the windowed average at window index ,  is the length of the window, and
 is the average movement at frame , from Equation~\ref{eq:gesture-detection-1}.
We experimentally chose , i.e.\ a second of movement is represented by  windows.

Third, the window moves until an average movement
above a threshold is found, which is considered the beginning of the gesture.
The gesture continues until  consecutive windows (30 frames, approximately 1 sec) are found below the movement threshold, which is thus considered the end of the gesture.



Table \ref{table:results-feature-notation-mapping} lists the set of body features we extract. Below we explain how we define each of these features for the overall body. Similarly, the localized features can be calculated for every localization/body part.
\begin{itemize}\item \textbf{Average frame movement} - the per-frame average movement (moving distance) of every pose point of the body. This is the only feature that is not based on detected gestures.
  \item \textbf{Proportion of total movement occurring during a gesture} - the proportion of total
  movement that occurred while a gesture is happening (within some localizations).
  \item \textbf{Average gesture surprise} - 
  defined as 
  ``fraction of frames with no gesture happening''  ``number of gestures''.




  For example, if two gestures occurred within a sample such that 80\% of the sample
  duration had no gesture occurring, the average gesture surprise would be .
  Whereas, if there were 100 gestures, 
the average surprise is 0.8\%, even though both samples had the same proportion without
  any gesture occurring.
  This matches the intuition that each gesture within 100 evenly spaced gestures would be unsurprising
  as they were regularly occurring, whereas the 2 evenly spaced gestures would be surprising because
  nothing was happening in between.
\item \textbf{Average gesture movement standard deviation} - the standard deviation of per-frame
  movement within a gesture is averaged across all detected gestures.
  This is intended to indicate the consistency of movement intensity
  through a gesture.
  \item \textbf{Number of gestures} - total number of detected gestures across all tracked localizations.




\end{itemize}
\begin{table}
  \begin{minipage}{.35\linewidth}
\vspace{-0.7cm}
    \begin{center}
\begin{tabular}{cc}
        \toprule
        \textbf{Localization} & \textbf{Abbr.} \\ [0.4ex]
        \midrule
         \textbf{\underline O}verall & \texttt{O} \\
        \cmidrule{1-2}
         \textbf{\underline H}a\textbf{\underline n}ds & \texttt{Hn} \\
        \cmidrule{1-2}
         \textbf{\underline{ He}}ad & \texttt{He} \\
        \cmidrule{1-2}
        \textbf{\underline L}egs & \texttt{L} \\
        \bottomrule
      \end{tabular}
    \end{center}
  \end{minipage}
  \begin{minipage}{.5\linewidth}
    \begin{center}
      \begin{tabular}{ccc}
        \toprule
        & \textbf{Feature} & \textbf{Abbr.} \\ [0.5ex]
        \midrule
        \parbox[c]{3mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Overall\ \ \ \ \ \ \ \ \ \ \ \ }}}}
        & Average  \textbf{\underline{F}}rame  \textbf{\underline{ M}}ovement & \texttt{FM} \\ \cmidrule{2-3}
        & \makecell{Proportion of total  \textbf{\underline M}ovement \\ occurring during a  \textbf{\underline G}esture} & \texttt{GM} \\ \cmidrule{2-3}
        & Average  \textbf{\underline G}esture  \textbf{\underline S}urprise & \texttt{GS} \\ \cmidrule{2-3}
        & \makecell{Average  \textbf{\underline G}esture movement \\ standard  \textbf{\underline D}eviation} & \texttt{GD} \\ \cmidrule{2-3}
        & \textbf{\underline N}umber of  \textbf{\underline G}estures & \texttt{GN} \\
        \midrule
        \parbox[c]{3mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Localized\ \ \ \ \ \ }}}}
        & Average  \textbf{\underline L}ength of  \textbf{\underline G}esture & \texttt{GL} \\ \cmidrule{2-3}
        & \makecell{\textbf{\underline{A}}verage per-frame \\ \textbf{\underline G}esture movement} & \texttt{GA} \\ \cmidrule{2-3}
        &  \textbf{\underline T}otal movement in  \textbf{\underline G}estures & \texttt{GT} \\ \cmidrule{2-3}
        & Average  \textbf{\underline G}esture  \textbf{\underline S}urprise & \texttt{GS} \\ \cmidrule{2-3}
        & \textbf{\underline{N}}umber of  \textbf{\underline G}estures & \texttt{GN} \\
        \bottomrule
      \end{tabular}
    \end{center}
  \end{minipage}
  \\
  \caption{Feature notation Abbrs. of \texttt{BodyGesture}.}
  \label{table:results-feature-notation-mapping}
\end{table}

\subsubsection{Feature Processing}
All the movement data is extracted from smoothed OpenPose data described in Section \ref{sec:visual_features}. All these body gesture features are concatenated (thereafter marked as \texttt{BodyGesture}, which has a feature vector of length 20 for each participant) and all features are normalized such that the length of the sample does not affect the results.

Sum-based features (e.g., \ gesture length, gesture count, total movement, etc.) are normalized against
the total number of frames in the sample.
Gesture average features, such as gesture surprise, are again normalized against the total number of gestures.

 

\subsection{Self-adaptors and Fidgeting features}\label{sec:automatic_fidget_detection}
In addition to the generic body features, we were interested in analyzing the self-adaptors and fidgeting behavior. In this section, we present our fidgeting detection system in three subsections. We start by exploring the self-adaptors/fidgeting encoding and the overall hierarchical design. Then we show the methods of building the two essential detectors of our hierarchical model in the following two subsections. For each detector, we demonstrate the detector's design, and then present the labeling strategy which provides reliable labels for training and evaluation. In order to validate the effectiveness of our automated fidgeting detection approach before moving onto distress classification, we evaluate our model thoroughly both on an acted dataset and on our newly collected dataset of natural expressions.

\subsubsection{Overall Design and Encoding}
\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{fidgeting_detection_workflow.png}
    \caption{Hierarchical self-adaptor detection workflow. (1) First, detect hand/leg location; (2) Classify motion using \textit{DYNAMIC/STATIC Classifier} and then finally combine location and motion to give high-level fidgeting event. The figure shows the detection of \texttt{H2H} (Hand to hand) fidget. The same principle applies to other fidgets.}
    \label{fig:fidgeting_detection_workflow}
\end{figure}
\begin{table}[]
    \centering
    \begin{tabular}{lp{4.5cm}}
\toprule
        \textbf{Self-adaptors} & \textbf{Description} \\
        \midrule
        \texttt{H2H} & \textbf{\underline H}and to \textbf{\underline H}and \\
        \texttt{H2A} & \textbf{\underline H}and to \textbf{\underline A}rm \\
        \texttt{H2L} & \textbf{\underline H}and to \textbf{\underline L}eg \\
        \texttt{H2F} & \textbf{\underline H}and to \textbf{\underline F}ace \\
        \texttt{HF} & \textbf{\underline H}and \textbf{\underline F}ree (when not belong to any of above) \\
\texttt{L2G} & Both \textbf{\underline L}egs on \textbf{\underline G}round \\
        \texttt{L2L} & \textbf{\underline L}eg on the other \textbf{\underline L}eg (crossed legs) \\
        \midrule
        \textbf{Action Events} & \textbf{Description}\\
        \midrule
        DYNAMIC & Moving obviously \\
        STATIC & No obvious movement is observed \\
        \bottomrule
        \\
\toprule
        \textbf{Fidgeting Type} & \textbf{Combination} \\
        \midrule
        \texttt{CHF} (\textbf{\underline C}ross \textbf{\underline H}and \textbf{\underline F}idgeting) & \texttt{H2H} + DYNAMIC \\
        \midrule
        \texttt{SHF} (\textbf{\underline S}ingle \textbf{\underline H}and \textbf{\underline F}idgeting) & \{\texttt{H2A}, \texttt{H2L}, \texttt{H2F}, \texttt{H2F}\} + DYNAMIC \\
        \texttt{SHF-L} (to \textbf{\underline L}eg only) & \texttt{H2L} + DYNAMIC \\
        \texttt{SHF-F} (to \textbf{\underline F}ace only) & \texttt{H2F} + DYNAMIC \\
        \texttt{SHF-A} (to \textbf{\underline A}rm only) & \texttt{H2A} + DYNAMIC \\
        \midrule
        \texttt{LFF} (\textbf{\underline L}eg/\textbf{\underline F}eet \textbf{\underline F}idgeting) & \{\texttt{L2G}, \texttt{L2L}\} + DYNAMIC\\
        \bottomrule
    \end{tabular}
    \vspace{0.1cm}
    \caption{Self-adaptor and fidgeting encoding book}
    \vspace{-0.6cm}
    \label{tab:event_list}
\end{table}


Given the lack of broad agreement on the definition of fidgeting so far, we utilize a two-step hierarchical model to identify fidgeting.
As shown in Table \ref{tab:event_list}, we first identify self-adaptors, which we define as low-level location events (e.g.\ \texttt{H2H}, \texttt{H2F}). 
Secondly, action events (i.e.\ DYNAMIC, STATIC) of hand/leg are classified by the \textit{DYNAMIC/STATIC Classifier}. Fidgeting is then defined as a combination of low-level self-adaptors and action events.
Specifically, we define three types of fidgeting: cross hand fidgeting, single-hand fidgeting, and leg/feet fidgeting.





\subsubsection{Self-adaptor Detector}

\paragraph{Design}
Each body location is represented using a bounding box. Self-adaptors are defined as overlapping bounding boxes.
We represent the hand and face using the smallest rectangular box bounding all corresponding hand or face keypoints.
The forearms, upper arms, lower legs, and upper legs`bounding boxes' long sides are aligned with the connection between two joints from OpenPose, while the width is a free parameter tuned for the best automatic detection performance.

First, \texttt{H2H} self-adaptor events are detected (i.e.,\ when the two hands' bounding boxes overlap). 
Then all other hand-based self-adaptor events are detected, for all segments of the video not containing \texttt{H2H} segments. 


All self-adaptors, except for \texttt{H2F}, must be longer than 100 frames (around 4 seconds with the frame rate of 26). 
This reduces noise from detected self-adaptor events.






\paragraph{Labeling and Evaluation}
In order to validate our self-adaptor detector, we manually labeled 4 participants' videos, a total duration of 59 minutes. 
The inter-labeler agreement was checked using Krippendorff's alpha.
Each frame was labeled with one of the self-adaptor codes from Table \ref{tab:event_list}. 
Within these videos, participants perform different self-adaptors and each event has a minimum total duration of 5 minutes, with the exception of \texttt{H2F}. 

As shown in Table \ref{tab:location_detector_evaluation}, the  Krippendorff’s alpha agreement for left-hand location is 0.823, for right-hand location is 0.888 and for leg location is 1.00. 
This suggests good agreement between the annotators and, thus, the reliability of the labels. 
The results show that our network is able to detect self-adaptor with excellent overall precision, and especially for the \texttt{H2H}, \texttt{H2F}, \texttt{L2L} and \texttt{L2G} events, the detector reached a very high accuracy respectively. 
Note that, `NA' in Table \ref{tab:location_detector_evaluation} means that there is no corresponding gestures in the evaluation set of 4 labelled participants.





\begin{table}[!htbp]
\centering
\begin{tabular}{lccc}
\multicolumn{4}{l}{Hand Self-adaptors (left/right)}\\
\toprule
 & Precision & Recall & F1 Score \\
\midrule
\texttt{H2H} & 1.00/1.00 & 0.99/0.99 & 1.00/1.00 \\
\texttt{H2A} & 1.00/NA & 0.64/NA & 0.79/NA \\
\texttt{H2L} & 0.96/0.88 & 0.86/0.82 & 0.91/0.85 \\
\texttt{H2F} & NA/1.00 & NA/1.00 & NA/1.00 \\
\texttt{H2F} & 0.63/0.83 & 0.99/0.98 & 0.77/0.90\\
\midrule
Alpha Score: & 0.823/0.888 \\
\bottomrule
\\
\multicolumn{4}{l}{Leg Location}\\
\toprule
{} & Precision & Recall & F1 Score \\
\midrule
\texttt{L2L} & 1.00 & 1.00 & 1.00\\
\texttt{L2G} & 1.00 & 1.00 & 1.00 \\
\midrule
Alpha Score: & 1.000 \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Self-adaptor Detection Evaluation}
\vspace{-5mm}
\label{tab:location_detector_evaluation}
\end{table}

\subsubsection{Fidgeting Detector} \label{sec:Fidgeting_Detector}
\paragraph{Design}








As shown in Fig. \ref{fig:fidgeting_detection_workflow}, the \textit{DYNAMIC/STATIC Classifier} operates on extracted \textbf{optical flow} from a sliding window across the video (size 100 frames, step 50 frames).
To classify the action (DYNAMIC/STATIC), hand movements (especially fingers) and leg movements require optical flow to obtain smooth trajectories, given OpenPose estimations become unreliable when hands intersect or are occluded. We thus initialize the optical flow with the OpenPose estimations at the beginning of each slice.


We choose Fast Fourier Transform (FFT), standard deviation (STD), and mean values (MEAN) of point trajectories as our input features (in this case, number of trajectories is 2  number of keypoints as we have 2-D data for each keypoint). 
For fidgeting, we are more interested in the cyclic motion with a frequency ranging from 0.5Hz to 2.5Hz \cite{mahmoud2013automatic}. Therefore, we extracted the spectrum data within the range  Hz.
As we analyze slices of length 100, the dimension of FFT spectrum data that is within  Hz is always fixed at  number of trajectories. 
We then average over the FFT values that have the same frequency to produce an FFT feature of length 41. As for the STD and MEAN features, we simply calculate along the time axis and give a vector with a length of the number of trajectories for each feature.


\paragraph{Labeling and Evaluation}
To train and evaluate the \textit{DYNAMIC/STATIC Classifiers}, accurate labeling is required. Three classifiers are required to cover the three categories of detected self-adaptors: \{\texttt{H2H}\}, \{\texttt{H2A}, \texttt{H2L}, \texttt{H2F}, \texttt{H2F}\}, and \{\texttt{L2G}, \texttt{L2L}\}.

We labelled DYNAMIC/STATIC on each of the three categories. We randomly sampled and labeled approximately 30\% of slices for each category in every video. 

Two researchers labeled the data independently. As shown in Table \ref{tab:action_detector_labelling}, we first manually dropped the slices with a wrong category label (e.g.\ a slice is detected as \texttt{H2H} while it's in fact not). The number of slices that have a correct category label is shown as ``Correct''. Secondly, we labeled DYNAMIC/STATIC and dropped the slices that lack a consensus between two researchers. The number of slices with an agreement is shown as ``Agreed''. The high percentage of both ``Correct'' and ``Agreed'' suggests the good performance of our self-adaptor detection and also the high reliability of action labels.









\begin{table}[htp]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Category & Total & Correct & Agreed \\
        \midrule
        BOTH: \texttt{H2H} & 3962 & 3922 (99\%) & 3793 (96\%)\\
        LEFT:\{\texttt{H2A}, \texttt{H2L}, \texttt{H2F}, \texttt{H2F}\} & 1614 & 1566 (97\%) & 1539 (96\%)\\
        RIGHT:\{\texttt{H2A}, \texttt{H2L}, \texttt{H2F}, \texttt{H2F}\} & 1620 & 1588 (98\%) & 1563 (96\%)\\
        \{\texttt{L2G}, \texttt{L2L}\} & 6536 & 6536 (100\%) & 6196 (95\%)\\
        \bottomrule
    \end{tabular}
    \vspace{0.1cm}
    \caption{Hand/Leg action labelling overview}
    \vspace{-5mm}
    \label{tab:action_detector_labelling}
\end{table}

Having reliable slice labels, we then partitioned participants into 5 folds and performed slice-level cross-validation.
For evaluation, we calculated accuracy, F1 score, and their respective standard deviations.

\begin{table}[htp]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Category & Acc. & Acc. Std. & F1 & F1 Std.\\
        \midrule
        BOTH: \texttt{H2H} & 0.833 & 0.019 & 0.834 & 0.019\\
        LEFT:\{\texttt{H2A}, \texttt{H2L}, \texttt{H2F}, \texttt{H2F}\} & 0.884 & 0.025 & 0.884 & 0.026\\
        RIGHT:\{\texttt{H2A}, \texttt{H2L}, \texttt{H2F}, \texttt{H2F}\} & 0.895 & 0.026 & 0.894 & 0.026\\
        \{\texttt{L2G}, \texttt{L2L}\} & 0.875 & 0.022 & 0.871 & 0.021\\
        \bottomrule
    \end{tabular}
    \vspace{0.1cm}
    \caption{\textit{DYNAMIC/STATIC Classifier} evaluation (LEFT means left hand, RIGHT means right hand, BOTH means both hands)}
    \vspace{-0.6cm}
    \label{tab:action_detector_evaluation}
\end{table}

As shown in Table \ref{tab:action_detector_evaluation}, the detector achieved generally high accuracy and F1 score with low standard deviations. Though the hand actions are difficult even for researchers to label, the detector can successfully classify more than 80\% of slices.
\vspace{-0.3cm}
\subsection{Feature encoding}\label{sec:feature_encoding}
This section describes how we encoded low-level frame-level features described in Sec \ref{sec:meta_feature_extraction} and \ref{sec:automatic_fidget_detection} in preparation for the final prediction step. The generic statistical \texttt{BodyGesture} will not need to be encoded since it represents global statistical features rather than time-series features.\vspace{-0.3cm}
\subsubsection{Fidgeting features processing}
Having extracted low-level features from each frame, we combine them to form high-level descriptors of fidgeting behavior (\texttt{CHF}, \texttt{CHF}, and \texttt{LFF} as shown in Table \ref{tab:event_list}). 
The \texttt{Fidget\_pure} feature group is formed by \{\texttt{HCF}, \texttt{SHF-L}(left hand), \texttt{SHF-L}(right hand), \texttt{SHF-A}(left hand), \texttt{SHF-A}(right hand), \texttt{SHF-F}(left hand), \texttt{SHF-F}(right hand), \texttt{LFF}\}. 
The \texttt{Fidget\_pure} group is combined with a participant speaking feature array to form the full fidget feature group, enabling us to investigate whether fidgeting and speaking co-occurrence is relevant.
This participant speaking feature array indicates whether the participant is speaking during a frame.
This is calculated using the previously described diarization data.


After all the feature extraction, we have several feature groups shown in Table \ref{tab:feature_group}.
\begin{table}[]
    \centering
    \begin{tabular}{ccl}
    \toprule
        Feature Group & Dimension & Description\\
        \midrule
        \texttt{BodyGesture} &  & Body Gesture Statistical Features\\
        \texttt{Fidget} &  & Fidget feature \& Speaking array\\
        \texttt{Fidget\_pure} &  & Fidget feature only\\
        \texttt{Gaze} &  & Gaze direction \\
        \texttt{AUs} & 3 & Action Units \\
        \texttt{MFCCs} &  & \\
    \bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \caption{Feature Groups.  is number of frames in each recording of participants.}
    \label{tab:feature_group}
    \vspace{-10mm}
\end{table}
\vspace{-0.3cm}
\subsubsection{Per-frame representation}
\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=15cm]{fusion_workflow.png}
    \end{center}
    \caption{
    Multi-modal fusion \& classification pipeline.
    The dashed arrow represents a fully connected neural network between dense layers.
    Pose estimation, gaze, Action Units, and MFCC data are extracted from videos.
    Fidget features are computed using the method described in Section \ref{sec:method}.
    (1) All features are fed into a Multi-modal Deep Denoising Auto-Encoder (multi-DDAE) to generate a compact per-frame encoded representation.
    (2) These per-frame features are then compressed into a whole video representation using a Gaussian Mixture Model (GMM) and Fisher Vector combination.
    (3) Random Forest feature selection is performed.
    (4) Finally, a classifier predicts a given label.
    We experiment with two classifiers, a logistic regression classifier and a Multi-layer Perception.}
    \label{fig:method_overview}
\end{figure*}
In order to capture more useful feature representations and reduce the dimensionality, and inspired by our previous work \cite{zhang2020multi}, different modalities are combined using a Multi-modal Deep Denoising Auto-Encoder (multi-DDAE). As shown in Fig.~\ref{fig:method_overview}, each modality is encoded through a dense layer and then all are concatenated to yield the last shared dense layer which provides the representation we use. The shared layer is then inversely decoded to generate each modality. We optimized the hyper-parameters of the auto-encoder via several experiments so that the dimensions of hidden layers are \{\} where  represents the input dimension of each node, and the noise applied at the input is 0.1 Gaussian noise. The training optimization target is the joint Mean Square Error (MSE) of the MSEs of the feature group at each node (later we fixed the loss weights to be 0.35 for the fidget feature group while 0.1 for others, as we are more interested in fidgeting in our experiments).
\vspace{-0.2cm}
\subsubsection{Whole video representation}
Due to varying lengths of the videos, it's necessary to unify the dimensionality of the per-video representation. 
Though Fisher Vector was originally proposed to aggregate visual features \cite{perronnin2010improving}, it has become popular in social signal processing such as bipolar disorder \cite{syed2018automated} and depression recognition \cite{dhall2015temporally}.
Inspired by these applications, we apply a Gaussian Mixture Model to cluster similar per-frame representations and then use an Improved Fisher Vector encoding to obtain a fixed-length representation. As a result, the feature is transformed from  to .

\vspace{-0.1cm}
\subsection{Classification of signs of distress} \label{sec:Distress_classification}
We apply a Random Forest to select important features from the per-video representation. The selected features are used by the classifier. We experiment with two classifiers: 1) a logistic regression-based classifier (LR) using a binary threshold of 0.5; 2) a Multi-Layer Perception (MLP) with two softmax outputs for binary classification.

As the available samples are limited and the useful features vary across individual differences, label smoothing~\cite{muller2019does} is applied to the MLP model in order to further boost the performance. 
More formally:

where  is the one-hot label at softmax outputs,  is the smoothing parameter, and  is the number of classification classes.
For example, when smoothing is 0.2, the one-hot label \{0, 1\} will become \{0.1, 0.9\}, which lowers the confidence on training samples but reduces overfitting.
