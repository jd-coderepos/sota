\section{Experiments}
\label{subsec:experiment}

\begin{table}[t!]
\centering
\caption{Performance comparisons with the state-of-the-art semantic segmentation methods on ADE20K \texttt{val}~\cite{zhou2017scene} with 150 categories. \#P and \#F indicate the number of parameters (M) and FLOPs (G). 
We report both single-scale (s.s.) and multi-scale (m.s.) inference results.
}
\vspace{-0.5em}
\resizebox{\linewidth}{!}{\begin{tabular}{c|c|ccccc}
Method & Backbone & \begin{tabular}[c]{@{}c@{}} mIoU \\  s.s. (\%) \end{tabular}    & \begin{tabular}[c]{@{}c@{}} mIoU \\  m.s. (\%) \end{tabular}  & \#P & \#F \\ \shline
UperNet~\cite{xiao2018unified} & R50 & 42.1 & - & 67 & 238 \\
DeepLab V3+~\cite{chen2018encoder} & R50 & 44.0 & 44.9 & 44 & 177  \\
SenFormer~\cite{bousselham2021efficient} & R50 & 44.7 & 45.2 & 144 & 179 \\
Maskformer~\cite{cheng2021per} & R50  & 44.5 & 46.7 & 41 & 53 \\ 
PFD~\cite{qin2022pyramid} & R50  & 45.6 & 48.3 & 74 & 61 \\ 
Mask2former~\cite{cheng2021masked} & R50 & 47.2 & 49.2 & 44 & 71 \\
FASeg (ours) & R50 & \textbf{48.3} & \textbf{49.3} & 51 & 72 \\
\shline
UperNet~\cite{xiao2018unified} & Swin-T & 44.4 & 46.1 & 60 & 236 \\
SenFormer~\cite{bousselham2021efficient} &  Swin-T & 46.0 & - & 144 & 179 \\
Maskformer~\cite{cheng2021per} & Swin-T & 46.7 & 48.8 & 42 & 55 \\
PFD~\cite{qin2022pyramid} & Swin-T & 48.3 & 49.6 & 74 & 65 \\
Mask2former~\cite{cheng2021masked} & Swin-T & 47.7 & 49.6 & 47 & 74 \\
FASeg (ours)& Swin-T & \textbf{49.6} & \textbf{51.3} & 54 & 75 \\\shline
SenFormer~\cite{bousselham2021efficient} &  Swin-B & 51.8 & - & 204 & 242 \\
Maskformer~\cite{cheng2021per} & Swin-B & 52.7 & 53.9 & 102 & 195 \\
PFD~\cite{qin2022pyramid} & Swin-B & 54.1 & 55.3 & 123 & 206 \\
Mask2former~\cite{cheng2021masked} & Swin-B & 53.9 & 55.1 & 107 & 223 \\
FASeg (ours) & Swin-B & \textbf{55.0} & \textbf{56.0} & 113 & 225 \\
\shline
UperNet~\cite{xiao2018unified} & Swin-L & 52.1 & 53.5& 234 & 647  \\
SenFormer~\cite{bousselham2021efficient} &  Swin-L & 53.1 & 54.2 & 314 & 546 \\
Maskformer~\cite{cheng2021per} & Swin-L & 54.1 & 55.6 & 212 & 375 \\
PFD~\cite{qin2022pyramid} & Swin-L & 56.0 & 57.2 & 242 & 385 \\
Mask2former~\cite{cheng2021masked} & Swin-L & 56.1 & 57.3 & 215 & 403 \\
FASeg (ours) & Swin-L & \textbf{56.3} & \textbf{57.7} & 222 & 405 \\
\end{tabular}}
\vspace{-0.5em}
\label{tab:ADE_new}
\end{table}


\begin{table}[t!]
\centering
\caption{Performance comparisons with the state-of-the-art semantic segmentation methods on Cityscapes \texttt{val}~\cite{cordts2016cityscapes}. We report single-scale (s.s.) inference results.  \#P and \#F indicate the number of parameters (M) and FLOPs (G). 
}
\vspace{-0.5em}
\resizebox{\linewidth}{!}{\begin{tabular}{c|c|ccc}
Method & Backbone & mIoU s.s. (\%) & \#P & \#F \\ \shline
Maskformer~\cite{cheng2021per} & R50 & 78.5 & 41 & 405 \\
Senformer~\cite{bousselham2021efficient} & R50 & 78.8 & 144 & 1,317 \\
DeepLab V3+~\cite{bousselham2021efficient} & R50 & 79.0 & - & - \\
Mask2former~\cite{cheng2021masked} & R50 & 79.4 & 44 & 526 \\
Maskformer~\cite{cheng2021per} & R101 & 79.1 & 60 & 561 \\
Mask2former~\cite{cheng2021masked} & R101 & 80.1 & 67 & 628 \\
SenFormer~\cite{bousselham2021efficient} & R101 & 80.3 & 162 & 1,473 \\
FASeg (ours) & R50 & \textbf{80.5} & 67  & 533 \\
\end{tabular}}
\label{tab:cityscapes}
\vspace{-1em}
\end{table}

\noindent\textbf{Implementation details.} Unless otherwise specified, we adopt the same training settings and implementation details as in Mask2former~\cite{cheng2021masked}. For our efficient HRCA in Section~\ref{subsec:efficient_cross_attention}, we select  from the low-resolution feature maps ( of the original image size). By default, we train our models with a batch size of 16 on 8 NVIDIA
V100 GPUs. We adopt ResNet~\cite{he2016deep} and Swin Transformer~\cite{swin} pre-trained backbones. For ResNet~\cite{he2016deep}, we use the ResNet-50 (R50) variant. For Swin Transformer~\cite{swin}, we use the Swin-T, Swin-B, and Swin-L backbones where Swin-B and Swin-L are pre-trained on ImageNet-22k~\cite{deng2009imagenet}. Unless specified, we adopt all training settings the same as the default settings of FASeg with R50~\cite{he2016deep} backbone on ADE20K \texttt{val}~\cite{zhou2017scene} with 150 categories for ablation experiments. We conduct the main experiments and ablation studies with the same seeds as Mask2former to seek fair comparisons.

\noindent\textbf{Datasets.} We conduct our experiments on ADE20K~\cite{zhou2017scene} and Cityscapes~\cite{cordts2016cityscapes}. ADE20K~\cite{zhou2017scene} is one of the most challenging large-scale datasets for semantic segmentation, which covers 150 fine-grained semantic concepts, where the training set and validation set contain 20,210 and 2,000 images, respectively. Cityscapes~\cite{cordts2016cityscapes} is an urban street-view dataset with high-resolution images from 50 cities with 19 semantic classes, which consists of 2,975 images for the training set and 2,725 images for the validation set.

\noindent\textbf{Evaluation metrics. } We use single-scale (s.s.) and multi-scale (m.s.) mean Intersection over Union (mIoU)~\cite{everingham2015pascal} as the evaluation metric. We also compare models in terms of their model size (number of parameters) and computational complexity with Floating-point Operations (FLOPs) to evaluate the efficiency of these models. For ablation studies on HRCA, we also show the training-time GPU memory consumption.
For ADE20K~\cite{zhou2017scene} and Cityscapes~\cite{cordts2016cityscapes}, we calculate FLOPs with fixed  and  image size, respectively.

\noindent\textbf{Compared methods.} We compare our method with the SOTA semantic segmentation methods, including DeepLab V3+~\cite{chen2018encoder}, UperNet~\cite{xiao2018unified}, Maskformer~\cite{cheng2021per}, SenFormer~\cite{bousselham2021efficient}, PFD~\cite{qin2022pyramid} and Mask2former~\cite{cheng2021masked}. Among them, SenFormer~\cite{bousselham2021efficient}, PFD~\cite{qin2022pyramid} and Mask2former~\cite{cheng2021masked} are the recent Transformer-based segmentors, where PFD learns a hierarchy of latent queries to enrich the multi-scale information and SenFormer ensembles the multi-scale predictions. We refer the readers to Section~\ref{subsec:related_work} for more details.

\vspace{-0.2em}
\subsection{Main Results}
\label{subsec:main_result}
\vspace{-0.2em}

We compare our FASeg with state-of-the-art semantic segmentation methods on ADE20K \texttt{val}~\cite{zhou2017scene} and Cityscapes \texttt{val}~\cite{cordts2016cityscapes}. The results are reported in Tables~\ref{tab:ADE_new} and~\ref{tab:cityscapes}. We observe that on ADE20K \texttt{val} (Table~\ref{tab:ADE_new}), with affordable number of extra parameters and FLOPs, our FASeg consistently outperforms the SOTA methods. Specifically, FASeg achieves 48.3\%, 49.6\%, 55.0\%, and 56.3\% mIoU for single-scale inference, outperforming the SOTA methods by 1.1\%, 1.3\%, 0.9\%, and 0.2\% on R50, Swin-T, Swin-B, and Swin-L backbones, respectively. The solid performance gain demonstrates the superiority of our FASeg framework. Our FASeg has more improvements with the smaller backbones (\eg, R50, Swin-T, and Swin-B). We conjecture that localizing the contextural features with smaller backbones under inferior representational capability is challenging. Nevertheless, our DFPQ provides more accurate positional priors, which ease the localization difficulty and lead to better results. For the comparisons on Cityscapes \texttt{val} in Table~\ref{tab:cityscapes},
we observe that with the R50 backbone, our FASeg outperforms all the SOTA methods under desirable numbers of parameters and FLOPs. Surprisingly, FASeg even outperforms the SOTA methods employing the R101 backbone, which demonstrates the effectiveness of our FASeg. To further investigate the flexibility and potential of our main contribution DFPQ, we show more experiments on instance segmentation in the supplementary material.

We next show some qualitative results in Figure~\ref{fig:vis} and find that our FASeg provides more accurate predictions with finer details. The improved segmentation results again show the superiority of our DFPQ and HRCA. We include more qualitative results in the supplementary material.

\begin{table}[tb!]
\centering
\caption{Effect of the positional encodings  for the image features on ADE20K \texttt{val}~\cite{zhou2017scene} with 150 categories.}
\vspace{-0.5em}
\resizebox{0.47\textwidth}{!}{\begin{tabular}{c|c|c}
 & \begin{tabular}[c]{@{}c@{}} Mask2former~\cite{cheng2021masked} \\ mIoU s.s. (\%) \end{tabular} & \begin{tabular}[c]{@{}c@{}} FASeg \\ mIoU s.s. (\%) \end{tabular} \\ \shline
Sinusoidal~\cite{carion2020end} & 47.2 & 46.9 \\
Learnable absolute~\cite{gehring2017convolutional} &  47.0 & 47.5 \\
Conditional~\cite{chu2021conditional} & 47.3 & 48.3 \\
\end{tabular}}
\label{tab:k_p}
\vspace{-0.8em}
\end{table}

\begin{table}[t!]
\centering
\caption{Ablation study for FASeg  on ADE20K \texttt{val}~\cite{zhou2017scene} and Cityscapes \texttt{val}~\cite{cordts2016cityscapes}. \#P and \#F indicate the number of parameters (M) and
FLOPs (G) evaluated on 512512 images.}
\vspace{-0.5em}
\resizebox{0.47\textwidth}{!}{\begin{tabular}{cc|cccc}
DFPQ & HRCA & \begin{tabular}[c]{@{}c@{}} ADE20K \texttt{val} \\ mIoU s.s. (\%) \end{tabular} & \begin{tabular}[c]{@{}c@{}} Cityscapes \texttt{val} \\ mIoU s.s. (\%) \end{tabular}  & \#P & \#F \\ \shline
 &  & 47.2 & 79.4 & 44 & 71  \\
\checkmark &  & 47.7 & 80.0 & 44 & 71 \\
 & \checkmark & 47.6 & 79.8 & 50 & 72 \\
\checkmark & \checkmark & 48.3 & 80.5 & 51 & 72 \\
\end{tabular}}
\vspace{-1em}
\label{tab:each_module}
\end{table}



\begin{table}[t!]
\centering
\caption{Performance comparisons between DFPQ and other positional queries variants on ADE20K \texttt{val}~\cite{zhou2017scene} with 150 categories.}
\vspace{-0.5em}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{c|c}
Method & mIoU s.s.(\%) \\ \shline
Learnable positional queries & 46.9  \\
Pre-defined grid anchor positional queries & 46.6  \\
Dynamic anchor positional queries &  47.0 \\
Dynamic foreground positional queries & 47.8  \\
DFPQ & 48.3  \\
\end{tabular}}
\label{tab:abl_positional}
\vspace{-1em}
\end{table}

\subsection{Ablation Study}\label{subsec:abl}

\noindent\textbf{Effect of .} We investigate the effect of the positional encodings  for the image features on ADE20k \texttt{val} with R50 backbone. The results are reported in Table~\ref{tab:k_p}. We observe that different  have similar performance for Mask2former~\cite{cheng2021masked}. However, more powerful  leads to much higher performance for our FASeg. For instance, FASeg with conditional positional encodings~\cite{chu2021conditional} outperforms Mask2former counterpart and FASeg with sinusoidal positional encodings~\cite{carion2020end} by 1.0\% and 1.4\% mIoU, respectively. The reason is that compared to Mask2former, our FASeg additionally aggregates  to get DFPQ as explained in Section~\ref{subsec:dfpq}. Therefore, more powerful  leads to higher representational capability of DFPQ that boosts the performance. We also find that with sinusoidal positional encodings, FASeg has even lower performance than Mask2former as the DFPQ aggregated from sinusoidal positional encodings reflects a single anchor point which cannot cover the fine-grained segmentation cues.

\noindent\textbf{Effectiveness of DFPQ and HRCA.} We investigate the effectiveness of our DFPQ and HRCA on ADE20k \texttt{val} and Cityscapes \texttt{val} with the ResNet-50 backbone. The results are reported in Table~\ref{tab:each_module}. We observe that both DFPQ and HRCA gain clear margins from the vanilla Mask2former~\cite{cheng2021masked}.
To be specific, integrating DFPQ on Mask2former boosts the performance by 0.5\% and 0.6\% mIoU on ADE20k \texttt{val} and Cityscapes \texttt{val}, respectively, with barely any extra parameter and computational cost. It is indicated that DFPQ is lightweight and contributes largely on the performance gain. Employing HRCA on Mask2former leads to 0.4\% mIoU gain on both datasets, which however, has 6M more parameters and 1G higher FLOPs. The additional parameters and FLOPs are brought by the extra decoder layers handling high-resolution image features. Finally, our FASeg with both DFPQ and HRCA improves 1.1\% mIoU for both ADE20k \texttt{val} and Cityscapes \texttt{val}, demonstrating the superiority of our FASeg.

\noindent\textbf{DFPQ vs. other positional queries.} We investigate the effectiveness of our DFPQ and compare it with other learnable query variants on ADE20K \texttt{val}~\cite{zhou2017scene}. The results are presented in Table~\ref{tab:abl_positional}. Here we adopt all the other settings the same as our FASeg with the R50 backbone and only differ the positional queries for all the competitors. Specifically, we compare with four settings: 1) learnable parameterized positional queries that are randomly initialized~\cite{carion2020end}; 2) positional queries as the pre-defined grid anchor points akin to~\cite{wang2021anchor}; 3) positional queries dynamically generated from the center of the foreground masks predicted by the previous layer similar to~\cite{liu2022dabdetr}; 4) positional queries dynamically generated from the entire predicted foreground masks. We find that our DFPQ outperforms all the competitors by large margins. For example, our DFPQ achieves 1.7\% and 1.3\% higher mIoU than the pre-defined grid and dynamic anchor positional queries, respectively. It is suggested that our DFPQ better suits semantic segmentation than the other positional query variants. We also visualize cross-attention maps among the different positional queries in Figure~\ref{fig:attention}. We observe that our DFPQ (Figure~\ref{fig:attention}~(c)) helps generate more compact and consistent cross-attention maps focusing on the target segments than the learnable parameterized positional queries (Figure~\ref{fig:attention}~(a)) and dynamic anchor positional queries (Figure~\ref{fig:attention}~(b)).

\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\linewidth]{vis.pdf}
  \vspace{-2.0em}
  \caption{Qualitative results on the ADE20K \texttt{val}~\cite{zhou2017scene}. Compared to Mask2former~\cite{cheng2021masked}, our FASeg predicts masks with finer details and yields more accurate predictions.}
  \vspace{-1.0em}
  \label{fig:vis}
\end{figure}

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{attention.pdf}
  \vspace{-2.0em}
  \caption{Visualizations of the cross-attention maps for learnable positional queries (\cite{carion2020end,cheng2021masked}), dynamic anchor positional queries (alike \cite{liu2022dabdetr}) and our DFPQ. We show the visualizations for the normalized cross-attention maps in the last three decoder blocks and indicate the target segments in the red boxes. The cross-attention maps with the learnable positional queries and the dynamic anchor positional queries are often scattered without a clear focus and mix up different segments, while the cross-attention maps with DFPQ are more compact and consistent to reflect the target segments.}
  \vspace{-1em}
  \label{fig:attention}
\end{figure*}


\begin{table}[t!]
\centering
\caption{Performance comparisons between our HRCA and other efficient cross-attention methods on ADE20K \texttt{val}~\cite{zhou2017scene} with 150 categories. \#F denotes the number of FLOPs (G). The training memory footprint (M) and FLOPs are measured under 512512 image resolutions with a batch size of 4 on a single GPU.}
\vspace{-0.5em}
\resizebox{0.47\textwidth}{!}{\begin{tabular}{c|c|c|c}
Method& mIoU s.s. (\%) & Training Memory (M)  & \#F \\ \shline
Vanilla & 47.3 & 7,451 & 83 \\
Random  & 46.7 & 6,343 & 72 \\
RCDA &  47.5 & 6,082 & 72  \\
HRCA & 48.3 & 6,343 & 72 \\
\end{tabular}}
\label{tab:abl_hrca}
\vspace{-1em}
\end{table}

\begin{table}[t!]
\centering
\caption{Effect of  in our efficient HRCA on ADE20K \texttt{val}~\cite{zhou2017scene} with 150 categories. \#F indicates the number of FLOPs (G).}
\vspace{-0.5em}
\resizebox{0.47\textwidth}{!}{\begin{tabular}{c|ccc}
 & mIoU s.s. (\%) & Training memory (M) & \#F \\ \shline
 & 47.3 & 7,451 & 83 \\
 &  47.7 & 6,381 & 72  \\
 & 48.3 & 6,343 & 72 \\
 & 48.0 & 6,317 & 71 \\
\end{tabular}}
\label{tab:abl_cross}
  \vspace{-1em}
\end{table}

\noindent\textbf{HRCA vs. other efficient cross-attention methods.} We investigate the effectiveness of our HRCA and compare it with other cross-attention methods on ADE20k \texttt{val}. The results are reported in Table~\ref{tab:abl_hrca}. For a fair comparison, we only replace HRCA for the other efficient cross-attention methods on our FASeg with the R50 backbone. We compare with three baselines: 1) the vanilla cross-attention that models the entire high-resolution features; 2) our HRCA with randomly sampled top-k pixels to form set  that  is the same as HRCA; 3) RCDA~\cite{wang2021anchor} that the cross-attention is decoupled to a row-wise and column-wise attention as introduced in Section~\ref{subsec:efficient_cross_attention}. We empirically find that compared with the vanilla cross-attention, our HRCA achieves 1.0\% mIoU gain while exhibiting 1,108M lower training-time GPU memory and 11G lower FLOPs. Our HRCA also outperforms the two efficient cross-attention methods by large margins. For example, HRCA achieves 0.8\% higher mIoU than RCDA with marginally increased training-time memory. The results demonstrate the superiority of our HRCA for efficiently identifying and utilizing contextual tokens in high-resolution features.

\noindent\textbf{Effect of  in HRCA.} We then investigate how  affects the performance, memory consumption and computational complexity on FASeg with R50 backbone on ADE20k \texttt{val}. The results are reported in Table~\ref{tab:abl_cross}.  determines the number of contextual tokens used in attention as introduced in Section~\ref{subsec:efficient_cross_attention}. Here we measure the memory consumption by the training-time memory with a batch size of 4 on a single GPU. In the vanilla cross-attention layers, cross-attention attends to the entire feature maps from the encoder, in which case . We observe that our HRCA outperforms the vanilla cross-attention by a significant margin.  We conjecture that the sparse property~\cite{frankle2018the,evci2020rigging} has reduced the redundancy in high-resolution feature maps in our HRCA and leads to higher performance and efficiency. Since our HRCA achieves the highest performance when , we set  by default for all the other experiments.


\begin{table}[tb!]
\centering
\caption{Effect of applying HRCA to other high-resolution feature scales for FASeg with Swin-B Backbone on ADE20K \texttt{val}~\cite{zhou2017scene} with 150 categories.}
\vspace{-0.5em}
\resizebox{\linewidth}{!}{\begin{tabular}{cc|cc}
 &  & mIoU s.s. (\%) & Training Memory (M) \\ \shline
\checkmark & & 55.0 & 20,418 \\
\checkmark & \checkmark & 54.9 & 19,898 \\
  & \checkmark & 54.8 & 17,817 \\
\end{tabular}}
\label{tab:abl_more_hrca}
\vspace{-2em}
\end{table}

\noindent\textbf{Effect of applying HRCA to other high-resolution feature scales.} By default, HRCA is applied only to the high-resolution  feature scale. We explore applying HRCA to  and both  and  feature scales for FASeg with Swin-B backbone on ADE20k \texttt{val}. We measure the training-time memory consumption with a batch size of 4 on
a single GPU and report the results in Table~\ref{tab:abl_more_hrca}. We find that the performance only fluctuates within 0.2\% mIoU. In particular, modeling the cross-attention only on the  feature scale with HRCA saves more than 2,000M training-time memory, suggesting the potential for extending HRCA to more high-resolution features to alleviate the memory burden.