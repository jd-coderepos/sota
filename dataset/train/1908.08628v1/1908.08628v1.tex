\section{Experiments}

\subsection{Dataset and Evaluation Metric}
We train and evaluate on the ISTD dataset \cite{Wang_2018_CVPR}. ISTD consists of image triplets: shadow image, shadow mask, and shadow-free image, captured from different scenes. The training split has 1870 image triplets from 135 scenes, whereas the testing split has 540 triplets from 45 scenes.


\def\subboxsize{0.3\subFigSzab}
\begin{figure}[]
 \centering
    \includegraphics[width=0.45\textwidth]{fig/fix_dataset.png}
     \makebox[\subboxsize]{Shad. Image}
    \makebox[\subboxsize]{Original GT}
    \makebox[\subboxsize]{Corrected GT}
    \caption{\textbf{An example of our color correction method.} From left to right: input shadow image, provided shadow-free ground truth image (GT) from ISTD dataset, and the GT image corrected by our method. Comparing to the input shadow image on the non-shadow area only, the root-mean-square distance of the original GT is 12.9. This value on our corrected GT becomes 2.9. 
    }
    \label{fig:fix_dataset}
\end{figure}

We notice that the testing set of the  ISTD dataset needs to be adjusted since the shadow images and the shadow-free images have inconsistent colors. This is a well known issue mentioned in the original paper \cite{Wang_2018_CVPR}. The reason is that the shadow and shadow-free image pairs were captured at different times of the day which resulted in slightly different environment lights for each image.
For example, Fig. \ref{fig:fix_dataset} shows a shadow  and shadow-free image pair. The root-mean-square difference between these two images in the non-shadow area is 12.9. This color inconsistency appears frequently in the testing set of the ISTD dataset. On the whole testing set, the root-mean-square distance between the shadow images and shadow-free images in the non-shadow area is 6.83, as computed by Wang \etal \cite{Wang_2018_CVPR}. 

In order to mitigate this color inconsistency, we use linear regression to transform the pixel values  in the non-shadow area of each shadow-free image to map into  their counterpart values in the shadow image. We use a linear regression for each color-channel, similar to our method for relighting the shadow pixels in Sec. \ref{sec:SP-Net}. This simple transformation transfers the color tone and brightness of the shadow image to its shadow-free counterpart. The third column of Fig. \ref{fig:fix_dataset} illustrates the effect of our color-correction method. Our proposed method reduces the root-mean-square distance between the shadow-free image and the shadow image from 12.9 to 2.9. The error reduction for the whole testing set of ISTD goes from 6.83 to 2.6. 

\subsection{Shadow Removal Evaluation}
 
We evaluate our method on the adjusted testing set of the ISTD dataset. For metric evaluation we follow \cite{Wang_2018_CVPR} and compute the RMSE in the LAB color space on the shadow area, non-shadow area, and the whole image, where all shadow removal results are re-sized into $256\times256$ to compare with the ground truth images at this size.  Note that in contrast to other methods that only output shadow free images at that resolution, our shadow removal system works for input images of any size. Since our method requires shadow masks, we use the model proposed by Zhu \etal \cite{zhu18b} pre-trained on the SBU dataset \cite{Vicente-etal-ECCV16} for detecting shadows. We take the model provided by the author and fine-tune it on the ISTD dataset for 3000 epochs. This model achieves 2.2 Balance Error Rate on the ISTD testing set. To remove the shadow effect in the image, we first use SP-Net to compute the shadow parameters $(w,b)$ using the input image and the shadow mask computed from the shadow detection network. We use $(w,b)$ to compute a relit image which is input to M-Net, together with the input image and the shadow mask to output a matte layer. We obtain the final shadow removal result via Eq. \ref{eq:decom}. In Table \ref{tab:basic}, we compare the performance of our method with the recent shadow removal methods of Guo \etal \cite{guoPami}, Yang \etal \cite{Yang12}, Gong \etal \cite{Gong16}, and Wang \etal \cite{Wang_2018_CVPR}. All numbers are computed on the adjusted testing images so that they are directly comparable. The first row shows the numbers for the input shadow images, i.e. no shadow removal performed.



We first evaluate our shadow removal performance using only SP-Net, i.e.  we use the binary shadow mask computed by the shadow detector to form the shadow-free image from the shadow image and the relit image. 
The binary shadow mask is obtained by simply thresholding the output of the shadow detector with a threshold of 0.95. As shown in  column ``\textit{SP-Net}'' (third from the right) in Fig. \ref{fig:main}, SP-Net correctly estimates the shadow parameters to relight the shadow area. Even with visible shadow boundaries, SP-Net alone outperforms the previous state-of-the-art, reducing the RMSE on the shadow area by 29\%, from 13.3 to 9.5. 


\def\subboxsize{0.22\subFigSzab}
\begin{figure}[]
 \centering
\includegraphics[width=0.45\textwidth]{fig/stcgan/101-3.png}
\includegraphics[width=0.45\textwidth]{fig/stcgan/110-8.png}
       \includegraphics[width=0.45\textwidth]{fig/stcgan/117-17.png}
\includegraphics[width=0.45\textwidth]{fig/stcgan/122-14.png}
\includegraphics[width=0.45\textwidth]{fig/stcgan/119-11.png} 
\makebox[\subboxsize]{Input}
    \makebox[\subboxsize]{Wang \etal \cite{Wang_2018_CVPR}}
        \makebox[\subboxsize]{Ours}
    \makebox[\subboxsize]{GT}
    \caption{\textbf{Comparison of shadow removal between our method and ST-CGAN \cite{Wang_2018_CVPR}.} ST-CGAN tends to produce blurry images, random artifacts, and incorrect colors of the lit pixels while our method handles all cases well. 
    }
    \label{fig:stcgan}
\end{figure}


We then evaluate the shadow removal results using both SP-Net and M-Net, denoted as 
``\textit{SP+M-Net}'' in Tab. \ref{tab:basic} and Fig. \ref{fig:main}. As shown in Fig. \ref{fig:main}, the results of M-Net do not contain  boundary artifacts. 
In the third row of Fig. \ref{fig:main}, SP-Net overly relights the shadow area but the shadow matte computed from M-Net effectively corrects these errors. This is because M-Net is trained to blend the relit and shadow images  to create the shadow-free image. Therefore, M-Net  learns to output a smaller weight for a pixel that is overly lit by SP-Net.
Using the matte layer of M-Net further reduces the RMSE on the shadow area by 17\%, from 9.5 to 7.9. 

Overall, our method generates better results than other methods.  Our method does a better job at estimating the overall illumination changes compared to the model of Gong \etal, which tends to overly relight  shadow pixels, as shown in Fig. \ref{fig:main}. Our method does not show color inconsistencies within the relit area contrary to all other methods.  Fig. \ref{fig:stcgan}  qualitatively compares  our method and ST-CGAN, which illustrates common issues present in images generated by deep networks \cite{isola2017image,zhang2016colorful}. ST-CGAN generally generates blurry images and introduces random artifacts. Our method, albeit not perfect, handles all cases well.  

Our method fails to recover the shadow-free pixels properly as shown  in Fig. \ref{fig:fail}. The first row, shows how our method overly relights the shadowed area while in the second row, the color of the lit area is incorrect. 

Finally, we trained and evaluated two alternative designs that do not require shadow masks as input: (1) The first is an end-to-end shadow-removal system where we jointly train a shadow detector together with our proposed SP-Net and M-Net. This framework is harder to train due to the increase in the number of network parameters. (2) The second is a version of our framework that does not input the shadow masks into both SP-Net and M-Net. Hence, SP-Net and M-Net need to learn to localize the shadow areas implicitly. As can be seen in the two bottom rows of  Tab. \ref{tab:basic}, both designs achieved slightly worse shadow removal results than our main setting.


\begin{table}[]
\centering
\caption{\textbf{Shadow removal results of our networks compared to state-of-the-art  shadow removal methods on the adjusted ground truth.} $(*)$ The method of Gong \etal \cite{Gong16} is an interactive method that defines the shadow/non-shadow regions via user inputs, thus generates minimal error on the non-shadow area. The metric is RMSE (the lower, the better). Best results are in bold.}
\begin{tabular}{lccc}
\toprule
Methods                    & Shadow& Non-Shadow& All  \\ 
\midrule
Input Image                & 40.2  & 2.6 & 8.5\\ 
\midrule
Yang \etal~\cite{Yang12}                 & 24.7  & 14.4 & 16.0\\ 
Guo \etal~\cite{guoPami}                  & 22.0  & 3.1 & 6.1\\ 
Wang \etal \cite{Wang_2018_CVPR}    & 13.4  & 7.7 & 8.7\\ 
Gong \etal~\cite{Gong16}            & 13.3  & \textbf{2.6*} & 4.2\\ 
\midrule
SP-Net (Ours)  & 9.5  &3.2 &4.1\\ 
SP+M-Net (Ours)   & \textbf{7.9}  &3.1 &\textbf{3.9}\\
\midrule
\midrule
\multicolumn{4}{c}{Our Method with Alternative Settings}\\
\midrule
With a Shad. Detector  & 8.4  &5.0 &5.5\\ 
No Input Shadow Mask   & 8.3  &4.9 &5.4\\
\midrule
\end{tabular}
\label{tab:basic}
\end{table}


\def\subboxsize{0.27\subFigSzab}
\begin{figure}[]
 \centering


    \includegraphics[width=0.45\textwidth]{fig/fail/114-1.png}
    \includegraphics[width=0.45\textwidth]{fig/fail/123-3.png}
\makebox[\subboxsize]{Input}
    \makebox[\subboxsize]{Ours}
        \makebox[\subboxsize]{GT}
 
    \caption{\textbf{Failure cases of our method.} In the first row, our method overly lights up the shadow area. In the second row, our method generates incorrect colors.  
    }
    \label{fig:fail}
\end{figure}


\subsection{Dataset Augmentation via Shadow Editing}
\label{sec:aug}
Many deep learning work focus on learning from more easily obtainable, weakly-supervised, or synthetic data \cite{Buhmann12weak,LeICCV2017,Liu2014FashionPW,Liu2013WeaklySupervisedDC,appleShrivastavaPTSW16,m_Le-etal-ECCV18,Le_2019_CVPR_Workshops}. In this section, we show that we can modify shadow effects  using our proposed illumination model to generate additional training data. 

Given a shadow matte $\alpha$, a shadow-free image, and  parameters $(w,b)$, we can form a shadow image by:
\begin{equation}
    I^{\textrm{shadow}} = I^{\textrm{shadow-free}}\cdot \alpha + I^{\textrm{darkened}}\cdot (1-\alpha) \\
    \label{eq:aug}
\end{equation}

\noindent where $I^{\textrm{darkened}}$ has undergone the shadow effect associated to the set of shadow parameters $(w,b)$. Each pixel $i$ of $I^{\textrm{darkened}}$ is computed by:
\begin{equation}
I_i^{\textrm{darkened}} = (I_i^{\textrm{shadow-free}}-b) \cdot w^{-1}
\end{equation}

For each training image, we first compute the shadow parameters and the matte layer via Eqs. \ref{eq:trans} and \ref{eq:alpha}. Then, we  generate a new synthetic shadow image via Eq. \ref{eq:aug} with a scaling factor $w_{\textrm{syn}} = w \times k$. 
As  seen in Fig. \ref{fig:aug}, a lower $w$ leads to an image with a lighter shadow area while a higher $w$  increases the shadow effects instead. 
Using this method, we augment the ISTD training set by simply choosing $k=\left[0.8,0.9,1.1,1.2\right]$ to generate a new set of 5320 images, which is four times bigger than the original training set.
We augment the original ISTD dataset with this dataset.
 Training our model on this new augmented ISTD dataset improves our results, as the RMSE drops by 6\%, from 7.9 to 7.4, as reported in Tab. \ref{tab:all}. 

\def\subboxsize{0.3\subFigSzab}
\begin{figure}[]
 \centering
    \includegraphics[width=0.45\textwidth]{fig/augment.pdf}
     \makebox[\subboxsize]{Syns. Image}
    \makebox[\subboxsize]{Real Image}
    \makebox[\subboxsize]{Syns. Image}
        \makebox[\subboxsize]{ $w_{\textrm{syn}}=w\times 0.8$}
    \makebox[\subboxsize]{ }
    \makebox[\subboxsize]{$w_{\textrm{syn}}=w\times 1.7$}
    \caption{\textbf{Shadow editing via our decomposition model.} We use Eq. \ref{eq:aug} to generate synthetic shadow images. As we change the shadow parameters, the shadow effects change accordingly. We show two example images from the ISTD training set where in the middle column are the original images and in the first and last column are synthetic.
    }
    \label{fig:aug}
\end{figure}

\begin{table}[]

\centering
\caption{\textbf{Shadow removal results of our networks train on the augmented ISTD dataset.} The metric is RMSE (the lower, the better).  Training our framework on the augumented ISTD dataset drops the RMSE on the shadow area from 7.9 to 7.4. }
\begin{tabular}{lcccc}
\toprule
Methods        &Train. Set            & Shad. & Non-Shad.& All  \\ 
\midrule
SP-Net &Aug. ISTD & 9.0  &3.2 &4.1\\ 
SP+M-Net &Aug. ISTD  & 7.4  &3.1 & 3.8\\
\midrule

\end{tabular}
\label{tab:all}
\end{table}








\def\subboxsize{0.12\subFigSzab}
\def\subfig{0.95\textwidth}
\begin{figure*}[]
 \centering

\includegraphics[width=\subfig]{fig/main_figure/112-3.png}\\
\includegraphics[width=\subfig]{fig/main_figure/93-3.png}\\
    \includegraphics[width=\subfig]{fig/main_figure/127-3.png}\\
\includegraphics[width=\subfig]{fig/main_figure/120-10.png}\\
\includegraphics[width=\subfig]{fig/main_figure/105-6.png}\\
     \includegraphics[width=\subfig]{fig/main_figure/122-14.png}\\
    \makebox[\subboxsize]{Input}
    \makebox[\subboxsize]{Guo \etal}
    \makebox[\subboxsize]{Yang \etal}
    \makebox[\subboxsize]{Gong \etal}
    \makebox[\subboxsize]{Wang \etal }
    \makebox[\subboxsize]{SP-Net}
    \makebox[\subboxsize]{SP+M-Net}
    \makebox[\subboxsize]{Ground }\\
    \makebox[\subboxsize]{}
    \makebox[\subboxsize]{~\cite{guoPami}}
    \makebox[\subboxsize]{~\cite{Yang12}}
    \makebox[\subboxsize]{~\cite{Gong16}}
    \makebox[\subboxsize]{\cite{Wang_2018_CVPR}}
    \makebox[\subboxsize]{(Ours)}
    \makebox[\subboxsize]{(Ours)}
    \makebox[\subboxsize]{Truth}
  
     \caption{\textbf{Comparison of shadow removal on ISTD dataset.}     Qualitative comparison between our method and previous state-of-the-art methods: Guo \etal \cite{guoPami}, Yang \etal \cite{Yang12}, Gong \etal \cite{Gong16}, and Wang \etal \cite{Wang_2018_CVPR}. ``SP-Net'' are the shadow removal results using the parameters computed from SP-Net and a binary shadow mask. ``SP+M-Net'' are the  shadow removal results using the parameters computed from SP-Net and the shadow matte computed from M-Net.
    }
 
    \label{fig:main}
\end{figure*}
