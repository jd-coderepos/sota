\section{Experiments} \label{sec:exp}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.45\textwidth]{images/model.pdf}
    \vskip -1em
    \caption{Our model architecture. Strong supervision over supporting facts is used in a multi-task setting.} \label{fig:model}
\end{figure}

\subsection{Model Architecture and Training} \label{sec:baseline}

To test the performance of leading QA systems on our data, we reimplemented the architecture described in \citet{clark2017simple} as our baseline model. We note that our implementation without weight averaging achieves performance very close to what the authors reported on \squad{} (about 1 point worse in \fone{}). Our implemented model subsumes the latest technical advances on question answering, including character-level models, self-attention \cite{wang2017gated}, and bi-attention \cite{seo2016bidirectional}.
Combining these three key components is becoming standard practice, and various state-of-the-art or competitive architectures \cite{liu2017stochastic,clark2017simple,wang2017gated,seo2016bidirectional,pan2017memen,salant2017contextualized,xiong2017dcn+} on SQuAD can be viewed as similar to our implemented model.
To accommodate yes/no questions, we also add a 3-way classifier after the last recurrent layer to produce the probabilities of ``yes'', ``no'', and span-based answers.
During decoding, we first use the 3-way output to determine whether the answer is ``yes'', ``no'', or a text span. If it is a text span, we further search for the most probable span.







\paragraph{Supporting Facts as Strong Supervision.}
To evaluate the baseline model's performance in predicting explainable supporting facts, as well as how much they improve QA performance, we additionally design a component to incorporate such strong supervision into our model.
For each sentence, we concatenate the output of the self-attention layer at the first and last positions, and use a binary linear classifier to predict the probability that the current sentence is a supporting fact.
We minimize a binary cross entropy loss for this classifier.
This objective is jointly optimized with the normal question answering objective in a multi-task learning setting, and they share the same low-level representations.
With this classifier, the model can also be evaluated on the task of supporting fact prediction to gauge its explainability.
Our overall architecture is illustrated in Figure \ref{fig:model}.
Though it is possible to build a pipeline system, in this work we focus on an end-to-end one, which is easier to tune and faster to train.






\subsection{Results}

\begin{table*}
    \small
    \centering
    \begin{tabular}{llcccccc}
        \toprule
        \multirow{2}{*}{Setting} & \multirow{2}{*}{Split} & \multicolumn{2}{c}{Answer} & \multicolumn{2}{c}{Sup Fact} & \multicolumn{2}{c}{Joint} \\
        \cmidrule{3-8}
        & & EM & \fone & EM & \fone & EM & \fone \\
        \midrule
        distractor & dev & 44.44 & 58.28 & 21.95 & 66.66 & 11.56 & 40.86 \\
        distractor & test & 45.46 & 58.99 & 22.24 & 66.62 & 12.04 & 41.37 \\
        \midrule
        full wiki & dev & 24.68 & 34.36 & \phantom{0}5.28 & 40.98 & \phantom{0}2.54 & 17.73 \\
        full wiki & test & 25.23 & 34.40 & \phantom{0}5.07 & 40.69 & \phantom{0}2.63 & 17.85 \\
        \bottomrule
    \end{tabular}
    \caption{Main results: the performance of question answering and supporting fact prediction in the two benchmark settings. We encourage researchers to report these metrics when evaluating their methods.} \label{tab:main}
    \vskip -0.7em
\end{table*}

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Set & MAP & Mean Rank & Hits@2 & Hits@10 \\
        \midrule
        dev & 43.93 & 314.71& 39.43 & 56.06  \\
        test & 43.21 & 314.05& 38.67 & 55.88 \\
        \bottomrule
    \end{tabular}
    \caption{Retrieval performance in the full wiki setting. Mean Rank is averaged over the ranks of two gold paragraphs.} \label{tab:fullwiki_ir}
    \vskip -0.2em
\end{table}





\begin{table}[t]
    \small
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Setting & Br EM & Br \fone & Cp EM & Cp \fone \\
        \midrule
distractor & 43.41 & 59.09 & 48.55 & 55.05 \\
        full wiki & 19.76 & 30.42 & 43.87 & 50.70 \\
        \bottomrule
    \end{tabular}
    \caption{Performance breakdown over different question types on the dev set in the distractor setting. ``Br'' denotes questions collected using bridge entities, and ``Cp'' denotes comparison questions.} \label{tab:break}
    \vskip -0.2em
\end{table}



\begin{table}[t]
    \small
    \centering
    \begin{tabular}{lcc}
        \toprule
        Setting & EM & \fone \\
        \midrule
        our model & 44.44 & 58.28 \\
        \midrule
        -- sup fact & 42.79 & 56.19 \\
        \midrule
        -- sup fact, self attention & 41.59 & 55.19 \\
        -- sup fact, char model & 41.66 & 55.25 \\
        \midrule
        -- sup fact, train-easy & 41.61 & 55.12 \\
        -- sup fact, train-easy, train-medium & 31.07 & 43.61 \\
        \midrule
        gold only & 48.38 & 63.58 \\
        sup fact only & 51.95 & 66.98 \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study of question answering performance on the dev set in the distractor setting. ``-- sup fact'' means removing strong supervision over supporting facts from our model. ``-- train-easy'' and ``-- train-medium'' means discarding the according data splits from training. ``gold only'' and ``sup fact only'' refer to using the gold paragraphs or the supporting facts as the only context input to the model.} \label{tab:ablation}
    \vskip -0.2em
\end{table}



We evaluate our model in the two benchmark settings.
In the full wiki setting, to enable efficient tf-idf retrieval among 5,000,000+ wiki paragraphs, given a question we first return a candidate pool of at most 5,000 paragraphs using an inverted-index-based filtering strategy\footnote{See Appendix \ref{sec:fullwiki_details} for details.} and then select the top 10 paragraphs in the pool as the final candidates using bigram tf-idf.\footnote{We choose the number of final candidates as 10 to stay consistent with the distractor setting where candidates are 2 gold paragraphs plus 8 distractors.}
Retrieval performance is shown in Table \ref{tab:fullwiki_ir}.
After retrieving these 10 paragraphs, we then use the model trained in the distractor setting to evaluate its performance on these final candidate paragraphs.



Following previous work \cite{rajpurkar2016squad}, we use exact match (EM) and \fone{} as two evaluation metrics.
To assess the explainability of the models, we further introduce two sets of metrics involving the supporting facts.
The first set focuses on evaluating the supporting facts directly, namely EM and \fone{} on the set of supporting fact sentences as compared to the gold set.
The second set features joint metrics that combine the evaluation of answer spans and supporting facts as follows.
For each example, given its precision and recall on the answer span () and the supporting facts (), respectively, we calculate joint \fone{} as

\vspace{-2em}

Joint EM is 1 only if both tasks achieve an exact match and otherwise 0.
Intuitively, these metrics penalize systems that perform poorly on either task.
All metrics are evaluated example-by-example, and then averaged over examples in the evaluation set.

The performance of our model on the benchmark settings is reported in Table \ref{tab:main}, where all numbers are obtained with strong supervision over supporting facts. From the distractor setting to the full wiki setting, expanding the scope of the context increases the difficulty of question answering.
The performance in the full wiki setting is substantially lower, which poses a challenge to existing techniques on retrieval-based question answering.
Overall, model performance in all settings is significantly lower than human performance as shown in Section \ref{sec:human}, which indicates that more technical advancements are needed in future work.

We also investigate the explainability of our model by measuring supporting fact prediction performance.
Our model achieves 60+ supporting fact prediction \fone{} and 40 joint \fone{}, which indicates there is room for further improvement in terms of explainability.



In Table \ref{tab:break}, we break down the performance on different question types.
In the distractor setting, comparison questions have lower \fone{} scores than questions involving bridge entities (as defined in Section 2), which indicates that better modeling this novel question type might need better neural architectures.
In the full wiki setting, the performance of bridge entity questions drops significantly while that of comparison questions decreases only marginally.
This is because both entities usually appear in the comparison questions, and thus reduces the difficulty of retrieval.
Combined with the retrieval performance in Table \ref{tab:fullwiki_ir}, we believe that the deterioration in the full wiki setting in Table \ref{tab:main} is largely due to the difficulty of retrieving both entities.

We perform an ablation study in the distractor setting, and report the results in Table \ref{tab:ablation}.
Both self-attention and character-level models contribute notably to the final performance, which is consistent with prior work.
This means that techniques targeted at single-hop QA are still somewhat effective in our setting.
Moreover, removing strong supervision over supporting facts decreases performance, which demonstrates the effectiveness of our approach and the usefulness of the supporting facts.
We establish an estimate of the upper bound of strong supervision by only considering the supporting facts as the oracle context input to our model, which achieves a 10+ \fone{} improvement over not using the supporting facts.
Compared with the gain of strong supervision in our model (2 points in \fone{}), our proposed method of incorporating supporting facts supervision is most likely suboptimal, and we leave the challenge of better modeling to future work.
At last, we show that combining all data splits (\textit{train-easy}, \textit{train-medium}, and \textit{train-hard}) yields the best performance, which is adopted as the default setting.



\subsection{Establishing Human Performance} \label{sec:human}

To establish human performance on our dataset, we randomly sampled 1,000 examples from the dev and test sets, and had at least three additional Turkers provide answers and supporting facts for these examples.
As a baseline, we treat the original Turker during data collection as the prediction, and the newly collected answers and supporting facts as references, to evaluate human performance.
For each example, we choose the answer and supporting fact reference that maximize the \fone{} score to report the final metrics to reduce the effect of ambiguity \cite{rajpurkar2016squad}.

As can be seen in Table \ref{tab:human_perf}, the original crowd worker achieves very high performance in both finding supporting facts, and answering the question correctly.
If the baseline model were provided with the correct supporting paragraphs to begin with, it achieves parity with the crowd worker in finding supporting facts, but still falls short at finding the actual answer.
When distractor paragraphs are present,  the performance gap between the baseline model and the crowd worker on both tasks is enlarged to 30\% for both EM and \fone{}.

We further establish the upper bound of human performance in \datasetname, by taking the maximum EM and \fone{} for each example.
Here, we use each Turker's answer in turn as the prediction, and evaluate it against all other workers' answers.
As can be seen in Table \ref{tab:human_perf}, most of the metrics are close to 100\%, illustrating that on most examples, at least a subset of Turkers agree with each other, showing high inter-annotator agreement.
We also note that crowd workers agree less on supporting facts, which could reflect that this task is inherently more subjective than answering the question.

\setlength{\tabcolsep}{4pt}
\begin{table}
    \small
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        \multirow{2}{*}{Setting} & \multicolumn{2}{c}{Answer} & \multicolumn{2}{c}{Sp Fact} & \multicolumn{2}{c}{Joint} \\
        \cmidrule{2-7}
        & EM & \fone & EM & \fone & EM & \fone \\
        \midrule
        gold only & 65.87 & 74.67 & 59.76 & 90.41 & 41.54 & 68.15 \\
        distractor & 60.88 & 68.99 & 30.99 & 74.67 & 20.06 & 52.37 \\
        \midrule
        Human & 83.60 & 91.40 & 61.50 & 90.04 & 52.30 & 82.55 \\
        Human UB & 96.80 & 98.77 & 87.40 & 97.56 & 84.60 & 96.37 \\
        \bottomrule
    \end{tabular}
    \caption{Comparing baseline model performance with human performance on 1,000 random samples. ``Human UB'' stands for the upper bound on annotator performance on \datasetname. For details please refer to the main body.} \label{tab:human_perf}
\end{table}
\setlength{\tabcolsep}{6pt}






