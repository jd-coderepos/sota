

\documentclass[a4paper,fleqn]{cas-dc}

\usepackage[numbers]{natbib}
\usepackage{amsfonts}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{algorithmic}
\usepackage{footnote}
\usepackage{threeparttable}
\usepackage{color}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}


\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\shorttitle{Divide-and-conquer based Large-Scale Spectral Clustering}
\shortauthors{Hongmin Li et~al.}

\title [mode = title]{Divide-and-conquer based Large-Scale Spectral Clustering}



\author{Hongmin Li}[type=editor,
                        auid=000,bioid=1,
orcid=0000-0003-0228-0600]
\cormark[1]
\ead{li.hongmin.xa@alumni.tsukuba.ac.jp}


\address{Department of Computer Science, University of Tsukuba, Tsukuba, Ibaraki 305-8577, Japan}

\author{Xiucai Ye}[auid=002,bioid=2,
orcid=0000-0002-5547-3919]
\ead{yexiucai@cs.tsukuba.ac.jp}
\author{Akira Imakura}[auid=003,bioid=3,
orcid=0000-0003-4994-2499]
\ead{imakura@cs.tsukuba.ac.jp}
\author{Tetsuya Sakurai}[auid=004,bioid=4,
orcid=0000-0001-9128-9396]
\ead{sakurai@cs.tsukuba.ac.jp}
\cortext[cor1]{Corresponding author}


\begin{abstract}
        Spectral clustering is one of the most popular clustering methods. 
        However, how to balance the efficiency and effectiveness of the large-scale spectral clustering with limited computing resources has not been properly solved for a long time.
        In this paper, we propose a divide-and-conquer based large-scale spectral clustering method to strike a good balance between efficiency and effectiveness.
        In the proposed method, a divide-and-conquer based landmark selection algorithm and a novel approximate similarity matrix approach are designed to construct a sparse similarity matrix within low computational complexities. Then clustering results can be computed quickly through a 
        bipartite graph partition process. 
        The proposed method achieves the lower computational complexity than most existing large-scale spectral clustering methods.
        Experimental results on ten large-scale datasets have demonstrated the efficiency and effectiveness of the proposed methods.
        The MATLAB code of the proposed method and experimental datasets are available at \url{https://github.com/Li-Hongmin/MyPaperWithCode}.

\end{abstract}

\begin{keywords}
Spectral Clustering  \sep Landmark selection \sep Approximate Similarity Computation \sep Large-scale clustering \sep Large-scale datasets
\end{keywords}


\maketitle



\section{Introduction}
Clustering is one of the most fundamental problems in data mining and machine learning, aiming to categorize data points into clusters such that the data points in the same cluster are more similar while data points in different clusters are more different from each other \cite{rokach2005clustering,xu2008clustering,liu2013understanding}.
Spectral clustering has attracted increasing attention due to the promising ability to deal with nonlinearly separable datasets \cite{filippone2008survey,ng2002spectral}.
It has been successfully applied to various problem domains such as biology \cite{pentney2005spectral}, image segmentation \cite{zhang2008spectral}, and recommend systems \cite{zhang2014detection,ye2018spectral}.
Although spectral clustering algorithm often provides better performances than traditional clustering algorithm likes -means especially for complex datasets, it is significantly limited to be applied to large-scale datasets due to its high computational complexity and space complexity \cite{li2015large,ye2018large}.

The conventional spectral clustering algorithm mainly consists of two high-cost steps, i.e., similarity matrix construction and eigen-decomposition.
For a dataset with  objects, the two steps take computational complexities of  and , respectively.
The computational consumption of these two steps is the main reason that hinders the application of spectral clustering algorithms on large-scale data.

In recent years, there has been an increasing amount of literature on alleviating the computational complexity of spectral clustering \cite{fowlkes2004spectral,cai2014large,li2015superpixel,bouneffouf2015sampling,zhang2016sampling,ye2018large,huang2019ultra,li2020hubness}.
Previous research \cite{chen2010parallel} has established that the sparse similarity matrix construed by only remaining -nearest neighbors or -nearest neighbors can efficiently reduce the space complexity. As a result, some sparse eigensolvers can solve the eigen-decomposition problems within the lower computational complexity.
The matrix specification strategy can avoid storing the dense similarity matrix to reduces the space complexity, but it still needs to compute the dense similarity matrix at first, which costs  computational complexity.
Besides the matrix specification, another commonly used strategy is based on a cross-similarity matrix construction \cite{fowlkes2004spectral, cai2014large, li2015superpixel, ye2018large,li2020hubness,huang2019ultra}.
Fowlkes et al. \cite{fowlkes2004spectral} apply the Nystr{\"{o}}m method to reduce the high complexity of spectral clustering algorithm, which first randomly selects a small subset of samples as landmarks, then construct a similarity sub-matrix between these landmarks and remaining samples.
Although the random landmark selection is very efficient, it is often unstable concerning the quality of the landmark set.
Moreover, it has been shown that a larger  is often favorable for better approximation. 
To address the potential instability of random selection, Cai et al. \cite{cai2014large} extend the Nystr{\"{o}}m method and propose the landmark-based spectral clustering method, which uses -means to obtain  cluster centers as  landmark points to construct the similarity sub-matrix.
With the constructed  sub-matrix, they then convert it into sparse by preserving the -nearest landmarks of the data points and filling with zeros to others.
By the -means based landmarks selection, the LSC algorithm shows better performance than Nystr\"{o}m.
On this basis, some studies \cite{zhang2008improved,bouneffouf2015sampling,zhang2016sampling,rafailidis2017landmark,ye2018large, li2020hubness} on the landmark selection are further proposed to improve the instability of sub-matrix based large-scale spectral clustering.
However, the computational complexity of the sub-matrix construction can still be a critical bottleneck when dealing with large-scale clustering tasks.
Huang et al. \cite{huang2019ultra} propose a hybrid representative (landmark) selection method that initializes candidate samples randomly from the dataset and performs -means to obtain  cluster centers as the representative points then computes the approximation of -nearest representatives. It does not compute the dense similarity sub-matrix but approximates a sparse sub-matrix, further reducing similarity construction costs.
However, those sub-matrix based spectral clustering algorithms are typically restricted by an  or  complexity bottleneck, which is still a critical hurdle for them to deal with large-scale datasets where a larger  is often desired for achieving better approximation.
Although some considerable studies have been proposed in recent years, it remains a highly challenging problem, i.e., how to make spectral clustering handle large-scale datasets efficiently and effectively within limited computing resources.

In this paper, to achieve a better balance between the effectiveness and efficiency of the spectral clustering for large-scale datasets, we propose the \textbf{d}ivide-a\textbf{n}d-\textbf{c}onquer \textbf{s}pectral \textbf{c}lustering (DnC-SC) method.
In DnC-SC, a novel divide-and-conquer based landmark selection method is proposed to generate high-quality  landmarks, which reduces the computational complexity of -means based selection from  to , where  is the \textit{selection rate parameter} that determines the upper bound of computational complexity.
Besides, a fast approximation method for -nearest landmarks is designed to efficiently build a sparse sub-matrix with  computational complexity and  space complexity.
A cross similarity matrix is constructed between the data points and the  landmarks, which can be interpreted as the edges matrix of a bipartite graph.
The bipartite graph partition is then conducted to solve the spectrum with , where  is the number of clusters.
Finally, the -means method is used to obtain the clustering result on the spectrum with , where  is the number of iterations during -means.
As it generally holds that , the computational and space complexity of our DnC-SC algorithm are respectively dominated by  and .
The experimental results on eight large-scale datasets (consisting of five real-word datasets and five synthetic datasets) show the priority performance of proposed methods on both efficiency and effectiveness.

The main contributions of the proposed method are summaries as follows: 

\begin{itemize}
  \item A divide-and-conquer-based landmark selection method is proposed to efficiently find  centralized subset centers as landmarks in a recursive manner.
  \item A fast -nearest landmarks search method is designed, which uses centers' nature of landmarks to identify the most possible -nearest landmarks candidates.
  \item A large-scale spectral clustering algorithm termed DnC-SC is proposed, which efficiently constructs the similarity matrix and uses bipartite graph partition to obtain final clustering results. Its computational and space complexity is dominated by  and , which achieves the lower computational complexity than most existing large-scale spectral clustering methods.

  
\end{itemize} 
\section{Preliminaries}
\label{sec:related_work}

This section reviews the literature related to spectral clustering and large-scale spectral clustering extensions.
\subsection{Spectral Clustering}

Spectral clustering aims to partition the data points into  clusters using the spectrum of the graph Laplacians \cite{von2007tutorial}.
Given a dataset  with  data points, spectral clustering algorithm first constructs similarity matrix , where  indicates the similarity between data points  and  via a similarity measure metric.

Let , where  is called graph Laplacian and  is a diagonal matrix with .
The objective function of spectral clustering can be formulated based on the graph Laplacian as follow:

where  denotes the trace norm of a matrix.
The rows of matrix  are the low dimensional embedding of the original data points.
Generally, spectral clustering computes  as the bottom  eigenvectors of , and finally applies -means on  to obtain the clustering results.


\subsection{Similarity Matrix for Large-scale Spectral Clustering}

To capture the relationship between all data points in , an  similarity matrix is needed to be constructed in conventional spectral clustering \cite{von2007tutorial}, which costs  time and  memory and is not feasible for large-scale clustering tasks.
Instead of a full similarity matrix, many accelerated spectral clustering methods \cite{fowlkes2004spectral, cai2014large, li2015superpixel, ye2018large,li2020hubness,huang2019ultra} are using a similarity sub-matrix to represent each data points by the cross-similarity between data points and a set of representative data points (i.e., landmarks) via some similarity measures, as

where  () is a set of landmarks with the same dimension to ,  indicate a similarity measure metric, and  is the similarity sub-matrix to represent the  with respect to the .

For large-scale spectral clustering using such similarity matrix,
a symmetric similarity matrix  can be designed as \cite{zha2001bipartite}

The size of matrix  is . 
Taking the advantage of the bipartite structure, some fast eigen-decomposition methods \cite{fowlkes2004spectral, cai2014large, li2015superpixel} can then  be used to obtain the spectral embedding.
Finally, -means is conducted on the embedding to obtain clustering results.

The clustering result is directly related to the quality of  that consists of the similarities between data points and landmarks.
Thus, the performance of landmark selection is crucial to the clustering result.

 
\section{Proposed Framework}
\label{sec:framework}

To further reduce the complexity of spectral clustering, we propose the DnC-SC method that complies with the sub-matrix based formulation \cite{fowlkes2004spectral,cai2014large} and aims to break through the efficiency bottleneck of previous algorithms.
DnC-SC consists of three phases:
(1) Divide-and-conquer based landmark selection:
we consider landmark selection as an optimization problem and present a divide-and-conquer based landmark selection method to find the landmarks via solving the sub-optimization problems recursively.
(2) Approximate similarity matrix construction: we design a novel strategy to efficiently approximate the -nearest landmarks for each data point and construct a sparse cross-similarity matrix between the  data points and the  landmarks.
(3) Bipartite graph partitioning: we interpret the cross-similarity as a bipartite graph and conduct a bipartite graph partition to obtain the clustering result.
We summary the proposed method in Figure \ref{fig:overview}.

\begin{figure*}
    \centering
    \includegraphics[width=1.0\textwidth]{figs/overview2.pdf}
    \caption{An overview of proposed DnC-SC method. Given a dataset, the DnC-SC method first finds the landmarks via divide-and-conquer based landmark selection, then approximately constructs the similarity matrix, finally conducts a bipartite graph partition to obtain final clustering results.
    Our main contributions focus on the first two phases (colored as orange), i.e., the landmark selection and similarity construction phases.}
    \label{fig:overview}
\end{figure*}


\subsection{Divide-and-conquer based Landmark Selection}
\label{sec:landmark_selection}

Let  denote a set of landmarks, where  has the same dimension as . 
Ideally, the landmark points  would roughly represent the distribution of .
Since the  nearest landmark graph will be utilized in the similarity construction,  would intuitively best represent the original data points when total distances between any  and their nearest landmarks are minimum.
Therefore, we measure how well  represent the nearest samples via the residual sum of squares (RSS) as follows,

where  denotes RSS and  indicate the subsets that are nearest to , respectively.
From the clustering view,  can be considered as the clusters. Therefore, the objective function of landmarks can be written as follows:

According to \eqref{eq: opt},  can be solved by -means directly.
Some previous studies \cite{cai2014large} show the effectiveness of -means based selection.
However, directly conducting -means on large-scale datasets faces a high time cost of .
Moreover, -means often needs more iterations to converges on large-scale datasets.


\begin{figure}
    \centering
    \includegraphics{figs/dnc_selection_dividing.pdf}
    \caption{An illustration of the divide-and-conquer base landmark selection: (1) The dataset is initially divided into  initial subsets; (2) Each subset is further divided into 3 smaller subsets and the total number of subsets reaches . Finally, the centers of subsets are turned as landmarks.
    }
    \label{fig:DnC Selection}
\end{figure}


Thus, we propose a divide-and-conquer based landmark selection method, which aims to find a set of high-quality landmarks efficiently.
Denote  as a small number and . 
We define  as \textit{selection rate parameter} that is the upper boundary of desired subset number in each dividing process to limit the computational complexity.
Instead of directly dividing data points into  subsets for landmark selection, we first divide data points into  subsets, then recursively divide each subset into  smaller subsets (), until the total number of subsets reaches .
Figure \ref{fig:DnC Selection} gives a simple example. The data points are
recursively divided into  subsets until the total number of subsets is , which avoids directly applying -means to obtain too many subsets at once.
In the divide-and-conquer strategy, the number of desired subsets in each iteration is much smaller than , and the subsets are smaller and smaller during iterations.
Suppose each dividing will divide  subsets of the same size and the number of iteration of each dividing is , we can estimate the total computational complexity as follows: 

According to \eqref{eq: complexity of DnC}, compared with -means based landmark selection, the divide-and-conquer strategy can naturally reduce the computational complexity from  to .
Moreover, we design an efficient dividing algorithm, named light--means, to further accelerate the whole process into .

\subsubsection{Divide-and-conquer Selection Strategy}

Before starting landmark selection, we first review the optimization problem \eqref{eq:repobj}.
The variables in \eqref{eq:repobj} are the  which are used to map the unique .
By setting the subsets  and landmark number as the variables, we can rewrite the optimization problem \eqref{eq:repobj} into a function form as follows:

where  indicates a centralized clustering problem that divides  into  subsets and  is the center of subset .
For any dividing problem that divides  into  subsets, the function  can be used to describe the dividing problem, and its computational complexity is , where  is the total number of samples in . 
More importantly, function  can be used to derive the recursive function as follows:

where  is a subset of  and ;  is the desired subset number of subset  and ; .
\eqref{eq: ai} can simply divide any optimization problem into  sub-problems, which builds a bridge between global problem  and local problem .
We can recursively apply \eqref{eq: ai} and \eqref{eq: gm} to divide the optimization problem \eqref{eq: obj function} into the sub-problems small enough and solve them locally and efficiently. 

Denote  as the total number of subsets during -th iteration.
We will stop the recursive process in the -th iteration when  reaches the desired total number of subsets .
Initially, we assign all data points as one subset.
As we only have one subset (), the dividing process happens.
Let  be the desired number of subset for dividing process on -th subset during -th iteration.
We naturally set the desired number .
However, directly apply  may be time-consuming.
For , we force  to obtain subsets partially.
As a result, we have the initial setting as follows:

In the first iteration, we divide  into  subsets in as follows: 

where  indicates the -th subset during first iteration and  is the total number of subsets.

From the second iteration, there are more and more subsets being obtained. 
Thus, we need a subset number allocation strategy to determine the desired number of subsets and guide iteration dynamically.
We define the RSS of subset  as 

where  is the center of the subset .
Consider the global problem \eqref{eq: opt}, the desired number of subsets should be proportional to their RSS.
We propose a dynamical allocation strategy as follows: 

where  is the allocated divide number for subset .
Then, all  are turned as integers and fix the , where .
After obtaining  (), we will divide each subset  into  smaller subsets via .
We then collect all subsets as follows: 

We repeat the above process until  subsets have been produced and set the  subset centers as the landmarks.

Take an example using Figure \ref{fig:DnC Selection}, where we set  and .
In the first iteration, we assign all data points as one subset. 
Since the desired landmark number , we set .
Then we initially divide the dataset  into  subsets. 
In the second iteration, there are three subsets ,  and .
According to \eqref{eq:DA}, we compute the , , .
Suppose , we then divide , ,  into  smaller subset respectively.
In the third iteration, there are 9 subsets .
Since the total number of subsets  reaches the desired landmark number , we stop the recursive process in the third iteration. 
Finally, compute the subset centers  of  and set them as the landmarks.

Note that the dividing process  can be directly solved by the -means method.
However, directly apply -means on large data is time-consuming.
To further reduce the complexity, we propose a modified -means method, named light--means.
When dataset size is large, we conduct the dividing process via light--means. 
Otherwise, we use the traditional -means method.
We summary the divide-and-conquer based landmark selection method in Algorithm \ref{ag: divide-and-conquer based landmark selection method}.


\begin{algorithm}
  \label{ag: divide-and-conquer based landmark selection method}
  \caption{Divide-and-conquer based landmark selection method}
  \SetAlgoLined
  \KwIn{Dataset , the number of landmarks , selection rate ;}
  \KwOut{Landmarks ;}
  Initialize: Set the  and  via \eqref{eq:k1};\\   
;\\
  ;\\
  \While {total number of subsets }{
    Set the  via \eqref{eq:DA};\\
    \ForEach(){}{
    \eIf{the size of  is larger than }
     {Conduct \eqref{eq:newcluter} via \textit{light--means};}
    {Conduct \eqref{eq:newcluter} via -means;}
    }
    ;\\ 
    ;
  }
  Collect the latest cluster centers as .
\end{algorithm}
\subsubsection{Light--means Algorithm}


We define  as an upper bound.
When the size of  is larger than , we will use light--means to compute .
The light--means is performed as the following steps:
\begin{enumerate}
\item Randomly select  representatives from  and denote them in a set as  and the complement of  is .
\item Conduct -means to divide  into  subsets;
\item Find the nearest subset centers for the remained data points in ;
\item Assign the remained data points in  to their the nearest subset (with the center nearest to these points).
\end{enumerate}



Figure \ref{fig:light_k_means} shows a comparison between -means and light--means method, which are the implementation examples of (1) in Figure \ref{fig:DnC Selection}.
Given a subset , the light--means first randomly select  data points and denotes them as  and the complement is  ().
Then the -means is used to divide  into  subsets, i.e., .
For each data points in , find its nearest center and assign it to the subset, i.e., , according to its nearest center.
Finally, return the combined subsets  as the results of this dividing process.







\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/ill_com_light_kmeans.pdf}
    \caption{An comparison between -means and light--means. (a) -means directly divide all samples  into 3 subsets.
    (b) In light--means, -means is applied on  representatives, which significantly reduce the complexity on large data.
    }
    \label{fig:light_k_means}
\end{figure}


\begin{algorithm}[]
  \label{ag: Light--means}
  \SetAlgoLined
  \KwIn{Data , the number of cluster , number of samples ;}
  \KwOut{ subsets;}
    Randomly select  samples from  and denote them as ;\\
    Denote the complement of  as ;\\
    Apply -means to divide  into  subsets;\\
    Find the nearest center of samples in ;\\
    Assign the samples in  to the subset according to their nearest centers.
  \caption{Light--means}
\end{algorithm}

Denote  as the number of samples in .
The computational complexity of light--means for the dividing process  should be , where  is the dominant term.
While, -means costs  for the same dividing process.
Compared with -means, light--means significantly alleviates the computational complexity on iterative optimization.
Empirically, the number of  is suggested to be several times larger than , e.g., , to provide enough samples for the -means algorithm.
Since our landmark selection focuses more on the local dividing, the light--means can effectively divide the large subsets into small ones and find more accurate subsets locally.
Finally, we summarise the light--means method in Algorithm \ref{ag: Light--means}.



\begin{figure}[]
  \begin{center}
    {\subfigure[]
      {\includegraphics[width=0.31\linewidth]{figs/random_selection}\label{fig:cmpSelStrategy_random}}}
    {\subfigure[]
      {\includegraphics[width=0.31\linewidth]{figs/kmeans_selection}}}
    {\subfigure[]
      {\includegraphics[width=0.31\linewidth]{figs/Dnc_selection}}}
    \caption{Comparison of the landmarks produced by (a) random selection, (b) -means based selection, and (c) Divide-and-conquer based selection.}
    \label{fig:cmpSelStrategy_all3}
  \end{center}
\end{figure}

By introducing the divide-and-conquer based landmark selection, the complexity of landmark selection is reduced to  from  of -means based selection.
Figure \ref{fig:cmpSelStrategy_all3} illustrates that the proposed divide-and-conquer based landmark selection can better represent data distribution than the random selection and has similar performance -means based selection, but it has the lower complexity than -means based selection.


\subsection{Approximate Similarity Matrix Construction}
\label{sec:approx_similiarty}

After landmark selection, the next object is to construct a similarity matrix between entire data points and the landmarks.
Instead of dense similarity matrix, we design a similarity matrix  according to -nearest neighbor as follows: 

where  denotes the set of -nearest landmarks of  and  is the bandwidth of Gaussian kernel.
Note that there are only  non-zero entries in the sparse matrix .

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/aknn.pdf}
    \caption{An illustration of our -nearest landmarks approximation: 
    (1) Find the -nearest () landmarks of , where  and  is the subset center of ;
    (2) Find the -nearest landmarks among -nearest landmarks of .
    }
    \label{fig:illustion_aknn}
\end{figure}

To estimate , we propose a new -nearest landmarks approximation method.
The main idea is to use the subset centers' nature of landmarks to estimate the possible nearest candidates, as shown in Figure \ref{fig:illustion_aknn}.
Formally, we denote  as the subset that  belongs to, and the landmark  as the center of .
Since  is the subset center of , it essentially is the nearest landmark of  according to \eqref{eq: opt}.
Take the advantage of this landmark nature, we search the -nearest landmarks of each data point  according to following two steps:
\paragraph{Step 1: Find  possibles candidates.}
As (1) of Figure \ref{fig:illustion_aknn} shows, we find the -nearest () landmarks of  and denoted them as .
Since the exact -nearest landmarks of  are highly possible closed to , we treat  as possible candidates set.
Empirically, the number of  is suggested to be several times larger than , e.g., , to provide enough candidates to search .
\paragraph{Step 2: Search the -nearest landmarks.}
As (2) of Figure \ref{fig:illustion_aknn} shows,
we search the -nearest landmarks of  among  and denote them as .

After the -nearest landmarks approximation, we compute the similarity matrix  according to \eqref{eq:gaussian kernel}.
For all data points, the complexity of \textit{step 1} is  and \textit{step 2} is .
The computational complexity of our similarity construction is .
As , the dominate term in the complexity is .
Compared with the exact similarity construction of  \cite{cai2014large,ye2018large}, our method is much faster.

\subsection{Bipartite Graph Partitioning}
\label{sec:bipartite_partition}

After obtaining the similarity matrix , we conduct graph partition on the graph Laplacian of the similarity matrix.
The similarity matrix  reflects the relationships between  and .
Therefore  can be interpreted as the cross-similarity matrix of the bipartite graph:

where  is the node-set.
As a result, the object is changed to a bipartite graph partition problem \cite{dhillon2001co,zha2001bipartite}.

The conventional spectral clustering finds a low dimensional embedding via the spectrum of graph Laplacian, which solves the generalized eigen-problem \cite{shi2000normalized}:

where  is the graph Laplacian and  is a diagonal matrix with .
Substituting the bipartite similarity matrix \eqref{eq: WusedB } into \eqref{eq:general_eigen_problem}, we have

where  and  are the diagonal matrices whose entries are  and , respectively.
Thus, the eigenvector  can be interpreted as two parts  and .
 is the spectral embedding on  side while  is the spectral embedding on  side.
To find the low dimensional embedding for  objects, we rewrite \eqref{eq:our_eigen_problem} as follows:

From \eqref{eq:a}, we compute eigenvector  as follows:

where  is also called the transition probability matrix \cite{li2015superpixel}.
Substituting \eqref{eq:u} into \eqref{eq:b}, we have

According to \eqref{eq:small_eigen_problem}, we design a smaller eigen-problem \eqref{eq:general_eigen_problem_reduced} as follows:

where , .
The matrix  is also the Laplacian of the graph .
The eigen-problem \eqref{eq:general_eigen_problem_reduced} can be solve within a smaller time cost .
Note that  and .
The value of  is unique and  increases along with .
Supposing  are the bottom  eigenvalues of the eigen-problem \eqref{eq:general_eigen_problem_reduced}, the  are also the bottom  eigenvalues of the eigen-problem \eqref{eq:general_eigen_problem}.
Therefore the bottom  eigenvalues can be obtained according to \eqref{eq:u}.
Since the  is constant value, in practice, we compute the spectral embedding  as follows:



\begin{algorithm}
  \label{ag: divided-and-conquer based large-scale spectral clustering}
  \caption{Divided-and-conquer based large-scale spectral clustering}
  \SetAlgoLined
  \KwIn{Dataset , the number of landmarks , selection rate , the number of nearest landmarks , the number of cluster ;}
  \KwOut{Label ;}
  Obtain  landmarks by divide-and-conquer based landmark selection method;\\
\ForEach(){}{
Let  be the subset that  belongs to and  be the center of ;\\
     Obtain -nearest ( landmarks of , denoted as ;\\
     Find the -nearest landmarks of  from , denoted as ;\\
    }
  Construct sparse similarity sub-matrix by \eqref{eq:gaussian kernel} and \eqref{eq:gaussian kernel};\\
  Calculate  by solve the eigen-problem \eqref{eq:small_eigen_problem};\\
  Obtain spectral embedding  by \eqref{eq:u_in_practice};\\
  Conduct -means on the bottom  eigenvectors of  to obtain final clustering results.
\end{algorithm}


After solving the eigen-problem, the  bottom eigenvectors in  are stacked to form a low dimensional embedding .
In practice, we normalized  by its 1-norm as , then apply -means clustering on  to obtain the final clustering results \cite{ng2002spectral}.
The -means clustering is then performed on this embedding to obtain the  clusters as the final clustering result with  computational complexity.
We summary the divided-and-conquer based large-scale spectral clustering in Algorithm \ref{ag: divided-and-conquer based large-scale spectral clustering}. 
\section{Discussion}

\subsection{Computational Complexity Analysis}
\label{sec:dnc-sc_complexity}
In this section, we summary the computational cost of the proposed method in each phase.

The divide-and-conquer based landmark selection takes  time. The similarity construction takes  time. The eigen-decomposition takes  time. The -means discretization takes  time. With consideration to , the overall computational complexity of DnC-SC is , where  is the dominant term. Table~\ref{table:cmp_complexity} provides a comparison of computational complexity of our DnC-SC algorithm against several other large-scale spectral clustering algorithms.
The space complexity of DnC-SC is .

\begin{table}\centering
  \caption{Comparison of the computational complexity of several large-scale spectral clustering methods.}
  \label{table:cmp_complexity}
  \begin{threeparttable}
    \begin{tabular}{p{1.4cm}<{\centering}p{1.65cm}<{\centering}p{1.6cm}<{\centering}p{2.435cm}<{\centering}}
      \toprule
      Method                                 & landmark selection & Similarity construction  & Eigen-decomposition \\
      \midrule
Nystr\"{o}m  & /                        &                &          \\
      LSC-R  & /                        &                &        \\
      LSC-K &                 &                &        \\
      U-SPEC           &                &  &     \\
      DnC-SC                                 &                  &                &     \\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \item[*] The final -means is  for each method.
    \end{tablenotes}
  \end{threeparttable}
\end{table}


\subsection{Relations with Other Methods}
As a large-scale spectral clustering method, the proposed method is closely related to the methods in \cite{cai2014large, huang2019ultra}. 
We compare the proposed method with the two methods to discuss the improvements of the proposed method. 

Firstly, we compare them on the landmark selection methods.
Both the two methods \cite{cai2014large, huang2019ultra} directly or indirectly apply -means based landmark selection.
LSC-K method \cite{cai2014large} directly conduct -means algorithm to select landmarks within a high computational complexity .
While the U-SPEC \cite{huang2019ultra} indirectly conduct -means algorithm on a random set of samples to select landmarks, which finds a balance between -means and random selection within  time cost. 
Despite U-SPEC can efficiently find landmarks, it also has two limitations:
1) The quality of landmarks highly depends on how good the random set of samples is set up;
2) Since landmarks are not the centers for all data points, the center's nature of landmark can not be used to approximate the similarity matrix.
The proposed method uses the divide-and-conquer based landmark selection, which can effectively produce high-quality landmarks. 
We design a objection function \eqref{eq: opt} is to find the landmarks that best represent all data points with minimum RSS.
We then propose a divide-and-conquer strategy to divide \eqref{eq: opt} into local sub-problems and use light--means to effectively solve them.
Finally, we combine all sub-problems and obtain landmarks.
The our landmark selection produces landmarks within  computational time.
Moreover, the our landmarks are essentially the centers of subsets for all data points, which can be used to approximate the similarity matrix next.

Secondly, we compare them on the similarity construction.
LSC-K needs to cost  to compute the dense similarity matrix at first to conduct the -nearest neighbor sparse.
The U-SPEC method indirectly computes the sparse similarity sub-matrix in a coarse-to-fine mechanism to approximate the -nearest landmarks within  time cost.
U-SPEC first cluster  landmarks into  clusters and then compute the distances between data points and  cluster centers to find the possible range of nearest landmarks.
For the proposed method, since the landmarks essentially are the cluster centers of data points, we can easily identify a highly possible range of -nearest landmarks according to the centers' nature of landmarks and find -nearest landmarks in this range. 
The proposed -nearest landmarks search method costs  computational time.

Overall, DnC-SC consists of divide-and-conquer based landmark selection, approximate similarity construction, and bipartite graph partition.
It conducts spectral clustering tasks within  computational complexity and  space complexity, which is faster than most existing large-scale spectral clustering methods.  
\section{Experiments}
\label{sec:experiment}

In this section, we conduct experiments on five real and five synthetic datasets to evaluate the performance of the proposed DnC-SC methods.
The comparison experiments against several state-of-the-art spectral clustering methods show better performance on clustering quality and efficiency for DnC-SC methods.
Besides that, the analysis of the parameters is performed.
For each experiment, the test method is repeated 20 times, and the average performance is reported.
All experiments are conducted in Matlab R2020a on a Mac Pro with 3 GHz 8-Core Intel Xeon E5 and 16 GB of RAM.

\subsection{Datasets and Evaluation Measures}

\begin{table}[!t]
  \centering
  \caption{Properties of the real and synthetic datasets.}
  \label{table:datasets}
  \begin{center}
    \begin{tabular}{p{1.2cm}<{\centering}|p{1.3cm}<{\centering}|p{1.5cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}}
      \toprule
      \multicolumn{2}{c|}{Dataset}         &\#Object     &\#Dimension      &\#Class\\
      \midrule
                                        & \emph{USPS}      & 9298    & 256 & 10 \\
      \multirow{5}{*}{\emph{Real}}      & \emph{PenDigits} & 10,992    & 16  & 10 \\
                                        & \emph{Letters}   & 20,000    & 16  & 26 \\
                                        & \emph{MNIST}     & 70,000    & 784 & 10 \\
                                        & \emph{Covertype} & 581,012   & 54  & 7  \\
      \midrule
      \multirow{5}{*}{\emph{Synthetic}} & \emph{TS-60K}    & 600,000   & 2   & 3  \\
                                        & \emph{TM-1M}     & 1,000,000 & 2   & 2  \\
                                        & \emph{TC-6M}     & 6,000,000 & 2   & 3  \\
                                        & \emph{CG-10M}    & 10,000,000 & 2   & 11  \\
                                        & \emph{FL-20M}    & 20,000,000 & 2   & 13  \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}


\begin{figure}\begin{center}
    {\subfigure[\emph{TS-60K} ()]
      {\includegraphics[width=0.31\columnwidth]{figs/data_TM_60K.pdf}}}
    {\subfigure[\emph{TM-1M} ()]
      {\includegraphics[width=0.31\columnwidth]{figs/data_TM_1M.pdf}}}
    {\subfigure[\emph{TC-6M} ()]
      {\includegraphics[width=0.31\columnwidth]{figs/data_threecircle.pdf}}}
    {\subfigure[\emph{CG-10M} ()]
        {\includegraphics[width=0.31\columnwidth]{figs/data_CG10M.pdf}}}
    {\subfigure[\emph{FL-20M} ()]
        {\includegraphics[width=0.31\columnwidth]{figs/Flower20M.pdf}}}
    \caption{Illustration of the five synthetic datasets. Note that only a  or  samples of each dataset is plotted.}
    \label{fig:fiveSynDS}
  \end{center}
\end{figure}

Our experiments are conducted on eight large-scale datasets, varying from nine thousand to as large as twenty million data points. Specifically, the five real datasets are \emph{USPS} \cite{cai2010graph} \footnote{\label{cai_deng_data} http://www.cad.zju.edu.cn/home/dengcai/Data/MLData.html}, \emph{PenDigits} \cite{asuncion2007uci} \footnote{https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits}, \emph{Letters} \cite{frey1991letter} \footnote{https://archive.ics.uci.edu/ml/datasets/Letter+Recognition}, \emph{MNIST} \cite{cai2011speed} \footref{cai_deng_data}, and \emph{Covertype} \cite{blackard1999comparative} \footnote{https://archive.ics.uci.edu/ml/datasets/covertype}. The five synthetic datasets are \emph{Two Spiral-60K} (\emph{TS-60K}), \emph{Two
  Moons-1M} (\emph{TM-1M}), \emph{Three Circles-6M} (\emph{TC-6M}), \emph{Cirecles and Gaussians-10M} (\emph{CG-10M}) \cite{huang2019ultra} \footnote{\label{huang}https://www.researchgate.net/publication/330760669}, \emph{Flower-20M} (\emph{FL-20M}) \cite{huang2019ultra} \footref{huang}. Figure ~\ref{fig:fiveSynDS} shows the synthetic datasets.
The properties of the datasets are summarized in Table~\ref{table:datasets}.

We adopt two widely used evaluation metrics, i.e., Normalized Mutual Information (NMI) \cite{slonim2000agglomerative} and Accuracy (ACC) \cite{yan2009fast}, to evaluate the clustering results.
Let  be the data matrix. For each data point , denote  and  as the cluster label of ground truth and obtained cluster label from clustering methods, respectively. The ACC is defined as:

where  is the number of data  and  is a function to check  and  are equal or not,  returning 1 if equals otherwise returning 0. The map is a best mapping function that maps each predicted label to the most possibly true cluster label by permuting operations \cite{xu2003document}.

Let  denote a set of clusters of ground truth and  obtained from clustering methods. Mutual information (MI) is defined as

where  and  are marginal probabilities that a sample happens to belong to cluster  or  while  is the joint probabilities that a sample happens to belong to cluster both  and .
The NMI is the normalization of MI by the joint entropy as follow:


A better clustering result will provide a larger value of NMI/ACC. Both NMI and ACC are in the range of .


\subsection{Baseline Methods and Experimental Settings}

In this experiments, we compare the proposed method with two baseline clustering methods, which are -means clustering and spectral clustering (SC) \cite{chen2010parallel} , as well as six state-of-the-art large-scale spectral clustering methods. The compared spectral clustering methods are listed as follows:

\begin{enumerate}
  \item \textbf{SC} \cite{chen2010parallel}: original spectral clustering \footnote{\label{psc}http://alumni.cs.ucsb.edu/~wychen/sc.html}.
  \item \textbf{Nystr\"{o}m} \cite{fowlkes2004spectral}: Nystr\"{o}m spectral clustering \footref{psc}.
  \item \textbf{LSC-K} \cite{cai2014large}: landmark based spectral clustering using -means based landmark selection \footnote{\label{LSC}http://www.cad.zju.edu.cn/home/dengcai/Data/Clustering.html}.
  \item \textbf{LSC-R} \cite{cai2014large}: landmark based spectral clustering using random landmark selection \footref{LSC}.
  \item \textbf{LSC-KH} \cite{ye2018large}: Landmark-based spectral clustering using -means partition to find the hubs as the landmarks \footnote{\label{mycode}https://github.com/Li-Hongmin/MyPaperWithCode}.
  \item \textbf{LSC-RH} \cite{ye2018large}: Landmark-based spectral clustering using random partition to find the hubs as the landmarks \footref{mycode}.
  \item \textbf{U-SPEC} \cite{huang2019ultra}: Ultra-Scalable Spectral Clustering \footref{huang}.
\end{enumerate}

There are several common parameters among the methods mentioned above. We set these parameters as follow:
\begin{itemize}
  \item We set the number of landmarks or landmarks as  for DnC-SC, U-SPEC, Nystr\"{o}m, LSC-K, and LSC-R methods. The parameter analysis on  will be further conducted in Section~\ref{sec:para_p}.
  \item We set the  for the number of nearest neighbor for DnC-SC, U-SPEC, LSC-K, and LSC-R.
        The parameter analysis on  will be further conducted in Section~\ref{sec:para_K}.
  \item The DnC-SC method has a unique parameter . In the experiments,  is used for the datasets whose size is less than 100,000, otherwise .
\end{itemize}


\begin{table*}[]
  \centering
  \caption{Clustering performance (ACC\%  std) for large-scale spectral clustering methods}
  \label{table:compare_spectrals_acc}
  \resizebox{0.95\textwidth}{!}{\begin{threeparttable}
    \begin{tabular}{@{}c||c||ccccccc|c@{}}
      \toprule
      Dataset          & KM                    & SC                 & Nystr{\"{o}}m             & LSC-K                       & LSC-R               & LSC-KH              & LSC-RH              & U-SPEC              & DnC-SC             \\
      \midrule
      \emph{USPS}      & 67.01    & 73.21 & 69.47  & 74.02          & 73.90  & 73.66  & 73.89  & 80.79  & \textbf{82.55} \\
      \emph{PenDigits} & 64.40    & 67.23 & 72.46  & 82.30          & 81.55  & 82.17  & 81.55  & 81.74  & \textbf{82.27} \\
      \emph{Letters}   & 25.56    & 31.21 & 31.30  & 33.20          & 32.34  & 31.13  & 31.60   & 33.20  & \textbf{33.54} \\
      \emph{MINST}     & 56.60    & N/A                & 57.02  & \textbf{80.96} & 62.00  & 66.59  & 67.60   & 72.00  & 74.24 \\
      \emph{Covertype} & 24.04    & N/A                & 21.65  & \textbf{24.71} & 23.62  & N/A                 & N/A                 & 24.40  & 23.48 \\
      \emph{TS-60K}    & 56.96    & N/A                & 55.94 & 70.37          & 62.91 & N/A                 & N/A                 & 65.78 & \textbf{81.00} \\
      \emph{TM-1M}     & 75.21    & N/A                & 64.63  & 51.76          & 66.41 & N/A                 & N/A                 & \textbf{99.96}  & \textbf{99.96} \\
      \emph{TC-6M}     & 33.34    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 99.86  & \textbf{99.87} \\
      \emph{CG-10M}     & 60.47    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 66.77   & \textbf{66.83} \\
      \emph{FL-20M}     & 50.07    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 80.17   & \textbf{81.90} \\
      \midrule
      \midrule
      Avg. score       & \multicolumn{1}{c}{-} & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 70.45               & \textbf{72.5}9\\
      \midrule
      \midrule
      Avg. rank        & \multicolumn{1}{c}{-} & 5.80               & 5.10                & 2.80                        & 3.90                & 4.70                & 4.5                 & 2.30                & \textbf{1.50}\\
      \bottomrule
    \end{tabular}\begin{tablenotes}
      \item[*] N/A denotes the case when MATLAB reports the error of out of memory.
    \end{tablenotes}    
    \end{threeparttable}
  }
    
\end{table*}



\begin{table*}[]
  \centering
  \caption{Clustering performance (NMI\%  std) for large-scale spectral clustering methods}
  \label{table:compare_spectrals_nmi}
  \resizebox{0.95\textwidth}{!}{\begin{tabular}{@{}c||c||ccccccc|c@{}}
      \toprule
      Dataset          & KM                    & SC                 & Nystr{\"{o}}m             & LSC-K                       & LSC-R               & LSC-KH              & LSC-RH              & U-SPEC              & DnC-SC                                  \\
      \midrule
      \emph{USPS     } & 61.28    & 77.90 & 65.07  & 81.37          & 76.22  & 76.41  & 76.24  & 81.86  & \textbf{82.86} \\
      \emph{PenDigits} & 67.65    & 71.70 & 65.48  & 80.78          & 79.15  & 80.78  & 79.15  & 81.68  & \textbf{82.01} \\
      \emph{Letters  } & 34.95    & 34.96 & 40.07  & 44.68          & 42.36  & 42.31  & 42.20  & 45.11  & \textbf{45.37} \\
      \emph{MINST    } & 50.90    & N/A                & 49.05  & \textbf{76.81} & 62.53  & 65.08  & 65.14  & 69.15  & 72.00 \\
      \emph{Covertype} & 7.55     & N/A                & 7.98   & \textbf{9.21}           & 8.06   & N/A                 & N/A                 & 8.19   & 8.30  \\
      \emph{TS-60K   } & 22.22    & N/A                & 21.64 & 39.16          & 39.80 & N/A                 & N/A                 & 62.52 & \textbf{73.84} \\
      \emph{TM-1M    } & 19.21    & N/A                & 8.03   & 0.10           & 28.11 & N/A                 & N/A                 & 99.52  & \textbf{99.52} \\
      \emph{TC-6M    } & 34.95    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 99.14  & \textbf{99.15} \\
      \emph{CG-10M    } & 64.94    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 79.98  & \textbf{80.91} \\
      \emph{FL-20M}     & 65.02    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 86.77   & \textbf{87.67} \\
      \midrule
      \midrule
      Avg. score       & \multicolumn{1}{c}{-} & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 71.39               & \textbf{72.39}                          \\
      \midrule
      \midrule
      Avg. rank        & \multicolumn{1}{c}{-} & 5.40               & 5.30                & 3.10                        & 4.20                & 4.50                & 4.70                & 2.00                & \textbf{1.40}                        \\
      \bottomrule
    \end{tabular}}
\end{table*}



\begin{table*}[]
  \centering
  \caption{Time costs(s) of large-scale spectral clustering methods.}
  \label{table:compare_spectrals_time}
  \resizebox{0.95\textwidth}{!}{\begin{tabular}{@{}c||c||ccccccc|c@{}}
      \toprule
      Dataset          & KM                    & SC                 & Nystr{\"{o}}m                & LSC-K                   & LSC-R                      & LSC-KH              & LSC-RH              & U-SPEC              & DnC-SC                                      \\
      \midrule
      \emph{USPS     } & 0.37     & 3.15  & 1.44      & 1.35       & \textbf{0.64} & 0.71   & 0.88   & 3.36   & 1.25            \\
      \emph{PenDigits} & 0.05     & 3.15  & 1.61      & 1.20       & 0.77          & 0.71   & 0.68   & 2.07   & \textbf{0.64}   \\
      \emph{Letters  } & 0.26     & 13.67 & 4.70      & 3.89       & 2.03          & 2.26   & 2.63   & 1.58   & \textbf{0.90}   \\
      \emph{MINST    } & 21.40    & N/A                & 6.54      & 17.29      & 5.80          & 18.04  & 15.38  & 11.96  & \textbf{5.11}   \\
      \emph{Covertype} & 14.02    & N/A                & 571.69  & 354.74    & 41.00        & N/A                 & N/A                 & 15.96  & \textbf{13.15}  \\
      \emph{TS-60K   } & 1.39     & N/A                & 1283.33 & 167.29    & 16.35         & N/A                 & N/A                 & 17.36 & \textbf{4.01}   \\
      \emph{TM-1M    } & 1.12     & N/A                & 3401.61 & 3997.21 & 591.02      & N/A                 & N/A                 & 7.85   & \textbf{6.46}   \\
      \emph{TC-6M    } & 35.23    & N/A                & N/A                    & N/A                     & N/A                        & N/A                 & N/A                 & 30.46  & \textbf{25.05}  \\
      \emph{CG-10M    } & 134.42    & N/A                & N/A                    & N/A                     & N/A                        & N/A                 & N/A                 & 381.72  & \textbf{281.05}  \\
      \emph{FL-20M}     & 311.94    & N/A                & N/A                 & N/A                         & N/A                 & N/A                 & N/A                 & 1530.30   & \textbf{837.38} \\
      \midrule
      \midrule
      Avg. score       & \multicolumn{1}{c}{-} & N/A                & N/A                    & N/A                     & N/A                        & N/A                 & N/A                 & 165.96               & \textbf{117.50}                             \\
      \midrule
      \midrule
      Avg. rank        & \multicolumn{1}{c}{-} & 5.80               & 4.50                   & 4.40                    & 2.60                       & 4.30                & 4.20                & 3.30                & \textbf{1.50 }                            \\
      \bottomrule
    \end{tabular}}
\end{table*}

\subsection{Comparison with Large-scale Spectral Clustering Methods}
\label{sec:cmp_spectral}


In this section, we compare the proposed DnC-SC method with five state-of-the-art spectral clustering methods, as well as the -means clustering and original spectral clustering methods as the baseline methods. 

We report the experimental results in Tables ~\ref{table:compare_spectrals_acc}, ~\ref{table:compare_spectrals_nmi} and ~\ref{table:compare_spectrals_time}, where we use N/A to denote the case when MATLAB reports the error of out of memory.
Only two methods (proposed DnC-SC and U-SPEC) pass all datasets because they can approximately compute the similarity matrix within a limited memory.
The proposed DnC-SC method achieves the best clustering performance of both ACC and NMI eight times on ten benchmark datasets according to Table ~\ref{table:compare_spectrals_acc} and ~\ref{table:compare_spectrals_nmi}.
The proposed DnC-SC method achieves the best efficiency nine times on ten benchmark datasets according to Table ~\ref{table:compare_spectrals_time}.

In addition, we report the average performance score and rank for each method in Tables ~\ref{table:compare_spectrals_acc}, ~\ref{table:compare_spectrals_nmi} and ~\ref{table:compare_spectrals_time}.
The proposed DnC-SC method achieves the best average scores of both ACC and NMI. 
The DnC-SC method shows average ranks of 1.50 of ACC and 1.40 of NMI, which implies the best clustering quality in all spectral clustering methods.
Moreover, the DnC-SC method costs much less average time than the other competitors and achieves a rank of 1.50, which implies the most efficient method in this experiment.
Overall, the proposed DnC-SC method shows significant effectiveness and efficiency comparing with six state-of-the-art large-scale spectral clustering methods. 

\begin{table*}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for different methods by varying number of landmark .}
  \label{table:compare_para_p}
  \begin{threeparttable}
    \begin{tabular}{m{0.08\textwidth}<{\centering}|m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}}
      \toprule
      \emph{Dataset} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_Lettersacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_MINSTacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_60Kacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_1Macc.pdf}\\
      NMI
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_Lettersnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_MINSTnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_60Knmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_1Mnmi.pdf}\\
      Time cost
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_Letterstime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_MINSTtime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_60Ktime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_para_p_TM_1Mtime.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=0.8\textwidth]{figs/legend_p.pdf}}\\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[*] LSC-KH and LSC-RH cannot be conduct on the \emph{TM-60K} and \emph{TM-1M} dataset due to the memory bottleneck.
    \end{tablenotes}
  \end{threeparttable}
\end{table*}

\begin{table*}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for different methods by varying number of nearest landmarks .}
  \label{table:compare_para_Knn}
  \begin{threeparttable}
    \begin{tabular}{m{0.08\textwidth}<{\centering}|m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}}
      \toprule
      \emph{Dataset} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_Lettersacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_MINSTacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_60Kacc.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_1Macc.pdf}\\
      NMI
      &\includegraphics[width=0.18\textwidth]{figs/para_k_Lettersnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_MINSTnmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_60Knmi.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_1Mnmi.pdf}\\
      Time cost
      &\includegraphics[width=0.18\textwidth]{figs/para_k_Letterstime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_MINSTtime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_60Ktime.pdf}
      &\includegraphics[width=0.18\textwidth]{figs/para_k_TM_1Mtime.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=0.6\textwidth]{figs/legend_k.pdf}}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}


\begin{table*}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for different methods by varying number of nearest landmark  and selection rate .}
  \label{table:compare_para_K_u}
  \begin{threeparttable}
    \begin{tabular}{m{0.08\textwidth}<{\centering}|m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}m{0.2\textwidth}<{\centering}}
      \toprule
      \emph{Dataset} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kLetters_acc.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kMINST_acc.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_60K_acc.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_1M_acc.pdf}\\
      NMI
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kLetters_nmi.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kMINST_nmi.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_60K_nmi.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_1M_nmi.pdf}\\
      Time cost
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kLetters_time.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kMINST_time.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_60K_time.pdf}
      &\includegraphics[width=0.2\textwidth]{figs/para_para_u_kTM_1M_time.pdf}\\
\bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}


\begin{table}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for DnC-SC using divide-and-conquer based landmark selection and -means based landmark selection.}
  \label{table:compare_sel_strategies}
  \begin{threeparttable}
    \begin{tabular}{m{0.75cm}<{\centering}|m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}}
      \toprule
      \emph{Data} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_acc_TM_1M.pdf}\\
      NMI
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_nmi_TM_1M.pdf}\\
      Time cost
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_dnc_sc_time_TM_1M.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=7cm]{figs/legend_com_selection.pdf}}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}

\begin{table}\centering
  \caption{Clustering performance (ACC(\%), NMI(\%), and time costs(s)) for DnC-SC using approximate -nearest landmarks and exact -nearest landmarks.}
  \label{table:compare_approxKNN}
  \begin{threeparttable}
    \begin{tabular}{m{0.75cm}<{\centering}|m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}m{1.45cm}<{\centering}}
      \toprule
      \emph{Data} & \emph{Letters} & \emph{MNIST} & \emph{TS-60K} & \emph{TM-1M} \\
      \midrule
      \multirow{1}{*}{ACC}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_acc_TM_1M.pdf}\\
      NMI
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_nmi_TM_1M.pdf}\\
      Time cost
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_Letters.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_MINST.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_TM_60K.pdf}
      &\includegraphics[width=1.7cm]{figs/com_aknn_time_TM_1M.pdf}\\
      &\multicolumn{4}{c}{\includegraphics[width=5cm]{figs/legend_com_k.pdf}}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}

We conduct a series of parameters analysis experiments to demonstrate the performance of the proposed method varying different parameter settings. 
We select four dataset (\emph{Letters}, \emph{MNIST}, \emph{TS-60K}, and \emph{TM-1M}) as benchmark datasets to conduct the following experiments. 

\subsubsection{Number of Landmarks }
\label{sec:para_p}

We first conduct parameter analysis to compare the large-scale spectral clustering methods by varying the number of landmarks  (also called landmarks) and report the experimental results in Table~\ref{table:compare_para_p}.
In general, we can see that a larger value of  brings a better performance of ACC and NMI but cost more time.
The proposed DnC-SC achieves the best ACC and NMI scores on all datasets except the \emph{MNIST}.
On \emph{MNIST} dataset, the proposed DnC-SC method shows the second best ACC and NMI scores after the LSC-K method. 
In terms of time cost, the proposed DnC-SC method shows the best efficiency on all datasets. 
Overall, the proposed DnC-SC method shows significant effectiveness and efficiency in this comparison. 


\subsubsection{Number of Nearest Landmarks }
\label{sec:para_K}

We then conduct parameter analysis to compare the large-scale spectral clustering methods by varying the number of nearest landmark  and report the experimental results in Table~\ref{table:compare_para_Knn}. 
Note that the Nystr{\"{o}}m method does not have the parameter .
Therefore, we do not show the results of the Nystr{\"{o}}m method in this experiment.
According to Table ~\ref{table:compare_para_Knn}, the performance of most methods varies for different  values.
Overall, the proposed DnC-SC show the best effectiveness and efficiency on all dataset. 

\subsubsection{Number of Nearest Landmarks  and selection rate }
\label{sec:para_K_u}

To further demonstrate the proposed method, we evaluate the performances by varying parameters  and  and report the experimental results in Table~\ref{table:compare_para_K_u}.
For proposed DnC-SC methods, the selection rate parameter  directly affects the computational complexity of landmark selection, while the number of nearest landmarks  affects similarity construction, respectively. 
As we can see, a larger  or  generally leads more time cost while not necessarily achieves better performance.
Overall, the proposed method shows considerable robustness with various parameters on ACC and NMI.

\subsection{Influence of Landmark Selection Strategies}
\label{sec:cmpSelStrat}

In this section, we compare the performances between the divide-and-conquer based landmark selection and the -means base landmark selection. The experimental results are reported in Table~\ref{table:compare_sel_strategies}.
As we mentioned, the divide-and-conquer based landmark selection algorithm recursively solves the optimization problems \ref{eq: opt}, which -means methods can also solve.
We have pointed out the lack of efficiency of directly applying -means on large-scale datasets in Section \ref{sec:dnc-sc_complexity}.
Note that the number of maximum iterations of -means in landmark selection is turned as 5, which is the same setting as LSC-K and U-SPEC implementation.
In Table~\ref{table:compare_sel_strategies}, -means based landmark selection algorithm generally shows better ACC and NMI on most datasets except \emph{TM-1M} dataset, while the difference in performance is not significant.
Compared to -means based selection, our divide-and-conquer based landmark selection algorithm strikes a balance between efficiency and effectiveness.
It achieves significantly better efficiency than the -means based selection and yields competitive clustering quality compared to the -means based selection.
\subsection{Influence of Approximated -nearest Landmarks }
\label{sec:cmpApproxKNN}

In this section, we compare the approximated -nearest landmarks and exact -nearest landmarks. 
The experimental results are reported in Table \ref{table:compare_approxKNN}.
The approximated -nearest landmarks approach first finds the possible candidates according to the center's nature of landmarks and then searches the -nearest landmarks among them.
The exact -nearest landmarks approach costs  computational time, while the proposed approximation can reduce the time cost to .
As the Tables~\ref{table:compare_approxKNN} shows, the exact -nearest landmarks approach achieves slightly better ACC and NMI scores than the proposed approximation.
However, the performances between the two methods are not significantly different. 
In term of time cost, the proposed approximation approach shows highly efficient performance compared with the exact -nearest landmarks.
Note that exact -nearest landmarks approach can not be conducted on datasets whose sizes are more than one million due to the high computational cost.
Overall, the proposed approximate -nearest landmark approach show the robustness and efficiency in this experiment. 

\section{Conclusion}
\label{sec:conclusion}
In this paper, we propose a large-scale clustering method, termed divide-and-conquer based spectral clustering (DnC-SC).
In DnC-SC, a divide-and-conquer based landmark selection algorithm is designed to obtain the landmarks effectively.
A new approximate similarity matrix construction approach is proposed to utilize the center's nature of the landmarks to fast construct the similarity matrix between data points and -nearest landmarks.
Finally, the bipartite graph partition is conducted to obtain the final clustering results. 
The experimental results on synthetic and real-world datasets show that the proposed method outperforms other state-of-the-art large-scale spectral clustering methods.

\section*{Acknowledgment}
This study was supported by in part by the New Energy and Industrial Technology Development Organization (NEDO) Grant (ID:18065620) and JST COI-NEXT.

\bibliographystyle{cas-model2-names}

\bibliography{main.bib}



\bio{figs/li.jpg}
Hongmin Li is currently working toward a Ph.D. degree at the Department of Computer Science, University of Tsukuba, Japan. He received his MS degree in computer science from the University of Tsukuba, Japan. His current research interests include clustering, machine learning, and its application fields.
\endbio

\bio{figs/ye.jpg}
Xiucai Ye received the Ph.D. degree in computer science from the University of Tsukuba, Tsukuba, Japan, in 2014. She is currently an Assistant Professor with the Department of Computer Science, and Center for Artificial Intelligence Research (C-AIR), University of Tsukuba. Her current research interests include feature selection, clustering, bioinformatics, machine learning and its application fields. She is a member of IEEE.
\endbio

\bio{figs/imakura.pdf}
Akira Imakura is an Associate Professor at Faculty of Engineering, Information and Systems, University of Tsukuba, Japan. He received Ph.D. (2011) from Nagoya University, Japan. He was appointed as Japan Society for the Promotion of Science Research Fellowship for Doctor Course Student (DC2) from 2010 to 2011, as a Research Fellow at Center for Computational Sciences, University of Tsukuba, Japan from 2011 to 2013, and also as a JST ACTI researcher from 2016 to 2019. His current research interests include developments and analysis of highly parallel algorithms for large matrix computations. Recently, he also investigates matrix factorization-based machine learning algorithms. He is a member of JSIAM, IPSJ and SIAM.
\endbio

\bio{figs/sakurai.jpg}
Tetsuya Sakurai is a Professor of Department of Computer Science, and the Director of Center for Artificial Intelligence Research (C-AIR) at the University of Tsukuba. He is also a visiting professor at the Open University of Japan, and a visiting researcher of Advanced Institute of Computational Science at RIKEN. He received a Ph.D. in Computer Engineering from Nagoya University in 1992. His research interests include high performance algorithms for large-scale simulations, data and image analysis, and deep neural network computations. He is a member of the Japan Society for Industrial and Applied Mathematics (JSIAM), the Mathematical Society of Japan (MSJ), Information Processing Society of Japan (IPSJ), Society for Industrial and Applied Mathematics (SIAM).
\endbio
\end{document}
