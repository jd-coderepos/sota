\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{bm}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{color}
\PassOptionsToPackage{hyphens}{url}
\definecolor{mypink}{RGB}{255,135,180}
\definecolor{myyellow}{RGB}{255,200,0}
\definecolor{mygreen}{RGB}{0,100,0}
\definecolor{mygold}{RGB}{255,185,0}
\definecolor{mymaroon}{RGB}{128,0,0}
\definecolor{myaquamarine}{RGB}{120,200,190}
\definecolor{myturquoise}{RGB}{64,224,208}
\definecolor{mymagenta}{RGB}{255,0,255}
\definecolor{myviolet}{RGB}{238,130,238}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{1820} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Weakly Supervised Energy-Based Learning for Action Segmentation}

\author{Jun Li\\
Oregon State University\\
{\tt\small liju2@oregonstate.edu}
\and  
 Peng Lei\thanks{The work was done at the Oregon State University before Peng Lei joined Amazon. \textsuperscript{\dag}The code is available at \textcolor{magenta}{https://github.com/JunLi-Galios/CDFL}.}\\
 Amazon.com Services, Inc.\\
 {\tt\small  leipeng@amazon.com}
 \and 
 Sinisa Todorovic\\
 Oregon State University\\
 {\tt\small sinisa@oregonstate.edu}
 }
\newcommand\model{Constrained Discriminative Forward Loss}
\newcommand\abbrmodel{CDFL}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
   This paper is about labeling video frames with action classes under weak supervision in training, where we have access to a temporal ordering of actions, but their start and end frames in training videos are unknown. Following prior work, we use an HMM grounded on a Gated Recurrent Unit (GRU) for frame labeling. Our key contribution is a new constrained discriminative forward loss (CDFL) that we use for training the HMM and GRU under weak supervision. While prior work typically estimates the loss on a single, inferred video segmentation, our CDFL discriminates between the energy of all valid and invalid frame labelings of a training video. A valid frame labeling satisfies the ground-truth temporal ordering of actions, whereas an invalid one violates the ground truth. We specify an efficient recursive algorithm for computing the CDFL in terms of the logadd function of the segmentation energy. Our evaluation on action segmentation and alignment gives superior results to those of the state of the art on the benchmark Breakfast Action, Hollywood Extended, and 50Salads datasets.\textsuperscript{\dag}\end{abstract}

\section{Introduction}\label{sec:Introduction}
This paper presents an approach to weakly supervised action segmentation by labeling video frames with action classes. Weak supervision means that in training our approach has access only to the temporal ordering of actions, but their ground-truth start and end frames are not provided. This is an important problem with a wide range of applications, since the more common fully supervised action segmentation typically requires expensive manual annotations of action occurrences in every video frame.




Our fundamental challenge is that the set of all possible segmentations of a training video may consist of multiple distinct {\em valid} segmentations that satisfy the provided ground-truth ordering of actions, along with {\em invalid} segmentations that violate the ground truth. It is not clear how to estimate loss (and subsequently train the segmenter) over multiple valid segmentations.  



{\bf Motivation:}  Prior work \cite{huang2016connectionist, richard2017weaklysupervised, richard2017weakly, ding2018weakly, richard2018neuralnetwork} typically uses a temporal model (e.g., deep neural network, or HMM) to infer a {\em single}, valid, optimal video segmentation, and takes this inference result as a pseudo ground truth for estimating the incurred loss. However, a particular training video may exhibit a significant variation (not yet captured by the model along the course of training), which may negatively affect estimation of the pseudo ground truth, such that the inferred action segmentation is significantly different from the true one. In turn, the loss estimated on the incorrect pseudo ground truth may corrupt  training by reducing, instead of maximizing, the discriminative margin between the ground truth and other valid segmentations.
In this paper, we seek to alleviate these issues.


{\bf Contributions:} Prior work shows that a statistical language model is useful for weakly supervised learning and modeling of video sequences  \cite{lin2017ctc, koller2017re, richard2016temporal, richard2018neuralnetwork, chang2019d3tw}. Following \cite{richard2018neuralnetwork}, we also adopt a Hidden Markov Model (HMM) grounded on a Gated Recurrent Unit (GRU) \cite{chung2014empirical} for labeling video frames. The major difference is that we do not generate a unique pseudo ground truth for training. Instead, we efficiently account for all candidate segmentations of a training video when estimating the loss. To this end, we formulate a new Constrained Discriminative Forward Loss (CDFL) as a difference between the energy of valid and invalid candidate video segmentations. In comparison with prior work, the CDFL improves robustness of our training, because minimizing the CDFL amounts to maximizing the discrimination margin between candidate segmentations that satisfy and violate ground truth, whereas prior work solely optimizes a score of the inferred single valid segmentation. Robustness of training is further improved when the CDFL takes into account only hard invalid segmentations whose edge energy is lower than that of valid ones.  Along with the new CDFL formulation, our key contribution is a new recursive algorithm for efficiently estimating the CDFL in terms of the {\em logadd} function of the segmentation energy.



\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{Intro.png}
\caption{{\bf Our weakly supervised training}: 
For a training video, we first estimate candidate segmentation cuts using a Hidden Markov Model (HMM) grounded on a Gated Recurrent Unit (GRU), and then build a fully connected segmentation graph whose paths represent candidate action segmentations (colors mark different action classes along the paths).  Then, we efficiently compute the Constrained Discriminative Forward Loss (CDFL) in terms of accumulated energy of all valid and invalid paths in the graph for our end-to-end training. (best seen in color)}
\label{fig:Introduction}
\vspace{-4mm}
\end{figure}


{\bf Our Approach:} Fig.~\ref{fig:Introduction} shows an overview of our weakly supervised training of the HMM with GRU that consists of two steps. In the first step, we run a constrained Viterbi algorithm for HMM inference on a given training video so the resulting segmentation is valid. This initial video segmentation is used for efficiently building a fully connected segmentation graph aimed at representing alternative candidate segmentations. In this graph, nodes represent segmentation cuts of the initially inferred segmentation -- i.e., video frames where one action ends and a subsequent one starts -- and edges represent video segments between every two temporally ordered cuts. For improving action boundary detection, we further augment the initial set of nodes with video frames that are in a vicinity of every cut, as well as the initial set of edges with corresponding temporal links between the added nodes. Directed paths of such a fully connected graph explicitly represent many candidate action segmentations, beyond the initial HMM's inference.


The second step of our training efficiently computes a total energy score of frame labeling along {\em all paths} in the segmentation graph. Efficiency comes from our novel recursive estimation of the segmentation energy, where we exploit the accumulative property of the {\em logadd} function. A difference of the accumulated energy of action labeling along the valid and invalid paths is used to compute the CDFL. In this paper, we also consider several other loss formulations expressed in terms of the energy of valid and invalid paths. The loss is then used for training HMM parameters and back-propagated to the GRU for end-to-end training.

For inference on a test video, as in the first step of our training, we use a constrained Viterbi algorithm to perform the HMM inference which will satisfy at least one action sequence seen in training. 
Then, we use this initial video segmentation as an anchor for building the segmentation graph that comprises paths with finer action boundaries. Our output is the MAP path in the graph. 

For evaluation, we consider the tasks of action segmentation and action alignment, where the latter provides additional information on the temporal ordering of actions in the test video.  For both tasks on the Breakfast Action dataset \cite{kuehne2014language}, Hollywood Extended dataset \cite{bojanowski2014weakly}, and 50-Salads dataset \cite{stein2013combining}, we outperform the state of the art.





In the following, Sec.~\ref{sec:Related Work} reviews  related work, Sec.~\ref{sec:HMM} formulates our HMM and Constrained Viterbi for action segmentation, Sec.~\ref{sec:SegmentationGraph} describes how we construct the segmentation graph, Sec.~\ref{sec:CDFL} specifies our CDFL and related loss functions, and Sec.~\ref{sec:Experiment} presents our evaluation.




\section{Related Work}\label{sec:Related Work}
This section reviews closely related work on weakly supervised action segmentation and Graph Transformer Networks. While a review of fully supervised action segmentation \cite{yeung2016end, lea2016segmental, rene2017temporal, lei2018temporal} is beyond our scope, it is worth mentioning that our approach uses the same recurrent deep models for frame labeling as in \cite{singh2016multi,yeung2016end, ding2017tricornet}. Also, our approach is motivated by \cite{kuehne2016end, richard2016temporal} which integrate HMMs  and modeling of action length priors  within a deep learning architecture.











{\bf Weakly supervised action segmentation} has recently made much progress \cite{stein2013combining, kuehne2014language, richard2017weakly, ding2018weakly, richard2018neuralnetwork}.  For example, Extended Connectionist Temporal Classification (ECTC) addresses action alignment under the constraint of being consistent with frame-to-frame visual similarity \cite{huang2016connectionist}. Also,  action segmentation has been addressed with a convex relaxation of discriminative clustering, and efficiently solved with the conditional gradient (Frank-Wolfe) algorithm  \cite{bojanowski2014weakly}. Other approaches use a local action model and a global temporal alignment model that are alternatively trained  \cite{richard2017weaklysupervised, richard2017weakly}. Some methods initially predict a video segmentation with a temporal convolutional network, and then iteratively refine the action boundaries \cite{ding2018weakly}. Other approaches first  generate pseudo-ground-truth labels for all video frames, e.g., with the Viterbi algorithm \cite{richard2018neuralnetwork}, and then train a classifier on these frame labels by minimizing the standard cross entropy loss. Finally, \cite{richard2018actionsets} addresses a different weakly supervised setting from ours when the ground truth provides only a set of actions present without their temporal ordering .

All these approaches base their learning and prediction on estimating a penalty or probability of labeling individual frames. In contrast, we use an energy-based framework with the following differences. First, in training, we minimize the total energy of valid paths in the segmentation graph rather than optimize labeling probabilities of each frame. Second, instead of considering a single optimal valid path in the segmentation graph, we specify a loss function in terms of {\em all} valid paths. Hence, the Viterbi-initialized training on pseudo-labels of frames \cite{richard2018neuralnetwork} represents a special case of our training done only for one valid path. In addition, our loss enforces discriminative training by accounting for invalid paths in the segmentation graph. Unlike \cite{chang2019d3tw} that randomly selects invalid paths, we efficiently account for all hard invalid paths in training.
Finally, our training is not iterative as in \cite{richard2017weaklysupervised, richard2017weakly}, and does not require iterative refinement of action boundaries as in \cite{ding2018weakly}. 


Our \abbrmodel\ extends the loss used for training of the Graph Transformer Network (GTN) \cite{lecun1998gradient, le1997reading, bottou2005graph, collobert2011deep}. To the best of our knowledge, the GTN has been used only for text parsing, and never for action segmentation. In comparison with the GTN training, we significantly reduce complexity by building the video's segmentation graph. Also, while the loss used for training the GTN accounts for both valid and invalid text parses, it cannot handle the special case when valid parses have lower scores than invalid ones. In contrast, our CDFL effectively accounts for the energy of valid and invalid paths, even when valid paths have significantly lower energy than invalid paths in the segmentation graph. 



\section{Our Model for Action Segmentation}\label{sec:HMM}
\noindent
{\bf Problem Setup:} For each training video of length , we are given unsupervised frame-level features, , and the ground-truth ordering of action classes , also referred to as the transcript.   is the length of the annotation sequence, and  is th action class in   that belongs to the set of  action classes, . Note that  and  may vary across the training set, and that there may be more than one occurrences of the same action class spread out in  (but of course ).



In inference, given frame features  of a video,  our goal is to find an optimal segmentation ,  where  is the predicted length of the action sequence, and   includes the predicted number of video frames  occupied by the predicted action .


{\bf The Model:} We use an HMM to model the posterior distribution of a video segmentation  given   as

In \eqref{eq:Markov}, the likelihood  is estimated as

where  is the GRU's softmax score for action  at frame , and the prior distribution of action classes  is an normalized frame frequency of action occurrences in the training dataset. The likelihood of action length is modeled as a class-dependent Poisson distribution 

where  is the mean length for class . Finally, the joint prior  is a constant if the transcript  exists in the training set; otherwise, . The same modeling formulation was well-motivated and used in state of the art \cite{richard2018neuralnetwork}.


{\bf Constrained Viterbi Algorithm:} Given a training video, we first find an optimal valid action segmentation  by maximizing \eqref{eq:Markov} with a constrained Viterbi algorithm, which ensures that  is equal to the annotated transcript, . Similarly, for inference on a test video, we first perform the constrained Viterbi algorithm against all transcripts  seen in training, i.e., ensure that the predicted  has been seen at least once in training. Thus, the initial step of our inference on a training or test video is the same as in \cite{richard2018neuralnetwork}.


Our key difference from \cite{richard2018neuralnetwork}, is that we use the initial  to efficiently build a fully connected segmentation graph of the video, as explained in Sec.~\ref{sec:SegmentationGraph}. 
Importantly, in training, the segmentation graph is not constructed to find a more optimal video segmentation that improves upon the initial prediction. Instead, the graph is used to efficiently account for all valid and invalid segmentations. 

Given a video  and a transcript , the constrained Viterbi algorithm recursively maximizes the posterior in \eqref{eq:Markov} such that the first  action labels of the transcript  are respected at time :

where . We set , and , where  is a constant.


\section{Constructing the Segmentation Graph}\label{sec:SegmentationGraph}

Given a video , we first run the constrained Viterbi algorithm to obtain an initial video segmentation  . For simplicity, in the following, we ignore the symbol . This initial segmentation is characterized by  cuts, , i.e., video frames where previous action ends and the next one starts including the very first frame  and last frame  at time . 

We use these cuts to anchor our construction of the fully connected segmentation graph, , where  is the set of nodes,  is the set of directed edges linking every two temporally ordered nodes, and  are the corresponding edge weights. 

Some of the estimated cuts in  may be false positives or may not exactly coincide with the true cuts. To improve action boundary detection, we augment the initial  with nodes representing neighboring video frames of each cut  within a temporal window of length  centered at , as illustrated in Fig.~\ref{fig:NodeAugment}. For the first and last frames, we set . Thus, each  can be viewed as a hyper-node comprising additional vertices in , , and accordingly additional  edges .  In the following, we simplify notation for vertices , and edges  .

Each edge  is assigned a weight vector  , where   is defined as the energy of labeling the video segment   with action class :

where  is the GRU's softmax score for action  at frame .



\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{NodeAugment.png}
\caption{{\bf Building the segmentation graph}  (best seen in color). The initial nodes of  represent segmentation cuts  obtained in the Constrained Viterbi (the predicted action classes are marked with different colors). Each  generates additional vertices  representing neighboring video frames within a window  centered at  (the black rectangles), and corresponding new edges  (the dashed lines) between all temporally ordered pairs of vertices in . For clarity, we show only a few edges.  has exponential many  paths, each representing a candidate action segmentation.}
\label{fig:NodeAugment}
\end{figure}

 comprises exponentially many directed paths , where each  represents a particular video segmentation. In each , every edge  gets assigned only one action class . Thus, the very same edge with  different class assignments belongs to  distinct paths in . We compute the energy of a path as


A subset of valid paths  satisfies the given transcript. The other paths are invalid, .

In the next section, we explain how to efficiently compute a total energy score of the exponentially many paths in  for estimating our loss in training.




\section{\model}\label{sec:CDFL}

In this paper, we study {\em three} distinct loss functions, defined in terms of a total energy score of paths in . As there are exponentially many paths in , our key contribution is the algorithm for efficiently estimating their total energy.  Below, we specify our three loss functions ordered by their complexity. As we will show in Sec.~\ref{sec:Experiment}, we obtain the best performance when using the CDFL in training.


\subsection{Forward Loss}

We define a forward loss,  , in terms of a total energy of all valid paths using the standard {\em logadd} function as

where energy of a path  is given by \eqref{eq:energy}. As there are exponentially many paths in , we cannot directly compute  as specified in (\ref{eq:ForwardTraining}).  Therefore, we derive a novel recursive algorithm for accumulating the energy scores of edges along multiple paths, as specified below.

We begin by defining the {\em logadd} function as

Note that the {\em logadd} function is commutative and associative, so  it can be defined on  a set  in a recursive manner:

where  is an element in .
Therefore, the forward loss given by \eqref{eq:ForwardTraining} can be expressed as

Below, we simplify notation as .


\begin{algorithm}[t]
\SetAlgoLined
\KwIn{}
\KwOut{Forward loss }
Initialization:  \;
\For{ \KwTo }{
\For{ {\normalfont in the neighborhood of}  }{
\;
\For{  {\normalfont in the neighborhood of} }{
\;
\;
}
}
}
\caption{Computing the Forward loss  . }
\label{alg:ForwardLoss}
\end{algorithm}


We recursively compute the energy score  of a path that ends at node  and covers first  labels of the ground truth    in terms of the {\em logadd} scores  of all valid paths that end at node , , and cover first  labels as

To prove \eqref{eq:ELL}, suppose that

where  is a path that ends at  with a transcript of . Then, we have

where  is a path that ends at  with a transcript of .
For a training video with length  and ground-truth constraint sequence , we define

The recursive algorithm for computing  is presented in Alg.~\ref{alg:ForwardLoss}. It is worth noting that in a special case of Alg.~\ref{alg:ForwardLoss}, when we take only the initial segmentation cuts  as nodes of  (i.e., the window size ), the forward loss is equal to the training loss used in \cite{richard2018neuralnetwork}.



\begin{algorithm}[t]
\SetAlgoLined
\KwIn{}
\KwOut{}
Initialization:  \;
\For{ \KwTo }{
\For{ {\normalfont in the neighborhood of}  }{
\;
\For{  {\normalfont in the neighborhood of} }{
\For{}{
\;
}
}
}
}
\caption{Computing the logadd score of all paths in , for  the discriminative forward loss . }
\label{alg:DF}
\end{algorithm}



\subsection{Discriminative Forward Loss}
We also consider the Discriminative Forward Loss, , which extends  by additionally accounting for invalid paths in :

where  aggregates  a total energy of all paths in , and   is a regularization factor that controls the relative importance of the valid and invalid paths for . Alg.~\ref{alg:DF} summarizes our recursive algorithm for computing  in \eqref{eq:DF}, whereas Alg.~\ref{alg:ForwardLoss} shows how to compute  in \eqref{eq:DF}.  

One advantage of  over  is that minimizing  amounts to  maximizing the decision margin between the valid and invalid paths. However, a potential shortcoming of  is that valid paths might have little effect in \eqref{eq:DF}. In the case, when the energy of valid paths dominates the total energy of all paths, the former gets effectively subtracted in \eqref{eq:DF}, and hence has very little effect on learning. 

Moreover, we observe that in some cases the back-propagation of  is dominated by the invalid paths. This can be clearly seen from the following derivation. We compute the gradient   as

where

From \eqref{eq:gradientDF}--\eqref{eq:c1c2}, we note that in the case of , the backpropagation will be dominated by the invalid paths, whereas there would be no effect for invalid paths in training if . Sec.~\ref{sec:Experiment} presents how different choices of  affect our performance.

In the next section, we define the constrained discriminative forward loss to address this issue.


\begin{algorithm}[t]
\SetAlgoLined
\KwIn{}
\KwOut{ }
Initialization:  \;
\For{ \KwTo }{
\For{ {\normalfont in the neighborhood of}  }{
\;
\For{  {\normalfont in the neighborhood of} }{
\For{}{
\;
\If{}{

}
\;
}
}
}
}
\caption{Computing the logadd score of a subset of invalid paths , for estimating the constrained discriminative forward loss .}
\label{alg:CDF}
\end{algorithm}





\subsection{Constrained Discriminative Forward Loss}

We define the CDFL as

where  consists of a subset of invalid paths in , where each edge  gets assigned an action class  such that its weight , where  is the pseudo ground truth class for . This constraint effectively addresses the aforementioned issue when the valid paths have significantly lower energy than the invalid paths. 
Alg.~\ref{alg:CDF} summarizes our recursive algorithm for computing  in \eqref{eq:CDFLoss}, whereas Alg.~\ref{alg:ForwardLoss} shows how to compute  in \eqref{eq:CDFLoss}. 

As  accounts for the invalid paths,   further accounts for the hard invalid paths. Therefore, the model robustness is further improved by minimizing  which amounts to maximizing the decision margin between the valid and hard invalid paths.

\begin{table}[b]
\begin{center}
\begin{tabular}{|c|c| c| c| c|}
\hline {\bf Breakfast}  &Mof & Mof-bg & IoU & IoD\\
\hline OCDC\cite{bojanowski2014weakly}& 8.9 & - & - & -\\
\hline CTC\cite{huang2016connectionist}& 21.8 & - & - & -\\
\hline HTK \cite{kuehne2016end}& 25.9 & - & 9.8 & -\\
\hline ECTC \cite{huang2016connectionist}& 27.7 & - & - & -\\
\hline HMM/RNN \cite{richard2017weakly}& 33.3 & - & - & -\\
\hline TCFPN \cite{ding2018weakly} & 38.4 & 38.4 & 24.2 & 40.6\\
\hline NN-Viterbi \cite{richard2018neuralnetwork} & 43.0 & - & - & -\\
\hline D3TW \cite{chang2019d3tw} & 45.7 & - & - & -\\
\hline Our \abbrmodel & {\bf 50.2} & {\bf 48.0} & {\bf 33.7} & {\bf 45.4} \\
\hline
\hline {\bf Hollywood Ext}  &Mof & Mof-bg & IoU & IoD\\
\hline HTK \cite{kuehne2016end}& 33.0 & - & 8.6 & -\\
\hline HMM/RNN \cite{richard2017weakly}& - & - & 11.9 & -\\
\hline TCFPN \cite{ding2018weakly} & 28.7 & 34.5 & 12.6 & 18.3\\
\hline D3TW \cite{chang2019d3tw} & 33.6 & - & - & -\\
\hline Our \abbrmodel & {\bf 45.0} & {\bf 40.6} & {\bf 19.5} & {\bf 25.8}\\
\hline
\hline {\bf 50Salads}  &Mof & Mof-bg & IoU & IoD\\
\hline CTC\cite{huang2016connectionist}& 11.9 & - & - & -\\
\hline HTK \cite{kuehne2016end}& 24.7 & - & - & -\\
\hline HMM/RNN \cite{richard2017weakly}& 45.5 & - & - & -\\
\hline NN-Viterbi \cite{richard2018neuralnetwork} & 49.4 & - & - & -\\
\hline Our \abbrmodel & {\bf 54.7} & {\bf 49.8} & {\bf 31.5} & {\bf 40.4} \\
\hline
\end{tabular}
\end{center}
\caption{Action segmentation evaluations on Breakfast, Hollywood Ext and 50Salads. The dash means no results reported by prior work.}
\label{table:result}
\end{table}
\setlength{\tabcolsep}{1.4pt}



\subsection{Our Computational Efficiency}\label{sec:Effectiveness}

As summarized in Alg.~\ref{alg:ForwardLoss}--\ref{alg:CDF}, our training first runs the constrained Viterbi algorithm (see Sec.~\ref{sec:HMM}) to get the initial segmentation cuts with complexity   for a video of length  and the ground-truth action sequence of length . Then, \abbrmodel\ efficiently accumulates the energy of both valid and invalid paths in  with complexity  for the neighborhood window size  and the class set size . Therefore, our total complexity of training is . 

Note that prior work \cite{richard2018neuralnetwork} also runs the Constrained Viterbi with complexity , so relative to theirs our complexity is increased by . This additional complexity is significantly smaller than  as . In our experimental evaluation, we get the best results for   frames, whereas video length  can go to several minutes.





\section{Results}\label{sec:Experiment}

Both action segmentation and alignment are evaluated on the Breakfast Actions \cite{kuehne2014language}, Hollywood Extended \cite{bojanowski2014weakly}, and 50Salads \cite{stein2013combining} datasets. We perform the same cross-validation strategy as the state of the art, and report our average results. 
We call our approach  \abbrmodel, trained with loss given by \eqref{eq:CDFLoss}.

{\bf Datasets.} For all datasets, we use as input the pre-processed, public, unsupervised frame-level features. The same frame features are used by \cite{huang2016connectionist, richard2017weaklysupervised, richard2017weakly, richard2018neuralnetwork}. The features are dense trajectories represented by PCA-projected Fisher vectors \cite{kuehne2016end}. {\em Breakfast} \cite{kuehne2014language} consists of  videos of people making breakfast with  cooking activities. The cooking activities are comprised of  action classes. On average, every video has  action instances, and the video length ranges from a few seconds to several minutes. {\em Hollywood Extended} \cite{bojanowski2014weakly} contains  video clips from different Hollywood movies, showing  action classes. Each clip contains  actions on average. {\em 50Salads} \cite{stein2013combining} has  very long videos showing  classes of human manipulative gestures. On average, each video has  action instances. There are  annotated frames.

{\bf  Evaluation  Metrics.} We use the following four standard metrics, as in \cite{bojanowski2014weakly, ding2018weakly}. The mean-over-frames ({\bf Mof}) is the average percentage of correctly labeled frames. To overcome the potential drawback that frames are dominated by background class, we compute mean-over-frames without background({\bf Mof-bg}) as the average percentage of correctly labeled video frames with background frames removed. The intersection over union ({\bf IoU}) and the intersection over detection ({\bf IoD}) are computed as IoU = , and IoD =  , where  denotes the extent of the ground truth segment and  is the extent of a correctly detected action segment.

{\bf Training.} We train a single-layer GRU with 
 hidden units
in  iterations, where for each iteration one training video is randomly selected. The initial learning rate of 0.01 is decreased to 0.001 at the th iteration. The mean action lengths   in \eqref{eq:Poisson},  and the action priors  in \eqref{eq:framelikelihood} are estimated from the history of pseudo ground truths. Unlike \cite{richard2018neuralnetwork}, we do not use the history of pseudo ground truths for computing loss in the current iteration. Consequently, our training time per iteration is less than that of \cite{richard2018neuralnetwork}. 









\subsection{Action Segmentation}
 Tab.~\ref{table:result} compares \abbrmodel\ with the state of the art. From the table,  \abbrmodel\ achieves the best performance in terms of all the four metrics.
Fig.~\ref{fig:prediction} qualitatively compares the ground truth and \abbrmodel's\ output on an example test video in Breakfast dataset.
As can be seen, \abbrmodel\ typically misses the true start or end of actions by only a few frames. In general, \abbrmodel\ successfully detects most action occurrences.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{Fig3.pdf}
\caption{Ground truth action sequence (\textcolor{red}{take\_cup}, \textcolor{blue}{spoon\_powder}, \textcolor{mypink}{pour\_milk}, \textcolor{myyellow}{stir\_milk}) (top) and our CDFL's action segmentation (bottom) on the sample test video  from Breakfast dataset. The background frames are marked in white. \abbrmodel\ may miss the true start and end of some actions, but successfully detects the actions.}
\label{fig:prediction}
\end{figure}



\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\cline{1-7}
Window & \multicolumn{2}{c|}{}&\multicolumn{2}{c|}{}&\multicolumn{2}{c|}{}\\
\cline{2-7}
Size & Mof & IoD & Mof & IoD & Mof & IoD \\
\hline 
30 & 43.5 & 39.4 & 46.6 & 40.5 & 49.4 & 44.1 \\
\hline 
20 & 44.3 & 40.9 & 47.0 & 41.8 & {\bf 50.2} & {\bf 45.4} \\
\hline 
10 & 43.8 & 40.0 & 46.2 & 41.3 & 49.6 & 44.6\\
\hline 
0 & 43.0 & 38.7 & 45.0 & 40.2 & 48.5 & 43.5\\
\hline
\end{tabular}
\end{center}
\caption{Mof and IoD evaluations on Breakfast for different neighborhood window sizes and different losses. CDFL with neighbor-window size of 20 shows the best result.}
\label{table:Different Training Strategies}
\end{table}


{\bf  Ablation Study for Action Segmentation.} Tab.~\ref{table:Different Training Strategies} compares our action segmentation performance on Breakfast when using different sizes of the neighborhood window placed around the initial segmentation cuts (as explained in Sec.~\ref{sec:SegmentationGraph}) and different loss functions (as specified in Sec.~\ref{sec:CDFL}). From the table, training by accounting for invalid paths in  and  gives better performance than only accounting for valid paths in . In addition, considering neighboring frames for action boundary refinement within a window around the initial segmentation cuts gives better performance than taking into account only a single optimal path in the segmentation graph when the window size is . The best test performance is achieved using  with window size of  in training.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{Fig4.pdf}
\caption{Top-down, the rows
correspond to ground truth sequence of actions (\textcolor{mygold}{pour\_oil}, \textcolor{mymaroon}{crack\_egg}, \textcolor{mygreen}{fry\_egg}, \textcolor{myaquamarine}{put\_egg2plate}) and our action segmentations with neighbor-window size of  on the sample video \textit{P03\_cam01\_P03\_friedegg} from Breakfast dataset using ,  and , respectively. The background frames are marked in white. The result for  is the best.}
\label{fig:diff_strategies}
\end{figure}


\begin{table}[t]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline  window size &  &  &  & \\
\hline 30 & 43.5& 46.6 & 38.8 & 34.0\\
\hline 20 & 44.3& 47.0 & 40.7 & 35.5\\
\hline 10 & 43.8& 46.2 & 41.0 & 35.4\\
\hline 0 & 43.0& 45.0 & 39.1 & 33.5\\
\hline
\end{tabular}
\end{center}
\caption{Mof evaluations on Breakfast using  in training with different regularization factors and neighbor-window sizes.}
\label{table:Different regularization}
\end{table}


\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{Fig5.pdf}
\caption{Ground truth action sequence (\textcolor{mygold}{pour\_oil}, \textcolor{mymaroon}{crack\_egg}, \textcolor{mygreen}{fry\_egg}, \textcolor{myviolet}{take\_plate}, \textcolor{myaquamarine}{put\_egg2plate}) (top) and CDFL's action segmentations using different neighbor-window sizes on the sample test video \textit{P04\_webcam02\_P04\_friedegg} from Breakfast. The background frames are marked in white. The window size of  gives the best performance.}
\label{fig:diff_win}
\end{figure}

Fig.~\ref{fig:diff_win} illustrates the CDFL's action segmentations on a sample test video from the Breakfast Action dataset using different window sizes and . As can be seen, considering neighboring frames around the anchor segmentation improves performance. 

Tab.~\ref{table:Different regularization} shows how different regularization factors   in  affect our action segmentation on the Breakfast Action dataset, for different neighbor-window sizes. As expected, using small  in training tends to give better performance. The best accuracy is achieved with  and window size of .

\begin{table}[!tp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline  {\bf Breakfast} & Mof & Mof-bg & IoU & IoD\\
\hline ECTC\cite{huang2016connectionist}& 35.0 & - & - & 45.0\\
\hline HTK \cite{kuehne2016end}& 43.9 & - & 26.6 & 42.6 \\
\hline OCDC \cite{bojanowski2014weakly}& - & - & - & 23.4\\
\hline HMM/RNN \cite{richard2017weakly}& - & - & - & 47.3\\
\hline TCFPN \cite{ding2018weakly}& 53.5 & 51.7 & 35.3 & 52.3 \\
\hline D3TW \cite{chang2019d3tw} & 57.0 & - & - & 56.3\\
\hline Our \abbrmodel & {\bf 63.0} & {\bf 61.4} & {\bf 45.8} & {\bf 63.9}\\
\hline
\hline  {\bf Hollywood Ext} & Mof & Mof-bg & IoU & IoD\\
\hline ECTC\cite{huang2016connectionist}& - & - & - & 41.0\\
\hline HTK \cite{kuehne2016end}& 49.4 & - & 29.1 & 46.9 \\
\hline OCDC \cite{bojanowski2014weakly}& - & - & - & 43.9\\
\hline HMM/RNN \cite{richard2017weakly}& - & - & - & 46.3\\
\hline TCFPN \cite{ding2018weakly}& 57.4 & 36.1 & 22.3 & 39.6 \\
\hline NN-Viterbi \cite{richard2018neuralnetwork}& - & - & - & 48.7\\
\hline D3TW \cite{chang2019d3tw} & 59.4 & - & - & 50.9\\
\hline Our \abbrmodel & {\bf 64.3} & {\bf 70.8} & {\bf 40.5} & {\bf 52.9}\\
\hline
\hline {\bf 50Salads} & Mof & Mof-bg & IoU & IoD\\
\hline Our \abbrmodel & {\bf 68.0} & {\bf 65.3} & {\bf 45.5} & {\bf 58.7}\\
\hline
\end{tabular}
\end{center}
\caption{Action alignment evaluations on Breakfast, Hollywood Ext and 50Salads. The dash indicates that no results reported by prior work.}
\label{table:alignment_result}
\end{table}
\setlength{\tabcolsep}{1.4pt}

\subsection{Action Alignment}

Tab.~\ref{table:alignment_result} shows that \abbrmodel \ outperforms the state-of-the-art approaches in action alignment on the three benchmark datasets. Fig.~\ref{fig:alignement} illustrates that \abbrmodel \ is good at action alignment on a sample test video from Hollywood Extended.


{\bf  Ablation Study for Action Alignment.} Tab.~\ref{table:Different Training Strategies on alignment} presents our alignment results using different loss functions as specified in Sec.~\ref{sec:CDFL}, and different neighbor-window sizes on Hollywood Ext. From the table, training with  and  that account for invalid paths, outperforms our approach trained with . In addition, taking into account neighboring frames around segmentation cuts of the initial segmentation (i.e., window size is greater than ) improves performance relative to the case when window size is . The best performance is achieved using  with the window sizes of  in training. 

Fig.~\ref{fig:diff_win_align} illustrates that CDFL gives good action alignment results on the sample test video from Hollywood Ext, using  and window size of  in training.

\begin{figure}[!tp]
\centering
\includegraphics[width=1.0\linewidth]{Fig6.pdf}
\caption{Ground truth action sequence (\textcolor{mypink}{StandUp},\textcolor{blue}{SitDown}, \textcolor{red}{DriveCar}, \textcolor{mygreen}{OpenDoor}, \textcolor{mygreen}{OpenDoor}, \textcolor{myyellow}{HugPerson}) (top) and our action alignments (bottom) on the sample video  from Hollywood Extend. The background frames are marked in white. \abbrmodel\ typically achieves a good action alignment.}
\label{fig:alignement}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline  Window size &  & & \\
\hline 8 & 48.7 & 49.8 & 51.6\\
\hline 6 & 49.3 & 50.5 & \\
\hline 4 & 49.0 & 50.0 & 52.0\\
\hline 2 & 48.5 & 49.5 & 50.7\\
\hline 0 & 48.7 & 49.3 & 49.8\\
\hline
\end{tabular}
\end{center}
\caption{IoD evaluations of our approach in action alignment on Hollywood Extended using different loss functions and different neighbor-window sizes in training. Using \abbrmodel{} with neighbor-window size of  shows the best result.}
\label{table:Different Training Strategies on alignment}
\end{table}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{Fig7.pdf}
\caption{Ground truth action sequence (\textcolor{myturquoise}{OpenDoor}, \textcolor{myturquoise}{OpenDoor}, \textcolor{mymagenta}{OpenCarDoor}) (top) and CDFL's action alignments on the sample test video \textit{0361} from Hollywood Extended, when trained using varying window sizes. The background frames are marked in white. Using \abbrmodel{} and neighbor-window size of  gives the best results. }
\label{fig:diff_win_align}
\vspace{-2mm}
\end{figure}


\section{Conclusion}\label{sec:Conclusion}
We have extended the existing work on weakly supervised action segmentation that uses an HMM and GRU for labeling video frames by formulating a new energy-based learning on a video's segmentation graph. The graph is constructed so as to facilitate computation of loss, expressed in terms of the energy of valid and invalid paths representing candidate action segmentations. Our key contribution is the new recursive algorithm for efficiently computing the accumulated energy of exponentially many paths in the segmentation graph. Among the three loss functions that we have defined, 
and evaluated,
the CDFL -- specified to maximize the discrimination margin between valid and high-scoring invalid paths -- gives the best performance. A comparison with the state of the art on both action segmentation and action alignment tasks, for the Breakfast Action, Hollywood Extended and 50Salads datasets, supports our novelty claim that using our CDFL in training gives superior results than a loss function estimated on a single inferred segmentation, as done by prior work. Our results on both action segmentation and action alignment tasks also demonstrate advantages of considering many candidate segmentations in neighbor-windows around the initial video segmentation, and maximizing the margin between all valid and hard invalid segmentations. Our small increase in complexity relative to that of related work seems justified considering our significant performance improvements.

\noindent{\bf{Acknowledgement}}. This work was supported in part by DARPA XAI Award N66001-17-2-4029 and AFRL STTR AF18B-T002.










{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
