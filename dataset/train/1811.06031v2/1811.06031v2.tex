\def\year{2019}\relax
\documentclass[letterpaper]{article} \usepackage{aaai19}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{graphicx}  \usepackage{todonotes}
\frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  

\usepackage{amsfonts} 


\usepackage{tabularx}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}


\pdfinfo{
/Title (A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks)
/Author (Victor Sanh, Thomas Wolf, Sebastian Ruder)
/Keyword (Multi-task Learning, Hierarchical Model, Natural Language Processing)}
\setcounter{secnumdepth}{0}  
 \begin{document}

\title{A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks}
\author{Victor Sanh\textsuperscript{1}, Thomas Wolf\textsuperscript{1}, Sebastian Ruder\textsuperscript{2,3}\\
\textsuperscript{1}{Hugging Face, 20 Jay Street, Brooklyn, New York, United States}\\
\textsuperscript{2}{Insight Research Centre, National University of Ireland, Galway, Ireland}\\
\textsuperscript{3}{Aylien Ltd., 2 Harmony Court, Harmony Row, Dublin, Ireland}\\
\texttt{\{victor, thomas\}@huggingface.co}, \texttt{sebastian@ruder.io}
}


\maketitle

\begin{abstract}
    Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.
\end{abstract}



\section{Introduction}

Recent Natural Language Processing (NLP) models heavily rely on rich distributed representations (typically word or sentence embeddings) to achieve good performance. One example are  so-called ``universal representations'' \cite{Conneau17} which are expected to encode a varied set of linguistic features, transferable to many NLP tasks. This kind of rich word or sentence embeddings can be learned by leveraging the training signal from different tasks in a multi-task setting. It is known that a model trained in a multi-task framework can take advantage of inductive transfer between the tasks, achieving a better generalization performance \cite{Caruana1993}. Recent works in sentence embeddings \cite{Subramanian2018,Jernite2017} indicate that complementary aspects of the sentence (e.g. syntax, sentence length, word order) should be encoded in order for the model to produce sentence embeddings that are able to generalize over a wide range of tasks. Complementary aspects in representations can be naturally encoded by training a model on a set of diverse tasks, such as, machine translation, sentiment classification or natural language inference. Although, (i) the selection of this diverse set of tasks, as well as, (ii) the control of the interactions between them are of great importance, a deeper understanding of (i) and (ii) is missing as highlighted in the literature~\cite{Caruana1997,Mitchell80theneed,Ruder2017}.
This work explores this line of research by combining, in a single model, four fundamental semantic NLP tasks: Named Entity Recognition, Entity Mention Detection (also sometimes referred as mention detection), Coreference Resolution and Relation Extraction. This selection of tasks is motivated by the inter-dependencies these tasks share. In Table~\ref{tab:motivating_examples}, we give three simple examples to exemplify the reasons why these tasks should benefit from each other. For instance, in the last example knowing that \textit{the company} and \textit{Dell} are referring to the same real world entity, \textit{Dell} is more likely to be an organization than a person. 

\begin{table}
    \caption{A few examples motivating our selection of tasks. \newline Abbreviations: CR: Coreference Resolution, RE: Relation Extraction, EMD: Entity Mention Detection, NER: Named Entity Recognition, X  Y: X is more likely to be Y.}
    \label{tab:motivating_examples}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{L{0.43\columnwidth}L{0.35\columnwidth}L{0.41\columnwidth}}
        \hline
        Example & Predictions on one task... & \hspace{0pt}...can help disambiguate other tasks \\
        \hline
        \hline
        \textit{X works for Y} & RE: \{\textit{work, X, Y}\} & \textit{X}  Person (EMD)\newline \textit{Y}   Organization or Person (NER)\\
        \hline
        \textit{I love Melbourne. I've lived three years in this city.} & CR: (\textit{Melbourne, this city}) \newline RE: \{\textit{live, I, this city}\} & \textit{Melbourne}  Location (EMD/NER)\\
        \hline
        \textit{Dell announced a \\rightsquigarroww_ts = (w_1, w_2, ..., w_n)g_{e}g_{e}g_{ner}g_{ner}[]_{PERS}[]_{VEH}[]_{VEH}[]_{GPE}[g_{e}, g_{ner}]g_{emd}[g_{e}, g_{emd}]g_{cr}[g_{e}, g_{emd}]g_{re}g_ig_j\mathbb{R}^{l}U \in \mathbb{R}^{d \times l}W \in \mathbb{R}^{d \times l}b \in \mathbb{R}^{d}V \in \mathbb{R}^{r \times d}ldr\phip = \sigma (t(w_i, w_j)) \in \mathbb{R}^{r}p_k1 \leq k \leq rw_iw_jkVUWbF_{1}F_{1}\phi_4F_{1}PRF_{1}PRF_{1}PRF_{1}F_{1}F_{1}\simF_{1}\sim\sim\simF_{1}\simF_{1}F_{1}F_{1}F_{1}F_{1}g_{e}\simF_{1}F_{1}F_{1}F_{1}F_{1}g_{e}LLg_{e}g_{e}g_{emb}g_{emb}g_{ner}g_{emd}g_{re}g_{cr}F_{1}F_{1}\Delta\Delta$\\
        \hline
        (B) & NER & -16\% & -0.02 \\
        (C) & EMD & -44\% & +1.14 \\
        (D) & RE & +78\% & +6.76\\
        (E-GM) & Coref-GM & -28\% & +0.91\\
        \hline
    \end{tabular}}
\end{table}



\section{Related work}

Our work is most related to \citeauthor{Hashimoto2017} \shortcite{Hashimoto2017} who develop a joint hierarchical model trained on syntactic and semantic tasks. The top layers of this model are supervised by semantic relatedness and textual entailment between two input sentences, implying that the lower layers need to be run two times on different input sentences. Our choice of tasks avoids such a setup. Our work also adopts a different approach to multi-task training (proportional sampling) therefore avoiding the use of complex regularization schemes to prevent catastrophic forgetting. Our results show that strong performances can be reached without these ingredients. In addition, the tasks examined in this work are more focused on learning semantic representations, thereby reducing the need to learn surface and syntactic information, as evidenced by the linguistic probing tasks.

Unlike \cite{Hashimoto2017} and other previous work \cite{Katiyar2017,Bekoulis2018,Augenstein2018}, we do not learn label embeddings, meaning that the (supervised) output/prediction of a layer is not directly fed to the following layer through an embedding learned during training. Nonetheless, sharing embeddings and stacking hierarchical encoders allows us to share the supervision from each task along the full structure of our model and achieve state-of-the-art performance.

Unlike some studies on multi-task learning such as \cite{Subramanian2018}, each task has its own contextualized encoder (multi-layer BiLSTM) rather than a shared one, which we found to improve the performance. 

Our work is also related to \citeauthor{Sogaard2016} \shortcite{Sogaard2016} who propose to cast a cascade architecture into a multi-task learning framework. However, this work was focused on syntactic tasks and concluded that adding a semantic task like NER to a set of syntactic tasks does not bring any improvement. This confirms the intuition that multi-task learning is mostly effective when tasks are related and that syntactic and semantic tasks may be too distant to take advantage of shared representations. The authors used a linear classifier with a softmax activation, relying on the richness on the embeddings. As a consequence the sequence tagging decisions are made independently for each token, in contrast to our work.

One central question in multi-task learning is the training procedure. Several schemes have been proposed in the literature. \citeauthor{Hashimoto2017} \shortcite{Hashimoto2017} train their hierarchical model following the model's architecture from bottom to top: the trainer successively goes through the whole dataset for each task before moving to the task of the following level. The underlying hypothesis is that the model should perform well on low-level tasks before being trained in more complicated tasks. \citeauthor{Hashimoto2017} avoid catastrophic forgetting by introducing \textit{successive regularization} using slack constraints on the parameters. \citeauthor{Subramanian2018} \shortcite{Subramanian2018} adopt a simpler strategy for each parameter update: a task is randomly selected and a batch of the associated dataset is sampled for the current update. More recently, \citeauthor{Mccann2018} \shortcite{Mccann2018} explored various batch-level sampling strategies and showed that an anti-curriculum learning strategy \cite{Bengio2009} is most effective. In contrast, we propose a novel proportional sampling strategy, which we find to be more effective.

Regarding the selection of the set of tasks, our work is closest to \cite{Durrett2014,Singh2013}. \citeauthor{Durrett2014} \shortcite{Durrett2014} combine coreference resolution, entity linking (sometimes referred to as \textit{Wikification}) and mention detection. \citeauthor{Singh2013} \shortcite{Singh2013} combine entity tagging, coreference resolution and relation extraction. These two works are based on graphical models with hand-engineered factors. We are using a neural-net-based approach fully trainable in an end-to-end fashion, with no need for external NLP tools (such as in \cite{Durrett2014}) or hand-engineered features. Coreference resolution is rarely used in combination with other tasks. The main work we are aware of is \cite{Dhingra2018}, which uses coreference clusters to improve reading comprehension and the works on language modeling by \citeauthor{Ji2017} \shortcite{Ji2017} and \citeauthor{Yang2016lm} \shortcite{Yang2016lm}.

Regarding the combination of entity mention detection and relation, we refer to our baselines detailed above. Here again, our predictors do not require additional features like dependency trees \cite{Miwa2016} or hand-engineered heuristics \cite{Li2014}.

\section{Conclusion}

We proposed a hierarchically supervised multi-task learning model focused on a set of semantic task. This model achieved state-of-the-art results on the tasks of Named Entity Recognition, Entity Mention Detection and Relation Extraction and competitive results on Coreference Resolution while using simpler training and regularization procedures than previous works. The tasks share common embeddings and encoders allowing an easy information flow from the lowest level to the top of the architecture. We analyzed several aspects of the representations learned by this model as well as the effect of each tasks on the overall performances of the model.

\section{Acknowledgments}

We thank Gianis Bekoulis and Victor Quach for helpful feedbacks and the anonymous reviewers for constructive comments on the paper.

\fontsize{9.0pt}{10.0pt} \selectfont
\bibliography{mybib} 
\bibliographystyle{aaai}


\end{document}
