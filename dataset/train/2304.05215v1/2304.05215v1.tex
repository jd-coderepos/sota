\section{Experimental Results}

In this section, we evaluate the performance of the proposed models on remote sensing downstream tasks, including rotated object detection and semantic segmentation, after pretraining. We then examine whether the performance increases in proportion to the number of parameters of the pretrained model across all datasets and benchmarks. We compare the performance with other studies, but the comparison group pretrained in remote sensing mainly includes the previous studies \cite{wang2022advancing}. This is because the amount of pretraining data (MillionAID) and the methodology used in pretraining (MAE) should be the same for fair comparison.

To demonstrate whether the ability to extract representations increases as the number of parameters included in the model increases, we conduct sample efficiency experiments with DIOR-R for rotated object detection and Potsdam for semantic segmentation. To obtain sample efficiency results, we use 1\%, 5\%, 10\%, 50\%, and 100\% of the training dataset and 100\% of the test dataset for evaluation.

\subsection{Rotated Object Detection}

\subsubsection{Dataset}

In this paper, two benchmark dataset is adopted to evaluate the performance of pretrained models: DOTA v2.0\cite{ding2021object} and DIOR-R\cite{cheng2022anchor}.

\textbf{DOTA v2.0}. The DOTA dataset is the most well-known dataset as a benchmark for rotated object detection in the remote sensing field. We use DOTA v2.0 for evaluation because of its better integrity than v1.0. To make the model robust at various resolutions, images are collected from multiple sources and consist of images with pixel ranges from 800 $\times$ 800 to 20,000 $\times$ 20,000. DOTA v2.0 has 18 categories including harbor, swimming-pool, roundabout, bridge, baseball-diamond, ground-track-field, soccer-ball-field, storage-tank, basketball-court, large-vehicle, plane, tennis-court, ship, helicopter, helipad, container-crane, airport, and small-vehicle. Also, it has 11,268 images and 1,793,658 instances. The training dataset contains 1,830 images and 268,627 instances. The validation dataset contains 593 images and 81,048 instances. And, the test-dev contains 2,792 images and 353,346 instances. However, the test-dev is published with only images. The test-challenge dataset contains 6,053 images and 1,090,637 instances. However, because the test-challenge dataset is only published during challenge, it can not be accessible now. We evaluate our model by submitting the inference result to the server\footnote{\url{https://captain-whu.github.io/DOTA/evaluation.html}}.

\textbf{DIOR-R}.  The DIOR-R dataset is also widely used as a benchmark for evaluating rotated object detection performance in remote sensing. The DIOR-R dataset is divided into training, validation and test. It has 20 categories including expressway-toll-station, chimney, baseball-field, vehicle, harbor, basketball-court, golf-field, tennis-court, storage-tank, windmill, train-station, bridge, ground-track-field, ship, airport, airplane, Expressway-Service-area, dam, stadium, and overpass. It includes 11,738 images and 68,073 instances by bundling training with validation. The test contains 11,738 images and consists of 124,445 instances. All images are 800x800 in size, pixel resolution is from 0.5m to 30m. Unlike DOTA v2.0, the training, validation and test datasets are totally published with both images and instances. We evaluate our model locally using the published dataset.

\subsubsection{Implementation Details and Experiment Settings}

As in other research, we train the rotated object detection models with the same hyperparameters, regardless of the dataset, and only change the pretrained backbone. The detection models are trained based on the mmrotated framework\footnote{\url{https://github.com/open-mmlab/mmrotate}} with added dataset functionality \cite{zhou2022mmrotate}. We use the AdamW optimizer \cite{loshchilov2017decoupled}, with a learning rate of 0.0001 and weight decay of 0.05. The training schedule is designed for a batch size of 2 and 12 epochs, with the learning rate being reduced by 10$\times$ at the 8th and 11th epochs. We also apply a 0.8 layer-wise learning rate decay strategy and a drop path rate of 0.1 to ViTDET.

We select and fix the roi transformer \cite{ding2019learning} as the detection head, which has the best performance in the mmrotate framework. Since \cite{wang2022advancing} has evaluated the performance with Oriented R-CNN \cite{xie2021oriented} but not on the DOTA v2.0 dataset, we re-implement and evaluate the detection model composed of the backbone weight published by \cite{wang2022advancing} and the roi transformer. The backbone is converted into the ViTDET architecture, as shown in \autoref{subsec:vitdet}.

Due to the wide range of pixel sizes in the DOTA v2.0 dataset, we crop each image to a size of 1024 $\times$ 1024 with a stride of 824. For the DIOR-R dataset, images are already set to a constant size of 800$\times$800, so cropping is not performed. During training, we use only random horizontal and vertical flips as data augmentation for both datasets. Test-time augmentation is not applied during inference on test images, and models are evaluated based on average precision (AP) for each category and mean average precision (mAP), which is the average of APs across all categories.


\begin{figure*}[p!bt]{}
    \centering
    \includegraphics[width=0.9\textwidth]{dota.png}
    \includegraphics[width=0.9\textwidth]{dior.png}
    \caption{Visualization results of the proposed model. The first through third rows are the results of the DOTA v2.0 dataset. Since the label of test dataset in DOTA v2.0 is unavailable, the images from left to right are ViT-B12$\times$1, ViT-L12$\times$4, ViT-H12$\times$4, and ViT-G12$\times$4. The fourth to sixth rows are the results of the DIOR-R dataset. The images from left to right are label, ViT-B12$\times$1, ViT-L12$\times$4, ViT-H12$\times$4, and ViT-G12$\times$4. }
    \label{fig:vis_rotated_od}
\end{figure*}

\subsubsection{Experiment Results}

\begin{table*}[ht]{\textwidth=0mm}
    \centering
    \caption{the results of class-wise AP and mAP on DOTA v2.0. Because existing benchmarks have not been performed on mmrotate framework, the $\dagger$ means the result of evaluation using RoI Transformer in Res50 with mmrotate framework. $\diamondsuit$ is the result re-implemented in mmrotate framework using the vision transformer weight published by \cite{wang2022advancing} with RoI Transformer.} \setlength{\tabcolsep}{5pt}
    {\tiny
    \begin{tabular}{l|l|c c c c c c c c c c c c c c c c c c | c}
    \hline
       Backbone & Method & PL & BD & BR & GTF & SV & LV & SH & TC & BC & ST & SBF & RA & HA & SP & HC & CC & AP & HL & mAP  \\ \hline
       
       Res50(IMP)\cite{he2016deep} & RetinaNet\cite{lin2017focal} & 70.63 & 47.26 & 39.12 & 55.02 & 38.1 & 40.52 & 47.16 & 77.74 & 56.86 & 52.12 & 37.22 & 51.75 & 44.15 & 53.19 & 51.06 & 6.58 & 64.28 & 7.45 & 46.68 \\
       Res50(IMP)\cite{he2016deep} & MR\cite{he2017mask}& 76.2 & 49.91 & 41.61 & 60 & 41.08 & 50.77 & 56.24 & 78.01 & 55.85 & 57.48 & 36.62 & 51.67 & 47.39 & 55.79 & 59.06 & 3.64 & 60.26 & 8.95 & 49.47 \\
       Res50(IMP)\cite{he2016deep} & CMR\cite{chen2019hybrid} & 77.01 & 47.54 & 41.79 & 58.2 & 41.58 & 51.74 & 57.86 & 78.2 & 56.75 & 58.5 & 37.89 & 51.23 & 49.38 & 55.98 & 54.59 & 12.31 & 67.33 & 3.01 & 50.04 \\
       Res50(IMP)\cite{he2016deep} & HTC\cite{chen2019hybrid} & 77.69 & 47.25 & 41.15 & 60.71 & 41.77 & 52.79 & 58.87 & 78.74 & 55.22 & 58.49 & 38.57 & 52.48 & 49.58 & 56.18 & 54.09 & 4.2 & 66.38 & 11.92 & 50.34 \\
       Res50(IMP)\cite{he2016deep} & FR OBB\cite{xia2018dota} & 71.61 & 47.2 & 39.28 & 58.7 & 35.55 & 48.88 & 51.51 & 78.97 & 58.36 & 58.55 & 36.11 & 51.73 & 43.57 & 55.33 & 57.07 & 3.51 & 52.94 & 2.79 & 47.31
       \\
       Res50(IMP)\cite{he2016deep} & FR OBB + Dp\cite{dai2017deformable} & 71.55 & 49.74 & 40.34 & 60.4 & 40.74 & 50.67 & 56.58 & 79.03 & 58.22 & 58.24 & 34.73 & 51.95 & 44.33 & 55.1 & 53.14 & 7.21 & 59.53 & 6.38 & 48.77 \\
       Res50(IMP)\cite{he2016deep} & FR H-OBB\cite{xia2018dota} & 71.39 & 47.59 & 39.82 & 59.01 & 41.51 & 49.88 & 57.17 & 78.36 & 56.87 & 58.24 & 37.66 & 51.86 & 44.61 & 55.49 & 54.74 & 7.56 & 61.88 & 6.6 & 48.9 \\
       Res50(IMP)\cite{he2016deep} & FROBB + RT\cite{ding2019learning} & 71.81 & 48.39 & 45.88 & 64.02 & 42.09 & 54.39 & 59.92 & \textbf{\textcolor{blue}{82.7}} & 63.29 & 58.71 & 41.04 & 52.82 & 53.32 & 56.18 & 57.94 & 25.71 & 63.72 & 8.7 & 52.81 \\
       Res50(IMP)$\dagger$ \cite{he2016deep} & FROBB + RT\cite{ding2019learning} & 77.84 & 51.54 & 45.97 & \textbf{\textcolor{red}{65.78}} & 43.25 & 55.03 & 60.38 & 79.45 & 62.98 & 59.85 & \textbf{\textcolor{red}{46.89}} & 56.53 & 54.06 & 56.71 & 51.65 & 21.31 & 68.05 & 8.32 & 53.64 \\ \hline
       ViT-B12$\times$1(MAE)$\diamondsuit$ \cite{wang2022advancing} & FROBB + RT\cite{ding2019learning} & 79.49 & \textbf{\textcolor{red}{55.86}} & \textbf{\textcolor{red}{50.12}} & 65.36 & 43.82 & 56.63 & 61.18 & 79.07 & 62.24 & 60.62 & 41.71 & 57.88 & \textbf{\textcolor{red}{58.48}} & \textbf{\textcolor{red}{64.84}} & 58.83 & \textbf{\textcolor{red}{35.13}} & \textbf{\textcolor{red}{89.41}} & \textbf{\textcolor{blue}{14.14}} & 57.49 \\
       ViT-L12$\times$4(MAE)(Ours) & FROBB + RT\cite{ding2019learning} & 79.75 & 53.34 & 49.02 & 65.18 & 43.8 & \textbf{\textcolor{blue}{56.83}} & \textbf{\textcolor{blue}{61.28}} & \textbf{\textcolor{red}{83.08}} & 60.77 & \textbf{\textcolor{red}{66.83}} & 42.33 & \textbf{\textcolor{red}{58.33}} & 57.39 & \textbf{\textcolor{red}{64.84}} & 67.31 & 30.81 & 80.69 & 13.8 & 57.52 \\
       ViT-H12$\times$4(MAE)(Ours) & FROBB + RT\cite{ding2019learning} & \textbf{\textcolor{blue}{79.8}} & 53.26 & 49.32 & 64.28 & \textbf{\textcolor{blue}{43.9}} & 56.46 & 61.18 & 78.98 & \textbf{\textcolor{blue}{63.53}} & \textbf{\textcolor{blue}{60.71}} & 42.76 & 57.74 & 57.84 & 64.43 & \textbf{\textcolor{blue}{67.34}} & \textbf{\textcolor{blue}{34.69}} & 89.35 & \textbf{\textcolor{red}{14.3}} & \textbf{\textcolor{blue}{57.77}} \\
       ViT-G12$\times$4(MAE)(Ours) & FROBB + RT\cite{ding2019learning} & \textbf{\textcolor{red}{80.12}} & \textbf{\textcolor{blue}{54.12}} & \textbf{\textcolor{blue}{50.07}} & \textbf{\textcolor{blue}{65.68}} & \textbf{\textcolor{red}{43.98}} & \textbf{\textcolor{red}{60.07}} & \textbf{\textcolor{red}{67.85}} & 79.11 & \textbf{\textcolor{red}{64.38}} & 60.56 & \textbf{\textcolor{blue}{45.98}} & \textbf{\textcolor{blue}{58.26}} & \textbf{\textcolor{blue}{58.31}} & \textbf{\textcolor{blue}{64.82}} & \textbf{\textcolor{red}{69.84}} & 32.78 & \textbf{\textcolor{blue}{89.37}} & 11.07 & \textbf{\textcolor{red}{58.69}} \\ \hline
    \end{tabular}
    }
    \label{tab:dota2 table}
\end{table*} \begin{table*}[ht]{\textwidth=0mm}
    \centering
    \caption{the results of class-wise AP and mAP on DIOR-R. As same with \autoref{tab:dota2 table}, $\diamondsuit$ is the result re-implemented in mmrotate framework using the vision transformer weight published by \cite{wang2022advancing} with RoI Transformer.} \setlength{\tabcolsep}{3.75pt}
    {\tiny
    \begin{tabular}{l|l|c c c c c c c c c c c c c c c c c c c c | c }
        \hline
       Backbone & Method & APL & APO & BF & BC & BR & CH & DAM & ETS & ESA & GF & GTF & HA & OP & SH & STA & STO & TC & TS & VE & WM & mAP \\ \hline

       Res50(IMP)\cite{he2016deep} & RetinaNet\cite{lin2017focal} & 61.49 & 28.52 & 73.57 & 81.17 & 23.98 & 72.54 & 19.94 & 72.39 & 58.2 & 69.25 & 79.54 & 32.14 & 44.87 & 77.71 & 67.57 & 61.09 & 81.46 & 47.33 & 38.01 & 60.24 & 57.55 \\

       Res50(IMP)\cite{he2016deep} & FR OBB\cite{xia2018dota} & 62.79 & 26.8 & 71.72 & 80.91 & 34.2 & 72.57 & 18.95 & 66.45 & 65.75 & 66.63 & 79.24 & 34.95 & 48.79 & 81.14 & 64.34 & 71.21 & 81.44 & 47.31 & 50.46 & 65.21 & 59.54 \\

       Res50(IMP)\cite{he2016deep} & FROBB + RT\cite{ding2019learning} & 63.34 & 37.88 & 71.78 & 87.53 & 40.68 & 72.6 & 26.86 & 78.71 & 68.09 & 68.96 & 82.74 & 47.71 & 55.61 & 81.21 & 78.23 & 70.26 & 81.61 & 54.86 & 43.27 & 65.52 & 63.87 \\

       Res50(IMP)\cite{he2016deep} & Gliding Vertex\cite{xu2020gliding} & 65.35 & 28.87 & 74.96 & 81.33 & 33.88 & 74.31 & 19.58 & 70.72 & 64.7 & 72.3 & 78.68 & 37.22 & 49.64 & 80.22 & 69.26 & 61.13 & 81.49 & 44.76 & 47.71 & 65.04 & 60.06 \\

       Res50(IMP)\cite{he2016deep} & AOPG\cite{cheng2022anchor} & 62.39 & 37.79 & 71.62 & 87.63 & 40.9 & 72.47 & 31.08 & 65.42 & 77.99 & 73.2 & 81.94 & 42.32 & 54.45 & 81.17 & 72.69 & 71.31 & 81.49 & 60.04 & \textbf{\textcolor{red}{52.38}} & 69.99 & 64.41 \\
       
       Res50(IMP)\cite{he2016deep} & DODet\cite{cheng2022dual} & 63.4 & 43.35 & 72.11 & 81.32 & 43.12 & 72.59 & 33.32 & 78.77 & 70.84 & 74.15 & 75.47 & 48 & 59.31 & 85.41 & 74.04 & 71.56 & 81.52 & 55.47 & 51.86 & 66.4 & 65.1 \\ \hline

       ViT-B(MAE)\cite{wang2022advancing} & Oriented R-CNN\cite{xie2021oriented} & 81.04 & 41.86 & 80.79 & 81.39 & 44.83 & 78.35 & 35.12 & 67.67 & 84.85 & 75.44 & 80.8 & 37.65 & 59.33 & 81.15 & 78.7 & 62.87 & 89.83 & 56.17 & 49.87 & 65.36 & 66.65 \\

       ViTAE-B(MAE)\cite{wang2022advancing} & Oriented R-CNN\cite{xie2021oriented} & 81.3 & 46.73 & 81.09 & 87.62 & 47.38 & 79.79 & 31.99 & 69.72 & 86.71 & 76.23 & 82.13 & 42.47 & 60.45 & 81.2 & 80.11 & 62.75 & 89.75 & 64.56 & 50.77 & 65.33 & 68.4 \\

       ViT-B + VSA(MAE)\cite{wang2022advancing} & Oriented R-CNN\cite{xie2021oriented} & 81.2 & 50.68 & 80.95 & 87.41 & 51.27 & \textbf{\textcolor{red}{80.87}} & 34.61 & 76.4 & 88.32 & 78.21 & 83.31 & 45.84 & 64.02 & 81.23 & 82.87 & 71.31 & 89.86 & 64.66 & 50.84 & 65.81 & 70.48 \\

       ViTAE-B + VSA(MAE)\cite{wang2022advancing} & Oriented R-CNN\cite{xie2021oriented} & 81.26 & 52.26 & 81.1 & 88.47 & 51.35 & 80.18 & 37.4 & 75.29 & 88.92 & 77.52 & 84.33 & 47.31 & 63.73 & 81.18 & 83.03 & 71.13 & 90.04 & 65.01 & 50.81 & 65.82 & 70.81 \\


       ViT-B + RVSA(MAE)\cite{wang2022advancing} & Oriented R-CNN\cite{xie2021oriented} & 80.92 & 49.88 & 81.05 & 88.52 & 51.52 & 80.17 & 37.87 & 75.96 & 88.83 & 78.46 & 84.01 & 46.53 & 64.18 & 81.21 & 84.04 & 71.34 & 89.99 & 65.41 & 50.53 & 66.49 & 70.85 \\


       ViTAE-B + RVSA(MAE)\cite{wang2022advancing} & Oriented R-CNN\cite{xie2021oriented} & 81.2 & 54.71 & \textbf{\textcolor{blue}{81.12}} & 88.13 & 51.83 & 79.93 & 36.79 & 76.06 & \textbf{\textcolor{blue}{89.23}} & 78.3 & \textbf{\textcolor{blue}{84.46}} & 47.29 & 65.01 & 81.19 & 82.17 & 70.69 & 90.03 & \textbf{\textcolor{red}{66.75}} & 50.73 & 65.4 & 71.05 \\ \hline

       ViT-B12$\times$1(MAE)$\diamondsuit$\cite{wang2022advancing} & FROBB + RT\cite{ding2019learning} & 72.25 & 53.86 & 80.55 & 81.49 & 45.15 & 79.96 & 31.56 & 71.44 & 85.42 & 78.67 & 83.72 & 47.57 & 59.55 & 81.26 & \textbf{\textcolor{blue}{84.87}} & 71.28 & 81.51 & 64.73 & 49.43 & 66.05 & 68.52 \\

       ViT-L12$\times$4(MAE)(Ours) & FROBB + RT\cite{ding2019learning} & 81.2 & \textbf{\textcolor{blue}{60.47}} & 81.01 & \textbf{\textcolor{red}{89.97}} & 51.76 & 80.46 & 39.98 & 78.75 & 89.12 & 78.77 & 84.06 & 53.85 & 60.93 & 81.24 & 84.4 & \textbf{\textcolor{blue}{71.77}} & \textbf{\textcolor{red}{90.15}} & 66.22 & 51.46 & 66.59 & 72.11 \\

       ViT-H12$\times$4(MAE)(Ours) & FROBB + RT\cite{ding2019learning} & \textbf{\textcolor{red}{81.59}} & 55.01 & 80.87 & 88.96 & \textbf{\textcolor{red}{54.27}} & 80.76 & \textbf{\textcolor{red}{40.61}} & \textbf{\textcolor{red}{79.73}} & \textbf{\textcolor{red}{89.47}} & \textbf{\textcolor{red}{79.47}} & 83.84 & \textbf{\textcolor{blue}{55.28}} & \textbf{\textcolor{blue}{65.27}} & \textbf{\textcolor{blue}{89.52}} & 84.42 & \textbf{\textcolor{red}{71.91}} & 90.06 & 65.97 & 51.8 & \textbf{\textcolor{red}{74.12}} & \textbf{\textcolor{blue}{73.15}} \\

       ViT-G12$\times$4(MAE)(Ours) & FROBB + RT\cite{ding2019learning} & \textbf{\textcolor{blue}{81.41}} & \textbf{\textcolor{red}{61.73}} & \textbf{\textcolor{red}{81.17}} & \textbf{\textcolor{blue}{89.86}} & \textbf{\textcolor{blue}{54.12}} & \textbf{\textcolor{blue}{80.84}} & \textbf{\textcolor{blue}{40.35}} & \textbf{\textcolor{blue}{79.42}} & 89.08 & \textbf{\textcolor{blue}{79.3}} & \textbf{\textcolor{red}{84.51}} & \textbf{\textcolor{red}{55.83}} & \textbf{\textcolor{red}{65.61}} & \textbf{\textcolor{red}{89.59}} & \textbf{\textcolor{red}{86.16}} & 71.52 & \textbf{\textcolor{blue}{90.11}} & \textbf{\textcolor{blue}{66.29}} & \textbf{\textcolor{blue}{51.88}} & \textbf{\textcolor{blue}{73.67}} & \textbf{\textcolor{red}{73.62}} \\ \hline

    \end{tabular}
    }
    \label{tab:dior table}
\end{table*} 
\autoref{tab:dota2 table} and \autoref{tab:dior table} show the performance results for DOTA v2.0 and DIOR-R, while \autoref{tab:dior sample efficiency table} displays the results of sample efficiency experiments using different amounts of training data. In the tables, red and blue text represent the highest and second-highest performance, respectively. Specifically, the smaller datasets are obtained by randomly sampling 1\%, 5\%, 10\%, 50\%, and 100\% of the images. The number of objects included in each dataset can be seen in \autoref{tab:dior label ratio}.

\textbf{DOTA v2.0}. \autoref{tab:dota2 table} displays the performance results for various models, including our experiments. The results for models other than ours are taken from the official paper \cite{ding2021object}. As noted in the caption of \autoref{tab:dota2 table}, the $\dagger$ represents the result of metrics using the mmrotate framework for fair evaluation. {The abbreviations of method are: Mask R-CNN (MR)\cite{he2017mask}, CMR-Cascade Mask R-CNN (CMR) \cite{chen2019hybrid}, Hybrid Task Cascade without a semanticbranch (HTC) \cite{chen2019hybrid}, Faster R-CNN(FR) \cite{xia2018dota}, Deformable Roi Pooling (Dp) \cite{dai2017deformable} and RoI Transformer (RT) \cite{ding2019learning}. The short names for categories are defined as: PL (Plane), BD (Baseball diamond), BR (Bridge), GTF (Ground track field), SV (Small-vehicle), LV (Large-vehicle), SH (Ship), TC (Tennis-court), BC (Basketball-court), ST (Storage-tank), SBF (Soccer-ball field), RA (Roundabout), HA (Harbor), SP (Swimming-pool), HC (Helicopter), CC (Container-crane), AP (Airport), and HL (Helipad). In backbone column, the IMP means ImageNet classification pretraining. The MAE means MAE on the MillionAID.  Because existing benchmarks have not been performed on mmrotate framework, the $\dagger$ means the result of evaluation using RoI Transformer in Res50 with mmrotate framework. In order to compare the results with the RoI Transformer, $\diamondsuit$ is the result re-implemented in mmrotate framework using the vision transformer weight published by \cite{wang2022advancing}.} Clearly, models pretrained with MAE outperform those pretrained with IMP, with mAP differences ranging from 3.85 to 5.05. In terms of the models proposed in this paper, the relationship between the number of parameters and mAP is such that the mAP tends to increase as the number of parameters increases, with no other changes except for the number of parameters. Although the increase in mAP becomes smaller as the number of model parameters grows, it is evident that performance improves.

\textbf{DIOR-R}. \autoref{tab:dior table} displays the performance results, with the results of models in the table taken from previous research \cite{wang2022advancing}. {The short names for categories are defined as: APL (Airplane), APO (Airport), BF (Baseball field), BC (Baseketball court), BR (Bridge), CH (Chimney), DAM (Dam), ETS (Expressway-toll-station), ESA (Expressway-service-area), GF (Golf field), GTF (Ground track field), HA (Harbor), OP (Overpass), SH(Ship), STA (Stadium), STO (Storage tank), TC (Tennis court), TS (Transition), VE (Vehicle), and WM (Windmill). As same with \autoref{tab:dota2 table}, the IMP means ImageNet classification pretraining. The MAE means MAE on the MillionAID.} Additionally, as shown in \autoref{tab:dior table}, the $\diamondsuit$ represents the result re-implemented in the mmrotate framework using the vision transformer weight published by \cite{wang2022advancing} with the RoI Transformer detection head. As expected, the performance of models pretrained with MAE is higher than those pretrained with IMP. Performance can be improved by changing the backbone structure using the RVSA \cite{wang2022advancing} and ViTAE \cite{xu2021vitae} modules proposed in previous studies. Observing performance changes based solely on the number of parameters in the backbone module, it is clear that mAP increases as the number of backbone parameters grows. Although the rate of improvement in mAP becomes less significant as the number of model parameters increases, it is still evident that performance improves with a higher number of parameters.

\begin{table*}[ht]{\textwidth=0mm}
    \centering
    \caption{the results of class-wise AP and mAP on DIOR-R for evaluating sample efficiency. The short names for categories are defined as same with \autoref{tab:dior table}. In order to compare the results according only backbone, the ViT-B12$\times$1 is retrained by mmrotate framework. The training data of DIOR-R is randomly sampled by ratio 0.01, 0.05, 0.1, 0.5, 1.0. }
    \setlength{\tabcolsep}{2.125pt}
    \renewcommand{\arraystretch}{1.25}
    {\scriptsize
    \begin{tabular}{c | l | c c c c c c c c c c c c c c c c c c c c | c }
        \hline
       Sample ratio & Backbone & APL & APO & BF & BC & BR & CH & DAM & ETS & ESA & GF & GTF & HA & OP & SH & STA & STO & TC & TS & VE & WM & mAP \\ \hline

       \multirow{4}{*}{1$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 9.10 & \textbf{\textcolor{blue}{0.40}} & 48.20 & 0.00 & 0.00 & 17.70 & 1.30 & 4.50 & 1.40 & 22.10 & 20.40 & 2.10 & 2.80 & 13.20 & \textbf{\textcolor{red}{24.30}} & 29.00 & 55.50 & 8.50 & 10.50 & 5.00 & 13.80 \\
       
        & ViT-L12$\times$4 & \textbf{\textcolor{blue}{10.40}} & \textbf{\textcolor{red}{0.60}} & \textbf{\textcolor{blue}{52.70}} & 0.00 & \textbf{\textcolor{red}{0.70}} & \textbf{\textcolor{blue}{28.00}} & 4.50 & \textbf{\textcolor{blue}{8.20}} & 4.50 & 25.70 & 20.00 & 2.20 & \textbf{\textcolor{blue}{4.50}} & 17.10 & 19.70 & \textbf{\textcolor{red}{42.70}} & 49.00 & \textbf{\textcolor{blue}{10.90}} & \textbf{\textcolor{blue}{11.60}} & \textbf{\textcolor{red}{12.90}} & 16.30 \\
        
        & ViT-H12$\times$4 & 10.20 & 0.30 & 52.20 & \textbf{\textcolor{blue}{0.20}} & 0.00 & \textbf{\textcolor{red}{28.70}} & \textbf{\textcolor{red}{9.10}} & 0.90 & \textbf{\textcolor{red}{9.10}} & \textbf{\textcolor{red}{30.00}} & \textbf{\textcolor{red}{21.80}} & \textbf{\textcolor{blue}{4.50}} & 1.20 & \textbf{\textcolor{blue}{19.00}} & 20.60 & \textbf{\textcolor{blue}{31.10}} & \textbf{\textcolor{blue}{61.60}} & \textbf{\textcolor{blue}{10.90}} & 11.50 & 5.10 & \textbf{\textcolor{blue}{16.40}} \\
        
        & ViT-G12$\times$4 & \textbf{\textcolor{red}{13.70}} & 0.30 & \textbf{\textcolor{red}{53.50}} & \textbf{\textcolor{red}{2.60}} & 0.00 & 20.10 & 3.00 & \textbf{\textcolor{red}{11.20}} & 2.20 & \textbf{\textcolor{blue}{26.00}} & \textbf{\textcolor{blue}{20.80}} & \textbf{\textcolor{red}{6.10}} & \textbf{\textcolor{red}{9.10}} & \textbf{\textcolor{red}{21.10}} & \textbf{\textcolor{blue}{21.40}} & 30.50 
        & \textbf{\textcolor{red}{61.70}} & \textbf{\textcolor{red}{12.10}} & \textbf{\textcolor{red}{12.00}} & \textbf{\textcolor{blue}{12.40}} & \textbf{\textcolor{red}{17.00}} \\ \hline


        \multirow{4}{*}{5$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & \textbf{\textcolor{blue}{61.50}} & \textbf{\textcolor{blue}{6.20}} & \textbf{\textcolor{blue}{63.20}} & \textbf{\textcolor{red}{67.80}} & \textbf{\textcolor{red}{11.70}} & \textbf{\textcolor{blue}{67.20}} & 9.70 & 38.00 & 38.00 & 57.70 & 56.40 & 14.20 & 19.30 & \textbf{\textcolor{red}{62.20}} & 52.10 & \textbf{\textcolor{red}{61.40}} & \textbf{\textcolor{blue}{80.20}} & 20.20 & 33.10 & 33.10 & 42.70 \\
        
        & ViT-L12$\times$4 & 53.10 & 3.60 & 63.00 & \textbf{\textcolor{blue}{66.80}} & 5.80 & \textbf{\textcolor{red}{70.30}} & 4.20 & 35.60 & \textbf{\textcolor{blue}{42.30}} & \textbf{\textcolor{red}{59.80}} & \textbf{\textcolor{blue}{58.30}} & \textbf{\textcolor{red}{18.20}} & 24.10 & 61.90 & \textbf{\textcolor{blue}{55.00}} & \textbf{\textcolor{blue}{61.10}} & 79.30 & \textbf{\textcolor{blue}{21.90}} & \textbf{\textcolor{blue}{33.20}} & \textbf{\textcolor{blue}{41.20}} & 42.90 \\
        
        & ViT-H12$\times$4 & 53.50 & 4.40 & 63.10 & 63.10 & 7.90 & 62.50 & \textbf{\textcolor{blue}{10.40}} & \textbf{\textcolor{red}{39.60}} & \textbf{\textcolor{red}{42.40}} & 58.40 & \textbf{\textcolor{red}{61.50}} & 15.40 & \textbf{\textcolor{blue}{25.50}} & \textbf{\textcolor{blue}{62.10}} & \textbf{\textcolor{red}{58.00}} & 61.00 & \textbf{\textcolor{red}{80.30}} & 19.20 & \textbf{\textcolor{blue}{33.20}} & 40.40 & \textbf{\textcolor{blue}{43.10}} \\
        
        & ViT-G12$\times$4 & \textbf{\textcolor{red}{62.30}} & \textbf{\textcolor{red}{10.20}} & \textbf{\textcolor{red}{63.30}} & 65.60 & \textbf{\textcolor{blue}{11.60}} & 63.40 & \textbf{\textcolor{red}{10.90}} & \textbf{\textcolor{blue}{38.40}} & \textbf{\textcolor{red}{42.40}} & \textbf{\textcolor{blue}{58.60}} & 57.00 & \textbf{\textcolor{blue}{16.80}} & \textbf{\textcolor{red}{27.80}} & 61.30 & 52.00 & 52.60 & 72.40 & \textbf{\textcolor{red}{25.10}} & \textbf{\textcolor{red}{33.40}} & \textbf{\textcolor{red}{41.50}} & \textbf{\textcolor{red}{43.30}} \\ \hline


        \multirow{4}{*}{10$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 53.50 & 11.70 & 62.90 & 68.60 & 12.90 & 71.70 & 12.50 & 39.70 & 43.10 & 56.80 & 66.10 & 15.20 & 26.70 & 71.30 & 53.20 & 59.60 & 78.80 & 29.30 & 33.10 & 42.90 & 45.50 \\
        
        & ViT-L12$\times$4 & 63.10 & 10.70 & 70.20 & \textbf{\textcolor{red}{79.70}} & 19.80 & \textbf{\textcolor{blue}{71.80}} & 11.50 & 47.60 & \textbf{\textcolor{blue}{52.00}} & 59.20 & 68.20 & 25.50 & \textbf{\textcolor{blue}{38.40}} & \textbf{\textcolor{blue}{79.10}} & \textbf{\textcolor{blue}{63.70}} & 61.40 & \textbf{\textcolor{red}{81.40}} & 32.30 & 39.80 & 51.80 & 51.40 \\
        
        & ViT-H12$\times$4 & \textbf{\textcolor{blue}{71.40}} & \textbf{\textcolor{blue}{15.10}} & \textbf{\textcolor{blue}{71.00}} & \textbf{\textcolor{blue}{79.10}} & \textbf{\textcolor{blue}{22.00}} & 71.60 & \textbf{\textcolor{blue}{15.20}} & \textbf{\textcolor{blue}{49.20}} & 51.10 & \textbf{\textcolor{red}{65.90}} & \textbf{\textcolor{blue}{68.70}} & \textbf{\textcolor{blue}{26.60}} & 36.90 & \textbf{\textcolor{blue}{79.10}} & 62.90 & \textbf{\textcolor{blue}{61.60}} & \textbf{\textcolor{blue}{81.30}} & \textbf{\textcolor{blue}{39.40}} & \textbf{\textcolor{red}{41.10}} & \textbf{\textcolor{blue}{52.50}} & \textbf{\textcolor{blue}{53.10}} \\
        
        & ViT-G12$\times$4 & \textbf{\textcolor{red}{71.90}} & \textbf{\textcolor{red}{15.30}} & \textbf{\textcolor{red}{71.60}} & 77.70 & \textbf{\textcolor{red}{23.70}} & \textbf{\textcolor{red}{72.10}} & \textbf{\textcolor{red}{15.70}} & \textbf{\textcolor{red}{50.80}} & \textbf{\textcolor{red}{52.30}} & \textbf{\textcolor{blue}{64.70}} & \textbf{\textcolor{red}{70.00}} & \textbf{\textcolor{red}{29.30}} & \textbf{\textcolor{red}{42.80}} & \textbf{\textcolor{red}{79.30}} & \textbf{\textcolor{red}{63.90}} & \textbf{\textcolor{red}{61.70}} & 81.00 & \textbf{\textcolor{red}{42.20}} & \textbf{\textcolor{blue}{41.00}} & \textbf{\textcolor{red}{53.00}} & \textbf{\textcolor{red}{54.00}} \\ \hline


        \multirow{4}{*}{50$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & \textbf{\textcolor{blue}{63.40}} & 39.30 & \textbf{\textcolor{blue}{80.20}} & 81.10 & 36.10 & \textbf{\textcolor{red}{72.60}} & 28.00 & 61.40 & 79.00 & 76.40 & 81.70 & 38.20 & 54.40 & \textbf{\textcolor{blue}{81.10}} & 77.90 & 62.90 & \textbf{\textcolor{blue}{81.50}} & 56.70 & 42.90 & 63.50 & 62.90 \\
        
        & ViT-L12$\times$4 & \textbf{\textcolor{red}{72.50}} & 43.20 & 72.40 & 81.00 & 45.00 & \textbf{\textcolor{red}{72.60}} & 29.60 & 70.10 & 86.40 & 75.80 & \textbf{\textcolor{blue}{82.10}} & 45.50 & 58.90 & \textbf{\textcolor{blue}{81.10}} & 77.30 & 63.00 & \textbf{\textcolor{blue}{81.50}} & 63.40 & 50.10 & \textbf{\textcolor{red}{64.90}} & 65.80 \\
        
        & ViT-H12$\times$4 & \textbf{\textcolor{red}{72.50}} & \textbf{\textcolor{blue}{53.40}} & \textbf{\textcolor{red}{80.90}} & \textbf{\textcolor{blue}{81.20}} & \textbf{\textcolor{red}{47.00}} & \textbf{\textcolor{red}{72.60}} & \textbf{\textcolor{red}{36.90}} & \textbf{\textcolor{red}{71.70}} & \textbf{\textcolor{red}{88.70}} & \textbf{\textcolor{red}{77.50}} & \textbf{\textcolor{red}{83.00}} & \textbf{\textcolor{red}{47.80}} & 60.20 & \textbf{\textcolor{blue}{81.10}} & \textbf{\textcolor{red}{83.80}} & \textbf{\textcolor{blue}{71.40}} & \textbf{\textcolor{red}{81.60}} & \textbf{\textcolor{red}{65.40}} & \textbf{\textcolor{red}{51.10}} & \textbf{\textcolor{blue}{64.40}} & \textbf{\textcolor{blue}{68.60}} \\
        
        & ViT-G12$\times$4 & \textbf{\textcolor{red}{72.50}} & \textbf{\textcolor{red}{53.70}} & \textbf{\textcolor{red}{80.90}} & \textbf{\textcolor{red}{81.40}} & \textbf{\textcolor{blue}{46.60}} & \textbf{\textcolor{blue}{72.50}} & \textbf{\textcolor{blue}{35.90}} & \textbf{\textcolor{blue}{71.30}} & \textbf{\textcolor{blue}{88.30}} & \textbf{\textcolor{blue}{76.60}} & \textbf{\textcolor{red}{83.00}} & \textbf{\textcolor{blue}{47.60}} & \textbf{\textcolor{red}{60.30}} & \textbf{\textcolor{red}{89.30}} & \textbf{\textcolor{blue}{83.50}} & \textbf{\textcolor{red}{71.60}} & \textbf{\textcolor{blue}{81.50}} & \textbf{\textcolor{blue}{64.60}} & \textbf{\textcolor{blue}{50.90}} & \textbf{\textcolor{red}{64.90}} & \textbf{\textcolor{red}{68.90}} \\ \hline
        

        \multirow{4}{*}{100$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 72.20 & 53.80 & 80.50 & 81.40 & 45.10 & 79.90 & 31.50 & 71.40 & 85.40 & 78.60 & 83.70 & 47.50 & 59.50 & \textbf{\textcolor{blue}{81.20}} & \textbf{\textcolor{blue}{84.80}} & 71.20 & 81.50 & 64.70 & 49.40 & 66.00 & 68.50 \\
        
        & ViT-L12$\times$4 & 81.20 & \textbf{\textcolor{blue}{60.40}} & \textbf{\textcolor{blue}{81.00}} & \textbf{\textcolor{red}{89.90}} & 51.70 & 80.40 & 39.90 & 78.70 & \textbf{\textcolor{blue}{89.10}} & 78.70 & \textbf{\textcolor{blue}{84.00}} & 53.80 & 60.90 & \textbf{\textcolor{blue}{81.20}} & 84.40 & \textbf{\textcolor{blue}{71.70}} & \textbf{\textcolor{red}{90.10}} & \textbf{\textcolor{red}{66.20}} & \textbf{\textcolor{blue}{51.40}} & 66.50 & 72.10 \\
        
        & ViT-H12$\times$4 & \textbf{\textcolor{red}{81.50}} & 55.00 & 80.80 & 88.90 & \textbf{\textcolor{red}{54.20}} & \textbf{\textcolor{blue}{80.70}} & \textbf{\textcolor{red}{40.60}} & \textbf{\textcolor{red}{79.70}} & \textbf{\textcolor{red}{89.40}} & \textbf{\textcolor{red}{79.40}} & 83.80 & \textbf{\textcolor{blue}{55.20}} & \textbf{\textcolor{blue}{65.20}} & \textbf{\textcolor{red}{89.50}} & 84.40 & \textbf{\textcolor{red}{71.90}} & \textbf{\textcolor{blue}{90.00}} & \textbf{\textcolor{blue}{65.90}} & \textbf{\textcolor{red}{51.80}} & \textbf{\textcolor{red}{74.10}} & \textbf{\textcolor{blue}{73.10}} \\

        & ViT-G12$\times$4 & \textbf{\textcolor{blue}{81.40}} & \textbf{\textcolor{red}{61.70}} & \textbf{\textcolor{red}{81.10}} & \textbf{\textcolor{blue}{89.80}} & \textbf{\textcolor{blue}{54.10}} & \textbf{\textcolor{red}{80.80}} & \textbf{\textcolor{blue}{40.30}} & \textbf{\textcolor{blue}{79.40}} & 89.00 & \textbf{\textcolor{blue}{79.30}} & \textbf{\textcolor{red}{84.50}} & \textbf{\textcolor{red}{55.80}} & \textbf{\textcolor{red}{65.60}} & \textbf{\textcolor{red}{89.50}} & \textbf{\textcolor{red}{86.10}} & 71.50 & \textbf{\textcolor{red}{90.10}} & \textbf{\textcolor{red}{66.20}} & \textbf{\textcolor{red}{51.80}} & \textbf{\textcolor{blue}{73.60}} & \textbf{\textcolor{red}{73.60}} \\ \hline
        

    \end{tabular}
    }
    \label{tab:dior sample efficiency table}
\end{table*} \begin{table*}[t]{\textwidth=0mm}
    \centering
    \caption{the distribution of instances corresponding to each class for measuring sample efficiency using the DIOR-R dataset. To ensure that the dataset is divided accurately based on images, we need to verify if the objects are distributed according to each ratio. Also, the short names for categories are defined as same with \autoref{tab:dior table}.}
    \setlength{\tabcolsep}{2.75pt}
    \renewcommand{\arraystretch}{1.25}
    {
    \begin{tabular}{c|c c c c c c c c c c c c c c c c c c c c}
        \hline
        \multirow{2}{*}{Sample ratio} & \multicolumn{20}{c}{The Number of Instance} \\ \cline{2-21}
        & APL & APO & BF & BC & BR & CH & DAM & ETS & ESA & GF & GTF & HA & OP & SH & STA & STO & TC & TS & VE & WM \\ \hline

        1 $\%$ & 13 & 8 & 22 & 5 & 17 & 7 & 7 & 7 & 10 & 3 & 14 & 16 & 25 & 260 & 6 & 37 & 50 & 7 & 247 & 19 \\ \hline

        5 $\%$ & 78 & 37 & 101 & 48 & 88 & 34 & 35 & 33 & 51 & 34 & 52 & 105 & 68 & 1195 & 32 & 216 & 220 & 22 & 766 & 136 \\ \hline
        10 $\%$ & 220 & 71 & 234 & 102 & 152 & 72 & 45 & 56 & 90 & 49 & 110 & 319 & 121 & 3040 & 59 & 282 & 601 & 44 & 1323 & 233 \\ \hline
        50 $\%$ & 965 & 325 & 1254 & 560 & 654 & 324 & 221 & 306 & 566 & 258 & 586 & 1244 & 633 & 13296 & 284 & 1730 & 2495 & 253 & 6858 & 1146 \\ \hline
        100 $\%$ & 1888 & 662 & 2384 & 1077 & 1367 & 649 & 512 & 610 & 1080 & 511 & 1162 & 2364 & 1330 & 27351 & 595 & 3042 & 4898 & 501 & 13725 & 2365 \\ \hline
        
    \end{tabular}
    }
    \label{tab:dior label ratio}
\end{table*} 
\textbf{Sample Efficiency in DIOR-R}. Sample efficiency in fine-tuning is an important capability that a foundation model should have \cite{zhang2023vitaev2, bommasani2021opportunities, brown2020language, kim2022fine}. In this context, we conduct experiments on the DIOR-R dataset using 1\%, 5\%, 10\%, 50\%, and 100\% of the training data. Each dataset is sampled based on the ratio of images, meaning that they are not sampled precisely based on individual objects. More details about the portion of objects are shown in \autoref{tab:dior label ratio}. To measure sample efficiency, the model changes only the backbone, like in other experiments, and a RoI Transformer is used as a detection head. As shown, the model achieves better performance as the number of parameters increases within the same sample ratio. Additionally, we can verify that using a backbone with a large number of parameters and well-prepared training can achieve comparable performance even if only half of the data is used, as seen with ViT-H12$\times$4, ViT-G12$\times$4 at 50\%, and ViT-B12$\times$1 at 100\%.

These results demonstrate the effectiveness of the proposed models in improving the performance of rotated object detection tasks in remote sensing datasets. The experiments confirm that utilizing larger backbones with more parameters leads to better performance, even though the rate of improvement becomes less significant as the number of parameters increases. Furthermore, the experiments on sample efficiency show that well-prepared training with larger backbones can yield comparable results even when using a smaller portion of the training data, highlighting the importance of optimizing the model architecture and training strategy in order to achieve the best performance possible.

\subsection{Semantic Segmentation}

\begin{table*}[t]{\textwidth=0mm}
    \centering
    \caption{the results of class-wise F1 score, mF1 score and overall accuracy (OA) on Potsdam. As mentioned in dataset detail, the clutter class is not included to evaluate the performance. In order to compare the results with the ViTDET with out any module such as ViTAE and RVSA, $\diamondsuit$ is the result re-implemented in mmsegmentation framework using the vision transformer weight published by \cite{wang2022advancing}.}
    \renewcommand{\arraystretch}{1.0}
    {
    \begin{tabular}{l|c c c c c | c | c}
    
        \hline
       
       
       \multirow{2}{*}{Method} & \multicolumn{5}{c|}{F1 score per category ($\%$)} & \multirow{2}{*}{mF1 ($\%$)} & \multirow{2}{*}{OA ($\%$)} \\ \cline{2-6}
        & Imper. surf. & Building & Low veg. & Tree & Car & &  \\ \hline

        FCN \cite{long2015fully} & 88.61 & 93.29 & 83.29 & 79.83 & 93.02 & 87.61 & 85.59 \\
        PSPNet \cite{zhao2017pyramid} & 91.61 & 96.3 & 86.41 & 86.84 & 91.38 & 90.51 & 89.45 \\
        DeeplabV3+ \cite{chen2018encoder} & 92.35 & 96.77 & 85.22 & 86.79 & 93.58 & 90.94 & 89.74 \\
        UperNet \cite{xiao2018unified} & \textbf{\textcolor{red}{93.14}} & 96.75 & 86.3 & 86.13 & 91.56 & 90.78 & 91.26 \\
        S-RA-FCN \cite{mou2020relation} & 91.33 & 94.7 & 86.81 & 83.47 & 94.52 & 90.17 & 88.59 \\ 
        UZ\_1 \cite{volpi2016dense}& 89.3 & 95.4 & 81.8 & 80.5 & 86.5 & 86.7 & 85.8 \\
        UFMG\_4 \cite{nogueira2019dynamic}& 90.8 & 95.6 & 84.4 & 84.3 & 92.4 & 89.5 & 87.9 \\
        Multi-filter CNN \cite{sun2018developing} & 90.94 & \textbf{\textcolor{blue}{96.98}} & 76.32 & 73.37 & 88.55 & 85.23 & 90.65 \\
        HMANet \cite{niu2021hybrid}& 92.38 & 96.08 & \textbf{\textcolor{blue}{86.93}} & 88.21 & 95.44 & 91.81 & 90.46 \\
        LANet \cite{ding2020lanet} & \textbf{\textcolor{blue}{93.05}} & \textbf{\textcolor{red}{97.19}} & \textbf{\textcolor{red}{87.3}} & 88.04 & 94.19 & 91.95 & 90.84 \\
        UperNet-SeCo \cite{manas2021seasonal} & 91.21 & 94.92 & 85.12 & 84.89 & 89.02 & 89.03 & 89.64 \\
        UperNet(RSP-ViTAEv2-S) \cite{wang2022empirical} & \textbf{\textcolor{blue}{93.05}} & 96.62 & 86.62 & 85.89 & 91.01 & 90.64 & 91.21 \\
        UperNet(ViT-B + RVSA) \cite{wang2022advancing} & 92.52 & 96.15 & 86.27 & 85.57 & 90.69 & 90.24 & 90.77 \\
        UperNet(ViTAE-B + RVSA) \cite{wang2022advancing} & 92.97 & 96.7 & 86.68 & 85.92 & 90.93 & 90.64 & 91.22 \\ \hline
        UperNet(ViT-B12$\times$1)$\diamondsuit$\cite{wang2022advancing} & 91.04 & 96.56 & 83.34 & 88.13 & 95.29 & 90.87 & 91.12 \\
        UperNet(ViT-L12$\times$4)(Ours) & 92.22 & 96.98 & 84.75 & \textbf{\textcolor{blue}{88.85}} & 95.92 & 91.75 & 92.17 \\
        UperNet(ViT-H12$\times$4)(Ours) & 92.73 & 96.92 & 85.64 & \textbf{\textcolor{blue}{88.85}} & \textbf{\textcolor{blue}{95.99}} & \textbf{\textcolor{blue}{92.02}} & \textbf{\textcolor{blue}{92.54}} \\
        UperNet(ViT-G12$\times$4)(Ours) & 92.76 & 96.93 & 85.88 & \textbf{\textcolor{red}{89.02}} & \textbf{\textcolor{red}{96.02}} & \textbf{\textcolor{red}{92.12}} & \textbf{\textcolor{red}{92.58}} \\ \hline
       

    \end{tabular}
    }
    \label{tab:potsdam table}
\end{table*} \begin{table*}[t]{\textwidth=0mm}
    \centering
    \caption{the results of class-wise IoU and mIoU on LoveDA. In order to compare the results with the ViTDET with out any module such as ViTAE and RVSA, $\diamondsuit$ is the result re-implemented in mmsegmentation framework using the vision transformer weight published by \cite{wang2022advancing}.}
    \renewcommand{\arraystretch}{1.0}
    {
    \begin{tabular}{l|c c c c c c c | c}
    
        \hline
       
       
       \multirow{2}{*}{Method} & \multicolumn{7}{c|}{Intersection of Union (IoU)} & \multirow{2}{*}{mIoU} \\ \cline{2-8}
        & Background & Building & Road & Water & Barren & forest & Argriculture &  \\ \hline

        FCN8S \cite{long2015fully} & 42.6 & 49.51 & 48.05 & 73.09 & 11.84 & 43.49 & 58.3 & 46.69 \\
        DeepLabV3+ \cite{chen2018encoder} & 42.97 & 50.88 & 52.02 & 74.36 & 10.4 & 44.21 & 58.53 & 47.62 \\
        PAN \cite{li2018pyramid} & 43.04 & 51.34 & 50.93 & 74.77 & 10.03 & 42.19 & 57.65 & 47.13 \\
        UNet \cite{ronneberger2015u} & 43.06 & 52.74 & 52.78 & 73.08 & 10.33 & 43.05 & 59.87 & 47.84 \\
        UNet++ \cite{zhou2018unet++} & 42.85 & 52.58 & 52.82 & 74.51 & 11.42 & 44.42 & 58.8 & 48.2 \\
        Unetformer \cite{wang2022unetformer} & 44.7 & 58.8 & 54.9 & 79.6 & 20.1 & 46 & 62.5 & 46.9 \\
        Semantic-FPN \cite{kirillov2019panoptic} & 42.93 & 51.53 & 53.43 & 74.67 & 11.21 & 44.62 & 58.68 & 48.15 \\
        PSPNet \cite{zhao2017pyramid} & 44.4 & 52.13 & 53.52 & 76.5 & 9.73 & 44.07 & 57.85 & 48.31 \\
        LinkNet \cite{chaurasia2017linknet} & 43.61 & 52.07 & 52.53 & 76.85 & 12.16 & 45.05 & 57.25 & 48.5 \\
        FarSeg \cite{zheng2020foreground} & 43.09 & 51.48 & 53.85 & 76.61 & 9.78 & 43.33 & 58.9 & 48.15 \\
        FactSeg \cite{ma2021factseg} & 42.6 & 53.63 & 52.79 & 76.94 & 16.2 & 42.92 & 57.5 & 48.94 \\
        HRNet \cite{wang2020deep} & 44.61 & 55.34 & 57.42 & 73.96 & 11.07 & 45.25 & 60.88 & 49.79 \\
        UperNet(ViTAE-B + RVSA) \cite{wang2022advancing} & \textbf{\textcolor{blue}{46.69}} & 58.14 & 57.12 & \textbf{\textcolor{blue}{79.66}} & 16.55 & 46.46 & 62.44 & 52.44 \\ \hline
        UperNet(ViT-B12$\times$1)$\diamondsuit$\cite{wang2022advancing} & 45.69 & 58.75 & 56.7 & 76.56 & 10.56 & \textbf{\textcolor{red}{48.52}} & 62.16 & 51.28 \\
        UperNet(ViT-L12$\times$4) & 46.17 & \textbf{\textcolor{blue}{60.56}} & 57.26 & 76.95 & 16.05 & 47.5 & 62.17 & 52.38 \\
        UperNet(ViT-H12$\times$4) & 46.64 & 59.79 & \textbf{\textcolor{blue}{58.36}} & 79.54 & \textbf{\textcolor{blue}{17.56}} & \textbf{\textcolor{blue}{47.88}} & \textbf{\textcolor{blue}{62.61}} & \textbf{\textcolor{blue}{53.2}} \\
        UperNet(ViT-G12$\times$4) & \textbf{\textcolor{red}{47.57}} & \textbf{\textcolor{red}{61.6}} & \textbf{\textcolor{red}{59.91}} & \textbf{\textcolor{red}{81.79}} & \textbf{\textcolor{red}{18.6}} & 47.3 & \textbf{\textcolor{red}{64}} & \textbf{\textcolor{red}{54.4}} \\ \hline
       

    \end{tabular}
    }
    \label{tab:loveda table}
\end{table*} 
\subsubsection{Dataset}
We evaluate pretrained models for the semantic segmentation downstream task using two benchmark datasets: Potsdam\footnote{\url{https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx}} and LoveDA.

\textbf{Potsdam}. The Potsdam dataset, published by ISPRS, consists of 38 high-definition images with a pixel resolution of 0.5m. Each image has a fixed pixel size of 6000$\times$6000 and is divided into 24 training images and 14 test images. This dataset is designed for semantic segmentation tasks, classifying six classes at the pixel level: impervious surfaces, buildings, low vegetation, trees, cars, and clutter. The evaluation criteria for the Potsdam dataset are the F1 score and overall accuracy, excluding the clutter category. The dataset is fully published, allowing models to be trained with training images and locally evaluated using test images.

\textbf{LoveDA}. The LoveDA dataset, designed for domain adaptation, features a pixel resolution of 0.3m and consists of 2522 training, 1669 validation, and 1796 test images. The dataset is divided into two areas, rural and urban, and includes seven classes: buildings, roads, water, barren land, forests, agriculture, and background. However, the background class is excluded during learning and evaluation. To demonstrate general performance, we construct the training and test datasets according to the official criteria, disregarding whether images are rural or urban. Unlike Potsdam, performance is estimated using the intersection over union (IoU) across all categories. Since only the test images are published, similar to DOTA v2.0, we evaluate our models by submitting the inference results to the server\footnote{\url{https://codalab.lisn.upsaclay.fr/competitions/421\#learn_the_details-overview}}.

\subsubsection{Implementation Details and Experiment Settings}

\begin{table*}[ht]{\textwidth=0mm}
    \centering
    \caption{the results of F1 score, mF1 score and overall accuracy (OA) for evaluatting sample efficiency in Potsdam. In order to compare the results with the ViTDET with out any module such as ViTAE and RVSA, the ViT-B12$\times$1 is retrained by mmsegmentation framework. The training data of Potsdam is randomly sampled by ratio 0.01, 0.05, 0.1, 0.5, 1.0.}
    \renewcommand{\arraystretch}{1.25}
    {
    \begin{tabular}{c | l | c c c c c | c | c }
        \hline

        \multirow{2}{*}{Sample ratio} & \multirow{2}{*}{Backbone} & \multicolumn{5}{c|}{F1 score per category ($\%$)} & \multirow{2}{*}{mF1 ($\%$)} & \multirow{2}{*}{OA ($\%$)} \\ \cline{3-7}
         &  & Imper. surf. & Building & Low veg. & Tree & Car & & \\ \hline

        \multirow{4}{*}{1$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 78.27 & 84.54 & 71.04 & 74.66 & 87.23 & 79.15 & 79.43 \\
        & ViT-L12$\times$4 & 79.60 & 84.21 & \textbf{\textcolor{red}{75.13}} & \textbf{\textcolor{red}{79.19}} & 88.44 & \textbf{\textcolor{blue}{81.31}} & \textbf{\textcolor{blue}{81.57}} \\
        & ViT-H12$\times$4 & \textbf{\textcolor{blue}{80.07}} & \textbf{\textcolor{red}{85.43}} & \textbf{\textcolor{red}{75.13}} & 78.19 & \textbf{\textcolor{blue}{88.95}} & \textbf{\textcolor{red}{81.56}} & \textbf{\textcolor{red}{81.78}} \\
        & ViT-G12$\times$4 & \textbf{\textcolor{red}{80.28}} & \textbf{\textcolor{blue}{85.10}} & \textbf{\textcolor{blue}{74.87}} & \textbf{\textcolor{blue}{78.42}} & \textbf{\textcolor{red}{89.13}} & \textbf{\textcolor{red}{81.56}} & \textbf{\textcolor{red}{81.78}} \\ \hline

        \multirow{4}{*}{5$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 86.15 & \textbf{\textcolor{blue}{92.47}} & 77.14 & 83.49 & 92.35 & 86.32 & 86.34 \\
        & ViT-L12$\times$4 & \textbf{\textcolor{blue}{88.09}} & 92.18 & 81.77 & 84.69 & 93.32 & 88.01 & \textbf{\textcolor{blue}{88.24}} \\
        & ViT-H12$\times$4 & 87.97 & 92.29 & \textbf{\textcolor{blue}{81.86}} & \textbf{\textcolor{blue}{85.05}} & \textbf{\textcolor{blue}{93.75}} & \textbf{\textcolor{blue}{88.19}} & 88.17 \\
        & ViT-G12$\times$4 & \textbf{\textcolor{red}{88.59}} & \textbf{\textcolor{red}{92.85}} & \textbf{\textcolor{red}{82.12}} & \textbf{\textcolor{red}{85.98}} & \textbf{\textcolor{red}{94.24}} & \textbf{\textcolor{red}{88.82}} & \textbf{\textcolor{red}{88.70}} \\ \hline

        \multirow{4}{*}{10$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 87.91 & 93.77 & 79.72 & 84.99 & 93.93 & 88.06 & 88.32 \\
        & ViT-L12$\times$4 & 89.37 & \textbf{\textcolor{red}{94.49}} & 82.30 & 86.14 & 94.13 & 89.29 & 89.70 \\
        & ViT-H12$\times$4 & \textbf{\textcolor{red}{89.75}} & \textbf{\textcolor{blue}{94.44}} & \textbf{\textcolor{blue}{83.33}} & \textbf{\textcolor{blue}{86.15}} & \textbf{\textcolor{red}{94.89}} & \textbf{\textcolor{blue}{89.71}} & \textbf{\textcolor{red}{90.21}} \\
        & ViT-G12$\times$4 & \textbf{\textcolor{blue}{89.65}} & 94.39 & \textbf{\textcolor{red}{83.80}} & \textbf{\textcolor{red}{86.52}} & \textbf{\textcolor{blue}{94.77}} & \textbf{\textcolor{red}{89.83}} & \textbf{\textcolor{blue}{90.11}} \\ \hline

        \multirow{4}{*}{50$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 89.65 & 96.20 & 80.96 & 87.26 & 95.15 & 89.84 & 89.95 \\
        & ViT-L12$\times$4 & 92.26 & 96.55 & 85.01 & \textbf{\textcolor{blue}{88.34}} & 96.00 & 91.63 & 92.01 \\
        & ViT-H12$\times$4 & \textbf{\textcolor{red}{92.52}} & \textbf{\textcolor{blue}{96.64}} & \textbf{\textcolor{blue}{85.53}} & 88.19 & \textbf{\textcolor{red}{96.13}} & \textbf{\textcolor{blue}{91.80}} & \textbf{\textcolor{blue}{92.17}} \\
        & ViT-G12$\times$4 & \textbf{\textcolor{blue}{92.35}} & \textbf{\textcolor{red}{96.68}} & \textbf{\textcolor{red}{85.77}} & \textbf{\textcolor{red}{88.56}} & \textbf{\textcolor{blue}{96.05}} & \textbf{\textcolor{red}{91.88}} & \textbf{\textcolor{red}{92.41}} \\ \hline

        \multirow{4}{*}{100$\%$} & ViT-B12$\times$1\cite{wang2022advancing} & 91.04 & 96.56 & 83.34 & 88.13 & 95.29 & 90.87 & 91.22 \\
        & ViT-L12$\times$4 & 92.22 & \textbf{\textcolor{red}{96.98}} & 84.75 & \textbf{\textcolor{blue}{88.85}} & 95.92 & 91.75 & 92.17 \\
        & ViT-H12$\times$4 & \textbf{\textcolor{blue}{92.73}} &\textbf{\textcolor{blue}{ 96.92}} & \textbf{\textcolor{blue}{85.64}} & \textbf{\textcolor{blue}{88.85}} & \textbf{\textcolor{blue}{95.99}} & \textbf{\textcolor{blue}{92.02}} & \textbf{\textcolor{blue}{92.55}} \\
        & ViT-G12$\times$4 & \textbf{\textcolor{red}{92.76}} & 96.93 & \textbf{\textcolor{red}{85.88}} & \textbf{\textcolor{red}{89.02}} & \textbf{\textcolor{red}{96.02}} & \textbf{\textcolor{red}{92.12}} & \textbf{\textcolor{red}{92.59}} \\ \hline


    \end{tabular}
    }
    \label{tab:potsdam sample efficiency table}
\end{table*} \begin{table}[ht]
    \centering
    \caption{this table shows the distribution of pixel corresponding to each class for measuring sample efficiency using the Potsdam dataset.}
    \setlength{\tabcolsep}{2.75pt}
    \renewcommand{\arraystretch}{1.25}
    {
    \begin{tabular}{c|c c c c c}
        \hline
        \multirow{2}{*}{Sample ratio} & \multicolumn{5}{c}{The Number of Pixel} \\ \cline{2-6}
        & Imper. surf. & Building & Low veg. & Tree & Car \\ \hline

        1 $\%$ & 2480947 & 1945647 & 1903485 & 996186 & 109314 \\ \hline
        5 $\%$ & 11201138 & 10877050 & 8637963 & 5901539 & 475325 \\ \hline
        10 $\%$ & 22595456 & 20900276 & 18146952 & 11961627 & 1009088 \\ \hline
        50 $\%$ & 112858442 & 113779095 & 93552823 & 58452932 & 5139681 \\ \hline
        100 $\%$ & 227566256 & 222620959 & 187276028 & 119657591 & 10574847 \\ \hline
        
    \end{tabular}
    }
    \label{tab:potsdam label ratio}
\end{table} 
All experiments benchmarking the proposed backbone are performed using mmsegmentation\footnote{\url{https://github.com/open-mmlab/mmsegmentation}}. As shown in \autoref{subsec:vitdet}, the intermediate features of layers 3, 6, 9, and 12 are resampled to achieve a ratio of 4, 2, 1, 0.5 through each scale block. Both datasets share the same hyperparameters, except for the training iteration. We train the semantic segmentation models using the AdamW optimizer with a base learning rate of 0.00006 and weight decay of 0.01. The learning rate schedule applies the polynomial decay rule with a power of 1.0 and a minimum learning rate of 0. Additionally, linear warm-up is performed during the first 1,500 iterations with a ratio of 0.000001. We apply 160k and 80k training iterations for the Potsdam and LoveDA experiments, respectively. As with rotated object detection, we apply a 0.8 layer-wise learning rate decay and a drop path rate of 0.1 to ViTDET. The UperNet \cite{xiao2018unified}, a popular choice with vision transformer series, is used as the semantic segmentation head.

Since the Potsdam dataset has a fixed pixel size of 6000 $\times$ 6000, we crop the training imagery into patches of size 512 $\times$ 512 with a stride of 384 $\times$ 384. For the LoveDA dataset, with images of size 1024 $\times$ 1024, we configure the data pipeline to randomly crop images into 512 $\times$ 512 patches. For both datasets, we apply data augmentation sequentially, including resize and crop with a ratio range of 0.5 to 2.0, horizontal flip, and photometric distortion. To evaluate full scenes, we perform inference with patches of size 512$\times$512 and a stride of 384 $\times$ 384. For overlapped areas, we set the pixel class based on the average of logits.

\begin{figure*}[p!tb]{}
    \centering
    \includegraphics[width=0.9\textwidth]{potsdam.png}
    \includegraphics[width=0.9\textwidth]{loveda.png}
    \caption{Visualization results of the proposed model. The first to third rows are the results of the Potsdam dataset. The images from left to right are image, label, ViT-B12$\times$1, ViT-L12$\times$4, ViT-H12$\times$4, and ViT-G12$\times$4. The red color in Potsdam dataset means the false prediction. The fourth through sixth rows are the results of the LoveDA. Since the label of test dataset in LoveDA is unavailable, the images from left to right are image, ViT-B12$\times$1, ViT-L12$\times$4, ViT-H12$\times$4, and ViT-G12$\times$4.}
    \label{fig:vis_semantic_seg}
\end{figure*}

\subsubsection{Experiment Results}
Tables \autoref{tab:potsdam table} and \autoref{tab:loveda table} present the semantic segmentation performance results for the Potsdam and LoveDA datasets, respectively. \autoref{tab:potsdam sample efficiency table} demonstrates the improvement in data efficiency as the number of model parameters increases, with fine-tuning results on a reduced number of Potsdam samples. The best performance is indicated in red, while the second-best is in blue. We generate datasets with smaller portions by randomly selecting 1\%, 5\%, 10\%, 50\%, and 100\% of the cropped images from the original dataset. The ratio of pixels representing each category is not accurately allocated, so for precise information on pixel ratios for each category, refer to \autoref{tab:potsdam label ratio}.

\textbf{Potsdam}. \autoref{tab:potsdam table} displays the F1 score, mF1 score, and overall accuracy (OA) for various models using the proposed backbone in Potsdam. As observed in the rotated object detection results, our proposed models outperform those pretrained by IMP in terms of mF1 score and overall accuracy. For the proposed models, a positive correlation exists between the number of parameters and both metrics, indicating that increasing the number of parameters tends to improve performance.

\textbf{LoveDA}. \autoref{tab:loveda table} presents the intersection over union (IoU) results for LoveDA. Although the evaluation metric differs from that of the Potsdam dataset, the performance exhibits a similar pattern.


\textbf{Sample Efficiency in Potsdam}. To evaluate sample efficiency, an essential ability for foundation models in semantic segmentation, we conducted experiments on the Potsdam dataset using varying percentages of training data, including 1\%, 5\%, 10\%, 50\%, and 100\%. As the dataset is partitioned based on images, the pixels corresponding to each class are not divided in an exact ratio. \autoref{tab:potsdam label ratio} details the number of pixels representing each class included in each dataset. The models used for evaluating sample efficiency vary only in the backbone, with UperNet as the segmentation head. As observed, the model performance improves with an increase in the number of parameters while maintaining the same sample ratio. In many cases, using only half or a fifth of the dataset results in superior performance, as seen when comparing the 5\% and 10\% datasets to the 50\% dataset.