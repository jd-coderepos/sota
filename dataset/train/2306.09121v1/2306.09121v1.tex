\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}


\usepackage{adjustbox}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{xspace}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\iid}{i.\,i.\,d.}
\newcommand{\noniid}{non-\iid}
\newcommand{\tXi}{\bm{\Xi}}
\newcommand{\GraphMLP}{Graph-MLP\xspace}
\DeclareMathOperator*{\concat}{\scalebox{1}[1.5]{}}

\definecolor{forestgreen}{rgb}{0, 0.6, 0.32}
\newcommand\good[1]{\textcolor{forestgreen}{#1}}
\newcommand\bad[1]{\textcolor{red}{#1}}
\usepackage{orcidlink}







\begin{document}
\title{The Split Matters: Flat Minima Methods for Improving the Performance of GNNs}
\titlerunning{Flat Minima Methods for GNNs}
\author{Nicolas Lell\orcidlink{0000-0002-6079-6480}
\and
Ansgar Scherp\orcidlink{0000-0002-2653-9245}}
\authorrunning{N. Lell and A. Scherp}
\institute{Universit√§t Ulm, Germany
\email{\{nicolas.lell, ansgar.scherp\}@uni-ulm.de}}
\maketitle              


\begin{abstract}
When training a Neural Network, it is optimized using the available training data with the hope that it generalizes well to new or unseen testing data.
At the same absolute value, a flat minimum in the loss landscape is presumed to generalize better than a sharp minimum.
Methods for determining flat minima have been mostly researched for independent and identically distributed (i.\,i.\,d.) data such as images. 
Graphs are inherently non-i.\,i.\,d. since the vertices are edge-connected. 
We investigate flat minima methods and combinations of those methods for training graph neural networks (GNNs). 
We use GCN and GAT as well as extend Graph-MLP to work with more layers and larger graphs. 
We conduct experiments on small and large citation, co-purchase, and protein datasets with different train-test splits in both the transductive and inductive training procedure.
Results show that flat minima methods can improve the performance of GNN models by over 2 points, if the train-test split is randomized. 
Following Shchur et al., randomized splits are essential for a fair evaluation of GNNs, as other (fixed) splits like ``Planetoid'' are biased. Overall, we provide important insights for improving and fairly evaluating flat minima methods on GNNs.
We recommend practitioners to always use weight averaging techniques, in particular EWA when using early stopping.
While weight averaging techniques are only sometimes the best performing method, they are less sensitive to hyperparameters, need no additional training, and keep the original model unchanged. 
All source code is available under \url{https://github.com/Foisunt/FMMs-in-GNNs}.



\end{abstract}

\section{Introduction}


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{trtes.png}
         \caption{Train and test loss}
         \label{fig:trte}
     \end{subfigure}\begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{nosam.png}
         \caption{Train loss without a flat minimum method}
         \label{fig:base}
     \end{subfigure}\begin{subfigure}[b]{0.3\textwidth} 
         \centering
         \includegraphics[width=\textwidth]{sam.png}
         \caption{Train loss after training using SAM}
         \label{fig:sam}
     \end{subfigure}
        \caption{Loss of GCN on CiteSeer with the Planetoid split. Plots following \cite{DBLP:conf/nips/Li0TSG18}.}
        \label{fig:three graphs}
\end{figure*}



Flat minima are regions in the weight space of a neural model where the error function remains largely stable.
It is argued that such larger regions of the error function with a constant, low score correspond to less chance of overfitting of the model and thus show higher generalization performance~\cite{HochreiterS94,flatminima}.
We demonstrate this in Figure \ref{fig:trte}, where we plot the training and testing loss of the same model when changing its weights following a random direction.
In that example, the loss landscapes are shifted between train and test data.
Therefore, finding a flat minimum or choosing a central point in a flat region can lead to better generalization compared to a model with the lowest possible loss in a sharper minimum.
Methods for determining flat minima have been researched in the past largely on toy examples and for data that are independent and identically distributed (\iid) such as images, \eg \cite{swa,Foret21sharpness,zhao2022penalizing,GSAM,orvieto2022anticorrelated,SAF}.

Graph neural networks (GNNs) deal with non-i.\,i.\,d. graph data, since vertices are connected via edges.
GNNs are powerful models but are likewise also known to be difficult to train and susceptible to the training procedure~\cite{Shchur18pitfalls}.
Even small changes in the hyperparameters, data split, etc. can lead to unstable training and lack of generalization performance.


We tackle these challenges of GNNs by transferring flat minima methods to graphs.
We consider a wide selection of weight-averaging and sharpness-aware flat minima methods, 
including the well known methods SWA~\cite{swa} and SAM~\cite{Foret21sharpness}, and lesser known or new ones like
Anticorrelated Perturbed Gradient Descent~\cite{orvieto2022anticorrelated}, Penalizing Gradient Norm~\cite{zhao2022penalizing}, and Sharpness Aware Training for Free~\cite{SAF}.
We also apply existing and new combinations such as Penalizing Gradient Norm~\cite{orvieto2022anticorrelated} plus ASAM~\cite{Kwon21asam}.
We evaluate the performance of flat minima methods on different GNN architectures using small and large benchmark datasets.
As GNNs, we use the well known Graph Convolutional Network (GCN)~\cite{Kipf17gcn} and Graph Attention Network (GAT)~\cite{Velickovic18gat} as well as the novel \GraphMLP~\cite{graphMLPwoMP}, which operates without the classical message passing.
Regarding the evaluation procedure, we follow Shchur et al.~\cite{Shchur18pitfalls} who warned that on the common benchmark datasets Cora, CiteSeer, and PubMed the train and test splits heavily impact the models' performance and can lead to an arbitrary reranking of similarly good GNNs.
Thus, in addition to the commonly employed (fixed) ``Planetoid'' split~\cite{planetoid16Yang}, we apply two randomized splits on those datasets.


Our results show that in most cases flat minima methods improve the performance.
But the improvement heavily depends on the used model, dataset, dataset split, and flat minima method.
An illustration of the effect of flat minima methods for GNNs can be seen in Figure~\ref{fig:base} showing the training loss surface of a GNN trained \textit{without} any flat minima method versus Figure~\ref{fig:sam} showing the loss surface of the same model but trained \textit{with} SAM.
To the best of our knowledge, we are the first to systematically transfer and analyze the impact of many flat minima methods to non-\iid\ graph data.
Only Kaddour et al.~\cite{Shchur18pitfalls} applied two flat minima methods SAM and SWA to study images, text, and graphs.
Thus, while their study covers multiple domains, it is limited w.r.t. to the number of minima methods used.
In addition, they only consider fixed train/test splits.
Overall, the contributions of this work are:
\begin{itemize}
    \item 
    We have transferred flat minima methods to operate on non-\iid\ graph data.
    We show that they can improve the performance of GNNs.
    
    \item We perform extensive systematic experiments to measure the influence of flat minima methods depending on the GNN architecture, dataset, and data splits.
    We use both, the transductive as well as inductive training procedure.

   \item We demonstrate that using random splits is essential for fairly evaluating not only GNN models but also the flat minima methods.
   
  \item We combine flat minima methods and show that this improves the performance even further.

\end{itemize}


Below, we review flat minima methods and introduce graph neural networks.
In Section~\ref{sec:methods}, we describe in more detail the flat minima methods used in our experiments.
Section~\ref{sec:expap} introduces the experimental apparatus.
The results are reported in Section~\ref{sec:results} and discussed in Section~\ref{sec:discussion}.

\section{Related Work}
\label{sec:RW}

First, we discuss works in the search of finding flat minima.
Second, we introduce graph neural networks and describe representative models, which we use in our experiments.

\subsection{Searching for Flat Minima}

Hochreiter and Schmidhuber \cite{HochreiterS94,flatminima} were among the first who searched for flat minima in neural networks.
They suggest that finding flatter minima leads to simpler neural networks with better generalization performance.

\paragraph*{SAM-based Approaches}
Foret at al.~\cite{Foret21sharpness} introduced a now popular method that improves generalization through the promotion of flatter minima which they call Sharpness Aware Minimization (SAM).
Their idea is to minimize the loss at the approximated worst (adversarial) point in an explicit region around the model's current parameters and they show that SAM improves the performance and robustness to label noise of Convolutional Neural Networks (CNNs).
SAM showed to also improve the performance of Vision Transformers~\cite{DBLP:conf/iclr/ChenHG22} and Language Models~\cite{Bahri21sharpness}.
Some follow up works used SAM to improve performance on other tasks like model compression~\cite{DBLP:journals/corr/abs-2111-12273,DBLP:journals/corr/abs-2205-12694}.
Other follow up work focused on improving the efficiency of SAM \cite{lookSam}.
For example, Brock et al.~\cite{Brock21high} sampled only a subset of each batch to accelerate the adversarial point calculation.

A different line of follow up work focused on improving the performance of SAM. \label{rw:sharpred}
Kwon et al.~\cite{Kwon21asam} introduced Adaptive SAM (ASAM), which compensates the influence of parameter scaling on the adversarial step.
Kim et al.~\cite{DBLP:conf/icml/Kim0HH22} proposed Fisher SAM which replaces the fixed euclidean balls used by SAM with ellipsoids induced by Fisher information.
Zhao et al.~\cite{zhao2022penalizing} proposed a method which penalizes the gradient norm to find flatter optima and show that SAM is a special case of this method.
Zhuang et al.~\cite{GSAM} proposed Surrogate Gap Guided SAM (GSAM), which in addition to the usual SAM objective, also explicitly minimizes the sharpness.

\paragraph*{Averaging Approaches}
One averaging approach is ensembling~\cite{zhou2012ensemble}, which combines multiple models' outputs to a single, usually more accurate prediction.
For example, Devlin et al.~\cite{bert} showed that an ensemble of BERT large models gain roughly  F1 score on SQuAD 1.1 compared to a single model.
\cite{snapshot} proposed a method called Snapshot Ensemble, which averages a single model's predictions at different points during training.

There are also averaging approaches other than ensembling.
Izmailov et al.~\cite{swa} proposed the now well known method Stochastic Weight Averaging (SWA) which averages a single model's weights at different points during training.
There are some follow up works on SWA, for example using SWA in low precision training can close the performance difference, even when using only 8 bits for each parameter and gradient\cite{DBLP:conf/icml/YangZKBWS19}.
Recently, Wortsman et al.~\cite{soup} showed that most of the good models obtained during hyperparameter tuning lie in the same flat region and that averaging those models' weights leads to better performance compared to simply using the best found model.
Further extension and uses of SWA are described by \cite{DBLP:conf/iclr/GuptaSD20,swaRe}.

\paragraph*{Other Approaches}
Perturbed Gradient Descent (PGD) is a version of gradient descent where noise is injected in every epoch. 
This helped to escape from local minima~\cite{pgd_localminima} and saddle points~\cite{pgd_saddle}.
Orvieto et al.~\cite{orvieto2022anticorrelated} proposed a modification of PGD, which they call Anticorrelated PGD (Anti-PGD).
The idea of Anti-PGD is to inject noise in the current epoch, depending on the noise injected in the previous epoch.
They prove for some special problems that this leads the optimizer to the widest optimum and show that it increases performance on benchmark datasets.
Damian et al.~\cite{DBLP:conf/nips/DamianML21} showed that adding noise to the labels when using Stochastic Gradient Descent (SGD) leads to flatter optima and better generalization.

Du et al.~\cite{SAF} proposed a method which they coin Sharpness-Aware Training for Free (SAF).
They consider SAM's adversarial point approximation as too costly, and instead rely on a trajectory loss to reduce sharpness.

\subsection{Graph Neural Networks}

Graph Neural Networks (GNNs) are neural networks that are designed to work with graph data.
That means in addition to the vertex features, a GNN also uses the adjacency information which connects different data points.
In the following, the adjacency matrix is denoted with , the normalized adjacency matrix with  with  and layer 's output with .
Many GNNs follow the message passing architecture~\cite{Hamilton2020graph,DBLP:journals/aiopen/ZhouCHZYLWLS20}, where the current vertex aggregates the features of neighboring vertices to update its own feature vector.
Different aggregation and update methods then lead to different GNNs.
A well known example is the Graph Convolution Network (GCN)~\cite{Kipf17gcn}, where the implementations are inspired by CNNs.
In each layer, every vertex combines its neighbors' features to calculate its output as follows: 
.

Another well known GNN is the Graph Attention Network (GAT)~\cite{Velickovic18gat}, which uses attention to weigh each neighboring vertex's importance for the current vertex.
The attention weights are calculated by , where  is a one layer feed forward network.
Those are then used to calculate vertex 's output with:

where  is the concatenation,  the number of attention heads, and  the -hop neighborhood of vertex  including itself.


These GNNs usually use the whole graph in a single batch, \ie require to load the full graph at once.
This makes it difficult to apply GCN and GAT to very large graphs.
There are different methods to scale GNNs that sample subgraphs and train on those instead of the full graph \cite{ChenZS18,graphSage,graphsaint-iclr20}.
Wu et al.~\cite{Wu19sgcn} propose Simplified GCN, which uses only a single message passing layer with the adjacency matrix to some power instead of multiple iterations with the normal adjacency matrix.

A common issue with the GNNs mentioned so far is over-smoothing~\cite{Hamilton2020graph}, which means that after multiple message passing steps, all vertex representations tend to be very similar.
This can either be avoided by restricting the GNNs to usually only one to three layers or adding residual or skip connections to the model.
The Jumping Knowledge model~\cite{jkn} uses skip connections from every layer to the last layer.
Chen et al.~\cite{gcnII} introduced GCNII which utilizes skip connections from the input layer to every hidden layer.
Both ideas make it possible to gain performance by increasing the model depth up to  layers.

\GraphMLP \cite{graphMLPwoMP} is a GNN approach that is not based on the message-passing architecture.
Rather, \GraphMLP employs a standard Multi Layer Perceptron (MLP) on the vertex features and uses a contrastive loss function on the -th power normalized adjacency matrix .
The neighbor contrastive (NC) loss for vertex  is calculated as

where  is the embedding/intermediate layer output of vertex  and  is a temperature parameter.
Other than GCN and GAT that are full batch by default, \GraphMLP randomly samples a batch from the input graph each epoch.

\section{Flat Minima Methods}
\label{sec:methods}

Here we give a brief introduction for a high level understanding as well as the modified parameter update rules for the flat minima methods used in our work.
For details, we refer to the primary literature.
In the following, we denote the learning rate with  and a model's weights as , \eg for a -layer GAT .
We begin with SAM and works extending SAM, followed by weight-averaging methods.
Finally, we discuss SAF and Anti-PGD.

SAM~\cite{Foret21sharpness} searches for a model that has a region with low loss around it, instead of finding the model with lowest loss.
SAM minimizes the loss of the approximately worst point  in the region of size  around the model.
The adversarial point is approximated via  and is used for the model's training by .

ASAM~\cite{Kwon21asam} considers that a model's parameters can be scaled without changing the loss.
By incorporating the weights' norms into the parameter update, the performance of SAM can be increased.
Formally, ASAM changes SAM's calculation to 
 with  being a normalization operator for the weights.

PGN: The gradient's norm directly corresponds to the sharpness of the model's current weights.
By penalizing the gradient norm (PGN) during training, the models tend to reach flatter optima~\cite{zhao2022penalizing}.
PGN generalizes SAM with the update rule , where  is a new balancing parameter.
We also experiment with a combination of PGN with ASAM, where we use ASAM to calculate , which we call \textbf{PGNA}.

GSAM: SAM only optimizes the worst point in a region around it.
But it might be better to explicitly minimize the sharpness of said region as well.
GSAM~\cite{GSAM} does this by adding a sharpness term to the loss while ensuring that the gradient of the sharpness term does not increase SAM's original loss via an orthogonal projection.
This results in , where  is a balancing parameter.
We also use a variant called \textbf{GASAM}, which uses ASAM to calculate .

SWA~\cite{swa} is based on the observation that models trained using SGD with cyclic or high constant learning rates tend to traverse flat regions of the loss.
As the loss landscapes are slightly shifted between training, validation, and test data, which can be seen in Figure \ref{fig:trte}, the center point of the training loss basin should generalize best.
To exploit this assumption, SWA calculates an average model  by proportionally adding the current weights every -th epoch by , with  being the number of models averaged. 
As we use early stopping following Shchur et al.~\cite{Shchur18pitfalls}, we do not know in advance for how many epochs each model trains.
Thus, different to the original SWA which used predefined compute budgets, we start averaging at epoch  and stop averaging  epochs after early stopping triggered.

EWA: 
Pre-experiments showed that the number of epochs a model trains heavily depend on the GNN architecture, dataset, split, and smoothing method used.
In our case, it ranges from  epochs (GCN on CiteSeer with the 622 split) to about  epochs (\GraphMLP on arXiv).
This makes it hard to choose the  parameter as one ideally only wants to average models that are already close to the optimum.
Therefore, we also experiment with exponential weight averaging (EWA), \ie .
With the introduction of the new hyperparameter , we expect that EWA works well independent of the number of training epochs.

Anti-PGD: Noise can be injected into gradient descent to improve the training through faster escape from saddle points or local minima.
When the loss is in a valley, anti-correlated noise additionally moves the model to a wider section of the valley \cite{orvieto2022anticorrelated}.
The model's weights are updated by , where  is a random tensor with variance .
After training the model for some epochs with noise injection, the noise injection is stopped for the remaining training to improve convergence of the model.


SAF: Since SAM computes two gradients, it uses about twice the time per weight update compared to standard SGD.
As mentioned in Section~\ref{rw:sharpred}, there are some methods to reduce the impact of computing the additional gradient, but SAF~\cite{SAF} removes the second gradient calculation all together.
Instead it approximates the sharpness by the change of the model output over the epochs.
Specifically, a new trajectory loss  is added to the normal loss.
In that case  is the Kullback‚ÄìLeibler divergence,  a batch,  is the model's output at the current epoch ,  is the model's output  epochs ago,  is a temperature, and  the loss weight.


\section{Experimental Apparatus}
\label{sec:expap}
In this section, we present our experimental apparatus, \ie the used datasets, models, procedure, and measures.

\subsection{Datasets}
\label{sec:ds}

We use different benchmark datasets to evaluate the flat minima methods.
Table~\ref{tab:datasets} reports statistics of the datasets.
Cora~\cite{CoraCiteseer}, CiteSeer~\cite{CoraCiteseer}, PubMed~\cite{pubmed}, and OGB arXiv~\cite{ogb} are citation graphs. 
Amazon Computers and Amazon Photo~\cite{Shchur18pitfalls} are co-purchase graphs.
For these datasets the task is single label vertex classification.
Protein Protein Interaction (PPI)~\cite{PPI} is a collection of 24 protein graphs with 20 of those used for training and 2 each for validation and testing.
The task for PPI is multi label vertex classification.
There are  different labels with each vertex having between  and  labels, an average of  labels.


The ``Planetoid'' (in tables ``plan'') train-test split~\cite{planetoid16Yang} is often used for Cora, CiteSeer, and PubMed.
It is a fixed split with  vertices per class for training,  vertices for validation, and  for testing.
Shchur et al.~\cite{Shchur18pitfalls} showed that changing the train-test split can arbitrarily rerank GNN methods of similar performance.
Thus, we also use two other kinds of randomly generated splits that we also use for the Computers and Photo datasets.
The random Planetoid ``ra-pl'' split follows \cite{Shchur18pitfalls} with  vertices per class for training,  per class for validation, and all other vertices for testing.
The ``622'' split, for example used in \cite{gcnII}, consists of  of the vertices for training,  for validation, and  for testing.

For OGB arXiv, we use the default training (paper before 2018), validation (paper from 2018), and test split (paper after 2018).
We add reverse and self edges, making the graph essentially undirected, which is needed for good performance.
This increases the number of edges from  to .
Note that reverse edges are already included by default in the other benchmark datasets.
Table~\ref{tab:ds_splits} summarizes all used splits.

OGB arXiv is used in the transductive setting, \ie all vertex features and edges are available during training.
PPI is used in the inductive setting, \ie no validation and test vertices and edges are used during training.
For the other dataset we use both settings.
However, we do not use the ra-pl split in the inductive setting.
The reason is that the induced subgraph over the 20 vertices drawn per class in the ra-pl split typically results in no connected vertices.
Thus, there are no edges in the subgraph for training, which renders this split ineffective.

\begin{table}[!ht]
    \centering
    \caption{Datasets used.  is the number of classes and  is the feature size.
    As PPI contains multiple graphs, the sum of vertices and edges is shown here. Number after adding self and reverse edges; number before is .}
    \label{tab:datasets}
\small
    \begin{tabular}{l|r|r|r|r}\toprule
         Dataset  &  &  &  &  \\ \midrule
         Cora     &   &  &    &    \\
         CiteSeer &   &  &    &     \\
         PubMed   &   &    &   &    \\\midrule
         Computers &   &    &   &    \\
         Photo    &   &    &   &    \\\midrule
         arXiv    &  &    &  & \\ 
         PPI      &  &    &  & \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Dataset splits. Besides the fixed ``Planetoid'' split, we use: ``ra-pl'' denotes random splits as used by \cite{Shchur18pitfalls} and ``622'' denotes random  train,  validation, and  test split.}
    \label{tab:ds_splits}
\small    
    \begin{tabular}{l|l|r|r|r}\toprule
          & Split type & Train  & Val     & Test  \\ \midrule
         Cora    & plan    &   &   &  \\
                 &ra-pl&   &   &  \\
                 & 622   & &   &   \\
         CiteSeer& plan    &   &   &  \\
                 &ra-pl&   &   &  \\
                 & 622   & &   &   \\
         PubMed  & plan    &    &   & \\
                 &ra-pl&    &    &\\
                 & 622   && & \\ \midrule
        Computers& ra-pl &  &  & \\
                 & 622 &  &  &  \\
        Photo   & ra-pl &  &  & \\
                 & 622 &  &  & \\ \midrule
         arXiv &default&&&\\ 
       PPI       &default&  &  & \\ \bottomrule
    \end{tabular}
\end{table}



\subsection{Procedure}
\label{sec:proc}

We precompute the random splits of our datasets (ra-pl, 622) such that they are consistent between models and methods.
PPI is used inductively, \ie only training vertices and edges connecting those are used for training. 
arXiv is used transductively, \ie labeled training vertices are available together with the other vertices but without labels.
We use both setups for the other datasets.
The actual experimental procedure is then executed in two steps.
First, we optimize the GNN models (GCN, GAT, \GraphMLP) in a traditional way without any flat minima methods as described in Section~\ref{sec:hyper}.
Second, using the hyperparameters fixed in the first step, we we add the flat minima methods and only optimize their respective hyperparameters(again described in Section~\ref{sec:hyper}).
For both hyperparameter searches, only the training and validation sets are used.
Subsequently, we evaluate the models.
Additionally, we combine promising flat minima methods (without further hyperparameter tuning) on a subset of the datasets.
We run multiple repeats with different seeds for each of our experiments.
For the smaller datasets, we use 100 repeats, and 10 repeats for the arXiv and PPI datasets.
We report the mean performance of the GNN models averaged over those repeats.
For the flat minima methods, we report the difference to the respective GNN model.
This allows for fast visual assessment of the results.
In addition, we the report the standard deviation over the different runs for all models.




\subsection{Hyperparameters}
\label{sec:hyper}
We tune the GNN hyperparameters, fix them, and then tune the flat minima methods' hyperparameters.
We optimized all hyperparameters individually per setting (in-, transductive), dataset, and split.
In summary, we use early stopping with a patience of 100 epochs, two to three layer models with a hidden size smaller or equal to 256 for the small datasets and slightly modify \GraphMLP.
For PPI and arXiv we use deeper (up to ten layers) and wider models that also use residual connections.
For the flat minima methods we tune each methods' hyperparameters while keeping the base ones from fixed.
For PGN and GSAM we reuse  we found for SAM and ASAM.
For details and all final values see \ref{appx:hyper}.

\subsection{Metrics}
For the multi-label PPI dataset, we report weighted Macro-F1 scores.
The F1 score is calculated per class and averaged with weights based on the support of each class.
For all other, single-label datasets we report accuracy.


\section{Results}
\label{sec:results}







The results of the transductive experiments are shown in Table~\ref{tab:trans} with standard deviations shown in Table~\ref{tab:transSD}. 
Regarding the base models, we see that on Cora GAT performs best.
On CiteSeer, PubMed, and Photo, \GraphMLP beats the message passing methods by  to  points.
On Computers \GraphMLP is the best model when using the 622 split but the worst model when using the ra-pl split.
On arXiv GCN performs best with a  point lead over GAT.
Regarding the different splits, we can see that compared to the Planetoid split the performance is lower on the ra-pl and higher on the 622 split.
Regarding the flat minima methods, we observe that there is no method that always works best.
The largest improvement is over  points on the CiteSeer ra-pl split with GAT+EWA.
On arXiv all non-weight averaging methods improve the performance of GCN, but only SAF for GAT.
For \GraphMLP on arXiv, EWA improves the performance by  points.
There are also some bad combinations.
For example, ASAM reduces the performance of GAT in most cases.

\setlength{\tabcolsep}{1.4pt}
\begin{table*}[!ht]
    \centering
    
    \caption{Transductive mean accuracy per split and dataset. Note: The minima method PGNA is a combination of PGN+ASAM. GASAM combines GSAM and ASAM.
    For the SD over the 100 and 10 runs, we refer to Table \ref{tab:transSD}.}
    \label{tab:trans}
    \scriptsize
    \resizebox{0.99\textwidth}{!}{

    \begin{tabular}{l|r r r | r r r | r r r | r r | r r | r}\toprule
    Dataset   & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{CiteSeer} & \multicolumn{3}{c|}{PubMed} & \multicolumn{2}{c|}{Computer} & \multicolumn{2}{c|}{Photo} & arXiv \\
    Split     & plan & ra-pl & 622 & plan & ra-pl & 622 & plan & ra-pl & 622 & ra-pl & 622 & ra-pl & 622 & - \\ \midrule
   GCN &  &  &  &  &  &  &  &  &  &  &  &  &  & \\   \hline
 +SAM & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \bad{} & \good{}\\
 +ASAM & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \bad{} & \good{}\\
 +PGN & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{}\\
 +PGNA & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{}\\
 +GSAM & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \bad{} & \good{} \\
 +GASAM & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \bad{} & \good{}\\  \hline
 +SWA & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \bad{} \\
 +EWA & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} \\ \hline
 +Anti-PGD & \bad{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} \\
 +SAF & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{}\\

 
 \hline 
\midrule

  GAT &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
  \hline
 +SAM & \bad{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \bad{} & \good{} & \bad{} \\
 +ASAM & \bad{} & \bad{} & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \bad{} & \bad{} &  \bad{} & \good{} & \bad{} & \good{} & \bad{} \\
 +PGN & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} \\
 +PGNA & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \bad{} \\
 +GSAM & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} &  \bad{} \\
 +GASAM & \bad{} & \good{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} &  \bad{} \\
 \hline
 +SWA & \bad{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \bad{} \\
 +EWA & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} \\
  \hline
 +Anti-PGD & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} \\
 +SAF & \bad{} & \bad{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} \\
  \hline
 
\midrule
   \GraphMLP &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
  \hline
 +SAM & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} \\
 +ASAM & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} \\
 +PGN & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} &  \good{} & \good{} &  \good{} \\
 +PGNA & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} \\
 +GSAM & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} \\
 +GASAM & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} \\ \hline
 +SWA & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} \\
 +EWA & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} \\  \hline
 +Anti-PGD & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} \\
 +SAF & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} \\

    \bottomrule
    \end{tabular}}
\end{table*}

 

\begin{table*}[!ht]
    \centering
        \caption{Inductive mean accuracy per split and dataset. For PPI we report weighted Macro-F1 scores. Note: The minima method PGNA is a combination of PGN+ASAM. GASAM combines GSAM and ASAM. For the SD over the 100 and 10 runs, we refer to Table \ref{tab:indSD}.}
    \label{tab:ind}
   \footnotesize
       \resizebox{0.8\textwidth}{!}{

    \begin{tabular}{l|r r | r r | r r | r | r | r }\toprule
    Dataset       & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c|}{PubMed} & Comp.. & Photo & PPI\\
    Split         & plan & 622 & plan & 622 & plan & 622 & 622 & 622 & -\\ \midrule

  GCN &  &  &  &  &  &  &  &  & \\
 + SAM   & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{}\\
 + ASAM  & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{}\\
 + PGN   & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{}\\
 + PGNA  & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{}\\
 + GSAM  & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{}\\
 + GASAM & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{}\\ \hline
 + SWA   & \bad{} & \good{} & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \bad{}\\
 + EWA   & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{}\\ \hline
 + Anti-PGD   & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} \\
 + SAF   & \bad{} & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \bad{} & \good{}\\ 


\hline
\midrule

  GAT &  &  &  &  &  &  &  &  & \\
 + SAM & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{}\\
 + ASAM & \bad{} & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{}\\
 + PGN & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{}\\
 + PGNA & \bad{} & \good{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{}\\
 + GSAM & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{}\\
 + GASAM & \bad{} & \good{} & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{}\\ \hline
 + SWA & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{}\\
 + EWA & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{}\\ \hline
 + Anti-PGD & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{}\\
 + SAF & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} \\

\hline
\midrule

  \GraphMLP &  &  &  &  &  &  &  &  & \\ \hline
 + SAM & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{}\\
 + ASAM & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \bad{}\\
 + PGN & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{}\\
 + PGNA & \good{} & \bad{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{}\\
 + GSAM & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{}\\
 + GASAM & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \bad{}\\ \hline
 + SWA & \bad{} & \good{} & \bad{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \bad{}\\
 + EWA & \bad{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{}\\ \hline
 + Anti-PGD & \good{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{}\\
 + SAF & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{}\\

   \bottomrule
    \end{tabular}}
\end{table*}


Table~\ref{tab:ind} shows the results for the inductive experiments with standard deviations shown in Table~\ref{tab:indSD}.
As explained in Section \ref{sec:ds}, the ra-pl split was not used here.
The performance of most models lies within  point of the performance in the transductive setting.
\GraphMLP drops over  points on Cora and around  points on CiteSeer, Computers, and Photo, but is still the best model for PubMed.
On PPI, GCN is the best model while \GraphMLP has a  point drop in F1 score.
Regarding the flat minima methods, the overall picture is the same.
In many cases the best performing method from the transductive setting still is one of the best ones in the inductive setting.
For example, for GCN on Cora, PubMed, and Computers, the same flat minima method works best in both settings.
In other cases it changes, for example for \GraphMLP on CiteSeer with the Planetoid split now only SAF increases the performance, while in the transductive setting PGN workes best and SAF reduces the performance.
On Computers and Photo nearly all flat minima methods improve the performance.
On PPI SAF works for GCN and all SAM variants improve GAT.
For \GraphMLP on PPI, the effects of the minima methods are mixed. 
For example, SAM reduces the performance by  points, PGN increases it by , and SAF brings a large improvement with  points.



\section{Discussion}
\label{sec:discussion}

\subsection{Key Insights}Regarding the flat minima methods, we can see that there is no method that always works best.
However, for each combination of a base GNN model and dataset, there is at least one flat minima method that improves the performance.
But in many cases some flat minima methods also reduce the performance.
For GCN and \GraphMLP, most methods improve the performance on the citation graphs, while for GAT the results are mixed. 
For the co-purchase dataset Computers and 622 split, all flat minima methods improve the results, while on the ra-pl split most of the methods decrease the performance.
We make a similar observation for the Photo dataset, but this time the ra-pl split is improved by the flat minima methods, except for four methods in combination GAT.
On PPI, the flat minima methods improve the results for GAT while the improvement is mixed for the other GNNs.

When comparing the flat minima methods overall, we notice that the methods extending SAM, \ie ASAM, PGN, and GSAM, do not consistently improve the results more than SAM. 
In most cases, one of the extensions works better than the original SAM, but this depends on the GNN and datasets.
For example, for GAT on the small datasets, SAM does on average not change the performance compared to the base model while ASAM reduces it by  and PGN increases it by  points.
Using ASAM instead of SAM for the adversarial calculation for PGN and GSAM works sometimes better, even though by itself SAM worked better than ASAM.
EWA works better than SWA with the highest improvement in the transductive setting of  points when using EWA with a GCN on the CiteSeer and ra-pl split (see Table~\ref{tab:trans}).
This is likely because early stopping negatively impacts SWA.
SWA's begin and end epoch heavily depend on the model and dataset.
EWA nearly always begins in epoch , ends one epoch after early stopping triggered, and in most cases uses a low  of  to  that favors more recent weights.

Anti-PGD works surprisingly well for a method that just adds noise to the model.
It is usually not the best method but, \eg on the small datasets with GAT it outperforms SAM, ASAM, SWA, EWA, and SAF.
It also reaches the overall highest accuracy on arXiv, which is achieved when applied to GCN.
While SAF is motivated by SAM, it impacts the models' performance differently.
For the small datasets with GCN it is worse than all the SAM-based methods, while with \GraphMLP it is better than all the SAM-based methods.
On arXiv, SAF is the only method that improves GAT's performance.
On PPI, SAF decrease the performance, while the SAM-based methods always improve it. 
The training of GAT with SAF on PPI was unstable and occasionally needed restarts.
SAF's recommended  works for the citation datasets.
However, on PPI with GCN and GAT using , early stopping is triggered at epoch , \ie once SAF starts the performance only decreases.
Training is feasible with lower values, with  GCN+SAF is the best model on PPI.

To the best of our knowledge, the only work who also applied flat minima methods to GNNs is the study by Kaddour et al.~\cite{Shchur18pitfalls}.
As said in the introduction, their work is limited to two flat minimal methods SAM and SWA and they also consider only one fixed train/test split.
We argue that it is crucial to consider randomized splits for a fair evaluation of flat minima methods on GNNs.
Considering the findings of \cite{kaddour2022questions}, they found that SAM works better than SWA on GNNs and that the results are influenced by the dataset and GNN architecture.
In contrast, we found that SAM works better than SWA.
This may be due to the additional randomized splits or the use of early stopping which affects SWA more than other methods.
Another reason may be that \cite{kaddour2022questions} used the original hyperparameter search space of SAM~\cite{Foret21sharpness}, while we additionally consider lower and higher values of . 

For GCN one should use GASAM, for GAT one should use PGN, and for \GraphMLP one should use SAF.
In any case, one should always run one of the weight averaging methods as they do not need additional gradient computations.
In addition, one always obtains the original model without the modifications from the flat minima methods as well.
Finally, in our experiments we use early stopping.
Thus, it is important to decide on the hyperparameter values when to begin and stop averaging in SWA.
This choice of the hyperparameter is easier for EWA, since we start at a fixed epoch to average the weights.
Our hyperparameter search showed that EWA works well when one begins to average soon after the training starts and ending it when early stopping triggers, while using a strong decay value of .
For \GraphMLP, SWA is the preferred flat minima method.
The reasons is that \GraphMLP trains for more epochs and SWA can better adapt the parameter weights.


\subsection{Combining up to Three Flat Minima Methods}
Above we mostly study existing methods, consider with EWA a variant of SWA, and with GASAM and PGNA combinations of two flat minima methods.
As proof of concept, we also further combine different flat minima methods without additional hyperparameter tuning.
As basis we use GCN in the transductive setting.
Table~\ref{tab:comb} shows the results for GCN with combinations of up to three flat minima methods with standard deviations in Table~\ref{tab:combsd}.
This shows that combining methods can increase the performance even further.
For example, on the CiteSeer ra-pl split, combining EWA and GASAM, which is a combination of EWA+GSAM+ASAM, increases the performance by  points.

\begin{table*}[!!ht]
    \centering
    \caption{Transductive GCN with combination of up to three flat minima methods. SDs in tables \ref{tab:transSD} and \ref{tab:combsd}.}
    \label{tab:comb}
    \scriptsize
        \resizebox{\textwidth}{!}{

    \begin{tabular}{l|r r r | r r r | r r r | r r | r r }\toprule
    Dataset   & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{CiteSeer} & \multicolumn{3}{c|}{PubMed} & \multicolumn{2}{c|}{Computer} & \multicolumn{2}{c}{Photo} \\
    Split     & plan & ra-pl & 622 & plan & ra-pl & 622 & plan & ra-pl & 622 & ra-pl & 622 & ra-pl & 622 \\ \midrule

  GCN       &   &   &   &   &   &   &   &   &  &  &  &  &  \\
 + PGNA     & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{}\\
 + GASAM    & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \bad{} & \good{} & \good{} & \bad{} \\ \hline
 + Anti-PGD+SAM  & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \bad{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} \\
 + Anti-PGD+GASAM & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} \\
 + Anti-PGD+SAF  & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} \\
 + EWA+Anti-PGD  & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \bad{} & \good{} & \good{} & \good{} \\
 + EWA+SAM  & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} \\
 + EWA+GASAM & \bad{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \bad{}  \\
 + EWA+SAF  & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \good{} & \bad{} & \good{} & \good{} & \good{} & \good{} & \bad{} \\


    \bottomrule
    \end{tabular}
    }
\end{table*}




\subsection{Influence of Dataset Splits}
Regarding the dataset splits, we can see that the random split ra-pl is more difficult than the often used Planetoid split.
The Planetoid split uses a fixed set of 20 vertices per class as training data.
This makes models more susceptible to overfit the hyperparameters to that specific split than for a randomized ra-pl split.
For the 622 splits, the much higher amount of training data explain the overall better result.
Especially for the PubMed dataset, the amount of training data is larger by a factor of  in the 622 split compared to the Planetoid split. 
In both the transductive and inductive settings, the dataset split impacts the performance one can gain from the flat minima methods.
The increase is on average higher and more consistent on the hardest ra-pl split compared to the other splits.

Shchur et al.~\cite{Shchur18pitfalls} show that randomized splits need to be used for a fair evaluation of GNNs.
We follow their suggestion by using two variants of randomized data splits. 
We confirm their observations that the commonly used ``Planetoid'' split is biased and should not be used on its own.
We extend on this observation and conclude from our experiments that randomized splits are also important for a fair evaluation of the flat minima methods applied to GNNs.



\subsection{Transductive vs. Inductive Training}
The hyperparameters and basic model performance are similar between the transductive and inductive setting.
In most cases the basic performance is within  point.
The ranking of the flat minima methods is similar as well, \ie in many cases the best transductive method also works well in the inductive setting.
The major exception to this is \GraphMLP, discussed below.

\subsection{Detailed Discussion of Graph-MLP}
Compared to the original \GraphMLP~\cite{graphMLPwoMP} our modifications improved it by over  point on Cora and CiteSeer and over  points on PubMed.
The reason is that the original hyperparameter optimization was suboptimal.
For example, we found a larger batch/sample size to be beneficial. 
On the arXiv dataset, we found  to be a critical hyperparameter and setting it outside the recommended  range to  increased the performance by roughly  points.
On PPI, \GraphMLP completely fails.
The main reason for this is PPI's inductive nature, which means that \GraphMLP never uses the validation and testing edges.
This also explains the performance drop of \GraphMLP when we compare the same dataset between the transductive and inductive setting.
The size of the performance drop probably corresponds to the importance of knowing the edges that include testing vertices.
These edges seem to be very important for PPI, quite important for Cora where the performance dropped by over  points, and not very important for PubMed where the drop was smaller than  point, which is similar to the other GNNs.
In the transductive case, information about the edges connected to test vertices is available in training through .


\subsection{Assumptions and Limitations}
We assume that our GNN models provide a fair foundation for evaluating the flat minima methods.
We optimized the hyperparameters and checked the performance of the GNN models with the original works.
GCN and \GraphMLP achieve a performance on all datasets better than the literature~\cite{Kipf17gcn,graphMLPwoMP}.
The performance of our GAT model is slightly lower on CiteSeer, within standard error on Cora and PubMed, and better on PPI, compared with \cite{Velickovic18gat}.
Depending on the hyperparameters, the training of some GAT models on PPI was unstable and required multiple restarts. 

Our study considers the task of vertex classification.
We cover most nuances like small to large datasets with fixed and random splits, training in transductive and inductive settings, and single- and multi-label classification.
There are larger datasets than arXiv, but it is computationally very expensive to tune hyperparameters on these dataset for the GNN models and many flat minima methods considered here.
Beyond vertex classification, future extensions could consider also other tasks such as edge prediction and graph classification.


\section{Conclusion}
\label{sec:conclusion}
Overall our results show that the choice of the best flat minima method depends on the GNN model used and dataset split.
For the realistic and challenging random split datasets (ra-pl, 622), the flat minima methods can improve the GNN model more than on a fixed dataset split.
Shchur et al.~\cite{Shchur18pitfalls} argue for the need of using such random splits to fairly evaluate GNN models.
We extend on this and argue that a realistic assessment of flat minima methods on graph models requires such an evaluation procedure as well.
We observe that combining up to three flat minima methods can even further improve the results.
We recommend to always use weight averaging as SWA and EWA do not need any additional gradient calculations while also producing the original, unmodified models.
When using early stopping, we especially recommend using EWA.



\bibliographystyle{splncs04}
\bibliography{mybibliography}
\clearpage


\section*{Appendix}
\appendix


\section{Hyperparameters}
\label{appx:hyper}
We present the searched and final hyperparameters in this section.
For all experiments, Adam optimizer with PyTorch's default values , , and  is used.
Early stopping with a patience of  epochs and  max epochs is applied.
After pre-experiments, we fixed the learning rates to the respective values and adjacency (edge) dropout to  as well as all parameters not noted in the grid search ranges below.
Unless otherwise noted (\GraphMLP with arXiv and PPI), we did a full grid search over all combinations of the listed hyperparameters.
All final values are reported in Tables~\ref{tab:gcn_params}, \ref{tab:gcn_params_ind} for GCN, in Tables~\ref{tab:gat_params}, \ref{tab:gat_params_ind} for GAT, and in Tables~\ref{tab:gmlp_params}, \ref{tab:gmlp_params_ind} for \GraphMLP.


\begin{table*}[!hb]
    \centering   
        \caption{Optimal hyperparameter values for GCN on transductive tasks.}
    \label{tab:gcn_params}
    \tiny
    \begin{tabular}{l|r r r | r r r | r r r | r r | r r| r }\toprule
    Dataset       & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{CiteSeer} & \multicolumn{3}{c|}{PubMed} & \multicolumn{2}{c|}{Computer} & \multicolumn{2}{c|}{Photo} & arXiv \\
    Split         & pl & ra-pl & 622 & pl & ra-pl & 622 & pl & ra-pl & 622 & ra-pl & 622 & ra-pl & 622 &- \\ \midrule
    input dropout &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    model dropout &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    weight decay  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    norm          & & id & & & id & & id & id & ln & ln & ln & ln & ln & ln \\
    residual con  & & no & & & no & & & no & & no & no & no & no & yes  \\
    num layers    & &  & & &  & & &  & &  &  &  &  &   \\
    hdim          & &  & & &  & &  &  &  &  &  &  &  &  \\
    lr            &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\ \midrule
    SAM     &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    ASAM    &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    PGN   &  &  &  &   &  &   &   &  &  &  &  &  &  &   \\
    PGNA  &  &  &  &   &  &   &   &  &  &  &  &  &  &   \\
    GSAM  &  &  &  &   &  &   &  &  &  &  &  &  &  &  \\
    GASAM  &  &  &  &   &  &   &  &  &   &  &  &  &  &   \\
    SWA begin     &  &  &  & &  & &  &  &  &  &  &  &  &  \\
    SWA end       &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    EWA begin     & &  & & &  & & &  & &  &  &  &  &   \\
    EWA end       & &  & &  &  &  &  &  &  &  &  &  &  &   \\
    EWA   &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    Anti-PGD   &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    Anti-PGD   & &  & &  &  &  &  &  &  &  &  &  &  &   \\
    SAF  &  &  &  &   &  &   & &  & &  &  &  &  &   \\
    SAF     &  &  &  &  &  &  &   &  &  &  &  &  &  &   \\
    \bottomrule
    \end{tabular}

\end{table*}

\begin{table*}[!hb]
    \centering   
        \caption{Optimal hyperparameter values for GCN on inductive tasks.}
    \label{tab:gcn_params_ind}
    \tiny
    \begin{tabular}{l|r r | r r | r r | r | r | r }\toprule
    Dataset       & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c|}{PubMed} & Computers & Photo & PPI\\
    Split         & pl & 622 & pl & 622 & pl & 622 & 622 & 622 & -\\ \midrule
    input dropout &  &  &  &  &  &  &  &  &   \\
    model dropout &  &   &  &  &  &  &  &  &   \\
    weight decay  &  &  &  &  &  &  &  &  &   \\
    norm          & id & id & id & id & id & ln & ln & ln & ln  \\
    residual con           & no & no & no & no & no & no & no & no & yes  \\
    num layers    &  &  &  &  &  &  &  &  &   \\
    hdim          &  &  &  &  &  &  &  &  &   \\
    lr            &  &  &  &  &  &  &  &  &  \\ \midrule
    SAM        &  &  &  &  &  &  &  &  &   \\
    ASAM       &  &  &  &  &  &  &  &  &  \\
    PGN    &  &  &  &  &  &  &  &  &   \\
    PGNA    &  &  &  &  &  &  &  &  &  \\
    GSAM    &  &  &  &  &  &  &  &  &    \\
    GASAM   &  &  &  &  &  &  &  &  &   \\
    SWA begin     &  &  &  &  &  &  &  &  &  \\
    SWA end       &  &  &  &  &  &  &  &  &  \\
    EWA begin     &  &  &  &  &  &  &  &  &  \\
    EWA end       &  &  &  &  &  &  &  &  &  \\
    EWA   &  &  &  &  &  &  &  &  &  \\
    Anti-PGD  &  &  &  &  &  &  &  &  &  \\
    Anti-PGD  &  &  &  &  &  &  &  &  &  \\
    SAF    &  &  &  &  &  &  &  &  &  \\
    SAF       &  &  &  &  &  &  &  &  & \\
    \bottomrule
    \end{tabular}

\end{table*}

\begin{table*}[!hb]
        \centering   
    \caption{Optimal hyperparameter values for GAT on transductive tasks.}
    \label{tab:gat_params}
        \tiny 
    \begin{tabular}{l|r r r | r r r | r r r | r r | r r | r }\toprule
    Dataset       & & Cora & & & CiteSeer & & & PubMed & & \multicolumn{2}{c|}{Computer} & \multicolumn{2}{c|}{Photo} & arXiv \\
    Split         & pl & rand pl & 622 & pl & rand pl & 622 & pl & rand pl & 622 & rp & 622 & rp & 622 &- \\ \midrule
    input dropout &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    model dropout &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    weight decay  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    norm          & ln & id & id & ln & id & id & & ln & & ln & ln & id & ln & bn \\
    residual con           & & no & & & no & & & no & & no & no & no & no & yes  \\
    num layers    & &  & & &  & & &  & &  &  &  &  &  \\
    hdim          &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    lr            &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    attn dropout  & &  & &  &  &  &  &  &  &  &  &  &  &   \\
    num attn head & &  & & &  & & &  & &  &  &  &  &   \\ \midrule
    SAM     &  &  &  &  &  &  &  &  &  &  &  & &  &   \\
    ASAM    &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    PGN   &  &  &  &   &  &   &   &  &  &  &  &  &  &   \\
    PGNA  &  &  &  &   &  &   &   &  &  &  &  &  &  &  \\
    GSAM  &  &  &  &   &  &   &   &  &   &  &  &  &  &   \\
    GASAM &  &  &  &   &  &   &   &  &   &  &  &  &  &   \\
    SWA begin     &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    SWA end       &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    EWA begin     & &  & & &  & & &  & &  &  &  &  &   \\
    EWA end       & &  & & &  & &  &  &  &  &  &  &  &  \\
    EWA   & &  & &  &  &  &  &  &  &  &  &  &  &   \\
    Anti-PGD   &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    Anti-PGD     &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    SAF    &  &  &  &   &  &   & &  & &  &  &  &  &  \\
    SAF       & &  &  &  &  &  &   &  &  &  &  &  &  &   \\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[!ht]
    \centering  
        \caption{Optimal hyperparameter values for GAT on inductive tasks.}
    \label{tab:gat_params_ind}
    \tiny
    \begin{tabular}{l|r r | r r | r r | r | r | r }\toprule
    Dataset       & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c|}{PubMed} & Computers & Photo & PPI\\
    Split         & pl & 622 & pl & 622 & pl & 622 & 622 & 622 & -\\ \midrule
    input dropout &  &  &  &  &  &  &  &  &  \\
    model dropout &  &  &  &  &  &  &  &  &  \\
    weight decay  &  &  &  &  &  &  &  &  &  \\
    norm          & ln & ln & id & ln & ln & ln & ln & ln & id \\
    residual con           & no & no & no & no & no & no & no & no & yes \\
    num layers    &   &   &   &   &   &  &  &  &   \\
    hdim          &  &  &  &  &  &  &  &  &  \\
    lr            & & & & & & &  &  & \\
    attn dropout  &  &  &  &  &  &  &  &  &   \\
    num attn head &    &  &  &  &  &  &  &  &  \\ \midrule
    SAM     &    &  &  &  &  &  &  &  &  \\
    ASAM    &&  &  &  &  &  &  &  &  \\
    PGN   &  &  &  &  &  &  &  &  &  \\
    PGNA  &  &  &  &  &  &  &  &  &  \\
    GSAM  &  &&  &  &  &  &  &  &  \\
    GASAM &&  &  &  &  &  &  &  &  \\
    SWA begin     &   &  &  &  &  &  &  &  &  \\
    SWA end       &  &  &  &  &  &  &  &  &  \\
    EWA begin     &    &  &  &  &  &  &  &  &   \\
    EWA end       &    &  &  &  &  &  &  &  &  \\
    EWA   &  &  &  &  &  &  &  &  &  \\
    Anti-PGD &&&  &  &  &  &  &  &  \\
    Anti-PGD   &  &  &  &  &  &  &  &  &  \\
    SAF  &  &  &  &  &  &  &  &  &  \\
    SAF     &  &  &  &  &  &  &  &  & \\
    \bottomrule
    \end{tabular}

\end{table*}

\begin{table*}[!ht]
    \centering  
    \caption{Optimal hyperparameter values for \GraphMLP on transductive tasks.}
    \label{tab:gmlp_params}
    \tiny
    \begin{tabular}{l|r r r | r r r | r r r | r r | r r | r }\toprule
    Dataset       & & Cora & & & CiteSeer & & & PubMed & & \multicolumn{2}{c|}{Computer} & \multicolumn{2}{c|}{Photo} & arXiv \\
    Split         & pl & rand pl & 622 & pl & rand pl & 622 & pl & rand pl & 622 & rp & 622 & rp & 622 &- \\ \midrule
    input dropout & &  & & &  & & &  & &  &  &  &  &   \\
    model dropout &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    weight decay  &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    norm          & & ln & & & ln & & & ln & & ln & ln & ln & ln & ln  \\
    residual con           & & no & & & no &  & & no & & no & no & no & no & yes \\
    num layers    & &  & & &  & & &  & &  &  &  &  &  \\
    hdim          & &  & & &  & & &  & &  &  &  &  &  \\
    lr            &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    NC @          &  &  &  & &  & & &  & &  &  &  &  &  \\
    NC weight     &  &  &  & &  & & &  & &  &  &  &  &  \\
    tau           &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    r             & &  & & &  & & &  & &  &  &  &  &   \\
    b ( of data)& &  & & &  & & &  & &  &  &  &  &  \\ \midrule
    SAM        &  &  &  &     &  &  &  &  &  &  &  &  &  &   \\
    ASAM       &    &    &    &  &  &    &  &    &  &  &  &  &  &   \\
    PGN    &  &  &  &   &  &   &   &  &  &  &  &  &  &   \\
    PGNA    &  &  &  & &  & &   &  &   &  &  &  &  &   \\
    GSAM    &  &  &  &  &  &  &   &  &   &  &  &  &  &   \\
    GASAM   &  &  &  &  &  &   &   &  &   &  &  &  &  &   \\
    SWA begin     &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    SWA end       &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    EWA begin     & &  & & &  & & &  & &  &  &  &  &   \\
    EWA end       & &  & & &  & & &  & &  &  &  &  &  \\
    EWA   & &  & & &  & &  &  &  &  &  &  &  &  \\
    Anti-PGD   &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    Anti-PGD     & &  & &  &  &  &  &  &  &  &  &  &  &   \\
    SAF    &  &  &  &  &  &  &  &  &  &  &  &  &  &   \\
    SAF       &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    \bottomrule
    \end{tabular}
\end{table*}


\begin{table*}[!ht]
    \centering   
    \caption{Optimal hyperparameter values for GMLP on inductive tasks.}
    \label{tab:gmlp_params_ind}
    \tiny
    \begin{tabular}{l|r r | r r | r r | r | r | r }\toprule
    Dataset       & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c|}{PubMed} & Computers & Photo & PPI\\
    Split         & pl & 622 & pl & 622 & pl & 622 & 622 & 622 & -\\ \midrule
    input dropout &  &  &  &  &  &  &  &  &  \\
    model dropout &  &  &  &  &  &  &  &  &  \\
    weight decay  &  &  &  &  &  &  &  &  &  \\
    norm          & ln & ln & ln & ln & ln & ln & ln & ln & ln\\
    residual con           & no & no & no & no & no & no & no & no  & yes \\
    num layers    &  &  &  &  &  &  &  &  &   \\
    hdim          &  &  &  &  &  &  &  &  &  \\
    lr            &  &  &  &  &  &  &  &  &  \\ 
    NC@           &  &  &  &  &  &  &  &  &  \\
    NC weight     &  &  &  &  &  &  &  &  &  \\
    tau           &  &  &  &  &  &  &  &  &  \\
    r             &  &  &  &  &  &  &  &  &  \\
     b ( of train)&  &  &  &  &  &  &  &  &  \\\midrule
    SAM      &  &  &  &  &  &  &  &  &  \\
    ASAM     &  &  &  &  &  &  &  &  &  \\
    PGN    &  &  &  &  &  &  &  &  &  \\
    PGNA   &  &  &  &  &  &  &  &  &  \\
    GSAM   &  &  &  &  &  &  &  &  &  \\
    GASAM  &  &  &  &  &  &  &  &  &  \\
    SWA begin     &  &  &  &  &  &  &  &  &  \\
    SWA end       &  &  &  &  &  &  &  &  &  \\
    EWA begin     &  &  &  &  &  &  &  &  &  \\
    EWA end       &  &  &  &  &  &  &  &  &  \\
    EWA   &  &  &  &  &  &  &  &  &  \\
    Anti-PGD  &  &  &  &  &  &  &  &  &  \\
    Anti-PGD  &  &  &  &  &  &  &  &  &  \\
    SAF    &  &  &  &  &  &  &  &  &  \\
    SAF       &  &  &  &  &  &  &  &  &   \\
    \bottomrule
    \end{tabular}
\end{table*}

\subsection{Base Models}
\paragraph{Small Datasets}
We use  repeats/seeds for the pre-experiments and parameter search on the small datasets.
We ran pre-experiments to fix some parameters, and then ran a full grid search over the remaining parameter ranges.
For GCN, the searched space is input dropout in , model dropout in , weight decay in , normalization in  ( means no normalization,  layer norm, and  batch norm), and hidden dimension in . 
For Computer and Photo, model dropout was extended by  .
For GAT, the number of attention heads is 8 in the first and 1 in the last layer.
The other parameters are searched with input dropout in , model dropout in , attention dropout in , weight decay in , norm in , and hidden dimension in  (times 8 attention heads).
For Computer and Photo, weight decay was instead searched in .
Different to the original \GraphMLP we use dropout, layer norm, and activations between all layers.
The batch size  to  of each dataset.
The other parameters searched are model dropout in , weight decay in , and  in .
Loss weight and the layer after which the loss is calculated in , where layer  means after the last,  after the penultimate layer and so on.
For Photo and Computer model dropout was extended by ,  by , and the loss was instead searched in .



\paragraph{OGB arXiv}
We added reverse and self edges to the arXiv dataset which increased the accuracy by over  points for most configurations.
We also used deeper models and added residual connections. 
We used  repeats for the arXiv and PPI pre-experiments and parameter selection experiments.
For GCN, we searched input dropout in , model dropout in , and weight decay in .
For GAT, we searched attention dropout in , input dropout in , model drop-out in , and weight decay in .
For \GraphMLP, we did not perform a full grid search over the hyperparameters due to their larger number and \GraphMLP's lower training speed.
After fixing the other hyperparameters, we searched over many combinations of  in ,  in , loss weight in ,  in  input dropout in , and model dropout in .


\paragraph{PPI}
We used deeper models with residual connections for PPI as well. 
For PPI, the threshold to assign a label was chosen as .
For GCN, we searched input dropout in , model dropout in , and weight decay in .
For GAT, we searched attention dropout in , input dropout in , model dropout in , and weight decay in .
For \GraphMLP, we searched with drop input in , model dropout in , weight decay in , loss weight in , and  in , and  in .
Afterwards we searched for  and found  (\ie  of each graph) to be the best value.


\subsection{Flat Minima Methods}

\paragraph{(A)SAM}
Both SAM and ASAM have the parameter  which is usually set higher for ASAM than for SAM~\cite{Kwon21asam}, so we search over  in  for SAM and  in  for ASAM.
On the small data-sets, we saw potential for improvement with higher  and thus additionally searched over  for SAM and  for ASAM.

\paragraph{PGN}
For PGN, we searched  in  in all cases.

\paragraph{GSAM}
For GSAM, we searched  in  for all models.

\paragraph{SWA and EWA}
For both SWA and EWA, we searched all combinations of  in  and  in  where   .
This was done to prevent cases where no models are averaged at all as the lowest observed number of trained epochs from the first hyperparameter search is .
For SWA, we averaged the model every epoch as we used fixed learning rates.
For EWA, we additionally tried the combinations above with  in .

\paragraph{Anti-PGD}
For Anti-PGD, we tried stopping the noise after  epochs and  in .
For the small datasets, we additionally used  in .

\paragraph{SAF}
We always started SAF at epoch  with a epoch difference  of .
We tested all combinations of  in  and  in .
We noticed that the optimal  value often was on the border of that range so, we extended it by  on all datasets except arXiv, additionally by  on the small datasets, and additionally by  on PPI.





\section{Standard Deviations of Results}
Here we present the standard deviations of the main result tables for completeness, as they did not fit into the main tables but we still want to present them for interested readers.
Table \ref{tab:transSD} shows the standard deviations for the transductive results, Table \ref{tab:indSD} for the inductive results, and Table \ref{tab:combsd} for the combination of more flat minima methods.

\begin{table*}[!!ht]
    \centering
\caption{SDs of Table \ref{tab:comb}, combined methods on transductive GCN.}
    \label{tab:combsd}
    \tiny
    \begin{tabular}{l|r r r | r r r | r r r | r r | r r }\toprule
    Dataset   & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{CiteSeer} & \multicolumn{3}{c|}{PubMed} & \multicolumn{2}{c|}{Computer} & \multicolumn{2}{c}{Photo} \\
    Split     & plan & ra-pl & 622 & plan & ra-pl & 622 & plan & ra-pl & 622 & ra-pl & 622 & ra-pl & 622 \\ \midrule
+ Anti-PGD+SAM &  &  &  &  &  &  &  &  &  &  &  &  &  \\
 + Anti-PGD+ASAM+GSAM &  &  &  &  &  &  &  &  &  &  &  &  &  \\
 + Anti-PGD+SAF &  &  &  &  &  &  &  &  &  &  &  &  &  \\
 + EWA+Anti-PGD &  &  &  &  &  &  &  &  &  &  &  &  &  \\
 + EWA+SAM &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + EWA+ASAM+GSAM &  &  &  &  &  &  &  &  &  &  &  &  &  \\
 + EWA+SAF &  &  &  &  &  &  &  &  &  &  &  &  &  \\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[!ht]
    \centering    
    \caption{Standard deviations for Table \ref{tab:trans}; 10 repeats on arXiv and 100 all other datasets}
    \label{tab:transSD}
    \tiny 
    \begin{tabular}{l| rrr | rrr | rrr | rr | rr | r}\toprule
    Dataset   & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c|}{CiteSeer} & \multicolumn{3}{c|}{PubMed} & \multicolumn{2}{c|}{Computers} & \multicolumn{2}{c|}{Photo} & arXiv \\
    Split     & plan & ra-pl & 622 & plan & ra-pl & 622 & plan & ra-pl & 622 & ra-pl & 622 & ra-pl & 622 & -\\ \midrule
  GCN    &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
 + SAM   &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + ASAM  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + PGN   &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + PGNA  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + GSAM  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + GASAM &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + SWA   &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + EWA   &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + Anti-PGD &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + SAF   &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 \midrule
  GAT   &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + SAM  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + ASAM &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + PGN  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + PGNA &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + GSAM &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + GASAM&  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + SWA  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + EWA  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + Anti-PGD &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + SAF  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
\midrule
  \GraphMLP &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + SAM &  &  &  &  &  &  &  &  &  &  &  &  &   & \\
 + ASAM &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + PGN &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + PGNA &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + GSAM &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + GASAM &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + SWA &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + EWA &  &  &  &  &  &  &  &  &  &  &  &  &  & \\ \hline
 + Anti-PGD &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
 + SAF &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
    \bottomrule
    \end{tabular}
\end{table*}




\begin{table*}[!ht]
    \centering
        \caption{Standard deviations for Table \ref{tab:ind}; 10 repeats on PPI and 100 all other datasets}
    \label{tab:indSD}
\tiny
    \begin{tabular}{l|r r | r r | r r | r | r | r }\toprule
    Dataset       & \multicolumn{2}{c|}{Cora} & \multicolumn{2}{c|}{CiteSeer} & \multicolumn{2}{c|}{PubMed} & Computers & Photo & PPI\\
    Split         & plan & 622 & plan & 622 & plan & 622 & 622 & 622 & -\\ \midrule

  GCN &  &  &  &  &  &  &  &  & \\
 + SAM   &  &  &  &  &  &  &  &  & \\
 + ASAM  &  &  &  &  &  &  &  &  & \\
 + PGN   &  &  &  &  &  &  &  &  & \\
 + PGNA  &  &  &  &  &  &  &  &  & \\
 + GSAM  &  &  &  &  &  &  &  &  & \\
 + GASAM &  &  &  &  &  &  &  &  & \\ \hline
 + SWA   &  &  &  &  &  &  &  &  & \\
 + EWA   &  &  &  &  &  &  &  &  & \\ \hline
 + Anti-PGD   &  &  &  &  &  &  &  &  &  \\
 + SAF   &  &  &  &  &  &  &  &  & \\ 

\hline
\midrule
  GAT &  &  &  &  &  &  &  &  & \\
 + SAM &  &  &  &  &  &  &  &  & \\
 + ASAM &  &  &  &  &  &  &  &  & \\
 + PGN &  &  &  &  &  &  &  &  & \\
 + PGNA &  &  &  &  &  &  &  &  & \\
 + GSAM &  &  &  &  &  &  &  &  & \\
 + GASAM &  &  &  &  &  &  &  &  & \\ \hline
 + SWA &  &  &  &  &  &  &  &  & \\
 + EWA &  &  &  &  &  &  &  &  & \\ \hline
 + Anti-PGD &  &  &  &  &  &  &  &  & \\
 + SAF &  &  &  &  &  &  &  &  &  \\

\hline
\midrule
  \GraphMLP &  &  &  &  &  &  &  &  & \\ \hline
 + SAM &  &  &  &  &  &  &  &  & \\
 + ASAM &  &  &  &  &  &  &  &  & \\
 + PGN &  &  &  &  &  &  &  &  & \\
 + PGNA &  &  &  &  &  &  &  &  & \\
 + GSAM &  &  &  &  &  &  &  &  & \\
 + GASAM &  &  &  &  &  &  &  &  & \\ \hline
 + SWA &  &  &  &  &  &  &  &  & \\
 + EWA &  &  &  &  &  &  &  &  & \\ \hline
 + Anti-PGD &  &  &  &  &  &  &  &  & \\
 + SAF &  &  &  &  &  &  &  &  & \\

   \bottomrule
    \end{tabular}
\end{table*}








\end{document}
