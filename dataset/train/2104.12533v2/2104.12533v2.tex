\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{Visformer: The Vision-friendly Transformer}

\author{Zhengsu Chen\textsuperscript{1}, Lingxi Xie\textsuperscript{2}, Jianwei Niu\textsuperscript{1}, Xuefeng Liu\textsuperscript{1}, Longhui Wei\textsuperscript{3}, Qi Tian\textsuperscript{4}\\
\textsuperscript{1}Beihang University, \textsuperscript{2}Johns Hopkins University, \textsuperscript{3}Peking University, \textsuperscript{4}Xidian University\\
{\tt\small danczs@buaa.edu.cn}, {\tt\small 198808xc@gmail.com}, {\tt\small niujianwei@buaa.edu.cn},\\
{\tt\small liu\_xuefeng@buaa.edu.cn}, {\tt\small longhuiwei@pku.edu.cn}, {\tt\small  }
}





\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}


The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the `Vision-friendly Transformer'. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at \url{https://github.com/danczs/Visformer}.
\end{abstract}

\section{Introduction}

In the past decade, convolution used to play a central role in the deep learning models~\cite{lecun2015deep, simonyan2014very, szegedy2015going, he2016deep} for visual recognition. This situation starts to change when the Transformer~\cite{vaswani2017attention}, a module that originates from natural language processing~\cite{vaswani2017attention, devlin2018bert,radford2018improving}, is transplanted to the vision scenarios. It was shown in the ViT model~\cite{dosovitskiy2020image} that an image can be partitioned into a grid of patches and the Transformer is directly applied upon the grid as if each patch is a visual word. ViT requires a large amount of training data (\textit{e.g.}, the ImageNet-21K~\cite{deng2009imagenet} or the JFT-300M dataset), arguably because the Transformer is equipped with long-range attention and interaction and thus is prone to over-fitting. The follow-up efforts~\cite{touvron2020training} improved ViT to some extent, but these models still perform badly especially under limited training data or moderate data augmentation compared with convolution-based models.

On the other hand, vision Transformers can achieve much better performance than convolution-based models when trained with large amount of data. Namely, vision Transformers have higher `upper-bound' while convolution-based models are better in `lower-bound'. Both upper-bound and lower-bound are important properties for neural networks. Upper-bound is the potential to achieve higher performance and lower-bound enables networks to perform better when trained with limited data or scaled to different complexity.

\begin{table}
\begin{center}
\setlength{\tabcolsep}{0.12cm}
\begin{tabular}{|l|l|c|c|c|}
\hline
\multicolumn{2}{|l|}{Network} & ResNet-50 & DeiT-S & Visformer-S \\
\hline\hline
\multicolumn{2}{|l|}{FLOPs (G)} & 4.1 & 4.6 & 4.9 \\
\hline
\multicolumn{2}{|l|}{Parameters (M)} & 25.6 & 21.8 & 40.2 \\
\hline\hline
Full & base setting  & 77.43 & 63.12 & 77.20 \\
\cline{2-5}
data & elite setting & 78.73 & 80.07 & 82.28 \\
\hline
Part of & 10\% labels & 58.37 &40.41 & 58.74 \\
\cline{2-5}
data & 10\% classes  & 89.90  &80.06  &  90.06 \\
\hline
\end{tabular}
\end{center}
\caption{The comparison among ResNet-50, DeiT-S, and the proposed Visformer-S model in ImageNet classification. Although DeiT-S performs well under the elite setting, its performance drops dramatically when the base setting is used or when fewer data are used for training. In comparison, Visformer-S is friendly to both the base and elite settings, and reports smaller accuracy drops using a limited number of training data. Please refer to the main texts for the detailed settings.}
\label{tab:introduction}
\end{table}

Based on the observation of lower-bound and upper-bound on Transformer-based and convolution-based networks, the main goal of this paper is to identify the reasons behind the difference, by which we can design networks with higher lower-bound and upper-bound. The gap between Transformer-based and convolution-based networks can be revealed with two different training settings on ImageNet. The first one is the base setting. It is the standard setting for convolution-based models, \textit{i.e.}, the training schedule is shorter and the data augmentation only contains basic operators such as random-size cropping~\cite{szegedy2016rethinking} and flipping. The performance under this setting is called \textbf{base performance} in this paper. The other one is the training setting used in~\cite{touvron2020training}. It is carefully tuned setting for Transformer-based models, \textit{i.e.}, the training schedule is longer and the data augmentation is stronger (\textit{e.g.}, RandAugment~\cite{cubuk2020randaugment}, CutMix~\cite{yun2019cutmix}, \textit{etc.}, have been added). We use the \textbf{elite performance} to refer to the accuracy produced by it.





We take DeiT-S~\cite{touvron2020training} and ResNet-50~\cite{he2016deep} as the examples of Transformer-based and convolution-based models. As shown in Table~\ref{tab:introduction}, Deit-S and ResNet-50 employ comparable FLOPs and parameter. However, they behave very differently trained on the full data under these two settings. Deit-S has higher elite performance, but changing the setting from elite to base can cause a  accuracy drop for DeiT-S. ResNet-50 performs much better under the base setting, yet the improvement for the elite setting is merely . This motivates us to study the difference between these models. With these two settings, we can roughly estimate the lower-bound and upper-bound of the models. The methodology we use is to perform step-by-step operations to gradually transit one model into another, by which we can identify the properties of modules and design in these two networks. The entire transition process, taking a total of  steps, is illustrated in Figure~\ref{fig:transition}.



Specifically, from DeiT-S to ResNet-50, one should (i) use global average pooling (not the classification token), (ii) introduce step-wise patch embeddings (not large patch flattening), (iii) adopt the stage-wise backbone design, (iv) use batch normalization~\cite{ioffe2015batch} (not layer normalization~\cite{ba2016layer}), (v) leverage  convolutions, (vi) discard the position embedding scheme, (vii) replace self-attention with convolution, and finally (viii) adjust the network shape (\textit{e.g.}, depth, width, \textit{etc.}). After a thorough analysis on the reasons behind the results, we absorb all the factors that are helpful to visual recognition and derive the \textbf{Visformer}, \textit{i.e.}, the Vision-friendly Transformer.



Evaluated on ImageNet classification, Visformer claims better performance than the competitors, DeiT and ResNet, as shown in Table~\ref{tab:introduction}. With the elite setting, the Visformer-S model outperforms DeiT-S and ResNet-50 by  and , respectively, under a comparable model complexity. Different from Deit-S, Visformer-S also survives two extra challenges, namely, when the model is trained with 10\% labels (images) and 10\% classes. Visformer-S even performs better than ResNet-50, which reveals the high lower-bound of Visformer-S. Additionally,  when scaled to tiny level, Visformer-Tiny significantly outperforms Deit-S by more than 6\%.



The contribution of this paper is three-fold. \textbf{First}, for the first time, we introduce the lower-bound and upper-bound to investigate the performance of Transformer-based vision models. \textbf{Second}, we close the gap between the Transformer-based and convolution-based models by a gradual transition process and thus identify the properties of the designs in the Transformer-based and convolution-based models. \textbf{Third}, we propose the Visformer as the final model that achieves satisfying lower-bound and upper-bound and enjoys good scalability at the same time.

\section{Related work}

Image classification is a fundamental task in computer vision. In the deep learning era, the most popular method is to use deep neural networks~\cite{krizhevsky2012imagenet,simonyan2014very,he2016deep}. One of the fundamental units to build such networks is convolution, where a number of convolutional kernels are used to capture repeatable local patterns in the input image and intermediate data. To reduce the computational costs as well as alleviate the risk of over-fitting, it was believed that the convolutional kernels should be of a small size, \textit{e.g.}, . However, this brings the difficulty for faraway contexts in the image to communicate with each other -- this is partly the reason that the number of layers has been increasing. Despite stacking more and more layers, researchers consider another path which is to use attention-based approaches to ease the propagation of visual information.

Since Transformers achieved remarkable success in natural language processing (NLP)~\cite{vaswani2017attention,devlin2018bert,radford2018improving}, many efforts have been made to introduce Transformers to vision tasks. These works mainly fall into two categories. The first category consists of pure attention models~\cite{ramachandran2019stand,hu2019local, zhao2020exploring, chen2020generative, dosovitskiy2020image, touvron2020training, wang2021pyramid}. These models usually only utilize self-attention and attempt to build vision models without convolutions. However, it is computationally expensive to relate all pixels with self-attention for realistic full-sized images. Thus, there has some interest in forcing self-attention to only concentrate on the pixels in local neighborhoods (\textit{e.g.}, SASA~\cite{ramachandran2019stand}, LRNet~\cite{hu2019local}, SANet~\cite{zhao2020exploring}). These methods replace convolutions with local self-attentions to learn local relations and achieve promising results. However, it requires complex engineering to efficiently apply self-attention to every local region in an image. Another way to solve the complexity problem is to apply self-attention to reduced resolution. These methods either reduce the resolution and color space first~\cite{chen2020generative} or regard image patches rather pixels as tokens (\textit{i.e.}, words)~\cite{dosovitskiy2020image, touvron2020training}. However, resolution reduction and patch flattening usually make it more difficult to utilize the local prior in natural images. Thus, these methods usually obtain suboptimal results~\cite{chen2020generative} or require huge dataset~\cite{dosovitskiy2020image} and heavy augmentation~\cite{touvron2020training}.  

The second category contains the networks built with not only self-attentions but also convolutions. Self-attention was first introduced to CNNs by non-local neural networks~\cite{wang2018non}. These networks aim to capture global dependencies in images and videos. Note that non-local neural networks are inspired by the classical non-local method in vision tasks~\cite{buades2005non} and unlike those in Transformers, the self-attentions in non-local networks are usually not equipped with multi-heads and position embedding~\cite{wang2018non, cao2019gcnet, li2020neural}. Afterwards, Transformers achieve remarkable success in NLP tasks~\cite{devlin2018bert,radford2018improving} and, therefore, self-attentions that inherits NLP settings (\textit{e.g.}, multi-heads, position encodings, classification token, \textit{etc.}) are combined with convolutions to improve vision tasks~\cite{ramachandran2019stand, dosovitskiy2020image}. A common combination is to utilize convolutions first and apply self-attention afterwards~\cite{dosovitskiy2020image, srinivas2021bottleneck}. \cite{dosovitskiy2020image} builds hybrids of self-attention and convolution by adding a ResNet backbone before Transformers.  Besides utilizing convolution in early layers, BotNet~\cite{srinivas2021bottleneck} designs bottleneck cells for self-attention. Additionally, self-attention has been used in many downstream vision tasks (detection~\cite{carion2020end}, segmentation~\cite{chen2021transunet}) and low vision tasks~\cite{chen2020pre}. These methods mostly utilize both self-attentions and convolutions.

\section{Methodology}
\label{methodology}

\subsection{Transformer-based and convolution-based visual recognition models}
\label{methodology:baselines}

Recognition is the fundamental task in computer vision. This work mainly considers image classification, where the input image is propagated through a deep network to derive the output class label. We denote this process using , where  is the image,  is the class label, and  denotes the learnable weights in the network. Most deep networks are designed in a hierarchical manner, where  is composed of a series of simpler mathematical functions, each of which is termed a network layer. Let there be  layers, and we use  and ,  to indicate the function and output of the -th layer, respectively. We define  and  for later convenience.

We consider two popular layers named convolution and Transformer. Convolution originates from the intuition to capture local patterns which are believed more repeatable than global patterns. It uses a number of learnable kernels to compute the responses of the input to different patterns, for which a sliding window is moved along both axes of the input data and the inner-product between the data and kernel is calculated. In this paper, we constrain our study in the scope of residual blocks, a combination of  or  convolutional layers and a skip-connection. Non-linearities such as activation and normalization are inserted between the neighboring convolutional layers. We use  to represent a residual block.

On the other hand, Transformer originates from natural language processing and aims to frequently formulate the relationship between \textit{any} two elements (called tokens) even when they are far from each other. This is achieved by generating three features for each token, named the query, key, and value, respectively. Then, the response of each token is calculated as a weighted sum over all the values, where the weights are determined by the similarity between its query and the corresponding keys. This is often referred to as multi-headed self-attention (MHSA), followed by other operations including normalization and linear mapping. Similarly, we use  to represent a Transformer module.

Throughout the remaining part, we consider DeiT-S~\cite{touvron2020training} and ResNet-50~\cite{he2016deep} as the representative of Transformer-based and convolution-based models, respectively. Besides the basic building block, there are also differences in design, \textit{e.g.}, ResNet-50 has a few down-sampling layers that partition the model into stages, but the number of tokens remains unchanged throughout DeiT-S. The impact of these details will be elaborated in Section~\ref{methodology:transition}.

\subsection{Settings: The base and elite performance}
\label{methodology:settings}

Although DeiT-S reports a  accuracy which is higher than  of ResNet-50, we notice that DeiT-S has changed the training strategy significantly, \textit{e.g.}, the number of epochs is enlarged by more than  and the data augmentation becomes much stronger. Interestingly, DeiT-S seems to heavily rely on the carefully-tuned training strategy, and other Transformer-based models including ViT~\cite{dosovitskiy2020image} and PIT~\cite{chen2020pre} also reported their dependency on other factors, \textit{e.g.}, a large-scale training set. In what follows, we provide a comprehensive study on this phenomenon.

We evaluate all classification models on the ImageNet dataset~\cite{russakovsky2015imagenet} which has  classes,  training images and  testing images. Each class has roughly the same number of training images. This is one of the most popular datasets for visual recognition.

There are two settings to optimize each recognition model. The first one is named the \textbf{base setting} which is widely adopted by convolution-based networks. Specifically, the model is trained for  epochs with the SGD optimizer. The learning rate starts with  for batch size  and gradually decays to  following the cosine annealing function. A moderate data augmentation strategy with random-size cropping~\cite{szegedy2016rethinking} and flipping is used. The second one is named the \textbf{elite setting} which has been verified effective to improve the Transformer-based models. The Adamw optimizer with an initial learning rate of  for batch size  is used. The data augmentation and regularization strategy is made much stronger to avoid over-fitting, for which intensive operations including RandAugment~\cite{cubuk2020randaugment}, Mixup~\cite{Zhang2017mixup}, CutMix~\cite{yun2019cutmix}, Random Erasing~\cite{zhong2020random}, Repeated Augmentation~\cite{berman2019multigrain,hoffer2020augment} and Stochastic Depth~\cite{huang2016deep} are used. Correspondingly, the training lasts  epochs, much longer than that of the base setting. The code and models in this paper will be released.

Throughout the remaining part of this paper, we refer to the classification accuracy under the base and elite settings as \textbf{base performance} and \textbf{elite performance}, respectively. We expect the numbers to provide complementary views for us to understand the studied models.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{visformer.pdf}
\end{center}
\caption{The transition process that starts with DeiT and ends with ResNet-50. To save space, we only show three important steps, that is, (i) replacing the patch flattening module with step-wise patch embedding (elaborated in Section~\ref{methodology:transition:flattening}), (ii) introducing the stage-wise design (in Section~\ref{methodology:transition:stage}), and (iii) replacing the self-attention module with convolution (Section~\ref{methodology:transition:feedforward}). The upper-right area shows a relatively minor modifications, inserting  convolution (Section~\ref{methodology:transition:conv}). The lower-right area compares the receptive fields of a  convolution and self-attention. This figure is best viewed in color.}
\label{fig:transition}
\end{figure*}

\subsection{The transition from DeiT-S to ResNet-50}
\label{methodology:transition}

This subsection displays a step-by-step process in which we gradually transit a model from DeiT-S to ResNet-50. There are eight steps in total. The key steps are illustrated in Figure~\ref{fig:transition}, and the results, including the base and elite performance and the model statistics, are summarized in Table~\ref{tab:transition}.

\subsubsection{Using global average pooling to replace the classification token}
\label{methodology:transition:token}

The first step of the transition is to remove the classification token and add global average pooling to the Transformer-based models. Unlike the convolution-based models, Transformers usually add a classification token to the inputs and utilize the corresponding output token to perform classification, which is inherited from NLP tasks~\cite{devlin2018bert}. As a contrast, the classification features in convolution-based models are obtained by conducting global average pooling in the space dimension. 

By removing the classification token, the Transformer can be equivalent translated to the convolutional version as shown in Figure~\ref{fig:transition}. Specifically, the patch embedding operation is equivalent to a convolution whose kernel size and stride is the patch size~\cite{dosovitskiy2020image}. The shape of the intermediate features can be naturally converted from a sequence of tokens (\textit{i.e.}, words) to a bundle feature maps and the tokens become the vector in channel dimension (illustrated in Figure~\ref{fig:transition}. The linear layers in MHSA and MLP blocks are equivalent to  convolutions.

The performance of the obtained network (Net1) is shown in Table~\ref{tab:transition}. As can be seen, this transition can substantially improve the base performance. Our further experiments show that adding global pooling itself can improve the base performance from 64.17\% to 69.44\%. In other words, the global average pooling operation which is widely used in convolution-based models since NIN~\cite{lin2013network}, enables the network to learn more efficiently under moderate augmentation. Furthermore, this transition can slightly improve the elite performance.

\begin{table*}
\begin{center}
\setlength{\tabcolsep}{0.12cm}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Model Name & \textbf{added} & \textbf{removed} & \textbf{base} perf. & \textbf{elite} perf. & FLOPs (G) & Params (M) \\
\hline\hline
DeiT-S & \multicolumn{2}{c|}{--} & 64.17 & 80.07 & 4.60 & 22.1\\
\hline
Net1 & global average pooling & classification token & 69.81 (\textcolor{red}{+5.64}) & 80.16 (\textcolor{red}{+0.09}) & 4.57 & 22.0 \\
Net2 & step-wise embeddings & large patch embedding & 73.01 (\textcolor{red}{+3.20}) & 81.35 (\textcolor{red}{+1.19}) & 4.77 & 23.9 \\
Net3 & stages-wise design & -- & 75.76 (\textcolor{red}{+2.75}) & 80.19 (\textcolor{blue}{-1.14}) & 4.79 &39.5\\
Net4 & batch norm & layer norm & 76.49 (\textcolor{red}{+0.73}) & 80.97 (\textcolor{red}{+0.78}) & 4.79 & 39.5\\
Net5 &  convolution & -- & 77.37 (\textcolor{red}{+0.88}) & 80.15 (\textcolor{blue}{-0.82}) &4.76 &39.2\\
Net6 & -- & position embedding & 77.31 (\textcolor{blue}{-0.06}) & 79.86 (\textcolor{blue}{-0.29}) & 4.76 & 39.0 \\
Net7 & convolution & self-attention & 76.24 (\textcolor{blue}{-1.07}) & 79.11 (\textcolor{blue}{-0.75}) & 4.88 & 45.7\\
\hline
ResNet-50 & \multicolumn{2}{c|}{network shape adjustment} & 77.43 (\textcolor{red}{+1.19}) & 78.73 (\textcolor{blue}{-0.38}) & 4.09 & 25.6\\
\hline
\end{tabular}
\end{center}
\caption{The classification accuracy on ImageNet during the transition procedure from DeiT-S to ResNet-50. Both the base setting and the elite setting are considered (for the details, see Section~\ref{methodology:settings}), and we mark the positive modifications in \textcolor{red}{red} and the negative modifications in \textcolor{blue}{blue}. Note that a modification can impact the base and elite performance differently. Though the number of parameters increases considerably at the intermediate status, the computational costs measured by FLOPs does not change significantly.}
\label{tab:transition}
\end{table*}

\subsubsection{Replacing patch flattening with step-wise patch embedding}
\label{methodology:transition:flattening}

DeiT and ViT models directly encoding the image pixels with a patch embedding layer which is equivalent to a convolution with large kernel size and stride (\textit{e.g.}, 16). This operation flattens the image patches to a sequence of tokens so that Transformers can handle images. However, patch flattening impairs the position information within each patch and makes it more difficult to extract the patterns within patches. To solve this problem, existing methods usually attach a preprocessing module before patch embedding. The preprocessing module can be a feature extraction convnet~\cite{dosovitskiy2020image} or a specially designed Transformer~\cite{yuan2021tokens}.

We found that there is a rather simple solution, which is factorizing the large patch embedding to step-wise small patch embeddings. Specifically, We first add the stem layer in ResNet to the Transformer, which is a  convolution layer with a stride of two. The stem layer can be seen as a  patching embedding operation with pixel overlap (\textit{i.e.},  kernel size). Since the patch size in the original DeiT model is 16, we still need to embed  patches after the stem. We further factorize the  patch embedding to a  embedding and a  embedding, which are  and  convolution layers with stride 4 and 2 in the perspective of convolution. Additionally, we add an extra  convolution to further upgrade the patch size from  to  before classification. These patch embedding layers can also be seen as the down-sampling layers and we double the channel numbers after embedding following the practice in convolution-based models.

By utilizing step-wise embeddings, the position prior within patches is encoded into features. As a result, the model can learn patterns more efficiently. As can be seen in Table~\ref{tab:transition}, this transition can significantly improve the base performance and elite performance of the network. It indicates that step-wise embedding is a better choice than larger patch embedding in Transformer-based models. Additionally, this transition is computationally efficient and only introduces about  extra FLOPs.

\subsubsection{Stage-wise design}
\label{methodology:transition:stage}

In this section, we split networks into stages like ResNets. The blocks in the same stage share the same feature resolution. Since step-wise embeddings in the last transition have split the network into different stages, the transition in this section is to reassign the blocks to different stages as shown in Figure~\ref{fig:transition}. However, unlike convolution blocks, the complexity of self-attention blocks increases by  with respect to the feature size. Thus we only insert blocks to the ,  and  patch embedding stages, which correspond to ,  and  feature resolutions respectively for  inputs. Additionally, we halve the head dimension and feature dimension before self-attention in  stage to ensure that the blocks in different stages utilize similar FLOPs.

This transition leads to interesting results. The base performance is further improved. It is conjectured that the stage-wise design leverages the image local priors and thus can perform better under moderate augmentation. However, the elite performance of the network decreases markedly. To study reasons, we conduct ablation experiments and find that self-attention does not work well in very large resolutions. We conjecture that large resolution contains too many tokens and it is much more difficult for self-attention to learn relations among them. We will detail it in section~\ref{methodology:visformer}.

\subsubsection{Replacing LayerNorm with BatchNorm}
\label{methodology:transition:norm}

Transformer-based models usually normalize the features with LayerNorm~\cite{ba2016layer}, which is inherited from NLP tasks~\cite{vaswani2017attention, devlin2018bert}. As a contrast, convolution-based models like ResNets usually utilize BatchNorm~\cite{ioffe2015batch} to stabilize the training process.
LayerNorm is independent of batch size and more friendly for specific tasks compared with BatchNorm, while BatchNorm usually can achieve better performance given appropriate batch size~\cite{wu2018group}. 
We replace all the LayerNorm layers with BatchNorm layers and the results show that BatchNorm performs better than LayerNorm. It can improve both the base performance and elite performance of the network. 

In addition, we also try to add BatchNorm to Net2 to further improve the elite performance. However, this Net2-BN network suffers from convergence problems. This may explain why BatchNorm is not widely used in the pure self-attention models. But for our mixed model, BatchNorm is a reliable method to advance performance.

\subsubsection{Introducing  convolutions}
\label{methodology:transition:conv}

Since the tokens of the network are present as feature maps, it is natural to introduce convolutions with kernel sizes larger than . The specific meaning of large kernel convolution is illustrated at the bottom right of Figure~\ref{fig:transition}. When global self-attentions attempt to build the relations among all the tokens (\textit{i.e.}, pixels), convolutions focus on relating the tokens within local neighborhoods. We chose to insert  convolutions between the  convolutions in feed-forward blocks, which transforms the MLP blocks into bottleneck blocks as exhibited at the top right of Figure~\ref{fig:transition}. Note that the channel numbers of the   convolution layers are tuned to ensure that the FLOPs of the feed-forward blocks are nearly unchanged. The obtained bottleneck blocks are similar to the bottleneck blocks in ResNet-50, although they have different bottleneck ratios (\textit{i.e.}, the factor of reducing the channel numbers before the  convolution). We replace the MLP blocks with bottleneck blocks in all three stages.

Not surprisingly,  convolutions which can leverage the local priors in images further improve the network base performance. The base performance (77.37\%) becomes comparable with ResNet-50 (77.43\%). However, the elite performance decreases by 0.82\%. We conduct more experiments to study the reasons. Instead of adding  convolutions to all stages, we insert  convolutions to different stages separately. We observe that  convolutions only work well on the high-resolution features. We conjecture that leveraging local relations is important for the high-resolution features in natural images. For the low-resolution features, however, local convolutions become unimportant when equipped with global self-attention. We will detail it in section~\ref{methodology:visformer}.

\subsubsection{Removing position embedding}
\label{methodology:transition:embedding}

In Transformer-based models, position embedding is proposed to encode the position information inter tokens. In the transition network, we utilize learnable position embedding as in~\cite{devlin2018bert} and add them to features after patch embeddings. To approaching ResNet-50, position embedding should be removed. 

The results are exhibited in Table~\ref{tab:transition}. The base performance is almost unchanged and the elite performance declines slightly (0.29\%). As a comparison, We test to remove the position embedding of DeiT-S and elite performance decreases significantly by 3.95\%. It reveals that position embedding is less important in the transition model than that in the pure Transformer-based models. It is because that the position prior inter tokens is preserved by the feature maps and convolutions with spatial kernels can encode and leverage it. Consequently, the harm of removing position embedding is remarkably reduced in the transition network. It also explains why convolution-based models do not need position embedding.

\subsubsection{Replacing self-attention with feed-forward}
\label{methodology:transition:feedforward}

In this section, we remove the self-attention blocks in each stage and use a feed-forward layer instead, so that the network becomes a pure convolution-based network. To keep the FLOPs unchanged, several bottleneck blocks are added to each stage. After the replacement, the obtained network consists of bottleneck blocks like ResNet-50. 

The performance of the obtained network (Net7) is shown in Table~\ref{tab:transition}. The pure convolution-based network performs much worse both in base performance and elite performance. \textit{It indicates that self-attentions do drive neural networks to higher elite performance and is not responsible for the poor base performance in ViT or DeiT. It is possible to design a self-attention network with high base performance and elite performance.} 

\subsubsection{Adjusting the shape of network}
\label{methodology:transition:shape}

There are still many differences between the translated model and ResNet-50. First, the shape of the translated network is different from ResNet-50. Their depths, widths, bottleneck ratios and block numbers in network stages are different. Second, they normalize the features in different positions. Then translated network only normalizes input features in a block, while ResNet-50 normalizes features after each convolutional layer. Third, ResNet-50 down-samples the features with bottleneck blocks but the translated network utilizes a single convolution layer (\textit{i.e.}, patch embedding layer). In addition, the translated network employs a few more FLOPs. Note that, both these two networks are convolution-based networks. The performance gap between these two networks can be attributed to architecture design strategy. 

As shown in Table~\ref{tab:transition}, the base performance is improved after transition. It demonstrates that ResNet-50 has better network architecture and can perform better with fewer FLOPs. However, ResNet-50 obtains worse elite performance. It indicates that the inconsistencies between base performance and elite performance exist not only in self-attention models but also in pure convolution-based networks. 


\subsection{Summary: the Visformer model}
\label{methodology:visformer}

We aim to build a network with high base performance and elite performance. The transition study has shown that there are some inconsistencies between base performance and elite performance. The first problem is the stage-wise design, which increases the base performance but decreases the elite performance. To study the reasons, we replace the self-attention blocks with bottleneck blocks in each stage separately for Net5, by which we can estimate the importance of self-attention in different stages. The results are shown in Table~\ref{tab:self-attention-ablation}. The replacement of self-attention in all three stages reduces both the base performance and the elite performance. There is a trend that self-attentions in lower resolutions play more important roles than those in higher resolutions. Additionally, replacing the self-attentions in the first stage almost has no effect on the network performance. Larger resolutions contain much more tokens and we conjecture that it is more difficult for self-attentions to learn relations among them.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Network & base perf.(\%)& elite perf.(\%)\\
\hline\hline
Net5 & 77.37  & 80.15 \\
\hline
Net5-DS1  & 77.29 (\textcolor{blue}{-0.08}) & 80.13 (\textcolor{blue}{-0.02})\\
Net5-DS2 &77.34 (\textcolor{blue}{-0.02}) &79.75 (\textcolor{blue}{-0.40}) \\
Net5-DS3 &77.05 (\textcolor{blue}{-0.32}) &79.59 (\textcolor{blue}{-0.56})\\
\hline
\end{tabular}
\end{center}
\caption{Impact of replacing the self-attention blocks with the bottleneck blocks in each stage of Net5. These experiments are performed individually.}
\label{tab:self-attention-ablation}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Network & base perf.(\%)& elite perf.(\%)\\
\hline\hline
Net4 & 76.49  & 80.97 \\
\hline
Net4-S1  & 77.02 (\textcolor{red}{+0.53}) & 81.10 (\textcolor{red}{+0.13}) \\
Net4-S2  & 76.55 (\textcolor{red}{+0.06}) & 80.50 (\textcolor{blue}{-0.47})\\
Net4-S3  & 76.82 (\textcolor{red}{+0.33}) & 80.44 (\textcolor{blue}{-0.53})\\
\hline
Net5 & 77.37 (\textcolor{red}{+0.88}) & 80.15 (\textcolor{blue}{-0.82})\\
\hline
\end{tabular}
\end{center}
\caption{Impact of replacing the MLP layers with the bottleneck blocks in each stage of Net4. These experiments are performed individually.}
\label{tab:convolution-ablation}
\end{table}



The second problem is adding  convolutions to the feed-forward blocks, which decreases the elite performance by 0.82\%. For Net4, We replace MLP blocks with bottleneck blocks in each stage separately. As can be seen in Table~\ref{tab:convolution-ablation}, although all stages obtain improvements in base performance, only the first stage benefits from bottleneck blocks in elite performance. The  convolutions are not necessary for the other two low-resolution stages when self-attentions already have a global view in these positions. On the high-resolution stage, for which self-attentions have difficulty in handling all tokens, the  convolutions can provide improvement.

Integrating the observation above, we propose the \textbf{Visformer} as vision-friendly, Transformer-based models. The detailed architectures are shown in Table~\ref{tab:network-archtecture}. Besides the positive transitions, Visformer adopts the stage-wise design for higher base performance. But self-attentions are only utilized in the last two stages, considered that self-attention in the high-resolution stage is computing inefficient even when the FLOPs are balanced. Visformer employs bottleneck blocks in the first stage and utilizes group  convolutions in bottleneck blocks inspired by ResNeXt~\cite{xie2016aggregated}. We also introduce BatchNorm to patch embedding modules as in CNNs. We name Visformer-S to denote the model that directly comes from DeiT-S. In addition, we can adjust the complexity by changing the output dimensionality of multi-head attentions. Here, we shrink the dimensionality by half and derive the Visformer-Ti model, which requires around  computational costs of the Visformer-S model.

\begin{table}
\setlength{\tabcolsep}{0.08cm}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
  & output size & Visformer-Ti & Visformer-S \\
\hline\hline

stem &  &  , , stride  &  , , stride   \\
\hline
emb. &  &  , ,  stride  &  , , stride  \\
\hline
\multirow{5}{*}{s1} & \multirow{5}{*}{ } &  \multirow{5}{*}{1\times11923\times338481\times196    } &   \multirow{5}{*}{1\times13843\times376881\times1192  }\\
& & & \\
& & & \\
& & & \\
& & & \\
\hline
emb. &  &  , ,  stride  &  , , stride  \\
\hline
\multirow{4}{*}{s2} & \multirow{4}{*}{ } &  \multirow{4}{*}{1921\times17681\times1192  }  &    \multirow{4}{*}{3841\times115361\times1384 } \\
& & & \\
& & & \\
& & & \\
\hline
emb.&  &  , ,  stride  &  , , stride  \\
\hline
\multirow{4}{*}{s3} & \multirow{4}{*}{ } &  \multirow{4}{*}{3841\times115361\times1384  }  &    \multirow{4}{*}{7681\times130721\times1768  } \\
& & & \\
& & & \\
& & & \\
\hline
&  & \multicolumn{2}{c|}{global average pool, 1000-d fc, softmax} \\
\hline
\multicolumn{2}{|c|}{FLOPS} &  &  \\
\hline
\end{tabular}
\end{center}
\caption{The configuration for constructing the Visformer-Ti and Visformer-S models, where `emb.' stands for feature embedding, and `s1'--`s3' indicate the three stages with different spatial resolutions.}
\label{tab:network-archtecture}
\end{table}








\section{Evaluating the Visformer}

\subsection{Comparison to the state-of-the-arts}

We first compare Visformer against DeiT, the direct baseline. Results are summarized in Table~\ref{tab:visformer-model}. Using comparable computational costs, the Visformer models outperform the corresponding DeiT models significantly. Specifically, the advantages of Visformer-S and Visformer-Ti over DeiT-S and DeiT-Ti under the elite setting are 2.21\% and 6.41\%, while under the base setting, the numbers grow to 14.08\% and 10.47\%, respectively. In other words, the advantage becomes more significant under the base setting, which is more frequently used for visual recognition.

We then compare Visformer to other Transformer-based approaches in Table~\ref{tab:comparison-with-sota}. At the tiny level, Visformer-Ti significantly outperforms other vision Transformer models. For larger models, Visformer-S performs much better than the models with similar FLOPs. Other models usually need to utilize much more FLOPs to achieve comparable performance. As for the state-of-the-art EfficientNet convnets, our models are below the EfficientNets with similar FLOPs. However, EfficientNets is computing inefficient on GPUs. The results in Table~\ref{tab:computing-time} show that our model is significantly faster than EfficientNet-b3 which performance is slightly worse than our model. Our model is as efficient as DeiT-S and ResNet-50 but with a rather better performance.

\begin{table}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Network & \tabincell{c}{base perf.\\%)} & \tabincell{c}{FLOPs\G)} & \tabincell{c}{Params\\%)} & \tabincell{c}{FLOPs\ms)} \\
\hline\hline
ResNet-50 & 78.7 & 4.1  & 33.9   \\
EfficientNet-B3~\cite{tan2019efficientnet} & 81.6 & 1.8 & 58.5 \\
DeiT-S & 80.1 & 4.6 & 36.9   \\
\hline
Visformer-S (ours)& 82.3 & 4.9 & 35.7 \\
\hline

\end{tabular}
\end{center}
\caption{Comparison of inference efficiency among Visformer-S and other models. A batch size of  is used for testing. Besides EfficientNet-B3, other models are trained using the elite setting.}
\label{tab:computing-time}
\end{table}

\subsection{Training with limited data}

Last but not least, we evaluate the performance of Visformer in the scenario with limited training data, which we consider is an important ability of being vision-friendly, while prior Transformer-based models mostly required abundant training data~\cite{dosovitskiy2020image}.

Four subsets of ImageNet are used, with 10\% and 1\% randomly chosen classes (all data), and with 10\% and 1\% randomly chosen images (all classes), respectively. To challenge the models, we still use the elite setting with 300 epochs (not extended). We observe that the DeiT-S model reports dramatic accuracy drops in all the four tests (note that the accuracy of using only 10\% and 1\% classes should be much higher if epochs are extended). In comparison, Visformer remains robust in these scenarios, showing its potential of used for visual recognition with limited data.

\begin{table}
\setlength{\tabcolsep}{0.12cm}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Network & \tabincell{c}{100\%\\classes} & \tabincell{c}{10\%\\classes} & \tabincell{c}{1\%\\classes} &\tabincell{c}{10\%\\images}&\tabincell{c}{1\%\\images}\\
\hline\hline
Visformer-S &82.28 & 90.06  & 91.60 &58.74 &16.56\\
DeiT-S & 80.07 & 80.06 & 73.40 & 40.41 &6.94\\
ResNet-50 & 78.73 & 89.90 &93.20 &58.37 &13.59 \\
\hline

\end{tabular}
\end{center}
\caption{Comparison among Visformer, DeiT, and ResNet, in terms of classification accuracy (\%) using limited training data. The elite setting with 300 epochs is used for all models.}
\label{tab:tiny-models}
\end{table}




\section{Conclusions}

This paper presents Visformer, a Transformer-based model that is friendly to visual recognition. We propose to use two protocols, the base and elite setting, to evaluate the performance of each model. To study the reason why Transformer-based models and convolution-based models behave differently, we decompose the gap between these models and design an eight-step transition procedure that bridges the gap between DeiT-S and ResNet-50. By absorbing the advantages and discarding the disadvantages, we obtain the Visformer-S model that outperforms both DeiT-S and ResNet-50. Visformer also shows a promising ability when it is transferred to a compact model and when it is evaluated on small datasets.

However, we shall notice that Transformer-based models still lack the flexibility of being transferred. For example, object detection and semantic/instance segmentation often require a large input image, but the complexity of Transformer increases by  with the input size. Also, the performance of self-supervised learning~\cite{chen2020simple,he2020momentum} on the Transformer-based models remains unclear. We expect the Visformer can inspire the community and offers new opportunities to solve these challenges.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}