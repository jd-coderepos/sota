We evaluate our FlowFormer on the Sintel~\cite{butler2012naturalistic} and the KITTI-2015~\cite{geiger2013vision} benchmarks.
Following previous works, we train FlowFormer on FlyingChairs~\cite{dosovitskiy2015flownet} and FlyingThings~\cite{mayer2016large}, and then respectively finetune it for Sintel and KITTI benchmark.
Flowformer achieves state-of-the-art performance on both benchmarks.

\noindent \textbf{Experimental setup}.
We use the average end-point-error~(AEPE) and F1-All(\%) metric for evaluation.
The AEPE computes mean flow error over all valid pixels.
The F1-all, which refers to the percentage of pixels whose flow error is larger than 3 pixels or over 5\% of length of ground truth flows.
The Sintel dataset is rendered from the same model but in two passes, i.e. clean pass and final pass.
The clean pass is rendered with smooth shading and specular reflections.
The final pass uses full rendering settings including motion blur, camera depth-of-field blur, and atmospheric effects.

\noindent \textbf{Implementation details}.
The image feature encoder of our final FlowFormer is chosen as the first two stages of ImageNet-pretrained Twins-SVT~\cite{chu2021twins}, which encodes an image into -channel feature map of 1/8 image size. The cost volume encoder patchifies each cost map to a -channel feature map and further summarizes the feature map to  cost tokens of  dimensions.
Then, the cost volume encoder encodes the cost tokens with 3 AGT layers.
Following previous optical flow training procedure~\cite{jiang2021learning}, we pre-train FlowFormer on FlyingChairs~\cite{dosovitskiy2015flownet} for 120k iterations with a batch size of 8, and on FlyingThings~\cite{mayer2016large} for 120k iterations with a batch size of 6~(denoted as `C+T').
After pre-training, we finetune FlowFormer on the data combined from FlyingThings, Sintel, KITTI-2015, and HD1K~\cite{kondermann2016hci}~(denoted as `C+T+S+K+H') for 120k iterations with a batch size of 6.
To achieve the best performance on the KITTI benchmark, we also further finetune FlowFormer on the KITTI-2015 for 50k iterations with a batch size of 6.
We use the one-cycle learning rate scheduler. The highest learning rate is set as  on FlyingChairs and  on the other training sets.
As positional encodings used in transformers are sensitive to image size, we crop the image pairs for flow estimation and tile them to obtain complete flows following Perceiver IO~\cite{jaegle2021perceiver}. 
We use the tile technique for evaluating optical flow on KITTI because the size of images in KITTI is quite different from training image size.
We use fixed Gaussian weights for tile, which will be detailed in the supplementary materials.


\subsection{Quantitative Experiment}

\begin{table}[t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{clccccccc}
\multicolumn{1}{c}{\multirow{2}{*}{Training Data}} & \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{Sintel (train)}                    & \multicolumn{2}{c}{KITTI-15 (train)}                    & \multicolumn{2}{c}{Sintel (test)}                     & \multicolumn{1}{c}{KITTI-15 (test)} \\
\cmidrule(r{1.0ex}){3-4}\cmidrule(r{1.0ex}){5-6}\cmidrule(r{1.0ex}){7-8} \cmidrule(r{1.0ex}){9-9} 
\multicolumn{1}{c}{}                               & \multicolumn{1}{c}{}                        & \multicolumn{1}{c}{Clean} & \multicolumn{1}{c}{Final} & \multicolumn{1}{c}{F1-epe} & \multicolumn{1}{c}{F1-all} & \multicolumn{1}{c}{Clean} & \multicolumn{1}{c}{Final} & \multicolumn{1}{c}{F1-all} \\ 
\hline
\multirow{3}{*}{A}                         &    Perceiver IO~\cite{jaegle2021perceiver}   & 1.81 & 2.42 & 4.98  & - & - & - & - \\ 
&    PWC-Net~\cite{sun2018pwc}   & 2.17 & 2.91 & 5.76 & - & - & - & -  \\ 
& RAFT~\cite{teed2020raft}  & 1.95 & 2.57 &  4.23 & - & - & - & - \\
\hline
\multicolumn{1}{c}{\multirow{9}{*}{C+T}}           & HD3~\cite{yin2019hierarchical} &  3.84      & 8.77      & 13.17  & 24.0  & - & - & - \\
 & LiteFlowNet~\cite{hui2018liteflownet} & 2.48 & 4.04 & 10.39 & 28.5 & - & - & -  \\
 & PWC-Net~\cite{sun2018pwc} & 2.55 & 3.93 & 10.35 & 33.7 & - & - & - \\
 & LiteFlowNet2~\cite{hui2020lightweight} & 2.24 & 3.78 & 8.97 & 25.9 & - & - & - \\
 & S-Flow~\cite{zhang2021separable}  & 1.30 & 2.59 & 4.60 & 15.9 & & & \\
 & RAFT~\cite{teed2020raft}   & 1.43 & 2.71 & 5.04 & 17.4 & - & - & - \\
 & FM-RAFT~\cite{jiang2021learning2}   & 1.29 & 2.95 & 6.80 & 19.3 & - & - & -  \\
 & GMA~\cite{jiang2021learning}   & 1.30 & 2.74 & 4.69 & 17.1 & - & - & - \\
& Ours   &  &  &  &  & - & - & - \\
\hline
\multirow{11}{*}{C+T+S+K+H}                         &    LiteFlowNet2~\cite{hui2020lightweight}    &    (1.30)                       &       (1.62)                    &   (1.47)                          &      (4.8)                      &       3.48                     &         4.69                   &          7.74   \\
 &  PWC-Net+~\cite{sun2019models} & (1.71) & (2.34) &  (1.50) &  (5.3) & 3.45 &  4.60 & 7.72\\
 & VCN~\cite{yang2019volumetric} & (1.66) & (2.24) & (1.16) & (4.1) & 2.81 & 4.40 & 6.30 \\
 &  MaskFlowNet~\cite{zhao2020maskflownet} & - & - & - & - & 2.52 & 4.17 & 6.10 \\
  &  S-Flow~\cite{zhang2021separable} & (0.69) & (1.10) & (0.69) & (1.60) & 1.50 & 2.67 &  \\
 &  RAFT~\cite{teed2020raft} & (0.76) & (1.22) & (0.63) & (1.5) & 1.94 & 3.18 & 5.10
\\
 &  FM-RAFT~\cite{jiang2021learning2} & (0.79) & (1.70) & (0.75) & (2.1) & 1.72 & 3.60 &  6.17 \\
 &  GMA~\cite{jiang2021learning} & - & - & - & - & 1.40 & 2.88 & 5.15 \\
&  Ours & (0.48) & (0.74) & (0.53) & (1.11) &  &  &   \\
\cmidrule(r{1.0ex}){2-9}
&  RAFT*~\cite{teed2020raft} & (0.77) & (1.27) & - & - & 1.61 & 2.86 & -
\\
 &  GMA*~\cite{jiang2021learning} & (0.62) & (1.06) & (0.57) & (1.2) & 1.39 & 2.47 & - \\
\hline
\end{tabular}
}
\caption{\label{Tab: comparison} Experiments on Sintel~\cite{butler2012naturalistic} and KITTI~\cite{geiger2013vision} datasets. `A' denotes the autoflow dataset. `C + T' denotes training only on the FlyingChairs and FlyingThings datasets. `+ S + K + H' denotes finetuning on the combination of Sintel, KITTI, and HD1K training sets.  * denotes that the methods use the warm-start strategy~\cite{teed2020raft}, which relies on previous image frames in a video.  is estimated via the tile technique elaborated in the supplementary.
Our FlowFormer achieves best generalization performance~(C+T) and ranks 1st on the Sintel benchmark~(C+T+S+K+H).
}
\end{table}


We evaluate FlowFormer on the well-known Sintel and KITTI benchmarks as shown in Tab.~\ref{Tab: comparison}.
GMA~\cite{jiang2021learning}, an improved version of RAFT~\cite{teed2020raft}, is the most competitive flow estimation method at present.
After being trained on FlyingChairs and FlyingThings, we evaluate the generalization performance of FlowFormer on the training set of Sintel and KITTI-2015.
By further finetuning FlowFormer on the combination of HD1K, Sintel and KITTI training sets,
we compare the dataset-specific accuracy of optical flow models.
Autoflow~\cite{sun2021autoflow} is a dataset that provides training data covering various challenging visual disturbance, but its training code is not released yet.


\noindent \textbf{Generalization performance.}
We train FlowFormer on the FlyingChairs and FlyingThings~(C+T), and evaluate it on the training set of Sintel and KITTI-2015. This settings evaluates the generalization performance of optical flow models.
FlowFormer ranks 1st among all compared methods on both benchmarks.
FlowFormer achieves 1.01 and 2.40 on the clean and final pass of Sintel.
On the KITTI-2015 training set, FlowFormer achieves 4.09 F1-epe and 14.72 F1-all.
Compared to GMA, FlowFormer reduces 22.3\% and 12.4\% errors on Sintel clean and final, and 13.9\% errors on KITTI-2015 F1-all, which shows its extraordinary generalization performance.
RAFT trained on the autoflow dataset~(A) significantly outperforms RAFT trained on the C+T on final pass because autoflow provides training image pairs that are more challenging.
We believe training FlowFormer with autoflow can achieve better accuracy but it is not released yet.

\noindent \textbf{Sintel benchmark.}
We finetune the pretrained FlowFormer on the combination of training data of FlyingThings, HD1K, Sintel and KITTI-2015, and then evaluate it on the Sintel test set.
FlowFormer achieves 1.16 and 2.09 on the Sintel clean and final, 16.5\% and 15.5\% lower error compared to GMA, which ranks both 1st on the Sintel benchmark.
It is noteworthy that RAFT and GMA use the warm-start strategy that requires image sequences while FlowFormer does not.
Compared with GMA, which also does not use the warm-start, FlowFormer obtains 17.2\% and 27.5\% error reduction.


\noindent \textbf{KITTI-2015 benchmark.}
We further finetune the FlowFormer on the KITTI-2015 training set after the Sintel finetuning stage and evaluate it on the KITTI test set.
FlowFormer achieves 4.68, ranking 2nd on the KITTI-2015 benchmark.
S-Flow~\cite{zhang2021separable} obtains slightly smaller error than FlowFormer on KITTI~(0.85\%), which, however, is significantly worse on Sintel~(31.6\% and 22.5\% larger error on clean and final pass).
S-Flow finds corresponding points by computing the coordinate expectation weighted by refined cost maps.
Images in the KITTI dataset are captured in urban traffic scenes, which contains objects that are mostly rigid.
Flows on rigid objects are rather simple, which is easier for cost-based coordinate expectation, but the assumption can be easily violated in non-rigid scenarios such as Sintel.

\begin{figure}[t!]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/input3.png}
\end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/flow_ours3.png}
\end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/flow_gma3.png}
\end{subfigure}
    }
    \resizebox{1.0\linewidth}{!}{
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/input1.png}
\end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/flow_ours1.png}
\end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/flow_gma1.png}
\end{subfigure}
    }
    \resizebox{1.0\linewidth}{!}{
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/input2.png}
        \caption{\small Input}
    \end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/flow_ours2.png}
        \caption{\small FlowFormer (Ours)}
    \end{subfigure}
    \begin{subfigure}[t]{0.33\linewidth}
        \includegraphics[width=\linewidth]{Figures/example_flows/flow_gma2.png}
        \caption{\small GMA}
    \end{subfigure}
    }
    \caption{Qualitative comparison on the Sintel test set. FlowFormer greatly reduces the flow leakage around object boundaries~(pointed by red arrows) and clearer details~(pointed by blue arrows).
    }
    \label{Fig: flow comparison}
\end{figure}


\subsection{Qualitative Experiment}

We visualize flows that estimated by our FlowFormer and GMA of three examples in Fig.~\ref{Fig: flow comparison} to qualitatively show how FlowFormer outperforms GMA.
As transformers can encode the cost information at a large perceptive field, FlowFormer can distinguish overlapping objects via contextual information and thus reduce the leakage of flows over boundaries.
Compared with GMA, the flows that are estimatd by FlowFormer on boundaries of the bamboo and the human body are more precise and clear.
Besides, FlowFormer can also recover motion details that are ignored by GMA, such as the hair and the holes on the box.





\subsection{Ablation Study}

\begin{table}[t]
\centering
\resizebox{0.92\linewidth}{!}{
\begin{tabular}{clccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Experiment}} & \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{Sintel (train)}                    & \multicolumn{2}{c}{KITTI-15 (train)}                 & \multirow{2}{*}{Params.}  \\
\cmidrule(r{1.0ex}){3-4}\cmidrule(r{1.0ex}){5-6}
\multicolumn{1}{c}{}                               & \multicolumn{1}{c}{}                        & \multicolumn{1}{c}{Clean} & \multicolumn{1}{c}{Final} & \multicolumn{1}{c}{F1-epe} & \multicolumn{1}{c}{F1-all} &    \\ 
\hline
\hline
baseline & RAFT  & 1.53 & 2.99 & 5.73 & 18.29 & 5.3M    \\ 
\hline
\multirow{3}{*}{MCRLCT+CMD}                       
& ,  & 1.66 & 2.93 & 5.60 & 19.67 &  5.5M \\
& ,    & 1.58 & 2.90 & 5.50 & 18.71 &  5.5M  \\ 
 & ,  & 1.44 & 2.80 & 5.22 & 17.64 &  5.6M  \\ 
\hline
\multirow{3}{*}{CNN  Twins}
 & CNN & 1.44 & 2.80 & 5.22 & 17.64 &  5.6M  \\ & Twins from Scratch & 1.44 & 2.86 & 5.38 & 17.58 &  14.0M \\
& Pretrained Twins    & 1.29 & 2.72 & 4.82 & 16.16 &   14.0M  \\ 
\hline
\multirow{5}{*}{Cost Encoding}                        
& None    & 1.29 & 2.72 & 4.82 & 16.16 &   14.0M  \\ 
& +Intra. & 1.29 & 2.89 & 4.74 & 15.71 &  14.1M \\
& AGT1 (+Intra.+Inter.) & 1.20 & 2.85 & 4.57 & 15.46 & 15.2M  \\ 
& AGT2 & 1.16 & 2.66 & 4.70 & 16.01 & 16.4M  \\ 
& AGT3 & 1.10 & 2.57 & 4.45 & 15.15 & 17.6M  \\ 
\hline
\end{tabular}
}
\caption{\label{Tab: ablation} Ablation study. We gradually change one component of the RAFT at a time to obtain our FlowFormer model. MCRLCT+CMD: replacing RAFT's decoder with OUR latent cost tokens + cost memory decoder. CNNTwins: replacing RAFT's CNN encoder with Twins-SVT transformer. Cost Encoding: adding intra-cost-map and inter-cost-map to form an Alternate-Group Transformer layer in the encoder. 3 AGT layers are used in our final model.
}
\end{table}


\begin{table}[h]
\centering
\resizebox{0.92\linewidth}{!}{
\begin{tabular}{lccccccc}
& \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{Sintel (train)}                    & \multicolumn{2}{c}{KITTI-15 (train)} & \multirow{2}{*}{parameters} \\
\cmidrule(r{1.0ex}){3-4}\cmidrule(r{1.0ex}){5-6}
\multicolumn{1}{c}{}                               & \multicolumn{1}{c}{}                        & \multicolumn{1}{c}{Clean} & \multicolumn{1}{c}{Final} & \multicolumn{1}{c}{F1-epe} & \multicolumn{1}{c}{F1-all} & \\ 
\hline
 & GMA~\cite{jiang2021learning}   & 1.30~(+30\%) & 2.74~(+12\%) & 4.69~(+15\%) & 17.1~(+16\%) & 5.9M \\
 & Ours~(small)  & 1.20~(+20\%) & 2.64~(+8\%) & 4.57~(+12\%) & 16.62~(+13\%) & 6.2M \\
 \cline{2-7}
& GMA-L~\cite{jiang2021learning}   & 1.33~(+33\%) & 2.56~(+4\%) & 4.40~(+8\%) & 15.93~(+8\%) & 17.0M \\
 & GMA-Twins~\cite{jiang2021learning}   & 1.15~(+15\%) & 2.73~(+11\%) & 4.98~(+22\%) & 16.82~(+14\%) & 14.2M \\
 & Ours   &  &  &  &  & 18.2M
\\ \hline
\end{tabular}
}
\caption{\label{Tab: param comparison} FlowFormer v.s. GMA. 
Ours~(small) is a small version of FlowFormer and uses the CNN image feature encoder of GMA.
GMA-L is a large version of GMA. 
GMA-Twins replace its CNN image feature encoder with pre-trained Twins.
(+x\%) indicates that this model obtains x\% larger error than ours.
}
\end{table}

We conduct a series of ablation experiments in Tab.~\ref{Tab: ablation}.
We start from RAFT as the baseline, which directly regresses residual flows with the multi-level cost retrieval~(MCR) decoder,
and gradually replace its components with our proposed components.
We first replace RAFT's MCR decoder with the latent cost tokenization (LCT) part of our encoder and the cost memory decoder (CMD) (denoted as `MCRLCT+CMD'). Note that our cost memory decoder cannot be used alone on top of the 4D cost volume of RAFT because of the too large number of tokens. It must be combined with our latent cost tokens ( from Eq. (1)). Encoding  latent tokens of  dimensions for each source pixel achieves the best performance.
Based on LCT+CMD with  and , we replace RAFT's CNN image feature encoder with Twins-SVT~(denoted as `CNNTwins').
We then further add attention layers of the proposed cost volume encoder to encode and update latent cost tokens.
The proposed Alternate-Group Transformer (AGT) layer consists of two types of attention, i.e., intra-cost-map attention and inter-cost-map attention.
We first add a single intra-cost-map attention layer (denoted as `+Intra.'), and then add the inter-cost-map attention (denoted as `AGT (+Intra.+Inter.)', which is equivalent to adding a single AGT layer.
We then test on increasing the number of AGT layers to 2 and 3.
Following RAFT, all models are trained on FlyingChairs~\cite{dosovitskiy2015flownet} with 100k iterations and FlyingThings~\cite{mayer2016large} with 60k iterations, and then evaluated on the training set of Sintel~\cite{butler2012naturalistic} and KITTI-2015~\cite{geiger2013vision}.

\noindent \textbf{MCR  LCT+MCD.}
The number of latent tokens  and token dimension  determine how much cost volume information the cost tokens can encode.
From  to , the AEPE decreases because the cost tokens summarizes more cost map information and benefits the residual flow regression.
The latent cost tokens are capable of summarizing whole-image information and our MCD can absorb interested information from them through co-attention, while the MCR decoder of RAFT only retrieves multi-level costs inside flow-guided local windows.
Therefore, even without our AGT layers in our encoder,  LCT+MCD still shows better performance than MCR decoder of RAFT.


\noindent \textbf{CNN vs. Transformer Image Encoder.}
In the CNNTwins experiment, 
the AEPE of Twins trained from scratch is marginally worse than CNN, but the ImageNet-pretraining is beneficial,
because Twins is a transformer architecture with larger receptive field and model capacity, which requires more training examples for sufficient training. 

\noindent \textbf{Cost Encoding.} 
In the cost volume encoder, we encode and update the latent cost tokens with an intra-cost-map attention operation and an inter-cost-map attention operation. The two operations form an Alternate-Group Transformer (AGT) layer.
Then we gradually increase the number of AGT layers to 3.
From no attention layer to AGT3, the errors gradually decrease, which demonstrates that encoding latent cost tokens with our AGT layers benefits flow estimation.

\noindent \textbf{FlowFormer vs. GMA.} We train all the models with the settings of GMA. The full version of FlowFormer has 18.2M parameters, which is larger than GMA. One of the causes is that FlowFormer uses the first two stages of ImageNet-pretrained Twins-SVT as the image feature encoder while GMA uses a CNN.
We present an experiment to compare FlowFormer and GMA with aligned settings in Tab.~\ref{Tab: param comparison}.
We first provide a small version of FlowFormer using GMA's CNN image encoder and also set , , and AGT1.
Although the smaller version of FlowFormer (denoted as 'Ours (small)') has a significant performance drop compared to the full version of FlowFormer, it still outperforms GMA in terms of all metrics.
We also design two enhanced GMA models and compare them with the full version of FlowFormer to show that the performance improvements are not simply derived from adding more parameters.
The first one is denoted as `GMA-L', a large version of GMA and the second one is denoted as `GMA-Twins' which also adopts the pretrained Twins as the image encoder. 
In this experiment, we train all models on FlyingChairs with 120k iterations and FlyingThings with 120k iterations.
Similar to reducing RAFT to RAFT~(small)~\cite{teed2020raft}, GMA-L enlarges GMA by doubling feature channels, which has 17M parameters, comparable to FlowFormer.
However, its performance degrades in Sintel clean, a 33\% larger error than FlowFormer.
GMA-Twins replaces the CNN image encoder with the shallow Image-Net pre-trained Twins-SVT as FlowFormer does.
The largest improvement of GMA-Twins upon GMA is on the Sintel clean, but it still has a 15\% larger error than FlowFormer. GMA-Twins does not lead to significant error reduction on other metrics and is even worse on the KITTI-15.
In conclusion, the performance improvement of FlowFormer is not derived from more parameters but the novel design of the architecture.
