\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage[accsupp]{axessibility}
\usepackage{float}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{times}
\usepackage{url}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{amsfonts} 
\usepackage{bbding}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{diagbox}

\usepackage{pifont}

\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}
\newtheorem{myAxio}{Axioms}

\definecolor{cred}{HTML}{FF6B6B}
\definecolor{cyellow}{HTML}{FEC260}
\definecolor{cgreen}{HTML}{70AD47}
\definecolor{cblue}{HTML}{4D96FF}
\definecolor{cpurple}{HTML}{2A0944}
\definecolor{ggray}{RGB}{127,127,127}
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}



\usepackage[breaklinks=true,bookmarks=false,colorlinks]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\makeatletter
\newcommand{\ssymbol}[1]{}
\makeatother

\begin{document}

\title{DiffusionRet: Generative Text-Video Retrieval with Diffusion Model}

\author{
    Peng Jin\footnotemark[1] \ \
    Hao Li\footnotemark[1] \ \ 
    Zesen Cheng \ 
    Kehan Li \ 
    Xiangyang Ji \ \and 
    Chang Liu \ 
    Li Yuan\footnotemark[2] \ \
    Jie Chen\footnotemark[2] \
p(v|t;\bm{\theta^{t}},\bm{\theta^{v}})\!=\!\frac{\exp \big(f_{\bm{\theta^{t}}}(t)^\top f_{\bm{\theta^{v}}}(v)/\tau \big)}{\sum_{v^{'}\in \bm{V}}\exp \big(f_{\bm{\theta^{t}}}(t)^\top f_{\bm{\theta^{v}}}(v^{'})/\tau \big)},

\begin{aligned}
\bm{\theta_{t}^{*}}, \bm{\theta_{v}^{*}}=\mathop{\arg\min}\limits_{\bm{\theta_{t}}, \bm{\theta_{v}}} -\frac{1}{2} \mathbb{E}_{(t,v)\in \mathcal{D}} \big [ \log p(v|t;\bm{\theta_{t}},\bm{\theta_{v}})\\ + \log p(t|v;\bm{\theta_{t}},\bm{\theta_{v}}) \big ],
\end{aligned}

\begin{aligned}
x^{1:N}=p(v, t|\bm{\phi})=f_{\bm{\phi}}\big(v, t, \mathcal{N}(0,\bm{\textrm{I}})\big),
\end{aligned}

C_v=\textrm{Softmax}(C_tF^\top/{{\tau}^{'}})F,

\begin{aligned}
&Q_t=W_{Q_t}\big(C_t+\textrm{Proj}(k)\big),\\
&K_v=W_{K_v}\big(C_v+\textrm{Proj}(k)\big),\\
&V_v=W_{V_v}\big(C_v+\textrm{Proj}(k)\big),
\end{aligned}

E_t=\textrm{Softmax}({Q_tK_v^\top}+x_{k})V_v.

\begin{aligned}
q(x_k|x_{k-1})&=\mathcal{N}(x_k;\sqrt{1-\beta_{k}}x_{k-1},\beta_{k}\bm{\textrm{I}}),\\
q(x_{1:K}|x_{0})&=\prod_{k=1}^{K}q(x_k|x_{k-1}),
\end{aligned}
\label{Markov}

x_k = \sqrt{\bar{\alpha}_{k}}x_0 + \sqrt{1-\bar{\alpha}_{k}}\epsilon,

\mathcal{L}_{G} = \mathbb{E}_{(t,v)\in \mathcal{D}} \Big [\textrm{KL}\big(x_0\|f_{\bm{\phi}}(v, t, x_k)\big) \Big ].

s=\frac{1}{2}(\underbrace{\sum_{i=1}^{N_t} g_t^i\ \underset{j}{\textrm{max}}\ a_{ij}}_{\textrm{text-to-video similarity}}+\underbrace{\sum_{j=1}^{N_v} g_v^j\ \underset{i}{\textrm{max}}\ a_{ij}}_{\textrm{video-to-text similarity}}),

\begin{aligned}
\mathcal{L}_{D}=-\frac{1}{2}\mathbb{E}_{(t,v)\in \mathcal{D}}\big [ \log \frac{\exp(s_{t,v}/\hat{\tau})}{\sum_{v^{'}\in \bm{V}}\exp(s_{t,v^{'}}/\hat{\tau})}\\
+\log \frac{\exp(s_{t,v}/\hat{\tau})}{\sum_{t^{'}\in \bm{T}}\exp(s_{t^{'},v}/\hat{\tau})} \big ],
\end{aligned}

where  is the similarity score between text  and video .  is the temperature hyper-parameter. This loss brings semantically similar texts and videos closer together in the representation space, thus helping the diffusion model to generate the joint distribution of text and video.

\begin{table*}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{3.pt}
{
\subfloat[\textbf{Effect of the generation loss type.}]
{
\begin{tabular}{lcccc}
\toprule[1.25pt]
Loss type & R@1 & R@5 & R@10 & MnR \\ \midrule
MSE & 46.9 & 75.0 & 82.5 & 12.2 \\
\rowcolor{aliceblue!60} KL & \Frst{49.0} & \Frst{75.2} & \Frst{82.7} & \Frst{12.1} \\
\bottomrule[1.25pt]
\end{tabular}
\label{tab:loss}
}
\quad \
\subfloat[\textbf{Effect of the sampling strategy.}]
{
\begin{tabular}{lcccc}
\toprule[1.25pt]
Sampling  & R@1 & R@5 & R@10 & MnR \\ \midrule
DDPM & 48.9 & \Frst{75.2} & 82.6 & \Frst{12.1} \\
\rowcolor{aliceblue!60} DDIM & \Frst{49.0} & \Frst{75.2} & \Frst{82.7} & \Frst{12.1} \\
\bottomrule[1.25pt]
\end{tabular}
\label{tab:sampling}
}
\quad \
\subfloat[\textbf{Effect of the schedule of .}]
{
\begin{tabular}{lcccc}
\toprule[1.25pt]
Schedule of   & R@1 & R@5 & R@10 & MnR \\ \midrule
Linear & 48.7 & 75.1 & \Frst{82.8} & 12.3 \\
\rowcolor{aliceblue!60} Cosine & \Frst{49.0} & \Frst{75.2} & 82.7 & \Frst{12.1} \\
\bottomrule[1.25pt]
\end{tabular}
\label{tab:schedule}
} 
\\ 
\subfloat[\textbf{Effect of the training strategy.}]
{
\begin{tabular}{lcccc}
\toprule[1.25pt]
Training strategy  & R@1 & R@5 & R@10 & MnR \\ \midrule
CLIP zero-shot & 25.3 & 46.5 & 56.0 & 75.8\\ \midrule
\underline{Gen}eration (Gen) & 40.2 & 68.6 & 78.8 & 16.1 \\
\underline{Dis}crimination (Dis) & 46.8 & 73.5 & \Frst{82.9} & 13.0 \\
\rowcolor{aliceblue!60} Both Gen and Dis  & \Frst{49.0} & \Frst{75.2} & 82.7 & \Frst{12.1}  \\
\bottomrule[1.25pt]
\end{tabular}
\label{tab:training}
}
\quad \
\subfloat[\textbf{{Effect of the diffusion steps on R@1.}}]
{
\begin{tabular}{lcccc}
\toprule[1.25pt]
\diagbox{{train}}{{eval}}  & 10 & 50 & 100 & 1000 \\ \midrule
10 & 48.7 & {\color{gray}\ding{56}} & {\color{gray}\ding{56}} & {\color{gray}\ding{56}} \\
50 & 48.6 & \colorbox{aliceblue!60}{\Frst{49.0}} & {\color{gray}\ding{56}} & {\color{gray}\ding{56}} \\
100 & 48.1 & 48.5 & 48.6 & {\color{gray}\ding{56}} \\
1000 & 48.2 & 48.4 & 48.5 & 48.6 \\
\bottomrule[1.25pt]
\end{tabular}
\label{tab:step}
}
\quad \
\subfloat[\textbf{Effect of the scale of .}]
{
\begin{tabular}{lcccc}
\toprule[1.25pt]
Scale of   & R@1 & R@5 & R@10 & MnR \\ \midrule
0.1 & 48.4 & \Frst{75.3} & \Frst{82.7} & 12.1 \\
0.5 & 48.5 & 75.1 & 82.6 & \Frst{12.0} \\
\rowcolor{aliceblue!60} 1.0 & \Frst{49.0} & 75.2 & \Frst{82.7} & 12.1 \\
1.5 & 48.3 & \Frst{75.3} & \Frst{82.7} & 12.2 \\
2.0 & 48.5 & 75.2 & 82.5 & 12.1 \\
\bottomrule[1.25pt]
\end{tabular}
\label{tab:scale}
}
\vspace{-0.5em}
\caption{\textbf{Ablation study on the MSRVTT dataset.} {``KL'' denotes Kullback-Leibler divergence. ``Gen'' denotes generation. ``Dis'' denotes discrimination. ``{\color{gray}\ding{56}}'' denotes that the evaluation process fails. Default settings are marked in \colorbox{aliceblue!60}{blue}.}}
\label{Comparisons to State-of-the-arts}
}
\end{table*}

\section{Experiments}
\subsection{Experimental Settings}
\myparagraph{Datasets.} 
\textbf{MSRVTT}~\cite{xu2016msr} contains 10,000 YouTube videos, each with 20 text descriptions. We follow the 1k-A split~\cite{liu2019use} with 9,000 videos for training and 1,000 for testing. \textbf{LSMDC}~\cite{rohrbach2015a} contains 118,081 video clips from 202 movies. We follow the split of \cite{gabeur2020multi} with 1,000 videos for testing. \textbf{MSVD}~\cite{chen2011collecting} contains 1,970 videos. We follow the official split of 1,200 and 670 as the train and test set, respectively. \textbf{ActivityNet Captions}~\cite{krishna2017dense} contains 20,000 YouTube videos. We report results on the ``val1'' split of 10,009 and 4,917 as the train and test set. \textbf{DiDeMo}~\cite{anne2017localizing} contains 10,464 videos annotated 40,543 text descriptions. We follow the training and evaluation protocol in \cite{luo2021clip4clip}.

\myparagraph{Metrics.} We choose Recall at rank K (R@K), the Sum of Recall at rank  (Rsum), Median Rank (MdR), and mean rank (MnR) to evaluate the retrieval performance.

\begin{table*}[t]
\footnotesize
\centering
\resizebox{1.\linewidth}{!}
{
\begin{tabular}{lc|ccc|ccc|ccc}
\toprule[1.25pt]
\multirow{3}{*}{\textbf{Methods}} & \multirow{3}{*}{\textbf{Steps}} & \multicolumn{3}{c|}{\textbf{Candidate gallery size is 1,000}} & \multicolumn{3}{c|}{\textbf{Candidate gallery size is is 3,000}} & \multicolumn{3}{c}{\textbf{Candidate gallery size is 5,000}}\\
\cmidrule(rl){3-5}\cmidrule(rl){6-8}\cmidrule(rl){9-11}
 &  &{Inference} &{GPU} &\multirow{2}{*}{{R@1}} &{Inference} &{GPU} &\multirow{2}{*}{{R@1}} &{Inference} &{GPU} &\multirow{2}{*}{{R@1}} \\
 & & Time (s) & Memory (M) & & Time (s) & Memory (M) &  & Time (s) & Memory (M) & \\
 \midrule
CLIP4Clip~{\cite{luo2021clip4clip}} & {\color{gray}\ding{56}} & 53.20 & 3043 & 43.3 & 137.24 & 3095 & 37.5 & 244.63 & 3147 & 32.6 \\
X-Pool~{\cite{gorti2022x}} & {\color{gray}\ding{56}} & 63.02 & 5585 & 45.9 & 330.02 & 5587 & 31.3 & 833.11 & 5590 & 27.0 \\
TS2-Net~{\cite{liu2022ts2}} & {\color{gray}\ding{56}} & 117.71 & 2803 & 46.5 & 702.65 & 3053 & 37.8 & 1866.18 & 3303 & 32.8 \\
\midrule
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & 1 & 59.02 & 3253 & 23.4 & 143.04 & 3747 & 16.6 & 260.67 & 4297 & 13.1 \\
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & 3 & 59.32 & 3253 & 47.8 & 144.42 & 3747 & 37.8 & 260.96 & 4297 & 30.5 \\
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & 5 & 59.55 & 3253 & 47.9 & 144.63 & 3747 & 37.8 & 261.11 & 4297 & 32.9 \\
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & 10 & 59.70 & 3253 & 48.6 & 145.97 & 3747 & 37.9 & 261.69 & 4297 & 33.0 \\
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & 30 & 60.66 & 3253 & 48.9 & 148.11 & 3747 & 38.1 & 274.49 & 4297 & 33.1 \\
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & 50 & 62.04 & 3253 & \textbf{49.0} & 151.31 & 3747 & \textbf{38.2} & 281.27 & 4297 & \textbf{33.2} \\ 
\bottomrule[1.25pt]
\end{tabular}
}
\caption{\textbf{Evaluation of the Inference costs with a single RTX3090 GPU.} We report the R@1 metric for the text-to-video task and repeat the experiment three times to take the average value. ``Steps'' denotes the diffusion steps. ``{\color{gray}\ding{56}}'' denotes that this method does not apply this parameter. ``'' denotes that higher is better. ``'' denotes that lower is better.}
\label{tab:cost}
\end{table*}

\begin{table*}[t]
\footnotesize
\centering
\resizebox{1.\linewidth}{!}
{
\begin{tabular}{l|cccc|cccc|cccc}
\toprule[1.25pt]
\multirow{2}{*}{\textbf{Method}} &\multicolumn{4}{c}{\textbf{MSRVTT}} &\multicolumn{4}{|c|}{\textbf{MSRVTT-\textgreater{}DiDeMo}} &\multicolumn{4}{c}{\textbf{MSRVTT-\textgreater{}LSMDC}}\\
\cmidrule(rl){2-5}\cmidrule(rl){6-9}\cmidrule(rl){10-13}
  & R@1 & R@5 & R@10 & MdR & R@1 & R@5 & R@10 & MdR & R@1 & R@5 & R@10 & MdR \\ \midrule
CLIP4Clip~{\cite{luo2021clip4clip}}\ssymbol{3}~\pub{Neurocomputing22}  & {43.8} & 70.6 & 81.4 &\textbf{2.0} & 31.8 & 57.0 & 66.1 & 4.0 & 15.3 & 31.3 & 40.5 & \textbf{21.0}\\
EMCL-Net~{\cite{jin2022expectation}}\ssymbol{3}~\pub{NeurIPS22}  & {47.0} & 72.3 & 82.6 & \textbf{2.0} & 30.0 & 56.1 & 65.8 & 4.0 & 16.6 & 29.3 & 36.5 & 24.0 \\
\midrule
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & \textbf{49.0} & \textbf{75.2} & \textbf{82.7} & \textbf{2.0} & \textbf{33.2} & \textbf{59.3} & \textbf{68.4} & \textbf{3.0} & \textbf{17.1} & \textbf{32.4} & \textbf{41.0} & \textbf{21.0} \\
\bottomrule[1.25pt]
\end{tabular}}
\caption{\textbf{Text-to-video retrieval performance in out-domain retrieval settings.} ``\textit{A}-\textgreater{}\textit{B}'' indicates that ``\textit{A}'' is the source domain and ``\textit{B}'' is the target domain. ``\ssymbol{3}'' denotes our own re-implementation of baselines.}
\label{tab:out-domain}
\end{table*}

\myparagraph{Implementation Details.}
Following previous works~\cite{luo2021clip4clip,liu2022ts2,jin2022expectation,jin2022video}, we utilize the CLIP (ViT-B/32)~\cite{radford2021learning} as the pre-trained model. The dimension of the feature is 512. The temporal transformer~\cite{vaswani2017attention,li2022locality} is composed of 4-layer blocks, each including 8 heads and 512 hidden channels. The temporal position embedding and parameters are initialized from the text encoder of the CLIP. We use the Adam optimizer~\cite{kingma2014adam} and set the batch size to 128. The initial learning rate is 1e-7 for the text encoder and video encoder and 1e-3 for other modules. We set the temperature  to 0.01 and  to 1. For short video datasets, \ie, MSRVTT, LSMDC, and MSVD, the word length is 32 and the frame length is 12. For long video datasets, \ie, ActivityNet Captions and DiDeMo, the word length is 64 and the frame length is 64. The training is divided into two stages. In the first stage, we train the feature extractor from the discrimination perspective. In the second stage, we optimize the generator from the generation perspective. For the MSRVTT and LSMDC datasets, the experiments are carried out on 2 NVIDIA Tesla V100 GPUs. For the MSVD, ActivityNet Captions, and DiDeMo datasets, the experiments are carried out on 8 NVIDIA Tesla V100 GPUs. In both of the tasks of text-to-video and video-to-text retrieval, we assume that only the candidate sets are known in advance. In the inference phase, we consider both the distance of video and text representations in the representation space and the joint probability of video and text.

\subsection{Comparison with State-of-the-art}
We compare the proposed DiffusionRet with other methods on five benchmarks. In Tab.~{\ref{MSRVTT results}}, we show the retrieval results on the MSRVTT dataset. Our model outperforms the recently proposed state-of-the-art methods on both text-to-video retrieval and video-to-text retrieval tasks. Tab.~{\ref{Experiments0}} shows text-to-video retrieval results on the LSMDC, MSVD, ActivityNet Captions, and DiDeMo datasets. DiffusionRet achieves consistent improvements across different datasets, which demonstrates the effectiveness of our method. 


\subsection{Ablation Study}
\myparagraph{Generation loss type.} In Tab.~\ref{tab:loss}, we compare two common generation losses, \ie, mean-squared loss~(MSE) and Kullback-Leibler~(KL) divergence. Results demonstrate that the KL divergence achieves optimal retrieval performance. We explain that it is because KL divergence can better measure the distance between probabilities than MSE, so it is more suitable for probability generation.

\myparagraph{Sampling strategy.} Denoising diffusion probabilistic models~\cite{ho2020denoising}~(DDPM) learn the underlying data distribution by a Markov chain as shown in Eq.~\ref{Markov}. To accelerate the sampling process of diffusion models, denoising diffusion implicit models~\cite{song2020denoising}~(DDIM) formulate a Markov chain that reverses a non-Markovian perturbation process. As shown in Tab.~\ref{tab:sampling}, we find that the two sampling strategies have similar performance. To speed up the sampling process, we use DDIM by default in practice.

\myparagraph{Schedule of .} The schedule of  controls how the step size increases. We compare the linear schedule and cosine schedule in Tab.~\ref{tab:schedule}. As shown in Tab.~\ref{tab:schedule}, we find that the cosine schedule performs well, so we adopt the cosine schedule by default, which is the same as the default setting in the motion generation task~\cite{tevet2022human}.

\myparagraph{Training strategy.} In Tab.~\ref{tab:training}, we compare different training strategies. Similar to the findings of the pioneer work~\cite{bernardo2007generative}, we find that pure discriminant training has better performance on limited data than pure generative training. The hybrid training method we proposed achieves the best results, which indicates that our hybrid training strategy can combine the advantages of the two training methods.

\myparagraph{The number of steps.} In Tab.~\ref{tab:step}, we explore the influence of the number of diffusion steps. Results demonstrate that the number of steps of 50 achieves optimal performance in the retrieval task, outperforming the standard value of 1000 of the image generation task~\cite{ho2020denoising,dhariwal2021diffusion}. We consider that this is due to the simpler probability distribution in retrieval compared to the pixel distribution in natural images. Therefore, compared with the image generation task, the cross-modal retrieval task only needs fewer diffusion steps.

\myparagraph{Scale of .} The scale of  indicates the signal-to-noise ratio of the diffusion process~\cite{chen2022diffusiondet}. We evaluate the scale range  as shown in Tab.~\ref{tab:scale}. We find that the model achieves the best performance at the scaling of 1.0, so we set the scale of  to 1.0 in practice, which is the same as the default setting in the image generation task~\cite{dhariwal2021diffusion}.

\subsection{The Efficiency of DiffusionRet}
We provide comparisons of the inference time and memory consumption of our method with other methods under different conditions (the number of diffusion steps and the size of the candidate gallery) in Tab.~\ref{tab:cost}. As shown in Tab.~\ref{tab:cost}, our method is as efficient as the existing state-of-the-art methods during the inference stage, which we explain in the following three aspects. \textbf{(1) Lightweight denoising network.} Our denoising network is lightweight (2.50 M parameter), while other methods use complex matching networks, \eg, TS2-Net which uses the token shift transformer and token selection transformer for fine-grained alignment. \textbf{(2) Efficient feature extractor.} About 80\% of the inference time is spent on feature extraction such as extracting query text features. Compared with other methods with bulky feature extractors, \eg, X-Pool which uses an additional transformer-based pooling module to aggregate features, our method only uses vanilla transformer to extract features and is therefore more efficient. \textbf{(3) Scalability.} We can increase the number of diffusion steps to boost performance at a negligible time cost, indicating the scalability of our method. Compared with other methods, our method is more flexible and better suited to different retrieval scenarios where accuracy and speed are required differently.

\begin{figure*}[tbp]
\centering
\includegraphics[width=1.\linewidth]{images/experiment.pdf}
\vspace{-1.5em}
\caption{\textbf{The visualization of the diffusion process of the probability distribution.} We highlight the ground truth in green, and show the process from randomly initialized noise input () to the final predicted distribution ().}
\label{fig:v}
\end{figure*}

\subsection{Out-domain Retrieval}
Current text-video retrieval methods are mainly evaluated on the same dataset. To further evaluate the generalization to unseen data, we conduct out-domain retrieval~\cite{chen2020fine}: we first pre-train a model on one dataset (the ``source'') and then measure its performance on another dataset (the ``target'') that is unseen in the training. As shown in Tab.~\ref{tab:out-domain}, we compare the proposed DiffusionRet with other baselines (\ie, CLIP4Clip~\cite{luo2021clip4clip} and EMCL-Net~\cite{jin2022expectation}) in out-domain retrieval settings. We find that the discriminant approaches fail to migrate the performance of in-domain retrieval to out-of-domain retrieval well. For example, the performance of EMCL-Net is significantly higher than that of CLIP4Clip in in-domain retrieval. However, in out-domain retrieval, the performance of EMCL-Net is slightly lower than that of CLIP4Clip. In contrast, DiffusionRet performs well for both in-domain and out-of-domain retrieval.

To further illustrate the benefits of our generative modeling, we provide the visualization of the similarity distribution in in-domain retrieval and out-domain retrieval. As shown in Fig.~\ref{fig:p}, compared to the baseline models, our DiffusionRet maximally keeps the distributions of positive and negative pairs separate from each other in both in-domain and out-domain settings. These results confirm that our method is more generalizable and transferrable for the unseen data than typical discriminant models.

\subsection{Qualitative Analysis}
The coarse-to-fine nature of the diffusion model enables it to progressively uncover the correlation between text and video, rendering it an effective approach for cross-modal retrieval. To better understand the diffusion process, we show the additional visualization of the diffusion process in Fig.~\ref{fig:v}. These results demonstrate that our method can progressively uncover the correlation between text and video.

\subsection{Why Diffusion Models}
Diffusion models have demonstrated remarkable generative power in various fields. Besides the powerful generative power of diffusion models, we explain other advantages of applying the diffusion model rather than other generative approaches to cross-modal retrieval, mainly in two aspects. \textbf{First}, the coarse-to-fine nature of the diffusion model enables it to progressively uncover the correlation between text and video, rendering it a more effective approach for retrieval tasks than other generation training methods, such as generative adversarial network~\cite{goodfellow2020generative} and variational autoencoder~\cite{kingma2013auto}. \textbf{Second}, the many-to-many nature of the diffusion model makes it more suitable for generating joint probabilities than the auto-regressive networks~\cite{frey1995does,radford2018improving}. In Fig.~\ref{fig:v}, we show the visualization of the diffusion process. These results further demonstrate the above two advantages of applying the diffusion model rather than other generative approaches to cross-modal retrieval.

\section{Conclusion}
In this paper, we propose DiffusionRet, the first generative diffusion-based framework for text-video retrieval. By explicitly modeling the joint probability distribution of text and video, DiffusionRet shows promise to solve the intrinsic limitations of the current discriminant regime. It successfully optimizes the DiffusionRet from both the generation perspective and discrimination perspective. This makes DiffusionRet principled and well-applicable in both in-domain retrieval and out-domain retrieval settings. To the best of our knowledge, we are the first to tackle text-video retrieval from the generative viewpoint. Besides, we show that our generation modeling method is superior to existing discriminant methods in terms of performance and generalization ability. This work is the first effort to promote generative methods for text-video retrieval, which may be meaningful and helpful to the community.

\noindent \textbf{Acknowledgements.} This work was supported in part by the National Key R\&D Program of China (No. 2022ZD0118201), Natural Science Foundation of China (No. 61972217, 32071459, 62176249, 62006133, 62271465, 62202014). Li Yuan is also sponsored by CCF Tencent Open Research Fund.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\appendix

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\theequation}{\Alph{equation}}
\renewcommand{\thefigure}{\Alph{figure}}

\setcounter{table}{0}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{equation}{0}

\begin{table*}[htb]
\centering
\footnotesize
\resizebox{1.\linewidth}{!}
{
\begin{tabular}{l|cccccc|cccccc}
\toprule[1.5pt]
\multirow{2}{*}{\textbf{Method}} &  \multicolumn{6}{c|}{\textbf{LSMDC}}        & \multicolumn{6}{c}{\textbf{MSVD}}  \\ 
\cmidrule(rl){2-7}\cmidrule(rl){8-13}
  & R@1 & R@5 & R@10 & Rsum & MdR & MnR
& R@1 & R@5 & R@10 & Rsum & MdR & MnR    \\ \midrule
TT-CE~{\cite{croitoru2021teachtext}}~\pub{ICCV21}  & 17.5 & 36.0 & 45.0 & 98.5 & 14.3 & - & 27.1 & 55.3 & 67.1 & 149.5 & 4.0 & - \\
CLIP4Clip~{\cite{luo2021clip4clip}}~\pub{Neurocomputing22}  & 20.8 & 39.0 & 48.6 & 108.4 & 12.0 & 54.2 & 62.0 & 87.3 & 92.6 & 241.9 & \Frst{1.0} & \textbf{4.3} \\
EMCL-Net~{\cite{jin2022expectation}}~\pub{NeurIPS22}  & 22.2 & 40.6 & 49.2 & 112.0 & 12.0 & - & 54.3 & 81.3 & 88.1 & 223.7 & \textbf{1.0} & 5.6 \\
\midrule
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)}  & \Frst{23.0} & \Frst{43.5} & {51.5} & \Frst{118.0} & \Frst{9.0} & {40.2} & \textbf{61.9} & \Frst{88.3} & \Frst{92.9} & \Frst{243.1} & \textbf{1.0} & 4.5     \\
\rowcolor{aliceblue!60} + QB-Norm~\cite{bogolin2022cross} & {22.8} & {43.2} & \textbf{51.6} & {117.6} &\textbf{9.0} & \textbf{40.0} & {60.3} & {86.4} & {92.0} & {238.7} & \textbf{1.0} & 4.5 \\
\midrule[1.25pt]
\multirow{2}{*}{\textbf{Method}} &  \multicolumn{6}{c|}{\textbf{ActivityNet Captions}}        & \multicolumn{6}{c}{\textbf{DiDeMo}}  \\ 
\cmidrule(rl){2-7}\cmidrule(rl){8-13}
  & R@1 & R@5 & R@10 & Rsum & MdR & MnR
& R@1 & R@5 & R@10 & Rsum & MdR & MnR    \\ \midrule
TT-CE~{\cite{croitoru2021teachtext}}~\pub{ICCV21}  & 23.0 & 56.1 & - & - & 4.0 & - & 21.1 & 47.3 & 61.1 & 129.5 & 6.3 & - \\
CLIP4Clip~{\cite{luo2021clip4clip}}~\pub{Neurocomputing22}  & 41.4 & 73.7 & 85.3 & 200.4 & \textbf{2.0} & 6.7 & 41.4 & 68.2 & 79.1 & 188.7 & {2.0} & 12.4 \\
EMCL-Net~{\cite{jin2022expectation}}~\pub{NeurIPS22}  & 42.7 & 74.0 & - & - & \textbf{2.0} & - & 45.7 & 74.3 & 82.7 & 202.7 & 2.0 & 10.9 \\
HBI~\cite{jin2022video}~\pub{CVPR23} & 42.4 & 73.0 & 86.0 & 201.4 & \textbf{2.0} & 6.5 & 46.2 & 73.0 & 82.7 & 201.9 & 2.0 & \textbf{8.7} \\
\midrule
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)}  & {43.8} & {75.3} & \Frst{86.7} & {205.8} & \Frst{2.0} & \Frst{6.3} & 46.2 & {74.3} & {82.2} & 202.7 & {2.0} & 10.7      \\
\rowcolor{aliceblue!60} + QB-Norm~\cite{bogolin2022cross} & \textbf{47.4} & \textbf{76.3} & \textbf{86.7} & \textbf{210.4} &\textbf{2.0} & {6.7} & \textbf{50.3} &\textbf{75.1} &\textbf{82.9} & \textbf{208.3} &\textbf{1.0} & 10.3 \\ \bottomrule[1.5pt]
\end{tabular}
}
\caption{\textbf{Video-to-text retrieval performance on the LSMDC, MSVD, and ActivityNet Captions datasets.} ``'' denotes that higher is better. ``'' denotes that lower is better.}
\label{tab:video-to-text}
\end{table*}

\begin{table*}[t]
\footnotesize
\centering
\resizebox{1.\linewidth}{!}
{
\begin{tabular}{l|ccccc|ccccc}
\toprule[1.25pt]
\multirow{2}{*}{\textbf{Method}}  &\multicolumn{5}{c}{\textbf{MSRVTT}} &\multicolumn{5}{|c}{\textbf{MSRVTT-\textgreater{}ActivityNet Captions}}\\
\cmidrule(rl){2-6}\cmidrule(rl){7-11}
  & R@1 & R@5 & R@10 & Rsum & MdR 
& R@1 & R@5 & R@10 & Rsum & MdR  \\ \midrule
CLIP4Clip~{\cite{luo2021clip4clip}}\ssymbol{3}~\pub{Neurocomputing22}  & 43.8 & 70.6 & 81.4 & 195.8 & \textbf{2.0} & {29.1} & 58.3 & 72.1 & 159.5 & 4.0  \\
EMCL-Net~{\cite{jin2022expectation}}\ssymbol{3}~\pub{NeurIPS22} & 47.0 & 72.3 & 82.6 & 201.9 & \textbf{2.0} & {28.7} & 56.8 & 70.6 & 156.1 & 4.0\\
\midrule
\rowcolor{aliceblue!60} \textbf{DiffusionRet (Ours)} & \textbf{49.0} & \textbf{75.2} & \textbf{82.7} & \textbf{206.9} & \textbf{2.0}  & \textbf{31.5} & \textbf{60.0} & \textbf{73.8} & \textbf{165.3} & \textbf{3.0} \\
\bottomrule[1.25pt]
\end{tabular}
}
\caption{\textbf{Text-to-video retrieval performance in out-domain retrieval settings.} ``\textit{MSRVTT}-\textgreater{}\textit{ActivityNet Captions}'' denotes that the generalization results on unseen ActivityNet Captions test setting using pre-trained models on the MSRVTT dataset.``\ssymbol{3}'' denotes our own re-implementation of baselines. ``'' denotes that higher is better. ``'' denotes that lower is better.}
\label{tab:out-domain_others}
\end{table*}

\section{Appendix}
This appendix provides the descriptions of datasets (Sec.~\ref{apendix:dataset}), implementation details (Sec.~\ref{apendix:Implementation}), video-to-text retrieval performance on the LSMDC, MSVD, and ActivityNet Captions datasets (Sec.~\ref{appendix:video-to-text}), additional experiments for the out-domain retrieval (Sec.~\ref{appendix:out-domain}), the reason for applying diffusion models (Sec.~\ref{why_diffusion}), discussion of the limitations (Sec.~\ref{appendix:limitations}), the additional visualization of the diffusion process (Sec.~\ref{appendix:diffusion_process}), the visualization of the text-frame attention map (Sec.~\ref{appendix:t_f_attention}), and the visualization of the text-to-video retrieval examples (Sec.~\ref{appendix:text-to-video}).

\subsection{Datasets and Implementation Details}
\subsubsection{Datasets}\label{apendix:dataset}
We compare the proposed DiffusionRet with other methods on five benchmark text-video retrieval datasets, including MSRVTT~\cite{xu2016msr}, LSMDC~\cite{rohrbach2015a}, MSVD~\cite{chen2011collecting}, ActivityNet Captions~\cite{krishna2017dense}, and DiDeMo~\cite{anne2017localizing}. 

\myparagraph{MSRVTT.} MSRVTT~\cite{xu2016msr} contains 10,000 YouTube videos, each with 20 text descriptions. We follow the training protocol in \cite{liu2019use,gabeur2020multi,miech2019howto100m} and evaluate on text-to-video and video-to-text search tasks on the 1K-A testing split with 1K video or text candidates defined by \cite{yu2018a}.

\myparagraph{LSMDC.} LSMDC~\cite{rohrbach2015a} contains 118,081 video clips from 202 movies. The duration of videos in the LSMDC dataset is short. We follow the split of \cite{gabeur2020multi} with 1,000 videos for testing. 

\myparagraph{MSVD.} MSVD~\cite{chen2011collecting} contains 1,970 videos. Each video has approximately 40 associated text description. Videos in the MSVD dataset are short in duration, lasting about 10 to 25 seconds. We follow the official split of 1,200 and 670 as the train and test set, respectively. 

\myparagraph{ActivityNet Captions.} ActivityNet Captions~\cite{krishna2017dense} consists densely annotated temporal segments of 20K YouTube videos. Following~\cite{gabeur2020multi,patrick2021support,wang2021t2vlad}, we concatenate descriptions of segments in a video to construct ``video-paragraph'' for retrieval. We report results on the ``val1'' split of 10,009 and 4,917 as the train and test set. 

\myparagraph{DiDeMo.} DiDeMo~\cite{anne2017localizing} contains 10,464 videos annotated 40,543 text descriptions. We concatenate descriptions of segments in a video to construct ``video-paragraph'' for retrieval. We follow the training and evaluation protocol in~\cite{luo2021clip4clip}.

\begin{figure*}[tbp]
\centering
\includegraphics[width=1.\linewidth]{images/diffusion.pdf}
\caption{\textbf{The visualization of the diffusion process of the probability distribution.} We highlight the ground truth in green, and show the process from randomly initialized noise input () to the final predicted distribution (). The iterative refinement property and many-to-many nature of the diffusion model render it an effective approach for text-video retrieval.}
\label{fig:v_more}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=1.\linewidth]{images/diffusion-appendix.pdf}
\caption{\textbf{The visualization of the text-frame attention map.} These results demonstrate that our method can capture the correlation between text and frames.}
\label{fig:t_f_attention}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=1.\linewidth]{images/retrieval.pdf}
\caption{\textbf{The visualization of the text-to-video results.} We highlight the ground truth in green. These results demonstrate that our method can mine the correlation between text and video effectively.}
\label{fig:retrieval}
\end{figure*}

\subsubsection{Implementation Details.}\label{apendix:Implementation}
Following previous works~\cite{luo2021clip4clip,jin2022expectation,jin2022video,jin2023text}, we utilize the CLIP (ViT-B/32)~\cite{radford2021learning} as the pre-trained model. The dimension of the feature is 512. The temporal transformer~\cite{vaswani2017attention,li2022locality} is composed of 4-layer blocks, each including 8 heads and 512 hidden channels. The temporal position embedding~\cite{yu2022position} and parameters are initialized from the text encoder of the CLIP. We use the Adam optimizer~\cite{kingma2014adam} and set the batch size to 128. The initial learning rate is 1e-7 for the text encoder and video encoder and 1e-3 for other modules. We set the temperature  to 0.01 and  to 1. For short video datasets, \ie, MSRVTT, LSMDC, and MSVD, the word length is 32 and the frame length is 12. For long video datasets, \ie, ActivityNet Captions and DiDeMo, the word length is 64 and the frame length is 64. 

The training is divided into two stages. In the first stage, we train the feature extractor from the discrimination perspective. In the second stage, we optimize the generator from the generation perspective. For the MSRVTT and LSMDC datasets, the experiments are carried out on 2 NVIDIA Tesla V100 GPUs. For the MSVD, ActivityNet Captions, and DiDeMo datasets, the experiments are carried out on 8 NVIDIA Tesla V100 GPUs. In both of the tasks of text-to-video and video-to-text retrieval, we assume that only the candidate sets are known in advance. In the inference phase, we consider both the distance of video and text representations in the representation space and the joint probability of video and text. Code is available at \href{https://github.com/jpthu17/DiffusionRet}{https://github.com/jpthu17/DiffusionRet}.

\subsection{Additional Results and Discussions}
\subsubsection{Video-to-Text Retrieval}\label{appendix:video-to-text}
We compare the proposed DiffusionRet with other meth-
ods on five benchmark. In addition to the text-to-video retrieval results in the main paper, we provide video-to-text retrieval results on the LSMDC, MSVD, ActivityNet Captions, and DiDeMo datasets in Tab.~{\ref{tab:video-to-text}}. Extensive experiments on five datasets, including MSRVTT, LSMDC, MSVD, ActivityNet Captions, and DiDeMo, demonstrate that our method is capable of dealing with both short and long videos. DiffusionRet achieves consistent improvements across different datasets, which demonstrates the effectiveness of our method. 

\subsubsection{Out-domain Retrieval}\label{appendix:out-domain}
Most text-video retrieval methods~\cite{luo2021clip4clip,jin2022expectation,jin2022video,jin2023text} are evaluated using the same dataset, which may not reflect their ability to generalize to unseen data. To this end, we perform out-domain retrieval by pre-training a model on one dataset (referred to as the ``source'') and evaluating its performance on another dataset (referred to as the ``target'') that is not included in the training. In addition to the out-domain retrieval experiments in the main paper, we provide additional experiments in the out-domain retrieval setting (MSRVTT-\textgreater{}ActivityNet Captions) in Tab.~\ref{tab:out-domain_others}. We find that discriminant approaches do not transfer well from in-domain to out-of-domain retrieval. For instance, EMCL-Net outperforms CLIP4Clip in in-domain retrieval, but its performance is slightly lower than CLIP4Clip in out-domain retrieval. In contrast, DiffusionRet achieves good performance in both in-domain and out-of-domain retrieval.

\subsubsection{Why Diffusion Models}\label{why_diffusion}
Diffusion models have demonstrated remarkable generative power in various fields. Besides the powerful generative power of diffusion models, we explain other advantages of applying the diffusion model rather than other generative approaches to cross-modal retrieval, mainly in two aspects. \textbf{First}, the coarse-to-fine nature of the diffusion model enables it to progressively uncover the correlation between text and video, rendering it a more effective approach for retrieval tasks than other generation training methods, such as generative adversarial network~\cite{goodfellow2020generative} and variational autoencoder~\cite{kingma2013auto}. \textbf{Second}, the many-to-many nature of the diffusion model makes it more suitable for generating joint probabilities than the auto-regressive networks~\cite{frey1995does,radford2018improving}. We recommend further investigation of the potential of the generative method for discriminant tasks in future research. In our future work, we will explore our algorithm in segmentation~\cite{li2022dynamic,li2023multi} and visual question answering~\cite{li2022joint,li2023weakly,li2023tg}.


\subsubsection{Limitations of our Work}\label{appendix:limitations}
Generative models have focused on generative tasks, \eg, image generation~\cite{ho2020denoising,song2020denoising}, natural language generation~\cite{austin2021structured,li2022diffusion}, and audio generation~\cite{popov2021grad}. Some other works have attempted to adapt the generative models for discriminant tasks, \eg, image segmentation~\cite{amit2021segdiff}, visual grounding~\cite{cheng2023parallel}, and detection~\cite{chen2022diffusiondet}. However, these precursor methods require additional discriminative training. To train on limited data, we optimize the proposed generation model from both generation and discrimination perspectives. Although such a hybrid training method can improve model performance with limited data, we believe that pure generative training is a more promising solution when the data is sufficient. We suggest exploring a pure generative training approach to the retrieval problem in the future.


\subsection{Additional Visualizations}
\subsubsection{Diffusion Process}\label{appendix:diffusion_process}
The coarse-to-fine nature of the diffusion model enables it to progressively uncover the correlation between text and video, rendering it an effective approach for cross-modal retrieval. To better understand the diffusion process, we show the additional visualization of the diffusion process in Fig.~\ref{fig:v_more}. These results demonstrate that our method can progressively uncover the correlation between text and video.

\subsubsection{Text-Frame Attention Map}\label{appendix:t_f_attention}
To extract the joint encoding of text and video, we propose the text-frame attention encoder, which takes text representation as query and frame representation as key and value. To better understand the process of joint encoding of text and video, we show the visualization of the text-frame attention map in Fig.~\ref{fig:t_f_attention}. As shown in Fig.~\ref{fig:t_f_attention}, the text-frame attention encoder adaptively extracts the frames that are similar to the text so that fine-grained video features can be extracted. These results demonstrate that our method can capture the correlation between text and frames.

\subsubsection{Text-to-Video Retrieval}\label{appendix:text-to-video}
We show two retrieval examples from the MSRVTT testing set for text-to-video retrieval in Fig.~\ref{fig:retrieval}. As shown in Fig.~\ref{fig:retrieval}, our method successfully retrieves the ground-truth video. These results demonstrate that our method can mine the correlation between text and video effectively.

\end{document}