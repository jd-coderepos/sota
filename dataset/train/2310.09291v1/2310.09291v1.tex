
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow,multicol}
\usepackage{graphicx}
\usepackage{url}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{wrapfig}
\usepackage{xspace}


\definecolor{asparagus}{rgb}{0.53, 0.66, 0.42}

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\massi}[1]{\textcolor{asparagus}{#1}}
\newcommand{\karsten}[1]{\textcolor{pink}{#1}}
\newcommand{\zeynep}[1]{\textcolor{red}{#1}}
\newcommand{\acronym}{{Compositional Image Retrieval through Vision-by-Language}}
\newcommand{\methodName}{\texttt{CIReVL}\xspace} \newcommand{\methodNameNS}{\texttt{CIReVL}}
\title{Vision-by-Language for \\Training-Free Compositional Image Retrieval}

\iclrfinalcopy

\author{Shyamgopal Karthik$^{1,*}$, Karsten Roth$^{1,*}$, Massimiliano Mancini$^{2}$, Zeynep Akata$^{1, 3}$\\
\small{$^{1}$University of Tübingen, Tübingen AI Center, $^{2}$University of Trento, $^{3}$MPI for Intelligent Systems}\\
\small{$^*$equal contribution}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle



\begin{abstract}
Given an image and a target modification (e.g an image of the Eiffel tower and the text ``without people and at night-time''), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we propose to tackle CIR in a training-free manner via our \acronym\ (\methodNameNS), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods. Moreover, the modularity of \methodNameNS\ offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results.  Finally, we show that \methodNameNS\ makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.
\end{abstract}





\section{Introduction}
Compositional Image Retrieval (CIR) necessitates a nuanced coupling between the image content and the semantics of the textual query to retrieve a new image that accurately embodies the relevant image elements and the modifications described in the textual query. 
To achieve this, previous works require curated triplets (query image, modifying text, target image) to train a specific CIR system.
However, annotating such triplets is both difficult and labor-intensive. To tackle this problem, recent research proposed Zero-Shot CIR (ZS-CIR)~\citep{pic2word,searle}. Based on large-scale pre-trained vision-language models (VLMs) (e.g. CLIP~\citep{clip}), these methods use image-caption pairs to train textual inversions~\citep{gal2022image,palavra} mapping images to text tokens. A static template merges tokens and textual modifications to obtain target captions, performing CIR without explicit supervision. Thus, even when leveraging large-scale VLMs, ZS-CIR methods still train additional mapping networks on large image-caption datasets.

In this work, we propose to achieve training-free ZS-CIR by leveraging ubiquitously available, off-the-shelf models already trained with large-scale training data. 
Our \acronym\ (\methodNameNS) follows the vision-by-language paradigm~\citep{zeng2022socratic,chatgptblip,languagelens,levy2023chatting}, which uses language as an abstraction layer for reasoning about visual content. 
Specifically, \methodName employs vision-language models like BLIP-2~\citep{blip2} or CoCa~\citep{coca} to generate a detailed description of the query image. 
Subsequently, a LLM (s.a. Llama~\citep{llama2} or GPT~\citep{gpt-3}) crafts a caption for the desired target image, using both the generated description and the respective textual query. Finally, a VLM like CLIP~\citep{clip} retrieves the image.  
\methodName abstracts away the compositional nature of the problem into the language domain, converting ZS-CIR into an inherently modular task comprising captioning, reasoning and cross-modal retrieval.

This opens up various benefits not available in traditional, trained ZS-CIR approaches. Beyond not requiring additional adaptation resources, the training-free and modular nature offers the flexibility for simple model changes and replacements, allowing us to scale up \methodName using freely available models. Consequently, while \methodName already matches and even outperforms trained methods on the common ZS-CIR benchmarks CIRCO~\citep{searle} and CIRR~\citep{cirr} using comparable model architectures, simple plug-and-play of large retrieval models raises improvements significantly, in parts more than doubling previous results.
In addition, \methodName is modular and operates primarily in the language domain, as the outputs of the captioning module and the LLM-generated modifications are textual, offering a degree of understanding over the compositional retrieval process to humans. 
This is further reflected in the ability for possible human intervention on the retrieval process to fix or post-hoc improve results (Fig.\ref{fig:intervention}). 
Finally, we show the generality of \methodName on domain conversion~\citep{pic2word} and conditional image similarity ~\citep{vaze2023genecis}, and ablation studies elucidate the role of each pipeline component.

Overall, our contributions are:
1) We explore training-free zero-shot compositional image retrieval, proposing \methodNameNS, a new approach that matches or outperforms existing training-based methods on four CIR benchmarks while only relying on off-the-shelf available pre-trained models. 2) We show how the inherent modularity of \methodName and its reasoning over the textual query in the language domain facilitates a degree of human understanding over the compositional retrieval process, even allowing for user-level intervention. 3) We conduct multiple additional studies to ablate pipeline components and point to the importance of language-level reasoning over the textual query, while highlighting the simple scalability of \methodName through its modular, training-free nature.

\section{Related Work}
\textbf{Compositional Image Retrieval.} 
The task of Compositional Image Retrieval has found significant application in conditional search~\citep{guo2019fashion,fashion200k,tirg}, where users perform interactive dialogue to refine a given query image toward retrieving specific items. Classical techniques often employ custom models that project text-image pairs into a common embedding space~\citep{tirg,combiner,chen2020image,chen2020learning,lee2021cosmo,anwaar2021compositional} or use cross-modal attention mechanisms~\citep{artemis} and is closely related to compositional learning~\citep{redwine,compcos,kgsp}. With the advent of vision-language foundation models~\citep{bommasani2021opportunities,clip,align}, interest in CIR has surged, especially in zero-shot settings that avoid the need for task-specific models. Two prominent directions exist: one utilizes a single token to represent the reference image, which is then concatenated with the reference caption \citep{pic2word,searle}; the other involves training foundation models on curated triplets tailored for CIR \citep{liu2023zeroshot,compodiff,ventura2023covr,levy2023data}. Our work explores a different pipeline where, by coupling VLMs with Large Language Model (LLM), we address CIR without any specialized training, in an effective and interpretable manner.\vspace{5pt}\\
\textbf{Vision-Language Models.} Models like CLIP~\citep{clip} and ALIGN~\citep{align} have been trained on expansive datasets such as LAION-400M/5B~\citep{laion400m,laion5b}, enabling them to map images and text into a shared embedding space. These models have seen wide-ranging usage, from generative tasks~\citep{rombach2022stablediffusion,dall-e2,makeascene,liu2022compositional,chefer2023attendandexcite,karthik2023if} to open-vocabulary classification~\citep{clip,openclip,menon2023visual,pratt2023does,udandarao2023susx,roth2023waffling} and (cross-modal) retrieval \citep{Bogolin_2022_CVPR,bain2022clip,langguidance,wu2023cap4video}. 
Further advancements include models like BLIP~\citep{blip,blip2} and Flava~\citep{flava}, which extend beyond shared space projection to address other vision-language tasks like captioning~\citep{vinyals2016show} and visual question answering~\citep{antol2015vqa}. While these models have been indirectly applied to CIR through specialized {modules}~\citep{tirg,combiner,artemis} and with fine-tuning \citep{compodiff}, our work demonstrates that vision-language models alone, when partnered with an LLM, can suffice for effective CIR without additional training.



\section{\methodNameNS \ Methodology}
\label{sec:method}
This section first details the specifics of the ZS-CIR task in \S\ref{subsec:prelims}, before presenting our proposed approach, \acronym\ (\methodNameNS), in \S\ref{subsec:method}.

\subsection{Preliminaries}
\label{subsec:prelims}
Let us define as $\mathcal{I}$  and $\mathcal{T}$ the image and text space, respectively. 
For compositional image retrieval (CIR), a text modifier $t\in\mathcal{T}$ describes hypothetical semantic changes on a query image $Q \in \mathcal{I}$, whose closest realization $I_q^t\in\mathcal{D}$ from some image database $\mathcal{D} = \{I_i, \cdots, I_n\}$ should be retrieved.
This inherently multi-modal task can be defined as a scoring task $\Phi: \mathcal{I}\times \mathcal{T} \times \mathcal{D} \rightarrow {\rm I\!R}$. 
While in standard CIR $\Phi$ is learned via supervised training, common zero-shot CIR (ZS-CIR) approaches sidestep this need by tuning specific modules to invert the query image into an associated text. 
Specifically, they learn an inversion function $\phi_i:\mathcal{I}\rightarrow\mathcal{Z}$ mapping a given query image to a pre-defined text-token embedding space $\mathcal{Z}$. 
Practically, $\phi_i$ is trained over intermediate image representation of a specific image encoder $\Psi_I$ \citep{pic2word,searle}, often part of a large-scale pre-trained vision-language representation system such as CLIP~\citep{clip}.
Template filling around the text modifier over the corresponding inverted embedding $\mathtt{inv}_q = \phi_i(\Psi_I(Q))$ is then used to aggregate the information into one target caption (e.g. \textit{"a photo of $\{\mathtt{inv}_q\}$ that $\{t\}$"}). 
This target caption is then used for target image retrieval by VLMs like CLIP, encoding it using the associated pre-trained text encoder $\Psi_T$ that projects the target caption and candidate images $I\in\mathcal{D}$ into a shared, searchable embedding space.
The respective matching score is then $\mathtt{cos\_sim}(\Psi_I(I), \Psi_T(\mathtt{inv}_q))$ with cosine similarity $\mathtt{cos\_sim}$.

While promising, such a pipeline exhibits certain shortcomings, in that one 
i) needs to train a specific inversion module $\phi_i$ dependent on the chosen VLM and a separate image-caption dataset, 
ii) text embedding vectors can not be ensured to be human-understandable and cannot be verified as a correct description of the image, and 
iii) using rigid template filling does not allow for free-form textual representations and semantically flexible target captions.  


\subsection{Compositional Image Retrieval through Vision-by-Language}
\label{subsec:method}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/method-v4.pdf}
    \caption{
    \acronym\ (\methodNameNS). Given an input image and a text modifier, we use an off-the-shelf vision-language model to caption the image. A LLM processes the generated caption and the text modifier to generate a description of the desired target image. To obtain the final image, we use a vision-language model and perform text-to-image retrieval. \methodName is modular, training-free and human-understandable in natural language.}
    \label{fig:method}
\vspace{-5pt}
\end{figure} We can alleviate all the aforementioned shortcomings through \acronym\ (\methodNameNS) - a simple approach that recombines existing and publicly available pre-trained VLMs and LLMs. 
Similar to existing ZS-CIR methods, we build on CLIP as our retrieval system, however in a fashion that operates entirely independent on the particular CLIP model choice.
We also assume access to pre-trained captioning models, such as BLIP~\citep{blip,blip2} or CoCa~\citep{coca} which are readily available online, to provide a textual caption for a given image. Finally, we leverage a LLM for textual reasoning~\citep{huang2022towards}, available e.g. through Llama~\citep{llama2}, Vicuna~\citep{vicuna2023} or the GPT-framework~\citep{gpt-3}. We visualize our framework in Fig.~\ref{fig:method}.\\

\textbf{From text embeddings to captions.} Textual inversion has two main issues. First, it relies on a specifically trained image-to-token-embedding mapping, tailored to image representations produced by a pre-defined, pre-trained encoder (e.g. CLIP). Second, predicted inversion tokens have no guarantee to be human-understandable.
Both shortcomings are created by the learned token-generation module, and can thus be tackled by replacing it with an alternative system.
As traditional token inversion methods rely on the existence of large-scale pre-trained models, a natural alternative is to find solutions from the larger corpus of large-scale pre-trained models.
Specifically, by replacing trained textual inversion functions with an available image captioning system $\Psi_C$ through pre-trained generative VLMs such as BLIP, a simple solution for both problems can be found.  Specifically, given a query $Q$, we obtain its textual representation as $c_q=\Psi_C(Q)\in\mathcal{T}$. As $c_q$ lives in the natural language text domain $\mathcal{T}$, it remains possible for humans to reason over - allowing the user clearer insights into the retrieval process and allowing for possible intervention, as we explore in \S\ref{subsec:analysis}.

\textbf{From templates to reasoning targets.} Using directly image captions $c_q$ is not sufficient as it does not incorporate the essential context provided by the text modifier $t$. While a simple pre-defined template may recombine these in specific, fixed ways, they have no flexibility to account for different forms of textual modifiers and caption-forms most suitable to a particular task at hand. To address this issue, we make use of the reasoning capabilities of existing LLMs. Rather than combining $c_q$ and $t$ in a fixed way using template filling, our goal is to obtain a unified target caption that models the hypothetical effect of $t$ on the query image $Q$ as a change in the thus resulting image caption $c_q$.
This can be done in various ways, but we found simple prompts $p$ to encode sufficient problem context already (see \S\ref{sec:app-implementation} for more details). 
In particular, given a LLM of choice $\Psi_R$, we generate an instruction-alterated image caption as $c_q^t = \Psi_R(p \circ c_q \circ t)$, which queries the LLM with a concatenation of the base prompt $p$, the generated image caption $c_q$ (prepended with \texttt{"Image Content:"}), and the instruction $t$ (prepended with \texttt{"Instruction:"})\footnote{For possible adaptation to the dataset without training, simple in-context learning (see e.g. \cite{dong2023survey} for an overview) can also be leveraged, in which conceptual examples depicting sample captions, instructions and desired LLM output are appended to the prompt $p$.}. Example queries and outputs are provided in \S\ref{subsec:analysis}. 
Note that the construction of the prompt is a one-time process requiring minimal effort and annotation, which we found to translate well across all problems. In addition, as the instruction-conditioning happens entirely in the language domain, a human-in-the-loop can fully reason about the impact of the instruction on the retrieval process by comparing $c_q$ and $c_q^t$ w.r.t. $t$.

\textbf{Compositional image retrieval.}
Given the adapted caption $c_q^t$, \methodName encodes the image-search database $\mathcal{D}$ alongside $c_q^t$ using a VLM (e.g. CLIP). The retrieved target $I_q^t$ is thus given as
\begin{equation}
    \label{eq:retrieval}
    I_q^t = \underset{I\in\mathcal{D}}{\mathtt{argmax}}\; \frac{{\Psi_I(I)}^\intercal \Psi_T(c_q^t)}{||\Psi_I(I)|| \cdot ||\Psi_T(c_q^t)||},
\end{equation}
where the final selected target image is the one most similar to the generated target caption. As the image retrieval system is only introduced and utilized \textit{after} the combination of query image and instruction, it is entirely detached and modular. Consequently, it can be easily exchanged with other ones depending on practical requirements and the desired trade-off between efficiency and efficacy.
Overall, this results in a ZS-CIR pipeline in which compositions are human understandable as it operates entirely in the language domain, and the retrieval process exists as a clearly separated module, without requiring training of any mapping function on top of it.










\section{Experiments}
\label{sec:experiments}
We first provide the experimental details in \S\ref{subsec:implementation_details}, before showcasing the results of our \methodName in four different ZS-CIR tasks in \S\ref{subsec:benchmarks}.
Finally, we provide an in-depth analysis of our method in \S\ref{subsec:analysis}, highlighting it's capacity as well as the impact of the various components. 

\begin{table}[t!]
    \centering
    \caption{\textbf{Comparison on CIRCO and CIRR Test Data.} On CIRCO, \methodNameNS~significantly outperforms even adaptive methods across retrieval models, while it achieves competitive results on CIRR despite the noise in the benchmark. Its modularity allows for simple further scalability for additional gains. ($^*$) ViT-G/14 uses OpenCLIP weights \citep{openclip}.}
    \vspace{-5pt}    
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|l|cccc||cccc|ccc}
    \toprule
    \multicolumn{2}{c}{Benchmark} & \multicolumn{4}{|c||}{\textbf{CIRCO}}& \multicolumn{7}{|c}{\textbf{CIRR}}\\
    \midrule
    \multicolumn{2}{c}{Metric} & \multicolumn{4}{|c||}{mAP@k}& \multicolumn{4}{|c|}{Recall@k}& \multicolumn{3}{c}{$R_s$@k}\\
     Arch & Method & k=5 & k=10 & k=25 & k=50 & k=1 & k=5 & k=10 & k=50 & k=1 & k=2 & k=3 \\
     \midrule
     \multirow{5}{*}{ViT-B/32} & Image-only & 1.34 & 1.60 & 2.12 & 2.41 & 6.89 & 22.99 & 33.68& 59.23 &21.04& 41.04& 60.31\\
      & Image + Text & 2.65 & 3.25 & 4.14 & 4.54 & 11.71& 35.06& 48.94& 77.49 & 32.77& 56.89& 74.96\\
      & PALAVRA  & 4.61 & 5.32 & 6.33 & 6.80& 16.62 & 43.49 & 58.51 & 83.95 & 41.61 & 65.30 & 80.94\\
     & SEARLE  & 9.35 & 9.94 & 11.13 & 11.84& 24.00 & 53.42 & 66.82 & 89.78 & 54.89 & 76.60 & 88.19 \\
       & \textbf{\methodNameNS} & 14.94 & 15.42 & 17.00 & 17.82& 23.94 & 52.51 & 66.0 & 86.95 & 60.17 & 80.05 & 90.19 \\
    \midrule
     \multirow{3}{*}{ViT-L/14}& Pic2Word  & 8.72 & 9.51 & 10.64 & 11.29 & 23.90 & 51.70 & 65.30 & 87.80 & - & - & - \\
      & SEARLE  & 11.68 & 12.73 & 14.33 & 15.12& 24.24 & 52.48 & 66.29 & 88.84 & 53.76 & 75.01 & 88.19\\
      & \textbf{\methodNameNS} & 18.57 & 19.01 & 20.89 & 21.80 & 24.55 & 52.31 & 64.92 & 86.34 & 59.54 & 79.88 & 89.69\\  
     \midrule
     ViT-G/14$^*$ & \textbf{\methodNameNS} & 26.77 & 27.59 & 29.96 & 31.03& 34.65 & 64.29 & 75.06 & 91.66 & 67.95 & 84.87 & 93.21\\
    \bottomrule
    \end{tabular}}    
    \label{tab:circo}
\end{table} \subsection{Implementation Details} 
\label{subsec:implementation_details}
For our experiments we use PyTorch~\citep{pytorch}, extending the public codebase of \cite{searle}, and using clusters of NVIDIA V100 and A100s. We experiment with different ViT-variants~\citep{vit} of CLIP, with weights taken from the official implementation in \citep{clip}. We use the OpenCLIP~\citep{openclip} models for the analysis on scaling laws. As captioner, we leverage the open-source BLIP-2~\citep{blip2} with a Flan-T5XXL language model~\citep{flant5}. Ablations consider also BLIP~\citep{blip} and CoCa~\citep{coca}. As LLM we use gpt-3.5-turbo \cite{gpt-3}, but we experiment also with Vicuna13B~\citep{vicuna2023}, Llama2-70B~\citep{llama2}, and GPT-4~\citep{gpt4}.

\paragraph{Datasets and Baselines.} We use the CIRR~\citep{cirr}, CIRCO~\citep{searle} and GeneCIS~\citep{vaze2023genecis} datasets which have all been used for CIR. CIRR, the first natural image dataset for CIR, suffers from false negatives~\citep{searle}, since it has only a single target image annotated. The recently introduced CIRCO dataset largely ameliorates this by having multiple positive images for each query. The GeneCIS dataset (sourced from MS-COCO~\citep{coco} and Visual Attributes in the Wild \citep{vaw,visualgenome}) introduces four task variations, retrieving or changing a specific attribute or object. Due to space constraints, we report results on the ImageNet Domain benchmark~\citep{pic2word} in the appendix. 

Following the original benchmarks, we use Recall@k as the main metric on the CIRR, GeneCIS, and ImageNet Domain. On CIRR, we also  evaluate with the subset setting where there are only a few selected images in the gallery. On the CIRCO dataset, since there are multiple positives, we use the mean average precision@k (mAP@k). We use the `image-only', `text-only' and `image+text' to denote directly performing retrieval with CLIP using only the reference image, modifying instruction, as well as averaging the embeddings for the reference image and modifying text. PALAVRA~\citep{palavra}, Pic2Word~\citep{pic2word}, SEARLE~\citep{searle} are the textual inversion methods either designed or adapted for ZS-CIR. Finally, we also have results with a Combiner~\citep{combiner} architecture being trained on a CIR dataset. 

\subsection{ZS-CIR Benchmark Comparisons}
\label{subsec:benchmarks}
\begin{table}[t]
\centering
\caption{\textbf{Evaluation on GeneCIS Test Data.} Methods using CIR triplets marked \gray{gray}. The competitive performance of \methodNameNS~across subtasks even against adapted baselines indicates its generality.
}
\vspace{-5pt}
\label{tab:genecis}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc|ccc|ccc|ccc|cc}
\toprule
& \multicolumn{3}{c}{Focus Attribute} & \multicolumn{3}{c}{Change Attribute} & \multicolumn{3}{c}{Focus Object} & \multicolumn{3}{c}{Change Object} \\
\cmidrule(rl){2-4}
\cmidrule(rl){5-7}
\cmidrule(rl){8-10}
\cmidrule(rl){11-13}
&  R@1 & R@ 2 & R@3 & R@1 & R@ 2 & R@3 & R@1 & R@ 2 & R@3 & R@1 & R@ 2 & R@3 & Avg. R@1 \\
\midrule
Image Only        &  17.7       & 30.9       & 41.9 & 11.9       & 20.8       & 28.8 & 9.3       & 18.2       & 26.2 & 7.2       & 16.7       & 24.9   & 11.5    \\
Text Only         &  10.2       & 20.5       & 29.6 & 9.5       & 17.6       & 26.4 & 6.5       & 16.8       & 22.4 & 6.2       & 13.9       & 21.4    & 8.1   \\
Image + Text      &  15.6       & 26.3       & 37.1 & 12.6       & 22.9       & 32.0 & 10.8       & 21.0       & 31.2 & 11.3       & 21.5       & 30.3   & 12.6    \\
\gray{Combiner (CIRR)}      &  \gray{15.1}       &  \gray{27.7}       & \gray{39.8} & \gray{12.1}       & \gray{22.8}      & \gray{31.8} & \gray{13.5}       & \gray{25.4}       & \gray{36.7} &     \gray{15.4}   & \gray{28.0}       & \gray{39.6}   & \gray{14.0}    \\

\gray{Combiner (CC3M)}              &  \gray{19.0}       & \gray{31.0}       & \gray{41.5} & \gray{16.6} & \gray{27.5} & \gray{36.5} & \gray{14.7} & \gray{25.9}       & \gray{36.1} & \gray{16.8} & \gray{29.1} & \gray{39.7} & \gray{16.8}    \\  
\midrule
\textbf{\methodNameNS} & 17.9 & 29.4 & 40.4 & 14.8 & 25.8 & 35.8 & 14.6 & 24.3 & 33.3 & 16.1 & 27.8 & 37.6 & 15.9\\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\end{table} 
\paragraph{CIRCO.} 
Our results in Tab.~\ref{tab:circo} list the performance on the hidden test set of CIRCO, accessible through the submission server provided \cite{searle}. 
As can be seen, using the default ViT-B/32 and ViT-L/14 CLIP variants, our approach significantly outperforms methods like Pic2Word, SEARLE and PALAVRA~\citep{palavra}. 
For instance, on ViT-L/14, we achieve a mAP@5 of $18.57\%$ - notably improving over the $11.68$\% by the best performing, \textit{trained} alternative SEARLE, and more than doubling the performance of Pic2Word, which achieves $8.72\%$.
These are strongly indicative results, as CIRCO constitutes the dataset with the cleanest annotations and - unlike other datasets in this field - the inclusion of multiple positives for an inherently ambiguous problem, as textual modifications of an image do not only have a single solution.
These strong results thus provide key evidence of the efficacy of our training-free, vision-by-language approach for ZS-CIR.

\vspace{-5pt}
\paragraph{CIRR.} For the hidden CIRR test set (accessible using a test server, see e.g. \citet{cirr}), we provide results in Tab.~\ref{tab:circo}. 
As previously noted, this dataset is very noisy, with results primarily dependent on the modifying instruction, and the actual reference image having much less relation to the target image \citep{pic2word,searle}.
Still, even on this benchmark, we are able to match the performance of prior ZS-CIR methods (e.g. $R@1=24.55\%$ for our method versus $24.24\%$ for SEARLE), without requiring any form of problem-specific training. The CIRR benchmark also provides another evaluation where the correct image has to be retrieved from 6 curated samples. In this evaluation, our results surpass prior work by a significant margin ($R_s@1=60.17\%$ versus $54.89\%$ for SEARLE).
This illustrates the versatility of our method, allowing it to perform well across different scenarios without major issues.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/Success_Cases.pdf}
    \vspace{-10pt}
    \caption{Examples from the CIRCO validation set where our method retrieves the desired image. We see that our method is able to perform this task for a wide variety of modifier texts.}
    \label{fig:success}
\vspace{-5pt}
\end{figure} 
\vspace{-5pt}
\paragraph{GeneCIS.} The versatility of \methodNameNS\ is further underlined when transferring it to the GeneCIS benchmark.
Unlike CIRCO and CIRR, modifiers do not involve detailed instructions, but rather a single word that has a different interpretation for each task, e.g. focusing/changing a particular attribute or object. In this case, without re-training any method, simple prompt modifications allow for convincing performance. In particular, for the "focus"-tasks, we simply task the LLM to \textit{retain the attribute/object listed in the instruction}. For "change"-tasks, we simply ask it to \textit{replace the corresponding object} in the caption.
As shown in Tab.~\ref{tab:genecis}, \methodNameNS\ nearly matches the performance of Combiner trained on a large filtered version of the CC3M dataset~\citep{cc3m} ($16.8\%$ Average $R@1$ versus $15.9\%$), and surpass Combiner model finetuned on the CIRR dataset ($14.0\%$), across all tasks. It also clearly outperforms any other reference baseline.
This is notable given the diversity, but also specificity of the tasks, ranging from settings resembling standard image retrieval (e.g. "focus attribute"), or more traditional CIR tasks instead (e.g. "change object").
Importantly, existing ZS-CIR methods like Pic2Word and SEARLE are not applicable to this benchmark, as the single-word instruction is too ambiguous to be effectively incorporated in a template \textit{without} an interpretable representation/caption of the query image (as provided by our model). 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/Failure_Cases.pdf}
    \caption{We analyze failure cases of our approach. Due to the interpretable nature, we can easily attribute errors to captioning, reasoning or the text-image retrieval. We also find examples where the model is penalized despite a plausible retrieval due to insufficient annotations in the dataset.}
    \label{fig:failure}
\end{figure} \begin{figure}[t]
     \centering
     \includegraphics[width=1.0\textwidth]{figs/sample_intervention.pdf}
     \vspace{-5pt}
\caption{We demonstrate the possibility of user interventions to enhance the performance of our method. For instance, by fixing the mistakes in the generated caption, we are able to correctly retrieve the desired image without having to make any other changes.}
\vspace{-5pt}
\label{fig:intervention}
\end{figure} 
\subsection{Ablation Study and Performance Analysis}
\label{subsec:analysis}
In this section, we conduct a large number of analyses to provide a better understanding of our proposed method through ablations and qualitative examples, and provide insights into performance bottlenecks. This also includes scalability studies, and insights into the benefits of human understandability via natural language operations, further enabling the possibility to perform interventions on the retrieval process in order to tackle failure cases.

\paragraph{On the importance of textual reasoning.} Since our proposed method heavily relies on the LLM to convert textual inputs to target captions, we study several LLMs to understand the effect they play on the final performance. The results on the CIRCO validation set in Table \ref{tab:llm} illustrate that the reasoning is critical to the overall performance. Most notably, we perform a sanity-check where the generated caption is used to fill a template as done in prior works~\citep{pic2word,searle}. This method (`Captioning') aims to test the necessity of using a LLM for textual reasoning on this task. We see that this method is outperformed by all the LLMs that we tested (mAP@5 of $9.22\%$). This highlights the necessity of performing textual reasoning to generate the final target caption, and the limits of static templates. For instance, both GPT-3.5-turbo and GPT-4 achieve strong results (mAP@5 of $13.33\%$ and $15.63\%$ respectively), with the improved capabilities of GPT-4 offering significant performance gains on top. When moving to publicly accessible LLMs such as Llama2-70B and Vicuna-13B ($10.22\%$ and $12.65\%$), we find a fair drop in performance. For cases where access to closed-source APIs such as GPT-3.5-turbo or GPT-4 is restricted, alternative usage of public LLMs can thus still offer significant benefits, particularly when evaluating the differences between Vicuna-13B and GPT-3.5-turbo.

\paragraph{On the importance of captioning quality.} As our CIR framework builds on the generated reference image caption, we also ablate the importance of the correct captioning model choice in Tab.~\ref{tab:llm}, which reports results on the CIRCO validation split for faster turnaround. 
Interestingly, we find that most state-of-the-art public captioning models perform similarly well, with minor differences between BLIP versions, and slightly improved performance of CoCa. To retain the generality of our approach, we stick with BLIP-2 with a Flan-T5 language model since the usage of a LLM decoder makes it a more generic tool usable across a variety of domains.

\begin{table}[t!]
    \centering
    \caption{\textbf{Analysis of the impact of LLM and captioner choice.} Comparison between different LLMs on CIRCO validation reveals a positive relation between performance and reasoning capacities. For captioning models, most recent off-the-shelf models achieve strong results.}
\vspace{-2pt}
        \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c c|cccc}
    \toprule
     Arch & Captioner & LLM & mAP@5 & mAP@10 & mAP@25 & mAP@50\\
     \midrule
     \multirow{5}{*}{ViT-B/32} & \multirow{5}{*}{BLIP-2} & - & 9.22 & 10.05 & 11.44 & 12.08\\
     & &LLama2-70B & 10.22 & 10.60 & 11.80 & 12.39\\
     & &Vicuna-13B & 12.65 & 13.17 & 14.48 & 15.20\\
      & &GPT-3.5-Turbo & 13.33 & 14.16 & 15.74 & 16.35\\
      & &GPT-4 & 15.63 & 16.31 & 17.02 & 18.12\\
    \midrule
     \multirow{3}{*}{ViT-B/32}  & BLIP & \multirow{3}{*}{GPT-3.5-Turbo}& 13.1 & 13.33 & 15.08 & 15.92\\
      & BLIP-2 & & 13.33 & 14.16 & 15.74 & 16.35\\
     & CoCa & & 13.64 & 13.37 & 15.10 & 15.90\\
    \bottomrule
    \end{tabular}}    
    \label{tab:llm}
\vspace{-8pt}     
\end{table} \begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-5pt}
\centering
    \includegraphics[width=0.5\textwidth]{figs/tifa_score_comp_small_edit-font.pdf}
    \caption{We use TIFA~\citep{tifa} to perform alignment comparisons between LLM description \& base caption and target image, and LLM description to target image and CLIP-retrieved image. The results show the impact of LLM reasoning over base captions, and CLIP retrieval as an essential bottleneck.}
    \label{fig:tifa}
\vspace{-10pt}
\end{wrapfigure} \paragraph{Reasoning Sanity Check by Measuring Text-Image Alignment.} 
We conduct a quantitative evaluation of our reasoning component by assessing the text-image alignment in the LLM-modified descriptions using TIFA~\citep{tifa}. Unlike simpler metrics such as CLIP score, TIFA offers a more accurate measure of cross-modal alignment by converting it into a series of question-answering tasks. Results are presented in Fig.~\ref{fig:tifa}. 
TIFA breaks down the alignment process by converting it into a series of question-answering tasks, for which we provide results in Fig.~\ref{fig:tifa}. 
Comparing the alignment between our generated modified descriptions and the generated plain image captions, we clearly see much higher and more consistent average alignment through our modification.

In addition, when looking at the alignment of our modified caption between the ground truth image and the actually retrieved image by CLIP (ViT-B/32) in Fig.~\ref{fig:tifa}, we see that the alignment of the retrieved image is actually notably lower than that of the ground truth image. 
Thus, while the CLIP similarity is higher for the retrieved image, its actual alignment with the modified caption is lower. This means that the standard CLIP backbone, even if the modified caption actually aligns well with the ground truth image, can often fail to retrieve it.
This indicates that the CLIP backbone used for text-image retrieval is a severe bottleneck~\citep{winoground,sugarcrepe,kamath2023text}, as the captioning and reasoning together generate valid target captions that a sub-optimal retrieval system cannot match.


\paragraph{Investigating Scaling Laws for ZS-CIR.} A critical benefit of the modular nature of our method is that we can replace and scale each component, without having to re-train. In particular, we can simply utilize our base model used for our benchmark comparisons in \S\ref{subsec:benchmarks}, and utilize a different CLIP model for the final retrieval process, without having to re-train or re-adapt anything.
This allows us to investigate scaling laws as studied and investigated in e.g. \citet{kaplan2020scaling,caballero2022broken,cherti2023reproducible} for the particular problem of CIR, and investigate if scale can address the retrieval bottleneck.
For this, we leverage CLIP variants provided through the OpenCLIP project \citep{openclip}, with models ranging from around 150M total parameters to around 2.5B. 
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/scaling_laws_fontedit.pdf}
    \vspace{-23pt}
    \caption{Leveraging our modular \methodNameNS\ to study scaling laws. For model availability and consistency, 
    we use LAION-2B pretrained CLIP models provided through OpenCLIP~\citep{openclip}.}
    \label{fig:scaling_laws}
\vspace{-8pt}
\end{figure} On both CIRCO and CIRR, we see a clear log-linear relationship between the model capacity and the performance as a result of simply upgrading our retrieval model in size. This clearly highlights the impact of scale even to the complex setting of ZS-CIR, and allows us to partly break the retrieval bottleneck shown above.
As we can easily plug-and-play different retrieval models, it also allows us to simply scale our overall pipeline. This allows us to boost the overall CIR scores on the CIRR dataset up to $R@1=34.64\%$ (c.f. $24.55\%$ for the highest reported SEARLE score), and up to $mAP@5=26.77\%$ on CIRCO (c.f. $11.68\%$ for the highest reported SEARLE score).
These results provide clear evidence of our method allowing for a simple transition between efficiency (with a smaller retrieval model) and efficacy with a larger one when needed.
Finally, we find that our insights partly contrast those in \cite{vaze2023genecis}, where only minimal gains from scaling can be observed. However, our results show that by incorporating conditioning through textual reasoning, \methodNameNS\  better leverages the benefits of large retrieval models without expensive re-training.

\paragraph{Qualitative Examples.} Fig.~\ref{fig:success} visualizes successful CI-retrievals with instructions impacting different semantic elements of the reference image such as viewpoint, color, object counts, background changes, object insertion, object adaptation or picture manipulations such as zoom. This provides further indication about the diverse applicability of our setup.
Following that, Fig.~\ref{fig:failure} visualize exemplary failure cases. As our CIR process operates primarily in the language domain, it becomes easy to understand the particular failure cases. For instance, we see some cases where the generated \textit{Caption} either does not correctly describe the image, or does not focus on the relevant aspect in the image (e.g. ignoring the grayscale aspect of the image and focusing on the pigeons). We also observe cases where the LLM does not generate an accurate description of the target image. Additionally, there are also cases where despite the captioning and reasoning working correctly, the CLIP model does not \textit{retrieve} an accurate image. Finally, we also see ambiguous failure cases where the retrieved target is debatable correct, but not labeled as such due to inexhaustive annotations (\textit{Ambiguity}).

\paragraph{Performing User Interventions.} Fortunately, by being able to break down the retrieval process, we can easily perform user interventions when needed. Simple example scenarios are shown in Fig.~\ref{fig:intervention}, where the default modified caption results in incorrect retrievals. As a user, intervention can happen at different stages of the retrieval process, modifying e.g. generated captions, utilized instructions or the final LLM-generated caption.
As a proof of concept, we showcase how intervention on the base caption level can already result in correct search queries on the image database. Simple human intervention can easily determine if a generated caption is incorrect, and simply replace it with an alternative. The rest of the pipeline can simply continue operating on top of this intervention. As can be seen in Fig.~\ref{fig:intervention}, intervening on the caption-level can already rectify various errors. This once again sharply contrasts with existing work that relies on an end-to-end pipeline with a rigid query template, where there is no possibility to alter the retrieval process.  


\section{Conclusion}
In this work, we present a novel, training-free approach for Zero-Shot Compositional Image Retrieval (CIR). Utilizing off-the-shelf pre-trained models, our method not only achieves strong performance across multiple CIR benchmarks but also, in some cases, doubles the performance of existing state-of-the-art methods. The method's inherent interpretability allows for user intervention, adding an extra layer of flexibility. We also explore the impact of scaling laws on our method, revealing that scaling the text-image retrieval component can substantially boost task performance. Collectively, these contributions set the stage for future research in training-free Compositional Image Retrieval.

\section*{Acknowledgements}
This work was supported by DFG project number 276693517, by BMBF FKZ: 01IS18039A, by the ERC (853489 - DEXIM), by EXC number 2064/1 – project number 390727645, and by the MUR PNRR project FAIR - Future AI Research (PE00000013) funded by the NextGenerationEU. Shyamgopal Karthik and Karsten Roth thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. Karsten Roth would also like to thank the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. The authors also thank Thomas Hummel (University of Tübingen) for help with visuals and valuable feedback.

\bibliography{main}
\bibliographystyle{iclr2024_conference}

\newpage
\appendix

\section{Additional Implementation Details}
\label{sec:app-implementation}
The prompt that we use draws upon the one used by \cite{liu2023zeroshot} for curating a dataset curation. Here, we write a similar prompt for the LLM to generate the edited description. We list the full prompt below:
\begin{quote}
    \small\texttt{"I have an image. Given an instruction to edit the image, carefully generate a description of the 
edited image. I will put my image content beginning with  'Image Content:'. The instruction I provide will begin with 'Instruction:'. 
The edited description you generate should begin with 'Edited Description:'. Each time generate one instruction and one edited description only."}    
\end{quote}

\section{Experiments on ImageNet Domain Conversion}
\begin{table}[t]
\centering
\caption{\textbf{Evaluation on ImageNet Domain Conversion experiment proposed in \cite{pic2word}.} The goal is to retrieve an appropriate domain of the object specified in the query image.} 
\vspace{-6pt}
\label{tab:imagenet}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc}
\toprule
& \multicolumn{2}{c}{Cartoon} & \multicolumn{2}{c}{Toy} & \multicolumn{2}{c}{Origami} & \multicolumn{2}{c}{Sculpture} \\
\cmidrule(rl){2-3}
\cmidrule(rl){4-5}
\cmidrule(rl){6-7}
\cmidrule(rl){8-9}
&  R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50\\
\midrule
Image-only & 0.3 & 4.5 & 0.2 & 1.8 & 0.6 & 5.7 & 0.3 & 4.0\\
Text-only & 0.2 & 1.1 & 0.8 & 3.7 & 0.8 & 2.4 & 0.4 & 2.0\\
Image+Text & 2.2 & 13.3 & 2.0 & 10.3 & 1.2 & 9.7 & 1.6 & 11.6\\
\midrule
Combiner (CIRR) \citep{combiner} & 6.1 & 14.8 & 10.5 & 21.3 & 7.0 & 17.7 & 8.5 & 20.4\\
Pic2Word \citep{pic2word} & 8.0 & 21.9 & 13.5 & 25.6 & 8.7 & 21.6 & 10.0 & 23.8\\
\midrule
\textbf{\methodNameNS} & \textbf{19.2} & \textbf{42.8} & \textbf{30.2} & \textbf{41.3} & \textbf{22.2} & \textbf{43.1} & \textbf{23.4} & \textbf{45.0}\\
\bottomrule
\end{tabular}
}
\vspace{-8pt}
\end{table}
 We also test \methodNameNS\ on the ImageNet domain conversion experiment proposed in ~\citep{pic2word}. Here we use images from 200 classes of the original ImageNet dataset~\citep{imagenet} as query, and for retrieval images of the same object but in the specified domain from ImageNet-R~\citep{imagenetr}
Unlike the previous benchmarks, the task is to simply retrieve an image of the appropriate domain for the same semantic object category (i.e a cartoon of a goldfish, with a natural goldfish reference image and the modifier \texttt{"cartoon"}).  
This requires no reasoning over image semantics, as the modifier affects an independent domain change, with significant improvements over Pic2Word or Combiner already be achieved by leveraging the final description \texttt{"a {domain} of a {caption}"}, as seen in Tab.~\ref{tab:imagenet}. Our model in parts more than double the performance of Pic2Word on this task (e.g. $R@1=19.2\%$ versus $8.0\%$ for a conversion to the cartoon domain). 
These findings mainly reiterate that combining off-the-shelf pre-trained models - here CLIP and BLIP-2 - can be more effective than customized models operating on top of pre-trained models.
\end{document}
