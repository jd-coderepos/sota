\documentclass{article}
\usepackage{amsthm}



\newfont{\seaddfnt}{phvr8t at 8pt}
\def\semail#1{{{\seaddfnt{\par #1}}}} 




\usepackage{amsmath,amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{booktabs}

\makeatletter
\newif\if@restonecol
\makeatother
\let\algorithm\relax
\let\endalgorithm\relax
\usepackage[lined,ruled,vlined]{algorithm2e}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definiton}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}

\DeclareMathOperator{\degree}{\operatorname{degree}}
\newcommand{\GO}{\mathcal{ O}}
\newcommand{\sO}{\mathcal{ O}\tilde\ }
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\minpoly}{{P_\text{min}^A}}
\newcommand{\charpoly}{{P_\text{char}^A}}
\urldef\jgdemail\url{Jean-Guillaume.Dumas@imag.fr}
\urldef\cpemail\url{Clement.Pernet@imag.fr}
\urldef\bdsemail\url{saunders@cis.udel.edu}


\begin{document}
\title{On finding multiplicities of characteristic polynomial factors
  of black-box matrices\thanks{Saunders supported by National Science
    Foundation Grants CCF-0515197, CCF-0830130.}.
}
\author{
Jean-Guillaume Dumas\thanks{Laboratoire J. Kuntzmann, Universit\'e de
  Grenoble. 51, rue des Math\'ematiques, umr CNRS 5224, bp 53X, F38041
  Grenoble, France, \jgdemail}
\and Cl\'ement Pernet\thanks{Laboratoire LIG, Universit\'e de
  Grenoble. umr CNRS, F38330 Montbonnot, France. \cpemail}
\and B. David Saunders\thanks{University of Delaware, Computer and
  Information Science Department. Newark / DE / 19716, USA. \bdsemail}
}

\maketitle


\begin{abstract}
We present algorithms and heuristics to compute the characteristic polynomial of a
matrix given its minimal polynomial. 
The matrix is represented as a black-box, i.e., by a function to compute
its matrix-vector product. 
The methods apply to matrices either over the integers or over a large enough finite field.
Experiments show that these methods perform efficiently in practice. Combined in
an adaptive strategy, these algorithms reach significant speedups in practice
for some integer matrices arising in an application from graph theory.
 \end{abstract}
 \noindent
 {\bf Keywords:} Characteristic polynomial ; black-box matrix ; finite field.

\section{Introduction}
Computing the characteristic polynomial of an integer matrix is a
classical mathematical problem. 
It is closely related to the computation of the Frobenius normal form
which can be used to test two matrices for similarity, or computing invariant
subspaces under the action of the matrix.
Although the Frobenius normal form contains more information on 
the matrix than the characteristic polynomial, most algorithms to
compute it are based on computations of characteristic polynomials (see
for example \cite[\S 9.7]{Storjohann:2000:thesis}).

Several matrix representations are used in computational linear algebra. In the
dense representation, a  matrix is considered as the array of all the 
coefficients. 
The sparse representation only considers non-zero coefficients using different possible
data structures. In the black-box representation,  the matrix is viewed  as a linear
operator, and no other operation than the application to a vector is
allowed. Though constraining, this limitation preserves the structure or
sparsity of the matrix and is therefore especially well suited for very large
sparse or structured matrices.

Computation of the characteristic polynomial of \textit{dense} matrices has already been well
studied both in theory and practice: over a finite field,
\cite{Keller-Gehrig:1985:charpoly,Pernet:2007:charp} set the best complexity
(using respectively a deterministic and a probabilistic algorithm), and 
\cite{jgd:2005:charp,Pernet:2007:charp} propose efficient implementations.
Over the integers, the best complexity is achieved in
\cite{Kaltofen:2005:CCDet}, but currently the  most efficient implementations 
rely on the Chinese remainder algorithm \cite{jgd:2005:charp}. 

In the latter article, a competitive approach is introduced that limits the use
of the Chinese remainder algorithm to the computation of the minimal polynomial.
The characteristic polynomial is then recovered by determining
the multiplicities of its irreducible factors. This task is done using the
deterministic algorithm for the characteristic polynomial over a randomly
chosen prime field.

In the black-box model, the minimal polynomial is used as a building block for
many algorithms over a finite field. Adapted from the iterative numerical
methods (Lanczos, Krylov), the Wiedemann minimal polynomial algorithm
\cite{Wiedemann:1986:SSLE,Kaltofen:1991:SSLS} has excellent asymptotic complexity
and is used in efficient black-box linear algebra software such as 
\texttt{LinBox}\footnote{\url{www.linalg.org}}.

However less is known concerning the characteristic
polynomial of black-box matrices. It is an open problem
\cite[Open Problem 3]{Kaltofen:1998:Open} to compute 
the characteristic polynomial as efficiently as the minimal polynomial, using
the Wiedemann method. The latter uses  products of a square 
matrix by a vector and  additional arithmetic
operations with  extra memory storage.
Eberly gives an algorithm  using  matrix vector products,
and  additional operations, where  is the number of
invariant factors of the matrix~\cite{Eberly:2000:BBFDOSF}. 
In the worst case,  and the
algorithm does not improve on the complexity of dense algorithms.
Villard proposed in \cite{Villard:2000:Frob} a black box
algorithm to compute the Frobenius normal form and therefore the characteristic
polynomial in  computations of minimal polynomials 
and  additional field operations. 

Instead, we propose here several algorithms and heuristics focusing on
efficiency in practice.
The general strategy is to compute the minimal
polynomial using  Wiedemann's algorithm and decompose it into irreducible factors.
There only remains to determine to which multiplicity each of
these factors appear in the characteristic polynomial. 
In section \ref{sec:multip} we propose
several methods to determine these multiplicities.
Adaptive combination of them is discussed in 
section \ref{sec:adap}. 
Under a conjectured hypothesis the latter is shown to
require   matrix vector products which improves by a logarithmic
factor on the complexity of Villard's algorithm.

Lastly, an algorithm for the computation over the ring of integers is derived in
section \ref{sec:spint}.
It is based on the  multifactor Hensel lifting of a gcd-free basis, following Storjohann~\cite{Storjohann:2000:Frob}. 
The benefit of this approach is verified by experiments
presented in section \ref{sec:exp}. Several sparse matrices are considered,
including a set of  adjacency matrices of strongly regular graphs, coming from an
application in graph theory.













\section{Three methods for computing multiplicities}\label{sec:multip}

In this section we consider a matrix  over a finite field
.
Let 

be the decomposition of the minimal polynomial of  in irreducible monic
factors. The characteristic polynomial is then 
 
for some . We also denote by  the degrees of
the factors: .

To recover the multiplicities , we will present three techniques,
based on black-box computations with the matrix :
the nullity method (\S \ref{sec:nullity}) uses the
rank of  to reveal information on the multiplicity~, 
the combinatorial search (\S \ref{sec:combs}) is a branch and bound
technique to solve the total degree equation whose integral unknowns are the
multiplicities and the index calculus technique (\S \ref{sec:dlog}) uses a
linear system solving based  on the discrete logarithm  
 of equation \ref{eq:charpol} evaluated in random values.


\subsection{The nullity method}
\label{sec:nullity}
\begin{defn}The nullity   of a matrix   is the dimension of its nullspace.
\end{defn}
We also recall the following definitions:


The \textit{companion matrix} of the monic polynomial 
 is the matrix
{\scriptsize
}
Its minimal polynomial and its characteristic polynomial are equal to .

The \textit{block Jordan matrix} of an irreducible polynomial  of degree  to a
power  is the  matrix  of the form

where the  matrix  is filled with zeros except for .
Its minimal polynomial and its characteristic polynomial are equal to .
This definition extends the usual notion of Jordan blocks for .
 
The \textit{Frobenius normal form} of a Matrix  is the unique  block diagonal matrix
 such that  for a nonsingular matrix
. The polynomials  are the invariant factors of  and satisfy  and  divides  for all .

The \textit{primary form} of a Matrix  (also called the second Frobenius form in
\cite{Gantmacher:1959:TMone}) is a further decomposition of the Frobenius
normal form where each companion block  is replaced by a block
diagonal matrix  . 
  The  are the irreducible factors of , with the respective
  multiplicities . 
The primary form is unique up to the order of the blocks. 


\begin{exmp}
Consider the matrix in Frobenius normal form 
{\small } over
. The corresponding 
 primary form is the matrix {\small }\\
{\scriptsize
}
\end{exmp}
The method of the nullity is based on the following lemma:
\begin{lem}
Let A be a square matrix and 
let  be an irreducible polynomial 
of degree , of multiplicity  in the minimal polynomial of ,
and of multiplicity  in the characteristic polynomial of .  
Then 
\end{lem}
\begin{proof}
Let F be the primary form of  over :  for a non singular
matrix .
 is block diagonal of the form 
.
Then .
On one hand  annihilates the blocks  where  and
.
On the other hand, the rank of  is full for ,
since  and  are relatively prime.
Thus the nullity of  exactly corresponds to the
total dimension of the blocks  where , which is .
\end{proof}
From this lemma the following algorithm, computing the multiplicity  of an
irreducible factor  is straight-forward:
\begin{algorithm}
  \dontprintsemicolon
  \caption{\texttt{Nullity}}\label{alg:nullity}
  \KwData{: an  matrix over a Field ,}
  \KwData{: an irreducible factor of ,}
  \KwData{: the multiplicity of  in ,}
  \KwResult{: the multiplicity of  in .}
  \Begin{
      \;
      \Return \;
    }
  \end{algorithm}

\begin{prop}
Algorithm \ref{alg:nullity} computes the multiplicity of  in
the characteristic polynomial of an  matrix  using  
field operations, where  is the cost of a matrix-vector
product with ,  is the degree of  and  is its multiplicity
in the minimal polynomial.
\end{prop}
\begin{proof}
Using Horner's rule, the matrix  can be written as . Hence, applying a vector to this blackbox
only requires  applications of a vector to the blackbox ,
ie.  field operations.
Thus applying a vector to  costs .
Lastly, the rank of this matrix can be
computed in  field operations, using 
Wiedemann's algorithm combined with preconditioners~\cite{jgd:2002:villard}.
\end{proof}
This algorithm is therefore suitable for irreducible factors  where the
product  is small.

Now if  is large, the computation of  may be too
 expensive. Still, some partial knowledge on the multiplicity can be
recovered from the rank of the first powers of . This can help to shorten
the computation of other algorithms, as will be shown in section \ref{sec:adap}.
We now describe how these partial multiplicities can be recovered.

The multiplicity  of  is formed by the contribution of several blocks of the 
type  for  in the primary form of .
Whereas the blocks with small  can be numerous, there must be few  blocks
with large , due to the limitation of the total dimension (since  is
large). 

We denote by  the number of occurrences of  in the
primary form of . From the determination of the , we can
directly deduce the multiplicity  by the relation 

We now show how to compute the  for
small , using algorithm \ref{alg:nullity}. 


\begin{lem}
Let  be an irreducible polynomial of degree  over a finite field 
and  and  be two integers. Then 

\end{lem}
\begin{proof}
Let  and . 
If , then  is a multiple of the minimal polynomial of
. Thus  is the zero matrix, and its nullity equals its dimension: .



Now suppose .
Let  be an extension of  such that  splits into
 degree one factors  over . Since any finite field is a perfect field, these factors are distinct.

The minimal polynomial of  over  is still . 
Consequently, the Frobenius normal form of A over
 is  and  
its primary form is . More precisely,
there exists  such that
.
We have therefore 

First consider the case : the minimal polynomial of each
 is  and so is the minimal polynomial of each
 (since the  are relatively prime). Hence the primary form
of  is . Therefore there
exist  such that 


Lastly the rank of  being , we deduce that
. The nullity of  is therefore .

For the general case, we have

Now  is  and nilpotent with ones on the super-diagonal
so that its -th power has rank .
Thus,  and .
\end{proof}
We now apply this result to the irreducible factors of the minimal polynomial
and denote the nullity of  by 
. 

First, the nullity of ,  can be decomposed
into the sum of the nullities of each  for every :

Now applying  to , every  for 
will be a zero matrix and therefore contribute with  to the
nullity. Otherwise, if , the contribution to the nullity
remains . Therefore we have:

From these two equations, we deduce the : first we have

Now, since:

the number of occurrences directly is:
 
Therefore we obtain corollary \ref{cor:occ} giving the expression of the :
\begin{cor}\label{cor:occ}

\end{cor}
The last formula for  is given for the sake of completeness: 
in practice, one will never compute every , since 
one would rather directly compute  the nullity of   instead, to
deduce the multiplicity  from algorithm~\ref{alg:nullity}.

\subsection{The combinatorial search}
\label{sec:combs}



In the following, we want to determine the values  of the unknown
.
They must satisfy the total degree
equation:



We can also discriminate potential candidates using the trace:
the degree  coefficient of
the characteristic polynomial is the negative of  
the trace of the matrix. Denote by  the degree  coefficient
of an irreducible factor . Then the degree  coefficient of  is . We thus  have the trace test:
 
\newcommand{\branchandcut}{\texttt{Branch-and-Cut}}
In a pure black-box model, the trace can be computed using  matrix-vector
products.  For many sparse or structured matrix representations, a faster method is available as well.

Then it suffices to use e.g. a {\branchandcut} algorithm to compute
all the integer -tuples satisfying both equations (\ref{eq:degree})
and (\ref{eq:tr}).
Of course, if some of the unknowns  are already computed
(e.g. by the nullity method) the set of candidates is accordingly reduced.

The remaining candidates will then be
discriminated by  evaluations of the characteristic polynomial
at random values, i.e. computations of determinants of  matrices. Indeed, we have efficient
methods of computing the determinant of a black-box matrix (see
e.g. \cite[\S 3.1 Determinant Preserving
Preconditioners]{Turner:2002:these} and references therein).
Algorithm \ref{alg:large:factor} sums up this combinatorial search
strategy.
\newcommand{\completelargefactors}{\texttt{Combinatorial-search}}
\begin{algorithm}
  \dontprintsemicolon
  \caption{\completelargefactors}\label{alg:large:factor}
  \KwData{, an  matrix}
  \KwData{, the degrees of the irreducible factors  of }
  \KwData{, a set of precomputed }
  \KwResult{ }
  \Begin{
\tcc{using degree and trace constraints}
       sol = \branchandcut (A, D, M)\;
      \While{}{
        Pick  at random\;
        \;
        Discard any  s.t. \;
      }
      \Return{}
  }
\end{algorithm}

\subsection{Index calculus method}
\label{sec:dlog}

Evaluating equation (\ref{eq:charpol}) at a point  leads to an
equation over the finite field, where the  multiplicities  are
the unknowns.
Inspired by index calculus techniques
\cite{Coppersmith:1986:IndexCalculus},
 the idea here is to consider the discrete
logarithm of such an equation (with an arbitrary choice of generator), to
produce a linear equation in the . Taking several of these
equations for different
 forms a linear system of equations, with dimension ,
the number of
unknown multiplicities.

In this discussion the base field is  and  is sufficiently
large with respect to  as discussed below. 
The characteristic polynomial evaluated at a given value   presents this 
equation in the unknown exponents :


Now, if  is not a root of the characteristic polynomial, taking the
discrete logarithm of these terms for a generator  of the field leads 
to this equation modulo :

which is linear in the unknowns .
We can therefore build a  linear system by randomly choosing  values
. This system is consistent since the multiplicities 
are a solution vector of this system. If the solution is unique, then it is the
vector of multiplicities over .

The computation of this vector can either be done by a dense Gaussian
elimination over the ring  or over a finite field  
where  is a large prime factor of  (larger than ).
In this last case, the result will be correct as long as the system
remains nonsingular modulo .






Algorithm \ref{alg:dlog} describes this techniques in more details.

\newcommand{\dlogsys}{\texttt{Index-calculus}}
\begin{algorithm}
  \dontprintsemicolon
  \caption{\dlogsys}\label{alg:dlog}
  \KwData{A, an  matrix over a finite field }
  \KwData{, the irreducible factors of ,}
  \KwData{, the set of indices of the unknown multiplicities,}
  \KwData{, the partial product of the
    irreducible factors with known multiplicity .}
  \KwResult{  or ``fail"}
\Begin{
    ;  ; ;\; Choose a generator  of \;
    Let  be a prime factor of \;
     \While{}{
       ; \lIf{}{\Return{``fail"}}\;
       Choose randomly \; \Repeat{  and , }{
          for all \;
         \;
       }
        \tcc{ is extended to the size }
        Stack the row  to \;
     }
    {} = \{ indices of the first  independent rows
    of \}\;
    Set  = a  nonsingular matrix of these rows\;
    Compute \;
    Solve \;
    \Return{}
}
\end{algorithm}

Let  be the number of unknown multiplicities. 
The first step is to find  values  forming a non
singular system. Therefore, we propose, in algorithm \ref{alg:dlog},
to evaluate the system at more than  points. The complexity of
forming a row of  costs only  arithmetic operations
using Horner's method. Therefore trying as many as  different values for  is
a negligible cost. 
Furthermore, one could use fast multi-point evaluation to get
blocks of rows simultaneously (up to  rows at a cost essentially linear in ).

The rank of  is computed all along the process, each new row being
incrementally added to the triangular decomposition of the current matrix.
This Gaussian elimination (performed by the LQUP algorithm
\cite{Ibarra:1982:LSP} for example) 
also provides the indices of the first  linearly independent
rows, and therefore the indices of the convenient .
Lastly the vector  is formed, using only  determinant computations.



In practice, it appeared, as in index calculus
\cite{Bender:1998:rdlog,Hess:2007:dlog}, that the
number of rows
required to get a
full rank matrix  is always quite close to .
However, we do not have a proof of this property, and we therefore 
state it as the conjecture
\ref{conj:dlog}. 
\begin{conj}\label{conj:dlog}
Let  be a  matrix over  and  be the
irreducible factors of its minimal polynomial.
Let  be a prime chosen randomly in finite set. Let  of the
form  where  is a prime number. Let  be a generator of 
  and   uniformly chosen at random in
  .
Let  where . 
Then  with high probability.


\end{conj}



Informally, in our system the
evaluations at the  are independent and can be
considered as seeds for the pseudo-random generator of taking the
discrete logarithm of the polynomial evaluation. 
Therefore the entries of the system modulo  
are at least close to random entries as soon as the polynomials are
distinct.
Would they be true random values,  the singularity/nonsingularity of the matrices
would follow the analysis of e.g. \cite[Corollary
2.4]{Blomer:1997:rank}: 
if  is a square matrix of uniformly random 
entries modulo  and  is a prime  diving , then the
probability that  is singular is of order .

\begin{thm}Assuming conj. \ref{conj:dlog},
algorithm \ref{alg:dlog} is correct and its asymptotic complexity is 
 where  is the cost of a multiplication of
 by a vector.
\end{thm}
\begin{proof} Let  be the
  number of rows required to get an invertible system, .
Each determinant computation requires  application of  to a vector
\cite[\S 3.1]{Turner:2002:these}. Building each row of the matrix requires a
Horner like evaluation of a polynomial with total degree less than ,
it therefore costs  operations. Triangularization of 
requires  operations. Solving the system , knowing
the triangular decomposition of  requires 
operations. The discrete logarithms can be tabulated
\cite{jgd:2004:dotprod} with  memory
(or to avoid this extra memory, one can compute the whole
sequence of powers of a generator of , sort the matrix and vector
entries and find the correspondences with some 
extra field operations). 
The overall complexity is thus 
 which is  
when . \end{proof}
The algorithm stops arbitrarily when .  We see here that a larger 
is acceptable for the complexity result, but in our experiments a very small 
(e.g. ) always suffices. 
In the following sections, this algorithm will be used with
, thus giving an expected  complexity.


\section{Adaptive black-box algorithm over a finite field}\label{sec:adap}

We show in the present section how to combine the ideas of the
previous section together with already existing techniques to form an adaptive
algorithm computing the characteristic polynomial of a black-box matrix
over a finite field. The algorithm is adaptive in the sense of
\cite{jgd:2006:AHA}, meaning that it chooses the best variant depending
on discovered properties of its input.

We first combine the nullity method with the combinatorial search.
We then show  an algorithm improving on the asymptotic
complexity. Finally we give some improvements which are efficient in practice 
on typical matrices.

\subsection{Nullity method and combinatorial search}

These two algorithms are complementary: the nullity is efficient for the
determination of the multiplicites of factors of small degree, whereas the
combinatorial search is adapted to the large degree factors.

More precisely, algorithm \ref{alg:nullcombs} sorts the list of the unknown occurences
 according to the increasing . The nullities are then computed
until there remain fewer than a fixed number  of unknowns to be determined by 
combinatorial search.

\newcommand{\nullcomb}{\texttt{Nullity-comb-search}}


\begin{algorithm}
  \dontprintsemicolon
  \caption{\nullcomb}
  \label{alg:nullcombs}
      \KwData{A: an  matrix over a finite field,} 
      \KwData{: the irreducible factors of ,}
      \KwData{: a static threshold}
      \KwResult{: the multiplicities of each  in .}
     \Begin{
         \;
         Sort  according to the increasing values of \;
         \While{()}{
           Pop  from \;
           Compute \;
         }
         \For{}{
           Let  be the largest index s.t.   is computed  \;
           \If{}{
             Compute \;
             }
           \lFor{}{Compute   using cor. \ref{cor:occ}}\;
         }
         \completelargefactors \;
         \lFor{}{\;}
         \Return{}
       }
\end{algorithm}
The combinatorial search has exponential complexity. The threshold
 must be small. In experiments, we found that  was the best choice for
various matrices.
In the case of numerous factors with large degree, this approach is of reduced effeciveness. We propose in the next section how to combine it with a third algorithm.


\subsection{Nullity method and system resolution} \label{sec:improv}

The index calculus method also enables the design of a hybrid
algorithm. If the multiplicities of some factors have already been computed by
another method, we can limit the system to the unknown multiplicities only, thus
reducing its dimension.  

Suppose that the multiplicities  of the factors
 for  are already known, then equation (\ref{eq:log}) reduces to 
{\small

}

A first simple hybrid approach is the following: the method of
the nullity  (section \ref{sec:nullity}) is applied to every degree one factor
with multiplicity one in the minimal polynomial, and the remaining factors 
are left to the index calculus method.

This approach is always worthy since the computation of the rank of ,
for a degree one polynomial  is cheaper than the computation of
 \cite{jgd:2002:villard}.




A second hybrid approach also introduces a combinatorial search to this
algorithm: the nullity method still handles the  degree one factors as
previously. 
The remaining factors  are sorted by decreasing degree. For a
convenient choice of , a list of every possible assignment for the
multiplicities of the first  factors is determined, using a combinatorial search.
Then for each partial assignment, the multiplicities of the remaining factors
are determined by the resolution of an index calculus system of the form:
{\small 
}


For each partial assignment, the system resolutions share the same matrix
. Therefore the expensive part of it, namely the Gaussian elimination, can be
performed only once  at cost , where .
There only remains to solve two triangular systems (in )
for each possible assignment. 
Lastly, the assignments will be discriminated against each other by a test on
the total degree. 
To sum up, this techniques makes it possible to balance the cost of the
computations of determinants, and the cost of the system solving, by
reducing the dimension of the system, but increasing the dimension of its right
hand side.
The most appropriate value for  has to be determined dynamically, according
to the number of possible assignments induced, and using an estimate of the cost
function of this algorithm: e.g.

where  denote the number of possible assignments for a chosen subset of
 factors.











\subsection{Index calculus and k invariant}

The best known black-box algorithm to compute the Frobenius normal form over a
field is given by Villard in \cite{Villard:2000:Frob}. It is proved that computing the
th invariant factor of a matrix reduces to the computation of a minimal
polynomial of the input matrix with a rank  additive perturbation.
Using a binary search technique, the algorithm only performs 
such computations, where  is the number of distinct invariant factors of the matrix.
Since  is smaller than  and  an invariant factor can
be recovered using  matrix vector products, this corresponds
to a total number of  matrix-vector products and an
additional cost of  arithmetic
operations.

We propose in algorithm \ref{alg:spcharpoly1} an alternative approach combining
the index calculus method with computations of individual
invariant factors. 

\begin{algorithm}
  \dontprintsemicolon
  \caption{\texttt{black-box-charpoly} \label{alg:spcharpoly1}}
  \KwData{A: an  matrix over a finite field K}
  \KwResult{  or ``fail"}
  \Begin{
       \;
      Factor  using Cantor-Zassenhaus\;
      Set  and \;
      \While{}{
         \;
        \ForAll{}{
          Compute  s.t. \;
          \lIf{} {\;}
              {\;}
        }
      }
\Return{\dlogsys }
    }
\end{algorithm}


The idea is to reduce the dimension of the index calculus system to  
by computing a few of the first invariant factors of the matrix.

After each computation of an invariant factor , the multiplicity of
each irreducible polynomial  is updated, and those  that are no longer
in  are removed from the list of the factors with unknown multiplicity.

The  loop is executed at most  times. Otherwise, there
would be more than  invariant factors having more than 
irreducible factors, and the total degree would be larger than .

Now the condition of exit for this loop ensures that the order of the
linear system will be smaller than . Therefore only 
determinants will be computed and the overall number of blackbox matrix-vector products 
is 

The remaining multiplicities are then determined by the
index calculus method  described previously,
requiring at most  applications of the matrix to a vector.

Under the conditions of validity for the index calculus algorithm, this
heuristic improves on the computation time of Villard's algorithm by a logarithmic factor.

\section{Lifting over the integers}\label{sec:spint}

Storjohann gives in \cite{Storjohann:2000:Frob} a method for the
computation of the Frobenius normal form of a black-box integer
matrix. It is based on a computation of the minimal polynomial over
 and on a computation of the Frobenius normal form over a prime
field. Then a gcd-free basis for the invariant factors over 
is computed and lifted over . 

We use the same idea but just for the characteristic polynomial, and
not for all the invariant factors. It is
thus simpler since we don't need to ensure that the Frobenius form
of  modulo  equals the integer Frobenius form reduced modulo
. We just need ensure that the minimal and characteristic 
polynomials of  modulo
 equal the minimal and characteristic polynomials over the integers
reduced modulo .

The goal of the following algorithm \ref{alg:intcharp}
is to compute the
integer characteristic polynomial from the integer minimal polynomial
and the characteristic polynomial modulo some prime , obtained via
the previous sections. Algorithm \ref{alg:intcharp} is just a
simplification of that of \cite{Storjohann:2000:Frob}:

\begin{algorithm}
  \dontprintsemicolon
  \caption{\texttt{Gcd-free lifting of the characteristic
      polynomial}\label{alg:intcharp}}
  \KwData{A: an  integer matrix,  its minimal
    polynomial over ,}
  \KwData{: a prime number,}
  \KwData{ the characteristic polynomial of ,}
  \KwResult{ the characteristic polynomial of  over }
  \Begin{
      Compute  the squarefree part of \;
      ,       \;

      Compute a modular gcd-free basis 
      of , together with exponents 
      such that \;
      Apply Hensel lifting on the basis  to produce
       so that 
      and \;
      \Return{}
    }
\end{algorithm}
The integer minimal polynomial is computed via \cite[Theorem
3.3]{jgd:2001:jsc} with an  probabilistic complexity,
where  is the size of its integer coefficients and , its
degree.
The characteristic polynomial modulo  is computed via algorithm
\ref{alg:spcharpoly1} with an  complexity and
 extra field operations.
Then the squarefree part \cite{Gerhard:2001:sqrfree} and 
the Hensel lifting of the gcd-free basis takes 
word operations with fast integer and  polynomial arithmetic
\cite[Theorem 15.18]{VonzurGathen:1999:MCA}.

The size of the coefficients of the integer minimal polynomial is bounded in the worst case by
 where  is the
largest entry in absolute value of the matrix , see \cite[Lemma 2.1]{jgd:2007:jipam}.
When the matrix entries are of constant size,   and
as the
degree of the minimal polynomial is bounded by , the dominant
asymptotic cost is that of the integer minimal polynomial computation. 
This result is already in \cite{Storjohann:2000:Frob}.
In practice, however, the coefficients of the minimal
polynomial are often much smaller than the bound and than those of the
integer characteristic polynomial. Furthermore, the degree of the
minimal polynomial can be extremely small, especially for structured
or sparse matrices (see e.g. homology matrices
in \cite{jgd:2001:jsc}). In those cases, the dominant cost will 
be the computation of the characteristic polynomial modulo .
Then, our algorithm enables faster computations since a factor of
 has been gained, as illustrated in table \ref{table:timings}.



A supplemental constraint can be introduced by 
computing det( ),
at random integer . 
Using e.g. \cite[Theorem 4.2]{jgd:2006:det} with \cite{Eberly:2006:sparse},
 of these 
can be done to speed-up the modular adaptive search,
without increased complexity.

\section{Experimental comparisons and applications}\label{sec:exp}

We have implemented some of the algorithms presented in the previous sections using the
\texttt{LinBox}\footnote{\url{www.linalg.org}} library for the black-box
computation of minimal polynomials, ranks and determinants.
In a first approach, we replaced the computation of the gcd-free basis of
algorithm \ref{alg:intcharp} by a factorization into irreducible factors, using
Hensel lifting. This algorithm is more expensive in the worst case, but the
efficient implementation by \texttt{NTL}\footnote{\url{www.shoup.net/ntl}} 
makes it practicable in numerous cases.

This work was partly motivated by an application from graph theory.
For a graph  on  vertices with vertex set  and edge set
 the -th symmetric power  is the graph with the
 -subsets of  as vertices and with two such
-subsets adjacent if their symmetric difference is in .

Graph theorists are interested in the spectrum of such graphs (defined as the
spectrum of their adjacency matrix) since they are closely related to the
description of their isomorphism class. 
More precisely, if a certain power  is found, such that the symmetric th 
power of a graph describes its isomorphism class, this would provide a polynomial
algorithm to solve the graph isomorphism problem. 

This motivated the team of Audenaert, Godsil, Royle and Rudolph to study in
\cite{Royle:2007:symm} the spectrum of  symmetric powers of a class of graphs:
the strongly regular graphs. They prove that there exist infinitely many graphs
having co-spectral symmetric squares. But concerning the symmetric cubes, no
pair of graph is known to have co-spectral symmetric cubes until now.

We helped Royle to investigate further the computation of the characteristic
polynomial of the symmetric cubes of strongly regular graphs. 
He was able to test the first 72 cases corresponding to the graphs with fewer
than 29 vertices. Using our implementations available in \texttt{LinBox}, we
have been able to test the   graphs with fewer than 36 vertices and
check that there is no pair of graphs among them having cospectral symmetric
cubes.

We used the matrices of this application to benchmark the implementations of the
previously presented algorithms.
These matrices are sparse and symmetric, and therefore especially suited to black-box
computations.
Moreover, several parameters such as the degrees of their
minimal polynomials or the average number  of nonzero elements per row
vary among the matrices. The matrices \texttt{EX1, EX3, EX5} correspond
respectively to the symmetric cubes of the strongly regular graphs with
parameters (16,6,2,2), (26,10,3,4) and (35,16,6,8). Their dimensions are
respectively
,  and .
The matrices \texttt{EX2} and \texttt{EX4} correspond to different graphs but
with similar parameters as \texttt{EX1} and \texttt{EX3}.

All the matrices used in the experiments, including adjacency matrices
of the symmetric powers, are avaible on-line in the {\em Sparse Integer
Matrices
Collection\footnote{\url{ljk.imag.fr/membres/Jean-Guillaume.Dumas/SIMC}}}.
In particular we used, in the following tables, matrices from the
\url{SPG}, \url{Forest}, \url{Trefethen} and  \url{Homology} sections of the
collection. When the tested matrix was not square, we considered the
square matrix obtained by padding it with zeroes.

\begin{table}[hptb]
\begin{center}
{\small
\begin{tabular}{lccccc}
\toprule
Matrix           &  EX1 & EX2 & EX3 & EX4 & EX5 \\ 
\midrule
: dimension   & 560  & 560 & 2600 & 2600 & 6545 \\
: deg ( & 54  & 103 & 1036 & 1552 & 2874  \\
: sparsity & 15.6 & 15.6 & 27.6 & 27.6 & 45.2\\
\midrule
-\texttt{Minpoly} & 0.11s & 0.26s & 117s & 260s & 5002s  \\
\midrule
 factorize& 0.02s & 0.07s & 9.4 & 18.15 & 74.09s \\
\midrule
Nullity/comb.& \textbf{3.37s} & 5.33s & \textbf{33.2s} & \textbf{30.15s} &\textbf{289s} \\
Total& \textbf{3.51s} & 5.66s & \textbf{159.4s} & \textbf{308.1s} & \textbf{5366s}\\
\midrule
Index calc.  & 3.46s & \textbf{4.31s} & 64.0s & 57.0s & 647s\\
Total&  3.59s & \textbf{4.64s} & 190.4s & 336.4s & 5641s\\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Computation time for tasks of the integer adaptive algorithm on a
  Pentium4 (3.2 GHz; 1 Gb)}\label{table:timings} 

\end{table}

We report in table \ref{table:timings} the computation time of the
different modules described in this paper. For each matrix, two computations are
compared: they share the computation of the minimal polynomial over . Then
the determination of the multiplicities is done either by the combination of the
nullity algorithm and the combinatorial search (with the threshold  set to 5), or by the index calculus method.

We first note that the determination of the multiplicities may be the dominant
operation when the degree of the minimal polynomial is small, as for the matrix
\texttt{EX1}. This makes the motivation for this study obvious.
For this task either method, nullity or resolution of the logarithmic system,
can be the most competitive option, depending on the structure of the irreducible
factors. This advocates for the adaptive approach of algorithm
\ref{alg:spcharpoly1} combining both methods, and the computation of the
th invariant factor.

In order to emphasize the improvement of the black-box determination of the
multiplicities over dense methods, we now compare it to the alternative technique presented in
\cite[\S 4.2.2]{jgd:2005:charp}. This also relies on the computation of the
minimal polynomial in  and its decomposition into irreducible factors. But
the multiplicities are then obtained using one \emph{dense} computation of the characteristic
polynomial in a randomly chosen finite field. It is
therefore not anymore a black-box algorithm. 
We will denote it by \texttt{dchar}.  \texttt{Comb} is the nullity-combinatorial
search algorithm, and \texttt{ind} is the index calculus method.  denotes \texttt{08blocks},  is \texttt{ch5-5.b3}, and  is \texttt{Tref500} from the Sparse Integer Matrices Collection.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{cccccc}
\toprule
Matrix & n &  & \texttt{dchar} & \texttt{null-comb} &
\texttt{ind.}\\
\midrule
 & 300 & 1.9  & 0.32s & 0.08s & \textbf{0.07s}\\
          & 300 & 2.95 & 0.81s & \textbf{0.12s} & \textbf{0.12s}\\
   & 600 & 4    & 4.4s  & \textbf{1.52s} & 1.97s\\
          & 600 & 13   & \textbf{2.15s}  & 3.96  &7.48s \\
\texttt{TF12}         & 552 & 7.6  & 6.8s   & \textbf{5.53s} & 5.75s\\
\texttt{mk9b3}        & 1260 & 3   & 31.25s & \textbf{10.51s} & 177s \\ 
\texttt{Tref500}      & 500 & 16.9 & 65.14s & \textbf{25.14s} & 25.17s\\
\bottomrule
\end{tabular}
\end{center}
\caption{Integer black-box approach for 
  multiplicities on an Athlon (1.8 GHz; 2 Gb)}
\label{tab:ciavsbn}
\end{table}

Table \ref{tab:ciavsbn} shows the improvement of the black-box approach for
several matrices coming from
different applications. Once again, the structure of the irreducible factors of
the minimal polynomials cause various behaviors for each variant. For example
the times of \dlogsys\xspace are similar to those of 
\nullcomb, sometimes better but also sometimes much slower, as
for the matrices  and \texttt{mk9b3}.

\section{Conclusion}
We developed several ways to recover the multiplicities of the factors of the 
characteristic polynomial  from a factorization of the minimal
polynomial. 
Over a finite field  hybrid heuristics are proposed, that compete with
the best theoretical complexity.
Over the ring of integers, our approach enables fast
computations particularly when the coefficients or degree of the minimal
polynomial are small.
This is illustrated on a family of strongly regular graphs, in order to verify
 that there are no symmetric co-spectral cubes.

Further studies on the theoretical complexity remain to be done, and could lead
to better implementations in practice. In particular, a recent algorithm for
dense matrices \cite{Pernet:2007:charp} might be adapted for black-box
matrices. 
In this regard, extending the block projections of
\cite{EGGSV:2007:BBinv} to the case of similarity transformations would play a
crucial role.



{\small
\begin{thebibliography}{10}

\bibitem{Royle:2007:symm}
K.~Audenart, C.~Godsil, G.~Royle, and T.~Rudolph.
\newblock Symmetric squares of graphs.
\newblock {\em Journal of Combinatorial Theory}, 97(1):74--90, Jan. 2007.

\bibitem{Bender:1998:rdlog}
R.~L.~Bender, and C.~Pomerance.
\newblock Rigorous Discrete Logarithm Computations in Finite
        Fields via Smooth Polynomials.
\newblock {\em Studies in Advanced Mathematics}, American Mathematical
        Society and International Press, 7, 1998.

\bibitem{Blomer:1997:rank}
J.~Bl{\"o}mer, R.~Karp, and E.~Welzl.
\newblock The rank of sparse random matrices over finite fields.
\newblock {\em RSA: Random Structures \& Algorithms}, 10, 1997.

\bibitem{Coppersmith:1986:IndexCalculus}
D.~Coppersmith, A.~M. Odlyzko, and R.~Schroeppel.
\newblock Discrete logarithms in {GF}(p).
\newblock {\em Algorithmica}, 1(1):1--15, 1986.

\bibitem{jgd:2006:AHA}
V.-D. Cung, V.~Danjean, J.-G. Dumas, T.~Gautier, G.~Huard, B.~Raffin,
  C.~Rapine, J.-L. Roch, and D.~Trystram.
\newblock Adaptive and hybrid algorithms: classification and illustration on
  triangular system solving.
\newblock In {\em {Transgressive Computing} 2006}, pages 131--148, Apr. 2006.

\bibitem{jgd:2004:dotprod}
J.-G. Dumas.
\newblock Efficient dot product over finite fields.
\newblock In {\em {CASC}'2004}, pages 139--154, July 2004.

\bibitem{jgd:2007:jipam}
J.-G. Dumas.
\newblock Bounds on the coefficients of the characteristic and minimal
  polynomials.
\newblock {\em Journal of Inequalities in Pure and Applied Mathematics},
  8(2):art. 31, 6 pp, Apr. 2007.

\bibitem{jgd:2005:charp}
J.-G. Dumas, C.~Pernet, and Z.~Wan.
\newblock Efficient computation of the characteristic polynomial.
\newblock In {\em {ISSAC}'2005}, pages 140--147, July 2005.

\bibitem{jgd:2001:jsc}
J.-G. Dumas, B.~D. Saunders, and G.~Villard.
\newblock On efficient sparse integer matrix {Smith} normal form computations.
\newblock {\em Journal of Symbolic Computation}, 32(1/2):71--99, July--Aug.
  2001.

\bibitem{jgd:2006:det}
J.-G. Dumas and A.~Urba\'nska.
\newblock An introspective algorithm for the determinant.
\newblock In {\em {Transgressive Computing} 2006}, pages 185--202, Apr. 2006.

\bibitem{jgd:2002:villard}
J.-G. Dumas and G.~Villard.
\newblock Computing the rank of sparse matrices over finite fields.
\newblock In {\em {CASC}'2002}, pages 47--62, Sept. 2002.

\bibitem{Eberly:2000:BBFDOSF}
W.~Eberly.
\newblock Black box frobenius decomposition over small fields.
\newblock In {\em {ISSAC}'2000}, Aug. 2000.

\bibitem{Eberly:2006:sparse}
W.~Eberly, M.~Giesbrecht, P.~Giorgi, A.~Storjohann, and G.~Villard.
\newblock Solving sparse rational linear systems.
\newblock In {\em {ISSAC}'2006}, pages 63--70, July 2006.

\bibitem{EGGSV:2007:BBinv}
W.~Eberly, M.~Giesbrecht, P.~Giorgi, A.~Storjohann, and G.~Villard.
\newblock Faster inversion and other black box matrix computations using
  efficient block projections.
\newblock In {\em {ISSAC}'2007}, pages 143--150, Jul. 29 -- Aug. 1 2007.

\bibitem{Gantmacher:1959:TMone}
F.~R. Gantmacher.
\newblock {\em The Theory of Matrices}.
\newblock Chelsea, New York, 1959.

\bibitem{VonzurGathen:1999:MCA}
J.~v. Gathen and J.~Gerhard.
\newblock {\em Modern Computer Algebra}.
\newblock Cambridge University Press, 1999.

\bibitem{Gerhard:2001:sqrfree}
J.~Gerhard.
\newblock Fast modular algorithms for squarefree factorization and hermite
  integration.
\newblock {\em Applicable Algebra in Engineering Communication and Computing},
  11(3):203--226, 2001.

\bibitem{Hess:2007:dlog}
F.~He{\ss}. 
\newblock Computing relations in divisor class groups of algebraic
curves over finite fields.
\newblock Technical report, 2007.
\newblock {\small \url{www.math.tu-berlin.de/~hess/personal/dlog.ps.gz}}

\bibitem{Ibarra:1982:LSP}
O.~H. Ibarra, S.~Moran, and R.~Hui.
\newblock A generalization of the fast {LUP} matrix decomposition algorithm and
  applications.
\newblock {\em Journal of Algorithms}, 3(1):45--56, Mar. 1982.

\bibitem{Kaltofen:1998:Open}
E.~Kaltofen.
\newblock Challenges of symbolic computation: My favorite open problems.
\newblock {\em Journal of Symbolic Computation}, 29(6):891--919, June 2000.

\bibitem{Kaltofen:1991:SSLS}
E.~Kaltofen and B.~D. Saunders.
\newblock On {Wiedemann's} method of solving sparse linear systems.
\newblock In {\em AAAAECC'91}, volume 539 of {\em Lecture Notes in Computer
  Science}, pages 29--38, Oct. 1991.

\bibitem{Kaltofen:2005:CCDet}
E.~Kaltofen and G.~Villard.
\newblock On the complexity of computing determinants.
\newblock {\em Computational Complexity}, 13(3-4):91--130, 2005.

\bibitem{Keller-Gehrig:1985:charpoly}
W.~Keller-Gehrig.
\newblock Fast algorithms for the characteristic polynomial.
\newblock {\em Theoretical computer science}, 36:309--317, 1985.

\bibitem{Pernet:2007:charp}
C.~Pernet and A.~Storjohann.
\newblock Faster algorithms for the characteristic polynomial.
\newblock In {\em {ISSAC}'2007}, pages 307--314, Jul. 29 -- Aug. 1 2007.

\bibitem{Storjohann:2000:thesis}
A.~Storjohann.
\newblock {\em Algorithms for Matrix Canonical Forms}.
\newblock PhD thesis, ETH, Z{\"u}rich, Switzerland, Nov. 2000.

\bibitem{Storjohann:2000:Frob}
A.~Storjohann.
\newblock Computing the frobenius form of a sparse integer matrix.
\newblock to be submitted, Apr. 2000.

\bibitem{Turner:2002:these}
W.~J. Turner.
\newblock {\em Blackbox linear algebra with the LinBox library}.
\newblock PhD thesis, North Carolina State University, May 2002.

\bibitem{Villard:2000:Frob}
G.~Villard.
\newblock Computing the {Frobenius} normal form of a sparse matrix.
\newblock In {\em CASC'00}, pages 395--407, Oct. 2000.

\bibitem{Wiedemann:1986:SSLE}
D.~H. Wiedemann.
\newblock Solving sparse linear equations over finite fields.
\newblock {\em IEEE Transactions on Information Theory}, 32(1):54--62, Jan.
  1986.

\end{thebibliography}
}
\end{document}
