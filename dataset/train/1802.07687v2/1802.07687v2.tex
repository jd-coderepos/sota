\section{Experiments}
\label{experiments}

\begin{figure*}[t!]
\centering
     \includegraphics[width=0.98\linewidth]{imgs/mnist_2-1.pdf}
 \caption{Qualitative comparison between SVG-LP and a
   purely deterministic baseline. The deterministic model
   produces sharp predictions until ones of the digits collides with a 
   wall, at which point the prediction blurs to account for the many
   possible futures. In contrast, samples from
   SVG-LP show the digit bouncing off in different plausible
   directions. 
} 
\label{fig:mnist_gen}
\end{figure*}

\begin{figure*}[t!]
\centering
     \includegraphics[width=0.98\linewidth]{imgs/kth_combined.pdf}
\caption{Qualitative comparison between SVG-LP and a purely deterministic baseline. Both models were conditioned on the first 10 frames (the final 5 are shown in the figure) of test sequences. The deterministic model produces plausible predictions for the future frames but frequently mispredicts precise limb locations. In contrast, different samples from SVG-FP reflect the variability on the persons pose in future frames. By picking the sample with the best PSNR, SVG-FP closely matches the ground truth sequence. }
\label{fig:kth_gen}
\end{figure*}

We evaluate our SVG-FP and SVG-LP model on one synthetic video dataset
(Stochastic Moving MNIST) and two real ones (KTH actions \cite{kth} and
BAIR robot \cite{ebert17}).  We show quantitative comparisons by computing
structural similarity (SSIM) and Peak Signal-to-Noise Ratio (PSNR)
scores between ground truth and generated video sequences.  Since
neither of these metrics fully captures perceptual fidelity of
generated sequences we also make a qualitative comparison between
samples from our model and current state-of-the-art methods. 
We encourage the reader to view additional generated videos at: \url{https://sites.google.com/view/svglp/}.

\subsection{Model architectures}
 is a two layer LSTMs with 256 cells in each layer.  
 and  are both single layer LSTMs with 256 cells in each layer.  
Each network has a linear embedding
layer and a fully connected output layer.
The output of  is passed through a tanh nonlinearity before going into the frame decoder.


For Stochastic Moving MNIST, the frame encoder has a DCGAN
discriminator architecture \cite{radford2016} with output dimensionality  = 128.
Similarly, the decoder uses a DCGAN generator architecture and a sigmoid output layer.
The output dimensionalities of the LSTM networks are 
. 

For KTH and BAIR datasets, the frame encoder uses the same architecture as VGG16 \cite{vgg} up until the final pooling layer with output dimensionality . The decoder is a mirrored version of the encoder with pooling layers replaced with spatial up-sampling and a sigmoid output layer.
The output dimensionalities of the LSTM networks are 
 for KTH and  for BAIR. 

For all datasets we add
skip connections from the encoder at the last ground truth frame to the decoder at , enabling the model to easily generate static
background features.

We also train a deterministic baseline with the same encoder, decoder and LSTM architecture as our frame predictor  but with the latent variables omitted.


\begin{figure*}[t!]
\centering
     \includegraphics[width=0.85\linewidth]{imgs/mnist_dist_aligned_1.png}
     \includegraphics[width=0.14\linewidth]{imgs/mnist_traj_aligned_1.png}
\noindent\makebox[\linewidth]{\rule{0.99\linewidth}{0.4pt}}
     \includegraphics[width=0.85\linewidth]{imgs/mnist_dist_aligned_2.png}
     \includegraphics[width=0.14\linewidth]{imgs/mnist_traj_aligned_2.png}
\noindent\makebox[\linewidth]{\rule{0.99\linewidth}{0.4pt}}
     \includegraphics[width=0.85\linewidth]{imgs/mnist_dist_aligned_3.png}
     \includegraphics[width=0.14\linewidth]{imgs/mnist_traj_aligned_3.png}
 \caption{Three examples of our SVG-LP model accurately capturing the
   distribution of MNIST digit trajectories following collision with
   a wall. On the right we show the trajectory of a digit prior to the
   collision. In the ground truth sequence, the angle and speed
   immediately after impact are drawn from at random from uniform distributions.
Each of
   the sub-plots shows the {\em distribution} of 
   at each time step. In the lower ground truth sequence, the
   trajectory is deterministic before the collision (occurring between
    and 
   in the first example), corresponding to a delta-function. Following
   the collision, the distribution broadens out to an approximate
   uniform distribution (e.g. ), before being reshaped by
   subsequent collisions. The upper row shows the distribution
   estimated by our SVG-LP model (after conditioning on ground-truth
   frames from ). Note how our model accurately captures
   the correct distribution many time steps into the future, despite its
   complex shape. The distribution was computed by drawing many
   samples from the model, as well as averaging over different digits
   sharing the same trajectory. The 2nd and 3rd
   examples show different trajectories with correspondingly different impact times
   ( and  respectively). }
\label{fig:mnist_dist}
\end{figure*}

We train all the models with the ADAM optimizer \cite{adam} and
learning rate . 
We set  1e-4 for KTH and BAIR and  1e-6 for KTH.
Source code and trained models are available at \url{https://github.com/edenton/svg}.


\subsection{Stochastic Moving MNIST}
We introduce the Stochastic Moving MNIST (SM-MNIST) dataset which
consists of sequences of frames of size , containing one
or two MNIST digits moving and bouncing off edge of the frame (walls).
In the original Moving MNIST dataset \cite{Srivastava15} the digits
move with constant velocity and bounce off the walls in a
deterministic manner.  By contrast, SM-MNIST digits move with a
constant velocity along a trajectory until they hit at wall at which
point they bounce off with a random speed and direction.  This
dataset thus contains segments of deterministic motion interspersed
with moments of uncertainty, i.e. each time a digit hits a wall.

Training sequences were
generated on the fly by sampling two different MNIST digits from the
training set (60k total digits) and two distinct trajectories. 
Trajectories were constructed by uniformly sampling  starting locations and initial velocity vectors .
Every time a digit hits a wall a new velocity vector is sampled.



\begin{figure}[t!]
    \centering
\mbox{
\includegraphics[width=0.98\linewidth]{imgs/2digit_mnist_prior_var.png}
 }
 \caption{Learned prior of SVG-LP accurately predicts collision points in SM-MNIST. Five hundred test video sequences with different MNIST test digits but synchronized motion were fed into the learned prior. The mean (  one standard deviation) of  is plotted for . The true points of uncertainty in the video sequences, i.e. when a digits hits a wall, are marked by vertical lines, colored red and blue for each digit respectively.} 
      \label{fig:mnist_prior}
\vspace{-3mm}
\end{figure}


We trained our SVG models and a deterministic baseline on SM-MNIST by conditioning on 5 frames and training the model to
predict the next 10 frames in the sequence.  
We compute SSIM for SVG-FP and SVG-LP by drawing 100 samples from the
model for each test sequence and picking the one with the best score
with respect to the ground truth. 
\fig{mnist_kth_ssim}(left) plots average SSIM on unseen test videos.
Both SVG-FP and SVG-LP outperform the deterministic baseline and SVG-LP performs best overall, particularly in later time steps.
\fig{mnist_gen} shows sample generations from the deterministic model and SVG-LP. 
Generations from the deterministic model are sharp for several time
steps, but the model rapidly degrades after a digit collides with the wall,
since the subsequent trajectory is uncertain. 

We hypothesize that the improvement of SVG-LP over the SVG-FP model is
due to the mix of deterministic and stochastic movement in the
dataset.  In SVG-FP, the frame predictor must determine how and if the
latent variables for a given time step should be integrated into the
prediction.  In SVG-LP , the burden of predicting points of high
uncertainty can be offloaded to the prior network.  

Empirically, we
measure this in \fig{mnist_prior}. Five hundred different video sequences were
constructed, each with different test digits, but whose
trajectories were synchronized. The plot shows the mean of
, i.e., the variance of the distribution over  predicted by the
learned prior over 100 time steps. Superimposed in red and blue are the time
instants when the the respective digits hit a wall. We see that the learned prior is
able to accurately predict these collisions that result in significant
randomness in the trajectory. 








\begin{figure}[t!]
    \centering
\mbox{
  \includegraphics[width=0.9\linewidth]{imgs/mnist_kth_ssim.png}
 }
 \caption{Quantitative evaluation of SVG-FP and SVG-LP video generation quality on
   SM-MNIST (left) and KTH 
   (right). The models are conditioned on the first 5 frames for
   SM-MNIST and 10 frames for KTH. The vertical bar
   indicates the frame number the models were trained to predict up
   to; further generations indicate generalization ability. 
   Mean SSIM over test videos is plotted with 95\% confidence interval shaded. 
     }
    \label{fig:mnist_kth_ssim}
\end{figure}

\begin{figure}[t!]
    \centering
\mbox{
  \includegraphics[width=0.9\linewidth]{imgs/bair_quant.png}
 }
 \caption{Quantitative comparison between our SVG models and \citet{Babaeizadeh1017} on the BAIR robot dataset. All models are conditioned on the first two frames and generate the subsequent 28 frames. The models were trained to predict up 10 frames in the future, indicated by the vertical bar; further generations indicate generalization ability. Mean SSIM and PSNR over test videos is plotted with 95\% confidence interval shaded.}
    \label{fig:bair_quant}
\end{figure}

\begin{figure}[t!]
    \centering
\mbox{
\includegraphics[width=0.95\linewidth]{imgs/bair_combined_small_vert.png}
}
 \caption{Qualitative comparison between our SVG-LP model and \citet{Babaeizadeh1017}. All models are conditioned on the first two frames of unseen test videos. SVG-LP generates crisper images and predicts plausible movement of the robot arm. }
\label{fig:bair}
\end{figure}


\begin{figure*}[t!]
    \centering
\mbox{
  \includegraphics[width=\linewidth]{imgs/bair_long_27.pdf}
 }
 \caption{Additional examples of generations from SVG-LP showing crisp and varied predictions. A large segment of the background is occluded in conditioning frames, preventing SVG-LP from directly copying these background pixels into generated frames. In addition to crisp robot arm movement, SVG-LP generates plausible background objects in the space occluded by the robot arm in initial frames. }
    \label{fig:bair_long}
\end{figure*}

\begin{figure*}[t!]
    \centering
\mbox{
  \includegraphics[width=\linewidth]{imgs/bair_100.pdf}
 }
 \caption{Long range generations from SVG-LP. The robot arm remains crisp up to 100 time steps and object motion can be seen in the generated video frames. Additional videos can be viewed at: \url{https://sites.google.com/view/svglp/}. }
\label{fig:bair_very_long}
\end{figure*}

One major challenge when evaluating generative video models is
assessing how accurately they capture the full distribution of
possible outcomes, mainly due to the high dimensionality of the space
in which samples are drawn. However, the synthetic nature of single digit SM-MNIST
allows us to investigate this in a principled way. 
A key point to note
is that with each sequence, the digit appearance remains constant with
the only randomness coming from its trajectory once it hits the image
boundary. Thus for a sequence generated from our model, we can
establish the digit trajectory by taking a pair of frames at any time
step and cross-correlating them with the digit used in the initial
conditioning frames. Maxima in each frame reveal the location of the
digit, and the difference between the two gives us the velocity vector
at that time.  By taking an expectation over many samples from our
model (also using the same trajectory but different digits), we can
compute the empirical distribution of trajectories produced by our
model. We can then perform the same operation on a validation set of
ground truth sequences, to produce the true distribution of digit
trajectories and compare it to the one produced by our model.


\fig{mnist_dist} shows SVG-LP (trained on {\em single} digit SM-MNIST) accurately capturing the distribution of MNIST digit trajectories for many time steps. 
The digit trajectory is deterministic before a collision.
This is accurately reflected by the highly peaked distribution of velocity vectors from SVG-LP in the time steps leading up to a collision.
Following a collision, the distribution broadens to approximately uniform before being reshaped by subsequent collisions.
Crucially, SVG-LP accurately captures this complex behavior for many time steps.
The temporally varying nature of the true trajectory distributions further supports the need for a learned prior .

\subsection{KTH Action Dataset}
The KTH Action dataset \cite{kth} consists of real-world videos of
people performing one of six actions (walking, jogging, running, boxing, handwaving, hand-clapping)
against fairly uniform backgrounds. 
The human motion in the video sequences is fairly regular, however there is still uncertainty regarding the precise locations of the person's joints at subsequent time steps. 
We trained SVG-FP, SVG-LP and the deterministic baseline on  video sequences by conditioning on 10 frames and  training the model to predict the next 10 frames in the sequence.

We compute SSIM for SVG-FP and SVG-LP by drawing 100 samples from the model for each test sequence and picking the one with the best score with respect to the ground truth. 
\fig{mnist_kth_ssim}(right) plots average SSIM on unseen test videos.
SVG-FP and SVG-LP perform comparably on this dataset and both outperform the deterministic baseline.
\fig{kth_gen} shows generations from the deterministic baseline and SVG-FP. 
The deterministic model predicts plausible future frames but, due to the inherent uncertainty in precise limb locations, often deviates from the ground truth.
In contrast, different samples from the stochastic model reflect the variability in future frames indicating the latent variables are being utilized even on this simple dataset. 






\subsection{BAIR robot pushing dataset}
The BAIR robot pushing dataset \cite{ebert17} contains videos of a
Sawyer robotic arm pushing a variety of objects around a table top.
The movements of the arm are highly stochastic, providing a good test
for our model. Although the dataset does contain actions given to the
arm, we discard them during training and make frame
predictions based solely on the video input.

Following \citet{Babaeizadeh1017}, we train SVG-FP, SVG-LP and the deterministic baseline by conditioning on the first two frames of a sequence and predicting the subsequent 10 frames. 
We compute SSIM for SVG-FP and SVG-LP by drawing 100 samples from the model for each test sequence and picking the one with the best score with respect to the ground truth. 
\fig{bair_quant} plots average SSIM and PSNR scores on 256 held out
test sequences, comparing to the state-of-the-art approach of \citet{Babaeizadeh1017}.  
This evaluation consists of conditioning on 2 frames and generating 28
subsequent ones, i.e. longer than at train time,  demonstrating the generalization capability of SVG-FP and SVG-LP. 
Both SVG-FP and SVG-LP outperform \citet{Babaeizadeh1017} in terms of SSIM.
SVG-LP outperforms the remaining models in terms of PSNR for the first few steps, after which  \citet{Babaeizadeh1017} is marginally better. 
Qualitatively, SVG-FP and SVG-LP produce significantly sharper generations than \citet{Babaeizadeh1017}, as illustrated in \fig{bair}.
PSNR is biased towards overly smooth (i.e. blurry) results which might explain the slightly better PSNR scores obtained by \citet{Babaeizadeh1017} for later time steps. 

SVG-FP and SVG-LP produce crisp generations many time steps into the future. \fig{bair_long} shows sample generations up to 30 time steps alongside the ground truth video frames.
We also ran SVG-LP forward for 100 time steps and continue to see
crisp motion of the robot arm (see \fig{bair_very_long}).



