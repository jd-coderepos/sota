We conducted experiments on two public available datasets and various modality fusion experiments. 
If not stated otherwise we use the top-1 accuracy as reporting metric for the final epoch of the trained model.

\subsection{Datasets}

\paragraph{\utdmhad} 

\utdmhad{} \cite{DBLP:conf/icip/ChenJK15} is a relatively small dataset containing 861 samples and 27 action classes, which thereby results in shorter training durations for neural networks. Eight individuals (four females and four males) perform each action a total of four times, captured from a front-view perspective by a single Kinect camera. 
\utdmhad{} also includes gyroscope and accelerometer modalities by letting each subject wear the inertial sensor on either the right wrist or on the right hip, depending on whether an action is primarily performed using the hands or the legs. 
For the following experiments using this dataset, the protocol from the original paper \cite{DBLP:conf/icip/ChenJK15} is used.

\paragraph{\mmact}
The \mmact{} dataset \cite{DBLP:conf/iccv/KongWDKTM19} contains more than 35k data samples and
35 available action classes. With 20 subjects and four scenes with four currently available different camera perspectives each, the dataset offers a larger variation of scenarios.
RGB videos are captured with a resolution of 1920 $\times$ 1080 pixels at a frame rate of 30 frames per second. 
For inertial sensors, acceleration, gyroscope and orientation data is obtained from a smartphone carried inside the pocket of a subject's pants. Another source for acceleration data is a smartwatch, resulting in data from four sensors in total. For the following experiments using this dataset, the protocol from the original paper \cite{DBLP:conf/iccv/KongWDKTM19} is used which proposes a cross-subject and a cross-view split. Since skeleton sequences are missing in the dataset, we create them from RGB data using OpenPose \cite{DBLP:journals/pami/CaoHSWS21}.

\subsection{Implementation}

Models are implemented using PyTorch 1.6 and trained on a Nvidia RTX 2080 GPU with 8GB of video memory. To guarantee a deterministic and reproducible behavior, all training procedures are initialized with a fixed random seed. Unless stated otherwise, experiments regarding UTD-MHAD use a cosine annealing learning rate scheduler \cite{DBLP:conf/iclr/LoshchilovH17} with a total of 60 epochs, warm restarts after 20 and 40 epochs, an initial learning rate of $1\mathrm{e}{-3}$ and ADAM \cite{DBLP:journals/corr/KingmaB14} optimization. Experiments using RGB data instead run for 50 epochs without warm restarts. Training for MMAct adopts the hyperparameters used by Shi \andothers\cite{DBLP:conf/cvpr/ShiZCL19a}.For the MMACT, skeleton and RGB features were extracted for every third frame for more efficient pre-processing and training. 
The base GCN model is a single-stream AGCN for all experiments.


\subsection{Comparison to the State-of-the-Art}

\begin{table}[h]
    
    \centering
    \subfloat[UTD-MHAD]{
        \tableutdmhad
    }
    \subfloat[MMAct]{
        \tablemmact
    }
    \caption{Comparison to the State-of-the-Art}
\end{table}
\paragraph{\utdmhad{}}
Table \ref{tab:comparison} shows a ranking of all conducted experiments in comparison with other recent state-of-the-art techniques that implement multimodal \gls{har} on \utdmhad{} with the proposed cross-subject protocol. Without \glspl{gcn} and all perform better than the default skeleton-only approach using a single-stream AGCN. 
Additionally, another benchmark using \glspl{gcn} on \utdmhad{} does not exist, thus, making a direct comparison of different approaches difficult. From the listing in Table \ref{tab:comparison}, it is clear that all fusion approaches skeleton and IMU modalities achieve the highest classification performance out of all methods introduced in this work. 
In comparison to the best performing fusion approach of skeleton with IMU nodes appended at its central node.
MCRL \cite{2019ISenJ..19.1862L} uses a fusion of skeleton, depth and RGB to reach 93.02\% (-1.4\%) validation accuracy on \utdmhad{}. Gimme Signals \cite{DBLP:conf/iros/MemmesheimerTP20} reach 93.33\% (-1.09\%) using a CNN and augmented image representations of skeleton sequences. PoseMap \cite{DBLP:conf/cvpr/LiuY18} achieves 94.5\% (+0.08\%) accuracy using pose heatmaps generated from RGB videos. This method slightly outperforms the proposed fusion approach.



\paragraph{\mmact{}}


To show better generalization, we also conducted experiments on the large-scale \mmact{} dataset which contains more modalities, classes and samples as the \utdmhad{} dataset. Note we only use the cross-subject protocol, the signal modalities can not be seperated by view.
A comparison of approaches regarding the MMAct dataset is given in Table \ref{tab:comparison2}. Kong \andothers\acite{DBLP:conf/iccv/KongWDKTM19} propose along with the \mmact{} dataset the MMAD approach, a multimodal distillation method utiliziting an attention mechanism that incorporates acceleration, gyroscope, orientation and RGB. For evaluation, they use the F1-measure and reach an average of 66.45\%. Without the attention mechanism, the approach (MMD) yields 64.33\%. An approach utilizing the standard distillation approach Single Modality Distillation (SMD) yields 63.89\%. The current baseline is set by SAKDN \cite{DBLP:journals/tip/LiuWLL21} which distills sensor information to enhance action recognition for the vision modality. Experiments show that the skeleton-based approach can be further improved by fusion with just the acceleration data to reach a recognition F1-measure of 89.60\% (+12.37\%). The \mmact{} dataset contains two accelerometers, where only the one from the smartwatch yields a mention-able improvement.
The most significant improvement of our proposed approach is yielded by introducing the skeleton graph. 
In contrast, while the fusion approaches of skeleton and all four sensors do not improve the purely skeleton-based approach of 88.65\% (+13.41\%), with 85.5\% (+10.26\%) without additional edges and 84.78\% (+9.54\%) with additional edges, both reach a higher F1-measure than the baseline but also impact the pure skeleton-based recognition negatively.  


\subsection{Ablation Study}

\paragraph{Fusion of Skeleton and RGB}

\begin{figure}[!ht]
    \centering
    \subfloat[Skeleton + RGB Patch Feature Fusion]{
        \includegraphics[width=.65\linewidth]{rgb_patches_network}
    }

    \subfloat[Skeleton + Resnet-18 Generated Feature Fusion]{
        \includegraphics[width=.7\linewidth]{rgb_encoder_network_1}
    }

    \subfloat[Skeleton + Modified R(2+1)D Generated Feature Fusion]{
        \includegraphics[width=.7\linewidth]{rgb_encoder_network_2}
    }
    \caption[Skeleton + RGB Fusion Models]{The three different skeleton + RGB Fusion models with reference of an image from UTD-MHAD. The first model generates a feature for each node, while the last two generate a feature for the entire image that is distributed to the nodes and adjusted as part of the supervised training.}
    \label{fig:rgb-models}
\end{figure}

Skeletons and RGB videos are combined using the three approaches depicted in \figname\ref{fig:rgb-models}. \figname\ref{fig:rgb-models}a shows an approach using RGB patches that are cropped around each skeleton node and passed to a Resnet-18 to compute a feature vector $\vec{X}_{RGB} \in \real^{(M, 512, T, N)}$ as part of preprocessing. 
The second approach, shown in \figname\ref{fig:rgb-models}b, uses Resnet-18 to compute a feature vector for each image. The resulting feature vector is rescaled to the size $C_E \cdot N \cdot M$ and reshaped to be able to be fused with skeleton data. Similarly, in \figname\ref{fig:rgb-models}c, the third approach uses R(2+1)D.
In terms of parameters, the basis Skeleton model has 3.454.099 parameters, only 2.532 parameters are added for incorporation of inertial measurements into the model Skeleton+IMU(Center) 3.456.631 for a 2.2\% accuracy improvement. Fusion with an RGB
encoder adds five times more parameters (Skeleton+RGB Encoder Resnet-18 with 17.868.514) and a massive training overhead.






\tabname\ref{tab:comparison} shows that the RGB approaches viewed individually (without fusion) do not reach the performance of R(2+1)D pre-trained for action recognition. 
Results regarding the fusion models show a low accuracy of 73.49\% for RGB patch features that have been created outside the training process and 44.6\% for the same procedure without a downscaling \gls{mlp}.
A similar conclusion can be drawn from the remaining two fusion models. 
Using R(2+1)D to produce features shows a slightly increased effectiveness of +1.79\% (91.62\%) over Resnet-18 (89.83\%) but -0.7\% in comparison to the solely skeleton-based approach.

\paragraph{Fusion of Skeleton and IMU}

Fusion of skeletal and inertial sensor data is done according to \figname\ref{fig:skeleton-imu-fusion}. \figname\ref{fig:skeleton-imu-nodes} shows the skeleton structure of UTD-MHAD and illustrates two possibilities for fusing the red IMU graph nodes with the skeleton by connecting them to different skeleton joint nodes. In Figure \ref{fig:skeleton-imu-nodes}a, nodes are appended at the central skeleton joint as it is defined in ST-GCN and AGCN papers. The configuration depicted in Figure \ref{fig:skeleton-imu-nodes}b is attributed to the way sensors are worn by subjects of UTD-MHAD. This configuration is therefore not used for MMAct. 
Additional configurations arise when additional edges are drawn between the newly added nodes. According to Figure \ref{fig:skeleton-imu-fusion}b, another experiment involves broadcasting the $\vecspace{\real}{6}$-sized IMU feature vector to each skeleton joint and fuse them at channel dimension.
\begin{wrapfigure}[13]{r}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/mmact-conf-skeleton-acc-watch-selection.png}
    \caption{Confusion matrix for the results on \mmact{} with the fusion of skeleton and accelerometer measurements from the smartwatch with highlighted high-confused actions.}
    \label{fig:mmact_confusion}
\end{wrapfigure}
From the results in \tabname\ref{tab:comparison}, it is observable that all skeleton graphs with additional associated IMU nodes at each point in time improve the classification performance by at least one percent. In comparison to a skeleton-only approach, variants with additional edges between the newly added nodes perform generally worse than their not-connected counterparts and are both at 93.26\% (+0.94\%). The average classification accuracy of both other variants reaches 94.42\% (+2.1\%) and 94.07\% (+1.75\%). Despite having a slightly increased accuracy for appending new nodes to the existing central node, both variants almost reach equal performance and the location where nodes are appended seemingly does not matter much. While all experiments with fusion at spatial dimension show increased accuracies, the only experiment that does not surpass the skeleton-based approach is about fusion of both modalities at channel dimension, reaching 90.29\% (-2.03\%) accuracy.

\begin{figure}[ht]
    \centering
\includegraphics[width=.9\linewidth]{images/mmact-barchart-all-results.png}
    \caption{Class specific accuracy for all \mmact{} classes for the fusion of various data modalities with \approachname{}.}
    \label{fig:mmact_barchart}
\end{figure}
For MMAct, all experiments are conducted using only the configuration in Figure \ref{fig:skeleton-imu-nodes}a and its variation with interconnected nodes. \tabname\ref{tab:comparison2} shows that the skeleton-based approach reaches 87.85\% accuracy for a cross-subject split, fusion approaches including all four sensors perform worse and reach only 84.85\% (-3\%) and 84.4\% (-3.45\%). Mixed results are achieved when individual sensors are not part of the fusion model. Fusion using only one of the phone's individual sensors, acceleration, gyroscope or orientation, reaches comparable results with 87.70\% (-0.15\%), 86.35\% (-1.5\%) and 87.65\% (-0.2\%) accuracy, respectively. On the contrary, performing a fusion of skeleton and acceleration data obtained by the smartwatch or with the fusion of both acceleration sensors shows an improved accuracy of 89.32\% (1.47\%) and 89.30\% (1.45\%).

\begin{wraptable}[10]{r}{0.32\textwidth}
    \centering
    \scriptsize
    \vspace{-3.0\baselineskip}
    \begin{tabular}{l|r|r}
    \hline
        \textbf{Class} & \textbf{Skl} &	\textbf{Skl + Acc} \\
        \hline
        carrying\_heavy &	24.69 & 43.83 \\
        checking\_time &	81.93 & 96.58 \\ 
        drinking &	85.00 & 95.00 \\
        transferring\_object &	87.23	& 96.10 \\
        pointing &	84.52 & 92.34 \\
        \hline
    \end{tabular}
    \caption{Top-5 most improved classes by the fusion of skeleton (Skl) and additional accelerometer (Acc) data from the smartwatch.}
    \label{tab:top_5_watch_improvement}
\end{wraptable}
\tabname \ref{tab:top_5_watch_improvement} shows the top-5 improved classes by the fusion with the accelerometer measurements of a smartwatch. All the top-5 improved actions have a high arm movement in common. In \figname \ref{fig:mmact_confusion} we give a confusion matrix for the Skeleton + Accelerometer (Watch) and highlight the most confused classes. Especially the variations of the "carrying" actions are hard to distinguish by their obvious similarity. Also, actions that contain sudden movements with high acceleration peaks are often confused ("jumping" is often considered as "falling"). In general, most of the activities can be recognized quite well.
\figname \ref{fig:mmact_barchart} gives a general comparison of all class-specific results on different fusion experiments. Especially the fusion from skeleton-sequences with the accelerometer measurements (skeleton + acc (watch)) suggest a high improvement on many classes, especially the similar "carrying" classes.










\paragraph{Fusion of Skeleton, RGB and IMU}



One experiment is conducted using skeleton, RGB and IMU with IMU nodes appended to the skeleton central node without additional edges in combination with and all three RGB early fusion approaches.
The results in \tabname\ref{tab:comparison} show that, like previously except for the RGB patch feature model, all models achieve an accuracy over 90\%, albeit not reaching the same values as the skeleton and IMU fusion approach.















\subsection{Limitations and Discussion}

Comparing skeleton and skeleton + IMU, the fused approach generally has less misclassifications in all areas. Especially similar actions, such as "throw", "catch", "knock" or "tennis swing", are able to be classified more confidently. The only action with decreased recognition accuracy using the fused approach is "jog" which is misclassified more often as "walk", two similar actions and some of the few with sparse involvement of arm movement.
Common problems for all RGB approaches regarding UTD-MHAD are a small number of training samples, resulting in overfitting in some cases that can not be lifted by either weight decay or dropout. Another fact is the absence of object interactions in UTD-MHAD. With the exception of "sit2stand" and "stand2sit", actions such as "throwing", "catching", "pickup\_throw" or sports activities never include any objects. As pointed out previously, skeleton is focused purely on human movements and, by that, omits all other objects inside of a scene. RGB still contains such visual information, making it supposedly more efficient in recognizing object interactions. In contrast, many of \mmact{}'s actions, like "transferring\_object", "using\_pc", "using\_phone" or "carrying", make use of real objects. While fusion with RGB modality achieves similar accuracies as other approaches, incorporating the data into the network increases the training time by up to a magnitude of ten; hence, the RGB fusion models do not provide a viable alternative to skeleton and IMU regarding the current preprocessing and training configurations.
Therefore, due to timely constraints, experiments for fusion of skeleton and RGB modalities on the larger dataset \mmact{} are omitted.
