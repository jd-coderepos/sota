\documentclass{article}
\usepackage{spconf,amsmath,graphicx,url}

\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[LFE,LAE]{fontenc}

\usepackage{arabtex}
\usepackage{utf8}
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}

\setcode{utf8}


\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{VoxLingua107: a Dataset for Spoken Language Recognition}
\name{Jörgen Valk, Tanel Alumäe}
\address{Tallinn University of Technology, Estonia}



\begin{document}
\selectlanguage{english}
\ninept
\maketitle
\begin{abstract}
This paper investigates the use of automatically collected web audio data
for the task of spoken language recognition.
We generate semi-random search phrases from language-specific Wikipedia data that are 
then used to retrieve videos from YouTube for 107 languages.
Speech activity detection and
speaker diarization are used to extract segments from the videos that contain
speech. Post-filtering is used to remove segments from the database that are likely 
not in the given language, increasing the proportion of correctly labeled segments to 98\%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances.
We use the data to build language recognition models for several spoken language identification
tasks. Experiments show that using the automatically retrieved training data gives competitive
results to using hand-labeled proprietary datasets. The dataset is publicly available\footnote{\url{http://bark.phon.ioc.ee/voxlingua107/}}.
\end{abstract}

\begin{keywords}
Spoken language recognition, web scraping, x-vectors, crowd-sourcing
\end{keywords}

\section{Introduction}

Spoken language recognition (SLR) is the task of automatically classifying an utterance based on the spoken language. SLR is used as a pre-processing step in several applications, such as automatic call routing, multilingual spoken translation and human-machine communication systems, multilingual speech transcription systems and spoken document retrieval. SLR is also often used in the area of intelligence and security.

In the past 20 years, development of SLR technology has been largely fostered through NIST Language Recognition Evaluations (LREs). As a result, the most popular benchmarks for evaluating new SLR models and methods are NIST LRE evaluation datasets \cite{sadjadi20182017}. The NIST LRE evaluations datasets contain mostly narrow-band conversational telephone speech. In order to build competitive systems for NIST LREs, large amounts of conversational telephone data from the particular languages are used. Such datasets are typically distributed by the Linguistic Data Consortium (LDC) and cost thousands of dollars. For example, the standard Kaldi \cite{kaldi} recipe for LRE07\footnote{\url{https://github.com/kaldi-asr/kaldi/blob/master/egs/lre07/v2/run.sh}} relies on 18 LDC SLR datasets that cost \<>\xrightarrow{}\xrightarrow{}\xrightarrow{}\xrightarrow{}\xrightarrow{}\xrightarrow{}F_{act}F_{act}C_{avg}$ (lower is better).  }
\label{tab:lre07}
\centering
\begin{tabular}{lrrrr}
\hline
System & 3 sec & 10 sec & 30 sec & Average \\
\hline
\multicolumn{5}{l}{Trained on in-domain data (telephone speech)} \\
\hline
GMM-MMI \cite{torres2008mitll} & 17.28 & 5.90 & 2.10 & 8.42 \\
Fusion of models \cite{torres2008mitll} & 13.32 &  3.55 & 0.97 & 5.95 \\
Phonotactic \cite{gelly2016divide} & 18.59 & 6.28 & 1.34 & 8.73 \\
Fusion of models \cite{gelly2016divide} & 15.29 & 4.54 & 1.30 & 7.04 \\
Kaldi i-vector\footnote{\url{https://github.com/kaldi-asr/kaldi/blob/master/egs/lre07/v1/run.sh}}  & 26.04 &  11.93 &  4.52  & 14.17  \\
Kaldi i-vector DNN\footnote{\url{https://github.com/kaldi-asr/kaldi/blob/master/egs/lre07/v2/run.sh}}  & 19.67  & 7.84  & 3.31 & 10.27  \\
CNN-SAP \cite{cai2018exploring} & 8.59 & 2.49 & 1.09 & 4.06  \\
CNN-LDE \cite{cai2018exploring} & 8.25 & 2.61 & 1.13 & 4.00 \\
\hline
\multicolumn{5}{l}{Our systems trained on our dataset} \\
\hline
Resnet34 (noisy data)   & 10.58 & 3.33 & 1.72 & 5.21 \\
Resnet34 (cleaned data) & 9.39 & 3.14 & 1.90 & 4.81 \\
\hline
\end{tabular}
\end{table}


\section{Analysis of language embeddings}


\begin{figure*}[tbh]
  \centering
  \includegraphics[width=\linewidth]{tsne.pdf}
  \caption{T-SNE plot of the language embeddings.}
  \label{fig:tsne}
\end{figure*}

After training an x-vector language recognition system, we extracted the embeddings of all utterances in the training data. LDA was used to reduce the dimensionality of the embeddings to 250. Language embeddings were then calculated by averaging language-specific utterances, transforming them to lower-dimensional space using LDA and applying length normalization.

Figure \ref{fig:tsne} shows the T-SNE plot of the resulting language embeddings.
It demonstrates that the learned language embeddings represent the structure of the language families remarkably well, although there are obvious deviations from the linguistic hierarchy, probably caused by cultural and geographical influences.

\section{Alternative approaches}



Using YouTube as the data source is not the only option for building a large-scale dataset for SLR.
The largest and most well-known free multilingual speech dataset is Mozilla Common Voice\footnote{\url{http://commonvoice.mozilla.org/}}. It contains prompted speech utterances donated by volunteers. Utterance metadata contains speaker identity, text of the prompt and language. Many utterances also include demographic metadata like age, sex, and accent. The dataset currently consists of 5671 validated hours in 54 languages. The main benefit of the this dataset over ours is that all the recordings in the validated corpus are confirmed by volunteers to really contain the given utterance. The main shortcoming of the corpus, with regard to using it for language recognition ``in the wild'', is that it contains only read speech. There is also a lot of variety in the amount available of data per language, and the language coverage is lower than in our data. Nevertheless, in the future we plan to experiment with combining Common Voice with our dataset.

Another practical alternative to our method is using podcasts as training data. 
Podcasts usually contain spontaneous speech which is beneficial for training SLR models.
Unfortunately, it seems that finding podcasts by language is not an easy task. There exists some domain-specific podcast listings by language (e.g., ``geek'' podcasts in 15 languages\footnote{\url{http://github.com/ayr-ton/awesome-geek-podcasts}}), but none of the large podcast directories allow browsing for podcasts by the language.

\section{Availability}

The dataset is publicly available at \url{http://bark.phon.ioc.ee/voxlingua107/}. Similarly to the VoxCeleb and ADI17 datasets, the VoxLingua107 dataset is distributed under the Creative Commons Attribution 4.0 International License. The copyright remains with the original owners of the video. 

While YouTube users own the copyright to their own videos, using the audio in the vidoes for training language identification models has very limited and transformative purpose and qualifies  thus as ``fair use'' of copyrighted materials. YouTube's terms of service forbid downloading, storing and distribution of videos. However, the aim of this rule is clearly to forbid unfair monetization of the content by third-party sites and applications. Our dataset contains the videos in segmented audio-only form that makes the monetization of the actual distributed content extremely difficult.

We also point out that the distribution of languages, accents, dialects, genders, races and societal factors in this dataset is not representative of the global population. Using this dataset for training and deploying models may thus introduce unintended biases.

\section{Conclusion}

We described a new speech dataset VoxLingua107 that is compiled from YouTube data. We showed that the dataset is suitable for training spoken language recognition models for classifying data ``from the wild''. Experiments with the KALAKA-3 dataset showed that the dataset can be used for building both a backend feature extractor and a frontend classifier, or only for training the backend, while the frontend is trained on small amounts of hand-labeled data.
Experiments on the NIST LRE07 evaluation data showed that using VoxLinugua107 data  results in a classifier that is not far in accuracy from a model trained on large amounts of in-domain data. 

Future work includes using the VoxLingua107 dataset together with in-domain training data, and augmenting input filterbank features with multilingual bottleneck features.


\bibliographystyle{IEEEbib}

\bibliography{mybib}


\end{document}
