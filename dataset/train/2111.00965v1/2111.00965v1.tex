\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[final]{neurips_2021}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{wrapfig,lipsum,booktabs}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath,bm}
\usepackage{amssymb}

\usepackage{booktabs, multirow}
\newcommand{\shifeng}[1]{\textcolor{red}{\textsf{[SF: #1]}}}
\newcommand{\chen}[1]{\textcolor{blue}{\textsf{[CZ: #1]}}}
\newcommand{\ning}[1]{\textcolor{purple}{\textsf{[KN: #1]}}}
\newcommand{\tom}[1]{\textcolor{orange}{\textsf{[TR: #1]}}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\usepackage{subfigure}
\renewcommand{\thesubfigure}{(\roman{subfigure})}

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multicol}

\newcommand{\round}[1]{\lfloor #1 \rceil}
\newcommand{\quant}[1]{\bar{#1}}
\newcommand{\qv}[1]{\bar{\mathbf{#1}}}
\newcommand{\ebar}[1]{{\tiny #1}}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{definition}[theorem]{Definition}

\title{iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder}



\author{Shifeng Zhang, Ning Kang, Tom Ryder, Zhenguo Li \\
  Huawei Noah's Ark Lab\\
  \texttt{\{zhangshifeng4, kang.ning2, tom.ryder1, li.zhenguo\}.huawei.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
It was estimated that the world produced  () of data in 2020,
resulting in the enormous costs of both data storage and transmission.
Fortunately, recent advances in deep generative models have spearheaded a new class of so-called "neural compression" algorithms, which significantly outperform traditional codecs in terms of compression ratio.
Unfortunately, the application of neural compression garners little commercial interest due to its limited bandwidth; therefore, developing highly efficient frameworks is of critical practical importance. In this paper, we discuss lossless compression using normalizing flows which have demonstrated a great capacity for achieving high compression ratios.
As such, we introduce iFlow, a new method for achieving efficient lossless compression. 
We first propose Modular Scale Transform (MST) and a novel family of numerically invertible flow transformations based on MST. Then we introduce the Uniform Base Conversion System (UBCS), a fast uniform-distribution codec incorporated into iFlow, enabling efficient compression. 
iFlow achieves state-of-the-art compression ratios and is  quicker than other high-performance schemes. Furthermore, the techniques presented in this paper can be used to accelerate coding time for a broad class of flow-based algorithms. 
\end{abstract}

\section{Introduction}

The volume of data, measured in terms of IP traffic, is currently witnessing an exponential year-on-year growth ~\cite{forecast2019cisco}. Consequently, the cost of transmitting and storing data is rapidly becoming prohibitive for service providers, such as cloud and streaming platforms. These challenges increasingly necessitate the need for the development of high-performance lossless compression codecs.

One promising solution to this problem has been the development of a new class of so-called “neural compression” algorithms ~\cite{mentzer2019practical,townsend2019practical,hoogeboom2019integer,berg2020idf++,ho2019compression,townsend2019hilloc,kingma2019bit,mentzer2020learning,cao2020lossless,zhang2021ivpf}. 
These methods typically posit a deep probabilistic model of the data distribution, which, in combination with entropy coders, can be used to compress data with the minimal codelength bounded by the negative log-likelihood~\cite{mackay2003information}.
However, despite reliably improved compression performance compared to traditional codecs ~\cite{gage1994new,rabbani2002jpeg2000,collet2016smaller,roelofs1999png}, meaningful commercial applications have been limited by impractically slow coding speed.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{iflow_overview.pdf}\\
\end{center}
\caption{Illustration of IDF(++), iVPF, LBB and the proposed iFlow method. Flow layers with darker color denote higher expressive power. The top-right of each model illustration shows the key procedure of exact bijections between  and . The compression ratio and bandwidth are listed below.}
\label{fig:illustration}
\end{figure}





In this paper, we focus on developing approaches with deep probabilistic models based on \textit{normalizing flows}~\cite{dinh2016density,kingma2018glow,prenger2019waveglow,kumar2019videoflow,chen2020vflow,lippe2020categorical,kim2020softflow}. A normalizing flow admits a learnable bijective mapping between input data and a latent variable representation. In this paradigm, inputs can be compressed by first transforming data to latent variables, with the resulting output encoded by a prior distribution. Compared to other classes of generative models, normalizing flows typically perform best in both tasks of probability density estimation (as compared with variational autoencoders ~\cite{kingma2013auto,higgins2016beta,child2020very}) and inference speed (as compared with autoregressive factorizations ~\cite{salimans2017pixelcnn++,van2016conditional,jun2020distribution}). This suggests that compression with flows can jointly achieve high compression ratios along with fast coding times. 





Unfortunately, lossless compression requires discrete data for entropy coding, and the continuous bijections of normalizing flows would not guarantee discrete latent variables. As such, transformations would require discretization, resulting in a loss of information ~\cite{zhang2021ivpf,behrmann2020understanding}. To resolve this issue, Integer Discrete Flows (IDF) ~\cite{hoogeboom2019integer, berg2020idf++} (Fig. \ref{fig:illustration}, left) proposed an invertible mapping between discrete data \textit{and} latent variables. Similarly, iVPF~\cite{zhang2021ivpf} achieves a discrete-space bijection using \textit{volume-preserving} flows. However, the above models must introduce constraints on flow layers to ensure a discrete-space bijection, which limits the expressivity of the transformation. Local Bits-Back Coding (LBB) ~\cite{ho2019compression} (Fig. \ref{fig:illustration}, middle) was the first approach to succeed in lossless coding with the flexible family of continuous flows. It resolves the information loss by coding numerical errors with the rANS coder~\cite{duda2013asymmetric}. However, the coder is extraordinarily slow, making the method impractical.







In this paper, we introduce iFlow: a numerically invertible flow-based neural compression codec that bridges the gap between high-fidelity density estimators and practical coding speed. To achieve this we introduce two novelties: Modular Scale Transform (MST) and Uniform Base Conversion Systems (UBCS). MST presents a flexible family of bijections in discrete space, deriving an invertible class of flows suitable for lossless compression. UBCS, built on the uniform distribution, permits a highly efficient entropy coding compatible with MST.
The main ideas are illustrated in Fig. \ref{fig:illustration}(right). We test our approach across a range of image datasets against both traditional and neural compression techniques. Experimentally we demonstrate that our method achieves state-of-the-art compression ratios, with coding time  quicker than that of the next-best scheme, LBB.











\section{Numerically Invertible Flows and Lossless Compression}

Let  be our normalizing flow, which we build as a composition of layers such that . Defining  -dimensional  and , the latents can be computed as , with  calculated according to

where  is the Jocabian matrix of the transformation  at . In this paper, we use  to denote \textit{base 2 logarithms}. Training the flow should minimize the negative log-likelihood .
The inverse flow is defined as , such that . There are many types of invertible bijections, . Popular choices include element-wise, autoregressive (e.g. coupling layers)~\cite{zhang2021ivpf} and  convolution transformations~\cite{kingma2018glow}.

To perform lossless compression with normalizing flows, the input data should be transformed to latent variables, which are then encoded with a prior distribution. As data and encoded bits should be in binary format,  and  must be discrete, with a bijection between discrete inputs and outputs established. However, as discussed, this operation is usually intractable with popular classes of transformations used in continuous flows due to the numerical errors induced by discretization.

In this section, we introduce a novel algorithm derived from continuous flows that allows us to achieve an exact bijiective mapping between discrete  \textit{and} discrete . We present the general idea of a flow layer's numerical invertibility in discrete space in Sec. \ref{sec:k_precision}. We then introduce our flow layers in Sec. \ref{sec:inv_eleflow} and \ref{sec:inv_general}, with their application to compression discussed in Sec. \ref{sec:inv_model} and \ref{sec:inv_deq}.

\subsection{Invertibility of Flows in Discrete Space}
\label{sec:k_precision}

To discretize the data, we follow ~\cite{zhang2021ivpf} and adopt {\em -precision quantization} to assign floating points to discretization bins such that

where  is the floor function. The associated quantization error is bounded by . Any -dimensional  can be quantized into bins with volume . The probability mass of  can then be approximated by , such that the theoretical codelength is .

Denote our proposed numerically invertible flow (iFlow) according to . The input and output should be subject to -precision quantization, where each layer should be appropriately invertible such that . Denoting  and , it is further expected that  and . We will show in the following subsections that  can be derived from \textit{any} continuous flow  in such a way that the error between  and  is negligible.

\subsection{Numerically Invertible Element-Wise Flows}
\label{sec:inv_eleflow}

We begin by describing our approach for element-wise flows , where  is a monotonic element-wise transformation. For notation simplicity, we assume that the flow contains just one layer.
In most cases,  does not present a unique inverse in discrete space, such that . 
In what follows, we introduce an approach to derive an invertible, discrete-space operation from any element-wise flow transformation.
For simplicity, we denote the inputs and output pairs of a continuous flow as  and ; and we denote the -precision quantized input-output pairs of iFlow as  and .

\subsubsection{Scale Flow}
\label{sec:scale_eleflow}

\begin{algorithm}[h]
\small
\caption{Modular Scale Transform (MST): Numerically Invertible Scale Flow .}
\begin{multicols}{2} 
\textbf{Forward MST: .} 

\begin{algorithmic}[1]
\STATE ;
\STATE Decode  using ; ;
\STATE ;
\STATE Encode  using ;
\RETURN .
\end{algorithmic}

\textbf{Inverse MST: .} 

\begin{algorithmic}[1]
\STATE ;
\STATE Decode  using ; ;
\STATE ;
\STATE Encode  using ;
\RETURN .
\end{algorithmic}
\end{multicols}
\vspace{-8pt}
\label{alg:mst}
\end{algorithm}

We begin with the scale transformation defined as , from which we are able to build more complex flow layers.
The input can be converted to an integer with , and the latent variable  can be recovered from integer  by . Inspired by MAT in iVPF~\cite{zhang2021ivpf}, we first approximate  with a fractional such that  where . Denote  where  is sampled uniformly from , we can obtain  with , and the remainder  can be encoded to eliminate the error. It is clear that  are bijections.

Let  be the uniform distribution such that . The numerically invertible scale flow  is displayed in Alg. \ref{alg:mst}. As the modular operation is essential to Alg. \ref{alg:mst}, we name it the \textit{Modular Scale Transform} (MST). Setting  and  to be large, we observe that the following propositions hold.
\begin{proposition}
\label{the:mst1}
~\cite{zhang2021ivpf} .
\end{proposition}
\begin{proposition}
The codelength of MST is .
\label{the:mst2}
\end{proposition}

Proposition \ref{the:mst1} establishes that the error is small if  and  are large. Proposition \ref{the:mst2} demonstrates that the codelength is almost exactly the log-Jocabian of .
The above properties and the correctness of the algorithm are discussed in the Appendix. 
Further note that, with the exception of the usual encoding and decoding processes, MST's operations can be parallelized, resulting in minimal additional overhead. In contrast, MAT's operation in iVPF~\cite{zhang2021ivpf} only deals with volume-preserving affine transform and is performed sequentially along dimensions, limiting its usage and efficiency.




\subsubsection{General Element-wise Flow}

\begin{algorithm}[h]
\small
\caption{Numerically Invertible Element-wise Flows}
\begin{multicols}{2} 
\textbf{Forward: .} 

\begin{algorithmic}[1]
\STATE Get  given , so that ;
\STATE Set large ; get  with Eq. (\ref{eq:f_inp_r}); ;
\STATE Get  with forward MST in Alg. \ref{alg:mst}, given input  and coefficients ;
\RETURN . 
\end{algorithmic}

\textbf{Inverse: .} 

\begin{algorithmic}[1]
\STATE Get  given , so that ;
\STATE Set large ; get  with Eq. (\ref{eq:f_inp_r}); ;
\STATE Get  with inverse MST in Alg. \ref{alg:mst}, given input  and coefficients ;
\RETURN . 
\end{algorithmic}
\end{multicols}
\vspace{-8pt}
\label{alg:ni_eleflow}
\end{algorithm}



For simplicity, we assume the non-linear  is monotonically increasing. For a monotonically decreasing , we simply invert the sign: .

MST in Alg. \ref{alg:mst} works with \textit{linear} transformations, and is incompatible with \textit{non-linear} functions. Motivated by linear interpolation, we can approximate the non-linear flow  with a piecewise linear function. In general, consider input  to be within an interval \footnote{All intervals must 
partition the domain of .} and . The linear interpolation of  is then

It follows that  can be derived from MST with input  followed by adding . Furthermore,  and  can be recovered with the corresponding inverse transformation.
To preserve monotonicity,  must be within interval . Denoting the invertible linear flow derived from  as , it must hold that . As we use MST in Alg. \ref{alg:mst}, we observe that when  is approximated with , the minimum possible value of  is  (when  in Line 2) and the maximum possible value of  is  (when  in Line 2). Given large , the largest possible value of  should be

Another difficulty is in determining the correct interpolation interval . One simple solution is to split the input domain into uniform intervals with length  such that . However, for the inverse computation given , it is not easy to recover the interpolation interval as  may not be within . Instead, as , the interval  can be obtained via a binary search in which  is obtained with . Another approach is to split the co-domain into uniform intervals such that , with . In this case, determining  during the inverse computation is simple. While for the forward pass,  should be determined with a binary search such that . In practice, we have simpler tricks to determine the correct interval  for both the forward and inverse computation, which can be found in the Appendix.

The general idea of the non-linear flow adaptations are summarized in Alg. \ref{alg:ni_eleflow}. Note that Alg. \ref{alg:ni_eleflow} can be used in any element-wise flow including linear flow. In Alg. \ref{alg:ni_eleflow}, Proposition \ref{the:mst2} holds such that 
. 
It is possible to arrive at a similar conclusion in Proposition \ref{the:mst1} where the corresponding error is . 

\subsection{Practical Numerically Invertible Flow Layers}
\label{sec:inv_general}

Whilst one can use many types of complex transformations to build richly expressive flow-based models, to ensure the existence and uniqueness of the inverse computation, these layers are generally constructed with element-wise transformations. In this subsection, we demonstrate the invertibility of discretized analogs to some of the most widely used flow layers using the operators as described in Sec. \ref{sec:inv_eleflow}. Such flows include autoregressive flows~\cite{huang2018neural} (including coupling flows~\cite{dinh2014nice,dinh2016density,ho2019flow++}) and  convolutional flows~\cite{kingma2018glow}. 

\subsubsection{Autoregressive and Coupling Flows}

\begin{algorithm}[h]
\small
\caption{Numerically Invertible Autoregressive Flow}
\begin{multicols}{2} 
\textbf{Forward: .} 

\begin{algorithmic}[1]
\FOR {}
\STATE  with Alg. \ref{alg:ni_eleflow} (forward);
\ENDFOR
\RETURN .
\end{algorithmic}

\textbf{Inverse: .} 

\begin{algorithmic}[1]
\FOR {}
\STATE  with Alg. \ref{alg:ni_eleflow} (inverse);
\ENDFOR
\RETURN .
\end{algorithmic}
\end{multicols}
\vspace{-8pt}
\label{alg:ni_ar}
\end{algorithm}

Supposing that the inputs and outputs,  and , are split into  parts , the autoregressive flow  can be represented as , where  is the element-wise flow (discussed in Sec. \ref{sec:inv_eleflow}), conditioned on . Now let  denote the invertible element-wise flow transformation as discussed in Alg. \ref{alg:mst}-\ref{alg:ni_eleflow}. Alg. \ref{alg:ni_ar} then illustrates the details of an invertible autoregressive flow .

Propositions  and  hold in our discretized autoregressive transformation. In fact, the log-determinant of Jacobian is given by , and the expected codelength is simply .

When  and , the autoregressive flow is reduced to a \textit{coupling flow}, which is widely used in flow-based models~\cite{dinh2014nice,dinh2016density,ho2019flow++}. It is therefore trivially clear that the coupling flow is additionally compatible with Alg. \ref{alg:ni_ar}.

\subsubsection{ Convolutional Flow}

 convolutional layers can be viewed as a matrix multiplication along a channel dimension~\cite{kingma2018glow}. Let  be inputs and outputs along channels, and  the weights of our network. The objective is to obtain  where .

We use the ideas of iVPF~\cite{zhang2021ivpf} to achieve a numerically invertible  convolutional transformation. In particular, we begin by performing an LU decomposition such that . It then follows that the  convolution is performed with successive matrix multiplications with  and . In iVPF, the authors extensively discussed matrix multiplications with factors  and ~\cite{zhang2021ivpf}. Meanwhile, one can view the matrix multiplication with  as a scale transform, such that , where  is an element-wise multiplication and  are the diagonal elements of . MST in Alg. \ref{alg:mst} can then be applied. In such a case, it is clear that Proposition \ref{the:mst1} and \ref{the:mst2} hold for a  convolutional flow. For Proposition \ref{the:mst2}, it is observed that .





\subsection{Building Numerically Invertible Flows}
\label{sec:inv_model}

Our flow model is constructed as a composition of layers , where each layer is a transformation of the type discussed in Sec. \ref{sec:inv_eleflow} and \ref{sec:inv_general}. Let us represent the resulting flow as , where  is a discretized transformation derived from the corresponding continuous . It is clear that the quantized input  and latent  establish a bijection with successive transformations between discrete inputs and outputs. For the forward pass,  is computed with ; for the inverse pass,  is recovered with the inverse flow  such that .

For our resultant flow model, we can draw similar conclusions as in Propositions \ref{the:mst1} and \ref{the:mst2}. Firstly, the error of  and  is small, bounded by . Secondly, the codelength is approximately .

\subsection{Lossless Compression with Flows via Bits-back Dequantization}
\label{sec:inv_deq}

Armed with our flow model , performing lossless compression is straight-forward. For the encoding process, the latent is generated according to . We then encode  with probability . For the decoding process,  is decoded with , and  is recovered with . The expected codelength is approximately  such that

However, we note that if  is large,  and the codelengths will also be large, resulting in a waste of bits. For what follows, we adopt the bits-back trick in LBB~\cite{ho2019compression} to reduce the codelength. In particular, consider coding with input data , where a -precision noise vector  is decoded with  and added to input data such that . In this way,  is then encoded with our flow . For the decoding process,  is recovered by applying the inverse transformation. We name this coding process \textit{Bits-back Dequantization}, which is summarized in Alg. \ref{alg:compression}.

\begin{algorithm}[h]
\small
\caption{Lossless Compression with iFlow.}
\begin{multicols}{2} 
\textbf{Encode .} 

\begin{algorithmic}[1]
\STATE Decode  using ;
\STATE ;
\STATE Encode  using .
\end{algorithmic}

\textbf{Decode.} 

\begin{algorithmic}[1]
\STATE Decode  using ;
\STATE ;
\STATE Encode  using ;
\RETURN .
\end{algorithmic}
\end{multicols}
\vspace{-8pt}
\label{alg:compression}
\end{algorithm}

In practice,  is constructed with a flow model such that  where .  is decoded by first decoding  with , and then applying . Thus decoding  involves  bits.
Overall, the expected codelength is exactly the dequantization lower bound~\cite{hoogeboom2020learning} such that


\subsection{Extensions}
\label{sec:iflow_extension}

With novel modifications, flow models can be applied to the generation of various data types, obtaining superior performance ~\cite{chen2020vflow,lippe2020categorical,kim2020softflow}. These models can be used for improved lossless compression. In general, given input data , these models generate intermediate data  with , and the density of  is modelled with flow model . For generation, when  is generated with the inverse flow,  is generated with . It is clear that  can be estimated with variational lower bound such that . For lossless compression,  can be coded with bits-back coding, which is similar with Alg. \ref{alg:compression}. In the encoding process, we first decode  with  and then encode  with  (similar with Line 1 in Alg. \ref{alg:compression}-Encode). We then obtain the prior  (Line 2), before finally encoding  with  (Line 3). In the decoding process,  is firstly decoded with  (similar to Line 1 in Alg. \ref{alg:compression}-Decode), and then recovered  and decoded  with  (Line 2). Finally, we encode using  with  \cite{townsend2019practical}. The expected codelength is approximately


We introduce a selection of recent, state-of-the-art flow-based models modified according to the above. Each model corresponds to a certain coding algorithm.

\textbf{VFlow~\cite{chen2020vflow}.} VFlow expands the input data dimension with variational data augmentation to resolve the bottleneck problem in the flow model. In VFlow, , where , is modelled with flows  such that  ( are priors). Then we have  (as ). Thus for the encoding process,  are decoded. To construct  we have ; and then  is encoded with iFlow. For the decoding process,  is decoded with the inverse iFlow, and then  is recovered with . Here  is encoded and  is the decoded output. As VFlow achieves better generation results compared with general flows, one would expect a better compression ratio with VFlow.

\textbf{Categorical Normalizing Flow~\cite{lippe2020categorical}.} Categorical Normalizing Flows (CNF) succeed in modelling categorical data such as text, graphs, etc. Given categorical data ,  is represented with word embeddings such that , in which  could be a Gaussian or logistic distribution. Then  with  being the prior over categories. Thus , and  can be coded with Alg. \ref{alg:general_iflow} given  and the iFlow.



\section{Uniform Base Conversion Systems}
\label{sec:uans}

\begin{algorithm}[h]
\small
\caption{Uniform Base Conversion Systems}
\begin{multicols}{2} 
\textbf{ENCODE  using .} 

\textbf{Input:} symbol , state , bit-stream \texttt{bs}.

\textbf{Output:} new state  and bit-stream \texttt{bs}.

\begin{algorithmic}[1]
\STATE ;
\IF {}
\STATE \texttt{bs.push\_back()}; \quad  push  bits to bit-stream.
\STATE ;
\ENDIF
\RETURN , \texttt{bs}.
\end{algorithmic}

\textbf{DECODE with .} 

\textbf{Input:} state , bit-stream \texttt{bs}.

\textbf{Output:} decoded , new state  and bit-stream \texttt{bs}.

\begin{algorithmic}[1]
\IF {}
\STATE \texttt{bs.pop\_back()}; \quad  get last  bits from bit-stream and pop them.
\ENDIF
\STATE ;
\STATE ;
\RETURN , , \texttt{bs}.
\end{algorithmic}
\end{multicols}
\vspace{-8pt}
\label{alg:uans}
\end{algorithm}

The previous section demonstrates that coding with a uniform distribution is central to our algorithm. Note that the distribution varies in each coding process, thus {\em dynamic} entropy coder is expected. Compared to the Gaussian distribution used in LBB~\cite{ho2019compression}, a uniform distribution is simpler, yielding improved coding speed. As follows, we introduce our \textit{Uniform Base Conversion Systems} (UBCS), which is easy to implement and the coding bandwidth is much greater than that of rANS~\cite{duda2013asymmetric}.

UBCS is implemented based on a number-base conversion. The code state  is represented as an integer.
For coding some symbol  with a uniform distribution , the new state  is obtained by converting an -base digit  to an integer such that 

For decoding with , given state , the symbol  and state  are recovered by converting the integer to an -base digit such that 

We note, however, that  will become large when more symbols are encoded, and computing Eq. (\ref{eq:uans_encode}-\ref{eq:uans_decode}) with large  will be inefficient. Similar to rANS, we define a ``normalized interval'' which bounds the state  such that  ( are some integers values) after coding each symbol. For the encoding process, if , the lower  bits are written to disk, and the remaining bits are reserved such that . For the decoding process, if , the stored  bits should be read and appended to the state before decoding. In this way, the decoded state is contained within the interval . The initial state can be set such that . We illustrate this idea in Alg. \ref{alg:uans}.

The correctness of Alg. \ref{alg:uans} is illustrated in the following theorem. \textbf{P1} demonstrates that the symbols can be correctly decoded with a UBCS coder, with coding performed in a first-in-last-out (FILO) fashion. \textbf{P2} shows that the codelength closely approximates the entropy of a uniform distribution, subject to a large  and . The proof of the theorem is in the Appendix.

\begin{wraptable}{r}{5.5cm}
\vspace{-19pt}
\centering
\small
\caption{Coding bandwidth (M symbol/s) of UBCS and rANS coder on different threads(thrd). We use the implementations in~\cite{ho2019compression} for evaluating rANS.}
\label{tab:coders}
\begin{tabular}{cccc}
\toprule
 & \# thrd & rANS & \textbf{UBCS} \\
\midrule
\multirow{2}{*}{Encoder} & 1 & 5.1\ebar{0.3} & \bf 380\ebar{5} \\
 & 16 & 21.6\ebar{1.1} & \bf 2075\ebar{353} \\
\midrule
\multirow{2}{*}{Decoder} & 1 & 0.8\ebar{0.02} & \bf 66.2\ebar{1.7} \\
 & 16 & 7.4\ebar{0.5} & \bf 552\ebar{50} \\
\bottomrule
\end{tabular}
\vspace{-30pt}
\end{wraptable} 

\begin{theorem}
Consider coding symbols  with  using Alg.  \ref{alg:uans}, and then decode  sequentially. Suppose (1) the initial state and bit-stream are  respectively; (2) After coding , the state is  and the bit-stream is ; (3) After decoding , the state is  and the bit-stream is . We have

\textbf{P1:}  for all .

\textbf{P2:} Denote by the codelength  where \texttt{len} is the total number of bits in the bit-stream. Then .
\label{the:uans}
\end{theorem}



In practice, we set  and . Compared to rANS~\cite{duda2013asymmetric} and Arithmetric Coding (AC)~\cite{witten1987arithmetic}, UBCS is of greater efficiency as it necessitates fewer operations (more discussions are shown in the Appendix). UBCS can achieve greater computational efficiency via instantiating multiple UBCS coders in parallel with multi-threading. Table \ref{tab:coders} demonstrates that UBCS achieves coding bandwidths in excess of giga-symbol/s -- speed significantly greater than rANS.

\section{Experiments}
\label{sec:exp}

In this section, we perform a number of experiments to establish the effectiveness of iFlow. We will investigate: (1) how closely the codelength matches the theoretical bound; (2) the efficiency of iFlow as compared with the LBB~\cite{ho2019compression} baseline; (3) the compression performance of iFlow on a series low and high-resolution images. 


\subsection{Flow Architectures and Datasets}

We adopt two types of flow architectures for evaluation: Flow++~\cite{ho2019flow++} and iVPF~\cite{zhang2021ivpf}. Flow++ is a state-of-the-art model using complex non-linear coupling layers and variational dequantizations~\cite{hoogeboom2020learning}. iVPF is derived from a volume-preserving flow in which numerically invertible discrete-space operations are introduced. The models are re-implemented or directly taken from the corresponding authors. 
Unless specified, we use  and set large  -- around , which we analyse further in the Appendix. To reduce the auxiliary bits in the bits-back coding scheme, we partition the -dimensional data into  splits and perform MST (in Alg. \ref{alg:mst} and \ref{alg:ni_eleflow}) for each split sequentially. In this case, the auxiliary bits can be reduced to  in MST. We use  in this experiment.

Following the lossless compression community~\cite{ho2019flow++,berg2020idf++,hoogeboom2019integer,townsend2019hilloc,zhang2021ivpf}, we perform evaluation using toy datasets CIFAR10, ImageNet32 and ImageNet64. Results for alternate methods are obtained via re-implementation or taken directly from the corresponding papers, where available. We further test the generalization capabilities of iFlow in which all toy datasets are compressed with a model trained on ImageNet32. For benchmarking our performance on high-resolution images, we evaluate iFlow using CLIC.mobile, CLIC.pro\footnote{\url{https://www.compression.cc/challenge/}} and DIV2k~\cite{agustsson2017ntire}. For this purpose, we adopt our ImageNet32/64 model for evaluation, and process an image in terms of  or  patches, respectively. The experiment is conducted with PyTorch framework with one Tesla P100 GPU.

\begin{table}[t]
\centering
\small
\caption{Coding performance of iFlow, LBB and iVPF on CIFAR10 dataset. We use batch size 64.}
\label{tab:baselines}
\begin{tabular}{llccccccc}
\toprule
flow & compression & & & & \multicolumn{2}{c}{encoding time (ms)} & \multicolumn{2}{c}{decoding time (ms)}\\
arch. & technique & nll & bpd & aux. bits & inference & coding &  inference & coding \\
\midrule
\multirow{2}{*}{Flow++} & LBB~\cite{ho2019compression} & \multirow{2}{*}{3.116} & \textbf{3.118} & 39.86 & \multirow{2}{*}{16.2\ebar{0.3}} & 116\ebar{1.0} & \multirow{2}{*}{32.4\ebar{0.2}} & 112\ebar{1.5} \\
 & \textbf{iFlow (Ours)} & & \textbf{3.118} & \textbf{34.28} & & \textbf{21.0\ebar{0.5}} & & \textbf{37.7\ebar{0.5}} \\
\midrule
\multirow{2}{*}{iVPF} & iVPF~\cite{zhang2021ivpf} & \multirow{2}{*}{3.195} & 3.201 & \textbf{6.00} & \multirow{2}{*}{5.5\ebar{0.1}} & 11.4\ebar{0.2} & \multirow{2}{*}{5.2\ebar{0.1}} & 13.5\ebar{0.3} \\
 & \textbf{iFlow (Ours)} & & \textbf{3.196} & 7.00 & & \textbf{7.1}\ebar{0.2} & & \bf 9.7\ebar{0.2} \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Compression Performance}

For our experiments, we use the evaluation protocols of codelength and compression bandwidth. The codelength is defined in terms of the average bits per dimension (bpd). For no compression, the bpd is assumed to be 8. 
The compression bandwidth evaluates coding efficiency, which we define in terms of symbols compressed per unit time.

Table \ref{tab:baselines} demonstrates the compression results on CIFAR10. Note that we only report the encoding time (results on decoding time are similar, and are available in the Appendix). For iVPF, we use settings almost identical to the original paper~\cite{zhang2021ivpf} such that . 

Firstly, we observe that, when using both the Flow++ and iVPF architectures, iFlow achieves a bpd very close to theoretically minimal codelength. When using Flow++, iFlow achieves identical performance as that of LBB. For the iVPF architecture, iFlow outperforms the underlying iVPF as it avoids the need to store  bits for each data sample. 


Secondly, the coding latency highlights the main advantage of iFlow: we achieve encoding  faster than that of LBB and over  that of iVPF. In fact, the use of UBCS only represents  of the total coding time for all symbols (4.8ms in Flow++ and 1.6ms in iVPF).
In contrast, the rANS coder of LBB commands over  of the total coding latency, which is the principal cause of LBB's impracticality. Indeed, Table  demonstrates that our UBCS coder achieves a speed-up in excess of  that of rANS (which results in a coder latency of 4.8ms in iFlow vs. 99.8ms in LBB). 


Lastly, compared with LBB, iFlow necessitates fewer auxiliary bits. In fact, LBB requires crica  bits per dimension (for  and small  in ~\cite{ho2019compression}). Meanwhile, iFlow requires approximately , and  is usually small with large .






\begin{table}[t]
\centering
\small
\caption{Compression performance in bpd on benchmarking datasets.  denotes the generation performance in which the models are trained on ImageNet32 and tested on other datasets.  denotes compression of high-resolution datasets with our ImageNet64-trained model.}
\label{tab:small}
\begin{tabular}{lccc|ccc}
\toprule
         & ImageNet32 & ImageNet64 & CIFAR10 & CLIC.mobile & CLIC.pro &DIV2K \\
\midrule
PNG \cite{boutell1997png}     & 6.39        & 5.71        & 5.87 &3.90 & 4.00 & 3.09 \\
FLIF \cite{sneyers2016flif}    & 4.52        & 4.19        & 4.19&2.49 & 2.78 & 2.91 \\
JPEG-XL \cite{alakuijala2019jpeg} & 6.39 & 5.74 & 5.89 &2.36 & 2.63 & 2.79 \\
\midrule
L3C \cite{mentzer2019practical}      & 4.76        & 4.42           & - &2.64 & 2.94 & 3.09 \\
RC \cite{mentzer2020learning}  &-&-&-   & 2.54 & 2.93 & 3.08 \\
Bit-Swap \cite{kingma2019bit} & 4.50        & -           & 3.82 &-&-&-\\
\midrule
IDF \cite{hoogeboom2019integer}     & 4.18        & 3.90        & 3.34 &-&-&-\\
IDF++ \cite{berg2020idf++}   & 4.12        & 3.81        & 3.26 &-&-&-\\
iVPF \cite{zhang2021ivpf}     & 4.03        & 3.75     & 3.20 & - & - & - \\
LBB \cite{ho2019compression}      & \textbf{3.88}        & \textbf{3.70}        & \textbf{3.12} &-&-&-\\
\textbf{iFlow (Ours)}      & \textbf{3.88}  & \textbf{3.70} & \textbf{3.12}  & - & - & -\\
\midrule
HiLLoC \cite{townsend2019hilloc}  & 4.20        & 3.90        & 3.56 &-&-&-\\
IDF \cite{hoogeboom2019integer}     & 4.18        & 3.94        & 3.60 &-&-&-\\
iVPF \cite{zhang2021ivpf}     & 4.03        & 3.79     & 3.49 & 2.47/2.39 & 2.63/2.54 & 2.77/2.68 \\
\textbf{iFlow (Ours)}     & \textbf{3.88}  & \textbf{3.65}  & \textbf{3.36} &\textbf{2.26/2.26} & \textbf{2.45/2.44} & \textbf{2.60/2.57} \\
\bottomrule
\end{tabular}
\end{table}
\subsection{Comparison with the State-of-the-Art}
To further demonstrate the effectiveness of iFlow, we compare the compression performance on benchmarking datasets against a variety of neural compression techniques. These include, L3C~\cite{mentzer2019practical}, Bit-swap~\cite{kingma2019bit}, HilLoc~\cite{townsend2019hilloc}, and flow-based models IDF~\cite{hoogeboom2019integer}, IDF++~\cite{berg2020idf++}, iVPF~\cite{zhang2021ivpf}, LBB~\cite{ho2019compression}. We additionally include a number of conventional methods, such as PNG~\cite{boutell1997png}, FLIF~\cite{sneyers2016flif} and JPEG-XL~\cite{alakuijala2019jpeg}.

\textbf{Experiments on Low Resolution Images.}
Compression results on our described selection of datasets are available in left three columns of Table \ref{tab:small}. 
Here we observe that iFlow obtains improved compression performance over all approaches with the exception of LBB on low-resolution images, for which we achieve identical results.

\textbf{Generalization.}
The last four rows in Table \ref{tab:small} demonstrate the literature-standard test of generalization, in which ImageNet32 trained model are used for testing. From these results, it is clear that iFlow achieves the best generalization performance in this test. It is worth noting that we obtain an improved performance on ImageNet64 when using our ImageNet32-trained model.

\textbf{Experiments on High Resolution Images.}
Finally, we test iFlow across a number of high-resolution image datasets. Here images are processed into non-overlapping  and  patches for our ImageNet32 and ImageNet64-trained models. The right three columns in Table \ref{tab:small} display the results, which is observed that iFlow outperforms all compression methods across all available benchmarks. 
Note that as we crop patches for compression, the compression bandwidth is the same as that in small images like CIFAR10, i.e., 5 times faster than LBB and 30\% speedup compared with iVPF.


\vspace{-3pt}
\section{Related Work}
\vspace{-3pt}









{\em Dynamic} Entropy coders, such as Arithmetic Coding (AC)~\cite{witten1987arithmetic} and Asymmetric Numerical Systems (ANS)~\cite{duda2013asymmetric}, form the basis of lossless compression.  However, the binary search protocol required at decode time and their comparatively high number of numerical operations make them both relatively time-consuming. The term {\em dynamic} means that the data symbols are in different distributions, in this case, efficient entropy coders like Huffman coding~\cite{huffman1952method} and tANS~\cite{duda2013asymmetric} are incompatible. Our proposed UBCS is {\em dynamic} coder, which requires only two operations per symbol, producing an faster algorithm than AC and ANS.

In order to utilise entropy coders, one must estimate the data distribution. For this purpose, the wider community has employed a variety of density estimators. One of the most popular, autoregressive models~\cite{salimans2017pixelcnn++}, estimates the joint density with per-pixel autoregressive factorizations. Whilst commonly achieving state-of-the-art compression ratios, the sequential pixel-by-pixel nature of encoding and/or decoding makes them impractically time-consuming. 
Alternatively, variational autoencoders (VAEs) ~\cite{kingma2013auto,kingma2019bit,townsend2019hilloc} maximise a lower bound on the marginal data likelihood (otherwise known as the \textit{ELBO}). With the bits-back coding framework ~\cite{townsend2019practical}, the theoretical codelength is exactly equal to the ELBO. However, in most cases, VAE formulations typically produce inferior compression ratios as there exists a gap between the ELBO and the true data likelihood.





As discussed, flow-based models~\cite{ho2019flow++,kingma2018glow,dinh2016density}, which admit exact likelihood computation, represent an alternative route for density estimation. 
IDF~\cite{hoogeboom2019integer} and IDF++~\cite{berg2020idf++} proposed the titular \textit{integer discrete flow} to preserve the existence and uniqueness of an invertible mapping between discrete data and latent variables. In a similar vein, iVPF~\cite{zhang2021ivpf} achieved a mapping with volume-preserving flows. Here the remainders of a division operation are stored as auxiliary states to eliminate the numerical error arising from discretizing latent variables. However, all of these models must introduce constraints on the underlying transform, limiting their representational power. LBB~\cite{ho2019compression} was the first flow-based lossless compression approach to admit a broad class of invertible flows based on \textit{continuous} transforms. LBB established this family of flexible bijections by introducing local bits-back coding techniques to encode numerical errors. However, LBB typical requires the coding of many such errors and does so with the ANS scheme, posing obvious challenges to computational efficiency.








\vspace{-3pt}
\section{Conclusions and Discussions}
\vspace{-3pt}
\label{sec:conclusion}

In this paper, we have proposed iFlow, a numerically invertible flow-based model for achieving efficient lossless compression with state-of-the-art compression ratios. To achieve this, we have introduced the Modular Scale Transform and Uniform Base Conversion Systems, which jointly permit an efficient bijection between discrete data and latent variables. 
Experiments demonstrate that the codelength comes extremely close to the theoretically minimal value, with compression achieved much faster than the next-best high-performance scheme. 
Moreover, iFlow is able to achieve state-of-the-art compression ratios on real-word image benchmarks.

We additionally consider the potential for extending iFlow. That is, recent advances in normalizing flows have achieved improved generation performance across various data types~\cite{lippe2020categorical,chen2020vflow,kim2020softflow}. We have discussed the possible extension to incorporate these advancements in Sec. \ref{sec:iflow_extension}, and consider its application as future work. We further recognise that compression aproaches, of which iFlow is one, present significant data privacy issues. That is, the generative model used for compression may induce data leakage; therefore, the codec should be treated carefully as to observe data-privacy laws. 






{\small
\bibliographystyle{plainnat}
\bibliography{flow}
}






\newpage

\appendix

\section{Proofs}

\subsection{Correctness of MST (Alg. \ref{alg:mst})}

As  are quantized to -precision,  and  form a bijection, and so do  and . 
Thus MST is correct if and only if  and  are valid bijections. In particular, denoting , we will show .

In fact, according to forward MST, , thus . Considering  is first accurately decoded by inverse MST (in Line 2), it is clear that , and . Thus the correctness of MST is proven.

\subsection{Propositions \ref{the:mst1}-\ref{the:mst2} in MST (Alg. \ref{alg:mst})}

Firstly, as , we have . Secondly, as  where , we have  and , thus . Then the error between  and  is ; therefore, Proposition \ref{the:mst1} holds.

For Proposition \ref{the:mst2}, decoding from  involves  bits, and encoding  involves  bits. As  and , the codelength of MST is . Thus Proposition \ref{the:mst2} holds.

\subsection{Correctness of our Invertible Non-linear Flows (Alg. \ref{alg:ni_eleflow})}

If the interpolation interval  and  are identical in both the forward and inverse processes,  is additionally identical in both the forward and inverse processes. Consequently, an exact bijection with MST algorithm is trivially achieved. Thus we principally seek to show that the interpolation interval can be correctly determined. Before the proof, it must be emphasised that the interpolation interval should demonstrate the following properties:
\begin{enumerate}
    \item The interpolation interval is counted and covers the domain/co-domain.  must be within the discretized set such that  (e.g. );
    \item The interpolation interval must be not intersected. There does not exist  such that  and . 
\end{enumerate}

Firstly, we show that in the forward computation,  will always be within . In fact,  where . As , . As , , following from Eq. (\ref{eq:f_inp_r}) it is clear that . Therefore, it holds that .

Secondly, we show that during the inverse computation, the interpolation intervals  and  are the same as that in the forward process. In other words, if the interpolation interval is  given , it holds that . In fact,  is not preserved, as . It then follows that  could not be in . Similarly,  is not preserved -- otherwise  and  could not be in . Overall,  and , such that only  satisfy this condition.

\subsection{Propositions \ref{the:mst1}-\ref{the:mst2} in Invertible Non-linear Flows (Alg. \ref{alg:ni_eleflow})}

In general,  is bounded for all . Consider  in the small interval such that  or  (). We first prove the following two propositions in :
\begin{enumerate}
    \item ;
    \item 
\end{enumerate}

In fact, by performing the Taylor expansion at , we have  and , where . 

Firstly, . As  are bounded, , and we have . As , then . Finally, .

Secondly, by denoting  and replacing  with its Taylor expansion, we have . As  is bounded, . Moreover, it is clear that , and as such it finally holds that .

For Proposition \ref{the:mst1}, with MST, we have  where , thus . Moreover, with Eq. (\ref{eq:f_inp_r}), it is easy to arrive at that , and therefore . Overall, , and finally .

For Proposition \ref{the:mst2}, with , it is clear that the expected codelength is . Overall,  if  are large and . 

\subsection{Theorem \ref{the:uans} in UBCS (Alg. \ref{alg:uans})}

\textbf{P1}. We begin by showing that  for all  with mathematical induction. In fact, when , . When  and , denote . It is therefore clear that . Note that  and therefore . If , , it follows that , . Overall, . Thus  for all  such that


We will now demonstrate that  for all . Denote . 

(i) Consider . (a) If , the last  bits (denoted by ) will be popped from  and added to . In this case, according to Eq. (\ref{eq:uans_states}),  must be encoded to form . Thus in the decoding process,  is popped from , and therefore , , and . (b) If , no bits are popped from  such that . In this case, according to Eq. (\ref{eq:uans_states}), no bits are pushed to  and therefore . In the decoding process, . Overall, {\bf P1} holds for .

(ii) If {\bf P1} holds for , we will prove that {\bf P1} holds for . (a) If , the last  bits will be popped from  and added to . In this case, in the encoding process, as , according to Eq. (\ref{eq:uans_states}),  must be encoded to form  to obtain . In the decoding process, as ,  is popped from  in the decoding process, it is therefore seen that . Finally we obtain , and . (b) If , no bits are popped from  such that . In this case, in the encoding process, as , according to Eq. (\ref{eq:uans_states}), no bits are pushed to  to obtain  and therefore . In the decoding process, we have . Overall, {\bf P1} holds for .

From (i)(ii), it is concluded that {\bf P1} holds by proof of mathematical induction.

{\bf P2}. Denote that the lower  bits of  need to be push to  at   for all ). In other words, we have


Firstly, it is clear that . Secondly, for any , as , it is clear that . Thus , and therefore


Note that the above inequality also holds for  in which . With Eq. (\ref{eq:uans_ranges}), . As , it holds that


If , ; otherwise,  and . Overall, we finally obtain



Note that as  and , it is clear that . With Eq. (\ref{eq:uans_applength}), the codelength is finally computed as 

which completes the proof.

\section{Details of Alg. \ref{alg:ni_eleflow} in iFlow}

The main difficulty is in determining the interpolation interval  given  or . The main paper discusses two interpolation tricks: (1) interpolating uniform intervals in domain  and (2) interpolating uniform intervals in co-domain .

\begin{algorithm}[ht]
\small
\caption{Uniform interpolating interval  in numerically invertible element-wise flows.}
\begin{multicols}{2} 
\textbf{Determine  given .} 

\begin{algorithmic}[1]
\STATE ;
\STATE ;
\RETURN .
\end{algorithmic}

\vspace{8pt}
\textbf{Determine  given .} 

\begin{algorithmic}[1]
\STATE ; 
\STATE ;
\STATE ;
\IF {}
\STATE ;
\ELSE
\STATE 
\ENDIF
\RETURN .
\end{algorithmic}
\end{multicols}
\vspace{-8pt}
\label{alg:ni_ele_x}
\end{algorithm}

\textbf{Interpolating uniform intervals in .}
This usually applies in the case that  is large (e.g. \texttt{inverse sigmoid}). The uniform interval in domain  is defined as . The corresponding interval in the co-domain is . For the forward pass, given , the interval can be obtained as above. For the inverse pass, we first compute  and then compute . Finally, we have . If , we set the interval ; otherwise . The method is summarized in Alg. \ref{alg:ni_ele_x}.

The correctness of the algorithm is guaranteed provided that  is Lipchitz continuous and the numerical error between  and  is limited. Let  for all  and  for all . We will show that Alg. \ref{alg:ni_ele_x} is correct if . In fact, it is clear that . Similarly, . As  is monotonically increasing and , it is clear that . As , we have . (i) When , it corresponds to . As , it is clear that . Note that  when ; therefore, it must hold that . (ii) When , it corresponds to . It is clear that  -- thus it must hold that . In fact, as ,  is bounded and  is rather small, such that , the correctness of Alg. \ref{alg:ni_ele_x} follows.

\begin{algorithm}[ht]
\small
\caption{Uniform interpolating interval  in numerically invertible element-wise flows.}
\begin{multicols}{2} 
\textbf{Determine  given .} 

\begin{algorithmic}[1]
\STATE ; 
\STATE ;
\STATE ;
\IF {}
\STATE ;
\ELSE
\STATE 
\ENDIF
\RETURN .
\end{algorithmic}

\vspace{8pt}
\textbf{Determine  given .} 

\begin{algorithmic}[1]
\STATE ;
\STATE ;
\RETURN .
\end{algorithmic}

\end{multicols}
\vspace{-8pt}
\label{alg:ni_ele_y}
\end{algorithm}



\textbf{Interpolating uniform intervals in .}
This usually applies in the case that  is small (e.g. \texttt{sigmoid}). The uniform interval in co-domain  is defined as . The corresponding interval in the domain is . For the inverse pass given , the interval can be obtained as above. For the forward pass, we first compute , and then compute , and . If , we set the interval ; otherwise . The method is summarized in Alg. \ref{alg:ni_ele_y}.

Similarly as in Alg. \ref{alg:ni_ele_x}, the correctness of Alg. \ref{alg:ni_ele_y} is ensured by  and the limited numerical errors. The proof is very similar as to that of Alg. \ref{alg:ni_ele_x}.

\section{Extensions of iFlow}

\begin{algorithm}[ht]
\small
\caption{Lossless Compression with Flows.}
\begin{multicols}{2} 
\textbf{Encode .} 

\begin{algorithmic}[1]
\STATE Decode  using ;
\STATE ;
\STATE Encode  using ;
\STATE Encode  using .
\end{algorithmic}

\textbf{Decode.} 

\begin{algorithmic}[1]
\STATE Decode  using ;
\STATE ;
\STATE Decode  using ;
\STATE Encode  using ;
\RETURN .
\end{algorithmic}
\end{multicols}
\vspace{-8pt}
\label{alg:general_iflow}
\end{algorithm}

The extension of iFlow for lossless compression is summarized in Alg. \ref{alg:general_iflow}. Note that for \textbf{Variational Dequantization Flow~\cite{hoogeboom2020learning,ho2019compression} (Alg. \ref{alg:compression})}, . Thus the above coding procedure reduces to Alg. \ref{alg:compression}.

\section{Dynamic Uniform Entropy Coder in AC, ANS and UBCS}

In this section we will demonstrate the effectiveness of UBCS compared with AC and ANS. Both AC and ANS use probability mass function (PMF) and cumulative distribution function (CDF) for encoding and inverse CDF for decoding. For ease of coding, the PMF and CDF are all mapped to integers in . For uniform distribution  in which , the most simple way to compute PMF and CDF are


Given , the output of inverse CDF , should be exactly . The general way to determine  is binary search. But for uniform distribution, we can directly obtain the inverse CDF such that


We summarize uniform entropy coder AC, rANS and UBCS as follows:

For AC:
\begin{itemize}
    \item {\bf Initial state}: interval ;
    \item {\bf Encoding}: get , get  with Eq. (\ref{eq:uniform_pc}), update interval  such that , get  as the encoded bits; 
    \item {\bf Decoding}: for encoded bits  and current interval , get , decode  with Eq. (\ref{eq:uniform_invc}), update interval  such that ;
    \item {\bf Number of atom operations in encoding}: one division, one multiplication\footnote{We omit add/sub operations as they are negligible compared with multiplication/division.};
    \item {\bf Number of atom operations in decoding}: two divisions, one multiplication. Binary search may involve if Eq. (\ref{eq:uniform_invc}) is not used. 
\end{itemize}

For rANS:
\begin{itemize}
    \item {\bf Initial state}: number ;
    \item {\bf Encoding}: set , get  with Eq. (\ref{eq:uniform_pc}), update ; 
    \item {\bf Decoding}: set , for encoded bits , get , decode  with Eq. (\ref{eq:uniform_invc}), update ;
    \item {\bf Number of atom operations in encoding}: two divisions, two multiplications\footnote{The multiplication/division/mod with  only involve bit operations. With the result of ,  only involve one multiplication.};
    \item {\bf Number of atom operations in decoding}: two divisions, one multiplication. Binary search may involve if Eq. (\ref{eq:uniform_invc}) is not used. 
\end{itemize}

For UBCS:
\begin{itemize}
    \item {\bf Initial state}: number ;
    \item {\bf Encoding}: update ; 
    \item {\bf Decoding}: for encoded bits , decode , update ;
    \item {\bf Number of atom operations in encoding}: one multiplication;
    \item {\bf Number of atom operations in decoding}: one division with remainder. 
\end{itemize}

Overall, UBCS uses the least number of atom operations, which conveys that UBCS performs the best. Moreover, with PMF and CDF in Eq. (\ref{eq:uniform_pc}), the optimal entropy coder cannot be guaranteed. 

\section{More Experiments}

In this section we will demonstrate our performance attributes across more benchmarking datasets. All experiments are conducted in PyTorch framework on one NVIDIA Tesla P100 GPU and Intel(R) Xeon(R) CPU E5-2690 @ 2.60GHz CPU. The code for LBB and the Flow++ model is directly taken from the original paper under MIT license. 


\subsection{Coding Efficiency of UBCS}

\begin{table}[t]
\centering
\small
\caption{More results on coding bandwidth (M symbol/s) of UBCS and rANS coder. We use the implementations in~\cite{ho2019compression} for evaluating rANS.}
\label{tab:more_coders}
\begin{tabular}{cccc}
\toprule
 & \# threads & rANS & \textbf{UBCS} \\
\midrule
\multirow{4}{*}{Encoder} & 1 & 5.1\ebar{0.3} & \bf 380\ebar{5} \\
 & 4 & 10.8\ebar{1.9} & \bf 709\ebar{56} \\
 & 8 & 15.9\ebar{1.4} & \bf 1297\ebar{137} \\
 & 16 & 21.6\ebar{1.1} & \bf 2075\ebar{353} \\
\midrule
\multirow{4}{*}{Decoder} & 1 & 0.80\ebar{0.02} & \bf 66.2\ebar{1.7} \\
 & 4 & 2.8\ebar{0.1} & \bf 248\ebar{8} \\
 & 8 & 5.5\ebar{0.2} & \bf 460\ebar{16} \\
 & 16 & 7.4\ebar{0.5} & \bf 552\ebar{50} \\
\bottomrule
\end{tabular}
\end{table} 



\begin{table}[h]
\centering
\small
\caption{Detailed results on the coding performance of iFlow and LBB on ImageNet32. We use a batch size of 64.}
\label{tab:baselines_img32}
\begin{tabular}{llccccccc}
\toprule
flow & compression & & & & \multicolumn{2}{c}{encoding time (ms)} & \multicolumn{2}{c}{decoding time (ms)}\\
arch. & technique & nll & bpd & aux. bits & inference & coding &  inference & coding \\
\midrule
\multirow{2}{*}{Flow++} & LBB~\cite{ho2019compression} & \multirow{2}{*}{3.871} & 3.875 & 45.96 & \multirow{2}{*}{58.7\ebar{0.1}} & 176\ebar{2.8} & \multirow{2}{*}{83.2\ebar{0.4}} & 172\ebar{4.7} \\
 & \textbf{iFlow (Ours)} & & \textbf{3.873} & \textbf{34.40} & & \textbf{66.6\ebar{0.3}} & & \textbf{95.3\ebar{0.3}} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\small
\caption{Detailed results on the coding performance of iFlow and LBB on ImageNet64. We use a batch size of 64.}
\label{tab:baselines_img64}
\begin{tabular}{llccccccc}
\toprule
flow & compression & & & & \multicolumn{2}{c}{encoding time (ms)} & \multicolumn{2}{c}{decoding time (ms)}\\
arch. & technique & nll & bpd & aux. bits & inference & coding &  inference & coding \\
\midrule
\multirow{2}{*}{Flow++} & LBB~\cite{ho2019compression} & \multirow{2}{*}{3.701} & \textbf{3.703} & 38.00 & \multirow{2}{*}{24.4\ebar{0.0}} & 284\ebar{2.5} & \multirow{2}{*}{35.7\ebar{0.1}} & 281\ebar{2.2} \\
 & \textbf{iFlow (Ours)} & & \textbf{3.703} & \textbf{34.42} & & \textbf{45.0\ebar{1.7}} & & \textbf{57.0\ebar{1.4}} \\
\bottomrule
\end{tabular}
\end{table}

The detailed experiments is shown in Table \ref{tab:more_coders}.

\subsection{Compression Performance}

Detailed experimental results on ImageNet32 and ImageNet64 datasets are displayed in Tables \ref{tab:baselines_img32} and \ref{tab:baselines_img64}. Note that we further report the decoding time, which we observe is close to the model inference time.

\subsection{Hyper-parameters}

As discussed in Sec. \ref{sec:inv_model}, the codelength will be affected by the choices of  and . As  is set to a large value -- and will minimally affect the codelength resulting from MST -- we mainly discuss  and . Tables \ref{tab:hparam_c10}, \ref{tab:hparam_img32} and \ref{tab:hparam_img64} illustrate the codelength and auxiliary bits (in bpd) for differing choices of  and . It is clear that, for large , the codelength decreases with a larger . This is expected as larger  corresponds to a greater numerical precision of our linear interpolation. On the other hand, for a fixed , the codelength becomes larger with a smaller , as a smaller  corresponds to a greater quantization error. A smaller  may even lead to the failure of iFlow entirely -- especially if  is close to , which would result in the potential of a zero-valued  in Eq. (\ref{eq:f_inp_r}) for sufficiently small . On the other hand, the auxiliary bits are principally affected by  and not . Therefore we note that a smaller  is preferred. To conclude, we can nonetheless achieve a near-optimal codelength with a considered choice of hyper-parameters. Thus we set  and  for the experiments.

\begin{table}[ht]
\centering
\caption{Codelengths in terms of bpd and auxiliary length on different  and  on the CIFAR10 dataset. N/A denotes the failure of the compression procedure. The theoretical bpd (nll) is \textbf{3.116}.}
\small
\label{tab:hparam_c10}
\begin{tabular}{cccccccc}
\toprule
    & & \multicolumn{5}{c}{} \\
     &  & 6 & 8 & 10 & 12 & 14 \\
    \midrule
    \multicolumn{7}{c}{bpd} \\
    \midrule
    \multirow{8}{*}{} & 18 & 3.229\ebar{0.000} & N/A & N/A & N/A & N/A \\
        & 20 & 3.225\ebar{0.000} & 3.152\ebar{0.000} & N/A & N/A & N/A \\
        & 22 & 3.224\ebar{0.000} & 3.147\ebar{0.000} & 3.130\ebar{0.000} & N/A & N/A \\
        & 24 & 3.224\ebar{0.000} & 3.146\ebar{0.000} & 3.126\ebar{0.000} & 3.124\ebar{0.000} & N/A \\
        & 26 & 3.224\ebar{0.000} & 3.146\ebar{0.000} & 3.125\ebar{0.000} & 3.119\ebar{0.000} & 3.122\ebar{0.000} \\
        & 28 & 3.224\ebar{0.000} & 3.146\ebar{0.000} & 3.124\ebar{0.000} & 3.118\ebar{0.000} & 3.118\ebar{0.000} \\
        & 30 & 3.224\ebar{0.000} & 3.146\ebar{0.000} & 3.124\ebar{0.000} & 3.118\ebar{0.000} & 3.116\ebar{0.000} \\
        & 32 & 3.224\ebar{0.000} & 3.146\ebar{0.000} & 3.124\ebar{0.000} & 3.118\ebar{0.000} & 3.116\ebar{0.000} \\
    \midrule
    \multicolumn{7}{c}{auxiliary length} \\
    \midrule
    \multirow{8}{*}{} & 18 & 24.25\ebar{0.01} & N/A & N/A & N/A & N/A \\
        & 20 & 26.25\ebar{0.01} & 26.27\ebar{0.01} & N/A & N/A & N/A \\
        & 22 & 28.26\ebar{0.01} & 28.27\ebar{0.01} & 28.27\ebar{0.01} & N/A & N/A \\
        & 24 & 30.25\ebar{0.01} & 30.27\ebar{0.01} & 30.27\ebar{0.01} & 30.27\ebar{0.01} & N/A \\
        & 26 & 32.25\ebar{0.01} & 32.27\ebar{0.01} & 32.27\ebar{0.01} & 32.27\ebar{0.01} & 32.27\ebar{0.01} \\
        & 28 & 34.25\ebar{0.01} & 34.27\ebar{0.01} & 34.27\ebar{0.01} & 34.27\ebar{0.01} & 34.27\ebar{0.01} \\
        & 30 & 36.25\ebar{0.01} & 36.26\ebar{0.01} & 36.27\ebar{0.01} & 36.27\ebar{0.01} & 36.27\ebar{0.01} \\
        & 32 & 38.25\ebar{0.01} & 38.26\ebar{0.01} & 38.27\ebar{0.01} & 38.27\ebar{0.01} & 38.27\ebar{0.01} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Codelengths in terms of bpd and auxiliary length on different  and  on a \textbf{SUBSET} of ImageNet32 dataset. N/A denotes the failure of the compression procedure. The theoretical bpd (nll) is \textbf{3.883}.}
\small
\label{tab:hparam_img32}
\begin{tabular}{cccccccc}
\toprule
    & & \multicolumn{5}{c}{} \\
     &  & 6 & 8 & 10 & 12 & 14 \\
    \midrule
    \multicolumn{7}{c}{bpd} \\
    \midrule
    \multirow{8}{*}{} & 18 & 3.994\ebar{0.000} & 3.953\ebar{0.000} & N/A & N/A & N/A \\
        & 20 & 3.985\ebar{0.000} & 3.919\ebar{0.000} & N/A & N/A & N/A \\
        & 22 & 3.983\ebar{0.000} & 3.910\ebar{0.000} & 3.900\ebar{0.000} & N/A & N/A \\
        & 24 & 3.983\ebar{0.000} & 3.908\ebar{0.000} & 3.892\ebar{0.000} & 3.896\ebar{0.000} & 3.928\ebar{0.000} \\
        & 26 & 3.983\ebar{0.000} & 3.908\ebar{0.000} & 3.890\ebar{0.000} & 3.887\ebar{0.000} & 3.894\ebar{0.000} \\
        & 28 & 3.983\ebar{0.000} & 3.908\ebar{0.000} & 3.889\ebar{0.000} & 3.885\ebar{0.000} & 3.886\ebar{0.000} \\
        & 30 & 3.982\ebar{0.000} & 3.908\ebar{0.000} & 3.889\ebar{0.000} & 3.885\ebar{0.000} & 3.884\ebar{0.000} \\
        & 32 & 3.983\ebar{0.000} & 3.908\ebar{0.000} & 3.889\ebar{0.000} & 3.885\ebar{0.000} & 3.884\ebar{0.000} \\
    \midrule
    \multicolumn{7}{c}{auxiliary length} \\
    \midrule
    \multirow{8}{*}{} & 18 & 24.37\ebar{0.01} & 24.37\ebar{0.01} & N/A & N/A & N/A \\
        & 20 & 26.38\ebar{0.01} & 26.39\ebar{0.01} & N/A & N/A & N/A \\
        & 22 & 28.38\ebar{0.01} & 28.39\ebar{0.01} & 28.39\ebar{0.01} & N/A & N/A \\
        & 24 & 30.38\ebar{0.01} & 30.39\ebar{0.01} & 30.40\ebar{0.01} & 30.39\ebar{0.01} & 30.38\ebar{0.01} \\
        & 26 & 32.38\ebar{0.01} & 32.39\ebar{0.01} & 32.40\ebar{0.01} & 32.40\ebar{0.01} & 32.39\ebar{0.01} \\
        & 28 & 34.38\ebar{0.01} & 34.39\ebar{0.01} & 34.40\ebar{0.01} & 34.40\ebar{0.01} & 34.40\ebar{0.01} \\
        & 30 & 36.38\ebar{0.01} & 36.39\ebar{0.01} & 36.39\ebar{0.01} & 36.40\ebar{0.01} & 36.40\ebar{0.01} \\
        & 32 & 38.38\ebar{0.01} & 38.39\ebar{0.01} & 38.39\ebar{0.01} & 38.39\ebar{0.01} & 38.39\ebar{0.01} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Codelengths in terms of bpd and auxiliary length on different  and  on a \textbf{SUBSET} of ImageNet64 dataset. N/A denotes the failure of the compression procedure. The theoretical bpd (nll) is \textbf{3.718}.}
\small
\label{tab:hparam_img64}
\begin{tabular}{cccccccc}
\toprule
    & & \multicolumn{5}{c}{} \\
     &  & 6 & 8 & 10 & 12 & 14 \\
    \midrule
    \multicolumn{7}{c}{bpd} \\
    \midrule
    \multirow{8}{*}{} & 18 & 3.829\ebar{0.000} & 3.779\ebar{0.000} & 3.867\ebar{0.000} & N/A & N/A \\
        & 20 & 3.823\ebar{0.000} & 3.753\ebar{0.000} & 3.760\ebar{0.000} & 3.863\ebar{0.000} & N/A \\
        & 22 & 3.821\ebar{0.000} & 3.746\ebar{0.000} & 3.733\ebar{0.000} & 3.755\ebar{0.000} & 3.861\ebar{0.000} \\
        & 24 & 3.821\ebar{0.000} & 3.744\ebar{0.000} & 3.727\ebar{0.000} & 3.729\ebar{0.000} & 3.754\ebar{0.000} \\
        & 26 & 3.821\ebar{0.000} & 3.744\ebar{0.000} & 3.725\ebar{0.000} & 3.722\ebar{0.000} & 3.727\ebar{0.000} \\
        & 28 & 3.821\ebar{0.000} & 3.744\ebar{0.000} & 3.725\ebar{0.000} & 3.720\ebar{0.000} & 3.721\ebar{0.000} \\
        & 30 & 3.821\ebar{0.000} & 3.744\ebar{0.000} & 3.725\ebar{0.000} & 3.720\ebar{0.000} & 3.719\ebar{0.000} \\
        & 32 & 3.821\ebar{0.000} & 3.744\ebar{0.000} & 3.725\ebar{0.000} & 3.720\ebar{0.000} & 3.719\ebar{0.000} \\
    \midrule
    \multicolumn{7}{c}{auxiliary length} \\
    \midrule
    \multirow{8}{*}{} & 18 & 24.40\ebar{0.01} & 24.41\ebar{0.01} & 24.39\ebar{0.01} & N/A & N/A \\
        & 20 & 26.40\ebar{0.01} & 26.41\ebar{0.01} & 26.41\ebar{0.01} & 26.39\ebar{0.01} & N/A \\
        & 22 & 28.40\ebar{0.01} & 28.42\ebar{0.01} & 28.42\ebar{0.01} & 28.41\ebar{0.01} & 28.39\ebar{0.01} \\
        & 24 & 30.40\ebar{0.01} & 30.42\ebar{0.01} & 30.42\ebar{0.01} & 30.42\ebar{0.01} & 30.41\ebar{0.01} \\
        & 26 & 32.40\ebar{0.01} & 32.42\ebar{0.01} & 32.42\ebar{0.01} & 32.42\ebar{0.01} & 32.42\ebar{0.01} \\
        & 28 & 34.40\ebar{0.01} & 34.42\ebar{0.01} & 34.42\ebar{0.01} & 34.42\ebar{0.01} & 34.42\ebar{0.01} \\
        & 30 & 36.40\ebar{0.01} & 36.42\ebar{0.01} & 36.42\ebar{0.01} & 36.42\ebar{0.01} & 36.42\ebar{0.01} \\
        & 32 & 38.40\ebar{0.01} & 38.42\ebar{0.01} & 38.42\ebar{0.01} & 38.42\ebar{0.01} & 38.42\ebar{0.01} \\
\bottomrule
\end{tabular}
\end{table}

\end{document}