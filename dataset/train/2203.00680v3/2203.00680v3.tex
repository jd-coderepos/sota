\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}






\def\cvprPaperID{1420} \def\confName{CVPR}
\def\confYear{2022}

\begin{document}

\title{CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding}  

\maketitle
\thispagestyle{empty}
\appendix

\noindent We thank the reviewers for their positive feedback. \textit{``The idea is simple, and the experiments (including baseline comparison and ablation studies) show its effectiveness.'' \textbf{(R1)}. ``The idea of capturing the correspondence between 3D objects and 2D images is interesting. The motivation is clear.'' \textbf{(R2)}. ``This paper introduced cross-modal correspondence as a self-supervised learning strategy to facilitate an effective 3D point cloud understanding. The proposed model is novel and can generalize to arbitrary 3D point cloud tasks.'' \textbf{(R3)}.} Our code and pretrained models will be released publicly.

\vspace{2mm}
\noindent \underline{\textcolor{red}{\textbf{Reviewer \#1}}} \textcolor{blue}{\textbf{R1.1: Cross-modal learning:}} We thank the reviewer for this constructive comment. Kindly note that, we have exhaustively cited the similar ideas on cross-modal learning in the related works section. However, we will enhance it with discussing the cross-modal learning ideas applied in other modalities in our final manuscript. Contrastive learning in a cross-modal setting had been explored and shown effectiveness in other modalities as well. CLIP \cite{CLIP_reb} aims to learn a multi-modal embedding space by maximizing cosine similarity between image and text modalities. Similarly, Morgado \etal \cite{AVID_reb} combines audio and video modalities to perform a cross-modal agreement which then achieves significant gains in action recognition and sound recognition tasks. \emph{In contrast, CrossPoint combines both the intra-modal and cross-modal learning objectives to enhance the performance than the individual learning objectives (refer to Fig. 3 in the manuscript)}.\\ DepthContrast \cite{any_pointcloud_reb}, which utilizes point cloud and voxel modalities can be considered the most similar approach with the proposed method. It introduces a joint learning objective combining the within format and across format loss functions. \emph{In contrast, aligning with the objective of 3D understanding, our CMID is designed in such a way that the 2D image feature is encouraged to be embedded close to the corresponding 3D point cloud \textit{prototype} (refer to Eqn. 2 and Eqn. 3 in the manuscript).} Also we don't perform IMID on 2D image modality, where DepthContrast performs within format discrimination in both point cloud and voxel modalities, thus encouraging the learning objective towards 3D understanding. Further, as mentioned in Table. 1 in the manuscript, the proposed method outperforms DepthContrast by a significant margin of 5.8\%. 









\noindent \underline{\textcolor{red}{\textbf{Reviewer \#2}}} \textcolor{blue}{\textbf{R2.1: Cannot be directly applied to 3D scene understanding tasks:}} As we have explained in the limitations and future work section, we are currently exploring the possibility of extending our work towards 3D scene understanding. One possible direction is to utilize knowledge distillation technique to feed the knowledge of both 3D object and 3D scene understanding in a continual manner such that catastrophic forgetting is avoided. However, given the short time for rebuttal we are unable to perform extensive experiments on that.\\ \textcolor{blue}{\textbf{R2.2: Only one modality is used in downstream tasks:}} Kindly note that we have performed few-shot image classification on well-known CIFAR-FS dataset and reported the results in Table. 7. Using the image feature extractor trained using our proposed approach as the weight initializer outperforms the baseline performance by a considerable margin. However, as we discussed in the Limitations and Future Work section in the manuscript, investigating multi-modal understanding in a self-supervised setting is a promising future direction.\\
\textcolor{blue}{\textbf{R2.3: Analysis of the number of 2D images:}} As the reviewer mentioned, directly using multiple image features is an alternate way of analysing the contribution from multiple 2D images. We will perform the experiments and report the comparison in the final version of the manuscript.\\
\textcolor{blue}{\textbf{R2.4: Data augmentation strategies are used for the 3D point cloud:}} Kindly note that we have used data augmentation strategies for 3D point clouds such as rotation, scaling, translation, jittering, normalization and elastic distortion. Although we performed experiments with changing data augmentation setting, we couldn't identify significant variation in the performance.

\noindent \underline{\textcolor{red}{\textbf{Reviewer \#3}}} \textcolor{blue}{\textbf{R3.1: Missing References:}} We thank the reviewer for pointing out the missing references. We will discuss them in the related works section. Achituve \etal [r1] introduces self-supervised domain adaptaion in 3D point clouds with a novel pretext prediction task named deformation reconstruction. However, our proposed method captures learning signals from different modalities in a self-supervised fashion. [r1] performs experiments on the subsets of ModelNet and ShapeNet datasets. Even though it is not a fair comparison, by training on ShapeNet and testing on ModelNet, our model achieves 91.2\% classification accuracy while [r1] obtains 79.8\%. In the meantime Thabet \etal [r2] proposes a point sequence prediction mechanism showing effectiveness 3D segmentation tasks. In 3D object part segmentation task, [r2] obtains a mean IoU value of 81.4\% where as our proposed approach obtains 85.5\% outperforming [r2] by 4.1\%.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
