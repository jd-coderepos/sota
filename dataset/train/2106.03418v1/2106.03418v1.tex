\section{Experiments}
\label{experiment}
In this section, we describe the experiment setting and implementation details of the proposed CCL. Extensive ablation studies and comparison with other MTDA and STDA methods are also provided. We show that our method can work well on multiple large scale urban driving datasets.
\begin{table*}[t]
	\footnotesize
	\renewcommand\arraystretch{1.2}
	\setlength\tabcolsep{2.5pt}
	\caption{Performance comparison between our method and baseline models on adaptation from GTA5 to Cityscapes and IDD. The mIoU is calculated by the average of the intersection-over-union (IoU) among all 19 categories. "R" represents the ResNet101-based model and "V" represents the VGG16-based model. "C" and "I" indicate the target domain on Cityscapes and IDD, respectively. "*" represents the method with multiple models that are individually trained for each target domain.}
	\vspace{-3pt}
	\begin{center}
		\begin{tabular}{ l| c|c|c c c c c c c c c c c c c c c c c c c| c}
			\toprule
\multicolumn{23}{ c }{\bf GTA5 $\rightarrow$ Cityscapes \& IDD } \\
\hline	
Method &\rotatebox{90}{Model} &\rotatebox{90}{Target} &\rotatebox{90}{road} &\rotatebox{90}{sidewalk} &\rotatebox{90}{building} &\rotatebox{90}{wall} &\rotatebox{90}{fence} &\rotatebox{90}{pole} &\rotatebox{90}{light} &\rotatebox{90}{sign} &\rotatebox{90}{veg.} &\rotatebox{90}{terrain} &\rotatebox{90}{sky} &\rotatebox{90}{person} &\rotatebox{90}{rider} &\rotatebox{90}{car} &\rotatebox{90}{truck} &\rotatebox{90}{bus} &\rotatebox{90}{train} &\rotatebox{90}{motor} &\rotatebox{90}{bike} &{\bf{mIoU}} \\ 
			\hline
			\multirow{2}{*}{Individual Model*} &\multirow{2}{*}{V} & C
			&88.4  &30.8  &78.4  &29.8  &25.9  &20.5  &17.6  &11.2  &79.2  &30.3  &65.1  &46.6  &9.1  &81.2  &22.9  &29.9  &0.1  &11.9  &0.5  &35.8 \\
			
			&  &I &68.8  &2.5  &61.4  &29.2  &20.8  &24.9  &7.3  &34.3  &75.6  &29.3  &91.2  &39.8  &28.3  &63.6 &35.8  &38.8  &0  &39.2  &7.8  &36.8  \\
			\hline
			
			\multirow{2}{*}{Source only} &\multirow{2}{*}{V} & C
			&64.0  &16.8  &67.0  &22.6  &18.9  &22.1  &20.6  &13.3 &76.8  &14.8  &63.9  &47.9  &5.7  &72.5  &12.3  &12.9  &9.5  &19.1  &2.3  &30.7 \\
			
			&  &I &50.9  &2.3  &45.8  &21.8  &20.5  &26.8  &6.8  &39.6  &76.1  &28.3  &82.0  &38.6  &28.8  &69.2  &38.2  &16.6  &0  &49.1  &9.7  &34.3  \\
\hline		
\multirow{2}{*}{Data Combination} &\multirow{2}{*}{V}  & C
			&86.8  &16.1  &77.1  &27.8  &16.6  &22.1  &16.4  &6.1  &80.9  &30.9  &68.0  &43.2  &8.9  &80.7  &23.3  &15.2  &0  &11.0  &1.3  &33.3   \\
			
			&  &I &73.8  &3.5  &52.3  &25.8  &19.4  &24.6  &8.4  &32.0  &78.9  &32.2  &84.6  &38.6  &37.5  &73.1  &38.5  &12.9  &0  &41.3  &5.1  &35.9  \\
\hline	
			\multirow{2}{*}{Ours}  &\multirow{2}{*}{V}  & C
			&89.3  &33.6  &79.6  &26.8  &22.6  &25.9 &25.1  &17.7  &81.8  &32.9  &72.3  &49.4  &15.2  &82.0  &22.5  &16.9  &9.6  &10.7  &4.3  &\bf 37.8 \\
			
			&  &I &85.4  &5.8  &64.2  &31.8  &19.2  &24.9  &5.6  &43.2  &77.3  &35.04  &91.3  &43.9  &37.6  &70.1  &42.2  &27.5  &0  &46.9  &9.7  &\bf 40.1 \\	
\hline
			\hline
			\multirow{2}{*}{Individual Model*} &\multirow{2}{*}{R} & C
			&88.8  &23.8 &81.5 &27.7 &27.3 &31.7  &33.2 &22.9  &83.1 &27.0 &76.4  &58.5  &28.9  &84.3  &30.0  &36.8  &0.3  &27.7  &33.1  &43.3  \\
			
			&  &I &94.1  &24.4  &66.1  &31.3  &22.0  &25.4  &9.3  &26.7  &80.0  &31.4  &93.5 &48.7  &43.8  &71.4  &49.4  &28.5  &0 &48.7 &34.3 &43.6 \\
			\hline
			\multirow{2}{*}{Source only} &\multirow{2}{*}{R} & C
			&79.0  &9.2  &76.1  &15.7  &17.1  &23.3  &28.0  &14.8  &82.4  &22.9  &70.8  &53.7   &27.1  &76.6  &35.9  &5.4  &0.7  &20.3  &39.6  &36.8  \\
			
			& &I &60.5  &8.3  &50.8  &8.2  &18.9  &27.0  &6.2  &33.3  &67.6  &22.4  &87.4  &52.0  &45.8  &71.8  &43.9  &37.1  &0  &50.7  &20.2  &37.5  \\
\hline	
\multirow{2}{*}{Data Combination} &\multirow{2}{*}{R} & C
			&86.1  &32.0  &79.8  &24.3  &22.3  &28.5  &27.9  &14.3  &85.1  &29.8  &79.9  &56.1  &20.5  &77.7  &34.4  &35.2  &0.7  &18.2  &13.1  &40.3 \\
			
			&  &I &92.8  &23.4 &60.9  &25.8  &23.4  &24.1  &8.6  &32.2  &77.5  &26.8  &92.3 &48.0  &41.0  &74.4  &48.4  &17.7  &0 &52.5  &28.2 &42.0  \\
\cline{1-23}
			
			\multirow{2}{*}{Ours} &\multirow{2}{*}{R} & C
			&90.3  &34.0  &82.5  &26.2  &26.6  &33.6 &35.4  &21.5  &84.7  &39.8  &81.1  &58.4  &25.8  &84.5  &31.4  &45.4  &0  &29.9  &24.7  &\bf45.0 \\
			
			&  &I &95.0  &30.5  &65.6  &29.4  &23.4  &29.2  &12.0  &37.8  &77.3  &31.3  &91.9  &52.4  &48.3  &74.9  &50.1  &36.6  &0  &56.1  &32.4  &\bf46.0 \\
\hline	
\end{tabular}
		\label{table:gta2city+iDD}
	\end{center}
\end{table*}



\begin{table}[t]
	\footnotesize
\setlength\tabcolsep{3.5pt}
	\caption{Comparison of our model with SOTA UDA methods, DG methods and MTDA methods with ResNet-101 as backbone. The mIoU and mIoU* are evaluated over the 19 and 13 classes, respectively. "G", "S", "C" and "I" represent "GTA5", "SYNTHIA", "Cityscapes" and "IDD", respectively. ${\dagger}$ means the results of our implementation. All numbers correspond to the results without using pseudo labels or model ensembling as reported in the original papers.}
	\vspace{-3pt}
	\begin{center}
		\begin{tabular}{P{1cm} | C{2.2cm}  | C{0.9cm}  C{0.9cm} | C{0.9cm}   C{0.9cm} }
			\toprule
			\multirow{2}{*}{Setting} &\multirow{2}{*}{Method}  &\multicolumn{2}{c|}{mIoU} &\multicolumn{2}{c}{mIoU*} \\ 
& &G $\to$ C  &G $\to$ I &S $\to$ C &S $\to$ I  \\
			\midrule
			\multirow{9}{*}{STDA} &AdaptSeg~\cite{tsai2018learning} &42.4  &-  &46.7  &-      \\
			&CLAN~\cite{luo2019taking}                           &43.2  &-  &47.8  &-     \\
			&ADVENT~\cite{vu2019advent} 	                     &43.8  &-  &47.8  &-     \\
			&BDL~\cite{li2019bidirectional}                      &41.1  &-  &-  &-     \\
			&SIBAN~\cite{luo2019significance}                    &42.6  &-  &46.3  &-     \\
			&AdaptPatch~\cite{tsai2019domain}                    &44.9  &-   &-  &-    \\
			&MaxSquare~\cite{chen2019domain}	                 &44.3  &-  &45.8 &-    \\
			&Kim et al.~\cite{kim2020learning}                   &44.6 &-  &-  &-   			 \\
			&FDA~\cite{yang2020fda}                				 &44.6  &-  &-  &-    \\
&IntraDA~\cite{pan2020unsupervised}         		 &46.3  &-  &48.9  &-    \\
\midrule
			DG &Yue et al.$^{\dagger}$~\cite{yue2019domain} &42.1  &42.8  &44.3  &41.2   \\
			\midrule
			\multirow{3}{*}{MTDA}&MTDA-ITA$^{\dagger}$~\cite{gholami2020unsupervised}	&40.3  &41.2  &42.7  &39.4  \\
			&MT-MTDA$^{\dagger}$~\cite{nguyen2020unsupervised} 			 &43.2  &44.0  &45.2  &42.2   \\
&Ours 			  							 &45.0  &46.0  &48.1  &44.0   \\
\bottomrule
		\end{tabular}
		\label{table:syn2city+idd-sota}
	\end{center}
	\vspace{-5mm}
\end{table}



\begin{table}[t]
	\footnotesize
\setlength\tabcolsep{3.5pt}
	\caption{Ablation studies of the proposed CCL framework on GTA5 to Cityscapes and IDD with ResNet-101 as backbone. }
	\vspace{-3pt}
	\begin{center}
		\begin{tabular}{C{1.1cm}| C{1.1cm} C{1.1cm} C{1.1cm} |  C{1.1cm}  C{1.1cm} }
			\toprule
			Model \# &$\mathcal{L}_{cl}$ &$\mathcal{L}_{okd}$ &$\mathcal{L}_{wr}$ &C &I\\
			\midrule
			1& &  &  	&42.3  &42.9  \\
			\midrule
			2&	 &\cmark  & &41.8  &43.9  \\
			3&  & &\cmark 	&43.1  &43.5     \\
			4&  &\cmark  &\cmark 	&44.0  &44.7  \\
			\midrule
			5&\cmark  &\cmark  & 	&42.4  &45.2   \\
			6&\cmark  &  &\cmark 	&44.2  &44.9   \\
			7&\cmark  &\cmark  &\cmark 	&45.0  &46.0  \\
			\midrule
			\multicolumn{4}{c|}{Individual Model} 	&43.3  &43.6  \\
			\bottomrule
		\end{tabular}
		\label{table:ablation_CCL}
	\end{center}
	\vspace{-5mm}
\end{table}


\subsection{Datasets}




Under the MTDA experiment setting, synthetic datasets including GTA5~\cite{richter2016playing} and SYNTHIA~\cite{ros2016synthia} are used as source domain respectively, along with multiple real-world datasets Cityscapes~\cite{cordts2016cityscapes}, Indian Driving (IDD)~\cite{varma2019idd} and Mapillary~\cite{neuhold2017mapillary} as the target domains. The proposed CCL model is trained with labeled source data and unlabeled target data from various domains. Results on the validation sets of the datasets corresponding to the multiple target domains are used to evaluate its performance.

\textbf{GTA5} contains 24,966 synthetic images with a resolution of 1914$\times$1052 pixels that are collected from the video game GTA5 along with pixel-level annotations that are compatible with Cityscapes, IDD and Mapillary in 19 categories.

\textbf{SYNTHIA} is another synthetic dataset. The SYNTHIA-RAND-CITYSCAPES split of SYNTHIA, which contains 9,400 rendered images of 1280$\times$760 resolution, is used as another source domain. We use the 16 common categories with Cityscapes, IDD and Mapillary for training and 13 common classes for testing.

\textbf{Cityscapes} is a real-world dataset with 5,000 street scenes taken from European cities and labeled into 19 classes. We use 2,975 images for training and 500 validation images. 

\textbf{IDD} is a more diverse dataset than Cityscapes which captures unstructured traffic on Indiaâ€™s road. It contains a total of 10,003 images, with 6,993 images for training, 981 for validation and 2,029 for testing.

\textbf{Mapillary} provides 25,000 images collected from all around the world and diverse source of image capturing devices. It includes 18,000 images for training, 5,000 images for testing, and 2,000 images for validation. 

\begin{figure*}[t]
	\begin{center}
\includegraphics[width=1.0\linewidth]{img/sota.pdf}
	\end{center}
	\label{fig:target_semantic}
	\caption{Qualitative results for GTA5 to Cityscapes and IDD.}
	\vspace{-3mm}
	\label{figure:sota}
\end{figure*}















\subsection{Training Details}
Similar to~\cite{tsai2018learning} and~\cite{vu2019advent}, we use the DeepLab-v2~\cite{chen2017deeplab} model with ResNet-101~\cite{he2016deep} and VGG-16~\cite{simonyan2014very} as backbones and initialize them with models pre-trained on ImageNet~\cite{deng2009imagenet}. For the discriminator, we also adopt the same network architecture as~\cite{tsai2018learning,vu2019advent}. The semantic segmentation model parameters are optimized with SGD optimizer~\cite{bottou2010large} where the weight decay and momentum are set
to 0.9 and $5\times10^{-4}$, respectively. The learning rate is initially set to $2.5 \times 10^{-4}$. The polynomial procedure~\cite{chen2017deeplab} is used as the learning rate schedule. The discriminator is optimized with Adam optimizer~\cite{kingma2014adam} with the momentum $0.9$ and $0.99$ with the learning rate is set to $10^{-4}$. We set $\lambda_{adv}$, $\lambda_{cl}$, $\lambda_{okd}$ and $\lambda_{wr}$ as $10^{-3}$. Here, we adopt a simple way to conduct image translation in gamut of LAB color space~\cite{reinhard2001color}.

\subsection{Comparison with Baseline Models}
We compare the segmentation performance of the proposed CCL with three baselines: "Individual Model", "Source Only" and "Data Combination". "Individual Model", similar to~\cite{vu2019advent}, is to train multiple models for each corresponding target. "Source Only" and "Data Combination" are the MTDA setting which trains a single model across multiple target domains. "Source Only" is to train a model with the data only from source domain. "Data combination" is trained by directly combine data from multiple target domains as one domain. Here, we conduct the experiment with two target domains (\textit{i.e.,}~$M$=2), but our method can be easily extended to the case of more number of target domains. The results of each method are reported in Table~\ref{table:gta2city+iDD}. In~Table~\ref{table:gta2city+iDD}, the method of "Individual Model" that trains two models individually on Cityscapes and IDD achieves 43.3\% and 43.6\% mIoU on the corresponding domain. However, it requires two models for each domain. Compared to that, "Source only" use a single model but suffers considerable performance drops by 6.5\% and 6.1\% on Cityscapes and IDD because of the domain shift between the synthetic and real data. By directly combining the multiple target data as one domain, the model trained by "Data Combination" also suffers the performance degradation lagging behind the method of "Individual Model" by 3.0\% and 1.6\% mIoU on Cityscapes and IDD. Our method with a single model achieves 45.0\% and 46.0\% mIoU on Cityscapes and IDD, which significantly outperforms the "Data Combination" by +4.7\% and +4.0\%. By fully exploring unlabeled data from multiple target domains, the proposed CCL even works better than the "Individual Model", which adopts two models and trained on each target domain individually, by +1.7\% and +2.4\% mIoU on Cityscapes and IDD. The qualitative comparison between different baselines and the proposed CCL are provided in Figure~\ref{figure:sota}.


\subsection{Comparison with State-of-the-arts}
We first compare our method with the single-target domain adaptation (STDA) method on GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes with using ResNet-101 as backbone. The results are shown in Table~\ref{table:syn2city+idd-sota}. Our method performs favorably against state-of-the-art domain-specialized UDA methods on both GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes. However, it is noteworthy that with one round of training the proposed obtains a single model that achieves good performance on both Cityscapes and IDD. We also compare our method with DG and MTDA on "GTA5 to Cityscapes and IDD" and "SYNTHIA to Cityscapes and IDD". Compared to the method of DG, where the unlabeled data were not be used in~\cite{yue2019domain} during training. We surpass~\cite{yue2019domain} on both Cityscapes and IDD, respectively. We compare our method with two previous methods on MTDA. Since the previous works on MTDA only focus on the classification task, we carefully implement these methods in semantic segmentation with the same network. Compared to "MTDA-ITA", our method achieves significantly better performance on both domains. "MT-MTDA" is the method that adopts multiple teachers to alternatively teach a student in an offline knowledge distillation manner. However, the method also not consider to explore the information from different target domains. Our method achieves better performance than~\cite{nguyen2020unsupervised} on both Cityscapes and IDD.


\begin{table}[t]
	\footnotesize
\setlength\tabcolsep{12pt}
	\caption{Results of adapting GTA5 to different target domains with ResNet-101 as backbone. "C", "I" and "M" represent "Cityscapes", "IDD" and "Mapillary", respectively.}
\begin{center}
		\begin{tabular}{C{0.85cm} | C{0.16cm} C{0.16cm} C{0.16cm}| C{0.3cm}  C{0.3cm} C{0.3cm} }
			\toprule
			\multirow{2}{*}{Method} &\multicolumn{3}{c|}{Target} &\multicolumn{3}{c}{mIoU}\\
			&C &I &M  &C  &I &M\\
			\midrule
			\multirow{3}{*}{STDA}&\cmark & & &43.3 &- &- \\
			& &\cmark & &- &43.6 &- \\
			& & &\cmark &- &- &45.8 \\
			\midrule
			\multirow{4}{*}{MTDA}&\cmark &\cmark & &45.0 &46.0 &- \\
			&\cmark  & &\cmark  &45.1 &- &48.8 \\
			& &\cmark  &\cmark &- &44.5 &46.4 \\
			&\cmark &\cmark &\cmark &46.7 &47.0 &49.9 \\
			\bottomrule
		\end{tabular}
		\label{table:MTDA_syn}
	\end{center}
	\vspace{-5mm}
\end{table}

\subsection{Ablation Study}
In this section, we evaluate each component in the proposed CCL framework by conducting ablation studies on GTA5 to Cityscapes and IDD task with ResNet-101 as backbone. Results are shown in Table~\ref{table:ablation_CCL}.

We conduct a set of ablation study to examine the role of different components of the proposed method. A baseline (Model 1) here is designed as a method of directly applying adversarial loss to both target domains, \textit{i.e.,} $\lambda_{cl}=\lambda_{okd}=\lambda_{wr}=0$. When online knowledge distillation loss $\lambda_{okd}$ is switched on, Model 2 gains +1.0\% mIoU improvement on IDD but suffers from 0.5\% mIoU drops on Cityscapes. That could be explained by the confusion caused by the domain shift with expert models. When the weight regularization loss $\lambda_{wr}$ is switched on, Model 3 gains evident improvement of +0.8\% and +0.6\% mIoU than the baseline on Cityscapes and IDD. Using $\lambda_{okd}$ and $\lambda_{wr}$ simultaneously improve the Model 1 by 1.7\% and 1.8\% mIoU on Cityscapes and IDD, and also outperforms "Individual Model" in both target domains. Consistent improvement over Model 2, Model 3 and Model 4 is gained when collaborative consistency learning is employed. Specifically, Model 7 gains evident 1.0\% and 1.3\% improvement from Model 4 on Cityscapes and IDD, simultaneously.


\subsection{Generalization to Different Datasets}
\textbf{Synthetic-to-real MTDA.}
Here, we conduct a set of experiments with different target domains. We consider the task of STDA as our baseline, that includes: (1) GTA5 to Cityscapes, (2) GTA5 to IDD and (3) GTA5 to Mapillary. Each STDA model is trained on the corresponding target domain, individually. In Table~\ref{table:MTDA_syn}, three STDA baselines with three individually trained models achieve 43.3\%, 43.6\% and 45.8\% mIoU on Cityscapes, IDD and Mapillary, respectively. It can also be extended to adaptation to all these three datasets. Experiment results show that our method with a single model consistently works better than the STDA baseline, which is individually trained on the corresponding target domains. Our method using a single model consistently works better than the STDA baseline on the corresponding target domains. 


\textbf{Real-to-real MTDA.}
In Table~\ref{table:MTDA_real}, we also conduct a domain experiment from real-world datasets to real-world datasets. Here one of the Cityscapes, IDD and Mapillary is adopted as the source domain and the rest two are taken as the two target domains. Experimental results show that the proposed method not only works well on syn-to-real adaptation but also does a good job on the case of real-to-real.


\begin{table}[t]
	\footnotesize
\setlength\tabcolsep{12pt}
	\caption{Results for real-to-real MTDA experiments. }
	\vspace{2mm}
	\begin{center}
		\begin{tabular}{C{0.85cm} | C{0.16cm} C{0.16cm} C{0.16cm}| C{0.3cm}  C{0.3cm} C{0.3cm} }
			\toprule
			\multirow{2}{*}{Souce} &\multicolumn{3}{c|}{Target} &\multicolumn{3}{c}{mIoU}\\
			&C &I &M  &C  &I &M\\
			\midrule
			\multirow{3}{*}{C}& &\cmark &  &- &51.4 &- \\
			& & &\cmark &- &- &49.6 \\
			& &\cmark &\cmark &- &53.6  &51.4 \\
			\midrule
			\multirow{3}{*}{I}&\cmark & &  &46.5 &- &- \\
			& & &\cmark &- &- &49.0 \\
			&\cmark & &\cmark &46.8 &-  &49.8 \\
			\midrule
			\multirow{3}{*}{M}&\cmark & & &57.9 &- &- \\
			& &\cmark & &- &52.3 &- \\
			&\cmark &\cmark & &58.5 &54.1  &- \\
			\bottomrule
		\end{tabular}
		\label{table:MTDA_real}
	\end{center}
	\vspace{-5mm}
\end{table}
