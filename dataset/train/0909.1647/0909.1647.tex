\documentclass{llncs}
 
\usepackage{array}
\usepackage[dvips]{graphicx}
\usepackage{epic, eepic}
\usepackage{amssymb,amsmath,stmaryrd}
\usepackage{latexsym}
\usepackage{subfigure}
\usepackage[usenames]{color}
\usepackage{multirow}
\usepackage{gastex}
\usepackage{times}
\usepackage{subfigure}
\sloppy


\setlength{\textwidth}{14cm}
\setlength{\textheight}{19.2cm}
\setlength{\oddsidemargin}{1cm}
\setlength{\evensidemargin}{1cm}
\setlength{\topmargin}{1cm}





\let\emptyset\varnothing

\def\break{\penalty-1000}

\newcounter{compressEnum}
\renewcommand{\thecompressEnum}{}
\newenvironment{compressEnum}
{\setcounter{compressEnum}{0}}{}\newcommand{\itCompress}{\stepcounter{compressEnum}{(\thecompressEnum) }}

\def\frp#1{\ensuremath{\langle #1\rangle}}


\newenvironment{myProof}[1][\unskip]{\medskip\par\noindent{\bfseries Proof
    #1.}\ \ 
        \global\def\qed{\origQED\global\def\qed{}}\penalty10000}{\qed\par\medskip\global\def\qed{\origQED\global\def\qed{}}}

\def\endof{\leavevmode
  \parfillskip=0pt\widowpenalty=10000\displaywidowpenalty=10000\finalhyphendemerits=0\unskip\nobreak\null\hfil\penalty50\hskip2em\null\hfill }
\def\eodsymbol{\ensuremath\square}
\def\eopsymbol{\ensuremath\blacksquare}

\def\origEOD{\nobreak\leavevmode\endof\eodsymbol\par}
\def\EOD{\origEOD\global\def\EOD{}}
\def\origQED{{\nobreak\leavevmode\endof\eopsymbol\par\medskip}}
\def\qed{\origQED\global\def\qed{}}


\def\sg{\mathrel[\joinrel\mathrel[}
\def\sd{\mathrel]\joinrel\mathrel]}

\def\abs#1{\ensuremath{\lvert #1\rvert}}
\def\bigabs#1{\ensuremath{\big\lvert #1\big\rvert}}
\def\normeinf#1{\ensuremath{\lVert #1\rVert_{\infty}}}


\renewcommand{\P}{\mathbb P} 
\newcommand{\E}{\mathbb E} 

\newcommand{\nat}{\mathbb N} 
\newcommand{\rat}{{\mathbb Q}}
\newcommand{\sposrat}{{\mathbb Q}^{> 0}}
\newcommand{\posrat}{{\mathbb Q}^{\geq 0}}
\newcommand{\posreal}{{\mathbb R}^{\geq 0}}
\newcommand{\sposreal}{{\mathbb R}^{> 0}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\zed}{{\mathbb Z}}

\newcommand{\tuple}[1]{\langle #1 \rangle}
\newcommand{\sem}[1]{\sg \mathrel{#1} \sd}

\renewcommand{\l}{{\ell}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{{\mathcal{B}}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\G}{{\mathcal{G}}}
\newcommand{\K}{\mathcal{K}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\pre}{\mathsf{pre}}
\newcommand{\reach}{\mathsf{Reach}}
\newcommand{\Post}{\mathsf{Post}}
\newcommand{\dif}{\mathsf{dif}}

\newcommand{\Langf}{{\sf L_f}}
\newcommand{\Langb}{{\sf L_b}}
\newcommand{\Langc}{{\sf L_c}}

\newcommand{\Lf}{{\sf L}^{\sf f}}


\newcommand{\Last}{{\sf Last}}
\newcommand{\Inf}{{\sf Inf}}
\newcommand{\Run}{{\sf Run}}
\newcommand{\Path}{{\sf Path}}


\newcommand{\true}{{\sf true}}
\newcommand{\false}{{\sf false}}

\newcommand{\La}{{\sc La}}
\newcommand{\Li}{{\sc Li}}
\newcommand{\Ls}{{\sc Ls}}
\newcommand{\Di}{{\sc Di}}



\newcommand{\nmaxf}{{\sc NMax}}
\newcommand{\nmax}{{\sc NSup}}
\newcommand{\ndi}{{\sc NDisc}}
\newcommand{\nla}{{\sc NLimAvg}}
\newcommand{\nli}{{\sc NLimInf}}
\newcommand{\nls}{{\sc NLimSup}}
\newcommand{\nbw}{{\sc NBW}}
\newcommand{\ncw}{{\sc NCW}}



\newcommand{\dmaxf}{{\sc DMax}}
\newcommand{\dmax}{{\sc DSup}}
\newcommand{\ddi}{{\sc DDisc}}
\newcommand{\dla}{{\sc DLimAvg}}
\newcommand{\dli}{{\sc DLimInf}}
\newcommand{\dls}{{\sc DLimSup}}
\newcommand{\dbw}{{\sc DBW}}
\newcommand{\dcw}{{\sc DCW}}



\newcommand{\amaxf}{{\sc AMax}}
\newcommand{\amax}{{\sc ASup}}
\newcommand{\adi}{{\sc ADisc}}
\newcommand{\ala}{{\sc ALavg}}
\newcommand{\ali}{{\sc ALinf}}
\newcommand{\als}{{\sc ALsup}}
\newcommand{\abw}{{\sc ABW}}
\newcommand{\acw}{{\sc ACW}}



\newcommand{\umaxf}{{\sc UMax}}
\newcommand{\umax}{{\sc USup}}
\newcommand{\udi}{{\sc UDisc}}
\newcommand{\ula}{{\sc ULimAvg}}
\newcommand{\uli}{{\sc ULimInf}}
\newcommand{\uls}{{\sc ULimSup}}
\newcommand{\ubw}{{\sc UBW}}
\newcommand{\ucw}{{\sc UCW}}



\newcommand{\zmax}{{\sc PosSup}}
\newcommand{\zdi}{{\sc PosDisc}}
\newcommand{\zla}{{\sc PosLimAvg}}
\newcommand{\zli}{{\sc PosLimInf}}
\newcommand{\zls}{{\sc PosLimSup}}
\newcommand{\zbw}{{\sc PosBW}}
\newcommand{\zcw}{{\sc PosCW}}

\newcommand{\asmax}{{\sc AsSup}}
\newcommand{\asdi}{{\sc AsDisc}}
\newcommand{\asla}{{\sc AsLimAvg}}
\newcommand{\asli}{{\sc AsLimInf}}
\newcommand{\asls}{{\sc AsLimSup}}
\newcommand{\asbw}{{\sc AsBW}}
\newcommand{\ascw}{{\sc AsCW}}

\newcommand{\pemax}{{\sc PMSupW}}
\newcommand{\pedi}{{\sc PDisc}}
\newcommand{\pela}{{\sc PLavg}}
\newcommand{\peli}{{\sc PLinf}}
\newcommand{\pels}{{\sc PLsup}}

\newcommand{\pomax}{{\sc PSupW}}
\newcommand{\podi}{{\sc PDisc}}
\newcommand{\pola}{{\sc PLavg}}
\newcommand{\poli}{{\sc PLinf}}
\newcommand{\pols}{{\sc PLsup}}

\newcommand{\pzmax}{{\sc PSupW}}
\newcommand{\pzdi}{{\sc PDiW}}
\newcommand{\pzla}{{\sc PLaW}}
\newcommand{\pzli}{{\sc PLiW}}
\newcommand{\pzls}{{\sc PLsW}}

\newcommand{\ptmax}[1]{{\sc P}{\sc SupW}}
\newcommand{\ptdi}[1]{{\sc PDiW}}
\newcommand{\ptla}[1]{{\sc PLaW}}
\newcommand{\ptli}[1]{{\sc PLiW}}
\newcommand{\ptls}[1]{{\sc PLsW}}

\renewcommand{\ptls}[1]{#1}


\newcommand{\ndcw}{{\sc 
\raisebox{2.0pt}{\scalebox{0.45}{\begin{tabular}{c}D\-3pt]N\end{tabular}}}Linf
}}
\newcommand{\ndmax}{{\sc 
\raisebox{2.0pt}{\scalebox{0.45}{\begin{tabular}{c}D\-3pt]N\end{tabular}}}
}}
\newcommand{\anbw}{{\sc 
\raisebox{2.0pt}{\scalebox{0.45}{\begin{tabular}{c}A\-3pt]N\end{tabular}}}Lsup
}}
\newcommand{\an}{\hspace{-2pt}{\sc 
\raisebox{2.0pt}{\scalebox{0.45}{\begin{tabular}{c}N\
(1-\epsilon)\cdot(l-l\cdot\epsilon) - \epsilon \cdot \beta\cdot (j+1) 
\geq \frac{l}{2} - \frac{l}{10} 
=\frac{2l}{5} 

L_z=\set{a^{k_1} b a^{k_2} b a^{k_3} b \ldots \mid k_1, k_2, \cdots \in \nat_{\geq 1} \cdot
\prod_{i=1}^\infty (1 -\frac{1}{2^{k_i}}) >0} \cup (a \cup b)^* \cdot a^\omega;

L_{\lambda} =\set{a^{k_1} b a^{k_2} b a^{k_3} b \ldots \mid k_1, k_2, \cdots \in \nat_{\geq 1}. 
\prod_{i=1}^\infty (1 -\lambda^{k_i}) >0}.
-8pt]
\cline{1-6}\cline{8-9}
\multirow{5}{*}{\rotatebox{90}{{\scriptsize almost-sure}\ }} & \asmax &  \ok & \ok  & \ko   & \ok & &   \ok &  \ok \\
\cline{2-6}\cline{8-9}
& \asls  &  \ok & \ok  & \ko   & \ok & &  \ok  &  \ok\\
\cline{2-6}\cline{8-9}
& \asli  &  \ok & \ok  & \ok   & \ok & &  \ko  &  \ko\\
\cline{2-6}\cline{8-9}
& \asla  &  \ko & \ok  & \ko   & \ko & &  ?     & ? \\
\cline{2-6}\cline{8-9}
& \asdi  &  \ko & \ok  & \ko   & \ok & &  ? (1)  &  \ok \\
\cline{1-6}\cline{8-9}
\multicolumn{9}{c}{{\large \strut}\parbox[t]{82mm}{The universality problem for \ndi\/ can be reduced to~(1). It is not known whether this problem is decidable.}}
\end{tabular}
\end{center}
\caption{Closure properties and decidability of the emptiness and universality problems.\label{tab:closure-properties}}
\end{table}


\subsection{Closure under  and }

\begin{lemma}[Closure by initial non-determinism]\label{lem:closure-max-one}
\zls, \zli\/ and \zla\/ is closed under ; and 
\asls, \asli\/ and \asla\/ is closed under .
\end{lemma}
\begin{myProof}
Given two automata  and  consider the automata  
obtained by initial non-deterministic choice of  and .
Formally, let  and  be the initial states of  and 
, respectively, then in  we add an initial state  and
the transition from  is as follows:
for , consider the set .
From , for input letter , the successors are from  
each with probability .
If  and  are \zls\/ (resp. \zli, \zla), then 
 is a \zls\/ (resp. \zli, \zla) such that .
Similarly, if  and  are \asls\/ (resp. \asli, \asla), then 
 is a \asls\/ (resp. \asli, \asla) such that .
\qed
\end{myProof}

\begin{lemma}[Closure by synchronized product]\label{lem:closure-max-two}
\asls\/ is closed under  and 
\zli\/ is closed under .
\end{lemma}
\begin{myProof}
We present the proof that \asls\/ is closed under . 
Let  and  be two probabilistic weighted automata with 
weight function  and , respectively.
Let  be the usual synchronized product of  and  with 
weight function  such that 
.
Given a path  in  we denote by
 the path in  that is the projection of the first component 
of  and we use similar notation for .
Consider a word , let . 
We consider the following two cases to show that .
\begin{enumerate}

\item W.l.o.g. let the maximum be achieved by , i.e., . 
Let  be the set of states  in  such that weight of  
is at least .
Since , given the word , in  the event  holds 
with probability~1.
Consider the following set of paths in 

Since given , the event  holds with probability~1 in ,
it follows that given , the event  holds with probability~1 in .
The  function ensures that every path  visits weights 
of value at least  infinitely often.
Hence .

\item Consider a weight value . 
Let  be the set of states  in  such that the weight of 
 is less than .
Given the word , since , it follows that 
probability of the event  in , given the word , 
is positive.
Hence given the word , the probability of the event 
 is positive in .
It follows that .
\end{enumerate}
The result follows. 
If  and  are \zli, and in  we assign weights 
such that every state in  has the minimum weight of its 
component states, and we consider  as a \zli, then 
.
The proof is similar to the result for \asls.
\qed
\end{myProof}



\begin{comment}
\begin{lemma}
\asli\/ is closed under .
\end{lemma}
\begin{myProof} 
{\bf This proof needs to be made precise and include the write-up part of Laurent.}
Let  and  be two probabilistic weighted automata with 
weight function  and , respectively.
Let  be the usual synchronized product of  and  with 
weight function  such that 
.

If  and  are \asli, then we show that  is 
an \asli\/ that is equivalent to the  of  and .
Consider a word , and let . We show that
 by considering the following two cases.

\begin{enumerate}
\item WLOG we assume that . Let  be the set of states in 
 with weight at least . 
Then given  in  we have , i.e., 
in  eventually always weights of value at least  is visited.
Fix , then there exists  such that given  in  
after  steps only states with weight at least  is visited with 
probability at least .
Hence given  in  with probability at least  
after  steps only states  are visited such that 
 (hence ).
Since this holds for all , it follows that given  
in  states of weight at least  is visited eventually always 
with probability~1. Hence .

\end{enumerate}

It follows from the results of~\cite{BG08}  that \zcw\/ is reducible to \ascw. 
Then we follow the product construction for different copies (follow the construction in the write-up of Laurent
and include it here). 
The correctness argument for the construction is as follows: given a word  if a \ascw\/ accepts it, 
then it follows that  probability that after -th position the automaton 
visit only coBuchi states  is 1
(as it eventually always visits only coBuchi states almost-surely).
Hence for every , there exists some n such that the
probability that  after -th position the automaton visit only coBuchi states is at least  .
Consider a word  and let us consider the product construction. 
Suppose in the given \zli\/ the value is some . 
Then all copies of the \ascw\/ coBuchi automata with value up to  accepts  with probability~1.
Hence for every  we can choose a  such that for all the \ascw\/ coBuchi automata 
copies up to  only visits the respective coBuchi states after -steps with probability at least 
. 
Hence for all , there exists a  such that the automata constructed 
visits only states of rewards at least  after  steps with probability 
at least . 
As  is arbitrary and  is bounded by the number of rewards, 
it follows that the value of the word for the automata 
constructed is at least . 
We now show that the value is not more than : 
since the value in the original automata is  it follows that for any value 
greater than , 
the coBuchi automata for  will reject the word  
with positive probability. 
Clearly, in the construction the coBuchi states of  
are visited eventually always with probability less than 1. 
Hence the value is smaller than . 
This concludes the proof. 
\qed
\end{myProof}	
\end{comment}

\begin{lemma}\label{lemm-closed-asliw-zlsw}
\zls\/ is closed under  and \asli\/ is 
closed under .
\end{lemma}
\begin{myProof} 
Let   and  be two \zls. We construct a \zls\/  
such that . Let  
be the set of weights that appear in  (for ), and let 
 and let  be the least value in .
For each weight , 
consider the \zbw\/  that is obtained from  by considering 
all states with weight at least  as accepting states.
Since \zbw\/ is closed under intersection(by the results of~\cite{BG05}), 
we can construct a \zbw\/  that is the intersection of  and 
, i.e. .
We construct a \zls\/  from  by assigning weights 
 to the accepting states of  and the minimum weight 
 to all other states.
Consider a word , and we consider the following cases.
\begin{enumerate}
\item If , then for all  such that 
 we have , (i.e., the \zbw\/  
accepts ).

\item If , then  and , 
i.e., .
\end{enumerate}
It follows from above that 
.
Since \zls\/ is closed under  (by initial non-determinism), 
it follows that \zls\/ is closed under .
The proof of closure of \asli\/ under  is similar.
\qed
\end{myProof}

\noindent The closure properties of -automata in the positive semantics rely on the following lemma.


\begin{lemma}\label{lem:limavg-min-comp}
Consider the alphabet , and consider the languages
 and  that assigns the long-run average number 
of 's and 's, respectively.
Then the following assertions hold.
\begin{enumerate}
\item There is no \zla\/ for the language .
\item There is no \zla\/ for the language .
\end{enumerate}
\end{lemma}

\begin{myProof}
To obtain a contradiction, assume that there exists a \zla\/ 
 (for either  or ). 
We first claim that if we consider the -Markov or the -Markov chain of , 
then there must be either an -closed recurrent set or a 
-closed recurrent set  that is reachable in  
such that the expected sum of the weights in  is positive.
Otherwise, if for all -closed recurrent sets and -closed recurrent sets we have that 
the expected sum of the weights is zero or negative, then we fool the automaton
as follows.
By Lemma~\ref{lemm-long-enough-b}, it follows that there exists a  such that 
, however, ,
i.e., we have a contradiction.
W.l.o.g., we assume that there is an -closed recurrent set  such that expected 
sum of weights of  is positive.
Then we present the following word : a finite word  to reach 
the cycle , followed by ; the answer of the automaton is positive, 
{\it i.e.}, ,  while .
Hence the result follows.
\end{myProof}


\begin{lemma}\label{lem:zla-min--asla-max}
\zla\/ is not closed under  and \asla\/ is not closed under .
\end{lemma}
\begin{myProof}
The result for \zla\/ follows from Lemma~\ref{lem:limavg-min-comp}.
We now show that \asla\/ is not closed under .
Consider the alphabet  and the quantitative languages
 and  that assign the value of long-run average
number of 's and 's, respectively.
There exists \dla\/ (and hence \asla) for  and .
We show that  cannot be expressed by 
an \asla\/. By contradiction, assume that  is an \asla\/
with set of states  that defines .
Consider any -closed recurrent  in . 
The expected limit-average of the weights 
of the recurrent set must be~1, as if we consider the 
word  where 
is a finite word to reach , the value
of  in  is . 
Hence, the limit-average of the weights of all the reachable 
-closed recurrent set  in  is~1.

Given , there exists  such that the following 
properties hold:
\begin{enumerate}
\item from any state of , given the word  with 
probability  an -closed recurrent set is reached 
(by property~1 for Markov chains);

\item once an  -closed recurrent set is reached, given the word 
, (as a consequence of property~2 for Markov chains)
we can show that the following properties hold:
(a)~the expected average of the weights is at least , 
and (b)~the probability distribution of the states is with  of 
the probability distribution of the states for the word 
 (this holds as the probability distribution 
of states on words  
converges to the probability distribution of states on the word ).

\end{enumerate}
\newcommand{\wh}{\widehat}
Let  be a number that is greater than the absolute maximum value 
of weights in .
We chose  such that .
Let  (such that  satisfies the 
properties above).
Consider the word  and the answer by  must be
, as .
Consider the word  and consider a closed
recurrent set in the Markov chain obtain from  on . 
We obtain the following lower bound on the expected limit-average of the 
weights: 
(a)~with probability at least , after  steps, -closed
recurrent sets are reached;  
(b)~the expected average of the weights for 
the segment between  and  is at least ;
and (c)~the difference in probability distribution of the states after 
 and  is at most .
Since the limit-average of the weights of  is 
, the lower bound on the limit-average of the weights is as 
follows
1ex] 
& \geq & \frac{4}{5} - \epsilon - 3\cdot \epsilon \cdot \beta \1ex]
& \geq & \frac{4}{5} -\frac{1}{10} \
It follows that .
This contradicts that  expresses . 
\end{myProof}








\subsection{Closure under complement}




\begin{lemma}\label{lem:asli-zls-complement}
\zls\/ and \asli\/ are closed under complement.
\end{lemma}
\begin{myProof}
We first present the proof for \zls. 
Let   be a \zls, and let  be the set of weights that appear in .
For each , it is easy to construct a \zbw\/  whose (boolean) language 
is the set of words  such that , by declaring to be accepting
the states with weight at least .
We then construct for each  a \zbw\/  (with accepting states) 
that accepts the (boolean) complement of the language accepted by  (such 
a \zbw\/ can be constructed since \zbw\/ is closed under complementation by the
results of~\cite{BG08}).
Finally, assuming that  with ,
we construct the \zls\/  for  where  is obtained
from  by assigning weight  to each accepting states,
and  to all the other states. The complement of  is then
 which is accepted by a \zls\/ 
(since \zls\/ is closed under ).
The result for \asli\/ is similar and it uses the closure of 
\ascw\/ under complementation which can be easily proved from the 
closure under complementation of \zbw.
\end{myProof}


\begin{lemma}\label{lem:asls-zli-complement}
\asls\/ and \zli\/ are not closed under complement.
\end{lemma}

\begin{proof}
It follows from Lemma~\ref{lemm-nzlstoasls}  that the language  of finitely 
many 's is not expressible by an \asls, whereas the complement  of 
infinitely many 's is expressible as a \dbw\/ and hence as a \asls.
It follows from Lemma~\ref{lemm-liminf-asvz} that language  is not expressible as an 
\zli, whereas its complement  is expressible by a \dcw\/ and hence 
a \zli.
\qed
\end{proof}

\begin{lemma}
\zla\/ and \asla\/ are not closed under complement.
\end{lemma}
\begin{myProof}
The fact that \zla\/ is not closed under complement follows from 
Lemma~\ref{lem:limavg-min-comp}.
We now show that \asla\/ is not closed under complement.
Consider the \dla\/  over alphabet  that consists of 
a single self-loop state with weight  for  and  for .
Notice that  and  for all 
.
To obtain a contradiction, assume that there exists a \asla\/  
such that . 
For all finite words , let  be the expected average weight 
of the finite run of  over .
Fix . For all finite words , there exists
a number  such that the average number of 's in  is at most 
, 
and there exists a number  such that 
(since ). 
Hence, we can construct a word  
such that 
 and .
Since , this implies that , a contradiction.
\qed
\end{myProof}



\subsection{Closure under sum}



\begin{lemma}\label{lemm:zls-asls-closed-under-sum}
\zls\/ and \asls\/ are closed under sum.
\end{lemma}
\begin{myProof}
Given two \zls\/ (resp. \asls)  and , 
we construct a \zls\/ (resp. \asls)  for the sum of their 
languages as follows. 
For a pair  of weights ( in , for ),
consider a copy of the synchronized product of  and . 
We attach a bit  whose range is  to each state to 
remember that we expect  to visit the guessed weight . 
Whenever this occurs, the bit  is set to , and the weight of the state 
is . All other states ({\it i.e.}
when  is unchanged) have weight 
.
Let the automata constructed be .
Then .
Since \zls\/ (resp. \asls) is closed under  the result follows. 
\end{myProof}




\begin{lemma}\label{lemm:zli-asli-closed-under-sum}
\zli\/ and \asli\/ are closed under sum.
\end{lemma}
\begin{myProof}
Given two \zli\/ (resp. \asli)  and , we construct a \zli\/
(resp. \asli)  for the sum of their languages as follows. 
For , let  be the set of weights that appear in .
Let .
For  and , for , 
consider the \zcw\/ (resp. \ascw)  obtained from  by 
making all states with weights at least  as accepting states.
Let  be the \zcw\/ (resp. \ascw) such that 
: such an \zcw\/ 
(resp. \ascw) exists since \zcw\/ (resp. \ascw) 
is closed under intersection. 
In other words, for a word  we have 
 iff  and .
Let  be the \zli\/ (resp. \asli) obtained 
from  by assigning weight  to all accepting 
states and weight  to all other states.
Then the automaton for the sum of  and  (denoted as ) 
is . 
Since \zli\/ (resp. \asli) is closed under  the result follows. 
\end{myProof}

\begin{lemma}
\asla\/ is not closed under sum.
\end{lemma}

\begin{myProof}
Consider the alphabet , and consider the \dla-definable languages 
 and  that assigns to each word  the long-run average number of 's 
and 's in  respectively. 
Let . 
We show that  is not expressible by \asla.
Assume towards contradiction that  is defined by an \asla\/  with set 
of states  (we assume w.l.o.g that every state in  is reachable).
Let  be greater than the maximum absolute value of the weights in .

First, we claim that from every state , if we consider the 
automaton  with  as starting state then :
this follows since if we consider a finite word  to reach 
, then  and hence .
It follows that from any state , as  tends to ,
the expected average of the weights converges almost-surely to~1.
This implies if we consider the -Markov chain arising from , then
from any state , for all closed recurrent set  of states reachable
from , the expected average of the weights of  is~1.
Hence for every  there exists a natural number  
such that from any state , for all   
given the word  the expected average of the weights is at least 
with probability  (this is because we can chose long enough  
such that the closed recurrent states are reached with probability  
by property~1 for Markov chains, and then the long enough sequence ensures that 
the expected average approaches~1 by property~2 for Markov chains), and for 
the first  steps the expected average of the weights is at least .
The same result holds if we consider as input a sequence of 's instead of 's.

Consider the word  generated inductively by the following procedure: 
(a)~ is the empty word; 
(b)~we generate  from  as follows:
\begin{compressEnum}
\itCompress the sequence of letters added to  to obtain  is at 
least ;
\itCompress first we  generate a long enough sequence  of 's 
after  such that the average number of 's in  falls below ;
\itCompress then generate a long enough sequence  of 's such that the average number of 
's in  falls below ;
\itCompress the word .
\end{compressEnum}
The word  is the limit of these sequences.
For , consider  (where  
satisfies the properties described above for ). 
By construction for , the length of  is at least 
, 
and hence it follows that in the segment constructed between 
 and , for all  with probability 
at least  the expected average of the weights is at least 

Hence for all , the expected average of the weights is 
at least  with probability at least .
Since this holds for all , it follows that the expected
average of the weights is at least  almost-surely, 
(i.e., ).
We have  and thus , while
.
Thus we have a contradiction.
\end{myProof}

\begin{lemma}\label{lemm:zdi-asdi-closed-under-sum}
\zdi\/ and \asdi\/ are closed under sum.
\end{lemma}

\begin{myProof}
The result for \zdi\/ follows from Theorem~\ref{theo:ndi-zdi}
and the fact that \ndi\/ and \udi\/ are closed under sum (which is easy to prove using 
a synchronized product of automata where the weight of a joint transition is the sum
of the weights of the corresponding transitions.
\end{myProof}

\noindent{\bf Open question.} Whether \zla\/ is closed under sum remains open.



\section{Decision Problems for Probabilistic Weighted Automata}

We conclude the paper with some decidability and undecidability results for classical decision problems
about quantitative languages (see Table~\ref{tab:closure-properties}). 
Most of them are direct corollaries of the results in~\cite{BG08}.
Given a weighted automaton  and a rational number , 
the \emph{quantitative emptiness problem} asks whether there exists a word 
such that , and the \emph{quantitative universality problem}
asks whether  for all words .





\begin{theorem}
The emptiness and universality problems for \zmax\/ and \asmax\/ are decidable.
\end{theorem}

\begin{proof}
By Theorem~\ref{theo:zmax-asmax-to-dmax}, these problems reduce to emptiness
of \dmax\/ which is decidable (\cite[Theorem~1]{CDH08a}).
\end{proof}



The following theorems are trivial corollaries of~\cite[Theorem~2]{BG08}.

\begin{theorem}
The emptiness problem for \zls\/ and the universality problem for \asls\/ are undecidable.
\end{theorem}

It is easy to obtain the following result as a straightforward generalization
of~\cite[Theorem~6]{BG08}.

\begin{theorem}
The emptiness problem for \asls\/ and the universality problem for \zli\/ are decidable.
\end{theorem}



\begin{theorem}
The emptiness problem for \zli\/ and the universality problem for \asls\/ are decidable.
\end{theorem}

\begin{myProof}[(Sketch)]
We sketch the main ideas of the proof that emptiness of  automata
in positive semantics is achievable in EXPTIME and with exponential memory. 
The proof extends easily to \zli\/ and to the universality problem for \asls.

Emptiness of  automata in positive semantics can be viewed as 
deciding the existence of a blind positive-winning strategy in a stochastic 
game with  objective. 
It follows from the results of~\cite{CDH-POMDP} that this problem can be decomposed
into positive winning for safety and reachability objectives.
\begin{comment}
\medskip\noindent{\bf Positive reachability.} For positive winning for 
reachability we have the following:
\begin{enumerate}

\item if there is a blind winning strategy for positive reachability,
then the memoryless strategy that plays all actions uniformly is a 
positive winning strategy for reachability;

\item from the randomized strategy there is a deterministic strategy
that is positive winning (since there is a finite path and we can select 
the letters of the path).

\end{enumerate} 

\medskip\noindent{\bf Positive safety.} We first assume that finite-state 
randomized strategy exists for positive safety. We then show that there is a 
finite-state deterministic strategy. If we fix a finite-state randomized 
strategy that is positive winning, then in the Markov chain obtained there 
must be a closed recurrent set that is subset of the safe set. If we restrict 
the strategy to a deterministic strategy obtained from the restriction of the 
randomized strategy, and the closed recurrent set will be a subset of the 
original closed recurrent set, and hence we would obtain a deterministic 
strategy for positive safety.


\medskip\noindent{\bf From positive reachability and safety to positive coB\"uchi.}
We present an iterative algorithm. Let  be the set of coB\"uchi states.
Let . We obtain  from  as follows: 
let  be the set of states such that player~1 can ensure staying safe in 
 with positive probability, and  is obtained as the 
set of states that can reach  with positive probability. 
Clearly player~1 can ensure from all  that coB\"uchi objective is satisfied
with positive probability.
Let  be the fixpoint. 
From every state in the complement of  player~1 cannot ensure positive 
probability to stay safe in , and hence for every player~1 strategy 
player~2 can ensure to reach  with probability~1.
From every state in the complement of  player~1 cannot ensure positive probability
to reach to , and hence against every player~1 strategy,  player~2 can ensure 
to stay safe in . 
Hence given a strategy for player~1, the player~2 strategy to reach 
 with probability~1 and stay safe in  ensure that the 
from  the coB\"uchi condition is falsified with probability~1.


\medskip\noindent{\bf Positive Safety Finite-memory.} 
Consider the knowledge based construction~\cite{CDHR07} where we consider essentially the probability 
support (instead of the precise probability). If player~1 can ensure positive probability
safety with knowledge based strategy, then clearly that is a positive winning strategy for 
safety. 
If there is no knowledge based strategy for positive safety, then for any observation 
based strategy, the target set is reached in  steps with probability  
(this holds for all deterministic strategies of length , and hence for all 
probabilistic choices as well). 
It follows that the target is reached with probability~1. 
\end{comment}
\end{myProof}

The following result is a particular case of~\cite[Corollary~3]{BG08}.

\begin{theorem}
The emptiness problem for \asli\/ and the universality problem for \zls\/ are undecidable.
\end{theorem}



Finally, by Theorem~\ref{theo:ndi-zdi} and the decidability of emptiness for \ndi,
we get the following result.

\begin{theorem}
The emptiness problem for \zdi\/ and the universality problem for \asdi\/ are decidable.
\end{theorem}


Note that by Theorem~\ref{theo:ndi-zdi}, the universality problem for \ndi\/ (which is not know to be decidable) 
can be reduced to the universality problem for \zdi\/ and to the emptiness problem for \asdi.



\bibliography{main}
\bibliographystyle{plain}
\end{document} 
