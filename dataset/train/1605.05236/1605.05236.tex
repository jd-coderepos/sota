


















\documentclass{sig-alternate-05-2015}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\maketitle}{\@copyrightspace}{}{}{}
\makeatother


\newcommand{\edit}[1]{{\color{red}\textbf{#1}}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\si}{\mathbf{S}}
\newcommand{\E}{\mathbf{E}}

\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\newtheoremstyle{slanted}{3pt}{3pt}{\slshape}{}{\bfseries}{.}{.5em}{}\theoremstyle{slanted}
\newtheorem{thm}{Theorem}[section]
\newtheorem{defn}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{clm}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}



\newcommand{\subparagraph}{}
\usepackage[raggedright]{titlesec}

\begin{document}














\title{Fast Concurrent Cuckoo Kick-out Eviction Schemes for High-Density Tables}
\subtitle{}


\numberofauthors{1} \author{
\alignauthor
William Kuszmaul\\
       \affaddr{Stanford University}\\
       \affaddr{450 Serra Mall}\\
       \affaddr{Stanford, CA 94305}\\
       \email{kuszmaul@stanford.edu}
}
\date{5 February 2016}


\maketitle
\begin{abstract}
  
  Cuckoo hashing guarantees constant-time lookups regardless of table
  density, making it a viable candidate for high-density
  tables. Cuckoo hashing insertions perform poorly at high table
  densities, however. In this paper, we mitigate this problem through
  the introduction of novel kick-out eviction
  algorithms. Experimentally, our algorithms reduce the number of bins
  viewed per insertion for high-density tables by as much as a factor
  of ten.

  We also introduce an optimistic concurrency scheme for transactional
  multi-writer cuckoo hash tables (not using hardware transactional
  memory). For delete-light workloads, one of our kick-out algorithms
  avoids all competition between insertions with high probability, and
  significantly reduces transaction-abort frequency. This result is
  extended to arbitrary workloads using a new synchronization
  mechanism called a claim flag.
\end{abstract}


\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10002952.10003190.10003193.10003427</concept_id>
<concept_desc>Information systems~Data locking</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10002952.10003190.10003195.10010836</concept_id>
<concept_desc>Information systems~Key-value stores</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Information systems~Data locking}
\ccsdesc[300]{Information systems~Key-value stores}



\printccsdesc



\keywords{Cuckoo hashing; kick-out victim; transactional correctness;
  serializability; transaction abort; optimistic concurrency}

\clearpage
\clearpage


\section{Introduction}


Unlike traditional hashing, Cuckoo hashing maps each key to two
distinct bins using two hash functions.  To insert a key, we simply
look through the bins identified by the two hash functions and insert
into the first containing a free slot. However, if neither bin
contains an empty slot, then we pick one of two bins and
\emph{kick-out} one of the keys it contains. The displaced key, the
\emph{victim}, is then recursively reinserted into the table. The
sequence of kick-outs that results from the initial insert is called a
\emph{kick-out chain}.

Introduced in 2004, Cuckoo hashing guarantees constant-time reads,
overwrites, and deletes \cite{cuckoo04}. In particular, once a key is
inserted into the table, it is guaranteed to be among the two bins to
which it is hashed -- regardless of the table density.  In settings
where memory is expensive this makes Cuckoo hashing especially
valuable. See, for example, the flash-based key-value store system
FlashStore \cite{flash10}.


High performance implementations of cuckoo hashing in both serial
\cite{ross07} and parallel \cite{fan13, li14} have proven cuckoo
hashing to have practical potential. Typically, designs choose for
each bin to contain either four or eight slots.

Unfortunately, whereas cuckoo hashing reads are constant time
regardless of table-density, inserts become very slow for high-density
tables. The kick-out chain resulting from an insertion in a
high-density table results in the viewing of dozens (or hundreds) of
bins, making cache-friendly or multi-threaded cuckoo hashing difficult
\cite{li14}. Commonly kick-out victims are selected in one of two
manners: \emph{random kicking} selects the victim randomly; and
\emph{breadth-first search} performs a search to find the shortest
possible kick-out chain. Experimentally, at high table densities both
kick-out eviction algorithms are equally bad in terms of bins viewed
per insertion (Figure~\ref{fig_A}).


This short paper demonstrates that simple algorithmic changes to both
breadth-first search and random kicking can yield significant
improvements. We introduce five mechanisms for reducing bins viewed
per insertion: ghost insertions, sorted search, queue-kicking, and
rattle-kicking.

\begin{itemize}
\item \textbf{Ghost insertions: }Ghost insertions allow a record to
  reside in two bins at once, marked as a duplicate in each (Section
 ~\ref{secghost}). Duplicates make good kick-out victims because they
  are guaranteed not to cause additional kick-outs.

\item \textbf{Sorted Search: }Whereas breadth-first search essentially
  maintains a queue of records whose children have not yet been
  examined, sorted search instead maintains a list sorted by a
  statistic called \emph{spawn count} (Section
 ~\ref{sec_sorted_search}); records with small spawn counts are more
  likely to lead to small kick-out chains.

\item \textbf{Queue-Kicking:} To select kick-out victims,
  queue-kicking picks the record present in the bin for the longest
  (Section~\ref{sec_queue_walk}). For delete-light workloads, by
  preemptively updating the queue, queue-kicking can be further
  harnessed to prevent concurrently planned kick-out chains from
  overlapping.

\item \textbf{Rattle-Kicking: }In -ary cuckoo hashing, records are
  hashed to  single-slot bins \cite{dary}. Rattle-kicking ensures
  that each of a record's hash functions are used once before any of them
  are used a second time (Section~\ref{sec_dary_walk}).
\end{itemize}

Although we sometimes are able to provide theoretical justification
for our results, our primary method of evaluation is experimental. At
high table densities (using bins of size four) we obtain a factor of
ten improvement by combining sorted search and ghost insertions. At
high table densities for -ary cuckoo hashing, rattle-kicking,
sorted search, and an algorithm of Khosla \cite{khosla13} each perform
approximately three times better than standard kick-out schemes.

In Section~\ref{secconcurrent}, we implement a transactional
multi-writer cuckoo hash table (without using hardware transactional
memory). For delete-light workloads, with 15 threads concurrently
running 100-operation transactions, queue-kicking reduces the
frequency of transaction aborts by a factor of around 256 in
low-density tables, and 32 in high-density tables (Section
\ref{secconcurrentexp}). To achieve similar results for arbitrary
workloads, we introduce a concurrency mechanism called a claim flag.
\begin{itemize}
  \item \textbf{Claim Flags: } By marking which slots have been
    scheduled for kick-out chains (and other write operations), claim
    flags eliminate all competition between inserts with high
    probability (even for polynomially many threads).
\end{itemize}
In our experimental evaluation for delete-heavy workloads, with 15
threads concurrently running 100-operation transactions, claim flags
reduce transaction-abort frequency by a factor of around 512 in
low-density tables, and 20 in high-density tables
(Section~\ref{secconcurrentexp}). Claim flags are a particularly
interesting direction for future work since they can likely be applied
to other transactional data structures.

The rest of this paper is organized as follows. Section
\ref{sec_past_work} discusses past work on fast cuckoo
insertions. Section~\ref{secexp} experimentally evaluates
ghost-insertions, sorted search, queue-kicking, and
rattle-kicking. Section~\ref{secghost} discusses ghost-insertions.
Section~\ref{sec_sorted_search} elaborates on sorted search, and
discusses its applications to concurrent cuckoo hashing. Section
\ref{sec_queue_walk} discusses queue-kicking, and proves for
delete-light workloads that a variant of queue-kicking eliminates all
competition between concurrent inserts with high probability. Section
\ref{sec_dary_walk} discusses rattle-kicking and compares it to an
algorithm of Khosla \cite{khosla13}. Section~\ref{secconcurrent}
implements an optimistic concurrency scheme for transactional cuckoo
hashing and introduces claim-flags; with high probability, claim-flags
eliminate all competition between inserts, regardless of
workload. Finally, Section~\ref{seccon} concludes with directions for
future work.
 














\section{Past work on fast cuckoo insertions} \label{sec_past_work}

Several techniques can be used to reduce kick-out chain length in
serial cuckoo hashing, the simplest of which would be to just use
large bins. One common technique is \emph{load balancing} in which,
upon inserting a record, one inserts into the least full of its two
bins. Another is \emph{stashing}, in which cuckoo chains longer than a
certain maximum length are aborted and the homeless key is inserted
into a separate stash \cite{kirsch09}. This helps especially in the
extremely rare event that no valid kick-out chain exists or if a
kick-out chain is abnormally long. Stashing was used, for example, in
the flash-based key-value store system FlashStore \cite{flash10}. One
of the cleverest modifications to cuckoo hashing is that of
\cite{lehman09}, in which rather than hashing keys to bins of size
, one hashes keys to  adjacent slots -- essentially allowing
bins to overlap. This simple improvement yields surprising performance
improvements. Although these techniques can be useful, to maintain
generality we evaluate our kick-out eviction algorithms in the
standard setting. In Section~\ref{secconcurrent}, however, we do take
advantage of load balancing to reduce contention in the multi-writer
setting.


Until now, not much work has been done on finding good kick-out
eviction schemes.
For -ary cuckoo hashing (i.e., using bins of size one and  hash
functions), however, algorithmic improvements to random-kicking have
been proposed by Khosla \cite{khosla13}, the performance of which we
compare to our results in Section~\ref{sec_queue_walk}.

\section{Experimental Evaluation of Kick-Out Eviction Algorithms}\label{secexp}

In the following sections, we will introduce a number of techniques
for reducing bins-viewed-per-insert in cuckoo hashing. In this section
we compare these techniques experimentally. Note that, in order to
facilitate comparison between algorithms, our graphs are on
logarithmic scales.

Figure~\ref{fig_A} compares random kicking, breadth-first-search,
sorted search (Section~\ref{sec_sorted_search}), and
queue-kicking\footnote{Our implementation of queue-kicking
  additionally uses hit-balancing, which is described in Section
 ~\ref{sec_queue_walk}.} (Section~\ref{sec_queue_walk}). For each
algorithm, we fill 1,000 hash tables to 97.5\% full and graph the
average number of bins viewed for insertions at each density. Each
table consists of  bins, each of which has four slots. For
each algorithm, we also consider in Figure~\ref{fig_B} the version of
the algorithm in which ghost insertions have been implemented (Section
\ref{secghost}). Figure~\ref{fig_A} additionally tests the sorted
search / breadth-first search hybrid discussed in Section
\ref{sec_sorted_search}.

Some implementations of cuckoo hashing use a variant called
\emph{d-ary cuckoo hashing}. In this variant, bins are of size one
and, in order to still get good performance,  hash functions are
used. Figure~\ref{fig_C} compares kick-out algorithms for 4-ary cuckoo
hashing, evaluating random-kicking, breadth-first search, sorted
search, rattle-kicking, and an algorithm due to Khosla
\cite{khosla13}, the final two of which are discussed in Section
\ref{sec_dary_walk}.  Just as in Figure~\ref{fig_A},
Figure~\ref{fig_C} reports the average number of bins viewed per
insertion over the course of 1000 trials; we use tables with 
single-slot bins.

Our experiments use uniformly randomly generated hashes. Moreover,
insertions assume that the key being inserted is not already present,
and are not responsible for verifying this themselves.

All of our experiments appear to scale. That is, if one runs the same
experiments on tables with arbitrarily many bins, then the data-points
will remain essentially unchanged.



\section{Ghost Insertions}\label{secghost}

In this section, we introduce the notion of ghost insertion, which can
be used to improve an any kick-out algorithm's performance for
high-density tables. When we insert a record , we may have room
for it in both bins  and  to which it is hashed. Preferably,
we would insert  into whichever of  or  there will be
the least demand for later. Ghost insertions simulate this by
temporarily inserting the record in both bins. Each copy of the record
is marked as a \emph{duplicate}, indicating to future insertions that
it can easily be removed to make space for another record.
  


Later we may find ourselves trying to insert a record  into bin
, only to discover that no slots are available. Fortunately,
because  appears in both  and , we can simply remove it
from . At this point, the copy of  in  is marked as no
longer being a duplicate record.

Surprisingly, all kick-out chains are guaranteed to terminate in a bin
containing a duplicate.

\begin{prop}\label{propghost}
  Let  be a cuckoo-hash table built using only insert operations,
  and with ghost-insertions enabled. Let  be the final bin in a
  kick-out chain in . Then prior to the kick-out chain, 
  contained at least one duplicate.
\end{prop}
\begin{proof}
  Call a bin \emph{available} if it contains either a free slot or a
  duplicate. Call a bin \emph{reachable} if some record in  is
  hashed to the bin but is contained (perhaps as a duplicate) in a
  different bin.

  It suffices to show that there does not exist a bin  which is
  available, reachable, and free of duplicates. Suppose otherwise. As
  an available and duplicate-free bin,  must contain at least one
  free slot. It follows that no record (duplicate or otherwise) has
  ever been kicked out of . But since  is reachable, some record
   is hashed to  and contained in a different bin. However,
  because  contained a free slot when  was inserted, and since
  no record has ever been kicked out of , record  must be
  present in  as a duplicate, a contradiction.
\end{proof}

Our experiments in Section~\ref{secexp} show that ghost insertions can
significantly reduce the number of bins viewed per insertion for each
kick-out eviction scheme. At high table densities, random kicking is
improved by a factor of roughly 2.5, and breadth-first search is
improved by a bit less than a factor of two. 

The factor of two is hinted at by Proposition~\ref{propghost}. At high
table densities, almost all available bins contain only one free or
duplicate slot. Proposition~\ref{propghost} tells us that only those
bins containing a duplicate are reachable. Thus ghost-insertions
essentially double the number of available reachable bins.










































































\section{Sorted Search} \label{sec_sorted_search}

In this section, we introduce sorted search, a kick-out
eviction algorithm based on breadth-first search. Experimentally, at
high table densities, sorted search can reduce the bins viewed per
insertion by a factor of eight (Figure~\ref{fig_A}).

We begin with a convention. A record's \emph{children} is the set
of slots which are in bins hashed to the record but not containing the
record. When conducting a search algorithm for a kick-out path, we use
the term \emph{spawning a record} to mean looking at the record's
children as part of the search. In turn, the record's children are its
\emph{spawn victims}.

Breadth-first search essentially maintains a queue of records that
have been viewed but not yet spawned in the search. At each step in
the search, we pop a record from the queue and check if any of its
children slots are free. If any are, then the search is
complete. Otherwise, we update the queue and continue.



Rather than maintaining a queue, sorted search maintains a list sorted
by some statistic. At each step, rather than picking the record which
has been in the list the longest to spawn next (as in breadth-first
search), we pick the smallest record according to the statistic.

After experimenting with many statistics, we have found one in
particular, which we call spawn count, to be the most effective
by far. The \emph{spawn count} of a record in bin  is the number of times
since the conception of the hash table that any search has previously
spawned any record that was, at the time, contained in
. Surprisingly, even statistics using information specific to the
record (and not just the bin) appear unable to perform better than
spawn count.



For delete-heavy workloads, spawn count could potentially get
large. One might choose to cap it at a given value. In our
(delete-free) experiments, four bits (per bin) is easily sufficient to
store spawn count. Consequently bucket sort can be used to efficiently
maintain the sorted list during each search.



Unlike breadth-first search, which guarantees to return the shortest
possible cuckoo path, sorted search could potentially return a path
containing a cycle -- that is, we may accidentally try to kick-out the
same record from the same bin twice in the same kick-out chain. One
low-overhead method to avoid revisiting buckets is to maintain a small
hash table of the already visited buckets; in particular, the index of
a bucket can be used as its hash. In order to fairly compare
breadth-first search and sorted search, we eliminate the revisiting of
buckets in our implementations of both.

Sorted search maintains two significant concurrency advantages of
breadth-first search over random kicking. It can be implemented to
take advantage of prefetching, and it provides short kick-out
chains.



\textbf{Prefetching:} Unlike random walking, breadth-first search
tells us the next few bins that will be fetched before we actually
need to fetch them. In turn, this facilitates the use of prefetching to
reduce fetch latency \cite{li14}.

Sorted search can also take advantage of prefetching. Indeed, sorted
search may be modified to spawn the  smallest-statistic records at
a time rather than just one (for an arbitrary ). Moreover, if more
than two hash functions are being used, one can take advantage of
prefetching even when just fetching the children of a single record.

\textbf{Short Critical Path:} Breadth-first search yields a kick-out
chain whose size is logarithmic in the number of bins visited
during the search. Previously, \cite{li14} utilized this to
significantly reduce critical path size in concurrent cuckoo hashing.

Experimentally, sorted search also satisfies this property. In
Figure~\ref{fig_D}, we compare the length of kick-out chains generated
by sorted search and breadth-first search both with and without
ghost-insertions (using the same experimental set-up as in
Figure~\ref{fig_A} from Section~\ref{secexp}). As can be seen by
comparing Figures~\ref{fig_A},~\ref{fig_B}, and~\ref{fig_C}, at 97.5\%
table density, all four search-based algorithms produce kick-out
chains an order of magnitude smaller than those produced by walk-based
algorithms.


If we model "picking the smallest-statistic bin" as "picking a random
bin," and assume that each spawn is equally likely to terminate the
search, then we can prove that sorted search's kick-out chains are
logarithmic in comparison to the number of bins viewed.

\begin{thm}
Suppose we search for a kick-out chain by, at each step in the search,
spawning a record selected randomly from those viewed but not yet
spawned. Moreover, suppose that each spawn is equally likely to
terminate the search.
  
Then the expected length of the resulting kick-out chain is , where  is the total number of spawns in the search.
\end{thm}
\begin{proof}
Let  be the size of each bin and  be the number of hash
functions. We require that . Then for ,g at step
 we have viewed  records. Since each step (after
zero) spawns one record, that leaves  records which
have been viewed but not spawned. Let  be the average
search-tree depth of the records so far viewed but not yet spawned
after  steps, with . The -th step in the algorithm
eliminates one record with expected depth  and introduces
 records with expected depth  each. Since after the
-th step there are  unspawned but viewed records,
we can treat all but  of those records as having average depth
 and  as having average depth , yielding
total average depth of   Since , it follows that . Thus in a search with  spawns, the expected depth of the
final spawn is .
\end{proof}


One may wish to use a sorted search and breadth-first search
hybrid. Here, a record's position in the sorted list is determined
first by the depth in the search at which it was a spawn victim, and
then secondarily by its spawn count. This erases the necessity of
monitoring for cycles and guarantees the kick-out path-length being
logarithmic in bins viewed. As expected, the hybrid's performance
is experimentally between that of sorted search and breadth-first
search (Section~\ref{secexp}).





\section{Queue-Kicking} \label{sec_queue_walk} 

Random walking is easy to implement and extremely low
overhead. Unfortunately, at high table densities it does poorly at
finding short kick-out chains. In this section, we introduce
queue-kicking, which improves upon random kicking in order to
significantly reduce bins viewed per insertion.

In order to develop a variant of random walking which will find
shorter kick-out chains, we must first understand why random walking
does poorly. Suppose one starts with an empty hash table (using bins
of size four), and then fills the table to 97\% full. Then the total
number of kick-outs during all of the insertions combined will only be
around the half the total number of records
inserted\footnote{Experimentally, regardless of table size, each bin
  tends to average 2.07 kick-outs.}


But if we pick kick-outs within a bin randomly, balls-in-bins
suggests that some slots will get picked several times while others
won't get picked at all. Consequently, we will end up kicking records
back into a bin they previously got kicked out of, when we instead
could have kicked out a record who had never been kicked around
before.

A simple resolution is to implement each bin as a queue. Rather then
selecting kick-out victims randomly, one simply selects the record
which has been in the bin the longest. We call this
\emph{queue-kicking}. Queue-kicking's performance is additionally
bolstered by the fact that the earlier a record is inserted into a
bin, the less likely it is there only because the
other bin to which it was hashed was full.

Our experiments in Section~\ref{secexp} show that experimentally
queue-kicking results in far shorter kick-out chains than does
random-walking, at least for high-density tables. We may not be the
first to observe the benefits of queue-kicking; in fact, it is the
kick-out scheme used by Kennith Ross in his high-performance cuckoo
hash table implementation \cite{ross07}. We are the first, however, to
explicitly observe its benefits -- previously it appeared only in a
single clause of a single sentence.

For delete-light loads, one could simulate queue-kicking by using a
counter for each bin. This counter, known as the \emph{hit-counter},
is incremented each time a record is inserted into a bin. The record
is then placed in the slot whose index is congruent to the hit-counter
modulo the size of the bin, kicking out another record if
needed.\footnote{In our experiments the hit-counter can easily be
  stored within a single byte (similarly to spawn-count for sorted
  search). One could choose to store the hit-counter modulo the number
  of slots in each bin.}

Using hit-counters would not work well for delete-heavy workloads
since they could result in a kick-out in a bin which has free
slots. For a workload not containing many deletes, however,
hit-counters have two surprising consequences, the first a small
benefit for performance, and the second a major benefit for concurrency.

\textbf{Hit Balancing: } Previously we discussed a common technique
called \emph{load balancing} in which, upon inserting a record, one
views both hashed bins and picks the less full one. When both
bins are full, however, we can generalize load balancing to
\emph{hit balancing} by picking the bin with a smaller
hit-counter. So that they can utilize hit balancing, our experiments
in Section~\ref{secexp} use hit counters to implement queue-kicking.

\textbf{Scheduling Kick-Out Chains: } One problem with long kick-out
chains is that, in multi-threaded systems, two concurrent insertions
may plan overlapping chains, forcing one of the insertions to start
over \cite{li14}. As we will see in Section ~\ref{secconcurrent}, this
can lead to a problematic number of transaction aborts in the context of
transactional cuckoo hashing. In particular, in transactional cuckoo
hashing, hundreds or thousands of inserts may be planned before any of
them are committed to the table. Any overlaps in their kick-out chains
will lead to transaction aborts. Worse still, if two kick-out chains
terminate in the same bin which contains only a single free slot, they
are guaranteed to compete for the slot.

  Interestingly, this problem can be resolved using hit-counters. If
  hit-counters are incremented with atomic fetch-and-adds, then we are
  guaranteed that any  threads trying to insert into the same bin
  will all be assigned different slots. (Here,  is the number of
  slots in the bin.) In practice, for delete-light workloads, this is
  enough to reduce transaction aborts by a factor of 30 (Section
 ~\ref{secconcurrent}).





In fact, we can prove with high probability that the use of
hit-counters completely eliminates overlaps between concurrent
inserts. This is formalized through the following
lemma.

\begin{lem}\label{lemhighprob}
  Consider a cuckoo hash table with  bins, each containing 
  slots. Suppose  threads concurrently each perform  operations,
  with each operation randomly touching at most  slots. If , then with probability  no bin will
  ever simultaneously receive more than  touches.
\end{lem}
\begin{proof}
  Observe that at most  touches can occur
  simultaneously. Consider an arbitrary bin . Out of the  ways to assign  distinct touches to  bins, at
  most  of
  them assign more than  touches to bin . By the union bound,
  the probability of any bin being over-subscribed is at most
   Applying the union bound to
  the  operations, the probability of any bin is ever
  over-subscribed is at most
  
\end{proof}







































\section{Rattle-Kicking} \label{sec_dary_walk} 

Until now, we have focused on cuckoo hash tables using two hash
functions and bins with several slots. Some applications, however, use
\emph{-ary cuckoo hashing}, in which bins contain one slot only and
 hash functions are used \cite{dary}.\footnote{Two examples of
  systems using this technique include Shore-MT \cite{flash10} and
  FlashStore \cite{johnson09}.}

For -ary cuckoo hashing, random kicking performs poorly with
respect to bins viewed per insertion, even for tables at smaller
densities. In this section, we resolve this with a simple technique
called rattle-kicking.

Refer to the  hash functions as , , ..., . For
each key  in the hash table, we maintain a \emph{rattle-counter}
denoted by  which is initially zero and is incremented every
time we try to insert  into a bin. When a key  is either
introduced to the table or displaced from its previous bin, we try to
insert it into bin . If that bin already contains a
key , however, then whichever of  or  has a higher
rattle-counter gets to stay in the bin. We increment the other's
rattle-counter and recursively try to insert it into another bin.



We refer to the act of picking the key with the smaller rattle-counter
as \emph{rattle-balancing}. Similarly to load balancing,
rattle-balancing ensures that there is little variance between
rattle-counters of records throughout the hash table.

At the cost of a small overhead (rattle-counters) per key, our
algorithm appears to significantly reduce the length of kick-out
chains. (See Section~\ref{secexp}.) At table density below 85\%, our
algorithm performs close to as best as one could hope. If a table has
density , then we would hope each insert and kick-out would
have a  chance of not inducing another kick-out. In turn,
this would yield chains of average length .

Random kicking performs poorly because it reuses hash functions
unnecessarily.  According to our data for -ary cuckoo hashing, when
we fill a table from empty to 95\% using random kicking, each record
has visited a total of only around 5.6 bins\footnote{This average
  appears to be mostly independent of table size.}. But if we stick
5.6 balls randomly in four cups (the number of hash functions), some
cups will get several balls while others may be totally empty. When we
try a hash function we've never used before for a key, we are
effectively picking a random bucket in the hash table in which to
insert that record -- this will lead to the  performance that
we desire. But when we instead reuse a hash function, we are sending
the record to a slot we already know is full.

Rattle counters solve this problem by going through all the hash
functions for a record before reusing any. Moreover, rattle-balancing
keeps rattle-counters for all records relatively balanced, rather than
some records having many unused hash functions while others have
none. Using rattle-counters to fill a table to 95\% full results in
records having used only 3.3 of their hash functions on
average. Consequently, most records have not reused a single hash
function.



In addition to testing random-walking, rattle-kicking, breadth-first
search, and sorted search for d-ary cuckoo hashing (Section
\ref{secexp}), we test an interesting kick-out eviction algorithm due
to Khosla \cite{khosla13}. For high-density tables Khosla's algorithm
performs similarly to rattle-kicking.

The computations in Section~\ref{secexp} assume a record's prior
absence for each insertion. Consequently, when the table is near
empty, our algorithms view fewer than  slots per
insertion. Khosla's algorithm must maintain certain
invariants which prevent it from easily doing this  \cite{khosla13}.

Rattle-kicking can be thought of as a -ary analogue for
queue-kicking. Is there a natural analogue for Khosla's algorithm as well?




\section{Transactional Multi-Writer Cuckoo Hashing}\label{secconcurrent} 

In sections~\ref{sec_sorted_search} and~\ref{sec_queue_walk}, we
discussed the relation of several of our kick-out algorithms to the
multi-writer setting. In this section we apply queue-kicking to the
multi-writer setting, and introduce a novel concurrency scheme which
allows a multi-writer Cuckoo hash table to achieve transactional
correctness without being inhibited by transaction aborts. Past
authors have proven cuckoo hashing's potential as a multi-writer table
with threads performing single operations at a time \cite{fan13,
  li14}. Our work extends this to transactional Cuckoo hashing, where
each thread wants transactions comprising many operations to be
performed atomically.

Each thread in a multi-writer hash table may wish to perform a series
of transactions, where each transaction comprises a collection of
reads, writes, deletes, and overwrites which are dependent on each
other. For example, if a thread were managing bank data, it might wish
to move \1000 and his friend
Liz's account contains less than \rbbb2^{17}TkTknO(n)2^{14}84$-ary kick-out
        algorithms.}
  \label{fig_C}
\end{figure}


\begin{figure}[!htb]
  \includegraphics[scale=.3]{plot_kickouts}
  \caption{The average length of kick-out chains at varying table
    densities, for various search-based kick-out algorithms.}
  \label{fig_D}
\end{figure}

\begin{figure}[!htb]
  \includegraphics[scale=.3]{plot_aborts2}
  \caption{The average number of transaction aborts over the lifetime
    of tables filled to varying densities, for various transactional
    cuckoo hashing implementations, using a delete-light workload and
    15 threads.}
  \label{fig_E}
\end{figure}


\begin{figure}
  \includegraphics[scale=.3]{plot_aborts1}
  \caption{The average number of transaction aborts over the lifetime
    of tables filled to varying densities, for various transactional
    cuckoo hashing implementations, using a delete-heavy workload and
    15 threads.}
  \label{fig_F}
\end{figure}


\end{document}
