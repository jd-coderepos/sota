\setcounter{section}{0}\renewcommand\thesection{\arabic{section}}
\setcounter{table}{0}\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}\renewcommand{\thefigure}{A\arabic{figure}}
\centerline{\Large{\textbf{Appendix for ``Causal Intervention for Weakly-Supervised}}}
\centerline{\Large{\textbf{Semantic Segmentation''}}}
\vspace{10mm}
This appendix includes the derivation of backdoor adjustment for the proposed structural causal model (Section~\ref{sec:a1}),
the normalized weighted geometric mean (Section~\ref{sec:a2}),
the detailed implementations for different baseline models (Section~\ref{sec:a3}), the supplementary ablation studies (Section~\ref{sec:a4}), and more visualization results of segmentation masks (Section~\ref{sec:a5}).
\section{Derivation of Backdoor Adjustment for the Proposed Causal Graph}~\label{sec:a1}
In the main paper, we used backdoor adjustment~\cite{pearl2016causal} to perform the causal intervention.
In this section, we show the derivation of backdoor adjustment for the proposed causal graph (in {\color{red}{Figure~3(b)}} of the main paper), by leveraging the following three -calculus rules~\cite{pearl2000}.

Given an arbitrary causal directed acyclic graph , there are four nodes respectively represented by , , , and .
Particularly,  denotes the intervened causal graph where all \emph{incoming} arrows to  are deleted, and  denotes another intervened causal graph where all \emph{outgoing} arrows
from  are deleted. We use the lower cases , , , and  to represent the respective values of
nodes: , , , and . For any interventional distribution compatible with , we have the following three rules:

\textbf{Rule 1.} Insertion/deletion of observations:\label{rule:1}



\textbf{Rule 2.} Action/observation exchange:\label{rule:2}



\textbf{Rule 3.} Insertion/deletion of actions: \label{rule:3}

where  is a subset of  that are not ancestors of any specific nodes related to  in . Based on these three rules, we can derive the interventional distribution  for our proposed causal graph (in Figure~3(b) of the main paper) by:
\vspace{5mm}

where Eq.~\ref{eq:a4:4} and Eq.~\ref{eq:a4:7} follow the law of total probability. We can obtain Eq.~\ref{eq:a4:5} via \textbf{Rule 3} that given  in , and Eq.~\ref{eq:a4:6} can be obtained via \textbf{Rule 2} which changes the intervention term into observation as  in . Eq.~\ref{eq:a4:8} is because in our causal graph,  is an image-specific context representation given by the function , and Eq.~\ref{eq:a4:9} is essentially equal to Eq.~\ref{eq:a4:8}.
\section{Normalized Weighted Geometric Mean}\label{sec:a2}
{\color{red}{This is Appendix to Section 3.2 ``Step 4. Computing ''.}}
In Section 3.2 of the main paper, we used the Normalized Weighted Geometric Mean (NWGM)~\cite{xu2015show} to move the outer sum  into the feature level: . Here, we show the detailed derivation. Formally, our implementation for the positive term (\ie,  in Eq.(2) of the main paper) can be derived by:
\vspace{4mm}

where  denotes the positive predicted score for the class label which is indeed associated with the input image, and  under this condition. We can obtain Eq.~\ref{eq:a5:10} via our implementation of the multi-label image classification model, and obtain Eq.~\ref{eq:a5:11} and Eq.~\ref{eq:a5:16} via the definition of the softmax function. Eq.~\ref{eq:a5:12} can be obtained via the results in~\cite{baldi2014dropout}. Eq.~\ref{eq:a5:13} to Eq.~\ref{eq:a5:15} follow the derivation in~\cite{xu2015show}. Since  in our implementation is a linear model, we can use Eq.(3) in the main paper to compute . In addition to the positive term, we can also obtain derivation for the negative term (\ie,  in Eq.(2) of the main paper) through the similar process as above.
\section{More Implementation Details}\label{sec:a3}
{\color{red}{This is Appendix to Section 4.1 ``Settings''.}}
In Section 4.1 of the main paper, we deployed CONTA on four popular WSSS models including SEAM~\cite{wang2020self}, IRNet~\cite{ahn2019weakly}, DSRG~\cite{huang2018weakly}, and SEC~\cite{kolesnikov2016seed}. In this section, we show the detailed implementations of these four models.


\subsection{Implementation of SEAM+CONTA}
\myparagraph{Backbone.} ResNet-38~\cite{wu2019wider} was adopted as the backbone network. It was pre-trained on ImageNet~\cite{deng2009imagenet} and its convolution layers of the last three blocks were replaced by dilated convolutions~\cite{yu2015multi} with a common input stride of  and their dilation rates were adjusted, such that the backbone network can return a feature map of stride , \ie, the output size of the backbone network was  of the input.

\myparagraph{Setting.} The input images were randomly re-scaled in the range of  by the longest edge and then cropped into a fix size of  using zero padding if needed.

\myparagraph{Training Details.} The initial learning rate was set to , following the poly policy  with  for decay. Online hard example mining~\cite{shrivastava2016training} was employed on the training loss to preserve only the top  pixel losses. The model was trained with batch size as  for  epochs using Adam optimizer~\cite{kingma2014adam}. We deployed the same data augmentation strategy (\ie, horizontal flip, random cropping, and color jittering~\cite{krizhevsky2012imagenet}), as in AffinityNet~\cite{ahn2018learning}, in our training process.

\myparagraph{Hyper-parameters.}
The hard threshold parameter for CAM was set to  by default and changed to  and  to amplify and weaken background activation, respectively. The fully-connected CRF~\cite{krahenbuhl2011efficient} was used to refine CAM, pseudo-mask, and segmentation mask with the default parameters in the public code. For seed areas expansion, the AffinityNet~\cite{ahn2018learning} was used with the search radius as , the hyper-parameter in the Hadamard power of the affinity matrix as , and the number of iterations in random walk as .



\subsection{Implementation of IRNet+CONTA}
\myparagraph{Backbone.} ResNet-50~\cite{he2016deep} was used as the backbone network (pre-trained on ImageNet~\cite{deng2009imagenet}). The adjusted dilated convolutions~\cite{yu2015multi} were used in the last two blocks with a common input stride of , such that the backbone network can return a feature map of stride , \ie, the output size of the backbone network was  of the input.

\myparagraph{Setting.} The input image was cropped into a fix size of  using zero padding if needed.

\myparagraph{Training Details.} The stochastic gradient descent was used for optimization with  iterations. Learning rate was initially set to , and decreased using polynomial decay  with  at every iteration. The batch size was set to 16 for the image classification model and 32 for the inter-pixel relation model. The same data augmentation strategy (\ie, horizontal flip, random cropping, and color jittering~\cite{krizhevsky2012imagenet}) as in AffinityNet~\cite{ahn2018learning} was used in the training process.

\myparagraph{Hyper-parameters.} The fully-connected CRF~\cite{krahenbuhl2011efficient} was used to refine CAM, pseudo-mask, and segmentation mask with the default parameters given in the original code. The hard threshold parameter for CAM was set to  by default and changed to  and  to amplify and weaken the background activation, respectively. The radius  that limits the search space of pairs was set to  when training, and reduced to  at inference (conservative propagation in inference).
The number of random walk iterations  was fixed to . The hyper-parameter  in the Hadamard power of the affinity matrix was set to .

\subsection{Implementation of DSRG+CONTA}
\myparagraph{Backbone.} ResNet-101~\cite{he2016deep} was used as the backbone network (pre-trained on ImageNet~\cite{deng2009imagenet}) where dilated convolutions~\cite{yu2015multi} were used in the last two blocks, such that the backbone network can return a feature map of stride , \ie, the output size of the backbone network was  of the input.

\myparagraph{Setting.} The input image was cropped into a fix size of  using zero padding if needed.

\myparagraph{Training Details.} The stochastic gradient descent with mini-batch was used for network optimization with  iterations. The momentum and the weight decay were set to  and , respectively. The batch size was set to 20, and the dropout rate was set to . The initial learning rate was set to  and it was decreased by a factor of  every  iterations.

\myparagraph{Hyper-parameters.} For seed generation, pixels with the top  activation values in the CAM were considered as foreground (objects) as in~\cite{zhou2016learning}. For saliency masks, the model in~\cite{jiang2013salient} was used to produce the background localization cues with the normalized saliency value . For the similarity criteria, the foreground threshold and the background threshold were set to  and , respectively. The fully-connected CRF~\cite{krahenbuhl2011efficient} was used to refine pseudo-mask and segmentation mask with the default parameters in the public code.

\subsection{Implementation of SEC+CONTA}
\myparagraph{Backbone.} VGG-16~\cite{simonyan2014very} was used as the backbone network (pre-trained on ImageNet~\cite{deng2009imagenet}), where the last two fully-connected layers were substituted with randomly initialized convolutional layers, which have  output channels and kernels of size , such that the output size of the backbone network was  of the input.

\myparagraph{Setting.} The input image was cropped into a fix size of  using zero padding if needed.

\myparagraph{Training Details.} The weights for the last (prediction) layer were randomly initialized from a normal distribution with mean  and variance . The stochastic gradient descent was used for the network optimization with  iterations, the batch size was set to , the dropout rate was set to  and the weight decay parameter was set to . The initial learning rate was  and it was decreased by a factor of  every  iterations.

\myparagraph{Hyper-parameters.} For seed generation, pixels with the top  activation values in the CAM were considered as foreground (objects) as in~\cite{zhou2016learning}. The fully-connected CRF~\cite{krahenbuhl2011efficient} was used to refine pseudo-mask and segmentation mask with the spatial distance was multiplied by  to reflect the fact that the original image was down-scaled to match the size of the predicted segmentation mask, and the other parameters are consistent with the public code.

\section{More Ablation Study Results}
\label{sec:a4}
{\color{red}{This is Appendix to Section 4.2 ``Ablation Study''.}}
In Section 4.2 of the main paper, we showed the ablation study results of SEAM~\cite{wang2020self}+CONTA on PASCAL VOC 2012~\cite{everingham2015pascal}. In this section, we show the results of IRNet~\cite{ahn2019weakly}+CONTA, DSRG~\cite{huang2018weakly}+CONTA, and SEC~\cite{kolesnikov2016seed}+CONTA on PASCAL VOC 2012. Besides, we also show the results of SEAM+CONTA, IRNet+CONTA, DSRG+CONTA, and SEC+CONTA on MS-COCO~\cite{lin2014microsoft}.
\begin{table}[t]
\centering
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{lc|c|c|c}
\multicolumn{2}{c|}{Setting} & CAM & Pseudo-Mask & Seg. Mask \\ \shline
~ & Upperbound~\cite{long2015fully} & -- & -- & 72.3 \\
~ & Baseline~\cite{ahn2019weakly} & 48.3 & 65.9 & 63.0 \\ \hline
(\textbf{A1}) &  & 48.1 &  65.5 & 62.1 \\ \hline
\multirow{4}*{(\textbf{A2})} & Round = 1 & 48.5 &  66.9 & 64.2 \\
~ & Round = 2 & 48.7 &  67.6 & 65.0 \\
~ & Round = 3 & \textbf{48.8} &  \textbf{67.9} & \textbf{65.3} \\
~ & Round = 4 & 48.6 &  67.2 & 64.9 \\ \hline
\multirow{5}*{(\textbf{A3})} & Block-2 & 48.3 & 66.2 & 63.4 \\
~ & Block-3 & 48.4 & 66.6 & 63.8 \\
~ & Block-4 & 48.7 &  67.3 & 64.6 \\
~ & Block-5 & \textbf{48.8} &  \textbf{67.9} & \textbf{65.3} \\
~ & Dense & 48.7 & 67.6 & 65.1 \\ \hline
\multirow{2}*{(\textbf{A4})} &  & 48.6 & 67.4 & 65.0 \\
~ &  &  \textbf{48.8} &  \textbf{67.9} & \textbf{65.3} \\
\end{tabular}}
\vspace{3mm}
\caption{Ablations of IRNet~\cite{ahn2019weakly}+CONTA on PASCAL VOC 2012~\cite{everingham2015pascal} in mIoU (\%). ``*'' denotes our re-implemented results. ``Seg. Mask'' refers to the segmentation mask of the \emph{val} set. ``--'' denotes that the result is N.A. for the fully-supervised model.}
\label{a:tab:1}
\end{table} 
%
 \begin{table}[t]
\centering
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{lc|c|c|c}
\multicolumn{2}{c|}{Setting} & CAM & Pseudo-Mask & Seg. Mask \\ \shline
~ & Upperbound~\cite{long2015fully} & -- & -- & 77.7 \\
~ & Baseline~\cite{huang2018weakly} & 47.3 & 62.7 & 61.4 \\ \hline
(\textbf{A1}) &   & 47.0 & 61.9 & 61.1 \\ \hline
\multirow{4}*{(\textbf{A2})} & Round = 1  & 47.7 & 63.5 & 62.2 \\
~ & Round = 2  & \textbf{48.0} & \textbf{64.0} & \textbf{62.8} \\
~ & Round = 3  & 47.8 & 63.8 & 62.5 \\
~ & Round = 4  & 47.4 & 63.5 & 62.1 \\ \hline 
\multirow{5}*{(\textbf{A3})} & Block-2  & 47.4 & 62.9 & 61.7 \\
~ & Block-3  & 47.6 & 63.2 & 62.1 \\
~ & Block-4  & 47.9 & 63.7 & 62.6 \\
~ & Block-5  & \textbf{48.0} & \textbf{64.0} & \textbf{62.8} \\
~ & Dense  & 47.8 & 63.8 & 62.7 \\  \hline 
\multirow{2}*{(\textbf{A4})} &  & 47.8 & 63.6 & 62.5 \\
~ &  & \textbf{48.0} & \textbf{64.0} & \textbf{62.8} \\
\end{tabular}}
\vspace{3mm}
\caption{Ablations of DSRG~\cite{huang2018weakly}+CONTA on PASCAL VOC 2012~\cite{everingham2015pascal} in mIoU (\%). ``*'' denotes our re-implemented results. ``Seg. Mask'' refers to the segmentation mask of the \emph{val} set. ``--'' denotes that the result is N.A. for the fully-supervised model.}
\label{a:tab:2}
\end{table}
%
 \begin{table}[t]
\centering
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{lc|c|c|c}
\multicolumn{2}{c|}{Setting} & CAM & Pseudo-Mask & Seg. Mask \\ \shline
~ & Upperbound~\cite{long2015fully} & -- & -- & 71.6 \\
~ & Baseline~\cite{kolesnikov2016seed} & 46.5 & 53.4 & 50.7 \\ \hline
(\textbf{A1}) &   & 46.4 & 53.1 & 50.3 \\ \hline
\multirow{4}*{(\textbf{A2})} & Round = 1  & 47.1 & 54.3 & 51.7 \\
~ & Round = 2  & 47.6 & 55.1 & 52.6 \\
~ & Round = 3  & \textbf{47.9} & \textbf{55.7} & \textbf{53.2} \\
~ & Round = 4  & 47.7 & 55.6 & 53.0 \\ \hline 
\multirow{5}*{(\textbf{A3})} & Block-2  & 46.8 & 53.9 & 51.2 \\
~ & Block-3  & 47.1 & 54.5 & 51.5 \\
~ & Block-4  & 47.6 & 55.1 & 52.4 \\
~ & Block-5  & \textbf{47.9} & \textbf{55.7} & \textbf{53.2} \\
~ & Dense  & 47.8 & 55.6 & 53.0 \\  \hline 
\multirow{2}*{(\textbf{A4})} &  & 47.7 & 55.3 & 52.9 \\
~ &  & \textbf{47.9} & \textbf{55.7} & \textbf{53.2} \\
\end{tabular}}
\vspace{3mm}
\caption{Ablations of SEC~\cite{kolesnikov2016seed}+CONTA on PASCAL VOC 2012~\cite{everingham2015pascal} in mIoU (\%). ``*'' denotes our re-implemented results. ``Seg. Mask'' refers to the segmentation mask of the \emph{val} set. ``--'' denotes that the result is N.A. for the fully-supervised model.}
\label{a:tab:3}
\end{table}
%
 \subsection{PASCAL VOC 2012}
Table~\ref{a:tab:1}, Table~\ref{a:tab:2}, and Table~\ref{a:tab:3} show ablation results of IRNet+CONTA, DSRG+CONTA, and SEC+CONTA on PASCAL VOC 2012, respectively.
We can observe that IRNet+CONTA and SEC+CONTA can achieve the best performance at round, and DSRG+CONTA can achieve the best mIoU score at round. In addition to results of SEAM+CONTA in our main paper, we can see that IRNet+CONTA can achieve the second best mIoU results:  on CAM, \% on pseudo-mask, and \% on segmentation mask.

\subsection{MS-COCO}
Table~\ref{a:tab:4}, Table~\ref{a:tab:5}, Table~\ref{a:tab:6}, and Table~\ref{a:tab:7} show the respective ablation results of SEAM+CONTA, IRNet+CONTA, DSRG+CONTA, and SEC+CONTA on MS-COCO.
We can see that SEAM+CONTA, IRNet+CONTA and, SEC+CONTA can achieve the top mIoU at round, and DSRG+CONTA can achieve the best performance at round. In particular, we see that the mIoU scores of IRNet+CONTA are the best on MS-COCO as respectively  on CAM,  on pseudo-mask, and  on segmentation mask.
\begin{table}[t]
\centering
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{lc|c|c|c}
\multicolumn{2}{c|}{Setting} & CAM & Pseudo-Mask & Seg. Mask \\ \shline
~ & Upperbound~\cite{long2015fully} & -- & -- & 44.8 \\
~ & Baseline~\cite{wang2020self} & 25.1 & 31.5 & 31.9 \\ \hline
(\textbf{A1}) &   & 24.8 & 31.1 & 31.4 \\ \hline
\multirow{4}*{(\textbf{A2})} & Round = 1  & 25.7 & 31.9 & 32.4 \\
~ & Round = 2  & 26.2 & 32.2 & 32.7 \\
~ & Round = 3  & \textbf{26.5} & \textbf{32.5} & \textbf{32.8} \\
~ & Round = 4  & 26.3 & 32.1 & 32.6 \\ \hline 
\multirow{5}*{(\textbf{A3})} & Block-2 & 25.7 & 32.0 & 32.3 \\
~ & Block-3 & 25.9 & 32.1 & 32.4 \\
~ & Block-4 & 26.3 & 32.4 & 32.6 \\
~ & Block-5 & \textbf{26.5} & \textbf{32.5} & \textbf{32.8} \\
~ & Dense  & \textbf{26.5} & 32.4 & 32.5 \\  \hline 
\multirow{2}*{(\textbf{A4})} &  & 26.4 & 32.0 & 32.6 \\
~ &  & \textbf{26.5} & \textbf{32.5} & \textbf{32.8} \\
\end{tabular}}
\vspace{3mm}
\caption{Ablation results of SEAM~\cite{wang2020self}+CONTA on MS-COCO~\cite{lin2014microsoft} in mIoU (\%). ``*'' denotes our re-implemented results. ``Seg. Mask'' refers to the segmentation mask of the \emph{val} set. ``--'' denotes that the result is N.A. for the fully-supervised model.}
\label{a:tab:4}
\end{table}
%
 \begin{table}[t]
\centering
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{lc|c|c|c}
\multicolumn{2}{c|}{Setting} & CAM & Pseudo-Mask & Seg. Mask \\ \shline
~ & Upperbound~\cite{long2015fully} & -- & -- & 42.5 \\
~ & Baseline~\cite{ahn2019weakly} & 27.4 & 34.0 & 32.6 \\ \hline
(\textbf{A1}) &   & 27.1 & 33.5 & 32.3 \\ \hline
\multirow{4}*{(\textbf{A2})} & Round = 1  & 28.0 & 34.3 & 32.9 \\
~ & Round = 2  & 28.4 & 34.8 & 33.2 \\
~ & Round = 3  & \textbf{28.7} & \textbf{35.2} & \textbf{33.4} \\
~ & Round = 4  & 28.5 & 35.0 & 33.2 \\ \hline 
\multirow{5}*{(\textbf{A3})} & Block-2 & 27.7 & 34.3 & 32.8 \\
~ & Block-3  & 27.9 & 34.5 & 32.9 \\
~ & Block-4  & 28.4 & 34.9 & 33.2 \\
~ & Block-5  & \textbf{28.7} & \textbf{35.2} & \textbf{33.4} \\
~ & Dense  & 28.6 & \textbf{35.2} & 33.1 \\  \hline 
\multirow{2}*{(\textbf{A4})} &  & 28.5 & 35.0 & 33.2 \\
~ &  & \textbf{28.7} & \textbf{35.2} & \textbf{33.4} \\
\end{tabular}}
\vspace{3mm}
\caption{Ablation results of IRNet~\cite{ahn2019weakly}+CONTA on MS-COCO~\cite{lin2014microsoft} in mIoU (\%). ``*'' denotes our re-implemented results. ``Seg. Mask'' refers to the segmentation mask of the \emph{val} set. ``--'' denotes that the result is N.A. for the fully-supervised model.}
\label{a:tab:5}
\end{table}
%
 \begin{table}[t]
\centering
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{lc|c|c|c}
\multicolumn{2}{c|}{Setting} & CAM & Pseudo-Mask & Seg. Mask \\ \shline
~ & Upperbound~\cite{long2015fully} & -- & -- & 45.0 \\
~ & Baseline~\cite{huang2018weakly} & 19.8 & 26.1 & 25.6 \\ \hline
(\textbf{A1}) &   & 19.5 & 25.9 & 25.5 \\ \hline
\multirow{4}*{(\textbf{A2})} & Round = 1  & 20.5 & 26.9 & 26.1 \\
~ & Round = 2  & \textbf{20.9} & \textbf{27.5} & \textbf{26.4} \\
~ & Round = 3  & 20.7 & 27.2 & 26.2 \\
~ & Round = 4  & 20.4 & 26.9 & 26.0 \\ \hline 
\multirow{5}*{(\textbf{A3})} & Block-2  & 20.1 & 26.8 & 25.9 \\
~ & Block-3  & 20.2 & 27.0 & 26.0 \\
~ & Block-4  & 20.5 & 27.2 & 26.2 \\
~ & Block-5 & \textbf{20.9} & \textbf{27.5} & \textbf{26.4} \\
~ & Dense  & 20.8 & 27.3 & 26.1 \\  \hline 
\multirow{2}*{(\textbf{A4})} &  & 20.7 & 27.2 & 26.1 \\
~ &  & \textbf{20.9} & \textbf{27.5} & \textbf{26.4} \\
\end{tabular}}
\vspace{3mm}
\caption{Ablation results of DSRG~\cite{huang2018weakly}+CONTA on MS-COCO~\cite{lin2014microsoft} in mIoU (\%). ``*'' denotes our re-implemented results. ``Seg. Mask'' refers to the segmentation mask of the \emph{val} set. ``--'' denotes that the result is N.A. for the fully-supervised model.}
\label{a:tab:6}
\end{table}
%
 \begin{table}[t]
\centering
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{lc|c|c|c}
\multicolumn{2}{c|}{Setting} & CAM & Pseudo-Mask & Seg. Mask \\ \shline
~ & Upperbound~\cite{long2015fully} & -- & -- & 41.0 \\
~ & Baseline~\cite{kolesnikov2016seed} & 18.7 & 24.0 & 22.4 \\ \hline
(\textbf{A1}) &   & 18.1 & 23.5 & 21.2 \\ \hline
\multirow{4}*{(\textbf{A2})} & Round = 1  & 20.1 & 24.4 & 23.0 \\
~ & Round = 2  & 21.2 & 24.7 & 23.4 \\
~ & Round = 3  & \textbf{21.8} & \textbf{24.9} & \textbf{23.7} \\
~ & Round = 4  & 21.4 & 24.5 & 23.5 \\ \hline 
\multirow{5}*{(\textbf{A3})} & Block-2  & 19.5 & 24.2 & 22.7 \\
~ & Block-3  & 19.9 & 24.4 & 22.9 \\
~ & Block-4  & 20.6 & 24.7 & 23.5 \\
~ & Block-5  & \textbf{21.8} & \textbf{24.9} & \textbf{23.7} \\
~ & Dense & \textbf{21.8} & 24.6 & 23.5 \\  \hline 
\multirow{2}*{(\textbf{A4})} &  & 21.5 & 24.7 & 23.4 \\
~ &  & \textbf{21.8} & \textbf{24.9} & \textbf{23.7} \\
\end{tabular}}
\vspace{3mm}
\caption{Ablation results of SEC~\cite{kolesnikov2016seed}+CONTA on MS-COCO~\cite{lin2014microsoft} in mIoU (\%). ``*'' denotes our re-implemented results. ``Seg. Mask'' refers to the segmentation mask of the \emph{val} set. ``--'' denotes that the result is N.A. for the fully-supervised model.}
\label{a:tab:7}
\end{table}
%
 \section{More Visualizations}\label{sec:a5}
{\color{red}{This is Appendix to Section 4.4 ``Comparison with State-of-the-arts''.}}
More segmentation results are visualized in Figure~\ref{a:fig:1}. We can observe that most of our resulting masks are of high quality. The segmentation masks predicted by SEAM+CONTA are more accurate and have better integrity, \eg, for cow, horse, bird, person lying next to the dog, and person standing next to the cows. In particular, SEAM+CONTA works better to prediction the edges of some thin objects or object parts, \eg, the tail (or the head) of bird, car, and person in the car.
\begin{figure*}[t]
\centering
\includegraphics[width=.9 \textwidth]{Appendix/a_figs/a_fig1.pdf}
\caption{More visualization results. Samples are from PASCAL VOC 2012~\cite{everingham2015pascal}. Red rectangles highlight the improved regions predicted by SEAM~\cite{wang2020self}+CONTA.}
\label{a:fig:1}
\end{figure*}
