\documentclass{article}



\PassOptionsToPackage{numbers}{natbib}


\usepackage[preprint]{neurips_2020}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype} 


\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}

\let\proof\relax \let\endproof\relax
\usepackage{amsfonts,bm,mathtools,amsthm}
\usepackage{xcolor}

\usepackage{epsfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{wrapfig}

\title{Discriminator Contrastive Divergence: Semi-Amortized Generative Modeling by Exploring Energy of the Discriminator}



\author{Yuxuan Song*  Qiwei Ye* Minkai Xu* 
  Tie-Yan Liu\\
  Shanghai Jiao Tong University Microsoft Research\\
  \texttt{\{songyuxuan,mkxu\}@apex.sjtu.edu.cn}, \texttt{\{qiwye,tie-yan.liu\}@microsoft.com}
}


\newcommand{\tpdv}[2]{\frac{\partial{#1}}{\partial{#2}}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}

\newcommand{\yuxuan}[1]{{\color{yellow} [Yuxuan: #1]}}
\newcommand{\qiwye}[1]{{\color{red} [Qiwei: #1]}}
\newcommand{\minkai}[1]{{\color{blue} [Minkai: #1]}}

\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\tq{{\Tilde{q}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\def\vpi{{\bm{\pi}}}
\def\vomega{{\bm{\omega}}}
\def\vlambda{{\bm{\lambda}}}
\def\vtheta{{\bm{\theta}}}
\def\vphi{{\bm{\phi}}}
\def\vmu{{\bm{\mu}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\fdivergence}{D_{f}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



\newcommand{\gse}{\gS_E}
\newcommand{\gsl}{\gS_L}
\newcommand{\gae}{\gA_E}
\newcommand{\gal}{\gA_L}
\newcommand{\sse}{s_E}
\newcommand{\ssl}{s_L}
\newcommand{\sae}{a_E}
\newcommand{\sal}{a_L}
\newcommand{\he}{h_E}
\newcommand{\hl}{h_L}

\newcommand{\js}[1]{{\color{teal} [JS: #1]}}
\newcommand{\mc}[1]{{\mathcal{#1}}}
\newcommand{\bb}[1]{{\mathbb{#1}}}

\newcommand{\sx}{{{\mathcal{S}_x}}}
\newcommand{\sy}{{{\mathcal{S}_y}}}
\newcommand{\ax}{{{\mathcal{A}_x}}}
\newcommand{\ay}{{{\mathcal{A}_y}}}
\newcommand{\ux}{{{\mathcal{U}_x}}}
\newcommand{\uy}{{{\mathcal{U}_y}}}
\newcommand{\vvx}{{{\mathcal{V}_x}}}
\newcommand{\vvy}{{{\mathcal{V}_y}}}


\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon} 
\begin{document}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution, with the order determined by flipping coins.}
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}    
    Generative Adversarial Networks (GANs) have shown great promise in modeling high dimensional data. The learning objective of GANs usually minimizes some measure discrepancy, \textit{e.g.}, -divergence~(-GANs~\cite{nowozin2016f}) or Integral Probability Metric~(Wasserstein GANs~\cite{arjovsky2017wasserstein}). With -divergence as the objective function, the discriminator essentially estimates the density ratio~\cite{uehara2016generative}, and the estimated ratio proves useful in further improving the sample quality of the generator~\cite{azadi2018discriminator,turner2018metropolis}. However, how to leverage the information contained in the discriminator of Wasserstein GANs (WGAN)~\cite{arjovsky2017wasserstein} is less explored.  In this paper, we introduce the Discriminator Contrastive Divergence, which is well motivated by the property of WGAN's discriminator and the relationship between WGAN and energy-based model. Compared to standard GANs, where the generator is directly utilized to obtain new samples, our method proposes a semi-amortized generation procedure where the samples are produced with the generator's output as an initial state. Then several steps of Langevin dynamics are conducted using the gradient of the discriminator. We demonstrate the benefits of significant improved generation on both synthetic data and several real-world image generation benchmarks.\footnote{Code is available at \url{https://github.com/MinkaiXu/Discriminator-Contrastive-Divergence}.}
\end{abstract}

\section{Introduction}

Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} proposes a widely popular way to learn likelihood-free generative models, which have shown promising results on various challenging tasks. Specifically, GANs are learned by finding the equilibrium of a min-max game between a generator and a discriminator, or a critic under the context of WGANs. Assuming the optimal discriminator can be obtained, the generator substantially minimizes some discrepancy between the generated distribution and the target distribution. 

Improving training GANs by exploring the discrepancy measure with the excellent property has stimulated fruitful lines of research works and is still an active area. Two well-known discrepancy measures for training GANs are -divergence and Integral Probability Metric (IPM)~\cite{muller1997integral}. -divergence is severe for directly minimization due to the intractable integral, -GANs provide minimization instead of a variational approximation of -divergence between the generated distribution  and the target distribution . The discriminator in -GANs serves as a density ratio estimator~\cite{uehara2016generative}.
The other families of GANs are based on the minimization of an Integral Probability Metric (IPM). According to the definition of IPM, the critic needs to be constrained into a specific function class. When the critic is restricted to be 1-Lipschitz function, the corresponding IPM turns to the Wasserstein-1 distance, which inspires the approaches of  Wasserstein GANs~(WGANs)~\cite{miyato2018spectral,arjovsky2017wasserstein,gulrajani2017improved}.

No matter what kind of discrepancy is evaluated and minimized, the discriminator is usually discarded at the end of the training, and only the generator is kept to generate samples. A natural question to ask is whether, and how we can leverage the remaining information in the discriminator to construct a more superior distribution than simply sampling from a generator. 

Recent work ~\cite{azadi2018discriminator,turner2018metropolis} has shown that a density ratio can be obtained through the output of discriminator, and a more superior distribution can be acquired by conducting rejection sampling or Metropolis-Hastings sampling with the estimated density ratio based on the original GAN~\cite{goodfellow2014generative}.

However, the critical limitation of previous methods lies in that they can not be adapted to WGANs, which enjoy superior empirical performance over other variants. How to leverage the information of a WGAN's critic model to improve image generation remains an open problem. In this paper, we do the following to address this: 

\begin{itemize}
\vspace{-3pt}
    \item We provide a generalized view to unify different families of GANs by investigating the informativeness of the discriminators.
    \vspace{-1pt}
    \item We propose a semi-amortized generative modeling procedure so-called discriminator contrastive divergence~(DCD), which achieves an intermediate between implicit and explicit generation and hence allows a trade-off between generation quality and speed.
\vspace{-3pt}
\end{itemize}
Extensive experiments are conducted to demonstrate the efficacy of our proposed method on both synthetic setting and real-world generation scenarios, which achieves state-of-the-art performance on several standard evaluation benchmarks of image generation.

\section{Related Works}
Both empirical~\cite{arjovsky2017wasserstein} and theoretical~\cite{heusel2017gans} evidence has demonstrated that learning a discriminative model with neural networks is relatively easy, and the neural generative model(sampler) is prone to reach its bottleneck during the optimization. Hence, there is strong motivation to further improve the generated distribution by exploring the remaining information. Two recent advancements are discriminator rejection sampling(DRS)~\cite{azadi2018discriminator} and MH-GANs~\cite{turner2018metropolis}. DRS conducts rejection sampling on the output of the generator. The vital limitation that lies in the upper bound of  is needed to be estimated for computing the rejection probability. MH-GAN sidesteps the above problem by introducing a Metropolis-Hastings sampling procedure with generator acting as the independent proposal; the state transition is estimated with a well-calibrated discriminator. However, the theoretical justification of both the above two methods is based on the fact that the output of discriminator needs to be viewed as an estimation of density ratio . As pointed out by previous work~\cite{zhou2019lipschitz}, the output of a discriminator in WGAN~\cite{arjovsky2017wasserstein} suffers from the free offset and can not provide the density ratio, which prevents the application of the above methods in WGAN. 

Our work is inspired by recent theoretical studies on the property of discriminator in WGANs~\cite{gulrajani2017improved,zhou2019lipschitz}. \cite{tanaka2019discriminator} proposes discriminator optimal transport~(DOT) to leverage the optimal transport plan implied by WGANs' discriminator, which is orthogonal to our  method. 
Moreover, turning the discriminator of WGAN into an energy function is closely related to the amortized generation methods in the context of the energy-based model (EBM)~\cite{kim2016deep,zhao2016energy,kumar2019maximum} where a separate network is proposed to learn to sample from the partition function in~\cite{finn2016connection}. Recent progress~\cite{song2019generative,du2019implicit} in the area of EBM has shown the feasibility of generating high dimensional data with Langevin dynamics. From the perspective of EBM, our proposed method can be seen as an intermediary between an amortized generation model and an implicit generation model, \emph{i.e.}, a semi-amortized generation method, which allows a trade-off between speed and flexibility of generation.  With a similar spirit, \cite{grathwohl2019your}  also illustrates the potential connection between neural classifier and energy-based model in supervised and semi-supervised scenarios.  
\section{Preliminaries}
\subsection{Generative Adversarial Networks}
\label{GANs}
Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} is an implicit generative model that aims to fit an empirical data distribution  over sample space . The generative distribution  is implied by a generated function , which maps latent variable  to sample , \emph{i.e.}, . Typically, the latent variable  is distributed on a fixed  prior distribution . With i.i.d samples available from  and , the GAN typically learns the generative model through a min-max game between a discriminator  and a generator :

With  and  as the function  and the  is constrained as 1-Lipschitz function, the Eq.~\ref{gan_obj} yields the WGANs objective which essentially minimizes the Wasserstein distance between  and . 
With  and  as the Fenchel conjugate\cite{hiriart2012fundamentals} of a convex and lower-semicontinuous function, the objective in Eq.~\ref{gan_obj} approximately minimize a variational estimation of -divergence\cite{nowozin2016f} between  and .

\subsection{Energy Based Model and MCMC basics}
\label{A}
The energy-based model tends to learn an unnormalized probability model implied by an energy function  to prescribe the ground truth data distribution . The corresponding normalized density function is:

where  is so-called normalization constant.  
The objective of training an energy-based model with maximum likelihood estimation is as:

The estimated gradient with respect to the MLE objective is as follows:

The above method for gradient estimation in Equation~\ref{cd} is called contrastive divergence~(CD). 
Furthermore, we define the \emph{score} of distribution with density function  as . We can immediately conclude that , which does not depend on the intractable .


Markov chain Monte Carlo is a powerful framework for drawing samples from a given distribution. An MCMC is specified by a transition kernel  which corresponds to a unique stationary distribution , \emph{i.e.},

More specifically, MCMC can be viewed as drawing  from the initial distribution  and iteratively get sample  at the -th iteration by applied the transition kernel on the previous step, \emph{i.e.}, . Following \cite{li2017approximate}, we formalized the distribution  of  as obtained by a fixed point update of form , and :

As indicated by the standard theory of MCMC, the following monotonic property is satisfied:

And  converges to the stationary distribution  as .

\section{Methodology}
\label{sec:methodology}
\subsection{Informativeness of Discriminator}\label{sec::iod}
In this section, we seek to investigate the following questions:
\begin{itemize}
\vspace{-3pt}
    \item What kind of information is contained in the discriminator of different kinds of GANs?
    \vspace{-1pt}
    \item Why and how can the information be utilized to further improved the quality of generated distribution?
\vspace{-3pt}
\end{itemize}
We discuss the discriminator of -GANs, and WGANs, respectively, in the following.

\subsubsection{-GAN Discriminator}
\label{f-GAN-d}
-GAN~\cite{nguyen2010estimating} is based on the variational estimation of -divergence~\cite{ali1966general} with only samples from two distributions available:
\begin{theorem}\cite{nguyen2010estimating}
\label{fgan}
With Fenchel Duality, the variational estimation of -divergence can be illustrated as follows:

where the  is the arbitrary class of function and  denotes the Fenchel conjugate of . And the supremum is achieved only when , \emph{i.e.} .
\end{theorem}
In -GAN~\cite{nowozin2016f}, the discriminator  is actually the function  parameterized with neural networks. Theorem.~\ref{fgan} indicates the density ratio estimation view of -GAN's discriminator, as illustrated in \cite{uehara2016generative}. More specifically, the discriminator in -GAN is  optimized to estimate a statistic related to the density ratio between  and , \emph{i.e.} , and the   can be acquired easily with . For example, in the original GANs~\cite{goodfellow2014generative}, the corresponding  in -GAN literature is . Assuming the discriminator is trained to be optimal, the output is , and we can get the density ratio . However, it should be noticed that the discriminator is hard to reach the optimality. In practice, without loss of generality, the density ratio implied by a sub-optimal discriminator can be seen as the density ratio between an  implicitly defined distribution  and the generated distribution . It has been studied both theoretically and empirically in the context of GANs~\cite{arjovsky2017wasserstein,heusel2017gans,hjelm2017boundary}, with the same inductive bias, that learning a discriminative model is more accessible than a generative model. Based on the above fact, the rejection-sampling based methods are proposed to use the estimated density ratio, \emph{e.g.},  in original GANs, to conduct rejection sampling\cite{azadi2018discriminator} or Metropolis-Hastings sampling\cite{turner2018metropolis} based on generated distribution . These methods radically modify the generated distribution  to , the improvement in empirical performance as shown in \cite{azadi2018discriminator,turner2018metropolis} demonstrates that we can construct a superior distribution    to prescribe the empirical distribution  by involving the remaining information in discriminator. 

\begin{figure*}[!t]\label{fig::overview}
	\centering
    \includegraphics[width=1.0\linewidth]{DCD-crop.pdf} 
    \caption{\label{fig:dcd} Discriminator Contrastive Divergence: 
    After WGAN training, a fine-tuning for critics can be conducted with several MCMC steps, which leverages the gradient of discriminator by Langevin dynamics;
    after the fine-tuning, the discriminator could be viewed as a superior distribution of , hence sampling from  can be implemented using the same Langevin dynamics as described in \ref{DCD}.
    }
    \vspace{-5pt}
\end{figure*}

\subsubsection{WGAN Discriminator}\label{sec::wgan-critic}
Different from -GANs, the objective of WGANs is derived from the Integral Probability Metric, and the discriminator can not naturally be derived as an estimated density ratio. Before leveraging the remaining information in the discriminator, the property of the discriminator in WGANs needs to be investigated first. We introduce the primal problem implied by WGANs objective as follows:

Let  denote the joint probability for transportation between  and , which satisfies the marginality conditions,

The primal form first-order Wasserstein distance  is defined as:

the objective function of the discriminator in Wasserstein GANs is the Kantorovich-Rubinstein duality of  Eq.~\ref{wgan_p}, and the optimal discriminator has the following property\cite{gulrajani2017improved}:
\begin{theorem}
\label{gradient_direction}
Let  as the optimal transport plan in Eq.~\ref{wgan_p} and  with . With the optimal discriminator  as a differentiable function and  for all , then it holds that:

\end{theorem}

Theorem.~\ref{gradient_direction} states that for each sample  in the generated distribution , the gradient on the  directly points to a sample  in the , where the  pairs are consistent with the optimal transport plan . All the linear interpolations  between  and  satisfy that . 
It should also be noted that similar results can also be drawn in some variants of WGANs, whose loss functions may have a slight difference with standard WGAN~\cite{zhou2019lipschitz}. 
For example, the SNGAN uses the hinge loss during the optimization of the discriminator, \textit{i.e.},  and  in Eq.~\ref{gan_obj} is selected as  for stabilizing the  training procedure. 
We provide a detailed discussion on several surrogate objectives in Appendix.~\ref{app:obj}.

The above  property of discriminator in WGANs can be interpreted as that given a sample  from generated distribution  we can obtain a corresponding  in data distribution  by directly conducting gradient decent with the optimal discriminator :

It seems to be a simple and appealing solution to improve  with the guidance of discriminator . However, the following issues exist: 

1) there is no theoretical indication on how to set  for each sample  in generated distribution. We noticed that a concurrent work~\cite{tanaka2019discriminator} introduce a search process called Discriminator Optimal Transport(DOT) by finding the corresponding  through the following:

However, it should be noticed that Eq.~\ref{dot} has a non-unique solution. As indicated by Theorem~\ref{gradient_direction}, all points on the connection between  and  are valid solutions. We further extend the fact into the following theorem:
\begin{theorem}
\label{opt_fail}
    With the  and  as the optimal solutions of the primal problem in Eq.~\ref{wgan_p} and  Kantorovich-Rubinstein duality of Eq.~\ref{wgan_p}, the distribution  implied by the generated distribution and the discriminator  is defined as( is defined in Eq.~\ref{dot}):
    
    when , there exists infinite numbers of  with  as a special case. 
\end{theorem}
Theorem~\ref{opt_fail} provides a theoretical justification for the poor empirical performance of conducting DOT in the sample space, as shown in their paper.

2) Another problem lies in that samples distributed outside the generated distribution () are never explored during training, which results in much adversarial noise during the gradient-based search process, especially when the sample space is high dimensional such as real-world images. 

To fix the issues mentioned above in leveraging the information of discriminator in Wasserstein GANs, we propose viewing the discriminator as an energy function. With the discriminator as an energy function, the stationary distribution is unique, and Langevin dynamics can approximately conduct sampling from the stationary distribution. Due to the monotonic property of MCMC, there will not be issues like setting  in Eq.~\ref{add grad}. Besides, the second issue can also be easily solved by fine-tuning the energy spaces with contrastive divergence.  In addition to the benefits illustrated above, if the discriminator is an energy function, the samples from the corresponding energy-based model can be obtained through Langevin dynamics by using the gradients of the discriminator which takes advantage of the  property of discriminator as shown in Theorem~\ref{gradient_direction}. With all the facts as mentioned above, there is strong motivation to explore further and bridge the gap between discriminator in WGAN and the energy-based model. 

\subsection{Semi-Amortized Generation with Langevin Dynamics}
\label{sec::energyofcritic}
We first introduce the Fenchel dual of the intractable partition function  in Eq.~\ref{partition_function}:
\begin{theorem}\cite{wainwright2008graphical}
    With , the Fenchel dual of log-partition  is as follows:
    
    where  denotes the space of distributions, and .
\end{theorem}
We put the Fenchel dual of  back into the MLE objective in Eq.~\ref{mle}, we achieve the following min-max game formalization for training energy-based model based on MLE:


\begin{algorithm}[t]
  \caption{Discriminator Contrastive Divergence}
  \label{DCD}
  \begin{algorithmic}[1]
  \STATE {\bfseries Input:} Pretrained generator , discriminator .
  \STATE Set the step size , the length of MCMC steps  and the total iterations .
  \FOR{iteration }
  \STATE Sample a batch of data samples  for empirical data distribution  and  for the prior distribution .\\
   \FOR{iteration }
     \STATE \textbf{Pixel Space:}  \OR
     \STATE \textbf{Latent Space:} 
   \ENDFOR
   \STATE Optimized the following objective w.r.t. :
   \STATE \textbf{Pixel Space:}  \OR
   \STATE \textbf{Latent Space}: 
  \ENDFOR
\end{algorithmic}
\end{algorithm}

The Fenchel dual view of MLE training in the energy-based model explicitly illustrates the gap and connection between the WGAN and Energy based model. If we consider the dual distribution  as the generated distribution , and the  as the energy function . The duality form for training energy-based models is essentially the WGAN's objective with the entropy of the generator is regularized.

Hence to turn the discriminator in WGAN into an energy function, we may conduct several fine-tuning steps, as illustrated in Eq.~\ref{fenchel_mle}. Note that maximizing the entropy of the  is indeed a challenging task, which needs to either use a tractable density generator, \emph{e.g.}, normalizing Flows~\cite{realnvp}, or maximize the mutual information between the latent variable  and the corresponding  when the  is a deterministic mapping. However, instead of maximizing the entropy of the generated distribution  directly, we derive our method based on the following fact: 
\begin{proposition}\cite{kim2016deep}\label{prop1}
    Update the generated distribution  according to the gradient estimated through Equation.~\ref{fenchel_mle}, essentially minimized the Kullback–Leibler (KL) divergence between  and the distribution , which refers to the distribution implied by using  as the energy function, as illustrated in Eq.~\ref{partition_function}, \emph{i.e.} .
\end{proposition}

To avoid the computation of , motivated by the monotonic property of MCMC, as illustrated in Eq.~\ref{MCMC_monotic}, we propose Discriminator Contrastive Divergence (DCD), which replaces the gradient-based optimization on () in Eq.~\ref{fenchel_mle} with several steps of MCMC for finetuning the critic in WGAN into an energy function. To be more specific, we use Langevin dynamics\cite{teh2003energy} which leverages the gradient of the discriminator to conduct sampling:

Where  refers to the step size. The whole finetuning procedure is illustrated in Algorithm~\ref{DCD}. 
The GAN-based approaches are implicitly constrained by the  dimension of the latent noise, which is based on a widely applied assumption that the high dimensional data, \emph{e.g.}, images, actually distribute on a relatively low-dimensional manifold. Apart from searching the reasonable point in the data space, we could also find the lower energy part of the latent manifold by conducting Langevin dynamics in the latent space which are more stable in practice, \emph{i.e.}:


Ideally, the proposal should be accepted or rejected according to the Metropolis–Hastings algorithm:

where  refers to the proposal which is defined as:

In practice, we find the rejection steps described in Eq.~\ref{mhrej} do not boost performance. For simplicity, following \cite{song2019generative,du2019implicit}, we apply Eq.~\ref{langevin} in experiments as an approximate version. 

After fine-tuning, the discriminator function can be approximated seen as an unnormalized probability function, which implies a unique distribution . And similar to the  implied in the rejection sampling-based method, it is reasonable to assume that  is a superior distribution of . Sampling from  can be implemented through the Langevin dynamics, as illustrated in Eq.~\ref{langevin} with  serves as the initial distribution.  


\section{Experiments}

\begin{figure*}[!t]
	\centering
	\begin{subfigure}{0.32\linewidth}
    	\includegraphics[width=1.0\columnwidth]{8.pdf} 
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
		\includegraphics[width=1.0\columnwidth]{frame_original708-crop.pdf}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
		\includegraphics[width=1.0\columnwidth]{frame_dynamic708-crop.pdf}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
		\includegraphics[width=1.0\columnwidth]{25.pdf} 
        \caption{Target distribution}
        \label{subfig:toy:true}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
		\includegraphics[width=1.0\columnwidth]{frame_original200-crop.pdf}
        \caption{SNGAN}
        \label{subfig:toy:sngan}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
		\includegraphics[width=1.0\columnwidth]{frame_dynamic400-crop.pdf}
        \caption{SNGAN-DCD}
        \label{subfig:toy:sngan-dcd}
    \end{subfigure}
    \caption{Density modeling on synthetic distributions. \textbf{Top}: 8 Gaussian distribution. \textbf{Bottom}: 25 Gaussian distribution. \textbf{Left}: Distribution of real data. \textbf{Middle}: Distribution defined by the generator of SNGAN. The surface is the level set of the critic. Yellow corresponds to higher value while purple corresponds to lower. \textbf{Right:} Distribution defined by the SNGAN-DCD. The surface is the level set of the proposed energy function.}
    \vspace{-5pt}
    \label{fig:toy}
\end{figure*}

In this section, we conduct extensive experiments on both synthetic data and real-world images to demonstrate the effectiveness of our proposed method. The results show that taking the optionally fine-tuned Discriminator as the energy function and sampling from the corresponding  yield stable improvement over the WGAN implementations.  


\subsection{Synthetic Density Modeling}
Displaying the level sets is a meaningful way to study learned critic. Following the \cite{azadi2018discriminator,gulrajani2017improved}, we investigate the impacts of our method on two challenging low-dimensional synthetic settings: twenty-five isotropic Gaussian distributions arranged in a grid and eight Gaussian distributions arranged in a ring (Fig.~\ref{subfig:toy:true}). For all different settings, both the generator and the discriminator of the WGAN model are implemented as neural networks with four fully connected layers and Relu activations. The Lipschitz constraint is restricted through spectral normalization~\cite{miyato2018spectral}, while the prior is a two-dimensional multivariate Gaussian with a mean of  and a standard deviation of .


To investigate whether the proposed Discriminator Contrastive Divergence is capable of tuning the distribution induced by the discriminator as desired energy function, \emph{i.e.} , we visualize both the value surface of the critic and the samples obtained from  with Langevin dynamics. The results are shown in Figure.~\ref{fig:toy}. As can be observed, the original WGAN (Fig.~\ref{subfig:toy:sngan}) is strong enough to cover most modes, but there are still some spurious links between two different modes. The enhanced distribution  (Fig.~\ref{subfig:toy:sngan-dcd}), however, has the ability to reduce spurious links and recovers the modes with underestimated density. More precisely, after the MCMC fine-tuning procedure (Fig.~\ref{subfig:toy:sngan-dcd}), the gradients of the value surface become more meaningful so that all the regions with high density in data distribution  are assigned with high  value, \emph{i.e.}, lower energy(). By contrast, in the original discriminator (Fig.~\ref{subfig:toy:sngan}), the lower energy regions in  are not necessarily consistent with the high-density region of .

\subsection{Real-World Image Generation}

To quantitatively and empirically study the proposed DCD approach, in this section, we conduct experiments on unsupervised real-world image generation with DCD and its related counterparts. On several commonly used image datasets, experiments demonstrate that our proposed DCD algorithm can always achieve better performance on different benchmarks with a significant margin.

\subsubsection{Experimental setup}

\textbf{Baselines.} 
We evaluated the following models as our baselines: we take PixelCNN~\cite{van2016conditional}, PixelIQN~\cite{ostrovski2018autoregressive}, and MoLM~\cite{ravuri2018learning} as representatives of other types of generative models. For the energy-based model, we compared the proposed method with EBM~\cite{du2019implicit} and NCSN~\cite{song2019generative}. For GAN models, we take WGAN-GP~\cite{gulrajani2017improved}, Spectral Normalization GAN (SNGAN)~\cite{miyato2018spectral}, and Progressiv eGAN~\cite{karras2017progressive} for comparison. We also take the aforementioned DRS~\cite{azadi2018discriminator}, DOT~\cite{tanaka2019discriminator} and MH-GAN~\cite{turner2018metropolis} into consideration. The choices of EBM and GANs are due to their close relation to our proposed method, as analyzed in Section \ref{sec:methodology}. We omit other previous GAN methods since as a representative of a state-of-the-art GAN model, SNGAN and Progressive GAN has been shown to rival or outperform several former methods such as the original GAN \cite{goodfellow2014generative}, the energy-based generative adversarial network \cite{zhao2016energy}, and the original WGAN with weight clipping \cite{arjovsky2017wasserstein}.

\textbf{Evaluation Metrics.}
For evaluation, we concentrate on comparing the quality of generated images since it is well known that GAN models cannot perform reliable likelihood estimations \cite{theis2015note}. We choose to compare the Inception Scores \cite{salimans2016improved} and Frechet Inception Distances (FID) \cite{heusel2017gans} reached during training iterations, both computed from 50K samples. A high image quality corresponds to high Inception and low FID scores. 
Specifically, the intuition of IS is that high-quality images should lead to high confidence in classification, while FID aims to measure the computer-vision-specific similarity of generated images to real ones through Frechet distance.

\textbf{Data.}
We use CIFAR-10 \cite{krizhevsky2009learning} and STL-10 \cite{coates2011analysis}, which are all standard datasets widely used in generative literature. STL-10 consists of
unlabeled real-world color images, while CIFAR-10 is provided with class labels, which enables us to conduct conditional generation tasks. For STL-10, we also shrink the images into  as in previous works. The pixel values of all images are rescaled into .


\textbf{Network Architecture.}
For all experiment settings, we follow Spectral Normalization GAN (SNGAN) \cite{miyato2018spectral} and adopt the same Residual Network (ResNet) \cite{he2016deep} structures and hyperparameters, which presently is the state-of-the-art implementation of WGAN. Details can be found in Appendix.~\ref{app:sec:network-arch}. We take their open-source code and pre-trained model as the base model for the experiments on CIFAR-10. For STL-10, since there is no pre-trained model available to reproduce the results, we train the SNGAN from scratch and take it as the base model.

\begin{figure*}[t]
\begin{minipage}{0.55\textwidth}
\begin{center}
\begin{adjustbox}{max width=.9\linewidth}
\begin{tabular}{lcc}
        \toprule
        Model & Inception & FID\\
        \midrule
        \multicolumn{3}{l}{\textbf{CIFAR-10 Unconditional}} \\
        \midrule
        PixelCNN~\cite{van2016conditional} &  & \\
        PixelIQN~\cite{ostrovski2018autoregressive} &  & \\
        EBM~\cite{du2019implicit} &  &  \\
        WGAN-GP~\cite{gulrajani2017improved} &  & \\
        MoLM~\cite{ravuri2018learning} &  & \\
        SNGAN~\cite{miyato2018spectral} &  &  \\
        ProgressiveGAN~\cite{karras2017progressive} &  & - \\
        NCSN~\cite{song2019generative} & {} & \\
        \midrule
DCGAN w/ DRS(cal)~\cite{azadi2018discriminator} &  & - \\
        DCGAN w/ MH-GAN(cal)~\cite{turner2018metropolis} &  & - \\
ResNet-SAGAN w/ DOT~\cite{tanaka2019discriminator} &  & \\
        \midrule
        \textbf{SNGAN-DCD (Pixel)} & {} & \\
        \textbf{SNGAN-DCD (Latent)} & {} & \\
        \bottomrule
        \toprule
        \multicolumn{3}{l}{\textbf{CIFAR-10 Conditional}} \\
        \midrule
        EBM~\cite{du2019implicit} &  &  \\
        SNGAN~\cite{miyato2018spectral} &  & \\
        \textbf{SNGAN-DCD (Pixel)} & {} & \\
        \textbf{SNGAN-DCD (Latent)} & {} & \\
        BigGAN~\cite{brock2018large} &  & \\
        \bottomrule
    \end{tabular}
\end{adjustbox}
\end{center}
\captionof{table}{Inception and FID scores for CIFAR-10.} \label{tab:score-cifar}
\end{minipage}
\hspace{+5pt}
\begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=0.99\columnwidth]{cifar-10.png}
\caption{Unconditional CIFAR-10 Langevin dynamics visualization.}
    \label{fig:mcmc-cifar}
\end{minipage}
\end{figure*}

\subsubsection{Results}

\begin{wraptable}{r}{0.5\textwidth}
\vspace{-10pt}
\begin{center}
\begin{adjustbox}{max width=1.2\linewidth}
\begin{tabular}{lcc}
        \toprule
        Model & Inception & FID\\
        \midrule
        SNGAN~\cite{miyato2018spectral} &  &  \\
        \textbf{SNGAN-DCD (Pixel)} & {} & \\
        \textbf{SNGAN-DCD (Latent)} & {} & \\
        \bottomrule
    \end{tabular} 
\end{adjustbox}
\end{center}
\caption{Inception and FID scores for STL-10} \label{tab:score-stl}
\end{wraptable}

For quantitative evaluation, we report the inception score~\cite{salimans2016improved} and FID \cite{heusel2017gans} scores on CIFAR-10 in Tab.~\ref{tab:score-cifar} and STL-10 in Tab.~\ref{tab:score-stl}. As shown in the Tab.~\ref{tab:score-cifar}, in pixel space, by introducing the proposed DCD algorithm, we achieve a significant improvement of inception score over the SNGAN. The reported inception score is even higher than most values achieved by class-conditional generative models. Our FID score of  on CIFAR-10 is competitive with other top generative models. When the DCD is conducted in the latent space, we further achieve a  inception score and a  FID, which is a new state-of-the-art performance of IS. When combined with label information to perform conditional generation, we further improve the FID to , which is comparable with current state-of-the-art large-scale trained models~\cite{brock2018large}. Some visualization of generated examples can be found in Fig~\ref{fig:mcmc-cifar}, which demonstrates that the Markov chain is able to generate more realistic samples, suggesting that the MCMC process is meaningful and effective. Tab.~\ref{tab:score-stl} shows the performance on STL-10, which demonstrates that as a generalized method, DCD is not over-fitted to the specific data CIFAR-10. More experiment details and the generated samples of STL-10 can be found in Appendix.~\ref{app:sec:mcmc-stl}.

\section{Discussion and Future Work}
Based on the density ratio estimation perspective, the discriminator in -GANs could be adapted to a wide range of application scenarios, such as mutual information estimation~\cite{hjelm2018learning} and bias correction of generative models~\cite{grover2019bias}.  However, as another important branch in GANs' research, the available information in WGANs discriminator is less explored. In this paper, we narrow down the scope of discussion and focus on the problem of how to leverage the discriminator of WGANs to further improve the sample quality in image generation. We conduct a comprehensive theoretical study on the informativeness of discriminator in different kinds of GANs. Motivated by the theoretical progress in the literature of WGANs, we investigate the possibility of turning the discriminator of WGANs into an energy function and propose a fine-tuning procedure of WGANs named as "discriminator contrastive divergence". The final image generation process is semi-amortized, where the generator acts as an initial state, and then several steps of Langevin dynamics are conducted.  We demonstrate the effectiveness of the proposed method on several tasks, including both synthetic and real-world image generation benchmarks. 

It should be noted that the semi-amortized generation allows a trade-off between the generation quality and sampling speed, which holds a slower sampling speed than a direct generation with a generator. Hence the proposed method is suitable to the application scenario where the generation quality is given vital importance.  Another interesting observation during the experiments is the discriminator contrastive divergence surprisingly reduces the occurrence of adversarial samples during training, so it should be a promising future direction to investigate the relationship between our method and bayesian adversarial learning. 

We hope our work helps shed some light on a generalized view to a method of connecting different GANs and energy-based models, which will stimulate more exploration into the potential of current deep generative models.


\newpage

\bibliography{99_bibliography}
\bibliographystyle{plainnat}


\onecolumn
\appendix
\section{Proof of Theorem~\ref{gradient_direction}}\label{proof1}
It should be noticed that Theorem.~\ref{gradient_direction} can be generalized to that Lipschitz continuity with -norm (Euclidean Distance) can guarantee that the gradient is directly pointing towards some sample\cite{zhou2019lipschitz}. We introduce the following lemmas, and Theorem.~\ref{gradient_direction} is a special case.  

Let  be such that , and we define  with . 

\begin{lemma}
If  is -Lipschitz with respect to  and , then .
\end{lemma} 

\begin{proof}
As we know  is -Lipschitz, with the property of norms, we have

 implies all the inequalities is equalities. Therefore, . \qedhere
\end{proof}

\begin{lemma}
Let  be the unit vector . If , then  equals to . 
\end{lemma}

\begin{proof}

\end{proof}
Then we derive the formal proof of Theorem~\ref{gradient_direction}. 

\begin{proof}  
Assume , if  is -Lipschitz with respect to  and  is differentiable at , then .  Let  be the unit vector . We have 

Because the equality holds only when , we have that .
\end{proof}



\section{Proof of Theorem~\ref{opt_fail}}\label{proof2}
Theorem.~\ref{opt_fail} states that following the following procedure as introduced in \cite{tanaka2019discriminator}, there is non-unique stationary distribution. The complete procedure is to find the following  for :

To find the corresponding , the following gradient based update is conducted:

For all the points  in the linear interpolation of  and target  as defined in the proof of Theorem~\ref{gradient_direction},

which indicates all points in the linear interpolation  satisfy the stationary condition.



\section{Proof of Proposition~\ref{prop1}}\label{proof3}

Proposition.~\ref{prop1} is the direct result of the following Lemma.~\ref{lem:dd}. Following \cite{li2017approximate}, we provide the complete proof as following.

\begin{lemma}\label{lem:dd}
\cite{cover2012elements}
Let  and  be two distributions for . Let  and  be the corresponded distributions of state  at time , induced by the transition kernel . Then  for all .
\end{lemma}
\begin{proof}

\end{proof}



\section{Network architectures}
\label{app:sec:network-arch}

ResNet architectures for CIFAR-10 and STL-10 datasets. We use similar architectures to the ones used in~\cite{gulrajani2017improved}.

\begin{table}[!h]
\centering
\begin{tabular}{c}
    \toprule
    \midrule
     \\
    \midrule
    dense,  \\
    \midrule
    ResBlock up 256\\
    \midrule
    ResBlock up 256\\
    \midrule
    ResBlock up 256\\
    \midrule
    BN, ReLU, 33 conv, 3 Tanh\\
    \midrule
    \bottomrule
\end{tabular}
\vspace{+5pt}
\caption{Generator}
\end{table}

\begin{table}[!h]
\centering
\begin{tabular}{c}
    \toprule
    \midrule
    RGB image  \\
    \midrule
    ResBlock down 128\\
    \midrule
    ResBlock down 128\\
    \midrule
    ResBlock 128\\
    \midrule
    ResBlock 128\\
    \midrule
    ReLU\\
    \midrule
    Global sum pooling\\
    \midrule
    dense  1\\
    \midrule
    \bottomrule
\end{tabular}
\vspace{+5pt}
\caption{Discriminator}
\end{table}



\section{Discussions on Objective Functions}
\label{app:obj}
Optimization of the standard objective of WGAN, \emph{i.e.} with  in Eq.~\ref{gan_obj}, are found to be unstable  due to the numerical issues and free offset~\cite{zhou2019lipschitz,miyato2018spectral}. Instead, several surrogate losses are actually used in practice. For example, the logistic loss() and hinge loss() are two widely applied objectives. Such surrogate losses are valid due to that they are actually the lower bounds of the Wasserstain distance between the two distributions of interest. The statement can be easily derived by the fact that  and . A more detailed discussion could also be found in \cite{tanaka2019discriminator}.

Note that  and  are in the function family proposed in \cite{zhou2019lipschitz}, and Theorem 4 in \cite{zhou2019lipschitz} guarantees the gradient property of discriminator. 



\section{More Experiment Details}
\label{app:sec:mcmc-stl}

\subsection{CIFAR-10}
For the meta-parameters in DCD Algorithm~\ref{DCD}, when the MCMC process is conducted in the pixel space, we choose  as the number of MCMC steps , and set the step size  as  and the standard deviation of the Gaussian noise as , while for the latent space we set  as ,  as  and the deviation as . Adam optimizer \cite{kingma2014adam} is set with  learning rate with . We use  critic updates per generator update, and a batch size of .

\subsection{STL-10}
We show generated samples of DCD during Langevin dynamics in Fig.~\ref{app:fig:stl}. We run 150 steps of MCMC steps
and plot generated sample for every 10 iterations. The step size is set as  and the noise is set as .

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\columnwidth]{mcmc.png}
    \caption{STL-10 Langevin dynamics visualization.}
    \label{app:fig:stl}
\end{figure}
 
\end{document}
