\documentclass[sigconf,screen,nonacm]{acmart}


\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{verbatim}
\usepackage{hyperref}

\renewcommand{\algorithmicrequire}{ \textbf{Input:}} 
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\copyrightyear{2022}
\acmYear{2022}
\setcopyright{acmcopyright}
\acmConference[MM '22]{Proceedings of the 30th ACM International Conference on Multimedia}{October 10--14, 2022}{Lisboa, Portugal}
\acmBooktitle{Proceedings of the 30th ACM International Conference on Multimedia (MM '22), October 10--14, 2022, Lisboa, Portugal}
\acmPrice{15.00}
\acmDOI{XXXXXXX.XXXXXXX}
\acmISBN{978-1-4503-XXXX-X/18/06}









\begin{document}
	
\title{Diverse Human Motion Prediction via Gumbel-Softmax Sampling from an Auxiliary Space}
	


	\author{Lingwei Dang}
	\affiliation{\institution{South China University of Technology}
		\city{Guangzhou}
		\state{Guangdong}
		\country{China}}
	\email{csdanglw@mail.scut.edu.cn}


	\author{Yongwei Nie}
	\authornote{Corresponding author.}
	\affiliation{\institution{South China University of Technology}
		\city{Guangzhou}
		\state{Guangdong}
		\country{China}}
	\email{nieyongwei@scut.edu.cn}
	
	\author{Chengjiang Long}
	\affiliation{\institution{Meta Reality Lab}
		\city{Burlingame}
		\state{CA}
		\country{USA}}
	\email{clong1@fb.com}
	
	\author{Qing Zhang}
	\affiliation{\institution{Sun Yat-sen University}
		\city{Guangzhou}
		\state{Guangdong}
		\country{China}}
	\email{zhangqing.whu.cs@gmail.com}
	
	\author{Guiqing Li}
	\affiliation{\institution{South China University of Technology}
		\city{Guangzhou}
		\state{Guangdong}
		\country{China}}
	\email{ligq@scut.edu.cn}


\begin{abstract}
		Diverse human motion prediction aims at predicting multiple possible future pose sequences from a sequence of observed poses. Previous approaches usually employ deep generative networks to model the conditional distribution of data, and then randomly sample outcomes from the distribution. While different results can be obtained, they are usually the most likely ones which are not diverse enough. Recent work explicitly learns multiple modes of the conditional distribution via a deterministic network, which however can only cover a fixed number of modes within a limited range. In this paper, we propose a novel sampling strategy for sampling very diverse results from an imbalanced multimodal distribution learned by a deep generative model. Our method works by generating an auxiliary space and smartly making randomly sampling from the auxiliary space equivalent to the diverse sampling from the target distribution. We propose a simple yet effective network architecture that implements this novel sampling strategy, which incorporates a Gumbel-Softmax coefficient matrix sampling method and an aggressive diversity promoting hinge loss function. Extensive experiments demonstrate that our method significantly improves both the diversity and accuracy of the samplings compared with previous state-of-the-art sampling approaches. Code and pre-trained models are available at \href{https://github.com/Droliven/diverse_sampling}{https://github.com/Droliven/diverse\_sampling}.
	\end{abstract}
	


	\begin{CCSXML}
		<ccs2012>
		<concept>
		<concept_id>10010147.10010178.10010224.10010225.10010228</concept_id>
		<concept_desc>Computing methodologies~Activity recognition and understanding</concept_desc>
		<concept_significance>500</concept_significance>
		</concept>
		</ccs2012>
	\end{CCSXML}
	
	\ccsdesc[500]{Computing methodologies~Activity recognition and understanding}
	


\keywords{Human motion prediction, stochastic prediction, diverse prediction}
	
	
\begin{teaserfigure}
		\centering
		\includegraphics[width=1.0\textwidth]{figs/teaser.pdf} \caption{Different strategies for sampling diverse results from an imbalanced multimodal distribution. The vanilla CVAE model randomly samples latent codes from a prior distribution which are then decoded into results that only reside in the major mode of the target distribution. DLow~\cite{yuan2020dlow} first generates multiple Gaussian distributions, and then samples latent codes from different Gaussian priors. The Gaussian priors can be viewed as corresponding to different modes of the target distribution, therefore this method can cover more modes than random sampling. Our method generates multiple Gaussian distributions by sampling points from an auxiliary space. Due to the high flexibility and capacity of the space, our method is able to cover even more modes than DLow. The rightmost are the last poses of future pose sequences predicted from a given input, all stacked together to visually show that our results are more diverse than the others.}
\label{fig:teaser}
	\end{teaserfigure}
	
	
	\maketitle
	
	\section{Introduction}
	Human Motion Prediction (HMP) has a wide range of applications in autonomous driving, human-robot interaction, and animation creation. Most previous works \cite{martinez2017human, song2017end, sang2020human, chiu2019action, corona2020context, aksan2019structured, li2017auto, fragkiadaki2015recurrent, liu2022investigating,mao2019learning, mao2020history, li2020dynamic, li2021symbiotic, cui2020learning, dang2021msr, cui2021towards, li2020multitask, liu2021motion, aksan2021spatio, martinez2021pose, aksan2020attention, cai2020learning, su2021motion, Ma_2022_CVPR} perform deterministic HMP that only generates one result in the future. Recently, many diverse HMP approaches can predict multiple possible future motions. Due to the stochasticity of human motion, multiple solutions naturally exist, and forecasting them is of great importance in practice. For example, it would be better for a vehicle to know that a pedestrian in front of it may not only walk ahead but also turn left suddenly. 
	
	Diverse HMP approaches like \cite{barsoum2018hp,kundu2019bihmp,yan2018mt} adopt deep generative networks, such as GAN~\cite{goodfellow2014generative} or CVAE~\cite{kingma2013auto}, to learn a conditional distribution of future poses given previous ones. Taking CVAE as an example (see Figure~\ref{fig:teaser} top), after training a CVAE, one can randomly sample latent codes (noises) from a prior distribution (\textit{e.g.}, a Gaussian prior), and then decode the random noises to future sequences by the CVAE decoder. However, since the CVAE model is obtained by maximizing the likelihood of the training data that is often highly imbalanced, it usually learns an imbalanced multimodal conditional distribution. Latent codes drawn at random from the prior distribution most probably correspond to the most likely results that fall in the dominant mode of the distribution of data, while ignoring other results of low probability but high fidelity.
	
	Recently, Yuan \textit{et al.} \cite{yuan2020dlow} proposed a method called DLow sampling. As shown in the second row of Figure~\ref{fig:teaser}, given observed poses, DLow uses a neural network to generate multiple Gaussian distributions, and then samples latent codes from all the generated Gaussian distributions. They optimize the network to diversify the Gaussian distributions, making them corresponding to different modes of the target distribution. However, directly generating Gaussian distributions has two limitations. Firstly, a network can only generate a fixed number of Gaussian distributions, while there may exist much more modes in the target data distribution. Secondly, it entangles the performance of diverse prediction with the learning of the network parameters, requiring the latter to consider all training data and make tradeoffs between them, thus in turn limiting the diverse prediction performance.
	
	In this paper, we propose a sampling strategy that disentangles the above direct dependency between a network and the intermediate Gaussian distributions. As shown in the third row of Figure~\ref{fig:teaser}, instead of Gaussian distributions, we learn a set of basis vectors from the observed poses. We assume that the basis vectors determine an auxiliary space, and any linear combination of the basis vectors corresponds to a point in the auxiliary space. We randomly sample a set of points from the auxiliary space by the Gumbel-Softmax sampling strategy, and then map them to Gaussian distributions which finally correspond to different modes of the target distribution. In other words, we use a network to learn an auxiliary space, and build the following connection between the auxiliary space and the target distribution: \textit{randomly sampling from the auxiliary space corresponds to diverse sampling from the target distribution}. The diverse prediction is now tied to the structure of the auxiliary space rather than directly to the parameters of a network. Since the auxiliary space can be flexibly deformed in terms of both size and shape, our sampling method can cover all modes of the target distribution in theory, supporting very diverse human motion prediction. Note that at the training stage, we sample a fixed number of points from the auxiliary space, which facilitates the training of the auxiliary space. After training, since the shape of the auxiliary space has already been constructed, we can sample any number of points from it.
	
	The Gumbel-Softmax sampling method samples points from the auxiliary space by generating a coefficient matrix that linearly combines the basis vectors. There exist other sampling strategies such as Uniform-Softmax sampling and Gaussian-Softmax sampling. We compare with them and find that the Gumbel-Softmax sampling is more effective as it is more aggressive in assigning larger weights to relatively fewer basis vectors. Training/testing with these weights can make better use of each basis vector to sample more distinctive points from the auxiliary space that correspond to more diverse modes of the target distribution. Finally, In order to train our model, we propose a hinge-diversity loss function which explicitly requires the distance between any pair of predictions to be greater than a user-specified threshold. The hinge-diversity loss further strengthens the diversity of predictions while less affecting their accuracy.
	
	In summary, the contributions of this work are three-fold:
	\begin{itemize}
		\item We propose a novel sampling method that is highly capable and convenient for diverse and accurate sampling from a complex imbalanced multimodal distribution, by converting sampling from the distribution into randomly sampling of points from an auxiliary space.
		\item We propose a Gumbel-Softmax sampling method to sample points from the auxiliary space, and a hinge-diversity loss to train our framework, both of which further improve the performance of our method.
		\item Extensive comparisons and ablation experimental results conducted on Human3.6M \cite{ionescu2013human3} and HumanEva-I \cite{sigal2010humaneva} demonstrate the effectiveness of our approach.
	\end{itemize}
	
	
	\section{Related Work}
	
	\textbf{Deterministic Human Motion Prediction.} Most previous approaches target deterministic HMP, by which only one output is produced per sequence of historical poses. Considering the ability of Recurrent Neural Networks (RNNs) in modeling temporal dependencies of sequential data, many approaches \cite{martinez2017human, song2017end, sang2020human, chiu2019action, corona2020context, aksan2019structured, li2017auto, fragkiadaki2015recurrent, liu2022investigating} use RNNs to tackle the sequence-to-sequence HMP problem, which however usually suffer from problems of discontinuity and error accumulation. Instead of RNNs, recent works \cite{mao2019learning, mao2020history, li2020dynamic, li2021symbiotic, cui2020learning, dang2021msr, cui2021towards, li2020multitask, liu2021motion, mao2021multi, Ma_2022_CVPR} employ Graph Convolutional Networks (GCNs)~\cite{Shi:CVPR2021, Duan:AAAI2022, Shi:AAAI2022} for this task, as GCNs are effective in discovering spatial and temporal relations between pairs of human joints. Similar to GCNs, Transformer \cite{vaswani2017attention, Dong:MM2021} can capture long-term dependencies between human joints, and has been adapted to handle the deterministic HMP problem \cite{cai2020learning, aksan2021spatio, martinez2021pose, aksan2020attention}. Different from the above methods, this paper attempts to tackle stochastic HMP which outputs multiple possible results given one input.

	\textbf{Diverse Human Motion Prediction.} Many efforts have been paid to the diverse HMP problem \cite{barsoum2018hp,yan2018mt,kundu2019bihmp,yuan2020dlow,aliakbarian2020stochastic,liu2021aggregated,tanke2021intention,lyu2021learning,mao2021generating,aliakbarian2021contextually,cai2021unified}. For example, Barsoum \textit{et al.} \cite{barsoum2018hp} proposed HP-GAN which is a generative adversarial framework that models the probability density function of future human poses conditioned on given poses. At test time, a random vector  controls the generation of different future poses. Yan \textit{et al.} \cite{yan2018mt} proposed MT-VAE using VAE \cite{kingma2013auto, Liu:ICCV2021} to model the conditional distribution of data. In MT-VAE, a random variable  encodes a latent transformation that transforms the observed poses to specific future poses. Both GANs and VAEs randomly sample latent vectors  from a prior distribution which however are usually decoded into similar results. To alleviate the problem, Kundu \textit{et al.} \cite{kundu2019bihmp} proposed BiHMP-GAN in which a discriminator is used to regress the random vector  originally fed into the generator, enforcing one-to-one mapping between the latent vector  and the corresponding motion prediction. Aliakbarian \textit{et al.} \cite{aliakbarian2020stochastic} believed that the generative models tend to ignore the random vectors. To prevent such ignoring, they proposed a Mix-and-Match perturbation mechanism to sufficiently mix random noises and conditional poses in \cite{aliakbarian2020stochastic}. In their later work \cite{aliakbarian2021contextually}, a random noise is generated directly conditioned on the input poses. Instead of randomly sampling , Yuan \textit{et al.} \cite{yuan2020dlow} proposed a sampling strategy called DLow by which different random vectors that correspond to diverse predictions are explicitly inferred, achieving impressive results in sampling from minor modes. However, DLow is limited by its design of inferring random vectors directly from a network. Our method disentangles this dependency and obtains more diverse results with higher accuracy. Recently, the method of \cite{mao2021generating} directly maps a random vector together with the observed poses to a future sequence, without relying on generative models. For results of different random vectors but the same input poses, it applies a diversity loss to enlarge the differences between them, and meanwhile uses many prior constraints to guarantee their plausibility. We compare with this very different method and show that our method outperforms it on diversity and accuracy metrics.
	






	\section{Methodology}
	\begin{figure*}[!t]
		\centering
		\includegraphics[width=1.0\linewidth]{figs/pipeline.pdf}
		\caption{On one hand, we use a network  to generate a base matrix from the observed poses. On the other hand, we employ the Gumbel-Softmax sampling method to generate a coefficient matrix. The multiplication of the two matrices samples multiple points from the auxiliary space determined by the base matrix. We then employ another network  to map these points to a set of Gaussian distributions from which latent codes are drawn and finally decoded into future pose sequences.}
		\label{fig:pipeline}
	\end{figure*}
	
	We use CVAE to model the distribution of data, then propose a post-hoc sampling strategy to sample diverse results from the distribution. We therefore first introduce the background of CVAE-based stochastic HMP. Since our method is based on DLow~\cite{yuan2020dlow}, we also briefly introduce DLow, and finally describe our method in detail.
	
	\subsection{Background}
	
	\subsubsection{CVAE-based Stochastic Prediction}
	Let  denote the distribution of  given , where  is an observed pose sequence and  is a possible future pose sequence that may appear after . To sample  from , one usually introduces a latent variable  and reparameterizes  as . Then,  can be generated in two steps:
	
	where a random vector  is sampled from a prior distribution  (\textit{e.g.}, Gaussian) at first, then  is generated by a deterministic function  parameterized by  taking  and  as input. To learn , a popular way is to use a CVAE which maximizes the log-likelihood of data  given , by introducing an approximate posterior  and maximizing the following evidence lower bound (ELBO):
	
	CVAE models the two distributions of  and  by two neural networks  and , and estimates their parameters by optimizing the following loss function:
	
	During training, the encoder  first generates  given  and , and then the decoder  reconstructs the input  given  and . 
At test time, one can sample a  from the prior distribution , and then predict a  by  given  and . For multiple predictions, one needs to sample  independently, and predict  accordingly using the same . However, extensive experiments demonstrate that the diversity of  is not satisfactory. 

	
	\subsubsection{DLow Sampling}
	To enable diverse prediction, Yuan \textit{et al.} \cite{yuan2020dlow} proposed DLow. Given , they used a network  parameterized by  to generate  Gaussian distributions: 
	
	where ,  are variance and mean of the  Gaussian distribution, and  is the dimension size. Then, they predicted  results by:
	
	Compared with Eq.~\ref{eq:random-sampling}, the above DLow sampling method learns  Gaussian distributions and uses the reparameterization trick to sample latent variables from these distributions: , and finally maps  and the input poses  to future poses using  that has already been learned by the CVAE model.
	
	More formally, DLow samples a result  from the distribution of  where  is the conditional distribution modeled by , and , \textit{i.e.,} , is the latent distribution modeled by the network . 
	
	To train , DLow minimizes the following diversity loss to enlarge the distances between pairs of results predicted from the same input :
	
	where  or  denotes a predicted pose sequence, and  calculates the Euclidean distance between two predictions. Besides, the following accuracy loss is minimized:
	
	This loss computes  distance between every prediction  and the ground truth  and returns the minimum one. Minimizing this loss makes at least one of the predictions similar to the ground truth. Finally, a Kullback-Leibler divergence loss is imposed:
	
where  is the prior distribution used to train the CVAE. This loss makes  correspond to a high-likelihood sample  under the generative model , guaranteeing the plausibility of the predicted poses.
	
	\subsection{Our method}
	\label{sec:our-method}
	While DLow improves sampling diversity compared to the random sampling method, we observe that its effectiveness is limited in two ways. (1) Firstly, DLow entangles its prediction performance with the learning of the network . However, the network is trained on all the training data, hence its performance is inevitably averaged over all the data, reducing its ability to make extreme predictions existing at minor modes. (2) Secondly, due to the entanglement, DLow can only sample  predictions at a time. However, it is more preferable that a sampling method can sample any number of samples at test time. 

	To solve these problems, we propose a new sampling method which disentangles the direct correlation between the tasks of diverse prediction and the network parameter learning. Figure~\ref{fig:pipeline} illustrates the sampling process of our method. On one hand, we design a network  parameterized by  that takes  as input and outputs a base matrix :
	
	where each row of  is a basis vector of dimension  and there are  basis vectors in total. One can imagine that the basis vectors together form a space which we call ``auxiliary space'' in this paper. A point in the space can be obtained by linearly combining the basis vectors. On the other hand, we use the Gumbel-Softmax sampling strategy (introduced later) to sample a coefficient matrix  in which each row  () contains  weights used to combine the basis vectors. These weights should satisfy . Then, we multiply  and  together to obtain a point matrix  where each row represents a point sampled from the auxiliary space. This operator samples  points from the auxiliary space in total. Finally, we use another network  parameterized by  to further transform the  points to  and :
	
	
	Based on the above preparations and incorporating with the sampling process defined in Eq.~\ref{eq:dlow-sampling}, our sampling process is (Alg. \ref{alg:our-sampling-process}):
	
	Compared with DLow which relies on a network to directly output different Gaussian distributions, our method samples the Gaussian distributions from the auxiliary space characterized by . At the training stage, the sampling number  is fixed to train the structure of the auxiliary space and make it match with the sampling strategy (\textit{e.g.}, Gumbel-Softmax random sampling) such that the points sampled from the space by the sampling strategy can yield predictions of high diversity. At test time, since the auxiliary space and the relationship between the space and the sampling strategy has already been established, we can sample any number of points as needed. We stress that although our model is trained on all the training data, these data are used to form the shape of the auxiliary space which is flexible and adjustable to accommodate all the data.
	
	\begin{algorithm}[!t]
		\caption{Diverse sampling from a complex distribution by randomly Gumbel-Softmax sampling from an auxiliary space}\label{alg:our-sampling-process}
		\begin{algorithmic}[1]
			\Require Observed pose sequence , number of samples , auxiliary space generation network , Gaussian distribution generation network , CVAE decoder network 
			\Ensure A set of samples 
			\State  // \textit{generate an auxiliary space given input poses}
			\State  // \textit{see \rm{Algorithm} \ref{alg:gumbel-softmax}}
			\State  // \textit{Multiply  and  to obtain a point matrix }
			\State  // \textit{convert points into means and variances}
			\State  // \textit{sampling an  from the normal distribution}
			\For{ to }
			\State  // \textit{reparameterization trick}
			\State  // \textit{decode  and  into a result }
			\EndFor 
		\end{algorithmic}
	\end{algorithm}
	
	
	In the following, we detail components of our model that help shape the auxiliary space.
	
	\subsubsection{Network Architectures}
	We have two sub-networks  and . For , the input is  where  is the length of the input sequence,  is the number of joints of a pose, and each joint has  coordinates. The output is . Firstly, we use a GCN \cite{mao2019learning} to extract features in  from  where  is the dimension size of the features. Then, we use an MLP to map the feature map in  to  in . For , we employ another MLP that maps a feature map in  to a feature map in . Please refer to the supplemental material for more details of the network designs.
	
	\subsubsection{Gumbel-Softmax Sampling}
	We randomly sample a coefficient matrix  by the Gumbel-Softmax sampling method (Alg. \ref{alg:gumbel-softmax}) by which each row  () of  is calculated as:
	 
	where  is the uniform distribution,  and  are parameters of the Gumbel distribution which are set to  and 1, respectively. 

	
	
	Besides the Gumbel distribution, we can also sample from a uniform or Gaussian distribution at first and then apply the Softmax normalization to obtain a coefficient matrix. However, Gumbel-Softmax sampling is more aggressive than Uniform-Softmax and Gaussian-Softmax sampling in assigning larger weights for a few basis vectors while making other basis vectors sharing just a small portion of the weight. In other words, the Gumbel-Softmax sampling strategy can samples points more near to the basis vectors. This benefits the learning of the basis vectors, because the network only needs to diversify the basis vectors to obtain diverse Gaussian distributions and eventually generate diverse poses. Please see our ablation study of comparisons among them.
	


	
	
	\begin{algorithm}[!t]
		\caption{Gumbel-Softmax coefficient matrix generation}\label{alg:gumbel-softmax}
		\begin{algorithmic}[1]
			\Require Number of coefficient vectors , dimension size  of a coefficient vector, Gumbel distribution parameters  and 
			\Ensure A coefficient matrix 
			\State Declare a matrix 
			\For{ to }
			\For{ to }
			\State  // \textit{sample a value from uniform distribution}
			\State 
			\State 
			\EndFor
			\State  // \textit{normalize the  row of }
			\EndFor
		\end{algorithmic}
	\end{algorithm}
	
	
	\begin{table*}[!t]
		\caption{Quantitative comparisons. All the results are calculated by sampling 50 times for each input historical pose sequence. The best results are marked in bold.}
		\label{tab:main_result}
		\resizebox{1\textwidth}{!}{
			\begin{tabular}{c|c|ccccc|ccccc}
				\toprule
				& \multirow{2}{*}{Method}   & \multicolumn{5}{c|}{Human3.6M \cite{ionescu2013human3}}         & \multicolumn{5}{c}{HumanEva-I \cite{sigal2010humaneva}}        \\ \cline{3-12} 
				&                           & APD    & ADE   & FDE   & MMADE  & MMFDE  & APD   & ADE   & FDE   & MMADE  & MMFDE  \\ \hline
\multirow{2}{*}{deterministic} & LTD \cite{mao2019learning}  & 0.000  & 0.516 & 0.756 & 0.627 & 0.795 & 0.000 & 0.415 & 0.555 & 0.509 & 0.613 \\
				& MSR \cite{dang2021msr}                  & 0.000  & 0.508 & 0.742 & 0.621 & 0.791 & 0.000 & 0.371 & 0.493 & 0.472 & 0.548 \\ \hline
				\multirow{12}{*}{stochastic}   & Pose-Knows \cite{walker2017pose}              & 6.723  & 0.461 & 0.560 & 0.522 & 0.569 & 2.308 & 0.269 & 0.296 & 0.384 & 0.375 \\
				&MT-VAE \cite{yan2018mt}                  & 0.403  & 0.457 & 0.595 & 0.716 & 0.883 & 0.021 & 0.345 & 0.403 & 0.518 & 0.577 \\
				&HP-GAN \cite{barsoum2018hp}                  & 7.214  & 0.858 & 0.867 & 0.847 & 0.858 & 1.139 & 0.772 & 0.749 & 0.776 & 0.769 \\ 
				&BoM \cite{bhattacharyya2018accurate}                     & 6.265  & 0.448 & 0.533 & 0.514 & 0.544 & 2.846 & 0.271 & 0.279 & 0.373 & 0.351 \\
				&GMVAE \cite{dilokthanakul2016deep}                   & 6.769  & 0.461 & 0.555 & 0.524 & 0.566 & 2.443 & 0.305 & 0.345 & 0.408 & 0.410 \\
				&DeLiGAN \cite{gurumurthy2017deligan}                 & 6.509  & 0.483 & 0.534 & 0.520 & 0.545 & 2.177 & 0.306 & 0.322 & 0.385 & 0.371 \\
				&DSF \cite{yuan2019diverse}                     & 9.330  & 0.493 & 0.592 & 0.550 & 0.599 & 4.538 & 0.273 & 0.290 & 0.364 & 0.340 \\
				&DLow \cite{yuan2020dlow}                   & 11.741 & 0.425 & 0.518 & 0.495 & 0.531 & 4.855 & 0.251 & 0.268 & 0.362 & 0.339 \\
				&GSPS \cite{mao2021generating}                    & 14.757 & 0.389 & {0.496} & 0.476 & {0.525} & 5.825 & 0.233 & 0.244 & 0.343 & 0.331 \\ \cline{2-12} 
				& Ours             &    \textbf{15.310}   &  \textbf{0.370}    & \textbf{0.485}  &    \textbf{0.475}   &  \textbf{0.516}    &   \textbf{6.109}   &  \textbf{0.220}     &    \textbf{0.234}   &   \textbf{0.342}    &    \textbf{0.316}   \\ 
				\bottomrule
		\end{tabular}}
	\end{table*}
	
	
\begin{figure*}[!t]
		\centering
		\includegraphics[width=1.0\linewidth]{figs/visualize_highquality.pdf}
		\caption{Qualitative comparisons. For the same input, we show end poses of 10 predicted results. Please see actions of the poses.}
		\label{fig:quality_visualize}
	\end{figure*}
	
	\subsubsection{Training Losses} 
	Let  be the  results predicted from an input, we impose three kinds of loss functions on  to train the proposed sampling framework.


	(1) \textit{Hinge-diversity loss.} In order to enhance the diversity of the results, we propose the following hinge-diversity loss:
	
	where  is a user-defined threshold. By the hinge-diversity loss, we explicitly enforce the distance between any pair of generated predictions to be no less than . Compared with the diversity loss defined in Eq.~\ref{eq:dlow-diversity-loss}, the hinge-diversity loss is more aggressive in enforcing the diversity of the predictions while less affecting the accuracy of the results (see ablation studies).
	
	(2) \textit{Accuracy loss.} To ensure the accuracy of results, we also adopt the accuracy loss  defined in Eq.~\ref{eq:dlow-ade-loss} that enforces at least one of the predictions to be similar to the ground truth. 
	
	(3) \textit{KL loss.} Finally the KL loss defined in Eq.~\ref{eq:kl-loss} is a very important loss which ensures the model to produce realistic and plausible results instead of those with high diversity but are physically invalid. Our KL loss is now defined as:
	
	where  is the latent distribution of  encoded by networks  and  with parameters of  and . 
	
	Altogether, our training loss is:
	
	where s are hyper-parameters used to balance the three terms.
	
	\section{Experiments}




	\subsection{Experimental Settings}
	
	\begin{figure*}[!t]
		\centering
		\includegraphics[width=1\linewidth]{figs/pca_1000.pdf}
		\caption{Holistic views of results. 1000 pose sequences are predicted and projected to 2D points. Note the regions marked with red boxes where our method can sample points from while DLow and GSPS fail to.}
		\label{fig:pca_diversity}
	\end{figure*}
	
\textbf{Datasets.} Following \cite{yuan2020dlow, mao2021generating}, we evaluate our method on two public motion capture datasets: Human3.6M\footnote{The authors Lingwei Dang and Yongwei Nie signed the license and produced all the experimental results in this paper. Meta did not have access to the Human3.6M dataset.}~\cite{ionescu2013human3} and HumanEva-I\footnote{We strictly comply with the agreement of using this dataset for non-commercial research purpose only.}~\cite{sigal2010humaneva}. (1) \textbf{Human3.6M} contains 7 subjects each performing 15 action categories. We use the data of five subjects (S1, S5, S6, S7, S8) for training, and the other two (S9, S11) for testing. After removing redundant joints, each pose has 17 joints. We input 25 frames, \textit{i.e.}, 0.5s (50fps), to forecast 100 frames (2s) in the future. (2) \textbf{HumanEva-I} comprises 3 subjects each performing 5 action categories. Each pose has 15 joints. We forecast 60 future poses (1s, 60fps) given 15 (0.25s) frames. 

	\textbf{Evaluation Metrics.} We use five metrics to evaluate our method. (1) \textbf{APD}: the Average Pairwise Distance of results predicted from an input \cite{aliakbarian2020stochastic}. This metric measures the diversity of the results. (2) \textbf{ADE} and \textbf{FDE}: ADE computes the Average Displacement Error between the ground truth and the result most similar to the ground truth. FDE, which stands for Final Displacement Error, only calculates the distance between the last pose of GT and the last pose of the most similar result to GT. (3) \textbf{MMADE} and \textbf{MMFDE} are the multimodal versions of ADE and FDE which were introduced in \cite{yuan2020dlow}. To compute them, for each training sample , one needs to search the whole dataset for a set of  whose past motion  is similar enough to , and take their future motion  as the pseudo ground truths of . MMADE is then computed as: . 
Similar to FDE, MMFDE only calculates the error of end poses. By ADE, FDE, MMADE, and MMFDE, we can know the accuracy of results.
	
We employ additional metrics, \textit{e.g.}, ADE-m, FDE-m, ACC, FID suggested by \cite{bie2022hit} for further evaluation. please refer to the supplementary material for more details.
	


	
	\textbf{Implementation Details.} 
In default, we use  basis vectors. At training time, we sample  points from the auxiliary space. At test time, we set  to compare with previous approaches, and set  to numbers from 2 to 1000 in ablation studies. We set , and . For Human3.6M, we set , , and , and  in Eq.~\ref{equ:hinge_loss} to 25. These numbers for the HumanEva-I dataset are 100, 25, 0.1 and 20, respectively. 
	
	We implement our method in PyTorch, training it by the Adam optimizer with a learning rate of  for the first 100 training epochs. Then the learning rate starts to decrease, eventually becoming  after a total of 500 epochs of training. Following \cite{yuan2020dlow,mao2021generating}, for each epoch, we randomly sample 5000 samples from Human3.6M or 2000 samples from HumanEva-I for training. The batchsize is set to 16 for both datasets.
	
	\begin{table*}[!t]
		\caption{We perform 5 groups of ablation studies.  indicates default choices. Please refer to the main text for details.}
		\label{tab:ablation}
		\resizebox{1\textwidth}{!}{
			\begin{tabular}{c|ccccc|ccccc|ccc|cc|ccc}
				\toprule
				& \multicolumn{5}{c|}{  Number of basis vectors}                                & \multicolumn{5}{c|}{ Dimension of auxiliary space} & \multicolumn{3}{c|}{ Sampling method}                    & \multicolumn{2}{c|}{ }         & \multicolumn{3}{c}{  \textit{v.s.} } \\ \cline{2-19}
				& 20    & 30     &  & 50     & \multicolumn{1}{c|}{60}     & 32 & 64 &  & 256 & \multicolumn{1}{c|}{512} &   & Gaussian & \multicolumn{1}{c|}{Uniform} &     & \multicolumn{1}{c|}{w/o } &   &  (25) &  (1300)  \\ \hline
				APD   & 5.929 & 5.946 & \textbf{5.993}   & 5.969 & \multicolumn{1}{c|}{5.951} & 5.885 & 5.931 & \textbf{5.993} & 5.963 & \multicolumn{1}{c|}{5.957} & \textbf{5.993} & 5.847    & \multicolumn{1}{c|}{5.730}   & \textbf{5.993}& \multicolumn{1}{c|}{5.182}  & \textbf{5.993}     & 3.843         & \textbf{5.993}  \\
				ADE  & 0.234 & 0.234 & 0.231   & \textbf{0.229} & \multicolumn{1}{c|}{0.233} & 0.236 & 0.233 & \textbf{0.231} & 0.233 &  \multicolumn{1}{c|}{0.236} & \textbf{0.231} & 0.240    & \multicolumn{1}{c|}{0.233}   & 0.231 & \multicolumn{1}{c|}{\textbf{0.229}}  & 0.231     & \textbf{0.211}        & 0.235  \\
				FDE  & 0.240 & 0.243 & 0.240   & \textbf{0.239}& \multicolumn{1}{c|}{0.241} & 0.245 & 0.243 & \textbf{0.240} & 0.241 & \multicolumn{1}{c|}{0.243} & \textbf{0.240}& 0.246    & \multicolumn{1}{c|}{0.241}   & 0.240 & \multicolumn{1}{c|}{\textbf{0.236}}  & 0.240     & \textbf{0.218}         & 0.242  \\
				MMADE  & 0.345 & 0.345 & 0.340   & 0.339 & \multicolumn{1}{c|}{\textbf{0.338}} & 0.337 & 0.342 & 0.340 & \textbf{0.336} & \multicolumn{1}{c|}{0.342} & \textbf{0.340} & 0.343    & \multicolumn{1}{c|}{0.344}   & 0.340 & \multicolumn{1}{c|}{\textbf{0.322}}  & 0.340     & \textbf{0.309}        &0.343  \\
				MMFDE   & 0.321 & 0.319 & 0.313   & 0.315 & \multicolumn{1}{c|}{\textbf{0.312}} & 0.312 & 0.318 & 0.313 & \textbf{0.310} & \multicolumn{1}{c|}{0.316} & \textbf{0.313} & 0.320    & \multicolumn{1}{c|}{0.323}   & 0.313 & \multicolumn{1}{c|}{\textbf{0.298}}  & 0.313     & \textbf{0.287}           & 0.321   \\ 
				\bottomrule
		\end{tabular}}
	\end{table*}
	


	


	\begin{figure*}[!t]
		\centering
		\includegraphics[width=0.9\linewidth]{figs/qianshou_lambda_near_far.pdf}
		\caption{Increase  from 0 to 100. The last poses of 50 pose sequences predicted from an input are stacked together to illustrate the holistic view of results. Poses most similar to and different from the ground truth are highlighted.}
		\label{fig:qian_shou_guan_yin}
	\end{figure*}
	
	\subsection{Comparison with Previous Approaches}
	
	We compare our method with both kinds of prediction methods: 
(1) The most recent deterministic methods including LTD \cite{mao2019learning} and MSR \cite{dang2021msr}. 
	(2) Stochastic methods including HP-GAN \cite{barsoum2018hp}, Pose-Knows \cite{walker2017pose}, MT-VAE \cite{yan2018mt}, BoM \cite{bhattacharyya2018accurate}, GMVAE \cite{dilokthanakul2016deep}, DeLiGAN \cite{gurumurthy2017deligan}, DSF \cite{yuan2019diverse}, DLow \cite{yuan2020dlow}, and GSPS \cite{mao2021generating}. For each input historical pose sequence, all stochastic methods predict 50 different future sequences.
	
	Table \ref{tab:main_result} shows comparisons among all the compared methods. On both datasets, our method outperforms all the other approaches on all the evaluation metrics. Since deterministic approaches can only generate one output, the diversity of their results is 0.000. In terms of prediction accuracy, deterministic methods are inferior to stochastic methods too. This may be due to two reasons. Firstly, deterministic approaches are not good at long-term prediction (\textit{e.g.}, more than 1 second). Secondly, stochastic methods can predict multiple results among which there may be a very good one. 
	Since our method is based on DLow, let us focus on the comparisons between them. On human3.6M, DLow achieves a diversity of 11.741, while that of our method is 15.310, which is a very significant improvement of about 30\%. Our method is also better than DLow in terms of prediction accuracy. For ADE, our accuracy is improved by 14.9\% ( 0.370 \textit{v.s.} 0.425). For FDE, MMADE, MMFDE, the improvements are: 6.8\%, 4.2\%, and 2.9\%, respectively. The comparisons on the HumanEva-I dataset show similar trends: our method improves DLow on all metrics. GSPS is one of the latest stochastic prediction methods which directly predicts very diverse results as long as they are reasonable under many prior constraints. Our method outperforms GSPS, reaching a new state-of-the-art.



	To show the quality of the predicted poses, we visualize end poses of pose sequences predicted by DLow \cite{yuan2020dlow}, GSPS \cite{mao2021generating} and our method. The examples in Figure \ref{fig:quality_visualize} show that our method produces more diverse results than DLow and GSPS. For example, on the left, our method can predict actions of ``raising hands'' and ``picking up things'' (marked by red boxes). Please refer to the supplemental video for how our method smoothly transitions the action from the input ``normal standing'' to the two very different actions.
	
	Figure \ref{fig:pca_diversity} illustrates the holistic views of results. Given an input, we generate 1000 results and project them into 2D space. Note that DLow can only sample 50 results at a time. To generate 1000 results for DLow, we repeatedly run DLow 20 times. 
As can be seen, our results occupy the 2D space more evenly. Please compare regions marked by red boxes where our method can sample points from while DLow and GSPS cannot.
	
	
	\subsection{Ablation Study}
	\label{sec:ablation}
	


	Table~\ref{tab:ablation} shows five groups of ablation studies that validate the design components of our method. All models are trained on the HumanEva-I dataset for 200 epochs.
	
	(1) \textbf{Number of basis vectors.} We set  to 20, 30, 40, 50, and 60. Overall,  has little effect on the results. For example, APD (\textit{i.e.}, diversity) varies within a narrow range of [5.929, 5.993], with the best diversity obtained when . MMADE and MMFDE steadily improve as  increases. We finally choose 40 as the default value of .
	(2) \textbf{Dimension size of auxiliary space.} We set  to 32, 64, 128, 256, and 512. The best APD, ADE, and FDE are obtained when . For MMADE and MMFDE, the best values are obtained when . We finally choose 128 as the default value of .
	(3) \textbf{Gumbel \textit{v.s.} Gaussian and Uniform.} The experimental results validate that Gumbel-Softmax sampling strategy is better than Uniform-Softmax and Gaussian-Softmax sampling methods when used in our sampling process. The fact that the differences between different sampling methods is not evident can be ascribed to the high capability of the proposed auxiliary-space-based resampling method. The space itself can be flexibly adjusted to match the three sampling methods to output good results. (4) \textbf{Using  or not.} In this ablation study, we remove the second MLP network , and directly use each row of the point matrix as a pair of  and . The diversity drops significantly (expected but undesired), while the accuracy increases. This is reasonable as diversity and accuracy are two conflict objectives: the increase of the diversity inevitably decreases the accuracy. We ultimately choose to integrate the network into our framework to achieve higher diversity with acceptable accuracy.
	(5) \textbf{Diversity loss \textit{v.s.} hinge-diversity loss.} We replace our hinge-diversity loss with the diversity loss defined in Eq.~\ref{eq:dlow-diversity-loss}. Note that the default weight of our hinge-diversity loss is . For the diversity loss, we first use a weight of 25. However, the diversity is much lower: 3.843 \textit{v.s.} our 5.993. We increase the weight of the diversity loss to 1300 until it produces the same diversity as ours, but now its accuracy is lower than that of our hinge-diversity loss. These studies show our hinge-diversity loss better prompts diversity while less affecting the accuracy.
	




	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=0.485\linewidth]{figs/lambda_hdiv.png}
		\hspace{0.1cm}
		\includegraphics[width=0.485\linewidth]{figs/K.png} \\
		\flushleft \hspace{2cm} (a) \hspace{3.5cm} (b)
		\caption{(a) As  increases, APD (diversity) drastically increases from 6.174 to 22.300, while accuracy decreases. (b) As  increases, accuracy becomes better while diversity nearly does not change.}
		\label{fig:two_curves}
	\end{figure}
	
	In Figure \ref{fig:qian_shou_guan_yin}, we perform an ablation study on the weight of the hinge-diversity loss. We set  to 0, 5, 10, 20, 50, and 100. All models are trained on Human3.6M for 200 epochs. We stack the end poses of all the results predicted from an input. The pose most similar to (in black and gray) and different from (red and blue) the ground truth are highlighted. Visually, as  increases, more diverse results are obtained. However, Figure~\ref{fig:two_curves} (a) shows that larger  leads to lower accuracy. We finally choose 20 as the default value of , obtaining both satisfactory diversity and accuracy.
	
	In Figure~\ref{fig:two_curves} (b), we increase  from 2 to 1000. As  increases, ADE, FDE, MMADE and MMFDE all decrease, meaning more accurate results are obtained. This indicates that we can obtain more accurate results by sampling more of them. The diversity nearly does not change as  increases, which is a good property as we can obtain diverse results with just a few samplings.
	


	\subsection{Limitations and Future Work}
	One limitation is that we have to adjust the weights of the loss functions to make a tradeoff between diversity and accuracy, though we note that DLow and GSPS suffer from this limitation too. Another limitation is that similar to DLow and GSPS our method occasionally generates odd poses with such as slightly long bones or unnatural actions. In the future, we can add more regularization terms, such as the bone and angle constraints adopted by GSPS, into our model to prevent these failures.
	
	\section{Conclusion}
	We have presented a diverse pose prediction algorithm. Our method first generates an auxiliary space from the input, then samples points from the auxiliary space by the Gumbel-Softmax sampling strategy, and finally maps the points to Gaussian distributions from which we sample latent codes and finally decode them into target predictions. We have demonstrated the influence of the dimension size of the auxiliary space and the number of basis vectors that characterize the auxiliary space on the performance of our method. We have also illustrated the effectiveness of the proposed hinge-diversity loss in promoting diversity while persisting accuracy. Although we only apply this method to tackle the task of stochastic human motion prediction, we believe it can also be used to handle many other stochastic prediction/generation problems.
	
\begin{acks}
		This work is sponsored by Prof. Yongwei Nie's and Prof. Guiqing Li's Natural Science Foundation of China projects (62072191, 61972160), and their Natural Science Foundation of Guangdong Province projects (2019A1515010860, 2021A1515012301).
	\end{acks}
	
\bibliographystyle{ACM-Reference-Format}
	\bibliography{diverse_sampling_arxiv}
	
\clearpage
	
	\appendix
	
	\noindent{\LARGE{\textbf{Supplementary Material}}}
	
	


	In this supplementary material, we provide more information that cannot be included in the paper due to the space limit. We first introduce network architectures of our method in detail. Then, we provide more quantitative and qualitative comparisons. Finally, we give some failure cases and the reasons for these failures. Please refer to our provided video demo to review the results more intuitively.
	
	
	\section{Auxiliary-space-based Sampling Network Architecture}
	
	
	Our auxiliary-space-based sampling network is illustrated in Figure \ref{fig:our-arch}. The input is , where  is the number of input poses,  is the number of joints of a pose, and  is the dimension size of each joint. For Human3.6M \cite{ionescu2013human3}, , , , while for HumanEva-I \cite{sigal2010humaneva}, , , and .
	Following \cite{mao2019learning}, we repeat the last pose of ,  times, and append them to . Now, the input is of size , where  is the number of poses to be predicted. 
For Human3.6M, , and for HumanEva-I, .
	Then following \cite{mao2019learning} again, we apply a Discrete Cosine Transform (DCT) operator to
transform the temporal information along the  dimension of the input data into the frequency space. By keeping only the coefficients of low frequency components and discarding those of high frequency components, we obtain data of  which becomes  after reshaping, where  is the number of the remained coefficients.
	Following, a network  made up of a GCN and an MLP learns a base matrix  from the DCT coefficients, where  and .
The GCN, which will be described later, extracts hidden features of shape  where  is the feature dimension size. Then, the MLP composed of a linear transformation layer, a Batch Normalization (BN) layer and a Tanh activation function, maps the hidden features into . The network structures of the GCN and MLP are shown in Table \ref{tab:component_ours}.
	Next, we sample a random coefficient matrix  by the Gumbel-Softmax sampling technique, where  is the sampling number. Then the multiplication of  and  outputs a point matrix of shape .
	Next, the second network  projects the  sampled points into the parameters of  Gaussian distributions , where  indicates means of these Gaussian distributions and  are the diagonal values of their co-variance matrices.
	In particular,  consists of two sub MLPs for generating  and , respectively. The detailed structure of  is shown in Table \ref{tab:component_ours}. Each MLP is made up of a Linear-BN-Tanh layer to transform the point features into hidden vectors of shape , and another Linear layer that maps the hidden vectors into Gaussian parameters of shape , where  and  are both set as 64.
	After that, a set of latent variables  are drawn from the Gaussian distributions by the reparameterization trick.
	We repeat the input data after DCT,  times and concatenate each of them with , and feed them into the pretrained CVAE decoder (which will be described later) to produce future motions in the frequency space of shape . Then we project the frequency features back into the pose space by the inverse DCT (i-DCT) function, obtaining  pose sequences of shape .
	Finally, a slice operator extracts only the future  frames and outputs results of shape  which are the  future pose sequences predicted by our network.
	
	\begin{table}[!t]
		\caption{The network structure of  and . For Human3.6M, . For HumanEva-I, . For both datasets, , , , , , , , .}
		\label{tab:component_ours}
		\resizebox{0.45\textwidth}{!}{
			\begin{tabular}{c|ccc|c|c}
				\toprule
				Component                    & \multicolumn{1}{c|}{Block}                         & \multicolumn{1}{c|}{Layer}     & Weight Size                 & Input Size             & Output Size              \\ 
				\midrule
				\multirow{12}{*}{}          & \multicolumn{1}{c|}{\multirow{10}{*}{GCN}}         & \multicolumn{1}{c|}{GCL}     & A(), W()  & ()    & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh} &  -                    & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{GCL}      & A(), W() & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh} &  -                     & ()   & ()                 \\ \cline{3-6}  
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{GCL}      & A(), W() & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh} &  -                      & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{GCL}     & A(), W() & ()   & ()                 \\ \cline{3-6}
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh}  & -                       & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{GCL}      & A(), W() & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh}  & -                       & ()   & ()                 \\ \cline{2-6} 
				
				& \multicolumn{1}{c|}{\multirow{2}{*}{MLP 1}}  & \multicolumn{1}{c|}{Linear}  & W( , )       & ()    & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh} & -               & ()   & ()                 \\ \hline
				\multirow{6}{*}{} & \multicolumn{1}{c|}{\multirow{3}{*}{MLP }} & \multicolumn{1}{c|}{Linear}   & W()             & ()  & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh} &  -                      & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{Linear}   & W()              & ()   & ()                 \\ \cline{2-6} 
				& \multicolumn{1}{c|}{\multirow{3}{*}{MLP }} & \multicolumn{1}{c|}{Linear}   & W()             & ()  & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{BN, Tanh} &  -                      & ()   & ()                 \\ \cline{3-6} 
				& \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{Linear}   & W()              & ()   & ()                 \\ \hline
				CVAE Decoder                 & \multicolumn{3}{c|}{See Table \ref{tab:component_cvae} }                                  & (), () & () \\ \bottomrule
			\end{tabular}
		}
	\end{table}
	
	
	\begin{table}[!t]
		\caption{The network structure of the employed CVAE. For Human3.6M, . For HumanEva-I, . For both datasets, , , , .}
		\label{tab:component_cvae}
		\resizebox{0.45\textwidth}{!}{
			\begin{tabular}{c|c|c|c|c|c}
				\toprule
				Component   & Block                   & Layer    & Weight Size              & Input Size    & Output Size \\ \midrule
				\multirow{20}{*}{Encoder} & \multirow{18}{*}{GCN 1} & GCL      & A(), W()  & ()       & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &   -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &   -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &   -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &   -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &    -                    & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                    & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh & -                    & ()      & ()    \\ \cline{2-6} 
				& MLP 1                   & Linear & W()             & ()       &          \\ \cline{3-6} \cline{2-6} 
				& MLP 2                   & Linear & W()             & ()       &          \\ \hline
				\multirow{19}{*}{Decoder} & \multirow{19}{*}{GCN 2} & GCL      & A(), W() & (), () & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                    & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                    & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh & -                       & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &   -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &   -                     & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()   & ()      & ()    \\ \cline{3-6}
				&                         & BN, Tanh &  -                   & ()      & ()    \\ \cline{3-6}
				&                         & GCL      & A(), W()    & ()      & ()     \\ 
				\bottomrule
		\end{tabular}}
	\end{table}
	
	
	Now, we introduce the Graph Convolutional Network (GCN). As shown in Table \ref{tab:component_ours}, our GCN is made up of five sequentially stacked GCL-BN-Tanh layers, where GCL stands for Graph Convolutional layer. Let  be the input to the  GCL where  is the hidden feature dimension size,  the adjacency matrix, and  the trainable parameters, the GCL executes the following computation:
	
	where  is the output of the  GCL. At the very beginning, .
	
	
	\begin{figure*}[!t]
		\centering
		\includegraphics[width=\textwidth]{figs/supp_ours.pdf}\\
		\caption{Detailed architecture of our auxiliary-space-based sampling model. A circle represents input, intermediate, or output data. The symbol above a circle indicates the size of the data. For example, [, ] means the data is a two-dimensional matrix of  rows and  columns. The symbol above an arrow indicates the size of the output of the corresponding previous operator. Please refer to the main text for detailed descriptions of the architecture. }
\label{fig:our-arch}
	\end{figure*}
	
	\begin{figure*}[!t]
		\centering
		\includegraphics[width=\textwidth]{figs/supp_CVAE.pdf}\\
		\caption{Detailed architecture of the adopted CVAE model. It is composed of an encoder and a decoder that are built on GCNs and MLPs. Please refer to the main text for detailed descriptions of the architecture.}
\label{fig:cvae-arch}
	\end{figure*}
	
	
	\section{CVAE Network Architecture}
	Recall that before applying our method to generate diverse results, we need to train a CVAE model beforehand. The CVAE network architecture adopted in this paper is shown in Figure \ref{fig:cvae-arch}. 
	Let  be an observed pose sequence, and  be the ground truth future poses. We compute the frequency coefficients of shape  from each of them and then concatenate both the frequency content into data of shape .
	Then, an encoder is used to learn the parameters of the posterior Gaussian distribution  of the latent code  given  and , where  is the mean of the posterior distribution, and  is the diagonal values of the co-variance matrix of the posterior distribution.
	Particularly, as shown in Table \ref{tab:component_cvae}, the encoder consists of a GCN and two MLPs. The GCN is composed of nine GCL-BN-Tanh layers to extract hidden features of shape . The two MLPs, each of which just comprises a single Linear layer, map the hidden features into  and , respectively.
	Then a latent variable  can be drawn from the posterior Gaussian distribution by the reparameterization trick.
	
	Next, a decoder is used to reconstruct  from the latent code .
	To achieve that, we first repeat ,  times and concatenate them with the DCT coefficients of , resulting in a feature of shape .
	Afterwards, we employ another GCN comprising 9 GCL-BN-Tanh layers to extract hidden features of shape , and a GCL layer that projects the hidden features back into the frequency space of shape .
	The frequency coefficients of  after DCT is added to this output.
	Finally, we use an i-DCT function to transform the above output back into the pose space and slice off the future  frames to obtain .
	


	\begin{table*}[!t]
		\caption{Comparison on four additional metrics: ADE-m, FDE-m, FID, and ACC.}
		\label{tab:other_matrics}
		\resizebox{0.7\textwidth}{!}{
			\begin{tabular}{c|cccc|cccc}
				\hline
				& \multicolumn{4}{c|}{Human3.6M \cite{ionescu2013human3}} & \multicolumn{4}{c}{HumanEva-I \cite{sigal2010humaneva}}                                                                   \\ \cline{2-9} 
				&  ADE-m  & FDE-m  & FID  & ACC  &  ADE-m  & FDE-m  & FID  & ACC \\ \hline
				DLow \cite{yuan2020dlow} & \textbf{0.896}   & \textbf{1.284}  & \textbf{1.566}    & 0.227  & \textbf{0.577}   & \textbf{0.717}  & 3.472    & 0.527 \\
				GSPS \cite{mao2021generating} & 1.013   & 1.372  & 1.915    & 0.222  & 0.686   & 0.794  & 1.604    & 0.516 \\
				Ours & 0.924   & 1.344  & 2.060    & \textbf{0.261}    & 0.716   & 0.770  & \textbf{1.106}    &  \textbf{0.609}\\ \hline
			\end{tabular}
		}
	\end{table*}
	
	\begin{table*}[!t]
		\caption{Comparison with DLow \cite{yuan2020dlow} when adding random noises of different variance to its Gaussian distributions.}
		\label{tab:noise_onto_dlow}
		\resizebox{1\textwidth}{!}{
			\begin{tabular}{c|cccccccc|cccccccc}
				\toprule
				\multirow{3}{*}{} & \multicolumn{8}{c|}{Human3.6M \cite{ionescu2013human3}}                                                                                                                                                   & \multicolumn{8}{c}{HumanEva-I \cite{sigal2010humaneva}}                                                                                                                                                  \\ \cline{2-17} 
				& \multicolumn{1}{c|}{\multirow{2}{*}{Ours}} & \multicolumn{1}{c|}{\multirow{2}{*}{DLow \cite{yuan2020dlow}}} & \multicolumn{3}{c|}{DLow-variant \cite{yuan2020dlow} w/o retraining}           & \multicolumn{3}{c|}{DLow-variant \cite{yuan2020dlow} w/ retraining} & \multicolumn{1}{c|}{\multirow{2}{*}{Ours}} & \multicolumn{1}{c|}{\multirow{2}{*}{DLow \cite{yuan2020dlow}}} & \multicolumn{3}{c|}{DLow-variant \cite{yuan2020dlow} w/o retraining}           & \multicolumn{3}{c}{DLow-variant \cite{yuan2020dlow} w/ retraining} \\ \cline{4-9} \cline{12-17} 
				& \multicolumn{1}{c|}{}                      & \multicolumn{1}{c|}{}                      &  = 1 &  = 1.7 & \multicolumn{1}{c|}{ = 2} &  = 1     &  = 1.7    &  = 2    & \multicolumn{1}{c|}{}                      & \multicolumn{1}{c|}{}                      &  = 1 &  = 5.7 & \multicolumn{1}{c|}{ = 6} &  = 1    &  = 5.7    &  = 6    \\ \hline
				APD               & \multicolumn{1}{c|}{15.310}                & \multicolumn{1}{c|}{11.741}                & 15.190 & 19.295   & \multicolumn{1}{c|}{\textbf{20.894}} & 11.100     & 15.373      & 18.667    & \multicolumn{1}{c|}{6.109}                 & \multicolumn{1}{c|}{4.855}                 & 4.753  & 6.147    & \multicolumn{1}{c|}{\textbf{6.243}}  & 4.488     & 6.135       & 6.205     \\
				ADE               & \multicolumn{1}{c|}{\textbf{0.370}}                 & \multicolumn{1}{c|}{0.425}                 & 0.560  & 0.721    & \multicolumn{1}{c|}{0.795}  & 0.526      & 0.675       & 0.748     & \multicolumn{1}{c|}{\textbf{0.220}}                 & \multicolumn{1}{c|}{0.251}                 & 0.305  & 0.647    & \multicolumn{1}{c|}{0.654}  & 0.297     & 0.638       & 0.649     \\
				FDE               & \multicolumn{1}{c|}{\textbf{0.485}}                 & \multicolumn{1}{c|}{0.518}                 & 0.674  & 0.863    & \multicolumn{1}{c|}{0.949}  & 0.627      & 0.799       & 0.883     & \multicolumn{1}{c|}{\textbf{0.234}}                 & \multicolumn{1}{c|}{0.268}                 & 0.327  & 0.658    & \multicolumn{1}{c|}{0.664}  & 0.321     & 0.648       & 0.657     \\
				MMADE              & \multicolumn{1}{c|}{\textbf{0.475}}                 & \multicolumn{1}{c|}{0.495}                 & 0.612  & 0.764    & \multicolumn{1}{c|}{0.835}  & 0.579      & 0.719       & 0.789     & \multicolumn{1}{c|}{\textbf{0.342}}                 & \multicolumn{1}{c|}{0.362}                 & 0.387  & 0.659    & \multicolumn{1}{c|}{0.666}  & 0.381     & 0.652       & 0.664     \\
				MMFDE              & \multicolumn{1}{c|}{\textbf{0.516}}                 & \multicolumn{1}{c|}{0.531}                 & 0.682  & 0.868    & \multicolumn{1}{c|}{0.953}  & 0.634      & 0.804       & 0.889     & \multicolumn{1}{c|}{\textbf{0.316}}                 & \multicolumn{1}{c|}{0.339}                 & 0.372  & 0.661    & \multicolumn{1}{c|}{0.667}  & 0.368     & 0.650       & 0.661     \\ \bottomrule
		\end{tabular}}
	\end{table*}
	
	
	\begin{table*}[!t]
\caption{Comparison with learning  (in Eq. 12) automatically.}
		\label{tab:learned_pi}
		\resizebox{1\textwidth}{!}{
			\begin{tabular}{c|ccccc|ccccc}
				\toprule
				\multirow{2}{*}{}      & \multicolumn{5}{c|}{Human3.6M \cite{ionescu2013human3}}         & \multicolumn{5}{c}{HumanEva-I \cite{sigal2010humaneva}}        \\ \cline{2-11} 
				& APD    & ADE   & FDE   & MMADE  & MMFDE  & APD   & ADE   & FDE   & MMADE  & MMFDE  \\ \hline
				DLow \cite{yuan2020dlow}     & 11.741 & 0.425 & 0.518 & 0.495 & 0.531 & 4.855 & 0.251 & 0.268 & 0.362 & 0.339 \\
				GSPS  \cite{mao2021generating}    & 14.757 & 0.389 & 0.496 & 0.476 & 0.525 & 5.825 & 0.233 & 0.244 & 0.343 & 0.331 \\
				Ours                         & \textbf{15.310} & 0.370 & \textbf{0.485} & \textbf{0.475} & \textbf{0.516} & \textbf{6.109} & \textbf{0.220} & \textbf{0.234} & 0.342 & 0.316 \\
				Ours-Individual- & 13.530 & 0.372 & 0.493 & 0.481 & 0.525 & 5.606 & 0.229 & 0.239 & 0.338 & 0.317 \\
				Ours-Shared-     & 14.440 & \textbf{0.368} & \textbf{0.485} & 0.477 & 0.519 & 5.690 & 0.228 & 0.235 & \textbf{0.324} & \textbf{0.295} \\ \bottomrule
		\end{tabular}}
	\end{table*}
	
	
	\section{Evaluation on FID, ACC, ADE-m, and FDE-m}
	
	We further evaluate our method on four additional metrics: FID, ACC, ADE-median (ADE-m), and FDE-median (FDE-m).
	
	\begin{itemize}
		\item Recognition Accuracy (\textbf{ACC}). A pre-trained action recognition classifier is used to classify the generated poses. ACC is the overall recognition accuracy. We train the action classifier in the way suggested by~\cite{bie2022hit}.
		\item Frechet Inception Distance (\textbf{FID}). Features are extracted from the generated and real data by the pre-trained action classifier. FID is then calculated as the Frechet inception distance between the two feature distributions.
		\item \textbf{ADE-m} and \textbf{FDE-m} are similar to ADE and FDE except that the median distance are reported.
	\end{itemize}
	
	




	The results on the four metrics are shown in Table~\ref{tab:other_matrics}.
	For ADE-m and FDE-m, DLow performs the best. That is because the diversity by DLow (11.741, Human3.6M) is much lower than that by GSPS (14.757) and our method (15.310). The lower the diversity, the lower the median distance. Therefore, it is not surprise that both our method and GSPS have larger ADE-m and FDE-m than DLow. Compared with GSPS, our method has lower ADE-m and FDE-m (ADE-m: 0.924 (our) \textit{v.s.} 1.013 (GSPS), FDE-m: 1.344 (our) \textit{v.s.} 1.372 (GSPS), Human3.6M), even though our method has greater diversity (APD: 15.310 (our) \textit{v.s.} 14.757 (GSPS)).
	
	For FID and ACC, our method is better than DLow and GSPS in terms of ACC. For HumanEva-I, our FID is the best. However, an exception is the FID of Human3.6M, for which DLow performs much better than GSPS and our method. Again, this is because the diversity of the results of DLow (APD=11.741) is much lower than those of GSPS (14.757) and our method (15.310).
	


	\section{Comparison with a variant of DLow}
	
	We simply add some random noise to the generated Gaussian distributions (i.e., adding noises to the mean and variance of the Gaussian distributions) in DLow \cite{yuan2020dlow} and compare with this variant of DLow.
	




	Firstly, we add noises to the Gaussian distributions generated by DLow without re-training the DLow model (DLow-variant w/o retraining). Secondly, we retrain the DLow model, and add noises to the Gaussian distributions at both training and testing phases (DLow-variant w/ retraining). The noises are randomly drawn from a Normal distribution . The mean of the noises is always zero, while for comparison we test noises of different variances.
	
	As shown in Table~\ref{tab:noise_onto_dlow}, adding noises can increase the diversity of the generated results (measured by APD). The heavier the noises (produced by larger ), the more diverse the results. However, the negative effect is that the accuracy of the results is decreased. For the Human3.6M dataset, please see the columns of ``DLow-variant w/o retraining '' and ``DLow-variant w/ retraining '' that produce similar APD as ours. Their accuracy metrics are much higher than ours. Although adding noises can yield very large APD (20.894 in the column of ``DLow-variant w/o retraining ''), the generated poses look unrealistically. And those results on the HumanEva-I dataset have the same trend. From this point of view, our method is better than directly adding noises to generated Gaussian distributions.
	
	\section{Learn  instead of setting a constant value}
	In the main paper, we set  (see Eq. 12) to a constant value. With constant  (1/40=0.025), each basis vector has the equal probability to be assigned with the highest weight among all the basis vectors. To treat all the basis vectors equally, we therefore use the same constant  (actually a probability) for each of them.
	
	As a variant,  can also be learned automatically. We conduct experiments to compare between learning and setting a constant . The results are shown in Table~\ref{tab:learned_pi}.
	
	
	In the first experiment, we learn a  for each input sample individually (Ours-Individual-), the results are slightly worse than those of directly indicating a constant . In the second experiment, we learn a  shared by all the input samples (Ours-Shared-), the new results are comparable to those of constant . We find that the values of the learned shared  fall in the range of [0.021, 0.029], which are nearly equally distributed.
	
	
	\begin{figure*}[!t]
		\flushleft \hspace{5.7cm} Human3.6M \hspace{4.4cm} HumanEva-I \\
		\centering
		\includegraphics[width=0.65\linewidth]{figs/supp_K.pdf} \\
		\caption{The first row shows how APD of different methods (including CVAE, DLow, GSPS, and our method) varies as  increases. The other rows show the trends of ADE, FDE, MMADE, MMFDE, respectively. The figures on the left are plotted based on the data computed on Human3.6M, while those on the right are plotted based on HumanEva-I.}
		\label{fig:samples_K}
	\end{figure*}
	
	\section{More ablation studies on parameter }
	
	In the main paper, we have demonstrated that , \textit{i.e.}, the number of predictions for an input, has a large effect on the accuracy but not the diversity of the results produced by our method. Here, we show more ablation studies on , and perform comparisons between CVAE random sampling, DLow~\cite{yuan2020dlow}, GSPS~\cite{mao2021generating} and our method.
	
	The results are plotted in Figure~\ref{fig:samples_K}. The first row shows the predicted results' diversity measured by \textit{APD}. As can be seen, for all the compared methods, \textit{APD} does not change much as  increases. We can also see that the diversity of our results is the largest among all the methods, while that of CVAE is the smallest.
	
	The second and third rows show the predicted results' accuracy measured by \textit{ADE} and \textit{FDE}. As can be seen, for all the compared methods, \textit{ADE} and \textit{FDE} decrease as  increases, indicating that more accurate results are obtained. 
	
	The fourth and fifth rows show the predicted results' accuracy measured by \textit{MMADE} and \textit{MMFDE}. For all the compared methods, \textit{MMADE} and \textit{MMFDE} decreases too as  increases. 
	
	Observing all the results in Figure~\ref{fig:samples_K}, we can see that our method outputs more diverse results than DLow and GSPS, and at the same time our results are more accurate than those of DLow and GSPS. Generally, diversity and accuracy are two contradictory objectives. Low diversity usually means high accuracy. That is why CVAE, which generates results of the lowest diversity, produces the most accurate results in the second to fifth rows.
	
	


	
	\section{More Qualitative Comparisons}
	
	\begin{figure*}[!t]
		\centering
		\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m28.pdf}
\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m34.pdf}\\
\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m39.pdf}
\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m46.pdf} \\
(a) Human3.6M \\
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva12.pdf}
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva26.pdf} \\
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva30.pdf}
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva33.pdf}\\ 
		(b) HumanEva-I \\
		\caption{More qualitative results of CVAE, DLow, GSPS, and our method. The numbers in the brackets below the names of different methods show the diversity of the results computed by these methods. In these examples, our results are more diverse than the results of the other methods.}
		\label{fig:more-qual_1}
	\end{figure*}
	
	
	\begin{figure*}[!t]
		\centering
		\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m20.pdf}
\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m29.pdf}\\
\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m38.pdf}
\includegraphics[width=0.49\linewidth]{figs/comparison_h36m/h36m49.pdf} \\
(a) Human3.6M \\
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva27.pdf}
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva29.pdf} \\
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva32.pdf}
		\includegraphics[width=0.49\linewidth]{figs/comparison_humaneva/humaneva40.pdf}\\ 
		(b) HumanEva-I \\
		\caption{More qualitative results of CVAE, DLow, GSPS, and our method. The numbers in the brackets below the names of different methods show the diversity of the results computed by these methods. In these examples, our results are more diverse than the results of the other methods.}
		\label{fig:more-qual_2}
	\end{figure*}
	
	
	In Figure~\ref{fig:more-qual_1} and Figure~\ref{fig:more-qual_2}, we show more qualitative comparisons between CVAE, DLow \cite{yuan2020dlow}, GSPS \cite{mao2021generating} and our method on the Human3.6M dataset \cite{ionescu2013human3} and the HumanEva-I dataset \cite{sigal2010humaneva}. For each input sequence, we generate 50 future pose sequences by these methods, and show the end poses of ten of them. In the brackets under the names of different methods, we show the diversity of the corresponding results computed by these methods. For these examples, our method produces more diverse results than the other compared methods.
	
	


	\section{Failure Cases}
	
\begin{figure*}[!t]
		\centering
		\includegraphics[width=0.49\linewidth]{figs/lowdiv_h36m/h36m47.pdf} 
		\includegraphics[width=0.49\linewidth]{figs/lowdiv_humaneva/humaneva46.pdf}
		\flushleft \hspace{4cm} (a) Human3.6M \hspace{6.5cm} (b) HumanEva-I 
		\caption{Two examples for which our method generates lower diverse results than GSPS.}
		\label{fig:lower-diversity}
	\end{figure*}
	
\begin{figure*}[!t]
		\centering
		\includegraphics[width=0.49\linewidth]{figs/invalid_h36m/h36m_cvae44.pdf} 
		\includegraphics[width=0.49\linewidth]{figs/invalid_h36m/h36m_dlow44.pdf} 
		\flushleft \hspace{4.5cm} (a) CVAE \hspace{7.8cm} (b) DLow 
		\includegraphics[width=0.49\linewidth]{figs/invalid_h36m/h36m_gsps44.pdf} 
		\includegraphics[width=0.49\linewidth]{figs/invalid_h36m/h36m_ours44.pdf} 
		\flushleft \hspace{4.5cm} (c) GSPS \hspace{7.8cm} (d) Ours 
		\caption{Examples of implausible poses in the results of DLow, GSPS, and our method.}
		\label{fig:invalid_poses}
	\end{figure*}
	
	Overall, our method is better than GSPS in term of diversity. Therefore, for most of the test cases our method generates more diverse results than GSPS. Inevitably there are cases for which our method generates less diverse results than GSPS. For example, Figure~\ref{fig:lower-diversity} shows such two cases. 
	
	While most of our results look reasonable, there are occasional ones that are implausible. Figure~\ref{fig:invalid_poses} shows some examples. We observe that DLow and GSPS suffer from this problem too, and the implausible poses in their results are highlighted too. One can reduce the number of implausible poses by toning down the requirement for diversity. A more effective way is to collect more data covering more actions of humans to enrich the training dataset.
	
	
\end{document}
