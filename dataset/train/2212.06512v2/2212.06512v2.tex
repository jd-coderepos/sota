\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{url}
\usepackage{array}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{color}
\usepackage{cite}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{2766} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\newcommand{\cavan}[1]{{\color{blue}(cavan: {#1})}} 

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{DifFace: Blind Face Restoration with Diffused Error Contraction}


\author{Zongsheng Yue, Chen Change Loy\\
S-Lab \\
Nanyang Technological University\\
{\tt\small zsyzam@gmail.com},
{\tt\small ccloy@ntu.edu.sg}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
    While deep learning-based methods for blind face restoration have achieved unprecedented success, they still suffer from two major limitations. First, most of them deteriorate when facing complex degradations out of their training data. Second, these methods require multiple constraints, e.g., fidelity, perceptual, and adversarial losses, which require laborious hyper-parameter tuning to stabilize and balance their influences. In this work, we propose a novel method named \textit{DifFace} that is capable of coping with unseen and complex degradations more gracefully without complicated loss designs. The key of our method is to establish a posterior distribution from the observed low-quality (LQ) image to its high-quality (HQ) counterpart. In particular, we design a transition distribution from the LQ image to the intermediate state of a pre-trained diffusion model and then gradually transmit from this intermediate state to the HQ target by recursively applying a pre-trained diffusion model. The transition distribution only relies on a restoration backbone that is trained with  loss on some synthetic data, which favorably avoids the cumbersome training process in existing methods. Moreover, the transition distribution can contract the error of the restoration backbone and thus makes our method more robust to unknown degradations. Comprehensive experiments show that \textit{DifFace} is superior to current state-of-the-art methods, especially in cases with severe degradations. Code and model are available at \url{https://github.com/zsyOAOA/DifFace}.
\end{abstract}

\section{Introduction}
Blind face restoration (BFR) aims at recovering a high-quality (HQ) image from its low-quality (LQ) counterpart, which usually suffers from complex degradations, such as noise, blurring, and downsampling. BFR is an extremely ill-posed inverse problem as multiple HQ solutions may exist for any given LQ image.

Approaches for BFR have been dominated by deep learning-based methods~\cite{wang2021towards,tu2021joint,feihong2022toward,gu2022vqfr}. The main idea of them is to learn a mapping, usually parameterized as a deep neural network, from the LQ images to the HQ ones based on a large amount of pre-collected LQ/HQ image pairs. In most cases, these image pairs are synthesized by assuming a degradation model that often deviates from the real one. Most existing methods are sensitive to such a deviation and thus suffer a dramatic performance drop when encountering mismatched degradations in real scenarios.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.45]{framework}
    \caption{Overview of the proposed method. The solid lines 
        denote the whole inference pipeline of our method. For ease
        of comparison, we also mark out the forward and reverse processes
        of the diffusion model by dotted lines.}
    \label{fig:framework}
    \vspace{-2mm}
\end{figure}

Various constraints or priors have been designed to mitigate the influence of such a deviation and improve the restoration quality. The  (or ) loss is commonly used to ensure fidelity, although these pixel-wise losses are known to favor the prediction of an average (or a median) over plausible solutions.
Recent BFR methods also introduce the adversarial loss~\cite{goodfellow2014generative} and the perceptual loss~\cite{johnson2016perceptual,zhang2018unreasonable} to achieve more realistic results.
Besides, some existing methods also exploit face-specific priors to further constrain the restored solution, e.g., face landmarks~\cite{chen2018fsrnet}, facial components~\cite{li2020blind}, and generative priors~\cite{chan2021glean,pan2021exploiting,wang2021towards,yang2021gan,toward2022zhou}.
Considering so many constraints together makes the training unnecessarily complicated, often requiring laborious hyper-parameter tuning to make a trade-off among these constraints.
Worse, the notorious instability of adversarial loss makes the training more challenging. 

\begin{figure*}[h]
    \centering
    \includegraphics[scale=0.97]{Method2.pdf}
    \caption{Comparative results with recent state-of-the-art methods on one typical real example. From left to right:
        (a) low-quality image, (b)-(g) restored results of GLEAN~\cite{chan2021glean}, PSFRGAN~\cite{chen2021progressive},
    GFPGAN~\cite{wang2021towards}, VQFR~\cite{gu2022vqfr}, CodeFormer~\cite{toward2022zhou}, and our proposed method.}
    \label{fig:method_visulization}
\end{figure*}

In this work, we devise a novel BFR method \textit{DifFace}, inspired by the recent success of diffusion model in image generation~\cite{sohl2015deep,dhariwal2021diffusion}.
Our method does not require multiple constraints in training, and yet is capable of dealing with unknown and complex degradations. 
Importantly, we leverage the rich image priors and strong generative capability of a pretrained diffusion model without retraining it on any manually assumed degradations. 

To this end, we establish a posterior distribution , aiming to infer the HQ image  conditioned on its LQ counterpart . Due to the complex degradations, solving for this posterior is non-trivial in blind restoration.
Our solution to this problem, as depicted in Fig.~\ref{fig:framework}, is to approximate this posterior distribution by a transition distribution , where  is a diffused version of the desirable HQ image , followed with a reverse Markov chain that estimates  from . The transition distribution  can be built up by introducing a neural network that is trained just using  loss, and the reverse Markov chain is readily borrowed from a pretrained diffusion model containing abundant image priors.

The constructed transition distribution is appealing in that it is well motivated by an important observation in DDPM~\cite{ho2020denoising}, where data is destroyed by re-scaling it with a factor of less than 1 and adding noise in the diffusion process. Bringing this notion to our context, the residual between  and  is also contracted by this factor after diffusion.
Our framework uniquely leverages this property by inferring the intermediate diffused variable  (where ) from the LQ image , of which the residual to HQ image  is reduced. And then from this intermediate state we infer the desirable . There are several advantages of doing so: i) our solution is more efficient than the full reverse diffusion process from  to , ii) we do not need to retrain the diffusion model from scratch, and iii) we can still take advantage of the pre-trained diffusion model via the reverse Markov chain from  to .

SR3~\cite{saharia2022image} also exploits the potentials of the diffusion model for blind restoration. It feeds the LQ image into the diffusion model as a condition to guide the restoration at each timestep. This requires one to retrain the diffusion model from scratch on pre-collected training data. Hence, it would still suffer from the issue of degradation mismatch when dealing with real-world data. Unlike SR3, our method does not need to train the diffusion model from scratch but sufficiently leverages the prior knowledge of the pretrained diffusion model. The unique design on transition distribution  further allows us to better cope with unknown degradations.

In summary, the contributions of this work are as follows:
\begin{itemize}[topsep=0pt,parsep=0pt,leftmargin=18pt]
    \item We devise a new diffusion-based BFR approach to cope with severe and unknown degradations. Formulating the posterior distribution as a Markov chain that starts from the LQ image and ends at the desirable HQ image is novel. We show that the Markov chain can compress the predicted error by a factor of less than 1.
    \item We show that the image prior captured in a pretrained diffusion model can be harnessed by having the Markov chain built partially on the reverse diffusion process. Such a unique design also allows us to explicitly control the restoration's fidelity and realism by changing the Markov chain's length.
    \item We show that BFR can be achieved without complicated losses as in existing methods. Our method only needs to train a neural network with  loss, simplifying the training pipeline. A preview of our results compared with existing methods is shown in Fig.~\ref{fig:method_visulization}.
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[scale=1.00]{recons_comparison}
    \caption{Illustration of the diffused  (top row) and the reconstructed results
        (bottom row) by a pretrained diffusion model from different
        starting timesteps. Note that the employed diffusion model is trained
        with 1000 discrete steps following~\cite{dhariwal2021diffusion}.}
    \label{fig:recons_comprison}
\end{figure*}
\section{Related Work}
The BFR approaches mainly focused on exploring better face priors. We review a few popular priors in this section, including geometric priors, reference priors, and generative priors. Diffusion prior can be considered as a type of generative prior.

\noindent\textbf{Geometric Priors}. Face images are highly structured compared with natural images. The structural information, such as facial landmarks~\cite{chen2018fsrnet}, face parsing maps~\cite{chen2021progressive,shen2018deep,chen2021progressive}, facial component heatmaps~\cite{li2020blind}, and 3D shapes~\cite{ren2019face,hu2020face,zhu2022blind}, can be used a guidance to facilitate the restoration. The geometric face priors estimated from degraded inputs can be unreliable, affecting the performance of the subsequent BFR task.

\noindent\textbf{Reference Priors}. 
Some existing methods~\cite{li2018learning,dogan2019exemplar} guide the restoration with an additional HQ reference image that owns the same identity as the degraded input. The main limitations of these methods stem from their dependence on the HQ reference images, which are inaccessible in some scenarios.
Li~\textit{et al.}~\cite{li2020blind} address this limitation by constructing an offline facial component dictionary based on the features extracted from HQ images. Then, it searches for the closest facial components in this dictionary for the given LQ images during restoration. 

\noindent\textbf{Generative Priors}. Unlike \cite{li2020blind},  more recent approaches directly exploit the rich priors encapsulated in generative models for BFR. Following the paradigm of GAN inversion~\cite{xia2022gan}, the earliest explorations~\cite{menon2020pulse,gu2020image,pan2021exploiting} iteratively optimize the latent code of a pretrained GAN for the desirable HQ target. To circumvent the time-consuming optimization, some studies~\cite{chan2021glean,wang2021towards,yang2021gan} directly embed the decoder of the pre-trained StyleGAN~\cite{karras2019style} into the BFR network, and evidently improve the restoration performance.
The success of VQGAN~\cite{esser2021taming} in image generation also inspires several BFR methods. These methods mainly design different strategies, e.g., cross-attention~\cite{wang2022restoreformer}, parallel decoder~\cite{gu2022vqfr}, and transformer~\cite{toward2022zhou}, to improve the matching between the codebook elements of the degraded input and the underlying HQ image.


Attributed to the powerful generation capability of the diffusion model, some works based on diffusion model have been proposed recently. Typically, SR3~\cite{saharia2022image} and SRDiff~\cite{li2022srdiff} both feed the LQ image into the diffusion model as a condition to guide the restoration in training.
To accelerate the inference speed, LDM~\cite{rombach2022high} proposed to train the diffusion model in latent space of VQGAN~\cite{esser2021taming}.
These methods require one to retrain the diffusion model from scratch on some pre-collected data. The learned model would still be susceptible to the degradation mismatch when generalizing to other datasets. Therefore, this paper devises a new learning paradigm based on a pretrained diffusion model to address these issues.

\section{Preliminaries}\label{sec:preliminary}
We provide a brief introduction to the diffusion probabilistic model~\cite{sohl2015deep} (known as diffusion model for brevity) to ease the subsequent presentation of our method.



Diffusion model consists of a forward process (or diffusion process) and a reverse process. Given a data point  with probability distribution , the forward process gradually destroys its data structure by repeated application of the following Markov diffusion kernel:

where ,  is a pre-defined or learned noise variance schedule. With a rational design on the variance schedule, it theoretically guarantees that  converges to the unit spherical Gaussian distribution. It is noteworthy that the marginal distribution at arbitrary timestep  has the following analytical form: 

where .

As for the reverse process, it aims to learn a transition kernel from  to , which is defined as the following Gaussian distribution:

where  is the learnable parameter. With such a learned transition kernel, we can approximate the data distribution  via the following marginal distribution:

where .

\section{Proposed Method} \label{sec:method}
In this section, we present our BFR method that exploits the image priors encapsulated in a pretrained diffusion model. To keep the notations consistent with Sec.~\ref{sec:preliminary}, we denote the LQ image and HQ image as  and . To restore the HQ image from its degraded counterpart, we aim at designing a rational posterior distribution of .

\subsection{Motivation} \label{subsec:motivation}
Considering a diffusion model with  discrete steps, it provides a transition function from  to . With the aid of this transition, we can construct the posterior distribution  as follows:

where  is an arbitrary timestep. Therefore, we can restore  from  by sampling from this posterior using ancestral sampling~\cite{bishop2006pattern} from timestpe  to  as follows:

Since the transition kernel  can be readily borrowed from a pretrained diffusion model, our goal thus turns to design the transition distribution of .

We have an important observation by delving into Eq.~(\ref{eq:sampling_restore}). If replacing  with the marginal distribution  defined in Eq.~(\ref{eq:xt_cond_x0}), Eq.~\eqref{eq:sampling_restore} degenerates into the diffusion and reconstruction process for  via a pretrained diffusion model, i.e.,

In Figure~\ref{fig:recons_comprison}, we show some diffused and reconstructed results under different settings for the starting timestep . One can observe that when  lies in a reasonable range (e.g., ), meaning that  is slightly ``destroyed'', it is possible to accurately reconstruct  using the pretrained diffusion model.

This observation indicates that  is an ideal choice for the desired  by setting a reasonable . Since the HQ image  is inaccessible in the task of BFR, we thus explore how to design a plausible  to approximate .

\subsection{Design}
Recall that our goal is to design a transition distribution  to approximate . Fortunately, the target distribution  has an analytical form as shown in Eq.~(\ref{eq:xt_cond_x0}). This inspires us to formulate  as a Gaussian distribution as follows:

where  is a neural network with parameter , aiming to provide an initial prediction for . It should be noted that the final restored result by our method is achieved by sampling from the whole Markov chain of Eq.~(\ref{eq:sampling_restore}) (see Fig.~\ref{fig:framework}), and not directly predicted by . As for , it is only used to construct the marginal distribution of , a diffused version of , and thus named as ``diffused estimator'' in this work.

Next, we consider the Kullback-Leibler (KL) divergence between the designed  and its target . By denoting the predicted error of  as , we have

where . As shown in Fig.~\ref{fig:curve_alpha},  strictly decreases monotonically with the timestep . Hence, larger  will offer a better approximation to , and further achieve a more realistic image via the designed posterior distribution of Eq.~\eqref{eq:bfr_postirior}. However,  will contain more noises when  is getting larger, as shown in Fig.~\ref{fig:recons_comprison}. Thus, an overly large  will inevitably deviate the restored result from the ground truth . Therefore, the choice of  induces a realism-fidelity trade-off for the restored HQ image. We provide the ablation study in Sec.~\ref{sec:model_analysis}.
\begin{figure}[t]
    \centering
    \vspace{-3mm}
    \includegraphics[scale=0.28]{alpha_klcoef}
    \caption{The curves of  and  with the starting timestep .}
    \label{fig:curve_alpha}
\end{figure}
\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.99]{timestep_restore_V3}
    \caption{An example restored by \textit{DifFace} under different settings of the starting timestep  and the diffused estimator . (a) LQ image, (b) HQ image, (c1)-(c2) restored results by SRCNN and SwinIR, (d1)-(i1) restored results by \textit{DifFace} that takes SRCNN as a diffused estimator, (d2)-(i2) restored results by \textit{DifFace} that takes SwinIR as a diffused estimator.}
    \label{fig:time_restore}
\end{figure*}

\subsection{Discussion} \label{subsec:remark}
Instead of directly learning a mapping from  to  under multiple constraints like current deep learning-based methods~\cite{chen2021progressive,yang2021gan,wang2021towards}, the proposed method circumvents this by predicting , a diffused version of , from . Such a new learning paradigm brings several significant advantages compared to existing approaches: 
\begin{itemize}[topsep=0pt,parsep=0pt,leftmargin=15pt]
    \item \textbf{Error Contraction}. Considering the diffused estimator , its predicted error is 
        denoted as . In our method, the problem of BFR is reformulated to predict  as explained in Sec.~\ref{subsec:motivation}. According to Eq.~\eqref{eq:distribution_xs_y0},
         can be accessed as follows:
        
        where .
        It can be seen that the predicted error  is contracted by a factor of  under our proposed paradigm of predicting ,
        where  is less than 1 as shown in Fig.~\ref{fig:curve_alpha}. 

        \vspace{1mm}
        Attributed to such an error contraction, our method has a greater error tolerance to the diffused estimator.
        Hence,  can be simply trained with  loss on some synthetic data in this work. This unique flexibility
        bypasses the sophisticated training process compared with most recent methods~\cite{li2020blind,wang2021towards,gu2022vqfr}. 
        What is more, this capability of compressing errors intuitively improves the robustness of our
        method, especially when dealing with severe and complex degradations.
\item \textbf{Diffusion Prior}. After obtaining the diffused  via Eq.~(\ref{eq:sampling_xs_error1}), our
        method gradually generates the desirable HQ result by sampling recursively from
         starting from  and ending at .
        Through this sampling procedure, we effectively leverage the rich image priors and powerful generation
        capability of the pretrained diffusion model to help the restoration task.
        Unlike existing methods, since the diffusion model is completely trained on the HQ images in an
        unsupervised manner, it thus reduces the dependence of our method on the manually synthesized
        training data, of which the distribution may deviate from the true degradation.
\end{itemize}


\textit{These two intrinsic properties are mainly delivered by the constructed posterior in Eq.~\eqref{eq:bfr_postirior}, which is the core formulation of our method. Such a posterior is specifically designed for BFR to mitigate the robustness issue caused by complicated degradations. Therefore, our proposed method indeed renders a new simple yet robust learning paradigm for BFR, rather than a straightforward application of diffusion model in this problem}.

\section{Model Analysis}\label{sec:model_analysis}
In this section, we analyze the influence of the starting timestep  and the diffused estimator  to our model. We consider two classical network architectures as the backbone for , i.e., SRCNN~\cite{dong2015image} and SwinIR~\cite{liang2021swinir}.
In the rest of this paper, we briefly denote the proposed method as \textit{DifFace} for convenience of presentation.

\begin{table*}[t]
    \centering
    \caption{Quantitative comparisons of different methods on CelebA-Test. The best and second best results
    are highlighted in \textbf{bold} and \underline{underline}, respectively.}
    \label{tab:metirc_celeba}
    \small
    \begin{tabular}{@{}C{1.4cm}@{}|
                    @{}C{1.9cm}@{} @{}C{2.0cm}@{} @{}C{2.0cm}@{}
                    @{}C{2.0cm}@{} @{}C{2.1cm}@{} @{}C{2.0cm}@{}
                    @{}C{2.2cm}@{} @{}C{1.8cm}@{}}
        \Xhline{0.8pt}
        \multirow{2}*{Metrics}  & \multicolumn{8}{c}{Methods} \\
        \Xcline{2-9}{0.4pt} & DFDNet~\cite{li2020blind}         & PULSE~\cite{menon2020pulse}   & PSFRGAN~\cite{chen2021progressive} 
                            & GLEAN~\cite{chan2021glean}        & GFPGAN~\cite{wang2021towards} & VQFR~\cite{gu2022vqfr}             
                            & CodeFormer~\cite{toward2022zhou}  & \textit{DifFace} \\
PSNR      & 22.39                             & 21.59                         & 21.80
                            & 22.34                             & 21.25                         & 21.11
                            & \underline{22.60}                 & \textbf{23.30} \\
SSIM      & 0.627                             & \underline{0.675}             & 0.615
                            & 0.648                             & 0.615                         & 0.570
                            & 0.646                             & \textbf{0.694} \\
LPIPS   & 0.582                             & 0.519                         & 0.522
                            & 0.502                             & 0.532                         & 0.506
                            & \textbf{0.440}                    & \underline{0.453} \\
FID     & 83.96                             & 50.54                         & 67.13
                            & 82.34                             & 68.91                         & 61.55
                            & \underline{26.67}                 & \textbf{20.72} \\
IDS     & 86.65                             & 76.70                         & 73.84
                            & 73.26                             & 73.49                         & 72.23  
                            & \textbf{65.02}                    & \underline{66.17} \\
        \Xhline{0.8pt}
    \end{tabular}
    \vspace{-2mm}
\end{table*}
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.325]{metrics2_iccv}
    \caption{From left to right: (a) FID and LPIPS with respect to the starting timestep  on the validation dataset, (b) LPIPS with respect to the downsampling factor, namely  in Eq.~\eqref{eq:degradation_synthetic}, on the testing dataset.}
    \label{fig:fid_lpips}
\end{figure}
\noindent\textbf{Training Settings.} We train the diffused estimator  on the FFHQ dataset~\cite{karras2019style} that contains 70000 HQ face images. We firstly resize the HQ images into a resolution of , and then synthesize the LQ images following a typical degradation model used in recent literature~\cite{wang2021towards}:

where  and  are the LQ and HQ image,  is the Gaussian kernel with kernel width ,  is Gaussian noise with standard deviation ,  is 2D convolutional operator,  and  are the Bicubic downsampling or upsampling operators with scale , and  represents the JPEG compression process with quality factor . And the hyper-parameters , , , and  are uniformly sampled from , , , and  respectively. To evaluate the performances of \textit{DifFace} under different settings for  and , we selected 4000 HQ images from CelebA-HQ~\cite{karras2018progressive} and generated the LQ images via Eq.~(\ref{eq:degradation_synthetic}) as our validation dataset.
\begin{figure*}[t]
    \centering
    \includegraphics[scale=1.05]{syn3_iccv.pdf}
    \caption{Visual comparisons of different methods on the synthetic dataset CelebA-Test.}
    \label{fig:syn}
    \vspace{-2mm}
\end{figure*}

We adopt the Adam~\cite{kingma2015adam} algorithm to optimize the network parameters  under  loss. The batch size is set as 16, and other settings of Adam follow the default configurations of Pytorch~\cite{paszke2019pytorch}. We train the model for 600k iterations, and the learning rate is decayed gradually from  to  with the cosine annealing schedule~\cite{DBLP:conf/iclr/LoshchilovH17}. The leveraged diffusion model was trained on the FFHQ~\cite{karras2018progressive} dataset based on the official code\footnote{\url{https://github.com/openai/guided-diffusion}} of~\cite{dhariwal2021diffusion}. It contains 1,000 discrete diffusion steps, and is accelerated to 250 steps using the technique in~\cite{nichol2021improved} when applied in~\textit{DifFace}.

\noindent\textbf{Starting Timestep}. In Fig.~\ref{fig:time_restore}, we show some restored results by \textit{DifFace} under different settings for the starting timesteps .
The results show that one can make a trade-off between realism and fidelity through different choices of . In particular, if we set a larger , the restored results would appear more realistic but with lower fidelity in comparison to the ground truth HQ images. The phenomenon is reflected by the average FID~\cite{heusel2017gans} and LPIPS~\cite{zhang2018unreasonable} scores with respect to  in Fig.~\ref{fig:fid_lpips}~(a).
The proposed \textit{DifFace} performs very well in the range of , and we thus set  as  throughout the whole experiments in this work. In practice, we speed up the inference four times following~\cite{nichol2021improved}, and thus sample 100 steps for each testing image.

\noindent\textbf{Diffused Estimator}. Figure~\ref{fig:time_restore} displays an example restored by \textit{DifFace}, which either takes SRCNN~\cite{dong2015image} or SwinIR~\cite{liang2021swinir} as the diffused estimator .
More details and comparisons on these two backbones are provided in Appendix~\ref{sec:backbone_supp}.
Even with the simplest SRCNN that only contains several plain convolutional layers, \textit{DifFace} is able to restore a plausible HQ image. Using a more elaborated architecture like SwinIR results in more apparent details (e.g., hairs). The results suggest the versatility of \textit{DifFace} in the choices of the diffused estimators. In the following experiments, we use SwinIR as our diffused estimator. 


\section{Experimental Results}\label{sec:exp}
In this section, we conducted extensive experiments to verify the effectiveness of \textit{DifFace} on both synthetic and real-world datasets. The training follows the settings detailed in Sec.~\ref{sec:model_analysis}.
\begin{table*}[t]
    \centering
    \caption{FID scores of different methods on three real-world testing datasets. The best
        and second best results are highlighted in \textbf{bold} and \underline{underline},
        respectively.}
    \label{tab:metirc_fid_real}
    \small
    \begin{tabular}{@{}C{2.4cm}@{}| @{}C{1.5cm}@{}|
                    @{}C{1.6cm}@{} @{}C{1.6cm}@{} @{}C{1.8cm}@{}
                    @{}C{1.6cm}@{} @{}C{1.7cm}@{} @{}C{1.6cm}@{}
                    @{}C{1.8cm}@{} @{}C{1.8cm}@{}}
        \Xhline{0.8pt}
        \multirow{2}*{Datasets} & \multirow{2}*{\# Images} & \multicolumn{8}{c}{Methods} \\
        \Xcline{3-10}{0.4pt}
&      & DFDNet                           & PULSE                             & PSFRGAN
                                 & GLEAN                            & GFPGAN                            & VQFR
                                 & CodeFormer                       & \textit{DifFace} \\
        \Xhline{0.4pt}
        WIDER-Test        & 970  & 58.23                             & 69.55                            & 49.85 
                                 & 70.12                             & 39.76                            & 44.54
                                 & \underline{39.21}                 & \textbf{37.49} \\
        LFW-Test          & 1711 & 59.83                             & 65.17                            & 49.80
                                 & \underline{46.39}                 & 50.13                            & 50.88
                                 & 52.42                             & \textbf{45.23} \\
        WebPhoto-Test     & 407  & 92.82                             & 86.40                            & 85.45
                                 & 94.34                             & 87.86                            & \textbf{75.46} 
                                 & \underline{83.70}                 & 85.52 \\
        \Xhline{0.8pt}
    \end{tabular}
\end{table*}

\subsection{Evaluation Setting}
\noindent\textbf{Testing Datasets}. We evaluate \textit{DifFace} on one synthetic dataset and three real-world datasets. The synthetic dataset, denoted as CelebA-Test, contains 4000 HQ images from CelebA-HQ~\cite{karras2018progressive}, and the corresponding LQ images are synthesized via Eq.~(\ref{eq:degradation_synthetic}). The specific settings on the hyper-parameters of the degradation can be found in Appendix~\ref{sec:evalation_setup_supp}.
As for the real-world datasets, we consider three typical ones with different degrees of degradation, namely LFW-Test, WebPhoto-Test~\cite{wang2021towards}, and WIDER-Test~\cite{toward2022zhou}. LFW-Test consists of 1711 mildly degraded face images in the wild, which contains one image for each person in LFW dataset~\cite{huang2008labeled}. WebPhoto-Test is made up of 407 face images crawled from the internet. Some of them are old photos with severe degradation. WIDER-Test selects 970 face images with very heavy degradations from the WIDER Face dataset~\cite{yang2016wider}, thus is suitable to test the robustness of different methods under severe degradations.

\noindent\textbf{Comparison Methods.} We compare \textit{DifFace} with seven recent BFR methods, including DFDNet~\cite{li2020blind}, PULSE~\cite{menon2020pulse}, PSFRGAN~\cite{chen2021progressive}, GLEAN~\cite{chan2021glean}, GFPGAN~\cite{wang2021towards}, VQFR~\cite{gu2022vqfr}, and CodeFormer~\cite{toward2022zhou}.

\subsection{Evaluation on Synthetic Data}
To evaluate different methods comprehensively, we adopt five quantitative metrics, namely PSNR, SSIM~\cite{wang2004image}, LPIPS~\cite{zhang2018unreasonable}, FID~\cite{heusel2017gans}, and Identity Score (IDS). Specifically, LPIPS is a learned perceptual similarity metric calculated based on deep features of VGG~\cite{DBLP:journals/corr/SimonyanZ14a}. IDS is the embedding angle of ArcFace~\cite{deng2019arcface} between two images, mainly reflecting identity preservation. FID measures the KL divergence between the feature distributions (assumed as Gaussian) of the restored images and the ground truth images.


We summarize the comparative results on CelebA-Test in Table~\ref{tab:metirc_celeba}. The proposed \textit{DifFace} achieves the best or second-best performance across all five metrics, indicating its effectiveness and superiority in the task of BFR.
To further verify the robustness of \textit{DifFace}, we compare it with recent state-of-the-art methods, namely CodeFormer and VQFR, using LPIPS under different degrees of degradation in Fig.~\ref{fig:fid_lpips}~(b). We generate different degrees of degradation by increasing the scale factors from 4 to 40 with step 4 gradually, to the images in CelebA-Test. A total of 400 image pairs are generated for each scale. Figure~\ref{fig:fid_lpips}~(b) records the averaged performance of different methods over these 400 images with respect to different downsampling scale factors. While \textit{DifFace} is slightly inferior to CodeFormer and VQFR under small scale factors (mild degradations), its performance drops more gracefully and surpasses them in the cases of larger factors (severe degradations). These results substantiate the robustness of our \textit{DifFace} in the scenarios with very severe degradations and is consistent with the analysis in Sec.~\ref{subsec:remark}.

For visualization, three typical examples of the CelebA-Test are shown in Fig.~\ref{fig:syn}. In the first example with mild degradation, most of the methods are able to restore a realistic-looking image. PULSE produces a plausible face but fails to preserve the identity since its optimization cannot find the correct latent code through GAN inversion.
In the second and third examples that exhibit more severe degradation, only CodeFormer and \textit{DifFace} can handle such cases and return satisfactory face images. However, the results of CodeFormer still contain some slight artifacts in the areas of whiskers and hairs (marked by red arrows in Fig.~\ref{fig:syn}). Compared with CodeFormer, \textit{DifFace} performs more stably under this challenging degradation setting. Such robustness complies with our observation in Fig.~\ref{fig:fid_lpips} (b).
\begin{table}[t]
    \centering
    \caption{Performance comparisons of \textit{DifFace} under different acceleration settings. ``\textit{Dif}(A/B)'' means that the whole reverse process of the pretained diffusion model contains A sampling steps, and the starting timesteps  in \textit{DifFace} is set as B. Besides, we also report the comparisons on the model size, i.e., the number of parameters (in megas), and the runtime (in seconds). It should be noted that, as for \textit{DifFace}, the number of parameters of the pretrained diffusion model is also included and marked by ``\textcolor[gray]{0.5}{gray}''.}
    \label{tab:accelerate_performance}
    \small
    \begin{tabular}{@{}C{2.1cm}@{}| 
                    @{}C{0.9cm}@{} @{}C{0.9cm}@{}
                    @{}C{1.0cm}@{} @{}C{0.9cm}@{} | @{}C{1.2cm}@{}|
                    @{}C{1.3cm}@{}}
        \Xhline{0.8pt}
        Methods               & PSNR   & SSIM      & LPIPS  & FID  & Runtime  &\# Params  \\
        \Xhline{0.4pt}
        \textit{Dif}(500/200) & 23.27  & 0.686   & 0.457  & 20.30  & 8.58         & \multirow{5}*{\makecell{15.79\\{\textcolor[gray]{0.5}{+159.59}}}} \\
        \textit{Dif}(250/100) & 23.30  & 0.694   & 0.453  & 20.72  & 4.32         & \\
        \textit{Dif}(100/40)  & 23.29  & 0.698   & 0.453  & 23.63  & 1.77         &\\
        \textit{Dif}(50/20)   & 23.20  & 0.701   & 0.459  & 27.40  & 0.92         &\\
        \textit{Dif}(20/8)    & 22.20  & 0.662   & 0.515  & 31.42  & 0.41         &     \\
        \Xhline{0.4pt}
        CodeFormer            & 22.60  & 0.646   & 0.440  & 26.67  & 0.08         & 94.11     \\
        DFDNet                & 22.39  & 0.627   & 0.582  & 83.96  & 1.40         & 240.11     \\
        PULSE                 & 21.59  & 0.675   & 0.519  & 50.54  & 3.93         & 24.11     \\
        \Xhline{0.8pt}
    \end{tabular}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.90]{real3_iccv.pdf}
    \caption{Visual comparisons of different methods on the real-world examples from LFW-Test (first row),
        WebPhoto-Test (second row), and WIDER-Test (third row).}
    \label{fig:real}
    \vspace{-2mm}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[scale=1.00]{random_real}
    \caption{Unlike most existing BFR methods, \textit{DifFace} can generate multiple
        diverse and plausible results given a LQ image, by setting different random seeds for
        the diffusion model. This example is extracted from real-world dataset WIDER-Test.}
    \label{fig:random_real}
\end{figure*}
\subsection{Evaluation on Real-world Data}\label{sec:exp_real}
In the experiments on real-world datasets, we mainly adopt FID as the quantitative metric since the ground truths are inaccessible. We first estimate the feature statistics on the restored images and the HQ images in FFHQ~\cite{karras2019style} dataset, respectively, and then calculate the KL divergence as FID. We attempted other non-reference metrics, such as NIQE~\cite{mittal2012making}, NRQM~\cite{ma2017learning}, and PI~\cite{blau20182018}. These metrics are not adopted here as we empirically found that they behave unusually pessimistic for diffusion-based methods. For completeness, we provide the experimental results on non-reference metrics in Appendix~\ref{sec:non_refer_supp}.

The comparative results are summarized in Table~\ref{tab:metirc_fid_real}. We can observe that \textit{DifFace} achieves the best performance on both WIDER-Test and LFW-Test. On the WebPhoto-Test, it also surpasses most recent BFR methods. It should be noted that the FID values on WebPhoto-Test may not be representative as this dataset contains too few images (total 407) to estimate the feature distribution of the restored images. To supplement the analysis, we show three typical examples of these datasets in Fig.~\ref{fig:real}, and more visual results are given in Fig.~\ref{fig:real_appedix} of Appendix. It is again observed that \textit{DifFace} provides better results, especially in the second and third examples with severe unknown degradations.

Most existing BFR methods produce only one HQ image for each LQ input, although there may be many reasonable possibilities. This is because they only learn  a deterministic mapping between the LQ and HQ images. It is interesting to note that \textit{DifFace}, as shown in Fig.~\ref{fig:random_real}, is capable of producing multiple diverse and plausible HQ solutions for any given LQ image by setting different random seeds for the pretrained diffusion model. More results are shown in Fig.~\ref{fig:random_real_appedix} of Appendix. This ``pluralistic'' property is favorable in BFR, as there exist many different HQ images that can generate the same LQ counterpart.

\subsection{Limitation}
Despite the good performance of \textit{DifFace}, the inference efficiency of our method is limited by the iterative sampling process inherited from the diffusion model. In this paper, we mainly report the results of \textit{DifFace} by setting the starting timestep to be 100. To more comprehensively evaluate its efficiency, we list the performances of \textit{DifFace} on CelebA-Test under different acceleration settings in Table~\ref{tab:accelerate_performance}. We can see that it is possible to reduce the sampling process to 20 steps without evident performance drop. Under this setting, \textit{DifFace} is still comparable to CodeFormer. This cuts the running time to 0.92s, lying between DFDNet and CodeFormer. In future work, we will explore more advanced accelerated techniques to further impove its inference speed. As for the model size, the diffused estimator and the pretrained diffusion model in \textit{DifFace} contains 15.79M and 159.59M parameters respectively. This is also comparable to DFDNet and CodeFormer.



\section{Conclusion}
We have proposed a new BFR method called \textit{DifFace} in this work. \textit{DifFace} is appealing as it only relies on a restoration backbone that is trained with  loss. This vastly simplifies the complicated training objectives in most current approaches. Importantly, we have proposed a posterior distribution that is well-suited for BFR. It consists of a transition kernel and a Markov chain partially borrowed from a pre-trained diffusion model. The former acts as an error compressor, and thus makes our method more robust to severe degradations. The latter effectively leverages the powerful diffusion model to facilitate BFR. Extensive experiments have demonstrated the effectiveness and robustness of our method both on synthetic and real-world datasets. We hope that this work could inspire more robust diffusion-based restoration methods in the future.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib_difface}
}

\clearpage
\appendix
\section{Setup on Validation and Testing Datasets}\label{sec:evalation_setup_supp}
In the main text, we synthesized one validation dataset and one testing dataset, each of which contains 4,000 HQ images randomly selected from CelebA-HQ~\cite{karras2018progressive}. The former is used to select a reasonable starting timestep  for \textit{DifFace}, and the latter to evaluate the performance of different methods. To synthesize the LQ images, we employ the degradation model of Eq.~(11) in the main text with the hyper-parameter settings in Table~\ref{tab:datasetup}. Note that , , and  is used to control the generation of the blur kernel  through a variance matrix defined as follows:


\section{Diffused Estimator}\label{sec:backbone_supp}
\subsection{Network Architectures}
We consider two typical restoration backbones for the diffused estimator , namely SRCNN~\cite{dong2015image} and SwinIR~\cite{liang2021swinir}. To apply them in our method, we slightly adjust their settings. The LQ/HQ image pairs in our method are both with size , we add two (or three) PixelUnshuffle~\cite{shi2016real} layers with a downscale factor 2 to reduce the input size to  (or ) for SRCNN (or SwinIR). After each PixelUnshuffle layers except the last one, one convolutional and LeakyReLU layers are followed to fuse the features. Similarly, two (or three) PixelShuffle~\cite{shi2016real} layers are also added to the tail of SRCNN (or SwinIR) to upsample the size back to .

As for SRCNN, we adopt nine convolutional layers with kernel size 5 between the PixelUnshuffle and PixelShuffle layers, and each convolutional layer has 64 channels. As for SwinIR, we follow the official settings\footnote{\url{https://github.com/JingyunLiang/SwinIR}} for real-world image super-resolution task with a scale factor 8.
\subsection{Performance Comparison}
To quantitatively evaluate these two restoration backbones and their influences to \textit{DifFace}, we conduct experiments on CelebA-Test and summarize the results in Table~\ref{tab:metirc_backbone}. We also provide some quantitative comparisons in Figure~\ref{fig:time_restore_supp_real} on the real datasets LFW-Test (top row) and WIDER-Test (bottom row). The results offer a few observations as follows:
\begin{itemize}[topsep=0pt,parsep=0pt,leftmargin=18pt]
    \item Even though taking the simplest backbone SRCNN as our diffused estimator, \textit{DifFace}
        is still able to produce highly natural-looking results with realistic image details, such as hairs and
        stubbles. This delivers that the proposed \textit{DifFace} does not rely on specific designs on the
        diffused estimator.
    \item Owning to more advanced network architectures as well as more learnable parameters, SwinIR outperforms
        SRCNN in both qualitative and quantitative comparisons.
    \item When the starting timestep  gets larger, \textit{DifFace} is capable of achieving more realistic results. However,
        larger  will introduce more randomness and thus reduce the fidelity.
\end{itemize}
Based on these observations, we suggest taking SwinIR as the diffused estimator attributed to its superior performance. As for the starting timestep , the proposed \textit{DifFace} performs stably and well when fixing it in the range .

\begin{table}[t]
    \centering
    \caption{Hyper-parameter settings of the degradation model on the valiation and testing data sets. Note that , , and  control the generation of the blur kernel  through a covariance matrix defined in Eq.~\eqref{eq:kernel_generation}.}
        \label{tab:datasetup}
    \small
    \begin{tabular}{@{}C{3.5cm}@{}|@{}C{4.8cm}@{}}
        \Xhline{0.8pt}
        Hyper-parameters & range set\\
        \Xhline{0.4pt}
                      &   \\
        \Xhline{0.4pt}
                 &  \\
        \Xhline{0.4pt}
                      &   \\
        \Xhline{0.4pt}
               &   \\
        \Xhline{0.4pt}
                 &  \\
        \Xhline{0.8pt}
    \end{tabular}
\end{table}
\begin{table*}[t]
    \centering
    \caption{Quantitative comparisons of SRCNN, SwinIR and \textit{DifFace} on 
        CelebA-Test. ``\textit{DifFace}\textcolor[gray]{0.5}{(X)}'' means that it takes the
        backbone ``\textcolor[gray]{0.5}{X}'' as the diffused estimator . The
        parameters in ``\textit{DifFace}\textcolor[gray]{0.5}{(X)}'' includes both of that in
        the backbone ``\textcolor[gray]{0.5}{X}'' and the pre-trained diffusion model (159.59M).}
        \label{tab:metirc_backbone}
    \small
    \begin{tabular}{@{}C{3.5cm}@{}|@{}C{2.1cm}@{} @{}C{2.1cm}@{}
                    @{}C{2.1cm}@{} @{}C{2.1cm}@{} @{}C{2.1cm}@{} @{}C{3.4cm}@{}}
        \Xhline{0.8pt}
        \multirow{2}*{Methods} & \multicolumn{6}{c}{Metrics} \\
        \Xcline{2-7}{0.4pt}
                        & PSNR  & SSIM  & LPIPS  & FID  & IDS & \# Parameters(M)     \\
        \Xhline{0.4pt}
        SRCNN           & 23.58           & 0.722           & 0.555              & 108.68           & 73.09           & 1.03   \\
        \textit{DifFace}\textcolor[gray]{0.5}{(SRCNN)}
                        & 22.88           & 0.685           & 0.487              & 32.06            & 70.65           & 1.03159.59 \\
        \hline\hline
        SwinIR          & 24.32           & 0.736           & 0.507              & 93.73            & 63.80           & 15.79    \\
        \textit{DifFace}\textcolor[gray]{0.5}{(SwinIR)}
                        & 23.30           & 0.694           & 0.453              & 20.72            & 66.17           & 15.79159.59 \\
        \Xhline{0.8pt}
    \end{tabular}
\end{table*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{timestep_restore_real_all}
    \caption{Qualitative comparisons of \textit{DifFace} on LFW-Test (top row) and
        WIDER-Test (bottom row) under different settings for the starting timestep  and the
        diffused estimator . (a) LQ image, (b1)-(b2) restored results
        by SRCNN and SwinIR, (c1)-(h1) restored results by \textit{DifFace} that
        takes SRCNN as diffused estimator, (c2)-(h2) restored results by \textit{DifFace}
        that takes SwinIR as diffused estimator.}
    \label{fig:time_restore_supp_real}
\end{figure*}

\section{Bicubic Super-Resolution}
Diffusion model can be used for image restoration by directly introducing the LQ image  as a condition to guide the generation of ~\cite{saharia2022palette,saharia2022image,song2020score}, namely

where ,  is the learned transition kernel from  to  conditioned on . To access such a transition kernel, an intuitive way is to retrain the diffusion model from scratch. Specifically, in each timestep ,  and  are concatenated together as input to predict  in training. Following this paradigm, SR3~\cite{saharia2022image} trained a diffusion model for image super-resolution based on bicubic degradation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{BicubicSR3}
    \vspace{-3mm}
    \caption{Visual comparisons of SR3 and \textit{DifFace} on 8x (left) and 16x (right) face image super-resolution.}
    \label{fig:bicubic_sr3}
\end{figure*}
\begin{table}[t]
    \centering
    \caption{Quantitative results of SR3 and \textit{DifFace} on bicubic face image super-resolution.}
    \label{tab:metirc_bicubic}
    \small
    \begin{tabular}{@{}C{1.5cm}@{}| @{}C{1.2cm}@{}|
                    @{}C{1.4cm}@{} @{}C{1.4cm}@{} @{}C{1.4cm}@{} @{}C{1.4cm}@{}}
        \Xhline{0.8pt}
        \multirow{2}*{Methods} & \multirow{2}*{\makecell{Scale\\factors}} & \multicolumn{4}{c}{Metrics} \\
        \Xcline{3-6}{0.4pt}
                          &                     & PSNR  & SSIM  & LPIPS    & IDS  \\
        \Xhline{0.4pt}
        SR3               &  \multirow{2}*{x8}  & 22.74           & 0.585           & 0.505                & 25.66             \\
        \textit{DifFace}  &                     & 27.24           & 0.754           & 0.375                & 38.89             \\
        \hline \hline
        SR3               &  \multirow{2}*{x16} & 20.50           & 0.506           & 0.645                & 64.19             \\
        \textit{DifFace}  &                     & 25.03           & 0.712           & 0.412                & 51.36             \\
        \Xhline{0.8pt}
    \end{tabular}
\end{table}

We omit the comparison against SR3~\cite{saharia2022image} in the main text as it can only deal with bicubic degradation. Here, we extend the proposed \textit{DifFace} to bicubic super-resolution and make a fair comparison with SR3. We do not make any changes apart from upsampling an LQ image with bicubic interpolation and feeding it into \textit{DifFace} to restore the high-resolution target. Although the degradation model we use (Eq.~(11) of the maintext) does not contain the bicubic degradation, we do not retrain or finetune our diffused estimator on such degradation.

Since the code of SR3 is not released, we thus adopt an unofficially re-implemented version\footnote{\url{https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement}} instead. This model is specifically trained for 8x face image super-resolution from size  to . To further evaluate the generalization capability to different degradation, we also test its performance on the task of 16x super-resolution from  to . As for the testing dataset, we randomly select 200 images from CelebA-HQ~\cite{karras2018progressive}.

Table~\ref{tab:metirc_bicubic} lists the quantitative results of SR3 and \textit{DifFace} on the task of bicubic super-resolution, and the corresponding visual comparisons are shown in Fig.~\ref{fig:bicubic_sr3}. \textit{DifFace} achieves better or at least comparable performance on 8x super-resolution, even though SR3 is specifically trained for this bicubic upsampling task. When generalized to 16x super-resolution, \textit{DifFace} outperforms SR3 on both qualitative and qualitative comparisons, indicating the robustness of \textit{DifFace} to unknown degradations. On the other hand, \textit{DifFace} is more efficient than SR3, because SR3 has to pass through the whole reverse process of the diffusion model while \textit{DifFace} starts from the intermediate state (i.e., ) of the reverse Markov chain.

Another popular diffusion based image super-resolution model is LDM~\cite{rombach2022high}. It trains a diffusion model to approximate the conditional distribution in Eq.~(\ref{eq:condition_SR3}) in the latent space of VQGAN~\cite{esser2021taming}, which evidently accelerates its inference speed. Since LDM only releases a pretrained model trained on natural images, we thus cannot provide comparisons with it on the task of BFR.

\begin{table*}[t]
    \centering
    \caption{Non-reference metrics of different methods on the real-world dataset WIDER-Test. ``\textcolor[gray]{0.5}{Diffusion}''
        denotes the average results on 3,000 images randomly generated by the pretrained diffusion model, which can be
        regarded as the upper bound of \textit{DifFace}. }
    \label{tab:non_reference_metirc}
    \small
    \begin{tabular}{@{}C{1.8cm}@{}|
                    @{}C{1.8cm}@{} @{}C{1.8cm}@{} @{}C{1.8cm}@{} @{}C{1.8cm}@{}
                    @{}C{1.8cm}@{} @{}C{1.8cm}@{} @{}C{1.8cm}@{} @{}C{1.8cm}@{} @{}C{1.8cm}@{}}
        \Xhline{0.8pt}
        \multirow{2}*{Metrics} & \multicolumn{9}{c}{Methods} \\
        \Xcline{2-10}{0.4pt}
                          & DFDNet & PULSE    & PSFRGAN & GLEAN  & GFPGAN & VQFR   & CodeFormer   & \textit{DifFace} & \textcolor[gray]{0.5}{Diffusion}\\
        \Xhline{0.4pt}
        NIQE  & 5.67   & 5.27     & 3.89    & 5.13   & 3.81   & 3.02   & 4.12         & 4.24             & \textcolor[gray]{0.5}{4.11} \\
NRQM    & 6.80   & 4.05     & 8.01    & 7.03   & 8.07   & 8.78   & 8.49         & 6.11             & \textcolor[gray]{0.5}{6.60} \\
f       PI    & 4.81   & 5.88     & 3.33    & 4.45   & 3.08   & 2.17   & 3.01         & 4.46             & \textcolor[gray]{0.5}{4.17} \\
        \Xhline{0.8pt}
    \end{tabular}
\end{table*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{random_real_supp}
    \caption{Three restoration examples of \textit{DifFace} on the real-world dataset WIDER-Test
        by setting different random seeds for the diffusion model.}
    \label{fig:random_real_appedix}
\end{figure*}
\section{Atmospheric Turbulence Restoration}
In this section, we consider a more challenging degradation type, namely atmospheric turbulence. It causes geometric distortion and blur effects on the captured images, and negatively affects the face-related downstreaming tasks. Note that none of the \textit{DifFace} and the other comparison methods contains any atmospheric turbulence or similar degradation types. Hence, it is suitable to compare the robustness of different methods on such a degradation type.

It is difficult to collect a large real-world dataset degraded by atmospheric turbulence, since most of the relevant datasets are privately protected due to their military use. We cropped eight real-world image pairs from the manuscript of~\cite{DBLP:journals/corr/abs-2112-02379}, and resize them to the resolution of 512. Each pair contains one image degraded by atmospheric turbulence and one corresponding reference image. We call this dataset as Turb8-Test in this work.

Figure~\ref{fig:real_turb} shows the qualitative comparison results of different methods on Turb8-Test dataset. \textit{DifFace} is able to achieve photo-realistic results and evidently outperforms other methods in visual quality. In particular, the first three examples represent the same person but with large pose variations, \textit{DifFace} still performs well in this scenario. Unfortunately, \textit{DifFace} sometimes cannot preserve the identity very well when the input loses too much image information, e.g., the last example in Fig.~\ref{fig:real_turb}. We leave this problem to future research.

\section{Discussion on Non-Reference Metrics}\label{sec:non_refer_supp}
In Sec.6.3, we evaluate the performance of different methods on real-world data using FID score. In this section, we further consider three commonly used non-reference metrics, namely NIQE~\cite{mittal2012making}, NRQM~\cite{ma2017learning}, and PI~\cite{blau20182018}.
Table~\ref{tab:non_reference_metirc} summarizes the detailed comparisons on WIDER-Test. Despite the strong superiority of \textit{DifFace}, it shows a surprisingly weak performance against GAN based methods (e.g., VQFR, GFPGAN, and PSFRGAN) across these three non-reference metrics.

To explore the underlying reasons behind this phenomenon, we randomly generate 3,000 HQ face images using the pre-trained diffusion model, and then evaluate their image quality based on these non-reference metrics. The average results are denoted as ``Diffusion'' and marked by \textcolor[gray]{0.5}{gray} color in Table~\ref{tab:non_reference_metirc}. One can regard this performance as the upper bound of our proposed \textit{DifFace}.

It is interesting to observe that even this upper bound is inferior to GAN based restoration methods. Such a conclusion is unreasonably pessimistic considering the various positive results of diffusion models reported againtst GANs~\cite{song2020score,dhariwal2021diffusion}. More investigations are called to study this gap introduced by the aforementioned non-reference metrics.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{briar8}
    \caption{Qualitative comparisons of different methods on the real-world Turb8-Test dataset.}
    \label{fig:real_turb}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{real8_supp_iccv}
    \caption{Visual comparisons of different method on three real-world datasets.}
    \label{fig:real_appedix}
\end{figure*}

\end{document}
