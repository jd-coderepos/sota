

\section{Experiments}

\begin{table*}[t]
\footnotesize
\begin{center}
\resizebox{2.0\columnwidth}{!}{\begin{tabular}{| c | l | l | c | c | c || c | c | c || c || c | c |}
\hline
&&& \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{|c|}{RefCOCO+} & \multicolumn{3}{|c|}{RefCOCOg}\\
\cline{2-12}
&&feature& val & testA & testB & val & testA & testB & val* & val & test\\
\hline\hline
1 & Mao~\cite{mao2015generation}       & vgg16 & - & 63.15 & 64.21 & - & 48.73 & 42.13 & 62.14 & - & - \\
2 & Varun~\cite{nagaraja16refexp}      & vgg16 & 76.90 & 75.60 & 78.00 & - & - & - & - & - & 68.40 \\ 
3 & Luo~\cite{luo2017comprehension}    & vgg16 & - & 74.04 & 73.43 & - & 60.26 & 55.03 & 65.36 & - & - \\
4 & CMN~\cite{ronghang16relationship}  & vgg16-frcn & - & - & - & - & - & - & 69.30 & - & - \\
5 & Speaker/visdif~\cite{yu2016refer}  & vgg16 & 76.18 & 74.39 & 77.30 & 58.94 & 61.29 & 56.24 & 59.40 & - & - \\
6 & Listener~\cite{yu2016joint}        & vgg16 & 77.48 & 76.58 & 78.94 & 60.50 & 61.39 & 58.11 & 71.12 & 69.93 & 69.03 \\
7 & \textbf{Speaker}+Listener+Reinforcer~\cite{yu2016joint}& vgg16 & 79.56 &  78.95 & 80.22 & 62.26 & 64.60 & 59.62 & 72.63 & 71.65 & 71.92 \\
8 & Speaker+\textbf{Listener}+Reinforcer~\cite{yu2016joint}& vgg16 & 78.36 & 77.97 & 79.86 & 61.33 & 63.10 & 58.19 & 72.02 & 71.32 & 71.72\\
\hline
9 & MAttN:subj(+attr)+loc(+dif)+rel & vgg16 & 80.94 & 79.99 & 82.30 & 63.07 & 65.04 & 61.77 & 73.08 & 73.04 & 72.79 \\
10 & MAttN:subj(+attr)+loc(+dif)+rel & res101-frcn & 83.54 & 82.66 & 84.17 & 68.34 & 69.93 & 65.90 & - & 76.63 & 75.92 \\ 
11& MAttN:subj(+attr+attn)+loc(+dif)+rel & res101-frcn & \textbf{85.65} & \textbf{85.26} & \textbf{84.57} & \textbf{71.01} & \textbf{75.13} & \textbf{66.17} & - & \textbf{78.10} & \textbf{78.12} \\ 
\hline
\end{tabular}
}
\end{center}
\vspace{-10pt}
\caption{Comparison with state-of-the-art approaches on ground-truth MS COCO regions.}
\label{table:comprehension_comparison}
\end{table*}

\begin{table*}[t]
\footnotesize
\begin{center}
\resizebox{2.0\columnwidth}{!}{\begin{tabular}{| c | l | c | c | c || c | c | c || c | c |}
\hline
&& \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{|c|}{RefCOCO+} & \multicolumn{2}{|c|}{RefCOCOg}\\
\cline{2-10}
&& val & testA & testB & val & testA & testB & val & test\\
\hline\hline
1 & Matching:subj+loc                 & 79.14 & 79.42 & 80.42 & 62.17 & 63.53 & 59.87 & 70.45 & 70.92 \\
2 & MAttN:subj+loc                      & 79.68 & 80.20 & 81.49 & 62.71 & 64.20 & 60.65 & 72.12 & 72.62 \\
3 & MAttN:subj+loc(+dif)                & 82.06 & 81.28 & 83.20 & 64.84 & 65.77 & 64.55 & 75.33 & 74.46 \\ 
4 & MAttN:subj+loc(+dif)+rel            & 82.54 & 81.58 & 83.34 & 65.84 & 66.59 & 65.08 & 75.96 & 74.56 \\
5 & MAttN:subj(+attr)+loc(+dif)+rel      & 83.54 & 82.66 & 84.17 & 68.34 & 69.93 & 65.90 & 76.63 & 75.92 \\ 
6 & MAttN:subj(+attr+attn)+loc(+dif)+rel & \textbf{85.65} & \textbf{85.26} & \textbf{84.57} & \textbf{71.01} & \textbf{75.13} & \textbf{66.17} & \textbf{78.10} & \textbf{78.12} \\ 
\hline
7 & parser+MAttN:subj(+attr+attn)+loc(+dif)+rel& 80.20 & 79.10 & 81.22 & 66.08 & 68.30 & 62.94 & 73.82 & 73.72 \\
\hline
\end{tabular}
}
\end{center}
\vspace{-10pt}
\caption{Ablation study of MAttNet using different combination of modules. The feature used here is res101-frcn.}
\label{table:comprehension_ablation}
\end{table*}

\begin{table*}[t]
\footnotesize
\begin{center}
\resizebox{2.0\columnwidth}{!}{\begin{tabular}{| c | l | c | c | c | c || c | c | c || c | c |}
\hline
&&& \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{|c|}{RefCOCO+} & \multicolumn{2}{|c|}{RefCOCOg}\\
\cline{2-11}
&&detector& val & testA & testB & val & testA & testB & val & test\\
\hline\hline
1 & \textbf{Speaker}+Listener+Reinforcer~\cite{yu2016joint}  & res101-frcn & 69.48 & 73.71 & 64.96 & 55.71 & 60.74 & 48.80 & 60.21 & 59.63\\
2 & Speaker+\textbf{Listener}+Reinforcer~\cite{yu2016joint}  & res101-frcn & 68.95 & 73.10 & 64.85 & 54.89 & 60.04 & 49.56 & 59.33 & 59.21\\
\hline
3 & Matching:subj+loc                 & res101-frcn & 72.28 & 75.43 & 67.87 & 58.42 & 61.46 & 52.73 & 64.15 & 63.25 \\
4 & MAttN:subj+loc                     & res101-frcn & 72.72 & 76.17 & 68.18 & 58.70 & 61.65 & 53.41 & 64.40 & 63.74 \\
5 & MAttN:subj+loc(+dif)               & res101-frcn & 72.96 & 76.61 & 68.20 & 58.91 & 63.06 & 55.19 & 64.66 & 63.88 \\ 
6 & MAttN:subj+loc(+dif)+rel           & res101-frcn & 73.25 & 76.77 & 68.44 & 59.45 & 63.31 & 55.68 & 64.87 & 64.01 \\
7 & MAttN:subj(+attr)+loc(+dif)+rel    & res101-frcn  & 74.51 & 77.81 & 68.39 & 62.13 & 66.33 & 55.75 & 65.33 & 65.19 \\ 
8 & MAttN:subj(+attr+attn)+loc(+dif)+rel & res101-frcn & 76.40 & 80.43 & 69.28 & 64.93 & 70.26 & 56.00 & \textbf{66.67} & 67.01 \\ \hline
9 & MAttN:subj(+attr+attn)+loc(+dif)+rel & res101-mrcn & \textbf{76.65} & \textbf{81.14} & \textbf{69.99} & \textbf{65.33} & \textbf{71.62} & \textbf{56.02} & 66.58 & \textbf{67.27} \\
\hline
\end{tabular}
}
\end{center}
\vspace{-10pt}
\caption{Ablation study of MAttNet on fully-automatic comprehension task using different combination of modules. The features used here are res101-frcn, except the last row using res101-mrcn.}
\label{table:comprehension_automatic}
\end{table*}

\begin{table*}[t]
\footnotesize
\begin{center}
\resizebox{1.8\columnwidth}{!}{\begin{tabular}{ c | c | c | c  c  c  c  c  c }
\multicolumn{9}{c}{RefCOCO}\\
\hline
Model & Backbone Net & Split & Pr@0.5 & Pr@0.6 & Pr@0.7 & Pr@0.8 & Pr@0.9 & IoU \\
\hline
D+RMI+DCRF~\cite{liu2017recurrent}    & res101-DeepLab & val & 42.99 & 33.24 & 22.75 & 12.11 & 2.23 & 45.18 \\
MAttNet                                & res101-mrcn & val & \textbf{75.16} & \textbf{72.55} & \textbf{67.83} &
\textbf{54.79} & \textbf{16.81} & \textbf{56.51} \\
\hline
D+RMI+DCRF~\cite{liu2017recurrent}    & res101-DeepLab & testA & 42.99 & 33.59 & 23.69 & 12.94 & 2.44 & 45.69 \\
MAttNet                                & res101-mrcn & testA & \textbf{79.55} & \textbf{77.60} & \textbf{72.53} & \textbf{59.01} & \textbf{13.79} & \textbf{62.37} \\
\hline
D+RMI+DCRF~\cite{liu2017recurrent}    & res101-DeepLab & testB & 44.99 & 32.21 & 22.69 & 11.84 & 2.65 & 45.57\\
MAttNet                                & res101-mrcn & testB & \textbf{68.87} & \textbf{65.06} & \textbf{60.02} & \textbf{48.91} & \textbf{21.37} & \textbf{51.70} \\
\hline
\end{tabular}
}
\end{center}

\begin{center}
\resizebox{1.8\columnwidth}{!}{\begin{tabular}{ c | c | c | c  c  c  c  c  c }
\multicolumn{9}{c}{RefCOCO+}\\
\hline
Model & Backbone Net & Split & Pr@0.5 & Pr@0.6 & Pr@0.7 & Pr@0.8 & Pr@0.9 & IoU \\
\hline
D+RMI+DCRF~\cite{liu2017recurrent}   & res101-DeepLab & val & 20.52 & 14.02 & 8.46 & 3.77 & 0.62 & 29.86 \\
MAttNet                               & res101-mrcn & val & \textbf{64.11} & \textbf{61.87} & \textbf{58.06} & \textbf{47.42} & \textbf{14.16} & \textbf{46.67} \\
\hline
D+RMI+DCRF~\cite{liu2017recurrent}   & res101-DeepLab & testA & 21.22 & 14.43 & 8.99 & 3.91 & 0.49 & 30.48 \\
MAttNet                               & res101-mrcn & testA & \textbf{70.12} & \textbf{68.48} & \textbf{63.97} & \textbf{52.13} & \textbf{12.28} & \textbf{52.39}\\
\hline
D+RMI+DCRF~\cite{liu2017recurrent}   & res101-DeepLab & testB & 20.78 & 14.56 & 8.80 & 4.58 & 0.80 & 29.50\\
MAttNet                              & res101-mrcn & testB & \textbf{54.82} & \textbf{51.73} & \textbf{47.27} & \textbf{38.58} & \textbf{17.00} & \textbf{40.08} \\
\hline
\end{tabular}
}
\end{center}

\begin{center}
\resizebox{1.8\columnwidth}{!}{\begin{tabular}{ c | c | c | c  c  c  c  c  c }
\multicolumn{9}{c}{RefCOCOg}\\
\hline
Model & Backbone Net & Split & Pr@0.5 & Pr@0.6 & Pr@0.7 & Pr@0.8 & Pr@0.9 & IoU \\
\hline
MAttNet & res101-mrcn & val  & 64.48 & 61.52 & 56.50 & 43.97 & 14.67 & 47.64 \\
\hline
MAttNet & res101-mrcn & test & 65.60 & 62.92 & 57.31 & 44.44 & 12.55 & 48.61 \\
\hline
\end{tabular}
}
\end{center}
\vspace{-10pt}
\caption{Comparison of segmentation performance on RefCOCO, RefCOCO+, and our results on RefCOCOg.}
\label{table:comprehension_segmentation}
\end{table*}


\begin{figure*}[t]
\includegraphics[width=0.92\textwidth]{figure/examples_correct.pdf}
\caption{Examples of fully automatic comprehension. The blue dotted boxes show our prediction with the relative regions in yellow dotted boxes, and the green boxes are the ground-truth. The word attention is multiplied by module weight.}
\label{fig:examples_correct}
\end{figure*}

\begin{figure}[t]
\includegraphics[width=0.45\textwidth]{figure/examples_wrong.pdf}
\caption{Examples of incorrect comprehensions. 
Red dotted boxes show our wrong prediction.
}
\label{fig:examples_wrong}
\end{figure}

\begin{figure}[t]
\includegraphics[width=0.45\textwidth]{figure/examples_segmentation.pdf}
\caption{Examples of fully-automatic MAttNet referential segmentation. 
}
\label{fig:examples_segmentation}
\end{figure}



\subsection{Datasets}
\vspace{-.1cm}
We use 3 referring expression datasets: RefCOCO, RefCOCO+~\cite{kazemzadeh2014referitgame}, and RefCOCOg~\cite{mao2015generation} for evaluation, all collected on MS COCO images~\cite{lin2014microsoft}, but with several differences.
1) RefCOCO and RefCOCO+ were collected in an interactive game interface, while RefCOCOg was collected in a non-interactive setting thereby producing longer expressions, 3.5 and 8.4 words on average respectively.
2) RefCOCO and RefCOCO+ contain more same-type objects, 3.9 vs 1.63 respectively.
3) RefCOCO+ forbids using absolute location words, making the data more focused on appearance differentiators.

During testing, RefCOCO and RefCOCO+ provide person vs. object splits for evaluation, where images containing multiple people are in ``testA'' and those containing multiple objects of other categories are in ``testB''.
There is no overlap between training, validation and testing images.
RefCOCOg has two types of data partitions.
The first~\cite{mao2015generation} divides the dataset by randomly partitioning objects into training and validation splits.
As the testing split has not been released, most recent work evaluates performance on the validation set.
We denote this validation split as RefCOCOg's ``val*''.
Note, since this data is split by objects the same image could appear in both training and validation.
The second partition~\cite{nagaraja16refexp} is composed by randomly partitioning images into training, validation and testing splits.
We denote its validation and testing splits as RefCOCOg's ``val'' and ``test'', and run most experiments on this split.


\subsection{Results: Referring Expression Comprehension}
\label{sec:results_comprehension}
\vspace{-.1cm}
Given a test image, , with a set of proposals/objects, , we use Eqn.~\ref{eqn:score} to compute the matching score  for each proposal/object given the input expression , and pick the one with the highest score.
For evaluation, we compute the intersection-over-union (IoU) of the selected region with the ground-truth bounding box, considering IoU  a correct comprehension.

First, we compare our model with previous methods using COCO's ground-truth object bounding boxes as proposals. Results are shown in Table.~\ref{table:comprehension_comparison}.
As all of the previous methods (Line 1-8) used a 16-layer VGGNet (vgg16) as the feature extractor, we run our experiments using the same feature for fair comparison. Note the flat fc7 is a single 4096-dimensional feature which prevents us from using the phrase-guided attentional pooling in Fig.~\ref{fig:subj_module}, so we use average pooling for subject matching. Despite this, our results (Line 9) still outperform all previous state-of-the-art methods.
After switching to the res101-based Faster R-CNN (res101-frcn) representation, the comprehension accuracy further improves another  (Line 10).
Note our Faster R-CNN is pre-trained on COCO's training images, excluding those in RefCOCO, RefCOCO+, and RefCOCOg's validation+testing.
Thus no training images are seen during our evaluation\footnote{Such constraint forbids us to evaluate on RefCOCOg's val* using the res101-frcn feature in Table~\ref{table:comprehension_comparison}.}.
Our full model (Line 11) with phrase-guided attentional pooling achieves the highest accuracy over all others by a large margin.

Second, we study the benefits of each module of MAttNet by running ablation experiments (Table.~\ref{table:comprehension_ablation}) with the same res101-frcn features.
As a baseline, we use the concatenation of the regional visual feature and the location feature as the visual representation and the last hidden output of LSTM-encoded expression as the language representation, then feed them into the matching function to obtain the similarity score (Line 1).
Compared with this, a simple two-module MAttNet using the same features (Line 2) already outperforms the baseline, showing the advantage of modular learning.
Line 3 shows the benefit of encoding location (Sec.~\ref{sec:loc_module}).
After adding the relationship module, the performance further improves (Line 4).
Lines 5 and Line 6 show the benefits brought by the attribute sub-branch and the phrase-guided attentional pooling in our subject module.
We find the attentional pooling (Line 6) greatly improves on the person category (testA of RefCOCO and RefCOCO+), demonstrating the advantage of modular attention on understanding localized details like ``girl with red hat''.

Third, we tried training our model using 3 hard-coded phrases from a template language parser~\cite{kazemzadeh2014referitgame}, shown in Line 7 of Table.~\ref{table:comprehension_ablation}, which is  lower than our end-to-end model (Line 6).
The main reason for this drop is errors made by the external parser which is not tuned for referring expressions.

Fourth, we show results using automatically detected objects from Faster R-CNN, providing an analysis of fully automatic comprehension performance.
Table.~\ref{table:comprehension_automatic} shows the ablation study of fully-automatic MAttNet.
While performance drops due to detection errors, the overall improvements brought by each module are consistent with Table.~\ref{table:comprehension_ablation}, showing the robustness of MAttNet.
Our results also outperform the state-of-the-art~\cite{yu2016joint} (Line 1,2) with a big margin.
Besides, we show the performance when using the detector branch of Mask R-CNN~\cite{he2017mask} (res101-mrcn) in Line 9, whose results are even better than using Faster R-CNN. 

Finally, we show some example visualizations of comprehension using our full model in Fig.~\ref{fig:examples_correct} as well as visualizations of the attention predictions. We observe that our language model is able to attend to the right words for each module even though it is learned in a weakly-supervised manner.
We also observe the expressions in RefCOCO and RefCOCO+ describe the location or details of the target object more frequently while RefCOCOg mentions the relationship between target object and its surrounding object more frequently, which accords with the dataset property.
Note that for some complex expressions like ``woman in plaid jacket and blue pants on skis'' which contains several relationships (last row in Fig.~\ref{fig:examples_correct}), our language model is able to attend to the portion that should be used by the ``in-box'' subject module and the portion that should be used by the ``out-of-box'' relationship module.
Additionally our subject module also displays reasonable spatial ``in-box'' attention, which qualitatively explains why attentional pooling (Table.~\ref{table:comprehension_ablation} Line 6) outperforms average pooling (Table.~\ref{table:comprehension_ablation} Line 5).
For comparison, some incorrect comprehension are shown in Fig.~\ref{fig:examples_wrong}.
Most errors are due to sparsity in the training data, ambiguous expressions, or detection error.

\vspace{-.2cm}
\subsection{Segmentation from Referring Expression}\label{sec:segmentation}
\vspace{-.2cm}
Our model can also be used to address referential object segmentation~\cite{hu2016segmentation,liu2017recurrent}.
Instead of using Faster R-CNN as the backbone net, we now turn to res101-based Mask R-CNN~\cite{he2017mask} (res101-mrcn).
We apply the same procedure described in Sec.~\ref{sec:model} on the detected objects, and use the one with highest matching score as our prediction.
Then we feed the predicted bounding box to the mask branch to obtain a pixel-wise segmentation.
We evaluate the full model of MAttNet and compare with the best results reported in~\cite{liu2017recurrent}.
We use Precision@X ()\footnote{Precision@0.5 is the percentage of expressions where the IoU of the predicted segmentation and ground-truth is at least 0.5.} and overall Intersection-over-Union (IoU) as metrics.
Results are shown in Table.~\ref{table:comprehension_segmentation} with our model outperforming state-of-the-art results by a large margin under all metrics\footnote{There is no experiments on RefCOCOg's val/test splits in~\cite{liu2017recurrent}, so we show our performance only for reference in Table~\ref{table:comprehension_segmentation}.}.
As both~\cite{liu2017recurrent} and MAttNet use res101 features, such big gains may be due to our proposed model.
We believe decoupling box localization (comprehension) and segmentation brings a large gain over FCN-style~\cite{long2015fully} foreground/background mask classification~\cite{hu2016segmentation,liu2017recurrent} for this instance-level segmentation problem, but a more end-to-end segmentation system may be studied in future work.
Some referential segmentation examples are shown in Fig.~\ref{fig:examples_segmentation}.



