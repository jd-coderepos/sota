\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{ltablex}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{fancyhdr}       \usepackage{graphicx}       \graphicspath{{media/}}     \usepackage{wrapfig}
\usepackage{times}
\usepackage{latexsym}
\usepackage{bm}
\usepackage{tikz}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{color}
\usepackage{float}
\usepackage{arydshln}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{fdsymbol}
\usepackage{subfig}
\usepackage[square,sort]{natbib}
\usepackage{hyperref}       \usepackage{cleveref}



\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 




\newcommand{\zhihan}[1]{{\color{red}{#1}}}

\title{DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome}

\author{
  Zhihan Zhou\thanks{Emails: {\{\tt \href{mailto:zhihanzhou@u.northwestern.edu}{zhihanzhou},  \href{mailto:yanrongji2021@u.northwestern.edu}{yanrongji},    \href{mailto:weijianli@u.northwestern.edu}{weijianli}\}@u.northwestern.edu, \href{mailto:hanliu@northwestern.edu}{\texttt{hanliu@northwestern.edu}}
  }}
  \quad 
  Yanrong Ji\footnotemark[1]
  \quad
  Weijian Li\footnotemark[1]
  \quad
  Pratik Dutta\thanks{Emails: \href{mailto:pratik.dutta@stonybrook.edu}{\texttt{pratik.dutta@stonybrook.edu}, \quad \href{mailto:Ramana.Davuluri@stonybrookmedicine.edu}{\texttt{Ramana.Davuluri@stonybrookmedicine.edu}
  }}}
  \quad
  Ramana Davuluri\footnotemark[2]
  \quad
  Han Liu$^*$\\[2mm]
  Northwestern University\footnotemark[1] \qquad Stony Brook University\footnotemark[2]
}


\begin{document}
\maketitle
\setcounter{footnote}{0}  



\begin{abstract}
Decoding the linguistic intricacies  of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on \textit{k-mer}, fixed-length permutations of \texttt{A}, \texttt{T}, \texttt{C}, and \texttt{G}, as the \textit{token} of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs \textit{tokens} by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization.
Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $28$ distinct datasets across $7$ tasks, with input lengths ranging from $70$ to $1000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with $21 \times$ fewer parameters and approximately $56 \times$ less GPU time \footnote{About 14 days on 8 NVIDIA RTX 2080Ti GPUs V.S. 17 days on 128 NVIDIA A100 GPUs. Estimated with the \textbf{Method 2: GPU Time} introduced by OpenAI in \url{https://openai.com/research/ai-and-compute}.} in pre-training. 
Compared to DNABERT, while being $3 \times$ more efficient, DNABERT-2 outperforms it on $23$ out of $28$ datasets, with an average improvement of $6$ absolute scores on GUE.
The code, data, and pre-trained model are publicly available at \url{https://github.com/Zhihan1996/DNABERT_2}. 

\end{abstract}




\section{Introduction}
\label{sec:intro}

Transformer-based foundation models \citep{foundation_models, bert, gpt4} have witnessed significant progress in recent years, particularly exemplified by the advent of groundbreaking language models like ChatGPT \citep{instructGPT, gpt4}. In parallel, the significance of foundation models has also been increasingly appreciated in the genomics field, as they represent the understanding of genome sequences via numerical embeddings that are directly applicable to various genome analysis tasks. These models can capture complex relationships and dependencies in DNA sequences, opening new avenues for understanding transcriptional regulation \citep{LI2023100384}, non-coding genetic variants associated with human diseases and traits \citep{Rozowsky2023-hl}, and the functional effects of regulatory elements \citep{Smith2023-pg}. Recent advancements in genome language modeling have demonstrated their superiority in a range of downstream applications, including promoter prediction \citep{bertpromoter, iPro-WAEL}, gene expression prediction \citep{enformer}, DNA methylation prediction \citep{Jin2022-bx}, chromatin state analysis \citep{lee2022learning_chromatin}, promoter-enhancer interaction prediction \citep{Chen2022-yh, Ni2022-ck}, TF-DNA binding prediction \citep{tfdna}, variant effect prediction \citep{Rozowsky2023-hl}, gene network prediction \citep{Theodoris2023-kc} and more. These models provide researchers with powerful tools to understand the functional importance of different genomics elements and unravel key biological processes and mechanisms.

In this context, we previously developed DNABERT \citep{dnabert}, an initial foundation model (FM), to unravel the human genome from a language perspective. Despite being widely applied in the community, several technical limitations still present at the time with the original DNABERT implementation, limiting its full potential. First, although proven to be generalizable to other organisms, the pretraining was solely done on the human reference genome, omitting the sequence conservation and diversity across species. Second, k-mer tokenization resulted in information leakage and overall poor computational efficiency during pre-training, which hampers its scalability. Lastly, the simplistic DNABERT-XL solution—intended to bypass the restriction of 512 input sequences imposed by the learned positional embedding \citep{bert}—fell short in handling long input sequences, both in efficiency and effectiveness. These limitations underlined the need for further advancements in the domain of DNA language models.



Recently, \citet{nt} introduced Nucleotide Transformers (NT), a series of genome foundation models scaling from $500M$ to $2500M$ parameters. NT alleviated the first two limitations of DNABERT by pre-training on a large collection of genomes from 850 species and replacing overlapping k-mer tokenization with a non-overlapping version, substantially reducing tokenized sequence length. Despite this, a hard input length limitation still exist, while, as we will discuss in Sec. \ref{sec:background}, non-overlapping k-mer tokenization also suffered from poor sample efficiency as it complicates the model's task of aligning significantly distinct representations of near-identical inputs.

In view of the aforementioned limitations, we introduce DNABERT-2, a multi-species genome foundation model that replaces k-mer tokenization with Byte Pair Encoding (BPE) \citep{bpe}, a data compression algorithm that has been widely used by large language models. We show that BPE effectively addresses the known issues of k-mer tokenization while maintaining the computational efficiency of non-overlapping tokenization.  Moreover, DNABERT-2 overcomes the limitation of DNABERT by replacing learned positional embeddings with Attention with Linear Biases (ALiBi) \citep{alibi} to get rid of the input length limitation, incorporating Flash Attention \citep{flashattention} to increase computational efficiency, and adjusting model architecture to increase model capability. 
As a result of the efficient tokenizer and advanced model architecture, DNABERT-2 achieves comparable performance to the state-of-the-art model with approximately $56 \times$ less computational cost and $21 \times$ fewer parameters, identifying its computation- and sample- efficiency and enabling efficient fine-tuning on most consumer GPUs.

Meanwhile, despite progress in genome foundational models, the absence of carefully curated benchmarks has posed a significant challenge. Owing to the unstandardized pre-processing pipeline of genome sequences, it is unjust to directly compare model performances with results reported in previous papers, even when the data originate from the same source. 
Moreover, many genome understanding evaluation datasets used in existing works \citep{nt} are either too trivial or too challenging, leading to similar scores for most models and failing to accurately reflect different models' capabilities. 
The scarcity of high-quality benchmark datasets hampers evaluating and comparing different models and further hinders the development of novel techniques.
To this end, we introduce Genome Understanding Evaluation (GUE), a standardized and comprehensive multi-species benchmark containing $28$ datasets across $7$ important genome analysis tasks on genomes of $4$ species with input lengths ranging from $70$ to $1000$. 
All the datasets are elaborately calibrated with a series of strategies to ensure they are suitable for reflecting the capability level of existing genome foundation models. 

Our main contributions can be therefore summarized as follows: 1) We identify key obstacles in genome tokenization and provide deep insights, presenting a simple yet effective solution that balances the efficiency and effectiveness of genome foundation models;  2) We introduce DNABERT-2, an efficient pre-trained foundation model for multi-species genome that delivers performance on par with the state-of-the-art model while being $21 \times$ smaller and utilizes approximately $56 \times$ less GPU time; 3) We introduce Genome Understanding Evaluation (GUE), a standardized, comprehensive, and well-calibrated multi-species genome classification benchmark including $7$ tasks and $28$ datasets to facilitate research in genome foundation model.



\section{Background}
\label{sec:background}





\begin{figure*}[t]
		\centering
		\begin{tikzpicture}
		\draw (0,0 ) node[inner sep=0] {\includegraphics[width=1\columnwidth, trim={0.3cm 13.35cm 10cm 0.3cm}, clip]{./figures/tokenizer.pdf}};
		\end{tikzpicture}
		\caption{Illustration of the drawbacks of k-mer tokenization. In the overlapping setting, information about a masked token is leaked by its adjacent tokens, while in the non-overlapping setting, adding/deleting one nucleotide base leads to a dramatic change in the tokenized sequence. }
		\label{fig:kmer}
\end{figure*}

Tokenization serves as a critical initial step in language modeling, significantly impacting the efficiency and effectiveness of the model. DNA sequences consist of $4$ unique nucleotide bases: \texttt{A}, \texttt{T}, \texttt{C}, and \texttt{G}. A majority of genome language models \citep{dnabert, nt} utilize the \texttt{k-mer} tokenization technique, in which each contiguous $k$-length genome segment is considered as a token. 
During tokenization, a sliding window with window size $k$ and stride $t$ is employed to convert the original genome sequence into a series of k-mers. Here, the stride $t$ is either set as $1$ or $k$, while the first one represents the overlapping version of k-mer tokenization and the other one represents the non-overlapping version. 
Figure \ref{fig:kmer} presents examples of overlapping (left) and non-overlapping (right) k-mer tokenizations.
Despite its wide application, we argue that both versions of the k-mer tokenization are less optimal.


Overlapping k-mers tokenization ensures adjacent tokens always overlap by $k-1$ characters, resulting in significant information leakage in masked language modeling. 
As depicted in Figure \ref{fig:kmer}, a masked token is entirely leaked when adjacent tokens from both sides are not masked, and it is partially leaked when adjacent tokens from only one side are present.
Generally, in the overlapping $k$-mer tokenization setting, let $l$ and $r$ denote the distances between a masked token $\textsc{[M]}$ and its closest unmasked adjacent token on the left and right sides, the number of possible options of $\textsc{[M]}$ is $4^{\min(l, r, k, \max(0, l + r - k))}$. 
In other words, to prevent the entire leakage of a masked token, at least $k-1$ tokens on its left and right sides in total must be masked, which explains why \citet{dnabert} opt to mask a continuous span of $k$ tokens. 
Furthermore, to guarantee no leakage of a masked token, at least $k$ tokens on both sides must be masked. 
Nevertheless, information leakage is still inevitable for the leftmost and rightmost $k-1$ masked tokens. 
Ideally, in masked language modeling, a model is required to select the best option from the \textit{entire} vocabulary, enabling it to differentiate and evaluate among a large number of options. However, if the search space is undesirably reduced due to information leakage, the model only needs to differentiate between a limited number of options. Consequently, this results in poor sample efficiency, as the model may not be sufficiently challenged to learn the underlying patterns in the data. Also, the tokenized sequence for an input of length $L$ consists of $L-k+1$ tokens, each with a length of $k$. This results in a tokenized sequence with considerable redundancy and a length nearly equivalent to the original sequence, leading to low computation efficiency considering the quadratic computation complexity of Transformer-based \citep{transformer} models.
This becomes particularly problematic when attempting to scale up the model. Therefore, \citet{nt} proposed the non-overlapping k-mer tokenization.

Non-overlapping k-mer tokenization, despite its advantage of reducing sequence length by a factor of $k$, is plagued by a notable issue of sample inefficiency. Figure \ref{fig:kmer} graphically illustrates this problem. Considering a scenario when the context window is reduced by $1$, the model input is then switched from \textit{\textbf{Sequence 1}} to \textit{\textbf{Sequence 2}}. In theory, this should involve a minor adjustment in tokenized output.
However, with the non-overlapping k-mer tokenizer, this minor shift instigates a dramatic alteration in the tokenized output. Despite the two sequences originating from the same genomic segment, their tokenized representations bear little resemblance. This inconsistent behavior introduces unnecessary hurdles for the model during training, as it poses unnecessary difficulty for the model to align distinct representations of identical or near-identical inputs. Consequently, the inefficiency in learning from the data could impede the overall model performance. The implications of these observations advocate for a re-evaluation of tokenization strategies for the genome language, with a focus on strategies that ensure robust and efficient representation.



To address the aforementioned issues, we propose to adapt SentencePiece \citep{sentencepiece}, a subword tokenization framework widely used in natural language processing, to replace k-mer tokenization for genome sequences. We employ Byte-Pair Encoding (BPE) \citep{bpe} to iteratively merge frequent pairs of nucleotides and genome segments, forming a vocabulary of variable-length tokens that effectively represent the entire genome dataset. 
Despite its conceptual simplicity, this method is well-suited for genome foundation models.
First, it not only prevents information leakage but also significantly reduces the sequence length by approximately $5$ times (detailed statistics are presented in Sec \ref{subsec:model_tokenization}), substantially improving computational efficiency. 
Moreover, its robust tokenization result is beneficial for sample efficiency since it allows the model to focus on understanding the genome language semantics without being distracted by the distinct representations of the same input.
Furthermore, unlike k-mer tokenization, BPE doesn't always produce tokens of length $k$. Consequently, when a token containing an unspecified number of nucleotides is masked, the model is challenged to predict both the number of nucleotides and the particular nucleotides themselves. This naturally transforms the masked language modeling objective into a T5-style \citep{t5} "replace spans of text" objective, which has been demonstrated to be more effective than standard masked language modeling in various scenarios.






\section{Method}
\label{sec:model}

In this section, we provide empirical analysis on the BPE tokenizer for genome language (\S\,\ref{subsec:model_tokenization}) and describe the model architecture (\S\,\ref{subsec:model_architecture}) and implementation details (\S\,\ref{subsec:model_implementation}) of DNABERT-2.

\subsection{Tokenizer}
\label{subsec:model_tokenization}

\begin{wrapfigure}{l}{0.5\textwidth}
    \centering
    \begin{tikzpicture}
    \node[inner sep=0] {\includegraphics[width=0.5\columnwidth, trim={2cm 12.56cm 16.1cm 2.46cm}, clip]{./figures/bpe.pdf}};
    \end{tikzpicture}
    \caption{Illustration of the BPE vocabulary constructions.}
    \label{fig:bpe}
\end{wrapfigure}


DNABERT-2 adapts SentencePiece \citep{sentencepiece} with Byte Pair Encoding (BPE) \citep{bpe} to perform tokenization for DNA sequences.
SentencePiece is a language-agnostic tokenizer that considers each input as a raw stream without assuming any pre-tokenization, which matches greatly with genome sequences where the definitions of \textit{word} and \textit{sentence} do not exist.
BPE is a compression algorithm that has been widely used in the area of natural language processing as a word segmentation strategy. It learns a fixed-sized vocabulary of variable-length tokens based on the co-occurrence frequency of the characters. 
Figure \ref{fig:bpe} illustrates the process of constructing a vocabulary from a given corpus with BPE. First, we initialize the vocabulary with all unique characters in the corpus. Then, in each iteration, we view the most frequent character segment (e.g., \texttt{TA} at iteration $1$) as a new \textit{word}, add it to the vocabulary, and update the corpus by replacing all the same segments with this new word. The iteration continues till we achieve the desired number of words in the vocabulary. Thus, the target vocabulary size plays a crucial role.

Due to the significant difference between natural language and DNA sequence, vocabulary sizes that are commonly used in the NLP area \citep{bert, transformer, t5, gpt4} may not be appropriate for genome sequences. To determine the most suitable vocabulary size, we constructed $8$ vocabularies with target sizes ranging from $2^8$ to $2^{15}$ on the multi-species genomes (see Sec. \ref{subsec:data_pretrain}) to empirically evaluate the impact of varying vocabulary sizes. As indicated in Figure \ref{fig:tokenizer_statistics_a}, larger vocabularies tend to encompass more lengthy tokens, which enables the tokenizer to represent the same input sequence with fewer tokens. Shorter tokenized sequences consequently reduce the computational cost (See Figure \ref{fig:tokenizer_statistics_b}), as the computational complexity of Transformers is quadratic in relation to the input sequence length. Therefore, from the computation efficiency perspective, a larger vocabulary size is favorable.

However, a larger vocabulary leads to more sparse updates to the embedding layer, given that each token would be used less frequently, which might compromise the model's performance. We empirically analyzed this issue by pre-training three different DNABERT-2 variants with vocabulary sizes of $2^8$, $2^{12}$, and $2^{15}$ on the multi-species genome dataset with a batch size of $2048$ for $150000$ steps and evaluating them on the GUE benchmark (see Sec. \ref{subsec:data_finetune}). Figure \ref{fig:tokenizer_statistics_c} displays the performance of each variant, where the model performance is measured by the dataset- and task-average scores. As depicted in the figure, unlike computational efficiency, the model's performance does not consistently improve as the vocabulary size increases. Therefore, we selected a vocabulary size of $2^{12} = 4096$ for training the final DNABERT-2 model, as it best balances model performance with computational efficiency among the candidates.



		
		 


		 


\begin{figure}\centering
    \subfloat[\centering \footnotesize Average token length and the length ratio of original sequence v.s. tokenized sequence. \label{fig:tokenizer_statistics_a}]{{\includegraphics[width=5cm]{figures/vocab_size_length.pdf} }}\quad
    \subfloat[\centering \footnotesize Training FLOPs on 500-length sequences compared to model with $2^8$ vocabulary. \label{fig:tokenizer_statistics_b}]{{\includegraphics[width=5cm]{figures/vocab_size_flops.pdf} }}\quad
    \subfloat[\centering \footnotesize Model performance averaged over each tasks (macro) and individual dataset (micro). \label{fig:tokenizer_statistics_c}]{{\includegraphics[width=5cm]{figures/vocab_size_performance.pdf} }}\caption{This figure presents the average token length, average sequence length reduced after tokenization, and model performance on the GUE benchmark with different vocabulary sizes.}\label{fig:tokenizer_statistics}\end{figure}





\subsection{Model}
\label{subsec:model_architecture}
DNABERT-2 adapts the Transformer Encoder architecture similar to BERT \citep{bert}. To address the limitations of existing models, we incorporate a series of recent advances in deep learning to increase the model's efficiency and capability, including: 1) replacing learned positional embeddings with the Attention with Linear Biases (ALiBi) \citep{alibi} to overcome the input length limitation; 2) utilizing FlashAttention \citep{flashattention} and Low Precision Layer Normalization to increase computation and memory efficiency; 3) employing the Low-Rank Adaptation (LoRA) \citep{lora} in the fine-tuning stage (if necessary) for parameter-efficient training.

\paragraph{Attention with Linear Biases. } Due to the permutation-invariant nature of the attention mechanism, explicit positional information is required in attention-based models. Existing solutions such as Sinusoidal \citep{transformer}, learned \citep{bert}, and Rotary \citep{su2021roformer} positional embedding methods either suffer from input length restriction or poor \textit{extrapolation} capability when applied to sequences longer than training data. Attention with Linear Biases (ALiBi) provides an efficient yet effective solution. Instead of adding position embeddings to the input, ALiBi adds a fixed set of static, non-learned biases to each attention calculation to incorporate positional information into attention scores. 
Specifically, let $\mathbf{q}_i$ define the $i$-$th$ query in the input sequence of length $L$ and $\mathbf{K}$ defines the key matrix, the attention score of query $i$ is calculated as: $\texttt{softmax}(\mathbf{q}_i\mathbf{K} + m * [-(i-1), ..., -2, -1, 0, -1, -2, ..., -(L-1-i)])$, where $m$ is a fixed head-specific constant. ALiBi used a geometric sequence (\textit{i.e.,} $\frac{1}{2^1}, \frac{1}{2^2}, ..., \frac{1}{2^n}$) of different $m$ to each attention head. Intuitively, ALiBi increasingly penalizes attention scores between key-query pairs as their distances increase, and $m$ determines the penalty rate. 
Replacing learned position embedding with ALiBi allows DNABERT-2 to effectively handle arbitrarily long sequences during fine-tuning and inference despite being pre-trained on relatively short sequences.


\paragraph{Flash Attention.} Flash attention is an IO-aware algorithm that implements the exact standard attention calculation in a more time- and memory-efficient way. It identifies a main bottleneck of standard attention implementation as the lack of taking the number of reads and writes to fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory (HBM) into account. To avoid reading and writing to the slow HBM, it splits Key/Query/Value matrices into blocks and incrementally performs softmax over the entire input. It also proposes to recompute large intermediate results like attention scores in backward pass to trade extra computation for fewer IO with HBM, which empirically leads to less computational time. It accelerates DNABERT-2 without sacrificing model performance.



\paragraph{Low-Rank Adaptation (LoRA). } Fine-tuning all the parameters of a model becomes increasingly expensive as the pre-trained model becomes much larger. Thus, we adopt LoRA, a parameter-efficient fine-tuning method that significantly reduces the computation and memory costs with ignorable performance sacrifice. Let $W_0, W_1 \in \mathbb{R}^{m \times n}$ define the same weight matrix before and after task-specific fine-tuning, and we have $W_1 = W_0 + \Delta W$, where $\Delta W \in \mathbb{R}^{m \times n}$ represents the change of each weight element during the fine-tuning. In ordinary fine-tuning, we independently update each weight based on its corresponding gradient, while in LoRA, we represent $\Delta W$ with a low-rank decomposition $\Delta W = BA$, where $B \in \mathbb{R}^{m \times r}$, $A \in \mathbb{R}^{r \times n}$, and $r \ll m, r \ll n$. Modeling $\Delta W$ with low-rank decomposition reduces the number of trainable parameters from $m \times n$ to $r \times (m+n)$, leading to significant improvement in training time and memory usage.


Besides, we replace the Relu activation function with GEGLU \citep{geglu}, a variant of GLU \citep{glu} that has been shown to improve the performance of Transformer models. The GEGLU function is calculated as $\texttt{GEGLU} (x, W, V, b, c) = \texttt{GELU} (xW + b) \otimes (xV + c)$, where $x$ is the function input, $W$ and $V$ are learnable weights, and $b$ and $c$ are learnable biases. The GELU function is defined as $\texttt{GELU} (x) = x\Phi(x)$, where $\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution.



\subsection{Implementation}
\label{subsec:model_implementation}
We pre-train DNABERT-2 with the Masked Language Modeling (MLM) loss with a mask ratio of $15\%$. 
Notably, we independently mask every token instead of masking spans of continuous tokens like \citet{dnabert}. We use a batch size of $4096$ and a max sequence length of $128$. We train the model for $500000$ steps using the \texttt{AdamW} \citep{adamw} optimizer with $\beta_1=0.9$, $\beta_2=0.98$, $\epsilon=1e\texttt{-}6$ and weight decay of $1e\texttt{-}5$. The learning rate linearly increases from $0$ to $5e\texttt{-}4$ during the first $30000$  steps while linearly decreasing to $0$ in the last  $470000$ steps. The pre-training stage takes approximately 14 days using eight Nvidia RTX 2080Ti GPUs. To train the model, we used the Transformers library by HuggingFace \citep{huggingface-transformers} and the Composer library by MosaicML \citep{mosaicml2022composer}.



\section{Data}
\label{sec:data}

In order to facilitate further research on large-scale genome foundational models, we have collated and made available multi-species genome datasets for both pre-training of models (Sec. \ref{subsec:data_pretrain}) and benchmarking (Sec. \ref{subsec:data_finetune}).

\subsection{Pre-Train: Human and Multi-Species Genome}
\label{subsec:data_pretrain}

To investigate the impact of species diversity on genome foundational models, we've compiled and made publicly available two datasets for foundational model pre-training: the human genome and the multi-species genome. The human genome dataset is borrowed from DNABERT's pre-training data \citep{dnabert}, which comprises $2.75$B nucleotide bases. The multi-species genome dataset encompasses genomes from $135$ species, spread across $7$ categories. In total, this dataset includes $32.49$B nucleotide bases, nearly $12$ times the volume of the human genome dataset. We exclude all sequences with \texttt{N} and retain only sequences that consist of \texttt{A}, \texttt{T}, \texttt{C}, and \texttt{G}. Detailed statistics are presented in Table \ref{tb:multi_species_details}.


\subsection{Benckmark: Genome Understanding Evaluation (GUE)}
\label{subsec:data_finetune}


\begin{table*}[t]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{2.4mm}{
	\begin{tabular}{llrrr}\toprule
		
		 {\textbf{Species}} & {\textbf{Task}} & {\textbf{Num. Datasets}} &  {\textbf{Num. Classes}} & {\textbf{Sequence Length}} \\
		 
		 \midrule
		 
		 \multirow{4}{*}{\textbf{Human}} 
   & Core Promoter Detection & 3 & 2 & 70  \\
   & Transcription Factor Prediction & 5 & 2 & 100 \\
   & Promoter Detection & 3  & 2 & 300  \\ 
   & Splice Site Detection & 1 & 3 & 400 \\ 

             \midrule

    \textbf{Mouse} & Transcription Factor Prediction & 5 & 2 & 100 \\

          \midrule

   \textbf{Yeast} & Epigenetic Marks Prediction & 10 & 2 & 500 \\

   \midrule

   \textbf{Virus} & Covid Variant Classification & 1 & 9 & 1000 \\




    
		 
		\bottomrule
	\end{tabular}}
	\caption{ \footnotesize  
		Summarization of the Genome Understanding Evaluation (GUE) benchmark.
	}\label{tb:gue}
\end{table*}



We introduce the Genome Understanding Evaluation (GUE) benchmark, which includes $7$ genome sequence classification problems with $28$ datasets. Table \ref{tb:gue} presents the summarization of the GUE benchmark. To evaluate models' capability in modeling sequences with different lengths, we select datasets with input lengths ranging from $70$ to $1000$. GUE contains tasks for $4$ species: human, mouse, virus, and yeast, to evaluate the multi-species transferability in genome understanding of each model. We explicitly define evaluation metrics for each task and split each dataset into training, validation, and test data for a fair comparison across different models.

To calibrate the GUE benchmark's difficulty level and better illuminate each model's capabilities, we carefully selected datasets that are neither too simple nor overly challenging for current models. For example, when the Nucleotide Transformer variants \citep{nt} were tested on the \textit{Splice Site Prediction} dataset, all variants achieved an accuracy between $97\%$ and $98\%$. Similar outcomes were observed in tasks like \textit{Promoter Prediction} and \textit{Enhancer Prediction}. These high scores might suggest these variants perform similarly, but as our experiments in Section \ref{sec:experiments} show, they vary significantly on more discerning datasets.



The construction of GUE starts with  the aggregation of various biologically important genome analysis datasets, followed by the assessment of existing models such as DNABERT \citep{dnabert} and Nucleotide Transformer \citep{nt} on these datasets. Datasets where the majority of models yielded moderate (e.g., F1-scores between 0.3 and 0.8) and distinguishable performance scores were retained. On the other hand, datasets that did not meet these criteria underwent a restructuring process involving various strategies such as class balancing, adversarial sample inclusion, and reduction of training sample volume, among others. After several iterations of this process, we ultimately arrived at 28 representative datasets of moderate difficulty. Due to space limits, we present the detailed data processing and statistics of each dataset in Sec. \ref{subsec:appen_data_gue}.



\section{Experiments}
\label{sec:experiments}

We evaluate DNABERT-2 using the proposed GUE benchmark to thoroughly investigate its versatility and robustness across a variety of tasks involving multi-species genomes.




\subsection{Baseline}
\label{subsec:experiments_baseline}

We compare DNABERT-2 with two state-of-the-art genome foundation models: DNABERT \citep{dnabert} and Nucleotide Transformer \citep{nt}. 

\textbf{DNABERT} was the first pre-trained foundational model for genome sequences, trained on human genome sequences. It has four variants, namely \textit{DNABERT (3-mer)}, \textit{DNABERT (4-mer)}, \textit{DNABERT (5-mer)}, and \textit{DNABERT (6-mer)}, which utilize overlapping $3/4/5/6$-kmer tokenization respectively. While DNABERT employs the same architecture as \texttt{BERT-base}, it has a different vocabulary size, which is dependent on the chosen $k$-mer.

\textbf{Nucleotide Transformer (NT)} scales up the data and model size to achieve state-of-the-art performance in $27$ DNA analysis tasks. It also has $4$ variants: \textit{NT-500M-human}, \textit{NT-500M-1000g}, \textit{NT-2500M-1000g}, and \textit{NT-2500M-multi}, where \textit{human}, \textit{1000g}, and \textit{multi} respectively refers to the GRCh38/hg38 human reference genome, 3202 high-coverage human genomes from the 1000 Genome project \citep{1000g}, and genome from 850 different species. 

It is important to note that NT models are 6 to 29 times larger than DNABERT, which precludes standard model fine-tuning on consumer GPUs. Therefore, we perform standard fine-tuning for DNABERT and DNABERT-2, while adapting the Low-Rank Adaptation (LoRA) technique for fine-tuning the Nucleotide Transformer to enhance efficiency. For a fair comparison, we conducted preliminary experiments to confirm that our implementation of NT achieves comparable results to those reported in their original paper \citep{nt} (see Appendix \ref{subsec:nt_preliminary} for more details).

\subsection{Setup and Metric}
\label{subsec:experiments_setup}

We evaluate the models from two perspectives: computational efficiency and performance on downstream tasks. To measure each model's computational cost, we consider the number of model parameters and the relative Floating Point Operations (FLOPs)—which is the total number of multiplication and addition operations during a forward pass—compared to DNABERT-2. We evaluate FLOPs on genome sequences with a length of 500, a commonly used setup in genome analysis. To measure model performance, we utilize F1-Score and Matthews Correlation Coefficient (MCC). We use different metrics for different tasks, following conventional practices (refer to Table \ref{tb:gue_details} for details).
Table \ref{tb:performance_efficiency} presents the overall performance of each model on the GUE benchmark. It provides the average score of each model and the number of times it ranks in the top two among all models. The average results across all tasks are reported in Table \ref{tb:main_results}, while task-specific results can be found in \ref{tb:all_results}, with full details relegated to this section due to space constraints. We also include statistics on the number of tokens each model processed during its pre-training phase, providing insight into the effects of training steps on model performance.
For each model, we keep most of the hyperparameters (e.g., learning rate, batch size, weight decay, etc.) constant across all datasets, adjusting only the maximum sequence length and the number of training steps according to the specific dataset. Hyperparameter tuning tailored to each dataset is left for future work. Throughout the training process, we validate the model every 200 steps, save the model that yields the smallest loss on the validation set, and report its evaluation results on the test set. We train each model using three different random seeds and report the average results.
\paragraph{Further Pre-Training.} We also investigate the impact of additional in-domain pre-training on DNA language models. We combine the training sets of the 28 GUE datasets and further pre-train DNABERT-2 on this combined set. Following \citet{sun2020finetune}, we train the model with a batch size of 32, a maximum sequence length of 128, and a learning rate of $5e\texttt{-}5$ for 100,000 steps. This results in the processing of 0.41B tokens, which only constitute 0.08\% of the tokens processed during the entire training process of DNABERT-2.



\subsection{Main Results}
\label{subsec:main_results}


\begin{table*}[t]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{2.4mm}{
	\begin{tabular}{lccccc}\toprule
		
		\textbf{Model} & \textbf{Num. Params.} $\downarrow$    & \textbf{FLOPs} $\downarrow$ & \textbf{Trn. Tokens } & \textbf{Num. Top-2} $\uparrow$ & \textbf{Ave. Scores} $\uparrow$ \\

		\midrule
		{\textbf{DNABERT (3-mer)} } & 86M & 3.27 & 122B  & 2 $\|$ 0 &  61.62 \\
		
		{\textbf{DNABERT (4-mer)} } & 86M & 3.26 & 122B  & 0 $\|$ 1 & 61.14  \\

            {\textbf{DNABERT (5-mer)} } & 87M & 3.26 & 122B  & 0 $\|$ 1 & 60.05 \\

            {\textbf{DNABERT (6-mer) } } & 89M & 3.25 & 122B  &   0 $\|$ 1 & 60.51  \\

            {\textbf{NT-500M-human} } & 480M & 3.19  & 50B & 0 $\|$ 0 &  55.43 \\

            {\textbf{NT-500M-1000g} } & 480M & 3.19 & 50B  & 0 $\|$ 1 &  58.23 \\

            {\textbf{NT-2500M-1000g} } & 2537M & 19.44 & 300B & 0 $\|$  1 &  61.41  \\

            {\textbf{NT-2500M-multi} } & 2537M  & 19.44 
 & 300B& \underline{7} $\|$ \underline{9} &  \underline{66.93}  \\

		
		\midrule
            {\textbf{DNABERT-2} } & 117M & 1.00 & 262B  &  8 $\|$ 4 & 66.80 \\

            {\textbf{DNABERT-2$\vardiamondsuit$} } & 117M & 1.00  & 263B & \textbf{11} $\|$ \textbf{10} & \textbf{67.77} \\
            
		\bottomrule
	\end{tabular}}
	\caption{ 
		The statistics and performance of each model. The five columns represent the number of model parameters, relative FLOPs compared to DNABERT-2, the number of tokens used in pre-training, and the number of being top-2 among all the models (\texttt{1st} $\|$ \texttt{2nd}) and the average evaluation scores on the 28 datasets of the GUE benchmark. $\vardiamondsuit$: perform further masked language modeling pre-training on the training sets of the GUE benchmark.
	}\label{tb:performance_efficiency}
\end{table*}



\begin{table*}[t]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{2.4mm}{
	\begin{tabular}{lccccccc}\toprule
 
   & \multicolumn{1}{c}{\textbf{Yeast}} & \multicolumn{1}{c}{\textbf{Mouse}}  & \multicolumn{1}{c}{\textbf{Virus}} &  \multicolumn{4}{c}{\textbf{Human}}\\
		\cmidrule(lr){2-2}  \cmidrule(lr){3-3}  \cmidrule(lr){4-4} \cmidrule(lr){5-8} 
		
		 & \textbf{EMP} & \textbf{TF-M} & \textbf{CVC} & \textbf{TF-H}   &  \textbf{PD} & \textbf{CPD} & \textbf{SSP}      \\

		\midrule
		{\textbf{DNABERT (3-mer)} } & 49.54 & 57.73 & 62.23 & 64.43  & 84.63 & \textbf{72.96} & 84.14    \\
		
		{\textbf{DNABERT (4-mer)} } & 48.59 & 59.58 & 59.87 & 64.41  & 82.99 & 71.10 & 84.05  \\

            {\textbf{DNABERT (5-mer)} } &48.62 & 54.85 & 63.64 & 50.46  & 84.04 & \underline{72.03} & 84.02 \\

            {\textbf{DNABERT (6-mer) } } & 49.10 & 56.43 & 55.50 &64.17  & 81.70 & 71.81 & 84.07   \\

            {\textbf{NT-500M-human} } & 45.35 & 45.24 &  57.13 & 50.82   & 85.51 & 66.54 & 79.71  \\

            {\textbf{NT-500M-1000g} } & 47.68 & 49.31  & 52.06 & 58.92 & 86.58 & 69.13 & 80.97  \\

            {\textbf{NT-2500M-1000g} } & 50.86 & 56.82 & 66.73 & 61.99  & \underline{86.61} & 68.17  & 85.78   \\

            {\textbf{NT-2500M-multi} } & \underline{58.06} & 67.01 & \textbf{73.04} & 63.32   & \textbf{88.14} & 71.62 & \textbf{89.36}  \\

		
		\midrule

            {\textbf{DNABERT-2} } & 55.98 & \underline{67.99} & \underline{71.02} &\textbf{70.10}  & 84.21 & 70.52 & 84.99     \\

            {\textbf{DNABERT-2$\vardiamondsuit$} } &\textbf{58.83} & \textbf{71.21}  & 68.49 &\underline{66.84} & 83.81 & 71.07 & \underline{85.93}    \\
            
		\bottomrule
	\end{tabular}}
	\caption{ \footnotesize  
		The models' averaged performance on the $8$ tasks in the GUE benchmark, including Epigenetic Marks Prediction (EMP), Transcription Factor Prediction on the Human genome and the Mouse genome (TF-H and TF-M), Covid Variants Classification (CVC), Promoter Detection (PD), Core Promoter Detection (CPD), and Splice Site Prediction (SSP).
	}\label{tb:main_results}
\end{table*}



Table \ref{tb:performance_efficiency} outlines the statistics and aggregate performance of the models. As indicated in the table, despite being $21 \times$ smaller and requiring $19 \times$ fewer FLOPs, DNABERT-2 delivers a performance comparable to the state-of-the-art model while significantly surpassing other baselines. When DNABERT-2 undergoes additional pre-training on the GUE benchmark, which requires negligible computational overhead, it delivers the highest average performance and consistently ranks in the top two across the 28 tasks of the GUE benchmark. These results showcase the model's remarkable efficiency and effectiveness.

Despite having 30\% more parameters than DNABERT, DNABERT-2 requires only one-third the number of FLOPs. This indicates the superiority of the Byte Pair Encoding (BPE)-based tokenization method over overlapping k-mer tokenization in terms of modeling efficiency. Armed with the new tokenization method and the Attention with Linear Biases (ALiBi) module, DNABERT-2 can effectively process long genome sequences arbitrarily, demonstrating enhanced efficiency. This improvement becomes even more significant as the length of the input sequence increases. Moreover, DNABERT-2 consistently outperforms DNABERT by a large margin, indicating the effectiveness of multi-species pre-training and new model architecture.


Although DNABERT-2 is $5$ times smaller, it surpasses NT-500M while using less FLOPs. This underscores the importance of providing the model with \textit{adequate} data, particularly when the model size is scaled up, and further highlights the inefficiency of overlapping k-mer tokenization. The comparison between DNABERT and NT-2500M-1000g exposes the sample inefficiency of non-overlapping k-mer tokenization. Despite being trained on $2.5$ times more tokens, NT-2500M-1000g achieves a performance similar to that of DNABERT.

The averaged results for each task are displayed in Table \ref{tb:main_results}. DNABERT-2 and NT-2500M-multi consistently achieve top-tier performance across most tasks. Their dominance over other baselines is particularly notable in non-human genome analysis tasks, demonstrating the effectiveness of pre-training on multi-species genomes. Furthermore, models trained on multi-species genomes also show strong performance on human genome analysis tasks, proving their ability to develop a comprehensive understanding of multi-species genomes without compromising their grasp of the human genome. However, we observe that additional pre-training does not uniformly enhance performance across all tasks, indicating that task-specific further pre-training might be beneficial when addressing a certain downstream task.


Additionally, DNABERT variants achieve optimal performance in the Core Promoter Detection task, where inputs are sequences of length $70$. However, their performance diminishes in the similar task of Promoter Detection, where the input length increases to $300$. These results highlight a common challenge associated with non-overlapping k-mer tokenization and BPE-based tokenization: the capacity to identify subtle signals from limited input. Although inefficient, the overlapping k-mer tokenization adopted by DNABERT retains most of the information in the original sequences. In contrast, the sequence length is significantly reduced (\textit{i.e.,} from $70$ to $15$) with non-overlapping k-mer and BPE tokenization, which might limit the retained information and hinder informed decision-making. This identifies a critical area for future exploration in DNA language models.














\section{Conclusion}
In this paper, we introduce DNABERT-2, an efficient foundational model pre-trained on extensive multi-species genomes. We identify the computational and sample inefficiencies of the existing k-mer tokenization method and propose the adaptation of Byte Pair Encoding (BPE) for DNA language modeling. We provide insightful and comprehensive empirical analyses, building DNABERT-2 based on these findings. Moreover, we integrate several techniques such as Attention with Linear Biases (ALiBi) and Low-Rank Adaptation (LoRA) to address the limitations of current DNA language models.
From a data perspective, we compile and introduce the Genome Understanding Evaluation (GUE), a benchmark for multi-species genome analysis comprising seven tasks and 28 datasets with well-defined training, validation, test sets, clear evaluation metrics, and elaborately calibrate difficulty. In addition, we release a multi-species genome dataset consisting of $32.49$ billion nucleotide bases derived from the genomes of 135 species across seven categories. We believe these datasets will significantly contribute to the progression of research on DNA language models.
For future work, we identify several promising directions: 1) the development of effective modeling strategies for short genome sequences; 2) scaling up the model size; and 3) the introduction of training targets and data processing/augmentation methods that leverage the unique double-strand structure of DNA.












\section*{Acknowledgments}
This work is supported by NIH R01LM01372201.


\bibliographystyle{plainnat}  
\bibliography{references}  



\newpage
\appendix



\section{Experiments}
\label{sec:appendix_experiments}



\subsection{All Experiment Results}
\label{subsec:all_results}


\begin{table}[H]
	\centering
	\footnotesize
	\begin{tabular}{lrrrrrr}
		\toprule
        	& \multicolumn{6}{c}{\textbf{Epigenetic Marks Prediction}} \\
		\cmidrule(lr){2-7}
		& \textbf{ H3 } & \textbf{ H3K14ac } & \textbf{ H3K36me3 } & \textbf{ H3K4me1 } & \textbf{ H3K4me2 } & \textbf{ H3K4me3 } \\
		\midrule
		{\textbf{DNABERT (3-mer)} } & 74.15 & 42.07 & 48.49 & 42.95 & 31.34 & 28.92 \\
		
		{\textbf{DNABERT (4-mer)} } & 73.03 & 41.88 & 48.03 & 41.06 & 30.66 & 25.31 \\

            {\textbf{DNABERT (5-mer)} } & 73.40 & 40.68 & 48.29 & 40.65 & 30.67 & 27.10  \\

            {\textbf{DNABERT (6-mer) } } &  73.10 & 40.06 & 47.25 & 41.44 & 32.27 & 27.81  \\
            
            {\textbf{NT-500M-human} } & 69.67 & 33.55 & 44.14 & 37.15 & 30.87 & 24.06 \\

            {\textbf{NT-500M-1000g} } & 72.52 & 39.37 & 45.58 & 40.45 & 31.05 & 26.16 \\

            {\textbf{NT-2500M-1000g} } & 74.61 & 44.08 & 50.86 & 43.10 & 30.28 & 30.87 \\

            {\textbf{NT-2500M-multi} } & \underline{78.77} & \underline{56.20} & \textbf{61.99} & \textbf{55.30} & \underline{36.49} & \underline{40.34} \\

		
		\midrule

            {\textbf{DNABERT-2} } & 78.27 & 52.57 & 56.88 & 50.52 & 31.13 & 36.27 \\

            {\textbf{DNABERT-2$\vardiamondsuit$} } &  \textbf{80.17} & \textbf{57.42} & \underline{61.90} & \underline{53.00} & \textbf{39.89} & \textbf{41.20}  \\
		\bottomrule
	\end{tabular}

	\begin{tabular}{lrrrrrrr}
		\toprule
  & \multicolumn{4}{c}{\textbf{Epigenetic Marks Prediction}} & \multicolumn{3}{c}{\textbf{Promoter Detection}} \\
		\cmidrule(lr){2-5}  \cmidrule(lr){6-8}
		& \textbf{ H3K79me3 } & \textbf{ H3K9ac } & \textbf{ H4 } & \textbf{ H4ac } & \textbf{all} & \textbf{notata} & \textbf{tata} \\
		\midrule
		{\textbf{DNABERT (3-mer)} } &  60.12 & 50.48 & 78.27 & 38.60 &90.44 & 93.61 & 69.83 \\
		
		{\textbf{DNABERT (4-mer)} } &  59.77 & 51.44 & 78.28 & 36.40 & 89.54 & 92.65 & 66.78\\

            {\textbf{DNABERT (5-mer)} } & 59.61 & 51.11 & 77.27 & 37.48  & 90.16 & 92.45 & 69.51\\

            {\textbf{DNABERT (6-mer) } } & 61.17 & 51.22 & 79.26 & 37.43 &  90.48 & 93.05 & 61.56\\

            {\textbf{NT-500M-human} } & 58.35 & 45.81 & 76.17 & 33.74 & 87.71 & 90.75 & 78.07 \\

            {\textbf{NT-500M-1000g} } &  59.33 & 49.29 & 76.29 & 36.79 & 89.76 & 91.75 & \underline{78.23} \\

            {\textbf{NT-2500M-1000g} } & 61.20 & 52.36 & 79.76 & 41.46 & \underline{90.95} & 93.07 & 75.80 \\

            {\textbf{NT-2500M-multi} } & 64.70 & \underline{56.01} & \underline{81.67} & 49.13 & \textbf{91.01} & 94.00 & \textbf{79.43} \\

		
		\midrule

            {\textbf{DNABERT-2} } & \textbf{67.39} & 55.63 & 80.71 & \textbf{50.43}   & 86.77 & \underline{94.27} & 71.59 \\

            {\textbf{DNABERT-2$\vardiamondsuit$} } &  \underline{65.46} & \textbf{57.07} & \textbf{81.86} & \underline{50.35} & 88.31 & \textbf{94.34} & 68.79\\
		\bottomrule
	\end{tabular}


        \begin{tabular}{lrrrrrrrr}
		\toprule
  & \multicolumn{5}{c}{\textbf{Transcription Factor Prediction (Human)}} & \multicolumn{3}{c}{\textbf{Core Promoter Detection}} \\
		\cmidrule(lr){2-6}  \cmidrule(lr){7-9}
		& \textbf{ 0 } & \textbf{ 1 } & \textbf{ 2 } & \textbf{ 3 } & \textbf{4} & \textbf{all} & \textbf{notata} & \textbf{tata} \\
		\midrule
		{\textbf{DNABERT (3-mer)} } & 67.95 & 70.90 & 60.51 & 53.03 & 69.76 & \textbf{70.92} & 69.82 & \textbf{78.15} \\
		
		{\textbf{DNABERT (4-mer)} } & 67.90 & \underline{73.05} & 59.52 & 50.37 & 71.23 & 69.00 & 70.04 & 74.25\\

            {\textbf{DNABERT (5-mer)} } & 66.97 & 69.98 & 59.03 & 52.95 & 69.26  &69.48 & 69.81 & \underline{76.79} \\

            {\textbf{DNABERT (6-mer) } } &  66.84 & 70.14 & 61.03 & 51.89 & 70.97 &  68.90 & \underline{70.47} & 76.06\\

            {\textbf{NT-500M-human} } & 61.59 & 66.75 & 53.58 & 42.95 & 60.81 & 63.45 & 64.82 & 71.34  \\

            {\textbf{NT-500M-1000g} } &63.64 & 70.17 & 52.73 & 45.24 & 62.82 & 66.70 & 67.17 & 73.52 \\

            {\textbf{NT-2500M-1000g} }  & 66.31 & 68.30 & 58.70 & 49.08 & 67.59 & 67.39 & 67.46 & 69.66  \\

            {\textbf{NT-2500M-multi} }  &  66.64 & 70.28 & 58.72 & 51.65 & 69.34 & \underline{70.33} & \textbf{71.58} & 72.97\\

		
		\midrule

            {\textbf{DNABERT-2} } & \textbf{71.99} & \textbf{76.06} & \textbf{66.52} & \textbf{58.54} & \textbf{77.43} & 69.37 & 68.04 & 74.17 \\

            {\textbf{DNABERT-2$\vardiamondsuit$} } &  \underline{69.12} & 71.87 & \underline{62.96} & \underline{55.35} & \underline{74.94} & 67.50 & 69.53 & 76.18\\
		\bottomrule
	\end{tabular}

 \begin{tabular}{lrrrrrrr}
		\toprule
  & \multicolumn{5}{c}{\textbf{Transcription Factor Prediction (Mouse)}} & \multicolumn{1}{c}{\textbf{Virus}}  & \multicolumn{1}{c}{\textbf{Splice}} \\
		\cmidrule(lr){2-6}  \cmidrule(lr){7-7}  \cmidrule(lr){8-8}
		& \textbf{ 0 } & \textbf{ 1 } & \textbf{ 2 } & \textbf{ 3 } & \textbf{4} & \textbf{Covid} & \textbf{Reconstruct} \\
		\midrule
		{\textbf{DNABERT (3-mer)} } & 42.31 & 79.10 & 69.90 & 55.40 & 41.97 & 62.23 & 84.14 \\
		
		{\textbf{DNABERT (4-mer)} } & 49.42 & 79.95 & 72.62 & 51.79 & 44.13& 59.87 & 84.05\\

            {\textbf{DNABERT (5-mer)} } & 42.45 & 79.32 & 62.22 & 49.92 & 40.34&  50.46 & 84.02\\

            {\textbf{DNABERT (6-mer) } } &  44.42 & 78.94 & 71.44 & 44.89 & 42.48 &55.50 & 84.07 \\

            {\textbf{NT-500M-human} } &  31.04 & 75.04 & 61.67 & 29.17 & 29.27 &  50.82 & 79.71 \\

            {\textbf{NT-500M-1000g} } & 39.26 & 75.49 & 64.70 & 33.07 & 34.01 &  52.06 & 80.97 \\

            {\textbf{NT-2500M-1000g} } & 48.31 & 80.02 & 70.14 & 42.25 & 43.40 &  66.73 & 85.78  \\

            {\textbf{NT-2500M-multi} } & \underline{63.31} & 83.76 & 71.52 & \underline{69.44} & 47.07 &  \textbf{73.04} & \textbf{89.35} \\

		
		\midrule

            {\textbf{DNABERT-2} } & 56.76 & \underline{84.77} & \underline{79.32} & 66.47 & \textbf{52.66} & \underline{71.02} & 84.99 \\

            {\textbf{DNABERT-2$\vardiamondsuit$} } &  \textbf{64.23} & \textbf{86.28} & \textbf{81.28} & \textbf{73.49} & \underline{50.80} & 68.49 & \underline{85.93}\\
		\bottomrule
	\end{tabular}
 
	\caption{
		This table presents the performance of all the models on the GUE benchmark.  $\vardiamondsuit$: perform further pre-training on the training sets of the GUE benchmark.
	}\label{tb:all_results}
\end{table}








		


 

\subsection{Hyperparameters}
\label{subsec:hyperparameters}

This section presents the hyperparameters we used in the fine-tuning stage on each model. Table \ref{tb:hyperparameters} shows the number of training steps we used for each task. We use \textbf{AdamW} \citep{adamw} as optimizer. We keep most of the other hyperparameters the same for all the models across all the datasets, including a batch size of $32$, a warmup step of $50$, and a weight decay of $0.01$. For DNABERT and DNABERT-2, we perform standard fine-tuning with a learning rate of $3e\texttt{-}5$, while for the Nucleotide Transformers, we perform parameter efficient fine-tuning (PEFT) using Low-Rank Adaptation (LoRA) with a learning rate of $1e\texttt{-}4$, a LoRA alpha of $16$, a LoRA dropout of $0.05$, and a LoRA $r$ of $8$. The hyperparameters are selected based on grid searches over commonly used ones in preliminary experiments.


\begin{table*}[t]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{2.4mm}{
	\begin{tabular}{lccccccccc}\toprule
		
		 & \textbf{EMP} & \textbf{TF-M} & \textbf{CVC} & \textbf{TF-H}   &  \textbf{PD-tata} &  \textbf{PD-other}  &  \textbf{CPD-tata} &  \textbf{CPD-other} & \textbf{SSP}      \\

		\midrule
		{\textbf{Num. Epochs} } & 3 & 1k steps & 8 & 3 & 10 & 4 & 10 & 4 & 5 \\
            
		\bottomrule
	\end{tabular}}
	\caption{ \footnotesize  
		The number of training steps we used for the following tasks: Epigenetic Marks Prediction (EMP), Transcription Factor Prediction on the Human genome and the Mouse genome (TF-H and TF-M), Covid Variants Classification (CVC), \textit{tata} dataset of Promoter Detection (PD-tata), \textit{notata} and \textit{all} datasets of Promoter Detection (PD-other), \textit{tata} dataset of Core Promoter Detection (CPD-tata), \textit{notata} and \textit{all} datasets of Core Promoter Detection (CPD-other), and Splice Site Prediction (SSP).
	}\label{tb:hyperparameters}
\end{table*}



\subsection{Preliminary Experiments on Nucleotide Transformer}
\label{subsec:nt_preliminary}

Since there is no official fine-tuning code of Nucleotide Transformer \citep{nt}, we use its open-sourced checkpoints in Huggingface Modelhub\footnote{https://huggingface.co/InstaDeepAI} and train it with our code base using LoRA. For a fair comparison with this model, in this section, we present preliminary experiments that compare the results reported in their paper with the performance of this model under our implementation. We select the epigenetic marks prediction task for benchmarking since it is the only shared task among \citet{nt} and GUE. The task contains $10$ datasets. For each dataset, we randomly split it into training and test sets with a ratio of 9:1. As shown in  Table \ref{tb:nt_preliminary}, our LoRA implementation leads to slightly better results than the results reported in the original paper, making our comparison to the model fair and convincing despite the fact that we do not have access to its official fine-tuning implementation.

\begin{table}[H]
	\centering
	\footnotesize
	\begin{tabular}{lrrrrrr}
		\toprule
		& \textbf{ H3 } & \textbf{ H3K14ac } & \textbf{ H3K36me3 } & \textbf{ H3K4me1 } & \textbf{ H3K4me2 } & \textbf{ H3K4me3 } \\
		\midrule
		\textbf{500M-human*} & 72.00 & 37.00 & 45.00 & 36.00 & 27.00 & 24.00 \\
		\textbf{500M-human}  & 69.67 & 33.55 & 44.14 & 37.15 & 30.87 & 24.06 \\
		\midrule
		\textbf{500M-1000g*} & 74.00 & 38.00 & 47.00 & 38.00 & 26.00 & 24.00 \\
		\textbf{500M-1000g}  & 72.52 & 39.37 & 45.58 & 40.45 & 31.05 & 26.16  \\
\midrule
{\textbf{2500M-1000g*} } &75.00 & 45.00 & 53.00 & 42.00 & 28.00 & 31.00  \\

  {\textbf{2500M-1000g} } & 74.61 & 44.08 & 50.86 & 43.10 & 30.28 & 30.87 \\
  \midrule
  {\textbf{2500M-multi*} } &79.00 & 54.00 & 62.00 & 54.00 & 32.00 & 41.00 \\

            {\textbf{2500M-multi} } & 78.77 & 56.20 & 61.99 & 55.30 & 36.49 & 40.34 \\

		\bottomrule
	\end{tabular}

	\bigskip 

	\begin{tabular}{lrrrrr}
		\toprule
		& \textbf{ H3K79me3 } & \textbf{ H3K9ac } & \textbf{ H4 } & \textbf{ H4ac } & \textbf{Average} \\
		\midrule
		\textbf{500M-human*} & 57.00 & 45.00 & 75.00 & 33.00 & 45.10 \\
		\textbf{500M-human}  & 58.35 & 45.81 & 76.17 & 33.74 & \textbf{45.35} \\
		\midrule
		\textbf{500M-1000g*} & 56.00 & 48.00 & 76.00 & 34.00 & 46.10 \\
		\textbf{500M-1000g}  &59.33 & 49.29 & 76.29 & 36.79 & \textbf{47.68} \\
  \midrule
  {\textbf{2500M-1000g*} } & 57.00 & 49.00 & 79.00 & 41.00 & 50.00  \\

   {\textbf{2500M-1000g} } & 61.20 & 52.36 & 79.76 & 41.46  & \textbf{50.86}\\
   \midrule
   {\textbf{2500M-multi*} } &62.00 & 55.00 & 81.00 & 49.00 & 56.90 \\

            {\textbf{2500M-multi} } & 64.70 & 56.01 & 81.67 & 49.13 & \textbf{58.06} \\
		\bottomrule
	\end{tabular}
	\caption{
		This table presents the performance of the Nucleotide Transformer on ten datasets of epigenetic marks prediction on the Yeast genome. As shown in the table, our implementation achieves better performance than the results reported in the paper, indicating the fairness of comparison in our experiments. *: Results taken from \citet{nt}.
	}\label{tb:nt_preliminary}
\end{table}





\section{Data}

\subsection{Multi-Species Genome for Pre-Training}
\label{subsec:appen_data_pretrain}

Table \ref{tb:multi_species_details} lists the 135 species in 7 categories that we randomly selected for genome foundation model pre-training and presents the number of nucleotides we achieved from each species.

\begin{longtable}{lll}

	\toprule
	\textbf{Category} & \textbf{Species} & \textbf{Num. of Nucleotides (M)}  \\
	\midrule
	\endfirsthead
	\multicolumn{3}{l}{\textit{(Continued from previous page)}} \\
	\midrule
	\textbf{Category} & \textbf{Species} & \textbf{Num. of Nucleotides (M)} \\
	\midrule
	\endhead
	\midrule
	\multicolumn{3}{r}{\textit{(Continued on next page)}} \\
	\endfoot
	\endlastfoot

 	\multirow{10}{*}{\textbf{Fungi}} & Ceratobasidium & 655.37 \\
& Claviceps Maximensis & 329.79 \\
& Fusarium Annulatum & 449.98 \\
& Melampsora & 699.52 \\
& Metschnikowia & 109.36 \\
& Mucor Saturninus & 391.17 \\
& Penicillium Chermesinum & 275.81 \\
& Saccharomyces Cerevisiae & 121.54 \\
& Sporopachydermia Quercuum & 155.71 \\
& Tranzscheliella Williamsii & 184.77 \\
& Xylariales & 399.96 \\
	
	\midrule
	
	\multirow{2}{*}{\textbf{Protozoa}} &  Phytophthora Sojae & 792.65 \\
& Pythium Apiculatum & 450.99 \\
	
	\midrule
	
	\multirow{7}{*}{\textbf{Mammalian}} 
 & Bubalus Bubalis & 28768.00 \\
& Camelus Dromedarius & 19757.02 \\
& Human & 31372.10 \\
& Macaca Assamensis & 27593.76 \\
& Macaca Nigra & 28217.13 \\
& Mus Musculus & 26545.98 \\
& Peromyscus Californicus & 24677.56 \\
	
	\midrule

 & Anas Zonorhyncha & 11697.08 \\
\textbf{Other} & Coregonus Clupeaformis & 26824.02 \\
\textbf{Vertebrate} & Gnathonemus Longibarbis & 7314.74 \\
& Myxocyprinus Asiaticus & 23407.19 \\
& Rhipidura Dahli & 10112.96 \\
	
 \midrule

\multirow{20}{*}{\textbf{Bacteria}} & Aeromonas & 47.33 \\
& Agrobacterium & 97.22 \\
& Alcaligenaceae Bacterium & 20.88 \\
& Aliivibrio & 46.48 \\
& Alphaproteobacteria Bacterium & 14.22 \\
& Amycolatopsis Antarctica & 63.43 \\
& Anaerostipes Faecis & 32.00 \\
& Arthrobacter & 36.27 \\
& Atopobium & 28.63 \\
& Bacillus Bc15 & 57.34 \\
& Bacillus Bs3 2021  & 43.51 \\
& Bacterium & 7.54 \\
& Bacteroidetes Bacterium Qs & 8.99 \\
& Breoghania Corrubedonensis & 53.32 \\
& Caldicoprobacter Oshimai & 27.25 \\
& Candidatus Cryptobacteroides Excrementipullorum & 27.63 \\
& Candidatus Dadabacteria Bacterium Rbg Combo & 11.49 \\
& Candidatus Dwaynia Gallinarum & 16.82 \\
& Candidatus Falkowbacteria Bacterium & 13.88 \\
& Candidatus Geothermincola Secundus & 24.76 \\
& Candidatus Gottesmanbacteria Bacterium & 11.08 \\
& Candidatus Nomurabacteria Bacterium Full & 6.29 \\
& Candidatus Portnoybacteria Bacterium Big Fil Rev & 8.17 \\
& Candidatus Regiella Insecticola & 20.62 \\
& Candidatus Roizmanbacteria Bacterium Combo All & 11.13 \\
& Candidatus Rokubacteria Bacterium & 22.06 \\
& Candidatus Saccharibacteria Bacterium & 6.55 \\
& Candidatus Staskawiczbacteria Bacterium Full & 6.79 \\
& Christensenella & 18.75 \\
& Clostridiaceae Bacterium & 29.62 \\
& Clostridiales Bacterium & 16.59 \\
& Clostridium Cag 505 & 21.26 \\
& Clostridium Mcc328 & 36.43 \\
& Clostridium Nexile & 38.43 \\
& Clostridium Uba3521 & 25.99 \\
& Collinsella Urealyticum & 19.45 \\
& Coprobacillus Cateniformis & 38.38 \\
& Cyanobium & 40.33 \\
& Dehalococcoidia Bacterium & 17.59 \\
& Enterobacteriaceae Bacterium & 41.46 \\
& Evtepia Gabavorous & 24.94 \\
& Firmicutes Bacterium & 36.66 \\
& Fulvivirga & 65.24 \\
\textbf{Bacteria} & Jeongeupia Chitinilytica & 39.11 \\
& Legionella Endosymbiont Of Polyplax Serrata & 5.30 \\
& Listeria Ilorinensis & 30.31 \\
& Maribacter Cobaltidurans & 46.40 \\
& Marinomonas & 37.73 \\
& Mesorhizobium & 65.15 \\
& Methyloceanibacter Caenitepidi & 34.25 \\
& Microvirga & 68.63 \\
& Mycolicibacter Engbaekii & 45.21 \\
& Novosphingobium & 46.18 \\
& Omnitrophica Wor Bacterium Rbg & 12.52 \\
& Pantoea & 43.14 \\
& Paraburkholderia Edwinii & 82.99 \\
& Parerythrobacter Lutipelagi & 30.98 \\
& Paulownia Witches Phytoplasma & 8.92 \\
& Polaromonas Eurypsychrophila & 41.61 \\
& Prevotella Ag 487 50 53 & 29.63 \\
& Prevotella Uba3619 & 31.72 \\
& Prevotella Uba634 & 18.51 \\
& Prochlorococcus Ag-321-I09 & 3.29 \\
& Prochlorococcus Ag-363-B18 & 15.54 \\
& Prochlorococcus Ag-402-L19 & 11.17 \\
& Prochlorococcus Scb243 498N4 & 14.12 \\
& Providencia & 41.89 \\
& Pseudomonas 35 E 8 & 63.56 \\
& Pseudomonas Bigb0408 & 59.52 \\
& Pseudomonas P867 & 62.01 \\
& Pseudomonas Promysalinigenes & 50.47 \\
& Roseobacter & 44.14 \\
& Salinicola Peritrichatus & 46.19 \\
& Salmonella S096 02912 & 48.09 \\
& Salmonella Zj-F75 & 47.87 \\
& Sinorhizobium & 65.53 \\
& Sodalis Ligni & 63.85 \\
& Sphaerochaeta & 28.61 \\
& Sphingobacterium & 36.55 \\
& Sphingomonas Carotinifaciens & 37.53 \\
& Sphingomonas Mesophila & 22.91 \\
& Sporosarcina Jiandibaonis & 36.30 \\
& Sporosarcina Ureilytica & 34.37 \\
& Staphylococcus Gdq20D1P & 28.50 \\
& Staphylococcus M0911 & 24.38 \\
& Streptococcus & 22.18 \\
\textbf{Bacteria} & Streptomyces 8401 & 88.39 \\
& Streptomyces Di166 & 88.71 \\
& Streptomyces Durbertensis & 59.24 \\
& Streptomyces Neau-Yj-81 & 118.84 \\
& Streptomyces Rk74B & 87.36 \\
& Thermopetrobacter & 26.06 \\
& Uncultured Kushneria & 35.31 \\
& Uncultured Phascolarctobacterium & 17.95 \\
& Uncultured Proteus & 35.66 \\
& Verrucomicrobiales Bacterium & 3.15 \\
& Vibrio & 41.47 \\
& Victivallis Lenta & 55.45 \\
& Virgibacillus Salexigens & 44.18 \\
& Xanthomonadales Bacterium & 37.47 \\

\bottomrule

 \caption{Details statistics of the multi-species genome dataset for pre-training.}\label{tb:multi_species_details} 
\end{longtable}




\subsection{Genome Understanding Evaluation (GUE)}
\label{subsec:appen_data_gue}



\begin{table*}[h]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{2.4mm}{
	\begin{tabular}{llll}
        \toprule
		
		 \textbf{Task} & \textbf{Metric} & \textbf{Datasets} & \textbf{Train / Dev / Test} \\
        \midrule

 	\multirow{3}{*}{\textbf{Core Promoter Detection}} & \multirow{3}{*}{\textbf{mcc}} 
        & tata & 4904 / 613 / 613 \\
	& & notata & 42452 / 5307 / 5307 \\
	& & all & 47356 / 5920 / 5920 \\

 \midrule

 \multirow{3}{*}{\textbf{Promoter Detection}} & \multirow{3}{*}{\textbf{mcc}} 
        & tata & 4904 / 613 / 613\\
	& & notata & 42452 / 5307 / 5307 \\
	& & all & 47356 / 5920 / 5920 \\

 \midrule


  \multirow{5}{*}{\textbf{Transcription Factor}} & \multirow{5}{*}{\textbf{mcc}} 
        & wgEncodeEH000552 & 32378 / 1000 / 1000 \\
	&& wgEncodeEH000606 & 30672 / 1000 / 1000 \\
	&& wgEncodeEH001546 & 19000 / 1000 / 1000 \\
       \textbf{Prediction (Human)} && wgEncodeEH001776 & 27294 / 1000 / 1000 \\
        && wgEncodeEH002829 & 19000 / 1000 / 1000 \\
	
	\midrule

\textbf{Splice Site Prediction} & \textbf{mcc} 
        & reconstructed & 36496 / 4562 / 4562 \\
	
	\midrule




\multirow{5}{*}{\textbf{Transcription Factor}} & \multirow{5}{*}{\textbf{mcc}} 
        & Ch12Nrf2Iggrab  & 6478 / 810 / 810 \\
	&& Ch12Znf384hpa004051Iggrab  & 53952 / 6745 / 6745 \\
	&& MelJundIggrab  & 2620 / 328 / 328 \\
       \textbf{prediction (Mouse)} & &MelMafkDm2p5dStd & 1904 / 239 / 239\\
      &  & MelNelfeIggrab  & 15064 / 1883 / 1883 \\
	
	\midrule

\multirow{10}{*}{\textbf{Epigenetic Marks Prediction}} & \multirow{10}{*}{\textbf{mcc}} 
        & H3  & 11971 / 1497 / 1497 \\
	&& H3K14ac  & 26438 / 3305 / 3305 \\
	&& H3K36me3  & 27904 / 3488 / 3488 \\
        && H3K4me1 & 25341 / 3168 / 3168 \\
        && H3K4me2  & 24545 / 3069 / 3069 \\
        && H3K4me3  & 29439 / 3680 / 3680 \\
	&& H3K79me3  & 23069 / 2884 / 2884 \\
	&& H3K9ac  & 22224 / 2779 / 2779 \\
        && H4 & 11679 / 1461 / 1461 \\
        && H4ac  & 27275 / 3410 / 3410 \\
	
	\midrule

\textbf{Virus}
        & \textbf{f1}
        & Covid variant classification & 77669 / 7000 / 7000 \\
	 

		\bottomrule
	\end{tabular}}
	\caption{ \footnotesize  
		Statistics of tasks in the GUE benchmark, including the name and the number of training, validation, and test samples in each dataset.
	}\label{tb:gue_details}
\end{table*}


The proposed benchmark Genome Understanding Evaluation (GUE) contains $28$ datasets of $7$ biological important genome analysis tasks for $4$ different species. To comprehensively evaluate the genome foundation models in modeling variable-length sequences, we select tasks with input lengths ranging from $70$ to $1000$. Table \ref{tb:gue_details} presents the details statistics of each evaluation dataset. The following tasks are included in the GUE benchmark.



\paragraph{Promoter detection (Human)} focuses on identifying (proximal) promoter regions, crucial sequences in the human genome responsible for instigating transcription. As many primary regulatory elements are located in this region, accurately detecting these sites is instrumental in advancing our grasp of gene regulation mechanisms and pinpointing the genomic underpinnings of numerous diseases. The dataset is divided twofold, TATA and non-TATA, based on whether a TATA box motif is present in the sequence. We extract -249~+50 bp around the transcription start site (TSS) from TATA and non-TATA promoters downloaded from Eukaryotic Promoter Database (EPDnew) \citep{dreos2013epd} and use it as our promoter class. Meanwhile, we construct the non-promoter class with equal-sized randomly selected sequences outside of promoter regions but with TATA motif (TATA non-promoters) or randomly substituted sequences (non-TATA, non-promoters). We also combine the TATA and non-TATA datasets to obtain a combined dataset named \textit{all}.

\paragraph{Core promoter detection (Human)} is similar to proximal promoter detection with a focus on predicting the core promoter region only, the central region closest to the TSS and start codon. A much shorter context window (center -34~+35 bp around TSS) is provided, making this a more challenging task than proximal promoter prediction. 



\paragraph{Transcription factor binding site prediction (Human)}  predicts binding sites of transcription factors (TF), the key proteins that regulate gene expression in the human genome. Their accurate prediction is key to deciphering complex genetic interactions and identifying potential targets for gene therapies. We accessed the legacy 690 ENCODE ChIP-seq experiments \citep{encode2012integrated} via the UCSC genome browser, which encompasses 161 TF binding profiles in 91 human cell lines. We extracted a 101-bp region around the center of each peak as TFBS class and nonoverlapping sequences with the same length and GC content as non-TFBS class. Finally, we randomly select $5$ datasets out of a subset of 690 that we curated by heuristically filtering out tasks that are either too trivial (e.g., over 0.95 F1) or too challenging (e.g., less than 0.50 F1) for existing language models.


\paragraph{Splice site prediction (Human)} predicts splice donor and acceptor sites, which are the exact locations in the human genome where alternative splicing occurs. This prediction is crucial to understanding protein diversity and the implications of aberrant splicing in genetic disorders. The dataset \citep{wang2019splicefinder} consists of 400-bp-long sequences extracted from Ensembl GRCh38 human reference genome. As suggested by \citet{dnabert}, existing models can achieve almost perfect performance on the original dataset, containing 10,000 splice donors, acceptors, and non-splice site sequences, which is overly optimistic on detecting non-canonical sites in reality. As such, we reconstruct the dataset by iteratively adding adversarial examples (unseen false positive predictions in hold-out set) in order to make this task more challenging.  




\paragraph{Transcription factor binding site prediction (Mouse)} predicts the binding site of transcription factors on mouse genomes. Similar to human binding site data, we obtain mouse ENCODE ChIP-seq data \citep{mouse}, which is the largest available collection on the UCSC genome browser (n=78). This time, the negative examples are created using dinucleotide shuffling while preserving relative frequencies, while all other settings stay the same as the human TFBS prediction dataset. We also randomly select $5$ datasets out of the $78$ datasets using the same process described above.


\paragraph{Epigenetic marks prediction (Yeast)}  predicts epigenetic marks in yeast, modifications on the genetic material that influence gene expression without altering the DNA sequence. Precise prediction of these marks aids in elucidating the role of epigenetics in yeast. We download the $10$ datasets from \url{http://www.jaist.ac.jp/~tran/nucleosome/members.htm} and randomly split each dataset into training, validation, and test sets with a ratio of 8:1:1.


\paragraph{Covid variant prediction (Virus)} aims to predict the variant type of the SARS\_CoV\_2 virus based on $1000$-length genome sequences. We download the genomes from the EpiCoV database \citep{covid} of the Global Initiative on Sharing Avian Influenza Data (GISAID). We consider $9$ types of SARS\_CoV\_2 variants, including \textit{Alpha}, \textit{Beta}, \textit{Delta}, \textit{Eta}, \textit{Gamma}, \textit{Iota}, \textit{Kappa}, \textit{Lambda} and \textit{Zeta}.



\end{document}
