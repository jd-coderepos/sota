

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}


\usepackage{amsmath,amsfonts,bm,cuted}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{graphicx}
\usepackage{tabularx} \usepackage{amssymb}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{upgreek}
\usepackage{url}



\newcommand\blue[1]{{\color{blue}#1}}
\newcommand\done[1]{{\color{black}#1}}
\newcommand\red[1]{{\color{red}#1}}

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{times}
\usepackage[accsupp]{axessibility}  \usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\normalsize
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{2} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Coarse-to-Fine Reasoning for Visual Question Answering}
\author{Binh X. Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D. Tran, Anh Nguyen\\
{AIOZ, Singapore}\\
{University of Liverpool, UK}\\
{\tt\small \{binh.xuan.nguyen, tuong.khanh-long.do, huy.tran, erman.tjiputra,quang.tran\}@aioz.io}\\
{\tt\small anh.nguyen@liverpool.ac.uk}}

\maketitle

\begin{abstract}
Bridging the semantic gap between image and question is an important step to improve the accuracy of the Visual Question Answering (VQA) task. However, most of the existing VQA methods focus on attention mechanisms or visual relations for reasoning the answer, while the features at different semantic levels are not fully utilized. In this paper, we present a new reasoning framework to fill the gap between visual features and semantic clues in the VQA task. Our method first extracts the features and predicates from the image and question. We then propose a new reasoning framework to effectively jointly learn these features and predicates in a coarse-to-fine manner. The intensively experimental results on three large-scale VQA datasets show that our proposed approach achieves superior accuracy comparing with other state-of-the-art methods. Furthermore, our reasoning framework also provides an explainable way to understand the decision of the deep neural network when predicting the answer. Our source codes can be found at: \url{https://github.com/aioz-ai/CFR_VQA}



\end{abstract}

\section{Introduction}
\label{sec:intro}

The Visual Question Answering (VQA) task aims to predict the correct answer of a given question such that the answer is consistent with the visual image content.
There are two main variants of VQA, i.e., Free-Form Opened-Ended (FFOE) and Multiple Choice (MC). In FFOE, an answer is a free-form response of a given image-question input pair, while in MC, the answer is chosen from a list of predefined ground-truth. In both cases, extracting meaningful features from the images and questions plays a key role.
Furthermore, mapping the semantic features from the images and questions also strongly affects the results \cite{gordon2018iqa}. Most of the existing solutions for the VQA task rely on visual relations \cite{chen2019routingGraph,cadene2019murel,zhang2020multimodal,yang2020trrnet}, attention mechanisms \cite{teney2017graphvqa,tan2019lxmert, Kim2018BilinearAN}, external knowledge \cite{gu2019externalGraph,li2019perceptual}, or message passing \cite{teney2017graphvqa} to link the visual clue with the associated information in the question. 


While both extracting and reasoning the features of the image and question are important for VQA, they are not trivial tasks in practice. Many questions (and answers) are composed of complex semantic information, which can have noise or ambiguous attributes. Current methods focus on utilizing  visual information \cite{chen2019routingGraph,liang2017deepAttGraph,Hudson2019LearningBA,lu2018r,nguyen2021graph,Wang2017FVQAFV,nguyen2020autonomous,narasimhan2018outofboxVQA,Marino2019OKVQAAV} without considering if the supporting information is useful or not \cite{gordon2018iqa}. Besides, many approaches aim to enrich the information extracted from both image and question regardless of the noisy information that may occur \cite{gu2019externalGraph,chen2019routingGraph,gao2020multi,gao2019DFAF}. This leads to the fact that although the image and question features can be extracted by a deep convolutional neural network, they may not be effectively utilized to reason and predict the correct answer.  

To bridge the semantic gap between images and questions in VQA, we introduce a new framework that focuses on reasoning the visual contents in the image and the semantic clues in the question in a coarse-to-fine manner. Our observation is that both image and question's features can be extracted gradually at different fine-grained levels. Therefore, we can map these features in each level to allow a stronger connection when reasoning. Our framework contains effective extractors for extracting meaningful features and predicates from the image and question. Furthermore, the answer outputted by our framework can be reasoned explicitly through the distribution maps during the prediction progress. These maps indicate the necessity of input features or predicates, allow us to understand which information is meaningful for predicting the answer. Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a simple, yet effective framework to extract meaningful features and predicates from the question and image. The extracted information can be used to explain the decision of the deep network.
    
    \item We introduce a new coarse-to-fine reasoning method to bridge the semantic gap between the question and image when predicting the answer.
    
    \item We conduct intensive experiments to validate our method. Our source code and trained models will be released for further study.
\end{itemize}







\section{Related Work}
\label{sec:related_work}
There are numerous reasoning VQA methods \cite{yi2018neuralreasoning,mao2019neuroSupervision,mascharka2018transparency,teney2021unshuffling,urooj2021found,zheng2021knowledge,zheng2020webly,chen2021meta,hong2021transformation, gao2020multi,do2021multiple,wu2019self,amizadeh2020neuro,luo2021just} that focus on learning the relations between visual regions and words in questions implicitly,  e.g., through message passing \cite{teney2017graphvqa}, pairwise relationship modeling \cite{cadene2019murel}, adversarial learning \cite{li2021adversarial,chi2020collaborative,minh2021deform}, or graph parsing methods defined by inter/intra-class edges \cite{gao2019DFAF}. Other works focus on leveraging external information \cite{gu2019externalGraph} or explicit scene graph \cite{chen2019routingGraph} to extract features from input images. ReGAT \cite{li2019regat} considers both explicit and implicit relations to enrich image representations.
Most of the current VQA works focus on enriching image representation without examining whether the enriched information is necessary for reasoning the answer or not \cite{le2020dynamic}. 


Extracting meaningful features from images, questions, and their joint embedding is crucial in the VQA task. For image representation, grid features \cite{jiang2020defense,zhu2016visual7w} or object features \cite{bottom-up2017,do2018affordancenet,tip-trick,nguyen2019object,Ren2015FasterRCNN} are  widely used. For question embedding, Glove \cite{pennington2014glove} and BERT \cite{Devlin2019BERTPO} are used to present words and sentences. Besides, using large-scale pre-training models on image-text pairs is also popular \cite{li2020oscar,chen2020uniter}.
For learning the joint embedding, many approaches use attention mechanisms \cite{teney2017graphvqa,tan2019lxmert,nguyen2019v2cnet,Kim2018BilinearAN, do2019cti,zheng2020cross,zhang2021dmrfnet}. The authors in \cite{Yang2016StackedAN} propose Stacked Attention Networks to localize image regions that are relevant to the question. In~\cite{Kim2018BilinearAN}, the authors propose Bilinear Attention Networks for VQA.
Recently, in \cite{do2019cti}, the authors introduce Compact Trilinear Interaction which simultaneously learns the interaction between images, questions, and answers.

Unlike other approaches that focus on enriching information from image and question, in this work, we consider the interaction among the semantic clues in questions and the visual contents of the image ranging from object-level to fine-grained level. Hence, we apply a simplified fine-grained detector inspired by Faster R-CNN model \cite{Ren2015FasterRCNN} to extract visual features and predicates, rather than leveraging complicated scene graph generators. This setup allows us to achieve competitive results compared with other approaches, while keeping the network at a reasonable computational cost.

\section{Methodology}
\label{sec:input_rep}
\subsection{Overview}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio=true]{images/CFRF_full_v5.png}
    \caption{An overview of our framework.}
    \label{fig_overview}
\end{figure*}

Our Coarse-to-Fine Reasoning (CFR) framework takes an image and a question as inputs. The image is passed through the Image Embedding module to extract the region of interest (RoI) features and visual predicates. The question is processed in the Question Embedding module to extract the question features and question predicates. The predicates are keywords about objects, relations, or attributes of the image/question. To effectively map the visual modality and language modality, we jointly learn their features, as well as their predicates in the Coarse-to-Fine Reasoning module. Figure \ref{fig_overview} illustrates an overview of our framework. 

\subsection{Image Embedding}
\label{subsec_image_embedding}

The goal of the Image Embedding module is to extract RoI features and visual predicates from the input image. The RoI features are extracted by a deep object detector to localize all potential regions of interest. The visual predicates are extracted by classifying attributes and relations based on the visual RoI features provided by the object detector.

In practice, as in \cite{Kim2018BilinearAN, tip-trick}, we use the pre-trained Faster R-CNN model \cite{bottom-up2017} to extract visual features for each RoI. Note that the RoI feature is an important visual input for the VQA task. Therefore, we retain the original Faster R-CNN multi-task loss for object detection, then adding two additional Cross-Entropy losses for attribute class predictor and relation class predictor. The extracted objects, as well as their attributes and relations, are then re-arranged to form predicates. Each predicate follows one of three forms: single predicate \texttt{<obj>}; attribute-based predicate \texttt{<attr, obj>}; and relation-based predicate \texttt{<obj1, rel, obj2>}. Following \cite{Kim2018BilinearAN, tip-trick, li2019regat}, we use a pretrained Faster R-CNN model on the Visual Genome dataset \cite{visualgenome} to extract predicates from the images. For each word in each predicate, we apply 300-dim Glove word embedding \cite{pennington2014glove} to extract predicate features.








\subsection{Question Embedding}

\label{subsec_question_embedding}

The Question Embedding module aims to extract question features and question predicates. To extract question features, following \cite{Kim2018BilinearAN, do2019cti, yu2019mcan}, we apply -dim Glove word embedding \cite{pennington2014glove} accompanied by GRU \cite{2014ChoGRU} to extract the features and learn the dependencies of all words in the question. 

To extract question predicates, we pass the whole question through a stop-word filter. The filter is the combination of two lists. The first list contains words in the NLTK based stop-words \cite{loper2002nltk} list, i.e., words that do not add much meaning in a sentence.  The second list contains words from all the questions that have the frequency of occurrence is less than . Words in the second list are considered as rare words and hard for the model to learn. For each word in each question predicate, we apply 300-dim Glove word embedding \cite{pennington2014glove} to extract the predicate features.


\subsection{Coarse-to-Fine Reasoning}
\label{sec:FFOE_vQA}

Given the image features and predicates (,  ) as well as the question features and predicates (,  ), our goal is to predict an answer  in a list of ground-truth  using a trainable model  as follow:


To effectively map the information of the question to the visual information in the image, the Coarse-to-Fine Reasoning module utilizes three steps: Information Filtering, Multimodal Learning, and Semantic Reasoning. The  Information Filtering aims to filter out unnecessary visual information from the image based on the predicates. The Multimodal Learning module learns the semantic mapping between the question and image at coarse-grained and fine-grained levels. Finally, the Semantic Reasoning module combines the output of the multimodal learning step to predict the answer.


\subsubsection{Information Filtering}
\label{subsec:Instruc_Guiding}
Since the features and predicates of both the question and image are extracted by pretrained models, they may have noise or incorrect information. Therefore, we design the Information Filtering module to filter out unnecessary information. In practice, this module also helps us understand the importance of each RoI for each question. The Information Filtering takes the feature  and the predicate  as input.
Both  and  have a matrix form; ,  denote the number of instances (e.g., number of RoIs or number of predicates); ,  denote the dimension of each instance. 

To filter out the unnecessary information in the feature , we consider the predicate  as the supervision information. Through the interaction mechanism, we compute a weighting map  which is then applied to output the filtered information .  is computed as follow:


where  and  are learnable linear projection funtions which project  and   into    and  , respectively. 


Given the weighting map , the filtered information  is calculated by Equation (\ref{eq:instruction_fuse}):


where  is a channel-scaled vector;  denotes the Hardamard product.


In practice, the Information Filtering module is applied on both the image features and predicates ,  , as well as the question features and predicates ,   to achieve the filtered information  and . Here we use the unified symbol  for simplicity.



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio=true]{images/CFRF_Vis_Adaptation_v2.png}
    \caption{Examples of the predicted confidence scores of the
    Coarse-grained Learning, Fine-grained Learning, and Coarse-to-Fine Reasoning module.}
    \label{fig:sample_Adaptation}
\end{figure*}
\subsubsection{Multimodal Learning}
\label{subsec_multi_integration}

Inspired by the Unitary Attention Mechanism \cite{Kim2018BilinearAN}, we design the Multimodal Learning module to jointly learn the features from the visual and language modalities. Multimodal learning is essential for identifying the correlation between each instance in the image and the question, then identifying which instances in the image are useful for answering the question.







In this module, the features are jointly learned at two levels: coarse-grained and fine-grained. The coarse-grained level learns the interaction between question features and image features, while the fine-grained level learns the interaction between filtered information of the image and question obtained from the Information Filtering step.





\textbf{Coarse-grained learning.} The inputs for coarse-grained learning are the image features  and question features . The output of coarse-grained learning is a joint representation , where  is the dimension of the joint representation. Each -th element  of the join representation  is computed as follows:


where  and  are learnable factor matrices; ,  denote the number of instances in question and image;   is the bilinear attention distribution map of the joint representation ; ,  denote the dimension of each instance. The subscript  indicates the index of matrix column.  is computed by Equation (\ref{eq:bf_att_1}):


where  and  are learnable factor matrices, and independent of  and .

\textbf{Fine-grained learning.} We apply the same process of coarse-grained learning for fine-grained learning. The only difference is the inputs for fine-grained learning are the image filtered information  and question filtered information . Similar to Equation \ref{eq:bf_fuse_1} and \ref{eq:bf_att_1}, the fine-grained joint representation is computed as follow:

where  is computed as:









\subsubsection{Semantic Reasoning}
\label{subsec:Sec_Adapt}

The goal of Semantic Reasoning is to selectively learn information from both the Coarse-grained and the Fine-grained learning steps using a learnable adaptive weight , where  is the number of possible answers. In practice, this module takes  and  as inputs and then outputs the distribution  over candidates of all answers .


where  and  are the learnable adaptive weights of coarse-grained learning and fine-grained learning;  and  are learnable projection functions that project 
and 

into  
 
and 
 , respectively. 
To satisfy the constraint in Equation (\ref{eq:adapt_fuse}), we apply the softmax function for each vector ; the subscript  indicates the index of an answer in the answer list .

Through an end-to-end training process, the learned adaptive weights  identify the contribution of each input information to predict the answer. These weights are expected to robust with noisy information from the question or image at both the coarse-grained and the fine-grained level.  






















\section{Experiments}
\label{sec:exp}
\subsection{Dataset, baseline and evaluation protocol}
\textbf{Dataset.} We use three popular datasets in our experiments: GQA  \cite{hudson2019gqa}, VQA 2.0 \cite{vqav22016}, and Visual7W  \cite{zhu2016visual7w} . We follow the same split in each dataset for training and testing.

\textbf{Implementation.}
\label{subsec:implement}
We conduct experiments on an NVIDIA TITAN V 12GB GPU. The network is trained with a batch size of  and a learning rate of  using Adam optimizer. Following \cite{Kim2018BilinearAN, kim2020hypergraph,tip-trick,Yang2016StackedAN}, we use the Visual Genome \cite{visualgenome} and Glove \cite{pennington2014glove} to extract the image embedding and question embedding. Then we train the whole framework from scratch. The parameters  and  are empirically set to . The learnable factor matrices ,  are initialized randomly at the beginning of the training phase and being learned through the training process. It takes approximately , , and  hours to train our network on Visual7W, VQA2.0, and GQA dataset, respectively. 

\textbf{Baselines.} We compare our results with various recent methods in VQA. These methods can be categorized into three groups: joint learning mechanisms: BAN \cite{Kim2018BilinearAN}, Pythia \cite{Jiang2018PythiaVT}, DFAF \cite{gao2019DFAF}, fPMC \cite{hu2018learningfPMC}, STL \cite{wang2018structuredSTL}, CTI \cite{do2019cti}, and MCAN \cite{yu2019mcan}; reasoning-based methods: Murel \cite{cadene2019murel}, ReGAT \cite{li2019regat}, MMN \cite{chen2021meta}, NMS \cite{Hudson2019LearningBA}, and HAN \cite{kim2020hypergraph}; and large-scale visual-language modeling: LXMERT \cite{tan2019lxmert}, OSCAR \cite{li2020oscar}, and UNITER \cite{chen2020uniter}. 

\textbf{Evaluation Metrics.} 
As the standard practice, we use the accuracy metric (\textit{Acc})~\cite{VQA} to evaluate the free-form opened ended dataset (GQA and VQA 2.0), and  \textit{Acc-MC}~\cite{zhu2016visual7w} to evaluate the multiple-choice dataset (Visual7W).



\begin{table}[!t]
\begin{center}
\small
\setlength{\tabcolsep}{0.3 em} {\renewcommand{\arraystretch}{1.2}\begin{tabular}{c|c|c|c|c|c|c}
\hline
\multirow{4}{*}{\textbf{Method}}                    & \multicolumn{6}{c}{\textbf{Dataset}} \\ \cline{2-7} 
& \multicolumn{2}{c|}{\textit{\textbf{GQA (Acc)}}} & \multicolumn{2}{c|}{\textit{\textbf{VQA 2.0 (Acc)}}} & \multicolumn{2}{c}{\textit{\textbf{\begin{tabular}[c]{@{}c@{}}Visual7W\\ (Acc-MC)\end{tabular}}}} \\ \cline{2-7} 
& \textbf{val}          & \textbf{tes-dev}         & \textbf{val}            & \textbf{test-dev}          & \textbf{val}               & \textbf{test}              \\ \hline
BAN \cite{Kim2018BilinearAN} & 61.5                  & 55.2                     & 66.0                    & 70.0                       & 65.7                       & 67.5                       \\  
Pythia \cite{Jiang2018PythiaVT}    &                  &                     & 66.3                    & 70.0                       &                       &                       \\ 
DFAF \cite{gao2019DFAF}            &                  &                     & 66.2                    & 70.2                       &                       &                       \\
fPMC \cite{hu2018learningfPMC}     &                  &                     & 61.7                    & 63.9                       &                       & 66.0                       \\ 
STL \cite{wang2018structuredSTL}   &                  &                     &                    &                       & 67.5                       & 68.2                       \\ 
CTI \cite{do2019cti}               & 61.7                  & 54.9                     & 66.0                    & 70.1                       & 67.0                       & 69.3                       \\ 
MCAN \cite{yu2019mcan}             &                  & 57.4                     & 67.2                    & 70.6                       &                       &                       \\\hline
MuRel \cite{cadene2019murel}       &                  &                     & 65.1                    & 68.0                       &                       &                       \\ 
ReGAT \cite{li2019regat}           &                  &                     & 67.2                    & 70.3                       &                       &                       \\  
MMN \cite{chen2021meta}            &                  & 60.4                     &                    &                       &                       &                       \\ 
NMS \cite{Hudson2019LearningBA}    &                  & 63.2                     &                    &                       &                       &                       \\ 
HAN \cite{kim2020hypergraph}       &                  & 69.5                     & 65.5                    & 69.1                       &                       &                       \\ \hline
LXMERT \cite{tan2019lxmert}        & 59.8                  & 60.0                     &                    & 72.4                       &                       &                       \\ 
OSCAR \cite{li2020oscar}           &                  & 61.6                     &                    & 73.6                       &                       &                       \\

UNITER-base \cite{chen2020uniter}           &                  &                     &                    & 72.7                      &                       &    \\

UNITER-large \cite{chen2020uniter}           &                  &                     &                    & \textbf{73.8}                      &                       &                       \\ \hline
\textbf{CFR (ours)}                                 & \textbf{73.6}         & \textbf{72.1}            & \textbf{69.7}           & 72.5              & \textbf{69.8}              & \textbf{71.9}              \\
\hline
\end{tabular}
}
\end{center}
\caption{The accuracy of our method and other approaches on three VQA datasets.
}
\label{tab:sota}
\end{table}

\begin{figure*}[!ht]
  \centering
    \subfigure[]{\includegraphics[width=0.31\linewidth, height=0.47\linewidth]{images/Good_Example_1_v3.png}}
    \subfigure[]{\includegraphics[width=0.31\linewidth, height=0.47\linewidth]{images/Good_Example_2_v3.png}}
    \subfigure[]{\includegraphics[width=0.31\linewidth, height=0.47\linewidth]{images/Good_Example_3_v3.png}}
    \subfigure[]{\includegraphics[width=0.31\linewidth, height=0.47\linewidth]{images/Good_Example_4_v3.png}}
    \subfigure[]{\includegraphics[width=0.31\linewidth, height=0.47\linewidth]{images/Good_Example_5_v3.png}}
    \subfigure[]{\includegraphics[width=0.31\linewidth, height=0.47\linewidth]{images/Bad_Example_1_v3.png}}
\caption{Visualization of the explicit contribution of RoIs and predicates in both input image and question. The \cmark  and  \xmark  symbols indicate the correct and the wrong answers, respectively. The arrow indicates the attribute or relation from the attribute classification or relation classification step in our Image Embedding module.}
 \label{fig:example}
\end{figure*}

\begin{table*}[!ht]
\begin{center}
\small
\setlength{\tabcolsep}{0.17em} {\renewcommand{\arraystretch}{1.2}\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}} Methods\end{tabular}}}}                                                                          & \multicolumn{3}{c|}{\textbf{Language Modality}}                                                                                                         & \multicolumn{3}{c|}{\textbf{Vision Modality}}                                                                                                           & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Acc \\ (\%)\end{tabular}}} \\ \cline{3-8}
\multicolumn{2}{|c|}{}                                                                                                                                                              & \textit{\begin{tabular}[c]{@{}c@{}}Question\\ Features\end{tabular}}                     & \textit{\begin{tabular}[c]{@{}c@{}}Predicates\end{tabular}}                   & \textit{\begin{tabular}[c]{@{}c@{}}Filtered\\ Info\end{tabular}} & \textit{\begin{tabular}[c]{@{}c@{}}Image\\ Features\end{tabular}}                        & \textit{\begin{tabular}[c]{@{}c@{}}Predicates\end{tabular}}                   & \textit{\begin{tabular}[c]{@{}c@{}}Filtered\\ Info\end{tabular}} &                                                                               \\ \hline
\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Multimodal\\ Learning\end{tabular}}} & \textit{\begin{tabular}[c]{@{}c@{}}Coars grained\end{tabular}}                & \cmark\ &                                       &                                                                         & \cmark\ &                                       &                                                                         & 62.6                                                                    \\ \cline{2-9} 
                                                                                        & \multirow{2}{*}{\textit{\begin{tabular}[c]{@{}c@{}}Fine grained\end{tabular}}} & \cmark\                                       & \cmark\ &                                                                         & \cmark\                                       & \cmark\ &                                                                         & 67.2 (+4.6)                                                                   \\ \cline{3-9} 
                                                                                        &                                                                                           &\cmark\                                       &                                       & \cmark\                                   &  \cmark\                                     &                                       & \cmark\                                   & 69.5 (+6.9)                                                                   \\ \hline
\multicolumn{2}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Semantic Reasoning\end{tabular}}}                                                                                & \cmark\ &    \cmark\                                      &\cmark\                                 & \cmark\ &  \cmark\                                     & \cmark\                                   & 73.6 (+11.0)                                                                  \\ \hline
\end{tabular}
}
\end{center}
\caption{The contribution of each module in our CFR framework. 
}
\label{tab:abl_module}
\end{table*}
\subsection{Module Contribution}
\label{subsec:abl}

\subsection{Results}
\label{subsec:sota}
Table~\ref{tab:sota} summarizes our results compared with different recent methods in the VQA task. In the GQA dataset, our proposed method outperforms the recent approach HAN~\cite{kim2020hypergraph} on the test-dev set by . Regarding the multiple-choice Visual7W dataset, our method outperforms the work CTI \cite{do2019cti} by  in the validation set and  in the test set, respectively. The results show that our CFR can deal with compositional reasoning questions through the selected information from both coarse-grained learning and fine-grained learning. It is worth noting that our CFR achieves new state-of-the-art results in GQA and Visual7W datasets. 


It is more challenging for our method to improve the result in the VQA2.0 dataset. While our CFR still outperforms the recent reasoning work ReGAT \cite{li2019regat} by  and , UNITER-large \cite{chen2020uniter} achieves  higher than our CFR in the test-dev set. We note that the VQA2.0 dataset has fairly fewer compositional reasoning questions comparing with the GQA dataset \cite{hudson2019gqa}. Thus, it limits the effectiveness of methods that focus on reasoning the question and images, including our CFR. Our method also uses simple modules to extract image and question features, which may not be robust enough comparing with features extracted from complicated modules such as large-scale visual-language models \cite{li2020oscar} \cite{chen2020uniter}.




To evaluate the contribution of each module in our framework, we conduct the following experiment: Given different level of information of language and vision modality (features, predicates, and filtered information of the image/question), we gradually choose different pairs of vision and language modality as the input to predict the answer. The experiment is conducted using the GQA dataset.


Table \ref{tab:abl_module} shows the contribution of each module when different inputs are used. By using only the question and image feature (coarse-grained learning), our framework only achieves  accuracy. When we combine the question and image features with their corresponding predicates (fine-grained learning), the accuracy increase to . This result indicates the effectiveness of predicates. By applying the filtered information of both question and image, the performance of fine-grained learning increases to . This result shows that by reducing the negative influence of noisy information, the prediction accuracy can be improved. To effectively leverage all coarse-grained and fine-grained information, the Semantic Module is integrated into the framework and achieves  accuracy. This result validates the potency of Semantic Reasoning in selecting information for answering the complicated question. Overall, our introduced framework outperforms the baseline coarse-grained learning method by a large margin, i.e.,  accuracy.






\subsection{Visualization} 




Figure \ref{fig:sample_Adaptation} illustrates the comparison between using Coarse-grained learning, Fine-grained learning, and Semantic Reasoning when we visualize the confidence score of the top 5 output answers. From this figure, we notice that if the Coarse-grained or Fine-grained learning are used separately, the output answer may not be correct, and there is usually an ambiguity in the top two predicted answers. However, when we apply our whole Coarse-to-Fine Reasoning framework, the network predicts both answers correctly, and also there is no ambiguity between the top predicted answer and the second predicted answer. These results show that our Coarse-to-Fine Reasoning framework successfully encodes both the features and predicates from the image and question in a coarse-to-fine manner, hence consequently improves the prediction results.


Figure \ref{fig:example} illustrates the explicit contribution of RoIs and predicates in both input image and question when our framework answers different compositional questions. Note that the transparency level of each RoI/word indicates the importance of each information. The RoIs and predicates with no opacity are crucial instances for answering the corresponding question. The visualizations in samples  indicate the effectiveness of our CFR framework in reasoning the correct answers from the inference process. The sample in  demonstrates the case when our CFR predicts the wrong answer. The incorrect prediction may come from the limitation of extractors, i.e., the extracted features are not robust enough (e.g., ``cap" and ``helmet" in our false example). Figure \ref{fig:example} also shows that our CRF framework not only can increase the accuracy of the VQA task but also provides an explainable way to understand the prediction results.








\section{Conclusion}
\label{sec:con}
We have introduced a new simple, yet effective Coarse-to-Fine Reasoning (CFR) framework for the VQA task. Our CRF framework first extracts the features and predicates of both question and image. Then we propose a new reasoning module to map the key information in the question to the visual clues in the image in a coarse-to-fine manner. The intensive experiments on GQA, VQA2.0, and Visual7W datasets show that our framework achieves competitive results comparing with recent approaches. Our source code and trained models will be released for reproducibility and further study.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
