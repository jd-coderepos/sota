\documentclass[twocolumn]{article}
\usepackage{commentstar}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath}

\makeatletter
\DeclareRobustCommand{\qed}{\ifmmode \else \leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
  \fi
  \quad\hbox{\qedsymbol}}
\newcommand{\openbox}{\leavevmode
  \hbox to.77778em{\hfil\vrule
  \vbox to.675em{\hrule width.6em\vfil\hrule}\vrule\hfil}}
\newcommand{\qedsymbol}{\openbox}
\newenvironment{proof}[1][\proofname]{\par
  \normalfont
  \topsep6\p@\@plus6\p@ \trivlist
  \item[\hskip\labelsep\itshape
    #1.]\ignorespaces
}{\qed\endtrivlist
}
\newcommand{\proofname}{Proof}
\makeatother
\usepackage[dvips]{color}
\newcommand{\figput}[2]{
\begin{figure}[h]\begin{center}
\input{#1.pstex_t}
\end{center}
\caption{#2}
\label{fig:#1}
\end{figure}
}
\newcommand{\figputW}[2]{
\begin{figure*}[t]\begin{center}
\input{#1.pstex_t}
\end{center}
\caption{#2}
\label{fig:#1}
\end{figure*}
}
\makeatother

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}

\newcommand{\bool}{\{0,1\} }
\newcommand{\union}{\cup}
\newcommand{\pto}{\rightsquigarrow}
\newcommand{\fromto}{\rightleftarrow}

\newcommand{\topfigure}[3]{\begin{figure}[t]
  \centering{
    \psfig{figure=#1.eps,height=#3}
  }
  \caption{#2}
  \label{fig:#1}
  \end{figure}}

\begin{document}
\title{Lower bounds for distributed \\ Markov chain problems}
\author{Rahul Sami \\ School of Information \\ University of Michigan
  \\ \texttt{rsami@umich.edu} \and Andrew Twigg \\ Computing Laboratory
  \\ University of Oxford \\ \texttt{andy.twigg@comlab.ox.ac.uk}
}
\maketitle

\begin{abstract}
We study the worst-case communication complexity of distributed algorithms computing a path problem based on stationary distributions of random walks in a network  with the caveat that  is also the communication network. The problem is a natural generalization of shortest path lengths to expected path lengths, and represents a model used in many practical applications such as pagerank and eigentrust as well as other problems involving Markov chains defined by networks.

For the problem of computing a single stationary probability, we prove an  bits lower bound; the trivial centralized algorithm
costs  bits and no known algorithm beats this. We also prove lower bounds for the related problems of approximately computing the stationary probabilities, computing only the ranking of the nodes, and computing the node with maximal rank. As a corollary, we obtain lower bounds for labelling schemes for the hitting time between two nodes.
\end{abstract}

\section{Introduction}
\label{section:introduction}

Let  be a strongly-connected, directed, unweighted graph on  vertices.  defines a communication network, where nodes are processors and communication can only occur along edges.  also defines a {\em random walk} process: a token walks over the nodes (states) of , at each time step choosing the next node uniformly at random from the outgoing edges of the current node. We shall refer to this stochastic process as the {\em harmonic random walk on }.

A basic path problem in distributed computing is as follows. Given a network  where each node only knows its neighbours, compute the lengths  of the shortest path in  from each node  to some fixed node . We consider the natural generalization of this problem to expected path lengths using the harmonic random walk on . Let  be the expected length of the walk that begins at node  and terminates on first hitting node . We shall be interested in the values , i.e. the expected return times of the token. If the Markov chain defined by  is irreducible then these values exist and the random walk has a unique {\em stationary distribution}  where  is known as the {\em stationary probability} of node  and .

If  is undirected then the stationary probability of any particular vertex is proportional to its degree -- regardless of the structure of . This remarkable fact is the key to numerous applications involving Markov chains. Crucially however, the networks we consider are directed and so in general, no such simple closed-form expression exists for .

Random walks have been studied extensively and have numerous applications in distributed computing including self-stabilizing networks and token management \cite{coppersmith93}. In the last decade there has been substantial interest in using random walks to construct ranking schemes in networks, the most obvious being the pagerank scheme used by Google to rank web pages \cite{page98pagerank}.

In this paper we consider distributed algorithms that compute properties of the harmonic random walk on some network , with the caveat that the algorithms must use  both as a communication network and as an input, where each node initially knows only its local edges.

Fix some node . We say a distributed algorithm {\em computes} a value  iff it
terminates with at least one node knowing . The {\em communication
complexity} of a distributed algorithm is the total number of bits sent,
over all edges. The communication complexity of computing a value  is
the minimum communication complexity of any distributed algorithm that
computes . Our main aim is to show good lower bounds on the communication complexity of distributed algorithms computing , and some related problems.
Note that, since the harmonic random walk uses only rational probabilities (the reciprocal of the outdegree of a node), the stationary probabilities are also rational and can be computed using finite precision. Our problem, then, is not how to efficiently approximate a real number, but how to understand its inherent complexity based on the network topology that generates it.

Our main result is a series of lower bounds that suggest that, in the worst-case, one can do better than the trivial centralized algorithm, and in some cases randomization may be of help in reducing the communication complexity.

\subsection{Technique}
\label{sec:technique}

Our main technique for proving lower bounds is to consider a related two-party
communication complexity problem: partition the nodes of  into  and let Alice know  and Bob know , and choose some node . We first lower bound the number of bits Alice and Bob must exchange in order to compute  in the worst-case (see \cite{nisan97communication} for examples of this technique). Then we lift the result to the distributed case by replacing the cut  by a linear array, `stretching' each edge of the cut by as many edges as possible while maintaining  nodes in the network. Because of this, many of our worst-case instances resemble the `barbell graph'. To do this we appeal to the linear array conjecture \cite{237817,258622} as follows.

If we can show that there exists a class of graphs having a cut where at least  bits must be communicated across this cut, and if the cut is sparse (contains  edges), we can replace it by a line of  edges. The question `does this increase the communication complexity by a factor ?' is the linear array conjecture \cite{237817}. The answer is that the randomized communication complexity increases by a factor , where  is some constant less than . In other words, each of these edges must see  bits.

We shall prove all our two-party lower bounds by reduction from two main known problems. For the purely information-theoretic bounds, we reduce from {\em set-disjointness}: Alice and Bob each have a subset  of  and they must decide whether . The randomized communication complexity of disjointness is  bits, for any protocol that decides with error probability less than . Some of our results give bounds much stronger than the information theoretic results, but only for deterministic algorithms. For these results we use the problem {\em greater-than}: Alice and Bob each have an -bit number  and they must decide whether . Any deterministic protocol must have communication complexity  bits, yet any randomized protocol must communicate at least  bits. Other results and proofs in communication complexity can be found in the book \cite{nisan97communication}.

Our technique resembles that of Tiwari \cite{32978}, where the network is simulated on a linear array and then one can use a reduction from a known bound on the complexity of the function when computed on a linear array of processors. Our work differs from Tiwari's however, since we require that algorithm must use the network both for communication between processors and as the input. For this reason we cannot consider the function to be computed and the network that it is to be computed on as two separate problems. In particular, there appears to be a tradeoff between the strength of a two-party lower bound and our ability to `lift' it onto a larger network to obtain good bounds in the distributed case. In this sense, our problem is similar in spirit to that of Hakowiak et al. \cite{314705} who consider the problem of distributedly computing maximal matchings of a network (although they consider upper bounds on the time complexity rather than communication).

We believe that strengthening our lower bounds requires a different technique. This is because we believe that the two-party lower bounds are tight if the cut is sparse, but the lifting onto a linear array only amplifies the result by a factor of . In other words, the two party bound is replicated across at most a {\em linear} number of edges, and yet there are potentially  edges in such a network of low expansion and high diameter.

\subsection{Related work}

Aside from being an interesting problem in its own right, the problem is an
abstract model underlying several basic problems in distributed computing, for
example randomized routing, self-stabilization, network flow \cite{48689} and load balancing (where a node offloads work to its neighbours in proportion to their difference in current workloads). Although distributed and parallel algorithms have been used to solve Markov chains simulating large systems, for example queuing or communication networks, our setting is different -- the Markov chain that we wish to sample from is defined precisely by the communication network on which it needs to be computed.

The problem also underlies the pagerank algorithm \cite{page98pagerank} for web page ranking. Here,  represents the web graph where each node is a web page and an edge represents a hyperlink between two pages. The pagerank of  is defined as the stationary distribution of the harmonic random walk on , where  is obtained by adding a `reset' transition from every node to every node in some root set . In a problem of this scale, distributed computing in the network could be extremely valuable
\footnote{For pagerank, the nodes do not map directly to network 
nodes, as there are usually many pages on a single site. However, even 
if we collapse all pages on a site to a single page and abstract to the level
of network nodes, the problem is still sufficiently large that communication 
bottlenecks could be significant.}; it is thus useful to understand the communication requirements of algorithms for this problem. By adding the reset transitions to a large enough set , it is possible to show that the harmonic walk on  is {\em rapidly-mixing}, and hence any iterative algorithm for computing the pageranks will converge quickly, typically in  iterations. As the results in this paper are primarily worst-case results, they are unlikely to be tight for this particular problem. However, we feel that it is important to understand the worst-case complexity of the more general problem as we define it.

The desirable properties of Markov chains (for example, based on the known stability of principal eigenvectors under small perturbations) have led to them finding new applications in distributed web searching \cite{wang04vldb,hpdc03pagerank}, distributed `reputation' systems \cite{eigentrust} and many other problems that can be expressed as finding the stationary distribution of a Markov chain on some network. Although several distributed algorithms have been proposed for pagerank \cite{eigentrust,hpdc03pagerank,kamvar03exploiting,page98pagerank,wang04vldb,shi03icpp}, to the best of our knowledge nothing nontrivial is known about the communication complexity of the problem, nor these algorithms.

In trying to model more faithfully browsing behaviour on the world-wide web, Fagin et al. \cite{fagin00} introduce backoff processes. Such a process can be defined by a graph  (and its harmonic random walk), and for each node, a {\em backoff probability} , where at each time step with probability , the token moves forward as defined by the walk, and with probability  it returns to its previous state. They show some interesting phenomena that are induced by this process, for example it does not always have a limit distribution independent of the starting state, even if the underlying chain is ergodic. It would seem interesting to extend our results to obtain lower bounds for the complexity of these processes.

Fogaras and Rasz \cite{fogaras04} consider a related problem known as `personalized pagerank'. The personalized pagerank for a node  is defined as the unique stationary vector  where  for . They prove simple lower bounds on the size of the database required for a centralized server to be able to answer queries about  (for all nodes ), in both exact and approximate models. Like our results, their lower bounds utilise communication complexity arguments, but they only require reductions from one-way communication complexity rather than the two-way results we require to lower bound arbitrary distributed computations. Because of this, their results are purely information-theoretic, yet for some of our problems we are able to give much stronger results than the information-theoretic bound, by showing how we can use the network to `do work' for us.

As far as we know, we are the first to consider communication complexity lower bounds for problems where the network structure itself forms the input, and the output depends nontrivially on its structure. There has, of course, been progress with proving lower bounds for problems in distributed computation. Abelson \cite{322200} obtained the first nontrivial lower bounds for a distributed protocol to solve a system of linear equations, although his result applies to differentiable real-valued functions. The lower bound is based on showing that the matrix that describes the system has sufficiently high rank that any protocol must make a large number of choices to locate the solution. This is related to the well-known `fooling set' lower bound technique now a staple part of communication complexity. However, Abelson's result gives a lower bound on the number of values that must be communicated whereas we give information-theoretic results on the number of bits that must be communicated over a network which also forms part of the input.

\subsection{Summary of our results}

In section \ref{section:reversible_chains}, we consider the case where the network  is undirected and unweighted. The harmonic random walk induced is then that of a reversible chain. For these graphs we show that there is a simple optimal algorithm to
compute the stationary distribution. This result shows that reversible
chains can only encode local information about the graph into the
stationary probabilities.

Next we consider the case where  is directed. Our results show that,
unlike the undirected case, interesting structural properties can be encoded into the stationary distribution, and
understanding the nature of this is our main tool in obtaining
good communication complexity lower bounds. In Section
\ref{section:general_unweighted} we prove a lower bound on the total
communication required for any distributed algorithm to compute the
stationary probability of a single node in the graph. The motivation for this result is that, in a large distributed network, it is not efficient to compute the values for all nodes if only one node requires its value. As we show, even though the stationary probability of a single node depends on the stationary probability of all other nodes, our results suggest that we can compute a single probability from scratch, at a lower cost than computing all the values.

We then consider variants of the basic problem. In
Section~\ref{section:distribution}, we look at computing
the entire stationary distribution and prove that, asympototically, there are the same number of distinct principal eigenvectors as there are unweighted graphs. In Section \ref{section:approximation} we turn to the problem of
approximately computing stationary probabilities. Currently we know of
no distributed approximation algorithm that achieves a specified
approximation factor, yet this appears to be a useful practical problem.
In Section \ref{section:computing_ranks} we consider an interesting
variant of the problem: computing the rank of a single node in the
stationary distribution . We prove a communication lower bound for
computing the node with maximal rank, and whether a node has even or odd
rank (which implies a bound on computing the actual rank).

Finally, in comparing our results to those for computing shortest path lengths, we use the elegant path algebra framework of Gondran and Minoux \cite{817} to formalize the problems, and discuss the complexity results in terms of algebraic properties of the problem. This appears to be a novel approach to accounting for the complexity differences.

\section{Bounds for reversible chains}
\label{section:reversible_chains}
A Markov chain is \emph{reversible} iff it satisfies the detailed
balance
equations  i.e. the probability flux between any two nodes
is the same in both directions. In particular, if  is undirected then
the harmonic random walk on  is a reversible Markov chain. Reversible chains have many remarkable properties. In particular, the key to efficient computation on reversible chains is the following: the stationary probability of any node is proportional to its degree -- regardless of the structure of . More precisely, if  is undirected then using the detailed balance equations, it is easy to verify that  is indeed a stationary probability, and if  is ergodic then this is unique. Hence the stationary probability  is determined solely by  and the degree of . If  is not reversible then in general there is no such local expression for computing .

A simple algorithm for computing  is then as follows: given a spanning tree  of , each node computes the sum of the degrees of
the nodes below it in the tree in a depth-first manner. There are  edges in  and each edge carries at most  bits, hence this algorithm sends  bits in total in the worst case. The following simple theorem shows that this is asymptotically optimal.

\begin{theorem}
Any distributed algorithm that computes  for an -node reversible chain must communicate  bits over  edges in the worst case.
\end{theorem}
\begin{proof}
We will show an information-theoretic bound on the communication required between two sides of a cut in a sufficiently large class of graphs. Consider an undirected graph  with  nodes , and add an extra node  with an edge . We will consider the amount of communication required between  and , i.e. across the edge .

Since the degree of  is fixed and the harmonic walk on  gives a reversible chain, only the number of edges between the  affects the stationary probability of . Since there are  strongly-connected graphs with a distinct number of edges, this gives  different possible values of . If each one is equally likely then at least  bits must cross the edge .

Now imagine replacing the edge  by a linear array of  edges. As described in Section \ref{sec:technique}, we can lift our lower bound onto the linear array with an increase by a factor , for some constant . Hence at least  bits must flow over at least  edges in the worst case.
\end{proof}

It seems that the requirement of reversibility precludes the existence of an interesting relationship between the structure of the graph and the stationary distribution of the harmonic walk on it.

\section{Directed Markov chains}
\label{section:general_unweighted}
In the remainder of the paper, we consider directed graphs. The Markov chain may not have a stationary distribution and certainly does not have a simple closed form, as for undirected graphs.

A Markov chain is said to be {\em irreducible} if for all  there is a positive probability of the token reaching  from . Assume that the chain is irreducible. A fundamental result is that there exists a unique stationary distribution  with  that satisifies the balance equations  We shall therefore assume that  is strongly-connected and non-bipartite, as this will guarantee the existence of a stationary distribution.

Let  be the transition probability from state  to , and  be the probability of the token being at  after exactly  steps, starting from . A state  is recurrent if it is visited infinitely often in an infinitely long walk, and aperiodic if . Recurrent, aperiodic states are said to be {\em ergodic}. An irreducible chain whose states are ergodic is said to be ergodic. If the chain is ergodic then in addition, the limit  exists and is independent of . This forms the basis for iterative algorithms that compute . Since we want to lower bound the communication of {\em any} algorithm that computes , we shall not require ergodicity, but only that the stationary distribution  exists.

\subsection{Lower bound}
We now show an information-theoretic lower bound on the communication complexity of computing the stationary probability of a single node. Assume that the graph  is unweighted (hence  iff  and  otherwise) and directed, and has  nodes.
\begin{theorem}
Any distributed algorithm, randomized or deterministic, that computes 
for some  must communicate at least  bits over 
edges in the worst case.
\label{thm:main}
\end{theorem}
We will first prove an  bound for sparse graphs then show
how it can be improved to  in the case of dense graphs.

The following two lemmas establish lower bounds on the communication complexity of a two-party version of the problem by reduction from set-disjointness. We show that, without any communication Alice and Bob can construct a graph  (where the edges are partitioned between Alice and Bob), such that knowing the stationary probability  for some node  allows them to solve disjointness.

\begin{lemma}
The randomized communication complexity is  bits in the case of sparse graphs.
\label{lemma:sparse_graphs}
\end{lemma}
\begin{proof}
Given two -element sets , Alice and Bob construct a sparse graph  as follows. Because the stationary value of  will reveal the entire set , Bob need not encode anything, and so his part of the graph is constant.

The graph contains  nodes 
on a cycle , and nodes  and .
There is a sink node , with edges  and .
Finally, add two nodes  and  with edges  and . The cut of the graph shall be .
For each element , add an edge  and for each element , add an edge . 
\figput{cycle1}{Construction for Lemma~\ref{lemma:sparse_graphs} when  and .}
The construction is illustrated in Figure~\ref{fig:cycle1}.
Intuitively, as the stationary probabilities halve on each step in the cycle, the binary expansion of  should reveal . We shall use  to denote the stationary probability of , and similarly  for . Now, the flux drops exponentially along the cycle so we have

Consider the stationary probability :


For node  to be able to obtain  from the binary expansion of , the
value  must be a constant, independent of the sets . Note that this
does not follow trivially; for example, if we replaced the edge between , with a self-loop at  (or even with nothing), then  would depend on
. The reason for this is that there must be a flow of constant flux
(independent of ) from the nodes on the cycle, and back to , and hence
 must also be a constant. We now show that for our
construction, this is indeed the case. For simplicity, let 
and . Then

and

Since  is a probability distribution, it must sum to unity:

Hence  for some constant . Now, suppose
node  computed  in this graph. We can assume that it knows  (as it
depends only on ). Then it can read off the -bit set  from the binary
expansion of  (note that the largest element of  is represented by the least significant bit of ). Since the randomized communication complexity of set-disjointness is  bits, at least this many bits must cross the cut between Alice and Bob.
\end{proof}
More precisely, the construction defines a class of sparse graphs where any algorithm that computes  with probability of error at most  allows some node to learn an -bit set with the same probability of error.

Now we show how the two-party lower bound can be improved to  bits in the case of dense graphs. The idea is that, instead of each node on the cycle encoding a single bit, each node can encode a small set of size  bits, since it can potentially transfer  different proportions of its flux to the node . These small sets are now encoded in blocks of  bits into .

\begin{lemma}
\label{lemma:denselowerbound}
The randomized communication complexity is  bits, for dense graphs.
\end{lemma}
\begin{proof}
Take the previous construction, and add  nodes ,
where each  links to  and each  links to . Now partition the
set  into -element sets  where  will be encoded
by node . Each node  on the cycle links to exactly  nodes: It
links to , and for
each , it links to either  or .

Since the edges are unweighted, each edge contributes the same flux from a
node, hence each node  can now give  different fractions of its
probability flux to , via the . The intuition is that each  can
independently encode a set of  elements, allowing us to encode  bits of information into . As before, we also need to show that
 is still constant.

The flux now drops by a factor  on each step of the cycle, so
. Hence . Let  denote the value of the binary
expansion of the set  (where ). Then each
node  links to exactly  of the  (and hence exactly 
of the ). Consider the stationary probability :


Now we show that the flux crossing the cut is indeed constant. As before, let  and . Then

and


Since  is a probability distribution, it must sum to unity:

and so  is a constant  independent of . Hence . Since the sets  are of  bits,  and hence  reveals all the values ,
and hence the  sets .
\end{proof}

The previous lemmas have given us lower bounds for the transfer across the cut between two parties, where Alice and Bob each know only their subgraph. Note that we have taken care that in both constructions, the cut  is sparse and so we can build a modified -node graph by replacing each edge in this cut by  edges.

We now appeal to the linear array conjecture and so in the worst case at least  bits must flow over each of these edges (using the constant in the lifting of randomized communication bounds onto a linear array).

\subsubsection{Remarks}
The expansion of  represented by Equation \ref{eqn:dense_expansion} gives a clue as to why we cannot hope to improve the lower bound using our current methods. See that it can be roughly rewritten as (ignoring constants) . Hence each  only has  bits in the expansion of , even though it is quite easy to build a construction where the 's can be of  bits, and in this case the sets cannot be recovered since they begin to overlap in the binary expansion of .

The result also gives lower bounds on the worst-case congestion and time incurred by any algorithm to compute .For congestion, the stretching trick means that there must be at least a linear number of edges that each have  bits communicated across them. For a time lower bound, there are  bits that must cross a cut of size , hence any algorithm must take time  in the worst case.

\subsubsection{Labelling schemes}

A simple and interesting corollary of the two-party lower bound in this section is a lower bound on the complexity of a labelling scheme. A {\em distance labelling scheme} for a graph  is an assignment of labels  to nodes  of  such that, by examining only the labels , one can determine the distance  between  and . By encoding global information about a graph into local labels, labelling schemes have many practical applications in large-scale distributed networks \cite{gavoille01distance}. A {\em hitting time labelling scheme} is the natural extension of a distance labelling scheme to the random walk on a graph: given , one can compute the expected length  of the random walk beginning at  and terminating on first hitting .

Since , Lemma \ref{lemma:denselowerbound} implies that for any hitting time labelling scheme, there must be some graph where some node must be assigned a label of size  bits (in particular, this must occur for some node computing the expected time for the token to return to itself). Clearly there is an upper bound of  on the size of labels (by encoding the whole graph into each label), but the aim of efficient labelling schemes is to do much better than this. In \cite{gavoille01distance} it is shown that  bits is the optimal distance label length for general unweighted graphs, so our result shows an increase in complexity, but we do not know if the increase is more than just a logarithmic factor.

\subsubsection{Upper bounds for computing }

A simple algorithm to compute  would be to construct a spanning tree  of  rooted at node , and for each node to send a description of its edges to  using . For general unweighted graphs, this would require  bits being sent over  edges in total. Constructing a distributed algorithm with  bits worst-case total communication appears to be a challenging problem. We conjecture that the true lower bound is  but have been unable to prove this for an algorithm. We also believe that, for the problem of {\em exactly} computing the stationary probabilities, randomization is of no help as regards worst-case communication complexity.

For the related shortest paths problem, there is a long and interesting history of efficient algorithms, both sequential and more recently, distributed. Understanding these may help in obtained nontrivial upper bounds for the path problems we consider here. The best known communication complexity upper bound in the distributed case is  bits and relies on a graph decomposition to represent the graph as a partition of sparsely-connected clusters \cite{afek92sparser}. It is reasonably easy to show that any distributed algorithm that computes the shortest path lengths must have worst-case communication complexity  bits (there exist graphs where the length of the path to each node requires  bits, and each must be sent over  edges). Hence its communication complexity is fairly well-understood.

\section{Computing the entire distribution}
\label{section:distribution}

In this section we consider the slightly different problem of some node  knowing the entire vector  of stationary probabilities. This may correspond
to a distributed crawling algorithm that terminates with some centralized server  knowing the entire vector.

Our results for this section illustrate an interesting weakness of our
lower bound technique. In the two-party case, we prove that the trivial
algorithm of sending the entire graph is optimal (and so we cannot do
any better here), but since the cut between the  and  nodes is
dense, we cannot amplify our bound by lifting onto a linear array (or
other sparse structure). Therefore we only obtain an  bound
in the distributed case, even though the trivial (spanning tree)
algorithm costs  in this model. Improving this situation with
our current technique would involve finding a construction with the same
two-party complexity but with a much sparser cut, say with a constant or
(poly)logarithmic number of edges. We feel that an  bound is not
possible with this number of edges.

The intuition for an information-theoretic lower bound might be something like
this: each edge in the graph can alter the vector , therefore there are
 possible vectors, so  bits must be communicated. Of
course, this is nontrivial because while each edge does indeed change , it
is still possible that many combinations of edges result in the same . For
example, an -clique and an -cycle (adding chords to make it ergodic) both
have the same . So what we need to prove is a bound on the number of {\em
distinct} vectors  (or, the number of distinct principal eigenvectors of a
set of -node graphs).

We prove the lower bound by exhibiting a family of -node graphs with
 distinct principal eigenvectors.
\begin{theorem}
There is a family of -node directed, unweighted Markov chains with  distinct stationary vectors .
\label{thm:cycledense}
\end{theorem}
\begin{proof}
The construction is as follows. There are nodes ,  and a sink node . The edges are as follows:
\begin{itemize}
\item a cycle 
\item , to get an exponential dropoff on the cycle
\item  and , for all 
\end{itemize}
Finally, call a matrix  legal if  and  for all . For each entry , if , add the edge  and if , add . First, note that this graph is strongly connected for all legal matrices (this is why we forced the first row elements to 1, second row to 0).
The construction is illustrated in Figure~\ref{fig:cycledense}.
\figputW{cycledense}{Construction for Theorem~\ref{thm:cycledense}.}

We now show that each legal matrix gives a different vector , over the nodes . Firstly, see that if we flip one bit , the only values that change are  and . Now we show that each distinct vector  gives a different value for .

To the contrary, assume there are two vectors  with , where  is the stationary probability of  under the vector  (the th column of the matrix ). By definition,

that is, if  then there must be two distinct subsets of  that sum to the same value. But this is impossible since the values fall off exponentially (with the same factor) on the cycle construction.

This proves that each legal matrix gives a different vector , therefore the number of different vectors  is equal to the number of different vectors , which is . 
\end{proof}

The above lemma shows constructively that there are a family of n-node graphs (the whole construction) with  distinct stationary vectors (taken over the -node subgraph of the 's) and so any node that is to know this vector must have at least  bits communicated to it in the worst case.

\section{Approximate computation}
\label{section:approximation}
In this section we turn to the natural problem of approximating the stationary probabilities . Firstly, see that we must be careful with our notion of approximation: if  is to be computed to within  bits of precision, we can use the construction of Lemma \ref{lemma:sparse_graphs} to encode a set of size  bits rather than  bits, and the lower bound is accordingly reduced to  bits across  edges. A more natural notion of approximation may be to compute  to within a given factor. Call  a -approximation to  if . In this section we prove that any distributed algorithm that computes a -approximation to  for some chosen  (even with high probability) must send send at least  bits across  edges in the worst case.

First, let us examine the case . See that the difficulty with using our original binary encoding of the set is that, in the binary representation, all the bits of lower order than the highest order bit can be changed arbitrarily while remaining within a factor . The basic idea is to use a simple error correcting code that resists changes to a numeric factor of 2. Just using the highest order bit is not sufficient, since for example  can become  (an increase by a factor 2) and  (a decrease by a factor 2). A simple solution is to pad out the expansion, using blocks of length 3 bits, for example  Then the highest order bit can never fall out of its block. In the lower bound, we use blocks of length  bits to withstand factor  approximations.

The idea then, is to use a variant of the construction from Lemma \ref{lemma:denselowerbound} to produce a binary string where the set  is encoded into the position of the highest order bit of the `blocked' binary expansion. Just considering encoding a single set , we are looking for a binary expansion of the form  where the 1 is at some position, determined by the value (as previously defined)  of the set. Encoding  elements requires  possible indices in a binary expansion, which is exactly what we have available from the original construction.

To encode an  bit set , we compute  and then use the node  to encode this value by linking it to , and all other nodes to . The claim is that the block containing the highest order bit (and hence the encoding of the set) of the binary expansion of  can be recovered from the binary expansion of . Let us now prove the main lower bound.
\begin{theorem}
Consider any distributed algorithm computes  with . Then it must send  bits over  edges in the worst case.
\end{theorem}
\begin{proof}
We extend the idea outlined above to an approximability-preserving reduction from set-disjointness. The main difference is the `block cycle' construction: for each node  on the cycle, replace it with a block of  nodes where  is now the center node of the th block (and the rest are dummy nodes). We will show how to encode a  bit set  using  nodes in the construction. Imagine that the set has value . Now, pick the node  on the cycle and add an edge , and for all other nodes on the cycle (including the dummy nodes) add an edge to . The block construction will let  recover the value  under a -approximation. The ratio between successive 's on the cycle is now , hence


A bit of algebra establishes that  is indeed a constant . Consider now the actual value . Imagine encoding the set . Let  be such that the node  will represent this set (as described above). Then


Now  can find  easily, since , and . Now we claim that the block containing the highest order bit is the same in both  and , by showing that the values must be separated by a factor of least . Assume the set being encoded has value , hence (ignoring constants) the largest -approximate value of  is

Now if the set had value  instead, then the smallest -approximate value would be (letting  be 's stationary probability using the set with value )


These two values are the closest possible (a -overapproximation with set  and a -underapproximation with set ) and are still separated by exactly one bit in their binary expansion, and so there is no overlap and the value of the set can be recovered. Intuitively, the binary expansions of  and  look like the following, for a set  with value .

where each block has  bits and the highest order bit of  is contained in block  iff the highest order bit of  is in block .

For the communication complexity bound, see that there are only a constant number of edges crossing the cut, and so any protocol that computes a -approximation  with probability  allows us to solve  disjointness with  nodes, with the same probability. Hence, for a graph of  nodes we can solve  disjointness. The result follows since the cut is sparse and we can appeal to the linear array result, and by the communication complexity of set-disjointness.
\end{proof}

\subsubsection{Remarks}
As before, the result yields analogous time and congestion lower bounds. Also, note that  for constant . It may be interesting to investigate what happens for .

\section{Computing the ranks}
\label{section:computing_ranks}
We say that a node  has rank()= iff there are exactly  nodes  with stationary probabilities at least as large as :  for . Hence  has maximal rank iff rank()=1, and it has minimal rank if rank() = .

In this section we consider the difficulty of computing the rank of some node. Clearly if there are  nodes then the rank of a node can be expressed with  bits, unlike the stationary probability, which by Lemma \ref{lemma:denselowerbound} can require  bits. On the other hand, knowing that rank implies that some node must know there are exactly  nodes having larger stationary probability and  nodes having smaller stationary probability.



We now investigate the case where some algorithm terminates with many nodes in the network knowing their ranks -- again we prove a lower bound via a two-party argument and a lifting of the lower bound onto a linear array. In the two-party case, both Alice and Bob will need to know the ranks of  nodes in each of their subgraphs.

In fact, the lower bound holds when only the parity of the ranks are known, i.e. whether a node's rank is odd or even. The lower bound shows
that at least  bits must be communicated in total. Although our original intention was to prove a result for knowing the exact rank, we have been unable to strengthen it beyond the result here.

Given this statement, a natural question is `what use is knowing whether your rank is odd or even, since it doesn't imply anything
about knowing how many nodes have larger or smaller stationary value than you
(except for the parity of this number)?' We feel that presenting the result in this form exposes more details about the problem, and our proof.

On the other hand, if each node
knows whether it has even or odd rank, this may provide a useful
partitioning or symmetry breaking of the network into two pieces where the
total stationary probability in each piece is approximately equal (since if
one side has the maximum node then the next largest will be in the other
side). An interesting thing would be to determine if this can be done
without explicitly computing the ranks first.

The following theorem shows that the communication complexity of computing the rank parities is surprisingly large.

\begin{theorem}
Consider any algorithm that terminates with each node  knowing . The communication complexity is , and this many bits must be sent over  edges in the worst case.
\label{thm:rank}
\end{theorem}
\begin{proof}
To construct the two-party problem, partition the network into  where Alice knows  and Bob knows , and form an `exponential cycle' in  with nodes , and add an edge from the sink . For each , add
edges ,  and . Now,
for each , if , add edges 
else add edges . The point of the
construction so far is that all the  nodes have higher stationary
probability than all the  nodes. The  partition is exactly symmetric,
with  if  else . Finally, connect the two partitions with a sparse
cut by adding edges  between the two sinks.

\figputW{rank}{Construction for lower bound on computing ranks
(Theorem~\ref{thm:rank}).}
So far, the construction is completely symmetric. But we want the 
nodes in  to have slightly lower stationary probabilities, so we add a
self-loop at node . The construction is illustrated in
Figure~\ref{fig:rank}.
Now, we consider the rankings of each node in the
construction. There are three claims:
\begin{enumerate}
\item The rankings of the  are independent of . This follows since the stationary probabilities of these nodes are constant (in the same way as for the construction of Lemma \ref{lemma:sparse_graphs}), and for each , rank() and rank() are always the same apart, i.e they are always separated by the  nodes, and the  nodes (which also have constant stationary probability). Finally, adding the self-loop at  only changes the relative stationary probabilities of the two sides, since both  are constant.
\item rank()  rank() and rank()  rank(), for all . This is because the  and  tap into the  and , which have the highest stationary probability on the two cycles.
\item For any sets , rank() is less than both rank() and rank(), i.e  is larger. Also, rank() is less than both rank() and rank(). This is because the probability flux drops off exponentially along the cycle.
\end{enumerate}

So, the only effect of the sets  on the rankings of the  and  is to change the relative rankings of  and , for each . 
\begin{lemma}
If  and  then rank()  rank(). On the other hand, if  then rank()  rank().
\end{lemma}
\begin{proof}
The first part follows from the fact that the  nodes in Alice's half (encoding ) have higher stationary values than the  nodes in Bob's half (due to the self-loop at ). For the second part, we need that the ratio . This is true because  by the self-loop at  and .
\end{proof}

Hence, for any  such that , node  can inspect whether its ranking is odd or even, and accordingly determine whether . From this, Alice can determine the set  held by Bob, and then can determine the inner product of  and . Since we have managed to ensure that the cut is sparse, we can lift our result onto a linear array of size  and the lower bound follows.
\end{proof}

Interestingly, in the construction, it is easy for each  node to compute its stationary probability, since it only depends on the structure of the  partition (and hence can be computed with no communication between the two partitions), but to compute the rank of  is difficult because it depends on the structure of the other partition and there is an interplay between the two sides of the network. So, although the nodes do not have to compute the stationary probabilities, they must still implicitly know something about the structure of the ordering of the stationary probabilities in the network.

\subsubsection{Remarks}
It may be possible to improve the bound by encoding information into the  orderings of the ranks of the  nodes (which would intuitively allow us to solve the greater-than problem on  bit sets), but it appears a challenging problem to achieve this with only a sparse cut. Without a sparse cut, we would be unable to appeal to the linear array conjecture to lift the bound onto a network.

\subsection{Computing the maximum node}

In this section we consider the problem of computing the node of maximal rank, i.e. for some , an algorithm that terminates with at least one node  knowing if  is of maximum rank.



The node with maximal rank in a Markov chain is analogous to the center of a network in the shortest paths framework\footnote{The center of a graph  is the set of nodes of maximal graph eccentricity, and the eccentricity of a node  is .}. There appear to be several interesting applications for algorithms for finding the maximal rank node in a chain. For example, in a distributed network one could select a node with maximal rank to store a file, or to act as a leader of a subset of nodes\footnote{For reversible chains the stationary probability is proportional to the degree of a node so computing the node of maximal rank is equivalent to traditional leader election -- elect the leader as the node with highest degree, and of course the degree is known to each node.}.

\subsubsection{Lower bound}
We will prove that any deterministic distributed algorithm must send  bits in total, and must cause congestion of  on at least  links. In the deterministic case, this is as strong as the lower bound of Theorem \ref{thm:main} for exactly computing the stationary probability at  (although we have been unable to show that deciding if  is of maximal rank is at least as hard as computing ).

The output size of the problem is a single bit, so a simple information-theoretic bound would be far from strong. However, we can do much better by showing how the network can do some useful computation for us. We first prove an easy lower bound on the complexity of the following problem. There are two nodes , and some node (in particular, this could be one of ) wants to decide whether .
\begin{lemma}
\label{lemma:max}
Any algorithm that terminates with some node knowing whether  for two nodes  can solve greater-than on numbers of size -bits, in the worst-case.
\end{lemma}
\begin{proof}
We shall use a modified version of the construction used in Lemma \ref{lemma:denselowerbound}, except that Bob's side will encode a set , and Alice will be able to determine which set is greater by looking at the value of . Let Bob build his side in a symmetric manner to Alice, using his set . Now, on both sides, only the even nodes on the cycle are used to encode the elements (as opposed using all the nodes as in the construction of Lemma \ref{lemma:denselowerbound}).



These sets are encoded in reverse, so the least signficant bit of the stationary probability represents the largest element of the set. More precisely, we can show (as in the proof of next theorem) that in the simplified case (ignoring constants) that the difference of the stationary values elegantly encodes the difference of the sets:

Since the elements are encoded in reverse, we define the `reversed' set . Now suppose some node knows which of  has highest rank:

The difference  then reveals the difference , where  are taken wrt their binary expansions. Since there is a bijection between  and  there is no loss in using this representation.

Therefore if Alice knows whether , she can solve greater-than on numbers of  bits (and with the same probability and error if they use a randomized protocol).
\end{proof}
Since the randomized communication complexity of greater-than is  yet any deterministic protocol must communicate at least  bits, the lemma suggests that randomization may be of some help in solving this problem. As before, we will appeal to the linear array result to lift our two-party lower bounds onto linear arrays to obtain bounds for the distributed case.

If there are no ties, then the lemma immediately implies the same communication bound for computing whether rank()  rank(). Now we can use the lemma to show the same lower bound for some node determining which side of a (specified) partition the node of maximal rank lies in. The idea is simple but the algebra tedious -- if we can modify the construction to force the top two nodes  to lie in opposite sides of the partition, then knowing which side contains the maximum implies knowing whether  or not, and the same result as in the lemma applies.
\begin{theorem}
\label{thm:ranks}
Consider a partition  of a graph. Any deterministic (randomized) distributed algorithm that terminates with at least one node knowing whether the node of maximal rank is in  must communicate at least  bits ( bits) over  edges in the worst case. In particular, this applies if at least one node  knows if it has maximal rank.
\end{theorem}
\begin{proof}
\figputW{ranks}{Construction for Theorem~\ref{thm:ranks}.}
The construction is based on the construction of the previous lemma, except that we want  to be able to determine the answer to an instance of greater-than by knowing if  has maximal rank. An interesting feature of the construction is that it does more than just encoding a set into the result; the network itself actually does some computation in solving the greater-than instance, and the result appears at . To achieve this, we need to build the construction twice, once in each partition of the network. This is quite a powerful idea and allows us to substantially improve on the purely information-theoretic lower bounds.

Let , and all nodes in  (Alice's subgraph) know only  and all nodes in  (Bob's subgraph) know only . The idea is that if the node with maximal rank is in the  partition then  (wrt their binary expansions), otherwise . We shall use the construction of the previous lemma, but with a modification since the two sink nodes ,  take the top two spots, and since their values are independent of the sets  (by the construction) we cannot use them to distinguish between . We shall add edges  in Alice's subgraph and  in Bob's subgraph, and self-loops at nodes . The idea is that this will force one of  to be of maximal rank without affecting the fundamental properties of the construction (since the flux transferred from  to  is constant). Therefore  can check whether it is the maximum (in which case ) and if not, then  must be the maximum (in which case ). The full construction is shown in Figure \ref{fig:ranks}. Note the similarity to the construction of Lemma \ref{lemma:denselowerbound}.

We will only work through the details of the construction for the case where each node  on the cycle links directly to  or  (and similarly for the  in Bob's half of the network) -- this will give the -bit encoding for sparse graphs, and it can be improved as before to  for dense graphs by the technique of having each  link to  intermediate nodes (in which case the algebra becomes quite messy).

For ease of notation, we shall use , and . Letting  and , we have:

and similarly . Combining these equations, we get

Since the sum of all stationary probabilities is 1, this gives

where  is a constant that depends only on . We can also show that both  are constant, since

which solves to give

Now we also have , so  and hence

Combining (\ref{eqn:s1}) and (\ref{eqn:s2}) gives  and hence . Therefore both are constants even though they are in different partitions; this means that the flux flowing around both cycles is independent of the sets . Now we can use this to find the value :

A little manipulation, and recalling that , gives

for some constant .

Now we just need to check that one of  is of maximal rank. The edge from  to  ensures that  has higher rank than , and the self-loops at  ensure that  has rank at least as high as , since  (and similarly for the other half of the network). By breaking ties in favour of , one of them has maximal rank.

Define . Since one of  is of maximal rank, Lemma \ref{lemma:max} implies that node  is of maximal rank iff  (wrt to their binary expansions).

Now if some node knows that  is of maximal rank then  otherwise  has maximal rank and so . As shown in the Figure, we can increase the separation factor on the cycle (between successive 's) to , and so if Alice knows whether  is of maximal rank, she can solve greater-than on sets of size  bits.
\end{proof}

By the deterministic communication complexity of greater-than we have the following corollary.
\begin{corollary}
Consider any deterministic algorithm that terminates with at least one node knowing if it is of maximal rank. Then at least  bits must be sent over  edges in the network, in the worst case.
\end{corollary}

For randomized algorithms, the situation is somewhat different. The randomized complexity of greater-than is .
\begin{corollary}
Consider any algorithm that terminates with at least one node knowing if it is of maximal rank, with probability at least 2/3. Then at least  bits must be sent over  edges in the network, in the worst case.
\end{corollary}

An interesting open problem is to find a distributed deterministic algorithm for computing the maximal node of a Markov chain.

\section{Discussion}
\label{section:conclusion}

We have presented several lower bounds for an interesting problem in distributed computing, where the structure of the communication network is the input to the function to be computed. Our technique is to embed an instance of some two-party version of the problem into a network, and by appealing to the linear array conjecture, lifting the two-party result onto a result concerning the total communication of a distributed algorithm. We discussed that strengthening our results is likely to require a different lifting technique, as the linear array lifting only lets us account for the flow of data across a linear number of edges in , even though there may be  edges present. Finding a better lifting technique appears to be a general problem in proving good lower bounds for distributed computing problems.

In considering worst-case complexity we have neglected the graph-theoretic properties of  -- it would be useful to know how the communication complexity of the problems is altered by restricting  to say, graphs of high conductance (in particular, this would seem to reduce the effectiveness of the lifting technique because a graph of high conductance could not contain two dense graphs separated by a long string of edges, as this would then resemble a `barbell graph').

Some of our lower bound reductions involving greater-than suggest that randomization may help. So far, we have been unable to confirm this but it would certainly seem natural, given the random walk interpretation of the problem.
 
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@rmstyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{coppersmith93}
D.~Coppersmith, P.~Tetali, and P.~Winkler, ``Collisions among random walks on a
  graph,'' \emph{SIAM Journal of Discrete Mathematics}, vol.~6, no.~3, pp.
  364--374, 1993.

\bibitem{page98pagerank}
\BIBentryALTinterwordspacing
L.~Page, S.~Brin, R.~Motwani, and T.~Winograd, ``The pagerank citation ranking:
  Bringing order to the web,'' Stanford Digital Library Technologies Project,
  Tech. Rep., 1998. [Online]. Available:
  \url{citeseer.ist.psu.edu/page98pagerank.html}
\BIBentrySTDinterwordspacing

\bibitem{nisan97communication}
E.~Kushilevitz and N.~Nisan, \emph{Communication Complexity}.\hskip 1em plus
  0.5em minus 0.4em\relax Cambridge University Press, UK, 1997.

\bibitem{237817}
E.~Kushilevitz, N.~Linial, and R.~Ostrovsky, ``The linear-array conjecture in
  communication complexity is false,'' in \emph{Proceedings of the
  twenty-eighth annual ACM symposium on Theory of computing}.\hskip 1em plus
  0.5em minus 0.4em\relax ACM Press, 1996, pp. 1--10.

\bibitem{258622}
M.~Dietzfelbinger, ``The linear-array problem in communication complexity
  resolved,'' in \emph{Proceedings of the twenty-ninth annual ACM symposium on
  Theory of computing}.\hskip 1em plus 0.5em minus 0.4em\relax ACM Press, 1997,
  pp. 373--382.

\bibitem{32978}
P.~Tiwari, ``Lower bounds on communication complexity in distributed computer
  networks,'' \emph{J. ACM}, vol.~34, no.~4, pp. 921--938, 1987.

\bibitem{314705}
M.~Hanckowiak, M.~Karonski, and A.~Panconesi, ``On the distributed complexity
  of computing maximal matchings,'' in \emph{Proceedings of the ninth annual
  ACM-SIAM symposium on Discrete algorithms}.\hskip 1em plus 0.5em minus
  0.4em\relax Society for Industrial and Applied Mathematics, 1998, pp.
  219--225.

\bibitem{48689}
B.~A. Sanders, ``An asynchronous, distributed flow control algorithm for rate
  allocation in computer networks,'' \emph{IEEE Trans. Comput.}, vol.~37,
  no.~7, pp. 779--787, 1988.

\bibitem{wang04vldb}
Y.~Wang and D.~DeWitt, ``Computing pagerank in a distributed internet search
  system,'' in \emph{Proceedings of Very Large Databases (VLDB)}, 2004.

\bibitem{hpdc03pagerank}
K.~Sankaralingam, S.~Sethumadhavan, and J.~Browne, ``Distributed pagerank for
  p2p systems,'' in \emph{Proceedings of the 12th IEEE International Symposium
  on High Performance Distributed Computing (HPDC'03)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2003, pp. 58--.

\bibitem{eigentrust}
S.~D. Kamvar, M.~T. Schlosser, and H.~Garcia-Molina, ``The eigentrust algorithm
  for reputation management in p2p networks,'' in \emph{Proceedings of the
  twelfth international conference on World Wide Web}.\hskip 1em plus 0.5em
  minus 0.4em\relax ACM Press, 2003, pp. 640--651.

\bibitem{kamvar03exploiting}
\BIBentryALTinterwordspacing
S.~Kamvar, T.~Haveliwala, C.~Manning, and G.~Golub, ``Exploiting the block
  structure of the web for computing pagerank,'' Tech. Rep., 2003. [Online].
  Available: \url{citeseer.ist.psu.edu/article/kamvar03exploiting.html}
\BIBentrySTDinterwordspacing

\bibitem{shi03icpp}
S.~M. Shi, J.~Yu, G.~W. Yang, and D.~X. Wang, ``Distributed page ranking in
  structured p2p networks,'' in \emph{Proceedings of the International
  Conference on Parallel Processing}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2003, pp. 179--.

\bibitem{fagin00}
R.~Fagin, A.~Karlin, and J.~K. et~al., ``Random walks with back buttons,'' in
  \emph{Proceedings of the thirty-second annual ACM symposium on Theory of
  computing}.\hskip 1em plus 0.5em minus 0.4em\relax ACM Press, 2000, pp.
  484--493.

\bibitem{fogaras04}
D.~Fogaras and B.~Racz, ``Towards scaling fully personalized pagerank,'' in
  \emph{Proceedings of the third workshop on Algorithms and Models for
  Web-graph (WAW)}, vol. 3243.\hskip 1em plus 0.5em minus 0.4em\relax LNCS,
  2004, pp. 105--117.

\bibitem{322200}
H.~Abelson, ``Lower bounds on information transfer in distributed
  computations,'' \emph{J. ACM}, vol.~27, no.~2, pp. 384--392, 1980.

\bibitem{817}
M.~Gondran, M.~Minoux, and S.~Vajda, \emph{Graphs and algorithms}.\hskip 1em
  plus 0.5em minus 0.4em\relax John Wiley \& Sons, Inc., 1984.

\bibitem{gavoille01distance}
\BIBentryALTinterwordspacing
C.~Gavoille, D.~Peleg, S.~Perennes, and R.~Raz, ``Distance labeling in
  graphs,'' in \emph{Symposium on Discrete Algorithms}, 2001, pp. 210--219.
  [Online]. Available: \url{citeseer.ist.psu.edu/gavoille00distance.html}
\BIBentrySTDinterwordspacing

\bibitem{afek92sparser}
\BIBentryALTinterwordspacing
Y.~Afek and M.~Ricklin, ``Sparser: A paradigm for running distributed
  algorithms,'' in \emph{Workshop on Distributed Algorithms}, 1992, pp. 1--10.
  [Online]. Available: \url{citeseer.ist.psu.edu/afek90sparser.html}
\BIBentrySTDinterwordspacing

\end{thebibliography}

\end{document}
