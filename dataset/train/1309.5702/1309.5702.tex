\documentclass[conference,letterpaper]{ieeeconf} 

\IEEEoverridecommandlockouts                              
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{latexsym,url,subfigure}
\usepackage{comment}
\usepackage{stfloats}




\newtheorem{property}{Property}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{assum}{Assumption}



\newcommand{\hess}{{\rm Hess}}
\newcommand{\grad}{{\rm grad}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\bR}{\bar{R}}

\newcommand{\SO}{SO}
\newcommand{\ve}{\varepsilon}
\newcommand{\so}{so}
\newcommand{\bz}{z}
\newcommand{\se}{se}
\newcommand{\SE}{SE}
\newcommand{\hr}{\hat{r}}
\newcommand{\tz}{\tilde{z}}
\newcommand{\Z}{{\mathcal Z}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\V}{{\mathcal V}}
\newcommand{\E}{{\mathcal E}}
\newcommand{\Q}{{\mathcal Q}}
\newcommand{\M}{{\mathcal M}}
\newcommand{\W}{{\mathcal W}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\cR}{{\cal{R}}}
\newcommand{\cP}{{\cal{P}}}
\newcommand{\cQ}{{\cal{Q}}}
\newcommand{\tr}{{\rm tr}}
\newcommand{\sk}{{\rm sk}}
\newcommand{\sym}{{\rm sym}}
\newcommand{\diag}{{\rm diag}}
\newcommand{\Ad}{{\rm Ad}}
\newcommand{\ea}{{e^{\hat{\xi}\theta_{a}}}}
\newcommand{\eam}{{e^{-\hat{\xi}\theta_{a}}}}

\newcommand{\ebco}{e^{\hat{\bar{\xi}}\bar{\theta}_{co}}}
\newcommand{\eco}{e^{\hat{\xi}\theta_{co}}}
\newcommand{\ee}{e^{\hat{\xi}\theta_{e}}}
\newcommand{\iee}{e^{-\hat{\xi}\theta_{e}}}
\newcommand{\dee}{\dot{e}^{\hat{\xi}\theta_{e}}}
\newcommand{\eab}{{e^{\hat{\xi}\theta_{ab}}}}
\newcommand{\eij}{{e^{\hat{\xi}\theta_{ij}}}}
\newcommand{\ewi}{R_{wi}}
\newcommand{\ewj}{{e^{\hat{\xi}\theta_{wj}}}}
\newcommand{\ewd}{{e^{\hat{\xi}\theta_{wd}}}}
\newcommand{\deab}{{\dot{e}^{\hat{\xi}\theta_{ab}}}}
\newcommand{\deij}{{\dot{e}^{\hat{\xi}\theta_{ij}}}}
\newcommand{\dewi}{{\dot{e}^{\hat{\xi}\theta_{wi}}}}
\newcommand{\dewj}{{\dot{e}^{\hat{\xi}\theta_{wj}}}}
\newcommand{\dewd}{{\dot{e}^{\hat{\xi}\theta_{wd}}}}

\newcommand{\ewib}{{e}^{\hat{{\bar{\xi}}\bar{\theta}_{wi}}}}
\newcommand{\ewjb}{{e}^{\hat{{\bar{\xi}}}\bar{\theta}_{wj}}}
\newcommand{\ecib}{{e}^{\hat{\bar{\xi}}\bar{\theta}_{ci}}}
\newcommand{\ecjb}{{e}^{\hat{\bar{\xi}}\bar{\theta}_{cj}}}
\newcommand{\eijb}{{e}^{\hat{\bar{\xi}}\bar{\theta}_{ij}}}

\newcommand{\eeei}{{e^{\hat{\xi}\theta_{eei}}}}
\newcommand{\eeeij}{{e^{\hat{\xi}\theta_{eeij}}}}
\newcommand{\eee}{{e^{\hat{\xi}\theta_{ee}}}}
\newcommand{\ewim}{{e^{-\hat{\xi}\theta_{wi}}}}
\newcommand{\ewjm}{{e^{-\hat{\xi}\theta_{wj}}}}
\newcommand{\eabm}{{e^{-\hat{\xi}\theta_{ab}}}}
\newcommand{\ewdm}{{e^{-\hat{\xi}\theta_{wd}}}}

\newcommand{\ewimb}{{{e}^{-\hat{\bar{\xi}}\bar{\theta}_{wi}}}}
\newcommand{\ewjmb}{{e}^{-\hat{\bar{\xi}}\bar{\theta}_{wj}}}
\newcommand{\eijmb}{{e}^{-\hat{\bar{\xi}}\bar{\theta}_{ij}}}

\newcommand{\eijm}{{e^{-\hat{{\xi}}{\theta}_{ij}}}}
\newcommand{\eeeim}{{e}^{-\hat{\xi}\theta_{eei}}}
\newcommand{\eeem}{{{e}^{-\hat{\xi}\theta_{ee}}}}

\newcommand{\pwii}{{p_{wi}^i}}
\newcommand{\pwji}{{p_{wj}^i}}
\newcommand{\pwiw}{{p_{wi}^w}}
\newcommand{\pwjw}{{p_{wj}^w}}
\newcommand{\dpwii}{{\dot{p}_{wi}^i}}
\newcommand{\dpwji}{{\dot{p}_{wj}^i}}
\newcommand{\dpwiw}{{\dot{p}_{wi}^w}}
\newcommand{\dpwjw}{{\dot{p}_{wj}^w}}
\newcommand{\pwiiT}{{p_{wi}^{iT}}}
\newcommand{\pwjiT}{{p_{wj}^{iT}}}
\newcommand{\pwiwT}{{p_{wi}^{wT}}}
\newcommand{\pwjwT}{{p_{wj}^{wT}}}
\renewcommand{\L}{{\cal L}}

\newcommand{\pwiib}{{\bar{p}_{wi}^i}}
\newcommand{\pwjib}{{\bar{p}_{wj}^i}}
\newcommand{\pwiwb}{{\bar{p}_{wi}^w}}
\newcommand{\pwjwb}{{\bar{p}_{wj}^w}}
\newcommand{\dpwiib}{\dot{{\bar{p}}_{wi}^i}}
\newcommand{\dpwjib}{\dot{{\bar{p}}_{wj}^i}}
\newcommand{\dpwiwb}{\dot{{\bar{p}}_{wi}^w}}
\newcommand{\dpwjwb}{\dot{{\bar{p}}_{wj}^w}}
\newcommand{\pwiibT}{{\bar{p}_{wi}^{iT}}}
\newcommand{\pwjibT}{{\bar{p}_{wj}^{iT}}}
\newcommand{\pwiwbT}{{\bar{p}_{wi}^{wT}}}
\newcommand{\pwjwbT}{{\bar{p}_{wj}^{wT}}}
\newcommand{\TSO}{T_{\ewi}SO(3)}
\newcommand{\peei}{{p_{eei}}}

\newcommand{\pee}{{p_{ee}}}

\newcommand{\piji}{{p_{ij}^i}}
\newcommand{\pijw}{{p_{ij}^w}}
\newcommand{\pijib}{{\bar{p}_{ij}^i}}
\newcommand{\dpiji}{{\dot{p}_{ij}^i}}
\newcommand{\dpijw}{{\dot{p}_{ij}^w}}
\newcommand{\dpijib}{{\dot{\bar{p}}_{ij}^i}}
\newcommand{\pijiT}{{p_{ij}^{iT}}}
\newcommand{\pijwT}{{p_{ij}^{wT}}}
\newcommand{\pijibT}{{\bar{p}_{ij}^{iT}}}

\newcommand{\gij}{{g_{ij}}}
\newcommand{\gijb}{{\bar{g}_{ij}}}
\newcommand{\gijm}{{g_{ij}^{-1}}}
\newcommand{\gijmb}{{\bar{g}_{ij}}^{-1}}

\newcommand{\gwi}{{g_{wi}}}
\newcommand{\gwib}{{\bar{g}_{wi}}}
\newcommand{\gwim}{{g_{wi}^{-1}}}
\newcommand{\gwimb}{{\bar{g}_{wi}^{-1}}}
\newcommand{\om}{\omega}
\renewcommand{\hom}{\hat{\omega}}


\title{3-D Visual Coverage Based on
Gradient Descent Techniques on Matrix Manifold
and Its Application to Moving Objects Monitoring}

\author{Takeshi Hatanaka, Riku Funada and Masayuki Fujita
\thanks{Takeshi Hatanaka, Riku Funada and Masayuki Fujita are with the Department of Mechanical and 
Control Engineering, Tokyo Institute of Technology, Tokyo 152-8552, JAPAN
        {\tt\small {hatanaka,funada,fujita}@ctrl.titech.ac.jp}}}



\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
This paper investigates coverage control 
for visual sensor networks based on gradient descent
techniques on matrix manifolds.
We consider the scenario 
that networked vision sensors with controllable orientations
are distributed over 3-D space to 
monitor 2-D environment.
Then, the decision variable must be constrained
on the Lie group .
The contribution of this paper is two folds.
The first one is technical, namely we formulate the coverage problem as an optimization
problem on  without introducing local parameterization like
Eular angles and directly apply the gradient descent algorithm on 
the manifold.
The second technological contribution is to 
present not only the coverage control scheme but
also the density estimation process including image processing
and curve fitting while exemplifying its effectiveness 
through simulation of moving objects monitoring.
\end{abstract}




\section{Introduction}



Aspirations for safety and security of human lives against 
crimes and natural disasters 
motivate us to establish smart monitoring systems 
to monitor surrounding environment.
In this regard, vision sensors are expected as powerful 
sensing components since they
provide rich information about the outer world. 
Indeed, visual monitoring systems have been already
commoditized and are working in practice.
Typically, in the current systems, 
various decision-making and situation awareness processes 
are conducted at a monitoring center by human operator(s),
and partial distributed computing at each sensor is, if at all, done 
independently of the other sensors.
However, as the image stream increases, 
it is desired to distribute the entire process
to each sensor while achieving total optimization
through cooperation among sensors.



Distributed processing over the visual sensor networks 
is actively studied in recent years motivated by a variety of
application scenarios \cite{amit2}--\cite{HWF_CDC13}.
Among them, several papers address optimal monitoring of the environment
assuming mobility of the vision sensors
\cite{EYE}--\cite{HWF_CDC13},
where it is required for the network to ensure
the best view of a changing environment \cite{EYE}.
The problem is related to coverage control \cite{CL_EJC05}--\cite{BCM_ES05},
whose objective is to deploy mobile sensors efficiently
in a distributed fashion.
A typical approach to coverage control is to employ
the gradient descent algorithm
for an appropriately designed aggregate objective function.
The objective function is usually formulated by
integrating the product 
of a sensing performance function of a point
and a density function indicating the 
relative importance of the point.
The approach is also applied to visual coverage 
in \cite{EYE}--\cite{GTF_CDC08}.
The state of the art of coverage control is compactly summarized in \cite{EYE},
and a survey of related works in the computer vision society is found in \cite{survey}.



In this paper, we consider a visual coverage problem 
under the situation where vision sensors with controllable 
orientations are distributed over the 3-D space
to monitor 2-D environment.
In the case, the control variables i.e. the rotation matrices
must be constrained on the Lie group ,
which distinguishes the present paper from the works
on 2-D coverage \cite{cortes}--\cite{ZM_SIAM13}.
On the other hand, \cite{EYE,DSMFC_SP12,HWF_CDC13} consider
situations similar to this paper.
\cite{DSMFC_SP12,HWF_CDC13} take game theoretic approaches
which allow the network to achieve globally optimal coverage with
high probability but instead the convergence speed tends
to be slower than the standard gradient descent approach.
In contrast, \cite{EYE} employs the gradient approach
by introducing a local parameterization of
the rotation matrix and regarding the problem as optimization on a vector space.



This paper approaches the problem differently from \cite{EYE}.
We directly formulate the problem as optimization on 
and apply the gradient descent algorithm on matrix manifolds \cite{AMS_BK}.
This approach will be shown to allow one to parametrize the 
control law for a variety of underactuations imposed by the hardware constraints.
This paper also addresses density estimation from acquired data,
which is investigated in \cite{IJRR} for 2-D coverage.
However, we need to take account of the following 
characteristics of vision sensors:
(i) the sensing process inherently includes
projection of 3-D world onto 2-D image, and
(ii) explicit physical data is not provided.
To reflect (i), we incorporate the projection into the 
optimization problem on the embedding manifold of .
The issue (ii) is addressed technologically, where 
we present the entire process including image processing 
and curve fitting techniques.
Finally, we demonstrate the utility of the present coverage 
control strategy through simulation of moving objects monitoring.




\subsection*{Preliminary: Gradient on Riemannian Manifold}








Let us consider a Riemannian manifold  whose
tangent space at  is denoted by ,
and the corresponding Riemannian metric, an smooth inner product, 
defined over  is denoted by .
Now, we introduce a smooth scalar field  defined over the manifold ,
and the derivative of  at an element  in the direction ,
denoted by .
We see from Definition 3.5.1 and (3.15) of \cite{AMS_BK}
that the derivative  is defined by

where  is a smooth curve
such that .
In particular, when  is a linear manifold with ,
 the derivative  is equal to the classical directional derivative




Now, the gradient of 
is defined as follows.
\begin{definition}\cite{AMS_BK}
\label{def:grad_M}
Given a smooth scalar field  defined over a Riemannian manifold ,
the gradient  of  at , denoted by , is defined as the unique
element of  satisfying

\end{definition}
Suppose now that  is a Riemannian 
submanifold of a Riemannian manifold ,
namely  is a subspace of  and
they share a common Riemannian metric.
In addition, the orthogonal projection of an element of 
onto  is denoted by .
Then, the following remarkable lemma holds true.
\begin{lemma}\cite{AMS_BK}
\label{lem:grad_proj}
Let  be a scalar field defined over 
such that the function  defined on  
is a restriction of .
Then, the gradient of  satisfies the equation

\end{lemma}













\begin{figure}[t]
\begin{center}
\includegraphics[width=7cm]{fig1.eps}
\caption{Targeted scenario.}
\label{fig:scenario}
\end{center}
\end{figure}







\section{Targeted Scenario}


\subsection{Vision Sensors and Environment}





We consider the situation illustrated in Fig. \ref{fig:scenario} where
 vision sensors  are located in 3-D Euclidean space.
Let the fixed world frame be denoted by 
 and the body fixed frame of sensor 
by .
We also denote the position vector of the origin of 
relative to  by ,
and the rotation matrix of  relative to 
by .
Then, the pair ,
called {\it pose}, represents
the configuration of sensor .
In this paper, each sensor's position  is assumed to be fixed,
and sensors can control only their orientations .
In addition, we suppose that sensors are localized and calibrated {\it a priori}
and  is available for control.






We use the notation  to describe not only the pose
but also a coordinate transformation operator similarly to \cite{MLS_BK}.
Take two frames  and . 
Let the pose of the frame  relative to  be denoted by ,
and the coordinates of a point relative to  by .
Then, the coordinates  of the point relative to  are 
given as








Let us next define the region to be monitored by a group of sensors .
In this paper, we assume that 
the region is a subset of a 2-D plane (Fig. \ref{fig:scenario}), where the 
2-D plane is called the environment and the subset to be monitored
is called the mission space.
Let the set of coordinates of all points in the environment and the mission space 
relative to  are respectively 
denoted by  and .
Just for simplicity, we suppose that 
the world frame  is attached so that its
-plane is parallel to the environment
(Fig. \ref{fig:scenario}).
Then, the set  is formulated as 

with some constant ,
where  is an -th standard basis.
Suppose that a metric ,
called a density function, indicating the relative importance of 
every point  is defined over .
In this paper, the function  is assumed to be small if point  is important
and to satisfy  with a constant
 such that .





\subsection{Geometry}






A vision sensor has an image plane containing the sensing array, 
whose elements, called pixels, 
provide the numbers reflecting 
the amount of light incident.
We assume that the image plane is a rectangle as illustrated in Fig. \ref{fig:image_plane1}.
The set of position vectors of all points on the image plane relative to the sensor frame 
is denoted by . 
Now, the axes of the sensor frame  is assumed to be selected 
so that its -plane is parallel to the image plane
and -axis perpendicular to the image plane passes through 
the focal center of the lens.
Then, the third element of any point in the set  
must be equal to the focal length
.


\begin{figure}[t]
\begin{center}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm]{fig2.eps}
\caption{Image plane and pixel.}
\label{fig:image_plane1}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm]{fig3.eps}
\caption{Vertices of image plane.}
\label{fig:image_plane2}
\end{center}
\end{minipage}
\end{center}
\end{figure}






We next denote the set of pixels of sensor  by  and
the position vector of the center of the -th pixel on the image plane of sensor 
relative to  by .
Since  in  and  deffer,
we may need to use the notation like  but we omit the subscript
to reduce notational complexity.
In addition, the positions of its vertices relative to 
 are denoted by 
(Fig. \ref{fig:image_plane2}).



When a point on the environment with coordinates  relative to 
is captured by a sensor  with ,
the point is projected onto the image plane as illustrated in
Fig. \ref{fig:proj1}.
If the coordinates of the projected point 
are denoted by ,
it is well known that the projection is formulated as

It is not difficult to show that the inverse map  
of the map  (Fig. \ref{fig:proj1}) from 
to  is given by

Note that, while  is independent of ,
the map  depends on  and hence we describe  as 
.



\begin{figure}[t]
\begin{center}
\includegraphics[width=6.5cm]{fig4.eps}
\caption{Projections  and .}
\label{fig:proj1}
\end{center}
\end{figure}



Using the map , we denote by  the set of coordinates
of the field of view (FOV) of each sensor  relative to , which is also a polytope.
Its -polytope representation is trivial, namely it is given by
the convex hull of the four points with coordinates

relative to 
(Fig. \ref{fig:FOV2}).
The -polytope representation is also 
computed efficiently as follows.



Suppose now that -th side line segment () specifying 
the boundary of the image plane connects the vertices  and 
without loss of generality.
Then, the line projected onto the environment is also a 
line segment whose vertices have coordinates 
 and 
relative to , and hence the line is formulated as

where the matrix  is derived as

from the fact that  and 
are on the line.
Since the coordinates 
for any interior  of  must be inside the FOV,
a half space specifying the FOV is described by the inequality
 with

In the same way, we can find the pair  for all .
Stacking them allows one to formulate the FOV as











\section{Coverage for a Single Sensor}


In this section, we consider a simple case with 
.











\subsection{Objective Function}



Let us first define the objective function
to be {\it minimized} by sensor .
In this paper, we basically take the concept of coverage control \cite{CL_EJC05,BCM_BK09}, 
where the objective function is defined by 
a sensing performance function and a density function at 
a point .
Note however that
we accumulate the function only at the center of the pixels
projected onto the environment  in order to reflect
the discretized nature of the vision sensors.
In the sequel, the sensing performance function and the density function
at  are denoted by
 and 
, respectively.









Let us first define a function  
providing the coordinates in  of the point on 
which is captured by -th pixel as

Then, the objective function takes the form of

where  is a weighting coefficient.
If we impose a large  on the pixel at around the center of the image,
the sensor tends to capture the important area at around the image center.
If we need to accelerate computation, 
replacing  in (\ref{eqn:obj_single}) by its subset
is an option.
In order to ensure preciseness, we need to introduce an extended function
allowing , but we will not mention it
since it can be easily avoided by choosing
 appropriately.



\begin{figure}[t]
\begin{center}
\includegraphics[width=6.5cm]{fig5.eps}
\caption{Field of view }
\label{fig:FOV2}
\end{center}
\end{figure}



Similarly to \cite{BCM_BK09}, we let
the performance function  depend only on the distance .
Remark however that, differently from \cite{BCM_BK09}, 
the third element of  is not controllable
since the sensor is fixed.
This may cause a problem that penalty of seeing distant area
does not work in the case that the element is large enough.
However, the element is not ignorable since it reflects heterogeneous 
characteristics of vision sensors in the multi-sensor case.
We thus use the weighting distance as

with , where  is introduced 
since the distance is scaled by the focal length. 
Suppose that  is set as .
Then, a large  imposes a heavy penalty on viewing distant area
and a small  a light penalty on it.
In particular, when  for some ,
(\ref{eqn:perf1}) is rewritten as



Once the density function  is given,
the goal is reduced to minimization of
(\ref{eqn:obj_single}) with (\ref{eqn:perf2})
under the restriction of .
In order to solve the problem,
this paper takes the gradient descent approach
which is a standard approach to coverage control.
For this purpose,
it is convenient to define an extension 
such that  if .
We first extend the domain
of  in (\ref{eqn:q_wl}) from 
to  as

Then, the vector  is not always 
on the environment when  but
the function  in (\ref{eqn:perf1}) is well-defined
even if the domain is altered from  to . 
We thus denote the function with the domain  by ,
and define the composite function 





\begin{figure}[t]
\begin{center}
\includegraphics[width=6cm]{fig6.eps}
\caption{Coordinates of  relative to  and .}
\label{fig:proj4}
\end{center}
\end{figure}




We next focus on the term  in
(\ref{eqn:obj_single}) and expand the domain of the composite function
from  to .
Here, since  is not always on ,
we need to design  such that
 if .
In this paper, we assign to a point  
the density of a point 

where the operations are illustrated in Fig. \ref{fig:proj4}.
Accordingly, the density function is defined by

Remark that, differently from , the function 
is not naturally extended and the selection of  is not unique. 
The motivation to choose (\ref{eqn:perf6}) will be clear in the 
next subsection.



Consequently, we define the extended objective function

from  to  by using
(\ref{eqn:perf5}) and (\ref{eqn:perf6}).
Let us finally emphasize that  holds for any .



\subsection{Density Estimation for Moving Objects Monitoring}



In the gradient descent approach,
we update the rotation 
in the direction of 
at each time .
This subsection assumes that
the density  is not given {\it a priori}
and that  needs to be estimated from acquired vision data
as investigated in \cite{EYE,IJRR}.







Let us first consider an ideal situation such that the density function
is exactly projected onto the image plane, namely

holds with respect to the density  over the image plane.
Then, the density function value  is available at
any point in the FOV.
We next consider a point 
which does not always lie on . 
Then, the value of  is also given by 
the same function as (\ref{eqn:psi_phi2}) since

Ensuring the equality is the reason for choosing 
(\ref{eqn:perf6}).



\begin{figure}
\begin{center}
\includegraphics[width=7.5cm]{./fig7.eps}
\caption{A snapshot and computed optical flows.}
\label{fig:snapshot}
\end{center}
\end{figure}















We next consider estimation of the density  on the image
since assuming (\ref{eqn:psi_phi2}) is unrealistic. 
Rich literature has been devoted to the
information extraction from the raw vision data,
and a variety of algorithms are currently available 
even without expert knowledge \cite{MATLAB1}.
For example, it is possible to detect and localize in the image plane
specific objects like cars or human faces, and even abstract targets such as everything moving 
or some environmental changes.



The present coverage scheme is indeed applicable to any scenario
such that a nonnegative number  reflecting 
its own importance is assigned to each pixel 
after conducting some image processing.
However, we mainly focus on a specific scenario of
monitoring moving objects on the mission space.
Suppose that a sensor captures a human walking from 
left to right in the image as in Fig. \ref{fig:snapshot}.
Then, a way to localize such moving objects is to
compute optical flows from consecutive images as in Fig. \ref{fig:snapshot},
where the flows are depicted by yellow lines.
We also let the data  be the norm of the flow vector
at each pixel.
Then, the plots of  over the image plane 
are illustrated by green dots in  Fig. \ref{fig:mix_gauss1}.

\begin{figure}
\begin{center}
\begin{minipage}{4.2cm}
{\includegraphics[width=4.2cm]{./fig8.eps}}
\caption{Plots of .}
\label{fig:mix_gauss1}
\end{minipage}
\begin{minipage}{4.2cm}
{\includegraphics[width=4.2cm]{./fig9.eps}}
\caption{Estimated density.}
\label{fig:mix_gauss2}
\end{minipage}
\end{center}
\end{figure}







We next fit the data of  by a continuous function
defined over  and use the function as .
Such algorithms are also available
even in real time \cite{MATLAB2}.
Similarly to \cite{IJRR}, we employ the mixed Gaussian function
known to approximate a variety of functions with excellent precision
by increasing the number of Gaussian functions,
and widely used in data mining, pattern recognition,
machine learning and statistical analysis.
Fig. \ref{fig:mix_gauss2} shows the Gaussian function with  computed so as to fit the data
in Fig. \ref{fig:mix_gauss1}.
Of course, using a larger  achieves a better approximation
as shown in Fig. \ref{fig:mix_gauss_various}.



As a result, we obtain a function in the form of
 
over the 2-D image plane coordinates .
Note that (\ref{eqn:mix_Gauss_image2}) is large when
 captures an important point, which is opposite to 
the density function.
Thus, we define the function
 
where  is a positive scalar guaranteeing 
for all .
It is also convenient to
define  for all
3-D vectors  on the image plane as
 







\begin{figure}
\begin{center}
\begin{minipage}{4cm}
{\includegraphics[width=4cm,height=2.8cm]{./fig10.eps}}
\end{minipage}
\hspace{0.2cm}
\begin{minipage}{4cm}
{\includegraphics[width=4cm,height=2.8cm]{./fig11.eps}}
\end{minipage}
\caption{Estimated densities for (left) and (right).}
\label{fig:mix_gauss_various}
\end{center}
\end{figure}




\subsection{Gradient Computation}



\subsubsection*{Full 3-D Rotational Motion}



Here, we will derive the gradient , given a rotation  and  in (\ref{eqn:mix_Gauss}).
It is widely known that  is formulated as
,
where  is the set of all skew symmetric matrices in .
We also define the operator (wedge) from  to 
such that  for the cross product .
The rotational group  is known to be a submanifold of
a Riemannian manifold  with 

and the Riemannian metric 

\cite{AMS_BK}.
It is also known that
the orthogonal projection  of matrix
 onto  
in terms of the Riemannian metric induced by (\ref{eqn:Riemannian})
is given by

See Subsection 3.6.1 of \cite{AMS_BK} for more details.











Now, we have the following theorem, where we use the notation

and .



\begin{theorem}
Suppose that the objective function  is formulated by (\ref{eqn:obj_single_fict})
with (\ref{eqn:perf5}), (\ref{eqn:perf6}) and (\ref{eqn:mix_Gauss}).
Then, the gradient  
is given by 

where

\end{theorem}


\begin{proof}
See Appendix \ref{app:1}.
\end{proof}



Namely, just running the dynamics 

leads  to the set of critical points of .
However, in practice, 
the vision data is usually obtained at discrete time instants
and hence 
we approximate the continuous-time algorithm (\ref{eqn:grad_descent1}) by

See \cite{AMS_BK} for the details on the selection of .



\subsubsection*{Rotational Motion with Underactuations}



\begin{figure}[t]
\begin{center}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm]{fig12.eps}
\caption{Pan motion.}
\label{fig:Pan}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm]{fig13.eps}
\caption{Tilt motion.}
\label{fig:Tilt}
\end{center}
\end{minipage}
\end{center}
\end{figure}




In the above discussion, we assume that the sensor can take full 3-D
rotational motion.
However, the motion of many commoditized cameras is restricted by
the actuator configurations.
Hereafter, we suppose that the sensor can be rotated 
around two axes  () and  (),
where these vectors are defined in  and assumed to be
linearly independent of each other.
These axes may depend on the rotation matrix .
For example, in the case of
Pan-Tilt (PT) cameras in Figs. \ref{fig:Pan} and \ref{fig:Tilt}, which are
typical commoditized cameras, 
the axis of the pan motion (Fig. \ref{fig:Pan}) 
is fixed relative to , 
while that of the tilt motion (Fig. \ref{fig:Tilt}) is fixed
relative to the sensor frame .
Then, only one of the two axes depends on .
Note that even when there is only one axis around which the sensor can be rotated, 
the subsequent discussions are valid just letting .







Let us denote a normalized vector  ()
orthogonal to the -plane.
Then, the three vectors ,  and 
 span .
Thus, any element  of  can be represented in the form of
.
Now, we define a {\it distribution}  \cite{MLS_BK} assigning
 to the subspace 

whose dimension is .
The distribution  is clearly regular and hence 
induces a submanifold  of  \cite{MLS_BK},
called integral manifold, such that its tangent space 
 at  is equal to (\ref{eqn:submanifold}).
The manifold  specifies 
orientations which the camera can take.



Since  is a submanifold of ,
a strategy similar to Theorem 1 is available and
we have the following corollary.




\begin{corollary}
Suppose that the objective function  is formulated by (\ref{eqn:obj_single_fict})
with (\ref{eqn:perf5}), (\ref{eqn:perf6}) and (\ref{eqn:mix_Gauss}).
Then, the gradient  
is given by 

where the orthogonal projection  of
 to  is defined by

with 
, where  if  and
 if .
\end{corollary}




We see from this corollary  
that the gradient
 on  
is utilized as it is and we need only to project it
through (\ref{eqn:proj4}).
Also, the projection (\ref{eqn:proj4}) is successfully 
parameterized by the vectors  and .







\section{Coverage for Multiple Sensors}



\begin{figure}[t]
\begin{center}
\includegraphics[width=7cm]{fig14.eps}
\caption{Overlaps of FOVs (green region).}
\label{fig:overlap}
\end{center}
\end{figure}




In this section, we extend the result of the previous section to the
multi-sensor case.
The difference from the single sensor case stems from the overlaps
of the FOVs with the other sensors as illustrated in Fig. \ref{fig:overlap}.
\cite{EYE,CL_EJC05,BCM_ES05} present sensing performance functions
taking account of the overlaps and their gradient decent laws.
However, in this paper, we present another simpler scheme to manage the overlap.




Let us first define the set of sensors capturing a point  within the FOV as

where .
We also suppose that, when  has multiple elements for some ,
only the data of the sensor with the minimal 
sensing performance (\ref{eqn:perf1})
among sensors in  is employed in higher-level decisions and recognitions.
This motivates us to partition  into the two region 

Then, what pixel  captures a point in 
is identified with what it captures a point outside of ,
whose cost is set as  in the previous section,
in the sense that both of the data are not useful at all. 
This is reflected by assigning  to the pixels 
with
.


Accordingly, we formulate the  function
to be minimized by  as  with

Remark that (\ref{eqn:obj_multi}) differs from
(\ref{eqn:obj_single}) only in the set .



Strictly speaking, to compute the gradient of (\ref{eqn:obj_multi}),
we need to expand  from  to 
. 
For this purpose, it is sufficient to define 
 from  to a subset of .
For example, an option is to define 
an extension

of (\ref{eqn:arxiv1}) similarly to (\ref{eqn:arxiv2}), and
to let  be the convex full of these points.
However, at the time instants computing the gradient with ,
the extension  for a sufficiently small
perturbation  is equivalent to
the original set 
irrespective of the selection of 
except for the pathological case when a pixel is located on the boundary
of .
Namely, ignoring such pathological cases which do not happen
almost surely for (\ref{eqn:grad_descent2}),
the gradient can be computed by using 
the set  instead of its extension.
Hence, the gradient is simply given as Theorem 1
by just replacing  by .
Note that the curve fitting process is run without 
taking account of whether  or not, 
and  is assigned
to  
at the formulation of  as in (\ref{eqn:mix_Gauss}).
This is because letting

at the curve fitting stage would degrade the density estimation
accuracy at around the boundary of . 








The remaining issue is efficient computation of the set .
Hereafter, we assume that each sensor acquires ,
i.e.  and  for all ,
and its index 
through (all-to-all) communication or with the help of a centralized computer.
The computation under the limited communication will be
mentioned at the end of this section.
In addition, we suppose that every sensor stores the set

for all  which can be computed off-line
since the sensor positions are fixed.






\begin{figure}[t]
\begin{center}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig15.eps}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig16.eps}
\end{center}
\end{minipage}
\caption{Images at s (left) and s (right).}
\label{fig:initial_image}
\bigskip

\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig17.eps}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig18.eps}
\end{center}
\end{minipage}
\caption{Optical flows of images at s (left) and s (right).}
\label{fig:of_image}
\bigskip

\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig19.eps}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig20.eps}
\end{center}
\end{minipage}
\caption{Approximation of data at s (left) and s (right).}
\label{fig:curve_fit}
\end{center}
\end{figure}



Then, the set  is computed as

in polynomial time with respect to .
Namely, checking 
for all 
provides  .













The computation process including image processing, 
curve fitting and gradient computation
is successfully distributed to each sensor but the resulting FOVs need to be shared
among all sensors to compute .
A way to implement the present scheme under 
limited communication
is to restrict the FOV of each sensor so that the FOV can overlap
with limited number of FOVs of the other sensors.
Such constraints on the FOVs are easily imposed by adding
an artificial potential to the objective function but 
 we leave the issue as a future work due to the page constraints.










\section{Simulation of Moving Objects Monitoring}



In this section, we demonstrate the utility of the
present approach through simulation using 4 cameras with mm .
Here, we suppose that the view of the environment from  with m and focal length 
mm is given as in 
Fig. \ref{fig:initial_image},
and that the mission space  is equal to the FOV corresponding to the image.
Since the codes of simulating the image acquisition and processing are never used in experiments,
we simplify the process as follows, and demonstrate only the present coverage control scheme 
with the curve fitting process.
Before running the simulation, we compute the optical flows
for the images of Fig. \ref{fig:initial_image} as in Fig. \ref{fig:of_image},
and also fitting functions of the data as in Fig. \ref{fig:curve_fit}.
The resulting data is uploaded at
\url{http://www.fl.ctrl.titech.ac.jp/paper/2014/data.wmv}.
Then, we segment the image by the superlevel set
of the function using a threshold ,
and assign a boolean variable  to  if 
 is inside of the set and assign  otherwise.
The experimental system is now under construction, and the
experimental verification of the total process
will be conducted in a future work.
Note however that it is at least confirmed that 
the skipped image acquisition and processing 
can be implemented within several milliseconds in a real vision system.



\begin{figure}[t]
\begin{center}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig21.eps}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig22.eps}
\end{center}
\end{minipage}
\caption{Initial FOVs (left) and final FOVs (right).}
\label{fig:static}
\medskip


\includegraphics[width=4cm]{fig27.eps}
\caption{Evolution of objective function .}
\label{fig:obj}
\end{center}
\end{figure}












Let the position vectors of cameras be selected as 
and the length of each side of the image plane be mm and mm.
The other elements of  are set as illustrated by the mark  in Fig. \ref{fig:static}.
The parameters in  is set as 
, , 
 
and .
The curve fitting process
is run with  and the gradient is computed
by evaluating the objective function not at all points in 
but at 121 points extracted from .
In order to confirm convergence of the orientations,
we first fix the image as in Fig. \ref{fig:static}
and run the present algorithm from the initial condition .
Then, the evolution of the function  is illustrated in Fig. \ref{fig:obj},
where we compute the value using not the individually estimated density but
the data as in Fig. \ref{fig:curve_fit}.
We see from the figure that the function  is decreasing
through the update process and eventually reaches a stationary point.
The final configuration is depicted in the right figure of Fig. \ref{fig:static}.
We next start to play the above movie and 
check adaptability to environmental changes,
where the orientations are assumed to be updated 
at each frame.
Then, the evolution of FOVs are shown in
\url{http://www.fl.ctrl.titech.ac.jp/paper/2014/sim.wmv}
whose snapshots at times  are depicted in Fig. \ref{fig:snaps}.
We see from the movie and figures that the cameras adjust their rotations
so as to capture moving humans.
The above results show the effectiveness of the present approach.





\section{Conclusions}


In this paper, we have investigated visual coverage control 
where the vision sensors
are assumed to be distributed over the 3-D space to 
monitor the 2-D environment
and to be able to control their orientations.
We first have formulated the problem as an optimization
problem on .
Then, in order to solve the problem, we have presented
the entire process including not only the gradient computation
but also image processing and curve fitting, which are 
required to estimate the density function
from the acquired vision data.
Finally, we have demonstrated the effectiveness of the approach
through simulation of moving objects monitoring.

\appendix


\section{Proof of Theorem 1}
\label{app:1}



For notational simplicity, we describe  by  in the sequel.
Substituting (\ref{eqn:perf5}), (\ref{eqn:perf6}) and (\ref{eqn:mix_Gauss})
into (\ref{eqn:obj_single_fict}), the objective function to be minimized
is formulated as




\begin{figure}[t]
\begin{center}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig23.eps}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig24.eps}
\end{center}
\end{minipage}
\medskip

\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig25.eps}
\end{center}
\end{minipage}
\hspace{.2cm}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=4cm,height=2.5cm]{fig26.eps}
\end{center}
\end{minipage}
\caption{Snapshots at s (top-left), s (top-right), s (bottom-left) and s (bottom-right).}
\label{fig:snaps}
\end{center}
\end{figure}




From Lemma 1 and the fact that 
is a submanifold of ,
we first compute the gradient .
From Definition 1 and (\ref{eqn:derivative_lin_M}),
we need to compute the directional derivative
.
From linearity of the directional derivative operator ,
it is sufficient to derive 
 and .



We first consider .
By calculation, we have

Hence, 
is given by





Let us next consider  
. We first have the equations

Hence, we also have

We also obtain

where , 
 and 
are introduced for notational simplicity.
Using ,
we can decompose low and high order terms in 
as

(\ref{eqn:app4}) is also simplified as

where

Substituting (\ref{eqn:app4}) and (\ref{eqn:app6}) 
into (\ref{eqn:app1}) yields




Let us now compute .
Substituting (\ref{eqn:app7}) into the definition of the directional derivative (\ref{eqn:derivative_lin_M}),
i.e.

we have

By calculation, the derivative  is given by

and hence

Substituting (\ref{eqn:app10}) and definitions of  and 
into (\ref{eqn:app9})
yields

Note that  is constant and 
 is independent of the matrix .



From (\ref{eqn:obj_single_fict3}), (\ref{eqn:modify2}) and (\ref{eqn:app13}),
we obtain

From Definition \ref{def:grad_M},
we have
.
Combining it with Lemma 1 and
(\ref{eqn:proj_so(3)}) completes the proof.


\begin{thebibliography}{99}




\bibitem{amit2}
B. Song, C. Ding, A. Kamal, J. A. Farrell and A. Roy-Chowdhury,
``Distributed camera networks: integrated sensing and analysis for wide
area scene understanding,'' {\it IEEE Signal Processing Magazine}, Vol. 28,
No. 3, pp. 20--31, 2011.

\bibitem{TAC13}
T. Hatanaka and M. Fujita,
``Cooperative Estimation of Averaged 3D Moving Target Object Poses via Networked Visual Motion Observers,'' 
{\it IEEE Trans. Automatic Control}, Vol. 58, No. 3, pp. 623--638, 2013.



\bibitem{TV_SP11}
R. Tron and R. Vidal,
``Challenges faced in deployment of camera sensor networks,''
{\it IEEE Signal Processing Magazine}, Vol. 28, No. 3, pp. 32--45, 2011.


\bibitem{EYE}
B. M. Schwager, B. J. Julian, M. Angermann and D. Rus,
``Eyes in the Sky: Decentralized
Control for the Deployment of Robotic Camera Networks,''
{\it Proc. of the IEEE}, Vol. 99, No. 9, pp. 1541--1641, 2011.


\bibitem{cortes}
K. Laventall and J. Cortes,
``Coverage control by robotic networks with limited-range anisotropic sensory,''
{\it Proc. of 2008 American Control Conf.}, pp. 2666--2671, 2008.

\bibitem{GTF_CDC08}
A. Gusrialdi, T. Hatanaka and M. Fujita,
``Coverage control for mobile networks with limited-range anisotropic sensors,''
{\it Proc. of 47th IEEE Conf. on Decision and Control}, pp. 4263--4268, 2008.

\bibitem{PhD}
A. Ganguli, {\it Motion coordination for mobile robotic networks with visibility sensors}, 
{\it PhD thesis, Electrical and Computer Engineering Department, University of Illinois at Urbana-Champaign}, 2007.


\bibitem{ZM_SIAM13}
M. Zhu and S. Martinez,
``Distributed coverage games for energy-aware mobile sensor networks,''
{\it SIAM J. Control and Optimization}, Vol. 51, No. 1, pp. 1--27, 2013.


\bibitem{DSMFC_SP12}
C. Ding, B. Song, A. Morye, J. A. Farrell and A. K. Roy-Chowdhury,
``Collaborative sensing in a distributed PT camera network,''
{\it IEEE Trans. Image Processing}, Vol. 21, No. 7, pp. 3282--3295, 2012.



\bibitem{HWF_CDC13}
T. Hatanaka, Y. Wasa and M. Fujita
Game Theoretic Cooperative Control of PTZ Visual Sensor Networks for Environmental Change Monitoring
{\it Proc. of 52nd IEEE Conf. on Decision and Control}, to appear, 2013





\bibitem{CL_EJC05}
C. G. Cassandras and W. Li, 
``Sensor networks and cooperative control,''
{\it Euro. J. Control}, Vol. 11, No. 4--5, pp. 436--463, 2005.


\bibitem{BCM_BK09}
S. Martinez, J. Cortes, and F. Bullo,
``Motion coordination with distributed information,'' 
{\it IEEE Control Systems Magazine}, Vol. 27, No. 4, pp. 75--88, 2007.

\bibitem{BCM_ES05}
J. Cortes, S. Martinez, and F. Bullo, 
``Spatially-distributed coverage optimization and control with limited-range interactions,'' 
ESAIM: Control, Optimisation \& Calculus of Variations, Vol. 11, pp. 691--719, 2005.

\bibitem{IJRR}
M. Schwager, D. Rus, and J. J. Slotine,
``Decentralized, adaptive coverage
control for networked robots,'' 
{\it Int. J. Robotic Research}, Vol. 28, No. 3, pp. 357--375,
2009.




\bibitem{survey}
A. Mavrinac and X. Chen,
``Modeling Coverage in Camera Networks: A Survey
International J. Computer Vision,''
Vol. 101, No. 1, pp. 205--226, 2013.





\bibitem{AMS_BK}
P. A. Absil, R. Mahony and R. Sepulchre, {\it Optimization Algorithms on
Matrix Manifolds}, Princeton University Press, 2008.











\bibitem{MATLAB1}
Mathworks, {\it Computer Vision Toolbox User's Guide}, 2013.

\bibitem{MATLAB2}
Mathworks, {\it Curve Fitting Toolbox User's Guide}, 2013.

\bibitem{MLS_BK}
R. Murray, Z. Li, and S. S. Sastry, {\it A Mathematical Introduction to
Robotic Manipulation}, CRC Press, 1994.
\end{thebibliography}

\end{document}