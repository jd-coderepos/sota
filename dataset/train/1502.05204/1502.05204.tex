\documentclass[11pt]{article}
\newcommand{\LONG}[1]{#1}\newcommand{\SHORT}[1]{}

\usepackage{fullpage,times}
\usepackage{amssymb,amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{rmk}[theorem]{Remark}
\newenvironment{remark}{\begin{rmk}\em}{\end{rmk}}

\newcommand{\eps}{\varepsilon}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\OO}{\widetilde{O}}
\newcommand{\CELL}{\textrm{cell}}
\newcommand{\IGNORE}[1]{}

 \usepackage{algpseudocode}
 \usepackage[]{algorithmicx}
\usepackage{xspace}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{algorithm}\usepackage[breaklinks,bookmarks=false]{hyperref}
\usepackage{enumerate}
\newcommand{\polylog}{\rm{polylog}}
\newcommand{\poly}{\rm{poly}}
\newcommand{\tab}{{ } \hspace{.1\textwidth}}
\newcommand{\smalltab}{{ } \hspace{.05\textwidth}}




\pagestyle{plain}
\begin{document}

\sloppypar
\title{Clustered Integer 3SUM via Additive Combinatorics}
\author{Timothy M. Chan\thanks{Cheriton School of Computer Science, University of Waterloo
(tmchan@uwaterloo.ca).  This research was done in part while
the author was on sabbatical in
Hong Kong University of Science and Technology.
The research is supported by an NSERC grant.}
 \and Moshe Lewenstein\thanks{Department of Computer Science, Bar-Ilan University
(moshe@cs.biu.ac.il).
This research was done in part while the author was on sabbatical in the University of Waterloo. The research is supported by BSF grant 2010437 and GIF grant 1147/2011.}}

\date{}
\maketitle
\setcounter{page}{0}
\thispagestyle{empty}

\begin{abstract}
We present a collection of new results on problems
related to 3SUM, including:
\begin{itemize}
\item The first truly subquadratic algorithm for
\begin{itemize}
\item computing the
(min,+) convolution for monotone increasing sequences with
integer values bounded by ,
\item solving 3SUM for monotone sets in 2D with integer coordinates
bounded by , and
\item preprocessing a binary string for histogram indexing
(also called jumbled indexing).
\end{itemize}
The running time is
 with randomization, or  deterministically.
This greatly improves the previous 
time bound obtained from Williams' recent result
on all-pairs shortest paths [STOC'14], and answers an open question raised by several researchers studying the histogram indexing problem.
\item The first algorithm for histogram indexing for any constant alphabet
size that achieves truly subquadratic preprocessing time and truly sublinear query time.
\item A truly subquadratic algorithm for integer 3SUM in the
case when the given set can be partitioned into 
clusters each covered by an interval of length , for any
constant .
\item An algorithm to preprocess any set of  integers so that
subsequently 3SUM on any given subset can be solved in
 time.
\end{itemize}
All these results are
obtained by a surprising new technique,
based on the Balog--Szemer\'edi--Gowers Theorem from additive
combinatorics.
\IGNORE{
Additive combinatorics is the field concerned with combinatorial properties of sums (and differences) of sets. For example, rich structure can be shown for dense sets.

Several well known algorithmic problems are sum-set problems or neighboring problems. Perhaps, the most prominent is 3SUM, and its extension -SUM. Another important neighboring problem is that of (min,+)-convolutions, which is strongly related to the All Pairs Shortest Path (APSP) problem.

We show how to apply results from additive combinatorics in order to obtain efficient algorithms for problems with sum-set nature. While there have been applications of additive combinatorics to computational complexity, these may be the first applications to algorithmic problems.

We first show how to efficiently implement an additive combinatoric result and then show truly subquadratic, that is  time, algorithms for; (a) {\em 3SUM for monotone sets in }, (b){\em monotone(min,+) convolution} (c) (the open problem of) a preprocessing algorithm for {\em histogram indexing}, (c) 3SUM when the data is clustered, (d) "3-SUM queries", left open in~\cite{BW12}.
}
\end{abstract}


\newpage

\section{Introduction}

\subsection{Motivation: Bounded Monotone (min,+) Convolution}

Our work touches on two of the most tantalizing open algorithmic
questions:
\begin{itemize}
\item Is there a truly subcubic (-time) algorithm
for \emph{all-pairs shortest paths} (APSP) in general dense edge-weighted
graphs?  If all the edge weights are small integers bounded
by a constant, then the
answer is known to be yes, using fast matrix multiplication~\cite{ZwickSURVEY},
but the question remains open not only for arbitrary real weights,
but even for integer weights in, say, .\footnote{ denotes .}
The current best combinatorial algorithms run
in slightly subcubic  time \cite{Chan10,HanTak}.  The recent breakthrough by
Williams~\cite{Williams14} achieves
 expected time (using
fast rectangular matrix multiplication).
\item Is there a truly subquadratic (-time) algorithm for the
\emph{3SUM} problem?  One way to state the problem (out of
several equivalent ways) is:
given sets  of size , decide whether
there exists a triple  such that ; in other words, decide whether
.
All 3SUM algorithms we are aware of actually solve
a slight extension which we will call
the \emph{3SUM} problem:
decide for every element  whether
 for some ; in other words,
report all elements in .
If , then the problem can
be solved in  time by fast Fourier transform (FFT),
since it reduces to convolution for 0-1 sequences of length .
However, the question remains open
for general real values, or just integers from .
The myriad ``3SUM-hardness'' results showing reductions
from both real and integer 3SUM to different problems
about computational geometry, graphs, and strings
\cite{GO95,Patrascu10,BDP08,PW10,VW09,AWW14,ACLL14,JV13,KPP14} tacitly
assume that the answer could be no.
The current best algorithm for integer 3SUM or 3SUM by
Baran et al.~\cite{BDP08} runs in slightly subquadratic
 expected time.
Gr\o nlund and Pettie's recent breakthrough for
general real 3SUM or 3SUM~\cite{GP14} achieves
 deterministic
time or  expected time.
\end{itemize}

Our starting point concerns one of the most basic special cases---of both problems simultaneously---for which finding a truly subquadratic algorithm has remained open.  Put another way, solving the problem below is
a prerequisite towards solving APSP in truly subcubic
or 3SUM in truly subquadratic time:

\begin{description}
\item[The Bounded Monotone (min,+) Convolution Problem:]
Given two monotone increasing sequences  and  lying in , compute their \emph{(min,+)
convolution} , where
.
\end{description}

If all the 's and 's are small integers bounded by , then (min,+) convolution can be reduced to classical convolution and can
thus be computed in  time by FFT\@.
If the differences  and  are randomly
chosen from , then we can subtract a linear function 
from  and  to get sequences lying in a smaller
range 
and thus solve the problem by FFT in  time w.h.p.\footnote{
The  notation hides polylogarithmic factors;
``w.h.p.'' means ``with high probability'', i.e., with probability
at least  for input size  and an arbitrarily large
constant .
}
However, these observations do not seem to help in obtaining truly subquadratic worst-case time
for arbitrary bounded monotone sequences.

We reveal the connection to APSP and 3SUM:
\begin{itemize}
\item
A simple argument~\cite{BCDEHILPT14} shows that
(min,+) convolution can be reduced to
\emph{(min,+) matrix multiplication}, which in turn is known to
be equivalent to APSP\@.  More precisely, if we can
compute the (min,+) matrix multiplication of two  matrices, or solve APSP, in  time, then we can compute the (min,+) convolution of two sequences of length  in  time.  The APSP result by Williams immediately leads to an -time algorithm for (min,+) convolution, the best result known
to date.  The challenge is to see if the bounded monotone case
can be solved more quickly.
\item
Alternatively, we observe that the bounded monotone (min,+) convolution problem can be reduced to 3SUM
for integer point sets in 2D, with at most a logarithmic-factor
slowdown, by setting
 and
 in , and
using  appropriately chosen sets  via
a simultaneous binary search for all the minima (see Section~\ref{sec:mono:appl} for the
details).
Two-dimensional 3SUM in  can be easily
reduced to one-dimensional 3SUM in .
The current best result for integer 3SUM
leads to worse bounds, but the above reduction requires only
a special case of 3SUM, when the points in each of the 2D sets
 in  form a monotone increasing sequence in both
coordinates simultaneously.  The hope is that the 3SUM
in this \emph{2D monotone} case can be solved more quickly.
\end{itemize}

The bounded monotone (min,+) convolution problem has a number
of different natural formulations and applications:
\begin{itemize}
\item
Computing the (min,+) convolution  for two integer sequences in
the \emph{bounded differences} case, where
 for some constant ,
can be reduced to the bounded monotone case by
just adding a linear function  to both  and .
(The two cases turn out to be equivalent\LONG{; see Remark~\ref{rmk-bounded}}.)
\item
Our original motivation concerns {\em histogram indexing}
(a.k.a.\ {\em jumbled indexing}) for a binary alphabet: the problem
is to preprocess a string , so that we can decide whether there is a substring with exactly  0's and  1's for any given  (or equivalently,
with length  and exactly  1's for any given ).
Histogram indexing has been studied in over a dozen papers in the string algorithms literature in the last several years, and
the question of obtaining a truly subquadratic preprocessing
algorithm in the binary alphabet case has been raised several times
(e.g., see \cite{BCFL10,MR10,MR12} and the introduction of~\cite{ACLL14} for a more detailed survey).
In the binary case,
preprocessing amounts to computing the minimum number 
(and similarly the maximum number ) of 's
over all length- substrings for every .
Setting  to be the prefix sum , we see
that , which is precisely
a (min,+) convolution after negating and reversing the second
sequence.  The sequences are monotone increasing and lie in 
(and incidentally also satisfy the bounded differences property).
Thus, binary histogram indexing can be reduced to bounded monotone
(min,+) convolution.
(In fact, the two problems turn out to be equivalent\LONG{; see Remark~\ref{rmk-jumble}}.)
\item
In another formulation of the problem, we are
given  integers in  and want to find an interval
of length  containing the smallest/largest number of elements,
for every ; or find an interval containing
 elements with the smallest/largest length, for every .
This is easily seen to be equivalent to binary histogram indexing.
\item
For yet another application,
a ``necklace alignment'' problem
studied by Bremner et al.~\cite{BCDEHILPT14}, when restricted to input
sequences in , can also be reduced to bounded monotone (min,+) convolution.
\end{itemize}

\subsection{New Result}
We present the first truly subquadratic algorithm
for bounded monotone (min,+) convolution, and thus for
all its related applications such as binary histogram indexing.
The randomized version of our algorithm runs in
 expected time; the curious-looking exponent is
more precisely .
The deterministic version of the algorithm has a slightly worse
 running time.  Our randomized algorithm uses FFT,
while
our deterministic algorithm uses both FFT and fast (rectangular) matrix multiplication.


\subsection{New Technique via Additive Combinatorics}

Even more interesting than the specific result
is our solution, which surprisingly relies on tools from a
different area: \emph{additive combinatorics}.  We explain
how we are led to that direction.

It is more convenient to consider the reformulation of
the bounded (min,+) monotone convolution problem, in terms
of solving 3SUM over certain 2D monotone sets 
in , as mentioned earlier.
A natural approach to get a truly subquadratic algorithm
is via divide-and-conquer.  For example, we can
partition each input set into
subsets by considering a grid of side length  and
taking all the nonempty grid cells;
because of monotonicity of the sets, there are  nonempty grid cells
each containing  points.
For every pair of a nonempty grid cell of 
and a nonempty grid cell of , if their sum lands
in or near a nonempty grid cell of , we need to
recursively solve the problem for the subsets of points in these
cells.  ``Usually'',
not many pairs out of the  possible pairs would satisfy this condition and require recursive calls.
However, there are exceptions;
the most obvious case is when the nonempty grid cells of  all lie on or near a line.
But in that case, we can subtract a linear function from
the -coordinates to make all -values small integers, and
then solve the problem by FFT directly!

Thus, we seek some combinatorial theorem roughly stating that
if many pairs of  have sum
in or near , the sets  and  must be ``special'', namely,
close to a line.
It turns out that
the celebrated {\em Balog--Szemer\'edi--Gowers Theorem} (henceforth,
the BSG Theorem) from additive combinatorics accomplishes exactly
what we need.
One version of
the BSG Theorem (out of several different versions) states:
\begin{quote}
Given sets  of size
 in any abelian group such that ,
we must have  for some
large subsets  and  with
.
\end{quote}
(We will apply the theorem to the sets of nonempty grid cells
in , with .)

The original proof by
Balog and Szemer\'edi~\cite{BS94}
used heavy machinery, namely, the regularity lemma, and
had a much weaker superexponential dependency on .
A much simpler proof with a polynomial -dependency later appeared in a (small part of a famous)
paper by Gowers~\cite{Gowers01}.  Balog~\cite{Balog07} and Sudakov
et al.~\cite{SSV94} further refined the factor to the stated .
Since then, the theorem has appeared in books~\cite{TV06} and surveys~\cite{Lovett14,Viola11}.
Although additive combinatorics, and specifically the BSG Theorem, have found some applications in theoretical computer science
before~\cite{Lovett14,Viola11} (for example, in
property testing~\cite{BLR}), we are not aware of any applications in classical algorithms---we believe this adds further interest to our work.


Four points about the BSG Theorem statement are relevant
to our algorithmic applications:
\begin{itemize}
\item
First, as it reveals, the right criterion of ``special'' is not that
the two sets  and  are close to a line, but rather that
their sumset  has small size.
According to another celebrated
theorem from additive combinatorics, {\em Freiman's Theorem}~\cite{Fre,TV06},
if a sumset  has size , then 
indeed has special structure in the sense that it must
be close to a projection of a higher-dimensional
lattice.  Fortunately, we do not need this theorem (which
requires a more complicated proof and has
superexponential -dependency): if  has small size,
we can actually compute  by FFT directly, as explained in the
``FFT Lemma'' of Section~\ref{sec:bsg}.
\item
Second, the theorem does not state that  and  themselves
must be special,
but rather that we can extract large subsets  and  which are special.
In our applications, we need to ``cover'' all possible pairs
in , and so we need a stronger version of the
BSG Theorem which allows us to remove already covered pairs and iterate.  Fortunately,
Balog~\cite{Balog07} and Szemer\'edi et al.~\cite{SSV94} provided
a version of the BSG Theorem that did precisely this;
see Section~\ref{sec:bsg} for the precise statement.
The resulting corollary on pairs covering is dubbed
the ``BSG Corollary'' in Section~\ref{sec:bsg}.
\item
The BSG Theorem was originally conceived with the setting
of constant  in mind, but
polynomial -dependency (which fortunately we have)
will be critical in obtaining truly
subquadratic algorithms in our applications, as we need to
choose  (and ) to balance the contribution of
the ``usual'' vs.\ ``special'' cases.
\item
The BSG Theorem is originally
a mathematical result, but the time complexity of the
construction will matter in our applications.
We present, to our knowledge, the first time bounds in
Theorem~\ref{runtime-corollary}.
\end{itemize}

Once all the components involving the BSG Corollary and the FFT Lemma
are in place, our main algorithm for bounded monotone (min,+)
convolution can be described simply, as revealed in Section~\ref{sec:mono}.

\subsection{Other Consequences of the New Technique}

The problem we have started with, bounded monotone (min,+) convolution, is just one of many applications that can
be solved with this technique.  We briefly list our
other results:
\begin{itemize}
\item
We can solve 3SUM not only in the 2D monotone case, but
also in the -dimensional monotone case
in truly subquadratic
 expected time
for any constant  (Theorem~\ref{thm-monotone}).
If only  and  are monotone,
a slightly weaker bound  still holds;
if just  is monotone, another
weaker bound  holds (Theorem~\ref{cor-monotone-offline}).
\item
In 1D,
we can solve integer 3SUM in truly subquadratic
 time
if the input sets are {\em clustered} in the sense that
they can be covered by 
intervals of length  (Corollary~\ref{cor-general}).
In fact, just one of the sets  needs to be clustered.
This is the most general setting of 3SUM we know that
can be solved in truly subquadratic time
(hence, the title of the paper).
In some sense, it ``explains'' all the other results.
For example, -dimensional
monotone sets, when mapped down to 1D in an appropriate way, become
clustered integer sets.

\item We can also solve a data structure version of 3SUM where
 is given online:
preprocess  and  so that we can decide whether
any query point  is in .
For example, if  and  are monotone in ,
we get truly subquadratic  expected
preprocessing time and truly sublinear 
query time for any sufficiently small 
(\LONG{Corollary~\ref{cor-monotone-online}}\SHORT{see the full paper}).
\item
As an immediate application,
we can solve the histogram indexing problem for any constant alphabet size : we can preprocess any string  in truly
subquadratic  expected time, so that we can decide
whether there is a substring whose vector of character counts
matches exactly a query vector in truly sublinear
 time for any sufficiently
small  (\LONG{Corollary~\ref{cor-jumble-online}}\SHORT{see the full paper}).
This answers an open question and improves a
recent work by Kociumaka et al.~\cite{KRR13}.
Furthermore, if  queries are given offline, we can answer all queries in total  expected time
(Corollary~\ref{cor-jumble-offline}).
As  gets large,
this upper bound approaches a conditional lower bound recently
shown by Amir et al.~\cite{ACLL14}.
\item
For another intriguing consequence, we can preprocess
any universes  of  integers so that given any subsets
, we can solve 3SUM for  in truly subquadratic
 time (\LONG{Theorem~\ref{thm-preproc1}}\SHORT{see the full paper}).  Remarkably, this is a result about
general integer sets.  One of the results in
Bansal and Williams' paper~\cite{BW12} mentioned precisely
this problem but obtained much weaker polylogarithmic speedups.
When  is not given, we can still achieve  time
(\LONG{Theorem~\ref{thm-preproc2}}\SHORT{see the full paper}).
\end{itemize}


\IGNORE{

 showed slightly better upper bounds for integer 3SUM in the RAM word model. Specifically, they showed that one can reduce a general 3SUM integer input to  and save a logarithmic factor on  input, i.e. achieve running time of . Moreover, they pointed out that if the input  one can generate an  FFT-based algorithm giving an improvement for input from . All this points to the fact that 3SUM is indeed a hard problem, even for integers from .


It is tempting to seek for a relation between (min,+) convolution and 3SUM, as both operate on sums of sets. In Section~\ref{sec:mono} we show that (min,+) convolution for monotone bounded sequences is in fact a form of monotone 3SUM for monotone sets in  which can be viewed more generally as {\em clustered 3SUM}, see formal definitions there. The reduction from monotone (min,+) in  to 3SUM for montone set in  is an elegant binary search on the structure of the sum of the sets. It appears in Corollary~\ref{cor-monotone-2d}.

For the problems above we propose a novel algorithm that is based upon structural theorems from additive combinatorics that we will expand upon shortly. We succeed in obtaining an  randomized algorithm and a deterministic algorithm of running time with a slightly larger exponent. The deterministic algorithm uses both FFT, in fact - sparse FFTs, and rectangular matrix multiplication. This answers the open question of binary histogram indexing and yields algorithms for the (min,+) convolution and 3SUM for monotone sequences. We later show how our method is useful in obtaining several other results.

The realization that at the core of (min,+) convolutions and 3SUM lies a common structure of sums of sets is of ultimate importance. In the 3SUM problem one asks whether for additive sets ,  and , that is sets of elements from a field with a  operator, there exists a triplet  such that . In other words,
is ?.  We also define its natural extension 3SUM.

\noindent
{\em The 3SUM problem:} decide for every element  whether
 for some  and .  In other words,
report all elements in .



The 3SUM (or 3SUM) problem can be solved straightforwardly in  time, but it seems hard to beat this running time. This seeming hardness led Gajentaan and Overmars~\cite{GO95} to define 3SUM-hardness. Moreover, they reduced many computation geometry problems to 3SUM. For example, the problems of minimum-area triangle, finding 3 collinear points, and determining whether  axis-aligned rectangles cover a given rectangle. 3SUM-hardness has been shown for numerous other problems.

P\u{a}tra\c{s}cu~\cite{Patrascu10}, inspired by his own works~\cite{BDP08,PW10} and by the work of Vassilevska and Williams~\cite{VW09}, showed how  to use 3SUM for reductions to purely combinatorial problems, such as those on graphs or strings. The P\u{a}tra\c{s}cu~\cite{Patrascu10} result has led to a slew of new 3SUM-hardness reductions, e.g.~\cite{AWW14,ACLL14,JV13,KPP14}, that are not by common arithmetic.

On the other hand, Baran et al.~\cite{BDP08} showed slightly better upper bounds for integer 3SUM in the RAM word model. Specifically, they showed that one can reduce a general 3SUM integer input to  and save a logarithmic factor on  input, i.e. achieve running time of . Moreover, they pointed out that if the input  one can generate an  FFT-based algorithm giving an improvement for input from . All this points to the fact that 3SUM is indeed a hard problem, even for integers from .

This leads us back to the structure of the 3SUM problem, i.e. the sum of additive sets, which can be viewed as a structure of high interest in the field of {\em additive combinatorics}.



-------------------------------




Additive combinatorics, see~\cite{TV06}, is the field concerned with the structure of additive sets, that is subsets of an abelian group  with group operation . For additive sets  and  in  we define the sum set to be



and the difference set to be

.


Typical questions addressed in the field relate to the structure of sets. For example for one additive set one may ask; is  small? Can  be covered by a small number of translates of ? Are there many quarduples  such that ? These questions have different answers depending on . For example, it is easy to verify that if  is a progression there will be much structure and when  is random and sparse then there will be little structure. Even more questions arise when there are two additive sets involved.

Additive combinatorics has a rich collection of techniques and results obtained from elementary combinatorics, additive geometry, harmonic analysis and graph theory among others. It also has a large set of applications. In fact it has been incorporated for use in computer science in the realm of computational complexity, see surveys~\cite{Lovett14,Viola11}. However, additive combinatorics has seemingly not been applied to the construction of efficient algorithms.

Recently we became interested in 3SUM for monotone sets in , for constant , because of its importance for a collection of other problems. Monotone sets are sets whose elements can be ordered into a sequence that is increasing/decreasing in every dimension. It seemed that a potential approach for solving it (in better than  might be to divide the problem into two cases:

\begin{enumerate}
\item
if the input does not have too many "sums", then
   brute force would take subquadratic time;
\item
otherwise, argue that the input must be "specially structured",
   namely, it is close to a linear sequence, in which case some form of FFT perhaps could
   solve the problem efficiently.
\end{enumerate}

This is exactly what additive combinatorics is about. There are several theorems that seem to be potentially useful for the problems we consider, e.g. Freiman's theorem and Rusza theorem. But, it turns out that the Balog-Szemer\'{e}di-Gowers (BSG) theorem seems to exactly accomplish what we need. In fact, we succeed in obtaining an  running time randomized algorithm for 3SUM for monotone sets in  and an  deterministic algorithm.



One of this result's consequences is that we obtain the same running times for (min,+) convolution for sequences of elements  that are monotonically increasing. Surprisingly, we prove that this problem is {\em equivalent} to the preprocessing of binary histogram indexing ((a.k.a. binary histogram indexing). The problem is to preprocess a binary string to allow for queries of the form "is there a substring with 's and 's?". This has been intensively researched for the last few years with a couple of dozen papers on the topic, see~\cite{ACLL14} for a more detailed background. The best result is based on the new APSP result of Williams~\cite{Williams14} and is . Beforehand, it was ~\cite{}. As a result of the equivalence we obtain an  time preprocessing algorithm with  query time, i.e. the first truly subquadratic preprocessing time algorithm. All these results appear in detail in Section~\ref{sec:mono}.


\subsection{BSG Outline}


The BSG theorem, fully stated in Section~\ref{sec:bsg}, is the result of the work that appeared in two papers~\cite{BS94,Gowers01}. A later paper by Balog~\cite{Balog07} expands upon the theorem. Terence and Vu~\cite{TV06} and Sudakov, Szemer\'{e}di and Vu~\cite{SSV94} reprove the theorem in simpler format. Viola~\cite{Viola11} and, lately, Lovett~\cite{Lovett14} present the theorem and applications from a TCS perspective, for applications of computational complexity and combinatorics.


There are different variants of the theorem.
For (i), "sums" may be defined in terms of the number of 
in a set  with  ("additive quadruples"), or the number of 
in  with , which are the types of condition necessary.

For (ii), the
condition that we need is: the size of the sumset  is close to linear.
This case can be handled directly by FFT.

There is an additional complication: in (ii), the theorem doesn't
say S has the special structure, but rather that we can extract a large
subset S' of S that has this special structure.  So, for our
purposes, we need to remove the external set and iterate when we apply the theorem. We give the detailed theorem and the iteration result in Section~\ref{sec:bsg}.




\subsection{More results}

In Section~\ref{sec:clustered} we show that 3SUM for monotone sets in  can be generalized to the more general 3SUM for clustered sets of elements . A clustering of a set is its partition into disjoint hypercubes. The result is dependant on the parameters of the number of hypercubes, the volume of the hypercubes and the upper bound on the number of points in each hypercube. For example, if  and  each contain  points  for a constant  then if  is covered by  disjoint hypercubes of volume  then we can solve 3SUM in  expected time.

This result has additional truly subquadratic applications for offline batched histogram indexing and the 3SUM monotone case, even when only  is monotone.

In Section~\ref{sec:online} we show that the techniques carry over, with additional ideas, to the online setting, i.e. when  is not given in advance, for clustered sets. More applications follow. An application worth noting is online histogram indexing for alphabets of size . Kociumaka et al. proposed an  space and  query time algorithm. However, the preprocessing time was not truly subquadratic. We obtain, with the same space bound, an  query time algorithm with truly subquadratic preprocessing!

Finally, in Section~\ref{sec:preprocessed}  we show how to solve 3SUM in a preprocessed universe. The problem is given  preprocess the sets to answer 3SUM for subsets  given as a query. This was considered by Bansal and Williams~\cite{BW12} where the question was attributed to Avrim Blum. In~\cite{BW12} a solution was proposed with  preprocessing time, for any , and  query time. We show how to use the BSG theorem to achieve preprocessing time of  preprocessing time and  query time.

\bigskip
To summarize and point out the additional highlights that we obtain:

\bigskip
\noindent
{\bf Highlights}
\begin{enumerate}
\itemsep0em
\item
The use of additive combinatorics for various algorithmic problems.
\item
An  time algorithm for {\em monotone 3SUM}  and an  algorithm for its generalization {\em clustered 3SUM} ( based on the clustering).
\item
A reduction from {\em monotone 3SUM}  to {\em bounded monotone (min,+) convolution}.
\item
An equivalence between {\em binary histogram indexing} and {\em bounded monotone (min,+) convolution}.
\item
The implication of the former is an  preprocessing time algorithm for binary histogram indexing (the first truly subquadratic algorithm for this open problem).
\item
Algorithms for higher dimension histogram indexing improving upon~\cite{KRR13} in query time with (first time) truly subquadratic preprocessing time.
\item
An  algorithm for 3SUM in a preprocessed universe. This is the first truly subquadratic algorithm for the problem (improving upon~\cite{BW12}).
\item
An  time deterministic algorithm for sparse convolutions~\cite{CH02,AKP07}. This improves over the previous best result of ~\cite{AKP07}.
\end{enumerate}

}

\section{Ingredients: The BSG Theorem/Corollary
and FFT Lemma}\label{sec:bsg}


As noted in the introduction,
the key ingredient behind all of our results is the Balog--Szemer\'edi--Gowers Theorem.  Below,
we state the particular version of the theorem we need, which can be found
in the papers by Balog~\cite{Balog07} and Sudakov et al.~\cite{SSV94}.  A complete proof is redescribed in
\LONG{Sections \ref{sec-graph} and \ref{sec-bsg-proof}}\SHORT{the full paper}.

\begin{theorem} {\bf(BSG Theorem)}\ \ Let  and  be finite subsets of an abelian group, and .
  Suppose that , , and .
  Then there exist subsets  and  such that

\begin{itemize}
\item[\rm (i)]   , and
\item[\rm (ii)]  .
\end{itemize}
\end{theorem}

The main case to keep in mind is when  and ,
which is sufficient for many of our applications, although the
more general ``asymmetric'' setting does arise in at least two of the
applications.

In some versions of the BSG Theorem,
 (or ) and we further insist that  (or ); there, the -dependencies are a bit worse.

In some simpler versions of the BSG Theorem that appeared in many papers (including the version mentioned in the introduction), we are not given ,
but rather a set  of size  with ; in other words, we are considering the special case .  Condition (ii) is replaced by
.
For our applications, it is crucial to consider the version with
a general .  This is because of the need to apply the
theorem iteratively.

If we apply the theorem iteratively, starting with
 for a given set , and repeatedly removing
 from , we obtain the following corollary, which
is the combinatorial result we will actually use in all our applications
(and which, to our knowledge, has not been stated explicitly before):

\begin{corollary}~\label{cor-BSG} {\bf(BSG Corollary)}\ \
Let  be finite subsets of an abelian group.
  Suppose that  and .  For any , there exist subsets
   and  such that

\begin{enumerate}
\item[\rm (i)]
the \emph{remainder set}  has
size at most ,
\item[\rm (ii)]
   for each , and
\item[\rm (iii)] .
\end{enumerate}
\end{corollary}

A naive argument gives only , as each
iteration removes  edges from ,
but a slightly more refined analysis, given
in \LONG{Section~\ref{sec-bsg-corollary-proof}}\SHORT{the full paper}, lowers
the bound to .

None of the previous papers on the BSG Theorem addresses the running
time of the construction, which will of course be
important for our
algorithmic applications.  A polynomial time bound can be easily
seen from most known proofs of the BSG Theorem, and
is already sufficient to yield some nontrivial result
for bounded monotone (min,+)-convolution and
binary histogram indexing.  However, a quadratic time bound is necessary to get
nontrivial results for other applications such as histogram
indexing for larger alphabet sizes.
In \LONG{Sections \ref{sec-graph-time} and \ref{sec-bsg-corollary-time}}\SHORT{the full paper, using a number of additional
ideas (e.g., sampling tricks for sublinear algorithms)}, we
show that the construction in the BSG Theorem/Corollary
can indeed be done in near quadratic time
with randomization, or in matrix multiplication time deterministically.


\newcommand{\MM}{{\cal M}}

\begin{theorem}~\label{runtime-corollary}
In the BSG Corollary, the subsets , the remainder set ,
and all the sumsets  can be constructed by
\begin{itemize}
\item[\rm (i)] a deterministic algorithm in time ,
or more precisely, ,
where  is the complexity of multiplying
an  and an  matrix, or
\item[\rm (ii)] a randomized Las Vegas algorithm in expected
time  for , or   otherwise.
\end{itemize}
\end{theorem}




We need one more ingredient.  The BSG Theorem/Corollary produces
subsets that have small sumsets.  The following lemma shows
that if the sumset is small, we can compute
the sumset efficiently:

\begin{lemma} {\bf(FFT Lemma)}\ \ Given sets 
of size  for a constant  with , and
given a set  of size 
which is known to be a superset of , we can compute  by
\begin{itemize}
\item[\rm (i)] a randomized Las Vegas algorithm in  expected time, or
\item[\rm (ii)] a deterministic algorithm that runs in 
time after preprocessing  in  time for
an arbitrarily small constant .
\end{itemize}
\end{lemma}

As the name indicates, the proof of the lemma uses fast Fourier transform. The randomized version
was proved by Cole and Hariharan~\cite{CH02}, who
actually obtained a more general result where the superset 
need not be given: they addressed the problem
of computing the (classical) convolution of two sparse vectors and
presented a Las Vegas algorithm that runs in time sensitive to the
number of nonzero entries in the output vector; computing
the sumset  can be viewed as an instance of the
sparse convolution problem and can be solved by their algorithm
in  expected time.
Amir et al.~\cite{AKP07} have given a derandomization technique
for a related problem (sparse wilcard matching), which can also
produce a deterministic algorithm for computing 
in the setting when  is given and has been preprocessed, but the preprocessing of  requires  time.

In \LONG{Section~\ref{sec-fft}}\SHORT{the full paper},
we give self-contained proofs of both the randomized
and deterministic versions of the FFT Lemma.  For the randomized
version, we do not need the extra complications of Cole and
Hariharan's algorithm, since  is given in our applications.
For the deterministic version, we significantly reduce
Amir et al.'s preprocessing cost to , which is
of independent interest.

\IGNORE{
\noindent
{\bf Proof outline:}

First we can replace  with , by mapping
   to  for a sufficiently large .

  Use a small number of "nearly additive" hash functions
  
  (e.g.,  for some random prime , or
  the hash function from the Baran-Demaine-Patrascu 3SUM paper). NEEDS MORE WORK.

  For each , multiply the polynomials
   and  with a sparse convolution.

  A sparse convolution is a convolution whose input has a high number of zero values. Hence, the input is represented as a list of the non-zero values, and its size  = the number of non-zero values. The size of the (non-zero) output is denoted by . Cole and Hariharan~\cite{CH02} claimed a randomized algorithm that runs in time  yielding the desired.

  In Section~\ref{determinstic-algo} we show our claim for the deterministic case. We point out that a weaker deterministic result that runs in  time~\cite{AKP07} existed previously. AMIR-PORAT DO NOT REALLY NEED T, IS IT COMPARABLE?
  \qed

}










\section{3SUM for Monotone Sets in }\label{sec:mono}

We say that a set in  is \emph{monotone (increasing/decreasing)}
if it can be written as  where
the -th coordinates of
 form a monotone (increasing/decreasing) sequence
for each .
Note that a monotone set in  can have size at most .


\subsection{The Main Algorithm}

\begin{theorem}\label{thm-monotone}
Given monotone sets  for a constant~,
we can solve 3SUM by
\begin{itemize}
\item[\rm (i)] a randomized Las Vegas algorithm in expected time
 for ,
 for ,
or more generally,
 for any~, or
\item[\rm (ii)] a deterministic algorithm in time
 for ,
 for ,
 for ,
 for ,
 for ,
or  for .
\end{itemize}
\end{theorem}
\begin{proof}
Divide  into 
grid cells of side length , for some parameter  to be set later.
Define  to be a label (in ) of the grid cell
containing the point ; more precisely,
.

We assume that all points 
satisfy  for every ;
when this is true, we say
that  is \emph{aligned}.  This is without loss of generality,
since  can be decomposed into a constant () number of subsets, each of which is a translated copy of an aligned set,
by shifting selected coordinate positions by .
Similarly, we may assume that  is aligned.
By alignedness, the following property holds: for any  and
,  implies .

Our algorithm works as follows:
\begin{description}
\item[Step 0:]
Apply the BSG Corollary to the sets ,
, .
This produces subsets  and a remainder set .

Note that  by monotonicity of .
The parameters in the BSG Corollary are thus 
and .
Hence, this step takes  expected time
by Theorem~\ref{runtime-corollary}.
\item[Step 1:]
For each ,
recursively solve the problem for the sets
, ,
.

Note that
this step creates  recursive calls,
where each set lies in a smaller universe, namely,
a translated copy of .
\item[Step 2:]
For each ,
apply the FFT Lemma to generate
,
which is contained in the
superset .
Report those generated elements that are in .

Note that the size of  is ,
and so the size of  is .
As , this step takes
 expected time.
\end{description}

Correctness is immediate from the BSG Corollary, since
 is covered by
.

The expected running time is characterized by the following interesting recurrence:

Note that the reduction to the aligned case increases only
the hidden constant factors in the three terms.
We can see that this recurrence leads to truly subquadratic
running time for any constant ---even if we use the trivial upper bound 
(i.e., don't use recursion)---by setting  and  to be some sufficiently small powers of .

For example, for , we can set  and  and obtain

which solves to .

More precisely, the recurrence solves to 
by setting  and  for  satisfying
the system of equations
.  One can check that the
solution for  indeed obeys the quadratic equation
.

\IGNORE{
\begin{theorem}
Alternatively, we can solve the problem in Theorem~\ref{thm-monotone} in
 deterministic time where
 is the larger root of
.
Here, , and
 and  are
the square and rectangular matrix multiplication exponents.
\end{theorem}
\begin{proof}
}

Alternatively, the deterministic version of the algorithm
has running time
given by the recurrence

with  and ,
which can be solved in a similar way.
The quadratic equation now becomes
.
\end{proof}

\IGNORE{
For example, for , the randomized time bound is
 (attained
by setting  and )
and the
deterministic time bound is  (attained
by setting  and ).
}
As  gets
large, the exponent in the randomized bound is
; however, the deterministic bound
is subquadratic only for  using
the current matrix multiplication exponents
(if , then we would have subquadratic deterministic
time for all ).
In the remaining applications, we will mostly emphasize randomized bounds for the sake of simplicity.

\IGNORE{

w := 2.3728639;
r := 0.30298;
mu := (3-r-w)/(1-r);
d := 2;
solve({z = mu*y+w*t, z = -y+2*t+(1-t)*z, z = 6*y + t + d*(1-t)});

w := 2.3728639;
r := 0.30298;
mu := (3-r-w)/(1-r);
z := proc(d)  a := 6-mu; b := d*(1+mu)-13+w+2*mu; c := -d*(w+2*mu);
              (-b + sqrt(b^2 - 4*a*c))/(2*a); end;
> z(2);
                           1.863158551

> z(3);
                           1.900083074

> z(4);
                           1.929989472

> z(5);
                           1.954841478

> z(6);
                           1.975898027

> z(7);
                           1.994013663

> z(8);
                           2.009793911

}


\subsection{Application to the 2D Connected Monotone Case,
Bounded Monotone (min,+) Convolution, and
Binary Histogram Indexing}\label{sec:mono:appl}

\SHORT{
As noted in the full paper,
bounded monotone (min,+) convolution reduces to
3SUM for 2D monotone sets, and is equivalent to
(min,+) convolution in the bounded differences case and
to binary histogram indexing.  Thus, all these
problems can now be solved in  expected time (or
 deterministic time).
}
\LONG{
We say that a set in  is \emph {connected}
if every two points in the set are connected by a path
using only vertices from the set and edges of unit -length.
In the case of connected monotone sets  in 2D, we show how
to compute a complete representation of .

\begin{corollary}\label{cor-monotone-2d}
Given connected monotone increasing sets ,
we can compute the boundary of , a region bounded by
two monotone increasing sets,
in  expected time (or
 deterministic time).
\end{corollary}
\begin{proof}
First we show that  is indeed a region bounded by
two monotone increasing sets.
Define  to be the set of -values of  at
the vertical line .
Then each  is a single interval: to see this,
express  as the union of intervals
 over all , and just observe that
each interval  overlaps with the
next interval  as  and  are connected and monotone increasing.  Since the lower/upper endpoints of  are clearly monotone increasing in , the conclusion follows.

We reduce the problem to 3SUM for three 2D monotone sets.
We focus on the lower boundary of , i.e.,
computing the lower endpoint of the interval , denoted
by , for all .   The upper
boundary can be computed in a symmetric way.
We compute all  by a simultaneous binary search in
 rounds as follows.

In round , divide  into grid intervals of length .  Suppose that at the beginning
of the round, we know which grid interval  contains 
for each .
Let  be the midpoint of~.
Form the set .
Since the 's are monotone increasing, we know that
the 's and 's are as well; hence,  is a
monotone increasing set in .
Apply the 3SUM algorithm to .
If  is found to be in , then
 contains  and so we know that .
Otherwise,  is either completely smaller than  or
completely larger than ; we can tell which is the case
by just comparing any one element of  with , and so we know whether  is smaller or
larger than .  (We can easily
pick out one element from 
by picking any  in the -range of  and  in the -range of  with , picking any point of  at  and any
point of  at , and summing their -coordinates.)
We can now reset  to the
half of the interval that we know contains , and proceed
to the next round.  The total running time is that of the
3SUM algorithm multiplied by .
\end{proof}

It is possible to modify the algorithm in Theorem~\ref{thm-monotone}
directly to prove the corollary and avoid the extra
logarithmic penalty, but the preceding black-box reduction
is nevertheless worth noting.

\begin{corollary}
Given two monotone increasing sequences
 and ,
we can compute their (min,+) convolution in
 expected time (or
 deterministic time).
\end{corollary}
\begin{proof}
We just apply Corollary~\ref{cor-monotone-2d} to
the connected monotone increasing sets 
and  in .  Then the lowest -value
in  at  gives the -th entry of the (min,+) convolution.
\end{proof}



\begin{remark}\label{rmk-convol}
The problems in the preceding two corollaries are
in fact equivalent.
To reduce in the other direction, given connected monotone increasing sets
, first we may assume that the -ranges
of  and  have the same length, since
we can prepend one of the sets with a horizontal line segment
without affecting the lower boundary of .  By translation,
we may assume that the -ranges of  and  are identical
and start with 0.  We define the monotone increasing
sequences  (lowest -value of
 at ) and  (lowest -value of  at ).
Then the (min,+) convolution of the two sequences gives
the lower boundary of .  The upper boundary can be computed
in a symmetric way.
\end{remark}

\begin{remark}\label{rmk-bounded}
We can now compute the (min,+) convolution of two integer
sequences with bounded differences property,
by reducing to the monotone case as noted in the introduction.

This version is also equivalent.
To reduce in the other direction, given connected monotone increasing sets
 and  in 
where ,
we apply the linear
transformation .
After the transformation,
.  When applying the same
reduction in Remark~\ref{rmk-convol}
to the transformed sets  and ,
the two resulting monotone increasing sequences will satisfy
the bounded differences property (the
differences of consecutive elements are all in ).
The boundary of  can be inferred from
the boundary of .
\end{remark}



\begin{corollary}\label{cor-jumble-binary}
Given a string ,
we can preprocess in
 expected time (or
 deterministic time) into an -space structure,
so that we can answer
histogram queries, i.e., decide whether there exists
a substring with exactly  's and  's for any
query values , in  time.
\end{corollary}
\begin{proof}
One reduction to bounded monotone (min,+) convolution has been noted
briefly in the introduction.  Alternatively, we can just
apply Corollary~\ref{cor-monotone-2d} to the
connected monotone increasing sets  and ,
where the - and -coordinates of  are the number of
0's and 1's in the prefix . (We can make 
lie in  by translation.)  Then  gives
the number of 0's and 1's in the substring  for any .  The boundary of  gives the desired structure.
\end{proof}


\begin{remark}\label{rmk-jumble}
This problem is also equivalent.
To reduce in the other direction, suppose we have
connected monotone increasing sets  and
 in , given in sorted order with .
We set  if ,
or  if ; and set  if
, or  if .
We then solve histogram indexing for
the binary string .
The minimum number of 1's over all substrings with  0's
(which can be found in  queries by binary search)
gives us the lowest point in  at .  The upper boundary
can be computed in a symmetric way.
\end{remark}

}


\section{Generalization to Clustered Sets}\label{sec:clustered}

In the main algorithm of the previous section, monotonicity is
convenient but not essential.
In this section, we identify the sole property needed: clusterability.  Formally, we say that
a set in  is \emph{-clustered} if it can be
covered by  disjoint hypercubes each of volume .
We say that it is \emph{-clustered} if
furthermore each such hypercube contains at most  points of the set.

\subsection{The Main Algorithm}

\begin{theorem}\label{thm-cluster}
Given -, -, and -clustered
sets  for a constant ,
we can solve 3SUM in expected time

where .
\end{theorem}
\begin{proof}
The algorithm is similar to the one in Theorem~\ref{thm-monotone},
except without recursion.
We use a grid of side length , and
as before,
we may assume that  and  are aligned.
\begin{description}
\item[Step 0:]
Apply the BSG Corollary to the sets ,
, .
This produces subsets  and a remainder set .

Note that .
The parameters in the BSG Corollary
are thus  and .
This step takes  expected time
by Theorem~\ref{runtime-corollary}.
\item[Step 1:]
For each ,
solve the problem for the sets
, ,
.

Note that the three sets have sizes , ,
, and so the naive brute-force algorithm which
tries all pairs from two of the three sets takes  time.
As , this step takes total
time .

\item[Step 2:]
For each ,
apply the FFT Lemma to generate
,
which is contained in the
superset .
Report those generated elements that are in .

Note that the size of  is ,
and so the size of  is .
As , this step takes expected time
.
\end{description}

The total expected time is
.
We set .
\end{proof}

It turns out that the  bounds on the number of points
per cluster are not essential, and neither is clusterability
of the third set .  In fact, clusterability of only one
set  is enough to obtain nontrivial results.

\begin{corollary}\label{cor-cluster}
Given sets  of size  for a constant  where
 and  are - and -clustered,
we can solve 3SUM in expected time


\end{corollary}
\begin{proof}
We say that a set of size  is \emph{equitably -clustered} if
it is -clustered.  Suppose that 
are equitably  -, -, and -clustered.
Then in Theorem~\ref{thm-cluster}, we set ,
, , and
upper-bound  in the second term by the following
weighted geometric mean
(with carefully chosen weights):

and we upper-bound  in the third term more simply by .
This leads to the expression
.
The third term is always dominated by the second (since ),
and so we get precisely the stated time bound.

What if  are not equitably clustered?
We can decompose  into 
equitably -clustered subsets: just put points in
hypercubes with between  and  points into the -th
subset.  We can do the same for  and .
The total time increases by at most an  factor
(since the above bound is nondecreasing in  and 
and independent of ).
\end{proof}

The corollary below now follows immediately by
just substituting , , and .

\begin{corollary}\label{cor-general}
Given sets  of size  for a constant  where
 is -clustered,
we can solve 3SUM in  expected time.
\end{corollary}

Although it may not give the best quantitive bounds, it
describes the most general setting under which
we know how to solve 3SUM in truly subquadratic time.

For example, for , the above corollary generalizes the well-known
fact that 3SUM for integer sets in  can be
solved in subquadratic time (by just doing one FFT), and
a not-so-well-known fact that 3SUM for three integer sets where
only one set is assumed to be in  can still be solved
in subquadratic time (by doing several FFTs, without requiring additive combinatorics---a simple exercise we leave to the reader).

\SHORT{
Although we have stated the above results in  dimensions,
the one-dimensional case of integers contains the essence, since we can
map higher-dimensional clustered sets to 1D, as noted in the full
paper.
}
\LONG{
\begin{remark}
Although we have stated the above results in  dimensions,
the one-dimensional case of integers contains the essence, since we can
map higher-dimensional clustered sets to 1D\@.
Specifically,
consider a grid of side length , and
without loss of generality,
assume that  are aligned.
We can map each point  to the integer

If  is -clustered, then the mapped set in 1D is
still -clustered.  By alignedness,
3SUM solutions are preserved by the mapping.
\end{remark}
}

\subsection{Application to the Monotone Case and Offline Histogram
Queries}

\begin{corollary}\label{cor-monotone-offline}
Given sets  of size  for a constant~
where  and  are monotone,
we can solve 3SUM in
 expected time.

If only  is monotone, we can solve the
problem in  expected time.
\end{corollary}
\begin{proof}
A monotone set in  is -clustered
for all .  For example, it is -clustered,
and so by Corollary~\ref{cor-general}, we know that truly
subquadratic running time is achievable.
For the best quantitive bound, we set
 and  in Corollary~\ref{cor-cluster}
and get

We set  to balance the two terms.

If only  is monotone, we get

We set .
\end{proof}

The above bounds are (predictably) slightly worse than in
Theorem~\ref{thm-monotone}, which assumes
the monotonicity of the third set .  The algorithm there also
exploits a stronger ``hierarchical'' clustering property enjoyed by
monotone sets (namely, that clusters are themselves clusterable),
which allows for recursion.


\begin{corollary}\label{cor-jumble-offline}
Given a string  for a constant
alphabet size  and a set  of  vectors,
we can answer histogram queries, i.e.,
decide whether there exists a substring with
exactly  's, \ldots, and  's,
for all the vectors ,
in  total expected time.
\end{corollary}
\begin{proof}
We just apply the 3SUM algorithm in Corollary~\ref{cor-monotone-offline} to the
\LONG{connected }monotone increasing sets  and
, and the (not necessarily monotone) set ,
where the  coordinates of  hold the number of 0's, \ldots, 's
in the prefix .
Then the  coordinates of  give
the number of 0's, \ldots, 's in the substring  for any .
\end{proof}

The above upper bound nicely complements
the conditional hardness results by
Amir et al.~\cite{ACLL14}.  They proved an  lower
bound on the histogram problem under the assumption that integer 3SUM requires at least  time, and an 
lower bound under a stronger assumption that
3SUM in  requires at least  time.  (Their results
were stated for online queries but hold in the offline
setting.)  On the other hand, if the assumption fails, i.e.,
integer 3SUM turns out to have a truly subquadratic algorithm,
then there would be a truly subquadratic algorithm
for offline histogram queries with an exponent independent of .



\LONG{
\section{Online Queries}\label{sec:online}

We now show how the same techniques can even
be applied to the setting where the points of the third set  are not
given in advance but arrive online.

\subsection{The Main Algorithm}

\begin{theorem}\label{thm-cluster-online}
Given - and -clustered sets  for a constant  and a parameter ,
we can preprocess in expected time

into a data structure with  space,
so that we can decide whether any query point  is in 
in  time.
\end{theorem}
\begin{proof}
The approach is similar to our previous algorithms, but with
one more idea: dividing into the cases of ``low popularity''
and ``high popularity'' cells.
As before, we use a grid of side length  and assume
that  and  are aligned.

The preprocessing algorithm works as follows:
\begin{description}
\item[Step 0:] Let  and
.
Place each 
in the \emph{bucket} for .  Store all these
buckets.  Define the \emph{popularity}
of  to be the number of elements in its
bucket.  Let  be the set of all 
with popularity .
Apply the BSG Corollary to .

Note that , , and , because
the total popularity over all possible  is
at most .  The parameters in the BSG Corollary
are thus  and .
The buckets can be formed in  time and space.
This step takes  expected time
by Theorem~\ref{runtime-corollary}.
\item[Step 1:]
For each , generate the list
.

Note that naively each such list can
be generated in  time.
Since , this step takes total
time .

\item[Step 2:]
For each , apply the FFT Lemma to generate the list
,
which is contained in the
superset .

Note that the size of  is ,
and so the size of  is .
As , this step takes expected time
.
\item[Step 3:]
Store the union  of all the lists generated in Steps
1 and~2.
Prune elements not in  from~.

Note that the pruned list  has size at most
.
\end{description}

The query algorithm for a given point  works as follows:
\begin{description}
\item[``Low'' Case:]  has popularity .
W.l.o.g., assume .
We look up the bucket for .  For each 
in the bucket, we search for some  with 
that satisfies .  The search time is  per bucket entry, for a total of .
\item[``High'' Case:]  has popularity .  We just test  for membership in  in
 time.
\end{description}

To summarize, the preprocessing time is
,
the space usage is ,
and the query time is .
We set .
\end{proof}


\begin{corollary}\label{cor-cluster-online}
Given - and -clustered sets  of size  for a constant  and a parameter ,
we can preprocess in expected time

into a data structure with  space,
so that we can decide whether any query element  is in 
in  time.
\end{corollary}
\begin{proof}
Recall the definition of equitable clustering in the proof of Corollary~\ref{cor-cluster}.
Suppose that 
are equitably  -, -, and -clustered.
Then in Theorem~\ref{thm-cluster-online}, setting ,
, , and the parameter
,
we get the desired preprocessing time

(the last term is always dominated by the second),
space ,
and query time .

We can reduce to the equitable case as in the proof of Corollary~\ref{cor-cluster}, by decomposing each set into
 subsets.
\end{proof}

\subsection{Application to the Monotone Case and
Online Histogram Queries}

\begin{corollary}\label{cor-monotone-online}
Given two monotone sets  for a constant~
and a parameter ,
we can preprocess in  expected time,
so that we can decide whether any query point  is in 
in  time.

If only  is monotone, the query time is
.
\end{corollary}
\begin{proof}
A monotone set in  is -clustered
for all .  We set  and 
in Corollary~\ref{cor-cluster-online}
and get 
preprocessing time and  query time.
We set  and .

If only  is monotone, we get
 
preprocessing time and  query time.
We set  and .
\end{proof}

If we want to balance the preprocessing cost with the
cost of answering  queries, in the case when 
and  are both monotone, we can set
 and obtain  preprocessing
time and  query time.
These bounds are (predictably) slightly worse than in Corollary~\ref{cor-monotone-offline} in the offline setting.

\begin{corollary}\label{cor-jumble-online}
Given a string  for a constant
alphabet size ,
we can preprocess in  expected time, so that
we can answer histogram queries, i.e.,
decide whether there exists a substring with
exactly  's, \ldots, and  's,
for any query vector ,
in  time.
\end{corollary}
\begin{proof}
We just apply Corollary~\ref{cor-monotone-online} to the same sets
 and  from the proof of Corollary~\ref{cor-jumble-offline}.
\end{proof}


\begin{remark}
The idea of dividing into the cases of low and high popularity cells has
previously been used by Kociumaka et al.~\cite{KRR13} for histogram indexing,
but they were able to obtain only a space/query-time tradeoff,
namely, a data structure with  space
and  query time.
Their data structure requires close to quadratic preprocessing time.
Incidentally, we can easily improve their space/query-time tradeoff:
substituting  and 
in Corollary~\ref{cor-cluster-online} gives
 space and  time.
Setting  and 
then gives  space and 
query time.  All this does not require additive combinatorics
(which helps only in improving the preprocessing time).
\end{remark}

\section{3SUM in Preprocessed Universes}\label{sec:preprocessed}

As one more application, we show that 3SUM can be solved in
truly subquadratic time if the universe has been preprocessed.
(Note, though, that the running time below is subquadratic in the
size of the three given universes , and not
of .)  This version of the problem was
considered by Bansal and Williams~\cite{BW12}, who only obtained time
bounds of the form .

\begin{theorem}\label{thm-preproc1}
Given sets  of size , we can
preprocess in  expected time into a data structure
with  space, so that given any sets ,
, , we can solve 3SUM
for  in  time.
\end{theorem}
\begin{proof}
Our algorithm works as follows:
\begin{description}
\item[Preprocessing:] Apply the BSG
Corollary to .  Store the resulting subsets   and remainder set , and
also store each .

The expected preprocessing time is  by Theorem~\ref{runtime-corollary}.
As , , and ,
the total space usage is .
\end{description}

Now, given , , ,
we do the following:
\begin{description}
\item[Step 1:] For each , if , , and
, then report .
This step takes  time.

\item[Step 2:] For each , apply the FFT Lemma to generate
, which is contained in the superset .  Report those generated elements that are in .
This step takes  expected time.
\end{description}

The total expected time
is .  We set 
to balance the two terms.
The part after preprocessing can be made deterministic,
after including
an extra  cost for preprocessing
the 's for the deterministic version of the FFT Lemma.
\end{proof}

In the above theorem,  is superfluous, since
we may as well take  when solving 3SUM.  In the
next theorem, we show that a slightly weaker
subquadratic time holds if the universe for  is not given in advance.

\begin{theorem}\label{thm-preproc2}
Given sets  of size , we can
preprocess in  expected time into a data structure
with  space, so that given any sets ,
,  of size , we can solve 3SUM
for  in  time
\end{theorem}
\begin{proof}
We incorporate one more idea: dividing into the cases of
low and high popularity.

\begin{description}
\item[Preprocessing:]
Place each  in the \emph{bucket} for
.  Store all these buckets.
Define the \emph{popularity}
of  to be the size of its bucket.
Let  be the set of all
elements  with popularity .  Apply the BSG
Corollary to , store the resulting subsets  and remainder set , and
also store each .

Note that ,
because the total popularity is .
The buckets can be formed in  time and space.
The expected preprocessing time is 
by Theorem~\ref{runtime-corollary}.
As , , and ,
the total space usage is .
\end{description}

Now, given , ,  of size , we do the following:
\begin{description}
\item[Step 0:] For each  of popularity , we look
up the bucket for , and report  if some  in the bucket
has  and .  The search time is  per
element in , for a total of .

\item[Step 1:] For each , if , , and
, then report .
This step takes  time.

\item[Step 2:] For each , apply the FFT Lemma to generate
, which is contained in the superset .  Report those generated elements that are in .
This step takes  expected time.
\end{description}

The total expected time
is .  We set

to balance the three terms.  Again the part after preprocessing
can be made deterministic.
\end{proof}

\begin{remark}
The above theorem holds for real numbers as well, if we assume an unconventional model of computation for the preprocessing algorithm.
To reduce the real case
to the integer case, first sort
 and compute
the smallest distance  between any two elements in  in  time.  Divide the real line into grid intervals
of length .  Without loss of generality, assume that
 and  are aligned.   Replace each real number 
in  and  with the integer .  Then for any , , and ,  iff .  This reduction however requires the floor function and
working with potentially very large integers afterwards.
\end{remark}


\begin{corollary}
Given a vertex-weighted graph  with  vertices, we can decide
whether there exists a (not necessarily induced) copy of
 (the star with four nodes) that has total
weight exactly equal to a given value  in  time.
\end{corollary}
\begin{proof}
Let  denote the weight of .
Preprocess .
Then for each , we solve 3SUM for
 and .
(We can exclude using a number twice or thrice in solving 3SUM by
a standard trick of appending each number with two or three extra bits.)
The  instances of 3SUM can be solved in 
time each, after preprocessing in  expected time.
(The result can be made deterministic, as we can
afford to switch to the slower
-time preprocessing algorithm.)
\end{proof}

The above ``application'' is admittedly contrived but demonstrates
the potential usefulness of solving 3SUM in preprocessed universes.
(Vassilevska Williams and Williams~\cite{VW09} had a more general
result for counting
the number of copies of any constant-size subgraph
with a prescribed total vertex weight, but their bound is not
subcubic for 4-vertex subgraphs.)

For another application, we can reduce 3SUM for -clustered
sets to preprocessing a universe of size 
and solving  3SUM instances.  This provides another
explanation why subquadratic algorithms are possible for certain
parameters of clusterability, although the time bounds obtained by
this indirect approach would not be as good as those from Section~\ref{sec:clustered}.

\section{Proof and Time Complexity of the BSG Theorem/Corollary}\label{sec:Details}

\newcommand{\Deg}{\textrm{deg}}
\newcommand{\cdeg}{\textrm{cdeg}}
\newcommand{\BAD}{\textsc{bad}}
\newcommand{\Ex}{\mathbb{E}}

In this section, we review one proof of the
Balog--Szemer\'{e}di--Gowers Theorem, in order to analyze its
construction time and derive the time bound for the BSG Corollary as given by Theorem~\ref{runtime-corollary}, which has been
used in all our applications.
The proof of BSG theorem we will present is due to
Balog~\cite{Balog07} and independently
 Sudakov et al.~\cite{SSV94}, with minor changes
to make it more amenable to algorithmic analysis.  The proof is
based on a combinatorial lemma purely about graphs (Balog and
Sudakov et al.\ gave essentially identical proofs of this
graph lemma, but the latter described a simpler reduction of the
theorem to the lemma).  The entire proof is short (see Sections \ref{sec-graph} and~\ref{sec-bsg-proof}), uses only
elementary arguments, but has intricate details.

To obtain the
best running time, we incorporate a number of
nontrivial additional ideas.  For our randomized time bound,
we need sampling tricks found in sublinear algorithms.
For our deterministic time bound, we need efficient dynamic updates
of matrix products.


\subsection{A Graph Lemma}\label{sec-graph}

\begin{lemma} {\bf (Graph Lemma)}\ \ Given a bipartite graph , with , there exist  and  such that

\begin{itemize}
\item[\rm (i)]  for every , , there are  length-
       paths from  to , and
\item[\rm (ii)] .
\end{itemize}
\end{lemma}

\begin{proof}

Let  denote the neighborhood of  in graph .  Let .
Let cdeg
(the number of common neighbors of  and , or equivalently,
the number of length- paths from  to ).
The existence of  and  is shown via the following
concise but clever algorithm.

\paragraph{Algorithm:}
\


\begin{quote}
\begin{algorithmic}[1]

 \State  
 \Repeat
 \State pick a random 
 \State 
 \State  cdeg
 \Until{ and }
 \State 
 \State 
 \end{algorithmic}
\end{quote}

\paragraph{Correctness of (i):}

Line~6 guarantees that the undirected graph  with vertex
set  has
at most  edges and thus average degree at most
.
From line~7 it follows that .

Fix  and .
By line~8, there are  vertices  that are
adjacent to .  By line~7, all but  such vertices  satisfy .
By line~5, for each such , there are  length- paths from  to .
Thus, there are 
length- paths from  to .



\paragraph{Correctness of (ii):}

Since , by line 1, deg for every  and hence .
From line   it follows that .

\paragraph{The process ends w.h.p.:}

Line  implies . From line  it follows that

Line  then implies


Define .
Then .
On the other hand, since we always have ,
.
Thus, .

When , we have simultaneously
 and .
Thus, the number of iterations in lines 2--6 is  w.h.p.
\end{proof}


\subsection{Time Complexity of the Graph Lemma}\label{sec-graph-time}


\begin{lemma}\label{lem-graph-runtime}
In the Graph Lemma,  and  can be constructed by

\begin{itemize}
\item
a deterministic algorithm in  time;
\item
a randomized Monte Carlo algorithm in  time
(which is correct w.h.p.), given the adjacency matrix of .
\end{itemize}
\end{lemma}

\begin{proof}\


\paragraph{Deterministic time analysis:}
An obvious deterministic implementation would try
all  in lines 2--5 until we find one that satisfies
the test in line~6.
For line~4, we can compute  for all  in
 total time.
For line~5, we can compute  for all  as follows:
First precompute cdeg for all ; this takes
 time by
computing a matrix product  where
 is the adjacency matrix of 
and  is its transpose.
Let .
For all , precompute count
the number of  with  and ;
this takes  time by
computing a matrix product  where
 is the adjacency matrix of  and
 is the adjacency matrix of .
Then for all , we can compute  by summing
count over all .
Lastly, lines 6--7 take  time.
The total time is , since 
is known to be invariant under permutation of its three arguments.
This is subcubic in .


\paragraph{Randomized time analysis:}
With Monte Carlo randomization, we now show how to improve the running time significantly to near linear in , which is
\emph{sublinear} in the size of the input adjacency matrix.
To achieve sublinear complexity, we modify the algorithm where  and 
are replaced by estimates obtained by \emph{random sampling}.
Let  be a sufficiently small constant and .

The following fact will be useful: given a random sample  of size , for any fixed subset 
we can estimate  by  with additive error 
w.h.p.  This follows from a Chernoff bound.\footnote{
Let .  One version of the Chernoff bound
states that 
(the first term of the min occurs when , the second
when ).  Set  for an arbitrarily large constant .  Then  and , implying that .  Thus,
 w.h.p.  Finally, multiply both sides by .
}
In particular, w.h.p.,  implies , and  implies
.

In line , we draw a random sample  of size .
Then for each , we can replace deg by

with additive error  w.h.p.
This gives  in  time.

Line 4 takes  time.

In line 5, we draw another (independent) random sample  of size .
Then for each , we can replace cdeg by

with additive error  w.h.p.
We do not explicitly construct ; rather, we can \emph{probe} any entry of
the adjacency matrix of  in  time.

In line~6, we draw another random sample  of size .
Then we can replace  by

with additive error  w.h.p.
This takes  probes to , and thus  time.

Recall that the number of iterations for lines 2--6 is  w.h.p.
Thus, the total time for lines 2--6 is .

In line 7, we draw another random sample  of size .
Then for each , we replace
deg by

with additive error  w.h.p.
This takes a total of 
probes to , and thus  time.

In line~8,  we draw one final random sample  of size .
Then for each , we can replace deg by

with additive error  w.h.p.
This takes  time.


The overall running time is .
Since , the first term can
be dropped.
The correctness proofs of (i) and (ii) still go through
after adjusting all constant factors by , if we make
 small enough.
\end{proof}

(We could slightly improve the -dependencies in the randomized time bound by incorporating matrix multiplication,
but they are small enough already that such improvements will not affect the final cost in our applications.)



\subsection{Proof of the BSG Theorem}\label{sec-bsg-proof}




We claim that the subsets  and  from the Graph Lemma
already satisfy the conditions stated in the BSG Theorem.
It suffices to verify condition (i).
To this end, let  and imagine
the following process:

\begin{quote}
\begin{algorithmic}

 \For {each }
 \State   take the lexicographically smallest  with 
 \For {each length- path }
 \State mark the triple 
\EndFor
\EndFor
 \end{algorithmic}
\end{quote}

By (i) in the Graph Lemma, the number of marks is at least
.
On the other hand, observe that each triple  is
marked at most once, because from the triple,
 is determined, from which
 and  are determined, and from which
 and  are determined.
Thus, the number of marks is at most .

Putting the two together, we get
.
\qed

\bigskip
The running time for the BSG Theorem is thus as given in Lemma~\ref{lem-graph-runtime}.

\subsection{Proof of the BSG Corollary}\label{sec-bsg-corollary-proof}

Note that although the BSG Corollary statement
has , we may assume that , since
we can change parameters to ,
, and .
Then , and
.

We can construct the subsets  and
the remainder set  in the BSG Corollary, simply by
repeated applications of the BSG Theorem:

\begin{quote}
\begin{algorithmic}[1]
  \State 
  \For {}
    \State if  then set , , and return
    \State apply the BSG Theorem to 
with parameter 
       to get subsets 
    \State  
  \EndFor
 \end{algorithmic}
\end{quote}
\IGNORE{
In line~3, the parameters in the BSG Theorem are
, , and .
Note that .
}


A naive upper bound on  would be , since  edges are removed in each iteration.
For a more careful analysis, observe that

which implies that

Iterating, we get .
Since , we conclude that .
\qed




\subsection{Time Complexity of the BSG Corollary}\label{sec-bsg-corollary-time}

We now analyze the running time for the BSG Corollary.
We may assume that  without loss of generality.
We may also assume that ,
because otherwise
 and so the trivial solution with
 would work.
We may further assume that ,
because otherwise , and so the trivial
solution with  would already work.
Putting the two assumptions together, we have
, and
so .

The following additional fact will be useful:
.
To see this, observe that

which implies that


This fact implies that the total cost of updating the
adjacency matrix as edges are deleted in line~5 is
at most .
Furthermore, we can construct all the
sumsets  naively, again in total time
.
It thus remains to bound the total cost of the invocations
to the BSG Theorem in line~4.

\paragraph{Deterministic time analysis.}
For the deterministic version of the algorithm,
we can naively upper-bound the total time of all
 iterations by .
We show how to improve the -dependency slightly.
To achieve the speedup, we modify the implementation
of the deterministic algorithm in
the Graph Lemma to support \emph{dynamic updates}
in , namely, deletions of subsets of edges.

Suppose we delete  from .
All the steps in the algorithm can be redone in
at most  time,
except for the computation of the products 
and .  As  is deleted, 
undergoes changes to 
rows of .  We can compute the
change in  by multiplying the change in 
(an  matrix if the all-zero rows are ignored),
with the matrix , in  time.
Now,  also undergoes changes to  columns.
We can similarly update  under these changes
in  time.

The product  itself
undergoes changes in  rows and columns, and so does
the next matrix .  Moreover,  undergoes 
additional row and column deletions where  is the number of
deletions to .  Also,  undergoes
 row changes and  row deletions.
We can update 
under changes to  rows in  in
 time.  Next we can update 
under changes to  columns in  in
 time.   Lastly we can update  under
changes to  rows in  in  time.

Recall that  is invariant under
permutation of its arguments, and .  Moreover, ,
since  undergoes only deletions.
The overall running time is
thus .


According to known upper bounds on rectangular matrix multiplication \cite{HuangPan,LeGall1,LeGall2,Vas},
 for
, which is true
since  by assumption.
So our time bound is
.



\paragraph{Randomized time analysis.}

For the randomized version of the algorithm, we can bound
the total time by 
The third and fourth terms can be dropped, because they are
dominated by the first and second since  by assumption.
In the case , the second term can also be dropped,
because it is dominated by the first
since  by assumption.

Since we can efficiently check whether the solution is correct,
the Monte Carlo algorithm can
be converted into a Las Vegas algorithm.
This completes the proof of Theorem~\ref{runtime-corollary}.
\qed


\bigskip
A gap remains between the deterministic and randomized results.
For constant , we believe it should be possible
to reduce the deterministic running time in the BSG Corollary
to , by replacing
matrix multiplication with FFT computations, but we are currently
unable to bound the -dependency polynomially in the time
bound (roughly
because as we
iterate, the graph  becomes less and less well-structured).


\section{Proof of the FFT Lemma}\label{sec-fft}

To complete the last piece of the puzzle, we now supply a proof of the FFT Lemma.
Note that although the statement of the FFT Lemma has ,
we may assume that , since we can map
each point  to an integer
.


The idea is to use hash to a smaller universe and
then solve the problem on the smaller universe by FFT\@.
As our problem involves sumsets,
we need a hash function that is ``basically'' additive.
The following definition suffices for our purposes: we say
that a function  is \emph{pseudo-additive} if there
is an associated function  such that
 for every .
For example, the function  is pseudo-additive
(with the associated function ).

\newcommand{\collide}{\textrm{collide}}
\newcommand{\HH}{{\cal H}}

We do not need a single perfect hash function (which would be
more time-consuming to generate and may not be pseudo-additive); rather,
it suffices to have a small number
of hash functions that ensure each element in  has no collisions
with respect to at least one hash function.  To be precise,
define .
We say that
a family  of functions is \emph{pseudo-perfect} for 
if for every  there is an  with .
\begin{description}
\item[New Problem:] Construct a family  of  pseudo-additive functions from  to  that is pseudo-perfect for .
\end{description}

\paragraph{Computing , given a pseudo-perfect pseudo-additive family for .}

\newcommand{\hh}{\hat{h}}
Given such an , we can compute  as follows.
For each , we first compute  by FFT in
 time and obtain .
Then for each , we identify an
 with , and if , we report .
The total time of the whole algorithm is 
(assuming that each  and  can be evaluated in constant time).
To prove correctness, just note that
for , we have
 iff  iff ,
assuming that  (since ).


It remains to solve the problem of constructing the hash functions .

\paragraph{A standard randomized construction of
a pseudo-perfect pseudo-additive family.}
With randomization, we can simply pick  random primes  for a sufficiently large constant ,
and put each function  in .

To see why this works, consider a fixed .
For any , the number of primes  with  is equal to the number of prime divisors of

and is at most .  Since the number of primes in 
is at least  for a sufficiently large  by the
prime number theorem, .
Thus, .
So, .  Therefore, the overall failure probability is at most .

Note that we can compute the numbers
 for all  for any given hash function
in linear time after assigning elements to buckets.
In particular, we can verify whether the
construction is correct in  time.
We conclude that there is a Las Vegas algorithm with total expected time  .

\paragraph{A new deterministic construction of a pseudo-perfect pseudo-additive family.}
An obvious way to derandomize the previous method is to try
all primes in  by brute force, but the running time
would be at least
.  Indeed, that was the approach taken by
Amir et al.~\cite{AKP07}.  We describe a faster deterministic
solution by replacing a large prime with multiple smaller
primes, using hash functions of the form
.  Such a function remains pseudo-additive
(with the associated function ).
The idea is to generate the  smaller primes in 
separate rounds.  The algorithm works as follows:

\begin{quote}
\begin{algorithmic}[1]

 \State  
 \While{}
   \For{ to }
     \State pick a prime  with
     \State \ \ \ \

   \EndFor
   \State put  in , and
remove all  with  from 
 \EndWhile
 \end{algorithmic}
\end{quote}

Consider the inner for loop.  Lines 4--5 take 
time by brute force.  But why does  always exist?
Suppose that  have already been chosen, and imagine that  is picked at random.
Let  be a shorthand for .
Consider a fixed  with .
For any fixed ,  by
an argument we have seen earlier.
Thus, 
By Markov's inequality,
.
Since we know from the previous iteration that there
are at least  elements 
with , we then have
.
So there exists  with the stated property.

Line~7 then removes at least  elements.
Hence, the
number of iterations in the outer while loop is
.
Each function 
maps to ,
which can easily be mapped back to one dimension in 
while preserving pseudo-additivity,
for any constant .  We conclude that there
is a deterministic algorithm with total running time
 for an arbitrarily large constant .
This gives .  (More precisely, we can bound
the total deterministic time by  by choosing a nonconstant .)
\qed



\begin{remark}
In the above results, the  notation hides
not only  but also  factors.
For many of our applications,  and so this is
not an issue.  Furthermore, in the randomized version,
we can use a different hash function~\cite{BDP08} to reduce  to
 first, before running the above algorithm.
In the deterministic version, it seems possible to
lower the dependency on  by using recursion.
\end{remark}


\begin{remark}
Our hash function family construction has other applications,
for example, to the \emph{sparse convolution}
problem: given two nonnegative vectors  and , compute
their classical convolution  (where ) in ``output-sensitive'' time,
close to , the number of nonzeros in .
The problem was raised by Muthukrishnan~\cite{Muthukrishnan95},
and previously solved by Cole and Hariharan~\cite{CH02}
with a randomized Las Vegas algorithm in  time.

Let  and .  Then
 is precisely .
If we are given a superset  of  of size , we can solve the problem deterministically
using a pseudo-perfect pseudo-additive family  as follows:
For each ,
precompute the vectors  and  of length  where
 and .
Compute  in  time by FFT\@.
Compute .
Then for each  with ,
set .
To prove correctness, first observe that
.
If , then
.

Cole and Hariharan's algorithm is more general and does not require the superset 
to be given.  It would be interesting to obtain a deterministic
algorithm that similarly avoids .
\end{remark}

\IGNORE{
To be more precise the algorithm proposed runs in time , given a specific family of random hash functions that are a pseudo-additive family for . Their algorithm computes, in our terminology, , where .

If we are given in advance a set  that is a superset of  then it is possible to employ the Cole and Hariharan~\cite{CH02} algorithm deterministically. This is true because one can show that their algorithm works for any pseudo-additive family. The crucial observation of correctness is as follows. Let . The claim is that  for  which satisfies that . The main concern for correctness is that a term  of  is uncomputable because one of  or  has two sources. This cannot be. Say,  and  are non-zero locations in  and  is a non-zero in  such that  and . Since  one of  and  is different than . Wlog, say . However, , a contradiction to . The reverse case can be resolved symmetrically.

Using the deterministic pseudo-additive family for  and this property, the rest of their algorithm is deterministic and works properly in the times mentioned above with a multiplicative factor of .

{\bf Note:}
Cole and Hariharan's algorithm~\cite{CH02} does not require the superset 
to be given.  It would be interesting to obtain a deterministic
algorithm that similarly avoids .

}

\begin{remark}
Another application is
\emph{sparse wildcard matching}.
The problem is: given a pattern and text that are sparse with few non-zeroes, find all alignments where every non-zero pattern element matches the text character aligned with it. Sparse wildcard matching has applications, such as subset matching, tree pattern matching, and geometric pattern matching; see~\cite{CH02}.

Cardoze and Schulman~\cite{CS98} proposed a Monte Carlo near-linear-time algorithm. Cole and Hariharan~\cite{CH02} transformed this into a Las Vegas algorithm. Amir et al.~\cite{AKP07} considered the indexing version of this problem where the text is preprocessed for subsequent pattern queries. In this setting they proposed an  preprocessing time algorithm, where  is the number of non-zeroes in the text. The query time is then . The latter is essentially based upon the construction of a deterministic pseudo-perfect
pseudo-additive family for i.
It can be checked that our new deterministic solution is
applicable here and thus improves their preprocessing time to , yielding the first quasi-linear-time deterministic
algorithm for sparse wildcard matching.
\end{remark}

\IGNORE{


\section{Deterministic algorithms
for computing A+B given T}~\label{determinstic-algo}




 It is possible to
reduce the  preprocessing time to  by
a two-level hashing scheme.  Here's the rough general idea:

At the first level, we find  primes of the order of 
so that each element of  hashes to a bucket of size  for
at least one of these primes.  This can be done in  time
like in the Porat et al. paper by setting up a 2D table with the
elements of T as columns and primes as rows.

Note that we would get into
trouble in the next level because each bucket could then use a
different prime, which would destroy the additivity property we want.
But actually, we don't need to use different primes for the different
buckets!  A random prime (or more accurately a set of  random
primes) of the order of  would work for all buckets
simultaneously.  So we can derandomize in the same way.

In other words, at the second level, we find another set of 
primes of the order of  so that for each first-level
bucket of size , each of its elements hashes to a
second-level sub-bucket of size at most 1 for at least one of
these primes.  This again takes  time by setting up
a 2D table (a single one for all the buckets at the same time)...

The final hash function would be 
where there are  choices for  and for , and for each ,
we can pre-compute one good  and  for .  Of course, the pair
in  can be mapped down to a single integer in 
in the usual way and we have near additivity (the number of possibilities
for  in terms of  becomes 4).

Repeating the idea for multiple but a constant number of levels should
bring the runtime to , and retain near additivity (with
4 increased to larger constants).

}


\section{Final Remarks}

We have given the first truly subquadratic algorithms for
a variety of problems related to 3SUM\@.
Although there is potential for improving the exponents in
all our results, the main contribution is that we have broken the
barrier.

An obvious direction for improvement would be to reduce the
-dependency in the BSG Theorem itself;
our work provides more urgency towards this well-studied combinatorial
problem.  Recently, Schoen~\cite{Sch14} has announced such an improvement of
the BSG Theorem, but it is unclear whether this result will be useful
for our applications for two reasons: First, the time complexity of
this construction needs to be worked out. Second, and more importantly,
Schoen's improvement is for a more basic version of the theorem without ,
and the extension with  is essential to us.

Here is one specific mathematical question that is particularly relevant to us:
\begin{quote}
Given subsets  of
an abelian group of size , we want to
cover 
by bicliques , so as to minimize the cost function
.  (There is no constraint on the
number of bicliques.)  Prove worst-case bounds on the minimum
cost achievable as a function of .
\end{quote}
A bound  follows from the BSG Corollary, simply by
creating  extra ``singleton'' bicliques
to cover , and setting  to minimize
.  An improvement on this
combinatorial bound would have implications to at
least one of our algorithmic applications, notably
Theorem~\ref{thm-preproc1}.

We hope that our work will inspire further applications of additive combinatorics in algorithms.
For instance, we have yet to study special cases of SUM for
larger ;
perhaps some multi-term extension of the BSG Theorem~\cite{BC} would be
useful there.  As an extension of bounded monotone (min,+)
convolution, we may also consider (min,+) matrix
multiplication for the case of integers in  where the
rows and columns satisfy monotonicity or the bounded differences
property.
It would be exciting if the general integer 3SUM or APSP
problem could be solved using tools from
additive combinatorics.

}
\bibliographystyle{plain}
\bibliography{bsg}

\end{document}
