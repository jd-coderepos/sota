\begin{figure*}[ht]
\begin{center}
\includegraphics[width=0.98\textwidth]{figure/fig_framework_J}
\end{center}
   \caption{An overview of our approach. The proposed model consists of three components including multimodal feature generation, cross-modal self-attention (CMSA) and a gated multi-level fusion. Multimodal feature generation are constructed from the visual feature, the spatial coordinate feature and the language feature for each word. For referring segmentation in video, a clip of adjacent frames is used to extract temporal correlation feature by cross-frame self-attention (CFSA) for producing visual feature of the current frame as shown in the gold box. Then the multimodal feature at each level is fed to a cross-modal self-attention module to build long-range dependencies across individual words and spatial regions. Finally, the gated multi-level fusion module combines the features from different levels to produce the final segmentation mask. The red arrows and gold arrows show the input and output for the image and video, respectively.}
\label{fig:overview}
\end{figure*}

\section{The Proposed Model}\label{sec:approach}

The overall architecture of our model is shown in Fig.~\ref{fig:overview}. The input consists of an image or a clip of a video, with a referring expression as the query. We first use a CNN to extract visual feature maps at different levels from the input image (red box at top left in Fig.~\ref{fig:overview}) or a clip of a video enhanced by the cross-frame self-attention module (gold box at bottom left in Fig.~\ref{fig:overview}). Each word in the referring expression is represented as a vector of word embedding. Every word vector is then appended to the visual feature map to produce a multimodal feature map. Thus, there is a multimodal feature map for each word in the referring expression. We then introduce self-attention~\cite{vaswani2017attention} mechanism to combine the feature maps of different words into a cross-modal self-attentive feature map. The self-attentive feature map captures rich information and long-range dependencies of both linguistic and visual information of the inputs. In the end, the self-attentive features from multiple levels are combined via a gating mechanism to produce the final features used for generating the segmentation output. For referring segmentation in videos, the input consists of a video and a referring expression (bottom left in Fig.~\ref{fig:overview}). In this case, we first use a cross-frame self-attention module to capture the temporal correlations between frames within a video clip. We then use the output of this module to build better visual features for frames in a video. Then we apply the remaining pipeline in the architecture to produce the segmentation mask in each frame of the video.


Our model is motivated by several observations. First of all, in order to solve referring segmentation, we typically require detailed information of certain individual words (e.g. words like ``left'', ``right''). Previous works~(e.g.~\cite{hu2016segmentation,li2018referring,shi2018key}) take word vectors as inputs and use LSTM to produce a vector representation of the entire referring expression. The vector representation of the entire  referring expression is then combined with the visual features for referring image segmentation. The potential limitation of this technique is that the vector representation produced by LSTM captures the meaning of the entire referring expression while missing sufficiently detailed information of some individual words needed for the referring image segmentation task. Our model addresses this issue by maintaining word vector for each word in the entire referring expression instead of using LSTM. Therefore, it can better capture more detailed word-level information. Secondly, some previous works~(e.g.~\cite{liu2017recurrent,margffoy2018dynamic}) process each word in the referring expression and concatenate it with visual features to infer the referred object in a sequential order using a recurrent network. The limitation of these methods is that these methods only look at each spatial region of feature maps locally and lack the interaction over long-range spatial regions in global context which is essential for semantic understanding and segmentation. In contrast, our model uses a cross-modal self-attention module that can effectively model long-range dependencies between linguistic and visual modalities. Thirdly, different from~\cite{li2018referring} which adopts ConvLSTM to refine segmentation with multi-scale visual features sequentially, the proposed method employs a novel gated fusion module for combining multi-level self-attentive features. 
Lastly, we introduce the cross-frame self-attention module to build long-range dependencies across frames in the video. It learns temporal correlations from adjacent frames for the current frame and extends our model to video domain.



\subsection{Multimodal Feature Generation}\label{sec:mfg}
We first introduce the generic modules of the proposed model by taking an image as input for simplification, then we will present details of the cross-frame self-attention module and illustrate how to extend our model to video domain. Let  denote an input image and  represent the length of a referring expression. Then each individual word in the referring expression can be denoted as . We use a backbone CNN network to extract visual features from the input image . The feature map is extracted from a specific CNN layer and represented as , where ,  and  are the dimensions of height, width and feature channel, respectively. For ease of presentation, we only use features extracted from one particular CNN layer for now. Later in Sec.~\ref{sec:gf}, we present an extension of our method that uses multi-level features from multiple CNN layers for gated fusion. 

For the language description with  words, each word  is encoded as a one-hot vector to project it into a compact word embedding represented as  by a lookup table. Different from previous methods~\cite{hu2016segmentation,li2018referring,shi2018key} that simply apply LSTM to process the word vectors sequentially and take the output hidden layer to encode the entire description sentence with a sentence vector, we propose to keep the individual word vector and introduce a cross-modal self-attention module to capture long-range correlations between these words and spatial regions in the image. More details will be presented in Sec.~\ref{sec:msa}.



In addition to visual features and word vectors, spatial coordinate features have also been shown to be useful for referring image segmentation~\cite{hu2016segmentation,li2018referring,liu2017recurrent}. Following prior works, we define an -D spatial coordinate feature map at each spatial position using the implementation in~\cite{liu2017recurrent}. Specifically, the first -dimensions of the spatial coordinate feature map encode the normalized horizontal positions. The next -dimensions encode normalized vertical positions. The last -dimensions encode the normalized width and height information of the image.  



After obtaining the visual, linguistic and spatial features, we finally construct a joint multimodal feature representation at each spatial position for each word by concatenating the visual features, word vectors, and spatial coordinate features. Let  be a spatial position in the feature map , i.e. . We use  to denote the ``slice'' of the visual feature vector at a specific spatial location . The spatial coordinate feature of the position  is denoted as . Thus the multimodal feature  can be defined corresponding to the position  and the -th word as follows:

where  denotes the  norm of a vector and  denotes the concatenation of several input vectors. The multimodal feature vector  encodes information about the combination of a specific position  in the image and the -th word  in the referring expression with a total dimension of . We use  to represent the collection of features  for different spatial positions and words. The final dimension of  is .



\subsection{Cross-Modal Self-Attention}\label{sec:msa}
The generated multimodal feature  maintains rich and fine features for each spatial position and word. However, it is quite huge to further process directly and may contain a lot of redundant information. Additionally, the size of  is variable depending on the number of words in the referring expression. It is difficult to directly exploit  to produce the segmentation output. In recent years, the attention mechanism~\cite{hu2017modeling,shi2018key,vaswani2017attention,xu2015show,yu2018mattnet} has been shown to be a powerful technique that can capture important information from raw features in either linguistic or visual representation. Different from above works, we propose a cross-modal self-attention module to jointly exploit attentions over detailed ultimodal features. In particular, inspired by the success of self-attention~\cite{vaswani2017attention,wang2018non}, our designed cross-modal self-attention module can capture long-range dependencies in three aspects including self-attention over language, self-attention over image representation and cross-modal attention between the words in a referring expression and different spatial positions in the input image. The proposed module takes  as the input and produces a feature map that summarizes  after learning the correlation between the language expression and the visual context. 
 
 



Given a multimodal feature vector , the cross-modal self-attention module first produces a set of query, key and value pair by linear transformations as  ,  and   at each spatial position  and the -th word, where  are part of the model parameters to be learned. Each query, key and value is reduced from the high dimension of multimodal features to the dimension of  in our implementation, i.e. , for computation efficiency.

We then compute the cross-modal self-attentive feature  as follows: 

where  is the attention score that takes into account of the correlation between  and any other combinations of spatial position and word .

Then  is transformed back to the same dimension as  via another linear layer and is added element-wise with  to form a residual connection as . This allows the insertion of this module into to the backbone network without breaking its behavior~\cite{he2016deep}. 



\noindent{\bf Word attention-aware pooling:}
In order to collect the attention distribution of each word and produce a  single output feature map for segmentation, we propose the word attention-aware pooling to summarize attention scores by pooling separate multimodal feature of individual word to a feature representation for the whole expression sentence.

From the collection of the attention scores  in Eq.~\ref{eq:self2}, we sum up all combinations for different pairs of  and  for each word . As a result, we can get a vector of length  as the accumulation of attention scores for all words.  The word attention score over all words in the expression  can be calculated by softmax operation as:

Using Eq.~\ref{eq:w_att}, we can obtain word attentions from the intermediate result of the self-attention matrix and use this information to adaptively pool individual word feature vector together for the expression sentence. To be more specific, we multiply each multimodal feature vector  by its corresponding word attention  and accumulate  over all words to result in the final feature representation:

The final feature map  denotes the collection of  at all spatial positions, i.e. .
Unlike previous methods that keep the entire input feature size~\cite{wang2018non} or use average-pooling to generate spatial feature map~\cite{ye2019cross}, the proposed word attention-aware pooling effectively reduces the size of input multimodal feature and fits different numbers of input words present in the sentence. It also obtains informative features by taking advantage of the intermediate result of the self-attention operation. The summarization of the attention score for each word help generate effective multimodal representation of the referring expression from word-level features. Fig.~\ref{fig:nl} illustrates the process of generating cross-modal self-attentive features. 


\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{figure/fig_sa_J}
    \caption{An illustration of the process of generating the cross-modal self-attentive (CMSA) feature from an image and a language expression (``man in yellow shirt''). We use  and  to denote matrix multiplication and element-wise summation, respectively. The softmax operation is performed over each row which indicates the attentions across each visual and language cell in the multimodal feature. The word attention-aware pooling summarizes word-level features for multimodal representation of the referring expression. We also visualize the internal linguistic and spatial representations. Please refer to Sec.~\ref{sec:result} and Sec.~\ref{sec:vis} for more details.}
\label{fig:nl}
\end{figure}




\subsection{Gated Multi-Level Fusion}\label{sec:gf}
The feature representation  obtained from Eq.~\ref{eq:final} is specific to a particular layer in CNN. Previous work~\cite{li2018referring} has shown that fusing features at multiple scales can improve the performance of referring image segmentation. In this section, we introduce a novel gated fusion technique to integrate multi-level features.

Let  be the cross-modal self-attentive feature map at the -th level. Following~\cite{li2018referring}, we use ResNet based DeepLab-101 as the backbone CNN and consider feature maps at three levels () corresponding to ResNet blocks ,  and . Let  be the channel dimension of the visual feature map at the -th level of the network. We use  to indicate the collection of cross-modal self-attentive features  for different spatial locations corresponding to the -th level. Our goal is to fuse the feature maps  () to produce a fused feature map for producing the final segmentation output.
Note that the feature maps   have different channel dimensions at different level . At each level, we apply a  convolutional layer to make the channel dimensions of different levels consistent and result in an output .



For the -th level, we use linear transformations for  to generate a memory gate  and a reset gate  (), respectively. These gates play similar roles to the gates in LSTM. Different from stage-wise memory updates~\cite{cho2014learning,hochreiter1997long}, the computation of gates at each level is decoupled from other levels. The gates at each level control how much the feature at each level contributes to the final fused feature. Each level also has its own contextual controller  which modulates the information flow from other levels to the -th level. This process can be summarized as: 
where  denotes Hadamard product.  is a learnable parameter to adjust the relative ratio of the memory gate which controls information flow of features from different levels  combined to the current level . 

In order to obtain the segmentation mask, we aggregate the feature maps  from the three levels and apply a  convolutional layer followed by the sigmoid function. This sequence of operations outputs a probability map () indicating the likelihood of each pixel being the foreground in the segmentation mask, i.e.:

where  and  denote the sigmoid and  convolution operation, respectively. A binary cross-entropy loss function is defined on the predicted output and the ground-truth segmentation mask  as follows:

where  is the whole set of pixels in the image and  is m-th pixel in it. We use the Adam algorithm~\cite{kingma2014adam} to optimize the loss in Eq.~\ref{eq:final_loss}.



\subsection{Cross-Frame Self-Attention}\label{sec:cfsa}


The task of actor and action video segmentation from a sentence extends a fixed number of actor and action pairs to an open set of vocabulary with rich language descriptions. It is similar to the referring image segmentation, but requires producing a segmentation mask in frames of a video according to the referring query. We use this task to demonstrate the referring segmentation in videos. We propose a cross-frame self-attention module to learn temporal correlations from adjacent  frames for  the current frame. For an input frame , we extract an adjacent clip of the current frame with the interval of .  The CNN backbone is then used to extract features for every frame in the clip and result in a set of features as . Similar to the implementation in Eq.~\ref{eq:self1} and Eq.~\ref{eq:self2}, let  be a feature vector in a frame  and at a specific spatial position . The cross-frame self-attention feature  can be obtained by:

where  is the attention score that represents the correlation between  and any other combinations of frame and spatial position .
Attention-aware pooling of frames is applied over frames to generate the temporal correlation feature  as follows:


The generated feature map  collects  at all spatial positions. Then the temporal correlation feature  is used to update the visual feature  at the frame  as: . The final visual feature   incorporates correlations learned at different positions across all frames in the clip to better capture temporal information for the referring segmentation in videos. Finally,  can be seen as the visual feature  in Sec.~\ref{sec:mfg} to combine with word vectors and spatial features for multimodal feature, and is followed by cross-modal self-attention module in Sec.~\ref{sec:msa} and gated multi-level fusion in Sec.~\ref{sec:gf} for producing segmentation masks.











