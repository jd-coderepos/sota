\section{Results and Discussion}
\label{sec:results}

In this section, we report the experiments carried out to verify the effectiveness of the proposed \gls*{alpr} system.
We first assess the detection stages separately since the regions used in the \gls*{lp} recognition stage are from the detection results, rather than cropped directly from the ground truth.
This is done to provide a realistic evaluation of the entire \gls*{alpr} system, in which well-performed vehicle and \gls*{lp} detections are essential for achieving outstanding recognition results.
Afterward, our system is evaluated in an end-to-end manner and the results achieved are compared with those obtained in previous works and by commercial~systems.

\subsection{Vehicle Detection}
\label{sec:results:vehicle_detection}

In this stage, we employed a confidence threshold of~ (defined empirically) to detect as many vehicles as possible, while avoiding high \gls*{fp} rates and, consequently, a higher cost of the proposed \gls*{alpr} system.
The following parameters were used for training the network:~K iterations~(max batches) and learning rate~=~[\textsuperscript{-},~\textsuperscript{-},~\textsuperscript{-}] with steps at K and K~iterations.

The vehicle detection results are presented in Table~\ref{tab:results:vehicle_detection}.
In the average of five runs, our approach achieved a recall rate of \% and a precision rate of \%. 
It is remarkable that the network was able to correctly detect all vehicles (i.e., recall = \%) in  of the  datasets used in the experiments.
Some detection results are shown in Fig.~\ref{fig:results:veicle_detection_tps}. 
As can be seen, well-located predictions were attained on vehicles of different types and under different conditions.

\begin{table}[!htb]
\centering
\caption{Vehicle detection results achieved across all datasets.}
\label{tab:results:vehicle_detection}
\vspace{1mm}
\resizebox{0.725\columnwidth}{!}{
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Dataset} & \textbf{Precision (\%)} & \textbf{Recall (\%)} \\ \midrule
\caltech &  &  \\
\englishlpd &  &  \\
\stills &  &  \\
\chinese &  &  \\
\aolp &  &  \\
\openalpreu &  &  \\
\ssig &  &  \\
\dataset &  &  \\ \midrule
\textbf{Average} &  &  \\ \bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[!htb]
    \centering
    
    \includegraphics[width=0.99\columnwidth]{Figure08.pdf} 
    
    \vspace{-2mm}
    
    \caption{Some vehicle detection results achieved in distinct datasets. Observe that vehicles of different types were correctly detected regardless of lighting conditions~(daytime and nighttime), occlusion, camera distance, and other factors.}
    \label{fig:results:veicle_detection_tps}
\end{figure}

To the best of our knowledge, with the exception of the preliminary version of this work~\citep{laroca2018robust}, there is no other work in the \gls*{alpr} context where both cars and motorcycles are detected at this stage.
This is of paramount importance since motorcycles are one of the most popular transportation means in metropolitan areas, especially in Asia~\citep{hsu2016comparison}.
Although motorcycle \glspl*{lp} may be correctly located by \gls*{lp} detection approaches that work directly on the frames, they can be detected with fewer false positives if the motorcycles are detected~first~\citep{hsu2015comparison}. 

The precision rates obtained by the network were only not higher due to unlabeled vehicles present in the background of the images, especially in the \aolp and \ssig datasets. 
Three examples are shown in Fig.~\ref{fig:results:vehicle_detection_fps_fns}a.
In Fig.~\ref{fig:results:vehicle_detection_fps_fns}b, we show some of the few cases where our network failed to detect one or more vehicles in the image. 
As can be seen, such cases are challenging since only a small part of each undetected vehicle is visible.

\begin{figure}[!htb]
    \centering
    
    \includegraphics[width=0.99\columnwidth]{Figure09.pdf}
    
    \vspace{-2mm}
    
    \caption{\gls*{fp} and \gls*{fn} predictions obtained in the vehicle detection stage. As can be seen in (a), the predicted \glspl*{fp} are mostly unlabelled vehicles in the background. In (b), one can see that the vehicles not predicted by the network (i.e.,~the \glspl*{fn}) are predominantly those occluded or in the~background.}
    \label{fig:results:vehicle_detection_fps_fns}
\end{figure}

\subsection{License Plate Detection and Layout Classification}
\label{sec:results:lp_detection}

In Table~\ref{tab:results:lp_detection}, we report the results obtained by the modified Fast-YOLOv2 network in the \gls*{lp} detection and layout classification stage.
As we consider only one \gls*{lp} per vehicle image, the precision and recall rates are identical.
The average recall rate obtained in all datasets was \% when disregarding the vehicles not detected in the previous stage and \% when considering the entire test set.
This result is particularly impressive since we considered as incorrect the predictions in which the \gls*{lp}~layout was incorrectly classified with a high confidence value, even in cases where the \gls*{lp} position was predicted~correctly.

\begin{table}[!htb]
	\centering
	
	\captionsetup{position=top}
	
	\caption{Results attained in the \gls*{lp} detection and layout classification stage. The recall rates achieved in all datasets when disregarding the vehicles not detected in the previous stage are presented in~(a), while the recall rates obtained when considering the entire test set are listed in~(b).}
	\label{tab:results:lp_detection}
    \resizebox{0.9\columnwidth}{!}{
	\subfloat[][\label{tab:results:lp_detection_a}]{\begin{tabular}{@{}cc@{}}
		\toprule
		\textbf{Dataset} & \textbf{Recall (\%)} \\ \midrule
		\caltech &  \\
		\englishlpd &  \\
		\stills &  \\
		\chinese &  \\
		\aolp &  \\
		\openalpreu &  \\
		\ssig &  \\
		\dataset &  \\ \midrule
		\textbf{Average} &  \\ \bottomrule
	\end{tabular}} \quad \vline \quad
	\subfloat[][\label{tab:results:lp_detection_b}]{\begin{tabular}{@{}cc@{}}
		\toprule
		\textbf{Dataset} & \textbf{Recall (\%)} \\ \midrule
		\caltech &  \\
		\englishlpd &  \\
		\stills &  \\
		\chinese &  \\
		\aolp &  \\
		\openalpreu &  \\
		\ssig &  \\
		\dataset &  \\ \midrule
		\textbf{Average} &  \\ \bottomrule
	\end{tabular}} \,}
\end{table}

According to Fig.~\ref{fig:results:lp_detection_tps}, the proposed approach was able to successfully detect and classify \glspl*{lp} of various layouts, including those with few examples in the training set such as \glspl*{lp} issued in the U.S. states of Connecticut and Utah, or \glspl*{lp} of motorcycles \minor{registered in the Taiwan~region}.


\begin{figure}[!htb]
    \centering
    
    \includegraphics[width=0.99\columnwidth]{Figure10.pdf}
    
    \vspace{-2mm}
    
    \caption[\glspl*{lp} correctly detected and classified by the proposed approach]{\glspl*{lp} correctly detected and classified by the proposed approach. Observe the robustness for this task regardless of vehicle type, lighting conditions, camera distance, and other factors.}
    \label{fig:results:lp_detection_tps}
    
\end{figure}

\colored{It should be noted that (i)~the \glspl*{lp} may occupy a very small portion of the original image and that (ii)~textual blocks (e.g., phone numbers) on the vehicles or in the background can be confused with \glspl*{lp}.
Therefore, as can be seen in Fig.~\ref{fig:results:lp_detection_without_vehicle_detection}, the vehicle detection stage is \emph{\textbf{crucial}} for the effectiveness of our \gls*{alpr} system, as it helps to prevent both \glspl*{fp} and~\glspl*{fn}.}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfloat]{captionskip=2pt,font={scriptsize}}
    
    \resizebox{\linewidth}{!}{
    \subfloat[][\colored{Examples of results obtained by detecting the \glspl*{lp} directly in the original image.} \label{fig:results:lp_detection_without_vehicle_detection:a}]{
    \includegraphics[height=11.5ex]{imgs/5-results/lp_detection/samples-lp-detection-without-vehicle-detection/no-vehicle-detection/002168_track0097_26_.jpg}
    \includegraphics[height=11.5ex]{imgs/5-results/lp_detection/samples-lp-detection-without-vehicle-detection/no-vehicle-detection/001835_Track36_23_.jpg}
    \includegraphics[height=11.5ex]{imgs/5-results/lp_detection/samples-lp-detection-without-vehicle-detection/no-vehicle-detection/000111_ac_39.jpg}
    } \,
    }
    
    \vspace{1.5mm}
    
    \resizebox{\linewidth}{!}{
    \subfloat[][\colored{Examples of results obtained by detecting the \glspl*{lp} in the vehicle patches.}\label{fig:results:lp_detection_without_vehicle_detection:b}]{
    \includegraphics[height=11.5ex]{imgs/5-results/lp_detection/samples-lp-detection-without-vehicle-detection/full-pipeline/002168_track0097_26_.jpg}
    \includegraphics[height=11.5ex]{imgs/5-results/lp_detection/samples-lp-detection-without-vehicle-detection/full-pipeline/001835_Track36_23_.jpg}
    \includegraphics[height=11.5ex]{imgs/5-results/lp_detection/samples-lp-detection-without-vehicle-detection/full-pipeline/000111_ac_39.jpg}
    } \,
    } 
    
    \vspace{-1mm}
    
    \caption{\colored{Comparison of the results achieved by detecting/classifying the \glspl*{lp} directly in the original image~(a) and in the vehicle regions predicted in the vehicle detection stage~(b).}}
    \label{fig:results:lp_detection_without_vehicle_detection}
\end{figure}

Some images where our network failed either to detect the \gls*{lp} or to classify the \gls*{lp} layout are shown in Fig.~\ref{fig:results:lp_detection_wrong}. 
As can be seen in Fig.~\ref{fig:results:lp_detection_wrong}a, our network failed to detect the \gls*{lp} in cases where there is a textual block very similar to an \gls*{lp} in the vehicle patch, or even when the \gls*{lp} of another vehicle appears within the patch (a single case in our experiments).
This is due to the fact that one vehicle can be almost totally occluded by another.
Regarding the errors in which the \gls*{lp} layout was misclassified, they occurred mainly in cases where the \gls*{lp} is considerably similar to \gls*{lp} of other layouts. 
For example, the left image in Fig.~\ref{fig:results:lp_detection_wrong}b shows a European \gls*{lp} (which has exactly the same colors and number of characters as standard Chinese \glspl*{lp}) incorrectly classified as Chinese.

\begin{figure}[!htb]
    \centering
    
    \includegraphics[width=0.99\columnwidth]{Figure11.pdf}
    
    \vspace{-2mm}
    
    \caption[Some images in which our network failed either to detect the \gls*{lp} or to classify the \gls*{lp} layout]{Some images in which our network failed either to detect the \gls*{lp} or to classify the \gls*{lp} layout.}
    \label{fig:results:lp_detection_wrong}
\end{figure}

It is important to note that it is still possible to correctly recognize the characters in some cases where our network has failed at this stage. For example, in the right image in Fig.~\ref{fig:results:lp_detection_wrong}a, the detected region contains exactly the same text as the ground truth (i.e., the~\gls*{lp}). Moreover, a Brazilian~\gls*{lp} classified as European (e.g., the middle image in Fig.~\ref{fig:results:lp_detection_wrong}b) can still be correctly recognized in the next stage since the only post-processing rule we apply to European~\glspl*{lp} is that they have between  and ~characters.

As mentioned earlier, in this stage we disabled the color-related data augmentation of the Darknet framework. 
In this way, we eliminated more than half of the layout classification errors obtained when the model was trained using images with changed colors. 
We believe this is due to the fact that the network leverages color information (which may be distorted with some data augmentation approaches) for layout classification, as well as other characteristics such as the position of the characters and symbols on the~\gls*{lp}.

\subsection{License Plate Recognition (end-to-end)}
\label{sec:results:lp_recognition}

\begin{table*}[!htb]
\centering
\setlength{\tabcolsep}{9pt}
\caption{Recognition rates (\%) obtained by the proposed system, \colored{modified versions of our system}, previous works, and commercial systems in all datasets used in our experiments. To the best of our knowledge, in the literature, only algorithms for \gls*{lp} detection and character segmentation were evaluated in the \caltech, \stills and \chinese datasets. 
Therefore, our approaches are compared only with the commercial systems in these~datasets.
}
\vspace{1mm}

\label{tab:results:lp_recognition}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\diagbox[trim=l,trim=r,innerrightsep=-2.5pt,font=\footnotesize]{Dataset}{Approach} & \citep{panahi2017accurate} & \citep{zhuang2018towards} & \citep{silva2018license} & \colored{\citep{silva2020realtime}} & \citep{laroca2018robust} & Sighthound & OpenALPR & \colored{\begin{tabular}[c]{@{}c@{}}No Vehicle\\\phantom{i}Detection\footnotesize{}\end{tabular}} &  \colored{\begin{tabular}[c]{@{}c@{}}No Layout\\\phantom{i}Classification\footnotesize{}\end{tabular}} & Proposed \\ \midrule
\caltech &  &  &  & \colored{} &  &  &  & \colored{} &  &  \\
\englishlpd &  &  &  & \colored{} &  &  &  & \colored{} &  &  \\
\stills &  &  &  & \colored{} &  &  &  & \colored{} &  &  \\
\chinese &  &  &  & \colored{} &  &  &  & \colored{} &  &  \\
\aolp &  & \footnotesize{} &  & \colored{} &  &  &  & \colored{} &  &  \\
\openalpreu &  &  &  & \colored{} &  &  &  & \colored{} &  &  \\
\ssig &  &  &  & \colored{} &  &  &  & \colored{} &  &  \\
\dataset &  &  &  & \colored{} &  &  &  & \colored{} &  &  \\ \midrule
Average &  &  &  & \colored{} &  &  &  & \colored{} & \colored{} &  \\ \bottomrule \0.5ex]
		\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\gls*{lp} Detection and\1.8ex]
		\gls*{lp} Recognition & CR-NET &  &  \\ \midrule
		\textbf{End-to-end} &  &  &  \\ \bottomrule
	\end{tabular}
	}
\end{table}

It should be noted that practically all images from the datasets used in our experiments contain only one labeled vehicle.
However, to perform a more realistic analysis of the execution time, we listed in Table~\ref{tab:times_more_lps} the time required for the proposed system to process images assuming that there is a certain number of vehicles in every image (note that vehicle detection is performed only once, regardless of the number of vehicles in the image).
According to the results, our system can process more than  \gls*{fps} even when there are  vehicles in the~scene.
This information is relevant since some \gls*{alpr} approaches, including the one proposed in our previous work~\citep{laroca2018robust}, can only run in real time if there is at most one vehicle in the~scene.

\begin{table}[!htb]
    \centering
    \caption{Execution times considering that there is a certain number of vehicles in every image.}
    \label{tab:times_more_lps}
    
    \vspace{1mm}
    
    \resizebox{0.45\columnwidth}{!}{ \begin{tabular}{@{}ccc@{}}
    \toprule
    \# Vehicles & Time~(ms) & \gls*{fps} \\ \midrule
     &  &  \\
     &  &  \\
     &  &  \\
     &  &  \-6pt]
     &  &  \\ \bottomrule
    \end{tabular}
    }\end{table}

The proposed approach achieved an outstanding trade-off between accuracy and speed, unlike others recently proposed in the literature. For example, the methods proposed in~\citep{silva2017realtime,goncalves2018realtime} are capable of processing more images per second than our system but reached poor recognition rates (i.e., below \%) in at least one dataset in which they were evaluated. On the other hand, impressive results were achieved on different scenarios in~\citep{li2018toward,li2018reading,silva2018license}.
However, the methods presented in these works are computationally expensive and cannot be applied in real time.
The Sighthound and OpenALPR commercial systems do not report the execution time.

\minor{We remark that real-time processing may be affected by many factors in practice.
For example, we measured our system's execution time when there was no other process consuming machine resources significantly. 
This is the standard procedure in the literature since it enables/facilitates the comparison of different approaches, despite the fact that it may not accurately represent some real-world applications, where other tasks must be performed simultaneously.
Some other factors that may affect real-time processing are the time it takes to transfer the image from the camera to the processing unit, hardware characteristics (e.g., CPU architecture, read/write speeds, and data transfer time between CPU and GPUs), and the versions of the frameworks and libraries used (e.g., OpenCV, Darknet and~CUDA).
}

\colored{
It is important to emphasize that, according to our experiments, the proposed \gls*{alpr} system is robust under different conditions while being efficient essentially due to the meticulous way in which we designed, optimized and combined its different parts, always seeking the best trade-off between accuracy and speed.
All strategies adopted are very important in some way for the robustness and/or efficiency of the proposed approach, and no specific part contributes more than the others in every scenario.
For example, as shown in Table~\ref{tab:results:lp_recognition} and Fig.~\ref{fig:results:lp_detection_without_vehicle_detection}, vehicle detection mainly helps to prevent false positives and false~negatives on complex scenarios, while layout classification (along with heuristic rules) mainly improves the recognition of \glspl*{lp} with a fixed number of characters and/or fixed positions for letters and~digits.
In the same way, both tasks and also \gls*{lp} recognition would not have been accomplished so successfully, or so efficiently, if not for careful modifications to the networks and exploration of data augmentation techniques (all details were given in Section~\ref{sec:proposed}).
}

\colored{
\subsection{Evaluation by Dataset}
\label{sec:results:detailed}
}

\colored{
In this section, we briefly discuss the results achieved by both the baselines and our \gls*{alpr} system on each dataset individually, striving to clearly identify what types of errors are generally made by each system.
For each dataset, we show some qualitative results obtained by the commercial systems and the proposed approach, since we know exactly which images/\glspl*{lp} these systems recognized correctly or not.
In the \openalpreu, \ssig and \dataset datasets, we also show some predictions obtained by the methods introduced in~\cite{silva2018license,laroca2018robust}, as their architectures and pre-trained weights were made publicly available by the respective authors.
Note that, as we are comparing different \gls*{alpr} systems, the \gls*{lp} images shown in this section were cropped directly from the ground~truth.
We focus on the recognition stage for visualization purposes and also because we consider this stage as the current bottleneck of \gls*{alpr} systems.
However, we pointed out cases where one or more systems did not return any predictions on multiple images from a given dataset, which may indicate that the \glspl*{lp} were not properly detected.\\
}

\colored{
\noindent
\textbf{\caltech~\cite{caltech}:} this is the dataset with fewer images for testing (only~).
Hence, a single image recognized incorrectly reduces the accuracy of the system being evaluated by more than \%.
By carefully analyzing the results, we found out that there is a challenging image in this dataset that neither the commercial systems nor the proposed system could correctly recognize.
Note that, in some executions, this image was not in the test subset, which explains the mean recognition rates above \% attained by both our system and OpenALPR.
As illustrated in Fig.~\ref{fig:detailed-caltech}, while OpenALPR only made mistakes in that image, the proposed system failed in another image as well (where an~`F' looks like an~`E' due to the \gls*{lp}'s frame), and Sighthound failed in some other \glspl*{lp} due to very similar characters (e.g.,~`'~and~`I') or false positives.\\
}

\begin{figure}[!htb]
    
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{\textcolor{red}{L}3WAZ301}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{\phantom{L}3WAZ301}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{\phantom{L}3WAZ301}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000005_image_0016.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{\textcolor{red}{I}KCM356}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{1KCM356}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{1KCM356}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000023_image_0067.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{2MFF674}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{2MFF674}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{2MF\textcolor{red}{E}674}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000012_image_0033.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{VZW818}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{VZW818}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{VZW818}]{
    		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000036_image_0089.jpg}} \hspace{1.5mm}
	}
	
\vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{997JDG}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{997JDG}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{997JDG}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000034_image_0092.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{4CY\textcolor{red}{2}275}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{4CYE275}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{4CYE275}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000002_image_0006.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{VFY818}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{VFY818}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{VFY818}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000046_image_0110.jpg}} \, \subfloat[][\centering \phantom{aa} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{\phantom{a}\textcolor{red}{F118}\phantom{a}}\phantom{i} \hspace{\textwidth} \phantom{aa} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{\phantom{aa}\textcolor{red}{n/a}\phantom{a}}\phantom{i} \hspace{\textwidth} \phantom{aa} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{\phantom{a}\textcolor{red}{IR69}\phantom{a}}\phantom{i}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/caltech/000039_image_0109.jpg}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on \caltech~\cite{caltech} by Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi} and the proposed system.}}
    \label{fig:detailed-caltech}
\end{figure}

\colored{
\noindent \textbf{\englishlpd~\cite{englishlpd}:} this dataset has several \gls*{lp} layouts and different types of vehicles such as cars, buses and trucks.
Panahi \& Gholampour~\cite{panahi2017accurate} reported a recognition rate of \% in this dataset, however, their method was executed only once and the images used for testing were not specified.
As can be seen in Table~\ref{tab:detailed-english}, using the same number of test images, our method achieved recognition rates above \% in two out of five executions (Sighthound also surpassed \% in one run).
In this sense, we consider that our system is as robust as the one presented in~\cite{panahi2017accurate}.
According to Fig.~\ref{fig:detailed-english}, neither the commercial systems nor the proposed system had difficulty in recognizing \glspl*{lp} with two rows of characters in this dataset.
Instead, as there are many different \gls*{lp} layouts in Europe and thus the number of characters on each \gls*{lp} is not fixed, most errors refer to a character being lost (i.e., false negatives) or, conversely, a non-existent character being predicted (i.e., false positives).
The low recognition rates achieved by OpenALPR are due to the fact that it did not return any predictions in some cases (as if there were no vehicles/\glspl*{lp} in the image).
In this sense, we conjecture that OpenALPR only returns predictions obtained with a high confidence value and that it is not as well trained for European \glspl*{lp} as it is for American/Brazilian~ones.
}

\begin{table}[!htb]
\centering
\setlength{\tabcolsep}{8pt}

\caption{\colored{Recognition rates (\%) achieved by Panahi \& Gholampour~\cite{panahi2017accurate}, Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi}, and our system on \englishlpd~\cite{englishlpd}.}}
\label{tab:detailed-english}
\vspace{1mm}
\colored{
\resizebox{0.65\linewidth}{!}{\begin{tabular}{@{}ccccc@{}}
\toprule
Run & \cite{panahi2017accurate} & \cite{masood2017sighthound} & \cite{openalprapi} & Proposed \\ \midrule
\#    &      &        &      &        \\
\#    &      &        &      &        \\
\#    &      &           &          &        \\
\#    &      &        &      &        \\
\#    &           &            &          &        \\ \midrule
Average &          &            &          &         \\ \bottomrule
\end{tabular}
}}
\end{table}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{ZG200ID}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{ZG200ID}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{ZG200ID}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000036_070603_P6070093.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{ZG594TS\phantom{H}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{ZG594TS\phantom{H}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{ZG594TS\textcolor{red}{H}}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000102_280503_P5280124.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{ZG511S\textcolor{red}{-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{ZG511\textcolor{red}{9-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{ZG511SF}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000037_141002_Pa140041.jpg}} \, \subfloat[][\centering \phantom{a} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{AHV\textcolor{red}{8}9002}\phantom{i} \hspace{\textwidth} \phantom{a} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{\phantom{aa}\textcolor{red}{n/a}\phantom{aaa}}\phantom{i} \hspace{\textwidth} \phantom{a} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{AHV\textcolor{red}{8}9002}\phantom{i}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000035_070603_P6070085.jpg}} \hspace{1.5mm}
	}
	
\vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{VU279A\textcolor{red}{-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{VU279AE}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{VU279AE}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000079_280503_P5280008.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{HGAS1802}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{HGAS180\textcolor{red}{-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{HGAS1802}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000007_040603_P6040062.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{RI393BD}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{RI393BD}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{RI393BD}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000090_280503_P5280077.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{VZ876C\textcolor{red}{T}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{VZ876C\textcolor{red}{T}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{V\textcolor{red}{2}876C\textcolor{red}{1}}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/englishlpd/000071_180902_P9190060.jpg}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on \englishlpd~\cite{englishlpd} by Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi} and the proposed system.}}
    \label{fig:detailed-english}
\end{figure}

\colored{
\noindent \textbf{\stills~\cite{ucsd}:} as \caltech, the \stills dataset also has few test images (only ).
Despite containing \glspl*{lp} from distinct U.S. states (i.e., different \gls*{lp} layouts) and under several lighting conditions, all \gls*{alpr} systems evaluated by us achieved excellent results in this dataset.
More specifically, both Sighthound and OpenALPR failed in just one image (interestingly, not in the same one).
This is another indication that these commercial systems are very well trained for American \glspl*{lp}.
Also very robustly, our system failed in just two images \underline{over  runs}, remarkably recognizing all  images correctly in one of them.
All images in which at least one system failed, as well as other representative ones, are shown in Fig.~\ref{fig:detailed-stills}.\\
}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}  \texttt{C\textcolor{red}{N}C3951}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{CKC3951}\hspace{\textwidth}  \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{CKC3951}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000006_cars062.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{RNM25X}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{RNM25X}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{RNM25X}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000058_cars4_102.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{1TM115\phantom{a}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{1TM115\phantom{a}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{1TM115\textcolor{red}{I}}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000020_cars2_058.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{5CGP522}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{5CGP522}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{5CGP522}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000033_cars4_006.jpg}} \hspace{1.5mm}
	}
	
\vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}  \texttt{3J66282}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{3J66282}\hspace{\textwidth}  \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{3J66282}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000005_cars053.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{4NFU116}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{4NF\textcolor{red}{-}116}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{4NFU116}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000012_cars2_022.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{AHA6497}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{AHA6497}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{AHA6497}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000052_cars4_079.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{4NIU770}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{4NIU770}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{4N\textcolor{red}{T}U770}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/stills/000016_cars2_041.jpg}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on \stills~\cite{ucsd} by Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi} and the proposed system.}}
    \label{fig:detailed-stills}
\end{figure}

\colored{
\noindent \textbf{\chinese~\cite{zhou2012principal}:} this dataset contains both images captured by the authors and downloaded from the Internet. We used  images for testing in each run.
An important feature of \chinese is that it has several images in which the \glspl*{lp} are tilted or inclined, as shown in Fig.~\ref{fig:detailed-chinese}.
In fact, most of the prediction errors obtained by commercial systems were in such images.
Our system, on the other hand, handled tilted/inclined \glspl*{lp} well and mostly failed in cases where one character become
very similar to another due to the \gls*{lp}
frame, shadows, blur, etc. 
It should be noted that Sighthound~(\%) misclassified the Chinese character (see Section~\ref{sec:proposed:lp_recognition} for details) as an English letter on some occasions.
This kind of recognition error was rarely made by the proposed system~(\%) and OpenALPR~(\%).
}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{ALA8\textcolor{red}{-{}-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{A\textcolor{red}{I}A82I}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{ALA82I}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000095_camera_SDC13400.JPG}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{ADT444}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{AD\textcolor{red}{I}444}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{AD\textcolor{red}{I}444}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000101_internet_1.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{\textcolor{red}{B}ABII57}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{\phantom{a}ABII57}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{\phantom{a}ABII57}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000052_camera_SDC13270.JPG}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{AK0473}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{AK0473}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{AK04\textcolor{red}{I}3}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000079_camera_SDC13323.JPG}} \hspace{1.5mm}
	}
	
\vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{C44444}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{\textcolor{red}{G}44444}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{C44444}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000105_internet_13.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{AEI4\textcolor{red}{L}I}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{AEI4II}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{AEI4II}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000031_camera_IMG_2687.JPG}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{\textcolor{red}{I}A6ITII}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}} \texttt{\phantom{a}A6ITI\textcolor{red}{7}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{\phantom{a}A6ITII}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000156_internet_97.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{\textcolor{red}{R}L0020I}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{\phantom{a}L0020\textcolor{red}{N}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{\textcolor{red}{R}L0020\textcolor{red}{-}}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/chinese/000139_internet_65.jpg}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on \chinese~\cite{zhou2012principal} by Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi} and the proposed system.}}
    \label{fig:detailed-chinese}
\end{figure}

\colored{
\noindent \textbf{\aolp~\cite{hsu2013application}:} this dataset has images collected \minor{in the Taiwan region} from front/rear views of vehicles and various locations, time, traffic, and weather conditions.
In our experiments,  images were used for testing in each run.
As OpenALPR does not support \minor{\glspl*{lp} from the Taiwan region} (as pointed out in Section~\ref{sec:results:overall}), here we compare the results obtained by Sighthound~(\%) and the proposed system~(\%).
As shown in Fig.~\ref{fig:detailed-aolp}, different from what we expected, both systems dealt well with inclined \glspl*{lp} in this dataset.
While our system failed mostly in challenging cases, such as very similar characters (`E'~and~`F', `B'~and~`', etc.), Sighthound also failed in simpler cases where our system had no difficulty in correctly recognizing all \gls*{lp}~characters.\\
}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{C\textcolor{red}{8}8117}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{C38117}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000462_le_721.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{RE9302}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{R\textcolor{red}{F}9302}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000212_ac_675.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{8695LS}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{8695LS}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000558_rp_320.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{Y\textcolor{red}{B}8096}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{Y\textcolor{red}{B}8096}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000136_ac_495.jpg}} \hspace{1.5mm}
	}
	
\vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
	\subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{\textcolor{red}{I}51735}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{\textcolor{red}{I}51735}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000013_ac_15.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{9J3167}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{9J3167}]{
	\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000493_rp_118.jpg}} \, \subfloat[][\centering \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}} \texttt{\textcolor{red}{1}2N4202}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Ours:}} \texttt{\phantom{a}2N4202}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000112_ac_424.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{\textcolor{red}{D}750J0}\hspace{\textwidth} \phantom{\,}  \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{0750J0}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/aolp/000590_rp_390.jpg}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on the \aolp~\cite{hsu2013application} dataset by Sighthound~\cite{masood2017sighthound} and the proposed system.}}
    \label{fig:detailed-aolp}
\end{figure}

\colored{
\noindent \textbf{\openalpreu~\cite{openalpreu}:} this dataset consists of  testing images, generally with the vehicle well centered and occupying a large portion of the image.
Therefore, both our \gls*{alpr} system and the baselines performed well on this dataset.
Over five executions, the proposed system~(\%) failed in just  different images, while the baselines failed in a few more.
Surprisingly, as can be seen in Fig.~\ref{fig:detailed-openalpreu}, the systems made distinct recognition errors and we were unable to find an explicit pattern among the incorrect predictions made by each of them.
In this sense, we believe that the errors in this dataset are mainly due to the great variability in the fonts of the characters in different \gls*{lp} layouts.
As an example, note in Fig.~\ref{fig:detailed-openalpreu} that the `W' character varies considerably depending on the \gls*{lp}~layout.
}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{BSE5579}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{BSE5579}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{BSE5579}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{BSE5579}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000077_test_066.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{BA2\textcolor{red}{2}8IM}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{BA268IM}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{\textcolor{red}{3}A268IM}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{BA268IM}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000053_test_042.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{VW4X4WP}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{VW4\textcolor{red}{7}4WP}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{VW4X4WP}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{VW4X4WP}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000005_eu07.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{GWAGEN}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{\textcolor{red}{-}WAGEN}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{G\textcolor{red}{VN}AGEN}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{G\textcolor{red}{VN}AGEN}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000002_eu02.jpg}} \hspace{1.5mm}
	}
	
\vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
\subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{WSQ3021}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{WS\textcolor{red}{0}3021}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{WSQ302\textcolor{red}{-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{WSQ3021}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000006_eu08.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{RK60\textcolor{red}{0}AB}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{RK605AB}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{RK605AB}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{RK605AB}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000062_test_051.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{1Z7\phantom{a}5233}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{1Z7\phantom{a}5233}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{1Z7\phantom{a}5233}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{1Z7\textcolor{red}{8}5233}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000082_test_071.jpg}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{RK161AG}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{\textcolor{red}{B}K161AG}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{RK161AG}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{RK161AG}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/openalpr-eu/000059_test_048.jpg}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on \openalpreu~\cite{openalpreu} by Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi}, Silva \& Jung~\cite{silva2018license}, and the proposed system.}}
    \label{fig:detailed-openalpreu}
\end{figure}

\colored{
\noindent \textbf{\ssig~\cite{goncalves2016benchmark}:} this dataset contains  images for testing.
All images were taken with a static camera on the campus of a Brazilian university.
Here, the proposed system achieved a high recognition rate of~\%, outperforming the best baseline by~\%.
As shown in Fig.~\ref{fig:detailed-ssig}, as well as in other datasets, our system failed mostly in challenging cases where one character becomes very similar to another due to motion blur, the position of the camera, and other factors.
This was also the reason for most of the errors made by OpenALPR and the system designed by Silva \& Jung~\cite{silva2018license}.
However, these systems also struggled to correctly recognize degraded~\glspl*{lp} in which some characters are distorted or erased.
In addition to such errors, Sighthound predicted  characters instead of  on several occasions, probably because it does not take advantage of information regarding the \gls*{lp}~layout.
Lastly, the preliminary version of our approach~\cite{laroca2018robust}, where the \gls*{lp} characters are first segmented and then individually recognized, had difficulty segmenting the characters `I'~and~`' in some cases, which resulted in recognition~errors.
}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{HIM2848}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{HIM2848}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{HIM2848}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{HIM2848}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{HIM2848}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000702_Track35_06_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{OQ\textcolor{red}{D}541\textcolor{red}{D}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{\textcolor{red}{D}QO5410}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{O\textcolor{red}{O}O5410}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{O\textcolor{red}{O}O5410}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{O\textcolor{red}{O}O5410}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000742_Track37_05_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{H\textcolor{red}{O}R8361}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{HDR8361}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{HDR8361}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{HDR8361}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{H\textcolor{red}{O}R8361}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000508_Track25_02_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{HJN208\textcolor{red}{-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{\textcolor{red}{R}JN2081}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{H\textcolor{red}{L}N2081}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{HJN208\textcolor{red}{-}}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{HJN2081}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000754_Track38_10_.png}} \hspace{1.5mm}
	}
	
    \vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
	\subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{\textcolor{red}{D}GQ6370}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{OG\textcolor{red}{O}6370}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{OG\textcolor{red}{O}6370}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{OG\textcolor{red}{O}6370}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{OG\textcolor{red}{O}6370}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000586_Track29_08_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{\textcolor{red}{D}XH86\textcolor{red}{J}7}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{OXH8617}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{OXH8617}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{OXH86\textcolor{red}{-}7}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{OXH8617}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000378_Track19_04_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{\textcolor{red}{-}TV7556}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{\textcolor{red}{K}TV7556}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{\textcolor{red}{4}TV7556}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{A\textcolor{red}{I}V7556}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{ATV7556}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000095_Track6_01_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{GMF\textcolor{red}{-}862}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{GMF2862}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{10}\cite{silva2018license}:}}~\texttt{G\textcolor{red}{N}F2862}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{G\textcolor{red}{N}F2862}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{GMF2862}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ssig/000428_Track22_02_.png}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on \ssig~\cite{goncalves2016benchmark} by Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi}, Silva \& Jung~\cite{silva2018license}, the preliminary version of our approach~\cite{laroca2018robust}, and the proposed system.}}
    \label{fig:detailed-ssig}
\end{figure}

 \vspace{1.5mm}
\colored{
\noindent \textbf{\dataset~\cite{laroca2018robust}:} this challenging dataset includes  testing images acquired from inside a vehicle driving through regular traffic in an urban environment, that is, both the vehicles and the camera (inside another vehicle) were moving and most \glspl*{lp} occupy a very small region of the image.
In this sense, the commercial systems did not return any prediction in some images from this dataset where the vehicles are far from the camera.
Regarding the recognition errors, they are very similar to those observed in the \ssig dataset.
Sighthound often confused similar letters and digits, while segmentation failures impaired the results obtained by the approach proposed in our previous work~\cite{laroca2018robust}.
According to Fig.~\ref{fig:detailed-ufpralpr}, the images were collected under different lighting conditions and the four \gls*{alpr} systems found it difficult to correctly recognize certain \glspl*{lp} with shadows or high exposure.
It should be noted that motorcycle \glspl*{lp} (those with two rows of characters) are challenging in nature, as the characters are smaller and closely spaced.
In this context, some authors have evaluated their methods, which do not work for motorcycles or for \glspl*{lp} with two rows of characters, exclusively in images containing cars, overlooking those with motorcycles~\cite{goncalves2018realtime,silva2020realtime}.
}

\begin{figure}[!htb]
    \centering
    \captionsetup[subfigure]{labelformat=empty,font={scriptsize}}


    \resizebox{\linewidth}{!}{
	\subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{ABN8528}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{ABN8528}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{ABN8528}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{ABN8528}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/000458_track0106_08_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{AM\textcolor{red}{DD}663}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{AM\textcolor{red}{D}0663}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{AMO0663}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{AMO0663}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/001356_track0136_06_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{ATT4025}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{ATT4025}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{AT\textcolor{red}{U}4025}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{ATT402\textcolor{red}{6}}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/001205_track0131_05_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{AUG\textcolor{red}{-}936}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{A\textcolor{red}{D}G0936}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{AU\textcolor{red}{S}0936}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{AUG0936}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/001419_track0138_09_.png}} \hspace{1.5mm}
	}
	
\vspace{2.25mm}
	
	\resizebox{\linewidth}{!}{
    \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{BBO851\textcolor{red}{-}}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{BBO8514}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{BBO85\textcolor{red}{-}4}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{BBO8514}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/000591_track0110_21_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{IO\textcolor{red}{2}3616}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{IOZ3616}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{IOZ3616}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{IOZ3616}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/000864_track0119_24_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{A\textcolor{red}{O}W1379}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{\textcolor{red}{N}QW1379}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{\textcolor{red}{-}OW\textcolor{red}{7}379}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{A\textcolor{red}{O}W1379}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/001083_track0127_03_.png}} \, \subfloat[][\centering  \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{masood2017sighthound}:}}~\texttt{AIQ\textcolor{red}{-Q}56}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{openalprapi}:}}~\texttt{A\textcolor{red}{T}Q1056}\hspace{\textwidth} \phantom{\,}
    \resizebox{\adj}{!}{\textbf{\phantom{1}\cite{laroca2018robust}:}}~\texttt{A\textcolor{red}{UC}1056}\hspace{\textwidth} \phantom{\,} \resizebox{\adj}{!}{\textbf{Ours:}}~\texttt{A\textcolor{red}{T}Q1056}]{
		\includegraphics[width=0.23\linewidth]{imgs/5-results/detailed/ufpr/000602_track0111_02_.png}} \hspace{1.5mm}
	}
	
	\vspace{-0.5mm}

    \caption{\colored{Some qualitative results obtained on \dataset~\cite{laroca2018robust} by Sighthound~\cite{masood2017sighthound}, OpenALPR~\cite{openalprapi}, the preliminary version of our approach~\cite{laroca2018robust}, and the proposed system.}}
    \label{fig:detailed-ufpralpr}
\end{figure}

\vspace{1.5mm}
\colored{
\noindent \textbf{Final remarks:} while being able to process in real time, the proposed system is also capable of correctly recognizing \glspl*{lp} from several countries/regions in images taken under different conditions.
In general, our \gls*{alpr} system failed in challenging cases where one character becomes very similar to another due to factors such as shadows and occlusions (note that some of the baselines also failed in most of these cases).
We believe that vehicle information, such as make and model, can be explored in our system's pipeline in order to make it even more robust and prevent errors in such~cases.
}