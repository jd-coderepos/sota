\documentclass[letterpaper]{article} \usepackage{aaai23}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \usepackage{algorithm}
\usepackage{algorithmic} 

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/TemplateVersion (2023.1)
}



\setcounter{secnumdepth}{0} 





\title{GRPE: Relative Positional Encoding for Graph Transformer}
\author{
\textsuperscript{\rm 1}Wonpyo Park\equalcontrib, \textsuperscript{\rm 2}Woonggi Chang\equalcontrib, \textsuperscript{\rm 2}Donggeon Lee, \textsuperscript{\rm 2}Juntae Kim, \textsuperscript{\rm 1}Seung-won Hwang
}
\affiliations{
\textsuperscript{\rm 1}Seoul National University, \textsuperscript{\rm 2}Standigm
}

\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author {
First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
\textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi

\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}



\usepackage{bibentry}
\usepackage{amsmath} \usepackage{amsfonts} 
\usepackage{booktabs}
\usepackage{subcaption}



\begin{document}

\maketitle
\begin{abstract}

We propose a novel positional encoding for learning graph on Transformer architecture.
Existing approaches either linearize a graph to encode absolute position in the sequence of nodes, or encode relative position with another node using bias terms.
The former loses preciseness of relative position from linearization, while the latter loses a tight integration of node-edge and node-topology interaction. 
To overcome the weakness of the previous approaches, our method encodes a graph without linearization and considers both node-topology and node-edge interaction.
We name our method Graph Relative Positional Encoding dedicated to graph representation learning.
Experiments conducted on various graph datasets show that the proposed method outperforms previous approaches significantly.
\end{abstract} \section{Introduction}
\label{sec:intro}












Transformer \cite{vaswani2017attention} built upon a self-attention module is permutation equivariant where an order of inputs does not affect corresponding outputs.
Therefore, Transformer requires explicit representations of the position of inputs to effectively learn any structured data. 
In the case of natural language processing or computer vision, absolute positional encoding is widely adopted as each input has its own absolute position, \eg, -th order of word in a sentence or coordinates of a patch within a grid.
The positional embedding vector of the absolute position is added to the input before feeding to Transformer.
However, adopting this absolute positional encoding on a graph is not trivial since nodes do not have absolute positions such as order.
Therefore, representing the position of each node in a graph is a key challenge in designing a Transformer for a graph.













Several works have been proposed to incorporate positional information of graph on Transformer, and we
categorize existing works into two: (a) linearizing graph to encode the absolute position of each node~\citep{gt_dwivedi2020generalization,san_kreuzer2021rethinking} using techniques like graph Laplacian or singular value decomposition (b) encoding position relative to another node with bias terms~\citep{ying2021transformers}. 
The former loses precision of position due to linearization, while the latter loses a tight integration of node-edge and node-topology information.






\begin{figure}[t]
\centering\includegraphics[width=2.2\linewidth]{resource/Figure1_final2.pdf}
\vspace{-5.2cm}
\caption{
How our method builds attention map  reflcting node-topology information, by interaction between node feature  and topology encoding  representing adjacency and shortest path.
We illustrate with  capturing
relations within 1-hop distance, though we generalize for .
Similarly, attention map  reflects diverse edge types, and interaction between node feature and edge encoding.
 We skip the illustration of node-edge information for simplification.
}
\label{fig:teaser}
\end{figure}


\begin{figure*}[t]
\centering\includegraphics[width=1\linewidth]{resource/Figure2_final3.pdf}
\vspace{-5.2cm}
\caption{
Contrasting ours from previous approaches by where graph positional information is encoded, \ie, components marked in red.
Existing works either (a) linearize a graph to obtain absolute positional encoding for initial inputs or (b) add bias terms for attention map. 
On the other hand, \textbf{(c) our method} encodes on both attention map and value, capturing node-topology and node-edge relations.
We skip the illustration of edge encoding for simplification.
}
\label{fig:figure2}
\end{figure*}



On the other hand, ours can be interpreted as overcoming the weakness of (a) and (b):
 Figure~\ref{fig:teaser} illustrates that our method reflects global topology as attention map, while incorporating node-topology interaction.
More specifically, unlike conventional approaches (a) and (b),  limited to encode graph only on either the initial input (Figure~\ref{fig:figure2}a) or attention map (Figure~\ref{fig:figure2}b),
our method  encodes positional information when node features interact with each other on self-attention.
Figure~\ref{fig:figure2}c shows our architecture where the topology and edge of a graph are encoded on both attention map and value.



To this end, we introduce two set of learnable positional encoding vectors which represent relative positional relation.
The first is topology encoding to represent topological relation between nodes.
The second is edge encoding to represent connection between nodes.
Node features and the two encoding vectors interact to integrate both node-topology and node-edge interaction when building attention map.
Furthermore, we leverage the two positional encodings to incorporate graph on the hidden representations of self-attention.
With these two positional encodings, topology of a graph can be represented in both attention map and value.




In terms of utilizing relative position between inputs and incorporating interaction between input feature and positional encoding, our work is related to the work of \citeauthor{shaw2018self}.
Its efficacy has been verified in sequences, e.g., natural language processing, but does not generalize to  a graph. 
Our proposed relative positional encoding generalizes from 1D sequence structure to relative positions in the graph,
 dedicated to graph representation learning covering graph-specific properties.














We name our method for graph representation learning as \textbf{G}raph \textbf{R}elative \textbf{P}ositional \textbf{E}ncoding (GRPE).
We extensively conducted experiments to validate the efficacy of our proposed method on various tasks, \eg, graph classification, graph regression and node classification.
Models built with our relative positional encoding achieve state-of-the-art performance on various graph datasets, showing the efficacy of our proposed method.


 \section{Related Work}
\label{sec:related_work}










Existing works leverage Transformer architecture to learn graph representation. We categorize those methods as follows.










Earlier models adopt Transformer without explicit encoding of positional information on a graph.
\citeauthor{velivckovic2017graph}  replace graph convolution operation with self-attention module where attention
is only performed within neighboring nodes.
\citeauthor{rong2020self} stack self-attention module next to the graph convolutional networks iteratively to consider long-range interaction between nodes.
In their method, affinity is considered only on the graph convolutional networks, and positional information is not given on self-attention.


Later works employ absolute positional encoding to explicitly encode positional information of graph on Transformer.
Their main idea is to linearize a graph into a sequence of nodes, and an absolute positional encoding is added to the input feature.
\citeauthor{gt_dwivedi2020generalization} adopted graph Laplacian as a positional encoding, where each cell of encoding represents partitions after graph min-cut. 
Nodes sharing many partitions after graph min-cut would have similar graph Laplacian vectors.
\citeauthor{san_kreuzer2021rethinking} employ a learnable positional encoding with a Transformer where its input is the Laplacian spectrum of a graph. Due to the linearization of a graph, those approaches lose the preciseness of position on the graph.


Meanwhile, encoding relative positional information has been studied to avoid losing the preciseness of position.
Graphormer introduced by \citeauthor{ying2021transformers} encodes relative position on scaled dot product attention map by adding bias terms. 
However, the bias terms are parameterized only relative position such as shortest path distance or edge type, and the interaction with node features is lost.
On the other hand, \citeauthor{shaw2018self} introduce relative positional encoding, 
for 1D sequence, on which we add relative position encoding, to capture
the interaction between nodes and graph-specific properties such as edge and topology.



























%
 


\section{Background}
\label{sec:background}


\smallbreak
\subsection{Notation}
We denote a set of nodes on the graph   and a set of edges on the graph , where  is the number of nodes and 
 is a set neighbors of a node . 
Both  and  are positive integer numbers to index the type of nodes or edges, \eg, atom numbers or bond types of a molecule.
 denotes a function encodes topological relationship between the node  and . 


\subsection{Self-attention} 
Transformer is built by stacking multiple self-attention layers.
Self-attention maps a query and a set of key pairs to compute an attention map. 
Values are weighted summed with the weight on the attention map to output the hidden feature for the following layer.

Specifically,  denotes the input feature of the node , and  denotes the output feature of the self-attention module.
The self-attention module computes query , key , and value  with independent linear transformations: ,  and .

The attention map is computed by applying a scaled dot product between the queries and the keys.

The self-attention module outputs the next hidden feature by applying weighted summation on the values.

 is later fed into a feed forward neural network with a residual connection \citep{he2016deep}. However, we defer detailed explanations since it is out of the scope of our paper.
In practice, self-attention module with multi-head is adopted.


\subsection{Graph with Transformer}

To encode graph topology in Transformer, previous methods focus on encoding graph information into either the attention map or input features fed to Transformer.
Graphormer \citep{ying2021transformers} adopted two additional terms on the self-attention module to encode graph information on the attention map.


 represents the topological relation between  and , which outputs the shortest path distance between the two nodes.
Learnable scalar bias  encodes topological relation between two nodes, \eg,  is a bias representing two nodes that are -hop apart.
An embedding vector  is a feature representing edge between the node  and the node , and  is a learnable vector.
 encodes edge between the two nodes.
Moreover, Graphormer adds centrality encoding into the input  which represents the number of edges of a node.
However, Graphormer encodes graphs on the attention map without considering node-topology and node-edge interaction, on the other hand GRPE considers the two.
We will explain the details in later.

\citeauthor{gt_dwivedi2020generalization} and \citeauthor{san_kreuzer2021rethinking}  utilize graph Laplacian \citep{belkin2003laplacian}  as positional encodings on the input feature ;  are the top- smallest eigenvectors of  where  is an identity matrix,  is an adjacency matrix and  is a degree matrix.
Each cell of a graph Laplacian vector represents partitions after graph min-cut, and neighbouring nodes sharing the many partitions would have similar graph Laplacian.
The graph Laplacian  represents the topology of a graph with respect to node .



\citeauthor{san_kreuzer2021rethinking} adopt an additional Transformer model  to produce learnable positional encoding:  where .
By adding the graph Laplacian into input , graph information can be encoded in both the attention map and the hidden representations.
Their methods lose relative positional information during the linearization of a graph to obtain absolute positional encoding.
However, our method encodes a graph directly on the attention map without linearization, thus relative positional information is encoded without loss. \section{Our Approach}
\label{sec:method}

\begin{figure}[t]
\hspace{4.7cm}\centerline{\includegraphics[width=2.2\linewidth]{resource/Figure3_final.pdf}}
\vspace{-5cm}
\caption{
Illustration of how GRPE processes relative relation between nodes.
We add virtual node  to represent entire graph. 
For topological relations, we adopt the shortest path distance between nodes.
The virtual node is connected with all other nodes with a special edge VN and a special topological relation VN.
}
\label{fig:figure3}
\end{figure}


Our distinction is twofold.
First, we integrate interaction between node and graph structural information on the attention map.
For that, we propose node-aware attention which considers the interactions existing in two pairs: node-topology relation and node-edge relation.
Second, we also encode the topological information of a graph to the hidden representation of self-attention. For that, we propose graph-encoded values that directly encode relative positional information on the features of value by addition. 
Our node-aware attention applies the attention mechanism in a node-wise manner, while our graph-encoded value applies the attention mechanism in a channel-wise manner.



\subsection{Relative Positional Encoding for Graph}


We define two encodings to represent relative positional relation between two nodes in a graph.
The first is topology encoding , and we define the encodings for query, key, and value respectively: .
Each vector of  represents the topological relation between two nodes, \eg,  represents the topological relation of two nodes where their shortest path distance is . 
 is the maximum shortest path distance that our method considers.

The second is edge encoding , and we define the encodings for query, key, and value respectively: .  is a vector representing edge between two nodes  and .
 is the number of types of edge.
The topology encodings and the edge encodings are shared throughout all layers.


\subsection{Node-Aware Attention}
\label{sec:context_aware}

We propose two attention maps to encode a graph on self-attention.
The first attention map is .
It encodes graph by considering interaction between node feature and topological relation of graph.



The second attention map is .
It encodes graph by considering interaction between node feature and edge in graph.


Finally, the two attention maps are added to scaled dot product attention map to encode graph information.



Our two attention maps consider topology and edge type, as node-topology and node-edge relation.
Meanwhile, Graphormer did not consider the interaction with node feature, such that
two nodes with the same distance apart have the same bias  on Eq~\ref{eq:graphormer_attention}.
In contrast, our  enables to deploy different values according to the node features of query and key. 


\subsection{Graph-Encoded Value}
\label{sec:graph_embedded_value}

Another distinction is that our method directly encodes graph information into the hidden features of value, as well.
Specifically, we encode a graph to the hidden features of self-attention, where values are weighted summed with the attention map, for both topology and edge encoding
 via summation:


While the attention weight  is applied equally for all channels (node-wise attention),  our graph-encoded value enriches 
the feature of each channel, and enables channel-wise attention as well.
This relative positional encoding enables to encode relative position directly on hidden features, without 
previous approaches that encode position on input  with a linearized graph, as in Figure 1a, \eg, centrality encoding~\citep{ying2021transformers} or graph Laplacian~\citep{san_kreuzer2021rethinking,gt_dwivedi2020generalization},
where linearization sacrifices the preciseness of position.





\subsection{Complexity Analysis}
\label{sec:efficient_impl}

A naive implementation of computing all pairs of  requires time complexity of , since it requires performing the dot product between all node pairs.
Instead, we pre-compute the dot product of all possible pairs of node features and topology encoding vectors  which requires time complexity of . 
Then we assign the pre-computed value according to the indices of node pairs.
Likewise, for the , we pre-compute the dot product of all possible pairs of node features and edge encoding vectors  which requires time complexity of .
The time complexity is reduced significantly with our implementation since  and  are much smaller than the number of nodes .



 \section{Experiment}
\label{sec:experiment}

\begin{table*}
\centering
\caption {Configurations of models that we utilize throughout our experiments.} 
\label{tab:models}

\scalebox{1.0}{
\begin{tabular}{l|c|ccccc}
\toprule
\multicolumn{1}{c|}{Model Configurations} & \# Params & \# Layers & Hidden dim [] & FFN layer dim & \# Heads \\ \hline
GRPE-Tiny & 106k & 4  & 64 & 64  & 8    \\
GRPE-Small  & 489k & 12 & 80 & 80 & 8  \\
GRPE-Standard & 46.2M & 12 & 768 & 768 & 32 \\
GRPE-Large  & 118.3M  & 18  & 1024 & 1024 & 32 \\ 
\bottomrule
\end{tabular}}
\end{table*} 

\begin{table}
\centering
\caption {Results on ZINC.  indicates a fine-tuned model. The lower the better.} 
\label{tab:zinc}
\vspace{-0.2cm}
	\scalebox{0.8}{
\begin{tabular}{ccc}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{\#Params} & MAE \\ \hline
\multicolumn{1}{l|}{GIN \citep{gin_xu2018powerful}}  & \multicolumn{1}{c|}{510k} & 
\multicolumn{1}{c}{0.5260.051}\\
\multicolumn{1}{l|}{GAT~\citep{velivckovic2017graph}}  & \multicolumn{1}{c|}{531k} & 
\multicolumn{1}{c}{0.3840.007} \\
\multicolumn{1}{l|}{GCN~\citep{gcn_kipf2016semi}}  & \multicolumn{1}{c|}{505k} & 
\multicolumn{1}{c}{0.3670.011} \\
\multicolumn{1}{l|}{GatedGCN~\citep{gated_bresson2017residual}}  & \multicolumn{1}{c|}{505k} & 
\multicolumn{1}{c}{0.2140.006} \\
\multicolumn{1}{l|}{MPNN (sum)~\citep{mpnn_gilmer2017neural}}  & \multicolumn{1}{c|}{481k} & \multicolumn{1}{c}{0.1450.007} \\
\multicolumn{1}{l|}{PNA~\citep{pna_corso2020principal}}  & \multicolumn{1}{c|}{387k} & 
\multicolumn{1}{c}{0.1420.010} \\
\hline
\multicolumn{1}{l|}{GT~\citep{gt_dwivedi2020generalization}}  & \multicolumn{1}{c|}{589k} & 
\multicolumn{1}{c}{0.2260.014} \\
\multicolumn{1}{l|}{SAN~\citep{san_kreuzer2021rethinking}}  & \multicolumn{1}{c|}{509k} & 
\multicolumn{1}{c}{0.1390.006} \\
\multicolumn{1}{l|}{Graphormer (slim) \citep{ying2021transformers}} & \multicolumn{1}{c|}{489k} & 
\multicolumn{1}{c}{0.1220.006} \\
\multicolumn{1}{l|}{EGT \cite{hussain2021edge}} &\multicolumn{1}{c|}{500k} & 
\multicolumn{1}{c}{0.1080.009} \\    
\hline
\multicolumn{1}{l|}{GRPE-Small (Ours)} &\multicolumn{1}{c|}{489k} & 
\multicolumn{1}{c}{ \bf{0.0940.002}} \\    
\bottomrule
\end{tabular}}
\end{table}
 
\begin{table}
\centering
\caption {Results on MolHIV.  indicates a fine-tuned model. The higher the better.} 
\label{tab:hiv}
\vspace{-0.2cm}
\hspace{-0.5cm}
\scalebox{0.8}{
\begin{tabular}{ccc}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{\#Params} & AUC (\%) \\ \hline
\multicolumn{1}{l|}{GCN-GraphNorm~\citep{cai2021graphnorm}}  & \multicolumn{1}{c|}{526k} & 
\multicolumn{1}{c}{78.831.00}\\
\multicolumn{1}{l|}{PNA~\citep{pna_corso2020principal}}  & \multicolumn{1}{c|}{326k} & 
\multicolumn{1}{c}{79.05 1.32}\\
\multicolumn{1}{l|}{PHC-GNN~\citep{phc_le2021parameterized}}  & \multicolumn{1}{c|}{111k} & 
\multicolumn{1}{c}{79.34 1.16} \\
\multicolumn{1}{l|}{DeeperGCN-FLAG~\citep{deepergn_li2020deepergcn}}  & \multicolumn{1}{c|}{532k} & 
\multicolumn{1}{c}{79.42 1.20} \\
\multicolumn{1}{l|}{DGN~\citep{dgn_beani2021directional}}  & \multicolumn{1}{c|}{114k} & 
\multicolumn{1}{c}{79.70 0.97} \\
\multicolumn{1}{l|}{GIN~\citep{gin_xu2018powerful}}  & \multicolumn{1}{c|}{3.3M} & 
\multicolumn{1}{c}{77.80 1.82} \\
\hline
\multicolumn{1}{l|}{Graphormer-FLAG~\cite{ying2021transformers}}  & \multicolumn{1}{c|}{47.0M} &  \multicolumn{1}{c}{80.510.53} \\
\multicolumn{1}{l|}{EGT-Large ~\cite{hussain2021edge}} & \multicolumn{1}{c|}{110.8M} &  \multicolumn{1}{c}{80.600.65} \\
\hline
\multicolumn{1}{l|}{GRPE-Standard (Ours)} &\multicolumn{1}{c|}{46.2M} & 
\multicolumn{1}{c}{ \bf{81.39   0.49}} \\    
\bottomrule
\end{tabular}}
\end{table}
 
\begin{table}
\centering
\caption {Results on MolPCBA.   indicates a fine-tuned model. The higher the better.} 
\label{tab:pcba}
\scalebox{0.8}{
\begin{tabular}{ccc}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{\#Params} & AP (\%) \\ \hline
\multicolumn{1}{l|}{DeeperGCN+FLAG~\citep{deepergn_li2020deepergcn}}  & \multicolumn{1}{c|}{5.6M} & 
\multicolumn{1}{c}{28.42 0.43}\\
\multicolumn{1}{l|}{DGN~\citep{dgn_beani2021directional}}  & \multicolumn{1}{c|}{6.7M} & 
\multicolumn{1}{c}{28.85 0.30}\\
\multicolumn{1}{l|}{PHC-GNN~\citep{phc_le2021parameterized}}  & \multicolumn{1}{c|}{1.7M} & 
\multicolumn{1}{c}{29.47 0.26} \\
\multicolumn{1}{l|}{GINE~\citep{gine_brossard2020graph}}  & \multicolumn{1}{c|}{6.1M} & 
\multicolumn{1}{c}{29.79 0.30} \\
\multicolumn{1}{l|}{GIN~\citep{gin_xu2018powerful} }  & \multicolumn{1}{c|}{3.4M} & 
\multicolumn{1}{c}{29.02 0.17} \\
\hline
\multicolumn{1}{l|}{Graphormer-FLAG~\citep{ying2021transformers}}  & \multicolumn{1}{c|}{119.5M} & 
\multicolumn{1}{c}{31.39 0.32} \\
\multicolumn{1}{l|}{EGT-Large~\citep{hussain2021edge}}  & \multicolumn{1}{c|}{110.8M} & 
\multicolumn{1}{c}{29.610.24} \\
\hline
\multicolumn{1}{l|}{GRPE-Standard (Ours)} &\multicolumn{1}{c|}{ 46.2M} & 
\multicolumn{1}{c}{30.77 0.07} \\   
\multicolumn{1}{l|}{GRPE-Large (Ours)} &\multicolumn{1}{c|}{ 118.3M} & 
\multicolumn{1}{c}{\bf{31.50} 0.10} \\  
\bottomrule
\end{tabular}
}
\end{table} 
\begin{table}
\centering
\caption {Results on PATTERN. The higher the better.} 
\label{tab:pattern}
\vspace{-0.2cm}
\hspace{-0.6cm}
	\scalebox{0.8}{
\begin{tabular}{ccc}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{2}{c}{Weighted Accuracy} \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{c}{\#Params} & \multicolumn{1}{c}{\#Params} \\ 
\multicolumn{1}{c|}{} & \multicolumn{1}{c}{100k} & \multicolumn{1}{c}{500k} \\ \hline

\multicolumn{1}{l|}{GIN \citep{gin_xu2018powerful}}  & \multicolumn{1}{c}{85.5900.011} & 
\multicolumn{1}{c}{85.3870.136}\\
\multicolumn{1}{l|}{GAT~\citep{velivckovic2017graph}}  & \multicolumn{1}{c}{75.8241.823} & 
\multicolumn{1}{c}{78.2710.186} \\
\multicolumn{1}{l|}{GCN~\citep{gcn_kipf2016semi}}  & \multicolumn{1}{c}{63.8800.074} & 
\multicolumn{1}{c}{71.8920.334} \\
\multicolumn{1}{l|}{GatedGCN~\citep{gated_bresson2017residual}}  & \multicolumn{1}{c}{84.4800.122} & 
\multicolumn{1}{c}{86.5080.085} \\
\multicolumn{1}{l|}{PNA~\citep{pna_corso2020principal}}  & \multicolumn{1}{c}{86.5670.075} & 
\multicolumn{1}{c}{-} \\
\hline
\multicolumn{1}{l|}{GT~\citep{gt_dwivedi2020generalization}}  & \multicolumn{1}{c}{-} & 
\multicolumn{1}{c}{84.8080.068} \\
\multicolumn{1}{l|}{SAN~\citep{san_kreuzer2021rethinking}}  & \multicolumn{1}{c}{-} & 
\multicolumn{1}{c}{86.5810.037} \\
\multicolumn{1}{l|}{Graphormer (slim) \citep{ying2021transformers}} & \multicolumn{1}{c}{-} & 
\multicolumn{1}{c}{86.6500.033} \\
\multicolumn{1}{l|}{EGT \citep{hussain2021edge}} & \multicolumn{1}{c}{\textbf{86.8160.027}} & 
\multicolumn{1}{c}{86.8210.020} \\
\hline
\multicolumn{1}{l|}{GRPE (Ours)} &\multicolumn{1}{c}{83.1050.045} & 
\multicolumn{1}{c}{\textbf{87.0200.042}} \\    
\bottomrule
\end{tabular}}
\end{table}
 
\begin{table}[t]
\centering
\caption {Results on CLUSTER. The number of parameters is around 500K for all models. The higher the better.}
\label{tab:cluster}
\vspace{-0.2cm}
	\scalebox{0.88}{
\begin{tabular}{cc}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{1}{c}{Weighted Accuracy} \\ \hline


\multicolumn{1}{l|}{GIN \citep{gin_xu2018powerful}}  & \multicolumn{1}{c}{64.7161.553}\\
\multicolumn{1}{l|}{GAT~\citep{velivckovic2017graph}}  & \multicolumn{1}{c}{70.5870.447} \\
\multicolumn{1}{l|}{GCN~\citep{gcn_kipf2016semi}}  & \multicolumn{1}{c}{68.4980.976}  \\
\multicolumn{1}{l|}{GatedGCN~\citep{gated_bresson2017residual}}  & \multicolumn{1}{c}{76.0820.196} \\
\hline
\multicolumn{1}{l|}{GT~\citep{gt_dwivedi2020generalization}}  & \multicolumn{1}{c}{73.1690.622}  \\
\multicolumn{1}{l|}{SAN~\citep{san_kreuzer2021rethinking}}  & \multicolumn{1}{c}{76.6910.650} \\
\multicolumn{1}{l|}{Graphormer (slim) \citep{ying2021transformers}} & \multicolumn{1}{c}{74.6600.236}  \\
\multicolumn{1}{l|}{EGT \citep{hussain2021edge}} & \multicolumn{1}{c}{79.2320.348}  \\
\hline
\multicolumn{1}{l|}{GRPE-Small (Ours)} &\multicolumn{1}{c}{\textbf{81.5860.190}} \\    
\bottomrule
\end{tabular}}
\end{table}
 
\begin{table*}[ht!]
\centering
\caption {Results on PCQM4M. * indicates the results are from the official leaderboard. VN indicates that the model used virtual node. The lower the better.} 
\label{tab:pcqm4m-lsc}
\vspace{0.1cm}
	\scalebox{1.0}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{\#Params} & Train MAE & Validate MAE & Test MAE \\ \hline
\multicolumn{1}{l|}{GCN~\citep{gcn_kipf2016semi} }  & \multicolumn{1}{c|}{2.0M} & 
\multicolumn{1}{c}{0.1318} & \multicolumn{1}{c}{0.1691 (0.1684)} & \multicolumn{1}{c}{0.1838} &  \\
\multicolumn{1}{l|}{GIN~\citep{gin_xu2018powerful} }  & \multicolumn{1}{c|}{3.8M} & 
\multicolumn{1}{c}{0.1203} & \multicolumn{1}{c}{0.1537 (0.1536)} & \multicolumn{1}{c}{0.1678} &  \\
\multicolumn{1}{l|}{GCN-VN~\citep{gcn_kipf2016semi}} & \multicolumn{1}{c|}{4.9M} & 
\multicolumn{1}{c}{0.1225} & \multicolumn{1}{c}{0.1485 (0.1510)} & \multicolumn{1}{c}{0.1579} &  \\
\multicolumn{1}{l|}{GIN-VN~\citep{gin_xu2018powerful}}  & \multicolumn{1}{c|}{6.7M} & 
\multicolumn{1}{c}{0.1150} & \multicolumn{1}{c}{0.1395 (0.1396)} & \multicolumn{1}{c}{0.1487} & \\
\multicolumn{1}{l|}{GINE-VN~\citep{gine_brossard2020graph}} & \multicolumn{1}{c|}{13.2M} & 
\multicolumn{1}{c}{0.1248} & \multicolumn{1}{c}{0.1430} & \multicolumn{1}{c}{-} &  \\
\multicolumn{1}{l|}{DeeperGCN-VN~\citep{deepergn_li2020deepergcn}} & \multicolumn{1}{c|}{25.5M} &
\multicolumn{1}{c}{0.1059} & \multicolumn{1}{c}{0.1398} & \multicolumn{1}{c}{-} &  \\
\hline
\multicolumn{1}{l|}{GT~\citep{gt_dwivedi2020generalization}} & \multicolumn{1}{c|}{0.6M} &
\multicolumn{1}{c}{0.0944} & \multicolumn{1}{c}{0.1400} & \multicolumn{1}{c}{-} &  \\  
\multicolumn{1}{l|}{GT-wide~\citep{gt_dwivedi2020generalization}} & \multicolumn{1}{c|}{83.2M} &
\multicolumn{1}{c}{0.0955} & \multicolumn{1}{c}{0.1408} & \multicolumn{1}{c}{-} &  \\  
\multicolumn{1}{l|}{Graphormer (small)~\citep{ying2021transformers}} & \multicolumn{1}{c|}{12.5M} &
\multicolumn{1}{c}{0.0778} & \multicolumn{1}{c}{0.1264} & \multicolumn{1}{c}{-} &  \\  
\multicolumn{1}{l|}{Graphormer~\citep{ying2021transformers}} & \multicolumn{1}{c|}{47.1M} &
\multicolumn{1}{c}{0.0582} & \multicolumn{1}{c}{0.1234} & \multicolumn{1}{c}{\textbf{0.1328}} &  \\    
\multicolumn{1}{l|}{EGT-Medium~\citep{hussain2021edge}} & \multicolumn{1}{c|}{47.4M} &
\multicolumn{1}{c}{-} & \multicolumn{1}{c}{\bf{0.1224}} & \multicolumn{1}{c}{-} &  \\    
\hline
\multicolumn{1}{l|}{GRPE-Standard (Ours)} &\multicolumn{1}{c|}{46.2M} & 
\multicolumn{1}{c}{\bf{0.0349}} & \multicolumn{1}{c}{0.1225} & \multicolumn{1}{c}{-} &  \\    

\bottomrule
\end{tabular}}
\end{table*} 
\begin{table*}[!ht]
\centering
\caption {Results on PCQM4Mv2. VN indicates that the model used the virtual node. The lower the better.} 
\label{tab:pcqm4m-lsc-v2}
\vspace{0.1cm}
	\scalebox{1.0}{
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{\#Params} & Validate MAE & Test-dev MAE \\ \hline
\multicolumn{1}{l|}{GCN~\citep{gcn_kipf2016semi} }  & \multicolumn{1}{c|}{2.0M} & \multicolumn{1}{c}{0.1379} & \multicolumn{1}{c}{0.1398} &  \\
\multicolumn{1}{l|}{GIN~\citep{gin_xu2018powerful} }  & \multicolumn{1}{c|}{3.8M} & \multicolumn{1}{c}{0.1195} & \multicolumn{1}{c}{0.1218} &  \\
\multicolumn{1}{l|}{GCN-VN~\citep{gcn_kipf2016semi}} & \multicolumn{1}{c|}{4.9M} & \multicolumn{1}{c}{0.1153} & \multicolumn{1}{c}{0.1152} &  \\
\multicolumn{1}{l|}{GIN-VN~\citep{gin_xu2018powerful}}  & \multicolumn{1}{c|}{6.7M} & \multicolumn{1}{c}{0.1083} & \multicolumn{1}{c}{0.1084} & \\
\hline
\multicolumn{1}{l|}{EGT-Medium~\citep{hussain2021edge}}  & \multicolumn{1}{c|}{47.4M} & \multicolumn{1}{c}{0.0881} & \multicolumn{1}{c}{-} & \\
\multicolumn{1}{l|}{EGT-Large~\citep{hussain2021edge}}  & \multicolumn{1}{c|}{89.3M} & \multicolumn{1}{c}{0.0869} & \multicolumn{1}{c}{\textbf{0.0872}} & \\
\hline
\multicolumn{1}{l|}{GRPE-Standard (Ours) } &\multicolumn{1}{c|}{46.2M} &  \multicolumn{1}{c}{0.0890} & \multicolumn{1}{c}{0.0898} &  \\    
\multicolumn{1}{l|}{GRPE-Large (Ours) } &\multicolumn{1}{c|}{118.3M} &  \multicolumn{1}{c}{\textbf{0.0866}} & \multicolumn{1}{c}{-} &  \\    
\bottomrule
\end{tabular}}
\end{table*} 



\subsection{Implementation Details}


\subsubsection{Virtual node}

Following \citeauthor{mpnn_gilmer2017neural} and \citeauthor{ying2021transformers}, we adopt a special node called virtual node which is connected to all other nodes.
The role of a virtual node is similar to special tokens such as a classification token \citep{devlin2018bert}, where its output feature  is used as the input for the branch that predicts downstream tasks.
We additionally define two encoding vectors to define both topology relation  and edge  for query, key and value respectively.
Note that, the virtual node does not involve to find the shortest path between two nodes.
Throughout all experiments, we add the virtual node to a graph to perform the downstream tasks.
Figure~\ref{fig:figure3} illustrates how a virtual node is connected with other nodes.

\subsubsection{Topological relation}
We utilize shortest path distance  to describe the topological relation between two nodes  and .
  is the maximum distance of the shortest path that we consider, and we utilize a special encoding vector  for the node pairs with a distance more than .
For the nodes pairs that are unreachable, we utilize another special encoding vector .
Finally, for the pairs that are connected with the virtual node, we utilize another special encoding vector .
Figure~\ref{fig:figure3} illustrates how topological relation is processed.


\subsubsection{Edge}

Some pair of nodes are not connected with edges.
Therefore, we utilize a special encoding vector  for the pairs of node that are not connected with any edges; .
For the pair of two identical nodes where , we use a special embedding vector .
Finally, for the pairs that are connected with the virtual node, we utilize another special encoding vector .
Figure~\ref{fig:figure3} illustrates how edges are processed.

\subsection{Graph Classification and Regression}

 We summarize the model configurations of our experiments in Table~\ref{tab:models}.
 For all results, 
 indicates the models adopted Transformer for learning graph representation
and text in \textbf{bold}
indicates the best result.






We first validate our method on the tasks of the molecule property prediction such as OGBG-MolPCBA (MolPCBA) \citep{hu2020open}, OGBG-MolHIV (MolHIV) \citep{hu2020open} and ZINC \citep{dwivedi2020benchmarking}.
MolPCBA consists of 437,929 graphs and the task is to predict multiple binary labels indicating various molecule properties. The evaluation metric is average precision (AP). 
MolHIV is a small dataset that consists of 41,127 graphs. 
The task is to predict a binary label indicating whether a molecule inhibits HIV virus replication or not. 
The evaluation metric is area under the curve (AUC).
ZINC is also a small dataset that consists of 12,000 graphs, and the task is to regress a molecule property. 
The evaluation metric is mean absolute error (MAE). 
All experiments are conducted for 5 times, and we report the mean and the standard deviation of the experiments.

We adopt the linear learning rate decay, and the learning rate starts from  and ends at .
We set  to 5. 
For ZINC dataset, we adopt GRPE-Small configuration with less than 500k parameters for a fair comparison.
For MolHIV and MolPCBA datasets, we initialize the parameter of the models with the weight of a pretrained model trained on PCQM4M~\citep{hu2020open} dataset.



Table~\ref{tab:zinc} shows the results on ZINC dataset, our model achieve state-of-the-art MAE score.
Table~\ref{tab:pcba} shows the results on MolPCBA dataset, our model achieves state-of-the-art AP score. 
Table~\ref{tab:hiv} shows the results on MolHIV dataset, our model achieves the state-of-the-art AUC with less parameters than Graphormer.



\subsection{Node Classification}
We validate our method on the task of the node-wise classification such as PATTERN and CLUSTER \cite{dwivedi2020benchmarking}. 
PATTERN consists of 14,000 graphs and the task is to recognize graph pattern where each node belong, and the number of classes are two.
CLUSTER consists of 12,000 graphs and the task is semi-supervised clustering where each node belong, and the number of classes are six.
The evaluation metric is the average node-level accuracy weighted with respect to the class sizes, we follow the evaluation code presented in the original work~\cite{dwivedi2020benchmarking}.


We adopt the linear learning rate decay, and the learning rate starts from  and ends at .
We set  to 5. 
For both datasets, we adopt GRPE-Small with less than 500k parameters and adopt GRPE-Tiny with less than 100k parameters for a fair comparison.
Table~\ref{tab:pattern} shows the results on PATTERN, our models achieve state-of-the-art accuracy on 500k parameters.
Table~\ref{tab:cluster} shows the results on CLUSTER, our models achieve state-of-the-art weighted accuracy with a significant improvement.







\subsection{OGB Large Scale Challenge}





We validate our method on two datasets of OGB large scale challenge~\citep{hu2020open}.
The two datasets aim to predict the DFT-calculated HOMO-LUMO energy gap of molecules given their molecular graphs. 
We conduct experiments on both the PCQM4M and PCQM4Mv2 datasets, which are currently the biggest molecule property prediction datasets containing about 4 million graphs in total.
PCQM4Mv2 contains the DFT-calcuated 3D strcuture of molecules.
For our experiments, we only utilize 2D molecular graphs not 3D structures.
Throughout experiments, we set  to 5. 
We adopt a GRPE-Standard for fair comparisons with Graphormer.
We linearly increase learning rate up to  for 3 epochs and linearly decay learning rate upto  for 400 epochs.
We are unable to measure the test MAE of PCQM4M, because the test dataset is deprecated as PCQM4Mv2 is newly released.

Table~\ref{tab:pcqm4m-lsc} shows the results on PCQM4M dataset. 
Our model achieves the second best validation MAE score, but with a very small gap with the best model of about . 
Table.~\ref{tab:pcqm4m-lsc-v2} shows the results on PCQM4Mv2 dataset.
Our large model achieves the best result on the validation dataset.
We couldn't report the large model's test-dev MAE, since the evaluation server only allow one submission per week. 
We will make sure to report the result for the final draft.
For the models with a similar number of parameters GRPE-Standard and EGT-Medium, we achieved competitive results.












\begin{figure*}
\centering
\hspace*{-3cm}\includegraphics[width=1.3\linewidth]{resource/attention_vis.pdf}
\vspace{-1.3cm}
\caption{Visualization of adjacency matrix and average attention map (starting from third column) of each layer on molecule graph with various size. We added virtual node as the first node marked with zero index. For adjacency matrix, yellow and dark purple indicate the two nodes are connected and disconnected respectively. For attention map, brighter color (yellow) indicates higher attention and dark color indicates lower attention.}
\label{fig:viz_att}
\end{figure*}


\subsection{Ablation Study on Components of GRPE}
\label{sec:gsa_effect}

We validate the effect of components of GRPE on ZINC dataset.
We adopt GRPE-Small.
Table~\ref{tab:zinc_comp_ablation} shows the ablation study results.
The first row is identical to the plain Transformer without any positional encodings, and it obviously shows the highest error.
Adding either  or  lowers the error, and using them together further lowers the error.
Finally, adding our Graph-Encoded Value does help to improve the performance.


\begin{table}
\centering
\caption {Effects of components of GRPE on ZINC dataset. The lower the better.} 
\label{tab:zinc_comp_ablation}
\vspace{-0.2cm}
\begin{tabular}{cccc}
\toprule                   


 &  & Graph-Encoded Value & Test MAE \\
\hline

- & - & - & 0.668 \\
 & - & - & 0.267\\
- &  & - & 0.218 \\
- & - &  & 0.116 \\
 &  & - & 0.147 \\
 &  &  & \textbf{0.093} \\


\bottomrule
\end{tabular}
\end{table}





%
 


\subsection{Effects of Maximum Shortest Path Distance }

We validate the effects of the maximum shortest path distance .
We adopt GRPE-Standard, and the models are trained from scratch.
Figure~\ref{fig:abl_shortest} shows the ablation study result.
Increasing  means that a model can identify the position of nodes that are further away.
The AUC consistently improves by increasing  from one to four, but  more than four does not further improve performance. 


\begin{figure}
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=0.75\textwidth]{resource/chart_pcba.pdf}
\caption{Effects of the maximum shortest path distance . The higher the better.}
\label{fig:abl_shortest}
\end{minipage}
\hfill
\end{figure}

\subsection{Sharing Topology Encoding and Edge Encoding}

We conduct ablation studies about the effects of sharing topology encoding  and edge encoding  for all layers. 
We adopt GRPE-Small.
We conduct five independent runs.
The results in Table~\ref{tab:abl_sharing} show that sharing two encodings does improve MAE but not significantly.



\begin{table}
\caption{Effects of sharing topology encoding and edge encoding for all layers. We report MAE on ZINC dataset. The lower the better.}
\centering
\vspace{-0.2cm}
\begin{tabular}{ccc}
\toprule
\multicolumn{1}{c|}{Is shared?} & \multicolumn{1}{c|}{\#Params} & MAE \\ \hline
\multicolumn{1}{c|}{yes}  & \multicolumn{1}{c|}{489k} & 
\multicolumn{1}{c}{\textbf{0.094} 0.002}\\
\multicolumn{1}{c|}{no}  & \multicolumn{1}{c|}{579k} & 
\multicolumn{1}{c}{0.101  0.003} \\
 
\bottomrule
\end{tabular}
\label{tab:abl_sharing}
\end{table} 







\subsection{Attention Map of GRPE} We visualize attention map of each layer on molecules with various size. We take average over the attention map of 32 heads from GRPE-Standard trained on graph regression task on PCQM4Mv2 dataset.
Note that, the first node is virtual node (with index zero), and its attention map and adjacency is represented at the first row and column.
Figure~\ref{fig:viz_att} shows that our method attend to neighboring nodes at lower layers. 
As layer goes deeper, attention map attends to virtual node with higher attention on the first column.
This shows that each layer focuses on the entire graph rather than each node.

 \section{Conclusion}
We studied the problem of positional encoding for representing structural information of the given graph better, specifically, by adding node-topology and node-edge relations to the model.
We validated the effectiveness of our approach, both quantitatively and qualitatively, in various tasks of diverse dataset sizes and characteristics, \eg, HIV replication prediction from molecule graphs (MolHIV) or semi-supervised clustering on synthetic graphs (CLUSTER).


 
\bibliography{aaai23}

\end{document}
