\documentclass{sig-alt-full}

\overfullrule=5mm

\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}
\usepackage{theorem}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amscd}
\usepackage{color}
\usepackage{url}
\usepackage[plainpages=false,pdfpagelabels,colorlinks=true,citecolor=blue,hypertexnames=false]{hyperref}
\usepackage{alltt}
\usepackage{bbm}
\usepackage{pst-grad,auto-pst-pdf}

\hyphenation{bi-de-gree}
\hyphenation{be-longs}


\def\gathen#1{{#1}}
\def\hoeven#1{{#1}}

\def\myproof{\noindent{\sc Proof.}~}
\def\foorp{\hfill}

\def\MM {\ensuremath{\mathsf{MM}}}
\def\C {\ensuremath{\mathsf{C}}}
\def\F {\ensuremath{\mathbb{F}}}
\def\K {\ensuremath{\mathbb{K}}}
\newcommand{\x}{x}
\newcommand{\Dx}{\partial}
\newcommand{\cN}{ {\cal N}}
\newcommand{\pa} { \partial}
\newcommand{\ie}{{\it i.e.}}
\newcommand{\rem }{ {\rm rem}}
\newcommand{\rank}   { {\rm rank} }
\newcommand{\vu} { {\bf u}}
\newcommand{\vv}{ {\bf v}}
\newcommand{\vw} { {\bf w}}
\newcommand{\lc} { {\rm lc}}
\newcommand{\ord} { {\rm ord}}
\newcommand{\lclm} { {\rm LCLM}}
\newcommand{\bigOsoft}{\widetilde{{O}}}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{define}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{fact}[theorem]{Fact}
\def\qed{\hfil {\vrule height5pt width2pt depth2pt}}

\begin{document}


\title{Fast Computation of Common Left Multiples \\
of Linear Ordinary Differential Operators\titlenote{\small We warmly thank the referees for their very helpful comments.
---
This work was supported in part by the MSR--INRIA Joint Centre,
and by two NSFC grants (91118001 and 60821002/F02). 
\vspace{-36pt}}}
\newfont{\authfntsmall}{phvr at 11pt}
\newfont{\eaddfntsmall}{phvr at 9pt}
\def\more-auths{\end{tabular}
\begin{tabular}{c}}
\numberofauthors{2}
\author{
\alignauthor {\authfntsmall Alin Bostan, Fr\'ed\'eric Chyzak, Bruno Salvy}\\
\affaddr{Algorithms Project, INRIA (France)}\\
\email{\eaddfntsmall{\{alin.bostan,frederic.chyzak,bruno.salvy\}@inria.fr}}
\alignauthor {\authfntsmall Ziming Li}\\
\affaddr{KLMM and AMSS (China)}\\
\email{\eaddfntsmall{zmli@mmrc.iss.ac.cn}}}

\maketitle
\begin{abstract}
We study tight bounds and fast algorithms
for LCLMs of 
\emph{several\/} 
linear differential operators with polynomial coefficients. We analyse the
arithmetic complexity of existing algorithms for LCLMs, as well as the size of
their outputs. We propose a new algorithm that recasts the LCLM computation in
a linear algebra problem on a polynomial matrix. This algorithm yields sharp
bounds on the coefficient degrees of the LCLM, improving by one order of
magnitude the best bounds obtained using previous algorithms. The complexity
of the new algorithm is almost optimal, in the sense that it nearly matches
the arithmetic size of the output. \end{abstract}

\vspace{1mm}
\noindent
{\bf Categories and Subject Descriptors:} \\
\noindent I.1.2 [{\bf Computing Methodologies}]: Symbolic and
Algebraic Manipulations --- \emph{Algebraic Algorithms}

\vspace{1mm}
\noindent {\bf General Terms:} Algorithms, Theory.

\vspace{1mm}
\noindent {\bf Keywords:} Algorithms, complexity, linear differential operators, common left multiples.



\section{Introduction} 
The complexity of operations in the polynomial ring  over a
field~ has been intensively studied in the computer algebra literature.
It is well established that polynomial multiplication is a \emph{commutative
complexity yardstick}, in the sense that the complexity of operations
in~ can be expressed in terms of the cost of multiplication, and for
most of them, in a quasi-linear way.

Linear differential operators in the derivation  and with coefficients in~ form a
non-commutative ring, denoted , that shares many
algebraic properties with the commutative ring~. The structural
analogy between polynomials and linear differential equations was discovered
long ago by Libri and Brassinne~\cite{Libri1833, Brassinne1864, Demidov83}.
They introduced the bases of a non-commutative elimination theory, by defining
the notions of greatest common right divisor (GCRD) and least common left
multiple (LCLM) for differential operators, and by designing a Euclidean-type
algorithm for GCRDs.
This was  formalised by Ore~\cite{Ore32,Ore33}, who set up a common
algebraic framework for polynomials and linear differential operators (and
other skew polynomials, including difference and -difference operators).
Yet, the algorithmic study of linear differential operators is currently much
less advanced than in the polynomial case. The cost of product in
 has been addressed only recently
in~\cite{VdHoeven02,BoChLe08}. 

The general aim of this work is to take a step towards a systematic study of
the complexity of operations in . We promote the
idea that (polynomial) matrix multiplication may well become the common
yardstick for measuring complexities in this non-commutative setting. The
specific goal of the present article is to illustrate this idea
for LCLMs. We focus on LCLMs since several higher level
algorithms rely crucially on the efficiency of this basic computational
primitive. For instance, algorithms for manipulating D-finite functions
represented by annihilating equations use common left multiples for performing
addition~\cite{Stanley80,SaZi94}. 
LCLMs of several operators are also needed as a basic task in various other higher-level algorithms~\cite{BaChLo03,Le03,ClHo04}. 
Our approach is based on using complexity analysis as a tool for algorithmic
design, and on producing tight size bounds on the various objects involved in
the algorithms.

It is folklore that Ore's non-commutative Euclidean algorithm is
computationally expensive; various other algorithms for computing common left
multiples of two operators have been
proposed~\cite{Heffter1896,Stanley80,SaZi94,Li98,Bostan03,AbLeLi05,VdHoeven11}.
As opposed to Ore's algorithm, these alternatives have the common
feature that they reduce the problem of computing LCLMs to linear algebra.
However, few complexity analyses~\cite{Giesbrecht92,Giesbrecht98,Bostan03,VdHoeven11} and performance comparisons~\cite{Li98,AbLeLi05} are available.

\begin{figure*} \begin{center} \renewcommand{\arraystretch}{1.2}
\tabcolsep4pt
	\begin{tabular}{|c|ccccc|} \hline
{Algorithm}  & {\, \bf Heffter's + DAC \, } & {\,\bf Li's + DAC \,} & {\,\bf van der Hoeven's + DAC\,} & {\,\bf van Hoeij's\,} & {\,\bf {\red New}\,} \\
{Complexity} & 
 & 
 &
 & 
 &
 \\ \hline
\end{tabular}
\caption{Costs of various algorithms for the LCLM computation of~ operators of bidegrees  in .
Algorithms marked by a star () also compute cofactors for the same complexity.
}\label{fig:complexity}
\end{center}
\vskip-15pt
\end{figure*}

\medskip \noindent {\bf Main contributions.} As a first contribution, we
design a new algorithm for computing the LCLM of \emph{several\/} operators.
It reduces the LCLM computation to a linear algebra problem on a polynomial
matrix. The new algorithm can be viewed as an adaptation of Heffter's
algorithm~\cite{Heffter1896} to several operators. At the same time, we use
modern linear-algebra algorithms~\cite{Storjohann03,StVi05} to achieve a low
arithmetic complexity. Our algorithm is similar in spirit to Grigoriev's
method~\cite[\S5]{Grigoriev90} for computing GCRDs of several operators.

Before stating more precisely our main results, we need the following
conventions and notations, that will be used throughout the paper. All
algorithms take as input linear differential operators \emph{with
polynomial coefficients}, that is, belonging to ,
instead of rational function coefficients. From the computational viewpoint,
this is not too severe a restriction, since the rational coefficients case 
is easily reduced to that of polynomial coefficients, by normalisation. For
 in , we write
 
for the primitive LCLM of  in  .
We say that an operator  has bidegree at most
 in  if it has order at most~ and polynomial
coefficients of degree at most~.
The cost of our algorithms is measured by the number of
arithmetic operations that they use in the base field~. The constant
 stands for a feasible exponent for matrix multiplication
over~ (see definition in~Section~\ref{SECT:pre}), and the soft-O notation  indicates that
polylogarithmic factors are neglected. 
Our main result is the following.
\begin{theorem} \label{th:main}
	Let  be  operators in 
of bidegrees at most  in .
Then~ has order at most~, degrees in
 at most~, and it can be computed in  arithmetic operations in~. 
\end{theorem}

The upper bound  on coefficient degrees is sharp, in the sense that it is reached on \emph{generic\/} inputs. 
It improves by one order of magnitude the best bound  obtained using
previous algorithms. Moreover, for fixed , the cost of the new
algorithm is almost optimal, in the sense that it nearly matches the
arithmetic size of the LCLM. 

As a second contribution, we analyse the worst-case arithmetic complexity of
existing algorithms for LCLMs, as well as the size of their outputs. For
instance, we show that the extension to several operators 
of the ``folklore'' algorithm
in~\cite{Stanley80,SaZi94}  has complexity
. We call this extension \emph{van
Hoeij's algorithm}, after the name of the implementor in Maple's package
\verb+DEtools+ of one of its variants. These estimates are in accordance with our experiments showing
that our new algorithm performs faster for large order~, while van Hoeij's
algorithm is well suited for large~. 

Using our tight degree bounds, we also show that any algorithm that
computes the LCLM of two operators of bidegree~ in  in
complexity  can be used as the building block of
a divide-and-conquer (DAC) algorithm that computes the LCLM of  operators
of bidegree~ in complexity . The costs of several algorithms
are summarised in Figure~\ref{fig:complexity}, where notation {\bf
 + DAC} indicates that algorithm {\bf } is used in
a DAC scheme.

As a third contribution, we prove an upper bound  on the total degree in  of nonzero common left multiples
(not necessarily of minimal order). 
This is a new instance of the philosophy, initiated
in~\cite{BoChLeSaSc07}, of relaxing order minimality for linear differential
operators, in order to achieve better arithmetic size. While, by
Theorem~\ref{th:main}, the total arithmetic size of the LCLM is typically
, there exist common left multiples of total size 
only.

A fourth contribution is a fast Magma implementation
that outperforms Magma's  LCLM routine.
Experimental results 
confirm that the practical complexity of the new algorithm behaves as predicted by our
theoretical results. 

Last, but not least, we have undertaken an extensive bibliographic search,
which we now proceed to describe.

\medskip \noindent {\bf Previous work.} Libri~\cite{Libri1833} and
Brassinne~\cite{Brassinne1864} (see also~\cite{Demidov83}) defined the notions
of GCRD and LCLM of linear differential operators, and sketched a
Euclidean-type algorithm for GCRDs. Von
Escherich~\cite{Escherich1883} defined the related notion of differential
resultant of two linear differential operators. 
Articles~\cite{Brassinne1864,Escherich1883} contain the embryo of an algorithm for the LCLM
based on linear algebra; that algorithm was explicitly stated by
Heffter~\cite{Heffter1896}, and later rediscovered by Poole in his classical
book~\cite{Poole36}. The roots of a subresultant theory for differential
operators are in Pierce's articles~\cite{Pierce1903,Pierce1904}.
Blumberg~\cite{Blumberg1912} gave one of the first systematic accounts of the
algebraic properties of linear differential operators. Building on
predecessors' works, Ore~\cite{Ore32,Ore33} extended the Euclidean-type theory
to the more general framework of \emph{skew polynomials}. He
showed~\cite[Theorem~8, \S3]{Ore33} that, while the LCLM is not related to the
GCRD by a simple formula as in the commutative case, there nevertheless exists
a formula expressing the LCLM in terms of the successive remainders in the
Euclidean scheme.
Almost simultaneously, Wedderburn~\cite[\S7-8]{Wedderburn1932} showed that the LCLM can also be computed
by an \emph{extended\/} version of the Euclidean-Ore algorithm, that computes
Bézout cofactors along the way.

In the computer algebra literature, algorithmic issues for skew polynomials
emerged in the 1990s, and were popularised by Bronstein and
Petkov{\v{s}}ek~\cite{BrPe94,BrPe96}. 
Grigoriev~\cite[\S6]{Grigoriev90} designed a fast algorithm for computing the
GCRD of a family of linear differential operators; to do so, he proved tight
bounds on the degree of the GCRD, by extending von Escherich's construction of
the Sylvester matrix for two differential operators to an arbitrary number of
operators. The bound is linear in the number of operators, in their maximal
order and in their maximal degree~\cite[Lemma~5.1]{Grigoriev90}.
Giesbrecht analysed the complexity of the
LCLM computation for two operators, but only in terms of their order~\cite{Giesbrecht92,Giesbrecht98}.
(Strictly speaking, his method was proposed for a different Ore ring, but it
extends to more general settings, including the differential case.) For two
operators  of orders at most~, the first (Heffter-style)
algorithm~\cite[Lemma~5]{Giesbrecht92}  computes  in
 operations in~, while the second
one~\cite[Lemma~2.1]{Giesbrecht98} (based on the extended Euclidean-Ore scheme) uses
 operations in~. To our knowledge, no algorithm currently exists similar
to the Lehmer-Knuth-Sch\"onhage half-gcd algorithm~\cite[Chapter~11]{GaGe03},
using a number of operations in  that is quasi-linear in~. 
Li~\cite{Li98} pointed out that algorithms for the LCLM computation that have
good complexity with respect to the order, such as the naive Euclidean-Ore
algorithm, do not necessarily behave well because of coefficient growth.
He developed a generalisation of the classical subresultant theory to Ore
polynomials, that provides determinantal formulas and degree bounds for the
GCRD and the LCLM~\cite{Li98}. He also compared the practical
efficiency of Maple implementations of several algorithms.

Giesbrecht and Zhang~\cite[Theorem~2.1]{GiZh03} mention a complexity
bound of  for the LCLM computation of two operators of bidegree
 in , based on an unpublished 2002 note of Li.
Over fields of characteristic zero,
Bostan~\cite[Chapter~10]{Bostan03} sketched a general strategy for computing
several constructions on differential operators (including LCLMs), based on an
evaluation-interpolation approach on power series solutions. He stated,
without proofs, several degree bounds and complexity results. For two
operators  of bidegree  in , he announced that using
fast Hermite-Padé approximation for the interpolation step yields an algorithm
that computes  in  operations. 
The approach was enhanced by van der Hoeven~\cite{VdHoeven11},
who showed that the costs of the basic operations on
differential operators can be expressed in terms of the cost of multiplication
in , and proved
the complexity bound  stated without proof in~\cite[\S10.5]{Bostan03}.

\section{Preliminaries} \label{SECT:pre} Let  be a differential
field, that is, a field  equipped with an additive map  satisfying the Leibniz rule  for all . We denote by~ the ring of
linear differential operators over the differential field~. 
A nonzero element~ in~ is of the form  where~ with~. We call~ the \emph{order} of~, and denote it
by~.
The
noncommutative ring~ is a left (and right) principal ideal
domain, for which a Euclidean algorithm exists~\cite{Ore32,Ore33}.



Let~ be nonzero elements in~. Then~ is
called a \emph{common left multiple\/} (CLM) of~ if~ for some~. A common
left multiple of the least order is called a \emph{least common left multiple\/} (LCLM). 
Two LCLMs of~, \ldots,~ are -linearly
dependent.

\smallskip
Our main focus is on the particular case , the field of
rational functions with coefficients in , and , the
usual derivation with respect to~. In this case, we use the notation
 for , and 
for the primitive LCLM of  in  ,
that is the LCLM of  computed in  and normalised in  with trivial content.
However, in order to keep the mathematical exposition as independent as
possible of any particular case, we stick to the more general setting  whenever we discuss mathematical properties and bird's-eye view
descriptions of algorithms.

\paragraph*{\bf Polynomial and matrix arithmetic} The cost of our algorithms
will be measured by the number of field operations in  they use. To
simplify the presentation, we assume that polynomials in  (\ie, of degree
less than~ in~) can be multiplied within  operations in~, using the FFT-based algorithms
in~\cite{ScSt71,CaKa91}. Most basic polynomial operations in 
(division, extended gcd, interpolation, etc.) have cost
~\cite{GaGe03}. We suppose that  is a feasible exponent
for matrix multiplication over~, that is, a real constant , such that two  matrices with coefficients in  can be
multiplied in time . The current tightest upper bound is
~\cite{VassilevskaWilliams11}, following work of
Coppersmith and Winograd~\cite{CoWi90}, and Stothers~\cite{Stothers10}.

The following result, due to Storjohann and
Villard~\cite{Storjohann03,StVi05}, will be helpful to estimate complexities
for solving linear systems arising from LCLM computations. Note that this is
currently the best complexity result on polynomial linear algebra. 
The probabilistic aspects of the algorithms described
in this article are entirely inherited from it. 

\begin{theorem}\emph{\cite{Storjohann03,StVi05}} \label{theo:SV}
Let~ be an 
matrix with entries in . 
The rank~ of~ can be computed together with~
linearly independent polynomial elements in the left kernel of~ within
 operations in 
by a (certified) randomised Las Vegas algorithm.

Moreover, if , then the determinant of~ can be computed using  operations in .
\end{theorem}


\section{Linear formulation for \\ common left multiples} \label{SECT:mat}

In order to connect the computation of common left multiples with linear algebra, we introduce some more notation.
For a nonnegative integer~, we denote by~ the -linear
subspace of~ consisting of all linear differential operators whose orders are at most~.
Moreover, we define a -linear bijection

For a nonzero element~ of order~, and for an integer~ with~, we define
the Sylvester-type matrix

The matrix~
has~ rows and~ columns. In particular,~ is the identity
matrix of size~. This matrix enables one to express multiplication by  in~ as a vector-matrix product. Precisely, for~,



Let~ be nonzero elements in~.
For , the  matrix

has~ rows and~ columns.

\smallskip The following theorem is the main result of this section.
\begin{theorem} \label{TH:mat}
Let~ be  elements in~ of  orders~,
and let~
\begin{enumerate}
\item[(i)]
If~ is a common left multiple of~ such that  and 

then the vector~
belongs to the left kernel of the matrix~ defined in~\eqref{EQ:mat}.
\item[(ii)] If the vector~ is a nonzero vector in the left kernel of~,
where~ for~ and~, then~ is nonzero,
and~ is a common
left multiple of~, \ldots,~ with respective left cofactors~,
\ldots,~.
\item[(iii)] If~ is the rank of~, then\\

\end{enumerate}
\end{theorem}
\myproof Suppose that  for~.
By~\eqref{EQ:multiply},

which is equivalent to

Therefore the vector~ belongs to the left kernel of~.
The first assertion is proved.

Conversely, suppose that~ is a nonzero vector in the left kernel of~.
Then~ for all~ with~.
It follows from~\eqref{EQ:multiply} that

Thus,  is nonzero, for otherwise, since  is an isomorphism, all the  would be equal to zero.
The second assertion follows.

To prove the last assertion, we set~ and~.
Assume further that~ for all~ with~.
Then~, \ldots,  are common left  multiples of~ \ldots,  of orders at most~, and such that

By the first assertion, for , the vector

belongs to the left kernel of~. These vectors are -linearly independent because~ , \ldots, 
are. 
On the other hand, if~ is a nonzero vector in the left kernel of~,
where~ and~, then~ is
a common left multiple of~, \ldots,~ with order no greater than~ by the second assertion.
Hence,~ is a -linear combination of~, , \ldots,~,
because it is a left multiple of~.
Hence, there exist~, , \ldots,  in~ such
that

which implies that the last  coordinates of the vector

are all equal to zero. Since this vector belongs to
the left kernel of~, all its coordinates are zero by the second assertion.
We conclude that  is a -basis of the left kernel of , and thus~ is its dimension. Then (iii) follows from the rank-nullity theorem, because~
has~ rows.
\foorp

\medskip
Since the rank of~ is at most~, a direct consequence of Theorem~\ref{TH:mat}~(iii) is the following classical result.
\begin{cor} \label{COR:bounds} 
For~,

\end{cor}



\section{Computing LCLMs} \label{SECT:comp}
In this section, we review a few known methods for computing LCLMs and present a new one based on
Theorem~\ref{TH:mat}.

\subsection{Computing an LCLM of two operators} \label{SUBSECT:comp2}
Given two nonzero elements~ and~ of respective orders~ and~, we consider various methods for computing their LCLMs.
The first methods  compute left cofactor(s) of
the given operator(s) first, and find an LCLM by multiplication in~. The last method is specific to .

\subsubsection{Heffter's algorithm} 
The first method can be traced back to Brassinne~\cite{Brassinne1864}, von Escherich~\cite{Escherich1883} and Heffter~\cite{Heffter1896}.
The sequence:

has~ elements, each of which is of order at most~.
Thus, these elements are -linearly dependent. To compute , the strategy is to 
find the maximal integer~ and corresponding elements~, 
with~ such that

Set

Then~. Therefore, the product~ is an LCLM of~ and~ due to the maximality of~.

This method can be reformulated using the notation introduced in Section~\ref{SECT:mat}.
For a vector~ represented by

in a finite-dimensional -vector space equipped with the standard basis

\sloppy we define~ to be~.
For~, define

Then, Heffter's method consists in computing a vector~ in the left kernel of~
such that~ is maximal.

The next lemma connects the order of~ with the rank of~. 
It easily follows from the observation that a maximal subset of
-linearly independent elements in~
consists of~ \ldots,  and~, where~.
\begin{lemma} \label{LM:prank}
Let~ be two nonzero elements in  of orders~.
Then 
\end{lemma}

\subsubsection{Euclidean algorithms}
The second family of methods is based on the Euclidean-Ore algorithm for differential operators~\cite{Ore33}. 

\medskip \noindent {\bf Ore's algorithm.} Assume that~. Setting~ , one can compute the Euclidean (right) divisions 

for quotients~, and remainders~ satisfying  for~, and . Then, as in the commutative case,  is shown to be the GCRD of  and . 
Ore~\cite[\S2]{Ore33} proved that the following product 
 is an LCLM of  and . (Here  denotes the exact left quotient of  and , that is  such that .)

\medskip \noindent {\bf Extended Euclidean-Ore algorithm.} Wedderburn~\cite[\S7-8]{Wedderburn1932} observed (see also~\cite{BrPe94})
that the
computation of~\eqref{eq:Ore} can be avoided, if one replaces the Euclidean
algorithm by its extended version. Precisely, letting , , and
   the product~ is an LCLM of~ and~.

\medskip \noindent {\bf Li's determinantal expression.}
As in the commutative case, a more efficient version of the extended Euclidean-Ore algorithm is based on subresultants~\cite[\S5]{Li98}.
To avoid technicalities, we present an alternative, efficient, variant of the subresultant algorithm, 
based on a determinantal formulation~\cite[Proposition~6.1]{Li98}. This method assumes that the order~ of the GCRD of  and  is already known. Then, one constructs a square matrix  of size  whose first  columns are the first  columns of the matrix 

and whose last column is the transpose of the vector


\noindent If  is denoted~, then   is an LCLM of  and ~.

\subsubsection{Van der Hoeven's algorithm}
The algorithm that we very briefly mention now is specific to the case , where the base field  has characteristic zero. It works by evaluation-interpolation. The idea, originating
from~\cite{Bostan03}, is to perform operations on differential operators by
working on their fundamental systems of solutions. Due to space limitations, and in view of its complexity analysis,
we do not give more details here, and refer the reader to the
article~\cite{VdHoeven11}.

\subsection{Computing an LCLM of several operators} \label{SUBSECT:comps}
Given several nonzero operators~ 
we describe various ways to compute~.

\subsubsection{Iterative LCLMs}
An obvious method is to compute an LCLM of  operators iteratively, that is,
	

A computationally more efficient (though mathematically equivalent) method is by a divide-and-conquer algorithm, based on the repeated use of the formula


Of course, the efficiency of an iterative algorithm depends on that of the algorithm used for the LCLM of two operators. This is quantified precisely in Section~\ref{sec:ABC}.

\subsubsection{Van Hoeij's algorithm}
Another algorithm for computing the LCLM of ~linear differential operators was implemented by van Hoeij as Maple's \verb+DEtools[LCLM]+ command; it seemingly was never published.
For , the method is folklore; it is implicit, for instance, in the proof of~\cite[Theorem~2.3]{Stanley80}. A variant of it is also implemented  by the \texttt{`diffeq+diffeq`} command in Salvy and Zimmermann's \verb+gfun+ package for Maple~\cite{SaZi94}.

Informally speaking, the method consists in considering a generic solution~ of~ for~, then in finding the first linear dependency between the row vectors .
In order to perform actual computations, these vectors are represented by the canonical forms

where ~denotes the remainder of the right Euclidean division of~ by~.
Let 
where~ are undetermined coefficients in .
For all~ with~, let~ be the right remainder
of~ in the division by~. Then 
Since~ has generic coefficients,   is equal to , e.g., by ~\cite[Lemma~2.3]{Li98}.
Note that~ depend linearly on the coefficients of the~'s.
There are  coefficients in~.
Equating~, for   we obtain a linear system

where~ is an  matrix over~. Thus, computing~
amounts to computing a nontrivial vector~ in the left kernel of~ with~ being maximal.
The rank of~ is equal to the order of~,
e.g., by~\cite[Proposition~4.3]{AbLeLi05}. Note that the original version of van Hoeij's algorithm does not make use of this last fact, and potentially needs to solve more linear systems, thus being less efficient when the LCLM is not of maximal order. 

\subsubsection{The new algorithm}
As a straightforward consequence of Theorem~\ref{TH:mat}, the  can be computed
by determining a nontrivial vector~ in the left kernel of~ given in~equation~\eqref{EQ:mat},
with~ being maximal. This method computes not only the LCLM, but also its left cofactors~,
, \ldots, , while van Hoeij's algorithm does not
compute any cofactor.

\section{\!\!\!\!\!\! Algorithms, bounds, complexity} \label{sec:ABC}


\begin{figure*} 
\begin{center} \renewcommand{\arraystretch}{3.7}
\tabcolsep2pt
	\begin{tabular}{|c|c|c|} \hline
	{
			\begin{minipage}{5.3cm}
			\smallskip
			\center{\bf Heffter's algorithm} 

			\medskip
			\begin{enumerate}
			\item Compute the matrix~ defined in~\eqref{EQ:mat2}. 
			\item Determine its rank~; set . \label{step:rankHeffter}
			\item Extract submatrix~ of~.
			\item Find the 1-dim kernel  of~. \label{step:kernelHeffter} 
			\item Construct 
			 
			from the first~ coordinates of .
			\item Compute and return .\label{step:mulHeffter}
			\end{enumerate}
			\end{minipage}
	}
		& 
		{
		\begin{minipage}{5.3cm}
		\smallskip
		\center{\bf van Hoeij's algorithm} 
		\smallskip
		\begin{enumerate}
		\item For all~ and~, compute
		
		\item View the  as rows in ; compute  rank  of	.\label{step:rankHoeij}
		\item Extract  submatrix~ of~.
		\item Find the 1-dim kernel  of~.\label{step:kernelHoeij}
		\item Construct the LCLM from .
		\end{enumerate}
		\end{minipage}
}	
		&   
		{
		\begin{minipage}{5.3cm}

		\medskip
		\center{\bf Our new algorithm}

		\medskip
		\begin{enumerate}
		\item Compute~ defined in~\eqref{EQ:mat}. 
		\item Determine  its rank~ ; set~.\label{step:rankNew}\item Extract submatrix~ of~.
		\item Find the 1-dim kernel  of~.\label{step:kernelNew}
		\item Construct the LCLM from the last~ coordinates of .
		\item Return the LCLM.
		\end{enumerate}
		\end{minipage}
} \\ 
\hline
\end{tabular}
\caption{Pseudo-code for Heffter's algorithm, van Hoeij's algorithm and our new algorithm.}
\label{fig:Heffter+vH+new}
\end{center}
\end{figure*}



In this section, we let~ be the field of rational functions with
coefficients in a field~, and  be the usual
derivation with respect to~. Recall that in this case we use the notation
 for , and 
for the \emph{primitive\/} LCLM of  in  .

All algorithms analysed below are specialisations of the algorithms reviewed
in the previous section to . Moreover,  we make the non-restrictive assumption that all algorithms take as input linear differential operators
\emph{with polynomial coefficients}, that is, belonging to .

The degree of a nonzero operator , denoted ,
is defined as the maximal degree of its coefficients.
As in the case of usual commutative polynomials,


\subsection{Tight degree bounds for the LCLM}
First, we give a sharp degree bound for LCLMs. As we show later, this bound improves upon the bound that can be derived from van Hoeij's algorithm.
\begin{theorem} \label{TH:db}
Let~ be operators in~. Let

If~, then

\end{theorem}

\myproof
By Corollary~\ref{COR:bounds}, . It follows from Theorem~\ref{TH:mat}
and Cramer's rule
that every nonzero coefficient of~ is a quotient of two 
minors of~. Note that every square submatrix of~ has size at
most~, since~ has~ rows and~ columns. Thus, the
degree of the determinant of such a submatrix is bounded by~,
because every entry of~ is of degree at most~, and the
last~ rows of~ are free of~. \foorp

As a consequence of Corollary~\ref{COR:bounds} and Theorem~\ref{TH:db}, 
the first part of Theorem~\ref{th:main} is easily deduced.

\subsection{LCLMs of two operators}
The following result encapsulates complexity analyses of LCLM algorithms for two operators. Heffter's, van Hoeij's and our new algorithm are summarised in Figure~\ref{fig:Heffter+vH+new}.

\begin{theorem}\label{theo:compl-2ops}
Let  be operators of bidegrees at most  in .	
Then it is possible to compute the LCLM of  and  in complexity
\begin{tabbing}
	\quad \emph{(a)}  by Heffter's and van der Hoeven's \\ algorithms,\\
	\quad \emph{(b)}  by Li's and by van Hoeij's algorithms,\\
	\quad \emph{(c)}  by the new algorithm.
\end{tabbing}
\end{theorem}

\myproof By~\cite[Theorems~5,~8 \& 23]{VdHoeven11}, and using bounds from
Theorem~\ref{TH:db}, the complexity of van der Hoeven's algorithm is
.
The most costly parts of Heffter's algorithm are
Steps~\ref{step:rankHeffter},~\ref{step:kernelHeffter}
and~\ref{step:mulHeffter}. Since the matrix~ has size~ and polynomial coefficients of degree at most~, the rank and kernel computations involved in Steps~\ref{step:rankHeffter} and~\ref{step:kernelHeffter} can be performed using  operations, by Theorem~\ref{theo:SV}. Step~\ref{step:mulHeffter} consists in 
multiplying two operators in  of bidegrees at most  and  in . This can be done using
. This proves (a). 

The dominant parts of Li's algorithm are the computation of , and the expansion of  minors of a polynomial matrix of size  and degree at most~. By using~\cite[Lemma~5.1]{Grigoriev90} and Theorem~\ref{theo:SV},  can be computed using  operations in , and the minors can be expanded in  .
The dominant parts of van Hoeij's algorithm are
Steps~\ref{step:rankHoeij} and~\ref{step:kernelHoeij}.
Since , matrix~ has size . By an easy induction, its th row has polynomial coefficients of degrees at most~, thus  has degree . By Theorem~\ref{theo:SV}, the rank and kernel computations have complexity . This proves (b).

The dominant parts of the new algorithm are
Steps~\ref{step:rankNew} and~\ref{step:kernelNew}. Since , the polynomial matrix~ has size  and degree at most . By Theorem~\ref{theo:SV} again, the rank and kernel computations have cost . This completes the proof.
\foorp

Quite surprisingly, the costs of Heffter's and of van der Hoeven's
algorithms are penalised by the complexity of multiplication of operators,
which is not well-understood yet for general bidegrees. Precisely,
it is an open problem whether two operators of bidegree  in
 can be multiplied in nearly optimal time . If such an algorithm were discovered, then the costs of both
algorithms would become , improving the
corresponding entries in Figure~\ref{fig:complexity}.

\subsection{LCLMs of several operators} \label{ssec:cost-sev} 
We analyse three algorithms for LCLMs of several operators: 
DAC, van Hoeij's and our new algorithm. 

\subsubsection{LCLMs by divide-and-conquer} \label{ssec:iter}

\vspace{-0.2cm}

\begin{theorem} \label{theo:DAC}
	Suppose that we are given an algorithm  computing the LCLM
of two differential operators which, on input  of bidegree at most  in , computes 
in complexity  for some constants  and  independent of~ and~. 

There exists an algorithm which, on input  of bidegrees at most  in , computes  using  operations in .
\end{theorem}

\myproof
Suppose without loss of generality that  is a power of~.	
To compute , we  use a strategy based on~\eqref{eq:iterativeByDAC}, similar to that of the subproduct tree~\cite[\S10.1]{GaGe03}: we partition the family  into pairs, compute the LCLM of each pair using algorithm~ available for two operators, remove the polynomial content, then compute LCLMs of pairs, and so on. Let  denote the LCLM of  with the content removed.
At level~1, the algorithm computes the  operators , at level~2 the  operators , and so on, the last computation at level~ being that of  as the LCLM of  and . Let  denote the complexity of algorithm~ on inputs of bidegrees at most .
By Theorem~\ref{TH:db}, the operators computed at level~ have bidegree at most~.
Thus, the total cost of the DAC algorithm on  inputs of bidegree at most  is bounded by

plus the cost of the content removal, which is negligible. 
Up to polylogarithmic factors, the cost is bounded by

which is . This concludes the proof.
\foorp	

The cost of the algorithm is essentially that of its last step; this is a typical feature of DAC
algorithms.
A similar analysis shows that the iterative algorithm based on
formula~\eqref{eq:iterative} is less efficient, and has complexity
.

\smallskip As a corollary of Theorems~\ref{theo:compl-2ops}
and~\ref{theo:DAC}, we get a proof of the complexity estimates in the
first three entries of Figure~\ref{fig:complexity}.

\subsubsection{Van Hoeij's and the new algorithm}
\vspace{-0.3cm}
\begin{theorem}\label{theo:compl-kops}
Let  have bidegrees at most  in .	
One can compute  
\begin{tabbing}
	\quad \emph{(a)} in  operations by van Hoeij's algorithm,	\\
	\quad \emph{(b)} in  operations by the new algorithm. 
\end{tabbing}
\end{theorem}

\myproof
The proof is similar to that of~Theorem~\ref{theo:compl-2ops}(b) and~(c).
The most costly parts of van Hoeij's algorithm are
Steps~\ref{step:rankHoeij} and~\ref{step:kernelHoeij}.
Matrix~ has size  and  polynomial coefficients of degree~. By Theorem~\ref{theo:SV}, the rank and kernel computations have complexity . This proves (a).
The dominant parts of the new algorithm are
Steps~\ref{step:rankNew} and~\ref{step:kernelNew}. The polynomial matrix~ has size  and degree at most . By Theorem~\ref{theo:SV}, the rank and kernel computations have cost . 
\foorp

\smallskip
As a corollary of Theorem~\ref{theo:compl-kops}, we
get a proof of the complexity estimates in the last two entries of Figure~\ref{fig:complexity}. Note that Cramer's rule applied to the matrix  analysed in the previous proof yields the bound  on the coefficient degrees of the LCLM. This bound is improved by Theorem~\ref{th:main}.

\vspace{-0.1cm}

\begin{section}{\!\!\!\! Smaller common left multiples}

Our approach to computing more common left multiples (CLMs), that are generally
not of minimal order, but smaller in total arithmetic size than the LCLM, is
similar to the linear-algebraic approach used in Section~\ref{SECT:mat}.
However, instead of considering a matrix encoding the~, with
\emph{polynomial\/} coefficients, we turn our attention to a matrix encoding
the~, with \emph{constant\/} coefficients.

\medskip \noindent {\bf Existence of smaller CLMs.} The new building block to consider is, for an operator~ in~ of total degree~ in  and~, and
an integer~, the 
matrix~ with scalar coefficients whose rows represent the operators
of the form~ for~, in any fixed
order, and whose columns are indexed by the monomials of total degree at
most~, in any fixed order.

Let , \dots,  be elements of~,  with respective total degrees , \dots, .
For~, let  be the matrix
\vspace{-0.3cm}

This matrix has ~rows and~~columns, where


Assuming all~ equal to a same value~, the matrix~ certainly has a nontrivial left kernel when~, that is when 
which happens when

where the approximation holds for large values of  or~.

\smallskip Using   yields the main result of this section.

\vspace{-0.3cm}
\begin{theorem} \label{th:CLM}
Let~ be elements in~ of orders at most~, and with coefficients of degrees at most~.
There exist nonzero common left multiples of total degree~ in
, and total arithmetic size .
\end{theorem}

\vspace{-0.3cm}
\medskip \noindent {\bf Algorithms for CLMs.} A simple algorithm for computing  common left multiples of total degree~ in  is  based on the left kernel
computation of the scalar matrix~. This matrix has sizes of order . The cost of the procedure is ; it is dominated by the kernel computation.

\smallskip To simplify the discussion, we assume in the remaining of the
section that  have bidegrees at most . 
Then, Theorem~\ref{th:CLM} implies that, while the LCLM has
order at most  and degrees at most , there exist common left
multiples of order and degree at most .
However, computing such a small multiple by the previous algorithm of
complexity  is more costly than computing the
LCLM by the last two algorithms in Figure~\ref{fig:complexity}. 

Here we briefly sketch a faster algorithm for computing a common left multiple
of order and degree at most , based on Hermite-Pad\'e
approximation~\cite[Chapter~10]{Bostan03}. One determines series
solutions of the~ at order~, takes a random linear
combination~ of them, computes its first  derivatives, and outputs a
Hermite-Padé approximant of  of type . The dominant complexity is that of the Hermite-Padé step,
~\cite{Storjohann06}.

\medskip \noindent {\bf A Fast LCLM Heuristic.} 
As an interesting consequence of this fast CLM computation, we
deduce a very efficient heuristic for LCLMs, asymptotically
faster than all algorithms in Figure~\ref{fig:complexity}. It proceeds in 3 steps:
() compute~ CLMs of order and degree at most ;
() take two random linear combinations with coefficients in ; ()
return their GCRD. The dominating steps are~() and (). By using
Hermite-Padé approximants for step () and Grigoriev's
algorithm~\cite{Grigoriev90} combined with Theorem~\ref{theo:SV} for step~(), the total complexity is
. This is nearly optimal, in view of
the LCLM size . 
However, we are not yet able to turn this heuristic into a fully proved algorithm.

\end{section}



\vspace{-0.1cm}

\section{Experiments}

We implemented\footnote{All computer calculations were performed on a
Quad-Core Intel Xeon X5160 processor at 3GHz, with 8GB of RAM.} two variants
of our new algorithm in Magma V2.16-7~\cite{magma} and compared them with
Magma's built-in LCLM routine (command \verb+LeastCommonLeftMultiple+).

Some experimental results are summarised in Table~\ref{tab:Magma-lclms}. We
take as input  \emph{random\/} operators in ,
each of bidegree~ in , where  is a medium-sized
prime and  is of the form , for .
Column {\sf New} gives timings for the first variant of the new algorithm,
that uses Magma's built-in polynomial linear algebra solver (the \verb+Kernel+
routine), while column {\sf New+S} gives timings for the second variant, based
on our own high-level implementation of Storjohann's \emph{high-order lifting
algorithm}~\cite{Storjohann03}. Column  displays the size~ and the
degree~ of the polynomial matrix dealt with by algorithms {\sf New} and
{\sf New+S}. The dominating part of these algorithms is the left kernel
computation for a polynomial matrix of size~ and degree~.
The most time consuming part of~{\sf New+S} consists in  polynomial
matrix multiplications of size  and degree . To facilitate comparisons,
column  shows the total time taken by 10 products of random
polynomial matrices of size~ and degree~ over~. Finally, column
{\sf output size} displays the total arithmetic size of the computed LCLM,
that is, its number of coefficients in~.

Several conclusions can be drawn from
Table~\ref{tab:Magma-lclms}. First, Magma's LCLM tool exhibits an
\emph{exponential\/} arithmetic complexity behaviour (when passing from
bidegree  to , timings are multiplied by a factor close
to~), but it is relatively efficient for small input sizes. Both variants
of the new algorithm are faster for , and {\sf New+S} gains a factor
65 for , and almost 1300 for .

Second, timings in column {\sf New} exhibit a practical complexity
proportional to~, which is inherited from Magma's linear algebra solver
on polynomial matrices. In contrast, {\sf New+S} has a practical complexity
proportional to  (but with a higher proportionality factor). This
good behaviour, closer to the theoretical complexity 
predicted by Theorem~\ref{th:main}, is inherited from Magma's very efficient
polynomial matrix multiplication, through Storjohann's algorithm.

Finally, timings in column {\sf New+S} grow nearly linearly in the
corresponding output sizes given in the last column, and these sizes match
exactly the sharp bounds in Theorem~\ref{th:main}. This experimentally
confirms that size bounds and worst-case complexity analyses predicted by our
theoretical results 
are reached in \emph{generic\/} cases.

\begin{table}[t]
\begin{scriptsize}
\tabcolsep2pt
\begin{center}
\begin{tabular}{r|ccccccc}
 & \sf Magma's LCLM & \sf New &  \sf New+S & & & \sf output size \\
\hline
 2 & 0.01 & 0.00 & 0.01 & (2,10) & 0.01 & 65 &  \\
 3 & 0.01 & 0.01 & 0.03 & (3,14) & 0.01 & 175 &  \\
 4 & 0.02 & 0.01 & 0.07 & (4,18) & 0.03 & 369 &  \\
 6 & 0.10 & 0.06 & 0.17 & (6,26) & 0.06 & 1105 &  \\
 8 & 0.49 & 0.19 & 0.54 & (8,34) & 0.15 & 2465 &  \\
12 & 6.84 & 0.91 & 1.37 & (12,50) & 0.41 & 7825 &  \\
16 & 49.24 & 3.48 & 4.93 & (16,66) & 0.91 & 17985 &  \\
23 & 718.02 & 20.51 & 11.09 & (23,94) & 2.60 & 51935 &  \\
32 & 9355.47 & 115.53 & 40.83 & (32,130) & 6.73 & 137345 &  \\ 
46 & 168434.66 & 791.01 & 130.40 & (46,186) &  21.51 & 402225 \\
\end{tabular}
\end{center}
\end{scriptsize}
\vskip-12pt
\begin{small}
\caption{Timings (in sec.) for LCLMs of  random operators in  of bidegrees  in . }\label{tab:Magma-lclms} 
\end{small}
\end{table}

\scriptsize

\def\cprime{'} \def\cprime{}
\begin{thebibliography}{10}

\medskip 

\bibitem{AbLeLi05}
S.~Abramov, H.~Le, and Z.~Li.
\newblock Univariate {O}re polynomial rings in {C}omputer {A}lgebra.
\newblock {\em J. Math. Sci.}, 131(5):5885--5903, 2005.

\bibitem{BaChLo03}
M.~Barkatou, F.~Chyzak, and M.~Loday-Richaud.
\newblock Remarques algorithmiques li\'ees au rang d'un op\'erateur
  diff\'erentiel lin\'eaire.
\newblock In {\em From combinatorics to dynamical systems}, volume~3 of {\em
  IRMA Lect. Math. Theor. Phys.}, pages 87--129. Berlin, 2003.

\bibitem{Blumberg1912}
H.~Blumberg.
\newblock {\em {\"U}ber algebraische {E}igenschaften von linearen homogenen
  {D}ifferentialausdr{\"u}cken}.
\newblock PhD thesis, Universit{\"a}t G{\"o}ttingen, 1912.

\bibitem{magma}
W.~Bosma, J.~Cannon, and C.~Playoust.
\newblock The {M}agma algebra system. {I}. {T}he user language.
\newblock {\em J. Symbolic Comput.}, 24(3-4):235--265, 1997.

\bibitem{Bostan03}
A.~Bostan.
\newblock {\em Algorithmique efficace pour des op{\'e}rations de base en Calcul
  formel.}
\newblock PhD thesis, {\'E}cole polytechnique, 2003.

\bibitem{BoChLe08}
A.~Bostan, F.~Chyzak, and N.~Le~Roux.
\newblock Products of ordinary differential operators by evaluation and
  interpolation.
\newblock In {\em I{SSAC}'08}, pages 23--30. ACM Press, New York, 2008.

\bibitem{BoChLeSaSc07}
A.~Bostan, F.~Chyzak, G.~Lecerf, B.~Salvy, and {\'E}.~Schost.
\newblock Differential equations for algebraic functions.
\newblock In {\em I{SSAC}'07}, pages 25--32. ACM Press, New York, 2007.

\bibitem{Brassinne1864}
E.~Brassinne.
\newblock Analogie des \'equations diff\'erentielles lin\'eaires \`a
  coefficients variables, avec les \'equations alg\'ebriques.
\newblock In {\em Note III du Tome 2 du Cours d'analyse de Ch. Sturm, {\'E}cole
  polytechnique, 2\`eme \'edition}, pages 331--347, 1864.

\bibitem{BrPe94}
M.~Bronshte{\u\i}n and M.~Petkovshek.
\newblock Ore rings, linear operators and factorization.
\newblock {\em Programmirovanie}, (1):27--44, 1994.

\bibitem{BrPe96}
M.~Bronstein and M.~Petkov{\v{s}}ek.
\newblock An introduction to pseudo- li\-near algebra.
\newblock {\em Theoret. Comput. Sci.}, 157(1):3--33, 1996.

\bibitem{CaKa91}
D.~G. Cantor and E.~Kaltofen.
\newblock On fast multiplication of polynomials over arbitrary algebras.
\newblock {\em Acta Inform.}, 28(7):693--701, 1991.

\bibitem{ClHo04}
T.~Cluzeau and M.~van Hoeij.
\newblock A modular algorithm for computing the exponential solutions of a
  linear differential operator.
\newblock {\em J. Symbolic Comput.}, 38(3):1043--1076, 2004.

\bibitem{CoWi90}
D.~Coppersmith and S.~Winograd.
\newblock Matrix multiplication via arithmetic progressions.
\newblock {\em J. Symb. Comput.}, 9(3):251--280, 1990.

\bibitem{Demidov83}
S.~S. Demidov.
\newblock On the history of the theory of linear differential equations.
\newblock {\em Arch. Hist. Exact Sci.}, 28(4):369--387, 1983.

\bibitem{Escherich1883}
G.~\gathen{von} Escherich.
\newblock {\"U}ber die {G}emeinsamkeit particul{\"a}rer {I}nte\-grale bei zwei
  linearen {D}ifferentialgleichungen.
\newblock {\em {\"O}sterreichische Akademie der Wissenschaften.
  Mathematisch-Naturwissenschaftliche Klasse}, 46:61--82, 1883.

\bibitem{GaGe03}
J.~\gathen{von zur} Gathen and J.~Gerhard.
\newblock {\em Modern Computer Algebra}.
\newblock Cambridge University Press, Cambridge, second edition, 2003.

\bibitem{Giesbrecht92}
M.~Giesbrecht.
\newblock Factoring in skew-polynomial rings.
\newblock In {\em LATIN '92}, volume 583 of {\em LNCS}, pages 191--203. 1992.

\bibitem{Giesbrecht98}
M.~Giesbrecht.
\newblock Factoring in skew-polynomial rings over finite fields.
\newblock {\em J. Symbolic Comput.}, 26(4):463--486, 1998.

\bibitem{GiZh03}
M.~Giesbrecht and Y.~Zhang.
\newblock Factoring and decomposing {O}re polynomials over {}.
\newblock In {\em ISSAC'03}, pages 127--134, 2003.

\bibitem{Grigoriev90}
D.~Y. Grigor'ev.
\newblock Complexity of factoring and calculating the {GCD} of linear ordinary
  differential operators.
\newblock {\em J. Symbolic Comput.}, 10(1):7--37, 1990.

\bibitem{Heffter1896}
L.~Heffter.
\newblock Ueber gemeinsame {V}ielfache linearer {D}ifferentialausdr{\"u}cke und
  lineare {D}ifferentialgleichungen derselben {K}lasse.
\newblock {\em J. Reine Angew. Math.}, 116:157--166, 1896.

\bibitem{VdHoeven02}
J.~\hoeven{van der} Hoeven.
\newblock F{FT}-like multiplication of linear differential operators.
\newblock {\em J. Symbolic Comput.}, 33(1):123--127, 2002.

\bibitem{VdHoeven11}
J.~\hoeven{van der} Hoeven.
\newblock On the complexity of skew arithmetic, 2011.
\newblock Technical Report, HAL 00557750, v1.

\bibitem{Le03}
H.~Q. Le.
\newblock A direct algorithm to construct the minimal {}-pairs for rational
  functions.
\newblock {\em Adv. Appl. Math.}, 30(1-2):137--159, 2003.

\bibitem{Li98}
Z.~Li.
\newblock A subresultant theory for {O}re polynomials with applications.
\newblock In {\em ISSAC'98}, pages 132--139. ACM Press, 1998.

\bibitem{Libri1833}
G.~Libri.
\newblock M\'emoire sur la r\'esolution des \'equations alg\'ebriques dont les
  racines ont entre elles un rapport donn\'e, et sur l'int\'egration des
  \'equations diff\'erentielles lin\'eaires dont les int\'egrales
  particuli\`eres peuvent s'exprimer les unes par les autres.
\newblock {\em J. Reine Angew. Math.}, 10:167--194, 1833.

\bibitem{Ore32}
O.~Ore.
\newblock Formale {T}heorie der linearen {D}ifferentialgleichungen.
\newblock {\em J. Reine Angew. Math.}, 167:221--234, 1932.

\bibitem{Ore33}
O.~Ore.
\newblock Theory of non-commutative polynomials.
\newblock {\em Ann. of Math.}, 34(3):480--508, 1933.

\bibitem{Pierce1903}
A.~B. Pierce.
\newblock Sufficient {C}ondition that two {L}inear {H}omogeneous {D}ifferential
  {E}quations shall have {C}ommon {I}ntegrals.
\newblock {\em Amer. Math. Monthly}, 10(3):65--68, 1903.

\bibitem{Pierce1904}
A.~B. Pierce.
\newblock The necessary and sufficient conditions under which two linear
  homogeneous differential equations have integrals in common.
\newblock {\em Ann. of Math. (2)}, 6(1):17--29, 1904.

\bibitem{Poole36}
E.~G.~C. Poole.
\newblock {\em Introduction to the theory of linear differential equations}.
\newblock Oxford Univ. Press, London, 1936.

\bibitem{SaZi94}
B.~Salvy and P.~Zimmermann.
\newblock Gfun: a {M}aple package for the manipulation of generating and
  holonomic functions in one variable.
\newblock {\em ACM Trans. Math. Software}, 20(2):163--177, 1994.

\bibitem{ScSt71}
A.~Sch{\"o}nhage and V.~Strassen.
\newblock {S}chnelle {M}ultiplikation gro\ss er {Z}ahlen.
\newblock {\em Computing}, 7:281--292, 1971.

\bibitem{Stanley80}
R.~P. Stanley.
\newblock Differentiably finite power series.
\newblock {\em European J. Combin.}, 1(2):175--188, 1980.

\bibitem{Storjohann03}
A.~Storjohann.
\newblock High-order lifting and integrality certification.
\newblock {\em J. Symbolic Comput.}, 36(3-4):613--648, 2003.

\bibitem{Storjohann06}
A.~Storjohann.
\newblock Notes on computing minimal approximant bases.
\newblock In {\em Challenges in Symbolic Computation Software}, number 06271 in
  Dagstuhl Seminar Proceedings, 2006.

\bibitem{StVi05}
A.~Storjohann and G.~Villard.
\newblock Computing the rank and a small nullspace basis of a polynomial
  matrix.
\newblock In {\em ISSAC'05}, pages 309--316. ACM Press, New York, 2005.

\bibitem{Stothers10}
A.~Stothers.
\newblock {\em On the Complexity of Matrix Multiplication}.
\newblock PhD thesis, University of Edinburgh, 2010.

\bibitem{VassilevskaWilliams11}
V.~{Vassilevska Williams}.
\newblock Breaking the {C}oppersmith-{W}inograd barrier, 2011.
\newblock \url{http://cs.berkeley.edu/~virgi/matrixmult.pdf}.

\bibitem{Wedderburn1932}
J.~H.~M. Wedderburn.
\newblock Non-commutative domains of integrity.
\newblock {\em J. Reine Angew. Math.}, 167:129--141, 1932.

\end{thebibliography}

\end{document}