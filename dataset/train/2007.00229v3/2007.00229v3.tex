

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{eacl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{451} 

\usepackage{url}
\usepackage{multirow,booktabs,makecell}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{color}
\usepackage{comment}
\usepackage{subcaption}
\usepackage[T1]{fontenc}
\usepackage{enumitem}

\setlength\abovecaptionskip{2pt}
\interfootnotelinepenalty=10000

\newcommand{\FIXME}[1]{\textcolor{red}{[#1]}}
\newcommand{\ww}[1]{\textcolor{cyan}{william: {#1}}}
\newcommand{\xin}[1]{\textcolor{red}{Xin: {#1}}}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 




\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Multimodal Text Style Transfer for\\ Outdoor Vision-and-Language Navigation}



\author{Wanrong Zhu\textsuperscript{\dag}, Xin Eric Wang\textsuperscript{\ddag}, Tsu-Jui Fu\textsuperscript{\dag}, An Yan\textsuperscript{\S}, 
\\ \textbf{Pradyumna Narayana\textsuperscript{*}, 
Kazoo Sone\textsuperscript{*}, Sugato Basu\textsuperscript{*}, William Yang Wang\textsuperscript{\dag}} \\
\textsuperscript{\dag}UC Santa Barbara, 
\textsuperscript{\ddag}UC Santa Cruz,
\textsuperscript{\S}UC San Diego,
\textsuperscript{*}Google \\
\texttt{\small \{wanrongzhu,tsu-juifu,william\}@cs.ucsb.edu}, 
\texttt{\small xwang366@ucsc.edu},\\
\texttt{\small ayan@eng.ucsd.edu},
\texttt{\small \{pradyn,sone,sugato\}@google.com}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

One of the most challenging topics in Natural Language Processing (NLP) is visually-grounded language understanding and reasoning. Outdoor vision-and-language navigation (VLN) is such a task where an agent follows natural language instructions and navigates a real-life urban environment. Due to the lack of human-annotated instructions that illustrate  intricate urban scenes, outdoor VLN remains a challenging task to solve.
This paper introduces a Multimodal Text Style Transfer (MTST) learning approach and leverages external multimodal resources to mitigate data scarcity in outdoor navigation tasks. 
We first enrich the navigation data by transferring the style of the instructions generated by Google Maps API, then pre-train the navigator with the augmented external outdoor navigation dataset.
Experimental results show that our MTST learning approach is model-agnostic, and our MTST approach significantly outperforms the baseline models on the outdoor VLN task, improving task completion rate by 8.7\% relatively on the test set. 
\footnote{Our code and dataset is released at \url{https://github.com/VegB/VLN-Transformer}.}
\end{abstract}

\section{Introduction}

A key challenge for Artificial Intelligence research is to go beyond static observational data and consider more challenging settings that involve dynamic actions and incremental decision-making processes~\cite{pearl2018book}.
Outdoor vision-and-language navigation (VLN) is such a task, where an agent navigates in an urban environment by grounding natural language instructions in visual scenes, as illustrated in Fig.~\ref{fig:style_transfer_example}. 
To generate a series of correct actions, the navigation agent must comprehend the instructions and reason through the visual environment. 

Different from indoor navigation ~\cite{anderson2018r2r,Wang-2018,fried2018speakerfollower,wang2019reinforced,ma2019self,tan2019learning,ma2019regretful,ke2019tactical}, the outdoor navigation task takes place in urban environments that contain diverse street views~\cite{mirowski2018learning,chen2019touchdown,mehta2020retouchdown}. 
The vast urban area leads to a much larger space for an agent to explore and usually contains longer trajectories and a wider range of objects for visual grounding. This requires more informative instructions to address the complex navigation environment. 
However, it is expensive to collect human-annotated instructions that depict the complicated visual scenes to train a navigation agent. The issue of data scarcity limits the navigator's performance in the outdoor VLN task. 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{img/style_transfer_example.pdf}
    \caption{An outdoor VLN example with instructions generated by Google Maps API (ground truth), the Speaker model, and our MTST model. Tokens marked in red indicate incorrectly generated instructions, while the blue tokens suggest alignments with the ground truth. The orange bounding boxes show that the objects in the surrounding environment have been successfully injected into the style-modified instruction.}
    \label{fig:style_transfer_example}
\end{figure}


To deal with the data scarcity issue, ~\citet{fried2018speakerfollower} proposes a Speaker model to generate additional training pairs. 
However, synthesizing instructions purely from visual signals is hard, especially for outdoor environments, due to visual complexity.
On the other hand, template-based navigation instructions on the street view can be easily obtained via the Google Map API, which may serve as additional learning signals to boost outdoor navigation tasks. 
But instructions generated by Google Maps API mainly consist of street names and directions, while human-annotated instructions in the outdoor navigation task frequently refer to street-view objects in the panorama. 
The distinct instruction style hinders the full utilization of external resources.

Therefore, we present a novel Multimodal Text Style Transfer (MTST) learning approach to narrow the gap between template-based instructions in the external resources and the human-annotated instructions for the outdoor navigation task. It can infer style-modified instructions for trajectories in the external resources and thus mitigate the data scarcity issue.
Our approach can inject more visual objects in the navigation environment to the instructions (Fig.~\ref{fig:style_transfer_example}), while providing direction guidance. The enriched object-related information can help the navigation agent learn the grounding between the visual environment and the instruction.

Moreover, different from previous LSTM-based navigation agents, we propose a new VLN Transformer to predict outdoor navigation actions.
Experimental results show that utilizing external resources provided by Google Maps API during the pre-training process improves the navigation agent's performance on Touchdown, a dataset for outdoor VLN~\cite{chen2019touchdown}.
In addition, pre-training with the style-modified instructions generated by our multimodal text style transfer model can further improve navigation performance and make the pre-training process more robust. In summary, the contribution of our work is four-fold:
\begin{itemize}
    \item We present a new Multimodal Text Style Transfer learning approach to generate style-modified instructions for external resources and tackle the data scarcity issue in the outdoor VLN task. \item We provide the Manh-50 dataset with style-modified instructions as an auxiliary dataset for outdoor VLN training. \item We propose a novel VLN Transformer model as the navigation agent for outdoor VLN and validate its effectiveness.  \item We improve the task completion rate by 8.7\% relatively on the test set for the outdoor VLN task with the VLN Transformer model pre-trained on the external resources processed by our MTST approach. \end{itemize}

\section{Related Work}
\noindent\textbf{Vision-and-Language Navigation (VLN)~} is a task that requires an agent to achieve the final goal based on the given instructions in a 3D environment. 
Besides the generalizability problem studied by previous works~\cite{Wang-2018,wang2019reinforced,tan2019learning,zhang2020diagnosing}, the data scarcity problem is another critical issue for the VLN task, expecially in the outdoor environment\cite{chen2019touchdown,mehta2020retouchdown,DBLP:conf/emnlp/Xiang0W20}. ~\citet{fried2018speakerfollower} obtains a broad set of augmented training data for VLN by sampling trajectories in the navigation environment and using the Speaker model to back-translate their instructions. However, the Speaker model might cause the error propagation issue since it is not trained on large corpora to optimize generalization.
While most existing works select navigation actions dynamically along the way in the unseen environment during testing, \citet{majumdar2020improving} proposes to test in previously explored environments and convert the VLN task to a classification task over the possible paths. This approach performs well in the indoor setting, but is not suitable for outdoor VLN where the environment graph is different.



\noindent\textbf{Multimodal Pre-training ~} has attracted much attention to improving multimodal tasks performances. The models usually adopt the Transformer structure to encode the visual features and the textual features \cite{Tan2019LXMERTLC, Lu2019ViLBERTPT,chen2019uniter, Sun2019VideoBERTAJ, Li2019VisualBERTAS, huang2020pixelbert, Luo2020UniViLMAU, Li2020UnicoderVLAU,DBLP:conf/acl/ZhengGK20,DBLP:conf/cvpr/WeiZLZW20,DBLP:conf/acl/TsaiBLKMS19}.
During pre-training, these models use tasks such as masked language modeling, masked region modeling, image-text matching to learn the cross-modal encoding ability, which later benefits the multimodal downstream tasks.
\citet{majumdar2020improving} proposes to use image-text pairs from the web to pre-train VLN-BERT, a visiolinguistic transformer-based model similar to the model proposed by \citet{Lu2019ViLBERTPT}.

A concurrent work by~\citet{DBLP:conf/cvpr/HaoLLCG20} proposes to use Transformer for indoor VLN. Our VLN Transformer is different from their model in several key aspects:
(1) The pre-training objectives are different: ~\citet{DBLP:conf/cvpr/HaoLLCG20} pre-trains the model on the same dataset for training, while we create an augmented, stylized dataset for outdoor VLN using the proposed MTST method.  
(2) Benefiting from the effective external resource, a simple navigation loss is employed in our VLN Transformer, while they adopt the masked language modeling to better train their model. 
(3) Model-wise, instead of encoding the whole instruction into one feature, we use sentence-level encoding since Touchdown instructions are much longer than R2R instructions.
(4) We encode the trajectory history, while their model encodes the panorama for the current step.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{./img/overview_new.pdf}
\caption{An overview of the Multimodal Text Style Transfer (MTST) learning approach for vision-and-language navigation in real-life urban environments. Details are described in Section ~\ref{sec:overview}.}
\label{fig:process_overview}
\end{figure*}

\noindent\textbf{Unsupervised Text Style Transfer~} is an approach to mitigate the lack of parallel data for supervised training. One line of work encodes the text into a latent vector and manipulate the text representation in the latent space to transfer the style. \citet{Shen2017StyleTF, Hu2017TowardCG, Yang2018UnsupervisedTS} use variational auto-encoder to encode the text, and use a discriminator to modify text style. \citet{John2018DisentangledRL, Fu2018StyleTI} rely on models with encoder-decoder structure to transfer the style.
Another line of work enriches the training data by generating pseudo-parallel data via back-translation
~\cite{Artetxe2018UnsupervisedNM, Lample2018PhraseBasedN, Lample2018UnsupervisedMT, Zhang2018StyleTA}. 




\section{Methods}


\subsection{Task Definition}

In the vision-and-language navigation task, the reasoning navigator is asked to find the correct path to reach the target location following the instructions (a set of sentences) .
The navigation procedure can be viewed as a series of decision making processes. At each time step , the navigation environment presents an image view . With reference to the instruction  and the visual view , the navigator is expected to choose an action . The action set  for urban environment navigation usually contains four actions, namely \textit{turn left}, \textit{turn right}, \textit{go forward}, and \textit{stop}. 



\subsection{Overview}
\label{sec:overview}
Our Multimodal Text Style Transfer (MTST) learning mainly consists of two modules, namely the \emph{multimodal text style transfer model} and the \emph{VLN Transformer}. Fig.~\ref{fig:process_overview} provides an overview of our MTST approach. 
We use the multimodal text style transfer model to narrow the gap between the human-annotated instructions for the outdoor navigation task and the machine-generated instructions in the external resources. The multimodal text style transfer model is trained on the dataset for outdoor navigation, and it learns to infer style-modified instructions for trajectories in the external resources. 
The VLN Transformer is the navigation agent that generates actions for the outdoor VLN task. It is trained with a two-stage training pipeline. We first pre-train the VLN Transformer on the external resources with the style-modified instructions and then fine-tune it on the outdoor navigation dataset.

\subsection{Multimodal Text Style Transfer Model}
\label{sec:mm_text_style_transfer}



\begin{table}
\begin{adjustbox}{width=\linewidth,center}
\begin{tabular}{c c }
\cmidrule[\heavyrulewidth]{1-2}
\textbf{Source} & \textbf{Instruction} \\ \cmidrule{1-2}
Google Maps API     & \makecell[l]{Head northwest on E 23rd St toward 2nd Ave.\\ Turn left at the 2nd cross street onto 3rd Ave. }\\ \cmidrule{1-2}
Human Annotator     & \makecell[l]{Orient yourself so you are facing the same as\\the traffic on the 4 lane road. Travel down this\\road until the first intersection. Turn left and go\\down this street with the flow of traffic. You'll\\see a black and white stripped awning on your\\right as you travel down the street.} \\ \cmidrule[\heavyrulewidth]{1-2}
\end{tabular}
\end{adjustbox}
\caption{For the outdoor VLN task, the instructions provided by Google Maps API is distinct from the instructions written by human annotators.}
\label{tab:instr_comparison}

\end{table}

\noindent\textbf{Instruction Style~}
\label{instr_style}
The navigation instructions vary across different outdoor VLN datasets. 
As shown in Table \ref{tab:instr_comparison}, the instructions generated by Google Maps API is template-based and mainly consists of street names and directions.
In contrast, human-annotated instructions for the outdoor VLN task emphasize the visual environment's attributes as navigation targets. It frequently refers to objects in the panorama, such as traffic lights, cars, awnings, etc. 
The goal of conducting multimodal text style transfer is to inject more object-related information in the surrounding navigation environment to the machine-generated instruction while keeping the correct guiding signals. 


\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{./img/style_transfer.pdf}
\caption{An example of the training and inference process of the multimodal text style transfer model. During training, we mask out the objects in the human-annotated instructions to get the instruction template. The model takes both the trajectory and the instruction skeleton as input, and the training objective is to recover the instructions with objects. When inferring new instructions for external trajectories, we mask the street names in the original instructions and prompt the model to generate new object-grounded instructions. }
\label{fig:style_transfer_process}
\end{figure*}


\noindent\textbf{Masking-and-Recovering Scheme~}
The multimodal text style transfer model is trained with a ``masking-and-recovering"~\cite{zhu2019text,Liu2019TIGSAI,Donahue2020EnablingLM,Huang2020INSETSI} scheme to inject objects that appeared in the panorama into the instructions. 
We mask out certain portions in the instructions and try to recover the missing contents with the help of the remaining instruction skeleton and the paired trajectory. 
To be specific, we use NLTK~\cite{BirdKleinLoper09} to mask out the object-related tokens in the human-annotated instructions, and the street names in the machine-generated instructions\footnote{We masked out the tokens with the following part-of-speech tags: [JJ, JJR, JJS, NN, NNS, NNP, NNPS, PDT, POS, RB, RBR, RBS, PRP\\gX'\vv' = [\vv'_v; \vv'_{\alpha}]\vv'_v \in \R^{ 512}\vv'_\alpha\in \R^{ 64}\vv'_v\vv'_\alpha\alpha[sin\alpha, cos\alpha]\hat{\vv_t}t\vh_{t-1}\mW_vattn_{v_{t,i}}i_{th}\vv'_i\vs'\hat{\vs_t}t\mW_sattn_{s_{t,j}}j_{th}\vs'_jtM\gX'\hat{\vv_t}\hat{\vs_t}\vv'_tt\gX'{x_1, x_2, \dots, x_n}\gXn\gXNt=3i_{th}s_i = \{x_{i,1}, x_{i,2}, \dots, x_{i,l_i}\}l_i\vh_i^s\vw_{i,j}x_{i, j}\gF\gC\rmI_t800 \times 460128 \times 100 \times 58128 \times 100 \times 464\evalpha_tt128 \times 100 \times 100100 \times 100\hat{\rmI_t}\hat{\rmI_t}\vh_t^v \in \R^{ 256}t\vv_t[\vh_1^s, \vh_2^s, \dots, \vh_M^s; \vh_1^v, \vh_2^v, \dots, \vh_{t-1}^v]Mta_t\vv_t\gF\gC\gT\uparrow\downarrow\uparrow\uparrow\uparrow\uparrow\uparrow\downarrow\uparrow\uparrow\uparrow\uparrow\uparrow\downarrow\uparrow\uparrow\uparrow\uparrow\uparrow\downarrow\uparrow\uparrow\uparrow\uparrow\downarrow\downarrow\downarrow\downarrow, PRP, MD, CD]}. This reflects the model's ability to inject object-related information during style transferring.  
Among the 9,326 trajectories in the Touchdown dataset, 9,000 are used to train the MTST model, while the rest form the validation set.

We report the quantitative results on the validation set in Table~\ref{tab:nlg_metrics}.
After adding textual attention to the Speaker, the evaluation performance on all seven metrics improved. Our MTST model scores the highest on all seven metrics, which indicates that the ``masking-and-recovering'' scheme is beneficial for the multimodal text style transfer process. The results validate that the MTST model can generate higher quality instructions, which refers to more visual objects and provide more matched guiding signals. 



\begin{table}[t]
\setlength{\tabcolsep}{2.5pt}
\begin{adjustbox}{width=\linewidth,center}

\begin{tabular}{l | c c c c c c c}
\cmidrule[\heavyrulewidth]{1-8}
Model       & BLEU  & METEOR    & ROUGE\_L  & CIDEr & SPICE & MR & \#infill  \\ \cmidrule[\heavyrulewidth]{1-8}
Speaker    & 15.1  & 20.6      & 22.2      & 1.4   & 20.7  & 8.3 &  160 \\ Text\_Attn & 23.8  & 23.3      & 29.6      & 10.0  & 24.6  & 35.7 &  182 \\ MTST      & \textbf{30.6}  & \textbf{28.8}      & \textbf{39.7}      & \textbf{27.8} & \textbf{30.6}  & \textbf{46.7} &  \textbf{308} \\
\cmidrule[\heavyrulewidth]{1-8}
\end{tabular}
\end{adjustbox}
\caption{Quantitative evaluation of the instructions generated by Speaker, Speaker with textual attention and our MTST model.}
\label{tab:nlg_metrics}
\end{table}



\begin{table*}[htbp]
\setlength{\tabcolsep}{4pt}
\begin{adjustbox}{width=0.9\linewidth,center}
\begin{tabular}{l | r r r | r r r | r r r }
\cmidrule[\heavyrulewidth]{1-10}
\multirow{2}{*}{Choice (\%)}      & \multicolumn{3}{c}{MTST \textit{vs} Speaker} & \multicolumn{3}{|c}{MTST \textit{vs} Text\_Attn} & \multicolumn{3}{|c}{Speaker \textit{vs} Text\_Attn} \\ \cmidrule{2-10}
                            & MTST & Speaker & Tie & MTST & Text\_Attn & Tie & Speaker & Text\_Attn & Tie \\ \cmidrule[\heavyrulewidth]{1-10}
Better describes the street view  & \textbf{67.9}& 22.8  & 9.3       & \textbf{44.3} & 35.8 & 19.9  & 28.2  & \textbf{62.7}  & 9.1    \\ More aligned with the ground truth & \textbf{64.6}  & 26.8  & 8.6   & \textbf{37.6} & 33.9 & 28.5  & 25.3  & \textbf{62.5}  & 12.2    \\ \cmidrule[\heavyrulewidth]{1-10}
\end{tabular}
\end{adjustbox}
\caption{Human evaluation results of the instructions generated by Speaker, Speaker with textual attention and our MTST model with pairwise comparisons.}
\label{tab:mturk}
\end{table*}


\paragraph{Human Evaluation}
We invite human judges on Amazon Mechanical Turk to evaluate the quality of the instructions generated by different models.
We conduct a pairwise comparison, which covers 170 pairs of instructions generated by Speaker, Speaker with textual attention, and our MTST model. The instruction pairs are sampled from the Touchdown validation set. 
Each pair of instructions, together with the ground truth instruction and the gif that illustrates the navigation street view, is presented to 5 annotators. The annotators are asked to make decisions from the aspect of guiding signal correctness and instruction content alignment.
Results in Table~\ref{tab:mturk} show that annotators think the instructions generated by our MTST model better describe the street view and is more aligned with the ground-truth instructions. 





\paragraph{Case Study}
We demonstrate case study results to illustrate the performance of our Multimodal Text Style Transfer learning approach. Fig.~\ref{fig:case_study_style} provides two showcases of the instruction generation results. As listed in the charts, the instructions generated by the vanilla Speaker model have a poor performance in keeping the guiding signals in the ground truth instructions and suffer from hallucinations, which refers to objects that have not appeared in the trajectory. 
The Speaker with textual attention can provide guidance direction. However, the instructions generated in this manner does not utilize the rich visual information in the trajectory. On the other hand, the instructions generated by our multimodal text style transfer model inject more object-related information (``the light", ``scaffolding") in the surrounding navigation environment to the StreetLearn instruction while keeping the correct guiding signals. 


\begin{figure}[t]
\vspace{-3ex}
\centering
\includegraphics[width=\linewidth]{./img/case_study_single_column.pdf}
\caption{Two showcases of the instruction generation results. 
The red tokens indicate incorrectly generated instructions, while the blue tokens suggest alignments with the ground truth. The orange bounding boxes show that the objects in the surrounding environment have been successfully injected into the style-modified instruction. }
\label{fig:case_study_style}
\vspace{-2ex}
\end{figure}





\section{Conclusion}
In this paper, we proposed the Multimodal Text Style Transfer learning approach for outdoor VLN. This learning framework allows us to utilize out-of-domain navigation samples in outdoor environments and enrich the original navigation reasoning training process. Experimental results show that our MTST approach is model-agnostic, and our MTST learning approach outperforms the baseline models on the outdoor VLN task.
We believe our study provides a possible solution to mitigate the data scarcity issue in the outdoor VLN task. In future studies, we would love to explore the possibility of constructing an end-to-end framework. We will also further improve the quality of style-modified instructions,  and quantitatively evaluate the alignment between the trajectory and the style-transferred instructions. 

\section*{Acknowledgments}
We would like to show our gratitude towards Jiannan Xiang, who kindly shares his experimental code on Touchdown, and Qi Wu, who provides valuable feedback to our initial draft. We also thank the anonymous reviewers for their thought-provoking comments.
The UCSB authors were sponsored by an unrestricted gift from Google. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the sponsor.


\bibliography{anthology,eacl2021}
\bibliographystyle{acl_natbib}

\clearpage

\appendix

\section{Appendix}

\subsection{Dataset Comparison~}

\begin{table}[h]
\setlength{\tabcolsep}{3pt}
\small
\begin{adjustbox}{width=\linewidth,center}
\begin{tabular}{l r r r r r r}
\cmidrule[\heavyrulewidth]{1-7}
\textbf{Dataset} & \textbf{\#path}   & \textbf{\#pano} &  \textbf{\#pano/path}  & \textbf{instr\_len} & \textbf{\#sent/path} & \textbf{\#turn/path} \\ \cmidrule{1-7}
Touchdown        & 6k  & 26k & 35.2 & 80.5 & 6.3 & 2.8  \\ \cmidrule{1-7}
Manh-50         & 31k  & 43k & 37.2 & 22.1 & 2.8 & 4.1  \\ \cmidrule{1-7}
StreetLearn      & 580k   & 114k & 29.0    & 28.6  & 4.0    & 13.2  \\ 
\cmidrule[\heavyrulewidth]{1-7}
\end{tabular}
\end{adjustbox}
\caption{Dataset statistics. \emph{path}: navigation path; \emph{pano}: panorama; \emph{instr\_len}: average instruction length; \emph{sent}: sentence; \emph{turn}: intersection on the path. 
}
\label{tab:dataset_details}
\end{table}

 Table \ref{tab:dataset_details} lists out the statistical information of the datasets used in pre-training and fine-tuning.
Even though the Touchdown dataset and the StreetLearn dataset are built upon Google Street View, and both of them contain urban environments in New York City, pre-training the model with the VLN task on the StreetLearn dataset does not raise a threat of test data leaking. This is due to several causes: 

First, the instructions in the two datasets are distinct in styles. The instructions in the StreetLearn dataset is generated by Google Maps API, which is template-based and focuses on street names. However, the instructions in the Touchdown dataset are created by human annotators and emphasize the visual environment's attributes as navigational cues. Moreover, as reported by \citet{mehta2020retouchdown}, the panoramas in the two datasets have little overlaps. In addition, Touchdown instructions constantly refer to transient objects such as cars and bikes, which might not appear in a panorama from a different time. The different granularity of the panorama spacing also leads to distinct panorama distributions of the two datasets.


\subsection{Training Details}

We use Adam optimizer~\cite{kingma2014adam} to optimize all the parameters. During pre-training on the StreetLearn dataset, the learning rate for the RCONCAT model, GA model, and the VLN Transformer is . We fine-tune BERT separately with a learning rate of . We pre-train RCONCAT and GA for 15 epochs and pre-train the VLN Transformer for 25 epochs.

When training or fine-tuning on the Touchdown dataset, the learning rate for RCONCAT and GA is . For the VLN Transformer, the learning rate to fine-tune BERT is initially set to , while the learning rate for other parameters in the model is initialized to be . The learning rate for VLN Transformer will decay.
The batch size for RCONCAT and GA is 64, while the VLN Transformer uses a batch size of 30 during training.




\begin{table}[h]
\begin{adjustbox}{width=\linewidth,center}
\begin{tabular}{l | r r r r r r }
\cmidrule[\heavyrulewidth]{1-7}
Model & TC    & SPD   & SED   & CLS  & nDTW     & SDTW  \\ \cmidrule[\heavyrulewidth]{1-7}
no split & 9.6  & 21.8  & 9.3  & 46.1 & 20.0      &  8.7  \\ split & \textbf{13.6}  & \textbf{20.5}  & \textbf{13.1}  & \textbf{47.6} & \textbf{24.0}      & \textbf{12.6}   \\ 
\cmidrule[\heavyrulewidth]{1-7}
\end{tabular}
\end{adjustbox}
\caption{Ablation results of the VLN Transformer's instruction split on Touchdown dev set. In \textit{split} setting, the instruction is split into multiple sentences before being encoded by the instruction encoder, while \textit{no split} setting encodes the whole instruction without splitting.}
\label{tab:split}
\end{table}

\subsection{Split Instructions vs. No Split~}
We compare VLN Transformer performance with and without splitting the instructions into sentences during encoding. Results in Table ~\ref{tab:split} show that breaking the instructions into multiple sentences allows the visual views and the guiding signals in sub-instructions to attend to each other during cross-modal encoding fully. Such cross-modal alignments lead to betters navigation performance.


\subsection{Amazon Mechanical Turk}
We use AMT for human evaluation when evaluating the quality of the instructions generated by different models. The survey form for head-to-head comparisons is shown in Figure~\ref{fig:mturk}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{./img/mturk.jpg}
\caption{Pairwise comparison form for human evaluation on AMT.}
\label{fig:mturk}
\end{figure*}

\end{document}
