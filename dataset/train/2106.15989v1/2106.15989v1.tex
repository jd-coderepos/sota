









\documentclass[journal]{IEEEtran}
















\usepackage{cite}



\usepackage{hyperref}



\usepackage[pdftex]{graphicx}
\ifCLASSINFOpdf
\else
\fi











































\usepackage{url}








\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{bm}


\usepackage{array}
\makeatletter
\newcommand{\thickhline}{\noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother

\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
\renewcommand\thesubtable{(\alph{subtable})}

\newcommand{\multiref}[2]{\ref{#1}-\ref{#2}} \newcommand{\multisubref}[2]{\ref{#1}-\subref{#2}} 


\begin{document}
\title{Word-level Sign Language Recognition with Multi-stream Neural Networks\\ Focusing on Local Regions}




\author{Mizuki~Maruyama,~\IEEEmembership{}
        Shuvozit~Ghose,~\IEEEmembership{}
        Katsufumi~Inoue,~\IEEEmembership{Member,~IEEE},
        Partha~Prathim~Roy,~\IEEEmembership{Member,~IEEE},
        Masakazu~Iwamura,~\IEEEmembership{Member,~IEEE},
        and~Michifumi~Yoshioka,~\IEEEmembership{}}
        





















\maketitle

\begin{abstract}
In recent years, Word-level Sign Language Recognition (WSLR) research has gained popularity in the computer vision community, and thus various approaches have been proposed. Among these approaches, the method using I3D network achieves the highest recognition accuracy on large public datasets for WSLR. However, the method with I3D only utilizes appearance information of the upper body of the signers to recognize sign language words. On the other hand, in WSLR, the information of local regions, such as the hand shape and facial expression, and the positional relationship among the body and both hands are important. Thus in this work, we utilized local region images of both hands and face, along with skeletal information to capture local information and the positions of both hands relative to the body, respectively. In other words, we propose a novel multi-stream WSLR framework, in which a stream with local region images and a stream with skeletal information are introduced by extending I3D network to improve the recognition accuracy of WSLR. From the experimental results on WLASL dataset, it is evident that the proposed method has achieved about 15\% improvement in the Top-1 accuracy than the existing conventional methods.

\end{abstract}

\begin{IEEEkeywords}
Word-level Sign language recognition, 3D Convolutional Neural networks, Deep learning, Optical flow, Skelton, Face, Hand
\end{IEEEkeywords}



\newcommand{\etal}{\textit{et al}.}


\IEEEpeerreviewmaketitle



\section{Introduction}
Sign language is one of the most important tools for speech impaired people to communicate with others. As sign language has an extensive vocabulary and complex expressions, it requires a lot of time and effort to learn, but it is not easy for anyone to master. It is more challenging to automatically perform Sign Language Recognition (SLR) because of an extensive vocabulary and complex expressions from a machine perspective. Although earlier research confined to isolated SLR~\cite{li2020transferring, wang2014similarity}, Word-level SLR (WSLR)~\cite{Hosain_2021_WACV,Camgoz_2020_CVPR}, research has gained popularity in the computer vision community in recent years. The introduction of deep learning in WSLR has accelerated the progress, and thus various deep learning based approaches have been proposed.

The conventional methods~\cite{li2020word, vaezi2019ms-asl} utilizing I3D network~\cite{joao2017i3d} have become state-of-the-art methods for WSLR. Although I3D was mainly proposed for video-based action recognition~\cite{joao2017i3d}, it has found success in other video recognition tasks, including SLR, due to its spatio-temporal representation capability. However, compared to the success of the action recognition, the success of I3D, especially in the context of WSLR, is not satisfactory. It is because I3D algorithm uses appearance information of  the upper body of the signers to recognize sign language words and treats various appearance information equally. It extracts global features by observing the whole image instead of capturing only the image's local regions.

Inherently WSLR is quite challenging due to various reasons. Firstly, a word in sign language is composed of a series of representations of gestures. Secondly, WSLR may require capturing complex information such as fine-grained hand gestures in a quick motion, body movements, hand shape, facial expression, and the positional relationship among body and both hands. Finally, most importantly, some signs represent different words with a similar gesture; only the hand shapes in such signs are different with the same hand movements. Because of these challenges, a single stream I3D network does not perform well on WSLR. It cannot capture local level image information like the hand shape and facial expressions and the positional relationship among the body and both the hands simultaneously.

\begin{figure}[tb]
  \centering
    \includegraphics[width=8.5cm]{images/overview.pdf}
    \caption{Overview of the proposed method.
    The proposed MSNN has three streams: 1) base stream, 2) local image stream, and 3) skeleton stream. Each stream is trained separately, and the recognition scores extracted from each stream are averaged to obtain the final recognition result. }
    \label{fig:overview}
\end{figure}


Motivated by the work of Adria~\etal~\cite{adria2018zoom} that focuses on local regions such as eyes in addition to the whole region of face for face recognition and considering the importance of the local information in the context of WSLR, we aimed to combine the local information along with the global information extracted from images. We propose a novel Multi-stream Neural Networks (MSNN) that exploits local information of hand and face and the positional relationship among the body and both hands. The proposed MSNN framework consists of three streams. They are Base Stream, Local Image Stream, and Skeleton Stream, respectively, as shown in Fig.~\ref{fig:overview}. The Base Stream deals with the global information of the upper body. The Base Stream is divided into two sub-streams. The first one is for the appearance information extracted from whole frame images in the input sign language video. The other utilizes the optical flow information to capture the signer's dynamic gesture movement with the consecutive frame images. 


On the other hand, Local Image Stream deals with local information such as hand shapes and facial expressions. Finally, Skeleton Stream is exploited to capture the positional relationship among the body and both hands. This information also lessens the effect of background noise and the signer's appearance, which is missing in the method using only appearance information. While Base Stream and Local Image Stream use I3D network, Skeleton Stream utilizes ST-GCN~\cite{yan2018stgcn} to capture the positional relationship. Note that each stream in our MSNN framework is trained separately, and the classification
scores obtained from each stream are simply averaged in the test phase. From these steps, finally, the word having the highest score is considered as the recognition result. 

Therefore, the contribution of our paper as follows:
\begin{itemize}
\item We present a novel MSNN, which exploits hand and face local information and the positional relationship among the body and both hands for WSLR.

\item Our framework is composed of three streams. 1) Base Stream, which deals with global information such as global appearance and optical flow information, 
2) Local Image Stream, which treats local information such as hand shapes and facial expression, and 3) Skeleton Stream, which handles the positional relationship among the body and both hands. 

\item Our approach has achieved about 15\% improvement in the Top-1 accuracy compared to the previous state-of-the-art methods on WLASL dataset, a large-scale dataset for WSLR~\cite{li2020word}.

\end{itemize}

The remainder of this paper is organized as follows. In Section~\ref{related}, some relevant works in the field of SLR are discussed. The proposed framework is detailed in Section~\ref{proposed_method}. Section~\ref{experiment} describes the datasets, implementation details, and experimental results. Section~\ref{conclusion} concludes the paper.





























\section{Related Works} \label{related}
SLR~\cite{li2020transferring, wang2014similarity, cui2019deep, evangelidis2014continuous, koller2015continuous} is a fundamental interpretation task that aims to overcome the communication barrier for speech-impaired individuals. An overview of some SLR models is discussed in depth by Rastgoo~\etal~\cite{rastgoo2020sign}. Categorically, SLR is divided into two main scenarios. They are isolated SLR~\cite{li2020transferring, wang2014similarity} and continuous SLR~\cite{cui2019deep, evangelidis2014continuous, koller2015continuous}. While earlier works are confined to isolated SLR, later works extend to continuous SLR. In isolated SLR, gloss information is temporally well-segmented, and the target is to recognize words or expressions. On the other hand, continuous SLR does not contain gloss segmentation, and sentence-level annotations are available as a whole. Compared to the isolated SLR, the continuous SLR task becomes more difficult because it recognizes glosses in scenarios based on sentence-level annotations.

Traditionally, handcrafted features~\cite{wang2014similarity, buehler2009learning, pfister2014domain, evangelidis2014continuous, koller2015continuous, monnier2014multi} were used to capture spatio-temporal representations for SLR. Sign language uses different body parts, such as head, fingers, body, arm, hand, and facial expression~\cite{cheok2019review}. Thus, handcrafted features like image pixel intensity~\cite{wang2014similarity}, Local Binary Pattern (LBP)~\cite{wang2014similarity}, Histogram of Oriented Gradients (HOG)~\cite{buehler2009learning, pfister2014domain}, HOG-3D~\cite{koller2015continuous}, and motion trajectories or velocities~\cite{evangelidis2014continuous, koller2015continuous, monnier2014multi} were utilized for SLR. For example, Koller~\etal~\cite{koller2015continuous} exploited the importance of tracking for SLR concerning the hands and facial landmarks and presented a statistical recognition approach performing large vocabulary continuous SLR across different signers. Evangelidis~\etal~\cite{evangelidis2014continuous} exploited Dynamic Programming (DP) for continuous gesture recognition from articulated poses. Pfister~\etal~\cite{pfister2013large} developed a multiple instance learning method based on the efficient discriminative search that could automatically learn many signs from sign language-interpreted TV broadcasts by utilizing supervisory information as labels available in the subtitles of the broadcasts. Next, Pfister~\etal~\cite{pfister2014domain} proposed a domain adaptation method for gesture recognition that could significantly boost the gesture classifier's performance by learning from a single strongly supervised training example and a reservoir of weakly supervised gesture examples. They introduced Global Alignment Kernel for time alignment instead of generally used Dynamic Time Warping (DTW) in their approach.

The recent trend of utilizing the Convolutional Neural Networks (CNNs) and the deep neural network has significantly boosted the performance of the SLR systems~\cite{pigou2014sign, wu2014leveraging, molchanov2016online, neverova2014multi}. Earlier 2D CNNs were used to learn visual feature representation for SLR which was later extended by introducing the 3D CNNs with the superior capability to learn spatio-temporal feature representation. For example, Pigou~\etal~\cite{pigou2014sign} first utilized CNNs for feature extraction and then applied an Artificial neural network (ANN) for sign language classification. For gesture recognition, Wu~\etal~\cite{wu2014leveraging} proposed a deep belief network to extract high-level skeletal joint features. Molchanov~\etal~\cite{molchanov2016online} employed a recurrent 3D CNN for spatio-temporal feature extraction from video streams on depth, color, and optical flow data to perform simultaneous detection and classification of dynamic hand gestures. One of their paper's main contributions was that they exploit auxiliary Connectionist Temporal Classification (CTC) as a cost function during training to accelerate the network's training process. Neverova~\etal~\cite{neverova2014multi} proposed a deep learning-based multi-scale and multi-modal framework along with a progressive learning procedure for gesture localization, detection, and recognition. 

However, subsequent works on SLR mainly focused on temporal models~\cite{gweth2012enhanced, koller2016deeps, wu2016deep,pigou2018beyond, cui2019deep}, where the target is to learn the correspondence between sequential representations and gloss labels. Gweth~\etal~\cite{gweth2012enhanced} presented a Gaussian HMM that utilized appearance-based features from the original images and features derived from a multilayer perceptron for automatic SLR. Koller~\etal~\cite{koller2016deeps} introduced a hybrid CNN-HMM framework that was end-to-end embedding a CNN into an HMM so that the sequence modeling capabilities of HMMs could be combined with the strong discriminative abilities of CNNs. Wu~\etal~\cite{wu2016deep} presented a semi-supervised hierarchical deep dynamic neural framework based on a HMM for multi-modal gesture recognition, where depth, skeleton joint information, and RGB images were treated as the multi-modal input observations. In contrast to the traditional approaches, which relied on the construction of complex handcrafted features, their approach learned high-level spatio-temporal representations using deep dynamic neural networks. A 3D CNN and a Gaussian-Bernouilli Deep Belief Network were used to manage and fuse batches of RGB and depth images and handle skeletal dynamics. Earlier, HMM were the most widely used temporal models. However, the success of  Recurrent Neural Networks (RNNs) in the context of text recognition~\cite{cheng2017focusing, cheng2018aon, li2019show}, speech recognition~\cite{graves2014towards}, and machine translation~\cite{bahdanau2014neural, sutskever2014sequence} had motivated the sign recognition researchers to explore the application of RNNs in SLR. Considering this direction, Pu~\etal~\cite{pu2019iterative} presented RNN based alignment network with iterative optimization for weakly supervised SLR. Their framework utilized a 3D convolutional residual network for feature extraction and an encoder-decoder network with CTC for sequence modeling. The RNN and CTC decoders were trained jointly using maximum likelihood criterion with a soft DTW alignment constraint. Pigou~\etal~\cite{pigou2018beyond} observed that the gesture recognition videos contained more discriminative temporal information than general videos used for the classification task. Thus, they developed  an end-to-end neural framework composed of recurrent neural network and temporal convolutions. For the sake of continuous SLR, Cui~\etal~\cite{cui2019deep} presented a deep CNN with stacked temporal fusion layers for feature extraction and bi-directional recurrent neural networks for sequence learning. Instead of using a single optimization process, an iterative optimization process was employed to utilize the network's representation capability with limited data fully.

Also, some works~\cite{koller2016deeph, li2020transferring, cui2017recurrent} were proposed to address the problem of limited labeled temporal boundaries and overfitting issues for continuous SLR exploring the weakly supervised learning. For example, Cui~\etal~\cite{cui2017recurrent} proposed a weakly-supervised framework utilizing the recurrent CNN for spatio-temporal feature extraction and sequence learning. They designed a three-stage optimization process. The first stage comprised a sequence learning scheme that employed CTC as the objective function for the alignment proposal. The next stage utilized the alignment proposal as more substantial supervision to tune the feature extractor. Finally, they designed a weakly supervised detection network for regularization and optimized the network with improved feature representations. Koller~\etal~\cite{koller2016deeph} developed an Expectation-Maximization algorithm (EM)~\cite{dempster1977maximum} integrating CNNs with HMMs within a weakly supervised learning framework. Li~\etal~\cite{li2020transferring} presented a cross-domain Knowledge scheme to utilize subtitled sign news videos as weakly supervised labels for WSLR. For that, they proposed a sign word localizer that extracted news signs using a base Word-level sign recognition model in a sliding window manner. Then a classifier was trained jointly on news and isolated signs so that these two domains aligned coarsely.



\section{Method} \label{proposed_method}
Inspired by the face recognition research~\cite{adria2018zoom}, the global and local information extracted from the images was combined with the skeletal information (by using MSNN) to improve the recognition performance of WSLR. For the global information, the signer's appearance information is extracted by globally focusing on the image. Besides, optical flow information is extracted from the consecutive images to understand the dynamic movement of signers. The regions of the left and right hands were extracted to obtain the rich hand shape. Similarly, to obtain the facial information as a critical factor for SLR, 
the face region is extracted because speech impaired people often read facial information, such as expression and lip movement, to understand the meaning of hand signs. In addition to this information, to capture the positional relationship among the body and both hands and lessen background noise, we introduce skeletal information. 

Our idea is to combine this information with MSNN. An overview of the proposed method is shown in Fig.~\ref{fig:overview}. 
In the proposed method, MSNN is divided into three streams: 
1) Base Stream, which deals with global information such as global appearance and optical flow information, 
2) Local Image Stream, which treats local information such as hand shapes and facial expression, 
and 3) Skeleton Stream, which handles the positional relationship among the body and both hands. The proposed method's characteristic points are that each stream is separately trained, and the classification scores obtained from each stream are averaged at the test phase. From these processes, finally, the word having the highest score is considered as the recognition result. Each stream is explained in more detail in the following subsections.







\subsection{Base Stream}

\begin{figure}[tb]
  \centering
    \begin{tabular}{c}
      \begin{minipage}{\hsize}
        \centering
          \includegraphics[width=8.5cm]{images/overview_i3d_a.pdf}
      \end{minipage}\\

      \begin{minipage}{\hsize}
        \centering
          \includegraphics[width=8.0cm]{images/overview_i3d_b.pdf}
      \end{minipage}
    \end{tabular}
    \caption{I3D network architecture (top) and its detailed inception submodule (bottom)~\cite{joao2017i3d}.}
    \label{fig:i3d}
  
\end{figure}
In Base Stream, as shown in Fig.~\ref{fig:overview}, there are two sub-streams to deal with the global information of the upper body. The first stream is for the appearance information extracted from whole frame images in the input sign language video;
it is solely used in the conventional methods~\cite{li2020word, vaezi2019ms-asl} to achieve state-of-the-art accuracies.
Hence, the first stream is considered as a baseline of the proposed method.
In the second sub-stream, optical flow information was used to capture the signer's dynamic gesture movement with the consecutive frame images. 
In an action recognition research, Carreira and Zisserman~\cite{joao2017i3d} have reported that 
optical flow information allows us to improve the recognition accuracy 
by combining the sequential appearance information with a two-stream structure network~\cite{joao2017i3d}. 
Inspired by this report, 
in this research, 
this Two-stream I3D was also considered as a baseline of the proposed method. 


Before explaining the concrete processes of the base stream, we briefly introduce I3D. Figure~\ref{fig:i3d} shows an overview of I3D. It expands 2D filters as pooling kernels of the inception network~\cite{chris2014incep} trained on ImageNet dataset~\cite{olga2015imagenet} into 3D ones, and these inflated 3D filters are incorporated in the network.  In this research, these filters are fine-tuned with Kinetics dataset~\cite{joao2017i3d} to extract better temporal and spatial features of an input video. In the stream for appearance information,  consecutive frame images of the input sign language video were given as input to I3D. On the other hand, in the stream for optical flow information,  optical flow images calculated from  consecutive frame images with TV-L1 algorithm~\cite{zach2007tvl1} are inputted to I3D. Like \cite{tu2018mscnn, bilen2018multi, zang2018multi}, each stream is trained separately and outputs classification scores of sign language words. Finally, these scores are combined for utilizing the final recognition process. 

\if0
In previous methods~\cite{li2020word,vaezi2019ms-asl}, the whole image of each frame cut from the sign language video is inputted to I3D. I3D is a network-based 3DCNN proposed by Carreira~\etal~\cite{joao2017i3d}. An overview of I3D is shown in Figure \ref{fig:i3d}. In I3D, they expand 2D filters and pooling kernels of Inception network~\cite{chris2014incep} trained on ImageNet~\cite{olga2015imagenet} into 3D. Then the inflated 3D filters are fine-tuned on Kinetics dataset~\cite{joao2017i3d} to better train the temporal and spatial feature of a video. Carreira~\etal~\cite{joao2017i3d} know that the accuracy is improved by adding the optical flow images to the input of I3D and using a Two-stream structure. In SLR, like in action recognition, a signer's movement is essential. Therefore the Two-stream structure is expected to improve the recognition performance. Therefore, Two-stream I3D is used with whole image and optical flow images as the base of the proposed method, and this stream is called ``Base Stream.'' The TV-L1 algorithm is adopted for calculating optical flow.
\fi




\subsection{Local Image Stream}

\begin{figure}[tb]
  \centering
    \includegraphics[width=\linewidth]{images/bbox.pdf}
    \caption{An example of bounding boxes extraction for face and both hands.
    These bounding boxes are extracted based on the skeletal points detected with OpenPose: 
    both hand regions are based on shoulder point , Elbow point , and wrist point , and the face region is based on the left ear point  and right ear point . 
    (The image are provided from WSASL dataset~\cite{li2020word})}
    \label{fig:bbox}
\end{figure}

In this section, we explain Local Image Stream. 
This stream can capture local information that cannot be sufficiently obtained from the upper body information. 
As mentioned above, to obtain the local information, the local regions of frame images are used, 
especially hand and face regions, in addition to the global information extracted from the upper body. 
In SLR, the shape of the hand is one of the most critical factors. 
If the hand-shapes of gestures are different even though the hand movements are the same, 
the gestures represent different sign words. 
Therefore, to improve the recognition performance of WSLR, the precise hand shape should be used. 
In addition to the hand shape information, facial information is also an important factor 
because deaf people read signs by capturing the signer's facial expression and mouth movements. 
Thus, a face region is also extracted and utilized for understanding the facial information accurately. 
For understanding this information precisely, 
the hand shape and facial information of each sign word are separately 
learned from the appearance information of the upper body of the signer. 
For realizing this in the Local Image Stream, 
an MSNN with three sub-streams is prepared and used for left and right-hand regions and the face region. 
The concrete processes of the Local Image Stream are explained below. 

First, the signer's skeletal points are detected with OpenPose~\cite{cao2017openpose}, 
a method to obtain the coordinates and likelihood of skeletal points of the signer. 
From the positions of these skeletal points, 
both hand and face regions are extracted with bounding boxes, as shown in Fig.~\ref{fig:bbox}. 
For extracting both hand regions, it is assumed that the hand is on the elbow's extension to the wrist. 
From this assumption, the bounding boxes of hands are obtained and calculated by the following equations:


where  is the central point of the bounding box,and  are the shoulder, elbow, and wrist points, respectively. 
 are the width and height of the bounding box, respectively. The arrows above the variables represent vectors. 
In this research, all constant numbers in Eq.~(\ref{eq:EChand}) and Eq.~(\ref{eq:WHhand}) are empirically determined. 
On the other hand, for the bounding box extraction of the face, 
we utilize the positions of the left and right ears detected by OpenPose; the calculations were made using the following equations based on the extracted positions: 

where  and  are the left and right ear points, respectively. The constants in Eq.~(\ref{eq:RCface}) and Eq.~(\ref{eq:WHface}) were also determined empirically. 
The local images were cropped from the whole image using the bounding boxes and resized to  pixels. With the cropped images, the network of each sub-stream was trained separately. I3D was also utilized for training the hand shape and facial information. The output of each the sub-stream is the classification score of a sign language word. Similar to the Base Stream, each score is utilized for the final classification process. 

\if0
In SLR, the shape of the hand is one of the most important factors.
Even if the hand movements are the same, if the hand shape is different, the gesture represents a different word.
In addition, facial information is also important factor when recognizing sign languages.
Deaf people read signs by capturing the facial expressions and mouth movements of the the signer.
Thus, information in local regions of hands and face is important in sign language.
Therefore, the local images of both hands and face were utilized and cropped from the whole images.
By inputting these images into I3D, the model is able to capture local features such as hand shapes and facial information.
This stream was called ``Local Image Stream'', which aims to allow the model to train local features that cannot be captured from the whole images.

OpenPose~\cite{cao2017openpose} was used for cropping local region images of hands and face. OpenPose is a method to obtain the coordinates and likelihood of skeletal points of persons in a RGB image. Figure~\ref{fig:bbox} shows an example of bounding boxes extraction for face and both hands. Assuming that the hand is on the extension of the elbow to the wrist, the bounding box of the hand region is obtained using the skeletal point coordinates estimated by OpenPose, as shown in the following equation:

For bounding box extraction of face, the skeletal coordinates obtained by OpenPose, as shown in the following equation, were used.

where  is the central point of bounding box,  are the shoulder, elbow and wrist points respectively,  are the right and left ear points respectively,  are width and height of bounding box and the arrows above the variables represent vectors. After cropping the local images, three I3D networks with right hand, left hand and face images (resized to  pixels) as input, were trained separately.
\fi


\begin{figure}[tb]
  \centering
    \includegraphics[width=\linewidth]{images/keypoints.pdf}
    \caption{27 keypoints inputted to ST-GCN in Skeleton Stream. five keypoints refer to the body, and 11 keypoints refer to each hand. (The left image is provided from~\cite{vaezi2019ms-asl})}
    \label{fig:keypoints}
\end{figure}




\subsection{Skeleton Stream}

\begin{figure}[tb]
  \centering
    \includegraphics[width=\linewidth]{images/stgcn.pdf}
    \caption{Overview of ST-GCN network architecture~\cite{yan2018stgcn}.}
    \label{fig:stgcn}
\end{figure}

In this section, we present Skeleton Stream, which can capture the signer's spatio-temporal skeletal information. In sign language, the signer often points out various body parts such as an eye, mouth to distinguish different sign words. That is, the meanings of sign words are varied depending on where the the signer points out. Therefore, to classify these sign words, the positional relationship among the body and both hands needs to be understood precisely. Besides, to understand the gesture difference, the skeletal change through consecutive frame images is also checked. Different from Base and Local Image Streams that use I3D, we use Spatial-Temporal Graph Convolutional Network (ST-GCN)~\cite{yan2018stgcn} for Skeleton Stream to obtain such spatio-temporal information (positional relationship and skeletal change) for WSLR. It is also used for action recognition using skeletal data and achieved high recognition accuracy. In the following, we explain the concrete process of Skeleton Stream. 

First, skeleton information from each frame image was extracted using OpenPose~\cite{cao2017openpose}, as illustrated in Fig.~\ref{fig:keypoints}. 
In this study, 27 keypoints are extracted from each frame image;  five and 11 keypoints are extracted from the body region and each hand region, respectively. After keypoint extraction, all the 2D coordinates of each keypoint are extracted from a frame image as represented by Eq.~(\ref{eq:skelfeature}) and regarded this concatenated vector  as a skeletal feature. 

where  are the coordinates of the -th keypoints of body, left, and right hands, respectively. Then the features extracted from the consecutive frame images are given as the input to ST-GCN. An overview of ST-GCN is shown in Fig.~\ref{fig:stgcn}. In ST-GCN, the skeletal data constructs two graph structures. One is a spatial graph that focuses on the spatial relationship among the keypoints extracted from a frame image. The other one is a temporal graph that focuses on the temporal change by connecting the same keypoints between frames. These graphs allow the classification model to train both spatial and temporal features of skeletal keypoints simultaneously. The model outputs the classification scores of sign language words using these mechanisms and is utilized for the final classification process.

\if0
In addition to the factors mentioned in the previous subsection, there are other important factors in recognizing sign languages: the position of the hand in relation to the body and the direction in which the hand is pointing.
To train the model to focus on these information, we use the skeletal information of the signers were used.
Furthermore, by using skeletal information, the effect of information such as background and the signer appearance was mitigated.

In this study, keypoints were extracted from a frame using OpenPose~\cite{cao2017openpose}, as illustrated in Figure~\ref{fig:keypoints}.
Then, all the 2D coordinates of each keypoint were concatenated as the input feature and fed to the ST-GCN~\cite{yan2018stgcn}. 

ST-GCN is a network proposed by Yan~\etal for action recognition using skeletal data, and archive high action recognition accuracy.
An overview of ST-GCN is shown in Figure~\ref{fig:stgcn}.
In this method, the skeletal data is considered as two graph structures.
One is a spatial graph that focuses on the relationship between joint points in the same frame.
The other is a temporal graph that focuses on the variation in time direction by connecting the same joint points between frames.
This allows the model to train both spatial and temporal features of skeletal points at the same time, thereby enabling human action recognition.
\fi




\section{Experiments}\label{experiment}
We quantitatively and qualitatively evaluated the proposed method for WSLR with the publicly available datasets and compared the proposed method with the state-of-the-art method. In this section, we present the datasets and the implementation details for the evaluation of WSLR performance.  
Subsequently, we show the experimental results and discuss the effect of local information 
and skeletal information via ablation studies. 





\subsection{Datasets}
As representative datasets for American WSLR, we present WLASL dataset~\cite{li2020word} and MS-ASL dataset~\cite{vaezi2019ms-asl}, including a large number of classes, signers, videos, and videos per class. In each video of these datasets, a native American Sign Language (ASL) signer or interpreter performs only one sign word.
As summarized in Table~\ref{tab:dataset}, both datasets have four subsets: WLASL100, WLASL300, WLASL1000, and WLASL2000 in WLASL dataset and MS-ASL100, MS-ASL200, MS-ASL500, and MS-ASL1000 in MS-ASL dataset.
These subsets consist of the top- classes when the classes are sorted in order of the number of videos per class in each dataset, where  in WLASL dataset and  in MS-ASL dataset.
The number after each subset name (e.g., ``1000'' of WLASL1000) indicates the number of classes included in the subset.
We evaluate the proposed method on all subsets of WLASL dataset and MS-ASL100 dataset.
Since some of the video links used to download MS-ASL have expired, we could obtain about 25\% fewer data from the original MS-ASL dataset. The average number of videos per class was also 25\% fewer, and the number of signers was fewer by 26 than the original MS-ASL dataset.
Therefore, the results on MS-ASL100 dataset should be taken as indicative.
Following the indication provided by the authors of each dataset, the datasets were split so that the ratio of training, validation, and test data was 4:1:1.



\subsection{Implementation Details}
In the proposed method, the training and testing strategies follow the conventional researches~\cite{li2020word, vaezi2019ms-asl}.
That is, as a preprocessing step, the bounding box of a signer was detected for all video frames in WLASL and MS-ASL datasets by YOLOv3~\cite{redmon2018yolov3} and by SSD~\cite{liu2016ssd}, respectively.
Then, the bounding box was enlarged by a factor of  in the following two-step process.
First, each frame was resized so that the diagonal size of the signer's bounding box was 256 pixels.
That is, the size of the bounding box was .
Second, for each frame, a  squared region whose center was the same as the signer's bounding box was cropped.
In the training phase, for spatial augmentation, a  patch was randomly cropped from each normalized frame.
In addition, random horizontal flipping with a probability of 0.5 was applied to the normalized frames because a mirrored ASL signs preserve the same meaning as the original.
Additionally, for temporal augmentation, 64 consecutive normalized frames were randomly selected as an input to I3D.
For videos shorter than 64 frames, either the first or the last frame is selected randomly and then 
the videos were padded by repeatedly duplicating the selected frame. 
We trained I3D using Adam optimizer with an initial learning rate of  and a weight decay of . 
Besides, we trained ST-GCN using Adam with an initial learning rate of 0.01  and a weight decay of .
All the models were trained with 200 epochs on each dataset.
In the testing phase, all frames of the videos were inputted into the model.



\subsection{Quantitative and Qualitative Results}
We compared the proposed method with two baselines.
The first baseline is I3D with only whole images.
This method is proposed in~\cite{li2020word, vaezi2019ms-asl} and has achieved state-of-the-art results on both datasets.
The second baseline is Two-stream I3D using whole images and optical flow images as input.
In other words, the baselines consist of only ``Base Stream'' in Fig.~\ref{fig:overview}.
We named these two baselines \textit{Baseline1} and \textit{Baseline2}, respectively.
For the ablation study, on top of the baselines, all combinations of using and not using with ``Local Image Stream'' and ``Skeleton Stream'' were evaluated to verify the effectiveness of these streams.
We call these models \textit{Ours1} through \textit{Ours6}.
Table~\ref{tab:wlasl} shows the compositions of all the models.
Besides, in this research, we evaluated the recognition performance of the methods using Top- classification accuracies with .


\begin{table}[tb]
    \centering
    \caption{The details of datasets. We used \#Classes, \#Videos and \#the signers to denote the number of classes, videos and the signers, respectively. Column ``Mean'' denotes the average number of videos per class.}
    \begin{tabular}{ccccc} \hline
        Subset & \#Classes & \#Videos & Mean & \#the signers \\ \hline \hline
        WLASL100~\cite{li2020word} & 100 & 2,038 & 20.4 & 97 \\
        WLASL300~\cite{li2020word} & 300 & 5,117 & 17.1 & 109\\
        WLASL1000~\cite{li2020word} & 1,000 & 13,168 & 13.2 & 116\\
        WLASL2000~\cite{li2020word} & 2,000 & 21,083 & 10.5 & 119 \\ \hline
        \multirow{2}{*}{MS-ASL100~\cite{vaezi2019ms-asl}} & \multirow{2}{*}{100} & 4,315 & 43.2 & 163 \\
         & & (-25\%) & (-25\%) & (-26 the signers) \\ \hline
    \end{tabular}
    \label{tab:dataset}
\end{table}



\subsubsection{Experimental results on WLASL dataset}
The experimental results on four subsets of WLASL dataset are shown in Table~\ref{tab:wlasl}.
A comparison between Baseline1 and Baseline2 in the table shows that the state-of-the-art accuracy (i.e., the accuracy of Baseline1)~\cite{li2020word} was surpassed by Baseline2 by just adding the optical flow.
This result indicates that (1)~motion information, which is important for WSLR, is not fully trained in a model by inputting only consecutive video frames in Baseline1, but (2)~inputting both consecutive video frames and motion optical flow images to a model leads to successful training in Baseline2.
A similar result has also been demonstrated by~\cite{joao2017i3d} in the field of action recognition research.
Ours6 with optical flow, Local Image, and Skeleton Streams achieved the highest accuracy on all subsets of WLASL dataset.
This proves that the proposed method has a higher recognition ability than the state-of-the-art method.
Next, the recognition performance of the models was compared using and not using ``Local Image Stream'' (e.g., comparison between Baseline1 and Ours1, and that between Baseline2 and Ours4).
We observed that the model using ``Local Image Stream'' outperformed those not using it; for example, in the comparison between Baseline1 and Ours1, the Top-1 accuracy on WLASL100 dataset was improved by 10.71\% from 65.89\% to 76.60\%.
This result indicates that the proposed ``Local Image Stream'' was effective in SLR.
To verify the effectiveness of ``Skeleton Stream,'' we compared the model using the stream and those not using it by, for example, comparison between Baseline1 and Ours2, and that between Baseline2 and Ours5.
On all subsets, the accuracy was better when ``Skeleton Stream'' was used, although not as significantly as ``Local Image Stream''; for example, in the comparison between Baseline1 and Ours2, the Top-1 accuracy on WLASL100 dataset was improved by 5.18\% from 65.89\% to 71.07\%.
From these results, it was confirmed that ``Skeleton Stream'' helps improve SLR accuracy.

\begin{figure}[tb]
  \centering
    \includegraphics[width=\linewidth]{images/man_woman_full.jpg}
  \caption{Example sequences of signs that represent different words with a similar gesture: from the top row to the bottom row, ``Man,'' ``Woman,'' and ``Full.''}
  \label{fig:man}
\end{figure}

We then discuss how the introduction of the proposed ``Local Image Stream'' and ``Skeleton Stream'' allowed us to improve the recognition accuracy.
In particular, the word ``man'' was focused upon, where the contribution of the proposed streams was evident.
The experimental results on WLASL100 dataset confirmed the Top-1 accuracy for ``man'' with Baseline2 was 0\%.
However, the accuracy improved to 100\% by Ours6, where ``Local Image Stream'' and ``Skeleton Stream'' were additionally utilized for Baseline2.
In the results of Baseline2, ``man'' was erroneously recognized as ``woman'' and ``full.''
As shown in Fig.~\ref{fig:man}, the signs for ``man,'' ``woman,'' and ``full'' involve a similar series of gestures: bringing one hand up to the face and then bringing the hand down.
In other words, it is difficult to recognize words that contain similar gestures with a model like Baseline2, which uses only the appearance information of the whole image.
The fact that Ours6 successfully recognized these words indicates that ``Local Image Stream'' and ``Skeleton Stream'' are effective for WSLR, especially recognizing signs that contain similar hand and body movements.

\begin{table*}[tb]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \caption{Compositions of methods and their recognition accuracies (\%) on four subsets of WLASL dataset.
    Columns ``Flow'', ``Local'' and ``Skeleton'' denote optical flow images, local image patches and skeletal information, respectively.}
    \scalebox{0.9}{
    \begin{tabular}{lcccccccccccccccccc} \hline
         & & & & \multicolumn{3}{c}{WLASL100} && \multicolumn{3}{c}{WLASL300} && \multicolumn{3}{c}{WLASL1000} && \multicolumn{3}{c}{WLASL2000}\\ \cline{5-7} \cline{9-11} \cline{13-15} \cline{17-19}
        Model & Flow & Local & Skeleton & Top 1 & Top 5 & Top 10 && Top 1 & Top 5 & Top 10 && Top 1 & Top 5 & Top 10 && Top 1 & Top 5 & Top 10 \\ \hline \hline
        Baseline1~\cite{li2020word} & & & & 65.89 & 84.11 & 89.92 && 56.14 & 79.94 & 86.98 && 47.33 & 76.44 & 84.33 && 32.48& 57.31 & 66.31\\
        Baseline2 & \checkmark & & & 77.55 & 91.25 & 94.92 && 66.96 & 87.61 & 92.03 && 56.35 & 83.03 & 88.77 && 38.67 & 68.43 & 76.39 \\ \hline
        Ours1 & & \checkmark & & 76.60 & 89.13 & 92.80 && 66.34 & 88.46 & 92.43 && 56.91 & 84.55 & 89.83 && 41.01 & 74.46 & 81.85 \\
        Ours2 & & & \checkmark & 71.07 & 90.13 & 93.42 && 65.10 & 85.49 & 90.64 && 53.75 & 80.01 & 86.56 && 37.65 & 67.61 & 75.97 \\
        Ours3 & & \checkmark & \checkmark & 77.48 & 92.38 & 95.72 && 69.99 & 89.91 & 93.50 && 60.85 & 86.98 & 91.41 && 45.12 & 79.17 & 85.65 \\
        Ours4 & \checkmark & \checkmark & & 80.38 & 93.38 & 95.97 && 73.07 & {\bf 90.85} & 94.44 && 62.76 & 88.02 & 92.32 && 45.30 & 79.63 & 85.75 \\
        Ours5 & \checkmark & & \checkmark & 78.05 & 91.63 & 95.42 && 69.77 & 88.61 & 92.33 && 58.68 & 84.05 & 90.25 && 41.94 & 73.16 & 81.68 \\
        Ours6 & \checkmark & \checkmark & \checkmark & {\bf 81.38} & {\bf 94.13} & {\bf 96.05} && {\bf 73.43} & 90.19 & {\bf 94.83} && {\bf 63.61} & {\bf 88.98} & {\bf 92.94} && {\bf 47.26} & {\bf 81.71} & {\bf 87.47} \\ \hline        
    \end{tabular}
    }
    \label{tab:wlasl}
\end{table*}


\subsubsection{Experimental results on MS-ASL dataset}

\begin{table}[tb]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \caption{Compositions of methods and their recognition accuracies (\%) on MS-ASL100, which is a subset of MS-ASL dataset.}
    \begin{tabular}{lccccccc} \hline
         & & & && \multicolumn{3}{c}{MS-ASL100} \\ \cline{6-8}
        Model & Flow & Local & Skeleton && Top 1 & Top 5 & Top 10 \\ \hline \hline
        Baseline1 & & & && 73.69 & 91.38 & 93.87 \\
        Baseline2 & \checkmark & & && 82.46 & 94.66 & 96.61 \\ \hline
        Ours1 & & \checkmark & && 75.12 & 91.33 & 93.87 \\
        Ours2 & & & \checkmark && 75.61 & 92.38 & 95.07 \\
        Ours3 & & \checkmark & \checkmark && 76.69 & 92.65 & 95.35 \\
        Ours4 & \checkmark & \checkmark & && 83.84 & 94.69 & 96.18 \\
        Ours5 & \checkmark & & \checkmark && {\bf 84.22} & 94.77 & 96.48 \\
        Ours6 & \checkmark & \checkmark & \checkmark && 83.86 & {\bf 94.86} & {\bf 96.66} \\ \hline
    \end{tabular}
    \label{tab:msasl}
\end{table}

Next, the proposed models were evaluated on another dataset, MS-ASL dataset, to show its versatility.
Table~\ref{tab:msasl} shows the experimental results on MS-ASL100, a subset of MS-ASL dataset.
Regarding Top-1 recognition accuracy, it was observed that Ours5 achieved the highest accuracy.
On the other hand, Ours6 achieved the highest on Top-5 and Top-10 accuracies and the second-best performance on Top-1 accuracy. 
Similar to the experimental results on WLASL dataset, by comparing the models using and those not using ``Local Image Stream'' and ``Skeleton Stream,'' it was confirmed that the accuracies of the models using both streams were higher than those of not using them.
These results confirmed that the proposed MSNN was effective for WSLR.
However, compared to WLASL dataset, the improvement of recognition accuracy by adding these streams was smaller on MS-ASL100 dataset.
This seems to be due to the diversity of the data variations contained in MS-ASL dataset.
Figure~\ref{fig:sample} shows some examples of the data in WLASL and MS-ASL datasets.
As shown in Fig.~\ref{fig:sample_WLASL}, WLASL dataset contained video frames whose camera views were limited to the frontal of the signers and focusing on the upper bodies of the signers.
On the other hand, MS-ASL dataset contained video frames of different view angles and different body parts, as shown in Fig.~\multisubref{fig:sample_MS-ASL_side}{fig:sample_MS-ASL_upperbody}.
In the proposed method, three keypoints (i.e., shoulder, elbow, and wrist) were acquired by OpenPose to detect a hand region.
Therefore, in the case of a close-up of the signer, the elbow point may not be estimated, which causes failure in extracting the bounding box of a hand.
In addition, the cropped local image patches and skeletal estimation results of the videos captured from the side viewpoints are far different from those captured from the front viewpoints.
From the discussion above, we consider that such data affected the training in ``Local Image Stream'' and ``Skeletal Stream'' and disturbed the SLR performance improvement.
Hence, in our future work, we need to improve the robustness against view changes, like shown in Fig.~\ref{fig:sample}.
Besides, we think making the extraction method of hand regions robust is effective, particularly in a close-up view.
The extraction of hand regions of the signer relies on the elbow point in the current implementation.
However, it is desirable to make the extraction of hand regions free from the elbow point.


\begin{figure}[t]
  \centering

      \begin{minipage}[b]{0.49\hsize}
        \centering
          \includegraphics[width=\linewidth]{images/WLASL_sample.pdf}
          \subcaption{WLASL}
          \label{fig:sample_WLASL}
      \end{minipage}

      \begin{minipage}[b]{0.49\hsize}
        \centering
          \includegraphics[width=\linewidth]{images/MSASL_sample_1.pdf}
          \subcaption{MS-ASL (Side View)}
          \label{fig:sample_MS-ASL_side}
      \end{minipage}
\hfill
\begin{minipage}[b]{0.49\hsize}
      \centering
        \includegraphics[width=\linewidth]{images/MSASL_sample_2.pdf}
         \subcaption{MS-ASL (Upper body)}
          \label{fig:sample_MS-ASL_upperbody}
      \end{minipage}
      
    \caption{Sample video frames of WLASL and MS-ASL datasets.}
    \label{fig:sample}
\end{figure}



\subsubsection{Verifying scalability}
\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{images/scalability.pdf}
    \caption{Scalability comparison of the methods and their Top-1 accuracies (\%) on four subsets of WLASL dataset. }
    \label{fig:scalability}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[width=\linewidth]{images/paper_school.jpg}
  \caption{Examples of sign words that are represented with similar gestures:
  ``Paper'' in the top row, and ``School'' in the bottom row in MS-ASL dataset~\cite{vaezi2019ms-asl}.}
  \label{fig:ambiguty}
\end{figure}

\begin{figure}[t]
  \centering
    \includegraphics[width=\linewidth]{images/pizza.jpg}
  \caption{Examples of different gestures that represent a sign word ``Pizza'' in WLASL dataset~\cite{li2020word}.
}
  \label{fig:diversity}
\end{figure}

Finally, the scalability of the proposed method was evaluated with four subsets of WLASL dataset.
As shown in Table~\ref{tab:wlasl} and Fig.~\ref{fig:scalability}, as the number of classes in the dataset increased, the recognition accuracies of all the models decreased significantly.
The Top-1 accuracy of WLASL2000 dataset, which had the closest number of classes to the vocabulary size used in daily life, was as low as 47.26\% even with Ours6, which was the model achieving the highest accuracy.
Therefore, from these results, it can be inferred that the proposed method in the current form is not enough for practical use in real life.
It seems to be caused by a relatively small \textit{number of videos per class} of the dataset.
The four subsets of WLASL dataset consisted of the top-100, 300, 1000, and 2000 classes, where the classes were sorted in order of the number of videos per class.
As shown in Table~\ref{tab:dataset}, the average number of videos per class decreased as the number of classes increased.
In this study, since the supervised learning approach for WSLR was used, more training data can lead to more accurate recognition performance by acquiring richer information.
Less training data may enhance the effects of the following situations.
One situation is the ambiguity and diversity of expressions in sign language.
As shown in Fig.~\ref{fig:ambiguty}, there are signs with ambiguous expressions where similar gestures are employed to express different words.
The other situation is, as shown in Fig.~\ref{fig:diversity}, some different gestures represent the same word in sign language.
These words are considered to be difficult to recognize even by humans.
More discriminative information should be extracted from the images to solve this problem, and a module to combine the information of different gestures should be built when a sign word having some representations of gesture should be recognized.
These are our future work.

On a different viewpoint for these results, Table~\ref{tab:wlasl} and Fig.~\ref{fig:scalability} showed that our proposed method constantly improved about 15\% in the Top-1 accuracy compared with baseline1~\cite{li2020word} regardless of the dataset size. 
Therefore, from this, it was confirmed that the additional information could contribute to lessening the recognition performance down and maintaining the scalability of the proposed method. 

































\section{Conclusion}\label{conclusion}
In this paper, a method with a multi-stream structure focusing on global information, local information, and skeletal information to improve the accuracy of WSLR was proposed. The proposed multi-stream structure consisted of ``Base Stream,'' ``Local Image Stream,'' and ``Skeleton Stream.'' ``Base Stream'' was used in the conventional methods. However, it did not exploit local information of hand and face and the positional relationship among the body and both hands. Hence, two other streams were introduced. ``Local Image Stream'' captures local information such as hand shape and facial expression. ``Skeleton Stream'' does hand position relative to the body. 
By combining three streams, the proposed method allows us to achieve higher recognition performance compared with the state-of-the-art methods. Especially, the experimental results on the WLASL dataset confirmed that the proposed multi-stream models achieved 81.38\%, 73.43\%, 63.61\% and 47.26\% of Top-1 accuracy on the WLASL100, 300, 1000, and 2000 datasets, respectively, and improved the recognition performance compared with the conventional methods which utilized only global information. Besides, to verify the effectiveness of "Local Image Stream" and "Skeleton Stream," the recognition performance is compared with and without each stream. As a result, the models with all three streams achieved higher recognition accuracies than others. This confirms that the three streams used in the proposed method were effective for WSLR.  Moreover, in the experiments on the MS-ASL dataset, the proposed method also enabled us to achieve 83.86\% of Top-1 accuracy, which was better recognition performance than conventional methods. Therefore, it confirms that the proposed method was not a data-specific method but a high versatility for WSLR.

In the future, we aim to further improve the recognition accuracy by designing a model that can handle various information and can have high scalability. In addition, the proposed model is expected to be able to apply to other sign languages than American such as British, Japanese, Indian. Hence, we perform the experiments on these sign languages.













\section*{Acknowledgment}
This work was supported by JSPS KAKENHI JP19K12023.




\ifCLASSOPTIONcaptionsoff
  \newpage
\fi








\begin{thebibliography}{10}
	\providecommand{\url}[1]{#1}
	\csname url@samestyle\endcsname
	\providecommand{\newblock}{\relax}
	\providecommand{\bibinfo}[2]{#2}
	\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
	\providecommand{\BIBentryALTinterwordstretchfactor}{4}
	\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
		\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
		\fontdimen4\font\relax}
	\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
			\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
			\language=\csname l@#1\endcsname
			\fi
			#2}}
	\providecommand{\BIBdecl}{\relax}
	\BIBdecl
	
	\bibitem{li2020transferring}
	D.~Li, X.~Yu, C.~Xu, L.~Petersson, and H.~Li, ``Transferring cross-domain
	knowledge for video sign language recognition,'' in \emph{Proc. of CVPR},
	2020, pp. 6205--6214.
	
	\bibitem{wang2014similarity}
	L.-C. Wang, R.~Wang, D.-H. Kong, and B.-C. Yin, ``Similarity assessment model
	for chinese sign language videos,'' \emph{IEEE Trans. Multimedia}, vol.~16,
	no.~3, pp. 751--761, 2014.
	
	\bibitem{Hosain_2021_WACV}
	A.~A. Hosain, P.~S. Santhalingam, P.~Pathak, H.~Rangwala, and J.~Kosecka,
	``Hand pose guided 3d pooling for word-level sign language recognition,'' in
	\emph{Proceedings of the IEEE/CVF Winter Conference on Applications of
		Computer Vision (WACV)}, January 2021, pp. 3429--3439.
	
	\bibitem{Camgoz_2020_CVPR}
	N.~C. Camgoz, O.~Koller, S.~Hadfield, and R.~Bowden, ``Sign language
	transformers: Joint end-to-end sign language recognition and translation,''
	in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and
		Pattern Recognition (CVPR)}, June 2020.
	
	\bibitem{li2020word}
	D.~Li, C.~Rodriguez, X.~Yu, and H.~Li, ``Word-level deep sign language
	recognition from video: A new large-scale dataset and methods comparison,''
	in \emph{Proc. of WACV}, 2020, pp. 1459--1469.
	
	\bibitem{vaezi2019ms-asl}
	H.~R. Vaezi~Joze and O.~Koller, ``{MS-ASL}: A large-scale data set and
	benchmark for understanding american sign language,'' in \emph{Proc. of
		BMVC}, 2019.
	
	\bibitem{joao2017i3d}
	J.~Carreira and A.~Zisserman, ``Quo vadis, action recognition? a new model and
	the kinetics dataset,'' in \emph{Proc. of CVPR}, 2017, pp. 6299--6308.
	
	\bibitem{adria2018zoom}
	R.~Adria, K.~Petr, S.~Simon, M.~Wojciech, and T.~Antonio, ``Learning to zoom: a
	saliency-based sampling layer for neural networks,'' in \emph{Proc. of ECCV},
	2018, pp. 51--66.
	
	\bibitem{yan2018stgcn}
	S.~Yan, Y.~Xiong, and D.~Lin, ``Spatial temporal graph convolutional networks
	for skeleton-based action recognition,'' in \emph{Proc. of AAAI}, 2018, pp.
	7444--7452.
	
	\bibitem{cui2019deep}
	R.~Cui, H.~Liu, and C.~Zhang, ``A deep neural framework for continuous sign
	language recognition by iterative training,'' \emph{IEEE Trans. Multimedia},
	vol.~21, no.~7, pp. 1880--1891, 2019.
	
	\bibitem{evangelidis2014continuous}
	G.~D. Evangelidis, G.~Singh, and R.~Horaud, ``Continuous gesture recognition
	from articulated poses,'' in \emph{Proc. of ECCV}, 2014, pp. 595--607.
	
	\bibitem{koller2015continuous}
	O.~Koller, J.~Forster, and H.~Ney, ``Continuous sign language recognition:
	Towards large vocabulary statistical recognition systems handling multiple
	signers,'' \emph{Computer Vision and Image Understanding}, vol. 141, pp.
	108--125, 2015.
	
	\bibitem{rastgoo2020sign}
	R.~Rastgoo, K.~Kiani, and S.~Escalera, ``Sign language recognition: A deep
	survey,'' \emph{Expert Systems with Applications}, p. 113794, 2020.
	
	\bibitem{buehler2009learning}
	P.~Buehler, A.~Zisserman, and M.~Everingham, ``Learning sign language by
	watching tv (using weakly aligned subtitles),'' in \emph{Proc. of CVPR},
	2009, pp. 2961--2968.
	
	\bibitem{pfister2014domain}
	T.~Pfister, J.~Charles, and A.~Zisserman, ``Domain-adaptive discriminative
	one-shot learning of gestures,'' in \emph{Proc. of ECCV}, 2014, pp. 814--829.
	
	\bibitem{monnier2014multi}
	C.~Monnier, S.~German, and A.~Ost, ``A multi-scale boosted detector for
	efficient and robust gesture recognition,'' in \emph{Proc. of ECCV}, 2014,
	pp. 491--502.
	
	\bibitem{cheok2019review}
	M.~J. Cheok, Z.~Omar, and M.~H. Jaward, ``A review of hand gesture and sign
	language recognition techniques,'' \emph{International Journal of Machine
		Learning and Cybernetics}, vol.~10, no.~1, pp. 131--153, 2019.
	
	\bibitem{pfister2013large}
	T.~Pfister, J.~Charles, and A.~Zisserman, ``Large-scale learning of sign
	language by watching tv (using co-occurrences).'' in \emph{Proc. of BMVC},
	2013.
	
	\bibitem{pigou2014sign}
	L.~Pigou, S.~Dieleman, P.-J. Kindermans, and B.~Schrauwen, ``Sign language
	recognition using convolutional neural networks,'' in \emph{Proc. of ECCV},
	2014, pp. 572--578.
	
	\bibitem{wu2014leveraging}
	D.~Wu and L.~Shao, ``Leveraging hierarchical parametric networks for skeletal
	joints based action segmentation and recognition,'' in \emph{Proc. of CVPR},
	2014, pp. 724--731.
	
	\bibitem{molchanov2016online}
	P.~Molchanov, X.~Yang, S.~Gupta, K.~Kim, S.~Tyree, and J.~Kautz, ``Online
	detection and classification of dynamic hand gestures with recurrent 3d
	convolutional neural network,'' in \emph{Proc. of CVPR}, 2016, pp.
	4207--4215.
	
	\bibitem{neverova2014multi}
	N.~Neverova, C.~Wolf, G.~W. Taylor, and F.~Nebout, ``Multi-scale deep learning
	for gesture detection and localization,'' in \emph{Proc. of ECCV}, 2014, pp.
	474--490.
	
	\bibitem{gweth2012enhanced}
	Y.~L. Gweth, C.~Plahl, and H.~Ney, ``Enhanced continuous sign language
	recognition using pca and neural network features,'' in \emph{Proc. of CVLR
		Workshops}, 2012, pp. 55--60.
	
	\bibitem{koller2016deeps}
	O.~Koller, O.~Zargaran, H.~Ney, and R.~Bowden, ``Deep sign: hybrid {CNN}-{HMM}
	for continuous sign language recognition,'' in \emph{Proc. of BMVC}, 2016,
	pp. 136.1--136.12.
	
	\bibitem{wu2016deep}
	D.~Wu, L.~Pigou, P.-J. Kindermans, N.~D.-H. Le, L.~Shao, J.~Dambre, and J.-M.
	Odobez, ``Deep dynamic neural networks for multimodal gesture segmentation
	and recognition,'' \emph{IEEE Trans. PAMI}, vol.~38, no.~8, pp. 1583--1597,
	2016.
	
	\bibitem{pigou2018beyond}
	L.~Pigou, A.~Van Den~Oord, S.~Dieleman, M.~Van~Herreweghe, and J.~Dambre,
	``Beyond temporal pooling: Recurrence and temporal convolutions for gesture
	recognition in video,'' \emph{International Journal of Computer Vision}, vol.
	126, no. 2-4, pp. 430--439, 2018.
	
	\bibitem{cheng2017focusing}
	Z.~Cheng, F.~Bai, Y.~Xu, G.~Zheng, S.~Pu, and S.~Zhou, ``Focusing attention:
	Towards accurate text recognition in natural images,'' in \emph{Proc. of
		CVPR}, 2017, pp. 5076--5084.
	
	\bibitem{cheng2018aon}
	Z.~Cheng, Y.~Xu, F.~Bai, Y.~Niu, S.~Pu, and S.~Zhou, ``Aon: Towards
	arbitrarily-oriented text recognition,'' in \emph{Proc. of CVPR}, 2018, pp.
	5571--5579.
	
	\bibitem{li2019show}
	H.~Li, P.~Wang, C.~Shen, and G.~Zhang, ``Show, attend and read: A simple and
	strong baseline for irregular text recognition,'' in \emph{Proc of AAAI},
	vol.~33, 2019, pp. 8610--8617.
	
	\bibitem{graves2014towards}
	A.~Graves and N.~Jaitly, ``Towards end-to-end speech recognition with recurrent
	neural networks,'' in \emph{Proc. of PMLR}, 2014, pp. 1764--1772.
	
	\bibitem{bahdanau2014neural}
	D.~Bahdanau, K.~Cho, and Y.~Bengio, ``Neural machine translation by jointly
	learning to align and translate,'' \emph{arXiv preprint arXiv:1409.0473},
	2014.
	
	\bibitem{sutskever2014sequence}
	I.~Sutskever, O.~Vinyals, and Q.~V. Le, ``Sequence to sequence learning with
	neural networks,'' \emph{Advances in neural information processing systems},
	vol.~27, pp. 3104--3112, 2014.
	
	\bibitem{pu2019iterative}
	J.~Pu, W.~Zhou, and H.~Li, ``Iterative alignment network for continuous sign
	language recognition,'' in \emph{Proc. of CVPR}, 2019, pp. 4165--4174.
	
	\bibitem{koller2016deeph}
	O.~Koller, H.~Ney, and R.~Bowden, ``Deep hand: How to train a cnn on 1 million
	hand images when your data is continuous and weakly labelled,'' in
	\emph{Proc. of CVPR}, 2016, pp. 3793--3802.
	
	\bibitem{cui2017recurrent}
	R.~Cui, H.~Liu, and C.~Zhang, ``Recurrent convolutional neural networks for
	continuous sign language recognition by staged optimization,'' in \emph{Proc.
		of CVPR}, 2017, pp. 7361--7369.
	
	\bibitem{dempster1977maximum}
	A.~P. Dempster, N.~M. Laird, and D.~B. Rubin, ``Maximum likelihood from
	incomplete data via the em algorithm,'' \emph{Journal of the Royal
		Statistical Society: Series B (Methodological)}, vol.~39, no.~1, pp. 1--22,
	1977.
	
	\bibitem{chris2014incep}
	C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~E. Reed, D.~Anguelov, D.~Erhan,
	V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in
	\emph{Proc. of CVPR}, 2014, pp. 1--9.
	
	\bibitem{olga2015imagenet}
	O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
	A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei, ``Imagenet
	large scale visual recognition challenge,'' \emph{IJCV}, vol. 115, no.~3, pp.
	211--252, 2015.
	
	\bibitem{zach2007tvl1}
	C.~Zach, T.~Pock, and H.~Bischof, ``A duality based approach for realtime
	{TV-L} optical flow,'' in \emph{Pattern Recognition}, 2007, pp. 214--223.
	
	\bibitem{tu2018mscnn}
	Z.~Tu, W.~Xie, Q.~Qin, R.~Poppe, R.~C. Veltkamp, B.~Li, and J.~Yuan,
	``Multi-stream {CNN}: Learning representations based on human-related regions
	for action recognition,'' \emph{Pattern Recognition}, vol.~79, pp. 32--43,
	2018.
	
	\bibitem{bilen2018multi}
	H.~{Bilen}, B.~{Fernando}, E.~{Gavves}, and A.~{Vedaldi}, ``Action recognition
	with dynamic image networks,'' \emph{IEEE Trans. PAMI}, vol.~40, no.~12, pp.
	2799--2813, 2018.
	
	\bibitem{zang2018multi}
	J.~Zang, L.~Wang, Z.~Liu, Q.~Zhang, G.~Hua, and N.~Zheng, ``Attention-based
	temporal weighted convolutional neural network for action recognition,'' in
	\emph{Proc. of AIAI}, 2018, pp. 97--108.
	
	\bibitem{cao2017openpose}
	Z.~{Cao}, G.~{Hidalgo Martinez}, T.~{Simon}, S.~{Wei}, and Y.~A. {Sheikh},
	``Realtime multi-person {2D} pose estimation using part affinity fields,'' in
	\emph{Proc. of CVPR}, 2017, pp. 1302--1310.
	
	\bibitem{redmon2018yolov3}
	J.~Redmon and A.~Farhadi, ``{YOLO}v3: An incremental improvement,''
	\emph{arxiv:1804.02767}, 2018.
	
	\bibitem{liu2016ssd}
	W.~Liu, D.~Anguelov, D.~Erhan, C.~Szegedy, S.~Reed, C.-Y. Fu, and A.~C. Berg,
	``{SSD}: Single shot multibox detector,'' in \emph{Proc. of ECCV}, 2016, pp.
	21--37.
	
\end{thebibliography}
















































\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/maruyama.jpg}}]{Mizuki Maruyama}
is a Master Student at Department of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University, Japan. He received the B.E. degrees in Engineering from Osaka Prefecture University in 2019. His research interests include sign language recognition.
\end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/Shuvozit.jpg}}]{Shuvozit Ghose} completed his B.Tech in 2020 from Institute of Engineering \& Management, Kolkata, India. Now, he is an intern at the Department of Computer Science and Engineering, IIT Roorkee, India.
\end{IEEEbiography}




\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/inoue.pdf}}]{Katsufumi Inoue}
is an Associate Professor at Department of Computer Science and Information Systems, 
Graduate School of Engineering, Osaka Prefecture University, Japan. He is received the B.E., M.E., and 
Ph.D. degrees in Engineering from Osaka Prefecture University in 2008, 2010, and 2012, respectively.
His current research interests include image sensing, first-person vision analysis, 
gesture recognition, 3D object recognition and reconstruction, voice synthesis, 
and music generation, etc. 
He won the Best Paper Award at the 5th Asian Conference on Information Systems (ACIS2016). 
\end{IEEEbiography}



\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/Proy_BW2.jpg}}]{Partha Pratim Roy} is an Associate Professor in the Department of Computer Science and Engineering, IIT Roorkee, India. Dr. Roy completed his MS and PhD from Universitat Autonoma de Barcelona, Spain. He was with Synchromedia Lab, Canada, in 2013 and RFAI Lab, France, during 2011-2012 as a postdoctoral research fellow. Dr. Roy worked in TATA Consultancy Services during 2003-2005 and Advanced Technology Group, Samsung Research Institute Noida, India, during 2013-2014. His research interests are Pattern Recognition, Human Computer Interaction, Bio-Signal Analysis, Multilingual Text Recognition. He has published more than 225 papers in international journals and conferences. Dr. Roy is an Associate Editor of IET Image Processing, IET Biometrics, IEICE Transactions on Information and Systems, Springer Nature Computer Science. He is Regional Editor of The Journal of Multimedia and Information System, and Guest Editor of International Journal of Distributed Sensor Networks.
\end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/MasaImage.jpeg}}]{Masakazu Iwamura} is an associate professor of the Department of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University. He received the B.E., M.E., and Ph.D degrees in engineering from Tohoku University, Japan, in 1998, 2000 and 2003, respectively. His research interests include text and object recognition, and visually impaired assistance. He received awards including IAPR/ICDAR Young Investigator Award in 2011, best paper award of IEICE in 2008, IAPR/ICDAR best paper awards in 2007, IAPR Nakano award in 2010, the ICFHR best paper award in 2010, and MVA best paper award in 2017. He worked as the vice-chair of the IAPR technical committee 11 (reading systems) in 2016-2018. He has been an Associate Editor of the International Journal of Document Analysis and Recognition since 2013, and an Associate Editor of the IEICE Transactions on Information and Systems since 2017.
\end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/yoshioka.pdf}}]{Michifumi Yoshioka}
was born in Osaka, Japan, on Dec., 10, 1968. He received the B.E., the M.E. and Ph.D. 
degrees in geo system engineering from University of Tokyo, in 1991,1993, and 1996, respectively. 
In 1996, he joined Osaka Prefecture University. Since 2010, he has been a Professor. 
His current interests center on image processing methods using neural networks.
\end{IEEEbiography}



\vfill





\end{document}
