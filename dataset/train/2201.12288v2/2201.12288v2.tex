
\documentclass[10pt,twocolumn,letterpaper]{article}



\usepackage[pagenumbers]{cvpr} 

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}




\usepackage{xcolor,color,framed}
\usepackage{url, overpic, cases, contour}
\usepackage{multirow, makecell}
\newcommand{\R}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand{\B}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\definecolor{shadecolor}{rgb}{0.92,0.92,0.92}
\newcommand{\MC}[1]{\multicolumn{2}{c} {#1}}
\newcommand{\et}{\textit{et al}.}
\newcommand{\cmark}{\text{\ding{51}}}\newcommand{\xmark}{\text{\ding{55}}}\def\comp{\ensuremath\mathop{\scalebox{.6}{}}}
\newcommand{\RT}[1]{\textcolor{blue}{\textbf{[RT:} \textit{#1}\textbf{]}}}
\usepackage{adjustbox}
\makeatletter \@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}
\newcommand{\widthscale}{0.10}
\newcommand{\widthscalefive}{0.105}
\newcommand{\name}{0}
\newcommand{\h}{0}
\newcommand{\w}{0.15}
\newcommand{\wa}{0.15}
\newlength \g

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[symbol]{footmisc}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\JY}[1]{\textcolor{red}{\textbf{[Liang:~}\textit{#1}\textbf{]}}}


\def\cvprPaperID{} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{VRT: A Video Restoration Transformer}
\author{Jingyun Liang \qquad Jiezhang Cao \qquad Yuchen Fan  \qquad Kai Zhang\thanks{Corresponding author.}\\ Rakesh Ranjan\qquad  \qquad Yawei Li \qquad Radu Timofte \qquad Luc Van Gool \\
 Computer Vision Lab, ETH Zurich, Switzerland \quad
 Meta Inc. \quad  KU Leuven, Belgium\\
{\tt\small \{jinliang, jiezcao, kaizhang, vangool, timofter\}@vision.ee.ethz.ch \quad\{ycfan, rakeshr\}@fb.com}\quad 
{\tt\small }\\
\url{https://github.com/JingyunLiang/VRT}
}
\maketitle

\begin{abstract}
Video restoration (\eg, video super-resolution) aims to restore high-quality frames from low-quality frames. Different from single image restoration, video restoration generally requires to utilize temporal information from multiple adjacent but usually misaligned video frames. Existing deep methods generally tackle with this by exploiting a sliding window strategy or a recurrent architecture, which either is restricted by frame-by-frame restoration or lacks long-range modelling ability. In this paper, we propose a Video Restoration Transformer (VRT) with parallel frame prediction and long-range temporal dependency modelling abilities. More specifically, VRT is composed of multiple scales, each of which consists of two kinds of modules: temporal mutual self attention (TMSA) and parallel warping. TMSA divides the video into small clips, on which mutual attention is applied for joint motion estimation, feature alignment and feature fusion, while self attention is used for feature extraction. To enable cross-clip interactions, the video sequence is shifted for every other layer. Besides, parallel warping is used to further fuse information from neighboring frames by parallel feature warping. Experimental results on five tasks, including video super-resolution, video deblurring, video denoising, video frame interpolation and space-time video super-resolution, demonstrate that VRT outperforms the state-of-the-art methods by large margins (\textbf{up to 2.16dB}) on fourteen benchmark datasets.
\end{abstract}



\section{Introduction}
Video restoration, which reconstructs high-quality (HQ) frames from multiple low-quality (LQ) frames, has attracted much attention recently. Compared with image restoration, the key challenge of video restoration lies in how to make full use of neighboring highly-related but misaligned supporting frames for reconstructing reference frames. 

Existing video restoration methods can be mainly divided into two categories: sliding window-based methods~\cite{caballero2017VESPCN, huang2017video, wang2019edvr, tian2020tdan, li2020mucan, su2017dvddeblur, zhou2019spatio, isobe2020tga, li2021arvo} and recurrent methods~\cite{huang2015bidirectional, sajjadi2018FRVSR, fuoli2019rlsp, haris2019recurrent, isobe2020rsdn, isobe2020rrn, chan2021basicvsr, chan2021basicvsr++, lin2021fdan, nah2019recurrent, zhong2020efficient, son2021recurrent}. As shown in Fig.~\ref{fig:intro_sliding}, sliding window-based methods generally input multiple frames to generate a single HQ frame and processes long video sequences in a sliding window fashion. Each input frame is processed for multiple times in inference, leading to inefficient feature utilization and increased computation cost. 

Some other methods are based on a recurrent architecture. As shown in Fig.~\ref{fig:intro_rnn}, recurrent models mainly use previously reconstructed HQ frames for subsequent frame reconstruction. Due to the recurrent nature, they have three disadvantages. First, recurrent methods are limited in parallelization for efficient distributed training and inference. Second, although information is accumulated frame by frame, recurrent models are not good at long-range temporal dependency modelling. One frame may strongly affect the next adjacent frame, but its influence is quickly lost after few time steps~\cite{greaves2019statistical, vaswani2017transformer}. Third, they suffer from significant performance drops on few-frame videos~\cite{cao2021videosr}.



\begin{figure}
\captionsetup{font=small}\scriptsize
\hspace{0.35cm}
 \begin{subfigure}{.15\textwidth}
   \centering
   \begin{overpic}[width=0.66\textwidth]{figures/intro_sliding.pdf}
     \put(-38,85){\color{black}{\small }}
     \put(-26,45){\color{black}{\small }}
     \put(-38,4){\color{black}{\small }}
     \put(3,-18){\color{black}{\scriptsize LQ}}
     \put(56,-18){\color{black}{\scriptsize HQ}}
     \put(114,-18){\color{black}{\scriptsize LQ}}
     \put(164,-18){\color{black}{\scriptsize HQ}}
     \put(225.5,-18){\color{black}{\scriptsize LQ}}
     \put(277.5,-18){\color{black}{\scriptsize HQ}}
     \end{overpic}
   \vspace{0.5cm}
   \caption{}
   \label{fig:intro_sliding}
 \end{subfigure}\begin{subfigure}{.15\textwidth}
   \centering
   \includegraphics[width=.66\linewidth]{figures/intro_recurrent.pdf}
   \vspace{0.5cm}
   \caption{}
   \label{fig:intro_rnn}
 \end{subfigure}
 \begin{subfigure}{.15\textwidth}
   \centering
   \includegraphics[width=.66\linewidth]{figures/intro_vrt.pdf}
   \vspace{0.5cm}
   \caption{}
   \label{fig:intro_vrt}
 \end{subfigure}
 \vspace{-0.1cm}
 \caption{Illustrative comparison of sliding window-based models (\ref{fig:intro_sliding}, \eg,~\cite{li2020mucan, tian2020tdan, wang2019edvr}), recurrent models (\ref{fig:intro_rnn}, \eg,~\cite{fuoli2019rlsp, huang2015bidirectional, isobe2020rsdn, chan2021basicvsr, chan2021basicvsr++}) and the proposed parallel VRT model (\ref{fig:intro_vrt}). Green and blue circles denote low-quality (LQ) input frames and high-quality (HQ) output frames, respectively. ,  and  are frame serial numbers. Dashed lines represent information fusion among different frames.}
 \label{fig:intro}
 \vspace{-0.2cm}
\end{figure}




In this paper, we propose a Video Restoration Transformer (VRT) that allows for parallel computation and long-range dependency modelling in video restoration. Based on a multi-scale framework, VRT divides the video sequence into non-overlapping clips and shifts it alternately to enable inter-clip interactions. Specifically, each scale of VRT has several temporal mutual self attention (TMSA) modules followed by a parallel warping module. In TMSA, mutual attention is focused on mutual alignment between neighboring two-frame clips, while self attention is used for feature extraction. At the end of each scale, we further use parallel warping to fuse neighboring frame information into the current frame. After multi-scale feature extraction, alignment and fusion, the HQ frames are individually reconstructed from their corresponding frame features. 

Compared with existing video restoration frameworks, VRT has several benefits. First, as shown in Fig.~\ref{fig:intro_vrt}, VRT is trained and tested on long video sequences in parallel. In contrast, both sliding window-based and recurrent methods are often tested frame by frame. Second, VRT has the ability to model long-range temporal dependencies, utilizing information from multiple neighbouring frames during the reconstruction of each frame. By contrast, sliding window-based methods cannot be easily scaled up to long sequence modelling, while recurrent methods may forget distant information after several timestamps. Third, VRT proposes to use mutual attention for joint feature alignment and fusion. It adaptively utilizes features from supporting frames and fuses them into the reference frame, which can be regarded as implicit motion estimation and feature warping. 

Our contributions can be summarized as follows:
\begin{itemize}
  \vspace{-0.2cm}
  \item[1)] We propose a new framework named Video Restoration Transformer (VRT) that is characterized by parallel computation and long-range dependency modelling. It jointly extracts, aligns, and fuses frame features at multiple scales.
  \vspace{-0.2cm}
  \item[2)] We propose the mutual attention for mutual alignment between frames. It is a generalized ``soft'' version of image warping after implicit motion estimation.
  \vspace{-0.2cm}
  \item[3)] VRT achieves state-of-the-art performance on video restoration, including video super-resolution, deblurring, denoising, frame interpolation and space-time video super-resolution. It outperforms state-of-the-art methods by up to 2.16dB on benchmark datasets.
\end{itemize}


\section{Related Work}
\subsection{Video Restoration}
Similar to image restoration~\cite{dong2014srcnn, zhang2017DnCNN, ledig2017srresnet, fan2017balanced, liu2018NLRN, yu2018wide, zhang2018RDN, zhang2018srmd, zhang2018rcan,zhang2019RNAN, wang2019learning,mei2020CSNLN, guo2020drn,fan2020scale, liang2021fkp, xiang2021boosting, liang21manet, mei2021NLSA, liang21hcflow, wang2021unsupervised, wang2021learning,kai2021bsrgan, li2021beginner, zhang2021DPIR, sun2021mefnet, liang21swinir}, learning-based methods, especially CNN-based methods, have become the primary workhorse for video restoration~\cite{liu2018learning, wang2019edvr, zhou2019spatio, yi2019pfnl_udm, xiang2020deep, xiang2020zooming, pan2020tspdeblur, wang2020deep, pan2020cascaded, yi2021omniscient, chan2021basicvsr, maggioni2021efficient, vaksman2021patch, lee2021restore, yang2021ntire}. 

\vspace{-0.4cm}
\paragraph{Framework design.}
From the perspective of architecture design, existing methods can be roughly divided into two categories: sliding window-based and recurrent methods. Sliding window-based methods often takes a short sequence of frames as input and merely predict the center frame~\cite{caballero2017VESPCN, huang2017video, wang2019edvr, tassano2019dvdnet, tian2020tdan, li2020mucan, su2017dvddeblur, zhou2019spatio, isobe2020tga,tassano2020fastdvdnet, sheth2021unsupervised, li2021arvo}. Although some works~\cite{li2019fast} predict multiple frames, they still focus on the reconstruction of the center frame during training and testing. Recurrent framework is another popular choice~\cite{huang2015bidirectional, sajjadi2018FRVSR, fuoli2019rlsp, haris2019recurrent, isobe2020rsdn, isobe2020rrn, chan2021basicvsr, chan2021basicvsr++, lin2021fdan, nah2019recurrent, zhong2020efficient, son2021recurrent}. Huang~\etal~\cite{huang2015bidirectional} propose a bidirectional recurrent convolutional neural network for SR. Sajjadi~\etal~\cite{sajjadi2018FRVSR} warp the previous frame prediction onto the current frame and feed it to a restoration network along with the current input frame. This idea is used by Chan~\etal~\cite{chan2021basicvsr} for bidirectional recurrent network, and further extended as grid propagation in~\cite{chan2021basicvsr++}. 

\vspace{-0.4cm}
\paragraph{Temporal alignment and fusion.}
Since supporting frames are often highly-related but misaligned, temporal alignment plays an critical role in video restoration~\cite{liao2015video, xue2019TOFlow-Vimeo-90K, tian2020tdan, wang2019edvr, chan2021understanding, chan2021basicvsr, chan2021basicvsr++}. Early methods~\cite{liao2015video, kappeler2016video, caballero2017VESPCN, liu2017robust, tao2017detail} use traditional flow estimation methods to estimate optical flow and warp the supporting frames towards the reference frame. To compensate occlusion and large motion, Xue~\etal~\cite{xue2019TOFlow-Vimeo-90K} utilize task-oriented flow by fine-tuning the pre-trained optical flow estimation model SpyNet~\cite{ranjan2017spynet} on different video restoration tasks. Jo~\etal~\cite{jo2018DUF} use dynamic upsampling filters for implicit motion compensation. Kim~\etal~\cite{kim2018spatio} propose a spatio-temporal transformer network for multi-frame optical flow estimation and warping. Tian~\etal~\cite{tian2020tdan} propose TDAN that utilize deformable convolution~\cite{dai2017deformable} for feature alignment. Based on TDAN, Wang~\etal~\cite{wang2019edvr} extend it to multi-scale alignment, while Chan~\etal~\cite{chan2021basicvsr++} incorporate optical flow as a guidance for offsets learning. 

\vspace{-0.4cm}
\paragraph{Attention mechanism.}
Attention mechanism has been exploited in video restoration in combination with CNN~\cite{liu2017robust, wang2019edvr, suin2021gated, cao2021videosr}. Liu~\etal~\cite{liu2017robust} learn different weights for different temporal branches. Wang~\etal~\cite{wang2019edvr} learn pixel-level attention maps for spatial and temporal feature fusion. To better incorporate temporal information, Isobe~\etal~\cite{isobe2020tga} divide frames into several groups and design a temporal group attention module. Suin~\etal~\cite{suin2021gated} propose a reinforcement learning-based framework with factorized spatio-temporal attention. Cao~\etal~\cite{cao2021videosr} propose to use self attention among local patches within a video.




\begin{figure*}[!tbp]
\captionsetup{font=small}\scriptsize
\begin{center}
\hspace{-0.3cm}
\begin{overpic}[width=17.9cm]{figures/arch.pdf}
\end{overpic}
\end{center}\vspace{-0.5cm}
\caption{The framework of the proposed Video Restoration Transformer (VRT). Given  low-quality input frames, VRT reconstructs  high-quality frames in parallel. It jointly extracts features, deals with misalignment, and fuses temporal information at multiple scales. On each scale, it has two kinds of modules: temporal mutual self attention (TMSA, see Sec.~\ref{sec:mama}) and parallel warping (see Sec.~\ref{sec:paw}). The downsampling and upsampling operations between different scales are omitted for clarity.}
\label{fig:framework}
\end{figure*}


\subsection{Vision Transformer}
Recently, Transformer-based models~\cite{vaswani2017transformer, shazeer2020geglu, li2021trear, wick2021transformer} have achieved promising performance in various vision tasks, such as image recognition~\cite{dosovitskiy2020ViT, carion2020DETR, li2021localvit, wick2021transformer, liu2021swin, guo2021deep, sun2021boosting, liu2021swin, liu2021transformer, liu2020deep} and image restoration~\cite{chen2021IPT, wang2021uformer, liang21swinir}. Some methods have tried to use Transformer for video modelling by extending the attention mechanism to the temporal dimension~\cite{bertasius2021timesformer, arnab2021vivit, neimark2021videotransfomernetwork, liu2021videoSwinT, li2021trear}. However, most of them are designed for visual recognition, which are fundamentally different from restoration tasks. They are more focused on feature fusion than on alignment. Cao~\etal~\cite{cao2021videosr} propose a CNN-transformer hybrid network for video super-resolution (SR) based on spatial-temporal convolutional self attention. However, it does not make full use of local information within each patch and suffers from border artifacts during testing.



\section{Video Restoration Transformer}
\subsection{Overall Framework}
\label{sec:overall}
Let  be a sequence of low-quality (LQ) input frames and  be a sequence of high-quality (HQ) target frames. , , ,  and  are the frame number, height, width and input channel number and output channel number, respectively.  is the upscaling factor, which is larger than 1 (\eg, for video SR) or equal to 1 (\eg, for video deblurring). The proposed Video Restoration Transformer (VRT) aims to restore  HQ frames from  LQ frames in parallel for various video restoration tasks, including video SR, deblurring, denoising, \etc. As illustrated in Fig.~\ref{fig:framework}, VRT can be divided into two parts: feature extraction and reconstruction.

\vspace{-0.4cm}
\paragraph{Feature extraction.}
At the beginning, we extract shallow features  by a single spatial 2D convolution from the LQ sequence . After that, based on \cite{ronneberger2015u}, we propose a multi-scale network that aligns frames at different image resolutions. More specifically, when the total scale number is , we downsample the feature for  times by squeezing each  neighborhood to the channel dimension and reducing the channel number to the original number via a linear layer. Then, we upsample the feature gradually by unsqueezing the feature back to its original size. In such a way, we can extract features and deal with object or camera motions at different scales by two kinds of modules: temporal mutual self attention (TMSA, see~\ref{sec:mama}) and parallel warping (see~\ref{sec:paw}). Skip connections are added for features of same scales. Finally, after multi-scale feature extraction, alignment and fusion, we add several TMSA modules for further feature refinement and obtain the deep feature .




\begin{figure*}
 \begin{subfigure}{.41\textwidth}
   \centering
   \begin{overpic}[width=1\textwidth]{figures/mama_warp.pdf}
\put(10,-2){\color{black}{\scriptsize supporting frame  }}
\put(61,-2){\color{black}{\scriptsize reference frame }}
\put(45,28.5){\color{black}{\scriptsize \color{black}{}}}
\put(45,22){\color{black}{\scriptsize \color{black}{}}}
\put(45,9){\color{black}{\scriptsize \color{black}{}}}
     \end{overpic}
    \vspace{0.23cm}
   \caption{Mutual attention}
   \label{fig:mama_warp}
 \end{subfigure}\hspace{1cm}
 \begin{subfigure}{.5\textwidth}
   \centering
      \begin{overpic}[width=1\textwidth]{figures/mama_temporal.pdf}
\put(-10.3,42){\color{black}{\scriptsize (layer)}}
\put(-10,37){\color{black}{\scriptsize }}
\put(-10,27){\color{black}{\scriptsize }}
\put(-10,17){\color{black}{\scriptsize }}
\put(-8,7){\color{black}{\scriptsize }}
\put(11,-0.75){\color{black}{\scriptsize }}
\put(21,-0.75){\color{black}{\scriptsize }}
\put(33,-0.75){\color{black}{\scriptsize }}
\put(45,-0.75){\color{black}{\scriptsize }}
\put(52,-0.75){\color{black}{\scriptsize }}
\put(63,-0.75){\color{black}{\scriptsize }}
\put(73,-0.75){\color{black}{\scriptsize }}
\put(84,-0.75){\color{black}{\scriptsize }}
\put(98,-0.75){\color{black}{\scriptsize (frame)}}
     \end{overpic}
     \vspace{0.03cm}
   \caption{Stacking of temporal mutual self attention (TMSA)}
   \label{fig:mama_temporal}
 \end{subfigure}
 \caption{Illustrations for mutual attention and temporal mutual self attention (TMSA). In \ref{fig:mama_warp}, we let the orange square (the -th element of the reference frame) query elements in the supporting frame and use their weighted features as a new representation for the orange square. The weights are shown around solid arrows (we only show three examples for clarity). When  and the rest , the mutual attention equals to warping the yellow square to the position of the orange square (illustrated as a dashed arrow). \ref{fig:mama_temporal} shows a stack of temporal mutual self attention (TMSA) layers. The sequence is partitioned into 2-frame clips at each layer and shifted for every other layer to enable cross-clip interactions. Dashed lines represent information fusion among different frames.}
 \label{fig:mama}
\end{figure*}



\vspace{-0.4cm}
\paragraph{Reconstruction.}
After feature extraction, we reconstruct the HQ frames from the addition of shallow feature  and deep feature . Different frames are reconstructed independently based on their corresponding features. Besides, to ease the burden of feature learning, we employ global residual learning and only predict the residual between the bilinearly upsampled LQ sequence and the ground-truth HQ sequence. In practice, different reconstruction modules are used for different restoration tasks. For video SR, we use the sub-pixel convolution layer~\cite{shi2016subpixel} to upsample the feature by a scale factor of . For video deblurring, a single convolution layer is enough for reconstruction. Apart from this, the architecture designs are kept the same for all tasks.

\vspace{-0.4cm}
\paragraph{Loss function.}
For fair comparison with existing methods, we use the commonly used Charbonnier loss~\cite{charbonnier1994Charbonnier} between the reconstructed HQ sequence  and the ground-truth HQ sequence  as

where  is a constant that is empirically set as . 



\subsection{Temporal Mutual Self Attention}
\label{sec:mama}
In this section, based on the attention mechanism~\cite{vaswani2017transformer, li2021trear, wick2021transformer}, we first introduce the mutual attention and then propose the temporal mutual self attention (TMSA).

\vspace{-0.4cm}
\paragraph{Mutual attention.}
Given a reference frame feature  and a supporting frame feature , where  is the number of feature elements and  is the channel number, we compute the \textit{query} , \textit{key}  and \textit{value}  from  and  by linear projections as 

where  are projection matrices.  is the channel number of projected features. Then, we use  to query  in order to generate the attention map , which is then used for weighted sum of . This is formulated as

where  means the row softmax operation.

Since  and  come from  and , respectively,  reflects the correlation between elements in the reference image and the supporting image. For clarity, we rewrite Eq.~\eqref{eq:ma} for the -th element of the reference image as

where  refers to the new feature of the -th element in the reference frame. As shown in Fig.~\ref{fig:mama_warp}, when  (\eg, the yellow square from the supporting frame) is the most similar element to  (\eg, the orange square from the reference frame),  holds for all  (). When all  are very dissimilar to , we have 


In this extreme case, by combining Eq.~\eqref{eq:mama_weighted} and~\eqref{eq:mama_extreme}, we have , which moves the -th element in the supporting frame to the position of the -th element in the reference frame (see the dashed red line in Fig.~\ref{fig:mama_warp}). This equals to image warping given an optical flow vector. When  does not hold, Eq.~\eqref{eq:mama_weighted} can be regarded as a ``soft'' version of image warping. In practice, the reference frame and supporting frame can be exchanged, allowing mutual alignment between two frames. Besides, similar to multi-head self attention, we can also perform the attention for  times and concatenate the results as multi-head mutual attention (MMA).

Particularly, mutual attention has several benefits over the combination of explicit motion estimation and image warping. First, mutual attention can adaptively preserve information from the supporting frame than image warping, which only focuses on the target pixel. It also avoids black hole artifacts when there is no matched positions. Second, mutual attention does not have the inductive biases of locality, which is inherent to most CNN-based motion estimation methods~\cite{dosovitskiy2015flownet, ranjan2017spynet,pytorch-spynet, sun2018pwc} and may lead to performance drop when two neighboring objects move towards different directions. Third, mutual attention equals to conducting motion estimation and warping on image features in a joint way. In contrast, optical flows are often estimated on the input RGB image and then used for warping on features~\cite{chan2021basicvsr, chan2021basicvsr++}. Besides, flow estimation on RGB images is often not robust to lighting variation, occlusion and blur~\cite{xue2019TOFlow-Vimeo-90K}.


\vspace{-0.4cm}
\paragraph{Temporal mutual self attention (TMSA).}
Mutual attention is proposed for joint feature alignment between two frames. To extract and preserve feature from the current frame, we use mutual attention together with self attention. Let  represent two frames, which can be split into  and . We use multi-head mutual attention (MMA) on  and  for two times: warping  towards  and warping  towards . The warped features are combined and then concatenated with the result of multi-head self attention (MSA), followed by a multi-layer perceptron (MLP) for the purpose of dimension reduction. After that, another MLP is added for further feature transformation. Two LayerNorm (LN) layers and two residual connections are also used as shown in the green box of Fig.~\ref{fig:framework}. The whole process formulated as follows

where the subscripts of  and  refer to the specified dimensions. However, due to the design of mutual attention, Eq.~\eqref{eq:masa} can only deal with two frames at a time.

One naive way to extend Eq.~\eqref{eq:masa} for  frames is to deal with frame-to-frame pairs exhaustively, resulting in the computational complexity of . Inspired by the shifted window mechanism~\cite{liu2021swin, liu2021videoSwinT}, we propose the temporal mutual self attention (TMSA) to remedy the problem. TMSA first partitions the video sequence into non-overlapping 2-frame clips and then applies Eq.~\eqref{eq:masa} to them in parallel. Next, as shown in Fig.~\ref{fig:mama_temporal}, it shifts the sequence temporally by 1 frame for every other layer to enable cross-clip connections, reducing the computational complexity to . The temporal receptive field size is increased when multiple TMSA modules are stacked together. Specifically, at layer  (), one frame can utilize information from up to  frames.


\vspace{-0.4cm}
\paragraph{Discussion.}
Video restoration tasks often need to process high-resolution frames. Since the complexity of attention is quadratic to the number of elements within the attention window, global attention on the full image is often impractical. Therefore, following~\cite{liu2021swin, liang21swinir}, we partition each frame spatially into non-overlapping  local windows, resulting in  windows. Shifted window mechanism (with the shift of  pixels) is also used spatially to enable cross-window connections. Besides, although stacking multiple TMSA modules allows for long-distance temporal modelling, distant frames are not directly connected. As will show in the ablation study, using only a small temporal window size cannot fully exploit the potential of the model. Therefore, we use larger temporal window size for the last quarter of TMSA modules to enable direct interactions between distant frames.


\subsection{Parallel Warping}
\label{sec:paw}
Due to spatial window partitioning, the mutual attention mechanism may not be able to deal with large motions well. Hence, as shown in the orange box of Fig.~\ref{fig:framework}, we use feature warping at the end of each network stage to handle large motions. For any frame feature , we calculate the optical flows of its neighbouring frame features  and , and warp them towards the frame  as  and  (\ie, backward and forward warping). Then, we concatenate them with the original feature and use an MLP for feature fusion and dimension reduction. Specifically, following~\cite{chan2021basicvsr++}, we predict the residual flow by a flow estimation model and use deformable convolution~\cite{dai2017deformable} for deformable alignment. More details are provided in the supplementary.








\section{Experiments}
\subsection{Experimental Setup}
For video SR, we use 4 scales for VRT. On each scale, we stack 8 TMSA modules, the last two of which use a temporal window size of 8. The spatial window size , head size , and channel size  are set to ,  6 and 120, respectively. After 7 multi-scale feature extraction stages, we add 24 TMSA modules (only with self attention) for further feature extraction before reconstruction. More details are provided in the supplementary.



\begin{table*}[!t]
    \caption{Quantitative comparison (average PSNR/SSIM) with state-of-the-art methods for \textbf{video super-resolution ()} on \textbf{REDS4}~\cite{nah2019ntireREDS}, \textbf{Vimeo-90K-T}~\cite{xue2019TOFlow-Vimeo-90K}, \textbf{Vid4}~\cite{liu2013bayesianVid4} and \textbf{UDM10}~\cite{yi2019pfnl_udm}. Best and second best results are in \R{red} and \B{blue} colors, respectively. We currently do not have enough GPU memory to train the fully parallel model VRT on 30 frames.}\label{tab:vsr_quan}
    \vspace{-0.6cm}
    \begin{center}\scalebox{0.84}{
            \tabcolsep=0.1cm
            \begin{tabular}{|l|c|c|c||c||c|c||c|c|c|}
                \hline
                \multirow{2}{*}{\makecell{\vspace{-0.5cm}Method}}                  &   \multirow{2}{*}{\scalebox{0.7}{\makecell{Training\\Frames\M)}}          &    \multirow{2}{*}{\vspace{-0.5cm}\makecell{Runtime\RGB channel)}           & \makecell{\scalebox{0.7}{Vimeo-90K-T}~\cite{xue2019TOFlow-Vimeo-90K}\Y channel)} & \makecell{UDM10~\cite{yi2019pfnl_udm}\Y channel)} & \makecell{Vid4~\cite{liu2013bayesianVid4}} & \makecell{2\16\times 168\times 8\times\times1\times 8\times 82\times 8\times 84\times 8\times 88\times 8\times 8$\\\hline
PSNR  & 27.10 & 27.13 & 27.18 &  27.28\\
\hline
\end{tabular}
\end{center}
\vspace{-0.1cm}
\end{table}




\section{Conclusion}
In this paper, we proposed the Video Restoration Transformer (VRT) for video restoration. Based on a multi-scale framework, it jointly extracts, aligns, and fuses information from different frames at multiple resolutions by two kinds of modules: multiple temporal mutual self attention (TMSA) and parallel warping. More specifically, TMSA is composed of mutual and self attention. Mutual attention allows joint implicit flow estimation and feature warping, while self attention is responsible for feature extraction. Parallel warping is also used to further enhance feature alignment and fusion. Extensive experiments on various benchmark datasets show that VRT 
leads to significant performance gains (up to 2.16dB) for video restoration, including video super-resolution, video deblurring, video denoising, video frame interpolation and space-time video super-resolution.





\vspace{0.8cm}
\noindent\textbf{Acknowledgements}~~ This work was partially supported by the ETH Zurich Fund (OK), a Huawei Technologies Oy (Finland) project, the China Scholarship Council and an Amazon AWS grant. Thanks Dr. Gurkirt Singh for insightful discussions. Special thanks goes to Yijue Chen.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{superresolution.bib}
}

\end{document}
