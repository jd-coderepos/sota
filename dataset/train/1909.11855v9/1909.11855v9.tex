\documentclass[twoside,leqno,twocolumn]{article}
\usepackage[letterpaper]{geometry}
\usepackage{ltexpprt}


\usepackage[algo2e,linesnumbered,ruled]{algorithm2e}
\usepackage{algorithm}


\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{pgfplotstable} 
\pgfplotsset{compat=newest}
\usepackage{enumitem}
\usepackage{balance}
\usepackage{wrapfig}


\newcommand{\citet}{\cite}
\newcommand{\citep}{\cite}
\newcommand{\citeyearpar}{\cite}


\begin{document}

\title{Universal Graph Transformer Self-Attention Networks}

\author{Dai Quoc Nguyen\thanks{Monash University, Australia, email: {\tt dai.nguyen@monash.edu}} \and Tu Dinh Nguyen\thanks{{\tt nguyendinhtu@gmail.com}} \and Dinh Phung\thanks{Monash University, Australia, email: {\tt dinh.phung@monash.edu}}
}

\date{}

\maketitle

\begin{abstract}
The transformer self-attention network has been extensively used in research domains such as computer vision, image processing, and natural language processing. But it has not been actively used in graph neural networks (GNNs) where constructing an advanced aggregation function is essential. To this end, we present U2GNN, an effective GNN model leveraging a transformer self-attention mechanism followed by a recurrent transition, to induce a powerful aggregation function to learn graph representations. Experimental results show that the proposed U2GNN achieves state-of-the-art accuracies on well-known benchmark datasets for graph classification.\footnote{Our code is available at: \url{https://github.com/daiquocnguyen/Graph-Transformer}.}
\\

\noindent \textbf{Keywords:} Graph neural networks $\cdot$ Graph classification $\cdot$  Transformer $\cdot$ Self-attention


\end{abstract}

\section{Introduction}

Graph-structured data appear in many real-world and scientific fields, e.g., knowledge graphs, recommender systems,  social and citation networks, as well as telecommunication and biological networks \citep{battaglia2018relational,zhang2018deep}. 
In general, we can view a graph as a network of nodes and edges, where nodes correspond to individual objects, and edges encode relationships among those objects. 
For example, in an online forum, each discussion thread can be constructed as a graph where nodes represent users, and edges represent commenting activities between users \citep{yanardag2015deep}.

Learning graph representations is one of the most important topics for the graph-structured data \citep{hamilton2017representation,zhou2018graph,wu2019comprehensive,zhang2020network}, where a goal is to construct vector representations for graphs to predict graph labels.
Early approaches measure similarities among graphs to build a graph kernel for graph classification \citep{gartner2003graph}, wherein graph kernel-based approaches decompose graphs into ``atomic subgraphs'' (such as random walks \citep{gartner2003graph,kashima2003marginalized,vishwanathan2010graph}, shortest paths \citep{Borgwardt2005}, graphlets \citep{shervashidze2009efficient}, and Weisfeiler-Lehman subtree patterns \citep{shervashidze2011weisfeiler}) to have a vector of frequencies of atomic subgraphs for each given graph.
Besides, word embedding-based approaches utilize Word2Vec \citep{MikolovSCCD13nips} and Doc2Vec \citep{le:2014} to learn embeddings for the atomic subgraphs \citep{yanardag2015deep} and the entire graphs \citep{narayanan2017graph2vec,ivanov2018anonymous}.


Recently, graph neural networks (GNNs) become an essential strand to learn low-dimensional continuous embeddings of the entire graphs to predict the graph labels \citep{scarselli2009graph,hamilton2017representation,zhou2018graph,wu2019comprehensive,zhang2020network}.
GNNs use an \textsc{Aggregation} function to update the vector representation of each node by transforming and aggregating the vector representations of its neighbors \citep{kipf2017semi,hamilton2017inductive,velickovic2018graph}.
Then GNNs apply a graph-level pooling function (i.e., a \textsc{ReadOut} operation such as simple sum pooling \citep{xu2019powerful}) to obtain graph embeddings.
Compared to the graph kernel and word embedding-based approaches, GNNs have been achieving state-of-the-art accuracies for the graph classification task. 

To further improve the classification performance, constructing a powerful aggregation function is essential for GNNs as also discussed in \citep{xu2019powerful}.
Nowadays, there are novel applications of the transformer self-attention network \citep{vaswani2017attention,dehghani2018universal} recognized, published, and used successfully in research domains such as computer vision, image processing, and natural language processing.
Hence we consider the use of the transformer to a new domain, i.e., graph neural networks (GNNs), as a novelty.
Inspired by this advanced self-attention network, we present U2GNN, an effective GNN model, which induces a powerful \textsc{Aggregation} function leveraging a self-attention mechanism \citep{vaswani2017attention} followed by a recurrent transition, to update the vector representations of nodes from their neighbors.
In particular, our U2GNN is different from related work as follows:

\begin{itemize}

\item A ``concurrent'' work -- Hyper-{SAGNN} \citep{zhang2020hypersagnn} -- utilizes the transformer self-attention network for hypergraphs that have diverse and different structures, hence requiring a different solution.
Besides, the {later} and closely related work, Graph-BERT \citep{zhang2020graph}, is an extension of our U2GNN for semi-supervised node classification.

\item Graph Attention Network (GAT) \citep{velickovic2018graph} borrows the standard attention technique from \citep{bahdanau2014neural} in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linearity function followed by the $\mathsf{softmax}$ to compute the importance weights of neighbors of a given node.
Note that our U2GNN adopts a scaled dot-product attention mechanism which is more robust and efficient than the attention technique used in GAT.

\item Regarding the model architecture, Graph Transformer Network (GTN) \citep{yun2019graph} identifies useful meta-paths \citep{wang2019heterogeneous} to transform graph structures and applies GCN \citep{kipf2017semi} to learn the node embeddings for the node classification task on heterogeneous graphs. 
Self-Attention Graph Pooling (SAGPool) \citep{Lee2019SelfAttentionGP} uses GCN as an attention mechanism to weight the nodes, employs a node selection method \citep{gao2019graph} to retain a portion of the nodes, and applies the existing graph-level $\textsc{ReadOut}$ pooling layers (consisting of global pooling \citep{zhang2018end} and hierarchical pooling \citep{cangea2018towards}) to obtain the graph embeddings.

\item To this end, we note that U2GNN is entirely different from GAT \citep{velickovic2018graph}, Graph Transformer Network \citep{yun2019graph}, and Self-Attention Graph Pooling \citep{Lee2019SelfAttentionGP}, except similar titles.

\end{itemize}

\noindent \textbf{Contributions.} 
Our main contributions in this paper are as follows:
\begin{itemize}


\item We propose U2GNN, an effective GNN model, leveraging the transformer self-attention network to construct an advanced aggregation function to learn the graph representations.

\item Experimental results show that U2GNN obtains state-of-the-art accuracies on well-known benchmark datasets for the graph classification task.

\end{itemize}


\section{Related work}
\label{sec:relatedwork}

\subsection{Graph kernel-based approaches} 
These early approaches decompose graphs into ``atomic subgraphs'' (such as random walks \citep{gartner2003graph,kashima2003marginalized,vishwanathan2010graph}, shortest paths \citep{Borgwardt2005}, graphlets \citep{shervashidze2009efficient}, and Weisfeiler-Lehman subtree patterns \citep{shervashidze2011weisfeiler}) to measure the similarities among graphs \citep{gartner2003graph}. 
Therefore, we can view each atomic substructure as a word term and each graph as a text document.
We then represent a collection of graphs as a document-term matrix that describes the normalized frequency of terms in documents.
We use an inner product to compute the graph similarities to derive a ``kernel matrix'' used for the kernel-based learning algorithms such as Support Vector Machines (SVM) \citep{hofmann2008kernel} to measure the graph classification performance.
We refer to an overview of the graph kernel-based approaches in \citep{nikolentzos2019graph,kriege2019survey}.

\subsection{Word embedding-based approaches} 
Since the introduction of word embedding models such as Word2Vec \citep{MikolovSCCD13nips} and Doc2Vec \citep{le:2014}, several works have used them for the graph classification task.
Deep graph kernel (DGK) \citep{yanardag2015deep} applies Word2Vec to learn the embeddings of the atomic substructures (such as the graphlets, the WL subtree patterns, and the shortest paths) to derive the kernel matrix.
Anonymous walk embedding (AWE) \citep{ivanov2018anonymous} maps random walks into ``anonymous walks'', views each anonymous walk as a word token, and utilizes Doc2Vec to achieve the graph embeddings to compute the graph similarities to construct the kernel matrix.
Graph2Vec \citep{narayanan2017graph2vec} employs Doc2Vec to learn the embeddings for the WL subtree patterns and to obtain the graph embeddings to train a SVM classifier.


\subsection{Graph neural networks} Recent works have focused on using graph neural networks (GNNs) to perform the graph classification task \citep{scarselli2009graph,li2015gated,niepert2016learning,Gilmer2017,zhang2018end,Ying2018diffpool,verma2018graph,xu2019powerful}. 
In general, GNNs use an \textsc{Aggregation} function which aims to update the vector representation of each node by recursively propagating the vector representations of its neighbors \citep{scarselli2009graph,kipf2017semi,hamilton2017inductive,velickovic2018graph}.
The \textsc{Aggregation} function can be a neural network such as gated recurrent units (GRU) \citep{li2015gated} or multi-layer perceptrons (MLPs) \citep{xu2019powerful}.
Besides, GCN \citep{kipf2017semi}, GraphSAGE \citep{hamilton2017inductive}, and GAT \citep{velickovic2018graph} are also used as the \textsc{Aggregation} functions.
GNNs then utilize a \textsc{ReadOut} pooling function to obtain the graph embeddings, which are fed to multiple fully-connected layers followed by a $\mathsf{softmax}$ layer to predict the graph labels.


Regarding the aggregation of node representations, GCN \citep{kipf2017semi} updates vector representation for a given node $\mathsf{v} \in \mathcal{V}$ from its neighbors, using multiple GCN layers stacked on top of each other to capture $k$-hops neighbors, as:
\begin{equation}
\boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(k+1)} = \mathsf{g}\left(\sum_{\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}a_{\mathsf{v},\mathsf{u}}\textbf{W}^{(k)}\boldsymbol{\mathsf{h}}_{\mathsf{u}}^{(k)}\right) , \forall \mathsf{v} \in \mathcal{V}
\label{equa:gcn2}
\end{equation}
where $k$ is the layer index; $a_{\mathsf{v},\mathsf{u}}$ is an edge constant between nodes $\mathsf{v}$ and $\mathsf{u}$ in the re-normalized adjacency matrix; $\textbf{W}^{(k)}$ is a weight matrix; $\boldsymbol{\mathsf{h}}_{\mathsf{u}}^{(0)}$ is a feature vector of node $\mathsf{u}$; $\mathsf{g}$ is a non-linear activation function; and $\mathcal{N}_\mathsf{v}$ is the set of neighbors of node $\mathsf{v}$.

GraphSAGE \citep{hamilton2017inductive} extends GCN to use a node-wise procedure of uniformly sampling a fixed number of neighbors for each node at each layer as:
\begin{equation}
\boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(k+1)} = \mathsf{g}\left(\textbf{W}^{(k)}\left[\boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(k)};\boldsymbol{\mathsf{h}}_{\mathcal{N}'_\mathsf{v}}^{(k)}\right]\right) , \forall \mathsf{v} \in \mathcal{V}
\label{equa:GraphSAGE1}
\end{equation}
where [;] denotes a vector concatenation, and $\boldsymbol{\mathsf{h}}_{\mathcal{N}'_\mathsf{v}}^{(k)}$ can be obtained using an element-wise max-pooling operation as:
\begin{equation}
\boldsymbol{\mathsf{h}}_{\mathcal{N}'_\mathsf{v}}^{(k)} = \mathsf{max}\left(\left\{\mathsf{g}\left(\textbf{W}_{pool}\boldsymbol{\mathsf{h}}_{\mathsf{u}}^{(k)} + \boldsymbol{\mathsf{b}}\right), \forall \mathsf{u} \in \mathcal{N}'_\mathsf{v}\right\}\right)
\label{equa:GraphSAGE3}
\end{equation}
where $\mathcal{N}'_\mathsf{v}$ is defined as a fixed-size, uniformly sampled from $\mathcal{N}_\mathsf{v}$ of $\mathsf{v}$. Besides, $\mathcal{N}'_\mathsf{v}$ is sampled differently through each layer.

Graph Attention Network (GAT) \citep{velickovic2018graph} extends GCN to compute edge weights following the standard attention technique \citep{bahdanau2014neural} as:
\begin{equation}
\boldsymbol{\mathsf{h}}^{(k+1)}_{\mathsf{v}} = \mathsf{g}\left(\sum_{\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}\tau^{(k)}_{\mathsf{v},\mathsf{u}}\textbf{W}^{(k)}\boldsymbol{\mathsf{h}}^{(k)}_{\mathsf{u}}\right) , \forall \mathsf{v} \in \mathcal{V}
\label{equa:gat1}
\end{equation}
where $\tau^{(k)}_{\mathsf{v},\mathsf{u}}$ is an edge weight between nodes $\mathsf{v}$ and $\mathsf{u}$, which is computed as: 
$\tau^{(k)}_{\mathsf{v},\mathsf{u}} =$
\begin{equation}
\mathsf{softmax}\left(\mathsf{LeakyReLU}\left(\boldsymbol{a}^{(k)\mathsf{T}}\left[\textbf{W}^{(k)}\boldsymbol{\mathsf{h}}^{(k)}_{\mathsf{v}}; \textbf{W}^{(k)}\boldsymbol{\mathsf{h}}^{(k)}_{\mathsf{u}}\right]\right)\right)
\label{equa:gat2}
\end{equation}



\section{U2GNN: Universal Graph Transformer Self-Attention Networks}
\label{sec:ourmodel}


In this section, we present the additional background of graph neural networks and detail our proposed U2GNN.


\subsection{Graph classification} We represent each graph $\mathcal{G} = \left(\mathcal{V}, \mathcal{E}, \{\boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(0)}\}_{\mathsf{v} \in \mathcal{V}}\right)$, where $\mathcal{V}$ is a set of nodes, $\mathcal{E}$ is a set of edges, and $\boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(0)} \in \mathbb{R}^d$ represents the feature vector of node $\mathsf{v} \in \mathcal{V}$. 
Given a set of $M$ graphs $\left\{\mathcal{G}_m\right\}_{m=1}^M$ and their corresponding class labels $\left\{\mathsf{y}_m\right\}_{m=1}^M \subseteq \mathcal{Y}$, the graph classification task is to learn an embedding $\boldsymbol{\mathsf{e}}_{\mathcal{G}_m}$ for each graph $\mathcal{G}_m$ to predict its label $\mathsf{y}_m$.



\begin{figure*}[!ht]
\centering
\includegraphics[width=1\textwidth]{U2GNN_overview.png}
\captionof{figure}{Illustration of our U2GNN.}
\label{fig:U2GNN}
\end{figure*}


\subsection{Graph neural networks} 
In general, GNNs aim to update the vector representation of each node by recursively aggregating and transforming the vector representations of its neighbors \citep{kipf2017semi,hamilton2017inductive,velickovic2018graph}. 
After that, GNNs use a \textsc{ReadOut} pooling function to obtain the vector representations of the entire graphs \citep{Gilmer2017,zhang2018end,Ying2018diffpool,verma2018graph,xu2019powerful}.
Mathematically, given a graph $\mathcal{G}$, we can formalize GNNs as follows:
\begin{equation}
\boldsymbol{\mathsf{h}}_\mathsf{v}^{(k+1)} = \textsc{Aggregation}\left(\left\{\boldsymbol{\mathsf{h}}^{(k)}_\mathsf{u}\right\}_{\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}\right)
\end{equation}
\begin{equation}
\boldsymbol{\mathsf{e}}_{\mathcal{G}} = \textsc{ReadOut}\left(\left\{\left\{\boldsymbol{\mathsf{h}}_\mathsf{v}^{(k)}\right\}_{k=0}^K\right\}_{\mathsf{v} \in \mathcal{V}}\right)
\end{equation}
where $\boldsymbol{\mathsf{h}}_\mathsf{v}^{(k)}$ is the vector representation of node $\mathsf{v}$ at the $k$-th iteration/layer, $\mathcal{N}_\mathsf{v}$ is the set of neighbors of $\mathsf{v}$.

Many methods have been proposed to construct the \textsc{Aggregation} functions, e.g., GCN \citep{kipf2017semi}, GraphSAGE \citep{hamilton2017inductive}, and GAT \citep{velickovic2018graph}. Recently, Graph Isomorphism Network (GIN-0) \citep{xu2019powerful} uses a more powerful \textsc{Aggregation} function based on a multi-layer perceptron (MLP) network of two fully-connected layers as:
\begin{equation}
\boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(k+1)} = \mathsf{MLP^{(k)}}\left(\sum_{\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}\boldsymbol{\mathsf{h}}_{\mathsf{u}}^{(k)}\right) , \forall \mathsf{v} \in \mathcal{V}
\label{equa:gin}
\end{equation}
Following \citep{xu2019powerful}, we employ a concatenation over the vector representations of node $\mathsf{v}$ at the different layers to construct a vector representation $\boldsymbol{\mathsf{e}}_\mathsf{v}$ for each node $\mathsf{v}$ as:
\begin{equation}
\boldsymbol{\mathsf{e}}_\mathsf{v} = \left[\boldsymbol{\mathsf{h}}_\mathsf{v}^{(1)}; \boldsymbol{\mathsf{h}}_\mathsf{v}^{(2)}; ...; \boldsymbol{\mathsf{h}}_\mathsf{v}^{(K)}\right] , \forall \mathsf{v} \in \mathcal{V}
\label{equa:noderepresentation}
\end{equation}
where $K$ is the index of the last layer.
The graph-level $\textsc{ReadOut}$ function can be a simple sum pooling \citep{xu2019powerful} or a complicated pooling such as hierarchical pooling \citep{cangea2018towards}, and differentiable pooling \citep{Ying2018diffpool}. 
As the sum pooling produces competitive results \citep{xu2019powerful}, we use the sum pooling to obtain the embedding $\boldsymbol{\mathsf{e}}_{\mathcal{G}}$ of the entire graph $\mathcal{G}$ as:
\begin{equation}
\boldsymbol{\mathsf{e}}_{\mathcal{G}} = \sum_{\mathsf{v} \in \mathcal{V}}\boldsymbol{\mathsf{e}}_\mathsf{v} =\sum_{\mathsf{v} \in \mathcal{V}}\left[\boldsymbol{\mathsf{h}}_\mathsf{v}^{(1)}; \boldsymbol{\mathsf{h}}_\mathsf{v}^{(2)}; ...; \boldsymbol{\mathsf{h}}_\mathsf{v}^{(K)}\right]
\label{equa:graphemb}
\end{equation}

After that, we can follow \citep{xu2019powerful} to feed the graph embeddings $\boldsymbol{\mathsf{e}}_{\mathcal{G}}$ to a single fully-connected layer followed by a $\mathsf{softmax}$ layer to predict the graph labels.



\subsection{The proposed U2GNN} 

The transformer self-attention network \citep{vaswani2017attention,dehghani2018universal} has widely applied as a novelty in research domains such as computer vision and NLP.
Similarity, we also consider the successful use of this recent advanced technique to a new domain, i.e., graph neural networks (GNNs), as a novel application.
Moreover, as also discussed in \citep{xu2019powerful}, constructing an powerful \textsc{Aggregation} mechanism is essential for GNNs. To this end, we induce an advanced \textsc{Aggregation} function, using the universal transformer network \citep{dehghani2018universal} consisting of a self-attention mechanism \citep{vaswani2017attention} followed by a recurrent transition (\textsc{Trans}) with adding residual connections \citep{he2016deep} and layer normalization (\textsc{LNorm}) \citep{ba2016layer}, as illustrated in Figure \ref{fig:U2GNN}.

The residual connections \citep{he2016deep} are used to add useful information learned in the lower layers to the higher layers, and more importantly, to allow gradients to directly pass through the layers to avoid vanishing gradient or exploding gradient problems.
The layer normalization (\textsc{LNorm}) \citep{ba2016layer} is used to normalize the inputs across the feature dimensions to stabilize the network to enable smoother gradients and faster training. 
The residual connections and the layer normalization are commonly used in many architectures and thus are omitted in the paper for simplicity.


Formally, given an input graph $\mathcal{G}$, we uniformly sample a set $\mathcal{N}_\mathsf{v}$ of neighbors for each $\mathsf{v} \in \mathcal{V}$ and then input $\mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}$ to the U2GNN learning process. 
Note that we sample a different $\mathcal{N}_\mathsf{v}$ for node $\mathsf{v}$ at each training batch.
We also construct multiple layers stacked on top of each other in our U2GNN.
Regarding the $k$-th layer, given a node $\mathsf{v} \in \mathcal{V}$, at each step $t$, we induce a transformer self-attention-based function to aggregate the vector representations for all nodes $\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}$ as:
\begin{equation}
\boldsymbol{\mathsf{h}}^{(k)}_{t,\mathsf{u}} = \textsc{Transformer-Aggregation}\left(\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}}\right)
\end{equation}
In particular,
\begin{equation}
\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}} = \textsc{LNorm}\left(\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}} + \textsc{ATT}\left(\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}}\right)\right)
\end{equation}
then,
\begin{equation}
\boldsymbol{\mathsf{h}}^{(k)}_{t,\mathsf{u}} = \textsc{LNorm}\left(\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}} + \textsc{Trans}\left(\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}}\right)\right) \label{equa:usa1}
\end{equation}
where $\boldsymbol{\mathsf{h}}^{(k)}_{t,\mathsf{u}} \in \mathbb{R}^d$; $\textsc{Trans}(.)$ and  $\textsc{ATT}(.)$  denote a MLP network (i.e., two fully-connected layers) and a self-attention network respectively: 
\begin{equation} 
\textsc{Trans}\left(\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}}\right) = \textbf{W}^{(k)}_2\mathsf{ReLU}\left(\textbf{W}^{(k)}_1\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}} + \boldsymbol{\mathsf{b}}^{(k)}_1\right) + \boldsymbol{\mathsf{b}}^{(k)}_2 
\label{equa:transition}
\end{equation}
where $\textbf{W}^{(k)}_1  \in \mathbb{R}^{s\times d}$ and $\textbf{W}^{(k)}_2  \in \mathbb{R}^{d\times s}$ are weight matrices, and $\boldsymbol{\mathsf{b}}^{(k)}_1$ and $\boldsymbol{\mathsf{b}}^{(k)}_2$ are bias parameters, and: 
\begin{equation} 
\textsc{ATT}\left(\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}}\right) = \sum_{\mathsf{u}' \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}\alpha^{(k)}_{\mathsf{u},\mathsf{u}'}\left(\boldsymbol{V}^{(k)}\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}'}\right) 
\label{equa:usatt1}
\end{equation}
where $\boldsymbol{V}^{(k)} \in \mathbb{R}^{d\times d}$ is a value-projection weight matrix;  $\alpha_{\mathsf{u},\mathsf{u}'}$ is an attention weight, which is computed using the $\mathsf{softmax}$ function over scaled dot products between nodes $\mathsf{u}$ and $\mathsf{u}'$:
\begin{equation}
\alpha^{(k)}_{\mathsf{u},\mathsf{u}'} = \mathsf{softmax}\left(\frac{\left(\boldsymbol{Q}^{(k)}\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}}\right)\cdot\left(\boldsymbol{K}^{(k)}\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}'}\right)}{\sqrt{d}}\right) 
\end{equation}
where $\boldsymbol{Q}^{(k)} \in \mathbb{R}^{d\times d}$ and $\boldsymbol{K}^{(k)} \in \mathbb{R}^{d\times d}$ are query-projection and key-projection matrices, respectively.


After $T$ steps, we feed $\boldsymbol{\mathsf{h}}^{(k)}_{T,\mathsf{v}} \in \mathbb{R}^d$ to the next $(k+1)$-th layer as:
\begin{equation}
\boldsymbol{\mathsf{h}}^{(k+1)}_{0,\mathsf{v}} = \boldsymbol{\mathsf{h}}^{(k)}_{T,\mathsf{v}} , \forall \mathsf{v} \in \mathcal{V}
\label{equa:nextlayerequa}
\end{equation}
Note that $\boldsymbol{\mathsf{h}}^{(0)}_{0,\mathsf{v}} = \boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(0)} \in \mathbb{R}^d$ is the feature vector of node $\mathsf{v}$.



We apply the vector concatenation across the layers to obtain the vector representations $\boldsymbol{\mathsf{e}}_\mathsf{v}$ of nodes $\mathsf{v}$ following Equation \ref{equa:noderepresentation} as:
\begin{equation}
\boldsymbol{\mathsf{e}}_\mathsf{v} = \left[\boldsymbol{\mathsf{h}}^{(1)}_{0,\mathsf{v}}; \boldsymbol{\mathsf{h}}^{(2)}_{0,\mathsf{v}}; ...; \boldsymbol{\mathsf{h}}^{(K)}_{0,\mathsf{v}}\right] , \forall \mathsf{v} \in \mathcal{V}
\label{equa:vectorev}
\end{equation}
where $K$ is the number of layers.
We use $\boldsymbol{\mathsf{e}}_{\mathsf{v}}$ as the final embedding of node $\mathsf{v}\in\mathcal{V}$ and then sum all the final embeddings of nodes in $\mathcal{G}$ to get the final embedding $\boldsymbol{\mathsf{e}}_{\mathcal{G}}$ of the entire graph $\mathcal{G}$.
We feed $\boldsymbol{\mathsf{e}}_{\mathcal{G}}$ to a single fully-connected layer followed by a $\mathsf{softmax}$ layer to predict the graph label as:
\begin{equation}
\boldsymbol{\mathsf{\hat{y}}}_{\mathcal{G}} = \mathsf{softmax}\left(\textbf{W}\boldsymbol{\mathsf{e}}_{\mathcal{G}} + \textbf{b}\right)
\label{equa:supervisedU2GNNloss}
\end{equation}
Finally, we learn the model parameters by minimizing the cross-entropy loss function. To sum up, we briefly present the learning process of our proposed U2GNN in Algorithm \ref{alg:U2GNNsup}. 

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoVlined

\textbf{Input}: $\mathcal{G} = \left(\mathcal{V}, \mathcal{E}, \{\boldsymbol{\mathsf{h}}_{\mathsf{v}}^{(0)}\}_{\mathsf{v} \in \mathcal{V}}\right)$ with its label $\mathsf{y}$

\For{$k = 0, 1, ..., K-1$}{
    
    \For{$\mathsf{v} \in \mathcal{V}$}{

        \textsc{Sample} $\mathcal{N}_\mathsf{v}$ for $\mathsf{v}$
        
        \For{$t = 1, 2, ..., T$}{
            
            $\forall \mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}$
        
            $\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}} \leftarrow \textsc{LNorm}\left(\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}} + \textsc{ATT}\left(\boldsymbol{\mathsf{h}}^{(k)}_{t-1,\mathsf{u}}\right)\right)$
            
            $\boldsymbol{\mathsf{h}}^{(k)}_{t,\mathsf{u}} \leftarrow \textsc{LNorm}\left(\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}} + \textsc{Trans}\left(\boldsymbol{\mathsf{x}}^{(k)}_{t,\mathsf{u}}\right)\right)$
        }
        
        $\boldsymbol{\mathsf{h}}^{(k+1)}_{0,\mathsf{v}} \leftarrow \boldsymbol{\mathsf{h}}^{(k)}_{T,\mathsf{v}} \in \mathbb{R}^d$
        
    }
}

$\boldsymbol{\mathsf{e}}_\mathsf{v} \leftarrow \left[\boldsymbol{\mathsf{h}}^{(1)}_{0,\mathsf{v}}; \boldsymbol{\mathsf{h}}^{(2)}_{0,\mathsf{v}}; ...; \boldsymbol{\mathsf{h}}^{(K)}_{0,\mathsf{v}}\right] , \forall \mathsf{v} \in \mathcal{V}$ (w.r.t Equation \ref{equa:vectorev})

$\boldsymbol{\mathsf{e}}_{\mathcal{G}} \leftarrow \sum_{\mathsf{v} \in \mathcal{V}}\boldsymbol{\mathsf{e}}_{\mathsf{v}}$

$\mathsf{y} \leftarrow \mathsf{softmax}\left(\textbf{W}\boldsymbol{\mathsf{e}}_{\mathcal{G}} + \textbf{b}\right)$

\caption{The U2GNN learning process.}
\label{alg:U2GNNsup}
\end{algorithm}


\subsection{Discussion} 
\label{subsec:discussion}



We discuss some findings in our proposed U2GNN as follows:

\begin{itemize}


\item If we set $T$ to 1, $\alpha^{(k)}_{\mathsf{u},\mathsf{u}'}$ to 1, $\boldsymbol{V}^{(k)}$ to the identity matrix in Equation \ref{equa:usatt1}, and do not use both the residual connections and the layer normalization, we simplify our U2GNN aggregation function (from Equations \ref{equa:nextlayerequa} and \ref{equa:usa1}) as:
\begin{eqnarray} 
\boldsymbol{\mathsf{h}}^{(k+1)}_{1,\mathsf{v}} &=&  \textsc{Trans}\left( \textsc{ATT}\left(\boldsymbol{\mathsf{h}}^{(k+1)}_{0,\mathsf{v}}\right)\right) \nonumber \\
&=& \textsc{Trans}\left(\sum_{\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}\boldsymbol{\mathsf{h}}^{(k+1)}_{0,\mathsf{u}}\right) \nonumber \\ 
&=& \textsc{Trans}\left(\sum_{\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}\boldsymbol{\mathsf{h}}^{(k)}_{1,\mathsf{u}}\right)
\label{equa:simplifiedU2GNN}
\end{eqnarray}
where $\textsc{Trans}(.)$ denotes the MLP network of two fully-connected layers (as defined in Equation \ref{equa:transition}). Thus, this implies that our U2GNN can be simplified (w.r.t Equation \ref{equa:simplifiedU2GNN}) to be equivalent to Graph Isomorphism Network (GIN-0) \citep{xu2019powerful} (w.r.t Equation \ref{equa:gin}) -- one of the recent state-of-the-art GNNs.
Experimental results presented in Section \ref{subsec:expresults} show that U2GNN outperforms GIN-0 on benchmark datasets for the graph classification task.


\item We probably could construct a complex architecture using a complicated graph-level pooling (such as hierarchical pooling \citep{cangea2018towards}) followed by multiple fully-connected layers \citep{Chen2019ArePG,maron2019invariant} to predict the graph labels.
However, we refrained from doing that, as our key purpose is to introduce a single, unified and effective model that can work well and produce competitive performances on the benchmark datasets. 
Therefore, using the sum pooling followed by a single fully-connected layer is reasonable for our U2GNN.

\item As established empirically, our results shown in Section \ref{subsec:expresults} imply that the U2GNN self-attention-based aggregation function is a powerful computation process compared to other existing functions.

\end{itemize}





\section{Experimental setup}



We use seven well-known datasets consisting of three social network datasets (COLLAB, IMDB-B, and IMDB-M)
and four bioinformatics datasets (DD, MUTAG, PROTEINS, and PTC).
The social network datasets do not have available node features; thus, we follow \citep{niepert2016learning,zhang2018end} to use node degrees as features.
Table \ref{tab:datasets} reports the statistics of these datasets.

\begin{table}[!ht]
\caption{Statistics of the experimental benchmark datasets. 
\textbf{\#G} denotes the numbers of graphs. \textbf{\#Cls} denotes the number of class labels. 
\textbf{Avg\#N} denotes the average number of nodes per graph. \textbf{Avg\#E} denotes the average number of neighbors per node. \textbf{$d$} is the dimension of feature vectors. Note that \textbf{$d$} is also equal to the node embedding size at each U2GNN layer.}
\centering
\resizebox{8.5cm}{!}{
\def\arraystretch{1.3}
\begin{tabular}{l|lclll}
\hline
\textbf{Dataset} & \textbf{\#G} & \textbf{\#Cls} & \textbf{Avg\#N} & \textbf{Avg\#E} & \textbf{$d$} \\ 
\hline 
COLLAB & 5,000 & 3 & 74.5 & 65.9 & --\\ 
IMDB-M & 1,500 & 3 & 13.0 & 10.1 & --\\
IMDB-B & 1,000 & 2 & 19.8 & 9.8 & --\\ 
\hline
DD & 1,178 & 2 & 284.3 & 5.0 & 82\\ 
PROTEINS & 1,113 & 2 & 39.1 & 3.7 & 3\\ 
PTC & 344 & 2 & 25.6 & 2.0 & 19\\ 
MUTAG & 188 & 2 & 17.9 & 2.2 & 7\\ 
\hline
\end{tabular}
}
\label{tab:datasets}
\end{table}



\begin{table*}[!ht]
\caption{Graph classification results (\% accuracy). The best scores are in {bold}.}
\centering
\resizebox{17.5cm}{!}{
\def\arraystretch{1.4}
\begin{tabular}{l|c|c|c|c|c|c|c}
\hline
{\bf Model} &  \textbf{COLLAB} &  \textbf{IMDB-B} &  \textbf{IMDB-M} & \textbf{DD} & \textbf{PROTEINS} & \textbf{MUTAG} & \textbf{PTC}\\
\hline
GK \citeyearpar{shervashidze2009efficient} & 72.84 $\pm$ 0.28 & 65.87 $\pm$ 0.98 & 43.89 $\pm$ 0.38 & 78.45 $\pm$ 0.26 & 71.67 $\pm$ 0.55 & 81.58 $\pm$ 2.11 & 57.26 $\pm$ 1.41\\
WL \citeyearpar{shervashidze2011weisfeiler} & 79.02 $\pm$ 1.77 & 73.40 $\pm$ 4.63 & 49.33 $\pm$ 4.75 & 79.78 $\pm$ 0.36 & 74.68 $\pm$ 0.49 & 82.05 $\pm$ 0.36 & 57.97 $\pm$ 0.49 \\
PSCN \citeyearpar{niepert2016learning} & 72.60 $\pm$ 2.15 & 71.00 $\pm$ 2.29 & 45.23 $\pm$ 2.84 & 77.12 $\pm$ 2.41 & 75.89 $\pm$ 2.76 & \textbf{92.63 $\pm$ 4.21} & 62.29 $\pm$ 5.68\\
GCN \citeyearpar{kipf2017semi} & \textbf{81.72 $\pm$ 1.64} & 73.30 $\pm$ 5.29 & 51.20 $\pm$ 5.13 & 79.12 $\pm$ 3.07 & 75.65 $\pm$ 3.24 & 87.20 $\pm$ 5.11 & --\\
GFN \citeyearpar{Chen2019ArePG} & 81.50 $\pm$ 2.42 & 73.00 $\pm$ 4.35 & 51.80 $\pm$ 5.16 & 78.78 $\pm$ 3.49 & 76.46 $\pm$ 4.06 & {90.84 $\pm$ 7.22} & -- \\
GraphSAGE \citeyearpar{hamilton2017inductive} & 79.70 $\pm$ 1.70 & 72.40 $\pm$ 3.60 & 49.90 $\pm$ 5.00 & 65.80 $\pm$ 4.90 & 65.90 $\pm$ 2.70 & 79.80 $\pm$ 13.9 & -- \\
GAT \citeyearpar{velickovic2018graph} & 75.80 $\pm$ 1.60 & 70.50 $\pm$ 2.30 & 47.80 $\pm$ 3.10 & -- & 74.70 $\pm$ 2.20 & 89.40 $\pm$ 6.10 & 66.70 $\pm$ 5.10 \\
DGCNN \citeyearpar{zhang2018end} & 73.76 $\pm$ 0.49 & 70.03 $\pm$ 0.86 & 47.83 $\pm$ 0.85 & 79.37 $\pm$ 0.94 & 75.54 $\pm$ 0.94 & 85.83 $\pm$ 1.66 & 58.59 $\pm$ 2.47\\
SAGPool \citeyearpar{Lee2019SelfAttentionGP} & -- & -- & -- & 76.45 $\pm$ 0.97 & 71.86 $\pm$ 0.97 & -- & -- \\
PPGN \citeyearpar{maron2019provably} & 81.38 $\pm$ 1.42 & 73.00 $\pm$ 5.77 & 50.46 $\pm$ 3.59 & -- & 77.20 $\pm$ 4.73 & 90.55 $\pm$ 8.70 &  66.17 $\pm$ 6.54\\
CapsGNN \citeyearpar{xinyi2019capsule} & 79.62 $\pm$ 0.91 & 73.10 $\pm$ 4.83 & 50.27 $\pm$ 2.65 & 75.38 $\pm$ 4.17 & 76.28 $\pm$ 3.63 & 86.67 $\pm$ 6.88 & -- \\
DSGC \citeyearpar{seo2019discriminative} & 79.20 $\pm$ 1.60 & 73.20 $\pm$ 4.90 & 48.50 $\pm$ 4.80 & 77.40 $\pm$ 6.40 & 74.20 $\pm$ 3.80 & 86.70 $\pm$ 7.60 & --\\
GIN-0 \citeyearpar{xu2019powerful} & 80.20 $\pm$ 1.90  & 75.10 $\pm$ 5.10 & 52.30 $\pm$ 2.80 & -- & 76.20 $\pm$ 2.80 & {89.40 $\pm$ 5.60} & 64.60 $\pm$ 7.00\\
GCAPS \citeyearpar{verma2018graph} & 77.71 $\pm$ 2.51 & 71.69 $\pm$ 3.40 & 48.50 $\pm$ 4.10 & 77.62 $\pm$ 4.99 & 76.40 $\pm$ 4.17 & -- & 66.01 $\pm$ 5.91\\
IEGN \citeyearpar{maron2019invariant} & 77.92 $\pm$ 1.70 & 71.27 $\pm$ 4.50 & 48.55 $\pm$ 3.90 & -- & 75.19 $\pm$ 4.30 & 84.61 $\pm$ 10.0 & 59.47 $\pm$ 7.30 \\
\hline
\textbf{U2GNN} & 77.84 $\pm$ 1.48 & \textbf{77.04 $\pm$ 3.45} & \textbf{53.60 $\pm$ 3.53} & \textbf{80.23 $\pm$ 1.48} & \textbf{78.53 $\pm$ 4.07} & 89.97 $\pm$ 3.65 & \textbf{69.63 $\pm$ 3.60}\\
\hline
\end{tabular}
}
\label{tab:expresult_sup}
\end{table*}

\begin{itemize}

\item \textbf{Social networks datasets:} COLLAB is a scientific dataset, where each graph represents a collaboration network of a corresponding researcher with other researchers from each of 3 physics fields; each graph is labeled to a physics field that the researcher belongs to.
IMDB-B and IMDB-M are movie collaboration datasets, where each graph is derived from actor/actress and genre information of different movies on IMDB; nodes correspond to actors/actresses, and each edge represents a co-appearance of two actors/actresses in the same movie; each graph is assigned to a genre.


\item \textbf{Bioinformatics datasets:} DD \citep{dobson2003distinguishingDD} is a collection of 1,178 protein network structures with 82 discrete node labels, where each graph is classified into enzyme or non-enzyme class.
MUTAG \citep{debnath1991structureMUTAG} is a collection of 188 nitro compound networks with 7 discrete node labels, where classes indicate a mutagenic effect on a bacterium.
PROTEINS comprises 1,113 graphs obtained from \citep{borgwardt2005PROTEINENZYMES} to present secondary structure elements (SSEs). 
PTC \citep{toivonen2003statisticalPTC} consists of 344 chemical compound networks with 19 discrete node labels where classes show carcinogenicity for male and female rats.

\end{itemize}



\subsection{Training protocol}
\label{subsec:training}


We vary the number $K$ of U2GNN layers in \{1, 2, 3\}, the number of steps $T$ in \{1, 2, 3, 4\}, the number of neighbors ($|\mathcal{N}_\mathsf{v}|=N$) sampled for each node in \{4, 8, 16\}, and the dimension $s$ of $\textsc{Trans}(.)$ (in Equation \ref{equa:transition}) in \{128, 256, 512, 1024\} (in Equation \ref{equa:transition}).
We set the batch size to 4.
We apply the Adam optimizer \citep{kingma2014adam} to train our U2GNN and select the Adam initial learning rate  $lr \in \left\{5e^{-5}, 1e^{-4}, 5e^{-4}, 1e^{-3}\right\}$.
We run up to 50 epochs to evaluate our U2GNN. 



\subsection{Evaluation protocol}
\label{subsec:eval}

{We follow \citep{xu2019powerful,xinyi2019capsule,maron2019provably,seo2019discriminative,Chen2019ArePG} to use the same data splits and the same 10-fold cross-validation scheme to calculate the classification performance for a fair comparison}.


We compare our U2GNN with up-to-date strong baselines as follows:
\begin{itemize}
\item \textbf{Unsupervised approaches:} 
Graphlet Kernel (GK) \citep{shervashidze2009efficient} and  Weisfeiler-Lehman kernel (WL) \citep{shervashidze2011weisfeiler}.

\item \textbf{Supervised approaches:} 
PATCHY-SAN (PSCN) \citep{niepert2016learning}, Graph Convolutional Network (GCN) \citep{kipf2017semi}, GraphSAGE \citep{hamilton2017inductive}, Graph Attention Network (GAT) \citep{velickovic2018graph}, Deep Graph CNN (DGCNN) \citep{zhang2018end}, Graph Capsule Convolution Neural Network (GCAPS) \citep{verma2018graph}, Capsule Graph Neural Network (CapsGNN) \citep{xinyi2019capsule}, Self-Attention Graph Pooling (SAGPool) \citep{Lee2019SelfAttentionGP}, Graph Isomorphism Network (GIN-0) \citep{xu2019powerful}, Graph Feature Network (GFN) \citep{Chen2019ArePG}, Invariant-Equivariant Graph Network (IEGN) \citep{maron2019invariant}, Provably Powerful Graph Network (PPGN) \citep{maron2019provably}, and Discriminative Structural Graph Classification (DSGC) \citep{seo2019discriminative}.
\end{itemize}

We report the baselines taken from the original papers or published in \citep{ivanov2018anonymous,verma2018graph,xinyi2019capsule,FAN2020107084,Chen2019ArePG,seo2019discriminative}.


\begin{figure*}[ht]
\centering
    \includegraphics[width=0.485\textwidth]{GIN_PTC_node.pdf}
    \includegraphics[width=0.485\textwidth]{U2GNN_PTC_node.pdf}
\caption[short]{A t-SNE visualization of the node embeddings learned by GIN-0 and our U2GNN on the PTC dataset.}
\label{fig:visualembeddings}
\end{figure*}

\section{Experimental results} 
\label{subsec:expresults}

Table \ref{tab:expresult_sup} presents the experimental results of U2GNN and other strong baseline models for the benchmark datasets,
and Figure \ref{fig:resultsof10folds} shows the classification accuracies of our proposed U2GNN across 10 folds for each dataset.
On the social network datasets, our U2GNN produces new state-of-the-art performances on IMDB-B and IMDB-M and gains a competitive score on COLLAB.
Especially, U2GNN obtains 4+\% absolute higher accuracies than all the supervised baseline models on IMDB-B and IMDB-M.
Regarding COLLAB, we found the best results of GCN in \citep{Chen2019ArePG}, where, after obtaining the graph embeddings, \citet{Chen2019ArePG} utilized two fully-connected layers to predict the graph labels for GCN and GFN. 
This is different from our U2GNN where we used a single fully-connected layer as we discussed in Section \ref{subsec:discussion}.


On the bioinformatics datasets, our U2GNN obtains the highest accuracies on DD, PROTEINS, and PTC. 
Moreover, U2GNN achieves a competitive accuracy compared with those of the baseline models on MUTAG. 
Additionally, there are no significant differences between our U2GNN and the baselines on MUTAG as this dataset only consists of 188 graphs, which explains the high variance in the results.

\begin{figure}[ht]
\centering
    \includegraphics[width=0.24\textwidth]{COLLAB_10.pdf}
\includegraphics[width=0.24\textwidth]{DD_10.pdf}\\
    \includegraphics[width=0.24\textwidth]{PROTEINS_10.pdf}
    \includegraphics[width=0.24\textwidth]{MUTAG_10.pdf}
\caption[short]{Classification accuracies across 10 folds for each dataset.}
\label{fig:resultsof10folds}
\end{figure}


\begin{figure}[!ht]
\centering
    \includegraphics[width=0.24\textwidth]{PROTEINS_T_sup.pdf}
    \includegraphics[width=0.24\textwidth]{PROTEINS_N_sup.pdf}
\caption[short]{Effects of the number of timesteps ($T$), and the number of neighbors sampled for each node ($N=|\mathcal{N}_\mathsf{v}|$).}
\label{fig:effectTN_PROTEINS}
\end{figure}

\textbf{Hyper-parameter analysis.} We investigate the effects of the number of timesteps ($T$) and the number of neighbors sampled for each node ($N=|\mathcal{N}_\mathsf{v}|$) in Figure \ref{fig:effectTN_PROTEINS}.
In general, we find that higher $T$ can help on most of the datasets as we may use more steps $T$ to encode the graph structures better.
Furthermore, the social network datasets are denser than the bioinformatics ones; hence we should use more sampled neighbors (i.e., using higher $N$) on the social network datasets rather than on the bioinformatics ones.


\textbf{Visualization.} To qualitatively demonstrate the effectiveness of our U2GNN, we use t-SNE \citep{maaten2008visualizing} to visualize the node embeddings learned by GIN-0 and our U2GNN on \textsc{PTC} where the node labels are available.
Compared to GIN-0, Figure \ref{fig:visualembeddings} shows that our U2GNN produces a higher quality of learned node embeddings wherein the nodes are well-clustered according to the node labels.

In general, the superior performance of our method over the up-to-date baselines (especially, \mbox{GIN-0}) indicates that the U2GNN self-attention-based aggregation function is an advanced computation process to improve the classification performance of GNNs.


\section{Conclusion}
\label{sec:conclusion}

We introduce U2GNN, an advantageous graph neural network model.
U2GNN induces a powerful aggregation function leveraging the transformer self-attention network to improve the graph classification performance. 
We evaluate our U2GNN using the same data splits and the same 10-fold cross-validation scheme on well-known benchmark datasets. 
Experimental results demonstrate that U2GNN outperforms up-to-date models and produces state-of-the-art accuracies on these datasets.
\footnote{We propose a new unsupervised learning in Appendix \ref{apd:a} to train GNNs and hope that future GNN works can consider the unsupervised learning beside the supervised one.}






\bibliographystyle{plain}
\balance
\bibliography{references}

\appendix

\section{Unsupervised Graph Neural Networks}\label{apd:a}

The unsupervised learning is essential in both industry and academic applications, where expanding unsupervised GNN models is more suitable due to the limited availability of class labels.
Therefore, we introduce a new unsupervised learning to train GNNs for the graph classification task.

\begin{table*}[!ht]
\caption{Graph classification results (\% accuracy) in the unsupervised learning. The best scores are in {bold}. uGCN denotes our unsupervised GCN. Note that we do not make any direct comparison between the \textit{unsupervised} approaches and the \textit{supervised} ones because of the difference in the training data.}
\centering
\resizebox{17.5cm}{!}{
\def\arraystretch{1.4}
\begin{tabular}{l|c|c|c|c|c|c|c}
\hline
\bf Model &  \textbf{COLLAB} &  \textbf{IMDB-B} &  \textbf{IMDB-M} & \textbf{DD} & \textbf{PROTEINS} & \textbf{MUTAG} & \textbf{PTC}\\
\hline
DGK \citeyearpar{yanardag2015deep} & 73.09 $\pm$ 0.25 & 66.96 $\pm$ 0.56 & 44.55 $\pm$ 0.52 & 73.50 $\pm$ 1.01 & 75.68 $\pm$ 0.54 & 87.44 $\pm$ 2.72 & 60.08 $\pm$ 2.55\\
AWE \citeyearpar{ivanov2018anonymous} & 73.93 $\pm$ 1.94 & 74.45 $\pm$ 5.83 & 51.54 $\pm$ 3.61 & 71.51 $\pm$ 4.02 & -- & {87.87 $\pm$ 9.76} & --\\
\hline
uGCN & 93.28 $\pm$ 0.99 & 94.50 $\pm$ 2.79 & {81.66 $\pm$ 3.16} & 94.31 $\pm$ 1.71 & \textbf{89.09 $\pm$ 3.25} & \textbf{95.36 $\pm$ 2.64} & \textbf{92.67 $\pm$ 4.60}\\
\textbf{U2GNN} & \textbf{95.62 $\pm$ 0.92} & \textbf{96.41 $\pm$ 1.94} & \textbf{89.20 $\pm$ 2.52} & \textbf{95.67 $\pm$ 1.89} & {80.01 $\pm$ 3.21} & 88.47 $\pm$ 7.13 & {91.81 $\pm$ 6.61}\\
\hline
\end{tabular}
}
\label{tab:expresult_unsup}
\end{table*}

\subsection{Learning process}


Most of the recent approaches have focused on the supervised learning where they use the graph labels during the training process \citep{xinyi2019capsule,xu2019powerful,Chen2019ArePG,maron2019invariant,seo2019discriminative}.
In a situation where {no graph labels are available during training}, some works (such as DGK \citep{yanardag2015deep}, Graph2Vec \citep{narayanan2017graph2vec}, and AWE \citep{ivanov2018anonymous}) have considered the unsupervised learning, where \textit{they can have access to all nodes from the entire dataset (i.e., additionally using all nodes in the test set during training)}.
But they produce lower classification accuracies compared to the supervised approaches.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoVlined
\textbf{Input}: $\mathcal{G} = \left(\mathcal{V}, \mathcal{E}, \{\boldsymbol{\mathsf{h}}_\mathsf{v}\}_{\mathsf{v} \in \mathcal{V}}\right)$

\For{$k = 0, ..., K-1$}{
    \For{$\mathsf{v} \in \mathcal{V}$}{
    
        $\boldsymbol{\mathsf{h}}_\mathsf{v}^{(k+1)} = \textsc{Aggregation}\left(\left\{\boldsymbol{\mathsf{h}}^{(k)}_\mathsf{u}\right\}_{\mathsf{u} \in \mathcal{N}_\mathsf{v}\cup\left\{\mathsf{v}\right\}}\right)$
    }
}

$\boldsymbol{\mathsf{e}}_\mathsf{v} \leftarrow \left[\boldsymbol{\mathsf{h}}_\mathsf{v}^{(1)}; \boldsymbol{\mathsf{h}}_\mathsf{v}^{(2)}; ...; \boldsymbol{\mathsf{h}}_\mathsf{v}^{(K)}\right] , \forall \mathsf{v} \in \mathcal{V}$ (with respect to Equation \ref{equa:noderepresentation})

$\boldsymbol{\mathsf{o}}_{\mathsf{v}} \leftarrow \boldsymbol{\mathsf{e}}_\mathsf{v}$ (with respect to Equation \ref{equa:unsupervisedU2GNNloss})

$\boldsymbol{\mathsf{e}}_{\mathcal{G}} \leftarrow \sum_{\mathsf{v} \in \mathcal{V}}\boldsymbol{\mathsf{o}}_\mathsf{v}$

\caption{The unsupervised learning.}
\label{alg:U2GNNunsup}
\end{algorithm}

To this end, we propose a new unsupervised learning to train GNNs for the graph classification task.
We can see $\boldsymbol{\mathsf{e}}_\mathsf{v}$ in Equation \ref{equa:noderepresentation} as a vector representation encoded for the substructure around node $\mathsf{v}$.
The goal of our unsupervised learning is to guide GNNs to recognize and distinguish the substructures within each graph, leading to improve the classification accuracies of unsupervised models.
To achieve this goal, we consider a final embedding $\boldsymbol{\mathsf{o}}_{\mathsf{v}}$ for each node $\mathsf{v}$, and make the similarity between $\boldsymbol{\mathsf{e}}_\mathsf{v}$ and $\boldsymbol{\mathsf{o}}_{\mathsf{v}}$ higher than that between $\boldsymbol{\mathsf{e}}_\mathsf{v}$ and the final embeddings of the other nodes, by minimizing the sampled $\mathsf{softmax}$ loss function \citep{Jean2015} applied to node $\mathsf{v}$ as:
\begin{equation}
\mathcal{L}_{\mathsf{U2GNN}}\left(\mathsf{v}\right) = -\log \frac{\exp(\boldsymbol{\mathsf{o}}_\mathsf{v} \cdot \boldsymbol{\mathsf{e}}_{\mathsf{v}})}{\sum_{\mathsf{v'} \in \mathcal{V'}} \exp(\boldsymbol{\mathsf{o}}_\mathsf{v'} \cdot \boldsymbol{\mathsf{e}}_{\mathsf{v}})} 
\label{equa:unsupervisedU2GNNloss}
\end{equation}
where $\mathcal{V'}$ is a subset sampled from $\left\{\cup\mathcal{V}_m\right\}_{m=1}^M$. 
Node embeddings $\boldsymbol{\mathsf{o}}_{\mathsf{v}}$ are learned implicitly as model parameters.
After that, we sum all the final embeddings $\boldsymbol{\mathsf{o}}_{\mathsf{v}}$ of nodes $\mathsf{v}$ in $\mathcal{G}$ to obtain the graph embedding $\boldsymbol{\mathsf{e}}_{\mathcal{G}}$. 
We then use the logistic regression classifier \citep{Fan:2008} with setting the termination criterion to 0.001 to evaluate our model. 
We outline the general process of our unsupervised learning in Algorithm \ref{alg:U2GNNunsup}. 



\subsection{Training protocol} 

We follow some unsupervised approaches such as DGK \citep {yanardag2015deep} and AWE \citep{ivanov2018anonymous} to train our unsupervised U2GNN on all nodes from the entire dataset (i.e., consisting of all nodes from the test set during training) for a fair comparison. The hyper-parameters are varied as same as in Section \ref{subsec:training}.

We also train our GCN baseline following our unsupervised learning.
We set the batch size to 4 and vary the number of GCN layers in \{1, 2, 3\} and the hidden layer size in \{32, 64, 128, 256\}. We also use the Adam optimizer \citep{kingma2014adam} to train this unsupervised GCN up to 50 epochs.


\begin{figure*}[ht]
\centering
    \includegraphics[width=0.24\textwidth]{imdbm_step.pdf}
    \includegraphics[width=0.24\textwidth]{ptc_step.pdf}
    \includegraphics[width=0.24\textwidth]{imdbm_nei.pdf}
    \includegraphics[width=0.24\textwidth]{ptc_nei.pdf}
\caption[short]{Effects of the number of timesteps ($T$) and the number of neighbors sampled for each node ($N=|\mathcal{N}_\mathsf{v}|$) in the unsupervised learning.}
\label{fig:hypereffectNT}
\end{figure*}


\subsection{Evaluation protocol} 
We utilizes the logistic regression classifier \citep{Fan:2008} with using the 10-fold cross-validation scheme to evaluate our models.
We compare our unsupervised GCN (denoted as uGCN) and U2GNN with the baselines: Deep Graph Kernel (DGK) \citep{yanardag2015deep} and Anonymous Walk Embedding (AWE) \citep{ivanov2018anonymous}.

\subsection{Experimental results}
\label{subsec:unsupresults}

Table \ref{tab:expresult_unsup} presents the experimental results in the unsupervised learning.
We encourage a re-evaluation to examine the existing GNNs from the supervised learning to our new unsupervised learning to see negative results.
For example, regarding the supervised learning, as shown in Table \ref{tab:expresult_sup}, our supervised U2GNN outperforms GCN on the bioinformatics datasets.
However, regarding the unsupervised learning, as shown in Table \ref{tab:expresult_unsup}, our uGCN works better than our unsupervised U2GNN on PROTEINS, MUTAG, and PTC.
These three datasets are much sparse, and node neighbors have a similar effect on each other. 
Hence, without using the graph labels during training, U2GNN does not increase the similar effects on node neighbors, leading to be outperformed by our uGCN on these datasets.

In general, both our unsupervised U2GNN and uGCN obtain the state-of-the-art accuracies on the benchmark datasets.
The significant gains demonstrate a notable impact of our unsupervised learning.
It aims to guide GNNs to identify the sub-graphs' structures for every node; hence, the models can memorize the structural differences among graphs to produce the plausible node and graph embeddings as visualized in Figure \ref{fig:visualunsupembeddings}, leading to improve the unsupervised performance.
We hope that future GNN works can consider the unsupervised learning beside the supervised one.

\paragraph{Hyper-parameter analysis.} As shown in both Figures \ref{fig:effectTN_PROTEINS} and \ref{fig:hypereffectNT}, we also see similar findings in both the supervised and unsupervised training settings.

\begin{figure}[ht]
\centering
    \includegraphics[width=0.24\textwidth]{DD_node.pdf}
    \includegraphics[width=0.24\textwidth]{DD_graph.pdf}
\caption[short]{A visualization of the node and graph embeddings learned by our unsupervised U2GNN on the DD dataset.}
\label{fig:visualunsupembeddings}
\end{figure}




\end{document}
