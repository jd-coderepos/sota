









\documentclass[journal]{IEEEtran}























\ifCLASSINFOpdf
\else
\fi



















































\hyphenation{op-tical net-works semi-conduc-tor}



\usepackage{mathptmx} \usepackage{times} \usepackage{amsmath} \usepackage{amssymb}  \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{textcomp}

\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{mathrsfs}
\usepackage{color}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\IM}{\mathrm{im}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\QNUM}{D}
\newcommand{\Rank}{\mathrm{rank}}
\newcommand{\SPAN}{\mathrm{span}}
\newcommand{\Rowspace}{\mathrm{rowspace}}
\newtheorem{Definition}{Definition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Lemma}{Lemma}
\newtheorem{Claim}{Claim}
\newtheorem{Proposition}{Proposition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Remark}{Remark}
\newtheorem{Notation}{Notation}
\newtheorem{Example}{Example}
\newtheorem{Construction}{Construction}
\newtheorem{Procedure}{Procedure}
\newtheorem{Problem}{Problem}
\newtheorem{Convention}{Convention}
\newtheorem{Assumption}{Assumption}

\newcommand{\COL}{\mathbf{C}}
\newcommand{\MAN}[1]{\mathrm{M}_{#1}}
\newcommand{\STABMAN}[1]{\mathrm{SM}_{#1}}
\newcommand{\Na}{\mbox{}}
\newcommand{\LSS}{DTLSS\ }
\newcommand{\BLSS}{DTLSS\ }
\newcommand{\SLSS}{DTLSSs\ }
\newcommand{\BSLSS}{DTLSSs\ }
\newcommand{\GCR}{\textbf{GCR}}
\newcommand{\J}{\mathbf{J}}
\newcommand{\red}[1]{\textcolor{red}{#1}}


\begin{document}
\title{Model Reduction by Moment Matching for Linear Switched Systems}


\author{Mert Ba\c{s}tu\u{g}, Mih\'{a}ly Petreczky, Rafael Wisniewski and John Leth\thanks{Department of Electronic Systems, Automation and Control, Aalborg University, 9220 Aalborg, Denmark {\tt\small mertb@es.aau.dk} Tel: +33 646897133 Fax: +33 327712917, {\tt\small raf@es.aau.dk} Tel: +45 99408762 Fax: +45 98151739, {\tt\small jjl@es.aau.dk} Tel: +45 99407973 Fax: +45 98151739}\thanks{Department of Computer Science and Automatic Control (UR Informatique et Automatique), \'{E}cole des Mines de Douai, 59508 Douai, France {\tt\small mihaly.petreczky@mines-douai.fr} Tel: +33 327712238 Fax: +33 327712917}}









\markboth{IEEE Transactions on Automatic Control,~Vol.~, No.~, October~2014}{Model Reduction by Moment Matching for Linear Switched Systems}














\maketitle

\begin{abstract}
Two moment-matching methods for model reduction of linear switched systems (LSSs) are presented. The methods are similar to the Krylov subspace methods used for moment matching for linear systems. The more general one of the two methods, is based on the so called ``nice selection'' of some vectors in the reachability or observability space of the LSS. The underlying theory is closely related to the (partial) realization theory of LSSs. In this paper, the connection of the methods to the realization theory of LSSs is provided, and algorithms are developed for the purpose of model reduction. Conditions for applicability of the methods for model reduction are stated and finally the results are illustrated on numerical examples.
\end{abstract}

\begin{IEEEkeywords}
Linear switched systems, model reduction, automata.
\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{A}{}linear switched system (abbreviated by LSS) is a model of a dynamical process whose behavior changes among a number of linear subsystems depending on a logical decision mechanism, i.e., an LSS is a concatenation of linear systems. That is, the state of the linear subsystem just before a switching instant serves as the initial state for the next active linear system. The information about which local mode operates in a specific time instant, is contained in the switching signal, which can be totally arbitrary. Hence, the switching signal serves as an external input. Linear switched systems represent the simplest class of hybrid systems, they have been studied  extensively,  see \cite{liberzon2003}, \cite{Sun:Book} for an overview.



Model reduction is the problem of approximating a dynamical system with another one of smaller complexity. ``Smaller complexity'' for LSSs can refer to ``smaller number of state variables of each local mode'' or to ``smaller number of local modes''. In this work, by complexity we mean the former, and thus by model reduction we mean the approximation of the original LSS by another one, with a smaller number of states.

\textbf{Contribution of the paper}
In this paper, first we present model reduction algorithms based on partial realization theory for LSSs \cite{petreczky}. 
The main idea is to replace the original LSS by an LSS of smaller order, such that certain Markov parameters of the two LSSs are equal. Markov parameters of an LSS are the coefficients appearing in the Taylor series expansion of its input-output map around zero. More precisely, they are the high-order partial derivatives of the zero-state and zero-input responses of the LSS with respect to the dwell times (time between two consecutive changes in the switching signal) of each operating mode. Hence, if some of the lower order derivatives of the responses of two LSSs coincide, it means that their input-output behaviors are close. 

We present two methods. The first one will preserve all the Markov parameters which correspond to high-order derivatives up to order  for some integer . We will call this method -moment matching. This is a direct counterpart of the well-known method of moment matching for linear systems, where the  reduced order model preserves the first  Markov parameters of the transfer function at hand, \cite{antoulas}. The second method preserves a certain selection (not necessarily finite) of Markov parameters. The selections we allow will be referred to as \emph{nice selections}. Intuitively, a nice selection corresponds to a choice of basis of the extended controllability (resp. observability) space of an LSS \cite{Sun:Book,MP:BigArticlePartI}. The notion of nice selections is a direct generalization of the corresponding notion for linear systems \cite{Hazewinkel1,gugercin}, and in a more restricted form it appeared in \cite{petreczkypeeters1}. The second method gives the user additional flexibility in choosing
which Markov parameters should be preserved. In turn, this allows the user to focus on those Markov parameters which are relevant for the dynamical properties one wishes to preserve. For example, by choosing certain Markov parameters, it is possible to preserve the input-output behavior of the system in a certain discrete mode or even for a sequence of modes. At the end of the paper, we will present results to this effect. 
From an algorithmic point of view, both methods represent an extension of the classical Krylov subspace  based methods. 




\textbf{Motivation}
One of the main motivations for developing model reduction methods is that the order of the controller and the computation complexity of controller synthesis increase with the number of state variables of the plant model. This curse of dimensionality can be particularly troublesome for hybrid systems. The reason is as follows: A finite-state abstraction of the plant model is acquired in many of the existing control synthesis methods \cite{TabuadaBook}, subsequently one applies discrete-event control synthesis techniques to find a discrete controller for the finite-state abstraction of the plant. Usually, the states of this abstraction are not directly measurable, only some events (transition labels) are. This means that the controller has to contain a copy of the abstracted plant model, in order to be able to estimate the state of the finite-state abstraction of the plant, \cite{TabuadaBook,Wonham3,VardKupfContr}. In addition, the complexity of the control synthesis algorithm is at best polynomial in the number of states of the finite-state abstraction \cite{TabuadaBook,Wonham3,GameBook}. The situation becomes even worse when one considers the case of partial observations, i.e., when not all events (transition labels) of the finite-state abstraction are observable. This can be caused by the nature of the problem \cite{MP:HybIoDADHS09} or by the non-determinism of the abstraction. In this case, the control synthesis algorithm can have exponential complexity, \cite{GameBook,arnold_games_2003,Wonham3}, and the number of the state of the controller can be exponential in the number of the states of the abstraction. Depending on the method used and on the application at hand, the size of the finite-state abstraction can be very large, it could even be exponential in the number of continuous states of the original hybrid model, \cite{TabuadaBook}. In such cases, synthesis or implementation of controller might become very difficult, even for hybrid system of moderate size. Clearly, model reduction algorithms could be useful for such systems.








\textbf{Related work} The possibility of model reduction by moment matching for LSSs was already hinted in \cite{petreczky}, but no details were provided, no efficient algorithm was proposed, and no numerical experiments were done. Note that a naive application of the realization algorithm of \cite{petreczky} yields an algorithm whose computational complexity is exponential. Some results of this paper have appeared in \cite{bastugACC2014}. Main contributions of this paper different from \cite{bastugACC2014} can be summarized as follows: 1) Proofs for the main theorems in \cite{bastugACC2014} are presented. 2) The model reduction framework given in \cite{bastugACC2014} is generalized with the notion of nice selections. Hence, a less conservative framework is built for model reduction of LSSs, which is useful for focusing on the approximation of specific local modes. 3) This generalized framework is used to state a theorem which can be used for matching the input output behavior of a \emph{continuous time} LSS for a \emph{certain} switching sequence, with another LSS of smaller order. In \cite{bastugCDC2014}, the moment matching framework is used for matching the input-output behavior of \emph{discrete time} LSSs with a \emph{certain set} of allowed switching sequences. With respect to \cite{bastugCDC2014}, the main differences are that this paper focuses on the continuous time case and it allows approximation as opposed to exact matching of the input-output behavior. In addition,
the current paper uses the framework of  ``nice selections''. This framework is not only more general, but it has a clear system theoretical interpretation.

In the linear case, model reduction is a mature research area \cite{antoulas}. The subject of model reduction for hybrid and switched systems was addressed in several papers \cite{French1,Zhang20082944,Mazzi1,Chahlaoui,Habets1,China2,China3,Lam1,Kotsalis2,Kotsalis1,6209392,shaker2011}.
Except \cite{Habets1}, the cited papers propose techniques which involve solving certain LMIs, and for this reason, they tend to be applicable only to switched systems for which the continuous subsystems are stable.
In contrast, the approach of this paper works for systems which are unstable. However, this comes at a price, since we are not able to propose analytic error bounds, like the ones for balanced truncation \cite{petreczky2013}. In addition, the time horizon on which the approximation is ``good enough'', depends on the LSS. From a practical point of view, the lack of an analytic error bound and related issues need not be a very serious disadvantage, since it is often acceptable to evaluate the accuracy of the approximation after the reduced model has been computed.




The model reduction algorithm proposed in this paper is similar in spirit to moment matching for linear systems \cite{antoulas,gugercin} and bilinear systems \cite{BilinearMomentMatching2,BilinearMomentMatching3,BilinearMomentMatching5}; however, the details and the system class considered are entirely different. The concept of nice selection of columns (resp. rows) of the reachability (resp. observability) matrix for model reduction of multi input - multi output (MIMO) linear systems appeared in \cite{gugercin}. The method presented in this paper is based on the generalization of this concept to LSSs. In fact, this is seen as another contribution of the present paper. The model reduction algorithm for LPV systems described in \cite{toth2012} is related to the method given in this paper, as it also relies on a realization algorithm and Markov parameters. In turn, the realization algorithms and Markov parameters of LPV systems and LSSs are closely related, \cite{PM12}. However, the algorithm of \cite{toth2012} applies to a different system class (namely LPV systems), and it is not yet clear if it yields a partial realization of the original system considered.

\textbf{Outline}
In Section \ref{sect:prelim}, we fix the notation and terminology of the paper. In Section \ref{sect:lin_switch_def}, we present the formal definition and main properties of LSSs. In Section \ref{sect:markov}, we recall the concept of Markov parameters for linear systems and LSSs, and the problem of model reduction by moment matching. The solution to the moment matching problem for LSSs analogous to the linear case is stated in \ref{sect:Npart}. This solution is generalized and made useful further for LSSs in Section \ref{sect:nice} where also the related algorithm is stated in detail. Finally, in Section \ref{sec:exam} the two methods are illustrated on numerical examples.






\section{Preliminaries: notation and terminology}
\label{sect:prelim}

Denote by  the set of natural numbers including . Denote by  the set  of nonnegative real numbers. In the sequel, let , with  a topological subspace of an Euclidean space , denote the set of \emph{piecewise-continuous and left-continous maps}. That is,  if it has finitely many points of discontinuity on any compact subinterval of , and at any point of discontinuity both the left-hand and right-hand side limits exist, and  is continuous from the left. Moreover, when  is a discrete set it will always be endowed with the discrete topology.

In addition, denote by  the set of \emph{absolutely continuous maps}, and  the set of \emph{Lebesgue measurable maps} which are integrable on any compact interval.

If  with  is a real matrix (or vector),  (resp. ) denotes the th row of  with  (resp. th column of  with ). The notation  is used for addressing the entry of  in its th row and th column. Lastly,  will be used to denote the th unit vector in the canonical basis for .

\section{Linear switched systems}
\label{sect:lin_switch_def}

In this section, we present the formal definition of linear switched systems and recall a number of relevant definitions. We follow the presentation of \cite{MP:BigArticlePartI,petreczky2013}.

\begin{Definition}[LSS] \label{LSS}
	A continuous time linear switched system (LSS) is a control system of the form
	 
			\frac{d}{dt}x(t)&=A_{\sigma(t)}x(t)+B_{\sigma(t)}u(t),\quad x(t_0)=x_0\label{sdyn} \\
			y(t)&=C_{\sigma(t)}x(t)\label{out} 
		
	where  is the switching signal,  is the input,  is the state, and  is the output and  is the set of discrete modes. Moreover, , ,  are the matrices of the linear system in mode , and  is the initial state. The notation
	
	or simply , are used as short-hand representations for an LSSs of the form \eqref{eq:LSSform}. The number  is the \emph{dimension (order) of } and will sometimes be denoted by .
\end{Definition}

Next, we present the basic system theoretic concepts for LSSs.

\begin{Definition}\label{def:stateandoutput}
The \emph{input-to-state} map  and \emph{input-to-output} map  of  are the maps 
	
	defined by letting  be the solution to the Cauchy problem \eqref{sdyn} with  and , and letting  as in \eqref{out}.
\end{Definition}

The input-output behavior of an LSS realization can be formalized as a map

The value  represents the output of the underlying (black-box) system. This system may or may not admit a description by an LSS. Next, we define when an LSS describes (realizes) a map of the form \eqref{eq:inputoutputmap}.

The LSS  of the form \eqref{eq:LSSform} is a \emph{realization} of an input-output map  of the form \eqref{eq:inputoutputmap}, if  is the input-output map of  which corresponds to the initial state , i.e., . The map  will be referred to as the \emph{input-output map of} .

Moreover, we say that the LSSs  and  are \emph{equivalent} if  where  and  denote the initial states of  and  respectively. The LSS  is said to be a \emph{minimal} realization of , if  is a realization of , and for any LSS  such that  is a realization of , . An LSS  is said to be \emph{observable}, if for any two states , .

Let  denote the reachable set of the LSS  relative to the initial condition , i.e.,  is the image of the map . The LSS  is said to be \emph{span reachable} if the linear span of states which are reachable from the initial state is , i.e., if . Span-reachability, observability and minimality are related as follows.
\begin{Theorem}[\cite{MP:BigArticlePartI}]
	An LSS  is a minimal realization of  if and only if it is a realization of , and it is span-reachable and observable. If  and  are two minimal realizations of , then they are \emph{isomorphic}, i.e., there exists a non-singular  such that
	
\end{Theorem}
Moreover, if  is a realization of , then there exists an algorithm for computing from  a minimal realization  of , \cite{MP:BigArticlePartI,petreczky2013}. Hence, in the sequel, unless stated otherwise we will tacitly assume that the LSSs are minimal realizations of their input-output maps.

\section{Background on Markov parameters and moment matching}
\label{sect:markov}

In this section, we recall the concepts of Markov parameters and moment matching for linear systems and draw the analogy with the linear switched case. We will begin by recalling model reduction by moment matching for linear systems \cite{antoulas}.

\subsection{Markov parameters and moment matching for linear systems}

Recall that a potential input-output map of a linear system is an affine map  for which there exist analytic functions  and , such that

for all . Existence of such a pair of maps is a necessary condition for  to be realizable by a linear system. Indeed, consider a linear system

where ,  and  are ,  and  real matrices and  is the initial state. The map  is said to be \emph{realized by} , if the output response at time  of  to any input  equals . This is the case if and only if  is of the form \eqref{rev1} with  and .


If  is of the form \eqref{rev1}, then  is uniquely determined by the analytic functions  and . In turn, these functions are uniquely determined by their Taylor-coefficients at zero. Consequently, it is reasonable to approximate  by the function

such that the first  Taylor series coefficients of  and  coincide, i.e.,  and  for all .
The larger  is, the more accurate the approximation is expected to be. One option is to choose  and  in such a way that  would be realizable by an LTI (linear time invariant) state-space representation. In this case, this LTI state-space representation is called an \emph{N-partial realization of }. Specifically, define the th Markov parameter of  as follows

Note that if  and  is the Laplace transform of , then the Markov parameters are the coefficients of the Laurent expansion of , i.e.,  for all , . For the general case, if the linear system \eqref{rev2} is a realization of , then the Markov-parameters can be expressed as , for all . Moreover, the linear system \eqref{rev2} is an -partial realization of , if , . It can also be shown that if  has a realization by an LTI system of order , then the linear system \eqref{rev2} is a realization of  if and only if it is a  partial realization of , i.e., in this case  is uniquely characterized by \emph{finitely many} Markov parameters.

The main idea behind model reduction of LTI systems using moment matching is as follows. Consider an LTI system  of the form \eqref{rev2} and fix . Let  be the input-output map of  from the initial state . Find an LTI system  of order  strictly less than  such that  is an -partial realization of . A relation between  and  will be discussed later in the paper.

There are several equivalent ways to interpret the relationship between the LTI systems  and . Assume that the system matrices of  are  and the initial state of  is . If  is a solution to the moment matching problem described above, then the first  coefficients of the Laurent series expansion of the transfer functions  and  coincide. Yet another way to interpret the LTI system  is to notice that  for all .

\subsection{Markov parameters and moment matching for linear switched systems}

In this paper, we will extend the idea of moment matching from LTI systems to LSSs. To this end, we will use the generalization of Markov parameters to the input-output maps of LSSs.

\begin{Notation} \label{Notation1}
	Consider a finite non-empty set  with  elements, which will be called the \emph{alphabet}. Denote by  the set of finite sequences of elements of . The elements of  are called \emph{strings} or \emph{words} over  and any set  is called a \emph{language} over . Each non-empty word  is of the form  for some . The element  is called the \emph{th letter of }, for , and  is called the \emph{length} of . The \emph{empty sequence (word)} is denoted by . The length of word  is denoted by ; note that . The set of non-empty words is denoted by , i.e., . The subset of  containing all the words of length at most (resp. at least)  will be denoted by  (resp. ). The \emph{concatenation} of word  with  is denoted by : If , and  , , then . If , then ; if , then . For simplicity, the finite set  will be identified with its index set, that is .
\end{Notation}

Next consider an input-output map  of the form \eqref{eq:inputoutputmap}. Notice that the restriction to a finite interval  of any  can be interpreted as finite sequence of elements from  of the form

where  and , , such that for all 

Clearly this encoding is not one-to-one, since if  for any  and  corresponds to , then  also corresponds to .

From \cite{MP:BigArticlePartI}, it follows that a necessary condition for  to be realizable by an LSS is that  has a \emph{generalized kernel representation}. For a detailed definition of a generalized kernel representation of , we refer the reader to \cite[Definition 19]{MP:BigArticlePartI}.
\footnote{Note that in \cite{MP:BigArticlePartI} the concept of generalized kernel representation was defined for families of input-output maps. In order to apply the definition and results of \cite{MP:BigArticlePartI} to the current paper, one has to take a family of input-output maps  which is the family consisting of one single map , i.e., . In addition, in \cite{MP:BigArticlePartI} the input-output maps were defined not for switching signals from , but for switching sequences of the form \eqref{eq:switching_sequence}, where the times  were allowed to be zero. However, by using the correspondence between switching signals from  and switching sequences \eqref{eq:switching_sequence}, and by using the properties (2) and (3) of \cite[Definition 19]{MP:BigArticlePartI}, we can easily adapt the definition and results from \cite{MP:BigArticlePartI} to the setting of the current paper.}

For our purposes, it is sufficient to recall that if  has a generalized kernel representation, then there exists a unique family of analytic functions  and , , , such that for all ,  and for any  which corresponds to ,

and the functions  satisfy a number of technical conditions, see \cite[Definition 19]{MP:BigArticlePartI} for details.

From \cite{MP:BigArticlePartI}, it follows that there is a one-to-one correspondence between  and the family of maps . The maps  play a role which is similar to the role of the functions  and  in the LTI case. If  has a realization by an LSS \eqref{LSS}, then the functions  and  satisfy

We can now define the Markov parameters of  as follows.

\begin{Definition}[Markov parameters] \label{MarkovParameters}
	The Markov parameters of  are the values of the map
	
	defined by
	
	where the vectors  and the matrices  are defined as follows. For all ,
	
	and for all , ,  by
	
	where , , .
\end{Definition}

That is, the Markov parameters of  are certain partial derivatives of the functions . From \cite{MP:BigArticlePartI}, it follows that the Markov parameters  determine the maps , and hence , uniquely.
If  has a realization by an LSS  of the form \eqref{LSS}, then the Markov-parameters of  can be expressed as products of the matrices of . In order to present the corresponding formula, we will use the following notation.

\begin{Notation} \label{Notation2}
	Let , ,  and , . Then the matrix  is defined as
	
	If , then  is the identity matrix.
\end{Notation}



From \cite{MP:BigArticlePartI}, it follows that an LSS \eqref{LSS} is a realization of the map  if and only if  has a generalized kernel representation and  for all , or in more compact form

with  and .
The main idea behind moment matching for LSSs (more precisely, for their input-output maps), is as follows: approximate  by another input-output map , such that some of the Markov parameters of  and  coincide. One obvious choice is to say that  for all ,  for some . This approach will be explained in detail in the next section after formally defining -partial realizations for an LSS. The other approach is based on the concept of nice selections of the columns (resp. rows) of the partial reachability (resp. observability) matrix of an LSS, and it will be presented in Section \ref{sect:nice}. The approach based on nice selections is less conservative and, as seen in Section \ref{sect:nice}, it can be used for matching the input output behavior of two LSSs along a certain switching sequence.

\section{Model reduction by  or -partial realizations} \label{sect:Npart}

In this section, the aim is to present an efficient  model reduction algorithm which transforms an LSS  into an LSS  such that  and some number of Markov parameters of  and  are equal. Firstly, we will formally define the concept of -partial realizations and state the problem taken at hand in this section.



\begin{Definition}[-partial realization] \label{def:Npartial}
	The LSS \eqref{LSS} is called \emph{-partial realization} of , if
	
	with  and .
\end{Definition}

If  is of the form \eqref{LSS} and  is the input-output map of , then the concept of -partial realization can be interpreted as follows:  is an -partial realization of , if those Markov parameters of  and  which are indexed by words of length at most  coincide. The analogous (to the linear case) problem of model reduction by moment matching for LSSs can now be formulated as follows.

\begin{Problem} \label{prob:momentmatching}
	\emph{(-Moment matching problem for an LSS).}
	Let  be an LSS of the form \eqref{LSS} and let  be its input-output map. Fix . Find an LSS  such that  and  is an -partial realization of .
\end{Problem}

An -partial realization  of  means that all the partial derivatives of order at most  of  and of  coincide, where . Intuitively, this will mean that for any input and switching signal , the outputs  and  are close, for small enough . In fact, this approach is the direct analogue of the moment matching methods for linear systems and it has a system theoretical interpretation. Namely, the following corollary of \cite[Theorem 4]{petreczky} clarifies this interpretation by stating how many Markov parameters of a map  must be matched by an LSS , for it to be a realization of . Note that there is a trade off between the choice of  and the dimension .

\begin{Corollary} \label{cor:rel_N_n}
	Assume that  is a minimal realization of  and  is such that . Then for any LSS  which is an -partial realization of ,  is also a realization of  and .
\end{Corollary}

That is, if we choose  too high, namely if we choose any  such that , where  is the dimension of a minimal LSSs realization of , then there will be no hope of finding an LSS which is an -partial realization of the original input-output map, and whose dimension is lower than .


In order to solve Problem \ref{prob:momentmatching}, one could consider applying the partial realization algorithm \cite{petreczky}. In a nutshell, \cite{petreczky} defines finite \emph{Hankel matrices} and proposes a Kalman-Ho like realization algorithm based on the factorization of the Hankel matrix, \cite[Algorithm 1]{petreczky}. The problem with this naive approach is that it involves explicit construction of Hankel matrices, whose size is exponential in . Consequently, the application of the partial realization algorithm would yield a model reduction algorithm whose memory-usage and run-time complexity is exponential. In the next section, we present a model reduction algorithm which yield a partial realization of the input-output map of the original system, and which does not involve the explicit computation of the Hankel matrix.





In the sequel, the image (column space) of a real matrix  is denoted by  and  is the dimension of . 



We will start with presenting the following definitions.

\begin{Definition} \label{ObservabilityMat}
	\emph{(Partial unobservability space).}
The partial unobservability space  of  up to words of length  is defined as follows:
	
\end{Definition}

In the rest, we will denote  by  if  is clear from the context. It is not difficult to see that  and for any , . From \cite{Sun:Book,MP:BigArticlePartI}, it follows that  is observable if and only if  for all .

\begin{Definition}[Partial reachability space] \label{ReachabilityMat}
The partial reachability space  of  up to words of length  is defined as follows:
	
\end{Definition}

In the rest, we will denote  by  if  is clear from the context. It is easy to see that  and , for  (note that here the summation operator must be interpreted as the Minkowski sum). It follows from \cite{MP:BigArticlePartI,Sun:Book} that  is span-reachable if and only if  for all .

Given the definition of partial observability / reachability spaces, one can define the corresponding matrix representations  and  such that  and , and hence the partial Hankel matrix  of an LSS  as . Howevever, this is only a side remark since the methods given in this paper will not use explicit representations of the Hankel matrices.

\begin{Theorem} \label{theo:mert1}
	\emph{(One sided moment matching for -partial realizations (reachability)).}
	Let
	
	be an LSS realization of the input-output map ,  be a full column rank matrix such that
	
	If  is an LSS such that for each , the matrices  and the vector  are defined as
	
	where  is a left inverse of , 
	then  is an -partial realization of .
\end{Theorem}

\begin{proof}
	Let , ,  and let . If , then . Since the conditions of Theorem~\ref{theo:mert1} imply  and  is a left inverse of , it is a routine exercise to see that .If , then  is also a subset of , . Hence, by induction we can show that 
	, , which ultimately yields
	
	Using a similar argument, we can show that
	
	Using \eqref{eq2:theo_mert1} and \eqref{eq21:theo_mert1}, and 
	, , we conclude that for all , ,
	, 
	
	from which the statement of the theorem follows.
\end{proof}

Note that the number  is the number of columns in the full column rank matrix , hence . This fact leads  to be of reduced order if  is sufficiently small, see Corollary \ref{cor:rel_N_n}. Using a dual argument, we can prove the following dual result.

\begin{Theorem}[One sided moment matching (observability)] \label{theo:mert2}
	Let  be an LSS realization of the input-output map ,  be a full row rank matrix such that
	
	Let  be any right inverse of  and let
	
	be an LSS such that for each , the matrices  and the vector  are defined as
	
	Then  is an -partial realization of .
\end{Theorem}

Finally, by combining the proofs of Theorem \ref{theo:mert1} and Theorem \ref{theo:mert2}, we can show the following.

\begin{Theorem}[Two sided moment matching] \label{theo:mert3}
	Let  be an LSS realization of the input-output map ,  and  be respectively full column rank and full row rank matrices such that
	
If  is an LSS such that for each , the matrices  and the vector  are defined as
	
	then  is a -partial realization of .
\end{Theorem}

Note that having a -partial realization as an approximation system would be more desirable than having an -partial realization, since number of matched Markov parameters would increase. However, it is only possible to get a -partial realization for the original system  when the additional condition  is satisfied. Now, we will present an efficient algorithm of model reduction by moment matching, which computes either an  or -partial realization  for an  which is realized by an LSS . First, we present algorithms for computing the subspaces  and . To this end, we will use the following notation: if  is any real matrix, then denote by  the matrix  such that  is full column rank,  and . Note that  can easily be computed from  numerically, see for example the Matlab command \texttt{orth}.

The algorithm for computing  such that  is presented in Algorithm \ref{alg1} below.

\begin{algorithm}[h]
	\caption{
		Calculate  a matrix representation of ,
		\newline
		\textbf{Inputs}:  and 
		\newline
		\textbf{Outputs:}  such that ,
		.
	}
	\label{alg1}
	\begin{algorithmic}
		\STATE , .
		\FOR{} 
		\STATE
		
		\ENDFOR
		\RETURN .
	\end{algorithmic}
\end{algorithm}

By duality, we can use Algorithm \ref{alg1} to compute a  such that , the details are presented in Algorithm \ref{alg2}.

\begin{algorithm}
	\caption{
		Calculate a matrix representation of 
		\newline
		\textbf{Inputs}:  and 
		\newline
		\textbf{Output:} , such that
		 and .
	}
	\label{alg2}
	\begin{algorithmic}
		\STATE Apply Algorithm \ref{alg1} with inputs  to obtain
		a matrix .
		\RETURN .
	\end{algorithmic}
\end{algorithm} 

Notice that the computational complexity of Algorithm \ref{alg1} and Algorithm \ref{alg2} is polynomial in  and , even though the spaces of  (resp. ) are generated by images (resp. kernels) of exponentially many matrices.

Using Algorithms \ref{alg1} and \ref{alg2}, we can formulate a model reduction algorithm, see Algorithm \ref{alg3}.
\begin{algorithm}
	\caption{Moment matching for LSSs
		\newpage 
		\textbf{Inputs:} ,  and .
		\newpage  
		\textbf{Output: } .
	}
	\label{alg3}
	\begin{algorithmic}
		\STATE Using Algorithm \ref{alg1}-\ref{alg2} compute matrices  and  such that
		 is full column rank,  is full row rank and ,
		.
		\IF{ and } 
		\STATE
		Let  and
		
		\ENDIF
		\IF{}
\STATE
		Let ,  be a left inverse of  and set
		
		\ENDIF
		\IF{}
		\STATE
		Let  and let  be a right inverse of . Set
		
		\ENDIF
		\RETURN .
	\end{algorithmic}
\end{algorithm}


Theorems \ref{theo:mert1} -- \ref{theo:mert3} imply the following corollary on correctness of Algorithm \ref{alg3}.

\begin{Corollary}[Correctness of Algorithm \ref{alg3}]
	Using the notation of Algorithm \ref{alg3}, the following holds: If  and , then Algorithm \ref{alg3} returns a -partial realization of  (if  and the rank condition does not hold, the algorithm returns nothing). Otherwise, Algorithm \ref{alg3} returns an -partial realization of .
\end{Corollary}

Note that the input variable  in Algorithm \ref{alg3} represents the choice of the user on which method to be used, i.e., if , the algorithm uses Theorem \ref{theo:mert1}; if , the algorithm uses Theorem \ref{theo:mert2} and if , the algorithm uses Theorem \ref{theo:mert3}. If  and the condition  does not hold, Algorithm \ref{alg3} can always be used for getting an -partial realization, by choosing  or  .

\section{Model reduction by nice selections}
\label{sect:nice}

In this section, a more general approach for moment matching of LSSs will be taken. In contrast to the -partial realization solution, the material in this section is not direct analogue of the moment matching for linear systems, it is more suited for LSSs specifically. The notion of nice selections of columns (resp. rows) of the reachability (resp. observability) matrices of an LSS, gives flexibility to the user of the method in this section in the following sense. The user may focus on the approximation of some specific modes more than the others. Moreover, as we will show in Theorem \ref{thm:nice_sel_switch_seq}, the method can be used for exactly matching (or approximating) the input-output behavior of a continuous time LSS with an LSS of possibly lower order for a \emph{certain} switching sequence.


Now, the concept of nice column (resp. row) selections for (partial) reachability (resp. observability) space of an LSS will be defined. This is the central tool for the moment matching method to be presented. 

\begin{Definition}[Nice selections]
	A subset  of ,  is called a nice row selection for an LSS , if  has the following property; if  for some , , then .
	
	Likewise, a subset  of , , is called a nice column selection for an LSS  , if  has the following property; if  for some , , then ; and if  for some , , then .
\end{Definition}

The spaces related to a row nice selection  or a column nice selection  can now be defined.
 
\begin{Definition}[-unobservability and -reachability spaces]
Let   be a minimal realization of . Let  be a nice row selection and  be a nice column selection related to . Then the subspaces

will be called -unobservability and -reachability spaces of  respectively.
\end{Definition}
Similarly to the previous section,  and  will be denoted by  and  if  is clear from the context. 

\begin{Example} \label{ex:nice_sel1}
	In order to illustrate the notion of a nice selection, let us consider the linear SISO case. Then , and hence  and  can be identified with its length, since . It then follows that an element  of a nice selection is of the form  and  and hence it can be identified with the natural number . Then a nice selection  can be identified with a subset  with the property that if , then . For the MIMO linear case, , and any sequence  can be identified with its length as explained above. Then a nice column selection  is a subset of , such that if , then  and if  then . A similar characterization holds for nice row selections. That is, for the linear case, our definition of nice selections yields the classical concept \cite{Hazewinkel1}.
\end{Example}

The moment matching method for LSSs to be presented is based on constructing matrix representations of the -reachability or -unobservability spaces of an LSS , i.e., again constructing the matrices  or  such that  and . For this purpose, it is crucial to find a basis for those spaces. The following lemma connects the notion of nice selections to this goal.

\begin{Theorem} \label{lem:nice_selections}
	Let  be an LSS of the form 
	
	For any , there exists a nice column selection  (resp. row selection ) such that  (resp. ). 
\end{Theorem}

\begin{proof}
	See Appendix.
\end{proof}

Let  denote the space  and  denote the space  for a nice row selection . Note that  is isomorphic to the orthogonal complement of  and  is isomorphic to the orthogonal complement of . From the proof of Theorem~\ref{lem:nice_selections}, it can be seen that there exists a nice column selection , and a nice row selection of , such that , , and the vectors of  indexed by the elements of  (respectively the vectors of  indexed by the elements of ) are linearly independent. It means that if , , then  has  elements, and  has  elements. Thus, if such a nice column selection  (respectively nice row selection ) has  elements, then  (respectively ).

We can now formulate the following extension of the method in the previous section in terms of nice selections. To this end, we extend the notion of a partial realization as follows.

\begin{Definition} \label{def:alphabeta_partreal}
	Let  be a nice row selection, and  be a nice column selection of an LSS  of the form \eqref{LSS}. Let
	,
	,
	
	\begin{enumerate}
		\item  is a -partial realization of , if for every , the th row of  equals the th row of . This can be formulated equivalently as
		
		where  denotes the th unit vector in the canonical basis for .
		\item  is a -partial realization of , if for every , the th column of  equals the th column of , and if for every , st column of  equals . This can be formulated equivalently as
		
		where  denotes the th unit vector in the canonical basis for .
		\item  is an  partial realization of , if for every  and for every , the entry of  in its th row and th column equals the th entry of , and if for every  and for every , the entry of  in its th row and st column equals the th row of ; alternatively,
		
	\end{enumerate}
	
\end{Definition}

Note that the same definition could have been formulated for the arbitrary sets  and  which are not necessarily nice selections. However, Definition \ref{def:alphabeta_partreal} formulated as it is, since the algorithms (which will be presented later on) to acquire ,  or -partial realizations make use of Theorems \ref{theo:krylov1}-\ref{theo:krylov3}, and for the proof of these theorems, it is crucial that the sets  and  define nice selections. This fact is also required for proving Theorem \ref{thm:nice_sel_switch_seq}, which gives the conditions for acquiring a reduced order LSS which has exactly the same input-output behavior as the original one, for a specific switching sequence.

\begin{Theorem} \label{theo:krylov1}
	\emph{(One sided moment matching by the column nice selection ).}
	Let  be a realization of  of the form \eqref{LSS}. In addition, let  be a full column rank matrix and  be a nice column selection such that
	
	For each , define
	
	where  is any left inverse of .
	Then
	
	is a -partial realization of .
\end{Theorem}

The theorem above is similar to Theorem \ref{theo:mert1}. The numerical task is again to compute a matrix  such that  in an efficient way. In the model reduction method to be presented, the solution for this task will be explained more in detail.

\begin{proof}\emph{(Theorem \ref{theo:krylov1}).}
	We first show that for any ,
	
	
	
	The proof is by induction on the length of . For ,  is a column of , and since ,  for some . Notice that  , and hence . Assume the claim holds for all , . Let  be such that , , , . Then from the properties of a nice selection it follows that , and hence by the induction hypothesis,
	
	It then follows that
	
	Notice that from  it follows that , and hence there exists  such that . It then follows that
	
	That is, we have shown that \eqref{theo:krylob1:eq1} holds.
	From \eqref{theo:krylob1:eq1} it follows that
	
	Similarly, we can show that  for all  i.e.,  is a -partial realization of .
\end{proof}

By duality, we could formulate nice row selections, and also a two sided Krylov subspace projection method, as demonstrated in Theorems \ref{theo:krylov2} and \ref{theo:krylov3}.

\begin{Theorem} \label{theo:krylov2}
	\emph{(One sided moment matching by the row nice selection ).}
	Let  be a realization of  of the form \eqref{LSS}. In addition, let  be a full row rank matrix and  be a nice row selection such that
	
	For each , define
	
	where  is any right inverse of . Then
	
	is an -partial realization of .
\end{Theorem}

\begin{proof}\emph{(Theorem \ref{theo:krylov2}).}
	This follows from Theorem \ref{theo:krylov1} by duality. Consider a  which satisfies the assumption of the theorem. Recall that . Then, it is easy to see that . We will show that for any ,
	
	The proof is again by induction on the length of . For ,  belongs to  and hence,  for some . Notice that  hence . Assume the claim holds for all , . Let  be such that , , , . Then from the properties of a nice selection it follows that , and hence, by the induction hypothesis
	
	It then follows that
	
	Notice that from  it follows that  and hence there exists  such that
	
	It then follows from \eqref{eq:proof_krylov2_2} and \eqref{eq:proof_krylov2_3} that
	
	That is, we have shown that \eqref{eq:proof_krylov2_1} holds. From \eqref{eq:proof_krylov2_1} it follows that
	
	Similarly, we can show that  for all  i.e.,  is an -partial realization of .
\end{proof}

\begin{Theorem} \label{theo:krylov3}
	\emph{(Two sided moment matching by row/column nice selections  and ).}
	Let  be a realization of  of the form \eqref{LSS}. Let  be a nice row selection,  be a nice column selection. Let  be a full row rank matrix and  be a full column rank matrix such that
	\begin{enumerate}
		\item ,
		\item ,
		\item .
	\end{enumerate}
	For all , define
	
	Then
	
	is an -partial realization of , and it is also an  and -partial realization of .
\end{Theorem}

\begin{proof}\emph{(Theorem \ref{theo:krylov3}).}
	Define . Notice that the conditions of the theorem imply  is nonsingular so its inverse  exists. Notice that since  is again  with full column rank, a left inverse  of  can be defined and  is a right inverse of . It then follows from Theorem~\ref{theo:krylov1} and Theorem~\ref{theo:krylov2} that  is an - and -partial realization of . More clearly, from the proof of Theorem \ref{theo:krylov1}, \eqref{theo:krylob1:eq1}, it follows that for any ,
	
	Using duality or from the proof of Theorem~\ref{theo:krylov2}, \eqref{eq:proof_krylov2_1}, it can be shown that for any ,
	
	Notice that  and hence, combining \eqref{theo:krylov3:eq1} and \eqref{theo:krylov3:eq2} implies
	
	The part about  can be proven similarly. It follows that  is an  partial realization of . 
\end{proof}

\begin{Remark}
	\emph{(Relation between ,  and , , -partial realizations).}
	Note that if the  matrix in Theorem~\ref{theo:krylov1} is such that  and  matrix in Theorem~\ref{theo:krylov2} is such that , then the acquired reduced order systems would be -partial realizations for each case. Likewise, if the  and  matrices in Theorem~\ref{theo:krylov3} can be found such that ,  and , then the acquired reduced order system would be a -partial realization. In this sense, the method given in this section is a generalization of the previous method. In other words,  or -partial realizations are just ,  or -partial realizations for a specific choice of ,  or . These choices would be in the following form: The set  contains all the elements of the form ; the set  contains all the elements of the form  and .
\end{Remark}


Now we will present three efficient algorithms of model reduction by moment matching, which compute either an , , -partial realization  for an  which is realized by an LSS . Firstly, we present algorithms for computing some subspaces of  and . Then, those algorithms will be used to acquire the matrices  and  in the Theorems \ref{theo:krylov1}-\ref{theo:krylov3} and hence, to formulate a global model reduction by moment matching method for LSSs.

\begin{Definition} \label{def:subsets}
	\emph{(The languages related to  and ).}
	Let  be a column nice selection and  . Define the corresponding languages related to  as
	
	Furthermore, let  be a row nice selection and . Define the corresponding languages related to  as
	
	The numbers  and  will be called the \emph{subset cardinality} of  and  respectively.
\end{Definition}

\begin{Example}
	Suppose a column nice selection  related to an LSS  with  is given by
	
	Then the set  and the corresponding languages  are given by
	
	Note that the number  is the subset cardinality of  i.e., there are  languages related to , namely  and .
\end{Example}

\begin{Definition} \label{def:NDFA}
	A non-deterministic finite state automaton (NDFA) is a tuple  such that
	\begin{enumerate}
		\item  is the finite state set,
		\item  is the set of accepting (final) states,
		\item  is the state transition relation labelled by ,
		\item  is the initial state.
	\end{enumerate}
	For every , define  inductively as follows:  and  for all . We denote the fact  by . The fact that there exists  such that  is denoted by . Define the language  accepted by  as 
	
	We say that  is \emph{co-reachable}, if from any state a final state can be reached, i.e., for any , there exists  and  such that . It is well-known that if  accepts , then we can always compute an NDFA  from  such that  accepts  and it is co-reachable.
\end{Definition}

In the sequel, we will assume that the languages , ,  associated with a nice selection  or  are regular i.e., there exists an NDFA accepting them. By using the definitions above  we can define the subspaces  and  for real matrices  and  as

and use them to rewrite the spaces  and  in the following form:


Now we are ready to present the two algorithms to compute a representation for the subspaces  and  respectively. Observe from \eqref{eq:new_Rbeta_Oalpha}, those two algorithms can be subsequently used for computing the  and  matrices such that  and  for a given  or . These algorithms are similar to the ones in \cite{bastugCDC2014} where they were used for model reduction of a discrete time LSS with respect to a certain set of switching sequences.

\begin{algorithm}
	\caption{
		Calculate  a matrix representation of , 
		\newline
		\textbf{Inputs}:  and  such that , , ,  and   is co-reachable.
		\newline
		\textbf{Outputs:}  such that ,
		. 
	}
	\label{alg4}
	\begin{algorithmic}[1]
		\STATE .
		\STATE .
		\label{alg4.0}
		\STATE .
\WHILE{}
		\label{alg4.1}
		\STATE 
		\FOR{}
		\STATE  
		\FOR{}
		\STATE 
		\ENDFOR
		\STATE 
		\ENDFOR
		\IF{)}
		\STATE{.}
		\ENDIF 
		\ENDWHILE
		\RETURN .
	\end{algorithmic}
\end{algorithm}

\begin{Lemma}[Correctness of Algorithm \ref{alg4} -- Algorithm \ref{alg5}] \label{lem:correctness}
	Assume  is regular and  is a co-reachable NDFA which accepts . Algorithm \ref{alg4} returns a full column rank matrix  such that , and Algorithm \ref{alg5} returns a full row rank matrix  such that .
\end{Lemma}

\begin{proof}\emph{(Lemma \ref{lem:correctness}).}
	We prove only the first statement of the lemma, the second one can be shown using duality. Let , . It then follows that after the execution of Step \ref{alg4.0},  for all . Moreover, by induction it follows that
	
	for all  and . Hence, by induction it follows that at the th iteration of the loop in Step \ref{alg4.1}, . Notice that  and hence there exists  such that , , and thus ,
	
	Let . It then follows that  for all  and hence after  iterations, the loop \ref{alg4.1} will terminate. Moreover, in that case,  , . But notice that for any , ,  if and only if , and  if and only if , . Hence,  and thus .
\end{proof}

\begin{algorithm}
	\caption{
		Calculate a matrix representation of , 
		\newline
		\textbf{Inputs}:  and  such that , , ,  and   is co-reachable.
		\newline
		\textbf{Outputs:}  such that ,
		.
	}
	\label{alg5}
	\begin{algorithmic}[1]
		\STATE .
		\STATE .
		\STATE .
\label{alg2.0}
		\WHILE{}
		\label{alg2.1}
		\STATE 
		\FOR{}
		\STATE  
		\FOR{}
		\STATE 
		\ENDFOR
		\STATE 
		\ENDFOR
		\IF{}
		\STATE{.}
		\ENDIF
		\ENDWHILE
		\RETURN .
	\end{algorithmic}
\end{algorithm}

Notice that the computational complexities of Algorithm \ref{alg4} and Algorithm \ref{alg5} are polynomial in , even though the spaces of  (resp. ) might be generated by images (resp. kernels) of exponentially many matrices.

Using Algorithms \ref{alg4} and \ref{alg5}, we can state Algorithms \ref{alg6}, \ref{alg7} and \ref{alg8} for getting reduced order ,  or  - partial realizations for an LSS  respectively. The matrices  and  computed in Algorithms \ref{alg6} and \ref{alg7} satisfy the conditions of Theorems \ref{theo:krylov1} and \ref{theo:krylov2} respectively.

\begin{algorithm}
	\caption{Reduction for -partial realization
		\newpage 
		\textbf{Inputs:} ,  nice column selection, ,  NDFAs such that  and  for all .
		\newpage  
		\textbf{Output:}  such that  is a -partial realization of .
	}
	\label{alg6}
	\begin{algorithmic}[1]
		\STATE Use Algorithm \ref{alg4} with inputs ,  and NDFA . Store the output  as .
		\FOR{}
		\STATE  Use Algorithm \ref{alg4} with inputs ,  and NDFA .  Store the output  as .
		\ENDFOR
		\STATE  where  is the subset cardinality of  as in Def. \ref{def:subsets}.
		\STATE
		Let ,  be a left inverse of  and set
		
		\RETURN .
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
	\caption{Reduction for -partial realization
		\newpage 
		\textbf{Inputs:} ,  nice row selection,  NDFAs such that  for all .
		\newpage  
		\textbf{Output:}  such that  is an -partial realization of .
	}
	\label{alg7}
	\begin{algorithmic}[1]
		\FOR{}
		\STATE  Use Algorithm \ref{alg5} with inputs ,  and NDFA .  Store the output  as .
		\ENDFOR
		\STATE  where  is the subset cardinality of  as in Def. \ref{def:subsets}.
		Let  and let  be a right inverse of . Set
		
		\RETURN .
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Reduction for -partial realization
		\newpage 
		\textbf{Inputs:} ,  nice column selection,  nice row selection, , ,  NDFAs such that  and  for all  and  for all .
		\newpage  
		\textbf{Output:}  such that  is an -partial realization of .
	}
	\label{alg8}
	\begin{algorithmic}[1]
		\STATE Compute the matrix  as in Algorithm \ref{alg6}.
		\STATE Compute the matrix  as in Algorithm \ref{alg7}.
		\IF{ and  and }
		\STATE 
		
		\ENDIF
		\RETURN .
	\end{algorithmic}
\end{algorithm}

\begin{Lemma} [Correctness of Algorithms \ref{alg6}, \ref{alg7} and \ref{alg8}]
	Let  be an LSS of the form \eqref{LSS}.
	\begin{enumerate}
		\item  Let  be a nice column selection and assume that , ,  are regular languages. Let  and ,  be co-reachable NDFAs which accept  and ,  respectively. Then the LSS  returned by Algorithm \ref{alg6} is a -partial realization of .
		
		\item Let  be an LSS of the form \eqref{LSS}. Let  be a nice row selection and assume that ,  are regular languages. Let ,  be co-reachable NDFAs which accept ,  respectively. Then the LSS  returned by Algorithm \ref{alg7} is an -partial realization of .
		
		\item Let  be an LSS of the form \eqref{LSS}. Let  be a nice row selection,  be a nice column selection and assume that , , , ,  are regular languages. Let , ,  and ,  be co-reachable NDFAs which accept , , , ,  respectively. Then the LSS  returned by Algorithm \ref{alg8} is an -partial realization of  if the condition  and  and  holds.
	\end{enumerate}
\end{Lemma}





The final result of the paper will be to connect a certain switching sequence for a continuous time LSS to a nice column or row selection. For this, we need additional definitions as below.



\begin{Definition}[The generating language] \label{def:generating_lang}
	The generating language  for a sequence of discrete modes , , , is defined as
	
\end{Definition}


\begin{Definition} \label{nice_sel_switch_seq}
	\emph{(Nice selection related to a switching sequence).}
	A nice column selection  related to a  sequence
	of discrete modes
	  , ,  is defined as

	In addition, a nice row selection  related to a sequence of discrete modes
	, , 
	is defined as
	
\end{Definition}

The following theorem makes it possible to use the model reduction method with nice selections with respect to a specific switching sequence.

\begin{Theorem} \label{thm:nice_sel_switch_seq}
	Consider a sequence of discrete modes
	, , 
Let  be an LSS which is a  (resp. ) - partial realization of . Then, for every switching signal which satisfies \eqref{eq:switch_seq_sigma_def} for some , and for all ,
	  
	  where .
\end{Theorem}

Intuitively, Theorem \ref{thm:nice_sel_switch_seq} says that if  is a  (resp. ) - partial realization of , then the outputs of  and  along the switching sequence  are the same. Hence, if we apply Algorithm \ref{alg6} or Algorithm \ref{alg7} with  or respectively , then we will get an LSS  which has the same input-output behavior as  along the switching sequence .

\begin{proof}\emph{(Theorem \ref{thm:nice_sel_switch_seq}).}
	Only the part related with the nice column selection will be proven, similar arguments can be used to prove the result for
	nice row selections.
	
Note that the output of  at a time ,  due to the switching signal , initial state  and input  is given by
	
	whereas  is given by
	
	Hence, for  and  to be equal, it is sufficient that the following equations hold:
	
	By Definition \ref{def:generating_lang}, the generating language for  can be defined as the set
	
	Therefore, if the Taylor series expansion of the matrix exponentials in the equations of \eqref{eq:pf_thm9_state_trans} is taken around , it can be seen that for
	\eqref{eq:pf_thm9_state_trans} to hold, it is sufficient that  
	holds. In turn, \eqref{eq:pf_thm9_Markov_eq} follows from the
	assumption that  is a -partial
	realization of , if we use the definition of
	.
	Hence,  for .
\end{proof}


The theorem above builds the relationship between a certain switching sequence and its related nice selection. Hence, it makes it possible to acquire an approximation to an LSS whose input-output behavior is identical for all switching sequences  for a fixed sequence of discrete modes , and whose order  is possibly smaller than  (Note that since  is of full column rank, ).

\section{Numerical examples}
\label{sec:exam}

In this section, two generic numerical examples are presented to illustrate the model reduction procedure. One of the numerical examples is for an LSS who has stable local modes. With this example, it is aimed to show the flexibility of the nice selections about choosing the specific local modes, on which the approximation should focus. Whereas in the other numerical example, the LSS has unstable local modes, and an -partial realization is acquired for the original system to illustrate a solution to the analogue of the moment matching problem for linear systems.

Firstly, the procedure is applied to a SISO, th order LSS with  discrete modes i.e., to an LSS  of the form  with , , . The randomly generated system has locally stable modes. The data of , ,  parameters and the initial state  used for simulation is also available from https://kom.aau.dk/\texttildelow mertb/. A random switching signal with minimum dwell time (time between two subsequent changes in the switching signal) of  for mode  and  for mode  is used for simulation. Note that the minimum dwell time for the first mode is chosen to be higher since for this example, the approximation will be focused more on mode  than mode . The input  used for simulation is an array of white Gaussian noise. The simulation time interval used is  \footnote{Recall that the method is based on matching the coefficients of the Taylor series expansion for  around , hence the simulation time horizon should be chosen ``small enough''. It should be noted that coming up with a priori error bounds for the moment matching problem is challenging even for the linear case \cite{antoulas}. Consequently, the matter of up to which time horizon the method gives a ``good'' approximation is an open problem, and for now, it can be decided by a posteriori experiments related to the specific problem at hand.}. For the nice selection  given as

an approximation LSS  of order  is acquired which is a -partial realization of . Note that, from the set , it can be seen that the approximation is desired to be focused more on mode  than mode . In Fig. \ref{fig:example1},  plots are shown for comparison of the outputs of  and  for random switching sequences  with given properties. It can be seen from Fig. \ref{fig:example1}, whenever the first operating mode is mode  and mode  operates much more than mode  in total time horizon, the approximation is better. Last point to mention about this example is that the same simulation is ran for  hundred times with random switching sequences with the given properties, for the case when the first operating mode is mode . The best fit rates (BFRs) for each simulation is calculated according to the following (\cite{ljung}, \cite{toth2012})

and mean of the BFRs over these  simulations is acquired as , whereas the best and worst acquired BFR is  and  respectively. The mean of BFR values over  simulations for this example implies that the method yields a good approximation for such a system  in the given time horizon.
\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{subplot_fig.png}
	\caption{The response  (in blue) of the original LSS  of order  and the response  (in red) of its -partial realization  of order  for various switching sequences. The switching sequences  for each plot are as follows:  ,  ,  ,  ,  ,  }
	\label{fig:example1}
\end{figure}

The procedure is also applied to get a reduced order approximation to an LSS whose local modes are unstable. The original LSS used in this case is an LSS of the form  with ,  and . The resulting reduced order model  is a -partial realization of  of order . Note that the precise number of matched Markov parameters of the form  is equal to the number of words in the set , and it is given by

The same parameters in the first example are used with the exception of minimum dwell time for both modes being  and the simulation time horizon being . Again the output  of the original system  and the output  of the reduced order system  are simulated for  random switching sequences and input trajectories. The mean of the BFRs for this example is ; whereas, the best acquired BFR is  and the worst . The outputs  and  of the most successful simulation for this example are illustrated in Fig.~\ref{fig:example2}.
\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{bestfit2_small.eps}
	\caption{The response  of the original LSS  of order  and the response  of the reduced order approximation LSS  of order .}
	\label{fig:example2}
\end{figure}



















\section{Conclusion}
\label{sec:conc}

Two moment matching procedures for model reduction of continuous time LSSs has been given. The first method is the direct analogue of the moment matching approaches in the linear case, for LSSs. The second method relies on the nice selections of some desired vectors in the reachability or observability space of an LSS. The notion of nice selections gives flexibility to the user of the procedure in the following sense: It is possible to focus the approximation on some preferred local modes more than the others. It has been proven that with this procedure, as long as a certain criterion is satisfied, it is possible to acquire at least one reduced order approximation to the original LSS whose Markov parameters related to the specific nice selection are matched with the original one's. Finally, it has been shown that nice selections can be used for matching the input - output behavior of an LSS with another one of possibly lower order, for a specific switching sequence. Discovering the relationship between a set of switching sequences with nice selections for continuous time LSSs would be a potential future research topic since it would solve the problem of approximation or minimization for restricted switching dynamics.







\appendices

\section{Proof of Theorem \ref{lem:nice_selections}}
To present the proof of Theorem \ref{lem:nice_selections} we will introduce an ordering on  as follows:

\begin{Definition} \label{def:lexico}
	\emph{(Ordering on ).} Suppose that . Let the map  be defined as follows:
	
	where , , .
	Then an \emph{ordering}  on the elements of  can be defined as follows: For any two words , if , then . 
\end{Definition}

Intuitively, this ordering states that  if  is bigger than  when the words  are interpreted as integer numbers in the basis . Note that for any ,   implies , and  implies  .

\begin{proof}[Proof of Theorem \ref{lem:nice_selections}]
\emph{i)} Let  denote the matrix

where  denotes the cardinality of the set ;  and  with respect to the ordering in Definition \ref{def:lexico}.

In this part of the proof,  of  is assumed to be zero for simplicity in notation, note that the proof can easily be modified for the case when  is nonzero. Since  is assumed to be minimal, for any , there exists  linearly independent columns of . Suppose these columns are picked in the following manner: Scanning through the columns of  from left to right, choose the first  columns linearly independent from the preceding columns. Our claim is that, this method would yield a nice column selection. To prove the theorem, we claim that if  is an element of the selection defined,  must also be an element i.e., if , . We prove this claim by contradiction. Suppose the columns are chosen in this way and for a ,  and , the th column of  is an element of this selection while the th column of  is not. This means  is a linear combination of the columns of  preceding it while  is not. Let  denote the columns of  which precede the column  and  with  denote the columns of  which precede the column . Note that for some 

Thus the column  can be written as

and since all the vectors  precede the column , each of them can also be written as a linear combination of the columns  which precede . That means for some , , , \eqref{eq:proof_nice_lin_indep1} can be rewritten as

i.e., the column  is a linear combination of its preceding columns. This contradicts our assumption and concludes the proof of the reachability part.

\emph{ii)} This part is the dual of part \emph{i}.
\end{proof}











\ifCLASSOPTIONcaptionsoff
  \newpage
\fi









\bibliographystyle{plain}
\bibliography{./bare_jrnl}















\end{document}
