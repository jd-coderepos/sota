To evaluate the proposed method, we conduct extensive experiments on four popular benchmarks: ICDAR 2015~\cite{karatzas2015icdar}, Rotated ICDAR 2013~\cite{karatzas2013icdar}, CTW1500~\cite{liu2019curved} and Total-Text~\cite{ch2017total}. We first perform ablation studies to investigate the effectiveness of our approach. Then, we compare our model with state-of-the-art methods in terms of performance and inference speed across various challenging spotting scenarios including oriented and curved text appearance. In particular, we make quantitative and qualitative comparisons between our method with ABCNet v2 and TESTR, which are representative methods of two-stage and single shot text spotting paradigms, respectively. 


\begin{comment}
\begin{table*}[!t]
\renewcommand{\arraystretch}{1.1}
  \caption{Comparison between \textbf{SRSTS} and \textbf{SRSTS v2} by Ablation study on CTW1500 and Total-Text. `Sampling-based': the text detection is performed based on the sampled points by the sampling module in \textbf{SRSTS v2}. `DTE': the deformable transformer encoder is employed in Feature Extractor. `SA': self-attention operation is used in Recognition Head of \textbf{SRSTS v2} to capture the long-range dependencies among sampled points. `P’, `R’, `F’ represent `Precision', `Recall' and `F-measure' respectively. `None' and `Full' are two metrics for measuring the end-to-end performance in terms of F-measure. `None’ represents the performance without using lexicon while ‘Full’ corresponds the performance using the lexicon containing all words appearing in the test set.}
  \label{tab:ablative}
  \centering
\begin{tabular}{l|ccc|ccccc|cccccc}
    \toprule
\multirow{3}{*}{Method} & \multicolumn{3}{c|}{\multirow{2}{*}{Model Components}} &\multicolumn{5}{c|}{CTW1500} & \multicolumn{6}{c}{Total-Text}\\
    \cmidrule(lr){5-9}
    \cmidrule(lr){10-15}
& & & &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c|}{E2E}&\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E}  &\multirow{2}*{FPS }\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-9}
    \cmidrule(lr){10-12}
    \cmidrule(lr){13-14}
&Sampling-based & DTE & SA& P&R&F& None &Full& P&R&F& None &Full& \\
\midrule
SRSTS~\cite{wu2022decoupling} & & &  & 88.92&83.30 &86.02 &55.59 &78.06 &91.99 & 82.96 &87.24&78.80& 86.33&18.74  \\
    \midrule
SRSTS-sampling  &  & & & 91.38 & 84.04&87.56& 56.98 & 82.29 & 92.13  &83.97 &87.86&79.75&87.05 & \textbf{20.22}  \\
    SRSTS-DTE  &     &  & &\textbf{91.70}&84.23&87.80&59.57&82.88 &92.42 &86.52 & 89.37 &81.37 & 87.29& 13.44  \\
    \midrule 
\textbf{SRSTS v2}  &    &  &   &90.53 & \textbf{86.46}& \textbf{88.45}&\textbf{61.24} & \textbf{83.54 }&\textbf{93.30}& \textbf{86.74}&
\textbf{89.90} &\textbf{82.05}& \textbf{88.05}&12.86 \\
\bottomrule
  \end{tabular}
\end{table*}
\end{comment}
\begin{table*}[!t]
\renewcommand{\arraystretch}{1.1}
  \caption{Comparison between \textbf{SRSTS} and \textbf{SRSTS v2} by ablation study on CTW1500 and Total-Text. `Sampling-based': the text detection is performed based on the sampled points by Sampling Module in \textbf{SRSTS v2}. `DTE': the deformable transformer encoder is employed in Feature Extractor. `SA': self-attention operation is used in Recognition Head of \textbf{SRSTS v2} to capture the long-range dependencies among sampled points. `P’, `R’, `F’ represent `Precision', `Recall' and `F-measure' respectively. `None' and `Full' are two metrics for measuring the end-to-end performance in terms of F-measure. `None’ represents the performance without using lexicon while ‘Full’ corresponds the performance using the lexicon containing all words appearing in the test set.}
  \label{tab:ablative}
  \centering
\begin{tabular}{l|ccc|ccccc|cccccc}
    \toprule
\multirow{3}{*}{Method} & \multicolumn{3}{c|}{\multirow{2}{*}{Model Components}} &\multicolumn{5}{c|}{CTW1500} & \multicolumn{5}{c}{Total-Text}\\
    \cmidrule(lr){5-9}
    \cmidrule(lr){10-14}
& & & &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c|}{E2E}&\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-9}
    \cmidrule(lr){10-12}
    \cmidrule(lr){13-14}
&Sampling-based & DTE & SA& P&R&F& None &Full& P&R&F& None &Full\\
\midrule
SRSTS~\cite{wu2022decoupling} & & &  & 88.92&83.30 &86.02 &55.59 &78.06 &91.99 & 82.96 &87.24&78.80& 86.33  \\
    \midrule
SRSTS-sampling  &  & & & 91.38 & 84.04&87.56& 56.98 & 82.29 & 92.13  &83.97 &87.86&79.75&87.05   \\
    SRSTS-DTE  &     &  & &\textbf{91.70}&84.23&87.80&59.57&82.88 &92.42 &86.52 & 89.37 &81.37 & 87.29 \\
    \midrule 
\textbf{SRSTS v2}  &    &  &   &90.53 & \textbf{86.46}& \textbf{88.45}&\textbf{61.24} & \textbf{83.54 }&\textbf{93.30}& \textbf{86.74}&
\textbf{89.90} &\textbf{82.05}& \textbf{88.05}\\
\bottomrule
  \end{tabular}
\end{table*}
 \subsection{Experimental Setup}


\smallskip\noindent\textbf{Evaluation benchmarks.}
Four challenging benchmarks are used in the evaluation. 1) ICDAR 2015 contains 1000 training images and 500 testing images. It is annotated with quadrangles and work-level text transcriptions and provides 3 lexicons named ‘Strong’, ‘Weak’, and ‘Generic’ for evaluation. 
2) ICDAR 2013 is a regular text benchmark that contains 229 training images and 223 testing images. In typical experiments~\cite{liao2020mask, huang2022swintextspotter,kittenplon2022towards}, this dataset is rotated at a specific angle for more challenging evaluation.
3) CTW1500 contains 1000 training images and 500 testing images. It is a line-level annotated scene text dataset that contains arbitrary-shaped text instances. A `Full' lexicon is provided which includes all words in the testing set. 
4) Total-Text contains 1255 training images and 300 testing images. It is annotated with polygons and word-level transcriptions. A `Full' lexicon is also provided for evaluation.


\smallskip\noindent\textbf{Datasets for joint training.}
Following the previous methods~\cite{qiao2020mango,liu2021abcnet, zhang2022text,wu2022decoupling}, we combine the following datasets for joint training: Synthtext~\cite{gupta2016synthetic} which is a synthetic dataset containing 800k images; Bezier Curve Synthetic Dataset~\cite{liu2020abcnet} that contains 90k synthetic straight text images and 50k curved text images; COCO-Text~\cite{veit2016coco} which a real-world dataset comprising 63686 images; ICDAR 2017 MLT~\cite{nayef2017icdar2017} that is a multi-language scene text dataset, and ICDAR 2019 ArT~\cite{chng2019icdar2019} containing 5,603 training images. The joint training dataset is a mixture of Synthtext, Bezier Curve Synthetic Dataset, COCO-Text, ICDAR 2017 MLT, ICDAR 2019 ArT, ICDAR 2015, and Total-Text with the sampling ratio 1:1:1:1:2:1:2 correspondingly.


\smallskip\noindent\textbf{Implementation details.}
\label{sec:training}
The training process consists of a warm-up pre-training stage, a joint pre-training stage, and a fine-tuning stage. The model is first optimized in the warm-up pre-training stage on synthetic datasets including Synthtext and Bezier Curve Synthetic Dataset for 300,000 steps of gradient descent, then it is trained with joint training set for another 300,000 steps and finally fine-tuned on the training set of the target benchmark to be evaluated for 30,000 steps.  We use the same data augmentation as \textbf{SRSTS}~\cite{wu2022decoupling} to train our model. In detail, we resize the input image with a randomly selected scale from 0.4 to 1.7 and keep the aspect ratio unchanged.  To handle rotated text well, we also randomly rotate the input image with an angle in the range of [-10,10]. We randomly crop patches from the input image and resize the longer side to the size of 640 and pad the resized image to  for effective training. In addition, random blur and color jitter are also used. In the inference stage, we resize the longer side of input image to be 1920, 1120, and 704 for ICDAR 2015, Rotated ICDAR 2013, and CTW1500 respectively and the shorter side to be 640 for Total-Text.

We use SGD to optimize our model with the initial learning rate of 1e-3 for the warm-up pre-training stage and the joint pre-training stage. The initial learning rate for the final fine-tuning stage is set to be 1e-5 for the word-level annotated benchmarks and 1e-4 for the line-level annotated benchmark, respectively. We set the weight decay to be 0.0001 and momentum to be 0.9, and delay the learning rate with a `poly’ learning rate strategy~\cite{chen2017deeplab}. Our model is trained with a batch size of 16. 

\begin{comment}
\begin{table*}[!t]
\renewcommand{\arraystretch}{1.1}
  \caption{Effect of varying the number of deformable transformer encoder layers in Feature Extractor of \textbf{SRSTS v2}.}
  \label{tab:encoder}
  \centering
  \begin{tabular}{c|ccccc|cccccc}
    \toprule
    \multirow{3}*{\#Layers} &\multicolumn{5}{c|}{CTW1500} & \multicolumn{6}{c}{Total-Text}\\
\cmidrule(lr){2-6}
    \cmidrule(lr){7-12}
   &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c|}{E2E} &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E} & \multirow{2}*{FPS}\\
   \cmidrule(lr){2-4}
   \cmidrule(lr){5-6}
   \cmidrule(lr){7-9}
    \cmidrule(lr){10-11}
&P&R&F & None & Full&P&R&F & None & Full\\
    \midrule
0 & 91.38 & 84.04&87.56& 56.98 & 82.29 &
 92.13  &83.97 &87.86&79.75&87.05 & \textbf{20.22}\\
    2 & 91.09&\textbf{84.45}&87.64&56.76&82.28&\textbf{93.97} & 85.20 & 89.37& 80.07&86.76 &17.02\\
    4 & \textbf{91.79}&83.97&87.70&57.31&82.81&92.89 &86.43 & \textbf{89.55} &81.00 & 87.05 &15.22\\ 
    6 &91.70&84.23&\textbf{87.80}&\textbf{59.57}&\textbf{82.88}&92.42 &\textbf{86.52} & 89.37 &\textbf{81.37} & \textbf{87.29} & 13.44\\  
    \bottomrule
  \end{tabular}
\end{table*}
\end{comment}
\begin{table*}[!t]
\renewcommand{\arraystretch}{1.1}
  \caption{Effect of varying the number of deformable transformer encoder layers in Feature Extractor of \textbf{SRSTS v2}.}
  \label{tab:encoder}
  \centering
  \begin{tabular}{c|ccccc|ccccc}
    \toprule
    \multirow{3}*{\#Layers} &\multicolumn{5}{c|}{CTW1500} & \multicolumn{5}{c}{Total-Text}\\
\cmidrule(lr){2-6}
    \cmidrule(lr){7-11}
   &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c|}{E2E} &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E}\\
   \cmidrule(lr){2-4}
   \cmidrule(lr){5-6}
   \cmidrule(lr){7-9}
    \cmidrule(lr){10-11}
&P&R&F & None & Full&P&R&F & None & Full\\
    \midrule
0 & 91.38 & 84.04&87.56& 56.98 & 82.29 &
 92.13  &83.97 &87.86&79.75&87.05\\
    2 & 91.09&\textbf{84.45}&87.64&56.76&82.28&\textbf{93.97} & 85.20 & 89.37& 80.07&86.76\\
    4 & \textbf{91.79}&83.97&87.70&57.31&82.81&92.89 &86.43 & \textbf{89.55} &81.00 & 87.05\\ 
    6 &91.70&84.23&\textbf{87.80}&\textbf{59.57}&\textbf{82.88}&92.42 &\textbf{86.52} & 89.37 &\textbf{81.37} & \textbf{87.29}\\  
    \bottomrule
  \end{tabular}
\end{table*} \begin{comment}
\begin{table*}[!t]
\renewcommand{\arraystretch}{1.1}
  \caption{Effect of varying the number of the self-attention layers in Recognition Head of \textbf{STSTS v2}.}
  \label{tab:sa}
  \centering
  \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{tabular}{c|ccccc|cccccc}
\toprule
    \multirow{3}*{\#Layers} 
     &\multicolumn{5}{c|}{CTW1500} & \multicolumn{6}{c}{Total-Text} \\
\cmidrule(lr){2-6}
    \cmidrule(lr){7-12}
     &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c|}{E2E}&\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E}&\multirow{2}*{FPS} \\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-6}
    \cmidrule(lr){7-9}
    \cmidrule(lr){10-11}
&P&R&F & None & Full &P&R&F & None & Full& \\
    \midrule
    0&\textbf{91.70}&84.23&87.80&59.57&82.88&92.42 &86.52 & 89.37 &81.37 & 87.29 & \textbf{13.44}\\
    2 &91.00&86.16&\textbf{88.52}&60.96&83.20&92.43 & 86.32  &89.27 & 81.81 &87.56 & 13.14
    \\
    4 &90.53 & \textbf{86.46}& 88.45&61.24 & \textbf{83.54 }& 93.30& 86.74&
89.90 &\textbf{82.05}& \textbf{88.05}&12.86\\ 
    6 &91.22&85.08&88.05&\textbf{62.02}&83.16& \textbf{93.46}&\textbf{86.95} & \textbf{90.08}&81.96&87.54 &12.55\\  
    \bottomrule
  \end{tabular}
\end{table*}
\end{comment}

\begin{table*}[!t]
\renewcommand{\arraystretch}{1.1}
  \caption{Effect of varying the number of the self-attention layers in Recognition Head of \textbf{STSTS v2}.}
  \label{tab:sa}
  \centering
  \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{tabular}{c|ccccc|ccccc}
\toprule
    \multirow{3}*{\#Layers} 
     &\multicolumn{5}{c|}{CTW1500} & \multicolumn{5}{c}{Total-Text} \\
\cmidrule(lr){2-6}
    \cmidrule(lr){7-11}
     &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c|}{E2E}&\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E} \\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-6}
    \cmidrule(lr){7-9}
    \cmidrule(lr){10-11}
&P&R&F & None & Full &P&R&F & None & Full\\
    \midrule
    0&\textbf{91.70}&84.23&87.80&59.57&82.88&92.42 &86.52 & 89.37 &81.37 & 87.29\\
    2 &91.00&86.16&\textbf{88.52}&60.96&83.20&92.43 & 86.32  &89.27 & 81.81 &87.56
    \\
    4 &90.53 & \textbf{86.46}& 88.45&61.24 & \textbf{83.54 }& 93.30& 86.74&
89.90 &\textbf{82.05}& \textbf{88.05}\\ 
    6 &91.22&85.08&88.05&\textbf{62.02}&83.16& \textbf{93.46}&\textbf{86.95} & \textbf{90.08}&81.96&87.54\\  
    \bottomrule
  \end{tabular}
\end{table*} 
\subsection{Ablation Study}

In this section, we conduct ablation studies on CTW1500 and Total-Text, which are line-level annotated and world-level annotated benchmarks respectively, to investigate the effectiveness of core components of \textbf{SRSTS v2}. We perform training with the same training protocol described in Section~\ref{sec:training} for a fair comparison.




\subsubsection{Comparison between \textbf{SRSTS} and \textbf{SRSTS v2}}
\label{sec: comparison}

Compared with the prior version \textbf{SRSTS}~\cite{wu2022decoupling}, \textbf{SRSTS v2} makes two improvements. First, it improves the spotting scheme by redesigning the detection module. Unlike \textbf{SRSTS} employing YOLACT for text detection, \textbf{SRSTS v2} performs both the text detection and recognition based on the sampled representative feature points by Sampling Module, which enables the collaborative optimization between two tasks and thereby allows for mutual enhancement between them. Second, \textbf{SRSTS v2} employs the deformable transformer encoder in Feature Extractor to improve the feature learning and adopt self-attention operation in Recognition Head  to capture the long-range dependencies among the sampled points. The experimental results of ablation study are presented in Table~\ref{tab:ablative}, which shows the performance gain from each improvement.

The performance gain between \textbf{SRSTS} and \textbf{SRSTS-sampling} shows the superiority of the detection scheme of \textbf{SRSTS v2} over that of \textbf{SRSTS}. \textbf{SRSTS} performs detection with YOLACT by instance segmentation and predicts the bounding box based on the anchor point. In contrast, \textbf{SRSTS v2} predicts the sampled points to bridge the gap between the anchor point and the detection boundaries, which is potentially more accurate than the detection scheme of YOLACT. Moreover, the collaborative optimization between detection and recognition leads to more effective sampling and thus results in better performance in both detection and recognition. For instance, the F-measure of detection increases 1.54\% and 0.62\% on CTW1500 and Total-Text respectively. Meanwhile, the recognition performance is improved by  in terms of `Full' metric on CTW1500.





\begin{table}
\renewcommand{\arraystretch}{1.1}
\caption{Effect of the recognition branch on the detection task to validate the advantage of collaborative optimization. `Detection-only' is the ablated variant of \textbf{SRSTS v2} by removing the recognition branch.}  
\label{tab:with_rec}
\centering
\begin{tabular}{l|ccc|ccc}
\toprule
 \multirow{3}*{Method} & \multicolumn{3}{c|}{CTW1500}&\multicolumn{3}{c}{Total-Text}\\
\cmidrule(lr){2-4}
 \cmidrule(lr){5-7}
 &\multicolumn{3}{c|}{Detection}&\multicolumn{3}{c}{Detection}\\
\cmidrule(lr){2-4}
 \cmidrule(lr){5-7}
 & P & R & F & P & R & F \\
\midrule
  Detection-only&88.77  &\textbf{86.76} &87.75 & 90.40 &\textbf{87.44}  &88.89\\ 
 \textbf{SRSTS v2}& \textbf{90.53} & 86.46& \textbf{88.45}&\textbf{93.30}  &86.74&\textbf{89.90} \\ 
\bottomrule
\end{tabular}
\end{table} 
We can also observe the performance gain from the enhancement of feature learning by employing the deformable transformer encoder and the modeling of long-range dependencies by self-attention operation in Table~\ref{tab:ablative}, respectively. In particular, the end-to-end F-measure for recognition (`None' metric) is improved by 4.26\% and 2.30\% on CTW1500 and Total-Text respectively. We also study the effect of varying the number of deformable transformer encoder layers and self-attention layers, which are listed in Table~\ref{tab:encoder} and Table~\ref{tab:sa}, respectively. Based on these results, we set the number of deformable transformer encoder layers in Feature Extractor and the number of self-attention layers in Recognition Head  to be 6 and 4, respectively.


\subsubsection{Effectiveness of Collaborative Optimization}
\label{collaborative_optimization}
In our \textbf{SRSTS v2}, both text detection and recognition are based on the sampled representative points from Sampling Module. In turn, the Sampling Module is optimized by joint supervision from both detection and recognition, which enables the collaborative optimization between two tasks and thereby yields mutual enhancement. 

The experimental comparisons between `SRSTS-sampling' and `SRSTS' in Table~\ref{tab:ablative} already show that the sampling-based detection strategy in \textbf{SRSTS v2} not only improves the detection performance by a large margin over \textbf{SRSTS} which employs YOLACT for detection, but also boosts the recognition performance substantially. These results reveal that the supervision from the detection branch guides the learning of Sampling Module and potentially yields higher-quality sampling, which can enhance the recognition performance.


\begin{figure*}[!t] 
\renewcommand{\arraystretch}{1.1}
   \centering
   \centerline{\includegraphics[width=0.8\textwidth]{sampling_v2.pdf}}
  \caption{The visualization of unsupervised sampling and weakly supervised sampling. While the unsupervised sampling can generally produce feasible but somewhat disordered sampled points, weak supervision is sufficient to guide the sampling module to generate a proper distribution of sampling points for detection and recognition along the centerline of the text. The images for showing the results are shaded to visualize the sampled points more clearly.}
  \label{fig:comparison_sampling_mode}
\end{figure*}

\begin{table*}[t]
\renewcommand{\arraystretch}{1.1}
  \caption{Comparison among different supervision modes for learning the sampling module of \textbf{SRSTS v2}.}
  \label{tab:sampling}
  \centering
\begin{tabular}{l|ccccc|ccccc} 
    \toprule
    \multirow{3}*{Supervision mode}&\multicolumn{5}{c|}{CTW1500}&\multicolumn{5}{c}{Total-Text}\\
\cmidrule(lr){2-6}
   \cmidrule(lr){7-11}
   &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c|}{E2E} &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E}\\
\cmidrule(lr){2-4}
   \cmidrule(lr){5-6}
   \cmidrule(lr){7-9}
   \cmidrule(lr){10-11}
   &P&R&F & None & Full&P&R&F & None & Full\\
    \midrule
    Unsupervised sampling &90.20 &85.31 & 87.69& 56.98& 82.75&91.09 &86.37 &88.67 &81.86 &86.92\\ 
    Weakly supervised sampling &90.53 & \textbf{86.46}& \textbf{88.45}&\textbf{61.24} & \textbf{83.54 }&93.30& 86.74&89.90 &\textbf{82.05}& \textbf{88.05} \\ 
    Fully supervised sampling &\textbf{91.46} &85.23 & 88.23&60.08 &82.69 &\textbf{94.19}&  \textbf{86.88}& \textbf{90.39}&81.52&87.54\\ 
    \bottomrule
  \end{tabular}
\end{table*}
 

\begin{table*}[t]
\renewcommand{\arraystretch}{1.1}
  \caption{Validation of feasibility of learning with limited detection supervision.  is learned with only synthetic data while  is trained with fully-annotated synthetic data and text-only annotated real-world data.}
  \label{tab:weak}
  \centering
  \begin{tabular}{l|ccccc|ccccc}
    \toprule
    \multirow{3}*{Method} &\multicolumn{5}{c|}{CTW1500}&\multicolumn{5}{c}{Total-Text}\\
    \cmidrule(lr){2-6}
    \cmidrule(lr){7-11}
    & \multicolumn{3}{c}{Detection}& \multicolumn{2}{c|}{E2E} &\multicolumn{3}{c}{Detection} & \multicolumn{2}{c}{E2E}\\
\cmidrule(lr){2-4}
    \cmidrule(lr){5-6}
    \cmidrule(lr){7-9}
    \cmidrule(lr){10-11}
& P& R&F &None&Full& P& R&F& None & Full\\
    \midrule
     &45.45 &51.64 &48.35 &25.04 & 40.30 &  77.64 &47.70  &  59.09&  43.71& 44.12   \\
     & \textbf{93.02} &72.36 &81.40&56.08  &76.63  & 83.34 & 64.57 & 72.76 &69.26 & 75.57   \\
\textbf{SRSTS v2} &90.53 & \textbf{86.46}& \textbf{88.45}&\textbf{61.24} & \textbf{83.54 }&\textbf{93.30}& \textbf{86.74}&
\textbf{89.90} &\textbf{82.05}& \textbf{88.05}\\
    \bottomrule
  \end{tabular}
\end{table*}

 
To further investigate the effect of supervision from the recognition branch on Sampling Module, we conduct another ablation study by removing the recognition branch from \textbf{SRSTS v2} and compare the detection performance between the ablated variant notated  as `Detection-only' and the intact \textbf{SRSTS v2}. As shown in Table~\ref{tab:with_rec}, the detection performance decreases on both two benchmarks, especially Total-Text.These results demonstrate distinctly the positive effect of the recognition branch on text detection. On the one hand, the recognition results can help filter out the false positives of detection results. On the other hand, the supervision from the recognition branch can guide the optimization of both Sampling Module as well as Feature Extractor, shared by detection and recognition, which leads to better detection results.

Above ablation studies show that the collaborative optimization between the text detection and recognition allows for mutual enhancement between each other, which is a prominent advantage of more integrated spotting framework of \textbf{SRSTS v2} compared with the prior version \textbf{SRSTS}. 





\subsubsection{Effect of Different Supervision Modes for Sampling Module}
In this set of experiments, we conduct ablation study to study the effect of different supervision modes ( in Equation~\ref{eqn:sampling_loss}) for Sampling Module of \textbf{SRSTS v2}. To be specific, three supervision modes are performed on Sampling Module respectively for comparison: 1) unsupervised sampling in which  is not applied in any training stage, 2) weakly supervised sampling which performs  only in the warm-up pre-training stage on the synthetic dataset, and 3) fully supervised sampling which performs  throughout all three training stages including warm-up, joint training and fine-tuning stages. 

Table~\ref{tab:sampling} presents the experimental results on both CTW1500 and Total-Text benchmarks. While the unsupervised sampling performs worst among all three modes, its performance for both detection and recognition is not far from that of other two modes with supervision signals, which is indeed encouraging. It implies that the supervision signal for Sampling Module is not critical to the performance, which is reasonable since text detection and recognition only require the sampled points to be distributed uniformly in the text region and involve all characters, but with no strict constraints for the location of sampled points.


\begin{table*}[t]
\renewcommand{\arraystretch}{1.1}
  \caption{Quantitative results on ICDAR 2015.  Methods marked with `*' are trained with character-level annotations. `S' (strong) means a customized lexicon of 100 words, including the groundtruth, is given for each image. `W' (weak) implies a lexicon that includes all words that appear in the test set is provided. `G' (generic) denotes a generic lexicon with 90k words.}
  \label{tab:ic15}
  \centering
  \begin{tabular}{l|l|ccccccccc}
    \toprule
   \multirow{2}*{Lexicon}& \multirow{2}*{Model} & \multicolumn{3}{c}{Detection}& \multicolumn{3}{c}{E2E} &\multicolumn{3}{c}{Word Spotting}\\
\cmidrule(lr){3-5}
   \cmidrule(lr){6-8}
   \cmidrule(lr){9-11}
   & &P&R&F &S&W & G &S&W & G\\
    \midrule
    \multirow{13}{*}{\makecell[l]{Official lexicon}}
    &TextNet~\cite{sun2018textnet} &89.42& 85.41&  87.37& 78.66&  74.90& 60.45 &82.38 &  78.43 &  62.36\\
     &FOTS~\cite{liu2018fots} & 91.00&85.17  & 87.99 &  81.09&  75.90&  60.80 & 84.68 & 79.32 & 63.29\\
     &TextDragon~\cite{feng2019textdragon} &  92.45 &83.75 & 87.88 & 82.54&  78.34&  65.15& 86.22&  81.62 & 68.03\\
     &Qin \textit{et al.}~\cite{qin2019towards}  &89.36& 85.75 & 87.52 & 83.38 & 79.94 &  67.98 &- &- &-\\
     &TextPerceptron~\cite{qiao2020text} & 92.30 & 82.50 & 87.10 & 80.50 & 76.60 & 65.10 & 84.10 & 79.40 & 67.90\\
     &PGNet~\cite{wang2021pgnet}  & 91.80 & 84.80& 88.20 & 83.30 & 78.30 & 63.50 & - & - & -\\
     &Boundary~\cite{wang2020all} & 89.80 & \textbf{87.50} & 88.60 & 79.70 & 75.20 & 64.10 & - & - & -\\
     &PAN++~\cite{wang2021pan++} &91.40& 83.90& 87.50 & 82.70 & 78.20 & 69.20& - & - & -  \\
     \cmidrule{2-11}
     &SRSTS (ours)~\cite{wu2022decoupling} &\textbf{96.05}& 81.96 &  88.44 & 82.83  & 80.70  &  69.47 & 87.37  &84.75 & 72.33\\   
     & \textbf{SRSTS v2} (ours) &93.43 &85.60 & \textbf{89.35}& \textbf{83.97}&\textbf{81.16} &\textbf{71.66} &\textbf{88.37} &\textbf{85.20} &\textbf{75.48}\\
    \cmidrule{2-11}
     & CharNet\cite{xing2019convolutional} &91.15 & 88.30 &  89.70 & 80.14&  74.45&  62.18& -&- &-\\
     &CRAFTS~\cite{baek2020character}  &89.00  & 85.30& 87.10&  83.10 & 82.10 &74.90 &- &- & - \\
     &MANGO~\cite{qiao2020mango} & -&  - & - & 81.80 &78.90&  67.30 & 86.40 & 83.10&  70.30  \\
     \midrule
    \multirow{9}{*}{\makecell[l]{Specific lexicon\\ provided by~\cite{liao2019mask}}}
     &Mask Textspotter v2~\cite{liao2019mask}  & 86.60& 87.30& 87.00& 83.00& 77.70& 73.50& 82.40& 78.10& 73.60\\
     &Mask Textspotter v3~\cite{liao2020mask}  & -& -& -& 83.30& 78.10& 74.20&83.10&79.10& 75.10\\
     &MANGO~\cite{qiao2020mango} &-& -& -& 85.40& 80.10& 73.90&85.20&81.10& 74.60\\ 
     &ABCNet v2~\cite{liu2021abcnet} & 90.40&86.00& 88.10& 82.70& 78.50& 73.00&-&-& -\\
     &TESTR~\cite{zhang2022text} & 90.30 & \textbf{89.70} & \textbf{90.00} & 85.20 & 79.40& 73.60 &- & - & -\\
     &SwinTextspotter~\cite{huang2022swintextspotter}&-& -& -& 83.90 &77.30  &70.50&-& -& - \\
     &TTS~\cite{kittenplon2022towards}  &- &- & -& 85.20 &81.70 &77.40 &85.00 &81.50& 77.30\\ &GLASS~\cite{ronen2022glass}&-&- &- & 84.70& 80.10 & 76.30&\textbf{86.80}& 82.50& \textbf{78.80}\\
    \cmidrule{2-11}
     &SRSTS (ours)~\cite{wu2022decoupling}  &\textbf{96.05}& 81.96 &  88.44 & 85.63 & 81.74 &  74.51&  85.84 & 82.61 & 76.82\\ 
&\textbf{SRSTS v2} (ours) &93.43 &85.60 & 89.35& \textbf{86.48}&\textbf{82.41} &\textbf{78.19} &86.72 &\textbf{83.17} &78.73\\
\bottomrule
  \end{tabular}
\end{table*} 
 

\begin{table*}[h]
\renewcommand{\arraystretch}{1.1}
  \caption{Quantitative results on Rotated ICDAR 2013.  Methods marked with `*' are trained with character-level annotations. `P', `R' and `F' denote `Precision', `Recall' and `F-measure' respectively.}
  \label{tab:ic13}
  \centering
  \begin{tabular}{l|cccccc|cccccc}
    \toprule
   \multirow{3}*{Model} & \multicolumn{6}{c|}{Rotation Angle: }& \multicolumn{6}{c}{Rotation Angle: }\\
   \cmidrule(lr){2-7}
   \cmidrule(lr){8-13}
      & \multicolumn{3}{c}{Detection}& \multicolumn{3}{c|}{E2E} & \multicolumn{3}{c}{Detection}& \multicolumn{3}{c}{E2E}\\
     \cmidrule(lr){2-4}
    \cmidrule(lr){5-7}
    \cmidrule(lr){8-10}
    \cmidrule(lr){11-13}
    &P &R &F &P &R &F &P &R & F &P &R &F \\
    
    \midrule
    CharNet R-50~\cite{xing2019convolutional} & -&- & 57.20& -& -& 33.90& - & - &58.80 &- &- &9.30\\ 
    Mask Textspotter v2~\cite{liao2019mask} & 64.80 
&59.90 & 62.20 &66.40& 45.80 & 54.20 & 70.50  & 61.20 & 65.50  &68.20  &48.30  &56.60\\ 
    Mask Textspotter v3~\cite{liao2020mask} & 91.60 &77.90  &84.20 &\textbf{88.50} & 66.80  & 76.10  & 90.70   & 79.40  & 84.70   &\textbf{88.50}   &67.60  &76.60\\ 
    SwinTextspotter~\cite{huang2022swintextspotter} & - &-  &- &83.40&72.50 & 77.60& - &-  &-&  84.60 &72.10&77.90 \\ 
    TTS~\cite{kittenplon2022towards} & - &-  &\textbf{89.90} &-&- &  80.10 & - &-  & \textbf{89.70}&-& - & 81.00\\  
    TTS~\cite{kittenplon2022towards} & - &-  &88.80 &-&- &  80.40 & - &-  & 87.60&-& - & 80.10\\  
    \midrule
    \textbf{SRSTS v2} (ours) &\textbf{92.68} & \textbf{85.56}&88.97 &85.36&\textbf{78.80}& \textbf{81.95} &\textbf{92.25} &\textbf{86.02}&89.03 &84.31 &\textbf{78.61} &\textbf{81.36} \\
    \bottomrule
  \end{tabular}
\end{table*}

 
On the other hand, the supervision  is designed to guide Sampling Module to sample uniformly along the centerline of the text instance, which is validated by the performance improvements of both weakly supervised and fully supervised modes over the unsupervised mode. Nevertheless, the performance of these two supervised modes are on par with each other. This is presumably because sampling along the center line of the polygon is just one of optional ways rather than the sole way. Weak supervision is already sufficient to guide the sampling module to generate a proper distribution of points for detection and recognition, as illustrated in Figure~\ref{fig:comparison_sampling_mode}. Thus, we adopt weak supervision for Sampling Module in our \textbf{SRSTS v2}.

\begin{comment}
\wu{We conduct experiments to verify the effectiveness of the proposed weak supervision of the sampling module. We evaluate \textbf{SRSTS v2} with different sampling supervision modes on CTW1500 and Total-Text: unsupervised sampling, weakly supervised sampling, and fully supervised sampling. Unsupervised sampling means the sampling module is only optimized by indirect supervision from the detection and recognition tasks. Weakly supervised sampling means the direct supervision  is only performed during the pre-training stage, and in the following training phases the sampling module is supervised indirectly by the detection and recognition loss. Fully supervised sampling denotes the supervision of the centerline is used throughout the whole training stage.}

\wu{The results are listed in Table~\ref{tab:sampling}. The unsupervised sampling performs poorly on both benchmarks. When evaluated without lexicon, the F-measure of the unsupervised sampling module is 4.26\% lower than the weakly supervised sampling module on CTW1500. In addition, the fully supervised sampling also performs unsatisfactorily. From the results, we notice the fully supervised sampling yields a certain detection precision gain but slight recognition performance degradation. By adopting fully supervised sampling, the end-to-end performance decreases 1.16\% and 0.53\% on CTW1500 and Total-Text in terms of F-measure.}

\wu{The experimental results confirm the mentioned three types of supervision modes for the learning of sampling module are feasible, while weakly supervised sampling performs best. To further analyze the reasons for the performance differences among different supervision strategies, we visualize some examples in Figure~\ref{fig:comparison_sampling_mode}. As shown, the sampled points generated by the unsupervised sampling module locate more randomly than the other two. As a result, it's easier to miss some characters, especially punctuation marks in long text instances. In addition, fully supervised sampling ensures that the sampled points are located on the text centerline, which may be more beneficial to the regression of boundary points. However, fully supervised sampling has limitations in some scenarios since the center line is not always the optimal sampling supervision. For example, as the size of `T' is much larger than the other letters, the centerline of `TEXAS' cannot cover all characters well, leading to the sub-optimal sampling positions ultimately. }
\end{comment}

\subsubsection{Learning with Limited Detection Supervision}
The recognition of our \textbf{SRSTS v2} does not rely on its detection prediction. Benefiting from such decoupling between recognition and detection, \textbf{SRSTS v2} can be potentially trained with only limited detection supervision, which can substantially reduce the annotation cost.
We conduct experiments to investigate the effectiveness of \textbf{SRSTS v2} in such learning setting. Specifically, we provide the recognition supervision on all training data (including the synthetic and real-world datasets) whilst the detection supervision is only performed on the synthetic data whose boundaries can be readily obtained without human annotation. We notate the learned \textbf{SRSTS v2} in such learning setting as . Since the boundary annotation is not provided for the real-world data,  has to match each predicted text instance for a positive anchor point with a text groundtruth by itself for multi-instance images. We use edit distance as the matching metric in our implementation (other matching algorithms are also feasible). Besides, to have a comprehensive comparison, we also evaluate the performance of \textbf{SRSTS v2} learned with only synthetic data denoted as , i.e., the real-world data is not used at all during training.

Table~\ref{tab:weak} presents the comparative performance for both detection and recognition on CTW1500 and Total-Text. We observe that  performs quite poorly for both detection and recognition on two datasets, which implies the large distribution gap between the synthetic data and the real-world data. By contrast, when providing the real-world training data with only recognition supervision,  performs significantly better than  for both detection and recognition. In particular, the recognition performance (`E2E') is even comparable with some classical methods (check Table~\ref{tab:tt} and \ref{tab:ctw}). These impressive results demonstrate the advantage of decoupling recognition from detection: it indeed enables our \textbf{SRSTS v2} to be trained with limited detection supervision. Moreover, the large performance gain for detection from  to  demonstrates the effectiveness of collaborative optimization again that the recognition supervision yields better sampling quality and thereby enhances the detection performance.

\begin{comment}
\wu{The detection and recognition are both based on sampled points guided by anchor points, leading to two merits: 1) Recognition Head of our proposed \textbf{SRSTS v2} can be trained without the precise boundary annotations; 2) it's cheaper to obtain text-only annotation rather than precise boundary points for text instance. And it's feasible for our \textbf{SRSTS v2} to be trained with such limited boundary annotation. To verify the practicability of limited boundary annotation supervision, we conduct experiments on CTW1500 and Total-Text.}

\wu{We adopt a similar training paradigm as recently proposed weakly-supervised spotter~\cite{kittenplon2022towards}. SRSTS v2 is the pre-trained model on synthetic datasets. SRSTS v2 means using fully-annotated synthetic data and text-only annotated real-world data and resumes from SRSTS v2. Since text-only annotated real-world data doesn't provide the precise text region for each image, we use a predefined confidence threshold to select positive anchors from the predicted confidence map in the training stage. The recognition loss only works on the positions of selected positive anchor points. To assign the prediction of each anchor with groundtruth, we match the predicted text to the corresponding groundtruth which has the closest edit distance with it. }

\wu{We evaluate SRSTS v2 and SRSTS v2 with the same evaluation protocol as \textbf{SRSTS v2}. As shown in Table~\ref{tab:weak}, SRSTS v2 performs poorly on CTW1500 and Total-Text. By using text-only annotated real-world data, SRSTS v2 achieves impressive performance on  end-to-end task. In particular, SRSTS v2 achieves 56.08\% and 69.26\% when evaluating without lexicon for end-to-end task on CTW1500 and Total-Text respectively.}\end{comment}

\begin{table*}[t]
  \renewcommand{\arraystretch}{1.1}
  \caption{Quantitative results on Total-Text. Methods marked with `*' are trained with character-level annotations. `L' and `S' denote the length of the longer side and shorter side of input images, respectively. \textbf{SRSTS v2-F} is a faster version of \textbf{SRSTS v2} by removing the deformable transformer encoder layers in Feature Extractor and self-attention layers in Recognition Head. }
  \label{tab:tt}
  \centering
  \begin{tabular}{l|l|ccccccc|cc}
    \toprule
    \multirow{2}*{Model}& 
     \multirow{2}*{Scale}&
     \multicolumn{3}{c}{Detection}& \multicolumn{2}{c}{E2E} &\multicolumn{2}{c|}{Word Spotting} &
     \multirow{2}*{\makecell[c]{FPS\re-evaluated) }}\\
    \cmidrule(lr){3-5}
    \cmidrule(lr){6-7}
    \cmidrule(lr){8-9}
    
& &P&R&F &None& Full &None& Full \\
    \midrule
    TextNet~\cite{sun2018textnet}&L: 920&  68.21& 59.45  &63.53 & 54.02 & -&-&-&-&-\\
    TextDragon~\cite{feng2019textdragon}&-& 85.60 &  75.70&  80.30& 48.80 &74.80&-&-&-&-\\
    TextPerceptron~\cite{qiao2020text}&L: 1350&  88.80&81.80& 85.20& 69.70&78.30&-&-&-&- \\
    Boundary~\cite{wang2020all}&L: 1100& 88.90& 85.00&  87.00& 65.00 &76.10&-&-&-\\
    Qin \textit{et al.}~\cite{wang2020all}&S: 600& 83.30 & 83.40&	83.30& 67.80& -&-&-&4.80&-\\
    ABCNet~\cite{liu2020abcnet}&S: 1000&  -& - & -& 63.74& 77.62& 67.10 &81.14&17.90&14.59 \\
    PGNet-A~\cite{wang2021pgnet} &L: 640 & 85.30&\textbf{86.80}& 86.10 &61.70&- &- &-&\textbf{38.20}& 15.37\\
    ABCNet v2~\cite{liu2021abcnet} &S : 1000&  90.20&84.10& 87.00& 67.89& 79.57& 71.82& 83.39&10.00&9.36\\
    PAN++~\cite{wang2021pan++}&S: 512 & 88.40 & 80.50 &84.20 & 64.90 & 75.70 &- &-&29.20 & 15.31\\
    Mask Textspotter v2~\cite{liao2019mask}&S: 1000& 81.80  &75.40  & 78.50& 65.30& 77.40&-&-&-&-\\ 
    Mask Textspotter v3~\cite{liao2020mask} &S: 1000 & -& -& -&71.20& 78.40&-&-&-&-\\
    CharNet H-57~\cite{xing2019convolutional} & -& 88.60&81.00 &84.60&63.60&-&-&-&-\\
    MANGO~\cite{qiao2020mango} &L: 1600 & -& -&-&72.90 &83.60&-&-&4.30&- \\
    CRAFTS~\cite{baek2020character}&L: 1920 &89.50&85.40 &	87.40&78.70 & -&-&- &-&-\\
TESTR~\cite{zhang2022text} &L: 1600&  \textbf{93.40} & 81.40&  86.90 &69.85& 80.51& 73.30 & 83.90 &5.30&8.20\\
    TTS~\cite{kittenplon2022towards}&-& - &-&-& 75.60 & 84.40& 78.20&  86.30&-&- \\ 
    SwinTextspotter~\cite{huang2022swintextspotter}&S: 1000& - &- & 87.20 &- &- & 72.40 &83.00 & -&-\\
    GLASS~\cite{ronen2022glass} &-&-&-&-&76.60& 83.00&79.90& 86.20&-&-\\ 
    \midrule
    SRSTS (ours)~\cite{wu2022decoupling}& S: 640 &91.99&82.96&87.24&78.80&86.33 &81.52 &90.18&18.74&18.74\\
    \textbf{SRSTS v2-F} (ours) & S: 640& 92.13  &83.97 &87.86&79.75& 87.05 &82.66 &90.89 &20.22 &\textbf{20.22}  \\
    \textbf{SRSTS v2} (ours) &S: 640 & 93.30& 86.74&
\textbf{89.90} &\textbf{82.05}& \textbf{88.05}&\textbf{84.66}&\textbf{91.59} & 12.86& 12.86  \\
    \bottomrule
  \end{tabular}
\end{table*} 
\begin{table}[t]
  \caption{Quantitative results on CTW1500. Methods marked with `*' are trained with character-level annotations.}
  \label{tab:ctw}
  \renewcommand{\arraystretch}{1.1}
  \centering
  \begin{tabular}{l|ccccc}
    \toprule
    \multirow{2}*{Model}& 
     \multicolumn{3}{c}{Detection}& \multicolumn{2}{c}{E2E}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-6}
&P&R&F &None& Full \\
    \midrule
    TextDragon~\cite{feng2019textdragon}&84.50 &82.80 &83.60 &39.70 &72.40\\
    TextPerceptron~\cite{qiao2020text}&87.50 &81.90 &84.60 &57.00 &-\\
    ABCNet~\cite{liu2020abcnet}& -& -& -& 45.20&74.10\\
    ABCNet v2~\cite{liu2021abcnet}& 85.60& 83.80& 84.70& 57.50&77.20\\
    MANGO~\cite{qiao2020mango}& -& -&-&58.90 &78.70 \\
    TESTR~\cite{zhang2022text} & \textbf{92.00}& 82.60& 87.10& 56.00& 81.50\\
    SwinTextspotter~\cite{huang2022swintextspotter}&- & -&88.00 &51.80 &77.00\\
    \midrule
    SRSTS (ours)~\cite{wu2022decoupling}&88.92 &83.30 &86.02 &55.59 &78.06\\
    \textbf{SRSTS v2} (ours)&90.53 & \textbf{86.46}& \textbf{88.45}&\textbf{61.24}&\textbf{83.54}\\
    \bottomrule
  \end{tabular}
\end{table} 
\subsection{Comparison with State-of-the-art Methods}
In this section, we compare our \textbf{SRSTS v2} with the state-of-the-art methods for text spotting. Specifically, we make two sets of comparisons on two types of benchmarks respectively: oriented text benchmarks and curved text benchmarks. Besides, we also evaluate the efficiency of our model by comparing it with other methods in terms of inference speed. Finally, we particularly compare our model with ABCNet v2 and TESTR, which are representative methods for two-stage and single shot spotting paradigms, respectively.

\subsubsection{Spotting Results on Oriented Text Benchmarks}
\smallskip\noindent\textbf{Results on ICDAR 2015.}
Table~\ref{tab:ic15} presents the detailed comparative results on ICDAR 2015. Considering that the character-level annotations provide much more additional information, we make comparisons between methods only using word-level annotations for training to have a fair comparison and list separately the methods using the character-level annotations for reference. In addition, we also divide the methods into two groups in terms of the lexicon that is used for decoding: the official lexicon and the specific lexicon~\cite{liao2019mask}. To compare comprehensively with other methods, we evaluate our \textbf{SRSTS v2} using each of two lexicons respectively.

As shown in Table~\ref{tab:ic15}, compared with methods trained with word-level annotations, our method achieves the best performance on both detection (except on `R') and recognition ( in terms of both `E2E' and `Word Spotting'). Impressively, our \textbf{SRSTS v2} even outperforms MANGO~\cite{qiao2020mango} and CharNet~\cite{xing2019convolutional} that use character-level annotations for supervision, which shows the effectiveness of our method. When evaluated with the specific lexicon, our method either achieves the best results (in terms of `E2E') or performs on par with the best performance. In particular, \textbf{SRSTS v2} surpasses both recently proposed TESTR~\cite{zhang2022text} and TTS~\cite{kittenplon2022towards} by a large margin in terms of F-measure when evaluated with generic lexicon on end-to-end task.



\smallskip\noindent\textbf{Results on Rotated ICDAR 2013.} To evaluate the robustness of text spotters to rotated text, Mask Textspotter v3~\cite{liao2020mask} proposed to rotate the images in ICDAR 2013 in various angles, which is followed by recent methods~\cite{huang2022swintextspotter, kittenplon2022towards}. The spotting results on Rotated ICDAR 2013 are shown in Table~\ref{tab:ic13}. Our method achieves the second place for text detection and the best recognition (E2E) performance in both cases of different rotation angles. Particularly, our method surpasses the state-of-the-art method TTS~\cite{kittenplon2022towards} by  and  on Rotation angle  and Rotation angle  in terms of end-to-end F-measure respectively, which shows the effectiveness and robustness of our method in the challenging scenarios involving rotated text.


\subsubsection{Spotting Results on Curved Text Benchmarks}


\smallskip\noindent\textbf{Results on CTW1500.}
CTW1500 is a challenging benchmark that contains plenty of long text instances with line-level annotations. Table~\ref{tab:ctw} shows the experimental results of our \textbf{SRSTS v2} and other methods for text spotting. Our method performs best for both detection and recognition (`E2E'). In particular, it surpasses other methods substantially in terms of both `None' and `Full' for recognition. 

\smallskip\noindent\textbf{Results on Total-Text.}
Total-Text is a popular benchmark that contains various arbitrary-shaped text instances. The experimental results on Total-Text are shown in Table~\ref{tab:tt}. As shown, our method achieves the best performance in both detection and recognition. In particular, the conference version of our method \textbf{SRSTS} already outperforms other methods for recognition while the extended version \textbf{SRSTS v2} further improves the performance by a large margin, outperforming other methods substantially on both `E2E' and `Word Spotting' with (`Full') or without (`None') lexicon. 


\begin{comment}
\subsubsection{Qualitative Results}
\wu{We visualize the detection results, sampled points, and predicted text transcriptions in Figure~\ref{fig:vis}. As shown, \textbf{SRSTS v2} performs well when facing challenging text instances which are varied in size, orientation, and length. }
\end{comment}

\subsubsection{Inference Speed}
The reported inference speed of different methods may be evaluated in different configurations or settings. To have a fair comparison, we re-evaluate the efficiency of all the methods which release the official codes using the same hardware (1x 3090Ti GPU) in the same experimental setting. Table~\ref{tab:tt} lists both the reported inference speed in their papers and our tested inference speed. Considering that the deformable transformer encoder in Feature Extractor and the self-attention layers in Recognition Head are computationally expensive and account for a large portion of inference time, we develop a faster version of our method by removing the deformable transformer encoder and the self-attention layers to evaluate the inference speed of the core components of our methods. The resulting variant that balances between efficiency and performance is termed as `\textbf{SRSTS v2-F}'. 

\begin{table}[!t]
  \renewcommand{\arraystretch}{1.1}
  \caption{Comparison among ABCNet v2, TESTR, and \textbf{SRSTS v2} in terms of F-measure for detection and recognition. `None' is the F-measure on end-to-end task when performing evaluation without lexicon. `\#Param.' is the number of trainable parameters.}
  \label{tab:comparison}
  \centering
  \begin{tabular}{l|cc|cc|c}
    \toprule
    \multirow{3}*{Method} & \multicolumn{2}{c|}{CTW1500}& \multicolumn{2}{c|}{Total-Text}& \multirow{3}*{\#Param.}\\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
   &Detection & E2E&Detection & E2E\\
   \cmidrule(lr){2-2}
   \cmidrule(lr){3-3}
   \cmidrule(lr){4-4}
   \cmidrule(lr){5-5}
   &F & None&F & None\\
    \midrule
    ABCNet v2 & 84.70&57.50&87.00 & 67.89& 47.75M\\
    TESTR  & 87.10 & 56.00 &87.10  &69.85  &49.26M\\
    \textbf{SRSTS v2} &\textbf{88.45} & \textbf{61.24}&\textbf{89.90} & \textbf{82.05}& 41.00M\\
    \bottomrule
  \end{tabular}
\end{table} 
As shown in Table~\ref{tab:tt}, although the performance of \textbf{SRSTS v2-F} decreases to some degree compared with \textbf{SRSTS v2}, \textbf{SRSTS v2-F} still outperforms other methods by a large margin for both detection and recognition, which demonstrates the effectiveness of the essential model components. More importantly, \textbf{SRSTS v2-F} achieves the fastest inference speed among all methods involved in the comparison, which shows the superiority of our method in terms of efficiency.





\subsubsection{Comparison with Representative Methods}
In this section, we compare our \textbf{SRSTS v2} with ABCNet v2 and TESTR particularly, which are two representative methods of two-stage and single shot methods for text spotting, respectively. We not only compare the general performance for text detection and recognition between these methods on CTW1500 and Total-Text datasets, but also measure the sensitivity of recognition performance to the detection performance for different methods. Finally, we also perform qualitative comparison to obtain more insight into their difference.





 

\smallskip\noindent\textbf{\textbf{SRSTS v2} vs. ABCNet v2.} ABCNet v2 is a classical two-stage text spotting method, which encodes text boundary as Bezier curve and employs FCOS~\cite{tian2019fcos} to conduct text detection. The generated detection proposals are further fed into the recognition head for recognition. We use the official code\footnote{\label{website}\url{https://git.io/AdelaiDet}} and their provided pretrained model for evaluation. As reported in Table~\ref{tab:comparison}, our \textbf{SRSTS v2} performs distinctly better than ABCNet v2 for both text detection and recognition, meanwhile it has relatively smaller model size, which reflects the advantages of \textbf{SRSTS v2} over ABCNet v2. 

\begin{figure}[!t] 
\renewcommand{\arraystretch}{1.1}
   \centering
   \centerline{\includegraphics[width=1.0\linewidth]{comparison.pdf}}
  \caption{The success rate of recognition as a function of detection performance to measure the sensitivity of recognition to the detection performance. Our \textbf{SRSTS v2} exhibits larger performance superiority than ABCNet v2 and TESTR at lower detection IoU, which reveals less dependencies between recognition and detection and more robustness of our \textbf{SRSTS v2} than the other two methods.}
  \label{fig:sensitivity}
\end{figure}


\begin{figure*}[!t] 
   \centering  \centerline{\includegraphics[width=\textwidth]{vis_all_v2.pdf}}
  \caption{Visualization of text spotting results of ABCNet v2, TESTR and our \textbf{SRSTS v2} on five challenging cases. The \textcolor[RGB]{255,0,0}{red} ‘’ represents the location of positive anchor point and the \textcolor[RGB]{0,0,255}{blue} dots denote the sampled points. The \textcolor[RGB]{0,255,0}{green} lines show the predicted text boundaries. The images for showing the results of \textbf{SRSTS v2} are shaded to visualize the sampled points more clearly.} 
  \label{fig:vis}
\end{figure*}

A prominent advantage of our \textbf{SRSTS v2} is the decoupling of recognition from detection by conducting both detection and recognition based on the sampling module concurrently, which can reduce the dependencies of text recognition on the detection and circumvent the potential error propagation from detection to recognition. To validate such advantage, we measure the sensitivity of recognition to detection in Figure~\ref{fig:sensitivity} by calculating the success rate of recognition as a function of detection performance measured by IoU. We observe that our \textbf{SRSTS v2} always outperforms ABCNet v2 at different levels of detection performance. More importantly, \textbf{SRSTS v2} exhibits larger performance superiority than ABCNet at lower detection IoU, which reveals less dependencies between recognition and detection and more robustness of our \textbf{SRSTS v2} than ABCNet v2.




\smallskip\noindent\textbf{SRSTS v2 vs. TESTR.}
TESTR is a single shot text spotting method that performs both text detection and recognition based on the guidance of the learned query embeddings. We also use their official code\footnote{\label{website}\url{https://github.com/mlpc-ucsd/TESTR.}} for evaluation. As shown in Table~\ref{tab:comparison}, our \textbf{SRSTS v2} outperforms TESTR on both text detection and recognition with smaller model size. 

Theoretically, TESTR can decouple the recognition from detection since the query embeddings for detection and recognition are learned independently. However, figure~\ref{fig:sensitivity} shows that our model has much less dependencies between recognition and detection than TESTR, especially at lower detection performance (indicated by IoU). We surmise that both detection and recognition of TESTR rely heavily on the quality of learned queries. Since TESTR adopts the similar strategy of learning query embeddings to conduct both detection and recognition, the challenging text instances for detection to learn effective queries are also difficult for recognition. In contrast, our \textbf{SRSTS v2} has no strict requirement for the location of the sampled points. It is able to recognize the text instances correctly as long as the sampled points can involve all characters, even though the precise text boundaries are challenging to detect.




\smallskip\noindent\textbf{Qualitative Evaluation.} 
We further perform qualitative comparison between our \textbf{SRSTS v2}, ABCNet v2 and TESTR by visualizing the spotting results of three methods on five challenging cases in Figure~\ref{fig:vis}. We make following observations. First, the examples in (1) and (2) show that ABCNet v2 and TESTR fail to recognize the text correctly if the bounding boxes of the text are difficult to be precisely detected whilst our \textbf{SRSTS v2} can still make correct recognition. These examples show the merit of \textbf{SRSTS v2} that decoupling the detection from detection can potentially alleviate the error propagation from detection to recognition. Second, the examples in (3) and (4) illustrate the challenging cases in which ABCNet v2 and TESTR may make wrong recognition due to the complicated text appearance or background even if the text detection is precise. In contrast, our method is able to recognize them correctly, which implies the robustness of our method for recognition. Finally, we also show a failure case in Figure~\ref{fig:vis} (5), in which all three methods fail to make correct prediction for the word `nonna' probably due to the rare typeface that never appears in the training data.







\begin{comment}
\subsection{Effectiveness of the Sampling-driven Spotting Strategy on Sole Text Recognition Task}
To investigate the effectiveness of the proposed sampling-based decoding strategy on sole text recognition task, we build a standalone recognition model by removing Positive Anchor Estimator and Detection Branch from \textbf{SRSTS v2}, and compare it with the state-of-the-art method for sole text recognition, namely the vision model of ABINet~\cite{fang2021read}. To have a fair comparison, we adopt exactly the same training and evaluation protocols as ABINet.

\smallskip\noindent\textbf{Datasets.}
Following ABINet, we use two synthetic datasets MJSynth~\cite{jaderberg2014synthetic} and Synthtext~\cite{gupta2016synthetic} for model training and conduct evaluation on six standard benchmarks: ICDAR 2013 (IC13)~\cite{karatzas2013icdar}, ICDAR 2015 (IC15)~\cite{karatzas2015icdar}, Street View Text (SVT)~\cite{wang2011end}, Stree View Text-Perspective (SVTP)~\cite{phan2013recognizing}, IIIT 5K Words (IIIT)~\cite{mishra2012scene} and  CUTE80 (CUTE)~\cite{risnumawan2014robust}. Note that all images in these datasets have been cropped into regions for each text instance before being fed into text recognition models, thus text detection is not required.


\smallskip\noindent\textbf{Consistent backbone with ABINet.} To have a fair comparison, we adopt the consistent feature extractor with ABINet. Specifically, we employ ResNet 45~\cite{he2016deep} and two Transformer units~\cite{lyu20192d,risnumawan2014robust} as the feature extractor to encode features. 






\begin{figure}[t]
  \centering
\includegraphics[width=\linewidth]{figs/rec_model.pdf}
\caption{The qualitative results of our standalone recognition model on IC13 (a), IC15 (b), SVT (c), SVTP (d), IIIT (e), and CUTE (f). Each \textcolor[RGB]{0,0,255}{blue} point denotes the sampled point whose feature is decoded as a valid character rather than blank. The text under each image denotes the final recognition result.}
   \label{fig:rec_model}
\end{figure}


\begin{table}[t]
  \renewcommand{\arraystretch}{1.1}
  \caption{The comparison between ABINet-SV (Small Vision) and our standalone recognition model on scene text recognition benchmarks in terms of correct word accuracy.}
  \label{tab:rec_model}
  \centering
  \begin{tabular}{l|cccccc|c}
    \toprule
Method & IC13& IC15 & SVT & SVTP & IIIT & CUTE& Avg\\
    \midrule
    ABINet-SV&\textbf{94.2}&\textbf{80.6}&\textbf{89.3}&\textbf{82.3} &93.7&85.1 &88.8\\
    Ours     &93.6 &80.5 &89.0&81.4&\textbf{94.1}&\textbf{89.2} &\textbf{88.9}\\
    \bottomrule
  \end{tabular}
\end{table}
 

\smallskip\noindent\textbf{Comparison with ABINet.} \wu{The quantitative results are shown in Table~\ref{tab:rec_model}. The ABINet-SV uses the same feature extractor as our standalone recognition model and adopts the query paradigm to obtain the vision feature for character probabilities prediction. As shown, our method achieves comparable recognition results on regular text benchmarks and surpasses ABINet-SV by a large margin on curved text benchmarks like `CUTE', which shows the natural advantages of the sampling-based strategy when facing curved text instance. We further visualize some examples in Figure~\ref{fig:rec_model}. As shown, the sampled points tend to cover all characters of the text instance with the guidance of recognition loss. Even when facing the challenging curved text instance, although the sampled points do not completely involve the characters, the model is also able to output the correct recognition results.}
\end{comment}
