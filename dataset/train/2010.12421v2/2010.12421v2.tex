

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage[normalem]{ulem}
\usepackage{booktabs}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{url}

\usepackage{multirow}
\usepackage{setspace}


\usepackage{tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

\usepackage{microtype}
\usepackage{graphicx}


\aclfinalcopy 

\usepackage{xcolor}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\luis}[1]{\textcolor{blue}{#1}}

\newcommand{\fre}[1]{\textcolor{orange}{#1}}

\newcommand\BibTeX{B\textsc{ib}\TeX}












\title{\textsc{TweetEval}: \\ Unified Benchmark and Comparative Evaluation for Tweet Classification}

\author{
 Francesco Barbieri ~ Jose Camacho-Collados \\
 \textbf{Leonardo Neves} ~ \textbf{Luis Espinosa-Anke}  \vspace{0.1cm} \\
Snap Inc., Santa Monica, CA 90405, USA\\
School of Computer Science and Informatics, Cardiff University, United Kingdom \vspace{0.1cm} \\
{ \tt \{fbarbieri,lneves\}@snap.com},\\
{  \tt \{camachocolladosj,espinosa-ankel\}@cardiff.ac.uk}\\ 
}


\date{}

\begin{document}
\maketitle
\begin{abstract}

The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. 
Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper,
we propose a new evaluation framework (\textsc{TweetEval}) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora. 

\end{abstract}




\section{Introduction}

Modern NLP systems are typically ill-equipped when applied to noisy user-generated text. The high-paced, conversational and idiosyncratic nature of social media, paired with platform-specific restrictions (e.g., Twitter's character limit), requires tackling additional challenges, for example, POS tagging \cite{derczynski-etal-2013-twitter}, lexical normalization \cite{han2011lexical,baldwin2015shared}, or named entity recognition \cite{ritter2011named,baldwin-etal-2013-noisy}. In other more generic contexts, these challenges can be considered solved or are simply non-existent. Moreover, other apparently simple tasks such as sentiment analysis have proven to be hard on Twitter data \cite{poria2020beneath}, among others, due to limited amount of contextual cues available in short texts \cite{kim2014sociolinguistic}.
In addition to these and other inherent difficulties, advances in NLP for user-generated data are hindered by its highly fragmented landscape and the lack of a unified evaluation framework. In the current era of pretraining and Language Models (LMs), this is particularly relevant, as these models exhibit a versatility that currently cannot be gauged comparably across Twitter datasets and tasks. This is not the case, however, in more ordinary textual genres and domains. For instance, well known benchmarks like SentEval \cite{conneau-kiela-2018-senteval}, GLUE \cite{wang2019glue} and SuperGLUE \cite{wang2019superglue} include standard NLP tasks such as language inference, paraphrase detection or sentiment analysis, among others. It is undisputable that these benchmarks have contributed to the fast development of language understanding techniques, and LMs in particular, as they have enabled comprehensive evaluations across several tasks in fair and reproducible conditions.

We thus take inspiration from the above to develop \textsc{TweetEval}, a benchmark for tweet classification in English. \textsc{TweetEval} is a standardized test bed for seven tweet classification tasks. These are: sentiment analysis, emotion recognition, offensive language detection, hate speech detection, stance prediction, emoji prediction, and irony detection. We develop a unified framework, unified criteria for train/validation/test splits, and evaluate strong baselines inspired by current SotA in these tasks. We also evaluate transformer-based models, trained entirely and partially on Twitter data, with which we aim to establish a competitive high bar for subsequent contributions. The contributions of this paper are therefore as follows: (1) we compile, curate and release a suite of tasks under the umbrella of a new benchmark: \textbf{\textsc{TweetEval}}\footnote{The unified \textsc{TweetEval} benchmark is available at:\\\scriptsize \url{https://github.com/cardiffnlp/tweeteval}}, a unified framework comprising several tweet classification tasks; and (2) we \textbf{evaluate state-of-the-art LMs in this new framework}, and shed light on the effect of training with different corpora.













\begin{table*}[ht]
\renewcommand{\arraystretch}{1.18}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lll@{}}
\toprule
\multicolumn{1}{l}{\large{\textbf{Dataset}}} & \multicolumn{1}{c}{\large{\textbf{Tweet}}}                                                                                                   & \multicolumn{1}{c}{\large{\textbf{Label}}} \\ \midrule
Emoji                       & Thx for showing this newbie passholder around @ Disneyland                                                                  & \includegraphics[height=0.45cm,width=0.45cm]{img/christmas-tree.png}          \\
Emotion                     & I love swimming for the same reason I love meditating...the feeling of weightlessness.                                      & joy                       \\
Hate                        & Another illegal alien that shouldn't be in America killed an innocent American couple! \#BuildThatWall & hateful                   \\
Irony                       & Leaving whilst its dark is fun. \#not                                                                                        & ironic                    \\
Offensive                   & Are we all ready to sit and watch Indakurate Passcott play football?                                                        & non-offensive             \\
Sentiment                   & Hmmmmm where are the \#BlackLivesMatter when matters like this a rise... kids are a disgrace!!                               & negative                  \\
Stance\textit{(fem)}             & Rather be an ``ugly'' feminist then be these sad people that throws hat on people that believes in equality!            & in favour                \\ \bottomrule
\end{tabular}
}
\caption{Tweet samples for each of the tasks we consider in TweetEval, alongside their label in their original datasets. We use \textit{(fem)} to refer to the \textit{feminism} subset of the stance detection dataset.}
\label{tab:datasamples}
\end{table*}

\section{TweetEval: The Benchmark}
\label{tasks}




In this section, we describe the compilation, curation and unification procedure behind the construction of \textsc{TweetEval} and its corresponding tasks, as well as relevant statistics and evaluation metrics. We also show, in Table \ref{tab:datasamples}, a sample tweet and its corresponding label from the original task.


\subsection{Tasks}







\textbf{Emotion Recognition}.
This task consists of recognizing the emotion evoked by a tweet. We use the dataset of the most participated task of SemEval2018, ``Affects in Tweets'' \cite{mohammad2018semeval}. The original competition was framed as a multi-label classification problem, including 11 emotions. The integration into \textsc{TweetEval} consists of re-purposing this multi-label dataset into multi-class classification, keeping only the tweets labeled with a single emotion. Since the amount of tweets with single labels was scarce, we selected the most common four emotions
(Anger, Joy, Sadness, Optimism)\footnote{We selected those emotions with a minimum frequency of 300 examples in the training set.}. 

\textbf{Emoji Prediction}.
This task consists in, given a tweet, predicting its most likely emoji, and is based on the Emoji Prediction challenge at Semeval2018 \cite{barbieri2018semeval}. It only considers tweets with one emoji (irrespective of its position), which is used as classification label. The test set is the same as in the original publication, but we limit the training and validation splits to 50,000 tweets, in order to comply with Twitter distribution policies. The label set comprises 20 different emoji, and due to their skewed distribution, this task proved to be highly difficult, with low overall numbers. Specifically, more than 42\% of the tweets are labeled with the 3 most frequent emoji
(\includegraphics[height=0.32cm,width=0.32cm]{img/red_heart}, \includegraphics[height=0.32cm,width=0.32cm]{img/smiling_face_with_hearteyes}, and \includegraphics[height=0.32cm,width=0.32cm]{img/face_with_tears_of_joy}). 

\textbf{Irony Detection}. This task consists of recognizing whether a tweet includes ironic intents or not. We use the Subtask A dataset of the SemEval2018 Irony Detection challenge \cite{van2018semeval}. Note that this dataset was artificially balanced to make the task more accessible. 

\textbf{Hate Speech Detection}. This task consists in predicting whether a tweet is hateful or not against any of two target communities: immigrants and women. Our dataset of choice stems from the SemEval2019 Hateval challenge \cite{basile-etal-2019-semeval}.

\textbf{Offensive Language Identification}. This task consists in identifying whether some form of offensive language is present in a tweet. For our benchmark we rely on the SemEval2019 OffensEval dataset \cite{zampieri-etal-2019-semeval}. 

\begin{table}
\begin{center}
{
\setlength{\tabcolsep}{4.8pt}
\scalebox{0.93}{ 
\begin{tabular}{lcrrr}

\toprule
{\bf Task}	& {\bf Lab}	& {\bf Train}  & {\bf Val}   & {\bf Test}     \\
\midrule

Emoji prediction	& 20	& 45,000  & 5,000   & 50,000     \\
Emotion	rec. & 4	& 3257  & 374   & 1421     \\Hate speech det.	& 2	& 9,000  & 1,000   & 2,970     \\
Irony detection	& 2	& 2,862  & 955   & 784     \\
Offensive lg. id.	& 2	& 11,916  & 1,324   & 860     \\
Sent. analysis	& 3	& 45,389  & 2,000   & 11,906     \\
Stance detection & 3 & 2620 & 294 & 1249 \\
\midrule
\midrule
Stance/Abortion	& 3	& 587  & 66 & 280     \\
Stance/Atheism	& 3	& 461  & 52   & 220     \\
Stance/Climate	& 3	& 355  & 40   & 169     \\
Stance/Feminism	& 3	& 597  & 67   & 285     \\
Stance/H. Clinton	& 3	& 620  & 69   & 295     \\



\bottomrule

\end{tabular}
}
}
\end{center}
\caption{\label{table-data} Number of labels and instances in training, validation, and test sets for each dataset. The specific statistics of each target domain in the stance detection task is included at the bottom.}
\end{table}


\textbf{Sentiment Analysis}. The goal for the sentiment analysis task is to recognize if a tweet is positive, negative or neutral. 
We use the Semeval2017 dataset for Subtask A \cite{rosenthal2019semeval}, which includes data from previous runs (2013, 2014, 2015, and 2016) of the same SemEval task. 

\textbf{Stance Detection}. Stance detection is the task to determine, given a piece of text, whether the author has a favourable, neutral, or negative position towards a proposition or target. We use the SemEval2016 shared task on Detecting Stance in Tweets \cite{mohammad2016semeval}. In the original task, five target domains are given: abortion, atheism, climate change, feminism and Hillary Clinton. Unlike the other tasks, training is provided separately for each target domain, which we use to extract individual validation sets.


\subsection{Statistics and evaluation metrics}

Table \ref{table-data} includes the \textsc{TweetEval} datasets statistics after unification.\footnote{The validation sets are randomly sampled from the training set for those tasks where no validation split is provided in the original dataset.} Data sizes range from a few hundred instances for training to over 40,000. Note that the preprocessing pipeline is equal for all tasks: user mentions are anonymized and line breaks and website links are removed. 

\paragraph{Evaluation metrics.}
We use the same evaluation metric from the original tasks, which is macro-averaged F1 over all classes, in most cases. There are three exceptions: stance (macro-averaged of F1 of favor and against classes), irony (F1 of ironic class), and sentiment analysis (macro-averaged recall). Similar to GLUE \cite{wang2019glue}, we also introduce a global metric (TE) based on the average of all dataset-specific metrics. 








\section{Language Models for Tweet Classification}
\label{method}




Transformer-based LMs such as GPT \cite{radford2018improving}, BERT \cite{devlin-etal-2019-bert} or XLNET \cite{yang2019xlnet} have taken the NLP field by storm, outperforming previous linear models and neural network methods based on LSTMs or CNNs in many tasks, including sentence and text classification \cite{wang2019glue}. 


The functioning of these language models for tweet classification is conceptually simple. First, they are trained on a large unlabeled corpus. Then, they are fine-tuned to the task for where an appropriate training set exists. For social media text, however, one may question whether existing pre-trained models trained on standard corpora are optimal. We thus compare three different strategies which differ in the training data:
(1) Using an existing large pre-trained LM;
(2) using an existing architecture, but training from scratch using only Twitter data; and
(3) starting with an original pre-trained LM and continue to train with Twitter data, keeping the original tokenizer and the same masked LM loss. 
 




We consider these three techniques as we are interested in exploring whether a Twitter-specific LM should be trained on Twitter only 
or if it should be initialized with weights learned during pre-training on standard corpora, \textit{and then} be trained on Twitter.
The latter option has indeed three theoretical advantages: (1) these models are generally trained on large amounts of text corpora, and reproducing the same experiment would be extremely expensive even if we had same amount of Twitter data; (2) learning on different types of text corpora make the models more robust and knowledgeable about the world; and (3) some models such as RoBERTa \cite{liu2019roberta} or GPT-2 \cite{radford2019language} are not unfamiliar with internet language and slang, as part of their underlying training corpora contains Reddit data (38GB).





\section{Evaluation}





\begin{table*} \renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{2.0pt}
\scalebox{0.75}{ 

\begin{tabular}{cc|c|c|c|c|c|c|c||c}
\toprule
\textbf{} &
  \textbf{} &
  \textbf{Emoji} &
  \textbf{Emotion} &
  \textbf{Hate} &
  \textbf{Irony} &
  \textbf{Offensive} &
  \textbf{Sentiment} &
  \textbf{Stance} &
  \textbf{ALL} \\ \hline \hline
\multicolumn{1}{c|}{\multirow{6}{*}{Val}} &
  SVM &
  25.0 &
  63.8 &
  73.1 &
  63.4 &
  72.7 &
  68.4 &
  67.9 &
  62.0 \\
\multicolumn{1}{c|}{} &
  FastText &
  23.2 &
  62.9 &
  71.7 &
  62.7 &
  70.0 &
  62.2 &
  67.3 &
  60.0 \\
\multicolumn{1}{c|}{} &
  BLSTM &
  19.4 &
  62.6 &
  72.1 &
  60.6 &
  72.1 &
  61.9 &
  63.4 &
  58.9 \\
\multicolumn{1}{c|}{} &
  RoB-Bs &
  24.7{\small 0.3} (24.3) &
  73.1{\small 1.7} (74.9) &
  76.5{\small 0.3} (76.6) &
  73.7{\small 0.6} (73.7) &
  77.1{\small 0.6} (77.6) &
  71.4{\small 1.9} (72.7) &
  71.4{\small 1.9} (73.9) &
  67.7 \\
\multicolumn{1}{c|}{} &
  RoB-RT &
  24.4{\small 1.5} (\textbf{26.2}) &
  75.4{\small 1.5} (\textbf{77.0}) &
  77.8{\small 1.1} (\textbf{79.6}) &
  74.7{\small 1.5} (\textbf{75.6}) &
  77.2{\small 0.6} (\textbf{77.7}) &
  73.0{\small 1.2} (\textbf{74.2}) &
  72.9{\small 1.0} (\textbf{75.2}) &
  \textbf{69.4} \\
\multicolumn{1}{c|}{} &
  RoB-Tw &
  23.4{\small 1.1} (24.6) &
  67.6{\small 0.9} (68.6) &
  74.3{\small 2.0} (76.6) &
  70.0{\small 0.3} (70.7) &
  76.1{\small 0.6} (76.2) &
  70.5{\small 1.0} (69.4) &
  68.3{\small 2.4} (71.4) &
  65.4 \\ \hline
\multicolumn{1}{c|}{\multirow{6}{*}{Test}} &
  SVM &
  29.3 &
  64.7 &
  36.7 &
  61.7 &
  52.3 &
  62.9 &
  67.3 &
  53.5 \\
\multicolumn{1}{c|}{} &
  FastText &
  25.8 &
  65.2 &
  50.6 &
  63.1 &
  73.4 &
  62.9 &
  65.4 &
 58.1 \\
\multicolumn{1}{c|}{} &
							
  BLSTM &
  24.7 &
  66.0 &
  52.6 &
  62.8 &
  71.7 &
  58.3 &
  59.4 &
  56.5  \\
\multicolumn{1}{c|}{} &
  RoB-Bs &
  30.9{\small 0.2} (30.8) &
  76.1{\small 0.5} (76.6) &
  46.6{\small 2.5} (44.9) &
  59.7{\small 5.0} (55.2) &
  79.5{\small 0.7} (78.7) &
  71.3{\small 1.1} (72.0) &
  68{\small 0.8} (70.9) &
  61.3 \\
\multicolumn{1}{c|}{} &
  RoB-RT &
  31.4{\small 0.4} (\textbf{31.6}) &
  78.5{\small 1.2} (\textbf{79.8}) &
  52.3{\small 0.2} (\textbf{55.5}) &
  61.7{\small 0.6} (62.5) &
  80.5{\small 1.4} (\textbf{81.6}) &
  72.6{\small 0.4} (\textbf{72.9}) &
  69.3{\small 1.1} (\textbf{72.6}) &
  \textbf{65.2} \\
\multicolumn{1}{c|}{} &
  RoB-Tw &
  29.3{\small 0.4} (29.5) &
  72.0{\small 0.9} (71.7) &
  46.9{\small 2.9} (45.1) &
  65.4{\small 3.1} (\textbf{65.1}) &
  77.1{\small 1.3} (78.6) &
  69.1{\small 1.2} (69.3) &
  66.7{\small 1.0} (67.9) &
  61.0 \\ \cline{2-10} 
\multicolumn{1}{c|}{} &
  \textit{SotA} &
  36.0* &
  - &
  65.1 &
  70.5 &
  82.9 &
  68.5 &
  71.0 &
  - \\ \hline \hline
\multicolumn{2}{c|}{\textbf{Metric}} &
  M-F1 &
  M-F1 &
  M-F1 &
  F &
  M-F1 &
  M-Rec &
  AVG (F,) & TE
   \\ \bottomrule
\end{tabular}
}
\caption{\label{table-results} TweetEval validation and test results. For neural models we report both the average result from three runs and its standard deviation, and the best result according to the validation set (parentheses). \textit{SotA} results correspond to the best systems in the original shared tasks - they are included for completeness as they not directly comparable.  Splits might differ, and * indicates that a larger training set is used.}
\end{table*}


\subsection{Experimental setting}

\textbf{Neural language model}.
Among all the available language models we selected RoBERTa \cite{liu2019roberta} as it is one of the top performing systems in GLUE. Moreover, it does not employ the Next Sentence Prediction (NSP) loss \cite{devlin2018bert}, making the model more suitable for Twitter where most tweets are composed of a single sentence. 

\noindent \textbf{Language model pre-training}. We use three different RoBERTa variants: 
pre-trained RoBERTa-base\footnote{RoBERTa-base was trained on 160G of uncompresed text.} (RoB-Bs), the same model but re-trained on Twitter (RoB-RT) and trained on Twitter from scratch (RoB-Tw). RoB-RT and RoB-Tw are trained with early stopping on the validation split and learning rate . Both models converged after about 8/9 days on 8 NVIDIA V100 GPUs.\footnote{We used the Huggingface \textit{transformers} library . The estimated cost for each language model is USD 4,000 on Google Cloud.}

\noindent \textbf{Twitter corpus}. We train RoB-RT and RoB-Tw on 60M tweets\footnote{584 million tokens (3.6G of uncompressed text).} obtained by extracting a large corpus of English tweets\footnote{Crawled with the stream API from May'18 to August'19.} (using the automatic labeling provided by Twitter). We only considered tweets with at least three tokens and without URLs, as to avoid bot tweets and spam advertising.

\noindent \textbf{Classification fine-tuning}. We use the same classification fine-tuning method used in \newcite{liu2019roberta}: we add one dense layer to reduce the dimensions of the RoBERTa's last layer to the number of labels in the classification task, and fine-tune the model on each classification task, training all the parameters simultaneously. We run a minimum parameter search on the starting learning rate (, , , and ), use early stopping (5 epochs) on the validation set and run each experiment three times with different seeds (,,). Then, we select the highest performing learning rate on the validation set, and use the corresponding model to evaluate on the test set. 

\noindent \textbf{Baselines}. FastText \cite{joulin-etal-2017-bag} provides an efficient baseline based on standard features and subword units. We also include an SVM-based baseline with both word and character n-gram features, a model and feature set that has seen great success in recent Twitter-based shared tasks such as emoji prediction \cite{ccoltekin2018tubingen} and stance prediction \cite{mohammad2018semeval}. We finally report the results of a bi-directional LSTM.\footnote{The LSTM has 128 cells, an embedding layer of 100 dimensions, dropout (0.5) and, similarly to the language models, the four learning rate values are tuned in the validation set.} Both FastText and the LSTM use 100-dimensional FastText word embeddings \cite{bojanowski-etal-2017-enriching} trained on the 60M Twitter corpus for the lookup table initialization.









\subsection{Results}



Table \ref{table-results} shows the results of all comparison systems on \textsc{TweetEval}. Perhaps surprisingly, RoBERTa-Base (RoB-Bs) performs well on all tasks, even outperforming the model trained on Twitter data only (RoB-Tw) in most tasks. This can also be attributed to the fact that Twitter is not only noisy text, and formal text can be also found regularly \cite{hu2013dude,xu2017shakespeare}. Using more Twitter data for training might further improve the results of RoB-Tw, but this would also translate into an even more expensive training.
However, RoBERTa-Base coupled with additional training on the same Twitter corpus (i.e. RoB-RT) proves more effective.

The only task where a model trained from scratch on Twitter performs better is Irony detection, where RoB-Tw shows to better generalize (RoB-RT F1 drops 13 points from validation to test set, while Rob-Tw F1 5 points).  This can be due to two factors: (1) irony used on social media might differ from irony on standard text, (2) tweets in our training data are generally short (79.3 characters on average compared to over 100 characters for most other tasks), and therefore tokenizing the text in less word pieces, and potentially less OOVs, becomes more important to generalize.
We note that the low results in the task of emoji prediction (when compared to those obtained in the official SemEval task) are due to the downscaling of the training data. Because of Twitter's data distribution policy, at \textsc{TweetEval} we release at most 50k tweets per task, whereas in the original competition, by id sharing, the training data was one order of magnitude bigger. As for the results in the hate speech task, the difference in performance between validation and test set is mainly due to these splits being collected at different timespans, as pointed out by the organizers of the task \cite{basile-etal-2019-semeval}. This causes a disparity in topic distribution and thus low performance of the systems optimized towards the validation set.









\subsection{Tokenizer analysis}

Table \ref{table-tokenizer} includes number of tokens\footnote{Tokenized with the Twitter-specific ``Twikenizer'': \url{github.com/Guilherme-Routar/Twikenizer}} per tweet for each of the tasks and the difference between word pieces of the pre-trained RoBERTa-base and RoBERTa trained on Twitter from scratch.
This comparison is useful to understand if a model recognizes more or less tokens: if the difference between the two RoBERTa tokenizers is high, it means that one model had to split more times a word.
We can note that the biggest difference in wordpieces between RoB-Bs and Rob-Tw is 6.8\% in the hate detection task. This is expected as these tweets include less standard words, such as insults. On the other hand, except for perhaps emotion recognition and offensive language identification, the difference is not significant, considering that the original RoBERTa tokenizer was not trained on Twitter text. Moreover, even if the tokenizer of Rob-RT was not retrained from scratch, this does not mean that Rob-RT could not learn new tokens as they could be learned as sequence of characters during the language modeling re-training phase. This is also the case of emoji, which were not learned in the original RoBERTa model, but BTE includes all their Unicode bytes. 

\begin{table}[ht]
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{3.0pt}
\scalebox{0.85}{ 
\begin{tabular}{lc|ccc}
\toprule
\textbf{Task} & \textbf{Tokens} & \textbf{RoB-Bs} & \textbf{RoB-Tw} & \textbf{\% Diff} \\ \midrule
Emoji & 14.3 {\small 7.4} & 22.4 {\small 7.4} & 21.6 {\small 6.8} & 2.8 {\small 6.9}  \\
Emotion & 19.2 {\small 10.2} & 27.2 {\small 10.2} & 25.7 {\small 9.6} & 5.1 {\small 8.1}  \\
Hate & 25.6 {\small 19.7} & 38.6 {\small 19.7} & 36 {\small 18.9} & 6.8 {\small 8.2}  \\
Irony & 17.9 {\small 9.3} & 26.1 {\small 9.3} & 25.1 {\small 8.9} & 3.8 {\small 7.1}  \\
Sentiment & 18.9 {\small 9.2} & 26.7 {\small 9.2} & 26.2 {\small 9.1} & 1.4 {\small 8.5}  \\
Offensive & 28.4 {\small 20.9} & 41.9 {\small 20.9} & 39.4 {\small 19.7} & 5.7 {\small 8.5}  \\
Stance & 20.6 {\small 7.1} & 30.7 {\small 7.1} & 30.5 {\small 6.9} & 0.5 {\small 4.8}  \\

\bottomrule
\end{tabular}
}
\caption{\label{table-tokenizer} Tokenization statistics for all \textsc{TweetEval} tasks.
``Tokens'' is the average number of tokens in each tweet using Twikenizer. RoB-RT and Rob-Tw refers to the average number of word pieces after tokenization with the original Roberta-base and with the model trained from scratch. ``Diff'' is the relative difference (\%) of tokens in each tweet between these two tokenizers (if the difference is positive, the original RoBERTa includes more tokens). For stance detection, we computed the average statistics among the five targets.}

\end{table}



\section{Conclusion}

We have presented \textsc{TweetEval}, a unified benchmark for tweet classification consisting of seven heterogeneous tasks that are core to social media NLP research. Along with the benchmark, we have included strong baselines as reference, and ran an analysis of LMs with different training strategies. Our results suggest that using a pre-trained LM may be sufficient, but can improve if topped with extra-training on in-domain data.

For this initial benchmark and in the interest of reproducibility and accessibility, we focused on a fixed setting (i.e. classification). However, we acknowledge that other important tasks may need to be evaluated differently. Thus,
for future work we would like to include more tasks in the context of social media NLP research. Potential improvements include, for example, accounting for the original multi-label nature of emotion classification, or covering more than only 20 emoji in emoji prediction. There are also other scenarios to be addressed as well, like sequence tagging \citep{baldwin2015shared, postwitter}, multimodality \citep{2016arXiv160802289S,lu2018visual}, and code-switching tasks \citep{barman2014code,vilares2016cs}. This is similar to the evolution of GLUE \cite{wang2019glue} into SuperGLUE \cite{wang2019superglue}, with both benchmarks contributing to the development of the field in different ways. It is also important to highlight that these datasets do not represent their underlying tasks as a whole but only a subsample,  and therefore contain biases - automatic models trained on them might not be able to generalize to other specific settings \cite{augenstein2017generalisation,wiegand-etal-2019-detection}.

Finally, this benchmark could foster research in multitask learning. The fact that several similar tasks co-exist (e.g. sentiment analysis and emotion recognition, or hate speech detection and offensive language identification) can lead to interesting analyses where the similarity of these tasks is exploited. 













\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}

















\end{document}
