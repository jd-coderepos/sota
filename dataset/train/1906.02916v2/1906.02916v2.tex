
\subsection{\hotpot}

We experiment on \hotpot{}~\citep{hotpot}, a recently introduced multi-hop \RC{} dataset over Wikipedia articles. There are two types of questions---bridge and comparison. Note that their categorization is based on the data collection and is different from our categorization (bridging, intersection and comparison) which is based on the required reasoning type. 
We evaluate our model on dev and test sets in two different settings, following prior work.

\vspace{.1cm}
\noindent \textbf{Distractor setting} contains the question and a collection of 10 paragraphs: 2 paragraphs are provided to crowd workers to write a multi-hop question, and 8 distractor paragraphs are collected separately via TF-IDF between the question and the paragraph.
The train set contains easy, medium and hard examples, where easy examples are single-hop, and medium and hard examples are multi-hop. The dev and test sets are made up of only hard examples.

\vspace{.1cm}
\noindent \textbf{Full wiki setting} is an open-domain setting which contains the same questions as distractor setting but does not provide the collection of paragraphs.
Following \citet{squad-open}, we retrieve 30 Wikipedia paragraphs based on TF-IDF similarity between the paragraph and the question (or \query).

\begin{comment}\begin{table*}[tb]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{X|ccccc} 
     \toprule
     \multirow{2}{*}{Model} & \multirow{2}{*}{All} & \multirow{2}{*}{Bridge} & \multirow{2}{*}{Comp} & 1-hop & 1-hop \\
     & & & & Solv & Non-solv \\
     \midrule
        Ours & \textbf{70.57} & \textbf{72.53} & \textbf{62.78} & 84.31 & \textbf{58.74} \\
        Ours -- No Multi-hop QA Pair & 61.73 & 61.57 & 62.36 & 79.38 & 46.53\\
     \midrule
        \bert~(trained on Multi-hop) & 67.08 & 69.41 & 57.81 & 82.98 & 53.38 \\
        \bert~(trained on Single-hop) & 56.27 & 62.77 & 30.40 & \textbf{87.21} & 29.64 \\
\midrule
BiDAF (\citet{bidaf};impl by~\citet{hotpot}) & 58.28 &  59.09 &  55.05 & - & - \\
     \bottomrule
\end{tabularx}
\caption{Overall result (F1 score) on the dev set of  \hotpot{} distractor setting. \hanna{you can add a column called training data and then write single-hop or Hotpot (hotpot is better than trained on multi-hop), then you can change the model names to BERT or Ours, etc.}} 
\label{tab:hotpot-distractor-result}
\vspace{-8pt}
\end{table*}

\begin{table*}[tb]
    \centering
    \footnotesize
    \begin{tabular}{l|ccccc} 
     \toprule
     \multirow{2}{*}{Model} & \multirow{2}{*}{All} & \multirow{2}{*}{Bridge} & \multirow{2}{*}{Comp} & 1-hop & 1-hop \\
     & & & & Solv & Non-solv \\
     \midrule
        Ours & \textbf{43.26} & \textbf{40.30} & \textbf{55.04} & \textbf{52.11} & \textbf{35.64} \\
        Ours -- No Multi-hop QA Pair & 39.17 & 35.30 & 54.57 & 50.03 & 29.83 \\
     \midrule
        \bert~(trained on Multi-hop in open-domain) & 38.40 & 34.77 & 52.85 & 46.14 & 31.74 \\
        \bert~(trained on Multi-hop) & 29.02 & 24.01 & 48.95 & 34.94 & 23.92 \\
        \bert~(trained on Single-hop) & 29.97 & 32.15 & 21.29 & 47.14 & 15.18 \\
     \midrule
BiDAF (\citet{bidaf};impl by~\citet{hotpot}) & 34.36 & 30.42 &  50.70 & - & - \\
     \bottomrule
\end{tabular}
\caption{Overall result (F1 score) on the dev set of \hotpot{} open-domain setting.} 
\label{tab:hotpot-open-result}
\vspace{-8pt}
\end{table*}
\end{comment}


\begin{comment}
\begin{table*}[tb]
    \centering
    \footnotesize
    \begin{tabular}{l|ccccc} 
     \toprule
     \multirow{2}{*}{Model} & \multirow{2}{*}{All} & \multirow{2}{*}{Bridge} & \multirow{2}{*}{Comp} & 1-hop & 1-hop \\
     & & & & Solv & Non-solv \\
     \midrule
       \multicolumn{6}{l}{\em \textbf{Distractor setting}} \\
     \midrule
        Ours & \textbf{70.57} & \textbf{72.53} & \textbf{62.78} & 84.31 & \textbf{58.74} \\
        Ours -- No Multi-hop QA Pair & 61.73 & 61.57 & 62.36 & 79.38 & 46.53\\
     \midrule
        \bert~(trained on Multi-hop) & 67.08 & 69.41 & 57.81 & 82.98 & 53.38 \\
        \bert~(trained on Single-hop) & 56.27 & 62.77 & 30.40 & \textbf{87.21} & 29.64 \\
BiDAF (\citet{bidaf};impl by~\citet{hotpot}) & 58.28 &  59.09 &  55.05 & - & - \\
     \midrule
       \multicolumn{6}{l}{\em \textbf{Open-domain setting}}  \\
     \midrule
        Ours & \textbf{43.26} &  \textbf{40.30} & \textbf{55.04} & \textbf{52.11} & \textbf{35.64} \\
        Ours -- No Multi-hop QA Pair & 39.17 & 35.30 & 54.57 & 50.03 & 29.83 \\
     \midrule
        \bert~(trained on Multi-hop in open-domain) & 38.40 & 34.77 & 52.85 & 46.14 & 31.74 \\
        \bert~(trained on Multi-hop) &  29.02 & 24.01 & 48.95 & 34.94 & 23.92 \\
        \bert~(trained on Single-hop) & 29.97 & 32.15 & 21.29 & 47.14 & 15.18 \\
BiDAF (\citet{bidaf};impl by~\citet{hotpot}) & 34.36 & 30.42 &  50.70 & - & - \\
     \bottomrule
\end{tabular}
\caption{Overall result (F1 score) on the dev set of \hotpot{}, distractor setting (top) and open-domain setting (bottom). {\em Bridge} and {\em Comparison} are questions collected by two different data collection procedures, labaled in \hotpot{}. {\em 1-hop Solv} and {\em 1-hop Non-solv} are categorized based on if all of three models of \bert~(trained on Single-hop) get the positive F1 score.
BiDAF is originally from \citet{bidaf}, implemented by~\citet{hotpot}.
} 
\label{tab:hotpot-result}
\vspace{-8pt}
\end{table*}\end{comment}

\begin{comment}
\begin{table}[tb]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{X|ccccc} 
     \toprule
        & {All} & {Bridge} & {Comp} & single & multi \\
     \midrule
       \multicolumn{6}{l}{\em \textbf{Distractor setting}} \\
     \midrule
        {\sys} & \textbf{70.57} & \textbf{72.53} & \textbf{62.78} & 84.31 & \textbf{58.74} \\
        \ \ \ 1hop train &61.73 & 61.57 & 62.36 & 79.38 & 46.53\\
{\bert} &67.08 & 69.41 & 57.81 & 82.98 & 53.38 \\
        \ \ \ 1hop train & 56.27 & 62.77 & 30.40 & \textbf{87.21} & 29.64 \\
BiDAF& 58.28 &  59.09 &  55.05 & - & - \\
     \midrule
       \multicolumn{6}{l}{\em \textbf{Open-domain setting}}  \\
     \midrule
        \sys  & \textbf{43.26} & \textbf{40.30} & \textbf{55.04} & \textbf{52.11} & \textbf{35.64} \\
        \ \ \ 1hop train & 39.17 & 35.30 & 54.57 & 50.03 & 29.83 \\
{\bert} &38.40 & 34.77 & 52.85 & 46.14 & 31.74 \\
\ \ \ 1hop train &29.97 & 32.15 & 21.29 & 47.14 & 15.18 \\
BiDAF & 34.36 & 30.42 &  50.70 & - & - \\
     \bottomrule
\end{tabularx}
\caption{F1 scores on the dev set of \hotpot{} in both distractor  (top) and open-domain settings (bottom). 
We compare \sys{} (our model), \bert{}, and BiDAF, and variants of the models that are only trained on single-hop QA data ({\em 1hop train}).
{\em Bridge} and {\em Comp} indicate original splits in \hotpot{}; {\em single} and {\em multi} refer to dev set splits  that can be solved (or not) by all of three \bert{} models trained on single-hop QA data.} 
\label{tab:hotpot-result}
\vspace{-8pt}
\end{table}\end{comment}

\begin{table*}[tb]
    \centering
\begin{tabulary}{\textwidth}{p{2cm}|ccccc|ccccc} 
     \toprule
        & \multicolumn{5}{c|}{\em \textbf{Distractor setting}} & \multicolumn{5}{c}{\em \textbf{Full wiki setting}} \\
        & {All} & {Bridge} & {Comp} & Single & Multi  & {All} & {Bridge} & {Comp} & Single & Multi \\
     \midrule
        {\sys} & \textbf{70.57} & \textbf{72.53} & \textbf{62.78} & 84.31 & \textbf{58.74}  & \textbf{43.26} & \textbf{40.30} & \textbf{55.04} & \textbf{52.11} & \textbf{35.64} \\
        \ \ \ \ \ \ 1hop train &61.73 & 61.57 & 62.36 & 79.38 & 46.53 & 39.17 & 35.30 & 54.57 & 50.03 & 29.83\\
{\bert} &67.08 & 69.41 & 57.81 & 82.98 & 53.38&38.40 & 34.77 & 52.85 & 46.14 & 31.74 \\
        \ \ \ \ \ \ 1hop train & 56.27 & 62.77 & 30.40 & \textbf{87.21} & 29.64  &29.97 & 32.15 & 21.29 & 47.14 & 15.18 \\
BiDAF& 58.28 &  59.09 &  55.05 & - & - & 34.36 & 30.42 &  50.70 & - & - \\
     \bottomrule
\end{tabulary}
\caption{F1 scores on the dev set of \hotpot{} in both distractor  (left) and full wiki settings (right). 
We compare \sys{} (our model), \bert{}, and BiDAF, and variants of the models that are only trained on single-hop QA data ({\em 1hop train}).
{\em Bridge} and {\em Comp} indicate original splits in \hotpot{}; {\em Single} and {\em Multi} refer to dev set splits  that can be solved (or not) by all of three \bert{} models trained on single-hop QA data.} 
\label{tab:hotpot-result}
\vspace{-8pt}
\end{table*}


\begin{table}[tb]
    \centering
\begin{tabular}{l|c|c} 
     \toprule
      Model & Dist F1 & Open F1 \\
     \midrule
        \sys & 69.63 & 40.65\\
     \midrule
        Cognitive Graph & - & {48.87} \\
        BERT Plus & {69.76} & -\\
        MultiQA & - & 40.23 \\
        DFGN+BERT & 68.49 & - \\
        QFE & 68.06 & 38.06 \\
        GRN & 66.71 & 36.48 \\
        BiDAF & 59.02 & 32.89 \\
     \bottomrule
\end{tabular}
\caption{F1 score on the test set of \hotpot{} distractor and full wiki setting. All numbers from the official leaderboard. All models except BiDAF are concurrent work (not published). \sys\ achieves the best result out of models reported to both distractor and full wiki setting.} 
\label{tab:hotpot-test}
\vspace{-8pt}
\end{table} 
\subsection{Implementations Details}\label{subsec:train-details}
\paragraph{Training {\textbf{Pointer}} for Decomposition.}
We obtain a set of 200 annotations for bridging to train \pointer{3}, and another set of 200 annotations for intersection to train \pointer{2}, hence 400 in total.
Each bridging question pairs with three points in the question, and each intersection question pairs with two points in the question.
For comparison, we create training data in which each question pairs with four points (the start and end of the first entity and those of the second entity) to train \pointer{4}, requiring no extra annotation.\footnote{Details in Appendix~\ref{app:heuristics}.}

\paragraph{Training Single-hop \RC{} Model.} We create {\em single-hop QA data} by combining \hotpot{} easy examples and SQuAD~\citep{squad} examples to form the training data for our single-hop \RC{} model described in Section~\ref{subsec:single}.
To convert \squad{} to a multi-paragraph setting, we retrieve  other Wikipedia paragraphs based on TF-IDF similarity between the question and the paragraph, using Document Retriever from DrQA~\citep{squad-open}.
We train 3 instances with  for an ensemble, which we use as the single-hop model.

To deal with sections/ungrammatical questions generated through our decomposition procedure, we augment the training data with ungrammatical samples.
Specifically, we add noise in the question by randomly dropping tokens with probability of , and  replace~\whword~into `the' with probability of .

\paragraph{Training Decomposition Scorer}
We create training data by making inferences for all reasoning types on \hotpot{} medium and hard examples.
We take the reasoning type that yields the correct answer as the gold reasoning type.
Appendix~\ref{app:impl-details} provides the full details.

\begin{table*}[!htb]
    \begin{minipage}{.5\linewidth}
      \centering
        \footnotesize
        \begin{tabularx}{0.95\textwidth}{X|c} 
         \toprule
         {Model} & F1 \\
         \midrule
            \sys & 70.57  59.07  \\
            \sys--1hop train & 61.73  58.30 \\
         \midrule
            \bert{} & 67.08  44.68  \\
            \bert{}--1hop train & 56.27   49.64  \\
         \bottomrule
        \end{tabularx}
    \end{minipage}
    \begin{minipage}{.5\linewidth}
      \centering
        \footnotesize
        \begin{tabularx}{0.95\textwidth}{X|c|c|c} 
         \toprule
          Model & Orig F1 & Inv F1 & Joint F1 \\
         \midrule
            \sys & 67.80 & 65.78 & 55.80 \\
            \bert & 54.65 & 32.49 & 19.27 \\
         \bottomrule
    \end{tabularx}
    \end{minipage} 
    \caption{
    \textbf{Left: modifying distractor paragraphs.} 
    F1 score on the original dev set and the new dev set made up with a different set of distractor paragraphs.
    \sys{} is our model and \sys{}--1hop train is \sys\ trained on only single-hop QA data and 400 decomposition annotations. \bert{} and \bert{}--1hop train are the baseline models, trained on \hotpot\ and single-hop data, respectively.
    \textbf{Right: adversarial comparison questions.} 
    F1 score on a subset of binary comparison questions. {\em Orig F1}, {\em Inv F1} and {\em Joint F1} indicate F1 score on the original example, the inverted example and the joint of two (example-wise minimum of two), respectively.
}\label{tab:robust}
\end{table*}



\begin{comment}
\begin{table}[tb]
    \centering
    \footnotesize
    \begin{tabular}{l|c|c|c} 
     \toprule
     {Model} & {All} & {Bridge} & {Comp}  \\
     \midrule
        \sys & 59.07 & 58.40 & 61.70 \\
        \sys--No MhQA & 58.30 & 57.50 & 61.52 \\
     \midrule
        \bert{} & 44.68 & 42.80  & 52.17 \\
     \bottomrule
\end{tabular}
\caption{\textbf{Robustness Text I.} F1 score on the new dev set made up with a different set of distractors.
\sys{} is our model, \sys{}--No MhQA is our model trained on only single-hop QA data and 400 decomposition annotations. \bert{} is the baseline model.
} 
\label{tab:newdist-result}
\vspace{-8pt}
\end{table}

\begin{table}[tb]
    \centering
    \footnotesize
    \begin{tabular}{l|c|c|c} 
     \toprule
      Model & Orig F1 & Inv F1 & Min F1 \\
     \midrule
        \sys & 67.80 & 65.78 & 55.80 \\
        \bert & 54.65 & 32.49 & 19.27 \\
     \bottomrule
\end{tabular}
\caption{\textbf{Robustness Text II.} F1 score on a subset of binary comparison questions. {\em Orig F1}, {\em Inv F1} and {\em Min F1} indicate F1 score on the original example, the inverted example and the minimum of two, respectively.
} 
\label{tab:comp-robust}
\vspace{-8pt}
\end{table}
\end{comment} 
\subsection{Baseline Models}
We compare our system~\sys\ with the state-of-the-art on the \hotpot{} dataset as well as strong baselines. 

\vspace{.1cm}
\noindent \textbf{BiDAF} is \sota{} \RC{} model on \hotpot{}, originally from \citet{bidaf} and implemented by \citet{hotpot}.

\vspace{.1cm}
\noindent \textbf{\bert} is a large, language-model-pretrained model, achieving \sota{} results across many different NLP tasks~\citep{bert}.
This model is the same as our single-hop model described in Section~\ref{subsec:single}, but trained on the entirety of \hotpot{}.

\noindent \textbf{\bert--1hop train} is the same model but trained on single-hop QA data without \hotpot{} medium and hard examples.

\noindent \textbf{\sys--1hop train} is a variant of \sys\ that does not use multi-hop QA data except 400 decomposition annotations.
Since there is no access to the groundtruth answers of multi-hop questions, a decomposition scorer cannot be trained. Therefore, a final answer is obtained based on the confidence score from the single-hop \RC{} model, without a rescoring procedure.

\subsection{Results}

Table~\ref{tab:hotpot-result} compares the results of \sys\ with other baselines on the \hotpot{} development set.
We observe that \sys{} outperforms all baselines in both distractor and full wiki settings, outperforming the previous published result by a large margin. 
An interesting observation is that \sys{} not trained on multi-hop QA pairs (\sys--1hop train) shows reasonable performance across all data splits.

We also observe that \bert{} trained on single-hop \RC{} achieves a high F1 score, even though it does not draw inferences across different paragraphs.
For further analysis, we split the \hotpot{} development set into 
{single-hop solvable ({\em Single})} and {single-hop non-solvable ({\em Multi})}.\footnote{We consider an example to be solvable if all of three models of the \bert{}--1hop train ensemble obtains non-negative F1. This leads to 3426 single-hop solvable and 3979 single-hop non-solvable examples out of 7405 development examples, respectively.}
We observe that \sys{} outperforms \bert{} by a large margin in single-hop non-solvable ({\em Multi}) examples. This supports our attempt toward more explainable methods for answering multi-hop questions.


Finally, Table~\ref{tab:hotpot-test} shows the F1 score on the test set for distractor setting and full wiki setting on the leaderboard.\footnote{Retrieved on March 4th 2019 from \url{https://https://hotpotqa.github.io}}
These include unpublished models that are concurrent to our work. 
\sys\ achieves the best result out of models that report both distractor and full wiki setting.

\begin{comment}\begin{table*}[tb]
    \centering
    \footnotesize
    \begin{tabular}{l|l} 
     \toprule
        Question & Little Big Girl was a Simpsons episode directed by the  animator and artist  of what nationality? \\
        \midrule
        \multirow{2}{*}{Span-based} & Little Big Girl was a Simpsons episode directed by which animator and artist? \\
        & \answer~of what nationality?  \\
     \midrule
        \multirow{2}{*}{Free-form} & Which animator and artist was a director of a Simpsons episode, Little Big Girl? \\
        & What is the nationality of~\answer? \\
         \bottomrule
        Question & Robert Smith founded the multinational company headquartered in what city? \\
     \midrule
        \multirow{2}{*}{Span-based} & Robert Smith founded which multinational company? \\
        & \answer~headquartered in what city? \\
     \midrule
        \multirow{2}{*}{Free-form} & Which multinational company was founded by Robert Smith? \\
        & Which city contains a headquarter of \answer?  \\
     \bottomrule
\end{tabular}
\caption{Examples of span-based \queries{} and free-form \queries, both by humans.} 
\label{tab:example-queries}
\vspace{-8pt}
\end{table*}

\begin{table}[tb]
    \centering
    \footnotesize
    \begin{tabular}{lc} 
     \toprule
        \Queries{} & F1 \\
     \midrule
        Span-based (\pointer{3} 200) & 65.44 \\
        Span-based (\pointer{3} 400) & 69.44 \\
        Span-based (human) & 70.41 \\
        Free-form (human) & 70.76 \\
     \bottomrule
\end{tabular}
\caption{F1 score on a sample of 50 bridging questions from the dev set of \hotpot{}, \pointer{3} n is our span-based model trained with n annotations. } \label{tab:ablation-queries}
\vspace{-8pt}
\end{table}

\begin{table}[tb]
    \centering
    \footnotesize
    \begin{tabular}{l} 
     \toprule
        Decomposition decision & F1 \\
     \midrule
        Confidence-based & 61.73 \\
        Pipeline & 63.59 \\
        Decomposition scorer (\sys) & 70.57 \\
        Oracle & 76.75 \\
     \bottomrule
\end{tabular}
\caption{F1 score on the dev set of \hotpot{}  with ablating Decomposition Scorer. Oracle indicates that the ground truth reasoning type is selected. } 
\label{tab:ablation-verifier}
\vspace{-8pt}
\end{table}

\hanna{Merge tables 7,8}\end{comment}

\begin{table*}[tb]
    \centering
    \footnotesize
    \begin{tabular}{ll} 
     \toprule
Question & Robert Smith founded the multinational company headquartered in what city? \\
     \midrule
        \multirow{2}{*}{Span-based} & Q1: Robert Smith founded which multinational company? \\
        & Q2: \answer~headquartered in what city? \\
     \midrule
        \multirow{2}{*}{Free-form} & Q1: Which multinational company was founded by Robert Smith? \\
        & Q2: Which city contains a headquarter of \answer?  \\
     \bottomrule
\end{tabular}
\caption{An example of the original question, span-based human-annotated \queries{} and free-form human-authored \queries.} 
\label{tab:example-queries}
\vspace{-8pt}
\end{table*}

\begin{table*}[!htb]\center
    \begin{minipage}{0.35\linewidth}
      \centering
      \footnotesize
      \begin{tabularx}{\textwidth}{Xc}
        \toprule
        \Queries{} & F1 \\
        \midrule
           Span (\pointer{c} trained on 200) & 65.44 \\
           Span (\pointer{c} trained on 400) & 69.44 \\
           Span (human) & 70.41 \\
           Free-form (human) & 70.76 \\
        \bottomrule
      \end{tabularx}
    \end{minipage}
    \hspace{2em}
\begin{minipage}{0.4\linewidth}
      \centering
        \footnotesize
        \begin{tabularx}{\textwidth}{Xcc}
        \toprule
        Decomposition decision method & F1 \\
     \midrule
        Confidence-based & 61.73 \\
        Pipeline & 63.59 \\
        Decomposition scorer (\sys) & 70.57 \\
        Oracle & 76.75 \\
     \bottomrule
        \end{tabularx}
    \end{minipage} 
    \caption{
    \textbf{Left: ablations in \queries.} 
    F1 score on a sample of 50 bridging questions from the dev set of \hotpot{}, \pointer{c} is our span-based model trained with 200 or 400 annotations.
\textbf{Right: ablations in decomposition decision method.} 
    F1 score on the dev set of \hotpot{}  with ablating decomposition decision method. Oracle indicates that the ground truth reasoning type is selected.
}\label{tab:ablation}
\end{table*}


 

\subsection{Evaluating Robustness}\label{subsec:robustness}
In order to evaluate the robustness of different methods to changes in the data distribution, we set up two adversarial settings in which the trained model remains the same but the evaluation dataset is different.

\paragraph{Modifying Distractor Paragraphs.} We collect a new set of distractor paragraphs to evaluate if the models are robust to the change in distractors.\footnote{We choose 8 distractor paragraphs that do not to change the groundtruth answer.} In particular, we follow the same strategy as the original approach~\citep{hotpot} using TF-IDF similarity between the question and the paragraph, but with no overlapping distractor paragraph with the original distractor paragraphs.
Table~\ref{tab:robust} compares the F1 score of \sys\ and \bert{} in the original distractor setting and in the modified distractor setting.
As expected, the performance of both methods degrade, but \sys\ is more robust to the change in distractors.
Namely, \sys{}--1hop train degrades much less (only 3.41 F1) compared to other approaches because it is only trained on single-hop data and therefore does not exploit the data distribution. 
These results confirm our hypothesis that the end-to-end model is sensitive to the change of the data and our model is more robust.





\paragraph{Adversarial Comparison Questions.}
We create an adversarial set of comparison questions by altering the original question  so that the correct answer is inverted.
For example, we change \example{Who was born earlier, Emma Bull or Virginia Woolf?} to \example{Who was born later, Emma Bull or Virginia Woolf?}
We automatically invert 665 questions (details in Appendix~\ref{app:invcomp}). 
We report the joint F1, taken as the minimum of the prediction F1 on the original and the inverted examples.
Table~\ref{tab:robust} shows the joint F1 score of \sys\ and \bert{}.
We find that \sys\ is robust to inverted questions, and outperforms \bert{} by 36.53 F1.


\subsection{Ablations}\label{subsec:ablation}
\paragraph{Span-based vs. Free-form \queries{}.}
We evaluate the quality of generated \queries{} using span-based question decomposition. We replace the question decomposition component using \pointer{3} with (i) \query{} decomposition through groundtruth spans, (ii) \query{} decomposition with free-form, hand-written \queries{} (examples shown in Table~\ref{tab:example-queries}).

Table~\ref{tab:ablation} (left) compares the question answering performance of \sys\ when replaced with alternative \queries{} on a sample of 50 bridging questions.\footnote{A full set of samples is shown in Appendix~\ref{app:analysis}.}
There is little difference in model performance between span-based and \queries{} written by human.
This indicates that our span-based \queries{} are as effective as free-form \queries{}.
In addition, \pointer{3}~trained on 200 or 400 examples obtains close to human performance.
We think that identifying spans often rely on syntactic information of the question, which \bert{} has likely learned from language modeling.
We use the model trained on 200 examples for \sys{} to demonstrate sample-efficiency, and expect performance improvement with more annotations.



\vspace{-.1cm}
\paragraph{Ablations in decomposition decision method.}
Table~\ref{tab:ablation} (right) compares different ablations to evaluate the effect of the decomposition scorer.
For comparison, we report the F1 score of the confidence-based method which chooses the decomposition with the maximum confidence score from the single-hop RC model, and the pipeline approach which independently selects the reasoning type as described in Section~\ref{subsec:recombination}.
In addition, we report an oracle which takes the maximum F1 score across different reasoning types to provide an upperbound. 
A pipeline method gets lower F1 score than the decomposition scorer.
This suggests that using more context from decomposition (e.g., the answer and the evidence) helps avoid cascading errors from the pipeline.
Moreover, a gap between \sys\ and oracle (6.2 F1) indicates that there is still room to improve.


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lc}
        \toprule
        \multicolumn{2}{c}{Breakdown of 15 failure cases} \\
        \midrule
        Incorrect groundtruth & 1  \\
        Partial match with the groundtruth & 3 \\
        Mistake from human & 3 \\
        Confusing question & 1\\
        \Query{} requires cross-paragraph reasoning & 2\\
        Decomposed \queries{} miss some information & 2  \\
        Answer to the first \query{} can be multiple & 3\\
        \bottomrule              
    \end{tabular}
    \caption{
    The error analyses of human experiment, where the upperbound F1 score of span-based \queries{} with no decomposition scorer is measured.
    }
    \label{tab:upperbound-breakdown}
\end{table}


\begin{table*}[t]
    \centering
    \small
    \begin{tabularx}{\textwidth}{X}
        \toprule
\textbf{Q} What country is the Selun located in? \\
        \textbf{P1} Selun lies between the valley of Toggenburg and Lake Walenstadt in the canton of St. Gallen. \\
        \textbf{P2} The canton of St. Gallen is a canton of {\color{red}Switzerland}. \\
        \midrule
        \textbf{Q} Which pizza chain has locations in more cities, Round Table Pizza or Marion's Piazza? \\
        \textbf{P1} {\color{red}Round Table Pizza} is a large chain of pizza parlors in the western United States. \\
        \textbf{P2} Marion's Piazza ... the company currently operates 9 restaurants throughout the greater Dayton area. \\
        \textbf{Q1} Round Table Pizza has locations in how many cities?
        \textbf{Q2} Marion 's Piazza has locations in how many cities? \\
        \midrule
        \textbf{Q} Which magazine had more previous names, Watercolor Artist or The General?\\
        \textbf{P1} Watercolor Artist, formerly Watercolor Magic, is an American bi-monthly magazine that focuses on ...  \\
        \textbf{P2} {\color{red}The General} (magazine): Over the years the magazine was variously called `The Avalon Hill General', `Avalon Hill's General', `The General Magazine', or simply `General'. \\
        \textbf{Q1} Watercolor Artist had how many previous names? \textbf{Q2} The General had how many previous names? \\
        \bottomrule              
    \end{tabularx}
    \caption{
    The failure cases of \sys, where {Q}, {P1} and {P2} indicate the given question and paragraphs, and {Q1} and {Q2} indicate \queries{} from \sys.
    (Top) The required multi-hop reasoning is implicit, and the question cannot be decomposed. (Middle) \sys{} decomposes the question well but fails to answer the first \query{} because there is no explicit answer. (Bottom) \sys{} is incapable of counting.
    }
    \label{tab:error}
\end{table*}
 
\paragraph{Upperbound of Span-based \Queries{} without a decomposition scorer.}
To measure an upperbound of span-based \queries{} without a decomposition scorer, where a human-level RC model is assumed, we conduct a human experiment on a sample of 50 bridging questions.\footnote{A full set of samples is shown in Appendix~\ref{app:analysis}.} In this experiment, humans are given each \query{} from decomposition annotations and are asked to answer it without an access to the original, multi-hop question.
They are asked to answer each \query{} with no cross-paragraph reasoning, and mark it as a failure case if it is impossible.
The resulting F1 score, calculated by replacing RC model to humans, is 72.67 F1.

Table~\ref{tab:upperbound-breakdown} reports the breakdown of fifteen error cases. 53\% of such cases are due to the incorrect groundtruth, partial match with the groundtruth or mistake from humans. 47\% are genuine failures in the decomposition. For example, a multi-hop question ``Which animal races annually for a national title as part of a post-season NCAA  Division I Football Bowl Subdivision college football game?" corresponds to the last category in Table~\ref{tab:upperbound-breakdown}. The question can be decomposed into ``Which post-season NCAA Division I Football Bowl Subdivision college football game?" and
``Which animal races annually for a national title as part of \answer{}?". However in the given set of paragraphs, there are multiple games that can be the answer to the first \query{}. Although only one of them is held with the animal racing, it is impossible to get the correct answer only given the first \query{}. We think that incorporating the original question along with the \queries{} can be one solution to address this problem, which is partially done by a decomposition scorer in \sys.

\vspace{-.1cm}
\paragraph{Limitations.}
We show the overall limitations of \sys\ in Table~\ref{tab:error}. First, some questions are not compositional but require implicit multi-hop reasoning, hence cannot be decomposed. Second, there are questions that can be decomposed but the answer for each \query{} does not exist explicitly in the text, and must instead by inferred with commonsense reasoning. Lastly, the required reasoning is sometimes beyond our reasoning types (e.g. counting or calculation). Addressing these remaining problems is a promising area for future work.