\documentclass{bmvc2k}



\title{Morphological Network: How Far Can We Go with Morphological Neurons?}

\addauthor{Ranjan Mondal}{https://ranjanz.github.io}{1}
\addauthor{Sanchayan Santra}{https://san-santra.github.io}{2}
\addauthor{Soumendu Sundar Mukherjee}{https://soumendu041.gitlab.io}{3}
\addauthor{Bhabatosh Chanda}{bchanda57@gmail.com}{3}

\addinstitution{
Samsung Research \\
Bangalore \\
India 
}
\addinstitution{
Institute for Datability Science\\
  Osaka University\\
 Osaka, Japan
}
\addinstitution{
Indian Statistical Institute\\
Kolkata\\
India
}

\runninghead{Mondal, Santra, Mukherjee and Chanda}{Morphological Network}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}




\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumerate}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}




\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 


\begin{document}

\maketitle

\begin{abstract}
Morphological neurons, that is morphological operators such as dilation and erosion with learnable structuring elements, have intrigued researchers for quite some time because of the power these operators bring to the table despite their simplicity. These operators are known to be powerful nonlinear tools, but for a given problem coming up with a sequence of operations and their structuring element is a non-trivial task. So, the existing works have mainly focused on this part of the problem without delving deep into their applicability as generic operators. A few works have tried to utilize morphological neurons as a part of classification (and regression) networks when the input is a feature vector. However, these methods mainly focus on a specific problem, without going into generic theoretical analysis. In this work, we have theoretically analyzed morphological neurons and have shown that these are far more powerful than previously anticipated. Our proposed morphological block, containing dilation and erosion followed by their linear combination, represents a sum of hinge functions. Existing works show that hinge functions perform quite well in classification and regression problems. Two morphological blocks can even approximate any continuous function. However, to facilitate the theoretical analysis that we have done in this paper, we have restricted ourselves to the 1D version of the operators, where the structuring element operates on the whole input. Experimental evaluations also indicate the effectiveness of networks built with morphological neurons, over similarly structured neural networks.


\end{abstract}

\section{Introduction}
Mathematical morphology is a set and lattice theoretic technique for the analysis of geometrical structures. Although it originated from the theoretical study of the geometry of porous materials, it is currently extensively used in the domain of digital image processing. 
It serves as a non-linear tool for processing digital images. 
Morphological operators decompose objects or shapes into meaningful parts which helps in understanding them in terms of the elements. Since the identification of objects and their features are directly correlated with their shapes and arrangement, morphological methods are quite suited for visual tasks~\cite{Haralick1992computer}. 
However, coming up with a sequence of transformation and their parameters for a given problem is not straightforward and requires expert knowledge of the problem. 
To this end, researchers have tried to automatically learn the parameters (structuring element) of mathematical morphology operators (i.e., dilation and erosion). These learnable structures are termed as \emph{morphological neurons}. Although this idea is motivated by the use of these operators in image processing, they are quite generic and can be used for tasks like classification~\cite{ritter_introduction_1996,sussner_morphological_2011} and regression~\cite{de_a._araujo_morphological_2012}. on applying morphological neurons for specific tasks.  Theoretical analysis of this structure and its properties are lacking in the literature. 
Intending to fill this gap, in this paper, we have theoretically analysed the properties of morphological neurons and shown that a specific arrangement of the neurons can approximate any continuous function. To be more precise, we have defined a structure called a \emph{Morphological Block} and shown that a sequence of two morphological blocks can work as a universal approximator. However, to facilitate the theoretical analysis, in this paper, we have restricted ourselves to the 1D version of the morphological operators, where the operators work over the whole input at once, not locally. 





The contributions of this work can be summarized as follows.
\begin{enumerate}
\item We have theoretically analyzed the properties of the morphological neurons and have shown that not all sequences of neurons are useful as some of them may be represented using fewer neurons.
\item One such useful sequence is a layer of both dilation and erosion operations, followed by a layer computing a linear combination of the outputs. We call this a \emph{Morphological Block}.
\item We have shown that a morphological block represents a sum of hinge functions. Sum hinge functions work well for tasks like regression, classification and function approximation~\cite{breiman1993hinging}.
\item We have proved that a sequence of two morphological blocks can approximate any continuous function over arbitrary compact sets.
\item We have shown due to the nonlinear nature of these operators, the neurons can learn more  complex decision boundaries with a similar number of parameters. 
\end{enumerate}

The rest of the paper is organized as follows. Existing works that has experimented with morphological neurons are briefly outlined in Section~\ref{sec:rel_work}. In Section~\ref{sec:morph_net}, we describe morphological neurons and theoretically analyse their properties. Section~\ref{sec:results} provides empirical validation of the proposed structure. Finally, concluding remarks are presented in Section~\ref{sec:conclustion}. 


\section{Related works}
\label{sec:rel_work}
The use of morphological operations in a learning framework is first proposed by Davidson and Hummer~\cite{davidson_morphology_1993} in their effort to learn the structuring element of dilation operation on images. A similar effort has been made to learn the structuring elements in more recent work by Masci \etal{}~\cite{masci2013learning}. The use of morphological neurons for problems other than images is first proposed by Ritter and Sussner~\cite{ritter_introduction_1996}. They propose to use single-layer network architecture and focused only on the binary classification task. To classify the data, their proposed network is able to learn two axis-parallel hyperplanes as the decision boundary. This single-layer architecture has later been extended to two-layer architecture by Sussner~\cite{sussner_morphological_1998}. This two-layer architecture can learn multiple axis-parallel hyperplanes, and therefore is able to solve arbitrary binary classification tasks. But, in general, the decision boundaries may not be axis-parallel, and so, a large number of hyperplanes may need to be learned by the network. So, Barmpoutis and Ritter~\cite{barmpoutis_orthonormal_2006} proposed to learn an additional rotational matrix that rotates the input before trying to classify data using axis-parallel hyperplanes. 
In a separate work by Ritter \etal{}~\cite{ritter_two_2014} the use of $L^1$ and $L^\infty$ norm has been proposed as a replacement of the $\emph{min/max}$ operation of dilation and erosion in order to smooth the decision boundaries. 
Ritter and Urcid~\cite{ritter_lattice_2003} introduced the dendritic structure of biological neurons to the morphological networks. This structure creates hyperbox-based decision boundaries instead of hyperplanes. The authors have proved that hyperboxes can estimate any compact region and, thus, any two-class classification problem can be solved. A generalization of this structure to the multiclass case has also been done by Ritter \etal{}~\cite{ritter_learning_2007}. Experimentation with network architecture has also been attempted by Sussner and Esmi~\cite{sussner_morphological_2011}, where they propose a new structure called morphological neurons with competitive learning. In this setting, the \emph{argmax} operator is utilized at the output of several neurons to implement the winner-take-all strategy. The authors claim with this setup the network is able to learn complex decision boundaries. 
=
Methods mentioned till now, employ special optimization techniques to learn the parameters since the $\max$ and $\min$ operations employed by dilation and erosion are not differentiable. So, altogether different strategies have been proposed to overcome this issue. 
Ara\'{u}jo~\cite{de_a._araujo_morphological_2012} utilized network architecture similar to morphological neurons with competitive learning to forecast stock markets. The \emph{argmax} operator was replaced with a linear activation function so that the network is able to regress forecasts and the gradient descent could be utilized for training. 
For morphological neurons with dendritic structure, Zamora and Sossa~\cite{zamora_dendrite_2017} proposed to replace the \emph{argmax} operator with a softmax function, in order to utilize the gradient descent optimizer. However, more recent methods don't consider this a hindrance. The gradient is computed where it is possible and taken to be 0 at other places.


The more recent works employing morphological neurons take altogether different approaches in using morphological operations.
Franchi \etal{}~\cite{franchi2020deep} proposed to utilize morphological operations as layers within neural networks. They have shown the pooling layer in CNNs can be replaced with a learned morphological pooling, and using CNNs with only morphological layers works well in denoising images. 
Nogueira \etal{}~\cite{nogueira2019introduction} proposed to utilize morphological operations to be able to learn novel deep features while training the network end-to-end with gradient descent. The authors' have shown experimentally that these features work well for different image classification tasks. 
Islam \etal{}~\cite{aminul2019deep} proposed using morphological hit-or-miss transform to build networks. Mondal \etal{}~\cite{mondal2020image} introduced the opening-closing network for image de-raining and dehazing using morphological opening and closing operations. Limonova \etal{}~\cite{limonova2020bipolar} proposed new morphological neurons called, bipolar morphological neurons. The authors claim to achieve better recognition results compared to neural networks. 

Although morphological neurons have been utilized in various ways for specific applications, their generic theoretical justification is scarce to date. It is still an open question how morphological networks should be designed so that they become a generic tool that can solve any learning problem.
In the following subsections, we have tried to answer these questions. 


\section{Morphological Neurons}
\label{sec:morph_net}


In this section, we first introduce morphological neurons and their properties. Then we define the Morphological Block and show that it computes a sum of hinge functions. As pointed out in \cite{breiman1993hinging}, hinge functions are a powerful alternative for classification, regression and function approximation. Although the function approximation capability of a morphological block is not known, we have proved that by using two morphological blocks sequentially, we can approximate any function.

\subsection{Dilation and Erosion neurons}
\label{sec:de_neurons} 
Dilation and Erosion neurons are the two most basic morphological neurons because all other morphological operations can be represented as a composition of these two. Note that, we are utilizing the 1D version of these operations to facilitate theoretical analysis.
Given an input $\vx \in \R^d$ and a structuring element $\vs \in \R^{d}$, the operation of \newterm{dilation} ($\oplus$) and \newterm{erosion} ($\ominus$) neurons are defined, respectively, as 
\begin{align}
    \vx \oplus \vs  = \max_{k}\{\evx_k+\evs_k\}, \\
    \vx \ominus \vs  = \min_{k}\{\evx_k-\evs_k\},  \label{eq:dilationerosion}
\end{align}
where  $\evx_k$ denotes $k^{th}$ element of  input vector $\vx$. After computing dilation and erosion we may set a \textit{limiter} or \textit{bias}, say, $\evs_{d+1}$ to compute final output from dilation and erosion neurons by $\max\{ {\vx \oplus \vs }, \evs_{d+1} \}$ and $\min\{ {\vx \ominus \vs }, -\evs_{d+1} \}$ respectively. Note that this ensures $\evs_{d+1}$ to be the lower bound of the output of the dilation neuron, whereas it is the upper bound for the erosion; hence, the term `limiter'. 
Alternatively, we can write it as follows. Let 0 is appended to the input $\vx$, i.e, $\vx' = [\vx, 0]^T$ and  $\evs_{d+1}$ is appended to $\vs$, then
\begin{align}
     \max\{ {\vx \oplus \vs }, \evs_{d+1} \}&=\max\{\max_{k}\{\evx_k+\evs_k\},\evs_{d+1}\} =\max_{{k=1,\dots,d+1}}\{\evx'_k+\evs'_k\}
                    =\vx' \oplus \vs'       \label{eq:dilation_bias} 
\end{align}
Where $\evs'_k$ is a element of structuring element  $\vs'$. Similarly we can get $\vx' \ominus \vs'$. It may be argued that ${d+1}^{th}$ component is selected if the input has no effect on the output or the function. 
In these neurons, the structuring element ($\vs'$) is learned in the training phase.

The $\max$ and $\min$ operators used in the dilation and erosion neurons are only piece-wise differentiable. As a result, only a single element of the structuring element is updated at each iteration. To overcome this problem we propose to use the soft version of $\max$ and $\min$~\cite{cook2011basic} to define \emph{soft dilation} and \emph{soft erosion} neurons as follows.
\begin{align}
    \vx' \hat{\oplus} \vs'  = {\frac{1}{\beta}\log\left( \sum_{k+1} e^{(\evx'_k+\evs'_k)\beta}\right)},  \\
    \vx' \hat{\ominus} \vs'  = -{\frac{1}{\beta}\log\left(\sum_{k+1} e^{(\evs'_k-\evx_k)\beta}\right)},
    \label{eq:softdilationerosion}
\end{align}
where $\hat{\oplus}$ and $\hat{\ominus}$ denote the soft dilation and soft erosion, respectively, and $\beta$ is the ``hardness'' of the soft operations. The soft version can be made close to its ``hard'' counterpart by making $\beta$ large enough \cite{cook2011basic}. Henceforth, for notational convenience, we use $\vx$ and $\vs$ to denote input and structuring element respectively for dilation (or erosion) with a limiter. In other words, the dilation and erosion neurons include the limiter. 

\subsection{Gradient of Morphological Neurons} 
\label{sec:decision_boundary}
Network build using morphological neurons can be trained using the backpropagation algorithm, provided we are able to find their derivative.
The $\max$ and $\min$ operations of dilation and erosion, respectively, are not differentiable. To be precise, they are not differentiable when the arguments to the $\max$ or $\min$ operation are equal. However, this rarely occurs in practice. So, we may define the derivative of the dilation and erosion operation in the following way.
\begin{align}
    \frac{\partial z^+}{\partial s_i} = 
    \begin{cases}
        1 & \text{if $x_i+s_i$ is the max}, \\
        0 & \text{otherwise}.
    \end{cases} 
    \quad
    \frac{\partial z^-}{\partial s_i} = 
    \begin{cases} 
        1 & \text{if $x_i-s_i$ is the min}, \\
        0 & \text{otherwise}.
    \end{cases}
    \label{gradeq}
\end{align}
So, the computed gradient is non-zero only in the element for which the maximum (or minimum) is attained. For this reason, the computed loss or error affects only one element of the structuring element for a given sample. 
As a result, only a single neuron is activated in the network and, consequently, only a single weight ($s_i$) is updated at a time. 
This may result in slow convergence of the training of the network. In practice, training a large morphological network is very slow. Soft morphological neurons mitigate this issue to some extent. 




\subsection{Equivalence of configurations}
The morphological neurons may be arranged in different ways to create a network with the aim of solving a particular task. But not all of these configurations are useful and some of them may be spurious. The following are true for different network architectures.
\begin{theorem}
If we denote $D_{m_1}E_{m_2}$ as a layer with $m_1$ dilation neurons and $m_2$ erosion neurons and $L$ as a linear combination layer, the following may be said about their configurations. 
\begin{enumerate}[(i)]
    \item The architecture $D_{m_1}E_{0}\rightarrow D_{m_2}E_{0} \rightarrow \cdots \rightarrow D_{m_\ell} E_{0}$ consisting only of dilation layers is equivalent to the architecture $D_{m_\ell}E_{0}$ with a single dilation layer. A similar statement is true if one considers architectures with only purely erosion layers.
    \item The architecture $D_1E_1 \rightarrow D_1$ is not equivalent to $D_1E_0$. Similarly, it is not equivalent to $D_0E_1$, and, consequently, the architectures $D_1E_1 \rightarrow D_1E_1$ and $D_1E_1$ are not equivalent.
    \item The architecture $D_1 E_1 \rightarrow D_1 \rightarrow L$ is not equivalent to $D_1E_0 \rightarrow L$.
    \item The architecture $D_{2}E_{0}\rightarrow D_{0}E_{2} \rightarrow D_1$ is not equivalent to $D_{2}E_{0} \rightarrow D_1$.
\end{enumerate}
\end{theorem}
The proof is provided in the supplementary material.


\subsection{Morphological block}
\label{sec:SL_morph}
Here we define Morphological Block, which is one configuration that can be utilized as a building block for making more complex networks. A Morphological block consists of a layer with dilation and erosion neurons followed by a linear combination of their outputs (\Figref{fig:single_layer_network}). We call the layer of dilation and erosion neurons the \newterm{dilation-erosion layer} and the following layer as the \newterm{linear combination layer}. 
Let us consider a morphological block with $n$ dilation neurons and $m$ erosion neurons in the dilation-erosion layer followed by $c$ neurons in the linear combination layer. Let $\vx \in \R^d$ be the input to the network and $z_{i}^+$ and $z_{j}^-$ be the output of the $i^{th}$ dilation neuron and the $j^{th}$ erosion neuron respectively: 
\begin{align}
    z_{i}^+ &= \vx \oplus \vs_i^+ , \\ z_{j}^- &= \vx \ominus \vs_j^-  \label{eq:erosion_net},
\end{align}
where $\vs_i^+$ and $\vs_j^-$ are the structuring elements of the respective neurons. Note that $i \in \{1, 2, \ldots, n\}$ and $j \in \{1, 2, \ldots, m\}$. The final output of a node in the linear combination layer is computed as 
\begin{equation}
    \mathcal{M}(\vx)=\sum_{i=1}^{n} z_{i}^+\omega_i^{+}+\sum_{j=1}^{m} z_{j}^- \omega_j^{-},
    \label{eq:total}
\end{equation}
where $\omega_i^{+}$ and $\omega_j^{-}$ are the weights of the combination layer. When the network is trained, it learns all $\vs_i^+$, $\vs_j^-$, $\omega_i^{+}$ and $\omega_j^{-}$. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth ]{./images/morph_net1.pdf}   
    \caption{Architecture of single layer morphological block. It contains an input layer, a dilation-erosion layer with $n$ dilation and $m$ erosion neuron and a linear combination layer with $c$ neurons producing the output. The limiter associated with the input $x_{d+1}=0$.}
    \label{fig:single_layer_network}
\end{figure}

\subsection{Morphological block as a sum of hinge functions}
In this subsection, we show that the simple morphological block can be represented as a sum of hinge functions. 
Hinge functions provide a powerful representation for problems like classification, and regression tasks~\cite{breiman1993hinging}. 
Additionally, we try to develop the intuition behind the morphological block with a toy example. 


\begin{definition}[$k$-order Hinge Function \cite{wang2005generalization}]
    A $k$-order hinge function $h^{(k)}(\vx)$ consists of $(k+1)$ hyperplanes continuously joined together. It may be defined as 
    \begin{equation}
        h^{(k)}(\vx) = \pm \max\{\vw_1^{T}\vx+b_1, \vw_2^{T}\vx+b_2, \ldots, \vw_{k+1}^{T}\vx+b_{k+1}\} 
        \label{eq:k-order_hinge}
    \end{equation}
\end{definition}


\begin{proposition}
\label{th:gx_sum_hinge}
The function computed by a Morphological Block (denoted by $\mathcal{M}(\vx)$) with $n$ dilation and $m$ erosion neurons followed by their linear combination, is a sum of multi-order hinge functions.
\end{proposition}
\noindent In fact, we can show that 
\begin{equation}
    \mathcal{M}(\vx) = \sum_{i=1}^{l} \alpha_{i} h^{(d)}_i(\vx), \label{eq:lemma_sum_of_hinge}
\end{equation}
where $l=m+n$, $\alpha_{i} \in \{1,-1\}$ and $h^{(d)}_i(\vx), 1 \le i \le l$, are $d$-order hinge functions. The proof is given in the supplementary material.

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{images/1D/legend.pdf}
    \smallskip
    
    \subfigure[NN-ReLU]{\includegraphics[width=0.247\linewidth]{images/1D/circle_tanh.pdf}
    \label{fig:decision_nn}}
    \subfigure[Maxout Network]{\includegraphics[width=0.24\linewidth]{images/1D/circle_maxout.pdf}
    \label{fig:decision_maxout}}
    \subfigure[Morphological Block]{\includegraphics[width=0.216\linewidth]{images/1D/circle_morph.pdf}
    \label{fig:decision_morph}}
    \subfigure[Soft Morphological Block]{\includegraphics[width=0.2175\linewidth]{images/1D/circle_Smorph.png}
    \label{fig:decision_morph_soft}}
\caption{Decision boundaries learned by different networks with two hidden neurons. (a)~Baseline neural network is able to learn only two planes. 
(b)~Maxout network is able to learn two more planes with the help of additional parameters. 
(c)~Morphological Block is able to learn more planes with the same number of parameters as NN-ReLU. 
(d)~Using the soft version of the block, smooths the learned decision boundary. 
This further enhances the discrimination capability of the network while retaining the same number of parameters.}
    \label{fig:pmap_circle}
\end{figure}

Since a morphological block computes a sum of hinge functions, it can potentially learn a large number of hyperplanes. It can be seen a morphological block can have maximum  $(d+1)^l-1$ hyperplanes. Out of all those, there can be almost $d! \times \binom{l}{d} \times {(d+1)^{l-d}}$ hyperplanes that are not parallel to any of the axes. (This has been further explained in the supplementary material.) These hyperplanes can act as decision boundaries as has been demonstrated experimentally using a toy dataset representing a two-class problem. 

The toy dataset contains samples that are distributed along two concentric circles, one circle for each class. The circles are centered at the origin.
We compare the results obtained by various networks with two neurons in the hidden layer. It is observed that the baseline neural network (NN-ReLU) fails to classify this data as with two hidden neurons it learns only two hyperplanes, one for each neuron (\Figref{fig:decision_nn}).
The result of maxout network\cite{goodfellow_maxout_2013} is better, because, in this case, the network learns $2k = 4$ hyperplanes as shown in \figref{fig:decision_maxout}. 
Note that with two morphological neurons in the dilation-erosion layer, our network has learned 6 hyperplanes to form the decision boundary (\Figref{fig:decision_morph}). We should get at most 8 hyperplanes from the morphological block. However, out of these only two decision boundaries are placed in any arbitrary orientation in the 2D space, while others are parallel to either of the axes. Whereas using the soft version of dilation and erosion smooths the boundary, making it aligned with the data (\Figref{fig:decision_morph_soft}). 

\subsection{Two Morphological Blocks: An universal approximator}
A single morphological block may be able approximate functions, but we don't know how well is its approximation capability (Supplementary material provides an empirical study on this). However, with two morphological blocks (applied sequentially) any function can be approximated. 
\begin{lemma}
Any linear combination of hinge functions $\sum_{i = 1}^m \alpha_i h^{(k_i)}(\vx)$ can be represented over an arbitrary compact set $K$ as a two sequential morphological block consisting of dilation neurons only.
\end{lemma} 
\begin{proof}
The proof is given in the supplementary material.
\end{proof}
\begin{theorem}[Universal approximation]
Two morphological blocks applied sequentially, can approximate continuous functions over arbitrary compact sets.
\end{theorem}
\begin{proof}
Continuous functions can be approximated over compact sets by sums of hinge functions (Theorem 3.1 of \cite{breiman1993hinging}). Therefore, by Lemma 1, it follows that any continuous function can be approximated over arbitrary compact sets by two-layer Morph-Nets.
\end{proof}





\section{Experimental Evaluation}
\label{sec:results}
To empirically evaluate the performance of our proposed Morphological block, we have done experiments using various benchmark datasets of several real-world problems. We have compared our results with similarly structured networks because the state-of-the-art methods employ more than just simple neurons to accomplish the results. Also, since our proposed morphological block utilizes 1D morphological operations, the comparison has been done with networks with 1D neurons. So, all the data has been flattened before feeding to the networks. Because of these reasons, we have evaluated our method on MNIST, Fashion-MNIST~\cite{xiao2017fashion}, CIFAR-10, and SVHN dataset only. We have refrained from using large image datasets as a 1D version of the operators won't be able to extract meaningful features from them. Yes, using the 2D version of the morphological operations would indeed be more appropriate for image data, but here our focus is the evaluation of our proposed morphological block, not its 2D version. 




\subsection{MNIST and Fashion-MNIST} 
For MNIST and Fashion-MNIST~\cite{xiao2017fashion} dataset, the network we have utilized contains an input layer and a single morphological block. Only a sigmoid activation has been utilised in the last layer, no other activation function has been used. The morphological block contains 200 dilation and 200 erosion neurons.
Table~\ref{tab:mnists} shows the accuracy of test data after training the network for 300 epochs. We get an average accuracy of $98.43\%$ and $89.84\%$, respectively, on MNIST and Fashion-MNIST datasets. Note that the reported state-of-the-art techniques make use of different data augmentation and pre-processing techniques to train the data.  We have not utilized any of such techniques, but still, we are able to get comparable results.  


\begin{table}
    \centering
    \label{tab:mnists}
\begin{tabular}[c]{lccc}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{Test Accuracy}} \\ \cmidrule{2-4}
        & Morph-Net & Soft Morph-Net ($\beta=8$) & Similar Network \\ 
        \cmidrule(r){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(l){4-4}
        MNIST & 98.39 & 98.90 & \textbf{99.79} \cite{wan2013regularization} \\ Fashion-MNIST & 89.87 & \textbf{89.84} & 89.70 \cite{xiao2017fashion} \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy on MNIST and Fashion-MNIST Datasets using a single hidden layer with 400 morphological neurons.}
\end{table}







\subsection{CIFAR-10 and SVHN}
CIFAR-10~\cite{krizhevsky_learning_2009} and SVHN~\cite{netzer2011reading} are two popular classification datasets that are far more challenging than the previous two. All the networks we have utilized here to report the results, follow a 3-layer architecture: input layer, hidden layer and output layer. For the Maxout network~\cite{goodfellow_maxout_2013}, we have taken $k=2$ which means each hidden neuron has two extra nodes over which the maximum is computed. For the network with (soft) morphological neurons, sigmoid activation has been utilized only in the last layer. Table~\ref{cifar10_svhn_table} shows the mean and standard deviation of test accuracy obtained over 5 runs of 300 epochs each and by varying the number of neurons in the hidden layer. It is seen from the table that the Morph-Nets achieve better accuracy for the CIFAR-10 dataset in all cases. However, for the SVHN dataset, its results are comparable with that of other networks. But for both datasets, the accuracy obtained by Morph-Net stays almost the same across training runs. That is not true for other networks.

\begin{table}
    \centering
    \label{cifar10_svhn_table}
\scalebox{0.82}{
    \begin{tabular}{m{0.2\linewidth}cccccc}
    \toprule
\multirow{2}{*}{\textbf{Architecture}} & \multicolumn{2}{c}{l=200} & \multicolumn{2}{c}{l=400} & \multicolumn{2}{c}{l=600}\\
    \cmidrule(r){2-3}\cmidrule(lr){4-5}\cmidrule(l){6-7}
& CIFAR10 &SVHN & CIFAR10 &SVHN & CIFAR10 &SVHN \\
    \cmidrule(r){1-1} \cmidrule(r){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}\cmidrule(lr){6-6}\cmidrule(l){7-7}
    NN-tanh & 46.6 $\pm$ 0.06 &73.9 $\pm$ 0.12  &46.9 $\pm$ 0.04 & 73.9 $\pm$ 0.23 &48.0 $\pm$ 0.05 & 75.6 $\pm$ 0.14  \\
    NN-ReLU & 47.2 $\pm$ 0.11&64.2 $\pm$ 0.88  &48.0 $\pm$ 0.05 & 76.2 $\pm$ 0.32 &48.1 $\pm$ 0.02 & \textbf{79.5 $\pm$ 0.11}  \\
    Maxout-Network ($k = 2$)~\cite{goodfellow_maxout_2013} &46.9 $\pm$ 0.05 &69.4 $\pm$ 0.10  &48.0 $\pm$ 0.10 & 74.1 $\pm$ 0.22 & 46.4 $\pm$ 0.33 & 37.8 $\pm$ 3.15 \\
    Our& 52.0 $\pm$ 0.02& 73.4 $\pm$ 0.03 &53.6 $\pm$ 0.01 & 76.9 $\pm$ 0.03  &54.0 $\pm$ 0.02 & 78.2 $\pm$ 0.03\\
    Our (Soft: $\beta=12$, $20$)&\textbf{53.5 $\pm$ 0.04} & \textbf{74.1 $\pm$ 0.06} &\textbf{55.8 $\pm$ 0.05} &  \textbf{77.0 $\pm$ 0.05} &\textbf{56.9 $\pm$ 0.04} &78.5 $\pm$ 0.05\\
    \bottomrule
    \end{tabular}
    }
    \caption{Test accuracy achieved on CIFAR-10 and SVHN dataset by different networks when the number of neurons ($l$) in the hidden layer is varied. The value  of $\beta$ is 12 and 20 for CIFAR10 and SVHN respectively.}
\end{table}





















\section{Conclusion}
\label{sec:conclustion}
In this paper, we have theoretically analysed the morphological neurons and have shown that our proposed morphological block is a good way to arrange morphological neurons. We have also shown, that a morphological block represents a sum of hinge functions and two morphological blocks can approximate any continuous function. This provides the theoretical basis that networks built with morphological neurons are equally capable and it is applicable to a variety of problems. The empirical results also show the applicability of morphological neurons. But there is huge scope for further explorations. Firstly, the networks build with morphological neurons (or morphological blocks) are very slow to train since only a small number of parameters are updated at each iteration (Remember that the gradient is 1 only where the max/min occurs). So, it may also require more number iterations to converge.
Improvements in this regard will greatly boost the scope for further exploration.
Secondly, to facilitate theoretical analysis, we have restricted ourselves to the 1D version of the morphological operations. This can be easily extended to 2D morphological operations to make CNN-like networks, but to compete with the state-of-the-art networks other advanced layers (\eg{} batch norm, drop-out) may need to be adapted for morphological neurons. 








\bibliography{egbib}
\end{document}
