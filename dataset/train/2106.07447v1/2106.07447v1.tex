
\begin{table*}[t]
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        Model & Unlabeled Data & LM & dev-clean & dev-other & test-clean & test-other \\
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{10-min labeled}}} \\
DiscreteBERT~\cite{baevski2019effectiveness} & LS-960 & 4-gram & 15.7 & 24.1 & 16.3 & 25.2 \\
        wav2vec 2.0 \textsc{Base}~\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 8.9 & 15.7 & 9.1 & 15.6 \\
        wav2vec 2.0 \textsc{Large}~\cite{baevski2020wav2vec}  & LL-60k & 4-gram & 6.3 & 9.8 & 6.6 & 10.3  \\
        wav2vec 2.0 \textsc{Large}~\cite{baevski2020wav2vec}  & LL-60k & Transformer & 4.6 & 7.9 & 4.8 & 8.2 \\
        \midrule
        HUBERT \textsc{Base}    & LS-960 & 4-gram & 9.1 & 15.0 & 9.7 & 15.3  \\
        HUBERT \textsc{Large}   & LL-60k & 4-gram & 6.1 & 9.4 & 6.6 & 10.1 \\
        HUBERT \textsc{Large}   & LL-60k & Transformer & 4.3 & 7.0 & 4.7 & 7.6 \\
        HUBERT \textsc{X-Large} & LL-60k & Transformer & 4.4 & 6.1 & 4.6 & 6.8 \\
        
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{1-hour labeled}}} \\
        DeCoAR 2.0~\cite{ling2020decoar} & LS-960 & 4-gram & - & - & 13.8 & 29.1 \\
        DiscreteBERT~\cite{baevski2019effectiveness} & LS-960 & 4-gram & 8.5 & 16.4 & 9.0 & 17.6 \\
        wav2vec 2.0 \textsc{Base}~\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 5.0 & 10.8 & 5.5 & 11.3 \\
        wav2vec 2.0 \textsc{Large}~\cite{baevski2020wav2vec}  & LL-60k & Transformer & 2.9 & 5.4 & 2.9 & 5.8 \\
        \midrule
        HUBERT \textsc{Base}    & LS-960 & 4-gram & 5.6 & 10.9 & 6.1 & 11.3  \\
        HUBERT \textsc{Large}   & LL-60k & Transformer & 2.6 & 4.9 & 2.9 & 5.4 \\
        HUBERT \textsc{X-Large} & LL-60k & Transformer & 2.6 & 4.2 & 2.8 & 4.8 \\
        
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{10-hour labeled}}} \\
        SlimIPL~\cite{likhomanenko2020slimipl} & LS-960 & 4-gram + Transformer & 5.3 & 7.9 & 5.5 & 9.0 \\
        DeCoAR 2.0~\cite{ling2020decoar} & LS-960 & 4-gram & - & - & 5.4 & 13.3 \\
        DiscreteBERT~\cite{baevski2019effectiveness} & LS-960 & 4-gram & 5.3 & 13.2 & 5.9 & 14.1 \\
        wav2vec 2.0 \textsc{Base}~\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 3.8 & 9.1 & 4.3 & 9.5 \\
        wav2vec 2.0 \textsc{Large}~\cite{baevski2020wav2vec}  & LL-60k & Transformer & 2.4 & 4.8 & 2.6 & 4.9 \\
        \midrule
        HUBERT \textsc{Base}    & LS-960 & 4-gram & 3.9 & 9.0 & 4.3 & 9.4 \\
        HUBERT \textsc{Large}   & LL-60k & Transformer & 2.2 & 4.3 & 2.4 & 4.6 \\
        HUBERT \textsc{X-Large} & LL-60k & Transformer & 2.1 & 3.6 & 2.3 & 4.0 \\
        
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{100-hour labeled}}} \\
IPL~\cite{xu2020iterative} & LL-60k & 4-gram + Transformer & 3.19 & 6.14 & 3.72 & 7.11 \\
        SlimIPL~\cite{likhomanenko2020slimipl} & LS-860 & 4-gram + Transformer & 2.2 & 4.6 & 2.7 & 5.2 \\
        Noisy Student\cite{park2020improved} & LS-860 & LSTM & 3.9 & 8.8 & 4.2 & 8.6 \\
        DeCoAR 2.0~\cite{ling2020decoar} & LS-960 & 4-gram & - & - & 5.0 & 12.1 \\
        DiscreteBERT~\cite{baevski2019effectiveness} & LS-960 & 4-gram & 4.0 & 10.9 & 4.5 & 12.1 \\
        wav2vec 2.0 \textsc{Base}~\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 2.7 & 7.9 & 3.4 & 8.0 \\
        wav2vec 2.0 \textsc{Large}~\cite{baevski2020wav2vec}  & LL-60k & Transformer & 1.9 & 4.0 & 2.0 & 4.0 \\
        
        \midrule
        HUBERT \textsc{Base}    & LS-960 & 4-gram & 2.7 & 7.8 & 3.4 & 8.1  \\
        HUBERT \textsc{Large}   & LL-60k & Transformer & 1.8 & 3.7 & 2.1 & 3.9 \\
        HUBERT \textsc{X-Large} & LL-60k & Transformer & 1.7 & 3.0 & 1.9 & 3.5 \\
        
        \bottomrule
    \end{tabular}
    \caption{Results and comparison with the literature on low resource setups (10-min, 1-hour, 10-hour, and 100-hour of labeled data).}
    \label{tab:main_lo}
\end{table*} 

\begin{table*}[t]
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        Model & Unlabeled Data & LM & dev-clean & dev-other & test-clean & test-other \\
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{Superivsed}}} \\
        Conformer L~\cite{gulati2020conformer} & - & LSTM & - & - & 1.9 & 3.9 \\
        
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{Self-Training}}} \\
        IPL~\cite{xu2020iterative} & LL-60k & 4-gram + Transformer & 1.85 & 3.26 & 2.10 & 4.01 \\
        Noisy Student~\cite{park2020improved} & LV-60k & LSTM & 1.6 & 3.4 & 1.7 & 3.4 \\
        
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{Pre-Training}}} \\
        wav2vec 2.0 \textsc{Large}~\cite{baevski2020wav2vec} & LL-60k & Transformer & 1.6 & 3.0 & 1.8 & 3.3 \\
        pre-trained Conformer XXL~\cite{zhang2020pushing} & LL-60k & LSTM & 1.5 & 3.0 & 1.5 & 3.1 \\
        
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{Pre-Training + Self-Training}}} \\
        wav2vec 2.0 + self-training~\cite{xu2020self} & LL-60k & Transformer & 1.1 & 2.7 & 1.5 & 3.1\\
        pre-trained Conformer XXL + Noisy Student~\cite{zhang2020pushing} & LL-60k & LSTM & 1.3 & 2.6 & 1.4 & 2.6 \\
        
        \midrule\midrule
        \multicolumn{7}{c}{\textit{\textbf{This work (Pre-Training)}}} \\
        HUBERT \textsc{Large}   & LL-60k & Transformer & 1.5 & 3.0 & 1.9 & 3.3 \\
HUBERT \textsc{X-Large} & LL-60k & Transformer & 1.5 & 2.5 & 1.8 & 2.9 \\
        
        \bottomrule
    \end{tabular}
    \caption{Comparison with the literature on high resource setups using all 960 hours of labeled LibriSpeech data.}
    \label{tab:main_hi}
\end{table*}
 
\section{Results}
\subsection{Main Results: Low- and High-Resource Setups}

Table~\ref{tab:main_lo} presents results for the low-resource setup, where pre-trained models are fine-tuned on 10 minutes, 1 hour, 10 hours, or 100 hours of labeled data. We include comparison with semi-supervised (iterative pseudo labeling (IPL)~\cite{xu2020iterative}, slimIPL~\cite{likhomanenko2020slimipl}, noisy student~\cite{park2020improved}) and self-supervised approaches (DeCoAR 2.0~\cite{ling2020decoar}, DiscreteBERT~\cite{baevski2019effectiveness}, wav2vec 2.0~\cite{baevski2020wav2vec}) in the literature.
Increasing the amount of unlabeled data and increasing the model size improve performance, demonstrating the scalability of the proposed HuBERT self-supervised pre-training method.
In the ultra-low resource setup with just 10 minutes of labeled data, the HuBERT \textsc{Large} model can achieve a WER of 4.7\% on the test-clean set and 7.6\% on the test-other set, which is 0.1\% and 0.6\% WER lower, respectively than the state-of-the-art wav2vec 2.0 \textsc{Large} model. By further scaling up the model size to 1B parameters, the HuBERT \textsc{X-Large} model can further reduce the WER to 4.6\% and 6.8\% on test-clean and test-other. The superiority of HuBERT persists across setups with different amounts of labeled data, with the only exceptions being fine-tuning on 100 hours of labeled data, where HuBERT \textsc{Large} is 0.1\% WER higher than wav2vec 2.0 \textsc{Large} on test-clean, and HuBERT \textsc{Base} is 0.1\% WER higher than wav2vec 2.0 \textsc{Base} on test-other.
In addition, HuBERT also outperforms DiscreteBERT by a large margin in all setups, while both are trained with a virtually identical objective - masked prediction of discovered units. The considerable performance gap suggests two things. First, using waveform as the input to the model is crucial for avoiding loss of information during quantization. Second, while vq-wav2vec~\cite{baevski2019vq}, the units that DiscreteBERT uses for training, may discover better units than k-means clustering of MFCC features, the proposed iterative refinement benefits from the improving HuBERT model and learn better units eventually. We will verify these statements in the ablation study sections.

We report results of fine-tuning HuBERT models on the full 960 hours of Librispeech data and compare with the literature in Table~\ref{tab:main_hi}. Prior studies using additional unpaired speech are classified into:
\begin{enumerate}
    \item self-training: first train an ASR on labeled data to annotate unlabeled speech, and then combine both golden and ASR-annotated text-speech pairs for supervised training.
    \item pre-training: first use unlabeled speech for pre-training a model, and then fine-tune the model on labeled data with a supervised training objective.
    \item pre-training + self-training: first pre-train and fine-tune a model, and then use it to annotate unlabeled speech for self-training combined with supervised data.
\end{enumerate}
HuBERT outperforms the state-of-the-art supervised and self-training methods and is on par with the two best pre-training results in the literature; both are based on wav2vec 2.0 contrastive learning.
In contrast, it lags behind methods combining pre-training with self-training. However, as observed in \cite{xu2020self} and \cite{zhang2020pushing}, we expect that HuBERT can achieve comparable or better performance after combining with self-training, since the pre-trained HuBERT model is on par or better than the pre-trained model those two methods use for pseudo labeling.


\subsection{Analysis: K-Means Stability}
To better understand why masked prediction of discovered units is effective, we conduct a series of analyses and ablation studies. We start with probing the stability of the k-means clustering algorithm concerning different numbers of clusters and different sizes of its training data.
Two features are considered: 39-dimensional MFCC features and 768-dimensional output from the 6-th transformer layer of the first iteration HuBERT-\textsc{Base} model. These two features are used to produce cluster assignments for the first and the second iteration HUBERT training, respectively.

For k-means clustering, we consider $K=\{100,500\}$ clusters fitted on \{1, 10, 100\} hours of speech sampled from the LibriSpeech training split. Each combination of the hyperparameters and the features are trained for 10 trials, and the mean and standard deviation of the supervised PNMI metric on the development set (combining dev-clean and dev-other from LibriSpeech) is reported in Table~\ref{tab:stability}.
The results show that the k-means clustering is reasonably stable given the small standard deviations across different hyperparameters and features. Furthermore, increasing the amount of data used for fitting k-means models improves PNMI in general, but the gain is only as much as 0.012, suggesting the feasibility of using k-means for unit discovery even with limited CPU memory relative to the feature matrix size. Lastly, the PNMI score is much higher when clustering on HuBERT features than clustering on MFCC features, and the gap is even larger with 500 clusters, indicating that iterative refinement significantly improves the clustering quality.

\begin{table}[ht]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccc}
        \toprule
        \multirow{2}{*}{feature} & \multirow{2}{*}{C} 
        & \multicolumn{3}{c}{PNMI (mean $\pm$ std) with K-means Training Size = } \\
        & & 1h & 10h & 100h \\
        \midrule\midrule
        \multirow{2}{*}{MFCC}
        & 100 & 0.251 $\pm$ 0.001 & 0.253 $\pm$ 0.001 & 0.253 $\pm$ 0.001 \\
        & 500 & 0.283 $\pm$ 0.001 & 0.285 $\pm$ 0.000 & 0.287 $\pm$ 0.001 \\
        \midrule
        \multirow{2}{*}{\textsc{Base}-it1-L6}
        & 100 & 0.563 $\pm$ 0.012 & 0.561 $\pm$ 0.012 & 0.575 $\pm$ 0.008 \\
        & 500 & 0.680 $\pm$ 0.005 & 0.684 $\pm$ 0.003 & 0.686 $\pm$ 0.004 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Stability of K-means as an unsupervised unit discovery algorithm with respect to different features, numbers of clusters, and training data sizes. PNMI stands for phone-normalized mutual information.}
    \label{tab:stability}
\end{table} 
\begin{table*}[t]
    \begin{minipage}[c]{0.68\textwidth}
        \centering
        \begin{tabular}{cc|c|ccc}
    \toprule
    \multirow{2}{*}{teacher} & \multirow{2}{*}{C} & 
    \multirow{2}{*}{PNMI} &
    \multicolumn{3}{c}{dev-other WER (\%)} \\
    & & & $\alpha = 1.0$ & $\alpha = 0.5$ & $\alpha = 0.0$ \\
    \midrule\midrule
    Chenone (supervised top-line)  & 8976 & 0.809 & 10.38 & 9.16 & 9.79 \\
    \midrule
    \multirow{3}{*}{K-means on MFCC} 
    & 50  & 0.227 & 18.68 & 31.07 & 94.60 \\
    & 100 & 0.243 & 17.86 & 29.57 & 96.37 \\
    & 500 & 0.276 & 18.40 & 33.42 & 97.66 \\
    \midrule
    K-means on \textsc{Base}-it1-layer6 & 500 & 0.637 & 11.91 & 13.47 & 23.29 \\
    K-means on \textsc{Base}-it2-layer9 & 500 & 0.704 & 10.75 & 11.59 & 13.79 \\
    \bottomrule
\end{tabular}         \caption{The effect of the training objective and clustering quality on performance. $C$ refers to the number of units, and $\alpha$ is the weight for masked frames.}
        \label{tab:loss}
    \end{minipage}
    \hspace{.5cm}
    \begin{minipage}[c]{0.28\textwidth}
        \centering
        \begin{tabular}{lc}
    \toprule
    teacher & WER \\
    \midrule\midrule
    K-means \{50,100\}       & 17.81 \\
    K-means \{50,100,500\}   & 17.56 \\
    \midrule
    Product K-means-0-100 & 19.26 \\
    Product K-means-1-100 & 17.64 \\
    Product K-means-2-100 & 18.46 \\
    Product K-means-\{0,1,2\}-100 & 16.73 \\
    \bottomrule
\end{tabular}%
         \caption{Cluster ensembles with k-means and product k-means.}
        \label{tab:ens}
    \end{minipage}
\end{table*}

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \centering
    \begin{tikzpicture}
\begin{axis}[
  title={},
  legend style={font=\tiny},
  ylabel={\small Cluster Purity (\%)},
  ymin=0.0,  ymax=0.4,
  xtick=data,
  legend pos=north west,
  legend columns=3,
  height=5cm,
  width=8cm,
  ymajorgrids=true,
  grid style=dashed,
  ylabel near ticks,
  xlabel near ticks,
]
\addplot[
    color=blue, mark=square, 
] table [y=hyp-pur, x=layer]{figures/qual_it1_km100.tex};
\addlegendentry{C=100, \textsc{Base}-it1}

\addplot[
    color=blue, mark=o, 
] table [y=hyp-pur, x=layer]{figures/qual_it1_km500.tex};
\addlegendentry{C=500, \textsc{Base}-it1}

\addplot[
    color=blue, mark=diamond, 
] table [y=hyp-pur, x=layer]{figures/qual_it1_km1000.tex};
\addlegendentry{C=1000, \textsc{Base}-it1}

\addplot[
    color=red, mark=square, 
] table [y=hyp-pur, x=layer]{figures/qual_it2_km100.tex};
\addlegendentry{C=100, \textsc{Base}-it2}

\addplot[
    color=red, mark=o, 
] table [y=hyp-pur, x=layer]{figures/qual_it2_km500.tex};
\addlegendentry{C=500, \textsc{Base}-it2}

\addplot[
    color=red, mark=diamond, 
] table [y=hyp-pur, x=layer]{figures/qual_it2_km1000.tex};
\addlegendentry{C=1000, \textsc{Base}-it2}

\end{axis}
\end{tikzpicture}
   \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \begin{tikzpicture}
\begin{axis}[
  title={},
  ylabel={\small Phone Purity (\%)},
  ymin=0.35,  ymax=0.75,
  xtick=data,
  legend style={font=\tiny},
  legend pos=south east,
  legend columns=2,
  height=4cm,
  width=8cm,
  ymajorgrids=true,
  grid style=dashed,
  ylabel near ticks,
  xlabel near ticks,
]
\addplot[
    color=blue, mark=square, 
] table [y=ref-pur, x=layer]{figures/qual_it1_km100.tex};
\addlegendentry{C=100, HUBERT-it1}

\addplot[
    color=red, mark=square, 
] table [y=ref-pur, x=layer]{figures/qual_it2_km100.tex};
\addlegendentry{C=100, HUBERT-it2}

\addplot[
    color=blue, mark=o, 
] table [y=ref-pur, x=layer]{figures/qual_it1_km500.tex};
\addlegendentry{C=500, HUBERT-it1}

\addplot[
    color=red, mark=o, 
] table [y=ref-pur, x=layer]{figures/qual_it2_km500.tex};
\addlegendentry{C=500, HUBERT-it2}

\addplot[
    color=blue, mark=diamond, 
] table [y=ref-pur, x=layer]{figures/qual_it1_km1000.tex};
\addlegendentry{C=1000, HUBERT-it1}

\addplot[
    color=red, mark=diamond, 
] table [y=ref-pur, x=layer]{figures/qual_it2_km1000.tex};
\addlegendentry{C=1000, HUBERT-it2}

\legend{}
\end{axis}
\end{tikzpicture}
   \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \centering
    \begin{tikzpicture}
\begin{axis}[
  title={},
  legend style={font=\tiny},
  xlabel={\small Layer},
  ylabel={\small PNMI (\%)},
  ymin=0.35,  ymax=0.75,
  xtick=data,
  legend pos=south east,
  legend columns=2,
  height=4cm,
  width=8cm,
  ymajorgrids=true,
  grid style=dashed,
  ylabel near ticks,
  xlabel near ticks,
]
\addplot[
    color=blue, mark=square, 
] table [y=MI/H(ref), x=layer]{figures/qual_it1_km100.tex};
\addlegendentry{C=100, HUBERT-it1}

\addplot[
    color=red, mark=square, 
] table [y=MI/H(ref), x=layer]{figures/qual_it2_km100.tex};
\addlegendentry{C=100, HUBERT-it2}

\addplot[
    color=blue, mark=o, 
] table [y=MI/H(ref), x=layer]{figures/qual_it1_km500.tex};
\addlegendentry{C=500, HUBERT-it1}

\addplot[
    color=red, mark=o, 
] table [y=MI/H(ref), x=layer]{figures/qual_it2_km500.tex};
\addlegendentry{C=500, HUBERT-it2}

\addplot[
    color=blue, mark=diamond, 
] table [y=MI/H(ref), x=layer]{figures/qual_it1_km1000.tex};
\addlegendentry{C=1000, HUBERT-it1}

\addplot[
    color=red, mark=diamond, 
] table [y=MI/H(ref), x=layer]{figures/qual_it2_km1000.tex};
\addlegendentry{C=1000, HUBERT-it2}

\legend{}
\end{axis}
\end{tikzpicture}
   \end{subfigure}
  \caption{Quality of the cluster assignments obtained by running k-means clustering on features extracted from each transformer layer of the first and the second iteration \textsc{Base} HuBERT models.}
  \label{fig:qual_layer}
\end{figure}


\subsection{Analysis: Clustering Quality Across Layers and Iterations}
We next study how each layer of the HuBERT model from each iteration performs when used for clustering to generate training targets.
The two \textsc{Base} HuBERT models from the first two iterations as described in Section~\ref{sec:pretrain} are considered, which are referred to as \textsc{Base}-it1 and \textsc{Base}-it2, respectively. There are 26 features representing 12 transformer layers plus the input to the first transformer layer (denoted as ``Layer 0'') from the two HuBERT models.
For each feature, we fit three k-means models ($K=\{100, 500, 1000\}$ clusters) on a 100 hour subset randomly sampled from the LibriSpeech training data. The teacher quality measured in cluster purity, phone purity, and phone normalized mutual information (PNMI) is shown in Figure~\ref{fig:qual_layer}.
As a baseline, MFCC achieves (cluster purity, phone purity, PNMI) = (0.099, 0.335, 0.255) for $K=100$ and (0.031, 0.356, 0.287) for $K=500$.

Both \textsc{Base}-it1 and \textsc{Base}-it2 features result in significantly better clustering quality on all three metrics than MFCC with the same number of clusters. On the other hand, the best \textsc{Base}-it2 feature is better than the best \textsc{Base}-it1 on phone purity and PNMI, but slightly worse on cluster purity.
Finally, we observe different trends across layers from \textsc{Base}-it1 and \textsc{Base}-it2: while \textsc{Base}-it2 model features generally improve over layers, \textsc{Base}-it1 has the best features in the middle layers around the 6th layer. Interestingly, the quality of the last few layers degrades dramatically for \textsc{Base}-it1, potentially because it is trained on target assignments of worse quality, and therefore the last few layers learn to mimic their bad label behavior.

\subsection{Ablation: The Importance of Predicting Masked Frames}
We present a series of ablation studies in the following sections to learn how pre-training objective, cluster quality, and hyperparameters affect the performance. 
The models for ablation studies are pre-trained for 100k steps and fine-tuned on the 10-hour libri-light split using fixed hyperaprameters. MFCC-based k-means units with C=100 are used if not otherwise mentioned. We report WERs on the dev-other set decoded with the $n$-gram language model using fixed decoding hyperparameters.

To understand the importance of our proposal to predict the masked frames only, we compare three conditions: 1) predicting masked frames, 2) predicting all frames, and 3) predicting unmasked frames, which can be simulated by setting $\alpha$ to 1.0, 0.5, and 0.0, respectively. 
We are comparing three k-means models learned from clustering MFCC teachers with 50, 100, 500 clusters, one learned from clustering HuBERT-\textsc{Base}-it1 6th transformer layer features, and supervised labels obtained from the forced-alignment of character-based HMM models (chenone)~\cite{le2019senones}.

Results shown in Table~\ref{tab:loss} indicate that when learning from bad cluster assignments, computing loss only from the masked regions achieves the best performance, while the inclusion of unmasked loss results in significantly higher WERs. 
However, as the clustering quality improves, the model would suffer less when computing losses on the unmasked frames (\textsc{Base}-it1-layer6) or even achieve better performance as the case of chenone.


\subsection{Ablation: The Effect of Cluster Ensembles}
To understand the effect of combining multiple k-means models for generating targets, we consider two setups. The first one has k-means models of different numbers of clusters presented in Table~\ref{tab:loss}, denoted with KM-\{50,100,500\}. The second one has k-means models trained on spliced MFCC features with a window of three; hence, each input feature is represented as a 117-dimensional vector. In this second case, we apply product quantization on the spliced features, where dimensions are split into the coefficients of the zeroth, first, and second-order derivatives, with each 39-dimensional subspace quantized to a codebook of 100 entries. We denote these codebooks with Product k-means-\{0,1,2\}-100, respectively.
By comparing the results from Table~\ref{tab:loss} and Table~\ref{tab:ens}, it is clear that using an ensemble leads to better performance than what a single k-means clustering can achieve.


\subsection{Ablation: Impact of Hyperparameters}
Figure~\ref{fig:prob_bs} and Table~\ref{tab:step} studies how hyperparameters affect HuBERT pre-training.
It is shown that
\begin{enumerate*}[label=(\arabic*)]
    \item the portion of frames selected as mask start is optimal at $p=$8\%;
    \item increasing the batch size can significantly improve the performance; \item training for longer consistently helps for both k-means models with C=\{50, 100\}, and the best model achieves a WER of 11.68\%.
\end{enumerate*}
These findings are also consistent with those from BERT-like models~\cite{clark2020electra}. In addition, we include a comparable result from DiscreteBERT~\cite{baevski2019effectiveness} in Table~\ref{tab:step} which applies k-means to quantize the same MFCC features into 13.5k units, used as both the output and the \textit{input} to the BERT model. Besides using continuous speech input rather than discrete units, We hypothesize that HuBERT achieves significantly better performance because its fewer k-means clusters of 100 or 500 help capture broad phonetic concepts without delving into inter/intra-speaker variation. 

\begin{table}[ht]
    \centering
    \begin{tabular}{cc|cccc}
    \toprule
    \multirow{2}{*}{teacher} & \multirow{2}{*}{C} &  
    \multicolumn{4}{c}{dev-other WER (\%)} \\
    & & steps=100k & 250k & 400k & 800k \\
    \midrule
    \multirow{2}{*}{K-means}  
    & 50  & 18.68 & 13.65 & 12.40 & 11.82 \\
    & 100 & 17.86 & 12.97 & 12.32 & 11.68 \\
    \midrule
    \cite{baevski2019effectiveness} & 13.5k & \multicolumn{4}{c}{26.6} \\
    \bottomrule
\end{tabular}     \caption{Varying the number of HuBERT pre-training steps. $p$ is set to 6.5\%.}
    \label{tab:step}
\end{table}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \begin{tikzpicture}
\begin{axis}[
xlabel={\small $p$},
  ylabel={\small WER (\%)},
ymin=16,
  ymax=24,
  xtick=data,
  width=\linewidth,
  height=2.5cm,
  ymajorgrids=true,
  grid style=dashed,
  ylabel near ticks,
  xlabel near ticks,
  ticklabel style={font=\scriptsize},
]
\addplot table [y=km100_100k, x=maskp]{figures/maskp.tex};
\end{axis}
\end{tikzpicture}   \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \begin{tikzpicture}
\begin{axis}[
  xlabel={\small \#GPUs},
  ylabel={\small WER (\%)},
  ymin=15,
  ymax=40,
  xtick=data,
  width=\linewidth,
  height=2.5cm,
  ymajorgrids=true,
  grid style=dashed,
  ylabel near ticks,
  xlabel near ticks,
  ticklabel style={font=\scriptsize},
]
\addplot table [y=km100_100k, x=ngpu]{figures/batchsize.tex};
\end{axis}
\end{tikzpicture}   \end{subfigure}
  \caption{Varying masking probability $p$ (left) and effective batch size through the number of GPUs (right).}
  \label{fig:prob_bs}
\end{figure}
