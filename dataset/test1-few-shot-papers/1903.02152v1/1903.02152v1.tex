\pdfoutput=1













\documentclass[journal]{IEEEtran}


















\usepackage[numbers,sort&compress,square]{natbib} 


\usepackage{color}



\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else
\fi






\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}






\usepackage{algorithmic}
\usepackage{algorithm}































\usepackage{url}


\usepackage[table]{xcolor}

\usepackage{multirow}

\usepackage[table]{xcolor}
\usepackage{collcell}
\usepackage{hhline}
\usepackage{pgf}

\newcommand\gray{gray}



\newcommand\ColCell[1]{\pgfmathparse{#1<40?1:0}\ifnum\pgfmathresult=0\relax\color{white}\fi
  \pgfmathparse{#1<1?1:0}\ifnum\pgfmathresult=1\relax\color{white}\fi
  \pgfmathparse{1-#1/100}\expandafter\cellcolor\expandafter[\expandafter\gray\expandafter]\expandafter{\pgfmathresult}#1}

\newcolumntype{E}{>{\collectcell\ColCell}c<{\endcollectcell}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}




\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Safeguarded Dynamic Label Regression for \\Generalized Noisy Supervision}


\author{Jiangchao~Yao,~Ya~Zhang,~Ivor~Tsang~and~Jun~Sun}





\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}














\maketitle

\begin{abstract}
Learning with noisy labels, which aims to reduce expensive labors on accurate annotations, has become imperative in the Big Data era. Previous noise transition based method has achieved promising results and presented a theoretical guarantee on performance in the case of class-conditional noise. However, this type of approaches critically depend on an accurate pre-estimation of the noise transition, which is usually impractical. Subsequent improvement adapts the pre-estimation along with the training progress via a Softmax layer. However, the parameters in the Softmax layer are highly tweaked for the fragile performance due to the ill-posed stochastic approximation. To address these issues, we propose a Latent Class-Conditional Noise model (LCCN) that naturally embeds the noise transition under a Bayesian framework. By projecting the noise transition into a Dirichlet-distributed space, the learning is constrained on a simplex based on the whole dataset, instead of some ad-hoc parametric space. We then deduce a dynamic label regression method for LCCN to iteratively infer the latent labels, to stochastically train the classifier and to model the noise. Our approach safeguards the bounded update of the noise transition, which avoids previous arbitrarily tuning via a batch of samples. We further generalize LCCN for open-set noisy labels and the semi-supervised setting. We perform extensive experiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and the agnostic noise data sets, Clothing1M and WebVision17. The experimental results have demonstrated that the proposed model outperforms several state-of-the-art methods.
\end{abstract}

\begin{IEEEkeywords}
Bayesian learning, noisy labels, Gibbs sampling.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{L}{arge} scale datasets with editorial labels have driven the success of deep neural networks (DNNs) in computer vision~\cite{NIPS2012_4824}, natural language processing~\cite{sutskever2014sequence}, and speech recognition~\cite{hinton2012deep}. However, for many real-world applications, it is usually expensive to collect accurately annotated data in large volume. Instead, samples with noisy supervision, as an alternative to alleviate the annotation burden, can be acquired inexhaustibly on the social websites and have shown potential to many applications in the deep learning area~\cite{7953515,8293849,8410611,yao2017deep}. 

It is challenging to train DNNs in the presence of noisy supervision since it can easily memorize the clean data as well as the noisy data~\cite{arpit2017closer}. To overcome the above issue, several methods have been explored from the perspective of model regularization and sample re-weighting, respectively. Arpit \textit{et al.}~\cite{arpit2017closer} applied the dropout regularization in DNNs to limit its speed of memorizing noise, which prevents the classifier from  noise pollution. Ren \textit{et al.}~\cite{pmlr-v80-ma18d} explored dynamically weighting noisy labels with the corresponding predictions to weaken the noise effect. However, the model regularization and sample re-weighting based methods usually require either a careful hyperparameter setting~\cite{arpit2017closer}, or auxiliary samples~\cite{pmlr-v80-ren18a} or elaborate curricula~\cite{Guo_2018_ECCV}. 

The study of this paper falls into the third popular perspective, learning with noise transition, which places a noise transition on top of the classifier. Early study~\cite{patrini2017making} presents a two-step solution, that is, first pre-estimate the noise transition and then fix it to train the classifier. However, it suffers from the inaccurate pre-estimation via an ideal but impractical anchor set. Subsequent improvement~\cite{goldberger2016training} uses the stochastic approximation to adapt the noise transition in the form of a Softmax layer along with the training progress. Although it shows promise, the optimization of the Softmax layer depends on highly tweaking and the model parameters easily fall into undesired local minimums. Essentially, such instability is due to the inconsideration of the global dependency in the stochastic approximation, yielding a ``local" mini-batch of samples can unbounded update the ``global" noise transition in the back-propagation. 

To solve this issue, we propose a Latent Class-Conditional Noise model (LCCN) that embeds the noise transition into a Dirichlet-distributed space. Compared to the previous Softmax layer~\cite{goldberger2016training}, LCCN constrains the learning of the noise transition as a global variable depending on the whole dataset. Namely, a ``local" mini-batch of samples can only partially affect the estimation of the ``global" noise transition. Besides, a new dynamic label regression method is derived to stochastically optimize LCCN. Although it iteratively infers the latent labels and applies them for the classifier training and the noise modeling, only a small amount of extra computational cost is introduced. We theoretically demonstrate our method safeguards the bounded update of the noise transition via a mini-batch of samples. Fig.~\ref{fig:network} provides a simple illustration of our safeguarded dynamic label regression for LCCN. As can be seen, images are first inputted to the classifier to have the prediction of latent labels. Noisy labels are also forwarded to Bayesian noise modeling to compute the conditional transition of latent labels. Then, the latent labels are sampled based on their product and used to supervise the classifier training and refine the noise modeling. 
\begin{figure*} 
\centering
\includegraphics[width=0.95\textwidth]{network.pdf}
\caption{Safeguarded dynamic label regression for LCCN. The images and noisy labels are respectively inputted to the classifier and the safeguarded Bayesian noise modeling to compute the prediction and the conditional transition. Then, the latent labels are sampled based on their product and then used for the classifier training and the safeguarded Bayesian noise modeling.}\label{fig:network}
\end{figure*}
In a nutshell, our main contributions can be summarized into the following three points.
\begin{itemize}
\item We propose a Latent Class-Conditional Noise model that embeds the noise transition into a Dirichlet space to emphasize its global dependency, and then deduce a scalable dynamic label regression method for its optimization.
\item The theoretical analysis on the convergence of the dynamic label regression, the generalization gap as well as the complexity is provided. Importantly, we prove that our optimization of the noise transition via a batch of samples is bounded to avoid previous non-trivial tweaking.
\item A more general variant of LCCN is further extended in order to handle the open-set noisy labels setting and the semi-supervised learning setting for the practical needs.
\item We conduct a range of experiments in the popular CIFAR-10, CIFAR-100 datasets and large real-world noisy datasets, Clothing1M and WebVision17. Comprehensive results have demonstrated the superior performance of our model compared with existing state-of-the-art methods.
\end{itemize}

The rest part of this paper is organized as follows. Section II briefly reviews the related research of learning with noisy labels in deep learning. Then, we introduce our Latent Class-Conditional model and the dynamic label regression method in Section III, where the corresponding theoretical analysis and the further extension of LCCN is also included. We validate the efficiency of our method over a range of experiments in Section IV. Section V concludes the whole paper.

\section{Related Work}
Recently, several approaches combined with deep learning have been developed for learning with noisy labels. In this section, we review these works according to noise transition, sample re-weighting and model regularization.
\subsubsection{Learning with Noise Transition}
This branch of research models a noise transition on top of the classifier to minimize the influence of label noise. \citet{sukhbaatar2014training} introduced a noise transition matrix on top of CNN to learn with noisy supervision. With a heuristic learning procedure, they gradually make the transition matrix absorb the noise among labels.
\citet{MisraNoisy16} considered the ``reporting bias'' phenomenon in human-centric annotations via a content-based transition, which is a special case of learning with noisy labels. \citet{patrini2017making} theoretically demonstrated: the backward correction with the inverse of the noise transition is unbiased to train the classifier in the presence of noisy labels; the forward noise transition make the training share the same minimizer with that on the clean data. However, the performance quite depends on the accuracy of the pre-estimated noise transition. 
Subsequent improvement in \cite{goldberger2016training} models the noise transition via a Softmax layer and tunes its parameters along with the training progress. Based on this research, \citet{yao2017deep} introduced an auxiliary variable to augment the noise transition with more uncertainty. The structure information~\cite{Han2018MaskingAN} is further added to constrain the optimization. Although better performance has been achieved, these methods depend on the carefully tweaking. However, our model embeds the noise transition into a nonparametric space and naturally constrains its optimization to avoid undesired minimums via a dynamic label regression method.

\subsubsection{Learning with Sample Re-weighting}
This line of works weight the contribution of each training sample in parameter estimation to reduce the effect of label noise~\cite{liu2016classification,pmlr-v80-ren18a}. It can be implemented by the label or the training pair re-weighting. For example, \citet{reed2014training} facilitated the notion of perceptual consistency to linearly combine the label and the prediction as the new supervision, which shows the substantial robustness to label noise. Then, \citet{li2017learning} substituted the prediction with the refined label by the graph distillation. \citet{wang2018iterative} leveraged the local intrinsic dimensionality to design an self-weighting strategy for Bootstrapping~\cite{reed2014training}. Recently, several works~\cite{jiang2017mentornet,NIPS2018_7454,Han2018CoteachingRT} also explore to collaboratively learn a weight or selection for each training pair and adjust their contribution to the training of the classifier. However, these methods critically depend on the elaborate sample re-weighting strategy.

\subsubsection{Learning with Model Regularization}
This type of methods attempt to regularize the training procedure in the presence of noisy supervision. \citet{zhang2016understanding} have shown DNNs can easily memorize the random labels completely, characterizing the challenge to deep learning with noisy labels. 
Their further study~\cite{zhang2017mixup} that used the convex combinations of images and noisy labels as the data augmentation, has been demonstrated as an efficient regularization to prevent DNNs from overfitting. \citet{arpit2017closer} investigated the memorization order of DNNs on feature patterns in noisy datasets and demonstrated dropout can efficiently limit the speed of memorization on noise in DNNs. \citet{tanaka2018joint} explicitly introduced a regularization term to prevent the trivial case of assigning all labels to a single class in label correction. Compared with above methods, we indirectly regularize the training by Bayesian noise modeling.

\section{The Proposed Framework}
\subsection{Preliminaries}
In the c-class classification setting, a collection of  noisy training pairs  is given, where  is the raw input data or the feature vector and  is the corresponding noisy label. Assume  denotes the latent label of , which is unknown in practice. Then the goal in this task is to train a deep network classifier from the noisy dataset  analogous to the one trained from the clean dataset , so that a promising performance can be achieved in a clean test dataset.  As shown in~\cite{zhang2016understanding}, directly minimizing the following equation will make DNNs memorize both the classification pattern and noise,

where  is from the function class , which is parameterized by  via DNNs, and  is the loss function between  and the prediction . Eq.~\eqref{eq:pse_goal} leads to a bad performance in the clean test dataset since it does not squeeze out the noise influence from . Therefore, we follows one mainstream of approaches to handle this dilemma, which models a noise transition  in simplex  when learning with noisy labels. The objective is then mathematically expressed with the following empirical risk minimization problem

\citet{patrini2017making} theoretically demonstrate Eq.~\eqref{eq:goal} trained with the noisy data shares the same minimizer with Eq.~\eqref{eq:pse_goal} trained with the clean data, if  is accurately estimated. Unfortunately, it is usually impractical to acquire such a  in advance. Thus, subsequent work~\cite{goldberger2016training} adapts the pre-estimation with a Softmax layer along with the training progress. Although this shows a promising performance, expensive tweaking is required due to the ill-posed stochastic approximation as a simple neural layer.

\subsection{Latent Class-Conditional Noise model}
In this section, we will present our Latent Class-Conditional Noise model (LCCN). Specifically, it avoids non-trivially tweaking for the fragile performance in~\cite{goldberger2016training} by modeling  in a Bayesian form. The graphical notation is illustrated in Fig.~\ref{fig:bccn} and the generative procedure is summarized as follows,
\begin{itemize}
\item The latent label , where  is a \emph{Categorical} distribution modeled by the deep neural network  and the given  is its input feature.
\item The transition vector of the th class , where  is the parameter of a  distribution and  constitutes the noise transition matrix.
\item The observed noisy label , where  is a \emph{Categorical} distribution parameterized by .
\end{itemize}

\begin{figure} 
\centering
\includegraphics[width=0.45\textwidth]{bccn.pdf}
\caption{Latent Class-Conditional Noise model.  and  is the observed training pair.  is the latent label.  is the unknown noise transition.  is a Dirichlet parameter.  is the sample number and  is the class number.}\label{fig:bccn}
\end{figure}
The general way to solve such a probabilistic model combined with deep learning is amortized variational inference \cite{kingma2013auto,MnihNVI}. However, this way for LCCN will require an approximate Categorical reparameterization~\cite{jang2017categorical,maddison2016concrete} and introduce an unstable Digamma function to optimize. To avoid this issue, we specifically deduce a dynamic label regression method for optimization and demonstrate its safeguarded update for .

\subsection{Dynamic Label Regression}
In the following, we will give the dynamic label regression method for LCCN, which stacks an \emph{autoencoded Gibbs sampling} to infer the latent labels and loss minimization for parameter learning. It naturally suits LCCN and we show its deduction via a two-step formulation. Note that, in despite of the complex deduction, only a small computational cost is extra introduced, which will be explained in the following section. Simply, the first step is computing the probability of each  conditional on the others \footnote{Note that  means removing the current object statistic from the whole collection of all object statistics.}, i.e., .
Then, with the samples from , the classifier training and the noise modeling can be explicitly decoupled as the following optimization problem,

 is the -clipped cross-entropy loss\footnote{The probabilistic prediction from the model is clipped between  and  for the computational stability, where  is set to  in this paper.} and  is the likelihood loss. Alternating between the sampling of  and the optimization of Eq.~\eqref{eq:minimization} constructs our final algorithm to learn with noisy supervision. Specifically, when  approach the true distribution of clean labels, the classifier training is similar to that on the clean dataset. This yields the asymptotically unbiased estimation as on the clean datasets.

\textbf{Autoencoded Gibbs sampling}. Firstly, according to the aforementioned generative process, we can easily deduce the posterior of  conditioned on the observed training pair  and the Dirichlet parameter . This is implemented by factorizing the target conditional probability based on Fig.~\ref{fig:bccn} and applying the Bayes theorem as follows,

where  represents  to simplify above equation.
If we use the notation  to represent the confusion matrix of the noisy dataset, then we have = and . Putting the later equation into Eq.~\eqref{eq:step1} and then using the conjugation characteristic between the Dirichlet distribution and the Multinomial distribution, the following form can be further deduced,

Unfortunately, Eq.~\eqref{eq:step2} is non-analytical and cannot be used to generate the samples of  directly, which can be solved by Gibbs sampling. According to the Gibbs sampling, we need to compute  first. And then based on , a sequence of observations can be sampled, which are approximately from . The following deduction facilitates Eq.~\eqref{eq:step2} and  to acquire the final conditional probability for our autoencoded Gibbs sampling.

With Eq.~\eqref{eq:step3}, we can sample a collection of latent labels . Such samples are then used to solve the optimization problem in Eq.~\eqref{eq:minimization}. Iterating the procedure of Eq.~\eqref{eq:step3} and Eq.~\eqref{eq:minimization}, we gradually approach the latent label, and at the same time train the classifier and estimate the noise transition. 
  
\begin{lemma}
For a reversible, irreducible and aperiodic Markov chain with state space , let  be the maximal absolute eigenvalue of the state transition matrix and  be the underlying stationary probability measure where . Then, the -mixing time from the initial arbitrary state to the equilibrium is characterized by the following bounds,

where  and  is the total variation distance between two probability measures.
\end{lemma}
The above lemma indicates~\cite{levin2017markov} the mixing time of LCCN is at most constantly linear to the inverse of . Although it is hard for Gibbs sampling to accurately quantify  due to the evolving state transition matrix, the recent work~\cite{johanmixing} shows Gibbs sampling is efficient enough and almost proportional to the logarithm of the dataset size . In experiments, we will show LCCN is well converged after same epochs as baselines. 

In statistical learning theory, the \emph{excess risk}~\footnote{\url{https://en.wikipedia.org/wiki/Risk_difference}} and \emph{the error bound} \textit{w.r.t.} the expected risk and Bayes risk, are two important quantities to measure model generalization performance. In the setting of noisy labels, such two quantities are bounded by the following generalization bound (see the Appendix A)  where  and  respectively represents the expectation on the clean data distribution and the empirical estimation with the data whose labels are from the Gibbs sampling. Thus, by analyzing the upper bound of , we can then understand which factors affect the generalization performance of LCCN. Specifically, we deduce the following theorem to interpret this.
\begin{theorem} \label{theorem}
Assume  and  respectively are the underlying groundtruth labeling functions  of clean test data and data from the Gibbs sampling. Define the composite function class . Then, for any probability , with probability at least ,

where ,  is the Rademacher complexity~\cite{bartlett2002rademacher} of  and  is the maximum of the -clipped cross entropy loss, i.e., .
\end{theorem}
The above theorem indicates the generalization performance of the classifier learned by LCCN depends upon three factors, i.e., the inherent gap  between the noisy training domain and the clean test domain, the function complexity  and the sample number . In particular, if LCCN can exactly infer all the latent labels of noisy data and eliminate the domain bias, we will have  and . In this case, Eq.~\eqref{eq:theorembound} will degenerate to the Rademacher bound~\cite{mansour2009domain} after scaling the loss to , and equal to the training on the clean data. However, it is usually hard to completely remove the domain bias, since the distribution of the corrected samples could still be different from that of the clean test data. For example, the web data may contain many outlier classes. Thus,  is an important factor to the generalization performance of LCCN.

\subsection{Safeguarded Transition Update}
In this section, we will show that our method safeguards the bounded update of the noise transition by a batch of samples, avoiding the arbitrarily tuning via a Softmax layer in~\cite{goldberger2016training}. 
\begin{theorem}\label{theorem:update}
Suppose  is a positive smoothing scalar,  is the current sample number of the th category (i=1,,),  is the sum of the sample numbers newly allocated into (positive) and removed from (negative) the th category after a batch of training samples, and  is its absolute sum of such two cases. Then, for the transition vector  of the th category, its variation via a training batch is characterized by the following equation,

where  and . According to the definition, we have ,  and .
\end{theorem}
\begin{proof}
The variation of  after a training batch is,

\end{proof}
\begin{corollary}
Suppose  is the batch size in the training. If it satisfies the condition , we have  in a small scale. Then the variation of  after a training batch will be bounded by  in a small scale.
\end{corollary}
The core drawback in~\cite{goldberger2016training} is the noise transition modeled by a Softmax layer can be arbitrarily updated via a batch of samples. This is because the gradients of the parameters estimated by a ``local" batch can be arbitrarily large in the backpropagation. Then, the noise transition decided by the ``global" dataset might be pushed into a bad local minimum by a batch of some extremely noisy training samples, yielding a serious harm on the classifier. The later experimental analysis in Fig.~\ref{fig:transvar} will confirm this point. Instead, our dynamic label regression theoretically safeguards the bounded update of the noise transition via a batch of samples. Specifically, with the bounded update, the conditional transition in Equation~\eqref{eq:step3} is gradually changing towards at a true distribution when the classifier is well trained. Similarly, with more reliable sampled labels, the classifier is better trained and the noise modeling is refined. Finally, we acquire a virtuous cycle for optimization.  

\subsection{Complexity Analysis}
The learning procedure is summarized in Algorithm~\ref{alg:dlr}. Note that, we give the complete implementation including details, like pretraining and warming-up used in the experiments.

\begin{algorithm}[t]
  \begin{algorithmic}[1]
  \REQUIRE A noisy dataset , a classifier  modeled by DNN , warming-up steps , the running epoch number  and the batch-size .
  \STATE Directly pretrain the classifier  on the noisy dataset .
  \STATE Compute the warming-up noise transition matrix .
  \FOR{epoch  to }
  \FOR{batch  to }
  \STATE Let step=ij and hook a batch of samples.
  \IF{step  }
  \STATE Substitute the transition in Equation~\eqref{eq:step3} with , and then sample  for each  in the batch.
  \ELSE
  \STATE Sample  with Equation~\eqref{eq:step3} for the batch.
  \ENDIF
  \STATE Update the confusion matrix  based on the existing sampling observations \{\}.
  \STATE Optimize Equation~\eqref{eq:minimization} to learn the classifier  and estimate the noise transition matrix .
  \ENDFOR
  \ENDFOR
  \STATE Output the classifier  and the noise transition .
  \end{algorithmic}
  \caption{Dynamic Label Regression for LCCN}
  \label{alg:dlr}
\end{algorithm}

As we know, the stochastic optimization of a DNN model involves two steps, the forward and backward computations. In each mini-batch update, its time complexity is , where  is the mini-batch size and  is the parameter size. Here, in Algorithm~\ref{alg:dlr}, we additionally add a sampling operation via Eq.~\eqref{eq:step3} whose complexity is  ( is the class size). Note that, the first term in the RHS of Eq.~\eqref{eq:step3} has been computed in the forward procedure. Since  and  is usually significant smaller than , the extra cost for the sampling is negligible compared to . Besides, the optimization for noise modeling in Eq.~\eqref{eq:minimization} can be ignored, as this only involves the normalization of a confusion matrix whose complexity is . In total, since the big-O complexity of each mini-batch remains the same, our method is scalable to big data. 

\begin{figure*}
\centering
\includegraphics[width=0.92\textwidth]{GLCCN.pdf}
\caption{Extensions based on the original safeguarded dynamic label regression for LCCN. The images and noisy labels are respectively input to the classifier and the safeguarded Bayesian noise modeling to compute the prediction (including the outlier component) and the conditional transition. When clean labels are not available as the latent labels, they are sampled based on the product of previous two quantities. Then, the latent labels composite by both the clean parts and those inferred from sampling are used to train the classifier, and only the latent labels inferred from the sampling are used to refine the noise model.}\label{fig:network2}
\end{figure*}

\begin{figure} 
\centering
\includegraphics[width=0.45\textwidth]{bccn_ext.pdf}
\caption{The Generalized Latent Class-Conditional Noise model.  and  is the observed training pair.  is the partially observed latent label (the observed samples are for semi-supervised learning).  is the unknown noise transition (the extra dimension is for outlier learning).  is a Dirichlet parameter.  is the sample number and  is the class number.}\label{fig:outlier_semi}
\end{figure}

\subsection{Extensions on Outlier and Semi-supervised Learning}
In this section, we will extend the original model in Fig.~\ref{fig:bccn} to the generalized version in Fig.~\ref{fig:outlier_semi}, which shares the optimization procedure but is more useful in the real-world applications. 

\textbf{Extension on Outlier Learning:} 
In practise, datasets collected from online websites or real-world scenarios, usually contains the open-set label noise~\cite{wang2018iterative}. That means data from other distributions might be disturbed as the given class samples involving in the training. Previous class-conditional noise model~\cite{reed2014training,xiao2015learning,sukhbaatar2014training,patrini2017making,goldberger2016training}, mainly focus on the closed-set label perturbation and thus can not handle the outlier classes. For example, it is impossible to estimate the transition matrix via the mentioned two-step formulation in~\cite{patrini2017making} since this requires the selection of the representative outlier samples. Similarly, learning the transition matrix by a noise adaptation layer~\cite{goldberger2016training} still suffers from the instability. Therefore, it is useful to extend LCCN to deal with this open-set noisy label setting. Actually, the modification for LCCN only requires to add an outlier choice for the latent variable , i.e., , where  indexes the collapsed outlier classes and then change the noise transition from  to . The model modification is shown in Fig.~\ref{fig:outlier_semi} and the the network modification is illustrated in Fig.~\ref{fig:network2}. Importantly, the above modifications do not alter the aforementioned deduction.

\textbf{Extension on Semi-supervised Learning:} It is common to improve the model performance by augmenting the large scale noisy dataset with a small set of clean samples. Many works~\cite{xiao2015learning,veit2017learning,patrini2017making,pmlr-v80-ren18a,Guo_2018_ECCV} have leveraged such a semi-supervised setting to calibrate the classifier and achieve a better result. In our model, it is naturally compatible with this case, where we can directly utilize the clean labels instead of labels from sampling when they are available. As illustrated in Fig.~\ref{fig:network2}, this configuration is specifically marked in red in parallel to sampling. The corresponding model modification is indicated in Fig.~\ref{fig:outlier_semi}. In a broad sense, clean labels can be as accurate as a given category or as weak as a coarse hint that tells outlier or not. In the former case, a standard cross entropy loss can be applied to the classifier; while in the latter case, a collapsed cross entropy loss that defines on outliers \textit{vs.} non-outliers could be applied. Since this is tightly related to the work on domain adaptation~\cite{Csurka2017DomainAF}, we leave the latter case in the future and only validate the former semi-supervised setting in this paper.

\section{Experiments}
The experiments involve both the simulated noisy datasets and the real-world noisy datasets. We verify the performance of our model by comparing with state-of-the-art methods.

\subsection{Datasets and Baselines}
\subsubsection{Datasets}
We conduct the toy experiments on CIFAR-10, CIFAR-100 and the real-world experiments on Clothing1M and WebVision.
CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning} respectively consist of 60,000 32x32 color images from 10 and 100 classes. Both of them contain 50,000 training samples and 10,000 test samples. For the toy experiments without outliers, we inject the asymmetric noise to disturb their labels to form the noisy datasets. Concretely, on CIFAR-10, we set a probability  to disturb the label to its similar class, i.e., truck  automobile, bird  airplane, deer  horse, cat  dog. For CIFAR-100, a similar  is set but the label flip only happens in each super-class. The label is randomly disturbed into the next class circularly within the super-classes. For the toy experiments that consider the open-set noisy labels, we randomly select 10,000 samples from the original datasets and shuffle the order of the pixel values as the outliers. In the semi-supervised learning, we utilize the clean labels of the first 5,000 clean samples and the first 500 outlier samples for the training.


Clothing1M~\cite{xiao2015learning} dataset has 1 million noisy clothes samples collected from the shopping websites. The authors in~\cite{xiao2015learning} pre-defined 14 categories and assigned the clothes images with the labels extracted from the surrounding text provided by sellers, which thus might be very noisy. According to~\cite{xiao2015learning}, only about  labels are reliable. Besides, this dataset contains 50k, 14k and 10k clean samples respectively for auxiliary training, validation and test. WebVision\footnote{Due to the reason of time and the computational resource, in this paper, we only use the original WebVision 1.0 dataset. The newest version, namely WebVision 2.0 dataset, contains more images and more classes.}~\cite{li2017webvision} is a more challenging noisy dataset, which contains more than 2.4 million images. It is crawled from the Internet by using the 1,000 concepts of ILSVRC~\cite{imagenet_cvpr09} as queries. In addition, a clean validation set which contains 50,000 annotated images, are provided to boost and validate the proposed models in diverse applications. We use the validation set of ImageNet~\cite{imagenet_cvpr09} as its test set.

\begin{table*} [t]
\centering
\caption{The average accuracy () over 5 trials on CIFAR-10 and CIFAR-100 with different noise levels.} 
{
\scalebox{1.2}{
    \begin{tabular}{ c | c | c  c  c  c  c | c  c  c  c  c }
    \hline
    \multicolumn{2}{c|}{Dataset} & \multicolumn{5}{|c|}{CIFAR-10} & \multicolumn{5}{|c}{CIFAR-100} \\ \hline\hline
    & Method \textbackslash~Noise Ratio & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\ \hline
  1 & CE & 90.10 & 88.12 & 76.93 & 59.01 & 56.85 & 66.15 & 64.31 & 60.11 & 51.68 & 33.37 \\ \hline
  2 & Bootstrapping & 90.73 & 88.12 & 76.29 & 57.04 & 56.79 & 66.48 & 64.61 & 63.01 & 55.27 & \textbf{34.52} \\ \hline
 3 & Forward & 90.86 & 89.03 & 82.47 & 67.11 & 57.29 & 65.43 & 62.72 & 61.28 & 52.64 & 33.82 \\ \hline
 4 & S-adaptation & 91.02 & 88.83 & 86.79 & 72.74 & 60.92 & 65.52 & 64.11 & 62.39 & 52.74  & 30.07 \\ \hline
 5 & LCCN & \textbf{91.35} & \textbf{89.33} & \textbf{88.41} & \textbf{79.48} & \textbf{64.82} & \textbf{67.83} & \textbf{67.63} & \textbf{66.86} & \textbf{65.52} & 33.71 \\ \hline \hline
 6 & CE with the clean data & \multicolumn{5}{|c|}{91.63} & \multicolumn{5}{|c}{69.41}  \\ \hline
 \end{tabular}}
}
\label{tab:cifar}
\end{table*}
 
 \begin{table*} [t]
\centering
\caption{The average accuracy () over 5 trials on outlier-corrupted CIFAR-10 and CIFAR-100 with different noise levels.} 
{
\scalebox{1.2}{
    \begin{tabular}{ c | c | c  c  c  c  c | c  c  c  c  c }
    \hline
    \multicolumn{2}{c|}{Dataset} & \multicolumn{5}{|c|}{CIFAR-10} & \multicolumn{5}{|c}{CIFAR-100} \\ \hline\hline
    & Method \textbackslash~Noise Ratio & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\ \hline
  1 & CE & 89.13 & 87.06 & 74.63 & 62.29 & \textbf{57.07} & 62.94 & 59.73 & 54.71 & 45.57 & 31.74 \\ \hline
  2 & Bootstrapping & \textbf{90.13} & 84.58 & 74.76 & 54.87 & 55.56 & 63.73 & 60.88 & 59.77 & 40.23 & 31.86 \\ \hline
 3 & Forward & 88.63 & 84.97 & 78.47 & 58.23 & 56.52 & 63.69 & 62.63 & 61.86 & 51.47 & \textbf{35.71} \\ \hline
 4 & S-adaptation & 88.58 & 87.28 & 61.17 & 57.12 & 56.73 & 63.51 & 61.50 & 60.59 & 53.22  & 32.19 \\ \hline
 5 & LCCN & 88.63 & 88.06 & 82.15 & 69.48 & 55.12 & 63.97 & 62.84 & 61.79 & 60.34 & 33.52 \\ \hline 
 6 & LCCN* & 89.59 & \textbf{88.43} & \textbf{84.34} & \cellcolor{black!25}\textbf{72.33} & \cellcolor{black!25}56.28 & \textbf{64.71} & \textbf{63.05} & \textbf{62.48} & \textbf{62.02} & \cellcolor{black!25}32.37 \\ \hline  \hline
 7 & LCCN+ & 90.30 & 88.93 & 88.21 & \cellcolor{black!25}87.42 & \cellcolor{black!25}86.33 & 65.67 & 64.24 & 63.52 & 63.19 & \cellcolor{black!25}62.39 \\ \hline 
    \end{tabular}}
}
\label{tab:cifar-outlier}
\end{table*}

\subsubsection{Baselines} For the toy experiments, we compare \textbf{LCCN} with the classifier that is directly trained on the dataset (termed as \textbf{CE}), the method \textbf{Bootstrapping} proposed in~\cite{reed2014training}, the transition based method \textbf{Forward}~\cite{patrini2017making} and the method that fine-tunes the transition \textbf{S-adaptation}~\cite{goldberger2016training}. Note that, we choose the hard mode for Bootstrapping, since it is empirically better than the soft mode. In the outlier corrupted datasets, we denote the extension of our model that considers the outlier as \textbf{LCCN*}. In the semi-supervised learning setting, we denote our model as \textbf{LCCN+}, meaning the clean samples are used in the training. For the experiments on real-world datasets, we also report the result of \textbf{Joint Optimization}~\cite{tanaka2018joint} that leverages the auxiliary noisy label distribution and the state-of-the-art result \textbf{Forward+}~\cite{patrini2017making} that finetunes on clean samples.

\subsection{Implementation Details}
For CIFAR-10 and CIFAR-100, the PreAct ResNet-32~\cite{he2016deep} is adopted as the classifier. The image data is augmented by horizontal random flip and 3232 random crops after padding with 4 pixels. Then, the per-image standardization is used to normalize pixel values. For the optimizer, we utilize SGD with a momentum of 0.9 and a weight decay of . The batch size is set to 128. The training runs totally 120 epochs and is divided into three phases in 40 and 80 epochs. In these three phases, we respectively set the learning rate as 0.5, 0.1 and 0.01. Note that, the reason that we adopt a large learning rate (others may set the learning rate smaller than 0.001), is that the small learning rate will lead to overfitting on the noisy dataset as claimed in~\cite{arpit2017closer}.
Following the benchmark in~\cite{patrini2017making}, we use CE to initialize the classifier in other baselines and LCCN. For S-adaptation, the following transition is computed to warm-up the transition parameters in the first 80 epochs.

Similarly on CIFAR-10, we use above transition to warm up the sampling procedure in LCCN for the first 20,000 steps. However, on CIFAR-100, we set  in the warming-up since Equation~\eqref{eq:warm} will induce the high sampling variance and need long time to converge. 

For Clothing1M and WebVision, the ResNet-50 is leveraged as the classifier. We resize the short side of their images to 224 and do the random crop of 224224. The training images are augmented with the random flip, whiteness and saturation.
For the optimizer, we deploy SGD with a momentum of 0.9 with a weight decay of . The batch size for Clothing1M is set to 32 and we fix the learning rate as 0.001 to run 5 epochs. For the warming-up transition, we both validate the one~\cite{xiao2015learning} from manual annotation and the one estimated by Equation~\eqref{eq:warm} for 40,000 steps. Note that, on the large real-world datasets, due to the strong capacity of ResNet-50, it is easy for LCCN to occur the sampling collapsed problem, i.e., the sampled latent label is identical to the noisy label. Thus, we norm Equation~\eqref{eq:step3} with a power annealed coefficient  to introduce the sufficient perturbation in avoid of this issue. On WebVision, the batch size is set to 128, and the learning rate is initialized with 0.1 is divided by 10 every 30 epochs until 90 epochs. We use the diagonal transition for 10,000 steps of warming-up and then update the confusion matrix to the end, since it contains 1,000 categories. The similar power-annealed strategy for sampling is leveraged. Finally, to fairly compare LCCN+ and Forward+ in semi-supervised learning, we use the similar fine-tuning in~\cite{patrini2017making} to run the experiments.

\subsection{Results on CIFAR10 and CIFAR-100}
\subsubsection{Classification experiments}
Table~\ref{tab:cifar} summarizes the performance of LCCN and baselines on the noisy datasets without outilers. Compared with the baselines, LCCN achieves the best performance at most of noise levels. In particular, even in the large noise rates, our model still acquires the competitive classification accuracy. For example, when =0.7 on CIFAR-10 and =0.4 on CIFAR-100, LCCN reaches  and , outperforming the best results of baselines by about 7 and 13 respectively. This demonstrates that our model is significantly better than baselines. Regarding =0.5 on CIFAR-100, the way to disturb the labels~\cite{patrini2017making} leads that there is one undesired minimum, since two classes are mixed into one class by equal quota after injecting noise. In this case, it is hard to say which model can achieve the best result. We include it here for the complete comparison as in other works~\cite{patrini2017making,goldberger2016training}.

\begin{figure*}[ht]
\centering
\centering
\includegraphics[width=0.96\textwidth]{transition.pdf}
\caption{The colormap of the confusion matrix on CIFAR-10 with r=0.5. We utilize the log-scale for each element in the confusion matrix for the fine-grained visualization. The left three maps are respectively learned by LCCN at the beginning, 30,000 step and the end, and the right one is the groundtruth.}
\label{fig:transition}
\end{figure*}

\begin{figure} 
\centering
\includegraphics[width=0.46\textwidth]{training.pdf}
\caption{The training loss (left) and the test accuracy (right) of LCCN on the CIFAR-10 dataset with different noise rates .}\label{fig:training}
\end{figure}


Table~\ref{tab:cifar-outlier} presents the results of LCCN and baselines on the outlier-corrupted datasets. Compared to those in Table~\ref{tab:cifar}, all the methods have a slight performance drop. Nevertheless, LCCN achieved the best performance in such an setting at =0.3, 0.5, 0.7 on CIFAR-10 and =0.1, 0.2, 0.3 and 0.4 on CIFAR-100. The baselines without the outlier detection mechanism usually have a significant degeneration. Specifically, on CIFAR-10, all the other baselines are even not better than CE, i.e., directly training. Instead, LCCN* that considers the outlier achieves a further improvement based on LCCN. Besides, as expected, by adding clean data, LCCN+ performs better than LCCN*, since the classifier is calibrated by the information of the clean data domain.
In the scenario of the extreme noise, as marked by the grey color in Table~\ref{tab:cifar-outlier}, this is quite useful and even necessary to guarantee an acceptable performance. In total, according to the quantitative analysis of Table~\ref{tab:cifar} and Table~\ref{tab:cifar-outlier}, we demonstrate the superiority of LCCN compared to baselines on toy datasets.

\subsubsection{On convergence visualization}
In Fig.~\ref{fig:training}, we trace the training of LCCN on CIFAR-10 to visualize its convergence. As can been in the left panel of Fig.~\ref{fig:training}, LCCN has a stable convergence on loss after the given epochs. Besides, we find the loss converges to irregular scales in different noise rates. Concretely, in most cases, i.e., =0.1, 0.3, 0.5, the final training loss increases as  increases, while the loss shows attenuation as 0.5. It is because in the low-level noise, the model can easily correct the labels via the sampling in LCCN, yielding a small loss. While in the case of the extreme noise, it is more challenging to prevent the model from fitting on noise, which incurs difficulty for optimization and thus achieves a big loss. Furthermore, according to the right penal of Fig.~\ref{fig:training}, the test accuracy also approximately converges and persists to the end of the training without performance drop. Actually, it is not a common phenomenon for previous methods to own this merit, since all baselines tends to overfitting on noise more or less in the final few epochs. This demonstrates the advantages of LCCN in the robust training with the noisy datasets.

\begin{figure} 
\centering
 \includegraphics[width=0.43\textwidth]{trans_var.pdf}
\caption{The test accuracy of LCCN and S-adaptation in the training on CIFAR-10 with =0.5 (left), and the corresponding histograms for the change of noise transition  via a mini-batch of samples (right).}\label{fig:transvar}
\end{figure} 

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{label_trace.pdf}
\caption{The label correction ratio in the training of LCCN on CIFAR-10 with =0.5 as well as some negatively corrected samples (the red box) and some positively corrected samples (the green box) with the high probabilities.}
\label{fig:label_trace}
\end{figure}

\begin{figure*}[ht] 
\centering
\includegraphics[width=0.95\textwidth]{outlier.pdf}
\caption{Some representative samples in the training set that are considered as the outliers by LCCN*. We intuitively summarize these photos into four categories based on their contents, multiple different objects (RED), implicit categories (GREEN), uncertain types (BLACK) and confusing appearance (BLUE), which are respectively marked by the color of the surrounded boxes. Outliers are relative to LCCN and may contain hard examples of the pre-defined 14 categories.}\label{fig:clothing1M_outlier}
\end{figure*}

\subsubsection{Safeguarded transition update}
To show LCCN safeguards the noise transition update compared to S-adaptation, we compute the statistics about their update of noise transition on CIFAR-10 at =0.5, and illustrate the histogram of changes in Fig.~\ref{fig:transvar}. Firstly, from the left panel of Fig.~\ref{fig:transvar}, we can see that there is a significant performance drop in the training of S-adaptation. The clue to this phenomenon can be found by inspecting the update of noise transition. As shown in the right panel of Fig.~\ref{fig:transvar}, the change magnitude of  in S-adaptation is higher than that of LCCN. One is in a large scale ranging from 0 to 16, while the other one is in a very small scale ranging from 0 to 0.02. This leads to S-adaptation suffering from a high risk of over-tuning to undesired local minimums in the presence of noise. Instead, according to  the histogram, LCCN updates  in a safeguarded small scale when approaching to the minimum. In summary, this quantitative analysis confirms the claim of our Theorem~\ref{theorem:update} in the perspective of experiments.

\subsubsection{The latent label and noise analysis}
Fig.~\ref{fig:transition} and Fig.~\ref{fig:label_trace} respectively depict the colormap of the confusion matrix and the label correction ratio when training LCCN on CIFAR-10 with =0.5. First, as can be seen in Fig.~\ref{fig:transition}, the initial confusion matrix does not approach the true matrix and there are many incorrect entries. However, with the training progressing, the matrix is gradually corrected and at the end of training, it is approximately similar to the provided groundtruth. Besides, As shown in Fig.~\ref{fig:label_trace}, the ratio of the image with the correct label increases along with the training progress. This reflects LCCN successfully models the class-conditional noise and gradually infer the latent labels. Specially, by visualizing the mis-corrected examples in the training process, we can find that the classifier at first make mistakes in even some simple samples, while finally has the wrong classification in only the hard examples. These two figures visualize how the dynamic label regression optimizes LCCN to infer the latent label and model the noise.

\begin{table} 
\caption{The average accuracy over 5 trials on Clothing1M.}
\begin{center}
{
    \begin{tabular}{ c | c | c }
    \hline
      & Method & Accuracy \\
     \hline
     \hline
     1 & CE & 68.94 \\
     \hline
     2 & Bootstrapping & 69.12\\
     \hline
     3 & Forward & 69.84 \\
     \hline
     4 & S-adaptation & 70.36 \\
     \hline
     5 & Joint Optimization & 72.16 \\
     \hline
     \multirow{3}{*}{6} & LCCN & 71.63 \\
     & LCCN warmed-up by  in~\cite{xiao2015learning}  & \textbf{73.07} \\
     & LCCN* & \textbf{72.80} \\
     \hline
     \hline
     7 & CE on the clean data & 75.28 \\
     \hline
     8 & Forward+ & 80.38 \\
     \hline
     9 & LCCN+ & \textbf{81.25} \\
     \hline
    \end{tabular}
}
\end{center}
\label{tab:clothing1M}
\end{table}

\subsection{Results on Clothing1M and WebVision}
Table~\ref{tab:clothing1M} lists the performance of LCCN and baselines on the large-scale Clothing1M. According to the results, we can see that Forward does not show the significant improvement in this dataset, even though they use the annotated noise transition matrix~\cite{xiao2015learning}. And S-adaptation only improves Forward by 0.5. Joint Optimization that trains the classifier with label correction~\cite{tanaka2018joint} achieves better results than the other baselines. Nevertheless, this method requires the provided noisy label distribution to prevent degeneration and is not scalable to the large number of classes~\cite{tanaka2018joint}. Instead, LCCN that contains both the label correction and Bayesian noise modeling, gets the competitive performance 71.63. With the warming-up of the auxiliary noise transition~\cite{xiao2015learning}, it further achieves the best 73.07. Even although there is no auxiliary information available, our extension LCCN* still outperforms the current state-of-the-art result. This demonstrates the potential of LCCN in handling the real-world noisy dataset. In addition, the results of LCCN+ indicates our model also has the advantages in the semi-supervised learning setting. 

\begin{table}
\caption{The learned noise transition on Clothing1M by LCCN.}
\centering
\renewcommand{\tabcolsep}{0.7mm}
{ \scalebox{1.15}{
\begin{tabular}{c|*{14}{E}|c}
 \multicolumn{1}{c}{class} & \multicolumn{1}{c}{1} 
  & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} 
  & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{12} & \multicolumn{1}{c}{13} & \multicolumn{1}{c}{14} & \multicolumn{1}{c}{} \\ \hhline{~*{14}{|-}|}
  1 & 77 & 0 & 6 & 0 & 0 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & {\tiny\bf T-shirt} \\ 
  2 & 2 & 88 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & {\tiny\bf shirt}\\ 
  3 & 12 & 4 & 48 & 4 & 7 & 7 & 0 & 0 & 0 & 0 & 0 & 5 & 0 & 0 & {\tiny\bf knitwear}  \\ 
  4 & 9 & 15 & 5 & 51 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 9 & 0 & 0 & {\tiny\bf chiffon}\\ 
  5 & 11 & 4 & 43 & 0 & 16 & 9 & 0 & 0 & 0 & 0 & 0 & 4 & 0 & 0 & {\tiny\bf sweater} \\ 
  6 & 3 & 0 & 3 & 0 & 0 & 86 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & {\tiny\bf hoodie}\\ 
  7 & 0 & 0 & 0 & 0 & 0 & 0 & 87 & 3 & 0 & 5 & 0 & 0 & 0 & 0 & {\tiny\bf windbreaker} \\ 
  8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 92 & 0 & 0 & 0 & 0 & 0 & 0 & {\tiny\bf jacket}\\ 
  9 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 0 & 96 & 0 & 0 & 0 & 0 & 0 & {\tiny\bf downcoat}\\ 
  10 & 0 & 0 & 0 & 0 & 0 & 0 & 4 & 2 & 0 & 91 & 0 & 0 & 0 & 0 & {\tiny\bf suit}\\ 
  11 & 0 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 81 & 3 & 0 & 0 & {\tiny\bf shawl}\\ 
  12 & 0 & 0 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 84 & 3 & 0 & {\tiny\bf dress}\\ 
  13 & 0 & 0 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 6 & 76 & 3 & {\tiny\bf vest}\\ 
  14 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 95 & {\tiny\bf underwear}\\ 
  15 & 12 & 10 & 0 & 7 & 0 & 9 & 6 & 4 & 0 & 7 & 0 & 14 & 6 & 3 & {\tiny\bf outlier}\\ \hhline{~*{14}{|-}|}
\end{tabular}}
}
\label{tab:confusion}
\end{table}

In Table~\ref{tab:confusion}, we present the noise transition matrix learned by LCCN*, where only significant transition probabilities are marked. From this noise transition, we can find the training samples in some classes are very noisy. For example, knitwear and sweater are two classes which transit most of labels to other classes. This is because such two classes usually occur with other categories like jacket or shawl in the common dress collocation, which may incur the label transition according to the visual appearance. Besides, in Table~\ref{tab:confusion}, we can observe the outlier transition to find which class contains a lot of outliers. Furthermore, to better understand the outliers, we give some represenative samples in Fig.~\ref{fig:clothing1M_outlier}. According to the image contents, we intuitively summarize the outlier into four sub-classes, multiple different objects, implicit categories, uncertain types and confusing appearance. As can be seen, it is usually improper to asign an unique label to these outliers, since some may contain multiple kinds of clothes. And in some challenging cases, e.g., the images in the blue box in Fig.~\ref{fig:clothing1M_outlier} , the hard example is also considered as the outliers by LCCN*, even if the label is correct. Actually, this can be seen as the imperfect sample for training or the potential drawback of our model that requires more explore in the future research. 


\begin{table}[ht]
\caption{The average accuracy over 5 trials on WebVision.}
\begin{center}
{\begin{tabular}{ c | c | c | c}
    \hline
      & Method & Accuracy1 & Accuracy5 \\
     \hline
     \hline
     1 & CE & 58.61 & 80.94 \\
     \hline
     2 & Bootstrapping & 58.48 & 80.81\\
     \hline
     3 & Forward & \textbf{58.93} & 81.06 \\
     \hline
     4 & S-adaptation & 58.00 & 80.16 \\
     \hline
     \multirow{2}{*}{5} & LCCN & 58.73 & \textbf{81.25} \\
     & LCCN* & \textbf{59.09}  & \textbf{81.33} \\
     \hline
     \hline
     6 & CE on the clean data & 53.52 & 77.84\\
     \hline
     7 & LCCN+ & 59.72 & 80.34\\
     \hline
    \end{tabular}
}
\end{center}
\label{tab:webvision}
\end{table}

Table~\ref{tab:webvision} gives the performance of LCCN and baselines on a more challenging noisy dataset WebVision. Both Top-1 and Top-5 accuracies are reported in the experiment. According to the results either in the perspective of Top-1 accuracy or Top-5 accuracy, LCCN achieves the best performance. Similarly, LCCN* and LCCN+ respectively have the further refinement based on LCCN. Nevertheless, as can be seen, the results of all methods do not present the significant gap. One possible explanation is that this task couples two challenging sub-tasks, perfectly decoupling the clean samples from the noisy dataset in the 1000 classes and well fitting the clean samples in the 1000 classes. From the current limiting performance of image recognition in ImageNet~\cite{imagenet_cvpr09}, we know either sub-task mentioned above needs a long way to go for a satisfying result in such a large-scale challenging scenario.

\section{Conclusion and Future Work}
In this paper, we present a Latent Class-Conditional Noise model to learn with the noisy supervision. Besides, a dynamic label regression method is deployed for LCCN to iteratively infer the latent labels and jointly train the classifier and model the noise. The theoretical analysis on the model convergence and the essential gap between training and test are also provided. Most importantly, we demonstrate that our method safeguards the bounded update of the noise transition to avoid previous arbitrarily tuning via a mini-batch of samples. Finally, we generalize our model to the open-set noisy labels setting and the semi-supervised learning setting. However, although we have shown the advantages of LCCN in a range of experiments with the generalized noisy supervision, other specific settings that considers more complex noise, e.g., image content based noise, could be explored. Besides, it is important to explore more effective models on the large scale noisy dataset. To the end, more works based on LCCN can be extended to train with noisy datasets.


\appendices
\section{Proof of  regarding to the two quantities}
By definition, the excess risk of the estimated model  is directly bounded by the absolute supremum  as follows,

Assume the Bayes optimal classifier is . Since  minimizes the empirical loss on , we have the following inequality,

Then, for the error bound \textit{w.r.t.} the expected risk and the Bayes risk, we can deduce with above inequalities as follows,

Eq.~\eqref{eq:excess_risk} and Eq.~\eqref{eq:error_bound} show the supremum of both the excess risk and the error bound are characterized by . Thus, we can universally analyze the corresponding upper bound of  to investigate the generalization performance of LCCN.

\section{Proof of Theorem~\ref{theorem}}
Assume  and  are the underlying groundtruth labeling functions  of clean test data and data from the Gibbs sampling respectively. Then, the  can be reformulated by applying these notations and further deduced as follows,

The second term  in the right-hand side of Eq.~\eqref{eq:theorem2_deduce1} is the popular discrepancy distance. It has been demonstrated by the following Rademacher bound~\cite{mansour2009domain} for any probability ,

where  is defined by the composite functional class  and  is the sample number. Then, combined with the given Rademacher bound, we finally proof the Theorem~\ref{theorem}. i.e., for any probability , the generalization bound of the models for learning with noisy labels is 

This bound theoretically points out three important factors to affect our model generalization performance, i.e., domain gap, the function complexity and the support sample number.







\ifCLASSOPTIONcaptionsoff
  \newpage
\fi







\bibliographystyle{IEEEtranN}
\bibliography{references}








\begin{IEEEbiography}{Michael Shell}
Biography text here.
\end{IEEEbiography}

\begin{IEEEbiographynophoto}{John Doe}
Biography text here.
\end{IEEEbiographynophoto}



\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}









\end{document}
