\documentclass{article}


\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}
\usepackage{amsmath, amsthm}\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}

\usepackage{fullpage}\usepackage{amsfonts}

\usepackage[export]{adjustbox}



\usepackage[ruled,vlined,linesnumbered, algo2e]{algorithm2e}
\usepackage{framed}

\usepackage{tikz}
\newcommand{\abs}[1]        {\left| #1\right|}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\inn}[1]{\langle
#1\rangle}
\newcommand{\br}[1]{\left\{#1\right\}}                            \newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}
\newcommand{\REAL}{\ensuremath{\mathbb{R}}}
\newcommand{\cost}{\ensuremath{\mathrm{cost}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\OPT}{\ensuremath{\mathrm{opt}}}
\newcommand{\var}{\ensuremath{\mathrm{var}}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\epp}{\eps}
\newcommand{\N}{N}
\newcommand{\q}{\{q\}}
\newcommand{\err}[1]{\hat{R}#1}
\renewcommand{\j}{j}
\newcommand{\algkmean}{\textsc{-Mean-Coreset}}
\newcommand{\onemean}{\textsc{OneMeanCoreset}}
\newcommand{\nnz}{\mathrm{nnz}}
\newcommand{\opt}{\OPT}
\newcommand{\bigdata}{\text{Big Data}}

\title{-Means for Streaming and Distributed Big Sparse Data}
\date{}
\author{
Artem Barger \and Dan Feldman \\
\small \hspace{-4.1cm}artem@barger.net  \hspace{1.2cm} dannyf.post@gmail.com\
            \cost(P ,Q) := \sum_{p \in P'}u(p)\cdot\dist^2(p,Q) + \rho,
        
\mu(P)=\frac{1}{\sum_{p'\in P'} u(p')}\sum_{p\in P'}u(p)\cdot p.

(1-\eps)\cost(P,Q) \leq \cost(S,Q) \leq (1+\eps)\cost(P,Q),

    \begin{split}
    (1 - \eps)\sum_{p \in P} u(p)\dist^2(p, Q) +\rho \leq \sum_{p \in S}w(p)\cdot \dist^2(p, Q) + \phi
    \leq (1 + \eps) \sum_{p \in P}u(p)\cdot\dist^2(p, Q) + \rho.
    \end{split}

\cost(P,x)=\cost(P,\mu(P))+\norm{\mu(P)-x}^2\sum_{p\in P}u(p).

\begin{split}
\cost(P,x)-\rho
&=\sum_{p\in P}u(p)\norm{p-x}^2
=\sum_{p\in P}u(p)\norm{(p-\mu(P))+(\mu(P_i)-x)}^2\\
&=\sum_{p\in P}u(p)\norm{p-\mu(P)}^2 +\sum_{p\in P}u(p)\norm{\mu(P)-x}^2+ 2(\mu(p)-x)\cdot \sum_{p\in P}u(p)(p-\mu(P)).
\end{split}

\begin{split}
\sum_{p\in P}u(p)(p-\mu(P))
&=\sum_{p\in P}u(p)\cdot p-\sum_{p\in P}u(p)\mu(P)
=\sum_{p\in P}u(p)\cdot p-\sum_{p\in P}u(p)\cdot p=0.
\end{split}

\cost(P,x)
=\rho+\sum_{p\in P}u(p)\norm{p-\mu(P)}^2 +\sum_{p\in P}u(p)\norm{\mu(P)-x}^2
=\cost(P,\mu(P))+\norm{\mu(P)-x}^2\sum_{p\in P}u(p).
\label{eq55}
 \cost(P,Q)\leq \min_{q \in Q}\cost(P, \q) \leq \cost(P,Q)\cdot\frac{1+2\eps}{1-2\eps}+\frac{\opt(P,1)-\opt(P,k)}{(1-2\eps)\eps}.
\label{left}
\begin{split}
\cost(P,Q)-\min_{q \in Q}\cost(P, \q)
&=\sum_{p\in P'}\min_{q\in Q}u(p)\norm{p-q}^2-\sum_{p\in P'}u(p)\norm{p-q^*}^2\\
&=\sum_{p\in P'}u(p)\left(\min_{q\in Q}\norm{p-q}^2-\norm{p-q^*}^2\right)
\leq 0.
\end{split}

\min_{q \in Q}\cost(P, \q)-\cost(P,Q)
=\sum_{p\in P'}u(p)\norm{p-q^*}^2-\sum_{p\in P'}u(p)\norm{p-q_p}^2.

&\sum_{p\in P'_i}u(p)\norm{p-q^*}^2-\sum_{p\in P'_i}u(p)\norm{p-q_p}^2
\label{aa}=\sum_{p\in P'_i}u(p)\norm{p-\mu(P_i)}^2+\norm{\mu(P_i)-q^*}^2\sum_{p\in P'_i}u(p)\\
&\label{cc}-\left(\sum_{p\in P'_i}u(p)\norm{p-\mu(P_i)}^2+\norm{\mu(P_i)-q^i}^2\sum_{p\in P'_i}u(p)\right)\\
&\label{bb}=\sum_{p\in P'_i}u(p)\left(\norm{q_p^*-q^*}^2
-\norm{q_p^*-q_p}^2\right),

&\nonumber\sum_{p\in P'}u(p)\norm{p-q^*}^2-\sum_{p\in P'}u(p)\norm{p-q_p}^2
=\sum_{p\in P'}u(p)\left(\norm{q_p^*-q^*}^2
-\norm{q_p^*-q_p}^2\right)\\
&=\sum_{p\in P'}u(p)\left(
\norm{(q_p^*-\mu(P))+(\mu(P)-q^*)}^2
-\norm{(q_p^*-\mu(P))+(\mu(P)-q_p)}^2\right)\\
&\label{kkc}=\sum_{p\in P'}u(p)\left(\norm{\mu(P)-q^*}^2-\norm{\mu(P)-q_p}^2\right)\\
&-2\sum_{p\in P'}u(p)(q_p^*-\mu(P))(q^*-q_p).

\begin{split}
\left(\norm{\mu(P)-q^*}^2-\norm{\mu(P)-q}^2\right)\sum_{p\in P'}u(p)
&=\cost(P, \{q^*\})-\cost(P,\mu(P))
-\left(\cost(P, \q)-\cost(P,\mu(P))\right)\\
&=\cost(P, \{q^*\})-\cost(P, \q)\leq 0.
\end{split}

\norm{\mu(P)-q^*}^2-\norm{\mu(P)-q_p}^2\leq 0.

&\sum_{p\in P'}u(p)\norm{p-q^*}^2-\sum_{p\in P'}u(p)\norm{p-q_p}^2\leq -2\sum_{p\in P'}u(p)(q_p^*-\mu(P))(q^*-q_p)\\
\label{cau}&\leq 2\sum_{p\in P'}u(p)\norm{q_p^*-\mu(P)}\norm{q^*-q_p}
=\sum_{p\in P'}u(p)\cdot 2\cdot\frac{\norm{q_p^*-\mu(P)}}{\sqrt{\eps}}\cdot \sqrt{\eps} \norm{q^*-q_p}\\
\label{ggt}&\leq \sum_{p\in P'}u(p) \left(\frac{\norm{q_p^*-\mu(P)}^2}{\eps}
+\eps\norm{q^*-q_p}^2\right)\\
&\label{kka}=\frac{1}{\eps}\sum_{p\in P'}u(p)\norm{q_p^*-\mu(P)}^2+\eps\sum_{p\in P'}u(p)\norm{q^*-q_p}^2,
\label{aacd}
\begin{split}
&\sum_{p\in P'}u(p)\norm{q_p^*-\mu(P)}^2
=\sum_{i=1}^k \norm{\mu(P_i)-\mu(P)}^2\sum_{p\in P'_i}u(p)
=\sum_{i=1}^k \left(\sum_{p\in P'_i}u(p)\norm{p-\mu(P)}^2
- \sum_{p\in P'_i}u(p)\norm{p-\mu(P_i)}^2\right)\\
&=\sum_{p\in P'}u(p)\norm{p-\mu(P)}^2
- \sum_{i=1}^k \sum_{p\in P'_i}u(p)\norm{p-\mu(P'_i)}^2
\leq \opt(P,1)-\opt(P,k).
\end{split}

\begin{split}
\sum_{p\in P'}u(p)\norm{q^*-q_p}^2
&=\sum_{p\in P'}u(p)\norm{(q^*-p)+(p-q_p)}^2\\
&\leq \sum_{p\in P'}u(p)\left(2\norm{q^*-p}^2+2\norm{p-q_p}^2\right)
=2\cdot\left(\cost(P,\{q^*\})+\cost(P,Q))\right).
\end{split}

\begin{split}
\cost(P,\{q^*\})-\cost(P,Q)
&=\sum_{p\in P'}u(p)\norm{p-q^*}^2-\sum_{p\in P'}u(p)\norm{p-q_p}^2 \\
&\leq \frac{\opt(P,1)-\opt(P,k)}{\eps}+2\cdot\eps\left(\cost(P,\{q^*\})+\cost(P,Q)\right).
\end{split}

\begin{split}
\cost(P,\{q^*\})&\leq \cost(P,Q)\cdot\frac{1+2\eps}{1-2\eps}+\frac{\opt(P,1)-\opt(P,k)}{(1-2\eps)\eps}\end{split}
\label{eqlemmaone}
        (1-\eps) \min_{q \in Q}\cost(P, \q)  \leq \min_{q \in Q}\cost (S, \q)  \leq (1+\eps) \min_{q \in Q} \cost(P, \q)
    
        \min_{q \in Q}\cost(S, \q)
        = \cost(S, \{q_S\})
        \leq \cost (S, \{q_P\})
         \leq (1 + \eps)\cost(P, \{q_P\})
        = (1 + \eps)\min_{q \in Q}\cost(P, \q), \label{finCoresetA}

\begin{split}
(1 - \eps)\min_{q \in Q}\cost(P, \q)
=(1 - \eps)\cost(P, \{q_P\})
& \leq (1 - \eps)\cost(P, \{q_S\})
\leq (1 - \eps)(1+\eps)\cost(S, \{q_S\})\\
&=(1-\eps^2) \min_{q \in Q}\cost(S, \q)
\leq \min_{q \in Q}\cost(S, \q).
\end{split}

            \cost(P_i,Q)\leq \min_{q \in Q}\cost(P_i, Q)
                \leq \cost(P_i,Q)\cdot \frac{1+2\eps}{1-2\eps}+\frac{\opt(P_i,1)-\opt(P_i,k)}{(1-2\eps)\eps}.
        \label{ee}
\begin{split}
            \cost(P,Q)\leq \sum_{i=1}^m \min_{q \in Q}\cost(P_i, Q)                
   &\leq             \cost(P,Q)\cdot  \frac{1+2\eps}{1-2\eps}+\frac{1}{(1-2\eps)\eps}
   \sum_{i=1}^m \left(\opt(P_i,1)-\opt(P_i,k)\right).
    \end{split}

\sum_{i=1}^m \opt(P_i,k)=\sum_{i=1}^m \cost(P_i,Q_i)\geq  \sum_{i=1}^m \cost(P_i,\cup_{j=1}^m Q_j)=\cost(P,\cup_{j=1}^m Q_j)\geq \opt(P,mk).

\sum_{i=1}^m \left(\opt(P_i,1)-\opt(P_i,k)\right)
\leq \opt(P,m)-\opt(P,mk)\leq \eps^2\opt(P,k)\leq \eps^2\cost(P,Q),
\label{eq3}
\begin{split}
            \cost(P,Q)\leq \sum_{i=1}^m \min_{q \in Q}\cost(P_i, Q)
   &\leq \cost(P,Q)\cdot  \frac{1+3\eps}{1-2\eps}.
    \end{split}

            (1-\eps) \min_{q \in Q}\cost(P_i, \q) \leq \min_{q \in Q}\cost(S_i, \q) \leq (1+\eps) \min_{q \in Q} \cost(P_i, \q)
        
            (1-\eps) \sum_{i=1}^{m}\min_{q \in Q}\cost(P_i, \q) \leq \sum_{i=1}^{m} \min_{q \in Q}\cost(S_i, \q) \leq (1+\eps) \sum_{i=1}^{m} \min_{q \in Q}\cost(P_i, \q).  \label{sumS_i}
        
            (1-\eps) \sum_{i=1}^{m}\min_{q \in Q}\cost(P_i, \q) \leq \cost(S,Q) 
            \leq (1+\eps) \sum_{i=1}^{m} \min_{q \in Q}\cost(P_i, \q).  \label{sumS_i}

\begin{split}
            (1-\eps)\cost(P,Q)&\leq (1-\eps)\sum_{i=1}^m \min_{q \in Q}\cost(P_i, Q)\\
            &\leq \cost(S,Q)\\
            &\leq (1+\eps) \sum_{i=1}^{m} \min_{q \in Q}\cost(P_i, \q)
            \leq (1+\eps)\cost(P,Q)\cdot \frac{1+3\eps}{1-2\eps}
            \leq (1+15\eps)\cost(P,Q).
    \end{split} 
\label{toprove}
\OPT(P,k^t)-\OPT(P,k^{t+1})\leq \eps^2 \cdot \OPT(P,k).

            \begin{split}
                 \OPT(P,k)-\OPT(P,k^{\lceil 1/\eps^2\rceil+1})
                    =\sum_{i=1}^{\lceil 1/\eps^2\rceil}\big(\OPT(P,k^i) - \OPT(P,k^{i+1})\big)
                > \lceil 1/\eps^2\rceil\cdot \eps^2\OPT(P,k)\geq \OPT(P,k).
            \end{split}
        
        Contradiction, since .
    \end{proof}

Using the mean of  in Line~\ref{four} of the algorithm yields a -coreset  as shown in Lemma~\ref{onelemma}.
The resulting coreset is not sparse, but gives the following result.
\begin{theorem}
There is  such that the -means of  is a -coreset for .
\end{theorem}

\begin{proof}[\textbf{Proof of Theorem~\ref{thm1}}:]
We compute  a  mean coreset for -mean of  at line~\ref{four} of Algorithm~\ref{algk} by using Frank-Wolfe~\cite{onecenter} algorithm. It follows that  for each , therefore the overall sparsity of  is . This and Lemma~\ref{mainthm} concludes the proof.
\end{proof}

\section{Why does it work?}\label{secphen}
In this section we try to give an intuition of why our coreset construction yields a smaller error for the same number of samples, compared to existing coreset constructions. Roughly, this is mainly due to the ``cost of independent sampling" that is used by the existing smallest coreset constructions, namely the sensitivity/importance sampling approach~\cite{KeChen06, LS10,FL11, feldman2007ptas}.

In Fig.~\ref{fig:Ex1} the input is a set of 16 points on the plane that is distributed over  clusters that are relatively far from each other. Each cluster consists of a single point, except for one cluster that has  points.
Given a ``budget" (coreset size) of  points, the ``optimal coreset" seems to have all the  isolated input points, including  input points inside the large cluster, that are well distributed in this cluster. What would be the expected coresets of size  using the existing techniques?
            \begin{figure}[htp]
                \centering
                \subfigure{\includegraphics[width=0.31\textwidth, frame]{clustered.png}}
                \subfigure{\includegraphics[width=0.31\textwidth, frame]{non_uniform.png}}
                \subfigure{\includegraphics[width=0.31\textwidth, frame]{uniform.png}}
                \caption{\small \it A set of  points that consists of  clusters that are far from each other. Each of the first  clusters contains a single point. The red points are the expected selected points for a coreset of  points (with repetitions) using: \textbf{(left)} Algorithm 1, \textbf{(middle)} Non-uniform (importance/sensitivity) sampling, and \textbf{(right)} Uniform sampling.}
                \label{fig:Ex1}
            \end{figure}

\textbf{Algorithm~\algkmean.} would return exactly this ``optimal coreset", as the -means of  consists of the  isolated clusters and the  means of the large cluster. For the case , the algorithm will pick exactly one representative in each cluster, as it is the -means of . See Fig.~\ref{fig:Ex1}(left).

\textbf{Uniform Sampling.} If the large cluster is sufficiently large, all the points in a uniform sample will be from this cluster, while all the other (singleton)  clusters will be missed. Since these clusters are far away from each other, the approximation error will then be very large. Even for large sample size, uniform sample misses isolated clusters that are crucial for obtaining a small error. See Fig.~\ref{fig:Ex1}(right).

\textbf{Non-Uniform Sampling.} The optimal distribution that will make sure that a representative from each cluster will be selected to the coreset, is to sample a point from each of the  clusters with roughly the same probability. However, due to the independent (i.i.d.) sampling approach, the number of samples that are needed in order to have a representative from each cluster is more than . In general, the expected sample size is . This phenomena is known as the coupon collector problem: if there is a coupon in each box at the supermarket, picked uniformly at random from a collection of  distinct coupons, then one need to buy  boxes in expectation to have the collection of all the  coupons. This is compared to the deterministic construction of Algorithm~1 that always pick the desired  representatives.

Even after having a representative from each of the isolated clusters a non-uniform sampling will keep sample a point from one of these clusters with probability . This means that from the total ``budget" of  points in the coreset, a large fraction will be used to sample the same point again and again. This is also why in Fig.~\ref{fig:Ex1}(middle) there is less number of red points than the other constructions.

\section{Practical and Simple Boosting of Existing Heuristics}\label{prac}
To get the desired phenomena that is described in Section~\ref{secphen} there is no need to actually compute the -means for many values of  and existing heuristics can be used. For example, any reasonable -means heuristics for  would yield a set of  red points as in Fig.~\ref{fig:Ex1}(left).

\textbf{The chicken and the egg phenomena.} As in Algorithm 1, coresets for solving optimization problems usually need to solve the optimization problem in order to decide which points are important to sample. This problem is solved in theory using rough approximation algorithms or bi-criteria approximations~\cite{edo, FL11} that replaces the optimal solution, or using the merge-and-reduce tree that apply the coreset constuction only on small sets.
In practice, algorithms that compute provable -approximations or even -approximations for the -means clustering are rarely used. Instead, heuristics such as the classic LLoyd's -means or -Means++~\cite{ostrovsky2006effectiveness, arthur2007k} are used.

Based on our experimental results, a rough approximation using existing heuristics seems to be suffices.
In addition, plugging  as an input, almost always produces coreset with error that is much smaller than . This is common also in other coresets and related to the facts that the analysis is (i) for the worst case input set and not a specific  that is usually well structured, (ii)  sub-optimal compared to the actual error, (iii) consider every set of  centers, while we usually care about the optimal solution under some constraints.


We suggest to take the coreset size  as the input and run  iterations of our algorithm. In fact, our experimental results suggest the following simple approach that use a single instead of  runs and yields only slightly less better results.

\textbf{Boosting technique.} Given a heuristic for solving the -means problem using some  iterations, Algorithm 1 suggests to run the heuristic only small number of  iterations. Then, we take the mean of each of the  clusters, weighted by the size of the cluster, or construct a -coresets on each of the  clusters. Then, we run the heuristic  times on this ``coreset" of size . Even if each iteration of the heuristics takes linear time of , the running time is reduced from  to .

\textbf{Example 1: LLyod'-means.} In the case of Lloyd's -means, each iteration takes  time for computing the distances from the existing seed of  centers, and then  time is needed to compute the next set of centers. The boosting technique above suggests to run  such iterations on  to produce a weighted coreset of size . Then run the  iterations on this coreset.

\textbf{Example 2: KMean++.} The KMean++ algorithm picks another point to the output set in each iteration, where the first point is a random seed. The next point is sampled with probability that is proportional to the distances of the input points to the center (points) that were already picked. This is very similar to the importance sampling that is used for constructing existing coresets with a crucial difference: the sampling is not independent and same for each new point, but \emph{adaptive}, i.e., based on points that were already picked. This is exactly the advantage of our approach compared to the non-uniform sampling, as described in Fig.~\ref{fig:Ex1} and Section~\ref{secphen}. Note that KMean++ will always select the right centers in Fig.~\ref{fig:Ex1}, no matter what is the seed and although it is a random algorithm.

The KMean++ algorithm is very natural for using with our boosting technique: We just run it for  iterations to get a coreset of size . Then we run KMean++  times on the coreset. In each of the th times we use a different seed (first point) and take the optimal among the  sets of -mean candidates. Line~3 of Algoirthm~1 suggests an interesting way to choose the size  of the coreset, based on our analysis in the supplementary material.


\section{Experimental Results}\label{sec:xp}
\textbf{\bf Datasets.} To produce experimental results we have use two well known datasets.

    {\bf MNIST handwritten digits\cite{mnist}.} The MNIST dataset consist of  grayscale images of handwritten digits. Each image of size 28x28 pixels was transformed to the the vector row of  dimensions.

    {\bf Pendigits\cite{pendigits}.} This is a dataset from the UCI repository. The dataset created out of 250 samples provided by 44 writers. These writers were asked to write 250 digits in random order inside boxes of 500 by 500 tablet pixel resolution. The tablet sends  and  tablet coordinates and pressure level values of the pen at fixed time intervals (sampling rate) of 100 miliseconds. Digits are represented as constant length feature vectors of size  the number of digits in the dataset is .

    {\bf NIPS dataset\cite{nipsOnline}.} The OCR data from the collection which represents 13 years of NIPS proceedings. The overall of 15,000 pages and 1958 articles. For each author there is a words count vector extracted, where ith entry in the vector represents count of the particular word which was used in one of the conference submissions by given author. There are overall  authors and words corpus size is .

    \textbf{Expirement.} We used our algorithm to boost the performance of Lloyd's -means heuristic as explained in Section~\ref{prac}. Give a coreset size  we run this heuristic for only  iterations with  centers.
    We compared our algorithm with uniform and importance sampling algorithms using both offline computation setting and streaming data model. For offline computation we used datasets above to produce coresets of sizes , then computed -means with values of  using Lloyd's heuristic for many iterations till convergence. While to simulate streaming data model we divided datasets into chunks and computed coresets of sizes  using map-and-reduce techniques to construct a coreset tree, later repeated computation of -means for same values of .

    For each set of  centers that was produced, we computed sum of squared distances to the original (full) set of points, and denoted these ``approximated solutions" by  and  for uniform, non uniform sampling and our algorithm respectively.
    The ``ground truth" or ``optimal solution"  was computed using -means on entire dataset until convergence. The empirical estimated error  is then defined to be  for coreset number .

    \textbf{Results for datasets}
        Fig.\ref{fig:offline} and Fig.\ref{fig:streaming} shows results for offline setting and streaming models respectevly. The results of out algorithm are outperforms the uniform sampling and non-uniform sampling algorithms. Important to note, that our algorithm starts with very small error value compared to others and improves error value gradually with sample size, while two others starts with greater error values and succeeds to converge to significantly smaller values only for large sample subsets.

    Fig.\ref{fig:boxplot_offline} and Fig.\ref{fig:boxplot_streaming}, shows the boxplot of error distribution for all three algorithms in offline and streaming settings. It's easy to see that that our algorithm show little variance across all experiments and mean error value is very close to the median, indicating that our algorithm produces very stable results, while running on streaming data whiskers are broader due to the multiplicative factor of .

    In Fig.\ref{FigMem} we present the memory (RAM) footprint during the coreset construction based on synthetically generated random data.  These results are common to other coresets papers.  The oscillations corresponds to the number of coresets in the tree that each new chunk needs to update. For example, the first point in a streaming tree is updated in ,  however the th point for some  climbs up through  levels in the tree, so  coresets need to be merged.

            \begin{figure*}
                \subfigure{\includegraphics[width=0.3\textwidth]{mnist_boxplot.png}}
                \subfigure{\includegraphics[width=0.3\textwidth]{pendigits_boxplot.png}}
                \subfigure{\includegraphics[width=0.3\textwidth]{nips_boxplot.png}}
                \caption{\small \it Error (y-axis) box-plots for real-data sets, ofline computation model.}
                \label{fig:boxplot_offline}
            \end{figure*}

            \begin{figure*}
                \subfigure{\includegraphics[width=0.3\textwidth]{mnist_boxplot_streaming.png}}
                \subfigure{\includegraphics[width=0.3\textwidth]{pendigits_boxplot_streaming.png}}
                \subfigure{\includegraphics[width=0.3\textwidth]{nips_boxplot_streaming.png}}
                \caption{\small \it Error (y-axis) box-plots for real-data sets, streaming computation model.}
               \label{fig:boxplot_streaming}
            \end{figure*}


            \begin{figure*}
                \subfigure[MNIST, k=10]{\includegraphics[width=0.3\textwidth]{MNIST_10.png}}
                \subfigure[Pendigits, k=10]{\includegraphics[width=0.3\textwidth]{Pendigits_20.png}}
                \subfigure[NIPS, k=5]{\includegraphics[width=0.3\textwidth]{NIPS_10.png}}

                \subfigure[MNIST, k=15]{\includegraphics[width=0.3\textwidth]{MNIST_15.png}}
                \subfigure[Pendigits, k=15]{\includegraphics[width=0.3\textwidth]{Pendigits_15.png}}
                \subfigure[NIPS, k=10]{\includegraphics[width=0.3\textwidth]{NIPS_10.png}}

                \subfigure[MNIST, k=20]{\includegraphics[width=0.3\textwidth]{MNIST_20.png}}
                \subfigure[Pendigits, k=20]{\includegraphics[width=0.3\textwidth]{Pendigits_20.png}}
                \subfigure[NIPS, k=15]{\includegraphics[width=0.3\textwidth]{NIPS_15.png}}

                \subfigure[MNIST, k=25]{\includegraphics[width=0.3\textwidth]{MNIST_25.png}}
                \subfigure[Pendigits, k=25]{\includegraphics[width=0.3\textwidth]{Pendigits_25.png}}
                \subfigure[NIPS, k=20]{\includegraphics[width=0.3\textwidth]{NIPS_20.png}}

                \caption{\small \it Offline setup comparison of uniform sampling, non uniform sampling and our algorithms.}
                \label{fig:offline}
            \end{figure*}

            \begin{figure*}
                \subfigure[MNIST, k=10]{\includegraphics[width=0.3\textwidth]{MNIST_10_streaming.png}}
                \subfigure[Pendigits, k=10]{\includegraphics[width=0.3\textwidth]{Pendigits_20_streaming.png}}
                \subfigure[NIPS, k=5]{\includegraphics[width=0.3\textwidth]{NIPS_10_streaming.png}}

                \subfigure[MNIST, k=15]{\includegraphics[width=0.3\textwidth]{MNIST_15_streaming.png}}
                \subfigure[Pendigits, k=15]{\includegraphics[width=0.3\textwidth]{Pendigits_15_streaming.png}}
                \subfigure[NIPS, k=10]{\includegraphics[width=0.3\textwidth]{NIPS_10_streaming.png}}

                \subfigure[MNIST, k=20]{\includegraphics[width=0.3\textwidth]{MNIST_20_streaming.png}}
                \subfigure[Pendigits, k=20]{\includegraphics[width=0.3\textwidth]{Pendigits_20_streaming.png}}
                \subfigure[NIPS, k=15]{\includegraphics[width=0.3\textwidth]{NIPS_15_streaming.png}}

                \subfigure[MNIST, k=25]{\includegraphics[width=0.3\textwidth]{MNIST_25_streaming.png}}
                \subfigure[Pendigits, k=25]{\includegraphics[width=0.3\textwidth]{Pendigits_25_streaming.png}}
                \subfigure[NIPS, k=20]{\includegraphics[width=0.3\textwidth]{NIPS_20_streaming.png}}
                \caption{\small \it Streaming setup comparison of uniform sampling, non uniform sampling and our algorithms.}
                \label{fig:streaming}
            \end{figure*}

			\begin{figure}
                \begin{center}
                \includegraphics[height=3cm, width=9cm]{memory.png}
			   \caption{\small \it  Allocated memory (y-axis) grows logarithmically during streaming coreset construction. The Zig-zag patterns caused by the binary merge-reduce tree in Fig.~\ref{fig:tree}.}
			   \label{FigMem}
                \end{center}
			\end{figure}

\newpage
\bibliographystyle{abbrv}
\bibliography{mybib}
\end{document}\