\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{resizegather}
\usepackage{marvosym}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage[section]{placeins}
\usepackage{color}
\usepackage[table]{xcolor}
\definecolor{visda_color_0}{rgb}{0.9922,0.0,0.0235}
\definecolor{visda_color_1}{rgb}{0.9922,0.4235,0.0314}
\definecolor{visda_color_2}{rgb}{1.0,1.0,0.3098}
\definecolor{visda_color_3}{rgb}{0.4471,1.0,0.0275}
\definecolor{visda_color_4}{rgb}{0.1686,1.0,0.0980}
\definecolor{visda_color_5}{rgb}{0.1333,1.0,0.4275}
\definecolor{visda_color_6}{rgb}{0.1333,1.0,1.0}
\definecolor{visda_color_7}{rgb}{0.0510,0.4,1.0}
\definecolor{visda_color_8}{rgb}{0.0,0.0,1.0000}
\definecolor{visda_color_9}{rgb}{0.4196,0.0,1.0}
\definecolor{visda_color_10}{rgb}{0.9882,0.0,1.0}
\definecolor{visda_color_11}{rgb}{0.9922,0.0,0.4275}
\definecolor{city_color_0}{rgb}{0.0,0.0,0.0}
\definecolor{city_color_1}{rgb}{0.5020,0.2510,0.5020}
\definecolor{city_color_2}{rgb}{0.9569,0.1373,0.9098}
\definecolor{city_color_3}{rgb}{0.2745,0.2745,0.2745}
\definecolor{city_color_4}{rgb}{0.4000,0.4000,0.6118}
\definecolor{city_color_5}{rgb}{0.7451,0.6000,0.6000}
\definecolor{city_color_6}{rgb}{0.6000,0.6000,0.6000}
\definecolor{city_color_7}{rgb}{0.9804,0.6667,0.1176}
\definecolor{city_color_8}{rgb}{0.8627,0.8627,0.0000}
\definecolor{city_color_9}{rgb}{0.4196,0.5569,0.1373}
\definecolor{city_color_10}{rgb}{0.5961,0.9843,0.5961}
\definecolor{city_color_11}{rgb}{0.2745,0.5098,0.7059}
\definecolor{city_color_12}{rgb}{0.8627,0.0784,0.2353}
\definecolor{city_color_13}{rgb}{1.0000,0.0000,0.0000}
\definecolor{city_color_14}{rgb}{0.0000,0.0000,0.5569}
\definecolor{city_color_15}{rgb}{0.0000,0.0000,0.2745}
\definecolor{city_color_16}{rgb}{0.0000,0.2353,0.3922}
\definecolor{city_color_17}{rgb}{0.0000,0.3137,0.3922}
\definecolor{city_color_18}{rgb}{0.0000,0.0000,0.9020}
\definecolor{city_color_19}{rgb}{0.4667,0.0431,0.1255}
\definecolor{citecolor}{RGB}{119,185,0}
\usepackage[hypertexnames=false,pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,citecolor=citecolor,bookmarks=false]{hyperref}

\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{property}{Property}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\iccvfinalcopy 

\def\iccvPaperID{0774} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Confidence Regularized Self-Training}

\author{
Yang Zou~~~~
Zhiding Yu\thanks{The authors contributed equally.}~~~~
Xiaofeng Liu~~~~
B.V.K. Vijaya Kumar~~~~
Jinsong Wang\thanks{Work done during the affiliation with General Motors R\&D.}\\
~Carnegie Mellon University~~~~
~NVIDIA~~~~
~General Motors R\&D\\
{\small\Letter~~\tt
\MYhref[black]{mailto:yzou2@andrew.cmu.edu}{yzou2@andrew.cmu.edu}, 
\MYhref[black]{mailto:zhidingy@nvidia.com}{zhidingy@nvidia.com}, 
\MYhref[black]{mailto:liuxiaofeng@cmu.edu}{liuxiaofeng@cmu.edu}}
}

\maketitle

\renewcommand{\thefootnote}{\Letter}
\footnotetext{Contact emails of corresponding authors.}
\renewcommand*{\thefootnote}{\arabic{footnote}}

\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at \href{https://github.com/yzou2/CRST}{https://github.com/yzou2/CRST}.
\end{abstract}

\section{Introduction}\label{sec:intro}
Transferring knowledge learned by deep neural networks from label-rich domains to a new target domain is an important but challenging problem. Such domain change naturally occurs in many applications, such as synthetic data training~\cite{peng2018visda,richter2016playing} and simulation for robotics/autonomous driving. The existence of cross-domain differences often leads to considerably decreased model performance, and unsupervised domain adaptation (UDA) aims to address this problem by adapting source model to target domain with the aid of unlabeled target data. To this end, a predominant stream of adversarial learning based UDA methods were proposed to reduce the discrepancy between source and target domain features
\cite{chen2018domain,chen2017no,hoffman2018cycada,kim2019unsupervised,long2018conditional,murez2018image,pinheiro2018unsupervised,saito2018adversarial,sankaranarayanan2018generate,Tsai_adaptseg_2018}.
\begin{figure}[!t]
\includegraphics[width=\linewidth]{figures/fig1.pdf}
\caption{Illustration of proposed confidence regularization. (a) Self-training without confidence regularization generates and retrains with hard pseudo-labels, resulting in sharp network output. (b) Label regularized self-training introduces soft pseudo-labels, therefore enabling outputs to be smooth. (c) Model regularized self-training also retrains with hard pseudo-labels, but incorporates a regularizer to directly promote output smoothness.}\label{fig:teaser}
\vspace{-2mm}
\end{figure}

More recently, self-training with networks emerged as a promising alternative towards domain adaptation \cite{busto2018open, Chen_2019_CVPR,inoue2018cross, lee2013pseudo, saito2017asymmetric, shu2018dirt, Zou_2018_ECCV}. Self-training iteratively generates a set of one-hot (or hard) pseudo-labels corresponding to large selection scores (i.e., prediction confidence) in target domain, and then retrains network based on these pseudo-labels with target data. Recently,~\cite{Zou_2018_ECCV} proposes class-balanced self-training (CBST) and formulates self-training as a unified loss minimization with pseudo-labels that can be solved in an end-to-end manner. Instead of reducing domain gap by minimizing both the task loss and domain adversarial loss, the self-training loss implicitly encourages cross-domain feature alignment for each class by learning from both labeled source data and pseudo-labeled target data.

Early work~\cite{lee2013pseudo} shows that the essence of deep self-training is entropy minimization - pushing network output to be as sharp as hard pseudo-label. However,  accuracy cannot always be guaranteed for pseudo-labels. Trusting all selected pseudo-labels as ``ground truth'' by encoding them as hard labels can lead to overconfident mistakes and propagated errors. In addition, semantic labels of natural images can be highly ambiguous. Taking a sample image from VisDA17~\cite{peng2018visda} (see Fig. \ref{fig:teaser}) as an example: both person and car dominate significant portions of this image. Enforcing a model to be very confident on only one of the class during training can hurt the learning behavior~\cite{bagherinezhad2018label}, particularly within the under-determined context of UDA.

The above issues motivate us to prevent infinite entropy minimization in self-training via confidence regularization. A natural idea is to generate soft pseudo-label that redistributes a certain amount of confidence to other classes. Learning with soft pseudo-labels attenuates the misleading effect brought by incorrect or ambiguous supervision. Alternatively, to achieve the same goal, one can also encourage the smoothness of output probabilities and prevent overconfident prediction in network training. Both ideas are illustrated in Fig. \ref{fig:teaser}. At high-level, the major goal of CRST is still aligned with entropy minimization. However, the confidence regularization serves as a safety measure to prevent infinite entropy minimization and degraded performance.

In this work, we choose CBST \cite{Zou_2018_ECCV} as a state-of-the-art non-regularized self-training baseline, and propose a variety of specific confidence regularizers to comprehensively validate CRST. Our contributions are listed as follows:

\begin{itemize}[noitemsep,topsep=0pt]
\item In Section~\ref{sec:ccbst}, We generalize CBST to continuous CBST as a necessary preliminary for introducing our CRST, where we relax the feasible space of pseudo-labels from one-hot vectors to a probability simplex, 
\item In Section~\ref{subsec:lr}, we introduce label regularized self-training (CRST-LR). CRST-LR generates soft pseudo-labels for self-training. Specifically, we propose a label entropy regularizer (LRENT). In Section \ref{subsec:mr}, we introduce model regularized self-training (CRST-MR). CRST-MR introduces an output smoothing regularizer to network training. Specifically, we introduce three model regularizers, including  (MRL2), entropy (MRENT), and KLD (MRKLD).
\item In Section~\ref{sec:theory}, we investigate theoretical properties of CRST, and prove that CRST is equivalent to regularized Classification Maximum Likelihood which can be solved via Classification Expectation Maximization (CEM). We also prove the convergence of CRST, and show that LRENT-regularized pseudo-label is equivalent to a generalized softmax with temperature~\cite{hinton2015distilling}.
\item In Section~\ref{sec:exp}, we comprehensively evaluate CRST on multiple domain adaptation tasks, including image classification (visDA17/Office-31) and semantic segmentation (GTA5/SYNTHIA  Cityscapes). We demonstrate state-of-the-art or competitive results from the proposed framework, and discuss the comparison between different regularizers in Section~\ref{sec:discuss}. We also show that LR+MR may benefit self-training.
\end{itemize}

\section{Related works}\label{sec:related}
\noindent\textbf{Self-training: } Self-training has been widely investigated in semi-supervised learning \cite{yarowsky1995unsupervised,amini2002semi,grandvalet2005semi}. An overview of different self-training techniques is presented in~\cite{triguero2015self}. Recent interests in self-training were revitalized with deep neural networks~\cite{lee2013pseudo}. A subtle difference between self-training on fixed features and deep self-training is that the latter involves the learning of embeddings which renders greater flexibility towards domain alignment than classifier-level adaptation. Within this context, \cite{Zou_2018_ECCV} proposed class-balanced self-training and achieved state-of-the-art performance in cross-domain semantic segmentation.

\noindent\textbf{Domain adaptation: } (Unsupervised) domain adaptation (UDA) has recently gained considerable interests. For UDA with deep networks, a major principle is to let the network learn domain invariant embeddings by minimizing the cross-domain difference of feature distributions with certain criteria. Examples of these methods include maximum mean discrepancy (MMD)~\cite{long2015learning,tzeng2014deep}, deep correlation alignment (CORAL)~\cite{sun2016deep}, sliced Wasserstein discrepancy \cite{lee2019sliced}, adversarial learning at input-level~\cite{hoffman2018cycada,gong2019dlow,dundar2020domain}, feature level~\cite{chen2019learning, ganin2015unsupervised,hoffman2018cycada,liu2019feature,saito2018adversarial,tzeng2017adversarial,wu2018dcan}, output space level~\cite{Tsai_adaptseg_2018}, and a variety of follow up works~\cite{chen2017no,long2018conditional,pinheiro2018unsupervised,sankaranarayanan2018generate} etc. Open set domain adaptation \cite{panareda2017open,saito2018open} focuses on the problem where classes are not totally shared between source and target domains. More recently, there have been multiple deep self-training/pseudo-label based methods that are proposed for domain adaptation~\cite{busto2018open,han2019unsupervised,inoue2018cross,saito2017asymmetric,shu2018dirt,Zou_2018_ECCV}.

\noindent\textbf{Semi-supervised learning (SSL): }There exist a natural strong connection between domain adaptation and semi-supervised learning with their problem definitions. A series of teacher-student based approaches have been recently proposed for both SSL \cite{laine2016temporal, tarvainen2017mean,luo2018smooth} and UDA problems\cite{french2018self}.

\noindent\textbf{Noisy label learning: }
Self-training can also be regarded as noisy label learning \cite{natarajan2013learning,reed2015training,sukhbaatar2014learning,yu2018seal} due to potential mistakes on pseudo-labels. \cite{reed2015training} introduced a bootstrapping method for noisy label learning. \cite{sukhbaatar2014learning} proposed an extra noise layer into the network adapting the network outputs to match the noisy label distribution.

\noindent\textbf{Network regularization: }
Regularization is a typical approach in supervised neural network training to avoid overfitting. Besides the standard weight decay, typical regularization techniques include label smoothing \cite{Goodfellow-et-al-2016,szegedy2016rethinking,liu2019wass}, network output regularization \cite{pereyra2017regularizing}, knowledge distillation \cite{hinton2015distilling}. Yet few principled research have considered regularized self-training within the context of SSL/UDA.

\section{Continuous class-balanced self-training}\label{sec:ccbst}
In this section, we review the class-balanced self-training algorithm in~\cite{Zou_2018_ECCV} and reformulate it under a continuous framework. Specifically, for an UDA problem, we have access to the labeled source samples  from source domain , and target samples  from unlabeled target domain data . Any target label  from  is unknown.  is the total number of classes. We define the network weights as  and  as the classifier's softmax probability for class .
 
CBST is a self-training framework that performs joint network learning and pseudo-label estimation under a unified loss minimization problem. The pseudo-labels are treated as discrete learnable latent variables being either one-hot or all-zero. Here, we first relax the pseudo-label variables to continuous domain, as shown in Eq. (\ref{cbst}):

The feasible set is the union of  and a probability simplex . The continuous CBST is solved by alternating optimization based on the following \textbf{a)}, \textbf{b)} steps:\\
\noindent \textbf{a) Pseudo-label generation} \label{a)} ~ Fix  and solve:

\textbf{b) Network retraining} \label{b)} ~ Fix  and solve:


We define going through step \textbf{a)} and \textbf{b)} once as one ``\textbf{self-training round}''. For solving step \textbf{a)}, there is a global optimizer for arbitrary  as follows.


For solving step \textbf{b)}, one can use typical gradient-based methods such as mini-batch gradient descent. Intuitively, solving \textbf{a)} by (\ref{cbst_a_solver}) is actually conducting pseudo-label learning and selection simultaneously. Note that  in (\ref{cbst_a_solver}) not only can be one-hot, but also can be a zero vector . For each target sample , if  is an one-hot, the sample is selected for model retraining. If , this sample is not chosen. Specifically,  is a parameter controlling sample selection. If a sample's predication is relatively confident with , it is selected and labeled as class . The less confident ones with  are not selected.

 are critical parameters to control pseudo-label learning and selection. The same class-balanced  strategy introduced in \cite{Zou_2018_ECCV} is adopted for all self-training methods in this work.  for each class  is determined by a single portion parameter  which indicts how many samples we want to select in target domain. Specifically, we define the confidence for a sample as the max of its output softmax probabilities. For each class ,  is determined by the confidence value selecting the most confident  portion of class  predictions in the entire target set. We emphasize that only one parameter  is used to determine all 's. Practically, we gradually increase  to incorporate more pseudo-labels for each additional round. For detailed algorithm, we recommend to read Algorithm 2 in \cite{Zou_2018_ECCV}.

\noindent\textbf{Remark:} The only difference between CBST and continuous CBST lies in the feasible set where continuous CBST has a probability simplex while CBST has a set of one-hot vectors. Although the feasible set relaxization does not change the solutions of CBST and the pseudo-labels are still one-hot vectors, continuous CBST allows generating soft pseudo-labels if specific regularizers are introduced into pseudo-label generation. Thus it serves as the basis for our proposed label regularized self-training.
\section{Confidence regularized self-training}\label{sec:crst}
As mentioned in Section~\ref{sec:intro}, we leverage confidence regularization to prevent the over-minimization of entropy that could lead to degraded performance in self-training. Below, we introduce the general definition of CRST:

 is the confidence regularizer and  is the weight coefficient. Similar to CBST, the optimization algorithm of CRST can be formulated as alternatively taking step \textbf{a)} pseudo-label generation and step \textbf{b)} network retraining. In this paper, we introduce two types of CRST frameworks: label regularized self-training and model regularized self-training.

\subsection{Label regularization}\label{subsec:lr}
The label regularizer has a general form of   and only depends on pseudo-labels . With fixed , the pseudo-label generation in step \textbf{a)} of CRST-LR is defined as follows:


The global minimizer of (\ref{crst_lr_a}) can be found via a two-stage optimization given the special structure of the feasible space. The first stage involves minimizing (\ref{crst_lr_a}) within  only, which gives . The second stage is to select between  or  by checking which leads to a lower cost:

where  is the cost of a single sample  in (\ref{crst_lr_a}):

Note that the above regularized term prefers selecting pseudo-labels with certain smoothness rather than sparse ones. In addition, CRST-LR and CBST share the same network retraining strategy in step \textbf{b)}.

Specifically, we introduce a negative entropy label regularizer (LRENT) in Table \ref{table:regs} with its definition and the corresponding solution of . For clarity, we write  as  for short.  can be obtained via solving with a Lagrangian multiplier (KKT conditions)~\cite{boyd2004convex}. The detailed derivations are shown in Section \ref{sec:derivlrent} of the Appendix.

\subsection{Model regularization}\label{subsec:mr}
The model regularizer has a general form of   where  is the network softmax output probabilites. Compared to CBST, CRST-MR has the same hard pseudo-label generation process. But in network retraining of step \textbf{b)}, CRST-MR uses a cross-entropy loss regularized by an output smoothness encouraging term. We define the optimization problem in step \textbf{b)} as follows:


Specifically, we introduce three model regularizers in Table \ref{table:regs} based on , negative entropy and KLD between uniform distribution  and softmax output. The gradients w.r.t. softmax logits  are also provided.  is the entropy.
\begin{table}[!t]
\centering
\resizebox{0.48\textwidth}{!}{
\centering
\begin{tabular}{c|c|c}
\hline
& Regularizer & Pseudo-label solution (LR)/Gradient (MR)\\
\hline
LRENT &   &                     \\ \hline \hline
MRL2     & 
 
&                 \\ \hline
 MRENT &  &   \\ \hline
MRKLD    &            &   \\ \hline
\end{tabular}
}
\caption{List of proposed regularizers with corresponding pseudo-label solution or gradients w.r.t. softmax logit .}
\label{table:regs}
\end{table}

\section{Theoretical properties}\label{sec:theory}
\subsection{A probabilistic view of CRST}\label{prob_explan}
There exists an inherent connection between the CRST and some probabilistic models. Specifically, the CRST self-training algorithm can be interpreted as an instance of classification expectation maximization~\cite{amini2002semi}:
\begin{proposition}\label{prop:cem}
	CRST can be modeled as a regularized classification maximum likelihood (RCML) problem optimized via classification expectation maximization.
\end{proposition}
\begin{proof}
	Please refer to Section~\ref{sec:proofprop1} of the Appendix.
\end{proof}
\begin{proposition}\label{prop:convergence}
	Given pre-determined , CRST is convergent under certain conditions.
\end{proposition}
\begin{proof}
	Please refer to Section~\ref{sec:proofprop2} of the Appendix.
\end{proof}

\subsection{Soft pseudo-label in LRENT}
There is an intrinsic connection between the soft pseudo-label of LRENT (given in Table \ref{table:regs}) and softmax with temperature. Softmax with temperature \cite{hinton2015distilling} is a common approach in neural network for scaling softmax probabilities with applications in knowledge distillation \cite{hinton2015distilling}, model calibration \cite{pmlr-v70-guo17a}, etc. Typically, networks produce categorical probabilities by a softmax activation layer to convert
the logit  for each class into a probability . And the softmax with temperature introduces a positive temperature  to scale its smoothness as follows.


For high temperature (), the new distribution is softened as a uniform distribution that has the highest entropy and uncertainty. For temperature , we recover the original softmax probabilities. For low temperature (), the distribution collapses to a sparse one-hot vector with all probability on the class with the most original softmax probability. Now we draw the connection of soft pseudo-label in LRENT to softmax with temperature:
\begin{proposition}\label{prop:prop3}
	If  are equal for all , the soft pseudo-label of LRENT given in Table \ref{table:regs} is exactly the same as softmax with temperature.
\end{proposition}
\begin{proof}
	
\end{proof}
The soft pseudo-label of LRENT can be regarded as a generalized softmax with temperature. In self-training, if selected properly,  can help to generate class-balanced soft pseudo-labels.

\begin{proposition}\label{prop:prop4}
	\textit{KLD model confidence regulared self-training is equivalent to self-training with pseudo-label uniformly smoothed by , where  is the regularizer weight.}
\end{proposition}
\begin{proof}
	Please refer to Section~\ref{sec:proofprop4} of the Appendix.
\end{proof}

\begin{proposition}\label{prop:prop5}
 KLD model regularizer (the reverse of the proposed  KLD regularizer) is equivalent to entropy model regularizer , where  is the uniform distribution.
\end{proposition}
\begin{proof}
	Please refer to Section~\ref{sec:proofprop5} of the Appendix.
\end{proof}

\section{Experiments}\label{sec:exp}
In this section, we conduct comprehensive evaluation on different domain adaptation tasks.\\
\noindent\textbf{Adaptation for image classification:} We consider two adaptation benchmarks: 1) VisDA17 \cite{peng2018visda} and 2) Office-31 \cite{saenko2010adapting}. VisDA17 contains  2D synthetic images of 12 classes in the source training set and  real images from MS-COCO~\cite{lin2014microsoft} as the target domain validation set. Office-31 is a small-scale dataset containing images of  classes from three domains - Amazon (A), Webcam (W) and DSLR (D). Each domain contains ,  and  images respectively. We follow the standard protocol in~\cite{saenko2010adapting,sankaranarayanan2018generate} and evaluate on six transfer tasks , , , , , and .\\
\noindent\textbf{Adaptation for semantic segmentation:} We consider two popular synthetic-to-real adaptation scenarios: 1) GTA5 \cite{richter2016playing} to Cityscapes \cite{cordts2016cityscapes}, and
2) SYNTHIA \cite{ros2016synthia} to Cityscapes. The GTA5 dataset includes  images rendered by GTA5 game engine. For SYNTHIA, we choose
SYNTHIA-RAND-CITYSCAPES which includes  labeled images. Following the standard protocols~\cite{hoffman2018cycada,Tsai_adaptseg_2018}, we adapt the model to the Cityscapes training set and evaluate the performance on the validation set.

To comprehensively demonstrate the improvement of CRST, we report the performance of CRST with all regularizers and compare with CBST in each task.

\subsection{Implementation details}
\noindent\textbf{Image classification: } For VisDA17/Office-31, we implement CBST/CRSTs using PyTorch~\cite{paszke2017automatic} and choose ResNet-101/ResNet-50~\cite{he2016deep} as backbones. For fair comparison, we compare to other works with the same backbone networks. Both backbones are pre-trained on ImageNet~\cite{deng2009imagenet}, and then fine-tuned on source domain using SGD, with learning rate , weight decay , momentum  and batch size . For self-training, we apply the same training strategy but a different learning rate .\\
\noindent\textbf{Semantic segmentation: } For semantic segmentation, we further consider DeepLabv2~\cite{chen2018deeplab} as a backbone besides the ResNet-38 backbone in~\cite{Zou_2018_ECCV}. For experiments with DeepLabv2, we implement CBST/CRSTs using PyTorch, while following the MXNet~\cite{chen2015mxnet} implementation of~\cite{Zou_2018_ECCV} for experiments with ResNet-38. DeepLabv2 is pre-trained on ImageNet and fine-tuned on source domain using SGD, with learning rate , weight decay , momentum , batch size , patch size , multi-scale training augmentation () and horizontal flipping. In self-training, we apply SGD with learning rate of . For fair comparison, we unify the total number of self-training rounds to be 3, each with 2 re-training epochs.

\subsection{Domain adaptation for image classification}
\noindent \textbf{VisDA17:}
We present the results on VisDA17 in Table \ref{table:visda17} in terms of per-class accuracy and mean accuracy. For each proposed approach, we report the averages and standard deviations of the evaluation results over  runs. Note that both MRKLD and LRENT outperform the non-regularized CBST, whereas MRL2 and MRENT show slightly worse results. Among CRSTs with single regularizer, MRKLD achieves the best performance with considerable improvement. The combination of MRKLD and LRENT further outperforms single regularizers and other recently proposed methods. The result even outperforms certain methods with stronger backbones (ResNet-152)~\cite{pinheiro2018unsupervised,sankaranarayanan2018generate}.\\
\noindent \textbf{Office-31:}
We compare the performance of different methods on Office-31 with the same backbone ResNet-50 in Table \ref{table:office}. All CRSTs achieve similar results that outperform the baseline CBST. In addition, MRKLD+LRENT again outperforms single regularizers, achieving comparable or better performance compared with other recent methods.

\begin{table*}[!t]
	\captionsetup{font=small}
	\centering
	\resizebox{\linewidth}{!}{
	\centering
	\begin{tabular}{c|cccccccccccc|c}
		\hline
		Method & Aero & Bike & Bus & Car & Horse & Knife & Motor & Person & Plant & Skateboard & Train & Truck & Mean\\
		\hline
		Source \cite{saito2018adversarial} & 55.1 & 53.3 & 61.9 & 59.1 & 80.6 & 17.9 & 79.7 & 31.2 & 81.0 & 26.5 & 73.5 & 8.5 & 52.4\\
		MMD \cite{long2015learning} & 87.1 & 63.0 & 76.5 & 42.0 & 90.3 & 42.9 & 85.9 & 53.1 & 49.7 & 36.3 & 85.8 & 20.7 & 61.1\\
		DANN \cite{ganin2016domain} & 81.9 & 77.7 & 82.8 & 44.3 & 81.2 & 29.5 & 65.1 & 28.6 & 51.9 & 54.6 & 82.8 & 7.8 & 57.4\\ 
		ENT \cite{grandvalet2005semi} & 80.3 & 75.5 & 75.8 & 48.3 & 77.9 & 27.3 & 69.7 & 40.2 & 46.5 & 46.6 & 79.3 & 16.0 & 57.0\\
		MCD \cite{saito2017maximum} & 87.0 & 60.9 & \textbf{83.7} & 64.0 & 88.9 & 79.6 & 84.7 & \textbf{76.9} & \textbf{88.6} & 40.3 & 83.0 & 25.8 & 71.9\\
		ADR \cite{saito2018adversarial} & 87.8 & 79.5 & \textbf{83.7} & 65.3 & \textbf{92.3} & 61.8 & \textbf{88.9} & 73.2 & 87.8 & 60.0 & \textbf{85.5} & {32.3} & 74.8\\  
		SimNet-Res152 \cite{pinheiro2018unsupervised} & \textbf{94.3} & 82.3 & 73.5 & 47.2 & 87.9 & 49.2 & 75.1 & 79.7 & 85.3 & 68.5 & 81.1 & 50.3 & 72.9\\
		GTA-Res152 \cite{sankaranarayanan2018generate} & - & - & - & - & - & - & - & - & - & - & - & - & 77.1\\
		\hline
		Source-Res101 & 68.7 & 36.7 & 61.3 & \textbf{70.4} & 67.9 & 5.9 & 82.6 & 25.5 & 75.6 & 29.4 & 83.8 &  10.9 & 51.6\\
		CBST & 87.22.4 & 78.81.0 & 56.52.2 & 55.43.6 & 85.11.4 & 79.210.3 & 83.80.4 &  77.74.0 & 82.82.8 & 88.83.2 & 69.02.9 & 72.03.8 & 76.40.9 \\
		MRL2 & 87.02.9 & 79.51.9 & 57.13.2 & 54.72.9 & 85.51.1 & 78.111.7 & 83.01.5 &  77.73.7 & 82.41.7 & 88.62.7 & 69.12.2 &  71.83.0 & 76.21.0        \\
		MRENT & 87.12.7 & 78.30.7 & 56.14.0 & 54.42.7 & 84.42.3 & 79.910.6 & 83.71.1 &  77.94.4 & 82.72.4 &  87.42.8 & 70.01.4 &  72.83.3 & 76.20.8 \\
		MRKLD & 87.32.5 & 79.41.9 & 60.52.4 & 59.72.5 & 87.61.4 & \textbf{82.44.4} & 86.51.1 & 78.42.6 & 84.61.7 & 86.42.8 & 72.52.4 &   69.82.5 & 77.90.5\\
		LRENT & 87.72.4 & 78.70.8 & 57.33.3 & 54.54.0 & 84.81.7 & 79.710.3 & 84.21.4 &  77.43.7 & 83.11.5 & \textbf{88.32.6} & 70.92.1 &   \textbf{72.62.4} & 76.60.9\\
		MRKLD+LRENT & 88.00.6 & 79.22.2 & 61.03.1 & 60.01.0 & 87.51.2 & 81.45.6 & 86.31.5 & 78.82.1 & 85.60.9 & 86.62.5 & 73.91.3 &   68.82.3 & \textbf{78.10.2}\\
		\hline
	\end{tabular}
	}
	\vspace{-2mm}
	\caption{Experimental results on VisDA17.}
	\label{table:visda17}
	\vspace{-2mm}
\end{table*}

\begin{table}[]
	\centering
	\resizebox{0.95\linewidth}{!}{
	\begin{tabular}{c|cccccc|c}
		\hline
		Method & AW & DW & WD & AD & DA & WA & Mean\\
		\hline
		ResNet-50 \cite{he2016deep} & 68.40.2 & 96.70.1 & 99.30.1 & 68.90.2 & 62.50.3 & 60.70.3 & 76.1\\
		DAN \cite{long2015learning} & 80.50.4 & 97.10.2 & 99.60.1 & 78.60.2 & 63.60.3 & 62.80.2 & 80.4\\
		RTN \cite{long2016unsupervised} & 84.50.2 & 96.80.1 & 99.40.1 & 77.50.3 & 66.20.2 & 64.80.3 & 81.6\\
		DANN \cite{ganin2016domain} & 82.00.4 & 96.90.2 & 99.10.1 & 79.70.4 & 68.20.4 & 67.40.5 & 82.2\\
		ADDA \cite{tzeng2017adversarial} & 86.20.5 & 96.20.3 & 98.40.3 & 77.80.3 & 69.50.4 & 68.90.5 & 82.9\\
		JAN \cite{long2017deep} & 85.40.3 & 97.40.2 & 99.80.2 & 84.70.3 & 68.60.3 & 70.00.4 & 84.3\\
		GTA \cite{sankaranarayanan2018generate} & \textbf{89.50.5} & 97.90.3 & 99.80.4 & 87.70.5 & 72.80.3 & 71.40.4 & 86.5\\
		\hline
		CBST & 87.80.8 & 98.50.1 & \textbf{1000.0} & 86.51.0 & 71.20.4 & 70.90.7 & 85.8\\
		MRL2 & 88.40.2 & 98.60.1 & \textbf{1000.0} & 87.70.9 & 71.80.2 & \textbf{72.10.2} & 86.4\\
		MRENT & 88.00.4 & 98.60.1 & \textbf{1000.0} & 87.40.8 & \textbf{72.70.2} & 71.00.4 & 86.4\\
		MRKLD & 88.40.9 & 98.70.1 & \textbf{1000.0} & 88.00.9 & 71.70.8 & 70.90.4 & 86.3\\
		LRENT & 88.60.4 & 98.70.1 & \textbf{1000.0} & \textbf{89.00.8} & 72.00.6 & 71.00.3 & 86.6\\
		MRKLD+LRENT & 89.40.7 & \textbf{98.90.4} & \textbf{1000.0} & 88.70.8 & 72.60.7 & 70.90.5 & \textbf{86.8}\\
		\hline
	\end{tabular}
	}
	\vspace{-2mm}
	\caption{Experimental results on Office-31.}
	\label{table:office}
	\vspace{-2mm}
\end{table}

\subsection{Domain adaptation for semantic segmentation}
\noindent\textbf{GTA5  Cityscapes:} Table~\ref{table:gtacity} shows the adaptation performance of CRSTs and other comparing methods. On a DeepLabv2 backbone, one could see that MRKLD achieves the best result outperforming previous state-of-the-art. In addition, Fig. \ref{fig:gta2city} visualizes the adapted prediction results obtained by CBST and CRSTs on Cityscapes validation set. Fig. \ref{fig:plgta2city} further compares the pseudo-label maps in the second round of self-training. On a wide ResNet-38 backbone, all CRSTs outperform the baseline CBST and we achieve the state-of-the-art system-level performance with the spatial priors (SP) and multi-scale testing (MST) from~\cite{Zou_2018_ECCV}.

\noindent\textbf{SYNTHIA  Cityscapes:} Table \ref{table:syncity} shows the adaptation results where CRSTs again show the performance on par with or better than the baseline CBST. In particular, MRKLD maintains the best performance among all regularizers and outperforms the previous state-of-the-art~\cite{Zou_2018_ECCV}.

\subsection{Parameter analysis}
 is an important parameter controling the pseudo-label generation and selection sensitivity. We adopt the same  policy as~\cite{Zou_2018_ECCV} where we start  from , and empirically add  to  in each additional self-training round. We conduct a sensitivity analysis for portion  similar to~\cite{Zou_2018_ECCV}, where we consider the starting portion  and the incremental portion  on a difficult task of Office-31: W  A. Table \ref{table:p} shows that CRSTs are not sensitive to  and  . 

In CRST, the coefficient  is an important parameter that balances the weight between self-training loss and confidence regularizer. In all the experiments, we unify  to be  for MRL2, MRENT, MRKLD and LRENT, respectively. Note that various regularizers have different  due to their intrinsic differences. We also present the sensitivity analysis of  on W  A in Table \ref{table:alpha}. We can see all CRSTs are not sensitive to  in certain intervals. 
\section{Discussion}\label{sec:discuss}
\subsection{How does confidence regularization work?}
\begin{table*}[!t]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c|c|ccccccccccccccccccc|c}
		\hline
		Method & Backbone & Road & SW & Build & Wall & Fence & Pole & TL & TS & Veg. & Terrain & Sky & PR & Rider & Car & Truck & Bus & Train & Motor & Bike & mIoU\\
		\hline
		Source & \multirow{2}{0.1\linewidth}{\centering{DRN-26}} & 42.7 & 26.3 & 51.7 & 5.5 & 6.8 & 13.8 & 23.6 & 6.9  & 75.5 & 11.5 & 36.8 & 49.3 & 0.9 & 46.7 & 3.4 & 5.0 & 0.0 & 5.0 & 1.4  & 21.7\\
		CyCADA~\cite{hoffman2018cycada} & & 79.1 & 33.1 & 77.9 & 23.4 & 17.3 & 32.1 & 33.3 & 31.8 & 81.5 & 26.7 & 69.0 & 62.8 & 14.7 & 74.5 & 20.9 & 25.6 & 6.9 & 18.8 & 20.4 & 39.5\\
		\hline
		Source & \multirow{2}{0.1\linewidth}{\centering{DRN-105}} & 36.4 & 14.2 & 67.4 & 16.4 & 12.0 & 20.1 & 8.7 & 0.7 & 69.8 & 13.3 & 56.9 & 37.0 & 0.4 & 53.6 & 10.6 & 3.2 & 0.2 & 0.9 & 0.0 & 22.2\\
		MCD~\cite{saito2017maximum} & & 90.3 & 31.0 & 78.5 & 19.7 & 17.3 & 28.6 & 30.9 & 16.1 & 83.7 & 30.0 & 69.1 & 58.5 & 19.6 & 81.5 & 23.8 & 30.0 & 5.7 & 25.7 & 14.3 & 39.7\\
		\hline
		Source & \multirow{2}{0.1\linewidth}{\centering{DeepLabv2}} & 75.8 & 16.8 & 77.2 & 12.5 & 21.0 & 25.5 & 30.1 & 20.1 & 81.3 & 24.6 & 70.3 & 53.8 & 26.4 & 49.9 & 17.2 & 25.9 & 6.5 & 25.3 & 36.0 & 36.6\\
		AdaptSegNet~\cite{Tsai_adaptseg_2018} & & 86.5 & 36.0 & 79.9& 23.4 & 23.3 & 23.9 & 35.2 & 14.8 & 83.4 & 33.3 & 75.6 & 58.5 & 27.6 & 73.7 & 32.5 & 35.4 & 3.9 & 30.1 & 28.1 & 42.4\\ \hline
		AdvEnt~\cite{vu2019advent} & DeepLabv2 & 89.4 & 33.1 & \textbf{81.0} & 26.6 & 26.8 & 27.2 & 33.5 & 24.7 & 83.9 & \textbf{36.7} & 78.8 & 58.7 & 30.5 & 84.8 & 38.5 & 44.5 & 1.7 & 31.6 & 32.4 & 45.5 \\
		\hline
		Source & \multirow{2}{0.1\linewidth}{\centering{DeepLabv2}} &  - & - & - & - & - & - & - & - & -& - & - & - & - & - & - & - & - & - & - & 29.2\\
		FCAN~\cite{zhang2018fully} &  & - & - & - & - & - & - & - & - & -& - & - & - & - & - & - & - & - & - & - & 46.6 \\
		\hline
		Source & \multirow{6}{0.1\linewidth}{\centering{DeepLabv2}} & 71.3 & 19.2 & 69.1 & 18.4 & 10.0 & 35.7 & 27.3 &  6.8 & 79.6 & 24.8 & 72.1 & 57.6 & 19.5 & 55.5 & 15.5 & 15.1 & 11.7 & 21.1 & 12.0 & 33.8\\
		CBST & & 91.8 & 53.5 & 80.5 & 32.7 & 21.0 & 34.0 & 28.9 & 20.4 & 83.9 & 34.2 & 80.9 & 53.1 & 24.0 & 82.7 & 30.3 & 35.9 & 16.0 & 25.9 & 42.8 & 45.9\\
		MRL2 & & \textbf{91.9} & 55.2 & 80.9 & 32.1 & 21.5 & 36.7 & 30.0 & 19.0 & 84.8 & 34.9 & 80.1 & 56.1 & 23.8 & 83.9 & 28.0 & 29.4 & 20.5 & 24.0 & 40.3 & 46.0\\
		MRENT & & 91.8 & 53.4 & 80.6 & 32.6 & 20.8 & 34.3 & 29.7 & 21.0 & 84.0 & 34.1 & 80.6 & 53.9 & 24.6 & 82.8 & 30.8 & 34.9 & 16.6 & 26.4 & 42.6 & 46.1\\
		MRKLD & & 91.0 & \textbf{55.4} & 80.0 & 33.7 & 21.4 & 37.3 & 32.9 & 24.5 & \textbf{85.0} & 34.1 & 80.8 & 57.7 & 24.6 & 84.1 & 27.8 & 30.1 & 26.9 & 26.0 & 42.3 & 47.1\\
		LRENT & & 91.8 & 53.5 & 80.5 & 32.7 & 21.0 & 34.0 & 29.0 & 20.3 & 83.9 & 34.2 & \textbf{80.9} & 53.1 & 23.9 & 82.7 & 30.2 & 35.6 & 16.3 & 25.9 & 42.8 & 45.9\\
		\hline
		Source & \multirow{6}{0.1\linewidth}{\centering{ResNet-38}} & 70.0 & 23.7 & 67.8 & 15.4 & 18.1 & 40.2 & 41.9 & 25.3 & 78.8 & 11.7 & 31.4 & 62.9 & 29.8 & 60.1 & 21.5 & 26.8 & 7.7 & 28.1 & 12.0 & 35.4\\
		CBST~\cite{Zou_2018_ECCV} & & 86.8  & 46.7 & 76.9 & 26.3 & 24.8  & 42.0 & 46.0 & 38.6 & 80.7 & 15.7 & 48.0 & 57.3 & 27.9 & 78.2 & 24.5 & 49.6 & 17.7 & 25.5 & 45.1 & 45.2\\
		MRL2 & & 84.4 & 52.7 & 74.7 & 38.0 & \textbf{32.2} & 43.7 & \textbf{53.7} & 38.6 & 73.9 & 24.4 & 64.4 & 45.6 & 24.6 & 63.2 & 3.22 & 31.9 & \textbf{45.9} & 44.2 & 34.8 & 46.0\\
		MRENT & & 84.6 & 49.5 & 73.9 & \textbf{35.8} & 25.1 & \textbf{46.2} & 53.3 & 43.3 & 75.2 & 24.2 & 63.8 & 48.2 & \textbf{33.8} & 65.7 & 2.89 & 32.6 & 39.2 & \textbf{50.0} & 34.7 & 46.4\\
		MRKLD & & 84.5 & 47.7 & 74.1 & 27.9 & 22.1 & 43.8 & 46.5 & 37.8 & 83.7 & 22.7 & 56.1 & 56.8 & 26.8  & 81.7 & 22.5 & 46.2 & 27.5 & 32.3 & \textbf{47.9} & 46.8\\
		LRENT & & 80.3 & 40.8 & 65.8 & 24.6 & 30.5 & 43.1 & 49.5 & 40.3 & 82.1 & 26.0 & 54.6 & 59.4 & 32.1 & 68.0 & 31.9 & 30.0 & 21.9 & 44.8 & 46.7 & 45.9\\
		\hline
		CBST-SP & \multirow{3}{0.1\linewidth}{\centering{ResNet-38}} & 85.6 & 55.1 & 76.9 & 26.8 & 23.4 & 38.9 & 47.1 & \textbf{46.9} & 83.4 & 25.5 & 68.7 & 45.6 & 15.7 & 79.7 & 27.7 & 50.3 & 38.2 & 33.4 & 44.6 & 48.1\\
		MRKLD-SP & & 90.8 & 46.0 & 79.9 & 27.4 & 23.3 & 42.3 & 46.2 & 40.9 & 83.5 & 19.2 & 59.1 & 63.5 & 30.8 & 83.5 & 36.8 & \textbf{52.0} & 28.0 & 36.8 & 46.4 & 49.2\\
		MRKLD-SP-MST & & 91.7 & 45.1 & 80.9 & 29.0 & 23.4 & 43.8 & 47.1 & 40.9 & 84.0 & 20.0 & 60.6 & \textbf{64.0} & 31.9 & \textbf{85.8} & \textbf{39.5} & 48.7 & 25.0 & 38.0 & 47.0 & \textbf{49.8}\\
		\hline
	\end{tabular}
	}
	\vspace{-2mm}
	\caption{Experimental results on GTA5  Cityscapes.}
	\label{table:gtacity}
\end{table*}

\begin{table*}[!t]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c|c|cccccccccccccccc|c|c}
		\hline
		Method & Backbone & Road & SW & Build & Wall* & Fence* & Pole* & TL & TS & Veg. & Sky & PR & Rider & Car & Bus & Motor & Bike & mIoU & mIoU*\\
		\hline
		Source & \multirow{2}{0.1\linewidth}{\centering{DRN-105}} & 14.9 & 11.4 & 58.7 & 1.9 & 0.0 & 24.1 & 1.2 & 6.0 & 68.8 & 76.0 & 54.3 & 7.1 & 34.2 & 15.0 & 0.8 & 0.0 & 23.4 & 26.8\\
		MCD \cite{saito2017maximum} & & 84.8 & \textbf{43.6} & 79.0 & 3.9 & 0.2 & 29.1 & 7.2 & 5.5 & 83.8 & 83.1 & 51.0 & 11.7 & 79.9 & 27.2 & 6.2 & 0.0 & 37.3  & 43.5\\
		\hline
		Source & \multirow{2}{0.1\linewidth}{\centering{DeepLabv2}} & 55.6 & 23.8 & 74.6 &  &  &  & 6.1 & 12.1 & 74.8 & 79.0 & 55.3 & 19.1 & 39.6 & 23.3 & 13.7 & 25.0 &  & 38.6\\
		AdaptSegNet \cite{Tsai_adaptseg_2018} & & 84.3 & 42.7 & 77.5 &  &  &  & 4.7 & 7.0 & 77.9 & 82.5 & 54.3 & 21.0 & 72.3 & 32.2 & 18.9 & 32.3 &  & 46.7\\
		\hline
		AdvEnt \cite{vu2019advent} & DeepLabv2 & \textbf{85.6} & 42.2 & \textbf{79.7} & 8.7 & 0.4 & 25.9 & 5.4 & 8.1 & 80.4 & \textbf{84.1} & 57.9 & 23.8 & 73.3 & \textbf{36.4} & 14.2 & 33.0 & 41.2 & 48.0\\
		\hline
		Source & \multirow{2}{0.1\linewidth}{\centering{ResNet-38}} & 32.6 & 21.5 & 46.5 & 4.8 & 0.1 & 26.5 & 14.8 & 13.1 & 70.8 & 60.3 & 56.6 & 3.5 & 74.1 & 20.4 & 8.9 & 13.1 & 29.2 & 33.6\\
		CBST~\cite{Zou_2018_ECCV} & & 53.6 & 23.7 & 75.0 & 12.5 & 0.3 & 36.4 & 23.5 & 26.3 & \textbf{84.8} & 74.7 & \textbf{67.2} & 17.5 & \textbf{84.5} & 28.4 & 15.2 & \textbf{55.8} & 42.5 & 48.4\\
		\hline
		Source & \multirow{6}{0.1\linewidth}{\centering{DeepLabv2}} & 64.3 & 21.3 & 73.1 & 2.4 & 1.1 & 31.4 & 7.0 & 27.7 & 63.1 & 67.6 & 42.2 & 19.9 & 73.1 & 15.3 & 10.5 & 38.9 & 34.9 & 40.3\\
		CBST & & 68.0 & 29.9 & 76.3 & 10.8 & 1.4 & 33.9 & 22.8 & \textbf{29.5} & 77.6 & 78.3 & 60.6 & 28.3 & 81.6 & 23.5 & 18.8 & 39.8 & 42.6 & 48.9\\
		MRL2 &  & 63.4 & 27.1 & 76.4 & 14.2 & 1.4 & 35.2 & \textbf{23.6} & 29.4 & 78.5 & 77.8 & 61.4 & \textbf{29.5} & 82.2 & 22.8 & 18.9 & 42.3 & 42.8 & 48.7\\
		MRENT & & 69.6 & 32.6 & 75.8 & 12.2 & \textbf{1.8} & 35.3 & 23.3 & \textbf{29.5} & 77.7 & 78.9 & 60.0 & 28.5 & 81.5 & 25.9 & 19.6 & 41.8 & 43.4 & 49.6\\
		MRKLD & & 67.7 & 32.2 & 73.9 & 10.7 & 1.6 & \textbf{37.4} & 22.2 & 31.2 & 80.8 & 80.5 & 60.8 & 29.1 & 82.8 & 25.0 & 19.4 & 45.3 & \textbf{43.8} & \textbf{50.1}\\
		LRENT & & 65.6 & 30.3 & 74.6 & \textbf{13.8} & 1.5 & 35.8 & 23.1 & 29.1 & 77.0 & 77.5 & 60.1 & 28.5 & 82.2 & 22.6 & \textbf{20.1} & 41.9 & 42.7 & 48.7\\
		\hline
	\end{tabular}
	}
	\vspace{-2mm}
	\caption{Experimental results on SYNTHIA  Cityscapes.}
	\label{table:syncity}
\end{table*}

Confidence regularization smooths the output by lowering the confidence (the max of output softmax) and raising the probability level of other classes. Such smoothing helps to reduce the confidence on false positives (FP), although the confidence of certain true positives (TP) may also decrease. To see the change w/wo CR, we compare CBST vs MRKLD/LRENT (DeepLabv2) on GTA5  Cityscapes, by presenting their per-class mean confidence of TP (), mean confidence of FP () and the  ratios at the end of first round in Table \ref{ConfComp}. For both TP and FP, the confidence of MRKLD/LRENT are lower than CBST, but either MRKLD or LRENT outperforms CBST on almost all per-class ratios and mean ratios. This intuitively illustrates how confidence regularization benefits self-training.
\begin{figure*}[!t]
	\centering
	\resizebox{0.98\textwidth}{!}{
	\begin{tabular}{@{}cccccccccc@{}}
		\cellcolor{city_color_1}\textcolor{white}{~~road~~} &
		\cellcolor{city_color_2}~~sidewalk~~&
		\cellcolor{city_color_3}\textcolor{white}{~~building~~} &
		\cellcolor{city_color_4}\textcolor{white}{~~wall~~} &
		\cellcolor{city_color_5}~~fence~~ &
		\cellcolor{city_color_6}~~pole~~ &
		\cellcolor{city_color_7}~~traffic lgt~~ &
		\cellcolor{city_color_8}~~traffic sgn~~ &
		\cellcolor{city_color_9}~~vegetation~~ & 
		\cellcolor{city_color_0}\textcolor{white}{~~ignored~~}\\
		\cellcolor{city_color_10}~~terrain~~ &
		\cellcolor{city_color_11}~~sky~~ &
		\cellcolor{city_color_12}\textcolor{white}{~~person~~} &
		\cellcolor{city_color_13}\textcolor{white}{~~rider~~} &
		\cellcolor{city_color_14}\textcolor{white}{~~car~~} &
		\cellcolor{city_color_15}\textcolor{white}{~~truck~~} &
		\cellcolor{city_color_16}\textcolor{white}{~~bus~~} &
		\cellcolor{city_color_17}\textcolor{white}{~~train~~} &
		\cellcolor{city_color_18}\textcolor{white}{~~motorcycle~~} &
		\cellcolor{city_color_19}\textcolor{white}{~~bike~~}
	\vspace{0.5mm}
	\end{tabular}
	}
	\vspace{0.5mm}
	\includegraphics[width=0.138\textwidth]{figures/fig2/rgb1.jpg}
	\includegraphics[width=0.138\textwidth]{figures/fig2/gt1.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/cbst1.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/mrl21.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/mrent1.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/mrkld1.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/lrent1.png}\\
	\includegraphics[width=0.138\textwidth]{figures/fig2/rgb2.jpg}
	\includegraphics[width=0.138\textwidth]{figures/fig2/gt2.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/cbst2.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/mrl22.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/mrent2.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/mrkld2.png}
	\includegraphics[width=0.138\textwidth]{figures/fig2/lrent2.png}\\
	\vspace{-2mm}
	\caption{Adaptation results on GTA5  Cityscapes. Rows correspond to sample images in Cityscapes. From left to right, columns correspond to original images, ground truth, and predication results of CBST, MRL2, MRENT, MRKLD, LRENT.}
	\label{fig:gta2city}
\end{figure*}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.138\textwidth]{figures/fig3/rgb.jpg}
	\includegraphics[width=0.138\textwidth]{figures/fig3/gt.png}
	\includegraphics[width=0.138\textwidth]{figures/fig3/plcbst.png}
	\includegraphics[width=0.138\textwidth]{figures/fig3/plmrl2.png}
	\includegraphics[width=0.138\textwidth]{figures/fig3/plmrent.png}
	\includegraphics[width=0.138\textwidth]{figures/fig3/plmrkld.png}
	\includegraphics[width=0.138\textwidth]{figures/fig3/pllrent.png}\\
	\vspace{-2mm}
	\caption{Pseudo-labels in GTA5  Cityscapes. Rows correspond to sample images in Cityscapes. From left to right, columns correspond to original images, ground truth, and pseudo-labels of CBST, MRL2, MRENT, MRKLD, LRENT.}
	\label{fig:plgta2city}
\end{figure*}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\linewidth]{figures/fig4/loss_curve_mrent.pdf}
	\includegraphics[width=0.45\linewidth]{figures/fig4/loss_curve_mrl2.pdf}\\
	\includegraphics[width=0.45\linewidth]{figures/fig4/loss_curve_mrkld.pdf}
	\includegraphics[width=0.45\linewidth]{figures/fig4/loss_curve_lrent.pdf}
	\vspace{-3mm}
	\caption{Loss curves regularized by different regularizers.}
	\label{fig:loss_curves}
	\vspace{-1mm}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\linewidth]{figures/fig5/lrent_minimizer.pdf}
	\includegraphics[width=0.45\linewidth]{figures/fig5/mrkld_minimizer.pdf}
	\vspace{-2mm}
	\caption{Minimizers of LRENT and MRKLD.}
	\label{fig:softmax_temp}
	\vspace{-3mm}
\end{figure}

\begin{table*}[!t]
	\centering
	\resizebox{0.75\textwidth}{!}{
	\begin{tabular}{c|ccccc|ccccc}
		\hline
		\multicolumn{11}{c}{W  A (Office-31)}\\
		\hline
 		& \multicolumn{5}{c|}{\textbf{MRL2}} & \multicolumn{5}{c}{\textbf{MRENT}}\\
		 & 20/5 & 15/5 & 25/5 & 20/2.5 & 20/7.5 & 20/5 & 15/5 & 25/5 & 20/2.5 & 20/7.5\\
		Accuracy &  72.10.2 & 71.30.2 & 71.41.0 & 71.60.4 & 71.30.5 &  71.00.4 & 71.00.6 & 70.80.5 & 71.00.6 & 71.00.7\\
		\hline
		& \multicolumn{5}{c|}{\textbf{MRKLD}} &  \multicolumn{5}{c}{\textbf{LRENT}}\\
		 & 20/5 & 15/5 & 25/5 & 20/2.5 & 20/7.5 & 20/5 & 15/5 & 25/5 & 20/2.5 & 20/7.5 \\
		Accuracy & 70.90.4 & 70.80.4 & 70.70.2 & 70.90.5 & 71.00.8 & 71.00.3 & 71.00.8 & 71.20.6 & 71.10.5 & 71.00.4 \\ \hline
	\end{tabular}
	}
	\vspace{-2mm}
	\caption{Sensitivity analysis of portion  and portion step .}
	\label{table:p}
	\vspace{-1mm}
\end{table*}

\begin{table*}[!t]
	\centering
	\resizebox{0.9\textwidth}{!}{
	\begin{tabular}{c|ccc|ccc|ccc|ccc}
		\hline
		\multicolumn{13}{c}{W  A (Office-31)}\\
		\hline
		& \multicolumn{3}{c|}{\textbf{MRL2}} & \multicolumn{3}{c|}{\textbf{MRENT}} & \multicolumn{3}{c|}{\textbf{MRKLD}} & \multicolumn{3}{c}{\textbf{LRENT}}\\
		 & 0.01 & 0.025 & 0.05 & 0.075 & 0.1 & 0.125 & 0.075 & 0.1 & 0.125 & 0.1 & 0.25 & 0.5\\
		Accuracy & 71.50.8 & 72.10.2 & 71.71.1 & 71.00.8 & 71.00.4 & 70.91.0 & 70.90.6 & 70.90.4 & 70.60.7  & 71.21.2 & 71.00.3 & 70.80.6\\
		\hline
	\end{tabular}
	}
	\vspace{-2mm}
	\caption{Sensitivity analysis of regularizer weight .}
	\label{table:alpha}
	\vspace{-1mm}
\end{table*}

\begin{table*}[!t]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{c|c|ccccccccccccccccccc|c}
		\hline
		& & Road & SW & Build & Wall & Fence & Pole & TL & TS & Veg. & Terrain & Sky & PR & Rider & Car & Truck & Bus & Train & Motor & Bike & mean\\
		\hline
		\multirow{3}{*}{CBST} &  () & 96.2 & 86.0 & 94.6 & 83.8 & 84.9 & 84.5 & 80.4 & 78.0 & 93.9 & 87.9 & 94.5 & 90.4 & 81.4 & 95.4 & 88.4 & 85.9 & 59.5 & 78.5 & 80.6 & 85.5\\
		&  () & 72.2 & 74.1 & 69.8 & 71.7 & 76.7 & 73.7 & 72.9 & 76.5 & 71.9 & 71.2 & 68.5 & 67.2 & 69.1 & 66.1 & 76.9 & 65.5 & 76.7 & 67.2 & 73.0 & 71.6\\
		&  & 1.33 & 1.16 & 1.36 & 1.17 & 1.11 &  1.15 &  1.10 & 1.02 & 1.31 & 1.23 & 1.38 & 1.35 & 1.18 & 1.44 & 1.15 & 1.31 & 0.78 & 1.17 & 1.10 &  1.19\\
		\hline
		\multirow{3}{*}{MRKLD} &  () & 94.7 & 82.8 & 92.4 & 81.7 & 77.8 & 84.4 & 77.0 & 76.4 & 93.4 & 86.5 & 94.4 & 88.8 & 79.7 & 93.9 & 87.0 & 84.9 & 71.9 & 77.6 & 79.2 & 84.5\\
		&  () & 67.7 & 70.3 & 65.4 & 68.5 & 69.2 & 66.7 & 69.4 & 71.3 & 66.7 & 68.8 & 66.7 & 60.0 & 65.5 & 63.0 & 74.6 & 63.6 & 70.2 & 59.3 & 53.2 & 66.3\\
		&  & 1.40 & 1.18 & 1.41 & 1.19 & 1.12 & 1.27 & 1.11 & 1.07 & 1.40 & 1.26 & 1.42 & 1.48 & 1.22 & 1.49 & 1.17 & 1.34 & 1.02 & 1.31 & 1.49 & 1.27\\
		\hline
		\multirow{3}{*}{LRENT} &  () & 95.9 & 84.4 & 94.0 & 80.7 & 75.3 & 84.8 & 77.8 & 78.3 & 93.9 & 86.3 & 94.5 & 89.2 & 79.3 & 95.3 & 89.3 & 80.5 & 76.4 & 86.4 & 78.8  & 85.3\\
		&  () & 69.5 & 72.1 & 68.0 & 67.8 & 71.3 & 69.7 & 71.5 & 75.4 & 69.5 & 69.9 & 70.1 & 64.1 & 67.6 & 67.3 & 77.7 & 70.3 & 63.4 & 58.6 & 55.2 & 68.4\\
		&  & 1.38 & 1.17 & 1.38 & 1.19 & 1.06 & 1.22 & 1.09 & 1.04 & 1.35 & 1.23 & 1.35 & 1.39 & 1.17 & 1.42 & 1.15 & 1.15 & 1.2 & 1.47 & 1.43 & 1.25\\
		\hline
	\end{tabular}
	}
	\vspace{-2mm}
	\caption{Comparison of ,  and  on GTA5  Cityscapes.}\label{ConfComp}
	\vspace{-1mm}
\end{table*}

\subsection{MR versus LR}\label{mr-lr}
We analyze MR/LR intuitively and theoretically to give suggestions for practical choice of confidence regularizers.\\
\noindent\textbf{Complexity analysis:} All model regularizers only introduce negligible extra costs for the gradient computation. Label regularizers, however, requires the storage of dataset-level soft pseudo-labels. This does not present an issue in image classification but may introduce extra I/O costs in segmentation, where labels are often too large to be stored in memory and need to be written to disk.\\
\noindent\textbf{Loss curves:} To further illustrate the different properties of regularizers, we visualize how they influence the original loss surfaces by reducing the problem into binary classification with a single sample. We assume a cross-entropy loss  plus an MR/LR weighted by . For MRs, we assume  and illustrate the regularized loss curves versus  in Fig. \ref{fig:loss_curves}. For all MRs,  becomes smoother when  increases. We notice that MRKLD serves as a better barrier to prevent sharp outputs than other MRs by having steeper gradient near . This accords with our observation that MRKLD overall works the best.
For LRENT, we assume  and illustrate the regularized loss curves versus  at different  in Fig. \ref{fig:loss_curves}. Again,  becomes smoother when  increases.\\
\noindent\textbf{Class ranking:} Based on the closed-form solution of LR in Table \ref{table:regs}, we can prove that LR preserves the confidence ranking order between classes. On the other hand, given one-hot labels, MRs tend to discard such order information by giving equal confidences to negative classes. Taking MRKLD as example: using Lagrangian multiplier, we can prove the closed-form global minimizer for regularized cross-entropy loss as , where  is class index. With  being one-hot, the global minimizer is uniformly smoothed on negative classes. Similar property can be also proved for MRENT/MRL2.

We illustrate two examples of LRENT and MRKLD in Fig.~\ref{fig:softmax_temp}, where we assume  for LRENT and  for MRKLD. One can see, LRENT sharpens the input  when  (one-hot when ), while smooths  when . In all cases, the inter-class confidence orders are always preserved, while the same property does not hold for MRKLD.\\
\noindent\textbf{MR+LR:} The combination of MR and LR can take advantages of both regularizers and achieve better performance compared to single regularizer, demonstrated in VisDA17 and Office-31. However, it will also introduce extra cost to validate both hyperparameters for MR and LR.\\
\noindent\textbf{Practical suggestions:} Overall, we recommend CRST-MRKLD most based on the above analysis and its better performance. Moreover, combining MR and LR may also benefit self-training at the cost of slight extra tuning.

\section{Conclusions}\label{sec:conclusion}
In this paper, we introduce a confidence regularized self-training framework formulated as regularized self-training loss minimization. Model regularization and label regularization are considered with a family of proposed confidence regularizers. We investigate theoretical properties of CRST, including its probabilistic explanation and connection to softmax with temperature. Comprehensive experiments demonstrate the effectiveness of CRST with state-of-the-art performance. We also systematically discuss the pros and cons of the proposed regularizers and made practical suggestions. We believe this work can inspire future research on novel designs of regularizations as desired inductive biases to benefit many UDA/SSL problems.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\clearpage

\section*{Appendix}
In this appendix, we present the additional details and results that are not covered by the main paper.

\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}

\section{Derivation of soft pseudo-label in LRENT}\label{sec:derivlrent}
For entropy label regularizer, the soft pseudo-label learning problem is defined as follows.

where the solution is given as below.


It is easy to see that the optimization in (\ref{label-ent-opt}) is a convex problem. Therefore, the global optimum can be found with a Lagrangian multiplier~\cite{boyd2004convex} defined as follows:

Setting the corresponding gradients equals to  gives the global optimum ().


\section{Theoretical properties of CRSTs}
\subsection{Proof of Proposition \ref{prop:cem}}\label{sec:proofprop1}
Classification maximum likelihood (CML) was initially proposed to model clustering tasks, and can be optimized via classification expectation maximization (CEM). Compared with traditional expectation maximization (EM) that has an ``expectation'' (E) step and a ``maximization'' (M) step, CEM has an additional ``classification'' (C) step (between E and M steps) that assigns a sample to the cluster with maximal posterior probability. In~\cite{amini2002semi}, CML is generalized to discriminant semi-supervised learning with both labeled and unlabeled data defined as follows:

where:

Note that .  is the posterior probability modeled by classifiers such as logistic classifier and neural network and  is the learnable weight. \cite{amini2002semi} uses a discriminant classifier which makes no assumptions about the data distribution . Thus maximizing (\ref{cml}) is equal to maximizing (\ref{ecml}). Below we draw the connection of the CRST self-training algorithm to CEM. We first show that CRST can be rewritten as the following regularized classification maximum likelihood model:

where the above problem contains an additional regularizer term () compared with CML, defined as:

In addition, the corresponding alternative self-training optimization can be written as the following CEM process:

\noindent\textbf{E-Step:} Given the model weight , estimate the posterior probability .

\noindent\textbf{C-Step:} Fix  and solve the following problem for :


\noindent\textbf{M-Step:} Fix  and use gradient ascent to solve the following problem for .

We have thus shown that the CRST self-training algorithm is an instance of CEM.

\subsection{Proof of Proposition \ref{prop:convergence}}\label{sec:proofprop2}
As a brief recap, the general form of CRST in (\ref{crst}) can be optimized via the following two steps:

\noindent \textbf{a) Pseudo-label learning} \label{a)} Fix  and solve:

which leads to the following solver for each :

where  is the minimizer of (\ref{crst_a}) with the feasible set being  only, and  is defined as:


\noindent\textbf{b) Network retraining} \label{b)} ~ Fix  and solve the following optimization by gradient descent:


We assume , and  is convex w.r.t.  and  given the listed regularizers in Table \ref{table:regs}. Note that the definition and optimization of continuous CBST is simply a special case of CRST with . Therefore, the convergence of CRST also indicates the convergence of CBST. With the above preliminaries, we have:

\noindent\textbf{Step a) is non-increasing:} (\ref{crst_solver}) is obtained by decomposing (\ref{crst_a}) into two subproblems with feasible sets being  and , respectively. The former is a convex problems which gives a globally optimal solution, while (\ref{crst_solver}) is the result of comparing this solution against  by taking the one with a smaller cost. As a result, (\ref{crst_solver}) is also a global minimizer and (\ref{crst_a}) is guaranteed to be non-increasing.

\noindent\textbf{Step b) is non-increasing:} One may use gradient descent to minimize the loss in (\ref{crst_b}). With a proper learning rate, the loss is guaranteed to decrease monotonically. In practice, network re-training is often done with mini-batch gradient descent instead of gradient descent. This may not strictly guarantee the monotonic decrease of the loss, but will almost certainly converge to a lower one.

One can prove that the self-training loss in (\ref{crst}) is lower bounded. Therefore, the optimization of (\ref{crst}) by alternatively taking step \textbf{a)} and \textbf{b)} is convergent.

\subsection{Proof of Proposition \ref{prop:prop4}}\label{sec:proofprop4}
As mentioned in~\cite{szegedy2016rethinking}, uniformly smoothed pseudo-label  with  is

And the self-training with uniformaly smoothed pseudo-labels is defined as follows.

where  follows (\ref{smooth_solver}). 
	
In KLD model regularized self-training, the model retraining needs to optimize the following problem:

where  are the fixed pseudo-labels and  is the regularizer weight. Here, we show the equivalence of the above two problems with the following proof:

Replacing  with a one-hot completes the proof.

\subsection{Proof of Proposition \ref{prop:prop5}}\label{sec:proofprop5}
In MRENT, the model retraining needs to optimize the following problem:

	
\noindent We will show the above problem is equivalent to the model retraining in the reverse KLD model regularized self-training, which is defined as follows.

To prove the above equivalence, we have the following. 

In (\ref{kld_rev}),  is a constant. Thus one can prove that the minimization in (\ref{mrent}) is equivalent to the minimization in (\ref{mrkld_rev}).


\section{Additional details on experiments}\label{sec:addexp}
\subsection{Accuracy curves}
\begin{figure}[!b]
	\centering
	\includegraphics[width=0.85\linewidth]{figures/fig6.pdf}
	\caption{Mean accuracy versus number of epochs.}
	\label{learning_curves}
\end{figure}

To show the learning behaviors on VisDA17, we plot the curves of mean accuracy (averaged over 5 runs) versus epochs for CBST and CRSTs in Fig. \ref{learning_curves}. One can see, the proposed self-training methods are generally stable with only slight fluctuations after  epochs. Among all comparing methods, MRKLD+LRENT gives the best performance and shows consistent improvement over the CBST baseline.

\begin{figure*}[!t]
	\centering
	\resizebox{0.94\textwidth}{!}{
		\begin{tabular}{@{}cccccccccccc@{}}
			\cellcolor{visda_color_1}{~~aero~~} &
			\cellcolor{visda_color_2}~~bike~~&
			\cellcolor{visda_color_3}{~~bus~~} &
			\cellcolor{visda_color_4}{~~car~~} &
			\cellcolor{visda_color_5}~~horse~~ &
			\cellcolor{visda_color_6}~~knife~~ &
			\cellcolor{visda_color_7}\textcolor{white}{~~motor~~} &
			\cellcolor{visda_color_8}\textcolor{white}{~~person~~} &
			\cellcolor{visda_color_9}\textcolor{white}{~~plant~~} & 
			\cellcolor{visda_color_0}\textcolor{white}{~~board~~}
			\cellcolor{visda_color_10}~~train~~ &
			\cellcolor{visda_color_11}~~truck~~ &
		\end{tabular}
	}
	\vspace{1mm}
	\includegraphics[width=0.26\textwidth]{figures/fig7/tsne_source.pdf}~~~~~~
	\includegraphics[width=0.26\textwidth]{figures/fig7/tsne_cbst.pdf}~~~~~~
	\includegraphics[width=0.26\textwidth]{figures/fig7/tsne_mrkld_lrent.pdf}
	\caption{Feature visualization for target domain of VisDA17. From left to right: Source model, CBST, MRKLD+LRENT.}\label{fig:feat_vis}
\end{figure*}

\begin{figure*}[!t]
	\centering
	\includegraphics[height=0.3\linewidth]{figures/fig8/conf_source.pdf}~~
	\includegraphics[height=0.3\linewidth]{figures/fig8/conf_cbst.pdf}~~
	\includegraphics[height=0.3\linewidth]{figures/fig8/conf_mrkld_lrent.pdf}
	\caption{Confusion matrices with normalization for CBST and CRSTs.}
	\label{confus_mat}
\end{figure*}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.19\linewidth]{figures/fig9/hist_cbst.pdf}
	\includegraphics[width=0.19\linewidth]{figures/fig9/hist_mrl2.pdf}
	\includegraphics[width=0.19\linewidth]{figures/fig9/hist_mrent.pdf}
	\includegraphics[width=0.19\linewidth]{figures/fig9/hist_mrkld.pdf}
	\includegraphics[width=0.19\linewidth]{figures/fig9/hist_lrent.pdf}
	\caption{Histograms of softmax probability entries in target domain of GTA5  Cityscapes.}
	\label{fig:softmax_dis}
\end{figure*}

\subsection{Feature visualization}
We also visualize the feature embeddings of the source model, CBST and MRKLD+LRENT features on VisDA17, and show them in Fig. \ref{fig:feat_vis}. Both CBST and MRKLD+LRENT obtain improved class-wise feature alignment than the source model. MRKLD+LRENT shows slightly more accurate feature alignment due to the improved performance from confidence regularization.

\subsection{Confusion matrix}
In Fig. \ref{confus_mat}, we illustrate the normalized confusion matrices of the source model, CBST and MRKLD+LRENT on VisDA17. One can see, both CBST and MRKLD+LRENT show more diagonalized confusion matrices than source model, and MRKLD+LRENT shows less mistakes. Specifically, the confusions between pairwise different classes such as ``person vs. horse'' and ``motor vs. bike'' have be reduced by confidence regularization.

\subsection{Distributions of softmax probability entries}
Following the analysis approach in~\cite{pereyra2017regularizing}, we present the distributions of predicted softmax probability entries in the target domain for different models. Specifically, we consider the ResNet-38 backbone on GTA5  Cityscapes, with the distributions shown in Fig. \ref{fig:softmax_dis}. One could see that confidence regularization promote softer distributions by significantly reducing the proportion of highly confident entries.

\subsection{Segmentation visualization}
For qualitative evaluation, we visualize the segmentation predictions obtained by different models in Fig. \ref{fig:gta2city}. Specifically, predictions are made on sampled Cityscapes validation images by GTA5  Cityscapes models. In Fig. \ref{fig:plgta2city}, we also visualize the pseudo-labels on sampled Cityscapes training images at the beginning second self-training round.

\begin{figure*}[!t]
	\centering
	\resizebox{0.98\textwidth}{!}{
		\begin{tabular}{@{}cccccccccc@{}}
			\cellcolor{city_color_1}\textcolor{white}{~~road~~} &
			\cellcolor{city_color_2}~~sidewalk~~&
			\cellcolor{city_color_3}\textcolor{white}{~~building~~} &
			\cellcolor{city_color_4}\textcolor{white}{~~wall~~} &
			\cellcolor{city_color_5}~~fence~~ &
			\cellcolor{city_color_6}~~pole~~ &
			\cellcolor{city_color_7}~~traffic lgt~~ &
			\cellcolor{city_color_8}~~traffic sgn~~ &
			\cellcolor{city_color_9}~~vegetation~~ & 
			\cellcolor{city_color_0}\textcolor{white}{~~ignored~~}\\
			\cellcolor{city_color_10}~~terrain~~ &
			\cellcolor{city_color_11}~~sky~~ &
			\cellcolor{city_color_12}\textcolor{white}{~~person~~} &
			\cellcolor{city_color_13}\textcolor{white}{~~rider~~} &
			\cellcolor{city_color_14}\textcolor{white}{~~car~~} &
			\cellcolor{city_color_15}\textcolor{white}{~~truck~~} &
			\cellcolor{city_color_16}\textcolor{white}{~~bus~~} &
			\cellcolor{city_color_17}\textcolor{white}{~~train~~} &
			\cellcolor{city_color_18}\textcolor{white}{~~motorcycle~~} &
			\cellcolor{city_color_19}\textcolor{white}{~~bike~~}
		\end{tabular}
	}
	
	\vspace{1mm}
	\includegraphics[width=0.24\textwidth]{figures/fig10/rgb3.jpg}
	\includegraphics[width=0.24\textwidth]{figures/fig10/rgb4.jpg}
	\includegraphics[width=0.24\textwidth]{figures/fig10/rgb5.jpg}
	\includegraphics[width=0.24\textwidth]{figures/fig10/rgb6.jpg}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig10/gt3.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/gt4.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/gt5.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/gt6.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig10/cbst3.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/cbst4.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/cbst5.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/cbst6.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrl23.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrl24.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrl25.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrl26.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrent3.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrent4.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrent5.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrent6.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrkld3.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrkld4.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrkld5.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/mrkld6.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig10/lrent3.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/lrent4.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/lrent5.png}
	\includegraphics[width=0.24\textwidth]{figures/fig10/lrent6.png}
	\caption{Adaptation results on GTA5  Cityscapes. Rows correspond to sample images in Cityscapes. From top to bottom, rows correspond to original images, ground truth, and predication results of CBST, MRL2, MRENT, MRKLD, LRENT.}
	\label{fig:gta2city}
\end{figure*}

\begin{figure*}[!t]
	\centering
	\resizebox{0.98\textwidth}{!}{
		\begin{tabular}{@{}cccccccccc@{}}
			\cellcolor{city_color_1}\textcolor{white}{~~road~~} &
			\cellcolor{city_color_2}~~sidewalk~~&
			\cellcolor{city_color_3}\textcolor{white}{~~building~~} &
			\cellcolor{city_color_4}\textcolor{white}{~~wall~~} &
			\cellcolor{city_color_5}~~fence~~ &
			\cellcolor{city_color_6}~~pole~~ &
			\cellcolor{city_color_7}~~traffic lgt~~ &
			\cellcolor{city_color_8}~~traffic sgn~~ &
			\cellcolor{city_color_9}~~vegetation~~ & 
			\cellcolor{city_color_0}\textcolor{white}{~~ignored~~}\\
			\cellcolor{city_color_10}~~terrain~~ &
			\cellcolor{city_color_11}~~sky~~ &
			\cellcolor{city_color_12}\textcolor{white}{~~person~~} &
			\cellcolor{city_color_13}\textcolor{white}{~~rider~~} &
			\cellcolor{city_color_14}\textcolor{white}{~~car~~} &
			\cellcolor{city_color_15}\textcolor{white}{~~truck~~} &
			\cellcolor{city_color_16}\textcolor{white}{~~bus~~} &
			\cellcolor{city_color_17}\textcolor{white}{~~train~~} &
			\cellcolor{city_color_18}\textcolor{white}{~~motorcycle~~} &
			\cellcolor{city_color_19}\textcolor{white}{~~bike~~}
		\end{tabular}
	}
	
	\vspace{1mm}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plrgb1.jpg}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plrgb2.jpg}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plrgb6.jpg}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plrgb5.jpg}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plgt1.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plgt2.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plgt6.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plgt5.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plcbst1.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plcbst2.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plcbst6.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plcbst5.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrl21.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrl22.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrl26.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrl25.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrent1.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrent2.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrent6.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrent5.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrkld1.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrkld2.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrkld6.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/plmrkld5.png}
	\quad\\\vspace{0.5mm}
	\includegraphics[width=0.24\textwidth]{figures/fig11/pllrent1.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/pllrent2.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/pllrent6.png}
	\includegraphics[width=0.24\textwidth]{figures/fig11/pllrent5.png}
	\caption{Adaptation results on GTA5  Cityscapes. Rows correspond to sample images in Cityscapes. From top to bottom, rows correspond to original images, ground truth, and pseudo-label maps of CBST, MRL2, MRENT, MRKLD, LRENT.}
	\label{fig:plgta2city}
\end{figure*}

\end{document}
