\documentclass{article} \usepackage{iclr2019_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}

\newcommand{\bert}{BERT\xspace}
\newcommand{\bertsize}[1]{BERT-#1\xspace} \newcommand{\roberta}{RoBERTa\xspace}
\newcommand{\xlnet}{XLNet\xspace}
\newcommand{\squad}{SQuAD\xspace}

\newcommand{\mingda}[1]{\textcolor{red}{[#1 --MC]}}
\newcommand{\kevin}[1]{\textcolor{blue}{[#1 --KG]}}
\newcommand{\lanzhzh}[1]{\textcolor{violet}{[#1 --LAN]}}
\newcommand{\radu}[1]{\textcolor{green}{[#1 --RS]}}
\usepackage{subcaption}

\usepackage{graphicx}
\graphicspath{ {./imgs/} }



\title{ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}



\author{
    \begin{tabular}[t]{c} 
        Zhenzhong Lan \hspace{15pt} Mingda Chen\thanks{Work done as an intern at Google Research, driving data processing and downstream task evaluations.} \hspace{15pt} Sebastian Goodman \hspace{15pt} Kevin Gimpel \1.5ex]
    \end{tabular} 
    \\
    \text{\hspace{50pt} Google Research \hspace{30pt} Toyota Technological Institute at Chicago} \
    p(n) = \frac{1/n}{\sum_{k=1}^N{1/k}}

We set the maximum length of -gram (i.e., ) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as ``White House correspondents'').

All the model updates use a batch size of 4096 and a \textsc{Lamb} optimizer with learning rate 0.00176~\citep{you2019reducing}.
We train all models for 125,000 steps unless otherwise specified.
Training was done on Cloud TPU V3.
The number of TPUs used for training ranged from 64 to 512, depending on model size. 

The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified. 


\subsection{Evaluation Benchmarks}


\subsubsection{Intrinsic Evaluation}
To monitor the training progress, we create a development set based on the development sets from \squad and RACE using the same procedure as in Sec.~\ref{sec:setup}. 
We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection. 



\subsubsection{Downstream Evaluation}
Following \cite{yang2019xlnet} and \cite{liu2019roberta}, we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark~\citep{wang-etal-2018-glue}, two versions of the Stanford Question Answering Dataset (\squad;~\citealp{rajpurkar-etal-2016-squad,rajpurkar-etal-2018-know}), and the ReAding Comprehension from Examinations  (RACE) dataset~\citep{lai2017race}. 
For completeness, we provide description of these benchmarks in Appendix~\ref{downstream_detailed_description}.
As in \citep{liu2019roberta}, we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs.  


\subsection{Overall Comparison between BERT and ALBERT}
We are now ready to quantify the impact of the design choices described in Sec.~\ref{sec:model}, specifically the ones around parameter efficiency.
The improvement in parameter efficiency showcases the most important advantage of ALBERT's design choices, as shown in Table~\ref{table:overall_comparison}: with only around 70\% of BERT-large's parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks:
SQuAD v1.1 (+1.9\%), SQuAD v2.0 (+3.1\%), MNLI (+1.4\%), SST-2 (+2.2\%), and RACE (+8.4\%).


Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). 
Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models.
If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure. 



\begin{table}[!htbp] 
\setlength{\tabcolsep}{5pt}
\small
\centering
\begin{tabular}{ l l | c c c c c c | c | c}
 \multicolumn{2}{c}{Model}    &Parameters        	&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE	&Avg & Speedup\\
\hline
\multirow{3}{*}{BERT} &base	     &108M       &90.4/83.2	&80.4/77.6	&84.5	&92.8	&68.2	&82.3 & 4.7x \\

    &large	        &334M	&92.2/85.5	&85.0/82.2	&86.6	&93.0	&73.9	&85.2 & 1.0\\
\hline 
\multirow{4}{*}{ALBERT}  &base	      &12M    	    &89.3/82.3	&80.0/77.1	&81.6	&90.3	&64.0	&80.1 &5.6x \\
&large	        	 &18M   &90.6/83.9	&82.3/79.4	&83.5	&91.7	&68.5	&82.4 &1.7x\\
&xlarge		   &60M   &92.5/86.1	&86.1/83.1	&86.4	&92.4	&74.8	&85.5 & 0.6x\\
&xxlarge	&235M	& \textbf{94.1/88.3}	&\textbf{88.1/85.1}	&\textbf{88.0}	&\textbf{95.2}	&\textbf{82.3}	&\textbf{88.7} & 0.3x\\
\end{tabular}
\caption{Dev set results for models pretrained over \textsc{BookCorpus} and Wikipedia for 125k steps.
Here and everywhere else, the Avg column is computed by averaging the scores of the downstream tasks to its left (the two numbers of F1 and EM for each SQuAD are first averaged).}
\label{table:overall_comparison}
\end{table}

Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT. 

\subsection{Factorized Embedding Parameterization}

Table \ref{table:voc_embedding} shows the effect of changing the vocabulary embedding size  using an ALBERT-base configuration setting (see Table~\ref{table:model_architecture}), using the same set of representative downstream tasks.
Under the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by much.
Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the best.
Based on these results, we use an embedding size  in all future settings, as a necessary step to do further scaling.

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ c c c | c c c c c | c }
Model  &  & Parameters     	&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE	&Avg \\
\hline
\multirow{4}{*}{\shortstack{ALBERT \\ base \\ not-shared}}
& 64  & 87M &89.9/82.9	&80.1/77.8	&82.9	&91.5	&66.7   &81.3 \\
& 128 & 89M &89.9/82.8	&80.3/77.3	&83.7	&91.5	&67.9	&81.7 \\
& 256 & 93M &90.2/83.2	&80.3/77.4	&84.1	&91.9	&67.3	&81.8 \\
& 768 & 108M &90.4/83.2	&80.4/77.6	&84.5	&92.8	&68.2	&82.3  \\
\hline
\multirow{4}{*}{\shortstack{ALBERT \\base \\ all-shared}}
&64     & 10M &88.7/81.4	&77.5/74.8  &80.8	&89.4	&63.5	&79.0 \\
&128    & 12M  &89.3/82.3	&80.0/77.1	&81.6	&90.3	&64.0	&80.1 \\
&256    & 16M &88.8/81.5	&79.1/76.3	&81.5	&90.3	&63.4	&79.6 \\
&768    & 31M &88.6/81.5	&79.2/76.6	&82.0	&90.6	&63.3	&79.8 \\
\end{tabular}
\caption{The effect of vocabulary embedding size on the performance of ALBERT-base.}
\label{table:voc_embedding}
\end{table}



\vspace{-3pt}

\subsection{Cross-layer parameter sharing}
\label{sec:sharing}
Table \ref{table:paramter-sharing} presents experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table~\ref{table:model_architecture}) with two embedding sizes ( and ).
We compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and intermediate strategies in which only the attention parameters are shared (but not the FNN ones) or only the FFN parameters are shared (but not the attention ones).

The all-shared strategy hurts performance under both conditions, but it is less severe for  (-1.5 on Avg) compared to  (-2.5 on Avg).
In addition, most of the performance drop appears to come from sharing the FFN-layer parameters, while sharing the attention parameters results in no drop when  (+0.1 on Avg), and a slight drop when  (-0.7 on Avg).

There are other strategies of sharing the parameters cross layers. For example, We can divide the  layers into  groups of size , and each size- group shares parameters. Overall, our experimental results shows that the smaller the group size  is, the better the performance we get. However, decreasing group size  also dramatically increase the number of overall parameters. We choose all-shared strategy as our default choice.  

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ l l | c | c c c c c | c }
\multicolumn{2}{c}{Model}	    &Parameters  	&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE &Avg \\
\hline
\multirow{4}{*}{\shortstack{ALBERT \\ base \\ =768}}
&all-shared       &31M   &88.6/81.5	&79.2/76.6   &82.0	&90.6	&63.3	&79.8 \\
&shared-attention &83M   &89.9/82.7	&80.0/77.2	 &84.0	&91.4	&67.7	&81.6 \\
&shared-FFN       &57M   &89.2/82.1	&78.2/75.4	 &81.5	&90.8	&62.6	&79.5 \\
&not-shared        &108M  &90.4/83.2	&80.4/77.6	 &84.5	&92.8	&68.2	&82.3 \\
\hline
\multirow{4}{*}{\shortstack{ALBERT \\base \\ =128}}
&all-shared       & 12M  &89.3/82.3	&80.0/77.1	&82.0	&90.3	&64.0	&80.1 \\
&shared-attention & 64M  &89.9/82.8	&80.7/77.9	&83.4	&91.9	&67.6	&81.7 \\
&shared-FFN       & 38M  &88.9/81.6	&78.6/75.6	&82.3	&91.7	&64.4	&80.2 \\
&not-shared   & 89M  &89.9/82.8	&80.3/77.3	&83.2	&91.5	&67.9	&81.6 \\
\end{tabular}
\caption{The effect of cross-layer parameter-sharing strategies, ALBERT-base configuration.}
\label{table:paramter-sharing}
\end{table}

\vspace{-5pt}

\subsection{Sentence order prediction (SOP)}
\label{sec:sop}
We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERT-base configuration.
Results are shown in Table \ref{table:sop}, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks.

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ l | c c c | c c c c  c | c }
&\multicolumn{3}{|c|}{Intrinsic Tasks} & \multicolumn{6}{c}{Downstream Tasks} \\
SP tasks	    	&MLM	&NSP & SOP &SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE & Avg \\
\hline
None &54.9 &52.4 & 53.3 &88.6/81.5	&78.1/75.3	&81.5	&89.9	&61.7		& 79.0 \\
NSP	 &54.5 &90.5 & 52.0 &88.4/81.5	&77.2/74.6	&81.6	&\textbf{91.1}	&62.3	& 79.2  \\
SOP	 &54.0 &78.9 &86.5  &\textbf{89.3/82.3}	&\textbf{80.0/77.1}	&\textbf{82.0}	&90.3	&\textbf{64.0}	 & \textbf{80.1}	\\
\end{tabular}
\caption{The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks.}
\label{table:sop}
\end{table}

The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0\% accuracy, similar to the random-guess performance for the ``None'' condition).
This allows us to conclude that NSP ends up modeling only topic shift.
In contrast, the SOP loss does solve the NSP task relatively well (78.9\% accuracy), and the SOP task even better (86.5\% accuracy).
Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1\% for SQuAD1.1, +2\% for SQuAD2.0, +1.7\% for RACE), for an Avg score improvement of around +1\%.

\subsection{What if we train for the same amount of time?}

The speed-up results in Table \ref{table:overall_comparison} indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge.
Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours).
In Table \ref{table:same-training-time}, we compare the performance of a  BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training).


\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ l c c|  c c c c c | c }
Models	& Steps& Time	&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE & Avg \\
\hline
BERT-large & 400k  & 34h      &93.5/87.4	&86.9/84.3	&87.8	&94.6	&77.3	&87.2\\
ALBERT-xxlarge & 125k & 32h  &\textbf{94.0/88.1}	&\textbf{88.3/85.3}	&87.8	&\textbf{95.4}	&\textbf{82.5}   &\textbf{88.7}\\
\end{tabular}
\caption{The effect of controlling for training time, BERT-large vs ALBERT-xxlarge configurations.}
\label{table:same-training-time}
\end{table}

After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5\% better on Avg, with the difference on RACE as high as +5.2\%.


\subsection{Additional training data and dropout effects}
The experiments done up to this point use only the Wikipedia and \textsc{BookCorpus} datasets, as in \citep{devlin2018bert}.
In this section, we report measurements on the impact of the additional data used by both XLNet~\citep{yang2019xlnet} and RoBERTa~\citep{liu2019roberta}.

Fig.~\ref{fig:additiona_data} plots the dev set MLM accuracy under two conditions, without and with additional data, with the latter condition giving a significant boost.
We also observe  performance improvements on the downstream tasks in Table \ref{table:additional_training_data}, except for the SQuAD benchmarks (which are Wikipedia-based, and therefore are negatively affected by out-of-domain training material).

\begin{figure}[!htbp] 
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{adding_additional_data}
  \caption{Adding data   \label{fig:additiona_data}}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{removing_dropout}
  \caption{Removing dropout   \label{fig:removing_dropout}}
\end{subfigure}
\caption{The effects of adding data and removing dropout during training.}
\label{fig:additional_data_removing_dropout}
\end{figure}

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ r |  c c c c c | c }
&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE & Avg \\
\hline
No additional data      &\textbf{89.3/82.3}	&\textbf{80.0/77.1}	&81.6	&90.3	&64.0	&80.1\\
With additional data      &88.8/81.7	&79.1/76.3	&\textbf{82.4}	&\textbf{92.8}	&\textbf{66.0}	&\textbf{80.8}\\
\end{tabular}
\caption{The effect of additional training data using the ALBERT-base configuration.}
\label{table:additional_training_data}
\end{table}


We also note that, even after training for 1M steps, our largest models still do not overfit to their training data.
As a result, we decide to remove dropout to further increase our model capacity.
The plot in Fig.~\ref{fig:removing_dropout} shows that removing dropout significantly improves MLM accuracy.
Intermediate evaluation on ALBERT-xxlarge at around 1M training steps (Table~\ref{table:removing_dropout}) also confirms that removing dropout helps the downstream tasks.
There is empirical \citep{szegedy2017inception} and theoretical \citep{li2019understanding} evidence showing that a combination of batch normalization and dropout in Convolutional Neural Networks may have harmful results.
To the best of our knowledge, we are the first to show that dropout can hurt performance in large Transformer-based models. However, the underlying network structure of ALBERT is a special case of the transformer and further experimentation is needed to see if this phenomenon appears with other transformer-based architectures or not.

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ r |  c c c c c | c }
&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE & Avg \\
\hline
With dropout     &94.7/89.2	&89.6/86.9	&90.0	&96.3	&85.7	&90.4\\
Without dropout  &\textbf{94.8/89.5}	 &\textbf{89.9/87.2}	&\textbf{90.4}	&\textbf{96.5}	&\textbf{86.1}	&\textbf{90.7}\\
\end{tabular}
\caption{The effect of removing dropout, measured for an ALBERT-xxlarge configuration.}
\label{table:removing_dropout}
\end{table}


\vspace{-5pt}


\subsection{Current State-of-the-art on NLU Tasks}

The results we report in this section make use of the training data used by~\cite{devlin2018bert}, as well as the additional data used by~\cite{liu2019roberta} and~\cite{yang2019xlnet}. 
We report state-of-the-art results under two settings for fine-tuning: single-model and ensembles.
In both settings, we only do single-task fine-tuning\footnote{Following \cite{liu2019roberta}, we fine-tune for RTE, STS, and MRPC using an MNLI checkpoint.}.
Following \cite{liu2019roberta}, on the development set we report the median result over five runs.




\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ l   c c c c c c c c c c}
Models &MNLI	&QNLI	&QQP	&RTE	&SST	&MRPC	&CoLA	&STS	&WNLI	&Avg \\
\hline
\multicolumn{11}{l}{\textit{Single-task single models on dev}}\\
BERT-large	 &86.6	&92.3	&91.3	&70.4	&93.2	&88.0	&60.6	&90.0 &- &-	\\	
XLNet-large	 &89.8	&93.9	&91.8	&83.8	&95.6	&89.2	&63.6	&91.8 &- &-	\\	
RoBERTa-large&90.2	&94.7	&\textbf{92.2}	&86.6	&96.4	&\textbf{90.9}	&68.0	&92.4 &- &-	\\	
ALBERT (1M)   &90.4	&95.2	&92.0	&88.1	&96.8	&90.2	&68.7	&92.7 &- &-	\\
ALBERT (1.5M) &\textbf{90.8}	&\textbf{95.3}	&\textbf{92.2}	&\textbf{89.2}	&\textbf{96.9}	&\textbf{90.9}	&\textbf{71.4}	&\textbf{93.0} &- &-	\\	
\hline
\multicolumn{11}{l}{\textit{Ensembles on test (from leaderboard as of Sept. 16, 2019)}}\\
ALICE	    &88.2	&95.7	&\textbf{90.7}	&83.5	&95.2	&92.6	&\textbf{69.2}	&91.1	&80.8	&87.0 \\
MT-DNN	    &87.9	&96.0	&89.9	&86.3	&96.5	&92.7	&68.4	&91.1	&89.0	&87.6 \\
XLNet	    &90.2	&98.6	&90.3	&86.3	&96.8	&93.0	&67.8	&91.6	&90.4	&88.4 \\
RoBERTa	    &90.8	&98.9	&90.2	&88.2	&96.7	&92.3	&67.8	&92.2	&89.0	&88.5 \\
Adv-RoBERTa	&91.1	&98.8	&90.3	&88.7	&96.8	&93.1	&68.0	&92.4	&89.0	&88.8 \\
ALBERT 	    &\textbf{91.3}	&\textbf{99.2}	&90.5	&\textbf{89.2}	&\textbf{97.1}	&\textbf{93.4}	&69.1	&\textbf{92.5}	&\textbf{91.8}	&\textbf{89.4} \\
\end{tabular}
\caption{State-of-the-art results on the GLUE benchmark.
For single-task single-model results, we report ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensemble uses models trained with 1M, 1.5M, and other numbers of steps.}
\label{table:glue}
\end{table}


The single-model ALBERT configuration incorporates the best-performing settings discussed: an ALBERT-xxlarge configuration (Table~\ref{table:model_architecture}) using combined MLM and SOP losses, and no dropout. 
The checkpoints that contribute to the final ensemble model are selected based on development set performance; the number of checkpoints considered for this selection range from 6 to 17, depending on the task. 
For the GLUE (Table~\ref{table:glue}) and RACE (Table~\ref{table:sota-squad-race}) benchmarks, we average the model predictions for the ensemble models, where the candidates are fine-tuned from different training steps using the 12-layer and 24-layer architectures.
For SQuAD (Table~\ref{table:sota-squad-race}), we average the prediction scores for those spans that have multiple probabilities; we also average the scores of the ``unanswerable'' decision.

Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. 
The latter appears to be a particularly strong improvement, a jump of +17.4\% absolute points over BERT~\citep{devlin2018bert,clark2019bam}, +7.6\% over XLNet~\citep{yang2019xlnet},  +6.2\% over RoBERTa~\citep{liu2019roberta}, and 5.3\% over DCMI+ \citep{zhang2019dcmn+}, an ensemble of multiple models specifically designed for reading comprehension tasks.
Our single model achieves an accuracy of , which is still  better than the state-of-the-art ensemble model. 



\begin{table}[!htbp] 
\setlength{\tabcolsep}{4pt}
\small
\centering
\begin{tabular}{ l   c c c c }
Models		&SQuAD1.1 dev	&SQuAD2.0 dev	& SQuAD2.0 test &RACE test (Middle/High) \\
\hline
\multicolumn{5}{l}{\textit{Single model (from leaderboard as of Sept.~23, 2019)}}\\
BERT-large          & 90.9/84.1 & 81.8/79.0 & 89.1/86.3 & 72.0 (76.6/70.1) \\
XLNet               & 94.5/89.0 & 88.8/86.1 & 89.1/86.3 & 81.8 (85.5/80.2) \\
RoBERTa             & 94.6/88.9 &89.4/86.5  & 89.8/86.8 & 83.2 (86.5/81.3) \\
UPM                 &  - & - & 89.9/87.2 & - \\
XLNet + SG-Net Verifier++ & - & - & 90.1/87.2 &  -\\
ALBERT (1M)          & 94.8/89.2 & 89.9/87.2 & - & 86.0 (88.2/85.1) \\
ALBERT (1.5M)        & \textbf{94.8/89.3}	& \textbf{90.2/87.4} & \textbf{90.9/88.1} & \textbf{86.5 (89.0/85.5)} \\
\hline
\multicolumn{5}{l}{\textit{Ensembles (from leaderboard as of Sept.~23, 2019)}}\\
BERT-large          & 92.2/86.2        & - & - &  - \\
XLNet + SG-Net Verifier & -& -& 	90.7/88.2	& -\\
UPM & - & - & 90.7/88.2 & \\ 
XLNet + DAAF + Verifier & - & - & 	90.9/88.6 & -\\
DCMN+               & - & -& - & 84.1 (88.5/82.3) \\
ALBERT     &    \textbf{95.5/90.1}      & \textbf{91.4/88.9} & \textbf{92.2/89.7}    &\textbf{89.4 (91.2/88.6)}  \\ 
\end{tabular}
\caption{State-of-the-art results on the SQuAD and RACE benchmarks. }
\label{table:sota-squad-race}
\vspace{-3pt}
\end{table}
	

 \section{Discussion}


While ALBERT-xxlarge has less parameters than BERT-large and gets significantly better results, it is computationally more expensive due to its larger structure. An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention \citep{child2019generating} and block attention \citep{shen2018bi}.
An orthogonal line of research, which could provide additional representation power, includes hard example mining \citep{mikolov2013distributed} and more efficient language modeling training \citep{yang2019xlnet}.
Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations.

 
\section*{Acknowledgement}
The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa;  Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the \href{https://github.com/CLUEbenchmark/CLUE}{CLUE community} for providing the training data and evaluation benechmark of the Chinese version of ALBERT models. 

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}
\appendix
\section{Appendix}
\label{appendix}

\subsection{Effect of network depth and width}
\label{sec:depth-and-width}

In this section, we check how depth (number of layers) and width (hidden size) affect the performance of ALBERT.
Table \ref{table:varying-depth} shows the performance of an ALBERT-large configuration (see Table~\ref{table:model_architecture}) using different numbers of layers.
Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).\footnote{If we compare the performance of ALBERT-large here to the performance in Table \ref{table:overall_comparison}, we can see that this warm-start technique does not help to improve the downstream performance. However, it does help the 48-layer network to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from a 6-layer network.} Similar technique has been used in \cite{gong2019efficient}.
If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although they have the same number of parameters, the performance increases significantly.
However, there are diminishing returns when continuing to increase the number of layers: the results of a 12-layer network are relatively close to the results of a 24-layer network, and the performance of a 48-layer network appears to decline.

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ c  c | c c c c c | c }
Number of layers	 & Parameters 	&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE & Avg \\
\hline
1   &18M    &31.1/22.9	&50.1/50.1	&66.4	&80.8	&40.1   &52.9\\
3   &18M    &79.8/69.7	&64.4/61.7	&77.7	&86.7	&54.0	&71.2\\
6   &18M    &86.4/78.4	&73.8/71.1	&81.2	&88.9	&60.9	&77.2\\
12  &18M    &89.8/83.3	&80.7/77.9	&83.3	&91.7	&66.7	&81.5\\
24  &18M    &\textbf{90.3/83.3}	&\textbf{81.8/79.0}	&83.3	&91.5	&\textbf{68.7}	&\textbf{82.1}\\
48  &18M    &90.0/83.1	&\textbf{81.8/78.9}	&\textbf{83.4}	&\textbf{91.9}	&66.9	&81.8\\
\end{tabular}
\caption{The effect of increasing the number of layers for an ALBERT-large configuration.}
\label{table:varying-depth}
\end{table}

A similar phenomenon, this time for width, can be seen in Table \ref{table:varying-width} for a 3-layer ALBERT-large configuration.
As we increase the hidden size, we get an increase in performance with diminishing returns.
At a hidden size of 6144, the performance appears to decline significantly.
We note that none of these models appear to overfit the training data, and they all have higher training and development loss compared to the best-performing ALBERT configurations.

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ c  c | c c c c c | c }
Hidden size	& Parameters      	&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE & Avg \\
\hline
1024    &18M    &79.8/69.7	&64.4/61.7	&77.7	&86.7	&54.0	&71.2\\
2048    &60M    &83.3/74.1	&69.1/66.6	&79.7	&88.6	&58.2	&74.6\\
4096    &225M   &\textbf{85.0/76.4}	&\textbf{71.0/68.1}	&\textbf{80.3}	&\textbf{90.4}	&\textbf{60.4}	&\textbf{76.3}\\
6144    &499M   &84.7/75.8	&67.8/65.4	&78.1	&89.1	&56.0	&74.0\\
\end{tabular}
\caption{The effect of increasing the hidden-layer size for an ALBERT-large 3-layer configuration.}
\label{table:varying-width}
\end{table}



\subsection{Do very wide ALBERT models need to be deep(er) too?}
In Section \ref{sec:depth-and-width}, we show that for ALBERT-large (=1024), the difference between a 12-layer and a 24-layer configuration is small.
Does this result still hold for much wider ALBERT configurations, such as ALBERT-xxlarge (=4096)?

\begin{table}[!htbp] 
\small
\centering
\begin{tabular}{ c |  c c c c c | c }
Number of layers		&SQuAD1.1	&SQuAD2.0	&MNLI	&SST-2	&RACE & Avg \\
\hline
12      &94.0/88.1	&88.3/85.3	&87.8	&95.4	&82.5   &88.7\\
24      &94.1/88.3	&88.1/85.1	&88.0	&95.2	&82.3	&88.7\\
\end{tabular}
\caption{The effect of a deeper network using an ALBERT-xxlarge configuration.}
\label{table:12-vs-24-ALBERT-xxlarge}
\end{table}

The answer is given by the results from Table \ref{table:12-vs-24-ALBERT-xxlarge}.
The difference between 12-layer and 24-layer ALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg score being the same.
We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer configuration.

\subsection{Downstream Evaluation Tasks}
\label{downstream_detailed_description}
\paragraph{GLUE} GLUE is comprised of 9 tasks, namely Corpus of Linguistic Acceptability (CoLA;~\citealp{warstadt2018neural}), Stanford Sentiment Treebank (SST;~\citealp{socher-etal-2013-recursive}), Microsoft Research Paraphrase Corpus
(MRPC;~\citealp{dolan-brockett-2005-automatically}), Semantic Textual Similarity Benchmark (STS;~\citealp{cer-etal-2017-semeval}),
Quora Question Pairs (QQP;~\citealp{qqp2016url}), Multi-Genre NLI (MNLI;~\citealp{williams-etal-2018-broad}), Question NLI (QNLI;~\citealp{rajpurkar-etal-2016-squad}), Recognizing Textual
Entailment (RTE;~\citealp{dagan2005pascal,bar2006second,giampiccolo-etal-2007-third,bentivogli2009fifth}) and
Winograd NLI (WNLI;~\citealp{levesque2012winograd}). It focuses on evaluating model capabilities for natural language understanding. 
When reporting MNLI results, we only report the ``match'' condition (MNLI-m). We follow the finetuning procedures from prior work \citep{devlin2018bert, liu2019roberta, yang2019xlnet} and report the held-out test set performance obtained from GLUE submissions. For test set submissions, we perform task-specific modifications for WNLI and QNLI as described by \cite{liu2019roberta} and \cite{yang2019xlnet}. 

\paragraph{\squad} \squad is an extractive question answering dataset built from Wikipedia. The answers are segments from the context paragraphs and the task is to predict answer spans. We evaluate our models on two versions of SQuAD: v1.1 and v2.0. \squad v1.1 has 100,000 human-annotated question/answer pairs. \squad v2.0 additionally introduced 50,000 unanswerable questions. For \squad v1.1, we use the same training procedure as \bert, whereas for \squad v2.0, models are jointly trained with a span extraction loss and an additional classifier for predicting answerability~\citep{yang2019xlnet,liu2019roberta}. We report both development set and test set performance.

\paragraph{RACE} RACE is a large-scale dataset for multi-choice reading comprehension, collected from English examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate answers. Following prior work~\citep{yang2019xlnet,liu2019roberta}, we use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the representations from the ``[CLS]'' token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. We train our models on both domains and report accuracies on both the development set and test set.


\subsection{Hyperparameters}

Hyperparameters for downstream tasks are shown in Table~\ref{tab:downstream-hyperparameter}.
We adapt these hyperparameters from \cite{liu2019roberta},  \cite{devlin2018bert}, and \cite{yang2019xlnet}.

\begin{table}[!htbp] 
    \small
    \centering

\begin{tabular}{c|ccccccc}
& LR & BSZ & ALBERT DR & Classifier DR & TS & WS & MSL \\\hline
CoLA & 1.00E-05 & 16 & 0 & 0.1 & 5336 & 320 & 512 \\
STS & 2.00E-05 & 16 & 0 & 0.1 & 3598 & 214 & 512 \\
SST-2 & 1.00E-05 & 32 & 0 & 0.1 & 20935 & 1256 & 512 \\
MNLI & 3.00E-05 & 128 & 0 & 0.1 & 10000 & 1000 & 512 \\
QNLI & 1.00E-05 & 32 & 0 & 0.1 & 33112 & 1986 & 512 \\
QQP & 5.00E-05 & 128 & 0.1 & 0.1 & 14000 & 1000 & 512 \\
RTE & 3.00E-05 & 32 & 0.1 & 0.1 & 800 & 200 & 512 \\
MRPC & 2.00E-05 & 32 & 0 & 0.1 & 800 & 200 & 512 \\
WNLI & 2.00E-05 & 16 & 0.1 & 0.1 & 2000 & 250 & 512 \\
SQuAD v1.1 & 5.00E-05 & 48 & 0 & 0.1 & 3649 & 365 & 384 \\
SQuAD v2.0 & 3.00E-05 & 48 & 0 & 0.1 & 8144 & 814 & 512 \\
RACE & 2.00E-05 & 32 & 0.1 & 0.1 & 12000 & 1000 & 512 \\
\end{tabular}
    \caption{Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch Size. DR: Dropout Rate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence Length.}
    \label{tab:downstream-hyperparameter}
\end{table}





 

\end{document}
