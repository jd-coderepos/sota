\documentclass[conference]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{esint}

\makeatletter

\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\theoremstyle{plain}
\newtheorem{cor}[thm]{\protect\corollaryname}











\usepackage{amsthm}



\newcommand{\lyxline}[1][1pt]{\par\noindent \rule[.5ex]{\linewidth}{#1}\par}
\providecommand{\tabularnewline}{\\}












\ifCLASSINFOpdf
\else
\fi




























\makeatother

\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\theoremname}{Theorem}

\begin{document}




\title{{\Huge Probe and Adapt: Rate Adaptation for HTTP Video Streaming
At Scale}}





\author{\IEEEauthorblockN{Zhi Li, Xiaoqing Zhu, Josh Gahm, Rong Pan, Hao
Hu, Ali C. Begen, Dave Oran} \IEEEauthorblockA{Cisco Systems, San
Jose, CA USA\\
 \{zhil2, xiaoqzhu, jgahm, ropan, hahu2, abegen, oran\}@cisco.com}}










\maketitle

\begin{abstract}
Today, the technology for video streaming over the Internet is converging
towards a paradigm named HTTP-based adaptive streaming (HAS), which
brings two new features. First, by using HTTP/TCP, it leverages network-friendly
TCP to achieve both firewall/NAT traversal and bandwidth sharing.
Second, by pre-encoding and storing the video in a number of discrete
rate levels, it introduces video bitrate adaptivity in a scalable
way so that the video encoding is excluded from the closed-loop adaptation.
A conventional wisdom is that the TCP throughput observed by an HAS
client indicates the available network bandwidth, and thus can be
used as a reliable reference for video bitrate selection.

We argue that this is no longer true when HAS becomes a substantial
fraction of the total traffic. We show that when multiple HAS clients
compete at a network bottleneck, the presence of competing clients
and the discrete nature of the video bitrates together result in difficulty
for a client to correctly perceive its fair-share bandwidth. Through
analysis and test bed experiments, we demonstrate that this fundamental
limitation leads to, for example, video bitrate oscillation that negatively
impacts the video viewing experience. We therefore argue that it is
necessary to design at the application layer using a ``probe-and-adapt''
principle for HAS video bitrate adaptation, which is akin to, but
also independent of the transport-layer TCP congestion control. We
present PANDA -- a client-side rate adaptation algorithm for HAS --
as practical embodiment of this principle. Our test bed results show
that compared to conventional algorithms, PANDA is able to reduce
the instability of video bitrate selection by over 75\% without increasing
the risk of buffer underrun.
\end{abstract}






\IEEEpeerreviewmaketitle


\section{Introduction}

Over the past few years, we have witnessed a major technology convergence
for Internet video streaming towards a new paradigm named HTTP-based
adaptive streaming (HAS). Since its inception in 2007 by Move Networks
\cite{move07}, HAS has been quickly adopted by major vendors and
service providers. Today, HAS is employed for over-the-top video delivery
by many major media content providers. A recent report by Cisco \cite{CiscoWhitePaper}
predicts that video will constitute more than 90\% of the total Internet
traffic by 2014. Therefore, HAS may become a predominant form of Internet
traffic in just a few years.

In contrast to conventional RTP/UDP-based video streaming, HAS uses
HTTP/TCP -- the protocol stack traditionally used for Web traffic.
In HAS, a video stream is chopped into short segments of a few seconds
each. Each segment is pre-encoded and stored at a server in a number
of versions, each with a distinct video bitrate, resolution and/or
quality. After obtaining a manifest file with necessary information,
a client downloads the segments sequentially using plain HTTP GETs,
estimates the network conditions, and selects the video bitrate of
the next segment on-the-fly. A conventional wisdom is that since the
bandwidth sharing of HAS is dictated by TCP, the problem of video
bitrate selection can be resolved straightforwardly. A simple rule
of thumb is to approximately match the video bitrate to the observed
TCP throughput. 



\begin{figure}
\begin{centering}
\hspace{-0.3in} \begin{minipage}[t]{1\columnwidth}\begin{center}
\hspace{0.2in}\footnotesize Fetched Bitrate Aggregated over 36 Streams\vspace{-0.23in}

\par\end{center}

\begin{center}
\includegraphics[scale=0.35]{testbed/36smooth_req_aggr2}
\par\end{center}\end{minipage}
\par\end{centering}

\begin{centering}
\vspace{0.05in}
\hspace{-0.3in} \begin{minipage}[t]{1\columnwidth}\begin{center}
\hspace{0.2in}\footnotesize Fetched Bitrate of Individual Streams
(Zoom In)\vspace{-0.23in}

\par\end{center}

\begin{center}
\includegraphics[scale=0.35]{testbed/36smooth_req2}
\par\end{center}\end{minipage}
\par\end{centering}

\begin{centering}
\vspace{0in}

\par\end{centering}

\centering{}\caption{Oscillation of video bitrate when 36 Microsoft Smooth clients compete
at a 100-Mbps link. For more detailed experimental setup, refer to
\ref{sub:Experimental-Setup}. }


\label{Flo:36smooth} \vspace{-0.15in}
\end{figure}


\vspace{-0.07in}


\subsection{Emerging Issues}

A major trend in HAS use cases is its large-scale deployment in managed
networks by service providers, which typically leads to aggregating
multiple HAS streams in the aggregation/core network. For example,
an important scenario is that within a single household or a neighborhood,
several HAS flows belonging to one DOCSIS\footnote{Data over cable service interface specification.} bonding group compete for bandwidth. In the unmanaged wide-area Internet,
as HAS is growing to become a substantial fraction of the total traffic,
it will also become more and more common to have multiple HAS streams
compete for available bandwidth at any network bottlenecks. 

While a simple rate adaptation algorithm might work fairly well for
the case where a single HAS stream operates alone or shares bandwidth
with non-HAS traffic, recent studies \cite{Jiang:CoNext12,Akhshabi:NOSSDAV12}
have reported undesirable behaviors when multiple HAS streams compete
for bandwidth at a bottleneck link. For example, while studies have
suggested that significant video quality variation over time is undesirable
for a viewer's quality of experience \cite{Mok:WMUST2011}, in \cite{Jiang:CoNext12}
the authors reported unstable video bitrate selection and unfair bandwidth
sharing among three Microsoft Smooth clients sharing a 3-Mbps link.
In our own test bed experiments (see Figure \ref{Flo:36smooth}),
we observed significant and regular video bitrate oscillation when
multiple Microsoft Smooth clients share a bottleneck link. We also
found that oscillation behavior persists under a wide range of parameter
settings, including the number of players, link bandwidth, start time
of clients, heterogeneous RTTs, random early detection (RED) queueing
parameters, the use of weight fair queueing (WFQ), the presence of
moderate web-like cross traffic, etc. 

Our study shows that these HAS rate oscillation and instability behaviors
are not incidental -- they are simply \emph{symptoms} of a much more
fundamental limitation of the conventional HAS rate adaptation algorithms,
in which \emph{the TCP downloading throughput observed by a client
is directly equated to its fair share of the network bandwidth}. This
fundamental problem would also impact a HAS client's ability to avoid
buffer underrun when the bandwidth suddenly drops. In brief, the problem
derives from the discrete nature of HAS video bitrates. This makes
it impossible to always match the video bitrate to the network bandwidth,
resulting in undersubscription of the network bandwidth. Undersubscription
is typically coupled with clients' on-off downloading patterns. The
off-intervals then become a source of ambiguity for a client to correctly
perceive its fair share of the network bandwidth, thus preventing
the client from making accurate rate adaptation decisions\footnote{In \cite{Akhshabi:NOSSDAV12}, Akhshabi et al. have reached similar
conclusions. But they identify the off-intervals instead of the TCP
throughput-based measurement as the root cause. Their sequel work
\cite{Akhshabi:NOSSDAV13} attempts to tackle the problem from a very
different angle using traffic shaping.}.

\vspace{-0.07in}


\subsection{Overview of Solution}

To overcome this fundamental limitation, we envision a solution based
on a ``probe-and-adapt'' principle. In this approach, the TCP downloading
throughput is taken as an input \emph{only when} it is an accurate
indicator of the fair-share bandwidth. This usually happens when the
network is oversubscribed (or congested) and the off-intervals are
absent. In the presence of off-intervals, the algorithm constantly
\emph{probes}\footnote{By probing, we mean small trial increment of data rate, instead of
sending auxiliary piggybacking traffic.} the network bandwidth by incrementing its sending rate, and prepares
to back off once it experiences congestion. This new mechanism shares
the same spirit with TCP's congestion control, but it operates independently
at the application layer and at a per-segment rather than a per-RTT
time scale. We present PANDA (Probe AND Adapt) -- a client-side rate
adaptation algorithm -- as a specific implementation of this principle.

\begin{figure}
\begin{centering}
\begin{minipage}[t]{1\columnwidth}\begin{center}
\includegraphics[scale=0.35]{pictures/delay_adaprobe2} 
\par\end{center}

\begin{center}
\vspace{-0.2in}
(a) PANDA\vspace{-0.01in}

\par\end{center}\end{minipage}
\par\end{centering}

\begin{centering}
\vspace{0.05in}
\begin{minipage}[t]{1\columnwidth}\begin{center}
\includegraphics[scale=0.35]{pictures/delay_baseline2} 
\par\end{center}

\begin{center}
\vspace{-0.05in}
(b) Conventional Bimodal 
\par\end{center}\end{minipage}
\par\end{centering}

\vspace{0.05in}
\caption{Illustration of PANDA's fine-granular request intervals vs. a conventional
algorithm's bimodal request intervals.}


\label{Flo:delay} \vspace{-0.05in}
\end{figure}


Probing constitutes fine-tuning the requested network data rate, with
continuous variation over a range. By nature, the available video
bitrates in HAS can only be discrete. A main challenge in our design
is to create a continuous decision space out of the discrete video
bitrate. To this end, we propose to \emph{fine-tune the intervals
between consecutive segment download requests} such that the \emph{average
data rate} sent over the network is a continuous variable (see Figure
\ref{Flo:delay} for an illustrative comparison with the conventional
scheme). Consequently, instead of directly tuning the video bitrate,
we probe the bandwidth based on the average data rate, which in turn
determines the selected video bitrate and the fine-granularity inter-request
time. 

There are various benefits associated with the probe-and-adapt approach.
First, it avoids the pitfall of inaccurate bandwidth estimation. Having
a robust bandwidth measurement to begin with gives the subsequent
operations improved discriminative power (for example, strong smoothing
of the bandwidth measurement is no longer required, leading to better
responsiveness). Second, with constant probing via incrementing the
rate, the network bandwidth can be more efficiently utilized. Third,
it ensures that the bandwidth sharing converges towards fair share
(i.e., the same or adjacent video bitrate) among competing clients.
Lastly, an innate feature of the probe-and-adapt approach is \emph{asymmetry
of rate shifting} -- PANDA is equipped with conservative rate level
upshift but more responsive downshift. Responsive downshift facilitates
fast recovery from sudden bandwidth drops, and thus can effectively
mitigate the danger of playout stalls caused by buffer underrun.

\begin{center}
\begin{table}[t]
\centering\scriptsize \begin{minipage}[t]{0.99\columnwidth}\begin{center}
{\small }\begin{tabular}{|c|l|}
\hline 
{\small Notation } & {\small Explanation}\tabularnewline
\hline 
{\small  } & {\small Probing additive increase bitrate }\tabularnewline
{\small  } & {\small Probing convergence rate}\tabularnewline
{\small  } & {\small Smoothing convergence rate}\tabularnewline
 & {\small Client buffer convergence rate}\tabularnewline
{\small } & {\small Quantization margin}\tabularnewline
 & {\small Multiplicative safety margin}\tabularnewline
{\small  } & {\small Video segment duration (in video time)}\tabularnewline
{\small  } & {\small Client buffer duration (in video time)}\tabularnewline
{\small ;  } & {\small Minimum/maximum client buffer duration}\tabularnewline
{\small  } & {\small Actual inter-request time}\tabularnewline
{\small  } & {\small Target inter-request time}\tabularnewline
{\small  } & {\small Segment download duration}\tabularnewline
{\small  } & {\small Actual average data rate}\tabularnewline
{\small  } & {\small Target average data rate (or bandwidth share)}\tabularnewline
{\small  } & {\small Smoothed version of }\tabularnewline
{\small  } & {\small TCP throughput measured, }\tabularnewline
{\small  } & {\small Set of video bitrates }\tabularnewline
{\small  } & {\small Video bitrate available from }\tabularnewline
 & {\small Rate smoothing function}\tabularnewline
 & {\small Video bitrate quantization function}\tabularnewline
\hline 
\end{tabular}
\par\end{center}\end{minipage}\caption{\label{tab:notation}Notations used in this paper}
\vspace{-0.15in}
\end{table}

\par\end{center}


\subsection{Paper Organization}

In the rest of the paper, after formalizing the problem (\ref{sec:Problem-Setup}),
we first introduce a method to characterize the conventional rate
adaptation algorithms (\ref{sec:Existing-Rate-Adaptation}),
based on which we analyze the root cause of its problems (\ref{sec:Analysis-of-Rule-of-Thumb}).
We then introduce our probe-and-adapt approach (\ref{sec:Proposed-Rate-Adaptation})
to directly address the root cause, and present the PANDA rate adaptation
algorithm as a concrete implementation of this idea. We provide comprehensive
performance evaluations (\ref{sec:Experimental-Results}). We
conclude the paper with final remarks and discussion of future work
(\ref{sec:Conclusions}).


\section{Problem Model\label{sec:Problem-Setup}}

In this section, we formalize the problem by first describing a representative
HAS server-client interaction process. We then outline a four-step
model for an HAS rate adaptation algorithm. This will allow us to
compare the proposed PANDA algorithm with its conventional counterpart.
Table \ref{tab:notation} lists the main notations used in this paper.


\subsection{Process of HAS Server-Client Interaction}

Consider that a video stream is chopped into segments of  seconds
each. Each segment has been pre-encoded at  video bitrates, all
stored at a server. Denote by 
the set of available video bitrates, with  for
.

For each client, the streaming process is divided into sequential
segment downloading steps . The process we consider here
generalizes the process used by conventional HAS clients by further
incorporating variable durations between consecutive segment requests.
Refer to Figure \ref{Flo:delay2}. At the beginning of each download
step , a rate adaptation algorithm: 
\begin{itemize}
\item Selects the video bitrate of the next segment to be downloaded, ;
\item Specifies how much time to give for the current download, until the
next download request (i.e., the inter-request time), .
\end{itemize}
The client then initiates an HTTP GET request to the server for the
segment of sequence number  and video bitrate , and the
downloading starts immediately. Let  be the\emph{ download
duration --} the time required to complete the download. Assuming
that no pipelining of downloading is involved, the next download step
starts after time 

where  is the \emph{actual inter-request time}. That is, if
the download duration  is shorter than the target delay
, the client waits time  (i.e.,
the off-interval) before starting the next downloading step (Scenario
A); otherwise, the client starts the next download step immediately
after the current download is completed (Scenario B).

\begin{figure}
\begin{centering}
\includegraphics[scale=0.38]{pictures/segment_downloading} 
\par\end{centering}

\vspace{-0.1in}
\caption{The HAS segment downloading process.}


\label{Flo:delay2} \vspace{-0.15in}
\end{figure}


Typically, a rate adaptation algorithm also measures its TCP throughput
 during the segment downloading, via:




The downloaded segments are stored in the client buffer. After playout
starts, the buffer is consumed by the video player at a natural rate
of one video second per real second on average. Let  be the
buffer duration (measured in video time) at the end of step .
Then the buffer dynamics can be characterized by:




\subsection{Four-Step Model\label{sub:Four-Step-Model}}

We present a four-step model for an HAS rate adaptation algorithm,
generic enough to encompass both the conventional algorithms (e.g.,
\cite{Liu:MMSys11,Tian:CoNext12,Zhou:VCIP12,Miller:PV12,Liu:SPIC12})
and the proposed PANDA algorithm. In this model, a rate adaptation
algorithm proceeds in the following four steps.
\begin{itemize}
\item \emph{Estimating}. The algorithm starts by estimating the network
bandwidth  that can legitimately be used.
\item \emph{Smoothing}.  is then noise-filtered to yield the
smoothed version , with the aim of removing outliers.
\item \emph{Quantizing}. The continuous  is then mapped to
the discrete video bitrate , possibly with the
help of side information such as client buffer size, etc.
\item \emph{Scheduling}. The algorithm selects the target interval until
the next download request, .
\end{itemize}

\section{Conventional Approach \label{sec:Existing-Rate-Adaptation}}

Using the four-step model above, in this section we introduce a scheme
to characterize a conventional rate adaptation algorithm, which will
serve as a benchmark.

To the best of our knowledge, almost all of today's commercial HAS
players\footnote{In this paper, the terms ``HAS player'' and ``HAS client'' are
used interchangeably.} implement the \emph{measuring} and \emph{scheduling} parts of the
rate adaptation algorithm in a similar way, though they may differ
in their implementation of the smoothing and quantizing parts of the
algorithm. Our claim is based on a number of experimental studies
of commercial HAS players \cite{akhashabi12SPIC,Huang:IMC12,Jiang:CoNext12}.
The scheme described in Algorithm \ref{alg:Baseline} characterizes
their essential ideas.

\begin{algorithm}
At the beginning of each downloading step : 
\begin{enumerate}
\item Estimate the bandwidth share  by equating it to the measured
TCP throughput: 


\item Smooth out  to produce filtered version 
by 


\item Quantize  to the discrete video bitrate 
by 


\item Schedule the next download request depending on the buffer fullness:


\end{enumerate}
\caption{Conventional\label{alg:Baseline}}
\vspace{-0.1in}
\end{algorithm}


\begin{figure*}
\begin{centering}
\begin{minipage}[t]{1\columnwidth}\begin{flushleft}
\includegraphics[scale=0.27]{pictures/bw_sharing1c} 
\par\end{flushleft}

\begin{flushleft}
\vspace{-0.1in}
\small\hspace{0.8in}(a) Perfectly Subscribed, Round-Robin\vspace{0.05in}

\par\end{flushleft}\end{minipage}\begin{minipage}[t]{1\columnwidth}\begin{flushleft}
\includegraphics[scale=0.27]{pictures/bw_sharing4c} 
\par\end{flushleft}

\begin{flushleft}
\vspace{-0.1in}
\small\hspace{1.2in}(d) Oversubscribed
\par\end{flushleft}\end{minipage} \begin{minipage}[t]{1\columnwidth}\begin{flushleft}
\includegraphics[scale=0.27]{pictures/bw_sharing2c} 
\par\end{flushleft}

\begin{flushleft}
\vspace{-0.1in}
\small\hspace{0.8in}(b) Perfectly Subscribed, Partially Overlapped
\vspace{0.05in}

\par\end{flushleft}\end{minipage}\begin{minipage}[t]{1\columnwidth}\begin{flushleft}
\includegraphics[scale=0.27]{pictures/bw_sharing5c} 
\par\end{flushleft}

\begin{flushleft}
\vspace{-0.1in}
\small\hspace{1.2in}(e) Undersubscribed \vspace{0.05in}

\par\end{flushleft}\end{minipage} \begin{minipage}[t]{1\columnwidth}\begin{flushleft}
\includegraphics[scale=0.27]{pictures/bw_sharing3c} 
\par\end{flushleft}

\begin{flushleft}
\vspace{-0.1in}
\small\hspace{0.8in}(c) Perfectly Subscribed, Fully Overlapped \vspace{0.05in}

\par\end{flushleft}\end{minipage} \begin{minipage}[t]{1\columnwidth}\begin{flushleft}
\includegraphics[scale=0.27]{pictures/bw_sharing6c} 
\par\end{flushleft}

\begin{flushleft}
\vspace{-0.1in}
\small\hspace{1.2in}(f) Single-Client
\par\end{flushleft}\end{minipage}
\par\end{centering}

\centering{}\caption{Illustration of various bandwidth sharing scenarios. In (a), (b) and
(c), the link is perfectly subscribed. In (d), the bandwidth sharing
starts with round-robin mode but then link becomes oversubscribed.
In (e), the bandwidth sharing starts with fully overlapped mode when
the link is oversubscribed. Starting from the second round, the link
becomes undersubscribed. In (f), a single client is downloading, and
the downloading on-off pattern exactly matches that of the blue segments
in (a).}


\label{Flo:bw_share} \vspace{-0.05in}
\end{figure*}


First, the algorithm equates the currently available bandwidth share
 to the past TCP throughput  observed
during the on-interval . As the bandwidth is inferred
reactively based on the previous downloads, we refer to this as \emph{reactive
bandwidth estimation}.

The algorithm then obtains a filtered version  using
a smoothing function  that takes as input the measurement
history , as described in (\ref{eq:baseline1}).
Various filtering methods are possible, such as sliding-window moving
average, exponential weighted moving average (EWMA) or harmonic mean
\cite{Jiang:CoNext12}.

The next step maps the continuous  to a discrete
video bitrate  using a quantization function
. In general,  can also incorporate side information,
including the past fetched bitrates  and the buffer
history . 

Lastly, the algorithm determines the target inter-request time .
In (\ref{eq:baseline3}),  is a mechanical function of
the buffer duration . If  is less than a pre-defined
maximum buffer ,  is set to , and by (\ref{eq:tn}),
the next segment downloading starts right after the current download
is finished; otherwise, the inter-request time is set to the video
segment duration , to stop the buffer from further growing.
This creates two distinct modes of segment downloading -- the \emph{buffer
growing} mode and the \emph{steady-state} mode, as shown in Figure
\ref{Flo:delay}(b). We refer to this as the \emph{bimodal download
scheduling}. 


\section{Analysis of the Conventional Approach\label{sec:Analysis-of-Rule-of-Thumb}}

In this section, we take a deep dive into the conventional rate adaptation
algorithms and study their limitations.


\subsection{Bandwidth Cliff Effect\label{sub:Bandwidth-Overestimation}}

As we have seen in the previous section, conventional rate adaptation
algorithms use reactive bandwidth estimation (\ref{eq:baseline2})
that equates the estimated bandwidth share to the TCP throughput observed
during the on-intervals. In the presence of competing HAS clients,
however, the TCP throughput does not always faithfully represent the
fair-share bandwidth. In this section, we present an intuitive analysis
of this phenomenon, by extending the one first presented in \cite{Akhshabi:NOSSDAV12}.\footnote{A main difference of our analysis compared to \cite{Akhshabi:NOSSDAV12}
is that we rigorously prove the convergence properties presented in
the bandwidth cliff effect.} A rigorous analysis of this phenomenon is presented in Appendix \ref{sec:Bandwidth-Cliff-Effect:}.

First, we illustrate with simple examples. Figure \ref{Flo:bw_share}
(a) - (e) show the various scenarios of how a link can be shared by
two HAS clients in steady-state mode. We consider three different
scenarios: perfect link subscription, link oversubscription and link
undersubscription. We assume ideal TCP behavior, i.e., perfectly equal
sharing of the available bandwidth when the transfers overlap.

\emph{Perfect Subscription}: In perfect link subscription, the total
amount of traffic requested by the two clients perfectly fills the
link. (a), (b) and (c) illustrate three different modes of bandwidth
sharing, depending on the starting time of downloads relative to each
other. Essentially, under perfect subscription, there are unlimited
number of bandwidth sharing modes.

\emph{Oversubscription}: In (d), the two clients start with round-robin
mode and perfect subscription. Starting from the second round of downloading,
the bandwidth is reduced and the link becomes oversubscribed, i.e.,
each client requests segments larger than its current fair-share portion
of the bandwidth. This will result in unfinished downloads at the
end of each downloading round. Then, the unfinished segment will start
overlapping with segments of the next round. This repeats and the
downloading will become more and more overlapped, until all the clients
enter the fully overlapped mode.

\emph{Undersubscription}: In (e), initially the bandwidth sharing
is in fully overlapped mode, and the link is oversubscribed. Starting
from the second round, the bandwidth increases and the link becomes
undersubscribed. Then the clients start filling up each other's off-intervals,
until a transmission gap emerges. The bandwidth sharing will eventually
converge to a mode which is determined by the download start times. 

In any case, the measured TCP throughput faithfully represents the
fair-share bandwidth \emph{only when} the bandwidth sharing is in
the fully overlapped mode; in all other cases the TCP throughput overestimates
the fair-share bandwidth. Thus, most of the time, the bandwidth estimate
is accurate when the link is oversubscribed. Bandwidth overestimation
occurs when the link is undersubscribed or perfectly subscribed. In
general, when the number of competing clients is , the bandwidth
overestimation ranges from one to  times the fair-share bandwidth.

Although the preceding simple examples assume idealized TCP behavior
which abstracts away the complexity of TCP congestion control dynamics,
it is easy to verify that similar behavior occurs with real TCP connections.
To see this, we conducted a simple test bed experiment as follows.
We implemented a ``thin client'' to mimic an HAS client in the steady-state
mode. Each thin client repeatedly downloads a segment every 2 seconds.
We run 100 instances of the thin client sharing a bottleneck link
of 100 Mbps, each with a starting time randomly selected from a uniform
distribution between 0 and 2 seconds. Figure \ref{Flo:bwcliff} plots
the measured average TCP throughput as a function of the link subscription
rate. We observe that when the link subscription is below 100\%, the
measured throughput is about 3x the fair-share bandwidth of \textasciitilde{}1
Mbps. When the link subscription is above 100\%, the measured throughput
successfully predicts the fair-share bandwidth quite accurately. We
refer to this sudden transition from overestimation to fairly accurate
estimation of the bandwidth share at 100\% subscription as the \emph{bandwidth
cliff} \emph{effect}.

We summarize our findings as follows: 
\begin{itemize}
\item Link oversubscription converges to fully overlapped bandwidth sharing
and accurate bandwidth estimation. 
\item Link undersubscription converges to a bandwidth sharing pattern determined
by the download start times and bandwidth overestimation.
\item In perfect link subscription, there exist unlimited bandwidth sharing
modes, leading to bandwidth overestimation.
\end{itemize}
\begin{figure}
\begin{centering}
\includegraphics[scale=0.5]{testbed/bwcliff}\vspace{-0.1in}

\par\end{centering}

\caption{Bandwidth cliff effect: measured TCP throughput vs. link subscription
rate for 100 thin clients sharing a 100-Mbps link. Each thin client
repeatedly downloads a segment every  seconds.}


\label{Flo:bwcliff} \vspace{-0.05in}
\end{figure}



\subsection{Video Bitrate Oscillation}

With an understanding of the bandwidth cliff effect, we are now in
a good position to explain the bitrate oscillation observed in Figure
\ref{Flo:36smooth}. 

Figure \ref{Flo:oscillation} illustrates this process. When the client
buffer reaches the maximum level \textbf{}, by (\ref{eq:baseline3}),
off-intervals start to emerge. The link becomes undersubscribed, leading
to bandwidth overestimation (a). This triggers the upshift of requested
video bitrate (b). As the available bandwidth cannot keep up with
the video bitrate, the buffer falls below \textbf{}. By
(\ref{eq:baseline3}), the client falls back to the buffer growing
mode and the off-intervals disappear, in which case the link again
becomes oversubscribed and the measured throughput starts to converge
to the fair-share bandwidth (c). Lastly, due to the quantization effect,
the requested video bitrate falls below the fair-share bandwidth (d),
and the client buffer starts growing again, completing one oscillation
cycle.

\begin{figure}
\begin{centering}
\vspace{0.1in}
\includegraphics[scale=0.33]{pictures/oscillation}\vspace{-0.1in}

\par\end{centering}

\caption{Illustration of vicious cycle of video bitrate oscillation. This plot
is obtained with 36 Smooth clients sharing a 100-Mbps link. For experimental
setup, refer to \ref{sub:Experimental-Setup}. }


\label{Flo:oscillation} \vspace{-0.05in}
\end{figure}



\subsection{Fundamental Limitation}



The bandwidth overestimation phenomenon reveals a more general and
fundamental limitation of the class of conventional reactive bandwidth
estimation approaches discussed so far. As video bitrates are chosen
solely based on measured TCP throughput from past segment downloads
during the on-intervals, such decisions completely ignore the network
conditions during the off-intervals. This leads to an \emph{ambiguity
of client knowledge} of available network bandwidth during the off-intervals,
which, in turn, hampers the adaptation process. 

To illustrate this point, consider two alternative scenarios as depicted
in Figures \ref{Flo:bw_share} (f) and (a). In (f), the client downloading
the blue (darker-shaded) video segments occupies the link alone; in
(a), it shares the same link with a competing client downloading the
green (lighter-shaded) video segments. Note that the on/off-intervals
for all the blue (darker-shaded) video segments follow exactly the
same pattern in both scenarios. Consequently, the client observes
exactly the same TCP throughput measurement over time. If the client
would obtain a complete picture of the network, it would know to upshift
its video bitrate in (f) but retain its current bitrate in (a). In
practice, however, an individual client cannot distinguish between
these two scenarios, hence, is bound to the same behavior in both. 

Note that as long as the off-intervals persist, such \emph{ambiguity
in client knowledge} is inherent to the bandwidth measurement step
in a network with competing streams. It cannot be resolved or remedied
by improved filtering, quantization, or scheduling steps performed
later in the client adaptation algorithm. Moreover, the bandwidth
cliff effect, as discussed in Section \ref{sub:Bandwidth-Overestimation},
suggests that the bandwidth overestimation problem does not improve
with more clients, and that it can introduce large errors even with
slight link undersubscription.

Instead, the client needs to take a more proactive approach in adapting
the video bitrate --- whenever it is known that the client knowledge
is impaired, it must \emph{avoid} using such knowledge in bandwidth
estimation. A way to distinguish the case when the knowledge is impaired
from when it is not, is to \emph{probe} the network subscription by
small increment of its data sending rate. We describe one algorithm
that follows such an alternative approach in the next section.




\section{Probe-and-Adapt Approach \label{sec:Proposed-Rate-Adaptation}}

In this section, we introduce our proposed probe-and-adapt approach
to directly address the root cause of the conventional algorithms'
problems. We begin the discussion by laying out the design goals that
a rate adaptation algorithm aims to achieve. We then describe the
PANDA algorithm as an embodiment of the probe-and-adapt approach,
and provide its functional verification using experimental traces.


\subsection{Design Goals\label{sec:Goals}}

Designing an HAS rate adaptation algorithm involves tradeoffs among
a number of competing goals. It is not legitimate to optimize one
goal (e.g., stability) without considering its tradeoff factors. From
an end-user's perspective, an HAS rate adaptation algorithm should
be designed to meet these criteria:
\begin{itemize}
\item \emph{Avoiding} \emph{buffer underrun}. Once the playout starts, buffer
underrun (i.e., complete depletion of buffer) leads to a playout stall.
Empirical study \cite{zhanghui2011} has shown that buffer underrun
may have the most severe impact on a user's viewing experience. To
avoid it, some minimal buffer level must be maintained at all times\footnote{Note that, however, the buffer level must also have an upper bound,
for a few different reasons. In live streaming, the end-to-end latency
from the real-time event to the event being displayed on user's screen
must be reasonably short. In video-on-demand, the maximum buffered
video must be limited to avoid wasted network usage in case of an
early termination of playback and to limit memory usage.}, and the adaptation algorithm must be highly responsive to network
bandwidth drops.
\item \emph{High quality smoothness}. In the simplest setting without considering
visual perceptual models, high video quality smoothness translates
into avoiding both frequent and significant video bitrate shifts among
available video bitrate levels \cite{Jiang:CoNext12,Mok:WMUST2011}. 
\item \emph{High average quality}. High average video quality dictates that
a client should fetch high-bitrate segments as much as possible. Given
a fixed network bandwidth, this translates into high network utilization.
\item \emph{Fairness}. In the simplest setting, fairness translates into
equal network bandwidth sharing among competing clients. 
\end{itemize}
Note that this list above is non-exhaustive. Other criteria, such
as low playout startup latency, are also important factors impacting
user's viewing experience.


\subsection{PANDA Algorithm}

In this section, we discuss the PANDA algorithm. Compared to the reactive
bandwidth estimation used by a conventional rate adaptation algorithm,
PANDA uses a more proactive probing mechanism. By probing, PANDA determines
a target average data rate . This average data rate is subsequently
used to determine the video bitrate  to be fetched, and the interval
 until the next segment download request.

The PANDA algorithm is described in Algorithm \ref{alg:panda}, and
a block diagram interpretation of the algorithm is shown in Figure
\ref{Flo:panda_diagram}. Compared to the conventional algorithm in
Algorithm \ref{alg:Baseline}, we only make modifications in the \emph{estimating}
and \emph{scheduling} steps -- we replace (\ref{eq:baseline2}) with
(\ref{eq:baseline2b}) for estimating the bandwidth share, and (\ref{eq:baseline3})
with (\ref{eq:baseline3b}) for scheduling the next download request.
We now focus on elaborating each of these two modifications.

\begin{algorithm}
At the beginning of each downloading step : 
\begin{enumerate}
\item Estimate the bandwidth share  by


\item Smooth out  to produce filtered version 
by 


\item Quantize  to the discrete video bitrate 
by 


\item Schedule the next download request via 


\end{enumerate}
\caption{PANDA\label{alg:panda}}
\vspace{-0.1in}
\end{algorithm}


\begin{figure*}
\begin{centering}
\includegraphics[scale=0.38]{pictures/panda_diagram}\vspace{-0.07in}

\par\end{centering}

\caption{Block diagram for PANDA (Algorithm \ref{alg:panda}). Module D represents
delay of one adaptation step.}


\label{Flo:panda_diagram} \vspace{-0.05in}
\end{figure*}


In the estimating step, (\ref{eq:baseline2b}) is designed to directly
address the root cause that leads to the video bitrate oscillation
phenomenon. Based on the insights obtained from \ref{sub:Bandwidth-Overestimation},
when the link becomes undersubscribed, the direct TCP throughput estimate
 becomes inaccurate in predicting the fair-share bandwidth,
and thus should be avoided. Instead, the client continuously increments
the target average data rate  by  per unit
time as a probe of the available capacity. Here  is the probing
convergence rate and  is the additive increase rate. The algorithm
keeps on monitoring the TCP throughput , and compares
it against the target average data rate . If ,
 would not be informative, since in this case the link
may still be undersubscribed and  may overestimate the
fair-share bandwidth. Thus, its impact is suppressed by the 
function. But if , then TCP throughput cannot
keep up with the target average data rate indicates that congestion
has occurred. This is when the target data rate  should
back off. The reduction imposed on  is made proportional
to . Intuitively, the lower the measured TCP throughput
, the more reduction that needs to be imposed on .
This design makes our rate adaptation algorithm very agile to bandwidth
changes.

PANDA's probing mechanism shares similarities with TCP's congestion
control \cite{Jacobson1988}, and has an additive-increase-multiplicative-decrease
(AIMD) interpretation:  is the additive increase term,
and  can be interpreted
as the multiplicative decrease term. The main difference is that in
TCP, congestion is indicated by packet losses (TCP Reno) or increased
round-trip time (delay-based TCP), whereas in (\ref{eq:baseline2b}),
congestion is indicated by the reduction of measured TCP throughput.
This AIMD property ensures that PANDA is able to efficiently utilize
the network bandwidth, and in the presence of multiple clients, the
bandwidth for each client eventually converges to fair-share status\footnote{Assuming the underlying TCP is fair (e.g., equal RTTs).}.

In the scheduling step, (\ref{eq:baseline3b}) aims to determine the
target inter-request time . By right,  should
be selected such that the smoothed target average data rate 
is equal to . But additionally,
the selection of  should also drive the buffer 
towards a minimum reference level , so the second term
is added to the right hand side of (\ref{eq:baseline3b}), where 
controls the convergence rate.

One distinctive feature of the PANDA algorithm is its hybrid closed-loop/open-loop
design. Refer to Figure \ref{Flo:panda_diagram}. In this system,
(\ref{eq:baseline2b}) forms a closed loop by itself that determines
the target average data rate . (\ref{eq:baseline3b}) forms
a closed loop by itself that determines the target inter-request time
. Overall, the estimating, smoothing, quantizing and scheduling
steps together form an open loop. The main motivation behind this
design is to reduce the bitrate shifts associated with quantization.
Since quantization is excluded from the closed loop of ,
it allows  to settle in a steady state. Since  is
a deterministic function of , it can also settle in a
steady state.

In Appendix \ref{sec:Analysis-of-PANDA}, we present an equilibrium
and stability analysis of PANDA. We summarize the main results as
follows. Our equilibrium analysis shows that at steady state, the
system variables settle at

where the subscript  denotes value of variables at equilibrium.
Our stability analysis shows that for the system to converge towards
the steady state, it is necessary to have:

where  is a parameter associated with the quantizer ,
referred to as the \emph{quantization margin}, i.e., the selected
discrete rate  must satisfy




\subsection{Functional Verification}

We verify the behavior of PANDA using experimental traces. For detailed
experiment setup (including the selection of function 
and ), refer to \ref{sub:Experimental-Setup}. 

First, we evaluate how a single PANDA client adjusts its video bitrate
as the the available bandwidth varies over time. In Figure \ref{Flo:plainPANDA},
we plot the TCP throughput , the target average data rate
, the fetched video bitrate  and the client buffer 
for a duration of 500 seconds, where the bandwidth drops from 5 to
2 Mbps at 200 seconds, and rises back to 5 Mbps at 300 seconds. Initially,
the target average data rate  ramps up gradually over time;
the fetched video bitrate  also ramps up correspondingly. After
the initial ramp-up stage,  settles in a steady state. It
can be observed that at steady state, the difference between 
and  is about 0.3 Mbps, equal to , which is consistent
with (\ref{eq:ss_xhat}). Similarly, the buffer  also settles
in a steady state, and after plugging in all the parameters, one can
verify that the steady state of buffer (\ref{eq:ss_B}) also holds.
At 200 seconds, when the bandwidth suddenly drops, the fetched video
bitrate quickly drops to the desirable level. With this quick response,
the buffer hardly drops. This property makes PANDA favorable for live
streaming applications. When the bandwidth rises back to 5 Mbps at
300 seconds, the fetched video bitrate gradually ramps up to the original
level.

Note that, in practical implementation, we can further add a startup
logic to improve PANDA's ramp-up speed at the stream startup stage,
akin to the slow-start mode of TCP. The idea is simple: since it is
necessary to add off-intervals \emph{only when} the buffer duration
 exceeds the minimum reference level , we can use
the conventional algorithm at startup or after playout stall, until
; after that, we switch to the main Algorithm
\ref{alg:panda}. Without the presence of the off-intervals, the conventional
algorithm works fast enough and reasonably well. Figure \ref{Flo:startup}
shows the startup behavior of a PANDA player with 5 Mbps link bandwidth,
with and without the startup logic. As can be seen, the startup logic
allows the video bitrate to ramp up efficiently, albeit at the expense
of somewhat dampened buffer growth.

\begin{figure}
\begin{centering}
\includegraphics[scale=0.36]{ns2/plainPanda_aa}\includegraphics[scale=0.36]{ns2/plainPanda_bb}
\vspace{-0.23in}

\par\end{centering}

\caption{A PANDA client adapts its video bitrate under a bandwidth-varying
link. The bandwidth is initially at 5 Mbps, drops to 2 Mbps at 200
seconds and rises back to 5 Mbps at 300 seconds. }


\label{Flo:plainPANDA} \vspace{-0.05in}
\end{figure}


\begin{figure}
\begin{centering}
\includegraphics[scale=0.36]{ns2/startup_bitrate}\includegraphics[scale=0.36]{ns2/startup_buffer}
\vspace{-0.23in}

\par\end{centering}

\caption{Comparison of the startup behavior of a PANDA player with and without
the startup logic. The bandwidth is 5 Mbps.}


\label{Flo:startup} \vspace{-0.05in}
\end{figure}
The more intriguing question is whether PANDA could effectively stop
the bitrate oscillation observed in the Smooth players. We conduct
an experiment with the same setup as the experiment shown in Figure
\ref{Flo:36smooth}, except that the PANDA player and the Smooth player
use slightly different video bitrate levels (due to different packaging
methods). The resulting fetched bitrates in aggregate and for each
client are shown in Figure \ref{Flo:36panda}. From the plot of the
aggregate fetched bitrate, except for the initial fluctuation, the
aggregate bitrate closely tracks the available bandwidth of 100 Mbps.
Zooming in to the individual streams' fetched bitrates, the fetched
bitrates are confined within two adjacent bitrate levels and the number
of shifts is much smaller than the Smooth client's case. This affirms
that PANDA is able to achieve better stability than the Smooth's rate
adaptation algorithm. In \ref{sec:Experimental-Results}, we
perform a comprehensive performance evaluation on each adaptation
algorithm.

\begin{figure}
\begin{centering}
\hspace{-0.3in} \begin{minipage}[t]{1\columnwidth}\begin{center}
\hspace{0.2in}\footnotesize Fetched Bitrate Aggregated over 36 Streams\vspace{-0.23in}

\par\end{center}

\begin{center}
\includegraphics[scale=0.35]{testbed/36panda_req_aggr2}
\par\end{center}\end{minipage}
\par\end{centering}

\begin{centering}
\vspace{0.05in}
\hspace{-0.3in} \begin{minipage}[t]{1\columnwidth}\begin{center}
\hspace{0.2in}\footnotesize Fetched Bitrate of Individual Streams
(Zoom In)\vspace{-0.23in}

\par\end{center}

\begin{center}
\includegraphics[scale=0.35]{testbed/36panda_req2}
\par\end{center}\end{minipage}
\par\end{centering}

\begin{centering}
\vspace{0in}

\par\end{centering}

\caption{36 PANDA clients compete at a 100-Mbps link in steady state. }


\label{Flo:36panda} \vspace{-0.05in}
\end{figure}


To help the reader develop a better intuition on why PANDA performs
better than a conventional algorithm, in Figure \ref{Flo:36panda_tput}
we plot the trace of the measured TCP throughput and the target average
data rate for the same experiment as in Figure \ref{Flo:36panda}.
Note that the fair-share bandwidth for each client is about 2.8 Mbps.
From the plot, the TCP throughput not only grossly overestimates the
fair-share bandwidth, it also has a large variation. If used directly,
this degree of noisiness gives the subsequent operations a very hard
job to extract useful information. For example, one may apply strong
filtering to smooth out the bandwidth measurement, but this would
seriously affect the responsiveness of the client. When the network
bandwidth suddenly drops, the client would not be able to respond
quickly enough to reduce its video bitrate, leading to catastrophic
buffer underrun. Moreover, the bias is both large and difficult to
predict, making any correction to the mean problematic. In comparison,
although also biased, the target average data rate estimated by the
probing mechanism is much less noisy than the TCP throughput. One
can easily correct the bias (via (\ref{eq:rydelta}) and quantization)
and select the right video bitrate without sacrificing responsiveness.

\begin{figure}
\begin{centering}
\hspace{-0.3in} \begin{minipage}[t]{1\columnwidth}\begin{center}
\hspace{0.2in}\footnotesize Measured TCP Throughput \vspace{-0.23in}

\par\end{center}

\begin{center}
\includegraphics[scale=0.35]{ns2/36panda_tput2}
\par\end{center}\end{minipage}
\par\end{centering}

\begin{centering}
\vspace{0.05in}
\hspace{-0.3in} \begin{minipage}[t]{1\columnwidth}\begin{center}
\hspace{0.2in}\footnotesize Target Average Data Rate \vspace{-0.23in}

\par\end{center}

\begin{center}
\includegraphics[scale=0.35]{ns2/36panda_avgdatarate2}
\par\end{center}\end{minipage}
\par\end{centering}

\centering{}\caption{The traces of the TCP throughput and the target average data rate
of 36 PANDA clients compete at a 100-Mbps link in steady state. The
traces of the first five clients are plotted.}


\label{Flo:36panda_tput} \vspace{-0.05in}
\end{figure}


In Figure \ref{Flo:panda_stability}, we verify the stability criteria
(\ref{eq:stability_k}) and (\ref{eq:stability_w}) of PANDA. With
, the system is stable if . This is demonstrated
by Figure \ref{Flo:panda_stability} (a), where we show the traces
of the target average rate  for two  values 0.9
and 1.1. In Figure \ref{Flo:panda_stability} (b), we show that when
, the buffer cannot converge towards the reference level
of 30 seconds.




\section{Performance Evaluation\label{sec:Experimental-Results}}

In this section, we conduct a set of test bed experiments to evaluate
the performance of PANDA against other rate adaptation algorithms.


\subsection{Evaluation Metrics}

In \ref{sec:Goals}, we discussed four criteria that are most
important for a user's watching experience -- i) ability to avoid
buffer underruns, ii) quality smoothness, iii) average quality, and
iv) fairness. In this paper, we use \emph{buffer undershoot} as the
metric for Criterion i), described as follows.
\begin{itemize}
\item \emph{Buffer undershoot}: We measure how much the buffer goes down
after a bandwidth drop as a indicator of an algorithm's ability to
avoid buffer underruns. The less the buffer undershoot, the less likely
the buffer will underrun. Let  be a reference buffer level
(30 seconds for all players in this paper), and  the buffer
level for player  at time . The buffer undershoot for player
 at time  is defined as .
The buffer undershoot for player  within a time interval 
(right after a bandwidth drop), is defined as the 90th-percentile
value of the distribution of buffer undershoot samples collected during
. 
\end{itemize}
We inherit the metrics defined in \cite{Jiang:CoNext12} -- \emph{instability},
\emph{inefficiency} and \emph{unfairness}, as the metrics for Criteria
ii), iii) and iv), respectively. We only make a slight modification
to the definition of inefficiency. Let  be the video bitrate
fetched by player  at time .
\begin{itemize}
\item \emph{Instability}: The instability for player  at time  is
,
where  is a weight function that puts more weight on more
recent samples.  is selected as 20 seconds.
\item \emph{Inefficiency}: Let  be the available bandwidth. \cite{Jiang:CoNext12}
defines inefficiency as 
for player  at time . But sometimes the sum of fetched bitrate
 can be greater than . To avoid unnecessary
penalty in this case, we revise the inefficiency metric to 
for player  at time .
\item \emph{Unfairness}: Let  be the Jain fairness index
calculated on the rates  at time  over all players.
The unfairness at  is defined as .
\end{itemize}
\begin{figure}
\begin{centering}
\begin{minipage}[t]{0.49\columnwidth}\begin{center}
\includegraphics[scale=0.36]{ns2/stability_k2}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(a) 
\par\end{center}

\begin{center}
\vspace{-0.15in}

\par\end{center}\end{minipage} \begin{minipage}[t]{0.49\columnwidth}\begin{center}
\includegraphics[scale=0.36]{ns2/stability_Ddown}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(b) 
\par\end{center}

\begin{center}
\vspace{-0.15in}

\par\end{center}\end{minipage}
\par\end{centering}

\vspace{-0.07in}
\caption{Experimental verification of PANDA's stability criteria. In (a), one
PANDA client streams over a link of 5 Mbps. In (b), 10 PANDA clients
compete over a 10 Mbps link.}


\label{Flo:panda_stability} \vspace{-0.05in}
\end{figure}



\subsection{Experimental Setup\label{sub:Experimental-Setup}}

\emph{HAS Player Configuration}: The benchmark players that we use
to compare against PANDA are:
\begin{itemize}
\item Microsoft Smooth player \cite{MSS}, a commercially available proprietary
player. The Smooth players are of version 1.0.837.34 using Silverlight
runtime version 4.0.50826. To our best knowledge, the Smooth player
as well as the Apple HLS and the Adobe HDS players all use the same
TCP throughput measurement mechanism, so we picked the Smooth player
as a representative.
\item FESTIVE player, which we implemented based on the details specified
in \cite{Jiang:CoNext12}. The FESTIVE algorithm is the first known
client-side rate adaptation algorithm designed to specifically address
the multi-client scenario.
\item A player implementing the conventional algorithm (Algorithm \ref{alg:Baseline}),
which differs from PANDA only in the estimating and scheduling steps.
\end{itemize}
For fairness, we ensure that PANDA and the conventional player use
the same smoothing and quantizing functions. For smoothing, we implemented
a \emph{EWMA} \emph{smoother} of the form: ,
where  is the convergence rate of  towards
. For quantization, we implemented a \emph{dead-zone
quantizer} , defined as follows: Let the
upshift threshold be defined as  subject to , and the downshift threshold
as  subject to , where 
and  are the upshift and downshift safety margin respectively,
with . The dead-zone quantizer
updates  as 

The ``dead zone''  created by setting 
mitigates frequent bitrate hopping between two adjacent levels, thus
stabilizing the video quality (i.e. hysteresis control). For the conventional
player, set  and ,
where  is the multiplicative safety margin. For
PANDA, due to (\ref{eq:stability_w}) and (\ref{eq:rydelta}), set
 and  \footnote{Note that this will not give PANDA any unfair advantage.}.

Table \ref{tab:parameters} lists the default parameters used by each
player, as well as their varying values. For fairness, all players
attempt to maintain a steady-state buffer of 30 seconds. For PANDA,
 is selected to be 26 seconds such that the resulting steady-state
buffer is 30 seconds (by (\ref{eq:ss_B})).

\emph{Server Configuration}: The HTTP server runs Apache on Red Hat
6.2 (kernel version 2.6.32-220). The Smooth player interacts with
an Microsoft IIS server by default, but we also perform experiments
of Smooth player interacting with an Apache server on Ubuntu 10.04
(kernel version 2.6.32.21).

\emph{Network} \emph{Configuration}: As service provider deployment
over a managed network is our primary case of interest, our experiments
are configured to highly match the imporant scenario where a number
of HAS flows compete for bandwidth within a DOCSIS bonding group.
The test bed is configured as in Figure \ref{Flo:testbed_network}.
The queueing policy used at the aggregation router-home router bottleneck
link is the following. For a link bandwidth of 10 Mbps or below, we
use random early detection (RED) with ;
if the link bandwidth is 100 Mbps, we use RED with .
The video content is chopped into segments of  seconds, pre-encoded
with  bitrates: 459, 693, 937, 1270, 1745, 2536, 3758, 5379,
7861 and 11321 Kbps. For the Smooth player, the data rates after packaging
are slightly different.

\begin{center}
\begin{table}[t]
\scriptsize\vspace{0.10in}\begin{minipage}[t]{0.99\columnwidth}\begin{center}
{\small }\begin{tabular}{|l|l|l|c|}
\hline 
Algorithm & Parameter & Default & Values\tabularnewline
\hline 
PANDA &  & 0.14 & 0.04,0.07,0.14,0.28,0.42,0.56\tabularnewline
 &  & 0.3 & \tabularnewline
 &  & 0.2 & 0.05,0.1,0.2,0.3,0.4,0.5\tabularnewline
 &  & 0.2 & \tabularnewline
 &  & 0.15 & 0.5,0.4,0.3,0.2,0.1,0\tabularnewline
 &  & 26 & \tabularnewline
\hline 
Conventional &  & 0.2 & 0.01,0.04,0.07,0.1,0.15,0.2\tabularnewline
 &  & 0.15 & \tabularnewline
 &  & 30 & \tabularnewline
\hline 
FESTIVE & Window & 20 & 20,15,10,6,3,1\tabularnewline
 &  & 30 & \tabularnewline
\hline 
\end{tabular}
\par\end{center}\end{minipage}\caption{\label{tab:parameters}Parameters used in experiments}
\vspace{-0.05in}
\end{table}

\par\end{center}

\begin{figure}
\begin{centering}
\includegraphics[scale=0.6]{pictures/sigcomm_setup3} \vspace{-0.05in}

\par\end{centering}

\centering{}\caption{The network topology configured in the test bed. Local indicates that
the bitrate is effectively unbounded and the link delay is 0 ms.}


\label{Flo:testbed_network} \vspace{-0.1in}
\end{figure}


\begin{figure*}
\begin{centering}
\begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/instability_undershoot}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(a)
\par\end{center}\end{minipage} \begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/instability_inefficiency}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(b) 
\par\end{center}\end{minipage} \begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/instability_unfairness}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(c) 
\par\end{center}\end{minipage} 
\par\end{centering}

\vspace{-0.01in}

\caption{The impact of varying instability on buffer undershoot, inefficiency
and unfairness for PANDA and other benchmark players. }


\label{Flo:tradeoff} \vspace{-0.05in}
\end{figure*}



\subsection{Performance Tradeoffs}

It would not be legitimate to discuss a single metric without minding
its impact on other metrics. In this section, we examine the performance
tradeoffs among the four metrics of interest. We designed an experimental
process under which we can measure all four metrics in a single run.
For each run, five players (of the same type) compete at a bottleneck
link. The link bandwidth stays at 10 Mbps from 0 seconds to 400 seconds,
drops to 2.5 Mbps at 400 seconds and stays there until 500 seconds.
We record the instability, inefficiency and unfairness averaged over
0 to 400 seconds over all players, and the buffer undershoot over
400 to 500 seconds averaged over all players. Figure \ref{Flo:tradeoff}
shows the tradeoff between stability and each of the other criteria
-- buffer undershoot, inefficiency and unfairness -- for each of the
types of player. Each data point is obtained via averaging over 10
runs, and each data point represents a different value for one of
the parameters of the corresponding algorithm, as indicated in the
Values column of Table \ref{tab:parameters}.

For the PANDA player, the three parameters that affect instability
the most are: the probing convergence rate , the smoothing
convergence rate  and the safety margin . Figure
\ref{Flo:tradeoff} (a) shows that as we vary these parameters, the
tradeoff curves mostly stay flat (except for at extreme values of
these parameters), implying that the PANDA player maintains good responsiveness
as the stability is improved. A few factors contribute to this advantage
of PANDA: First, as the bandwidth estimation by probing is quite accurate,
one does not need to apply strong smoothing. Second, since after a
bandwidth drop, the video bitrate reduction is made proportional to
the TCP throughput reduction, PANDA is very agile to bandwidth drops.
On the other hand, for both the FESTIVE and the conventional players,
the buffer undershoot significantly increases as the scheme becomes
more stable. Overall, PANDA has the best tradeoff between stability
and responsiveness to bandwidth drop, outperforming the second best
conventional player by more than 75\% reduction in instability at
the same buffer undershoot level. It is worth noting that the conventional
player uses exactly the same smoothing and quantization steps as PANDA,
which implies that the gain achieved by PANDA is purely due to the
improvement in the estimating and scheduling steps. FESTIVE has the
largest buffer undershoot. We believe this is because the design of
FESTIVE has mainly concentrated on stability, efficiency and fairness,
but ignored responsiveness to bandwidth drops. As we do not have access
to the Smooth player's buffer, we do not have its buffer undershoot
measure in Figure \ref{Flo:tradeoff} (a).

Figure \ref{Flo:tradeoff} (b) shows that PANDA has the lowest inefficiency
over all as we vary its instability. The probing mechanism ensures
that the bandwidth is most efficiently utilized. As the instability
increases, the inefficiency also increases moderately. This makes
sense intuitively, as when the bitrate fluctuates, the average fetched
bitrate also decreases. The efficiency of the conventional algorithm
underperforms PANDA, but outperforms FESTIVE. Lastly, the Smooth player
has the highest inefficiency.

Lastly, Figure \ref{Flo:tradeoff} (c) shows that in terms of fairness,
FESTIVE achieves the best performance. This may be due to the randomized
scheduling strategy of FESTIVE. PANDA and the conventional players
have similar fairness; both of them outperform the Smooth player. 


\subsection{Increasing Number of Players}

In this section, we focus on the question of how the number of players
affects instability, inefficiency and unfairness at steady state.
Two scenarios are of interest: i) we increase the number of players
while fixing the link bandwidth at 10 Mbps, and ii) we increase the
number of players while varying the bandwidth such that the bandwidth/player
ratio is fixed at 1 Mbps/player. Figure \ref{Flo:fixbw} and Figure
\ref{Flo:varybw} report results for these two cases, respectively.
Each data point is obtained by averaging over 10 runs.

Refer to Figure \ref{Flo:fixbw} (a). In the single-player case, all
four schemes are able to maintain their fetched video bitrate at a
constant level, resulting in zero instability. As the number of players
increases, the instability of the conventional player and the Smooth
player both increase quickly in a highly consistent way. We speculate
that they have very similar underlying structure. The FESTIVE player
is able to maintain its stability at a lower level, due to the strong
smoothing effect (smoothing window at 20 samples by default), but
the instability still grows with the number of players, likely due
to the bandwidth overestimation effect. The PANDA player exhibits
a rather different behavior: at two players it has the highest instability,
then the instability starts to drop as the number of players increases.
Investigating into the experimental traces reveals that this is related
to the specific bitrate levels selected. More importantly, via probing,
the PANDA player is immune to the symptoms of the bandwidth overestimation,
thus it is able to maintain its stability as the number of clients
increases. The case of varying bandwidth in Figure \ref{Flo:varybw}
(a) exhibits behavior fairly consistent with Figure \ref{Flo:fixbw}
(a), with PANDA and FESTIVE exhibiting much less instability compared
to the Smooth and the conventional players.

Figure \ref{Flo:fixbw} (b) and Figure \ref{Flo:varybw} (b) for the
inefficiency metric both show that PANDA consistently has the best
performance as the number of players grow. The conventional player
and FESTIVE have similar performance, both outperforming the Smooth
player by a great margin. We speculate that the Smooth player has
some large bitrate safety margin by design, with the purpose of giving
cross traffic more breathing room.

Lastly, let us look at fairness. Refer to Figure \ref{Flo:fixbw}
(a). We have found that when the overall bandwidth is fixed, the unfairness
measure has high dependence on the specific bitrate levels chosen,
especially when the number of players is small. For example, at two
players, when the fair-share bandwidth is 5 Mbps, the two PANDA players
end up in steady state with 5.3 Mbps and 3.7 Mbps, resulting in a
high unfairness score. At three players, when the fair-share bandwidth
is 3.3 Mbps, the three PANDA players each end up with 3.7, 3.7 and
2.5 Mbps for a long period of time, resulting in a lower unfairness
score. FESTIVE exhibits lowest unfairness overall, which is consistent
with the results obtained in Figure \ref{Flo:tradeoff} (c). In the
varying-bandwidth case in Figure \ref{Flo:varybw} (c), The unfairness
ranking is fairly consistent as the number of players grow: FESTIVE,
PANDA, the conventional, and Smooth.

\begin{figure*}
\begin{centering}
\begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/instability_fixbw}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(a) 
\par\end{center}\end{minipage} \begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/inefficiency_fixbw}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(b) 
\par\end{center}\end{minipage} \begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/unfairness_fixbw}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(c) 
\par\end{center}\end{minipage} 
\par\end{centering}

\vspace{-0.01in}

\caption{Instability, inefficiency and unfairness as the number of clients
increases. The link bandwidth is fixed at 10 Mbps.}


\label{Flo:fixbw} \vspace{-0.05in}
\end{figure*}


\begin{figure*}
\begin{centering}
\begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/instability_varybw}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(a) 
\par\end{center}\end{minipage} \begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/inefficiency_varybw}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(b) 
\par\end{center}\end{minipage} \begin{minipage}[t]{0.66\columnwidth}\begin{center}
\includegraphics[scale=0.35]{testbed/unfairness_varybw}
\par\end{center}

\begin{center}
\vspace{-0.1in}
(c) 
\par\end{center}\end{minipage} 
\par\end{centering}

\vspace{-0.01in}

\caption{Instability, inefficiency and unfairness as the number of clients
increases. The link bandwidth increases with the number of players,
with the bandwidth-player ratio fixed at 1 Mbps/player.}


\label{Flo:varybw} \vspace{-0.05in}
\end{figure*}



\subsection{Competing Mixed Players}

One important question to ask is how PANDA will behave in the presence
of different type of players? If it behaves too conservatively and
cannot grab enough bandwidth, then the deployment of PANDA will not
be successful. To answer this question, we take the four types of
players of interest and have them compete on a 10-Mbps link. For the
Smooth player, we test it with both a Microsoft IIS server running
on Windows 7, and an Apache HTTP server running on Ubuntu 10.04. A
single trace of the fetched bitrates for 500 seconds is shown in Figure
\ref{Flo:competing}. The plot shows that the Smooth player's ability
to grab the bandwidth highly depends on the server it streams from.
Using the IIS server, which runs on Windows 7 with an aggressive TCP,
it is able to fetch video bitrates over 3 Mbps. With the Apache HTTP
server, which uses Ubuntu 10.04's conservative TCP, the fetched bitrates
are about 1 Mbps. The conventional, PANDA and FESTIVE players all
run on the same TCP (Red Hat 6), so their differences are due to their
adaptation algorithms. Due to bandwidth overestimation, the conventional
player aggressively fetches high bitrates, but the fetched bitrates
fluctuate. Both PANDA and FESTIVE are able to maintain a stable fetched
bitrate at about the fair-share level of 2 Mbps.


\subsection{Summary of Performance Results}
\begin{itemize}
\item The PANDA player has the best stability-responsiveness tradeoff, outperforming
the second best conventional player by 75\% reduction in instability.
PANDA also has the best bandwidth utilization.
\item The FESTIVE player has been tuned to yield high stability, high efficiency
and good fairness. However, it underperforms other players in responsiveness
to bandwidth drops.
\item The conventional player yields good efficiency, but lacks in stability,
responsiveness to bandwidth drops and fairness.
\item The Smooth player underperforms in efficiency, stability and fairness.
When competing against other players, its ability to grab bandwidth
is a consequence of the aggressiveness of the underlying TCP stack.
\end{itemize}

\section{Related Work\label{sec:related} }

\emph{AIMD Principle}: The design of the probing mechanism in PANDA
shares similarity with Jacobson's AIMD principle for TCP congestion
control \cite{Jacobson1988}. Kelly's framework on network rate control
\cite{kelly98} provides a theoretical justification for the AIMD
principle, and proves its stability in the general network setup.

\emph{HAS Measurement Studies}: Various research efforts have focused
on understanding the behavior of several commercially deployed HAS
systems. One such example is \cite{akhashabi12SPIC}, where the authors
characterize and evaluate HTTP streaming players such as Microsoft
Smooth Streaming, Netflix, and Adobe OSMF via experiments in controlled
settings. The first measurement study to consider HAS streaming in
the multi-client scenarios is \cite{Akhshabi:NOSSDAV12}. The authors
identify the root cause of the player's rate oscillation problem as
the existence of on-off patterns in HAS traffic. In \cite{Huang:IMC12},
the authors measure behavior of commercial video streaming services,
i.e., Hulu, Netflix, and Vudu, when competing with other long-lived
TCP flows. The results revealed that inaccurate estimations can trigger
a feedback loop leading to undesirably low-quality video. 

\emph{Existing HAS Designs}: To improve the performance of adaptive
HTTP streaming, several rate adaptation algorithms \cite{Liu:MMSys11,Tian:CoNext12,Zhou:VCIP12,Miller:PV12,Liu:SPIC12}
have been proposed, which, in general, fit into the four-step model
discussed in Section \ref{sub:Four-Step-Model}. In \cite{Jarnikov:SPIC11},
a sophisticated Markov Decision Process (MDP) is employed to compute
a set of optimal client strategies in order to maximize viewing quality.
The MDP requires the knowledge of network conditions and video content
statistics, which may not be readily available. Control-theoretical
approaches, including use of a PID controller, are also considered
by several works \cite{DeCicco:MMSys11,Tian:CoNext12,Zhou:VCIP12}.
A PID controller with appropriate parameter choice can improve streaming
performance. Server-side mechanisms are also advocated by some works
\cite{Houdaille:MMsys12,Akhshabi:NOSSDAV13}. Two designs have been
considered to address the multi-client issues: in \cite{Akhshabi:NOSSDAV13},
a rate-shaping approach aiming to eliminate the off-intervals, and
in \cite{Jiang:CoNext12}, a client rate adaptation algorithm design
implementing a combination of randomization, stateful rate selection
and harmonic mean based averaging.




\section{Conclusions\label{sec:Conclusions}}

This paper identifies an emerging issue for HTTP adaptive streaming,
which is expected to become the predominant form of the Internet traffic,
and lays out a solution direction that can effectively address this
issue. Our main contributions in this paper can be summarized as follows:
\begin{itemize}
\item We have identified the bandwidth cliff effect as the root cause of
the bitrate oscillation phenomenon and revealed the fundamental limitation
of the conventional reactive measurement based rate adaptation algorithms.
\item We have envisioned a general probe-and-adapt principle to directly
address the root cause of the problems, and designed and implemented
PANDA, a client-based rate adaptation algorithm, as an embodiment
of this principle.
\item We have proposed a generic four-step model for an HAS rate adaptation
algorithm, based on which we have fairly compared the proposed approach
with the conventional approach.
\end{itemize}
The probe-and-adapt approach and our PANDA realization thereof achieve
significant improvements in stability of HAS systems at no cost in
responsiveness. Given this framework, we plan to explore further improvements
in our future work.

\begin{figure}
\begin{centering}
\hspace{-0.40in}\begin{minipage}[t]{1\columnwidth}\begin{center}
\includegraphics[scale=0.36]{testbed/competing2}
\par\end{center}\end{minipage} 
\par\end{centering}

\vspace{-0.01in}\caption{PANDA, Smooth (w/ IIS), Smooth (w/ Apache), FESTIVE and the conventional
players compete at a bottleneck link of 10 Mbps.}


\label{Flo:competing} \vspace{-0.05in}
\end{figure}


{\scriptsize

\bibliographystyle{plain}
\bibliography{leeoz,ctech2012}


}


\appendices{}


\section{Bandwidth Cliff Effect: Theoretical Analysis\label{sec:Bandwidth-Cliff-Effect:}}


\subsection{Problem Formulation\label{sec:Problem-Formulation}}

Consider that  clients share a bottleneck link of capacity .
The streaming process of each client consists of discrete downloading
steps , where during each step one video segment is downloaded. 

\vspace{0.05in}

\emph{Fixed requested video bitrate.} As we are interested in how
the measured TCP throughput causes HAS clients to shift their video
bitrates requested, we analyze the stage of the dynamics where the
rate shift \emph{has not occurred} \emph{yet}. Thus, in our model,
we assume that each HAS client does not change its requested video
bitrate over the time interval of analysis. For , each
-th client requests a video segment of fixed size 
at each downloading step, where  is the video
bitrate selected.

\vspace{0.05in}

\emph{Segment requesting time and downloading duration. }Denote by
 the instantaneous data downloading rate of the -th
client at time  (note that  and  are different).
Denote by  the time that the -th client requests its
-th segment (which, for simplicity, is also assumed to be the
time that the downloading starts). For each client, assume that the
requesting time of the first segment  is given. For ,
the requesting time of the next segment is determined by:

where  is the actual duration of the -th client
downloading its -th segment. This is a reasonable assumption where
the buffer level of a HAS client has reached the maximum level. The
actual duration of downloading, , can be related
to  by

The \emph{TCP throughput} measured by the -th client for downloading
the -th segment, is defined as .
In a conventional HAS algorithm, the TCP throughput is used as an
estimator of a client's fair-share bandwidth, which, ideally, is equal
to .

\vspace{0.05in}

\emph{Idealized TCP behavior.} We assume that the network obeys a
simplified bandwidth sharing model, where we do not consider the effects
such as TCP slow-start restart and heterogenous RTTs.\footnote{It is trivial to extend the analysis to the case of heterogenous RTTs.} At any moment  when there are  active TCP flows
sharing the bottleneck link, each active flow will receive a fair-share
data rate of  instantaneously, and the total
traffic rate in the link is ; at any moment when there is no active
TCP flows, or , the total traffic rate in the link is .
For this case, we have the following definition:
\begin{defn}
A \emph{gap} is an interval within which the total traffic rate of
all clients is .
\end{defn}
\vspace{0.05in}

\emph{Client initial state. }We assume that each client may have some
arbitrary initial state, including:
\begin{itemize}
\item Time of requesting the first segment , , as
forementioned.
\item Initial data downloading rate, i.e., it may be that 
for , , where the rate may be due to requesting
a segment earlier than the first segment being considered. In practice,
this may correspond to the cases where the clients have already started
downloading but may be in a different state, before the first segment
being considered (e.g., link is oversubscribed before it becomes undersubscribed).
\end{itemize}

\subsection{Link Undersubscription}

The link is \emph{undersubscribed} by the  HAS clients if the
sum of the requested video bitrates is less than the link capacity,
i.e., . We would like to show that even the
\emph{slightest} undersubscription of the link would lead to convergence
into a state where each client has a TCP throughput greater than its
fair-share bandwidth .

To begin with, we prove a set of lemmas. We first show that any two
adjacent requesting times  and  are spaced
by exactly  if there exists a gap between them.
\begin{lem}
\label{lem:offinterval1} if there exists
a gap  with  and . \end{lem}
\begin{IEEEproof}
By (\ref{eq:start_time1}), the only case that 
is when . But this cannot hold, since otherwise
there cannot be a gap  with 
and .
\end{IEEEproof}
The rest of the lemmas in this section make the assumption of .
First, we show that at least one gap must emerge after some time.
\begin{lem}
\label{lem:undersubscribed1}There exists a gap ,
where .\end{lem}
\begin{IEEEproof}
By (\ref{eq:start_time1}), within  for any , each
client can download at most one segment, or data of size at most .
Consider within an interval 
for some . The maximum size of the data that can
be downloaded by the  clients is ,
where  is the total size of the residue data from segments
requested prior to , including those prior to the
first segments being considered, as discussed in Section \ref{sec:Problem-Formulation},
\emph{client initial state}. By the idealized TCP behavior, at any
instant, the total traffic rate can be either  or . If zero
total traffic rate does not happen, the total downloaded data size
within the interval being considerd is . Therefore, a sufficient
condition for a gap  to occur is to have ,
where , or .
\end{IEEEproof}
The next lemma shows that within an interval of duration  immediately
following this gap, each client must request one and only one segment.
\begin{lem}
Within the interval , each client must request
one and only one segment.\end{lem}
\begin{IEEEproof}
First, we show that each client must request at least one segment.
Invoking Lemma \ref{lem:offinterval1} and the fact that 
is a gap, the request times immediately preceding and following 
must be spaced exactly by . This can never hold if no segment
is requested within . 

Second, we show that each client can request at most one segment within
. This directly follows applying (\ref{eq:start_time1})
to any interval of duration , similar to the proof of Lemma
\ref{lem:undersubscribed1}. 
\end{IEEEproof}
Since exactly one segment is requested by each client within ,
for convenience, let us re-label these segments using a new index
.
\begin{lem}
\label{lem:offinterval12} is a gap.\end{lem}
\begin{IEEEproof}
First, invoking Lemma \ref{lem:offinterval1} and the fact that 
is a gap, we have  for .
In other words, the starting time patterns within intervals 
and  exactly repeat.

Second, consider the data traffic within  and
. The only difference is that within ,
there might be unfinished residue data from the previous segments
whereas within , there is no such unfinished
residue data due to the gap . By (\ref{eq:duration1}),
the exact starting times and the idealized TCP behavior, the downloading
completion time can only get delayed with the extra residue data,
thus we must have ,
. Therefore, the data traffic within 
must finish no later than , and 
must also be a gap.
\end{IEEEproof}
The following theorem shows that in the case of link undersubscription,
regardless of the client initial states, the banwidth sharing among
the clients will eventually converge to a periodic pattern.
\begin{thm}
\label{thm:undersubscribed1}If , the data
downloading rate  of each client will converge to a periodic
pattern with period , i.e., there exists some  such
that for all , , .\end{thm}
\begin{IEEEproof}
The interval  has no residue data from the previous
unfinished segments, because of the gap . Using this
fact and the idealized TCP behavior, the data downloading rates ,
 for  is a deterministic function
of the starting times , . The same argument
applies to ,  for 
and , . 

By Lemma \ref{lem:offinterval1} and Lemma \ref{lem:offinterval12},
we have  for . In other words,
other than the offset , the starting time patterns within 
and  are the same. Invoking the deterministic
function argument, we can show , 
for . Taking  and repeatly applying
the argument above to the intervals ,
 complete the proof.
\end{IEEEproof}
Note that by the idealized TCP behavior, no client's TCP throughput
would overestimate its fair-share bandwdith  \emph{only
if} the number of active flows  during all the active time.
After the bandwith sharing pattern converges to periodic, this happens
only when the starting times of all clients are all aligned and their
segment data size are equal. The following corollary is an immediate
consequence of Theorem \ref{thm:undersubscribed1}:
\begin{cor}
If , and  for some
, then for some clients the TCP throughput will converge
to a value that is greater than its fair-share bandwdith, i.e., there
exists  with  for .
In particular, if , then for all ,
 for .
\end{cor}
Note that the start times pattern ,  will
dictate exactly by how much the TCP throughput overestimates the fair-share
bandwidth, with the range .


\subsection{Link Oversubscription}

We next consider the case that the link is \emph{oversubscribed} by
the  HAS clients, i.e., . We first show
a sufficient condition under which the TCP throughput would correctly
predict the fair-share bandwidth.
\begin{thm}
\label{thm:oversubscribed2}If  for all ,
all clients' TCP throughput converges to the fair-share bandwidth,
i.e., there exists  such that ,
 for .\end{thm}
\begin{IEEEproof}
We first want to show that there exists  such that 
for all  for . Assume that the opposite is
true, i.e., there exists at least a client  such that, 
for all . Since , this would hold
only if within the active intervals of client , at least another
client  must be inactive. Denote by 
the first such inactive interval. Consider within an interval 
for some . By the idealistic TCP behavior, the total
number of bits that can be downloaded by the  clients (other
than ) must be . This contradicts
the fact that , for , .
Thus, the assumption is invalid.

Then, by (\ref{eq:start_time1}), for , all clients are
active all the time. By the idealistic TCP behavior, we have ,
 for .
\end{IEEEproof}
By slightly extending the above argument, a sufficient and necessary
condition for the correct fair-share bandwidth estimation of \emph{all
clients} can be found.
\begin{thm}
\label{thm:oversubscribed3}If , all clients'
TCP throughput converges to the fair-share bandwidth, if and only
if  for all  and there exists 
such that .
\end{thm}

\section{Analysis of PANDA\label{sec:Analysis-of-PANDA}}

We perform an equilibrium and stability analysis of PANDA. For simplicity,
we only analyze the single-client case where the system has an equilibrium
point.


\subsection{Analysis of }

Assume the measured TCP download throughput  is equal
to the link capacity . Consequently, \eqref{eq:baseline2b} can
be re-written in two scenarios (refer to Figure \ref{Flo:delay2})
as:



At equilibrium, setting \eqref{eqn:newx} to zero leads to ,
hence, .

Considering the simplifying assumptions of  (no
smoothing) and a quantizer with a margin  such that ,
we need  at equilibrium.
Therefore, the quantization margin of the quantizer needs to satisfy

so that the system stays on the multiplicative-decrease side of the
equation at steady state.

Note that close to equilibrium, the intervals between consecutive
segment downloads match the segment playout duration ,
therefore, calculation of the estimated bandwidth  follows
the simple form of a difference equation:

The two constants are:  and .
The sequence converges if and only if . Hence, we need: ,
leading to the stability criterion: 




\subsection{Analysis of  and }

Assume no smoothing . During transient states, when
, update of  pushes the playout buffer
size in the right direction: 
leads to a steady growth in buffer size when . At steady
state, . It can be derived
from \eqref{eq:baseline3b} that: 

where  is playout buffer at equilibrium.
\end{document}
