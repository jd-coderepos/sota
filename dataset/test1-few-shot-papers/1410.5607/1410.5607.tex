\documentclass[11pt,amssymb]{article}



\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{fullpage}

\newcommand{\comment}[1]{}

\def\il{i_{\sf left}}
\def\ir{i_{\sf right}}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{observation}{Observation }
\newtheorem{fact}{Fact}
\newtheorem{ire}{Implementation Remark}
\def\squareforqed{\hbox{\rlap{}}}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}
\newcommand{\proof}{\noindent\bf Proof: \rm}
\newcommand{\proofsketch}{\noindent\bf Proof Sketch: \rm}
\newcommand{\remark}{\noindent\bf Remark: \rm}
\newcommand{\qed}{\vrule height6pt width4pt\medskip}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\xor}{\oplus}
\newcommand{\QED}{\hfill\qed}
\def\squareforqed{\hbox{\rlap{}}}
\def\qed{\ifmmode\squareforqed\else{\unskip\nobreak\hfil
\def\mod{\ {\rm mod}\ }
\penalty50\hskip1em\null\nobreak\hfil\squareforqed
\parfillskip=0pt\finalhyphendemerits=0\endgraf}\fi}



\newcommand{\Z}{{\Bbb Z}}
\newcommand{\C}{{\Bbb C}}
\newcommand{\R}{{\Bbb R}}
\newcommand{\Q}{{\Bbb Q}}
\newcommand{\F}{{\Bbb F}}
\newcommand{\N}{{\mathbb N}}


\begin{document}
\title{Polynomials: a New Tool for Length Reduction in Binary Discrete
Convolutions}

\author{
\begin{tabular}{cccc}
Amihood Amir\thanks{ Department of Computer Science, Bar-Ilan
University, Ramat-Gan 52900, Israel, +972 3 531-8770; {\tt
amir@cs.biu.ac.il}; and Department of Computer Science, Johns Hopkins
University, Baltimore, MD 21218. Partly supported by NSF grant
CCR-09-04581 and ISF grant 347/09, and BSF grant 2008217.}
&
Oren Kapah\thanks{Department of Computer Science,
Bar-Ilan U., 52900 Ramat-Gan, Israel, (972-3)531-8408, {\tt
kapaho@cs.biu.ac.il}.}
&
Ely Porat\thanks{ Department of Computer Science, Bar-Ilan
University, 52900 Ramat-Gan, Israel, (972-3)531-7620; {\tt
porately@cs.biu.ac.il}.}
&
Amir Rothschild\thanks{ Department of
Computer Science, Bar-Ilan University, 52900 Ramat-Gan, Israel,
(972-3)531-7620; {\tt amirrot@gmail.com}.}
\\
{\small Bar-Ilan University} & {\small Bar-Ilan University}
& {\small   Bar-Ilan University} & {\small Bar-Ilan University}\\
{\small and} \\
{\small Johns Hopkins University} \\
\end{tabular}
}


\date{}

\maketitle
\begin{abstract}
Efficient handling of sparse data is a key challenge in Computer Science.
Binary convolutions, such as polynomial multiplication or the Walsh
Transform are a useful tool in many applications and are efficiently solved.

In the last decade, several problems required efficient solution of
sparse binary convolutions. Both randomized and deterministic
algorithms were developed for efficiently computing the sparse
polynomial multiplication.
The key operation in all these algorithms was length reduction. The
sparse data is mapped into small vectors that preserve the convolution
result. The reduction method used to-date was the modulo function
since it preserves location (of the "1" bits) up to cyclic shift.

To date there is no known efficient algorithm for computing the sparse
Walsh Transform. Since the modulo function does not preserve the Walsh
transform a new method for length reduction is needed.
In this paper we present such a new method -â€“ polynomials.
This method enables the development of an efficient algorithm for
computing the binary sparse Walsh Transform. To our knowledge, this is
the first such algorithm. We also show that this method allows a
faster deterministic computation of the sparse polynomial
multiplication than currently known in the literature.  

\end{abstract}


\section{Introduction\label{sec:intro}}

Handling sparse data is one of the grails of Computer Science, and is
a challenge in many of its sub-fields, from Machine Learning and Data
Bases to Combinatorics and Statistics. Even in one dimension sparse
data needs to be handled, and it is a tool for solving a number of
problems. An example is the {\em point-set matching problem}, where
two sets of points  consisting of  points,
respectively, are given. The goal is to determine if there is a rigid
transformation under which all the points in  are covered with
points in . Among the important application domains to which this
problem contributes are model based object
recognition~\cite{rucklidge:96,cyc:02}, pharmacophore
identification~\cite{kavraki:02}, searching in music archives,
~\cite{ulm:03}, and more.

The point-set matching problem has been studied in the literature in
many variation, not the least of which in the algorithms literature
(see e.g.~\cite{sc:98Sch,imv:99,CH:02,akof:03,acg:00,ahsw:03,ds:11}).
The template of many of these algorithms is as follows: First do a
{\it Dimension Reduction} where the inputs  are linearized into
raw vectors  of size polynomial in the number of non-zero values.
reduced the -Dimensional point set matching problem into a
-Dimensional point set matching problem, and then solve the one
dimensional problem. This problem is also known as the {\em Sparse
Convolution Problem} 
We define it formally: 

\begin{definition}
Let  and  be binary functions. We may view  and
 as vectors of bits, whose lengths are  and , respectively.

{\sl INPUT:} Binary vectors  and  of length  and ,
respectively.\\
{\sl OUTPUT:} All indices  where, for all ,
either  or  and . We call such an index
  a {\em match}.
\end{definition}

{\bf Intuition:} If we consider a  as depicting a point on the line
and a  as having no point there, then the meaning of the problem is
to find all locations where every point of  matches some point of .

\begin{example}
, and
. There is a match in location  because the situation
is:\\
\hskip 1in \\
\hskip 1in \\
There are matches also at locations  and .
\end{example}

The above problem can be trivially solved in time . It can be
solved in time , in a computational model with word size
, using the Fast Fourier Transform (FFT)~\cite{CLR-92}.

Polynomial multiplication is a special case of a general convolution. 
A general convolution uses two initial functions,  and , to
produce a third function . We consider a subclass of discrete
convolutions which we call {\em discrete dot-product convolutions}.

\begin{definition}\label{d:dot}
Let  be vectors in
. The {\em dot product} of  and , denoted as
, is: 

\end{definition}

A discrete dot product convolution, operates  bijections on the
indices of  and computes the dot product. Formally:

\begin{definition}\label{d:ddpt}
Let . Let  be bijections from  to
.
Then the {\em discrete dot-product B convolution of  and },
denoted as , is the vector of length  defined as:
 
\end{definition}

The polynomial multiplication we have seen above, as well as the
Discrete Walsh Transform (used e.g. in~\cite{aalp07} for computing
matching with flipped bits), are special cases of discrete dot-product
convolutions. Formally:

\begin{definition}\label{d:2dts}
\begin{enumerate}
\item The {\em Discrete Walsh Transform (DWT)} of  is the {\em discrete dot-product B convolution of 
and } where   and
 is defined as:
.  and  are the binary number
representations of  and  (i.e. binary strings of length )
and  is the {\em exclusive or} operation.
\item The {\em polynomial multiplication} of  is the {\em discrete dot-product B
convolution of  and } where   and
 is defined as:
. In this definition the dot product is of vectors
  in , i.e.   
\end{enumerate}
\end{definition}

A special feature both these convolutions have is that they can be
solved in time  via a divide-and-conquer approach. The
algorithm for the polynomial multiplication uses the FFT and the
algorithm for the Discrete Walsh Transform is the {\em Fast Walsh
Transform (FWT)}. Of course, similar to the DFT case, one may also 
consider a binary version of the DWT. In both
these cases, the  divide-and-conquer algorithm is a
wonderful solution if the input vectors have many points. Suppose,
however, that our arrays are {\em sparse}. In the sparse case, many
values of  and  are . Thus, they do not contribute to the
convolution value. 

{\bf Convention:} Throughout this paper, a capital letter (e.g. )
is used to denote the size of the vector, which is equivalent to the
largest index of a non-zero value, and a small letter (e.g. ) is
used  to denote the number of non-zero values. It is assumed that the
vectors are not given explicitly, rather they are given as a set of
 pairs, for all the non-zero values.

In our convention, the number of non-zero values of  is . Clearly, we can compute the convolution either in time  or in time . The challenge was (see e.g.~\cite{muthu-open})
whether the convolution can be computed in time .

To our knowledge, the only efficient algorithms for computing sparse
discrete binary convolutions, are for the FFT. The state-of-the art in
computing such sparse convolutions is to use a locality preserving
function to {\em reduce the length} of the sparse vectors, and then
to use the fast convolution algorithm. In Section~\ref{s:pre} we 
summarize the details of current length reduction methods. The
locality preserving function is the {\sl modulo} function.

{\bf Paper's Contribution:} The main contribution of this paper is
introducing a {\em novel tool for length reduction} -- {\sl
  polynomials}.
We show a number of advantages to using polynomials:
\begin{enumerate}
\item The new polynomials technique leads to an elegant algorithm for
  the sparse DWT. To our knowledge, no such algorithm is known to-date.
\item Our technique can be used without preprocessing to achieve a
  Las-Vegas algorithm for the sparse FFT which runs in time . This matches the expected running time of the Las-Vegas
  algorithm presented by Cole and Hariharan in~\cite{CH:02}. However,
  our algorithm has the added advantage of guaranteeing a worst case
  time of  whereas the algorithm in~\cite{CH:02} has no
  bound on its worst case running time.
\item Allowing  preprocessing time on the text, our technique
  can then {\em deterministically} solve the sparse FFT for incoming
  patterns in time . This improves the best known
  deterministic solution to the problem (\cite{LR07}), whose time was
  , with the same preprocessing time.
\end{enumerate}

 \section{Summary of Length Reduction Methods}\label{s:pre}

Recently~\cite{hikpa:12,hikpb:12}, Hassanjeh et
al. considered algorithms for approximating the sparse Fourier
transform. We are interested in exactly computing sparse convolutions 
(the Walsh transform, as well as the fast Fourier transform-based
polynomial multiplication). In this section we summarize Cole and
Hariharan's {\em length reduction} method, which exactly computes
sparse polynomial multiplication. in order to pinpoint the role of the
locality preserving function. 

Suppose we can map all the non-zero values into a smaller vector, say
of size . Suppose also that this mapping is alignment
preserving in the sense that applying the same transformation on 
will guarantee that the alignments are preserved. Then we can simply
map the the vectors  and  into the smaller vectors and then
use FFT for the convolutions on the smaller vectors, achieving time
. We then map the results back to the original vectors.

The problem is that to-date there is no known mapping with that
alignment preserving property. Cole and Hariharan~\cite{CH:02}
suggested using the modulo function as the alignment preserving
mapping, in a randomized setting that answers the problem with high
probability. The reason their algorithm is not deterministic is the
following:
In their length reduction phase, several indices of
non-zero values in the original vector may be mapped into the same
index in the reduced size vector. If the index of only one non-zero
value is mapped into an index in the reduced size vector, then this
index is denoted as {\it singleton} and the non-zero value is said
to appear as a {\it singleton}. If more then one non-zero value is
mapped into the same index in the reduced size vector, then this
index is denoted as {\it multiple}. The multiple case is problematic
since we can not be sure of the right alignment. The proposed solution
of Cole and Hariharan was to create a set of  pairs of vectors
using  hash function rather then a single pair of vectors.
They showed that in  attempts, the probability that some
index will {\em always} be in a multiple situation is small.

A different, deterministic, solution was shown in~\cite{LR07}. The
idea was to find  hash functions that reduce the size of the
vectors to . The algorithm guaranteed that each
non-zero value appears with no collisions in {\em at least} one of the
vectors, thus eliminating the possibility of en error.

The ultimate goal of the length reduction is as follows: Given two
vectors  whose sizes are , with  non-zero
elements respectively (where , obtain two vectors
 of size  such that all the non-zero elements in  and 
in  will appear as singletons in  and in 
respectively while maintaining the distance property.

The {\em distance property} which need to be maintained is defined as
follows: If  is aligned with , then
 will be aligned with .

This goal was not reached yet, rather a set of  vectors
of size  were obtained in ~\cite{LR07}, where each
non-zero in the text appears at least once as a singleton in the set
of vectors. This length reduction gave an 
algorithm for convolution in sparse data with a preprocessing time of
.

The general outline of this idea could, conceivably, work for the
sparse FWT case. However, the modulo function can not serve as a
locality preserving mapping for the Walsh transform, since the
convolution bijection is an exclusive or rather than a shift. Thus
to-date there is no known efficient algorithm for the sparse DWT.

\section{The New Length Reduction Technique for the FWT}\label{s:new_fwt}

We employ a length reduction algorithm that applies to any
convolution. The algorithm has four stages:
\begin{enumerate}
\item {\em Reduction}: A locality preserving function that reduces the
  range of indices.
\item {\em Convolution}: Perform the convolution on the shorter
  strings.
\item {\em Verification}: Verify that the results are consistent,
  i.e. that all multiplied elements indeed needed to be
  multiplied. 
\item {\em Solution Expansion}: Map the solution of the shorter
  strings, to the original longer strings.
\end{enumerate}

Below we explain the intuition for the reduction function chosen for
the DWT, and then give the details of how polynomials perform the
required length reduction.

Recall that in the case of FWT both the text and pattern have the same
length . For ease of exposition we assume that . Assume that
the text has  non-zero values, and the pattern has  non-zeros
values. Recall also that we denote the {\em exclusive or} operation by
. As in the sparse FFT case, we want to check the value of the
FWT for all  for which   has 
non-zero summands. As we mentioned above, the algorithm is the same as in the
case of the DFT. The problem is that here the 'modulo' function is no
help at all as a length reduction function. 

\subsection{Intuition}\label{ss:int_fwt}
Split the text to two strings of size
 and merge them with the ``or'' operator. Due to
the fact that , with high probability there will be no
collisions, and thus all cases will be singletons. We do the same
thing to the pattern and then run the FWT on the smaller strings. The
multiples are handled similarly to the FFT case, i.e. ignore at this
stage and in another reduction they will be unlikely to collide. We
also need a verification stage to make sure that we have not
multiplied values from different substrings.

This argument works well if the inputs are generated under a uniform
random distribution. For supporting all inputs we need some
randomization tool. The simple way to do it is as follow: Let 
be the first half of  and  the second. Choose a random bit
string  of length , set its most significant bit to be
, and calculate . Now do the `or'
operation between  and . In effect, we have randomly
permuted . The following lemma, which immediately follows from
the commutativity of the exclusive-or operation,  guarantees that this 
random permutation preserves the locality for the Walsh transform.

\begin{lemma}\label{l:local_walsh}
Pattern index  matches text index  for location  of the
Walsh transform iff pattern index  matches text index .
\end{lemma}


\begin{example}\label{ex:1} Consider the following text and pattern:

\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
address & 000 & 001 & 010 & 011 & 100 & 101 & 110 & 111\\
\hline\hline
T & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\
\hline
P & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}

The Walsh transform is:  (since at locations  and
 the 's in the text and pattern match).

Consider now . (The most significant bit (MSB) is always
 at the mask because it is a mask on , which is the second
half of the text). The smaller text and pattern are as below:

\begin{tabular}{|l|c|c|c|c|}
\hline
address & 000 & 001 & 010 & 011 \\
\hline\hline
T' & 0 & 1 & 0 & 1\\
\hline
P' & 1 & 0 & 1 & 0\\
\hline
\end{tabular}

The Walsh transform is . 

\end{example}

The above example lacks the verification and solution expansion
stages. We need to verify that the results are {\em
  consistent}. Consistency means that one of two cases happen:
\begin{enumerate}
\item Every text value with an unchanged address is multiplied by a
pattern value with an unchanged address, and every text value with a
masked address is multiplied by a pattern value with a masked address. 
\item Every text item with an unchanged address is multiplied by a
  pattern item with a masked address and every text item with a masked
  address is multiplied by a pattern item with an unchanged address.
\end{enumerate}

In addition it is necessary to be able to identify which of the non-zero
results belong to their location ( in the example) and which should
be xor-ed with the mask ( in the example).

{\bf Verification Idea:} In the reduction stage, construct two strings
of length  where for each non-zero value write  if it
is static, i.e., was not moved by the mask, and  if it was moved by
the mask. We are interested in products where all values are either , or , and in products where all values are
. This can be calculated by a constant number of
appropriate binary DWTs.

\begin{example}\label{ex:2}
In Example~\ref{ex:1} we have

\begin{tabular}{|l|c|c|c|c|}
\hline
address & 000 & 001 & 010 & 011 \\
\hline\hline
T' & 0 & s & 0 & m\\
\hline
P' & s & 0 & m & 0\\
\hline
\end{tabular}

Locations  and  have value .
In location  we have a product of  and , and
in location  we have products of . Thus the reduction
gives a consistent result and, furthermore, it means that the result
in location  remains in that location, and the result in location
 belongs to the index xor-ed with the mask, i.e. .
 
\end{example}

The idea above reduces the length by a half, from  to
. It is easy to see that this process can be recursively
repeated. However, the analysis of the probability of collision is
then a bit tedious. Our polynomial framework for length reduction
enables this analysis in a clear and elegant manner.

\subsection{Polynomials over a Finite Field}\label{ss:poly_fwt}

We will consider our indices as elements in the finite field
. The motivation for this is that  in .
Now in order to reduce the length of the string we will write each
element in  as a polynomial in  of degree
. In order to do that, let us look at the binary
presentation of a non-zero element index . The
index binary presentation is divided into set of  bits starting
from the LSB. By this division we obtain a set of 
numbers in . These numbers will be used as the
coefficients of the polynomial representing this index, thus this
polynomial belongs to  and its degree is
. 

\begin{example} 

Consider the non-zero element index . let us assume that
 by breaking the binary presentation of  into blocks of 
bits we obtain . 

\end{example}

Note that under , addition is calculated as XOR, and
multiplication is calculated as polynomial multiplication modulo some
irreducible polynomial. Below is an example of the multiplication
table of  

\begin{example} Multiplication table of . 

\begin{tabular}{|l|c|c|c|c|}
\hline
 & 0 & 1 & a & b \\
\hline\hline
0 & 0 & 0 & 0 & 0\\
\hline
1 & 0 & 1 & a & b\\
\hline
a & 0 & a & b & 1\\
\hline
b & 0 & b & 1 & a\\
\hline
\end{tabular}

Note that  and  can be thought as representing  and  respectively.
\end{example}

After we obtain the polynomials representing
the indices of the non-zeros elements, we can obtain different reduced
size vector by applying different values to  in the
polynomials. 
To prove the correctness of this length reduction for FWT, we have to
prove that the following lemma still holds: 

\begin{lemma}\label{l:WalAllignment}
For any assignment of , if  is aligned with the base
polynomial representing , then  will be aligned with
one of the polynomials representing .
\end{lemma}

\begin{proof}

The proof is simple, and it is based on the building method. The
polynomial representing the index of  is  and it is
aligned with , which is the polynomial representing the index
, thus the polynomial representing  is , and it is
aligned with the polynomial . Recall that the
coefficients of the polynomials  and  are the bits
representing  and , and the addition under  is XOR,
thus we get that  is the polynomial representing the
index . 

\end{proof}

In this framework, the analysis of the probability of collision is
easy. Two indices  and  can collide only on
 different evaluations, as any collision implies
that the value assigned to  in the polynomials is a root of the
difference polynomial representing those two indices. Since the degree
of this polynomial is bounded by , then there can be no more then
 different roots to this polynomial. We conclude: 

\begin{lemma}\label{l:prob_collision}
The probability of a collision when choosing a random evaluation 
is no greater than .
\end{lemma}

It is easy to see that appropriate binary Walsh transforms allow us to
detect singletons and delete them from the next rounds. Appropriate
binary Walsh transforms also easily provide the expansion.

\section{The New Length Reduction Technique for the FFT}\label{s:new_fft}

\subsection{Sparse Vector of Polynomial Length}\label{s:length}

The proposed technique deals with the case that  is polynomial
in , thus the indices are bounded by . In the case
where,  is exponential in , the reduction to a polynomial
case can be used.

{\bf Main Idea:} Derive a set of unique polynomials
from each non-zero index in , and one polynomial for each non-zero
in . Each assignment for the polynomials in , where  is a
prime number of size  will give a different mapping of
the non-zeros in  and in  to vectors of size . The
convolution will be performed between the vectors obtained from
 and  under the same assignments.

The first step of the algorithm is to choose a prime number of size
, and create a polynomial for each non-zero index in
. The created polynomial of index  will be denoted as the {\em
base polynomial} of . The creation of the polynomial is done by
representing the index as a number in base . Each
digit is interpreted as a coefficient of the polynomial.

\begin{example}
Let . Consider index  in base . This equals  in
base . Each digit is a coefficient, thus the
polynomial representing index  is .
\end{example}

Since the indices in  are bounded by , and  is , then the degree of the polynomials which created in this step
is bounded by . In the next step, from each polynomial we create
 polynomials. This is done by giving two choices for each
coefficient of the polynomial:
(1) Leave it as is. (2) Add  to the coefficient and
decrease by 1 the coefficient of the higher degree. We do this for all
the coefficients of the polynomial except for the coefficient of the
highest degree.

\begin{example}
Suppose we have a non-zero index , using  we get the base
polynomial . After the second step we will obtain 
polynomials: , ,,.\\
The first polynomial is the base polynomial. The second polynomial was
obtained by adding  to the first coefficient and decreasing the
second coefficient by one. The 3rd and the 4th polynomials were created
by adding  to the second coefficient of the first and second
polynomials respectively, and decreasing the third coefficient by one.
\end{example}

The duplication of the polynomials was made to meet the distance
preserving requirement of the length reduction. The following lemma
formalizes it.

\begin{lemma}\label{l:PolAllignment}
For any assignment of , if  is aligned with the base
polynomial representing , then  will be aligned with
one of the polynomials representing .
\end{lemma}
\begin{proof}
Let  be the chosen prime number. Index  in  is
represented by the polynomial , and index  in  is
represented by the a polynomial .
Index  in  is represented by a polynomial of the form
, and index  in  is
represented by a polynomial . Note
that the coefficients  and  are smaller then .

Clearly, if  is aligned with , then for any
assignment of ,  will be aligned with the polynomial
. Now consider
the first coefficient of . Since  and  are
smaller then , then there are only two cases: (1)
, thus . (2)
, thus 
which is covered by the
polynomial where  was added to the first coefficient.\\
In the later case, one was added to the second coefficient, thus we
decrease the next coefficient whenever we add  to the
current coefficient. The same cases exist also in all the
coefficients, but a polynomial was created for each possible case (
cases), thus one of the created polynomials will be equal to the
polynomial . 
\end{proof}

Note that all the  created polynomials are unique, and
in . Assigning a value to the polynomials in  will give a
vector of size .

\begin{lemma}\label{l:maxRoots}
Any two polynomials can be mapped to the same location in at most
 assignments.
\end{lemma}
\begin{proof}
The distance between any two polynomials gives a polynomial, where the
degree of the difference polynomial is bounded by . Since both
polynomials give the same index under the selected assignment, then the
assigned value is a root of the difference polynomial. The degree of
this polynomial is bounded by , thus it can have at most 
different roots in . 
\end{proof}

Since any polynomial can be mapped into the same location with at most
 other polynomials, and with each of them at most 
times, due to Lemma \ref{l:maxRoots}, then we get the following
Corollary:
\begin{corollary}\label{c:maxPolMultiples}
Any polynomial can appear as a {\it multiple} in not more then  vectors.
\end{corollary}

The last step of the length reduction algorithm is to find a set of
 assignments which will ensure that each polynomial will
appear as a singleton at least once.

The selection of the  assignments is done as follows:
Construct table  with  columns and  rows. Row  correspond to an assigned value
 and the corresponding reduced length vector . A
column corresponds to a polynomial . The value of  is set
to  if polynomial  appears as a {\it singleton} in vector
. Due to Corollary \ref{c:maxPolMultiples}, the number of
zeros in each column can not exceed . Thus,
in each column there are 's in at least half of the rows, which
means that the table is at least half full. Since the table is at
least half full there exists a row in which there is one in at least
half of the columns. The assignment value which generated this row
is chosen, and all the columns where there was a  in the selected
row are deleted from the table.

Recursively another assignment value is chosen and the table size is
halved again, until all the columns are deleted. Since at each step
at least half of the columns are deleted, the number of prime number
chosen can not exceed  .

{\bf Time:} Creating vector  (row ) takes  time.
Since we start with a full matrix of  rows then the
initialization takes  time. Choosing the 
assignment values is done recursively. The recurrence is:

The closed form of this recurrence is .

Note that if we choose the assignments randomly in running time we can
skip the preprocessing step. In the expected case we will succeed
after  assignments. But even in the worst case 
assignments will guarantee success.

\subsection{Sparse Vector of Exponential Length}\label{s:Exp}

In this case, as proposed in ~\cite{LR07}, each of the vectors 
and  is reduced into a single vector of size , where
all the non-zeros appear as singletons. The reduction is performed
using the modulus function with a prime number  of size
. It was already proven there that there are at most
 prime number of size , which generate at least one
multiple. Thus, by testing  prime numbers we ensure that at
least one of them produces a vector with no multiples.

In order to find such a prime number, we find  prime numbers of
size . Then we multiply all the prime numbers to receive a
large number . In addition we have at most  different
distances between any two non-zeros. We multiply all of them to
receive a large number . The next step is to find the greatest
common divisor () between  and . Since there is at least
one prime number in  which does not divide , then 
is less then . Dividing  by the  will give  which
is the multiplication of all the prime numbers that create only
singletons. The last step is to find at least one of them. This is
done using a binary search on the prime numbers. We take the
multiplication of half of the prime numbers , and find the
. If  we continue with this set of prime
numbers and multiply half of them iteratively. Otherwise, we
continue with the other half of the prime numbers. After  iterations we will find one prime number which will generate
only singletons.

The algorithm appears in detail below.

\fbox{
\begin{minipage}{16cm}
{\bf Algorithm  --  is exponential in } {\sf
\begin{enumerate}
    \item Find  prime numbers of size .
    \item Multiply all the prime numbers to obtain .
    \item Multiply all the difference between any two non-zero indices
      to obtain .
    \item Set .
    \item Let  be the set of all prime numbers.
    \item While the size of  is larger then  do:
\begin{enumerate}
    \item Let  be a set of the first half of prime numbers in .
    \item Set  to be the multiplication of all the prime numbers in .
    \item If  then set , otherwise set .
\end{enumerate}
\end{enumerate}
{\bf end Algorithm} }
\end{minipage}
}

{\bf Correctness:} Immediately follows from the discussion.

{\bf Time:} Step 1 is performed in time 
using the primality testing described in ~\cite{berri:02}. Step 2 is
done by building a binary tree of products where each node
contain the product of the two number in the lower level. This
tree has  levels. In the leaves there are  prime
numbers with  bits, so the total number of bits in each
level is . A product of two numbers can be
computed in time  ~\cite{SS-71}, where  is
the number of bits. Thus each level can be computed in time  and the total time for step 2 is . step 3 is preformed in the same way, but this time
in the leaves there are  numbers with  bits, thus each
level has  bits and the time for this step is . In step 4 we calculate the  of two numbers with  bits. This can be done in time 
using ~\cite{SZ:02}. The calculation for step 6(b) was
already performed in step 2, and step 6(c) can be calculated in time
, thus the time of step 6 is . Following this discussion the total time of this
algorithm is .



\section{Conclusion and Open Problems}\label{s:conc}

A new tool for Length Reduction and Sparse Convolution was introduced
in this paper - encoding the indices as polynomials. This enabled the
first algorithm for efficient calculation of sparse binary discrete
Walsh transform. It also provided better and faster algorithms for
several well known problems, such as randomized efficient sparse FFT
and deterministic efficient sparse FFT computation.

An important problem remains: Can the Length Reduction and Sparse
DFT problems be solved deterministically without the need of the
preprocessing step or, alternately, can the preprocessing time be
reduced from quadratic?

\bibliographystyle{plain}
\small{
\bibliography{/home/amir/Tex/paper}
}

\end{document}
