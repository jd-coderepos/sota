\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage{colortbl}
\usepackage{xcolor}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy

\def\iccvPaperID{4495}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{High-Resolution Representations
for Labeling Pixels and Regions}

\author{Ke Sun ~~~ Yang Zhao\samethanks[1]~~~Borui Jiang\samethanks[1] ~~~ Tianheng Cheng\samethanks[1]
	~~~ Bin Xiao \\ Dong Liu~~~ Yadong Mu~~~Xinggang Wang~~~Wenyu Liu~~~ Jingdong Wang\\
	University of Science and Technology of China~~~ Microsoft Research Asia\\The University of Adelaide Peking University Huazhong University of Science and Technology\\	
	{\tt\small sunk@mail.ustc.edu.cn, yang.zhao4@griffithuni.edu.au, jbr@pku.edu.cn, muyadong@gmail.com}\\ {\tt\small \{vic,liuwy,xgwang\}@hust.edu.cn, dongleiu@ustc.edu.cn, \{Bin.Xiao,jingdw\}@microsoft.com}
}

\maketitle
\thispagestyle{empty}
\begin{abstract}
High-resolution representation learning
plays an essential role in many vision problems,
e.g., pose estimation and semantic segmentation.
The high-resolution network (HRNet)~\cite{SunXLW19},
recently developed for human pose estimation,
maintains high-resolution representations
through the whole process
by connecting high-to-low resolution convolutions in \emph{parallel}
and produces strong high-resolution representations
by repeatedly conducting fusions across parallel convolutions.

In this paper, we conduct a further study
on high-resolution representations
by introducing a simple yet effective modification
and apply it to a wide range of vision tasks.
We augment the high-resolution representation
by aggregating the (upsampled) representations
from all the parallel convolutions
rather than only the representation
from the high-resolution convolution as done in~\cite{SunXLW19}.
This simple modification leads to stronger representations,
evidenced by superior results.
We show top results
in semantic segmentation
on Cityscapes, LIP,
and PASCAL Context,
and facial landmark detection on AFLW, COFW, W, and WFLW.
In addition,
we build a multi-level representation
from the high-resolution representation
and apply it to the Faster R-CNN object detection framework and the extended frameworks.
The proposed approach achieves superior results
to existing single-model networks on COCO object detection.
The code and models have been publicly available at \url{https://github.com/HRNet}.
\end{abstract}

\section{Introduction}
Deeply-learned representations
have been demonstrated
to be strong and
achieved state-of-the-art results in many vision tasks.
There are two main kinds of representations:
low-resolution representations
that are mainly for image classification,
and high-resolution representations
that are essential for many other vision problems,
e.g., semantic segmentation,
object detection,
human pose estimation,
etc.
The latter one,
the interest of this paper,
remains unsolved and is attracting a lot of attention.


There are two main lines for
computing high-resolution representations.
One is to recover high-resolution representations from low-resolution representations outputted by a network
(e.g., ResNet)
and optionally intermediate medium-resolution representations,
e.g., Hourglass~\cite{NewellYD16},
SegNet~\cite{BadrinarayananK17},
DeconvNet~\cite{NohHH15},
U-Net~\cite{RonebergerFB15},
and encoder-decoder~\cite{PengFWM16}.
The other one is to maintain high-resolution representations
through high-resolution convolutions
and strengthen the representations
with parallel low-resolution convolutions~\cite{SunXLW19,FourureEFMT017,ZhouHZ15,SaxenaV16}.
In addition, dilated convolutions
are used to replace some strided convolutions
and associated regular convolutions
in classification networks
to compute medium-resolution representations~\cite{ChenPKMY18,ZhaoSQWJ17}.



We go along the research line
of maintaining high-resolution representations
and further study the high-resolution network (HRNet),
which is initially developed for human pose estimation~\cite{SunXLW19},
for a broad range of vision tasks.
An HRNet maintains high-resolution representations
by connecting high-to-low resolution convolutions in parallel
and repeatedly conducting multi-scale fusions across parallel convolutions.
The resulting high-resolution representations are not only strong
but also spatially precise.

\begin{figure*}[t]
\footnotesize
    \centering
    \includegraphics[width = 0.99\textwidth]{figs/HRNet+2.pdf}
    \caption{A simple example
    of a high-resolution network.
    There are four stages.
    The st stage consists of high-resolution convolutions.
    The nd (rd, th) stage
    repeats two-resolution
    (three-resolution, four-resolution)
    blocks. The detail is given in Section~\ref{sec:HRNetV1}.}
    \label{fig:HRNet}
    \vspace{-2mm}
\end{figure*}

We make a simple modification
by exploring the representations
from all the high-to-low resolution parallel convolutions
other than only the high-resolution representations in the original HRNet ~\cite{SunXLW19}.
This modification adds a small overhead and
leads to stronger high-resolution representations.
The resulting network is named as HRNetV.
We empirically show the superiority to the original HRNet.





We apply our proposed network
to semantic segmentation/facial landmark detection
through estimating segmentation maps/facial landmark heatmaps
from the output high-resolution representations.
In semantic segmentation,
the proposed approach achieves
state-of-the-art results
on PASCAL Context, Cityscapes, and LIP
with similar model sizes and lower computation complexity.
In facial landmark detection,
our approach achieves overall best results
on four standard datasets: AFLW, COFW, W, and WFLW.

In addition, we construct a multi-level representation
from the high-resolution representation,
and apply it to the Faster R-CNN object detection framework
and its extended frameworks, Mask R-CNN \cite{HeGDG17} and Cascade R-CNN \cite{CaiV18}.
The results show that our method gets great detection performance improvement and in particular dramatic improvement
for small objects.
With single-scale training and testing,
the proposed approach achieves better COCO object detection results
than existing single-model methods.

\section{Related Work}
Strong high-resolution representations play an essential role
in pixel and region labeling problems,
e.g., semantic segmentation, human pose estimation, facial landmark detection,
and object detection.
We review representation learning techniques developed
mainly in the semantic segmentation,
facial landmark detection~\cite{SunWT13,KowalskiNT17,LvSXCZ17,XiaoFXLYK16,ZhangLLT14,TrigeorgisSNAZ16,ZhangSKC14} and object detection areas\footnote{The techniques developed for human pose estimation
are reviewed in~\cite{SunXLW19}.},
from low-resolution representation learning,
high-resolution representation recovering,
to high-resolution representation maintaining.


\noindent\textbf{Learning low-resolution representations.}
The fully-convolutional network (FCN) approaches~\cite{LongSD15,SermanetEZMFL13}
compute low-resolution representations
by removing the fully-connected layers in a classification network,
and estimate from their coarse segmentation confidence maps.
The estimated segmentation maps are improved
by combining the fine segmentation score maps
estimated from intermediate low-level medium-resolution representations~\cite{LongSD15},
or iterating the processes~\cite{KowalskiNT17}.
Similar techniques have also been applied
to edge detection, e.g., holistic edge detection~\cite{XieT15}.

The fully convolutional network
is extended,
by replacing a few (typically two) strided convolutions
and the associated convolutions with dilated convolutions,
to the dilation version,
leading to medium-resolution representations~\cite{ZhaoSQWJ17,ChenPKMY18, YuKF17,ChenPKMY14, LiPYZDS18}.
The representations are further augmented
to multi-scale contextual representations~\cite{ZhaoSQWJ17,ChenPKMY18,ChenYWXY16} through feature pyramids
for segmenting objects at multiple scales.

\vspace{.1cm}
\noindent\textbf{Recovering high-resolution representations.}
An upsample subnetwork,
like a decoder,
is adopted
to gradually recover the high-resolution representations
from the low-resolution representations outputted by the downsample process.
The upsample subnetwork could be a symmetric version
of the downsample subnetwork,
with skipping connection over some mirrored layers
to transform the pooling indices,
e.g., SegNet~\cite{BadrinarayananK17} and DeconvNet~\cite{NohHH15},
or copying the feature maps, e.g., U-Net~\cite{RonebergerFB15} and Hourglass~\cite{NewellYD16, YangLZ17, BulatT17, DengTZZ17, BulatT17a},
encoder-decoder~\cite{PengFWM16},
FPN~\cite{LinDGHHB17}, and so on.
The full-resolution residual network~\cite{PohlenHML17}
introduces an extra full-resolution stream
that carries information at the full image resolution,
to replace the skip connections,
and each unit in the downsample and upsample subnetworks
receives information from and sends information to
the full-resolution stream.

The asymmetric upsample process is also widely studied.
RefineNet~\cite{LinMSR17} improves the combination
of upsampled representations and
the representations of the same resolution
copied from the downsample process.
Other works include:
light upsample process~\cite{BulatT16};
light downsample and heavy upsample processes~\cite{ValleBVB18},
recombinator networks~\cite{HonariYVP16};
improving skip connections with more or complicated convolutional units~\cite{PengZYLS17, ZhangZPXS18, IslamRBW17}, as well as sending information from low-resolution skip connections to high-resolution skip connections~\cite{ZhouSTL18}
or exchanging information between them~\cite{GuoDXZ18};
studying the details the upsample process~\cite{WojnaUGSCFF17};
combining multi-scale pyramid representations~\cite{ChenZPSA18, XiaoLZJS18};
stacking multiple DeconvNets/U-Nets/Hourglass~\cite{FuLWL17, Wu0YWC018}
with dense connections~\cite{TangPGWZM18}.




\vspace{.1cm}
\noindent\textbf{Maintaining high-resolution representations.}
High-resolution representations
are maintained through the whole process,
typically by a network that is formed
by connecting multi-resolution (from high-resolution to low-resolution) parallel convolutions
with repeated information exchange across parallel convolutions.
Representative works include GridNet~\cite{FourureEFMT017}, convolutional neural fabrics~\cite{SaxenaV16}, interlinked CNNs~\cite{ZhouHZ15},
and the recently-developed high-resolution networks (HRNet)~\cite{SunXLW19}
that is our interest.

The two early works, convolutional neural fabrics~\cite{SaxenaV16}
and interlinked CNNs~\cite{ZhouHZ15},
lack careful design
on when to start low-resolution parallel streams
and how and when to exchange information across parallel streams,
and do not use batch normalization and residual connections,
thus not showing satisfactory performance.

GridNet~\cite{FourureEFMT017}
is like a combination of multiple U-Nets
and includes two symmetric information exchange stages:
the first stage only passes information from high-resolution
to low-resolution,
and the second stage only passes information
from low-resolution to high-resolution.
This limits its segmentation quality.






\begin{figure}[t]
    \centering
    \footnotesize
   \subfigure[]{\includegraphics[height=0.33\linewidth]{figs/MultiResolutionGroupConvolution.pdf}}~~
    \subfigure[]{\includegraphics[height=0.33\linewidth]{figs/MultiResolutionConvolution.pdf}}~~
    \subfigure[]{\includegraphics[height=0.33\linewidth]{figs/NormalConvolution.pdf}}
    \caption{Multi-resolution block:
    (a) multi-resolution group convolution and (b) multi-resolution convolution.
    (c) A normal convolution (left) is equivalent to
    fully-connected multi-branch convolutions (right).}
    \label{fig:multiresolutionblock}
    \vspace{-3mm}
\end{figure}



\begin{figure*}[t]
\footnotesize
    \centering
    (a)~~\includegraphics[scale=.8,angle=90]{figs/HighResolutionRepresentation12.pdf}~~~~~~~~~~
    (b)~~\includegraphics[scale=.8,angle=90]{figs/HighResolutionRepresentation22.pdf}~~~~~~~~~~
    (c)~~\includegraphics[scale=.8,angle=90]{figs/HighResolutionRepresentation32.pdf}
    \caption{
    (a) The high-resolution representation proposed in~\cite{SunXLW19} (HRNetV );
    (b) Concatenating the (upsampled) representations
    that are from all the resolutions
    for semantic segmentation and facial landmark detection (HRNetV );
    (c) A feature pyramid formed over (b)
    for object detection (HRNetVp).
    The four-resolution representations at the bottom in each sub-figure are outputted from the network in Figure~\ref{fig:HRNet},
    and the gray box indicates how the output representation is obtained from the input four-resolution representations.}
    \label{fig:highresolutionhead}
    \vspace{-3mm}
\end{figure*}

\section{Learning High-Resolution Representations}
\label{sec:HRNetV1}
The high-resolution network~\cite{SunXLW19}, which we named HRNetV
for convenience,
maintains high-resolution representations
by connecting high-to-low resolution convolutions in parallel,
where there are repeated multi-scale fusions across parallel convolutions.

\vspace{.1cm}
\noindent\textbf{Architecture.}
The architecture is illustrated in Figure~\ref{fig:HRNet}.
There are four stages,
and the nd, rd and th stages are formed
by repeating modularized multi-resolution blocks.
A multi-resolution block
consists of a multi-resolution group convolution
and a multi-resolution convolution
which is illustrated in Figure~\ref{fig:multiresolutionblock} (a) and (b).
The multi-resolution group convolution is a simple extension
of the group convolution, which divides the input channels
into several subsets of channels
and performs a regular convolution
over each subset
over different spatial resolutions separately.

The multi-resolution convolution is depicted
in Figure~\ref{fig:multiresolutionblock} (b).
It resembles the multi-branch full-connection manner
of the regular convolution, illustrated in in Figure~\ref{fig:multiresolutionblock} (c).
A regular convolution
can be divided as multiple small convolutions
as explained in~\cite{ZhangQ0W17}.
The input channels are divided into several subsets,
and the output channels are also divided into several subsets.
The input and output subsets are connected
in a fully-connected fashion,
and each connection is a regular convolution.
Each subset of output channels
is a summation of the outputs of the convolutions
over each subset of input channels.

The differences lie in two-fold.
(i) In a multi-resolution convolution
each subset of channels is over a different resolution.
(ii)
The connection between input channels
and output channels
needs to handle
The resolution decrease is
implemented in~\cite{SunXLW19}
by using several -strided  convolutions.
The resolution increase is simply implemented in~\cite{SunXLW19}
by bilinear (nearest neighbor) upsampling.

\vspace{.1cm}
\noindent\textbf{Modification.}
In the original approach HRNetV,
only the representation (feature maps)
from the high-resolution convolutions
in~\cite{SunXLW19}
are outputted,
which is illustrated in Figure~\ref{fig:highresolutionhead} (a).
This means that
only a subset of output channels
from the high-resolution convolutions
is exploited
and other subsets from low-resolution convolutions are lost.

We make a simple yet effective modification
by exploiting other subsets of channels
outputted from low-resolution convolutions.
The benefit is that the capacity of the multi-resolution
convolution is fully explored.
This modification only adds a small parameter and computation overhead.

We rescale the low-resolution representations through bilinear upsampling
to the high resolution,
and concatenate the subsets of representations,
illustrated in Figure~\ref{fig:highresolutionhead} (b),
resulting in the high-resolution representation,
which we adopt for estimating segmentation maps/facial landmark heatmaps.
In application to object detection,
we construct a multi-level representation
by downsampling the high-resolution representation
with average pooling
to multiple levels,
which is depicted in Figure~\ref{fig:highresolutionhead} (c).
We name the two modifications as HRNetV and HRNetVp, respectively,
and empirically compare them in Section~\ref{sec:empiricalstudy}.

\vspace{.1cm}
\noindent\textbf{Instantiation}
We instantiate the network using a similar manner as HRNetV~\cite{SunXLW19}\footnote{\url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}}.
The network starts from a stem
that consists of two strided  convolutions
decreasing the resolution to .
The st stage contains  residual units
where each unit
is formed by a bottleneck with the width ,
and is followed by one  convolution
reducing the width of feature maps
to .
The nd, rd, th stages
contain , ,  multi-resolution blocks, respectively.
The widths (number of channels) of the convolutions
of the four resolutions
are , , , and , respectively.
Each branch in the multi-resolution group convolution
contains  residual units
and each unit contains two  convolutions in each resolution.


In applications to semantic segmentation
and facial landmark detection,
we mix the output representations (Figure~\ref{fig:highresolutionhead} (b)),
from all the four resolutions
through a  convolution,
and produce a -dimensional representation.
Then, we pass the mixed representation
at each position
to a linear classifier/regressor
with the softmax/MSE loss
to predict the segmentation maps/facial landmark heatmaps.
For semantic segmentation,
the segmentation maps are upsampled ( times)
to the input size by bilinear upsampling
for both training and testing.
In application to object detection,
we reduce the dimension of the high-resolution representation
to , similar to FPN~\cite{LinDGHHB17},
through a  convolution
before forming the feature pyramid in Figure~\ref{fig:highresolutionhead} (c).

\section{Experiments}
\subsection{Semantic Segmentation}
Semantic segmentation
is a problem of assigning
a class label to each pixel.
We report the results over
two scene parsing datasets, PASCAL Context~\cite{MottaghiCLCLFUY14} and
Cityscapes~\cite{CordtsORREBFRS16},
and a human parsing dataset, LIP~\cite{GongLSL17}. The mean of class-wise intersection over union (mIoU)
is adopted as the evaluation metric.

	\setlength{\tabcolsep}{6.0pt}
	\begin{table}[t]
		\scriptsize
		\centering
		\caption{Segmentation results on
		Cityscapes \texttt{val}
	    (single scale and no flipping).
		The GFLOPs is calculated on the input size .}
		\label{tab:cityscapevalresults}
		\begin{tabular}{l|lrr|c}
			\hline\noalign{\smallskip}
			 & backbone & \#param. & GFLOPs & mIoU\\
			 \hline
			
			 \hline
			UNet++~\cite{ZhouSTL18} & ResNet- & M &  &  \\
			DeepLabv3~\cite{ChenPSA17} & Dilated-ResNet- & M &  &  \\
			DeepLabv3+~\cite{ChenZPSA18} & Dilated-Xception- & M &  &  \\
			PSPNet~\cite{ZhaoSQWJ17} & Dilated-ResNet- & M &  &  \\
			\hline
			Our approach &  HRNetV-W & M &  &  \\
			Our approach & HRNetV-W & M &  &  \\
			\hline
		\end{tabular}
		\vspace{-4mm}
	\end{table}

\vspace{.1cm}
\noindent\textbf{Cityscapes.}
The Cityscapes dataset~\cite{CordtsORREBFRS16} contains  high quality pixel-level finely annotated scene images.
The finely-annotated images are divided into  images for training, validation and testing.
There are  classes, and  classes among them are used for evaluation.
In addition to the mean of class-wise intersection over union (mIoU),
we report other three scores on the test set:
IoU category (cat.), iIoU class (cla.) and iIoU category (cat.).

We follow the same training protocol~\cite{ZhaoSQWJ17, ZhaoZLSLLJ18}.
The data are augmented by random cropping (from  to ), random scaling in the range of , and random horizontal flipping. We use the SGD optimizer with the base learning rate of ,
the momentum of  and the weight decay of . The poly learning rate policy with the power of  is used for dropping the learning rate. All the models are trained for  iterations with the batch size of  on  GPUs and syncBN.



\renewcommand{\arraystretch}{1.3}
	\setlength{\tabcolsep}{3pt}
	\begin{table}[t]
		\scriptsize
		\centering
		\caption{Semantic segmentation results on Cityscapes \texttt{test}.}
		\label{tab:cityscaperesults}
		\begin{tabular}{l|l|cccc}
			\hline\noalign{\smallskip}
			  & backbone & mIoU  & iIoU cla. & IoU cat. & iIoU cat.\\
			\hline
			
			\hline
			\multicolumn{3}{l}{\emph {Model learned on the \texttt{train} set}}\\
			\hline
			PSPNet~\cite{ZhaoSQWJ17} & Dilated-ResNet- &  &  &   &  \\
			PSANet~\cite{ZhaoZLSLLJ18} & Dilated-ResNet- &  & - & - & - \\
			PAN~\cite{LiXAW18} & Dilated-ResNet- &  & - & - & - \\
			AAF~\cite{KeHLY18} & Dilated-ResNet- &  & - & - & -\\
			\hline
			Our approach & HRNetV-W &  &  &  & \\
			\hline
			
			\hline
			\multicolumn{3}{l}{
			\emph {Model learned on the \texttt{train+valid} set}}\\
			\hline
			GridNet~\cite{FourureEFMT017} & - &  &  &  & \\
			LRR-4x~\cite{GhiasiF16} & - &  &  &  & \\
			DeepLab~\cite{ChenPKMY18} & Dilated-ResNet- &  &  &  & \\
			LC~\cite{LiLLLT17}& - &  & - & - & - \\
			Piecewise~\cite{LinSHR16}& VGG- &  &  &  & \\
			FRRN~\cite{PohlenHML17}& - &  &  &  & \\
			RefineNet~\cite{LinMSR17}& ResNet- &  &  &  & \\
			PEARL~\cite{JinLXSLYCDLJFY17} & Dilated-ResNet- &  &  &  &  \\
			DSSPN~\cite{LiangZX18} & Dilated-ResNet- &  &  &  & \\
			LKM~\cite{PengZYLS17}& ResNet- &  & - & - & - \\
			DUC-HDC~\cite{WangCYLHHC18}& - &  &  &  & \\
			SAC~\cite{ZhangTZLY17} & Dilated-ResNet- &  & - & - & - \\
			DepthSeg~\cite{KongF18} & Dilated-ResNet- & & - & - & - \\
			ResNet38~\cite{WuSH16e} & WResNet-38 & & & & \\
			BiSeNet~\cite{YuWPGYS182} & ResNet- &  & - & - & - \\
			DFN~\cite{YuWPGYS18} & ResNet- &  & - & - & - \\
			PSANet~\cite{ZhaoZLSLLJ18} & Dilated-ResNet- &  & - & - & - \\
			PADNet~\cite{OWS18} & Dilated-ResNet- &  &  &  & \\
			DenseASPP~\cite{ZhaoSQWJ17} & WDenseNet- &  &  &  &  \\
			\hline
			Our approach &  HRNetV-W  &  &  &  &  \\
			\hline
		\end{tabular}
		\vspace{-3mm}
	\end{table}

Table~\ref{tab:cityscapevalresults} provides the comparison with several representative methods
on the Cityscapes validation set
in terms of parameter and computation complexity and mIoU class.
(i) HRNetV-W ( indicates the width of the high-resolution convolution), with similar model size to DeepLabv+
and much lower computation complexity, gets better performance:
 points gain over UNet++,  points gain over DeepLabv3
and about  points gain over PSPNet, DeepLabv3+.
(ii) HRNetV-W, with similar model size to PSPNet and much lower computation complexity, achieves much significant improvement:
 points gain over UNet++,  points gain over DeepLabv3
and about  points gain over PSPNet, DeepLabv3+.
In the following comparisons,
we adopt HRNetV-W that is pretrained on ImageNet
\footnote{The
description about
ImageNet pretraining
is given in the Appendix.}
and has similar model size
as most Dilated-ResNet- based methods.

Table~\ref{tab:cityscaperesults} provides the comparison of our method
with state-of-the-art methods on the Cityscapes test set.
All the results are with six scales and flipping.
Two cases w/o using coarse data are evaluated:
One is about the model learned
on the \texttt{train} set,
and the other is about the model
learned on the \texttt{train+valid} set.
In both cases,
HRNetV-W achieves the best performance
and outperforms the previous state-of-the-art
by  point.
	
	\renewcommand{\arraystretch}{1.3}
	\setlength{\tabcolsep}{3.0pt}
	\begin{table}[t]
	\scriptsize
	\centering
	\caption{Semantic segmentation results on PASCAL-context. The methods are
	evaluated on  classes and  classes.}
	\label{tab:pasctxresults}
	\begin{tabular}{l|l|cc}
		\hline\noalign{\smallskip}
		  & backbone & mIoU ( classes) & mIoU ( classes) \\
		\hline
		
		\hline
		FCN-s~\cite{ShelhamerLD17} & VGG- & - &  \\
		BoxSup~\cite{DaiHS15} & - & - &  \\
		HO\_CRF~\cite{ArnabJ0T16} & - & - &  \\
		Piecewise~\cite{LinSHR16} & VGG- & - & \\
		DeepLab-v~\cite{ChenPKMY18} & Dilated-ResNet- & -&  \\
		RefineNet~\cite{LinMSR17} & ResNet- & - &  \\
		UNet++~\cite{ZhouSTL18} & ResNet- &  & - \\
		PSPNet~\cite{ZhaoSQWJ17} & Dilated-ResNet- &  & - \\
		Ding et al.~\cite{DingJSL018} & ResNet- &  & - \\
		EncNet~\cite{0005DSZWTA18} & Dilated-ResNet- &  & - \\
		\hline
		Our approach & HRNetV-W &   &  \\	
		\hline
	\end{tabular}
	\end{table}

\renewcommand{\arraystretch}{1.3}
	\setlength{\tabcolsep}{2.8pt}
	\begin{table}[t]
	\scriptsize
	\centering
	\caption{Semantic segmentation results on LIP. Our method doesn't exploit
	any extra information, e.g., pose or edge. }
	\label{tab:lipresults}
	\begin{tabular}{l|lc|ccc}
		\hline
		\noalign{\smallskip}
		 & backbone & extra. & pixel acc. & avg. acc. & mIoU \\
		\hline
		
		\hline
		Attention+SSL~\cite{GongLSL17} & VGG & Pose &  &  &  \\
		DeepLabV+~\cite{ChenZPSA18} & Dilated-ResNet- & - &  &  &  \\
		MMAN~\cite{LuoZZGYY18} & Dilated-ResNet- & - & - & - &  \\
		SS-NAN~\cite{ZhaoLNZCWFY17} & ResNet- & Pose &  &  &  \\
		MuLA~\cite{NieFY18} & Hourglass & Pose &  &  &  \\
		JPPNet~\cite{XL18} & Dilated-ResNet- & Pose &  &  &  \\
		CE2P~\cite{TL18}  & Dilated-ResNet- & Edge &  &  &  \\
		\hline
		Our approach & HRNetV-W & N &  &  &  \\	
		\hline
	\end{tabular}
	 \vspace{-3mm}
	\end{table}
	
\vspace{.1cm}
\noindent\textbf{PASCAL context.}
The PASCAL context dataset~\cite{MottaghiCLCLFUY14}  includes  scene images for training and
 images for testing with  semantic labels and  background label.

The data augmentation and learning rate policy are the same as Cityscapes.
Following the widely-used training strategy~\cite{0005DSZWTA18, DingJSL018},
we resize the images to  and set the initial learning rate to 
and weight decay to . The batch size is  and the number of iterations is .

We follow the standard testing procedure~\cite{0005DSZWTA18, DingJSL018}.
The image is resized to 
and then fed into our network.
The resulting  label maps are then resized to the
original image size.
We evaluate the performance of our approach and other approaches
using six scales and flipping.


Table~\ref{tab:pasctxresults}
provides the comparison of our method
with state-of-the-art methods.
There are two kinds of evaluation schemes:
mIoU over  classes and  classes ( classes + background).
In both cases,
HRNetV-W performs superior to previous state-of-the-arts.


\vspace{.1cm}
\noindent\textbf{LIP.}
The LIP dataset \cite{GongLSL17}
contains  elaborately annotated human images,
which are divided into  training images,
and  validation images. The methods are evaluated on
 categories ( human part labels and
 background label).
Following the standard training and testing settings~\cite{TL18}, the images are resized to  and
the performance is evaluated
on the average of the segmentation maps of the original and flipped images.

The data augmentation and learning rate policy are the same as Cityscapes.
The training strategy follows the recent setting~\cite{TL18}.
We set the initial learning rate to  and
the momentum to  and the weight decay to .
The batch size is  and the number of iterations is K.

Table~\ref{tab:lipresults}
provides the comparison of our method
with state-of-the-art methods.
The overall performance of HRNetV-W
performs the best
with fewer parameters and lighter computation cost.
We also would like to mention that
our networks do not use extra information such as pose or edge.

\renewcommand{\arraystretch}{1.3}
	\begin{table}[t]
	    \centering\setlength{\tabcolsep}{2.9pt}
	    \scriptsize
	    \caption{GFLOPs and \#parameters
	    of Faster R-CNN for COCO object detection.
	    The numbers are obtained
	    with the input size 
	    and
	     proposals fed into R-CNN.
	    ResNet--FPN (R-),
	    X--d (X-101),
	    HRNetV2p-W (H-).
	    }
	    \label{tab:det_training_comparision}
	    \begin{tabular}{l|rr|rr|rr|rr}
	        \hline \noalign{\smallskip}
	        ~ & R- & H-& R- &H-& R- &H- & X- & H-\\
	        \hline
	
	        \hline
	        \#param. (M) &  &  &  &  &  &  &  &  \\
	        GFLOPs &  &  &  &  &  &  &  &  \\
	        \hline
	    \end{tabular}
	\end{table}

\renewcommand{\arraystretch}{1.3}
	\begin{table}[t]
	\setlength{\tabcolsep}{4.4pt}
	\scriptsize
	\centering
	\caption{Object detection results evaluated on COCO \texttt{val}
	in the Faster R-CNN framework.
	LS = learning schedule.}
	\label{tab:object_detection_fpn}
	\begin{tabular}{l|c|ccc|ccc}
		\hline \noalign{\smallskip}
		backbone & LS & AP & AP & AP & AP & AP & AP\\
		\hline
		
		\hline

	  	ResNet--FPN  &   &  &  &  &  &  &  \\
		HRNetVp-W &  &  &  &  &  &  &  \\
	    ResNet--FPN &   &  &  &  &  &  &  \\
		HRNetVp-W &  &  &  &  &  &  &  \\
	    \hline
	    ResNet--FPN &  &  &  &  &  &  &  \\
		HRNetVp-W &  &  &  &  &  &  &  \\
		ResNet--FPN &  &  &  &  &  &  &  \\
	    HRNetVp-W &  &  &  &  &  &  &   \\
	    \hline
	
	    ResNet--FPN &  &  &  &  &  &  &  \\
	    HRNetVp-W &  &  &  &  &  &  &   \\

	    ResNet--FPN &  &  &  &  &  &  &  \\

	    HRNetVp-W &  &  &  &  &  &  &   \\
	    \hline
	
	    X--d-FPN &  &  &  &  &  &  &  \\
	    HRNetVp-W &  &  &  &  &  &  &   \\
	    X--d-FPN &  &  &  &  &  &  &  \\
	    HRNetVp-W &  &  &  &  &  &  &   \\

	    \hline
	\end{tabular}
	\vspace{-1.5mm}
	\end{table}

\renewcommand{\arraystretch}{1.3}
	\begin{table}[t]
	\setlength{\tabcolsep}{2.3pt}
	\centering
	\caption{Object detection results evaluated on COCO \texttt{val}
	in the Mask R-CNN framework.
	LS = learning schedule.}
	\label{tab:object_detection_maskrcnn}
	\scriptsize
	\begin{tabular}{l|c|cccc|cccc}
		\hline \noalign{\smallskip}
		\multirow{2}{*}{backbone} &
		\multirow{2}{*}{LS} & \multicolumn{4}{c|}{mask} & \multicolumn{4}{c}{bbox} \\
		\cline{3-10}&  & AP & AP & AP & AP & AP & AP & AP & AP \\
		\hline

		\hline
	    ResNet--FPN &   &  &  &  &  &  &  &  &   \\
		HRNetVp-W &  &  &  &  &  &  &  &  &   \\
ResNet--FPN &   &  &  &  &  &  &  &  &   \\
		HRNetVp-W &  &  &  &  &  &  &  &  &   \\
	    \hline
		ResNet--FPN &  &  &  &  &  &  &  &  &   \\
		HRNetVp-W &  &  &  &  &  &  &  &  &   \\
ResNet--FPN &  &  &  &  &  &  &  &  &   \\
		HRNetVp-W &  &  &  &  &  &  &  &  &  \\
		\hline
	\end{tabular}
	\vspace{-0.4cm}
	\end{table}

\renewcommand{\arraystretch}{1.3}
    \begin{table*}[ht]
	\setlength{\tabcolsep}{9pt}
	\centering
	\caption{Comparison with the state-of-the-art single-model object detectors on COCO \texttt{test-dev} without mutli-scale training and testing.
	We obtain the results of Faster R-CNN and Cascade R-CNN
by using our implementations publicly
	available from the mmdetection platform\cite{mmdetection2018}
	except that 
	is from the original paper~\cite{CaiV18}.}
	\scriptsize
	\label{tab:recent_object_detection_results_single}
	\begin{tabular}{l|lcr|ccc|ccc}\hline \noalign{\smallskip}
    	 & backbone & size & LS & AP & AP & AP & AP & AP & AP\\

		\hline
		
		\hline
		MLKP \cite{WangWGLZ18} & VGG & - & - &   &  &  &  &  &  \\
		
		STDN \cite{ZhouNGHX18} & DenseNet- &  & - &  &  &  &  &  & \\
		
		DES \cite{ZhangQX0WY18} & VGG &   & - &  &  &  &  &  &  \\
		CoupleNet \cite{ZhuZWZWL17} & ResNet- & - & - &  &  &  &  &  &  \\
		DeNet \cite{Tychsen-SmithP17} & ResNet- &  & - &  &  &  &  &  &  \\
	    RFBNet \cite{LiuHW18} & VGG &  & -  &  &  &   &  &  &  \\
		DFPR \cite{KongSHL18} & ResNet- &  &  &  &  &  & - & - & - \\

	    PFPNet \cite{KimKSKK18} & VGG &  & - &  &  &   &  &  &  \\
	
	    RefineDet\cite{ZhangWBLL18} & ResNet- &  & - &  &  &  &  &  &   \\

	    Relation Net \cite{HuGZDW18} & ResNet- &  & - &  &  &  & - & - & - \\
	    C-FRCNN \cite{ChenHT18} & ResNet- &  &  &  &  &  &  &  &  \\
	    RetinaNet \cite{LinGGHD17} & ResNet--FPN &  &  &  &  &  &  &  &  \\

        Deep Regionlets \cite{XuLWRBC18} & ResNet- &  &   &  &  &  - &  &  &  \\
		
		FitnessNMS \cite{Tychsen-SmithP18} & ResNet- &  & - &  &  &  &  &  &  \\
        DetNet \cite{LiPYZDS18} & DetNet-FPN &  &  &  &  &  &  &  &  \\		
	    CornerNet \cite{LawD18} & Hourglass- &  & - &  &  &  &  &  &  \\
	    M2Det \cite{M2DETQ} & VGG &  &  &  &  &  &  &  &  \\
	
	    \hline
        Faster R-CNN \cite{LinDGHHB17} & ResNet--FPN &  &  &  &  &  &  &  &  \\
        Faster R-CNN & HRNetVp-W &  &  &  &  &  &  &  &   \\\arrayrulecolor{gray}\hline\arrayrulecolor{black}
		Faster R-CNN \cite{LinDGHHB17} & ResNet--FPN &  &  &  &  &  &  &  &  \\
		Faster R-CNN & HRNetVp-W &  &   &  &  &  &  &  &  \\	
		\arrayrulecolor{gray}\hline\arrayrulecolor{black}
		Faster R-CNN \cite{LinDGHHB17} & ResNet--FPN &  &  &  &  &  &  &  &  \\
		Faster R-CNN & HRNetVp-W &  &   &  &  &  &  &  &  \\
		\arrayrulecolor{gray}\hline\arrayrulecolor{black}
	    Faster R-CNN \cite{mmdetection2018} & X--d-FPN &  &  &  &  &  &  &  &  \\
		Faster R-CNN & HRNetVp-W &   &  &  &  &  &  &  &  \\

	
	   \arrayrulecolor{gray}\hline\arrayrulecolor{black}
	    Cascade R-CNN \cite{CaiV18} & ResNet--FPN &  &  &  &  &   &  &  &  \\
	    Cascade R-CNN & ResNet--FPN &  &  &  &  &  &  &  &  \\
	    Cascade R-CNN & HRNetVp-W  &  &  &  &  &  &  &  &  \\
		\arrayrulecolor{black}\hline
	\end{tabular}
	\end{table*}

\subsection{COCO Object Detection}
We apply our multi-level representations (HRNetVp)\footnote{Same as FPN~\cite{LinGGHD17},
we also use  levels.},
shown in Figure~\ref{fig:highresolutionhead} (c),
in the Faster R-CNN~\cite{RenHG015} and Mask R-CNN~\cite{HeGDG17} frameworks.
We perform the evaluation on the MS-COCO  detection dataset,
which contains k images for training,
k for validation (\texttt{val}) and k testing without provided annotations (\texttt{test-dev}).
The standard COCO-style evaluation is adopted.

We train the models for both our HRNetVp and the ResNet
on the public mmdetection platform~\cite{mmdetection2018}
with the provided training setup,
except that we use the learning rate schedule suggested in~\cite{DBLP:journals/corr/abs-1811-08883}
for .
The data is augmented
by standard horizontal flipping. The input images are resized such that the shorter edge is 800 pixels \cite{LinDGHHB17}.
Inference is performed on a single image scale.

Table~\ref{tab:det_training_comparision}
summarizes \#parameters and GFLOPs.
Table~\ref{tab:object_detection_fpn}
and Table~\ref{tab:object_detection_maskrcnn}
report the detection results on COCO \texttt{val}.
There are several observations.
(i) The model size and computation complexity of HRNetVp-W (HRNetVp-W)
are smaller than ResNet--FPN (ResNet--FPN).
(ii) With ,
HRNetV2p-W performs better than ResNet--FPN.
HRNetV2p-W performs worse than ResNet--FPN,
which might come from insufficient optimization iterations.
(iii) With , HRNetV2p-W and HRNetV2p-W perform better
than ResNet--FPN and ResNet--FPN, respectively.

 Table~\ref{tab:recent_object_detection_results_single}
reports the comparison
of our network to state-of-the-art single-model object detectors on COCO \texttt{test-dev} without
using multi-scale training and
multi-scale testing that are done in~\cite{LIUQQSJ18, QiLSJ18, LiCYD18, SinghND18,SinghD18,PengXLJZJYS18}.
In the Faster R-CNN framework,
our networks perform better than ResNets with similar
parameter and computation complexity:
HRNetVp-W vs. ResNet--FPN,
HRNetVp-W vs. ResNet--FPN,
HRNetVp-W vs. X--d-FPN.
In the Cascade R-CNN framework,
our HRNetVp-W performs better.

\subsection{Facial Landmark Detection}
Facial landmark detection
a.k.a. face alignment
is a problem of detecting the keypoints
from a face image.
We perform the evaluation over
four standard datasets:
WFLW~\cite{Wu0YWC018},
AFLW~\cite{KostingerWRB11},
COFW~\cite{Burgos-ArtizzuPD13},
and W~\cite{SagonasTZP13}.
We mainly use the normalized mean error (NME) for evaluation.
We use the inter-ocular distance as normalization
for WFLW, COFW, and W, and the face bounding box as normalization
for AFLW.
We also report area-under-the-curve scores (AUC) and failure rates.

We follow the standard scheme~\cite{Wu0YWC018} for training.
All the faces are cropped by the provided boxes according
to the center location and resized to .
We augment the data by  degrees in-plane rotation,
 scaling, and
randomly flipping.
The base learning rate is  and is dropped to
 and  at the th and th epochs. The models
are trained for  epochs with the batch size of  on
one GPU. Different from semantic segmentation,
the heatmaps are not upsampled from  to the input size,
and the loss function is optimized over
the  maps.

At testing,
each keypoint location is predicted
by transforming the highest heatvalue location from 
to the original image space
and adjusting it with a quarter offset
in the direction from the highest response
to the second highest response~\cite{ChenSWLY17}.

We adopt HRNetV-W for face landmark detection
whose parameter and computation cost
are similar to or smaller than
models with widely-used backbones: ResNet- and Hourglass~\cite{NewellYD16}.
HRNetV-W: \#parameters M, GFLOPs G;
ResNet-: \#parameters M, GFLOPs G;
Hourglass: \#parameters M, GFLOPs G.
The numbers are obtained on the input size .
It should be noted that
the facial landmark detection methods adopting ResNet- and Hourglass as backbones
introduce extra parameter and computation overhead.

\vspace{.1cm}
\noindent\textbf{WFLW.}
The WFLW dataset \cite{Wu0YWC018} is a recently-built
dataset based on the WIDER Face \cite{YangLLT16}.
There are  training and  testing images
with  manual annotated landmarks.
We report the results on the test set and several subsets:
large pose ( images), expression ( images),
illumination ( images), make-up ( images), occlusion ( images) and blur (
images).

Table \ref{table:comparison_wflw_testset} provides the comparison of our method with state-of-the-art methods.
Our approach is significantly better than other methods on the test set and all the subsets,
including LAB that exploits extra boundary information~\cite{Wu0YWC018} and PDB that uses stronger data augmentation~\cite{FengKA0W18}.

\renewcommand{\arraystretch}{1.3}
\begin{table}[t]
\scriptsize
\setlength{\tabcolsep}{0.5pt}
\centering
\caption{Facial landmark detection results
(NME) on WFLW \texttt{test} and  subsets:
pose,
expression (expr.),
illumination (illu.),
make-up (mu.),
occlusion (occu.)
and blur.
LAB~\cite{Wu0YWC018} is trained with extra boundary information (B).
PDB~\cite{FengKA0W18} adopts stronger data augmentation (DA).
Lower is better.}
\label{table:comparison_wflw_testset}
\begin{tabular}{l|l|r|r|r|r|r|r|r }
\hline \noalign{\smallskip}
   & backbone & test & pose & expr. & illu. & mu & occu. & blur\\
\hline

\hline
 ESR \cite{CaoWWS12}& - &  &  &  &  &  &  & \\
SDM \cite{XiongT13}& - & &  &  &  &  &  & \\
CFSS \cite{ZhuLLT15}& - & &  &  &  &  &  & \\
DVLN \cite{WuY17} & VGG-16& &  &  &  &  &  & \\
\hline
Our approach & HRNetV-W &  &  &  &  &  &  & \\
\hline

\hline
\multicolumn{3}{l}{
\emph {Model trained with \texttt{extra} info.}}\\
\hline
LAB (w/ B)~\cite{Wu0YWC018}& Hourglass &  &  &  &  &  &  & \\
PDB (w/ DA)~\cite{FengKA0W18}& ResNet- &  &  &  &  &  &  & \\
\hline
\end{tabular}
\end{table}

\vspace{.1cm}
\noindent\textbf{AFLW.}
The AFLW \cite{KostingerWRB11} dataset is
a widely used benchmark dataset,
where each image has  facial landmarks. Following \cite{ZhuLLT15, Wu0YWC018}, we train our models on  training images, and report the results on the AFLW-Full set ( testing images) and the AFLW-Frontal set ( testing images selected from  testing images).


Table \ref{table:comparison_aflw_testset} provides the comparison of our method with state-of-the-art methods.
Our approach achieves the best performance among methods without extra information and stronger data augmentation and even outperforms DCFE with extra D information.
Our approach performs slightly worse than
LAB that uses extra boundary information~\cite{Wu0YWC018}
and PDB~\cite{FengKA0W18}
that uses stronger data augmentation.

\renewcommand{\arraystretch}{1.3}
\begin{table}[t]
\setlength{\tabcolsep}{12.5pt}
\scriptsize
\centering
\caption{Facial landmark detection results (NME) on AFLW.
DCFE~\cite{ValleBVB18} uses extra D information (D). Lower is better.}
\label{table:comparison_aflw_testset}
\begin{tabular}{l|l|c|c }
\hline \noalign{\smallskip}
 & backbone &  full &  frontal \\
\hline

\hline
RCN \cite{HonariYVP16} &- &  & \\
CDM \cite{YuHZYM13} & -&  & \\
ERT \cite{KazemiS14}&- &  & \\
LBF \cite{RenCW014} & -&  & \\
SDM \cite{XiongT13} &- &  & \\
CFSS \cite{ZhuLLT15}&- &  &  \\
RCPR \cite{Burgos-ArtizzuPD13}&- & & \\
CCL \cite{ZhuLLT16}&- & & \\
DAC-CSR \cite{FengKC0W17}& & & \\
TSR \cite{LvSXCZ17}&VGG-S & & - \\
CPM + SBR \cite{DongYWW0S18}& CPM &&-\\
SAN \cite{DongYO018}& ResNet- & & \\
DSRN \cite{MiaoZLDAH18}& - & & -\\
LAB (w/o B) \cite{Wu0YWC018}& Hourglass &  & \\
\hline
Our approach & HRNetV2-W &  &  \\
\hline

\hline
\multicolumn{3}{l}{
\emph {Model trained with \texttt{extra} info.}}\\
\hline
DCFE (w/ D)~\cite{ValleBVB18}& - & & - \\
PDB (w/ DA)~\cite{FengKA0W18}& ResNet-& & -\\
LAB (w/ B)~\cite{Wu0YWC018}& Hourglass & & \\
\hline
\end{tabular}
\end{table}

\vspace{.1cm}
\noindent\textbf{COFW.}
The COFW dataset \cite{Burgos-ArtizzuPD13}
consists of  training
and  testing faces
with occlusions,
where each image has  facial landmarks.

Table~\ref{table:comparison_cofw_testset} provides the comparison of our method with state-of-the-art methods.
HRNetV outperforms other methods by a large margin.
In particular, it achieves the better performance than LAB with
extra boundary information and PDB with stronger data augmentation.

\renewcommand{\arraystretch}{1.3}
\begin{table}[t]
\setlength{\tabcolsep}{12.8pt}
\scriptsize
\centering
\caption{Facial landmark detection results on COFW \texttt{test}.
The failure rate is calculated at the threshold .
Lower is better for NME and FR.
}
\begin{tabular}{l|l|cc }
\hline\noalign{\smallskip}
 & backbone & NME & FR\\
\hline

\hline
Human & - &  & -\\
ESR \cite{CaoWWS12} &-&&\\
RCPR \cite{Burgos-ArtizzuPD13}& - & & \\
HPM \cite{GhiasiF14}& - & &  \\
CCR \cite{FengHKCW15}& -&  &  \\
DRDA \cite{ZhangKSC16}& - & &  \\
RAR \cite{XiaoFXLYK16}& - & &  \\
DAC-CSR \cite{FengKC0W17}& - & &  \\
LAB (w/o B) \cite{Wu0YWC018}& Hourglass &  & \\
\hline
Our approach & HRNetV-W &  &  \\
\hline

\hline
\multicolumn{3}{l}{
\emph {Model trained with \texttt{extra} info.}}\\
\hline
PDB (w/ DA)~\cite{FengKA0W18}& ResNet- &  & \\
LAB (w/ B)~\cite{Wu0YWC018}& Hourglass & &  \\
\hline
\end{tabular}
\label{table:comparison_cofw_testset}
\end{table}


\vspace{.1cm}
\noindent\textbf{W.}
The dataset~\cite{SagonasTZP13} is a combination
of HELEN~\cite{LeBLBH12}, LFPW ~\cite{BelhumeurJKK13}, AFW~\cite{ZhuR12}, XM2VTS~ and IBUG datasets,
where each face has  landmarks.
Following~\cite{RenCWS16}, we use the  training images, which
contains the training subsets of HELEN and LFPW and the full set of AFW.
We evaluate the performance
using two protocols, full set and test set.
The full set contains  images and is further divided
into a common subset ( images) from HELEN and LFPW, and a challenging subset
( images) from IBUG.
The official test set, used for competition,  contains  images ( indoor and  outdoor images).

Table \ref{table:comparison_300w_fullset}
provides the results
on the full set, and its two subsets:
common and challenging.
Table \ref{table:comparison_300w_testset} provides the results
on the test set.
In comparison to Chen et al. \cite{ChenSWLY17}
that uses Hourglass with large parameter and computation complexity
as the backbone,
our scores are better except the AUC scores.
Our HRNetV gets the overall
best performance among methods without extra information and stronger data augmentation, and is even better than LAB with extra boundary information and DCFE~\cite{ValleBVB18} that explores extra D information.


\renewcommand{\arraystretch}{1.3}
\begin{table}[t]
\setlength{\tabcolsep}{6.2pt}
\centering
\scriptsize
\caption{Facial landmark detection results (NME)
on W:
common, challenging and full.
Lower is better.}

\label{table:comparison_300w_fullset}
\begin{tabular}{ l|l|ccc  }
\hline\noalign{\smallskip}
  & backbone &common & challenging & full \\
\hline

\hline
RCN \cite{HonariYVP16} &-&  &  & \\
DSRN \cite{MiaoZLDAH18} &-&  &  &  \\
PCD-CNN \cite{KumarC18} &-&  &  &  \\
CPM + SBR \cite{DongYWW0S18} & CPM &  &  &  \\
SAN \cite{DongYO018} &ResNet-152&  &  &  \\
DAN \cite{KowalskiNT17} &-&  &  &  \\
\hline
Our approach & HRNetV-W & &  & \\
\hline

\hline
\multicolumn{3}{l}{
\emph {Model trained with \texttt{extra} info.}}\\
\hline
LAB (w/ B) \cite{Wu0YWC018}& Hourglass &  &  &  \\
DCFE (w/ D) \cite{ValleBVB18}& - & &  &  \\
\hline
\end{tabular}
\end{table}

\renewcommand{\arraystretch}{1.3}
\begin{table}[t]
\setlength{\tabcolsep}{1.8pt}
\scriptsize
\centering
\caption{Facial landmark detection results on W \texttt{test}.
DCFE~\cite{ValleBVB18} uses extra 3D information (3D).
LAB~\cite{Wu0YWC018} is trained with extra boundary information (B).
Lower is better for NME, FR and FR,
and higher is better for AUC and AUC.}
\label{table:comparison_300w_testset}
\begin{tabular}{l|l|c|c|c|c|c}
\hline\noalign{\smallskip}
 & backbone &NME & AUC & AUC & FR & FR\\
\hline

\hline
Balt. et al. \cite{Baltrusaitis0M13} & - & - & &-&  & -\\
ESR \cite{CaoWWS12}& - & &  & - &  & - \\
ERT \cite{KazemiS14}& - & &  & - &  & - \\
LBF \cite{RenCW014}& - & &  & - &  & - \\
Face++ \cite{ZhouFCJY13} & - & - & & - &  & - \\
SDM \cite{XiongT13}& - & &  & - &  & - \\
CFAN \cite{ZhangSKC14}& - & &  & - &  & - \\
Yan et al. \cite{YanLYL13} & -&- &&-&&-\\
CFSS \cite{ZhuLLT15}& - & &  & - &  & - \\
MDM \cite{TrigeorgisSNAZ16}& - & &  & - &  & - \\
DAN \cite{KowalskiNT17}& - & &  & - &  & - \\
Chen et al. \cite{ChenSWLY17}& Hourglass & &  & - &  & - \\
Deng et al. \cite{DengLYT16}& - & - & - &  & - &  \\
Fan et al. \cite{FanZ16}& - &- & - &  & - &  \\
DReg + MDM \cite{GulerTASZK17}& ResNet101 &- & - &  & - &  \\
JMFA \cite{DengTZZ17}&Hourglass &- & - &  & - &  \\
\hline
Our approach & HRNetV-W&   & 52.09  &  &  &   \\
\hline

\hline
\multicolumn{3}{l}{
\emph {Model trained with \texttt{extra} info.}}\\
\hline
LAB (w/ B) \cite{Wu0YWC018}& Hourglass &- & -&  & - &  \\
DCFE (w/ D) \cite{ValleBVB18}& - & &  & - &  & - \\
\hline
\end{tabular}
\end{table}
	

\subsection{Empirical Analysis}
\label{sec:empiricalstudy}
We compare the modified networks, HRNetV and HRNetVp,
to the original network~\cite{SunXLW19} (shortened as HRNetV)
on semantic segmentation and COCO object detection.
The segmentation and object detection results, given in
Figure~\ref{fig:empiricalstudy} (a)
and Figure~\ref{fig:empiricalstudy} (b),
imply that HRNetV outperforms HRNetV significantly,
except that
the gain is minor
in the large model case
in segmentation for Cityscapes.
We also test a variant
(denoted by HRNetVh),
which is built by appending a  convolution to
increase the dimension of the output high-resolution representation.
The results in Figure~\ref{fig:empiricalstudy} (a) and Figure~\ref{fig:empiricalstudy} (b)
show that the variant achieves slight improvement
to HRNetV,
implying that aggregating the representations
from low-resolution parallel convolutions in our HRNetV
is essential for increasing the capability.

\begin{figure}[t]
    \centering
    \footnotesize
    (a)~\includegraphics[height=4.7cm]{figs/hrnet_v1v2_cropped.pdf}
    (b)~\includegraphics[height=4.7cm]{figs/coco_empirical.pdf}
    \caption{Empirical analysis.
    (a) Segmentation on Cityscapes \texttt{val} and PASCAL-Context \texttt{test} for comparing HRNetV and its variant HRNetVh, and HRNetV (single scale and no flipping).
    (b) Object detection on COCO \texttt{val} for comparing HRNetV and its variant HRNetVh, and HRNetVp (LS = learning schedule).}
    \label{fig:empiricalstudy}
    \vspace{-0.3cm}
\end{figure}

\section{Conclusions}
In this paper,
we empirically study
the high-resolution representation network
in a broad range of vision applications
with introducing a simple modification.
Experimental results demonstrate
the effectiveness of strong high-resolution representations
and multi-level representations
learned by the modified networks
on semantic segmentation, facial landmark detection
as well as object detection.
The project page is \url{https://jingdongwang2017.github.io/Projects/HRNet/}.


\section*{Appendix: Network Pretraining}
We pretrain our network,
which is augmented by a classification head shown in Figure~\ref{fig:classificationhead},
on ImageNet~\cite{RussakovskyDSKS15}.
The classification head is described as below.
First, the four-resolution feature maps are fed into
a bottleneck and the output channels are
increased
from
, , , and 
to , , , and , respectively.
Then, we downsample the high-resolution representation
by a -strided  convolution
outputting  channels
and add it to the representation of the second-high-resolution.
This process is repeated two times
to get  feature channels over the small resolution.
Last, we transform the  channels to  channels
through a  convolution, followed by a global average pooling operation.
The output -dimensional representation is fed into the classifier.

We adopt the same data augmentation scheme for training images as in \cite{HeZRS16},
and train our models for  epochs with a batch size of .
The initial learning rate is set to 
and is reduced by  times at epoch ,  and .
We use SGD with a weight decay of  and a Nesterov momentum of . We adopt standard single-crop testing, so that  pixels are cropped from each image. The top-
and top- error are reported on the validation set.

Table~\ref{tab:ImageNetClassificationComparison}
shows our ImageNet classification results.
As a comparison, we also report the results of ResNets.
We consider two types of residual units:
One is formed by a bottleneck,
and the other is formed by two  convolutions. We follow the
PyTorch implementation of ResNets and replace the 
convolution in the input stem with two
-strided  convolutions decreasing the resolution to 
as in our networks. When the residual units are formed by two  convolutions,
an extra bottleneck is used to increase the dimension of output feature maps from  to .
One can see that under similar \#parameters
and GFLOPs, our results are comparable to and slightly better
than ResNets.

In addition, we look at the results
of two alternative schemes:
(i) the feature maps on each resolution go through a global pooling separately and then are concatenated
together to output a -dimensional representation vector,
named HRNet-W-Ci;
(ii) the feature maps on each resolution are fed into several -strided residual units (bottleneck, each dimension is increased to the double)
to increase the dimension to ,
and concatenate and average-pool them together to reach a -dimensional representation vector,
named HRNet-W-Cii,
which is used in~\cite{SunXLW19}.
Table~\ref{tab:ImageNetClassificationAblationStudy}
shows such an ablation study.
One can see that the proposed manner is superior to the two alternatives.


\begin{figure}
\footnotesize
    \centering
    \includegraphics[width=0.8\linewidth]{figs/ClassificationRepresentation3.pdf}
    \caption{Representation for ImageNet classification.
    The input of the box
    is the representations
    of four resolutions.}
    \label{fig:classificationhead}
    \vspace{-4mm}
\end{figure}

	\begin{table}[ht]
	\setlength{\tabcolsep}{9.8pt}
	\scriptsize
	\centering
	\caption{ImageNet Classification results of HRNet and ResNets. The proposed method is named HRNet-W-C.}
	\begin{tabular}{l|cc|cc}
		\hline
		& \#Params. & GFLOPs & top-1 err. & top-5 err. \\
		\hline
		
		\hline
		\multicolumn{5}{l}{\emph {Residual branch formed by two  convolutions}}\\
		\hline
		ResNet- & M &  &  &  \\
		HRNet-W-C& M &  &  &  \\
		\hline
		ResNet- & M &  &  &  \\
		HRNet-W-C& M &  &  &  \\
		\hline
		ResNet-& M &  &  &  \\	
		HRNet-W-C& M &  &  &  \\
		\hline
		
		\hline
		\multicolumn{5}{l}{\emph {Residual branch formed by a bottleneck}}\\
		\hline
		ResNet-& M &  &  &  \\
		HRNet-W-C& M &  &  &  \\
		\hline
		ResNet-& M &  &  &  \\
		HRNet-W-C& M &  &  &  \\
		\hline
		ResNet-& M &  &  &  \\
		HRNet-W-C& M &  &  &  \\
		\hline
	\end{tabular}
	\label{tab:ImageNetClassificationComparison}
	\vspace{-4mm}
	\end{table}
	

	\begin{table}[ht]
	\setlength{\tabcolsep}{8.8pt}
	\tiny
	\scriptsize
	\centering
	\caption{Ablation study on ImageNet classification
	by comparing our approach (abbreviated as HRNet-W-C) with two alternatives: HRNet-W-Ci and HRNet-W-Cii (residual branch formed by two  convolutions).}
	\begin{tabular}{l|cc|cc}
		\hline
		& \#Params. & GFLOPs & top-1 err. & top-5 err.\\
		\hline
	    HRNet-W-Ci& M &  &  &  \\
        HRNet-W-Cii& M &  &  &  \\
	    HRNet-W-C& M &  &  &  \\
        \hline
        HRNet-W-Ci& M &  &  &  \\
        HRNet-W-Cii& M &  &  &  \\
	    HRNet-W-C& M &  &  &  \\
        \hline
        HRNet-W-Ci& M &  &  &  \\
        HRNet-W-Cii&  M &  &  &  \\
	    HRNet-W-C& M &  &  &  \\
	    \hline
	\end{tabular}
	\label{tab:ImageNetClassificationAblationStudy}
	\vspace{-5mm}
	\end{table}
	
	{\small
\begin{thebibliography}{100}\itemsep=-1pt

\bibitem{ArnabJ0T16}
A.~Arnab, S.~Jayasumana, S.~Zheng, and P.~H.~S. Torr.
\newblock Higher order conditional random fields in deep neural networks.
\newblock In {\em {ECCV}}, pages 524--540, 2016.

\bibitem{BadrinarayananK17}
V.~Badrinarayanan, A.~Kendall, and R.~Cipolla.
\newblock Segnet: {A} deep convolutional encoder-decoder architecture for image
  segmentation.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 39(12):2481--2495,
  2017.

\bibitem{Baltrusaitis0M13}
T.~Baltrusaitis, P.~Robinson, and L.~Morency.
\newblock Constrained local neural fields for robust facial landmark detection
  in the wild.
\newblock In {\em {ICCVW}}, pages 354--361, 2013.

\bibitem{BelhumeurJKK13}
P.~N. Belhumeur, D.~W. Jacobs, D.~J. Kriegman, and N.~Kumar.
\newblock Localizing parts of faces using a consensus of exemplars.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 35(12):2930--2940,
  2013.

\bibitem{BulatT16}
A.~Bulat and G.~Tzimiropoulos.
\newblock Human pose estimation via convolutional part heatmap regression.
\newblock In {\em {ECCV}}, volume 9911 of {\em Lecture Notes in Computer
  Science}, pages 717--732. Springer, 2016.

\bibitem{BulatT17a}
A.~Bulat and G.~Tzimiropoulos.
\newblock Binarized convolutional landmark localizers for human pose estimation
  and face alignment with limited resources.
\newblock In {\em {ICCV}}, pages 3726--3734. {IEEE} Computer Society, 2017.

\bibitem{BulatT17}
A.~Bulat and G.~Tzimiropoulos.
\newblock How far are we from solving the 2d {\&} 3d face alignment problem?
  (and a dataset of 230, 000 3d facial landmarks).
\newblock In {\em {ICCV}}, pages 1021--1030, 2017.

\bibitem{Burgos-ArtizzuPD13}
X.~P. Burgos{-}Artizzu, P.~Perona, and P.~Doll{\'{a}}r.
\newblock Robust face landmark estimation under occlusion.
\newblock In {\em {ICCV}}, pages 1513--1520, 2013.

\bibitem{CaiV18}
Z.~Cai and N.~Vasconcelos.
\newblock Cascade {R-CNN:} delving into high quality object detection.
\newblock In {\em {CVPR}}, pages 6154--6162, 2018.

\bibitem{CaoWWS12}
X.~Cao, Y.~Wei, F.~Wen, and J.~Sun.
\newblock Face alignment by explicit shape regression.
\newblock In {\em {CVPR}}, pages 2887--2894, 2012.

\bibitem{mmdetection2018}
K.~Chen, J.~Pang, J.~Wang, Y.~Xiong, X.~Li, S.~Sun, W.~Feng, Z.~Liu, J.~Shi,
  W.~Ouyang, C.~C. Loy, and D.~Lin.
\newblock mmdetection.
\newblock \url{https://github.com/open-mmlab/mmdetection}, 2018.

\bibitem{ChenPKMY14}
L.~Chen, G.~Papandreou, I.~Kokkinos, K.~Murphy, and A.~L. Yuille.
\newblock Semantic image segmentation with deep convolutional nets and fully
  connected crfs.
\newblock {\em CoRR}, abs/1412.7062, 2014.

\bibitem{ChenPKMY18}
L.~Chen, G.~Papandreou, I.~Kokkinos, K.~Murphy, and A.~L. Yuille.
\newblock Deeplab: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected crfs.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 40(4):834--848,
  2018.

\bibitem{ChenPSA17}
L.~Chen, G.~Papandreou, F.~Schroff, and H.~Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock {\em CoRR}, abs/1706.05587, 2017.

\bibitem{ChenYWXY16}
L.~Chen, Y.~Yang, J.~Wang, W.~Xu, and A.~L. Yuille.
\newblock Attention to scale: Scale-aware semantic image segmentation.
\newblock In {\em {CVPR}}, pages 3640--3649, 2016.

\bibitem{ChenZPSA18}
L.~Chen, Y.~Zhu, G.~Papandreou, F.~Schroff, and H.~Adam.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock In {\em {ECCV}}, pages 833--851, 2018.

\bibitem{ChenSWLY17}
Y.~Chen, C.~Shen, X.~Wei, L.~Liu, and J.~Yang.
\newblock Adversarial posenet: {A} structure-aware convolutional network for
  human pose estimation.
\newblock In {\em {ICCV}}, pages 1221--1230, 2017.

\bibitem{ChenHT18}
Z.~Chen, S.~Huang, and D.~Tao.
\newblock Context refinement for object detection.
\newblock In {\em {ECCV}}, pages 74--89, 2018.

\bibitem{CordtsORREBFRS16}
M.~Cordts, M.~Omran, S.~Ramos, T.~Rehfeld, M.~Enzweiler, R.~Benenson,
  U.~Franke, S.~Roth, and B.~Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In {\em {CVPR}}, pages 3213--3223, 2016.

\bibitem{DaiHS15}
J.~Dai, K.~He, and J.~Sun.
\newblock Boxsup: Exploiting bounding boxes to supervise convolutional networks
  for semantic segmentation.
\newblock In {\em {ICCV}}, pages 1635--1643, 2015.

\bibitem{DengLYT16}
J.~Deng, Q.~Liu, J.~Yang, and D.~Tao.
\newblock M {CSR:} multi-view, multi-scale and multi-component
  cascade shape regression.
\newblock {\em Image Vision Comput.}, 47:19--26, 2016.

\bibitem{DengTZZ17}
J.~Deng, G.~Trigeorgis, Y.~Zhou, and S.~Zafeiriou.
\newblock Joint multi-view face alignment in the wild.
\newblock {\em CoRR}, abs/1708.06023, 2017.

\bibitem{DingJSL018}
H.~Ding, X.~Jiang, B.~Shuai, A.~Q. Liu, and G.~Wang.
\newblock Context contrasted feature and gated multi-scale aggregation for
  scene segmentation.
\newblock In {\em {CVPR}}, pages 2393--2402, 2018.

\bibitem{DongYO018}
X.~Dong, Y.~Yan, W.~Ouyang, and Y.~Yang.
\newblock Style aggregated network for facial landmark detection.
\newblock In {\em {CVPR}}, pages 379--388, 2018.

\bibitem{DongYWW0S18}
X.~Dong, S.~Yu, X.~Weng, S.~Wei, Y.~Yang, and Y.~Sheikh.
\newblock Supervision-by-registration: An unsupervised approach to improve the
  precision of facial landmark detectors.
\newblock In {\em {CVPR}}, pages 360--368, 2018.

\bibitem{FanZ16}
H.~Fan and E.~Zhou.
\newblock Approaching human level facial landmark localization by deep
  learning.
\newblock {\em Image Vision Comput.}, 47:27--35, 2016.

\bibitem{FengHKCW15}
Z.~Feng, P.~Huber, J.~Kittler, W.~J. Christmas, and X.~Wu.
\newblock Random cascaded-regression copse for robust facial landmark
  detection.
\newblock {\em {IEEE} Signal Process. Lett.}, 22(1):76--80, 2015.

\bibitem{FengKA0W18}
Z.~Feng, J.~Kittler, M.~Awais, P.~Huber, and X.~Wu.
\newblock Wing loss for robust facial landmark localisation with convolutional
  neural networks.
\newblock In {\em {CVPR}}, pages 2235--2245. {IEEE} Computer Society, 2018.

\bibitem{FengKC0W17}
Z.~Feng, J.~Kittler, W.~J. Christmas, P.~Huber, and X.~Wu.
\newblock Dynamic attention-controlled cascaded shape regression exploiting
  training data augmentation and fuzzy-set sample weighting.
\newblock In {\em {CVPR}}, pages 3681--3690. {IEEE} Computer Society, 2017.

\bibitem{FourureEFMT017}
D.~Fourure, R.~Emonet, {\'{E}}.~Fromont, D.~Muselet, A.~Tr{\'{e}}meau, and
  C.~Wolf.
\newblock Residual conv-deconv grid network for semantic segmentation.
\newblock In {\em {BMVC}}, 2017.

\bibitem{FuLWL17}
J.~Fu, J.~Liu, Y.~Wang, and H.~Lu.
\newblock Stacked deconvolutional network for semantic segmentation.
\newblock {\em CoRR}, abs/1708.04943, 2017.

\bibitem{GhiasiF14}
G.~Ghiasi and C.~C. Fowlkes.
\newblock Occlusion coherence: Localizing occluded faces with a hierarchical
  deformable part model.
\newblock In {\em {CVPR}}, pages 1899--1906. {IEEE} Computer Society, 2014.

\bibitem{GhiasiF16}
G.~Ghiasi and C.~C. Fowlkes.
\newblock Laplacian pyramid reconstruction and refinement for semantic
  segmentation.
\newblock In {\em {ECCV}}, pages 519--534, 2016.

\bibitem{GongLSL17}
K.~Gong, X.~Liang, X.~Shen, and L.~Lin.
\newblock Look into person: Self-supervised structure-sensitive learning and
  {A} new benchmark for human parsing.
\newblock {\em CoRR}, abs/1703.05446, 2017.

\bibitem{GulerTASZK17}
R.~A. G{\"{u}}ler, G.~Trigeorgis, E.~Antonakos, P.~Snape, S.~Zafeiriou, and
  I.~Kokkinos.
\newblock Densereg: Fully convolutional dense shape regression in-the-wild.
\newblock In {\em {CVPR}}, pages 2614--2623, 2017.

\bibitem{GuoDXZ18}
J.~Guo, J.~Deng, N.~Xue, and S.~Zafeiriou.
\newblock Stacked dense u-nets with dual transformers for robust face
  alignment.
\newblock In {\em {BMVC}}, page~44, 2018.

\bibitem{DBLP:journals/corr/abs-1811-08883}
K.~He, R.~B. Girshick, and P.~Doll{\'{a}}r.
\newblock Rethinking imagenet pre-training.
\newblock {\em CoRR}, abs/1811.08883, 2018.

\bibitem{HeGDG17}
K.~He, G.~Gkioxari, P.~Doll{\'{a}}r, and R.~B. Girshick.
\newblock Mask {R-CNN}.
\newblock In {\em {ICCV}}, pages 2980--2988, 2017.

\bibitem{HeZRS16}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em {CVPR}}, pages 770--778, 2016.

\bibitem{HonariYVP16}
S.~Honari, J.~Yosinski, P.~Vincent, and C.~J. Pal.
\newblock Recombinator networks: Learning coarse-to-fine feature aggregation.
\newblock In {\em {CVPR}}, pages 5743--5752, 2016.

\bibitem{HuGZDW18}
H.~Hu, J.~Gu, Z.~Zhang, J.~Dai, and Y.~Wei.
\newblock Relation networks for object detection.
\newblock In {\em {CVPR}}, pages 3588--3597, 2018.

\bibitem{IslamRBW17}
M.~A. Islam, M.~Rochan, N.~D.~B. Bruce, and Y.~Wang.
\newblock Gated feedback refinement network for dense image labeling.
\newblock In {\em {CVPR}}, pages 4877--4885, 2017.

\bibitem{JinLXSLYCDLJFY17}
X.~Jin, X.~Li, H.~Xiao, X.~Shen, Z.~Lin, J.~Yang, Y.~Chen, J.~Dong, L.~Liu,
  Z.~Jie, J.~Feng, and S.~Yan.
\newblock Video scene parsing with predictive feature learning.
\newblock In {\em {ICCV}}, pages 5581--5589, 2017.

\bibitem{KazemiS14}
V.~Kazemi and J.~Sullivan.
\newblock One millisecond face alignment with an ensemble of regression trees.
\newblock In {\em {CVPR}}, pages 1867--1874, 2014.

\bibitem{KeHLY18}
T.~Ke, J.~Hwang, Z.~Liu, and S.~X. Yu.
\newblock Adaptive affinity fields for semantic segmentation.
\newblock In {\em {ECCV}}, pages 605--621, 2018.

\bibitem{KimKSKK18}
S.~Kim, H.~Kook, J.~Sun, M.~Kang, and S.~Ko.
\newblock Parallel feature pyramid network for object detection.
\newblock In {\em {ECCV}}, pages 239--256, 2018.

\bibitem{KongF18}
S.~Kong and C.~C. Fowlkes.
\newblock Recurrent scene parsing with perspective understanding in the loop.
\newblock In {\em {CVPR}}, pages 956--965, 2018.

\bibitem{KongSHL18}
T.~Kong, F.~Sun, W.~Huang, and H.~Liu.
\newblock Deep feature pyramid reconfiguration for object detection.
\newblock In {\em {ECCV}}, pages 172--188, 2018.

\bibitem{KostingerWRB11}
M.~K{\"{o}}stinger, P.~Wohlhart, P.~M. Roth, and H.~Bischof.
\newblock Annotated facial landmarks in the wild: {A} large-scale, real-world
  database for facial landmark localization.
\newblock In {\em {ICCV}}, pages 2144--2151, 2011.

\bibitem{KowalskiNT17}
M.~Kowalski, J.~Naruniec, and T.~Trzcinski.
\newblock Deep alignment network: {A} convolutional neural network for robust
  face alignment.
\newblock {\em CoRR}, abs/1706.01789, 2017.

\bibitem{KumarC18}
A.~Kumar and R.~Chellappa.
\newblock Disentangling 3d pose in a dendritic {CNN} for unconstrained 2d face
  alignment.
\newblock In {\em {CVPR}}, pages 430--439. {IEEE} Computer Society, 2018.

\bibitem{LawD18}
H.~Law and J.~Deng.
\newblock Cornernet: Detecting objects as paired keypoints.
\newblock In {\em {ECCV}}, pages 765--781, 2018.

\bibitem{LeBLBH12}
V.~Le, J.~Brandt, Z.~Lin, L.~D. Bourdev, and T.~S. Huang.
\newblock Interactive facial feature localization.
\newblock In {\em {ECCV} {(3)}}, volume 7574 of {\em Lecture Notes in Computer
  Science}, pages 679--692. Springer, 2012.

\bibitem{LiXAW18}
H.~Li, P.~Xiong, J.~An, and L.~Wang.
\newblock Pyramid attention network for semantic segmentation.
\newblock In {\em {BMVC}}, page 285, 2018.

\bibitem{LiLLLT17}
X.~Li, Z.~Liu, P.~Luo, C.~C. Loy, and X.~Tang.
\newblock Not all pixels are equal: Difficulty-aware semantic segmentation via
  deep layer cascade.
\newblock In {\em {CVPR}}, pages 6459--6468, 2017.

\bibitem{LiCYD18}
Z.~Li, Y.~Chen, G.~Yu, and Y.~Deng.
\newblock {R-FCN++:} towards accurate region-based fully convolutional networks
  for object detection.
\newblock In {\em {AAAI}}, pages 7073--7080, 2018.

\bibitem{LiPYZDS18}
Z.~Li, C.~Peng, G.~Yu, X.~Zhang, Y.~Deng, and J.~Sun.
\newblock Detnet: Design backbone for object detection.
\newblock In {\em {ECCV}}, pages 339--354, 2018.

\bibitem{XL18}
X.~Liang, K.~Gong, X.~Shen, and L.~Lin.
\newblock Look into person: Joint body parsing {\&} pose estimation network and
  {A} new benchmark.
\newblock {\em CoRR}, abs/1804.01984, 2018.

\bibitem{LiangZX18}
X.~Liang, H.~Zhou, and E.~Xing.
\newblock Dynamic-structured semantic propagation network.
\newblock In {\em {CVPR}}, pages 752--761, 2018.

\bibitem{LinMSR17}
G.~Lin, A.~Milan, C.~Shen, and I.~D. Reid.
\newblock Refinenet: Multi-path refinement networks for high-resolution
  semantic segmentation.
\newblock In {\em {CVPR}}, pages 5168--5177, 2017.

\bibitem{LinSHR16}
G.~Lin, C.~Shen, A.~van~den Hengel, and I.~D. Reid.
\newblock Efficient piecewise training of deep structured models for semantic
  segmentation.
\newblock In {\em {CVPR}}, pages 3194--3203, 2016.

\bibitem{LinDGHHB17}
T.~Lin, P.~Doll{\'{a}}r, R.~B. Girshick, K.~He, B.~Hariharan, and S.~J.
  Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em {CVPR}}, pages 936--944, 2017.

\bibitem{LinGGHD17}
T.~Lin, P.~Goyal, R.~B. Girshick, K.~He, and P.~Doll{\'{a}}r.
\newblock Focal loss for dense object detection.
\newblock In {\em {ICCV}}, pages 2999--3007, 2017.

\bibitem{LiuHW18}
S.~Liu, D.~Huang, and Y.~Wang.
\newblock Receptive field block net for accurate and fast object detection.
\newblock In {\em {ECCV}}, pages 404--419, 2018.

\bibitem{LIUQQSJ18}
S.~Liu, L.~Qi, H.~Qin, J.~Shi, and J.~Jia.
\newblock Path aggregation network for instance segmentation.
\newblock In {\em {CVPR}}, pages 8759--8768, 2018.

\bibitem{TL18}
T.~Liu, T.~Ruan, Z.~Huang, Y.~Wei, S.~Wei, Y.~Zhao, and T.~Huang.
\newblock Devil in the details: Towards accurate single and multiple human
  parsing.
\newblock {\em CoRR}, abs/1809.05996, 2018.

\bibitem{LongSD15}
J.~Long, E.~Shelhamer, and T.~Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em {CVPR}}, pages 3431--3440, 2015.

\bibitem{LuoZZGYY18}
Y.~Luo, Z.~Zheng, L.~Zheng, T.~Guan, J.~Yu, and Y.~Yang.
\newblock Macro-micro adversarial network for human parsing.
\newblock In {\em {ECCV}}, pages 424--440, 2018.

\bibitem{LvSXCZ17}
J.~Lv, X.~Shao, J.~Xing, C.~Cheng, and X.~Zhou.
\newblock A deep regression architecture with two-stage re-initialization for
  high performance facial landmark detection.
\newblock In {\em {CVPR}}, pages 3691--3700, 2017.

\bibitem{MiaoZLDAH18}
X.~Miao, X.~Zhen, X.~Liu, C.~Deng, V.~Athitsos, and H.~Huang.
\newblock Direct shape regression networks for end-to-end face alignment.
\newblock In {\em {CVPR}}, pages 5040--5049, 2018.

\bibitem{MottaghiCLCLFUY14}
R.~Mottaghi, X.~Chen, X.~Liu, N.~Cho, S.~Lee, S.~Fidler, R.~Urtasun, and A.~L.
  Yuille.
\newblock The role of context for object detection and semantic segmentation in
  the wild.
\newblock In {\em {CVPR}}, pages 891--898, 2014.

\bibitem{NewellYD16}
A.~Newell, K.~Yang, and J.~Deng.
\newblock Stacked hourglass networks for human pose estimation.
\newblock In {\em {ECCV}}, pages 483--499, 2016.

\bibitem{NieFY18}
X.~Nie, J.~Feng, and S.~Yan.
\newblock Mutual learning to adapt for joint human parsing and pose estimation.
\newblock In {\em {ECCV}}, pages 519--534, 2018.

\bibitem{NohHH15}
H.~Noh, S.~Hong, and B.~Han.
\newblock Learning deconvolution network for semantic segmentation.
\newblock In {\em {ICCV}}, pages 1520--1528, 2015.

\bibitem{PengXLJZJYS18}
C.~Peng, T.~Xiao, Z.~Li, Y.~Jiang, X.~Zhang, K.~Jia, G.~Yu, and J.~Sun.
\newblock Megdet: {A} large mini-batch object detector.
\newblock In {\em {CVPR}}, pages 6181--6189, 2018.

\bibitem{PengZYLS17}
C.~Peng, X.~Zhang, G.~Yu, G.~Luo, and J.~Sun.
\newblock Large kernel matters - improve semantic segmentation by global
  convolutional network.
\newblock In {\em {CVPR}}, pages 1743--1751, 2017.

\bibitem{PengFWM16}
X.~Peng, R.~S. Feris, X.~Wang, and D.~N. Metaxas.
\newblock A recurrent encoder-decoder network for sequential face alignment.
\newblock In {\em {ECCV} {(1)}}, volume 9905 of {\em Lecture Notes in Computer
  Science}, pages 38--56. Springer, 2016.

\bibitem{PohlenHML17}
T.~Pohlen, A.~Hermans, M.~Mathias, and B.~Leibe.
\newblock Full-resolution residual networks for semantic segmentation in street
  scenes.
\newblock In {\em {CVPR}}, pages 3309--3318, 2017.

\bibitem{QiLSJ18}
L.~Qi, S.~Liu, J.~Shi, and J.~Jia.
\newblock Sequential context encoding for duplicate removal.
\newblock In {\em {NeurIPS}}, pages 2053--2062, 2018.

\bibitem{RenCW014}
S.~Ren, X.~Cao, Y.~Wei, and J.~Sun.
\newblock Face alignment at 3000 {FPS} via regressing local binary features.
\newblock In {\em {CVPR}}, pages 1685--1692, 2014.

\bibitem{RenCWS16}
S.~Ren, X.~Cao, Y.~Wei, and J.~Sun.
\newblock Face alignment via regressing local binary features.
\newblock {\em {IEEE} Trans. Image Processing}, 25(3):1233--1245, 2016.

\bibitem{RenHG015}
S.~Ren, K.~He, R.~B. Girshick, and J.~Sun.
\newblock Faster {R-CNN:} towards real-time object detection with region
  proposal networks.
\newblock volume abs/1506.01497, 2015.

\bibitem{RonebergerFB15}
O.~Ronneberger, P.~Fischer, and T.~Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em {MICCAI}}, pages 234--241, 2015.

\bibitem{RussakovskyDSKS15}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~S. Bernstein, A.~C. Berg, and F.~Li.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{SagonasTZP13}
C.~Sagonas, G.~Tzimiropoulos, S.~Zafeiriou, and M.~Pantic.
\newblock 300 faces in-the-wild challenge: The first facial landmark
  localization challenge.
\newblock In {\em {ICCV} Workshops}, pages 397--403. {IEEE} Computer Society,
  2013.

\bibitem{SaxenaV16}
S.~Saxena and J.~Verbeek.
\newblock Convolutional neural fabrics.
\newblock In {\em NIPS}, pages 4053--4061, 2016.

\bibitem{SermanetEZMFL13}
P.~Sermanet, D.~Eigen, X.~Zhang, M.~Mathieu, R.~Fergus, and Y.~LeCun.
\newblock Overfeat: Integrated recognition, localization and detection using
  convolutional networks.
\newblock {\em CoRR}, abs/1312.6229, 2013.

\bibitem{ShelhamerLD17}
E.~Shelhamer, J.~Long, and T.~Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 39(4):640--651,
  2017.

\bibitem{SinghD18}
B.~Singh and L.~S. Davis.
\newblock An analysis of scale invariance in object detection {\-} {SNIP}.
\newblock In {\em {CVPR}}, pages 3578--3587, 2018.

\bibitem{SinghND18}
B.~Singh, M.~Najibi, and L.~S. Davis.
\newblock {SNIPER:} efficient multi-scale training.
\newblock In {\em {NeurIPS}}, pages 9333--9343, 2018.

\bibitem{SunXLW19}
K.~Sun, B.~Xiao, D.~Liu, and J.~Wang.
\newblock Deep high-resolution representation learning for human pose
  estimation.
\newblock In {\em {CVPR}}, 2019.

\bibitem{SunWT13}
Y.~Sun, X.~Wang, and X.~Tang.
\newblock Deep convolutional network cascade for facial point detection.
\newblock In {\em {CVPR}}, pages 3476--3483. {IEEE} Computer Society, 2013.

\bibitem{TangPGWZM18}
Z.~Tang, X.~Peng, S.~Geng, L.~Wu, S.~Zhang, and D.~N. Metaxas.
\newblock Quantized densely connected u-nets for efficient landmark
  localization.
\newblock In {\em {ECCV}}, pages 348--364, 2018.

\bibitem{TrigeorgisSNAZ16}
G.~Trigeorgis, P.~Snape, M.~A. Nicolaou, E.~Antonakos, and S.~Zafeiriou.
\newblock Mnemonic descent method: {A} recurrent process applied for end-to-end
  face alignment.
\newblock In {\em {CVPR}}, pages 4177--4187, 2016.

\bibitem{Tychsen-SmithP17}
L.~Tychsen{-}Smith and L.~Petersson.
\newblock Denet: Scalable real-time object detection with directed sparse
  sampling.
\newblock In {\em {ICCV}}, pages 428--436, 2017.

\bibitem{Tychsen-SmithP18}
L.~Tychsen{-}Smith and L.~Petersson.
\newblock Improving object localization with fitness {NMS} and bounded iou
  loss.
\newblock In {\em {CVPR}}, pages 6877--6885, 2018.

\bibitem{ValleBVB18}
R.~Valle, J.~M. Buenaposada, A.~Vald{\'{e}}s, and L.~Baumela.
\newblock A deeply-initialized coarse-to-fine ensemble of regression trees for
  face alignment.
\newblock In {\em {ECCV}}, pages 609--624, 2018.

\bibitem{WangWGLZ18}
H.~Wang, Q.~Wang, M.~Gao, P.~Li, and W.~Zuo.
\newblock Multi-scale location-aware kernel representation for object
  detection.
\newblock In {\em {CVPR}}, pages 1248--1257, 2018.

\bibitem{WangCYLHHC18}
P.~Wang, P.~Chen, Y.~Yuan, D.~Liu, Z.~Huang, X.~Hou, and G.~W. Cottrell.
\newblock Understanding convolution for semantic segmentation.
\newblock In {\em {WACV}}, 2018.

\bibitem{WojnaUGSCFF17}
Z.~Wojna, J.~R.~R. Uijlings, S.~Guadarrama, N.~Silberman, L.~Chen, A.~Fathi,
  and V.~Ferrari.
\newblock The devil is in the decoder.
\newblock In {\em {BMVC}}, 2017.

\bibitem{Wu0YWC018}
W.~Wu, C.~Qian, S.~Yang, Q.~Wang, Y.~Cai, and Q.~Zhou.
\newblock Look at boundary: {A} boundary-aware face alignment algorithm.
\newblock In {\em {CVPR}}, pages 2129--2138, 2018.

\bibitem{WuY17}
W.~Wu and S.~Yang.
\newblock Leveraging intra and inter-dataset variations for robust face
  alignment.
\newblock In {\em {CVPR}}, pages 2096--2105, 2017.

\bibitem{WuSH16e}
Z.~Wu, C.~Shen, and A.~van~den Hengel.
\newblock Wider or deeper: Revisiting the resnet model for visual recognition.
\newblock {\em CoRR}, abs/1611.10080, 2016.

\bibitem{XiaoFXLYK16}
S.~Xiao, J.~Feng, J.~Xing, H.~Lai, S.~Yan, and A.~A. Kassim.
\newblock Robust facial landmark detection via recurrent attentive-refinement
  networks.
\newblock In {\em {ECCV}}, pages 57--72, 2016.

\bibitem{XiaoLZJS18}
T.~Xiao, Y.~Liu, B.~Zhou, Y.~Jiang, and J.~Sun.
\newblock Unified perceptual parsing for scene understanding.
\newblock In {\em {ECCV}}, pages 432--448, 2018.

\bibitem{XieT15}
S.~Xie and Z.~Tu.
\newblock Holistically-nested edge detection.
\newblock In {\em {ICCV}}, pages 1395--1403, 2015.

\bibitem{XiongT13}
X.~Xiong and F.~D. la~Torre.
\newblock Supervised descent method and its applications to face alignment.
\newblock In {\em {CVPR}}, pages 532--539. {IEEE} Computer Society, 2013.

\bibitem{OWS18}
D.~Xu, W.~Ouyang, X.~Wang, and N.~Sebe.
\newblock Pad-net: Multi-tasks guided prediction-and-distillation network for
  simultaneous depth estimation and scene parsing.
\newblock In {\em {CVPR}}, pages 675--684, 2018.

\bibitem{XuLWRBC18}
H.~Xu, X.~Lv, X.~Wang, Z.~Ren, N.~Bodla, and R.~Chellappa.
\newblock Deep regionlets for object detection.
\newblock In {\em {ECCV}}, pages 827--844, 2018.

\bibitem{YanLYL13}
J.~Yan, Z.~Lei, D.~Yi, and S.~Z. Li.
\newblock Learn to combine multiple hypotheses for accurate face alignment.
\newblock In {\em {ICCVW}}, pages 392--396, 2013.

\bibitem{YangLZ17}
J.~Yang, Q.~Liu, and K.~Zhang.
\newblock Stacked hourglass network for robust facial landmark localisation.
\newblock In {\em {CVPR}}, pages 2025--2033, 2017.

\bibitem{YangLLT16}
S.~Yang, P.~Luo, C.~C. Loy, and X.~Tang.
\newblock {WIDER} {FACE:} {A} face detection benchmark.
\newblock In {\em {CVPR}}, pages 5525--5533. {IEEE} Computer Society, 2016.

\bibitem{YuWPGYS182}
C.~Yu, J.~Wang, C.~Peng, C.~Gao, G.~Yu, and N.~Sang.
\newblock Bisenet: Bilateral segmentation network for real-time semantic
  segmentation.
\newblock In {\em {ECCV}}, pages 334--349, 2018.

\bibitem{YuWPGYS18}
C.~Yu, J.~Wang, C.~Peng, C.~Gao, G.~Yu, and N.~Sang.
\newblock Learning a discriminative feature network for semantic segmentation.
\newblock In {\em {CVPR}}, pages 1857--1866, 2018.

\bibitem{YuKF17}
F.~Yu, V.~Koltun, and T.~A. Funkhouser.
\newblock Dilated residual networks.
\newblock {\em CoRR}, abs/1705.09914, 2017.

\bibitem{YuHZYM13}
X.~Yu, J.~Huang, S.~Zhang, W.~Yan, and D.~N. Metaxas.
\newblock Pose-free facial landmark fitting via optimized part mixtures and
  cascaded deformable shape model.
\newblock In {\em {ICCV}}, pages 1944--1951. {IEEE} Computer Society, 2013.

\bibitem{0005DSZWTA18}
H.~Zhang, K.~J. Dana, J.~Shi, Z.~Zhang, X.~Wang, A.~Tyagi, and A.~Agrawal.
\newblock Context encoding for semantic segmentation.
\newblock In {\em {CVPR}}, pages 7151--7160, 2018.

\bibitem{ZhangKSC16}
J.~Zhang, M.~Kan, S.~Shan, and X.~Chen.
\newblock Occlusion-free face alignment: Deep regression networks coupled with
  de-corrupt autoencoders.
\newblock In {\em {CVPR}}, pages 3428--3437. {IEEE} Computer Society, 2016.

\bibitem{ZhangSKC14}
J.~Zhang, S.~Shan, M.~Kan, and X.~Chen.
\newblock Coarse-to-fine auto-encoder networks {(CFAN)} for real-time face
  alignment.
\newblock In {\em {ECCV} {(2)}}, volume 8690 of {\em Lecture Notes in Computer
  Science}, pages 1--16. Springer, 2014.

\bibitem{ZhangTZLY17}
R.~Zhang, S.~Tang, Y.~Zhang, J.~Li, and S.~Yan.
\newblock Scale-adaptive convolutions for scene parsing.
\newblock In {\em {ICCV}}, pages 2050--2058, 2017.

\bibitem{ZhangWBLL18}
S.~Zhang, L.~Wen, X.~Bian, Z.~Lei, and S.~Z. Li.
\newblock Single-shot refinement neural network for object detection.
\newblock In {\em {CVPR}}, pages 4203--4212, 2018.

\bibitem{ZhangQ0W17}
T.~Zhang, G.~Qi, B.~Xiao, and J.~Wang.
\newblock Interleaved group convolutions.
\newblock In {\em {ICCV}}, pages 4383--4392, 2017.

\bibitem{ZhangLLT14}
Z.~Zhang, P.~Luo, C.~C. Loy, and X.~Tang.
\newblock Facial landmark detection by deep multi-task learning.
\newblock In {\em {ECCV}}, pages 94--108, 2014.

\bibitem{ZhangQX0WY18}
Z.~Zhang, S.~Qiao, C.~Xie, W.~Shen, B.~Wang, and A.~L. Yuille.
\newblock Single-shot object detection with enriched semantics.
\newblock In {\em {CVPR}}, pages 5813--5821, 2018.

\bibitem{ZhangZPXS18}
Z.~Zhang, X.~Zhang, C.~Peng, X.~Xue, and J.~Sun.
\newblock Exfuse: Enhancing feature fusion for semantic segmentation.
\newblock In {\em {ECCV}}, pages 273--288, 2018.

\bibitem{ZhaoSQWJ17}
H.~Zhao, J.~Shi, X.~Qi, X.~Wang, and J.~Jia.
\newblock Pyramid scene parsing network.
\newblock In {\em {CVPR}}, pages 6230--6239, 2017.

\bibitem{ZhaoZLSLLJ18}
H.~Zhao, Y.~Zhang, S.~Liu, J.~Shi, C.~C. Loy, D.~Lin, and J.~Jia.
\newblock Psanet: Point-wise spatial attention network for scene parsing.
\newblock In {\em {ECCV}}, pages 270--286, 2018.

\bibitem{ZhaoLNZCWFY17}
J.~Zhao, J.~Li, X.~Nie, F.~Zhao, Y.~Chen, Z.~Wang, J.~Feng, and S.~Yan.
\newblock Self-supervised neural aggregation networks for human parsing.
\newblock In {\em {CVPRW}}, pages 1595--1603, 2017.

\bibitem{M2DETQ}
Q.~Zhao, T.~Sheng, Y.~Wang, Z.~Tang, Y.~Chen, L.~Cai, and H.~Ling.
\newblock M2det: {A} single-shot object detector based on multi-level feature
  pyramid network.
\newblock {\em CoRR}, abs/1811.04533, 2018.

\bibitem{ZhouFCJY13}
E.~Zhou, H.~Fan, Z.~Cao, Y.~Jiang, and Q.~Yin.
\newblock Extensive facial landmark localization with coarse-to-fine
  convolutional network cascade.
\newblock In {\em {ICCVW}}, pages 386--391, 2013.

\bibitem{ZhouNGHX18}
P.~Zhou, B.~Ni, C.~Geng, J.~Hu, and Y.~Xu.
\newblock Scale-transferrable object detection.
\newblock In {\em {CVPR}}, pages 528--537, 2018.

\bibitem{ZhouHZ15}
Y.~Zhou, X.~Hu, and B.~Zhang.
\newblock Interlinked convolutional neural networks for face parsing.
\newblock In {\em {ISNN}}, pages 222--231, 2015.

\bibitem{ZhouSTL18}
Z.~Zhou, M.~M.~R. Siddiquee, N.~Tajbakhsh, and J.~Liang.
\newblock Unet++: {A} nested u-net architecture for medical image segmentation.
\newblock In {\em {MICCAI}}, pages 3--11, 2018.

\bibitem{ZhuLLT15}
S.~Zhu, C.~Li, C.~C. Loy, and X.~Tang.
\newblock Face alignment by coarse-to-fine shape searching.
\newblock In {\em {CVPR}}, pages 4998--5006. {IEEE} Computer Society, 2015.

\bibitem{ZhuLLT16}
S.~Zhu, C.~Li, C.~C. Loy, and X.~Tang.
\newblock Unconstrained face alignment via cascaded compositional learning.
\newblock In {\em {CVPR}}, pages 3409--3417, 2016.

\bibitem{ZhuR12}
X.~Zhu and D.~Ramanan.
\newblock Face detection, pose estimation, and landmark localization in the
  wild.
\newblock In {\em {CVPR}}, pages 2879--2886. {IEEE} Computer Society, 2012.

\bibitem{ZhuZWZWL17}
Y.~Zhu, C.~Zhao, J.~Wang, X.~Zhao, Y.~Wu, and H.~Lu.
\newblock Couplenet: Coupling global structure with local parts for object
  detection.
\newblock In {\em {ICCV}}, pages 4146--4154, 2017.

\end{thebibliography}
}




\end{document} 