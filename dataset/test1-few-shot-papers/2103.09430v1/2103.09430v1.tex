

\documentclass{article}

\usepackage{booktabs}       \newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{url}            



\usepackage{amsmath,amsthm,amssymb,bm} \usepackage{xspace}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{times}
\usepackage{graphicx} \usepackage{sidecap}
\usepackage{wrapfig}
\usepackage[sort,numbers,square,comma]{natbib}
\usepackage{colortbl}
\usepackage{arydshln}
\usepackage{tikz}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{titletoc}
\usepackage{hyperref}

\newcommand{\reals}{\mathbf{R}}
\newcommand{\integers}{\mathbf{Z}}
\newcommand{\naturals}{\mathbf{N}}
\newcommand{\rationals}{\mathbf{Q}}

\newcommand{\calr}{\mathcal{R}} 

\newcommand{\ind}[1]{1_{#1}} \newcommand{\pr}{\mathbb{P}} \newcommand{\ex}{\mathbb{E}} \newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}
\newcommand{\sgn}{\textrm{sgn}}
\newcommand{\kl}{\textrm{KL}} 

\newcommand{\law}{\mathcal{L}}  \newcommand{\normal}{N} 

\newcommand{\convd}{\stackrel{d}{\longrightarrow}} \newcommand{\convp}{\stackrel{p}{\longrightarrow}} \newcommand{\convas}{\stackrel{\textrm{a.s.}}{\longrightarrow}} 

\newcommand{\eqd}{\stackrel{d}{=}} \newcommand{\conv}{\textrm{conv}} 

\newcommand{\inv}[1]{{#1}^{-1}}



\newcommand{\refalgo}[1]{Alg.~\ref{#1}}
\newcommand{\refsec}[1]{Sec.~\ref{#1}}
\newcommand{\reffig}[1]{Fig.~\ref{#1}}
\newcommand{\reftab}[1]{Tab.~\ref{#1}}
\newcommand{\refeq}[1]{Eq.~\ref{#1}}
\newcommand{\refdef}[1]{Def.~\ref{#1}}
\newcommand{\refthm}[1]{Thm.~\ref{#1}}
\newcommand{\reflem}[1]{Lemma~\ref{#1}}
\newcommand{\refcor}[1]{Corr.~\ref{#1}}

\newcommand{\bp}{\mathbb{P}}

\newcommand{\Ph}{\widehat{P}_k}
\newcommand{\PC}{P_{\cc,k}}
\newcommand{\Lc}{\widetilde{L}}
\newcommand{\Yt}{\widetilde{Y}}
\newcommand{\Pt}{\widetilde{P}}
\newcommand{\Oc}{\mathcal{O}}

\newcommand{\dist}{\kappa_{\Pi}} \newcommand{\pns}{\delta_{\Pi}} \newcommand{\epsm}{\varepsilon_{\Pi}} \newcommand{\nys}{Nystr\"om}

\newcommand{\leftgr}[1]{{#1}^{\text{lr}}} \newcommand{\rightgr}[1]{{#1}^{\text{rr}}} \newcommand{\lobatto}[1]{{#1}^{\text{lo}}} 

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\newcount\COMMENTs  \COMMENTs=1   \usepackage{color}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{purple}{rgb}{1,0,1}
\newcommand{\comm}[2]{\ifnum\COMMENTs=1\textcolor{#1}{#2}\fi}
\newcommand{\yx}[1]{\comm{blue}      {[YD: #1]}}
\newcommand{\jure}[1]{\comm{purple}      {[JL: #1]}}
\newcommand{\wh}[1]  {\comm{cyan}   {[WH: #1]}}
\newcommand{\mf}[1]{\textcolor{red}{[MF: #1]}}
\newcommand{\bl}[1]{\comm{orange}     {[BL: #1]}}
\newcommand{\michele}[1]{\textcolor{darkgreen}     {[MC: #1]}}
\newcommand{\hr}[1]{\comm{blue}      {[Hongyu: #1]}}

\newcommand{\xhdr}[1]{{\noindent\bfseries #1}.}

\definecolor{dkred}{rgb}{0.5,0,0}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  morekeywords={import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert},
  numbers=none,
  numberstyle=\tiny\color{blue},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  frame = single, 
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  literate={0}{{\textcolor{dkred}{0}}}{1}{1}{{\textcolor{dkred}{1}}}{1}{2}{{\textcolor{dkred}{2}}}{1}{3}{{\textcolor{dkred}{3}}}{1}{4}{{\textcolor{dkred}{4}}}{1}{5}{{\textcolor{dkred}{5}}}{1}{6}{{\textcolor{dkred}{6}}}{1}{7}{{\textcolor{dkred}{7}}}{1}{8}{{\textcolor{dkred}{8}}}{1}{9}{{\textcolor{dkred}{9}}}{1}{.0}{{\textcolor{dkred}{.0}}}{2}{.1}{{\textcolor{dkred}{.1}}}{2}{.2}{{\textcolor{dkred}{.2}}}{2}{.3}{{\textcolor{dkred}{.3}}}{2}{.4}{{\textcolor{dkred}{.4}}}{2}{.5}{{\textcolor{dkred}{.5}}}{2}{.6}{{\textcolor{dkred}{.6}}}{2}{.7}{{\textcolor{dkred}{.7}}}{2}{.8}{{\textcolor{dkred}{.8}}}{2}{.9}{{\textcolor{dkred}{.9}}}{2}}

\newcommand{\name}{Open Graph Benchmark}
\newcommand{\nameshort}{OGB} 
\newcommand{\challenge}{OGB Large-Scale Challenge}
\newcommand{\challengeshort}{OGB-LSC}
\newcommand{\ag}{\texttt{MAG240M-LSC}}
\newcommand{\wiki}{\texttt{WikiKG90M-LSC}}
\newcommand{\mol}{\texttt{PCQM4M-LSC}}

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\cf}{\textit{cf.}}

\newcommand{\dataset}[7]{
\xhdr{Practical relevance and dataset overview} #1

\xhdr{Graph} #2 

\xhdr{Prediction task and evaluation metric} #3

\xhdr{Dataset split} #4

\xhdr{Baseline} #5

\xhdr{Hyper-parameters} #6

\xhdr{Discussion} #7
}

\usepackage[accepted]{icml2021}

\icmltitlerunning{\challengeshort{}: A Large-Scale Challenge for ML on Graphs}

\begin{document}

\twocolumn[
\icmltitle{OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Weihua Hu}{st}
\icmlauthor{Matthias Fey}{do}
\icmlauthor{Hongyu Ren}{st}
\icmlauthor{Maho Nakata}{ri}
\icmlauthor{Yuxiao Dong}{fb}
\icmlauthor{Jure Leskovec}{st}
\end{icmlauthorlist}

\icmlaffiliation{st}{Stanford University}
\icmlaffiliation{do}{TU Dortmund University}
\icmlaffiliation{fb}{Facebook AI}
\icmlaffiliation{ri}{RIKEN}

\icmlcorrespondingauthor{OGB-LSC Team}{ogb-lsc@cs.stanford.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]









\printAffiliationsAndNotice{} 

\begin{abstract}
Enabling effective and efficient machine learning (ML) over large-scale graph data 
(\eg, graphs with billions of edges) 
can have a huge impact on both industrial and scientific applications.
However, community efforts to advance large-scale graph ML have been severely limited by the lack of a suitable public benchmark. 
For KDD Cup 2021, we present \challenge{} (\challengeshort{}), 
a collection of three real-world datasets 
for advancing the state-of-the-art in large-scale graph ML.
\challengeshort{} provides graph datasets that are orders of magnitude larger than existing ones and covers
three core graph learning tasks---link prediction, graph regression, and node classification. 
Furthermore, \challengeshort{} provides dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. 
We show that the expressive models significantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. 
Our datasets and baseline code are released and maintained as part of our \nameshort{} initiative~\citep{hu2020open}.
We hope \challengeshort{} at KDD Cup 2021 can empower the community to discover innovative solutions for large-scale graph ML.\footnote{\url{https://ogb.stanford.edu/kddcup2021/}}
\end{abstract}

\section{Introduction}
\label{sec:intro}
Machine Learning (ML) on graphs has attracted immense attention in recent years because of the prevalence of graph-structured data in real-world applications. 
Modern application domains include web-scale social networks~\citep{ugander2011anatomy}, recommender systems~\citep{ying2018graph}, hyperlinked web documents~\citep{kleinberg1999authoritative}, knowledge graphs (KGs)~\citep{bollacker2008freebase,vrandevcic2014wikidata}, as well as molecule simulation data generated by the ever-increasing scientific computation~\citep{nakata2017pubchemqc,chanussot2020open}. These domains involve large-scale graphs with billions of edges or a dataset with millions of graphs.
Deploying accurate graph ML at scale will have a huge practical impact, enabling better recommendation results, improved web document search, more comprehensive KGs, and accurate ML-based drug and material discovery.

However, community efforts to advance state-of-the-art in large-scale graph ML have been extremely limited.
In fact, most of graph ML models have been developed and evaluated on extremely small datasets~\citep{yang2016revisiting,morris2020tudataset,bordes2013translating}. 

\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{Basic statistics of the three \challengeshort{} datasets.}
    \centering
    \label{tab:stats}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cclr}
      \toprule
      \textbf{Task type} & \textbf{Dataset} & \mc{2}{c}{\textbf{Statistics}}  \\
      \midrule
        \mr{2}{Node-level} & \mr{2}{\textbf{\ag{}}} & \#nodes: & 244,160,499 \\
                           &                        & \#edges:& 1,728,364,232 \\
        \midrule
        \mr{2}{Link-level} & \mr{2}{\textbf{\wiki{}}} & \#nodes: & 87,143,637 \\
                           &                          & \#edges:& 504,220,369 \\
        \midrule
        \mr{2}{Graph-level} & \mr{2}{\textbf{\mol{}}} & \#graphs: & 3,803,453 \\
                            &    & \#edges (total): & 55,399,880 \\
      \bottomrule
    \end{tabular}
    }
\end{figure}

\begin{figure*}[t]
  \centering
  \hfill{}
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[height=4.3cm]{fig/ogb-lsc1.pdf}
    \caption{\ag{}}
  \end{subfigure}
  \hfill{}
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[height=4.3cm]{fig/ogb-lsc2.pdf}
    \caption{\wiki{}}
  \end{subfigure}
  \hfill{}
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[height=4.3cm]{fig/ogb-lsc3.pdf}
    \caption{\mol{}}
  \end{subfigure}
  \hfill{}
  \caption{\textbf{Overview of the three \challengeshort{} datasets, covering node-, link-, and graph-level prediction tasks.} \textbf{(a)} \ag{} is a heterogeneous academic graph, and the task is to predict the subject areas of papers (\cf~Section~\ref{subsec:node}) situated in the heterogeneous graph. 
      \textbf{(b)} \wiki{} is a knowledge graph, and the task is to impute missing triplets (\cf~Section~\ref{subsec:link}). \textbf{(c)} \mol{} is a quantum chemistry dataset, and the task is to predict an important molecular property, the HOMO-LUMO gap, of a given molecule (\cf~Section~\ref{subsec:graph}).}
  \label{fig:overview}
\end{figure*}

Handling large-scale graphs is challenging, especially for state-of-the-art expressive Graph Neural Networks (GNNs)~\citep{kipf2016semi,hamilton2017inductive,velivckovic2017graph} because they make prediction on each node based on the information from many other nodes. Effectively training these models at scale requires sophisticated algorithms that are well beyond standard SGD over i.i.d.~data~\citep{hamilton2017inductive,chen2018fastgcn,chiang2019cluster,zeng2019graphsaint}. 
More recently, researchers improve model scalability by significantly simplifying GNNs~\citep{wu2019simplifying,rossi2020sign,huang2020combining}, which inevitably limits their expressive power.

However, in deep learning, it has been demonstrated over and over again that one needs big expressive models and train them on big data to achieve the best performance~\citep{he2016deep,russakovsky2015imagenet,vaswani2017attention,devlin2018bert}. In graph ML, the trend has been the opposite---models get simplified and less expressive to be able to scale to large graphs. Thus, there is a massive opportunity to move the community to work with realistic and large-scale graph datasets and move the state of the field forward to where it needs to be.

For KDD Cup 2021, we present a large-scale graph ML competition, \challenge{} (\challengeshort{}), to encourage the development of state-of-the-art graph ML models for massive modern datasets. 
Specifically, we introduce three large-scale, realistic, and challenging datasets: \ag{}, \wiki{}, \mol{}, that are unprecedentedly large in scale (see Table~\ref{tab:stats}) and cover prediction at the level of nodes, links, and graphs, respectively.
An overview of the three \challengeshort{} datasets is provided in Figure~\ref{fig:overview}.
For each dataset, we carefully design its prediction task and data split so that achieving high prediction performance on the task will have direct impact on the corresponding application, as we detail in Section \ref{sec:problem}.

Beyond providing the datasets, we perform an extensive baseline analysis on each dataset, implementing simple baseline models as well as advanced expressive models at scale.
We find that advanced expressive models, despite requiring more efforts to scale up, do benefit from large data and significantly outperform simple baseline models that are easy to scale.
All of our baseline code is made publicly available to facilitate public research.\footnote{\url{https://github.com/snap-stanford/ogb/tree/master/examples/lsc}} 

Overall, \challengeshort{} will encourage the community to develop and scale up expressive graph ML models, and can yield breakthrough in their performance. 
We hope \challengeshort{} at KDD Cup 2021 will serve as an ``ImageNet Large Scale Visual Recognition Challenge''~\citep{russakovsky2015imagenet} in the field of graph ML, encouraging the community to work on the realistic and large-scale graph datasets and significantly advance the state-of-the-art.

 
\section{\challengeshort{} Datasets}
\label{sec:problem}
W describe the three datasets in \challengeshort{}, covering three key task categories (node-, link-, and graph-level prediction tasks) of ML on graphs.

We particularly emphasize the practical relevance and data split for each dataset, making our task closely aligned to realistic applications. Through our extensive baseline analysis, we show that advanced expressive models tend to give much better performance than simple graph ML models, leaving room for further improvement.

\subsection{\ag{}: Node-Level Prediction}
\label{subsec:node}
\dataset{
The volume of scientific publication has been increasing exponentially, doubling every 12 years~\cite{dong2017century}.
Currently, subject areas of \textsc{arXiv} papers are manually determined by the paper's authors and \textsc{arXiv} moderators.
An accurate automatic predictor of papers' subject categories not only reduces the significant burden of manual labeling, but can also be used to classify the vast number of non-\textsc{arXiv} papers, thereby allowing better search and organization of academic papers.

\ag{} is a heterogeneous academic graph extracted from the Microsoft Academic Graph (MAG)~\citep{wang2020mag}.
Given arXiv papers situated in the heterogeneous graph, whose schema diagram is illustrated in Figure~\ref{fig:mag_schema}, we aim to automatically annotating their topics, \ie, predicting the primary subject area of each \textsc{arXiv} paper.
}
{
We extract 121M academic papers in English from MAG (version: 2020-11-23) to construct a heterogeneous academic graph. The resultant paper set is written by 122M author entities, who are affiliated with 26K institutes. Among these papers, there are 1.3 billion citation links captured by MAG. 
Each paper is associated with its natural language title and most papers' abstracts are also available. 
We concatenate the title and abstract by period and pass it to a \textsc{RoBERTa} sentence encoder~\citep{liu2019roberta,reimers-2019-sentence-bert}, generating a 768-dimensional vector for each paper node. 
Among the 121M paper nodes, approximately 1.4M nodes are \textsc{arXiv} papers annotated with 153 \textsc{arXiv} subject areas, \eg, \texttt{cs.LG~(Machine Learning)}.
}
{
The task is to predict the primary subject areas of the given \textsc{arXiv} papers, which is cast as an ordinary multi-class classification problem. The metric is the classification accuracy.

To understand the relation between the prediction task and the heterogeneous graph structure, we analyze the graph homophily~\citep{mcpherson2001birds}---tendency of two adjacent nodes to share the same labels---to better understand the interplay between heterogeneous graph connectivity and the prediction task.
Homophily is normally analyzed over a homogeneous graph, but we extend the analysis to the heterogenous graph by considering meta-paths~\citep{sun2011pathsim}---a path consisting of a sequence of relations defined between different node types.
Given a meta-path, we can say two nodes are adjacent if they are connected by the meta-path.
Table \ref{tab:analysis_metapath} shows the homophily for different kinds of meta-paths with different levels of connection strength.
Compared to the direct citation connection (\ie, P-P), certain meta-paths (\ie, P-A-P) give rise to much higher degrees of homophiliness, while other meta-paths (\ie, P-A-I-A-P) provide much less homophily.
As homophily is the central graph property exploited by many graph ML models, we believe that discovering essential heterogeneous connectivity is important to achieve good performance on this dataset.

\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{\textbf{Analysis of graph homophily for different meta-paths connecting 1,251,341 arXiv papers (only train+validation).} Connection strength indicates the number of different possible paths along the template meta-path, \eg, meta-path ``Paper-Author-Paper (P-A-P)'' with connection strength 3 means that at least 3 authors are shared for the two papers of interest. Homophily ratio is the ratio of two nodes having the same target labels. }
    \centering
    \label{tab:analysis_metapath}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrrr}
      \toprule
    \mr{2}{\textbf{Meta-path}} & \textbf{Connect.} & \textbf{Homophily} & \mr{2}{\textbf{\#Edges}}  \\
      & \textbf{strength} & \textbf{ratio (\%)} &   \\
      \midrule
      P-P & 1 & 57.80 & 2,017,844 \\
      \midrule
        \mr{5}{P-A-P} & 1 & 46.12 & 88,099,071 \\
         & 2 & 57.02 & 12,557,765  \\
         & 4 & 64.03 & 1,970,761 \\
         & 8 & 66.65 & 476,792 \\
         & 16 & 70.46 & 189,493 \\
         \midrule
        \mr{5}{P-A-I-A-P} & 1 & 3.83 & 159,884,165,669 \\
         & 2 & 4.61 & 81,949,449,717 \\
         & 4 & 5.69 & 33,764,809,381 \\
         & 8 & 6.85 & 12,390,929,118 \\
         & 16 & 7.70 & 4,471,932,097 \\
         \midrule
         All pairs & 0 & 1.99 & 782,926,523,470 \\
      \bottomrule
    \end{tabular}
    }
\end{figure}
}
{
We split the data according to time. Specifically, we train models on \textsc{arXiv} papers published until 2018, validate the performance on the 2019 papers, and finally test the performance on the 2020 papers. The split reflects the  practical scenario of helping the authors and moderators annotate the subject areas of the newly-published \textsc{arXiv} papers.

}
{
We consider a broad range of graph ML models for our initial benchmarking efforts in both homogeneous (where only paper to paper relations are considered) and full heterogeneous settings.
For both settings, we convert the directed graph into an undirected graph for simplicity.
First, for the homogeneous setting, we benchmark the simple baseline models: graph-agnostic MLP, Label Propagation, and the recently-proposed simplified graph methods: \textsc{SGC} \citep{wu2019simplifying}, \textsc{SIGN} \citep{rossi2020sign} and \textsc{MLP+C\&S} \citep{huang2020combining}, which are inherently scalable by decoupling predictions from propagation.
Furthermore, we benchmark state-of-the-art expressive GNNs trained with neighborhood sampling (\textsc{NS})~\citep{hamilton2017inductive}, where we recursively sample 25 neighbors in the first layer and 15 neighbors in the second layer during training time. At inference time, we sample at most 160 neighbors for each layer.
Here, we benchmark two types of strong models: the \textsc{GraphSAGE} \citep{hamilton2017inductive} model (performing mean aggregation and utilizing skip-connections), and the more advanced \textsc{Graph Attention Network (GAT)} model \citep{velivckovic2017graph}.
For the full heterogeneous setting, we follow \citet{schlichtkrull2018modeling} and learn distinct weights for each individual relation type (denoted by \textsc{R-GraphSAGE} and \textsc{R-GAT}, where ``\text{R}'' stands for ``Relational''). 
We obtain the input features of authors and institutions by averaging the features of papers belonging to the same author and institution, respectively.
The models are trained with \textsc{NS}.
We note that the expressive GNNs trained with \textsc{NS} require more efforts to scale up, but are more expressive than the simple baselines.
}
{
Hyper-parameters are selected based on their best validation performance.
For all the models without NS, we tuned the hidden dimensionality , MLP depth , dropout ratio , propagation layers (for \textsc{SGC}, \textsc{SIGN}, and \textsc{C\&S}) .
For all the GNN models with NS, we use a hidden dimensionality of 1024.
We make use of batch normalization \citep{ioffe2015batch} and ReLU activation in all models.
All models were trained using the Adam optimizer \citep{kingma2014adam}.
}
{
Validation and test performances of all models considered are shown in Table~\ref{tab:results_mag}.
First, the graph-agnostic MLP and Label Propagation algorithm perform poorly, indicating that both graph structure and feature information are indeed important for the given task.
Across the graph ML models operating on the homogeneous paper graph, GNNs with NS perform the best, with slight gains compared to their simplified versions.
In particular, the advanced expressive graph attention aggregation is favourable compared to the uniform mean aggregation in \textsc{GraphSAGE}.
Furthermore, considering all available heterogeneous relational structure in the heterogeneous graph setting yields significant improvements, with performance gains up to 3 percentage points. 
Again, the advanced attention aggregation provides favorable performance.

Overall, our experiments highlight the benefits of developing and evaluating advanced expressive models on the larger scale.
However, due to the exponential neighbor expansion in \textsc{NS}, we were only able to train the expressive GNNs up to two layers, while the other simplified models utilize three-layer propagation to achieve their best performance.
It is an interesting direction to explore how to train deeper and even more expressive GNNs and whether they can yield further improvements.
}

\begin{figure}
      \centering
      \includegraphics[width=0.9\linewidth]{fig/mag_schema.pdf}
\caption{\textbf{A schema diagram of \ag{}.} Paper nodes have text features and publication year information.
      }
      \label{fig:mag_schema}
\end{figure}

\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{\textbf{Results of \ag{} measured by the accuracy.} \textsc{R-GraphSAGE} and \textsc{R-GAT} utilize the full heterogeneous graph information, while the other models operate on the homogeneous paper citation graph.}
    \centering
    \label{tab:results_mag}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrcc}
      \toprule
    \textbf{Model} & \textbf{\#Params} & \textbf{Validation} & \textbf{Test}  \\
      \midrule
        \textsc{MLP} & 0.5M & 52.67 & 52.73 \\
        \textsc{LabelProp} & 0 & 58.44 & 56.29 \\
        \textsc{SGC} & 0.7M & 65.82 & 65.29 \\
        \textsc{SIGN} & 3.8M & 66.64 & 66.09 \\
        \textsc{MLP+C\&S} & 0.5M & 66.98 & 66.18 \\
        \textsc{GraphSAGE (NS)} & 4.9M & 67.32 & 66.25 \\
        \textsc{GAT (NS)} & 4.9M & 67.71 & 66.63 \\
        \midrule
        \textsc{R-GraphSAGE (NS)} & 12.2M & 70.21 & 68.94 \\
        \textsc{R-GAT (NS)} & 12.3M & \textbf{70.48} & \textbf{69.49} \\
      \bottomrule
    \end{tabular}
    }
\end{figure}
 

\subsection{\wiki{}: Link-Level Prediction}
\label{subsec:link}
\dataset{
Large encyclopedic Knowledge Graphs (KGs), such as Wikidata~\citep{vrandevcic2014wikidata} and Freebase~\citep{bollacker2008freebase}, represent factual knowledge about the world through triplets connecting different entities, \eg, {\it Hinton  Canada}. They provide rich structured information about many entities, aiding a variety of knowledge-intensive downstream applications such as information retrieval, question answering~\citep{singhal2012introducing}, and recommender systems~\citep{guo2020survey}.
However, these large KGs are known to be far from complete~\citep{min2013distant}, missing many relational information between entities.
Using ML to automatically impute missing triplets significantly reduces the manual curation of knowledge and provides a more comprehensive KG, which in turn improves the aforementioned downstream applications.

\wiki{} is a Knowledge Graph (KG) extracted from the \emph{entire} Wikidata knowledge base. The task is to automatically impute missing triplets that are not yet present in the current KG. Accurate imputation models can be readily deployed on the Wikidata to improve its coverage.
}
{
Each triplet (\textmd{head}, \textmd{relation}, \textmd{tail}) in \wiki{} represents an Wikidata claim, where \textmd{head} and \textmd{tail} are the Wikidata items, and \textmd{relation} is the Wikidata predicate. We extracted triplets from the public Wikidata dump downloaded at three time-stamps: September 28, October 26, and November 23 of 2020, for training, validation, and testing, respectively.
We retain all the entities and relations in the September dump, resulting in 87,143,637 entities, 1,315 relations, and 504,220,369 triplets in total.

\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{\textbf{Analysis of how entities in training and validation triplets connect to the training KG in the \wiki{} dataset.} For head (resp.~tail) in each triplet, we count the number of training triplets (excluding the triplet of interest) that also contain the entity as head (resp.~tail). 
    A large (resp.~small) count value implies that the triplet is densely (resp.~sparsely) connected to the training KG.
    As the summary statistics over the training and validation triplets, we report the average and the Proportion below Threshold (PbT), \ie, percentage of triplets whose counts are below a threshold.}
    \centering
    \label{tab:analysis_wikikg}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrrrr}
      \toprule
    \mr{2}{\textbf{Split}}  & \mc{4}{c}{\textbf{Connectivity of triplets to training KG (head)}} \\
     & \textbf{Avg} & \textbf{PbT} & \textbf{PbT} & \textbf{PbT}  \\
    \midrule
        Train & 28.0 & 43.4\% & 24.6\% & 8.8\% \\
        Validation & 6.5 & 74.6\% & 48.5\% & 32.1\%  \\
    \midrule
        \mr{2}{\textbf{Split}}  & \mc{4}{c}{\textbf{Connectivity of triplets to training KG (tail)}} \\
     & \textbf{Avg} & \textbf{PbT} & \textbf{PbT} & \textbf{PbT}  \\
    \midrule
        Train & 3,394,933 & 21.2\% & 12.3\% & 7.8\% \\
        Validation & 341,743 & 28.9\% & 24.3\% & 21.0\% \\
      \bottomrule
    \end{tabular}
    }
\end{figure}


\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{\textbf{Textual representation of validation triplets whose head entities only \emph{appear once} as head in the training KG.}}
    \centering
    \label{tab:triplet_wikikg}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lll}
      \toprule
      \textbf{Head} & \textbf{Relation} & \textbf{Tail} \\
    \midrule
      Food and drink companies of Bulgaria & combines topics & Bulgaria \\
      Performing arts in Denmark & combines topics & performing arts \\
      Anglicanism in Grenada & combines topics & Anglicanism \\
      Chuan Li & occupation & researcher \\
      Petra Junkova & given name & Petra \\
      \bottomrule
    \end{tabular}
    }
\end{figure}

In addition to extracting triplets, we provide text features for entities and relations. Specifically, each entity/relation in Wikidata is associated with a title and a short description, \eg, one entity is associated with the title `Geoffrey Hinton` and the description `computer scientist and psychologist`.
Similar to \ag{}, we provide \textsc{RoBERTa} embeddings~\citep{reimers-2019-sentence-bert,liu2019roberta} as node and edge features.\footnote{We concatenate the title and description with comma, \eg, `Geoffrey Hinton, computer scientist and psychologist`, and pass the sentence to a \textsc{RoBERTa} sentence encoder (Note that the \textsc{RoBERTa} model was trained before September 2020, so there is no obvious information leak). 
The title or/and description are sometimes missing, in which case we simply use the blank sentence to replace it.}

}
{
The task is the KG completion, \ie, given a set of training  triplets, predict a set of test triplets. 
For evaluation, we follow the protocol similar to how KG completion is evaluated~\citep{bordes2013translating}.\footnote{One difference compared to the standard KG completion evaluation~\citep{bordes2013translating} is that we evaluate the corruption of only \textmd{tail} entities. This is because in Wikidata, (\textmd{head}, \textmd{relation}, \textmd{tail}) represents a particular claim about the \textmd{head} entity. Hence, predicting the tail from the head and relation, \eg, (\textmd{Geoffrey Hinton}, \textmd{citizen of}, \textmd{?}), is often more natural and useful than predicting head entity from the tail and relation, \eg, (\textmd{?}, \textmd{citizen of}, \textmd{Canada}). }
Specifically, for each validation/test triplet, (\textmd{head}, \textmd{relation}, \textmd{tail}), we corrupt \textmd{tail} with randomly-sampled 1000 negative entities, \eg,~\textmd{tail\_neg}, such that (\textmd{head}, \textmd{relation}, \textmd{tail\_neg}) does not appear in the train/validation/test KG. 
The model is asked to rank the 1001 candidates (consisting of 1 positive and 1000 negatives) for each triplet and predict the top 10 entities that are most likely to be positive. The goal is to rank the ground-truth positive entity as high in the rank as possible, which is measured by Mean Reciprocal Rank (MRR).~\footnote{Note that this is more strict than the standard MRR since there is no partial score for positive entities being ranked outside of top 10.}
} 
{
We split the triplets according to time, simulating a realistic KG completion scenario of imputing missing triplets not present at a certain timestamp.
Specifically, we construct three KGs using the aforementioned September, October, and November KGs,  where we only retain entities and relation types that appear in the earliest September KG.
We use the triplets in the September KG for training, and use the additional triplets in the October and November KGs for validation and test, respectively.

We analyze the time split in Table \ref{tab:analysis_wikikg}.
We observe that head entities of validation triplets are much less well-connected to the training KG compared to training triplets, which is expected since new triplets tend to be added for less popular entities. The sparse connectivity to the training KG suggests that learning signals for predicting validation (and test) triplets are extremely sparse.
Nonetheless, even for the sparsely-connected triplets, we find the textual information provides important clues, as illustrated in Table \ref{tab:triplet_wikikg}.
Hence, we expect that advanced graph models that effectively incorporate textual information will be key to achieve good performance on the challenging time split.
}
{
We consider two representative KG embedding models: \textsc{TransE}~\cite{bordes2013translating} and \textsc{ComplEx}~\cite{trouillon2016complex}.
These models define their own \emph{decoders} to score knowledge triplets using the corresponding entity and relation embeddings. For instance, \textsc{TransE} uses  as the decoder, where , , and  are embeddings of head, relation, and tail, respectively.
For the encoder function (mapping each entity and relation to its embedding), we consider the following three options.
\textbf{Shallow:} We use the distinct embedding for each entity and relation, as normally done in KG embedding models.
\textbf{RoBERTa:} We use two MLP encoders (one for entity and another for relation) that transform the \textsc{RoBERTa} features into entity and relation embeddings. This method has an extremely small number of learnable parameters since the MLPs are shared across all entities and relations. 
\textbf{Concat:} To enhance the expressive power of the previous encoder, we augment it with shallow embeddings.
Specifically, we concatenate the shallow learnable embeddings into the \textsc{RoBERTa} features, and use the MLPs to transform the concatenated vectors to get the final embeddings. This way, the MLP encoders can adaptively utilize the \textsc{RoBERTa} features and the shallow embeddings to fit the large amount of triplet data.
To implement our baselines, we utilize DGL-KE~\cite{zheng2020dgl}.
}
{
For the loss function, we use the negative sampling loss from \citet{sun2019rotate}, where we pick margin  from \{1,4,8,10,100\}. In order to balance the performance and the memory cost, we use the embedding dimensionality of 200 for all the models. The learning rate is chose from \{0.01, 0.1\} and we train all models for 1M steps.
}
{
 Table \ref{tab:results_wiki} shows the validation and test performance of the six different models, \ie, combination of two decoders (\textsc{TransE} and \textsc{ComplEx}) and three encoders (\textsc{Shallow}, \textsc{RoBERTa}, and \textsc{Concat}). Notably, in terms of the encoders, we see that the most expressive \textsc{Concat} outperforms both \textsc{Shallow} and \textsc{RoBERTa}, indicating that both the textual information (captured by the \textsc{RoBERTa} embeddings) and structural information (captured by node-wise learnable embeddings) are useful in predicting validation and test triplets. In terms of the decoders, \textsc{TransE} and \textsc{ComplEx} show similar performance with the \textsc{Concat} encoder, while they show somewhat mixed results with the \textsc{Shallow} and \textsc{RoBERTa} encoders.
 
Overall, our experiments suggest that the expressive encoder that combines both textual information and structural information gives the most promising performance.
In the KG completion literature, the design of the encoder has been much less studied compared to the decoder designs. Therefore, we believe there is a huge opportunity in scaling up more advanced encoders, especially GNNs~\citep{schlichtkrull2018modeling}, to further improve the performance on this dataset.
}

\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{\textbf{Results of \wiki{} measured by Mean Reciprocal Rank (MRR).}}
    \centering
    \label{tab:results_wiki}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrcc}
      \toprule
    \textbf{Model} & \textbf{\#Params} & \textbf{Validation} & \textbf{Test}  \\
      \midrule
        \textsc{TransE-Shallow} & 17.4B & 0.7559 & 0.7412 \\
        \textsc{ComplEx-Shallow} & 17.4B & 0.6142 & 0.5883 \\
        \textsc{TransE-RoBERTa} & 0.3M & 0.6039 & 0.6288 \\
        \textsc{ComplEx-RoBERTa} & 0.3M & 0.7052 & 0.7186 \\
        \textsc{TransE-Concat} & 17.4B & \textbf{0.8494} & 0.8548 \\
        \textsc{ComplEx-Concat} & 17.4B & 0.8425 & \textbf{0.8637} \\
      \bottomrule
    \end{tabular}
    }
\end{figure}
 
\subsection{\mol{}: Graph-Level Prediction}
\label{subsec:graph}
\dataset{
Density Functional Theory (DFT) is a powerful and widely-used quantum physics calculation that can accurately predict various molecular properties such as the shape of molecules, reactivity, responses by electromagnetic fields~\citep{helgaker2014molecular}. 
However, DFT is time-consuming and takes up to several hours per small molecule.
Using fast and accurate ML models to approximate DFT enables diverse downstream applications, such as property prediction for organic photovaltaic devices~\citep{OPV2014} and structure-based virtual screening for drug discovery~\citep{molecules200713384}.

\mol{} is a quantum chemistry dataset originally curated under the PubChemQC project~\citep{nakata2015pubchemqc,nakata2017pubchemqc}.
Based on the PubChemQC, we define a meaningful ML task of predicting DFT-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs.
The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties of molecules since it is related to reactivity, photoexcitation, and charge transport~\citep{griffith1957ligand}.
Moreover, predicting the quantum chemical property only from 2D molecular graphs without their 3D equilibrium structures is also practically favorable. This is because obtaining 3D equilibrium structures requires DFT-based geometry optimization, which is expensive on its own.
}
{
We provide molecules as the SMILES strings~\citep{weininger1988smiles}, from which 2D molecule graphs (nodes are atoms and edges are chemical bonds) as well as molecular fingerprints (hand-engineered molecular feature developed by the chemistry community) can be obtained. 
By default, we follow \nameshort{}~\citep{hu2020open} to convert the SMILES string into a molecular graph representation, where each node is associated with a 9-dimensional feature (\eg, atomic number, chirality) and each edge comes with a 3-dimensional feature (\eg, bond type, bond stereochemistry).
\citet{hu2020open} show that using the rich graph features beyond the atomic number and the bond type is often effective in improving generalization capability of GNNs, although the optimal set of input graph features remains to be explored.
}
{
The task is graph regression: predicting the HOMO-LUMO energy gap in electronvolt (eV) given 2D molecular graphs. Mean Absolute Error (MAE) is used as evaluation metric.
}
{
Following \nameshort{}, we adopt the scaffold split with ratio 80/10/10~\citep{wu2018moleculenet}. Specifically, we split the molecules based on their 2D structural frameworks, resulting in validation and test molecules that are structurally very different from training ones.
Prediction over out-of-distribution molecular structure is commonplace in ML-based virtual screening. This is because training molecules represent an extremely tiny and biased subset of the entire chemical space (estimated to be around  molecules)~\citep{reymond2012exploring}, to which fast ML models are applied for virtual screening.
}
{
We benchmark two types of models: a simple MLP over the Morgan fingerprint~\citep{morgan1965generation} and more advanced GNN models.
For GNNs, we use the four strong models developed for graph-level prediction: Graph Convolutional Network (GCN)~\citep{kipf2016semi} and Graph Isomorphism Network (GIN)~\citep{xu2018how}, as well as their variants, \textsc{GCN-virtual} and \textsc{GIN-virtual}, which augment graphs with a virtual node that is bidirectionally connected to all nodes in the original graph~\citep{gilmer2017neural}.
Adding the virtual node is shown to be effective across a wide range of graph-level prediction datasets in \nameshort{}~\citep{hu2020open}.
Edge features are incorporated following \citet{hu2020pretraining}. At inference time, the model output is clamped between 0 and 50.\footnote{These values are set because HOMO/LUMO gap is always positive and the maximum target value in train and validation sets is 47.02. The clamping is crucial to avoid model's anomalously large/small prediction on a small number molecules that skews the entire result.}
}
{
For the MLP over Morgan fingerprint, we set the fingerprint dimensionality to be 2048, and tune the fingerprint radius , as well as MLP's hyper-parameters: hidden dimensionality , number of hidden layers , and dropout ratio .
For GNNs, we tune hidden dimensionality, \ie, width , number of GNN layers, \ie, depth . Simple summation is used for graph-level pooling and dropout rate is set to 0. 
For all MLPs (including \textsc{GIN}'s), we use batch normalization~\citep{ioffe2015batch} and ReLU activation.
For all models, we use the Adam optimizer~\citep{kingma2014adam}, batch size of 256, and train models for 100 epochs. The initial learning rate is set to 0.001, which is multiplied by 0.25 every 30 epochs. 
For our ablation study using only 10\% of training data (hence, 1 epoch is 10 times shorter), we train models for 1000 epochs and anneal the learning rate every 300 epochs.
}
{
The validation and test results are shown in Table \ref{tab:results_mol}. 
We see both the GNN models significantly outperform the simple fingerprint baseline.
Expressive GNNs (\textsc{GIN} and \textsc{GIN-virtual}) outperform less expressive ones (\textsc{GCN} and \textsc{GCN-virtual}); especially, the most advanced and expressive \textsc{GIN-virtual} model significantly outperforms the other GNNs.
Nonetheless, the current performance is still much worse than the chemical accuracy of 0.043eV---an indicator of practical usefulness established by the chemistry community.

In the same Table \ref{tab:results_mol}, we show our ablation, where we use only 10\% of data to train the \textsc{GIN-virtual} model. We see the performance significantly deteriorate, indicating the importance of training the model on large data.

Finally, in Table \ref{tab:results_mol_modelsize}, we show the relation between model sizes and validation performance. 
We see that the largest models always achieve the best performance. In fact, we observe the same trend for the other GNN models, as well as \textsc{GIN-virtual} trained on only 10\% of training data.

Overall, we find that advanced, expressive, and large GNN model gives the most promising performance on the \mol{} dataset, although the performance still needs to be improved for practical use.
We believe further advances in advanced modeling, expressive architectures, and larger model sizes could yield breakthrough in large-scale molecular property prediction---the holy-grail in graph-level ML tasks.
}

\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{\textbf{Results of \mol{} measured by MAE [eV].} The lower, the better. Ablation study of using only 10\% of training data is also shown. Chemical accuracy indicates the final goal for practical usefulness.}
    \centering
    \label{tab:results_mol}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrclcc}
      \toprule
    \textbf{Model} & \textbf{\#Params} & \textbf{Validation} & \textbf{Test}  \\
    \midrule
        \textsc{MLP-fingerprint} & 16.1M & 0.2044 & 0.2068 \\
\textsc{GCN}& 2.0M & 0.1684 & 0.1838 \\
        \textsc{GCN-virtual} & 4.9M & 0.1510 & 0.1579 \\
        \textsc{GIN}& 3.8M & 0.1536 & 0.1678 \\
        \textsc{GIN-virtual} & 6.7M & \textbf{0.1396} & \textbf{0.1487} \\
    \midrule
        \textsc{MLP-fingerprint} (10\% train) & 6.8M  & 0.2708 & 0.2657 \\
\textsc{GIN-virtual} (10\% train) & 6.7M  & 0.1790 & 0.1887 \\
    \midrule
    Chemical accuracy (goal) & -- & -- & 0.0430 \\
      \bottomrule
    \end{tabular}
    }
\end{figure}


\begin{figure}[t]
    \captionsetup{justification=centering}
    \captionof{table}{\textbf{Model size and the MAE performance [eV].} For both models, the width indicates the hidden dimensionality. For \textsc{GIN-virtual}, the depth represents the number of GNN layers, while for the \textsc{MLP-fingerprint}, the depth represents the the number of hidden layers in MLP. }
    \centering
    \label{tab:results_mol_modelsize}
    \renewcommand{\arraystretch}{1.1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccrc}
      \toprule
    \textbf{Model} & \textbf{Width} & \textbf{Depth} & \textbf{\#Params} & \textbf{Validation}   \\
    \midrule
        \mr{4}{\textsc{MLP-fingerprint}} 
        & \textbf{1600} & \textbf{6} & 16.1M & \textbf{0.2044} \\
          & \textbf{1600} & \textbf{4} & 11.0M  & \textbf{0.2044} \\
          & 1600 & 2 & 5.8M & 0.2220 \\
          & 1200 & 6 & 9.7M & 0.2083 \\
          
    \midrule
\mr{5}{\textsc{GIN-virtual}} & \textbf{600} & \textbf{5} & 6.7M & \textbf{0.1410} \\
          & 600 & 3 & 3.7M & 0.1462 \\
          & 300 & 5 & 1.7M & 0.1442 \\
          & 300 & 3 & 1.0M & 0.1512 \\
      \bottomrule
    \end{tabular}
    }
\end{figure} 

\section{\challengeshort{} Python Package}
\label{sec:python}
To make our datasets easily accessible, we have integrated them into the \nameshort{} Python package.\footnote{\url{https://github.com/snap-stanford/ogb}}
As shown in the Code Snippet 1, this allows our large datasets to be downloaded and processed with a few lines of code. 
In addition, for each dataset, we provide the evaluator object that evaluates and saves model's predictions (on the hidden test set) in a standardized way, making the test submission easy and reliable.
In our website,\footnote{\url{https://ogb.stanford.edu/kddcup2021/}} we provide the detailed tutorial of the package usage for each dataset.

\begin{figure}[t]
\begin{lstlisting}[title={Code Snippet 1: \textbf{OGB Python package}},captionpos=b]
  ### Here we focus on MAG240M-LSC.
  ### Dataset object
  >>> from ogb.lsc import MAG240MDatase
  >>> dataset = MAG240MDataset()
  # Get split
  >>> dataset.get_idx_split('train')
  # Get feature
  >>> dataset.paper_feat

  ### Evaluator object
  >>> from ogb.lsc import MAG240MEvaluator
  >>> evaluator = MAG240MEvaluator()
  # standardized evaluation
  >>> evaluator.eval(...)
  # saving the test prediction
  >>> evaluator.save_test_submission(...)
\end{lstlisting}
\end{figure} 
\section{Conclusion}
\label{sec:conclusion}
Modern applications of graph ML involve large-scale graph data with billions of edges or millions of graphs.
ML advances on large graph data have been limited due to the lack of a suitable benchmark.
For KDD Cup 2021, we present \challengeshort{}, with the goal of advancing state-of-the-art in large-scale graph ML. 
\challengeshort{} provides the three large-scale realistic benchmark datasets, covering the core graph ML tasks of node classification, link prediction, and graph regression.
We perform dedicated baseline analysis, scaling up advanced graph models to large graphs.
We show that advanced and expressive models can significantly outperform simpler baseline models, suggesting opportunities for further dedicated effort to yield even better performance.
We hope \challengeshort{} facilitates dedicated community efforts to tackle the important but challenging problem of large-scale graph ML. 
\section*{Acknowledgement}
We thank Michele Catasta for helpful discussion, Shigeru Maya for motivating the project, Adrijan Bradaschia for setting up the server for the project, and Amit Bleiweiss and Benjamin Braun for providing helpful feedback on our baseline code. 

Stanford University is supported by DARPA under Nos. N660011924033 (MCS);
ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP);
NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID);
Stanford Data Science Initiative, 
Wu Tsai Neurosciences Institute,
Chan Zuckerberg Biohub,
Amazon, JPMorgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, Intel, and UnitedHealth Group. 
Weihua Hu is supported by Funai Overseas Scholarship and Masason Foundation Fellowship.
Matthias Fey is supported by the German Research Association (DFG) within the Collaborative Research Center SFB 876 ``Providing Information by Resource-Constrained Analysis'', project A6.
Hongyu Ren is supported by Masason Foundation Fellowship and Apple PhD Fellowship.
Jure Leskovec is a Chan Zuckerberg Biohub investigator.

Our baseline code and Python package are built on top of excellent open-source software, including \textsc{Numpy}~\citep{harris2020array}, \textsc{Pytorch}~\citep{paszke2017automatic}, \textsc{Pytorch Geometric}~\citep{fey2019fast}, \textsc{DGL}~\citep{wang2019dgl}, and \textsc{DGL-KE}~\citep{zheng2020dgl}.

The HOKUSAI facility was used to perform some of the quantum calculations. This work was supported by the Japan Society for the Promotion of Science (JSPS KAKENHI Grant no. 18H03206). We are also grateful to Maeda Toshiyuki for helpful discussions.



\bibliography{reference}
\bibliographystyle{icml2021}

\end{document}
