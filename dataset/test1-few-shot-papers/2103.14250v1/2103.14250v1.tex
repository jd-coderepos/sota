



\documentclass[final,5p,times,twocolumn]{elsarticle}

\usepackage{graphicx}
\usepackage{amssymb}


\usepackage{lineno}

 
\usepackage{amssymb}


\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{algorithmic}

\usepackage{algorithm2e} 
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{verbatim}
\usepackage{color,soul}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{tabularx,ragged2e,booktabs,caption,array,multirow,multicol}
\usepackage{csquotes}
\usepackage{mathtools}
\usepackage{lineno}
\usepackage{amsthm}  
\usepackage{listings}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage{float}
\usepackage{xspace} 
\usepackage{caption}
\usepackage{float}
\usepackage{booktabs} 
\usepackage{siunitx} \usepackage{pgfplotstable} \usepackage{csvsimple}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{longtable}
\usepackage{multicol}
\usepackage{url}




\sisetup{
  round-mode          = places, round-precision     = 5, }
\journal{-TBA}

\begin{document}

\begin{frontmatter}



\title{ Evaluation of deep  learning models for multi-step ahead time series prediction}







\author[ctds,geo]{Rohitash Chandra}
\ead{rohitash.chandra@unsw.edu.au} 

\author[iitd]{Shaurya Goyal} 


\author[iitk]{Rishabh Gupta} 




\address[ctds]{School of Mathematics and Statistics, UNSW  Sydney, 
NSW 2006, Australia} 

\address[iitd]{Department of Mathematics, Indian Institute of Technology, Delhi, 110016 , India}
 
\address[iitk]{Department of Geology and Geophysics, Indian Institute of Technology,Kharagpur, 721302 , India}

\begin{abstract}
Time series prediction with neural networks have been focus of much research in the past few decades. Given the recent deep learning revolution, there has been much attention in using deep learning models for time series prediction, and hence it is important to evaluate their strengths and weaknesses.  In this paper, we present an evaluation study that compares the performance  of  deep learning models  for multi-step ahead time series prediction.  Our deep learning methods compromise of simple recurrent neural networks,   long short term memory  (LSTM) networks, bidirectional  LSTM,  encoder-decoder LSTM networks, and convolutional neural networks. We also provide comparison with  simple neural networks use stochastic gradient descent and  adaptive gradient method (Adam) for training. We focus on  univariate   and multi-step-ahead prediction from benchmark time series datasets and compare with results from from the literature. The results show that bidirectional and encoder-decoder LSTM provide the best performance in accuracy for the given  time series problems with different properties. 
  
  

\end{abstract}

\begin{keyword}
 Recurrent neural networks \sep LSTM \sep Deep Learning, \sep Time Series Prediction

\end{keyword}

\end{frontmatter}



\section{Introduction}
\label{S:1}



Apart from econometric models, machine learning methods became extremely popular for time series  prediction or forecasting in the last few decades \cite{tealab2018time,cheng2015time,taieb2012review,ahmed2010empirical,Gooijer2006,li2005recent,hendry1983econometric}. Some of the popular categories include one-step, multi-step, and multivariate prediction. Recently some attention has been given to dynamic time series prediction where the size of the input to the model can   dynamically change \cite{chandra2018co}. Just as the term indicates,  one-step prediction refers to the use of a model to make a prediction one-step ahead in time whereas a multi-step prediction refers to a series of steps ahead in time  from an observed trend in a time series 
\cite{sandya2013feature, chandra2012cooperative}. In the latter case, the \textit{prediction horizon}
defines the extent  of future prediction. The challenge is to develop models that 
produce low  
prediction errors as the prediction horizon increases  given the chaotic nature 
and noise in the dataset
\cite{taieb2015bias,chang2012reinforced,bone2002multi}. There are two major strategies for  multi-step-ahead 
prediction which include  \textit{recursive} and  
\textit{direct}  strategies. The recursive strategy features the  prediction from 
a one-step-ahead prediction model  as the input for future
prediction horizon
\cite{zhang2013iterated,ben2012recursive}, where 
error in the prediction for the next horizon is accumulated in future horizons. 
The direct strategy encodes  the multi-step-ahead problem   as a multi-output problem   \cite{Sorjamaa2007,BenTaieb2010}  which in the case of neural networks can be represented by multiple neurons in the output later, where each neuron denotes the prediction horizon. 
The major challenges in multi-step-ahead prediction include   highly chaotic time series 
and those that have missing data which has been approached  with non-linear filters and neural 
networks
\cite{Wu2014missingdate}.  

 
 
Neural networks have been popular for time series prediction for various applications \cite{frank2001time}. Different neural network architectures have different strengths and weaknesses and given that time series prediction is concerned with careful integration of knowledge in temporal sequences according to different dimensions, it is important to choose the right neural network architecture and training algorithm. 

Recurrent neural networks (RNNs) are known to be better suited for  modelling temporal sequences  \cite{elman_Zipser1988,Werbos_1990,hochreiter1997long,schmidhuber2015deep} and are also   more suitable  for modeling dynamical systems when compared to feedforward networks  \cite{Omlin_thonberetal1996, Omlin_Giles1992,Giles_etal1999}.  RNNs have shown  to be robust methods for time series prediction \cite{connor1994recurrent}.   The Elman RNN \cite{elman_Zipser1988,Elman_1990} is one of the earliest architecture trained by backpropagation through-time which is an extension of  the backpropagation algorithm for feedforward networks \cite{Werbos_1990}. The limitations in   learning by RNNs for long-term dependencies in sequences   that span hundreds or thousands of time-steps  \cite{hochreiter1998vanishing,bengio1994learning} were addressed by  \textit{long short-term memory} networks (LSTMs)   \cite{hochreiter1997long}.

More recently, with the deep learning revolution \cite{schmidhuber2015deep}, there has been further improvements   such as \textit{gated recurrent unit} (GRU)  \cite{chung2014empirical,cho2014learning} networks, which provides similar performance than LSTMs, but are simpler to implement. Some of the other extensions include  predictive state  RNNs \cite{downey2017predictive} that combined RNNs with  power  of predictive state representations \cite{singh2004predictive}.  Bidirectional RNNs connect two hidden layers of opposite directions to the same output where the output layer can get information from past   and future  states simultaneously \cite{schuster1997bidirectional}. The idea was further extended into bidirectional-LSTMs for phoneme classification \cite{graves2005framewise} which performed better than standard RNNs and LSTMs. More work has been made by combining bidirectional LSTMS with convolutional neural networks (CNNs) for natural language processing with problem of named entity recognition \cite{chiu2016named}. Further extensions have been made by encoder-decoder LSTM that used a LSTM to map the input sequence to a vector of a fixed dimensionality and used another LSTM to decode the target sequence from the vector for English to French translation task \cite{sutskever2014sequence}.


On the other hand, backpropgation neural network  which is  also slowly known as shallow learning as opposed to deep learning has a number of improvements for training and generalisation. Deep learning methods such as convolutional neural networks, presented idea of regularisation using dropouts during training that has been helpful for generalisation \cite{srivastava2014dropout}. Adaptive gradient methods such as the Adam optimiser has become popular for training shallow networks \cite{Adams2017}. Apart from these, other ways of training feedforward networks such as evolutionary algorithms have been used for time series problems \cite{Chandra2018NC-CMTL,chandra2018co}.
Furthermore, RNNs have also been training by evolutionary algorithms with applications for time series prediction \cite{cai2007time,Chandra2012times}.

Noting these advances, it is important to evaluate the performance for a challenging problem which in our case is multi-step time series prediction. 
 We note that limited work has been done comparison done between FNN and RNNs for time series \cite{koskela1996time,chandra2016evaluation}. We note that while most of the LSTM applications have been natural language processing and signal processing applications such as phoneme recognition, there is no work that evaluates their performance for time series prediction, particularly multi-step ahead prediction. Since the underlying feature of LSTMs is in handing temporal sequences, it is worthwhile to investigate about their predictive power, i.e. as the prediction horizon increases.  The recent advances in technologies such as \textit{Tensorflow} have   improved computational efficiency of RNNs  \cite{abadi2016tensorflow} and enabled easier implementation for providing comprehensive evaluation of their performance. 
 
 
In this paper, we present an evaluation study that compares the performance  of selected   deep learning models  for multi-step ahead time series prediction.  Our deep learning methods compromise of   standard LSTMs, bidirectional  LSTMS,    encoder-decoder LSTMs, and CNNs. Our shallow learning models use stochastic gradient descent and prominent adaptive gradient method known as Adam optimiser for training. We   examine univariate  time series prediction  with selected models and learning algorithms for benchmark time series datasets. We also compare our results with other related machine learning methods for multi-step time series problems from the literature. 
 
 

The rest of the paper is organised as follows. Section 2 presents a background and literature review of related work. Section 3 presents the details of the different deep learning models,  and Section 4 presents experiments and results. Section 5 provides a discussion and Section 6 concludes the paper with discussion of future work. 

\section{Related Work} 

\subsection{Multi-step time series prediction}

  
The recursive strategy of multi-step-ahead prediction features the predicted value of the current prediction horizon as inputs to next prediction horizon which iterates, hence it is also known as iterated
strategy. One of the first attempts for  recursive strategy was using state-space Kalman filter and smoothing      \cite{ng1990recursive} followed by  recurrent neural 
networks  \cite{Su1992}.  Later,   a dynamic recurrent network used current and delayed  observations  as inputs
to the network which reported   
excellent generalization performance  
\cite{Parlos2000}. Then the
non-parametric Gaussian process model was used to   incorporate the uncertainty about intermediate regressor 
values \cite{BeckerNIPS2002}.  The \textit{Dempster–Shafer} regression technique  for  prognosis of data-driven machinery used iterative strategy      with promising 
performance \cite{Niu2009}.   Lately, reinforced real-time recurrent learning  was used with iterative strategy for  flood forecasts  
\cite{chang2012reinforced}.    



The direct strategies for multi-step-ahead prediction feature all the  prediction horizons during training typically as multiple outputs.
Initial progress was made using recurrent neural networks trained by backpropagation through-time 
algorithm \cite{bone2002multi}. A  review of single-output vs. multiple-output 
approaches  showed  direct strategy 
more  promising choice over recursive strategy  \cite{BenTaieb2010}.    A  multiple-output support vector regression (M-SVR)   achieved  
better  forecasts  when compared to   standard SVR using direct 
and iterated strategies \cite{Bao2014}. 
  
The third  strategy features the  combination of recursive and direct strategies. Initial work featured  multiple SVR   models  that were trained 
independently 
based on the same training data and with different
targets \cite{zhang2013iterated}. An optimally pruned 
extreme learning machine (OP-ELM) was  used    using 
recursive, direct and a combination of the two strategies in an ensemble 
approach where the combination   gave better 
performance than  standalone methods \cite{Grigorievskiy2014}. Chandra et. al \cite{chandra2017CMTLMulti} presented a recursive neural network inspired by multi-task learning and cascaded neural networks for multi-step ahead prediction, training by evolutionary algorithm which gave very promising performance when compared to the literature . 
Ye and Dai \cite{YE2019227} presented a multitask learning method  which considers different prediction  horizons as different tasks and explores the relatedness among horizons while forecasting them in parallel. The method consistently achieved lower error values over all horizons when compared to other related  iterative and direct prediction methods. 
 
 A  comprehensive study on  the  different strategies was given
using a large experimental
benchmark (NN5 forecasting competition) \cite{BenTaieb2012}, and further comparison for  macroeconomic time series where it 
was reported that the  iterated forecasts
typically outperformed the direct forecasts \cite{marcellino2006} and relative performance of the iterated forecasts improved with the
forecast horizon, with further  comparison that  presented  an encompassing 
representation for  derivation  auto-regressive coefficients  
 \cite{Proietti2011}. A study on the properties  
shows that direct strategy provides prediction values that are relatively robust 
to breaks and the benefits increases with the prediction horizon \cite{Chevillon2016}.
 





 
 






 Some applications of the different machine learning methods that apply multi-step-ahead prediction for  real-world 
problems. These  include 1.)  auto-regressive models   for    predicting critical levels of abnormality in 
physiological
signals \cite{TranCyber2016}, 2.) flood forecasting 
using  recurrent neural networks \cite{Chen2013Flood,Chang2014}, 3.) emissions of 
nitrogen oxides using a neural network and related approaches 
\cite{Smrekar2013Nox}, 4.) photo-voltaic power forecasting using hybrid support 
vector machine  \cite{DeGiorgi2016}, 5.) Earthquake ground motions
and seismic response prediction \cite{Yang2016}, and 6.   central-processing unit (CPU) load prediction \cite{YANG20131257}. More recently, Wu \cite{WU2019} employed an adaptive-network-based fuzzy inference system  with uncertainty quantification  the prediction of short-term wind and wave conditions for marine operations. Wang and Li \cite{WANG2018429} presented multi-step ahead prediction for wind speed which was based on optimal feature extraction, deep learning with LSTMs, and error correction strategy. The method  showed lower error values for one, three and five-step ahead predictions in comparison to  related methods. Wang and Li \cite{WANG2019296} also presented another hybrid approach to the multi-step ahead wind speed prediction with empirical wavelet transformation for feature extraction,  autoregressive fractionally integrated moving average to detect long memory characteristics and  swarm-based   backpropagation neural network. 



 








\subsection{Deep learning: LSTM and  applications for time series }


Deep learning naturally features robust spacial temporal information processing \cite{lecun2015deep,schmidhuber2015deep} and has been popular for modelling temporal sequences. Deep learning has  became very successful for computer vision \cite{he2016deep}, reinforcement learning for games \cite{mnih2013playing}, and big data related problems. Deep learning typically refer to recurrent networks (RNNs), convolutional neural networks (CNNs) \cite{najafabadi2015deep,mnih2013playing}, deep belief networks, and LSTM networks which are a special class of RNNs addressing long-term dependency problem in time series datasets \cite{schmidhuber2015deep}. RNNs   have been popular for forecasting time series with their ability to capture temporal information \cite{connor1994recurrent,husken2003recurrent, chandra2012cooperative,chandra2015competition,SALINAS2020}.  In terms of uncertainity quantification in predictions, Mirikitani and Nikolaev   used \cite{mirikitani2010recursive}  variational  inference  for Bayesian RNNs for time series forecasting. 

CNNs have gained attention recently in  forecasting time series. Wang et. al  \cite{wang2017deep}  used CNNs with  wavelet transform   probabilistic wind power forecasting. Xingjian et. al   \cite{xingjian2015convolutional} used   CNNs  in conjunction with LSTMs to capture spatial-temporal sequences   for forecasting precipitation. Amarasinghe et al. \cite{Amarasinghe2017Deepelf} employed  CNNs for   energy load forecasting, and  Huang and Kuo \cite{Huang2018CNN-LSTM} combined CNNs and LSTMs for air pollution quality forecasting. Sudriani et al. \cite{sudriani2019long} employed  LSTMs for forecasting discharge level of a river for  managing water resources. Ding et al. \cite{Ding2015} employed  CNNs to evaluate   different events on stock price behavior,  and  Nelson et al. \cite{nelson2017stock} used LSTMs to forecast stock market trends. Chimmula and Zhand  employed LSTMs for forecasting COVID-19 transmission in Canada  \cite{CHIMMULA2020}. The authors  predicted the possible ending point of the  outbreak   around June 2020, Canada reached the daily new cases peak by 2nd May\footnote{\url{https://www.worldometers.info/coronavirus/country/canada/}} which further reduced. 


\section{Methodology} 

\subsection{Data reconstruction }
   
The original time series data is reconstructed for    multi-step-ahead  
prediction. Taken's theorem expresses that the
reconstruction can reproduce  important
features  of the original time series  \cite{Takens1981}. Therefore, given an 
observed time series , an embedded  phase
space  can be generated; where  is the time delay,  is
the embedding dimension (window size) given , and  is the length 
of the
original time series. A study needs to be done to determine good values for  and 
 in order to efficiently apply Taken's theorem 
\cite{frazier2004}. We note that Taken's proved that if the original attractor is of
dimension , then  would be sufficient \cite{Takens1981}.    

 \subsection{Shallow learning via simple neural networks }
   


 
  
  


We refer to the the backpropagation neural network and multiplayer perception as simple neural network which has been typically trained by the stochastic gradient descent (SGD) algorithm. SGD maintains a single learning rate for all weight updates which does not change during the training. 
 Adaptive moment estimation (Adam) learning algorithm \cite{kingma2014adam} extends the  stochastic gradient descent by maintaining and adapting the learning rate  for each network weight  as learning unfolds. Using first and second moments of the gradients, Adam computes individual adaptive learning rates which is inspired by the \textit{adaptive gradient algorithm} (AdaGrad) \cite{duchi2011adaptive}.  In the literature, Adam has shown better results when compared to stochastic gradient descent and AdaGrad, and our experiments will consider evaluating it further for multi-step time series prediction.  Adam's learning procedure for  iteration    is formulated as



 where  are the respective first  and second  moment vectors for iteration ;  are constants ,  is the learning rate, and  is a close to zero constant. 

  
 
 \subsection{Simple recurrent neural networks}
 
  The Elman RNN \cite{Elman_1990} is a prominent example of   \textit{simple recurrent networks} that  feature  a  context layer to act as memory  and incorporate current state for propagating information into future states, given future inputs. The   use context layer  to store the output of the state neurons from computation of the previous time steps  makes them applicable for time-varying patterns in  data.  The context layer   maintains memory of the prior hidden layer result as shown in Figure \ref{fig:FNN&RNN}.  
  
 

 \begin{figure*}[tb]
  \begin{center}  
   \includegraphics[scale=0.25]{Shaurya/FNNRNN.png} \\
    \caption{ Feed Forward Neural Network and Elman RNN for time series prediction }
\end{center}
   \label{fig:FNN&RNN}
\end{figure*}
A vectorised formulation can be given as follows
 
  
 
 
 
 \noindent  where;  input vector,  hidden layer vector,  output vector,  represent the weights for hidden  and output layer,   is the context state weights,     is the bias,  and    and    are the respective activation functions. Backpropagation through time (BPTT) \cite{Werbos_1990} which is an extension of the backpropagation algorithm  is a prominent method for training simple recurrent networks which  features   gradient descent  with the major difference   that the error is backpropagated for a deeper network architecture that features states defined by time. 
 
 \subsection{LSTM neural networks}
 
 Simple recurrent networks have the \cite{hochreiter1997long} limitation of learning long-term dependencies with problems in vanishing and exploding gradients  \cite{Hochreiter_1998} in simple recurrent neural networks. LSTMs employ using memory cells and gates for much better capabilities in remembering the   long-term dependencies   in temporal sequences   as shown in Figure \ref{fig:LSTM}

\begin{figure}[htbp!]
  \begin{center}  
   \includegraphics[scale=0.3]{Shaurya/LSTM.png} 
    \caption{ Long Short-Term Memory (LSTM)  neural networks 
}
 \label{fig:LSTM}
  \end{center}
\end{figure}

 
 
 
   LSTM units are trained in a supervised fashion on a set of training sequences  using an adaptation of the BPTT algorithm that considers the respective gates \cite{hochreiter1997long}.   Neuroevolution provides an alternate training method that does not request gradients \cite{rawal2016evolving}. Policy gradient methods in reinforcement learning framework can be used in case where no training labels are present \cite{bakker2002reinforcement}.
  
  
  LSTM networks calculate a hidden state   as
 

 where,  ,  and  refer to the input, forget and output gates, at time , respectively.   and   refer to the number of input features and number of hidden units, respectively.    and  is the weight matrices adjusted during learning along with   which is the bias.   The initial values are  and   . All the gates have the same dimensions , the size of your hidden state.  is a “candidate” hidden state, and   is the internal memory of the unit as shown in Figure \ref{fig:LSTM}.  Note that we  denote (*) as element-wise multiplication.


 
 
 
  \subsection{Bi-directional LSTM networks }
  
  
A major shortcoming of conventional RNNs is that they 
only  make use of previous context state for determining future states. Bidirectional
RNNs (BD-RNNs) \cite{schuster1997}  process information in both
directions with two separate hidden layers, which are then
propagated forward to the same output layer. 
BD-RNNs hence consist of  placing two independent RNNs together to allow the networks to have both backward and forward information about the sequence at every time step. BD-RNN computes the forward hidden sequence , the backward hidden sequence , and the output sequence  by iterating the backward layer from  to , the forward layer from  to  
and then updating the output layer.  


Originally proposed for world-embedding in natural language processing,  bi-directional LSTM networks (BD-LSTM)  \cite{graves2005}, can access longer-range context or state in both  directions similar to BD-RNNs.  
BD-LSTM would intake inputs in two ways, one from past to future and one from future to past which different from conventional LSTM since by running information backwards, state information from the future is preserved. Hence, with two hidden states combined, in any point in time the network can preserve information from both past and future as shown in Figure \ref{fig:BDLSTM}.
BD-LSTM networks have been used in several real-world sequence processing problems such as phoneme classification
\cite{graves2005}, continuous speech recognition \cite{Fan2014TTSSW} and speech synthesis \cite{graves2013hybrid}.
 
 
 
 \begin{figure}[htbp!]
  \begin{center}  
   \includegraphics[scale=0.3]{Shaurya/Bi-LSTM.png} \\
    \caption{ Bi-directional LSTM}
\label{fig:BDLSTM}
  \end{center}
\end{figure}
 
 \subsection{Encoder-Decoder LSTM networks} 
 
 Sutskever et. al \cite{NIPS2014_5346} introduced the encoder-decoder LSTM network (ED-LSTM) which is a sequence to sequence model for mapping a fixed-length input to a fixed-length output where the length of the input and output may differ, which is applicable in automatic language translation tasks (English to French for example). In the case of multi-step series prediction and multivariate analysis,  both the input and outputs are of
variable lengths. Hence,  the input is the sequence of video frames
, and the output is the sequence of words
. Therefore, we estimate the conditional probability of
an output sequence  given an input sequence
 i.e.
.




ED-LSTM networks  handle variable-length input and outputs by  first encoding the input sequences, one at a time,
 using a latent vector representation,
and then decoding from that representation. 
In the encoding phase, given an input sequence, the ED-LSTM computes a sequence of hidden
states In decoding phase, it defines a distribution over the output sequence   given the input sequence  as shown in Figure \ref{fig:EN-DC LSTM}. 







\begin{figure}[htbp!]
  \begin{center}  
   \includegraphics[scale=0.3]{Shaurya/En-Dc_LSTM.png} 
    \caption{ Encoder-Decoder LSTM }
\label{fig:EN-DC LSTM}
  \end{center}
\end{figure}






\subsection{CNNs} 

 CNNs introduced by introduced by LeCun \cite{lecun1990cnn,lecun1998cnn} are  prominent deep learning architecture inspired by the natural visual system of mammals. CNNs could classify handwritten digits and could be trained using backpropagation algorithm \cite{Hecht1989backprop} which later has been prominent in many computer vision and image processing tasks. More recently, CNNs have been applied for time series prediction \cite{Amarasinghe2017Deepelf,xingjian2015convolutional,wang2017deep} with promising results.  

CNNs   learn spatial hierarchies of features by using multiple building blocks, such as convolution, pooling layers, and fully connected layers.   Figure  \ref{fig:cnn} shows an example of a CNN used for time series prediction, given a univariate time series input and multiple output neurons representing different prediction horizons. We note that multivariate time series is more appropriate  to take advantage of feature extraction via the convolutional and the pooling layers. 
 
 
\begin{figure*}[tb]
\centering 
   \includegraphics[scale = 0.25] {Shaurya/CNN-Page-1.png}
  
    
\caption{ One-dimensional Convolutional Neural Network for time series 
}
\label{fig:cnn}
\end{figure*}




 
 \section{Experiments and Results}
 We present experiments and results that consider simple neural networks featuring SGD and Adam learning, and   and deep learning methods that feature RNNs,  LSTM networks, ED-LSTM, BD-LSTM and CNNs. 
 


\subsection{Experimental Design}
  
 
 

 The selected benchmark problems are are combination of simulated and real-world time series. The 
simulated time series are Mackey-Glass\cite{Mackey1977}, Lorenz 
\cite{lorenz1963}, Henon \cite{Henon1976}, and 
Rossler \cite{rossler}. The real-world time series are Sunspot  
\cite{Sunspot2001}, Lazer \cite{weigendtime} and ACI-financial time 
series \cite{timeDataSet}. They  have been used in our previous works and are prominent benchmarks for time series problems \cite{ChandraLangevinNC2019,ChandraTNNLS2015,chandra2017_CMTL}. The  
Sunspot 
time series indicates solar activities from November 1834 to June 2001 and consists of 2000 data 
points \cite{Sunspot2001}. The ACI-finance time series  contains
closing stock prices from December 2006 to February 2010,
featuring 800 data points \cite{timeDataSet}. The  
Lazer time 
series is from the  \textit{Santa Fe 
competition} that consists of 500 points \cite{weigendtime}. 


The respective time series were pre-processed into a state-space 
vector \cite{Takens1981} with embedding dimension
 and time-lag   for 10-step-ahead prediction. We considered respective  neural network models with  number of hidden 
neurons, selected learning rate and other papers and in trial experiments to determine appropriate models. Table \ref{tab:config} gives details for the topology of the respective models in terms of input, hidden and output layers.  In the respective datasets, we used first 1000 data points  from which the first 60\% was used for training and remaining for testing.   All the respective time series  are scaled in the range [0,1]. 
   
  
We use the root-mean-squared error (RMSE)  in Equation \ref{rmse}   as the main performance measures
for different prediction horizons. 
 
 
 
\noindent where    are the observed data,
predicted data, respectively.  is the length of the observed data. We apply RMSE in Equation 
\ref{rmse} for each 
prediction horizon, and also report  the mean error for all the 
respective prediction horizons.
 
 
 
\begin{table*}[htbp!]
 \small 
 \centering
\begin{tabular}{llllp{11cm}}
\hline
 &  Input & Hidden Layers & Output & Comments  \\
\hline
\hline
FNN-Adam &      5&1&10& Hidden Layer size= 10, Optimizer= adam, Activation= relu, Epochs=1000\\
FNN-SGD & 5 & 1&10& Hidden Layer size= 10, Optimizer= SGD, Activation= relu, Epochs=1000\\

LSTM &      5&1&10& Hidden Layer of 10 cells, Optimizer= adam, Activation= relu, Epochs=1000\\
BD-LSTM &      5&1&10&Forward and Backward layer of 10 Cells each, Optimizer= adam, Activation= relu, Epochs=1000\\
ED-LSTM&      5&4&10& Two LSTM, Repeat Vector and a Time distributed layer, Optimizer= adam, Activation= relu, Epochs=1000\\
RNN &      5&2&10& Hidden Layers consist of 10 Cells and 10 neurons resp., Optimizer= SGD, Epochs=1000\\
CNN &      5&4&10&Convolution, Pooling, Flatten and Dense Layer, Optimizer= adam, Activation= relu, Epochs=1000, Filters=64, Convolutional Window size=3, Max-Pooling Window size=2\\ 

\hline &
\end{tabular}
\caption{Configuration of models }
\label{tab:config}
\end{table*}

 
 
 
\subsection{Results}


We report the mean and 95 \% confidence interval of RMSE for each prediction horizon for the respective problem  for train and test datasets from 30 experimental runs with different initial neural network weights.

The results are shown in  Figure \ref{fig:henon} to Figure  \ref{fig:rossler} for the simulated time series problems ( Table \ref{tab:henon} to \ref{tab:rossler} in Appendix). Figure \ref{fig:finance} to \ref{fig:lazer} show results for the real-world time series problem (Table \ref{tab:finance} to \ref{tab:lazer} in Appendix). We define robustness as the confidence interval which must be as low as possible to indicate high confidence in prediction. We consider scalability as the ability to provide consistent performance to some degree of error given the prediction horizon increases.

 


Note that the results are given in terms of the RMSE where the lower values indicate better performance. Note that each problem reports 10-step-ahead prediction results for 30 experiments  with RMSE mean and 95\% confidence interval  as histogram and error bars, shown in Figures \ref{fig:finance} to \ref{fig:rossler}. 


  We first consider results for real world time series that naturally feature noise (ACI-Finance, Sunspot, Lazer). Figure \ref{fig:finance} shows the results for the ACI-fiancee problem. We observe that the test performance is better than the train performance in Figure \ref{fig:finance} (a) where deep learning models provide more reliable performance. The prediction error increases with the prediction horizon, and the deep learning methods do much better than simple learning methods (FNN-SGD and FNN-Adam). We find that LSTM provides the best overall   performance as shown in Figure \ref{fig:finance} (b).  The overall test performance shown in Figure \ref{fig:finance} (a) indicates that FNN-Adam  and  LSTM  provide similar performance, which are better than rest of the problems.  Figure \ref{fig:financeBest} shows ACI-finance prediction performance of the best experiment run with selected prediction horizons that indicate how the prediction deteriorates  as prediction horizon increases. 

Next, we consider the results for the  Sunspot time series  shown in Figure \ref{fig:sunspot} which follows a similar trend as the ACI-finance problem in terms of the increase in prediction error along with the prediction horizon. Also, the test performance is better than the train performance as evident from Figure \ref{fig:sunspot} (a). The LSTM methods (LSTM, ED-LSTM, BD-LSTM) gives better performance than the other methods as can be observed from Figure \ref{fig:sunspot} (a) and \ref{fig:sunspot} (b). Note that the FNN-SGD gives the worst performance and the performance of RNN is better than that of CNN, FNN-SGD, FNN-Adam but poorer than LSTM methods. Figure \ref{fig:sunspotsingle}  shows Sunspot prediction performance of the best experiment run with selected prediction horizons. 


The results for  Lazer time series  is shown in Figure \ref{fig:lazer}, which exhibits a similar trend in terms of the train and test performance as the other real-world time series problems. Note that the Lazer problem is highly chaotic (as visually evident in Figure \ref{fig:lazersingle}), which seems to be the primary reason behind the difference in performance for the prediction horizon in contrast to other problems as displayed in Figure \ref{fig:lazer} (b). It is striking that none of the methods appear to be showing any trend for the prediction accuracy along the prediction horizon, as seen in previous problems. In terms of scalability, all the methods appear to be performing better in comparison with the other problems. The performance of CNN is better than that of RNN, which is different from other real-world time series. Figure \ref{fig:lazersingle}  shows Lazer prediction performance of the best experiment run using ED-LSTM with selected prediction horizons. We note that due to the chaotic nature of the time series, the prediction performance is visually not clear. 
 

We now consider simulated time series that do not feature noise (Henon, Mackey-Glass, Rosssler, Lorenz). The Henon time series in  Figure \ref{fig:henon}  shows that ED-LSTM provides the best performance. Note that there is a more significant difference between the three LSTM methods   when compared to other problems. The trends are similar to the ACI-finance  and the Sunspot problem  given the prediction horizon performance in Figure \ref{fig:henon} (a) and \ref{fig:henon} (b) where the simple learning methods (FNN-SGD and FNN-Adam) appear to be more scalable than the other methods along the prediction horizon although they perform poorly.  Figure \ref{fig:mackeysingle}  and Figure \ref{fig:henonsingle} shows Mackey-Glass and Henon prediction  performance of the best experiment run using ED-LSTM with selected prediction horizons. The Henon prediction in Figure \ref{fig:henonsingle}  indicates that it is far more  chaotic than Mackey-Glass  and hence it  faces more challenges. We show them since these are cases with no noise when compared to real-world time series previously shown that has a larger  deviation or deterioration in prediction performance as the prediction horizon increases (Figures \ref{fig:financeBest} and Figure \ref{fig:sunspotsingle}). 

For the Lorenz, Mackey-Glass and Rossler simulated time series, the deep learning methods are performing far better than the simple learning methods as can be seen in Figures \ref{fig:lorenz}, \ref{fig:mackey} and \ref{fig:rossler}. The trend along the prediction horizon is similar to previous problems, i.e., the prediction error increases along with the prediction horizon. If we consider scalability, the deep learning  methods are more scalable in the Lorenz, Mackey-Glass and Rossler problems than the previous problems.   This is the first instance where the CNN has outperformed LSTM for Mackey-Glass and Rossler time series. 
  


 We note that there have been distinct trends in prediction for the different types of problems. In the simulated time series problems, if we exclude Henon,  we find similar trend for Mackey-Glass, Lorenz and Rossler time series. The trend indicates  that simple neural networks face major difficulties and ED-LSTM and BD-LSTM networks provides the best performance, which also applies to Henon time series, except that it has close performance for simple neural networks when compared to deep learning models for 7-10  prediction horizons (Figure 9 b). This difference  reflects in the nature of the time series which is  highly chaotic in nature (Figure 15). We further note that the simple neural networks, in the Henon case (Figure 9) does not   deteriorate in performance as the prediction horizon increases when compared to Mackley-Glass, Lorenz and Rossler problems, although they give poor performance.  
 
 The performance of simple neural networks in Lazer problem shows a similar trend in Lazer time series, where the predictions are poor from the beginning and its striking that LSTM networks actually improve the performance as the prediction horizon increases (Figure 8 b). This trend is a clear outline when compared to rest of real-world and simulated problems, as all of them have results where the deep learning models deteriorate as the prediction horizon increase.



\begin{figure*}[htbp!]
\centering
\subfigure[RMSE  across 10 prediction horizons]{
\includegraphics[scale =0.55]{Shaurya/ACFinance/Train_Test_RMSE_Mean_Comparison.png}
 }
 \subfigure[10 step-ahead prediction]{
   \includegraphics[scale =0.55] {Shaurya/ACFinance/Step_RMSE_Comparison.png}
 }
\caption{ACI-finance time series: performance evaluation of respective methods}
\label{fig:finance}
\end{figure*}


\begin{figure*}[htbp!]
\centering
\subfigure[RMSE  across 10 prediction horizons]{
\includegraphics[scale =0.55]{Shaurya/Sunspot/Train_Test_RMSE_Mean_Comparison.png}
 }
 \subfigure[10 step-ahead prediction]{
   \includegraphics[scale =0.55] {Shaurya/Sunspot/Step_RMSE_Comparison.png}
 }
\caption{Sunspot time series: performance evaluation of respective methods (RMSE mean and 95\% confidence interval as error bar)}
\label{fig:sunspot}
\end{figure*}

\begin{figure*}[htbp!]
\centering
\subfigure[RMSE  across 10 prediction horizons]{
\includegraphics[scale =0.55]{Shaurya/Lazer/Train_Test_RMSE_Mean_Comparison.png}
 }
 \subfigure[10 step-ahead prediction]{
   \includegraphics[scale =0.55] {Shaurya/Lazer/Step_RMSE_Comparison.png}
 }
\caption{Lazer time series: performance evaluation of respective methods (RMSE mean and 95\% confidence interval as error bar)}
\label{fig:lazer}
\end{figure*}


\begin{figure*}[htb]
\centering
\subfigure[RMSE-Mean]{
\includegraphics[scale =0.55]{Shaurya/Henon/Train_Test_RMSE_Mean_Comparison.png}
 }
 \subfigure[10 step-ahead prediction]{
   \includegraphics[scale =0.55] {Shaurya/Henon/Step_RMSE_Comparison.png}
 }
\caption{Henon time series: performance evaluation of respective methods (RMSE mean and 95\% confidence interval as error bar)}
\label{fig:henon}
\end{figure*}


\begin{figure*}[htbp!]
\centering
\subfigure[RMSE  across 10 prediction horizons]{
\includegraphics[scale =0.55]{Shaurya/Lorenz/Train_Test_RMSE_Mean_Comparison.png}
 }
 \subfigure[10 step-ahead prediction]{
   \includegraphics[scale =0.55] {Shaurya/Lorenz/Step_RMSE_Comparison.png}
 }
\caption{Lorenz time series: performance evaluation of respective methods (RMSE mean and 95\% confidence interval as error bar)}
\label{fig:lorenz}
\end{figure*}


\begin{figure*}[htbp!]
\centering
\subfigure[RMSE  across 10 prediction horizons]{
\includegraphics[scale =0.55]{Shaurya/Mackey/Train_Test_RMSE_Mean_Comparison.png}
 }
 \subfigure[10 step-ahead prediction]{
   \includegraphics[scale =0.55] {Shaurya/Mackey/Step_RMSE_Comparison.png}
 }
\caption{Mackey-Glass time series: performance evaluation of respective methods (RMSE mean and 95\% confidence interval as error bar)}
\label{fig:mackey}
\end{figure*}


\begin{figure*}[htbp!]
\centering
\subfigure[RMSE  across 10 prediction horizons]{
\includegraphics[scale =0.55]{Shaurya/Rossler/Train_Test_RMSE_Mean_Comparison.png}
 }
 \subfigure[10 step-ahead prediction]{
   \includegraphics[scale =0.55] {Shaurya/Rossler/Step_RMSE_Comparison.png}
 }
\caption{Rossler time series: performance evaluation of respective methods (RMSE mean and 95\% confidence interval as error bar)}
\label{fig:rossler}
\end{figure*}











\begin{figure*}[htb]
\centering
\subfigure[Step 1]{
\includegraphics[scale =0.13]{Shaurya/ACFinance/pred_Step1copy.png}
 }
 \subfigure[Step 3]{
\includegraphics[scale =0.13]{Shaurya/ACFinance/pred_Step3copy.png}
 }
 \subfigure[Step 5]{
\includegraphics[scale =0.13]{Shaurya/ACFinance/pred_Step5copy.png}
 }
 \subfigure[Step 10]{
\includegraphics[scale =0.13]{Shaurya/ACFinance/pred_Step10copy.png}
 }
\caption{ACI-finance actual vs predicted values for Encoder-Decoder LSTM Model}
\label{fig:financeBest}
\end{figure*}


\begin{figure*}[htb]
\centering
\subfigure[Step 1]{
\includegraphics[scale =0.13]{Shaurya/Sunspot/pred_Step1copy.png}
 }
 \subfigure[Step 3]{
\includegraphics[scale =0.13]{Shaurya/Sunspot/pred_Step3copy.png}
 }
 \subfigure[Step 5]{
\includegraphics[scale =0.13]{Shaurya/Sunspot/pred_Step5copy.png}
 }
 \subfigure[Step 10]{
\includegraphics[scale =0.13]{Shaurya/Sunspot/pred_Step10copy.png}
 }
\caption{Sunspot actual vs predicted values for Encoder-Decoder LSTM Model}
\label{fig:sunspotsingle}
\end{figure*}

 

\begin{figure*}[htb]
\centering
\subfigure[Step 1]{
\includegraphics[scale =0.13]{Shaurya/Henon/pred_Step1copy.png}
 }
 \subfigure[Step 3]{
\includegraphics[scale =0.13]{Shaurya/Henon/pred_Step3copy.png}
 }
 \subfigure[Step 5]{
\includegraphics[scale =0.13]{Shaurya/Henon/pred_Step5copy.png}
 }
 \subfigure[Step 10]{
\includegraphics[scale =0.13]{Shaurya/Henon/pred_Step10copy.png}
 }
 
\caption{Henon actual vs predicted values for Encoder-Decoder LSTM Model}

\label{fig:henonsingle}
\end{figure*}
 
 
\begin{figure*}[htb]
\centering
\subfigure[Step 1]{
\includegraphics[scale =0.13]{Shaurya/Lazer/pred_Step1copy.png}
 }
 \subfigure[Step 3]{
\includegraphics[scale =0.13]{Shaurya/Lazer/pred_Step3copy.png}
 }
 \subfigure[Step 5]{
\includegraphics[scale =0.13]{Shaurya/Lazer/pred_Step5copy.png}
 }
 \subfigure[Step 10]{
\includegraphics[scale =0.13]{Shaurya/Lazer/pred_Step10copy.png}
 }
 
\caption{Lazer actual vs predicted values for Encoder-Decoder LSTM Model}

\label{fig:lazersingle}
\end{figure*} 


\begin{comment}

\begin{figure*}[htb]
\centering
\subfigure[Step 1]{
\includegraphics[scale =0.13]{Shaurya/Lorenz/pred_Step1copy.png}
 }
 \subfigure[Step 3]{
\includegraphics[scale =0.13]{Shaurya/Lorenz/pred_Step3copy.png}
 }
 \subfigure[Step 5]{
\includegraphics[scale =0.13]{Shaurya/Lorenz/pred_Step5copy.png}
 }
 \subfigure[Step 10]{
\includegraphics[scale =0.13]{Shaurya/Lorenz/pred_Step10copy.png}
 }
 
\label{fig:lorenzsingle}
\caption{Lorenz actual vs predicted values for Encoder-Decoder LSTM Model}
\end{figure*}
\end{comment}

\begin{figure*}[htb]
\centering
\subfigure[Step 1]{
\includegraphics[scale =0.13]{Shaurya/Mackey/pred_Step1copy.png}
 }
 \subfigure[Step 3]{
\includegraphics[scale =0.13]{Shaurya/Mackey/pred_Step3copy.png}
 }
 \subfigure[Step 5]{
\includegraphics[scale =0.13]{Shaurya/Mackey/pred_Step5copy.png}
 }
 \subfigure[Step 10]{
\includegraphics[scale =0.13]{Shaurya/Mackey/pred_Step10copy.png}
 }
\caption{Mackey-Glass actual vs predicted values for Encoder-Decoder LSTM Model}
\label{fig:mackeysingle}
\end{figure*}

\begin{comment}

\begin{figure*}[htb]
\centering
\subfigure[Step 1]{
\includegraphics[scale =0.13]{Shaurya/Rossler/pred_Step1copy.png}
 }
 \subfigure[Step 3]{
\includegraphics[scale =0.13]{Shaurya/Rossler/pred_Step3copy.png}
 }
 \subfigure[Step 5]{
\includegraphics[scale =0.13]{Shaurya/Rossler/pred_Step5copy.png}
 }
 \subfigure[Step 10]{
\includegraphics[scale =0.13]{Shaurya/Rossler/pred_Step10copy.png}
 }
\caption{Rossler actual vs predicted values for Encoder-Decoder LSTM Model}
\label{fig:rosslersingle}
\end{figure*}

 
 
\end{comment}

 
 
 
 
 
 
 
\begin{table*}[htbp!]
 \small 
 \centering
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN & CNN\\
\hline
\hline
ACI-finance  &      
2&7&1&3&4&5&6\\
Sunspot &      
6&7&2&1&3&4&5\\
Lazer &      
6&7&1&2&3&5&4\\
Henon &      
6&7&3&2&1&5&4\\
Lorenz &      
6&7&2&3&1&5&4\\
Mackey-Glass &      
6&7&4&2&1&5&1\\
Rossler &      
6&7&4&1&2&5&3\\
\hline
Mean-Rank & 5.42 & 7.00  & 2.42  & 2.00  & 2.14  & 4.85  & 3.85 \\

\hline &
\end{tabular}
\caption{Performance (rank) of different models for respective  time-series problems. Note lower rank denotes better performance. }
\label{tab:resultranks}
\end{table*}

 
 

 \subsection{Comparison with the literature}
  
 
 
 
\begin{table*}[htbp!]

\centering
 \small
 \caption{Comparison with Literature for Simulated time series. }
  
\label{tab:Simulated}
\begin{tabular}{llllll}

\hline
Problem & Method & 2-step & 5-step & 8-step & 10 steps\\
 \hline
 \hline
Mackey-Glass & &  	   &	& &\\
 


 &2SA-RTRL* \cite{chang2012reinforced}&  0.0035  	  &	 & & \\



 &ESN*\cite{chang2012reinforced}&    0.0052	&   	&   &\\
 &EKF\cite{Wu2013AMC}&     	&   0.2796	 &   &\\
 &G-EKF \cite{Wu2013AMC}&     	&   0.2202	 &   &\\
  &UKF \cite{Wu2013AMC}&     	&   	 0.1374&   &\\
  &G-UKF \cite{Wu2013AMC}&     	&   	0.0509&  & \\
 &GPF\cite{Wu2013AMC} &     	&   	0.0063&   &\\

 &G-GPF\cite{Wu2013AMC} &     	&   0.0022	&   &\\

&Multi-KELM\cite{YE2019227} &		0.0027&0.0031&0.0028&0.0029\\
&MultiTL-KELM\cite{YE2019227} &	\textbf{0.0025}&	\textbf{0.0029}&\textbf{0.0026}&\textbf{0.0028}\\
&CMTL \cite{chandra2017CMTLMulti}&  0.0550 & 	0.0750 & 0.0105 & 0.1200	\\





&ANFIS(SL) \cite{ZHOU2019343} &0.0051 &0.0213 & 0.0547 & \\
&R-ANFIS(SL) \cite{ZHOU2019343}&0.0045 &0.0195  &	0.0408 & \\
&R-ANFIS(GL) \cite{ZHOU2019343}&0.0042 & 	0.0127 & 0.0324 & \\

&FNN-Adam &   0.0256	0.0038   	&   0.0520	0.0044     &0.0727	0.0050	       &0.0777	0.0043\\

&FNN-SGD  &     0.0621	0.0051 	&  0.0785	0.0025       &0.0937	0.0022	      &0.0990	0.0026\\

&LSTM   &   0.0080	0.0014   	&    0.0238	0.0024    &0.0381	0.0029	       &0.0418	0.0033\\

&BD-LSTM   &   0.0083	0.0015   	&   0.0202	0.0026     &0.0318	0.0027	       &0.0359	0.0026\\

&ED-LSTM   &   0.0076	0.0014   	&    0.0168	0.0027    &	0.0248	0.0036       &0.0271	0.0040\\

&RNN  &    0.0142	0.0001  	&   0.0365	0.0001     &0.0547	0.0001	       &0.0615	0.0001\\

&CNN &     0.0120		0.0010 	&   0.0262		0.0016     &0.0354		0.0018	       &0.0364		0.0017\\

\hline





Lorenz &    	   &	& \\
 


 &2SA-RTRL*\cite{chang2012reinforced}  &  0.0382	   &	 &  \\



 &ESN*\cite{chang2012reinforced}&    0.0476 	   &	&   \\
 
 &CMTL \cite{chandra2017CMTLMulti} & 0.0490	 &  0.0550	&0.0710 & 0.0820  \\
&FNN-Adam &    0.0206	0.0046  	&  0.0481	0.0072      & 0.0678	0.0058	       &0.0859	0.0065\\

&FNN-SGD  & 0.0432	0.0030     	&   0.0787	0.0030     &0.1027	0.0025	       &0.1178	0.0026\\

&LSTM   & \textbf{0.0033}	0.0010     	&  0.0064	0.0026      &	0.0101	0.0038       &0.0129	0.0042\\

&BD-LSTM   & 0.0054	0.0026     	&  0.0079	0.0036      &0.0125	0.0057	       &0.0146	0.0059\\

&ED-LSTM   &0.0044	0.0012      	&  \textbf{0.0059	0.0009}      &\textbf{0.0090	0.0009}	       &\textbf{0.0110	0.0012}\\

&RNN  &   0.0129	0.0012   	& 0.0155	0.0024       &0.0186	0.0042	       &0.0226	0.0058\\

&CNN &  	0.0067	0.0007    	&  0.0098	0.0009      &0.0132	0.0011	       &0.0157	0.0015\\
\hline



 
Rossler &    	  &	& \\

&CMTL \cite{chandra2017CMTLMulti} & 0.0421   & 0.0510  &0.0651	& 0.0742 	   \\


&FNN-Adam & 0.0202	0.0024     	&  0.0400	0.0039      &0.0603	0.0050	       &0.0673	0.0056\\

&FNN-SGD  &  0.0666	0.0058    	& 0.1257	0.0082       &0.1664	0.0075	       &0.1881	0.0078\\

&LSTM   &  0.0086	0.0011    	&  0.0135	0.0015      &0.0185	0.0022	       &0.0225	0.0026\\

&BD-LSTM   &  \textbf{0.0047	0.0014}    	& \textbf{ 0.0084	0.0021 }     &	\textbf{0.0142	0.0027 }      &\textbf{0.0178	0.0032}\\

&ED-LSTM   &  0.0082	0.0019    	&   0.0128	0.0021     &0.0159	0.0024	       &0.0180	0.0030\\

&RNN  &    0.0218	0.0005  	&   0.0314	0.0004     &0.0382	0.0004	       &0.0424	0.0004\\

&CNN & 0.0105		0.0011     	&  0.0122		0.0016      &	0.0157		0.0020	       &	0.0220		0.0022\\
\hline


 
 
 
 

 
 
Henon &    	  & 	& \\
&Multi-KELM\cite{YE2019227}&0.0041&	0.2320&	0.2971&	0.2968\\
&MultiTL-KELM\cite{YE2019227}&	 \textbf{0.0031}&	0.1763&0.2452&	0.2516\\
&CMTL \cite{chandra2017CMTLMulti} & 0.2103   & 0.2354  &0.2404	   & 0.2415 \\


&FNN-Adam &  0.1606   0.0024     	&0.1731   0.0005         &	0.1781   0.0005        & 0.1762   0.0009\\

&FNN-SGD  & 0.1711    0.0018     	&  0.1769    0.0007      &0.1805    0.0012	       &0.1773    0.0011\\

&LSTM   & 0.0682    0.0058     	& 0.1584    0.0010       &	 0.1707    0.0008       & 0.1756    0.0005 \\

&BD-LSTM   & 0.0448   0.0026     	&  0.1287   0.0046        &0.1697   0.0008	       &0.1733   0.0003\\

&ED-LSTM   & 0.0454   0.0069     	& \textbf{0.0694   0.0161}        &	\textbf{0.1371   0.0107}        & \textbf{0.1689   0.0046}\\

&RNN  &    0.1515 0.0016    	& 0.1718   0.0001       &	 0.1768   0.0001       &0.1751   0.0002\\

&CNN & 0.0859	 0.0038     	& 	0.1601 	0.0007       &	0.1718 	0.0003       &0.1737 	0.0002\\
\hline

\end{tabular}
\end{table*}




\begin{table*}[htbp!]

\centering
 \small
 \caption{Comparison with Literature for Real World time series. }
  


\label{tab:Real}
\begin{tabular}{llllll}

  
 \hline
Problem& Method& 2-step & 5-step &8-step& 10-step\\
 \hline
 \hline


Lazer &    	&   	& \\
 






 
&CMTL \cite{chandra2017CMTLMulti} &  0.0762  &  0.1333  	& 0.1652& 0.1885  \\


&FNN-Adam &  0.1043   0.0018    	& 0.0761   0.0019       &	0.0642	 0.0020       &0.0924	0.0018\\

&FNN-SGD  & 0.0983    0.0046     	&   0.0874   0.0072     &	0.0864 	0.0053       &0.0968	0.0052\\

&LSTM   &  \textbf{0.0725  0.0027 }   	&  \textbf{0.0512    0.0015}      &	0.0464 	0.0015       &\textbf{0.0561	0.0044}\\

&BD-LSTM   &  0.0892   0.0022     	&   0.0596   0.0036      &\textbf{0.0460 	0.0015}	       &0.0631	0.0037\\

&ED-LSTM   &  0.0894   0.0013     	& 0.0694   0.0073       &	0.0510 	0.0027       &0.0615	0.0030\\

&RNN  &   0.1176   0.0019    	&  0.0755  0.0011      &0.0611 	0.0015	       &0.0947	0.0027\\

&CNN &  0.0729	0.0014    	&  0.0701	0.0020      & 0.0593	0.0029	       & 0.0577	0.0018\\

 \hline
 
 
Sunspot &    	  &	& \\
 

 &M-SVR \cite{zhang2013iterated} &   	&   &	 & 0.2355   0.0583 \\
 
 &  SVR-I \cite{zhang2013iterated}&   	&   &	 &  0.2729 0.1414  \\
 
 & SVR-D \cite{zhang2013iterated} &   	&   &	 & 0.2151  0.0538 \\
 
 
&CMTL \cite{chandra2017CMTLMulti} &   0.0473 &  0.0623  &0.0771	 & 0.0974  \\


&FNN-Adam & 0.0236	0.0015     	&0.0407	0.0012        &	0.0582	0.0019       &0.0745	0.0020\\

&FNN-SGD  &  0.0352	0.0022    	& 0.0610	0.0024       &	0.0856	0.0023       &0.1012	0.0019\\

&LSTM   &   \textbf{0.0148	0.0007 }  	& 0.0321	0.0006       &	0.0449	0.0007       &0.0587	0.0010\\

&BD-LSTM   &  0.0155	0.0007    	&   \textbf{0.0318	0.0007}     &\textbf{0.0440	0.0005}	       &\textbf{0.0576	0.0010}\\

&ED-LSTM   &   0.0170	0.0004   	& 0.0348	0.0004       &0.0519	0.0016	       &0.0673	0.0022\\

&RNN  &  0.0212	0.0003    	& 0.0395	0.0002       &0.0503	0.0002	       &0.0641	0.0003\\

&CNN &   0.0257	0.0002   	& 	0.0419	0.0004       &	0.0555	0.0006       &0.0723	0.0008\\
 \hline


 
 

ACI-Finance &    	  &	 &\\

&CMTL \cite{chandra2017CMTLMulti} &  0.0486  &  0.0755  &0.08783&0.1017  \\


&FNN-Adam & 0.0203   0.0012     	&  0.0272   0.0008       &\textbf{0.0323   0.0004}	       &\textbf{0.0357   0.0008}\\

&FNN-SGD  &   0.0242    0.0020 	&  0.0299    0.0015       &0.0350   0.0021	       &0.0380    0.0018\\

&LSTM   & 0.0168    0.0003    	& \textbf{0.0248    0.0006}      &0.0333   0.0010	       &0.0367   0.0015 \\

&BD-LSTM   &   \textbf{0.0165   0.0002}  	&  0.0253   0.0004     &0.0356  0.0010& 0.0409   0.0015\\

&ED-LSTM   &   0.0171   0.0003     	&  0.0271  0.0010       & 0.0359   0.0014 	       &0.0395   0.0014\\

&RNN  &   0.0202   0.0003   	&  0.0284  0.0004     &0.0348  0.0004 	       & 0.0384   0.0003\\

&CNN &  0.0217	0.0004   	&   0.0290	0.0002    &0.0363	0.0006	       & 0.0401	0.0005\\
\hline
\end{tabular}
\end{table*}

  
 
 Table \ref{tab:Simulated} and \ref{tab:Real} show  a comparison with related methods from the 
literature for simulated and real-world time series, respectively. We 
note that the comparison is not truly fair as other 
methods may have employed different models with different 
  data processing and also in reporting of results with 
different measures of error as some papers report best experimental run and do not show mean and standard deviation. We highlight in bold the best performance for respective prediction horizon. Table \ref{tab:Simulated}, we compare the Mackey-Glass and Lorenz time series 
performance for  two-step-ahead prediction by real-time recurrent 
learning (RTRL) and  echo state networks (ESN) \cite{chang2012reinforced}. Note 
that the * in  the results  implies  that the comparison 
was not very fair due to  different embedding 
dimension in state-space reconstruction  and it is not clear if the mean or best run has been 
reported. We show further comparison for Mackey-Glass for 5th prediction horizon 
using Extended Kalman Filtering (EKF), the Unscented Kalman Filtering (UKF) and 
the
 Gaussian Particle Filtering (GPF),  along with their    generalized   
versions G-EKF, G-UKF
 and G-GPF, respectively \cite{Wu2013AMC}. Considering MultiTL-KELM \cite{YE2019227}   for Mackey-Glass and Henon time series, it is observed that it is performing very well for the Mackey-Glass time series and beats all our proposed methods but fails to perform better for the Henon time series.  In general, we find that our proposed deep learning methods (LSTM, BD-LSTM, ED-LSTM) have beaten most of the methods from the literature for the simulated time series, except for the Mackey-Glass time series. 
 
In Table  \ref{tab:Real}, we compare the performance of Sunspot  time series  with support vector 
regression(SVR),  iterated (SVR-I), direct (SVR-D), and  multiple models
(M-SVR) methods \cite{zhang2013iterated}. In the respective problems, we also compare with   coevolutionary multi-task learning (CMTL) \cite{chandra2017CMTLMulti}.   We observe that  our proposed deep learning methods have given the best performance for the respective problems for most of the prediction horizon. Moreover, we find  the FNN-Adam overtakes  CMTL in all time-series problems except in 8-step ahead prediction in Mackey-Glass and 2-step ahead prediction in Lazer time series. It should also be noted that except for the Mackey-Glass and ACI-Finance time series, the deep learning methods are the best  which motivates further applications for these methods for challenging forecasting problems. 
 
 
 \section{Discussion}
 
 
We provide a ranking of the methods in terms of performance accuracy over the test dataset across the prediction horizons in Table \ref{tab:resultranks}. We observe that FNN-SGD gives the worst performance for all time-series problems followed by FNN-Adam in most cases, which is further followed by RNN and CNN. We observe that the BD-LSTM and ED-LSTM models provide one of the best performance across different problems with differed properties. We also note that in across all the problems, the confidence interval of RNN is the lowest followed by CNN which indicates that they provide more robust training performance given different initialisation in weight space.
 
  We note that its natural for the results to deteriorate as the prediction horizons increases in multi-step ahead problems since the prediction is based on current values and the gap in the missing information increases as the horizon decreases since the next predictions are not used as inputs due to our problem formulated as direct strategy of multi-step ahead prediction, as opposed to iterated prediction strategies. ACI-finance problem is  unique in  since there is not major difference with simple neural networks and deep learning models (Figure 7 b) from 7 - 10 prediction horizon. 
  
 
 Long term dependency problems  arise in the analysis of   time series where  the rate of decay of statistical dependence of two points with increasing time interval between the points. Canonical RNNs had difficulties in training with long-term dependencies \cite{hochreiter1998vanishing}, hence LSTM networks were proposed to address the learning imitates with memory cells  for addressing vanishing error problem in learning long term dependencies \cite{hochreiter1997long}.
  We note that the time series  problems in our experiments  are not long-term dependency problems, yet LSTM give  better performance  when compared to  simple RNNs. It seems that  the memory gates in LSTM networks help better capture information in temporal sequences, even though they do not have  long-term dependencies. We note that the memory gates in LSTM networks were originally designed to cater for the vanishing gradient problem. It seems the memory gates of LSTM networks are helpful in capturing salient features in temporal series that help in predicting future tends much better than simple RNNs. We note that simple RNNs provided better results than simple neural networks (FNN-SGD and FNN-Adam) since they are more suited for temporal series. Moreover, we find striking results given that CNNs which are suited for image processing task performances better than simple RNNs in general. This could be due to the convolutional layers in CNNs that help in better capturing salient features for the temporal sequences. 
  
  Moving on, it is important to understand why the novel LSTM network model (ED-LSTM and BD-LSTM)  have given much better results. ED-LSTMS were designed for language modeling tasks, primarily sequence to sequence model for language translation where  encoder LSTM maps a source sequence to a fixed-length vector, and the decoder LSTM maps the vector representation back to a variable-length target sequence \cite{sutskever2014sequence}. In our case, the encoder maps an input time series to a fixed length vector and then the decoder LSTM maps the vector representation to the different prediction horizons. Although the application is different, the underlying task of mapping inputs to outputs remains the same and hence, ED-LSTM models have been very effective for multi-step ahead prediction. 
  
  We note that conventional recurrent  networks make use of only the previous context states for determining future states. BD-LSTMS on the other hand   processes information using two  LSTM models to feature forward and backward information about the sequence at every time step \cite{graves2005}. Although these have been useful for language modelling tasks, our results show that they are   applicable for mapping current and  future states for time series modelling since  information from past and future states are preserved that seems to be the key feature in achieving better performance for multi-step prediction problems when compared to conventional LSTM models. 
 
  
 
 \section{Conclusion and Future Work}
 
 In this paper, we  provide a comprehensive evaluation of emerging deep learning models for multi-step-ahead time series problems. Our results indicate that encoder-decoder and bi-directional LSTM networks provide   best performance for both simulated and real-world time series problems. The results have significantly improved over other related time series prediction methods given in the literature. 
 
 In future work, it would be worthwhile to provide similar evaluation for multivariate time series prediction problems. Moreover, it is worthwhile to investigate the performance of given deep learning models for spatial-temporal time series, such as the prediction of behavior of storms and cyclone and also further applications in other real-world problems such as air pollution and energy forecasting.  


 \section*{Software and Data}
We provide open source implementation in Python along with data for the respective methods  for further research \footnote{\url{https://github.com/sydney-machine-learning/deeplearning_timeseries}}.
 
 
\section*{Appendix}








\begin{table*}[htbp]
 \small 
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN & CNN\\
\hline
\hline
Train &  0.1628   0.0012 & 0.1703    0.0018  & 0.1471    0.0014  &  0.1454   0.0021 &  0.1437   0.0019  &  0.1930   0.0018   & 0.1655	0.0013\\
 Test &   0.0885   0.0011 & 0.0988    0.0040  & 0.0860    0.0025  &  0.0915   0.0023 & 0.0923   0.0032  & 0.0936   0.0009 & 0.0978	0.0011\\
Step-1 &  0.0165   0.0011 & 0.0209   0.0022  & 0.0127   0.0003  &  0.0127  0.0002 &  0.0130   0.0005  &  0.0173   0.0004  & 0.0193	0.0006\\
Step-2 &  0.0203   0.0012 & 0.0242    0.0020  & 0.0168    0.0003  &  0.0165   0.0002 &  0.0171   0.0003  &  0.0202   0.0003   &0.0217	0.0004\\
Step-3 &  0.0217   0.0008 & 0.0266    0.0032  & 0.0190    0.0004  &  0.0194   0.0002 &  0.0204   0.0006  &  0.0228   0.0003 & 0.0247	0.0003 \\
Step-4 &  0.0249  0.0009 & 0.0277    0.0023  & 0.0220   0.0004  &  0.0229   0.0003 & 0.0239   0.0008  &  0.0258   0.0004  &0.0266	0.0002 \\


Step-5 &  0.0272   0.0008 & 0.0299    0.0015  & 0.0248    0.0006  &  0.0253   0.0004 &  0.0271  0.0010  &  0.0284  0.0004 & 0.0290	0.0002 \\

Step-6 &  0.0289  0.0006 & 0.0325    0.0016  & 0.0281   0.0008 & 0.0292   0.0007 &  0.0302   0.0012  &  0.0304   0.0004 & 0.0315	0.0004 \\

Step-7 & 0.0311   0.0005 & 0.0342    0.0020  & 0.0302    0.0008  &  0.0331   0.0010 &  0.0334   0.0014  &  0.0327   0.0004 & 0.0340	0.0003 \\

Step 8 &  0.0323   0.0004 & 0.0350   0.0021  & 0.0333   0.0010  &  0.0356  0.0010 &  0.0359   0.0014  &  0.0348  0.0004  & 0.0363	0.0006 \\

Step 9 &  0.0339   0.0005 & 0.0357    0.0012  & 0.0364    0.0013  &  0.0388  0.0011 &  0.0380   0.0014  &  0.0371   0.0003 & 0.0386	0.0006 \\

Step 10 &  0.0357   0.0008 & 0.0380    0.0018  & 0.0367   0.0015  &  0.0409   0.0015 &  0.0395   0.0014  &  0.0384   0.0003 & 0.0401	0.0005 \\
\hline
\end{tabular}
\caption{ACI-finance reporting RMSE mean and 95 \% confidence interval   ().}
\label{tab:finance}
\end{table*}



\begin{table*}[htbp]
 \small 
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN & CNN\\
\hline
\hline
							
Train &      
0.2043	0.0047&
0.3230	0.0064&
0.1418	0.0034&
0.1369	0.0033&
0.1210	0.0057&
0.1875	0.0011&0.1695	0.0019\\
Test &      
0.1510	0.0024&
0.2179	0.0041&
0.1160	0.0021&
0.1147	0.0020&
0.1322	0.0032&
0.1342	0.0004&0.1487	0.0015\\
Step-1 &      
0.0163	0.0016&
0.0281	0.0032&
0.0072	0.0005&
0.0086	0.0006&
0.0109	0.0006&
0.0132	0.0004&0.0186	0.0002\\
Step-2 &      
0.0236	0.0015&
0.0352	0.0022&
0.0148	0.0007&
0.0155	0.0007&
0.0170	0.0004&
0.0212	0.0003&0.0257	0.0002\\
Step-3 &      
0.0311	0.0013&
0.0441	0.0025&
0.0220	0.0005&
0.0222	0.0006&
0.0237	0.0003&
0.0285	0.0002&0.0321	0.0003\\
Step-4 &      
0.0350	0.0006&
0.0518	0.0022&
0.0275	0.0005&
0.0276	0.0006&
0.0292	0.0003&
0.0346	0.0002&0.0376	0.0003\\
Step-5 &      
0.0407	0.0012&
0.0610	0.0024&
0.0321	0.0006&
0.0318	0.0007&
0.0348	0.0004&
0.0395	0.0002&	0.0419	0.0004\\
Step-6 &      
0.0464	0.0016&
0.0677	0.0027&
0.0360	0.0006&
0.0358	0.0006&
0.0402	0.0006&
0.0431	0.0002&0.0457	0.0004\\
Step-7 &      
0.0514	0.0019&
0.0771	0.0020&
0.0397	0.0006&
0.0395	0.0006&
0.0458	0.0011&
0.0463	0.0002&	0.0498	0.0005\\
Step 8 &      
0.0582	0.0019&
0.0856	0.0023&
0.0449	0.0007&
0.0440	0.0005&
0.0519	0.0016&
0.0503	0.0002&0.0555	0.0006\\
Step 9 &      
0.0653	0.0016&
0.0931	0.0023&
0.0509	0.0009&
0.0498	0.0007&
0.0590	0.0020&
0.0564	0.0002&	0.0633	0.0007\\
Step 10 &      
0.0745	0.0020&
0.1012	0.0019&
0.0587	0.0010&
0.0576	0.0010&
0.0673	0.0022&
0.0641	0.0003&	0.0723	0.0008\\
\hline
 
\end{tabular}
\caption{Sunspot reporting RMSE mean and 95 \% confidence interval   ().}
\label{tab:sunspot}
\end{table*}




\begin{table*}[htbp]
 \small 
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN &CNN\\
\hline
\hline
									
Train &  0.3371   0.0026 & 0.4251    0.0157  & 0.1954    0.0082  & 0.1619   0.0106 &  0.1166   0.0147  &  0.3210   0.0020  & 0.2151	0.0029\\

Test &  0.2537   0.0024 & 0.2821    0.0098  & 0.1910    0.0042  &  0.2007   0.0042 &  0.2020   0.0057  &  0.2580   0.0031  & 0.2240	0.0025\\

Step-1 &  0.0746   0.0027 & 0.0895    0.0041  & 0.0577    0.0021  &  0.0439   0.0027 & 0.0490   0.0039  &  0.0641   0.0037 & 0.0942	0.0025\\

Step-2 &  0.1043   0.0018 & 0.0983    0.0046  & 0.0725  0.0027  &  0.0892   0.0022 &  0.0894   0.0013  & 0.1176   0.0019  & 0.0729	0.0014\\

Step-3 &  0.0820  0.0026 & 0.0816    0.0028  & 0.0807    0.0007  &  0.0773  0.0016 & 0.0707   0.0014  & 0.0832  0.0026 & 0.0684	0.0022\\

Step-4 &  0.0764  0.0017 &0.0852    0.0048  &0.0697    0.0015  &  0.0547  0.0018 &  0.0601   0.0029  & 0.0762   0.0008  &	0.0671	0.0013 \\

Step-5 &  0.0761   0.0019 & 0.0874   0.0072  & 0.0512    0.0015  &  0.0596   0.0036 &  0.0694   0.0073  &  0.0755  0.0011 & 0.0701	0.0020\\

Step-6 &  0.0691   	0.0013 & 0.0787    0.0037  & 0.0540   0.0016 &  0.0655   	0.0014 &  0.0606   0.0041  & 0.0730   0.0015&  0.0677	0.0014 \\

Step-7 &  0.0632   0.0013 & 0.0740    	0.0061  & 0.0537    0.0024  & 0.0601   0.0007 &  0.0582   0.0031  &  0.0643  	0.0009& 	0.0643 	0.0041 \\

Step 8 &  
0.0642	 0.0020 & 
0.0864 	0.0053 & 
0.0464 	0.0015& 
0.0460 	0.0015& 
0.0510 	0.0027& 
0.0611 	0.0015& 0.0593	0.0029\\
Step 9 &  
0.0891	0.0021&
0.1032	0.0042&
0.0507	0.0021&
0.0599	0.0019&
0.0527	0.0021&
0.0882	0.0023& 0.0773	0.0019\\
Step 10 &  
0.0924	0.0018&
0.0968	0.0052&
0.0561	0.0044&
0.0631	0.0037&
0.0615	0.0030&
0.0947	0.0027& 0.0577	0.0018\\
\hline
 
\end{tabular}
\caption{Lazer reporting RMSE mean and 95 \% confidence interval   ().}
\label{tab:lazer}
\end{table*}





\begin{table*}[htbp]
 \small 
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN & CNN\\
\hline
\hline
		
Train &  0.5470   0.0023 & 0.5670    0.0015  & 0.4542    0.0071  &  0.4014   0.0100 &  0.3235   0.0316  &  0.5247   0.0027 & 0.4728 	0.0038\\

Test &  0.5378   0.0022 & 0.5578   0.0016  & 0.4516   0.0052  &  0.4127  0.0066 &  0.3294   0.0290  &  0.5162   0.0027&  	0.4779 	0.0027 \\

Step-1 &  0.1465   0.0058 & 0.1725    0.0031  & 0.0287    0.0045  & 0.0241   0.0014 &  0.0226   0.0039  & 0.0885   0.0093& 	0.0650 	0.0018  \\

Step-2 &  0.1606   0.0024 & 0.1711    0.0018  & 0.0682    0.0058  &  0.0448   0.0026 &  0.0454   0.0069  &  0.1515 0.0016 & 	0.0859	 0.0038 \\

Step-3 &  0.1610   0.0008 & 0.1707    0.0017  & 0.0920   0.0066  &  0.0610   0.0077 &  0.0517   0.0122  &  0.1577   0.0003&  	0.1411 	0.0021 \\

Step-4 &  0.1714   0.0009 & 0.1760    0.0011  & 0.1386   0.0044  &  0.0925   0.0077&  0.0609  0.0154  &  0.1643   0.0011 & 	0.1519 	0.0022\\

Step-5 &  0.1731   0.0005 &0.1769    0.0007  &0.1584    0.0010  &  0.1287   0.0046 &  0.0694   0.0161  &  0.1718   0.0001 & 	0.1601 	0.0007 \\

Step-6 &  0.1758   0.0006 & 0.1777    0.0010  & 0.1642    0.0007  &  0.1538   0.0017 &  0.0868   0.0165  &  0.1730   0.0003 & 	0.1674 	0.0004 \\

Step-7 &  0.1786   0.0006 & 0.1814    0.0009  & 0.1684  0.0011  &  0.1593   0.0016 &  0.1120   0.0139  &  0.1764   0.0003& 	0.1721 	0.0006  \\

Step 8 &  0.1781   0.0005 & 0.1805    0.0012  & 0.1707    0.0008  &  0.1697   0.0008 &  0.1371   0.0107  &  0.1768   0.0001 & 0.1718 	0.0003\\

Step 9 &  0.1757   0.0004 & 0.1788    0.0011  & 0.1723    0.0005  &  0.1737  0.0004 &  0.1524   0.0077  &  0.1752   0.0001 &  	0.1752 	0.0003\\

Step 10 &  0.1762   0.0009 & 0.1773    0.0011  & 0.1756    0.0005  &  0.1733   0.0003 &  0.1689   0.0046  &  0.1751   0.0002 &  0.1737 	0.0002\\

\hline
 
\end{tabular}
\caption{Henon reporting RMSE mean and 95 \% confidence interval   ().}
\label{tab:henon}
\end{table*}




\begin{table*}[htbp]
 \small 
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN & CNN\\
\hline
\hline
	
Train &  
0.1932	0.0124&
0.2761	0.0048&
0.0242	0.0086&
0.0300	0.0127&
0.0225	0.0031&
0.0538	0.0091&0.0354	0.0032\\
Test &  
0.1809	0.0119&
0.2649	0.0048&
0.0254	0.0093&
0.0310	0.0137&
0.0234	0.0031&
0.0542	0.0097&	0.0347	0.0029\\
Step-1 &  
0.0183	0.0042&
0.0336	0.0035&
0.0025	0.0006&
0.0043	0.0019&
0.0051	0.0015&
0.0113	0.0013&	0.0055	0.0006\\
Step-2 &  
0.0206	0.0046&
0.0432	0.0030&
0.0033	0.0010&
0.0054	0.0026&
0.0044	0.0012&
0.0129	0.0012&	0.0067	0.0007\\
Step-3 &    
0.0253	0.0043&
0.0547	0.0028&
0.0042	0.0019&
0.0064	0.0031&
0.0046	0.0010&
0.0143	0.0016&	0.0077	0.0009\\
Step-4 &    
0.0334	0.0048&
0.0651	0.0032&
0.0051	0.0020&
0.0074	0.0035&
0.0052	0.0009&
0.0151	0.0018&0.0087	0.0009\\
Step-5 &    
0.0481	0.0072&
0.0787	0.0030&
0.0064	0.0026&
0.0079	0.0036&
0.0059	0.0009&
0.0155	0.0024&	0.0098	0.0009\\
Step-6 &    
0.0527	0.0076&
0.0866	0.0033&
0.0073	0.0029&
0.0094	0.0046&
0.0068	0.0008&
0.0164	0.0029&	0.0109	0.0010\\
Step-7 &    
0.0613	0.0064&
0.0944	0.0031&
0.0089	0.0033&
0.0107	0.0047&
0.0079	0.0009&
0.0171	0.0036&	0.0120	0.0010\\
Step 8 &    
0.0678	0.0058&
0.1027	0.0025&
0.0101	0.0038&
0.0125	0.0057&
0.0090	0.0009&
0.0186	0.0042&	0.0132	0.0011\\
Step 9 &    
0.0885	0.0060&
0.1116	0.0027&
0.0117	0.0045&
0.0133	0.0056&
0.0100	0.0010&
0.0199	0.0049&	0.0142	0.0013\\
Step 10 &    
0.0859	0.0065&
0.1178	0.0026&
0.0129	0.0042&
0.0146	0.0059&
0.0110	0.0012&
0.0226	0.0058&	0.0157	0.0015\\
\hline
 
\end{tabular}
\caption{Lorenz reporting RMSE mean and 95 \% confidence interval   ().}
\label{tab:lorenz}
\end{table*}



\begin{table*}[htbp]
 \small 
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN & CNN\\
				
\hline
\hline
Train &      
0.1810	0.0097&
0.2576	0.0053&
0.0890	0.0076&
0.0750	0.0067&
0.0587	0.0090&
0.1317	0.0003& 0.0854		0.0052\\
Test &      
0.1822	0.0098&
0.2599	0.0054&
0.0897	0.0075&
0.0765	0.0068&
0.0602	0.0090&
0.1321	0.0003&0.0868		0.0048\\
Step-1 &      
0.0162	0.0037&
0.0485	0.0061&
0.0056	0.0011&
0.0052	0.0009&
0.0059	0.0010&
0.0078	0.0001&	0.0075		0.0007\\
Step-2 &      
0.0256	0.0038&
0.0621	0.0051&
0.0080	0.0014&
0.0083	0.0015&
0.0076	0.0014&
0.0142	0.0001&0.0120		0.0010\\
Step-3 &      
0.0398	0.0051&
0.0686	0.0046&
0.0120	0.0018&
0.0117	0.0019&
0.0103	0.0020&
0.0214	0.0001&	0.0167		0.0013\\
Step-4 &      
0.0457	0.0045&
0.0745	0.0049&
0.0178	0.0020&
0.0155	0.0023&
0.0133	0.0024&
0.0290	0.0001&0.0216		0.0015\\
Step-5 &      
0.0520	0.0044&
0.0785	0.0025&
0.0238	0.0024&
0.0202	0.0026&
0.0168	0.0027&
0.0365	0.0001&	0.0262		0.0016\\
Step-6 &      
0.0581	0.0042&
0.0880	0.0031&
0.0298	0.0025&
0.0244	0.0027&
0.0200	0.0031&
0.0434	0.0001&0.0301		0.0017\\
Step-7 &      
0.0680	0.0047&
0.0912	0.0031&
0.0341	0.0028&
0.0288	0.0028&
0.0227	0.0033&
0.0496	0.0001&	0.0332		0.0018\\
Step 8 &      
0.0727	0.0050&
0.0937	0.0022&
0.0381	0.0029&
0.0318	0.0027&
0.0248	0.0036&
0.0547	0.0001&	0.0354		0.0018\\
Step 9 &      
0.0761	0.0046&
0.0963	0.0031&
0.0406	0.0030&
0.0343	0.0027&
0.0261	0.0038&
0.0586	0.0001&	0.0364		0.0017\\
Step 10 &      
0.0777	0.0043&
0.0990	0.0026&
0.0418	0.0033&
0.0359	0.0026&
0.0271	0.0040&
0.0615	0.0001&	0.0364		0.0017\\
\hline
 &
\end{tabular}
\caption{Mackey-Glass reporting RMSE mean and 95 \% confidence interval   ().}
\label{tab:mackey}
\end{table*}



\begin{table*}[htbp]
 \small 
\begin{tabular}{llllllll}
\hline
 &  FNN-Adam& FNN-SGD & LSTM & BD-LSTM  & ED-LSTM & RNN& CNN \\
\hline
\hline
		
Train &      
0.1546	0.0087&
0.3757	0.0100&
0.0416	0.0074&
0.0281	0.0098&
0.0374	0.0082&
0.1088	0.0009&0.0367		0.0059\\
Test &      
0.1473	0.0098&
0.4314	0.0122&
0.0488	0.0054&
0.0349	0.0070&
0.0427	0.0072&
0.1030	0.0008&	0.0454		0.0052\\
Step-1 &      
0.0148	0.0026&
0.0467	0.0077&
0.0080	0.0009&
0.0038	0.0008&
0.0085	0.0025&
0.0186	0.0009&	0.0086		0.0010\\
Step-2 &      
0.0202	0.0024&
0.0666	0.0058&
0.0086	0.0011&
0.0047	0.0014&
0.0082	0.0019&
0.0218	0.0005&0.0105		0.0011\\
Step-3 &      
0.0252	0.0022&
0.0910	0.0083&
0.0099	0.0013&
0.0061	0.0017&
0.0098	0.0018&
0.0250	0.0005&	0.0118		0.0011\\
Step-4 &      
0.0322	0.0024&
0.1060	0.0078&
0.0117	0.0014&
0.0072	0.0020&
0.0112	0.0021&
0.0290	0.0004&	0.0122		0.0014\\
Step-5 &      
0.0400	0.0039&
0.1257	0.0082&
0.0135	0.0015&
0.0084	0.0021&
0.0128	0.0021&
0.0314	0.0004&0.0122		0.0016\\
Step-6 &      
0.0490	0.0063&
0.1424	0.0069&
0.0155	0.0019&
0.0100	0.0023&
0.0141	0.0021&
0.0339	0.0004&	0.0128		0.0019\\
Step-7 &      
0.0527	0.0049&
0.1572	0.0063&
0.0170	0.0020&
0.0120	0.0024&
0.0151	0.0022&
0.0360	0.0004&	0.0139		0.0020\\
Step 8 &      
0.0603	0.0050&
0.1664	0.0075&
0.0185	0.0022&
0.0142	0.0027&
0.0159	0.0024&
0.0382	0.0004&	0.0157		0.0020\\
Step 9 &      
0.0621	0.0036&
0.1818	0.0067&
0.0204	0.0024&
0.0157	0.0030&
0.0167	0.0026&
0.0404	0.0005&	0.0185		0.0021\\
Step 10 &      
0.0673	0.0056&
0.1881	0.0078&
0.0225	0.0026&
0.0178	0.0032&
0.0180	0.0030&
0.0424	0.0004&	0.0220		0.0022\\
\hline
 
\end{tabular}
\caption{Rossler reporting RMSE mean and 95 \% confidence interval   ().}
\label{tab:rossler}
\end{table*}



\bibliographystyle{IEEEtran}
\bibliography{usyd,Chandra-Rohitash,Bays,2018,rr,sample,sample_,aicrg,2020June}














\end{document}
