\pdfoutput=1


\documentclass{article}

\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure} 

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2016}


\icmltitlerunning{Exploring the Limits of Language Modeling}

\newcommand\todo[1]{\textcolor{blue}{\textbf{TODO:} #1}}

\begin{document} 

\twocolumn[
\icmltitle{Exploring the Limits of Language Modeling}

\icmlauthor{Rafal Jozefowicz}{rafalj@google.com}
\icmlauthor{Oriol Vinyals}{vinyals@google.com}
\icmlauthor{Mike Schuster}{schuster@google.com}
\icmlauthor{Noam Shazeer}{noam@google.com}
\icmlauthor{Yonghui Wu}{yonghui@google.com}
\icmladdress{\vspace{0.2cm} Google Brain}



\vskip 0.3in
]

\begin{abstract} 
In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.
\end{abstract} 
 \section{Introduction}
\label{intro}

Language Modeling (LM) is a task central to Natural Language Processing (NLP) and Language Understanding. Models which can accurately place distributions over sentences not only encode complexities of language such as grammatical structure, but also distill a fair amount of information about the knowledge that a corpora may contain. Indeed, models that are able to assign a low probability to sentences that are grammatically correct but unlikely may help other tasks in fundamental language understanding like question answering, machine translation, or text summarization.

LMs have played a key role in traditional NLP tasks such as speech recognition \cite{mikolov2010recurrent, arisoy2012deep}, machine translation \cite{schwenk2012large, vaswani2013decoding}, or text summarization \cite{rush2015neural,filippova2015sentence}. Often (although not always), training better language models improves the underlying metrics of the downstream task (such as word error rate for speech recognition, or BLEU score for translation), which makes the task of training better LMs valuable by itself.

Further, when trained on vast amounts of data, language models compactly extract knowledge encoded in the training data. For example, when trained on movie subtitles \cite{serban2015,vinyals2015neural}, these language models are able to generate basic answers to questions about object colors, facts about people, etc. Lastly, recently proposed sequence-to-sequence models employ conditional language models \cite{mikolov2012context} as their key component to solve diverse tasks like machine translation \cite{sutskever2014sequence, cho2014learning, kalchbrenner2014convolutional} or video generation \cite{srivastava2015unsupervised}.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figure1_LMPAPER.png}}
\caption{A high-level diagram of the models presented in this paper. (a) is a standard LSTM LM. (b) represents an LM where both input and Softmax embeddings have been replaced by a character CNN. In (c) we replace the Softmax by a next character prediction LSTM network.}
\label{main_figure}
\end{center}
\vspace{-10pt}
\end{figure} 

Deep Learning and Recurrent Neural Networks (RNNs) have fueled language modeling research in the past years as it allowed researchers to explore many tasks for which the strong conditional independence assumptions are unrealistic. Despite the fact that simpler models, such as N-grams, only use a short history of previous words to predict the next word, they are still a key component to high quality, low perplexity LMs. Indeed, most recent work on large scale LM has shown that RNNs are great in combination with N-grams, as they may have different strengths that complement N-gram models, but worse when considered in isolation \cite{mikolov2011empirical, mikolov2012statistical, chelba2013one,williams2015scaling,Blackout,shazeer2015sparse}.

We believe that, despite much work being devoted to small data sets like the Penn Tree Bank (PTB) \cite{marcus1993building}, research on larger tasks is very relevant as overfitting is not the main limitation in current language modeling, but is the main characteristic of the PTB task. Results on larger corpora usually show better what matters as many ideas work well on small data sets but fail to improve on larger data sets. Further, given current hardware trends and vast amounts of text available on the Web, it is much more straightforward to tackle large scale modeling than it used to be. Thus, we hope that our work will help and motivate researchers to work on traditional LM beyond PTB -- for this purpose, we will open-source our models and training recipes. 

We focused on a well known, large scale LM benchmark: the One Billion Word Benchmark data set \cite{chelba2013one}. This data set is much larger than PTB (one thousand fold, ~800k word vocabulary and ~1B words training data) and far more challenging. Similar to Imagenet \cite{deng2009imagenet}, which helped advance computer vision, we believe that releasing and working on large data sets and models with clear benchmarks will help advance Language Modeling.

The contributions of our work are as follows:

\begin{itemize}
\item We explored, extended and tried to unify some of the current research on large scale LM.
\item Specifically, we designed a Softmax loss which is based on character level CNNs, is efficient to train, and is as precise as a full Softmax which has orders of magnitude more parameters.
\item Our study yielded significant improvements to the state-of-the-art on a well known, large scale LM task: from 51.3 down to 30.0 perplexity for single models whilst reducing the number of parameters by a factor of 20.
\item We show that an ensemble of a number of different models can bring down perplexity on this task to 23.7, a large improvement compared to current state-of-art.
\item We share the model and recipes in order to help and motivate further research in this area.
\end{itemize}

In Section~\ref{relwork} we review important concepts and previous work on language modeling. Section~\ref{model} presents our contributions to the field of neural language modeling, emphasizing large scale recurrent neural network training. Sections \ref{exps} and \ref{results} aim at exhaustively describing our experience and understanding throughout the project, as well as emplacing our work relative to other known approaches.
 \section{Related Work}
\label{relwork}

In this section we describe previous work relevant to the approaches discussed in this paper. A more detailed discussion on language modeling research is provided in \cite{mikolov2012statistical}.

\subsection{Language Models}
\label{lms}

Language Modeling (LM) has been a central task in NLP. The goal of LM is to learn a probability distribution over sequences of symbols pertaining to a language. Much work has been done on both parametric (e.g., log-linear models) and non-parametric approaches (e.g., count-based LMs). Count-based approaches (based on statistics of N-grams) typically add smoothing which account for unseen (yet possible) sequences, and have been quite successful. To this extent, Kneser-Ney smoothed 5-gram models \cite{kneser1995improved} are a fairly strong baseline which, for large amounts of training data, have challenged other parametric approaches based on Neural Networks \cite{bengio2006neural}.

Most of our work is based on Recurrent Neural Networks (RNN) models which retain long term dependencies. To this extent, we used the Long-Short Term Memory model \cite{hochreiter1997long} which uses a gating mechanism \cite{gers2000learning} to ensure proper propagation of information through many time steps. Much work has been done on small and large scale RNN-based LMs \cite{mikolov2010recurrent,  mikolov2012statistical, chelba2013one,zaremba2014recurrent, williams2015scaling,Blackout,wang2015larger,ji2015document}. The architectures that we considered in this paper are represented in Figure~\ref{main_figure}.

In our work, we train models on the popular One Billion Word Benchmark, which can be considered to be a medium-sized data set for count-based LMs but a very large data set for NN-based LMs. This regime is most interesting to us as we believe learning a very good model of human language is a complex task which will require large models, and thus large amounts of data. Further advances in data availability and computational resources helped our study. We argue this leap in scale enabled tremendous advances in deep learning. A clear example found in computer vision is Imagenet \cite{deng2009imagenet}, which enabled learning complex vision models from large amounts of data \cite{krizhevsky2012imagenet}.

A crucial aspect which we discuss in detail in later sections is the size of our models. Despite the large number of parameters, we try to minimize computation as much as possible by adopting a strategy proposed in \cite{sak2014long} of projecting a relatively big recurrent state space down so that the matrices involved remain relatively small, yet the model has large memory capacity.

\subsection{Convolutional Embedding Models}
\label{input_cnn}

There is an increased interest in incorporating character-level inputs to build word embeddings for various NLP problems, including part-of-speech tagging, parsing and language modeling \cite{ling2015finding, kim2015character, ballesteros2015improved}. The additional character information has been shown useful on relatively small benchmark data sets.

The approach proposed in \cite{ling2015finding} builds word embeddings using bidirectional LSTMs \cite{schuster1997brnn,graves2005framewise} over the characters. The recurrent networks process sequences of characters from both sides and their final state vectors are concatenated. The resulting representation is then fed to a Neural Network. This model achieved very good results on a part-of-speech tagging task.

In \cite{kim2015character}, the words characters are processed by a 1-d CNN \cite{le1990handwritten} with max-pooling across the sequence for each convolutional feature. The resulting features are fed to a 2-layer highway network \cite{srivastava2015training}, which allows the embedding to learn semantic representations. The model was evaluated on small-scale language modeling experiments for various languages and matched the best results on the PTB data set despite having 60\% fewer parameters.

\subsection{Softmax Over Large Vocabularies}
\label{large_soft}

Assigning probability distributions over large vocabularies is computationally challenging. For modeling language, maximizing log-likelihood of a given word sequence leads to optimizing cross-entropy between the target probability distribution (e.g., the target word we should be predicting), and our model predictions . Generally, predictions come from a linear layer followed by a Softmax non-linearity:  where  is the logit corresponding to a word . The logit is generally computed as an inner product  where  is a context vector and  is a ``word embedding'' for .

The main challenge when  is very large (in the order of one million in this paper) is the fact that computing all inner products between  and all embeddings becomes prohibitively slow during training (even when exploiting matrix-matrix multiplications and modern GPUs). Several approaches have been proposed to cope with the scaling issue: importance sampling \cite{bengio2003quick, bengio2008adaptive}, Noise Contrastive Estimation (NCE) \cite{gutmann2010noise, mnih2013learning}, self normalizing partition functions \cite{vincent2015efficient} or Hierarchical Softmax \cite{morin2005hierarchical, mnih2009scalable} -- they all offer good solutions to this problem. We found importance sampling to be quite effective on this task, and explain the connection between it and NCE in the following section, as they are closely related.
 \section{Language Modeling Improvements}
\label{model}

Recurrent Neural Networks based LMs employ the chain rule to model joint probabilities over word sequences:


where the context of all previous words is encoded with an LSTM, and the probability over words uses a Softmax (see Figure~\ref{main_figure}(a)).


\subsection{Relationship between Noise Contrastive Estimation and Importance Sampling}
\label{is_soft}

As discussed in Section~\ref{large_soft}, a large scale Softmax is necessary for training good LMs because of the vocabulary size. A Hierarchical Softmax \cite{mnih2009scalable} employs a tree in which the probability distribution over words is decomposed into a product of two probabilities for each word, greatly reducing training and inference time as only the path specified by the hierarchy needs to be computed and updated. Choosing a good hierarchy is important for obtaining good results and we did not explore this approach further for this paper as sampling methods worked well for our setup.

Sampling approaches are only useful during training, as they propose an approximation to the loss which is cheap to compute (also in a distributed setting) -- however, at inference time one still has to compute the normalization term over all words. Noise Contrastive Estimation (NCE) proposes to consider a surrogate binary classification task in which a classifier is trained to discriminate between true data, or samples coming from some arbitrary distribution. If both the noise and data distributions were known, the optimal classifier would be:


where  is the binary random variable indicating whether  comes from the true data distribution,  is the number of negative samples per positive word, and  and  are the data and noise distribution respectively (we dropped any dependency on previous words for notational simplicity).

It is easy to show that if we train a logistic classifier  where  is the logistic function, then,  is a good approximation of  ( is a logit which e.g. an LSTM LM computes).

The other technique, which is based on importance sampling (IS), proposes to directly approximate the partition function (which comprises a sum over all words) with an estimate of it through importance sampling. Though the methods look superficially similar, we will derive a similar surrogate classification task akin to NCE which arrives at IS, showing a strong connection between the two.

Suppose that, instead of having a binary task to decide if a word comes from the data or from the noise distribution, we want to identify the words coming from the true data distribution in a set , comprised of  noise samples and one data distribution sample. Thus, we can train a multiclass loss over a multinomial random variable  which maximizes , assuming w.l.o.g. that  is always the word coming from true data. By Bayes rule, and ignoring terms that are constant with respect to , we can write:


and, following a similar argument than for NCE, if we define  then  is a good approximation of . Note that the only difference between NCE and IS is that, in NCE, we define a binary classification task between true or noise words with a logistic loss, whereas in IS we define a multiclass classification problem with a Softmax and cross entropy loss. We hope that our derivation helps clarify the similarities and differences between the two. In particular, we observe that IS, as it optimizes a multiclass classification task (in contrast to solving a binary task), may be a better choice. Indeed, the updates to the logits with IS are tied whereas in NCE they are independent.

\subsection{CNN Softmax}
\label{cnn_softmax}

The character-level features allow for a smoother and compact parametrization of the word embeddings. Recent efforts on small scale language modeling have used CNN character embeddings for the input embeddings \cite{kim2015character}. Although not as straightforward, we propose an extension to this idea to also reduce the number of parameters of the Softmax layer. Recall from Section~\ref{large_soft} that the Softmax computes a logit as  where  is a context vector and  the word embedding. Instead of building a matrix of  (whose rows correspond to ), we produce  with a CNN over the characters of  as  -- we call this a CNN Softmax. We used the same network architecture to dynamically generate the Softmax word embeddings without sharing the parameters with the input word-embedding sub-network. For inference, the vectors  can be precomputed, so there is no computational complexity increase w.r.t. the regular Softmax.

We note that, when using an importance sampling loss such as the one described in Section~\ref{is_soft}, only a few logits have non-zero gradient (those corresponding to the true and sampled words). With a Softmax where  are independently learned word embeddings, this is not a problem. But we observed that, when using a CNN, all the logits become tied as the function mapping from  to  is quite smooth. As a result, a much smaller learning rate had to be used. Even with this, the model lacks capacity to differentiate between words that have very different meanings but that are spelled similarly. Thus, a reasonable compromise was to add a small correction factor which is learned per word, such that:


where  is a matrix projecting a low-dimensional embedding vector  back up to the dimensionality of the projected LSTM hidden state of . This amounts to adding a bottleneck linear layer, and brings the CNN Softmax much closer to our best result, as can be seen in Table~\ref{main-results-table}, where adding a 128-dim correction halves the gap between regular and the CNN Softmax.

Aside from a big reduction in the number of parameters and incorporating morphological knowledge from words, the other benefit of this approach is that out-of-vocabulary (OOV) words can easily be scored. This may be useful for other problems such as Machine Translation where handling out-of-vocabulary words is very important \cite{luong2014addressing}. This approach also allows parallel training over various data sets since the model is no longer explicitly parametrized by the vocabulary size -- or the language. This has shown to help when using byte-level input embeddings for named entity recognition \cite{gillick2015multilingual}, and we hope it will enable similar gains when used to map onto words.

\begin{table*}[t!]
\caption{Best results of single models on the 1B word benchmark. Our results are shown below previous work.}
\label{main-results-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcr}
\hline
\abovespace\belowspace
Model & Test Perplexity & Number of Params [billions] \\
\hline
\abovespace
Sigmoid-RNN-2048 \cite{Blackout} & 68.3 & 4.1 \\
Interpolated KN 5-gram, 1.1B n-grams \cite{chelba2013one} & 67.6 & 1.76 \\
Sparse Non-Negative Matrix LM \cite{shazeer2015sparse} & 52.9 & 33 \\
RNN-1024 + MaxEnt 9-gram features \cite{chelba2013one} & 51.3 & 20 \\
\hline
\abovespace
LSTM-512-512 & 54.1 & 0.82 \\
LSTM-1024-512 & 48.2 & 0.82 \\
LSTM-2048-512 & 43.7 & 0.83 \\
LSTM-8192-2048 (No Dropout) & 37.9 & 3.3 \\
LSTM-8192-2048 (50\% Dropout) & 32.2 & 3.3 \\
2-Layer LSTM-8192-1024 (BIG LSTM) & 30.6 & 1.8 \\
BIG LSTM+CNN Inputs & \textbf{30.0} & \textbf{1.04} \\
\abovespace
BIG LSTM+CNN Inputs + CNN Softmax & 39.8 & \textbf{0.29} \\
BIG LSTM+CNN Inputs + CNN Softmax + 128-dim correction & 35.8 & \textbf{0.39} \\
BIG LSTM+CNN Inputs + Char LSTM predictions & 47.9 & \textbf{0.23} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Char LSTM Predictions}
\label{charlstm}

The CNN Softmax layer can handle arbitrary words and is much more efficient in terms of number of parameters than the full Softmax matrix. It is, though, still considerably slow, as to evaluate perplexities we need to compute the partition function. A class of models that solve this problem more efficiently are character-level LSTMs \cite{sutskever2011generating, graves2013generating}. They make predictions one character at a time, thus allowing to compute probabilities over a much smaller vocabulary. On the other hand, these models are more difficult to train and seem to perform worse even in small tasks like PTB \cite{graves2013generating}. Most likely this is due to the sequences becoming much longer on average as the LSTM reads the input character by character instead of word by word.

Thus, we combine the word and character-level models by feeding a word-level LSTM hidden state  into a small LSTM that predicts the target word one character at a time (see Figure~\ref{main_figure}(c)). In order to make the whole process reasonably efficient, we train the standard LSTM model until convergence, freeze its weights, and replace the standard word-level Softmax layer with the aforementioned character-level LSTM.

The resulting model scales independently of vocabulary size -- both for training and inference. However, it does seem to be worse than regular and CNN Softmax -- we are hopeful that further research will enable these models to replace fixed vocabulary models whilst being computationally attractive.
 \section{Experiments}
\label{exps}

All experiments were run using the TensorFlow system \cite{tensorflow2015-whitepaper}, with the exception of some older models which were used in the ensemble.

\subsection{Data Set}
\label{data set}

The experiments are performed on the 1B Word Benchmark data set introduced by \cite{chelba2013one}, which is a publicly available benchmark for measuring progress of statistical language modeling. The data set contains about 0.8B words with a vocabulary of 793471 words, including sentence boundary markers. All the sentences are shuffled and the duplicates are removed. The words that are out of vocabulary (OOV) are marked with a special UNK token (there are approximately 0.3\% such words).

\subsection{Model Setup}
\label{experiments}

The typical measure used for reporting progress in language modeling is perplexity, which is the average per-word log-probability on the holdout data set: . We follow the standard procedure and sum over all the words (including the end of sentence symbol).

We used the 1B Word Benchmark data set without any pre-processing. Given the shuffled sentences, they are input to the network as a batch of independent streams of words. Whenever a sentence ends, a new one starts without any padding (thus maximizing the occupancy per batch).

For the models that consume characters as inputs or as targets, each word is fed to the model as a sequence of character IDs of preespecified length (see Figure~\ref{main_figure}(b)). The words were processed to include special begin and end of word tokens and were padded to reach the expected length. I.e. if the maximum word length was 10, the word ``\texttt{cat}'' would be transformed to ``\texttt{\<S><S><S><S><S><S>$ Yuri Zhirkov was in attendance at the Stamford Bridge at the start of the second half but neither Drogba nor Malouda was able to push on through the Barcelona defence .
\end{quotation}

 \section{Discussion and Conclusions}
\label{discussion}

In this paper we have shown that RNN LMs can be trained on large amounts of data, and outperform competing models including carefully tuned N-grams. The reduction in perplexity from 51.3 to 30.0 is due to several key components which we studied in this paper. Thus, a large, regularized LSTM LM, with projection layers and trained with an approximation to the true Softmax with importance sampling performs much better than N-grams. Unlike previous work, we do not require to interpolate both the RNN LM and the N-gram, and the gains of doing so are rather marginal.

By exploring recent advances in model architectures (e.g. LSTMs), exploiting small character CNNs, and by sharing our findings in this paper and accompanying code and models (to be released upon publication), we hope to inspire research on large scale Language Modeling, a problem we consider crucial towards language understanding. We hope for future research to focus on reasonably sized datasets taking inspiration from recent advances seen in the computer vision community thanks to efforts such as Imagenet \cite{deng2009imagenet}. 
\section*{Acknowledgements} 

We thank Ciprian Chelba, Ilya Sutskever, and the Google Brain Team for their help and discussions. We also thank Koray Kavukcuoglu for his help with the manuscript.

\bibliography{lm_paper}
\bibliographystyle{icml2016}

\end{document} 
