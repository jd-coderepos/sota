

\documentclass[final]{cvpr}
\pdfoutput=1

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{2065} \def\confYear{CVPR 2021}



\begin{document}

\title{TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events}
\author{Li Xu \qquad He Huang \qquad Jun Liu\thanks{Corresponding Author.} \\
Information Systems Technology and Design \\
Singapore University of Technology and Design \\
{\tt\small \{li\_xu, he\_huang\}@mymail.sutd.edu.sg, jun\_liu@sutd.edu.sg}}


\maketitle
\pagestyle{empty}
\thispagestyle{empty}

\begin{figure*}[htbp]
\begin{center}
    \includegraphics[width=0.8\linewidth,,trim=1.5cm 0.5cm 1.5cm 1cm,clip]{featured_v2_2.pdf}
\end{center}
\vspace{-0.6cm}
    \caption{An example of our TrafficQA dataset showing that a white sedan had missed the highway exit. Hence it chose to change the lane illegally for driving towards the exit. To avoid collision, the blue truck had to brake suddenly, and then an accident occurred.
Six reasoning tasks are designed, covering a broad range of inference problems from basic understanding to complex reasoning and attribution analysis.
    To accurately answer these questions, the models need to explore the causal, logic, and spatio-temporal structures of the video event.
    }
\label{fig:example}
\vspace{-0.2cm}
\end{figure*}




\begin{abstract}
Traffic event cognition and reasoning in videos is an important task that has a wide range of applications in intelligent transportation, assisted driving, and autonomous vehicles.
In this paper, we create a novel dataset, TrafficQA (Traffic Question Answering), which takes the form of video QA based on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of 
causal inference and event understanding models in complex traffic scenarios.
Specifically, we propose 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events. 
Moreover, we propose \textbf{Eclipse}, a novel \textbf{E}ffi\textbf{c}ient g\textbf{li}m\textbf{pse} network via dynamic inference, in order to achieve computation-efficient and reliable video reasoning. The experiments show that our method achieves superior performance while reducing the computation cost significantly. The project page: \url{https://github.com/SUTDCV/SUTD-TrafficQA}.


\end{abstract}

\section{Introduction}

Intelligent transportation \cite{5959985} has been receiving increasing attention recently, and for the applications, such as assisted driving, violation detection, and congestion forecasting, accurate and efficient cognition and reasoning over the traffic events captured by video cameras is extremely important. As shown by previous works \cite{10.3115/1073012.1073017,35179}, well-designed datasets are often crucial for the development, adaptation and evaluation of different data-driven approaches. 
This indicates the significance of creating comprehensive and challenging benchmarks for video causal reasoning
and cognitive development of models, 
that explore the underlying causal structures of various traffic events. To this end, we introduce a novel dataset, TrafficQA (Traffic Question Answering), to facilitate the research of causal reasoning in complex traffic scenarios.

In our dataset, to help develop models for addressing several major and concerning issues in intelligent transportation, we design 6 challenging reasoning tasks, which require exploring the complex causal structures within the inference process of the traffic events. As shown in Figure \ref{fig:example},
these tasks correspond to various traffic scenarios involving both road-agents and surroundings, and the models are required to forecast future events, infer past situations, explain accident causes, provide preventive advice, and so on. 

To present these reasoning tasks, video question answering \cite{zhu2017uncovering} is a natural and effective choice, and is used for our dataset construction, since to accurately answer the given questions, the models need to acquire strong capabilities of performing various levels of logical reasoning and spatio-temporal cognition for the events.

Besides providing the challenging and useful reasoning tasks, 
we adopt a combination scheme of online collection and offline capturing to collect videos, such that the data in our benchmark covers various traffic events, diversified road-agents and surroundings, and different capturing perspectives in the wild. With the provided various tasks and diverse videos, our dataset shall be able to serve as a comprehensive benchmark for video reasoning of traffic events.

In some application scenarios, (e.g., assisted driving), the computational resource and energy budget can be constrained.
Thus 
both the inference accuracy and the computation efficiency are important for video event reasoning in these scenarios.
Existing video QA methods \cite{Kim_2020_CVPR,Le_2020_CVPR,lei-etal-2020-tvqa} mainly focus on strengthening the reasoning accuracy without emphasizing much efficiency, and most of
these works apply fixed computation pipelines 
to answer different questions, while ignoring to conduct adaptive and efficient computation resource allocation based on the logic structure behind reasoning over video events. 




In this paper, to achieve reliable and efficient video reasoning, we propose \textbf{Eclipse}, an \textbf{E}ffi\textbf{c}ient g\textbf{li}m\textbf{pse} network.
Specifically, considering there is often large redundancy among video frames, via dynamic inference, our network adaptively determines where to skip and glimpse at each step, and what computation granularity needs to be allocated for the glimpsed frame. Such a dynamic reasoning scheme avoids feature extraction for the irrelevant segments in the video, and hence significantly reduces the overall computation cost towards 
reliable and efficient reasoning.
It is noteworthy that both the determination of selecting a glimpse frame and the decision of computation granularity for each glimpse are essentially discrete operations, which are 
not trivial to optimize. To handle this issue, an effective joint Gumbel-Softmax mechanism is also introduced in this paper, which makes our Eclipse framework fully differentiable and end-to-end trainable. 

To the best of our knowledge, this is the first work that simultaneously performs adaptive frame localization and feature granularity determination in a novel dynamic reasoning process for reliable and efficient causal reasoning and video QA. A joint Gumbel-Softmax operation is also introduced in this work to optimize the two decisions jointly.




\section{Related Works}
\textbf{Intelligent Transportation.} 
With the rapid development of deep learning techniques \cite{krizhevsky2012imagenet, ren2015faster, vaswani2017attention}, data-driven intelligent transportation \cite{chandra2020forecasting,lou2019veri,you2020traffic,chandra2020forecasting} has emerged as a prominent research topic.
Lou et al. \cite{lou2019veri} presented a dataset together with an adversarial learning model for vehicle re-identification.
Different from existing intelligent transportation datasets and methods,
in this paper, we investigate the problem of causal reasoning with video QA over various traffic scenarios. A new benchmark, TrafficQA, together with a novel model, Eclipse, is proposed for this challenging task.


\begin{table*}[t]
\caption{Comparison among TrafficQA and some other video QA datasets. Providing challenging \textbf{traffic-scenario reasoning tasks} with \textbf{real-world videos} and \textbf{human-generated QA pairs}, our dataset shall serve as a comprehensive and challenging benchmark for video reasoning over traffic events.}
\vspace{-0.2cm}
\begin{center}
\scriptsize
\setlength\tabcolsep{0.5pt}
\begin{tabular}{c|c|ccccccc} \hline
\multirow{2}{*}{\textbf{Dataset}} & \textbf{Synthetic Videos} & \multicolumn{7}{c}{\textbf{Real-World Videos}} \\
\cline{2-9}
& CLEVRER \cite{yi2019clevrer} 
        & MovieQA \cite{tapaswi2016movieqa} & MSRVTT-QA \cite{xu2017video} & TGIF-QA \cite{jang2017tgif} & TVQA \cite{lei2018tvqa} &  MarioQA \cite{mun2017marioQA} & Social-IQ \cite{zadeh2019social} &  \textbf{TrafficQA (Ours)} \\ \hline
Basic Understanding  &  &  &  &  &  &  &  & \textbf{} \\ 
Attribution  &  &  &  &  &  &  &  & \textbf{} \\
Event Forecasting &  &     &  &      &      &      &      & \textbf{}  \\ 
Reverse Reasoning  &      &      &  &      &      &      &      & \textbf{} \\ 
Counterfactual Inference &  &     &  &      &      &      &      & \textbf{} \\ 
Introspection   &      &     & &       &      &      &      & \textbf{} \\
\hline
QA Generation & Automatic & Human & Automatic & Automatic \& Human & Human & Automatic & Human & Human \\ \hline
Topic & Synthetic Object Collision & Movies & Various Scene & GIFs & TV-Shows & Gameplay & Social Behavior & Traffic Events\\ \hline
\end{tabular}
\end{center}
\label{table:dataset_comparison}
\vspace{-0.5cm}
\end{table*}

\textbf{Video QA Datasets.}
Recently, there emerges a great interest in visual reasoning and question answering in videos \cite{10.1007/s11263-016-0987-1,maharaj2017dataset,zhu2017uncovering,ijcai2017-280}, and several video QA datasets \cite{xu2017video,zeng2016leveraging,tapaswi2016movieqa,zadeh2019social,jang2017tgif,garcia2020knowit,emrvqasongMM18,yu2019activityqa} have been developed. Among them, 
MovieQA \cite{tapaswi2016movieqa} and TVQA \cite{lei2018tvqa} present the movie and TV-show videos respectively with human-generated questions. More recently, 
CLEVRER \cite{yi2019clevrer} focuses on collision event reasoning among several simple visual objects
in a controlled environment using fully synthetic videos. Differently, our TrafficQA focuses on reasoning over the complex traffic scenarios in the wild, where 6 challenging tasks for traffic event reasoning are introduced based on the diverse real-world
traffic
videos.  

\textbf{Video QA Methods.}
Extensive studies have been conducted for video QA \cite{yu2017end,jang2017tgif,garcia2020knowledge,Jiang_2020_CVPR,Kim_2020_CVPR,lei-etal-2020-tvqa,tsai2019GSTEG,li2019beyond,Kim_2018_ECCV,Yu_2018_ECCV,sukhbaatar2015end,wang2018movie,fan2019heterogeneous,liang2018focal,tensoremnlp17,8654010}. 
Yu et al. \cite{yu2017end} employed LSTM to encode videos and QA pairs, and adopted an attention mechanism \cite{you2016image}. 
Jang et al. \cite{jang2017tgif} used LSTMs with a different attention scheme to capture spatio-temporal patterns in videos. 
Different from existing video QA methods, our Eclipse model investigates the direction of learning an effective glimpse policy for adaptive reasoning to achieve reliable reasoning with computation efficiency.

\textbf{Computation-Efficient Models.}
Recent works \cite{Bhardwaj_2019_CVPR,Mu_2019_CVPR,strubell2019energy,figurnov2017spatially, korbar2019scsampler,wu2019adaframe,schwartz2019green,NEURIPS2019_bd853b47, fan2020adaptive} have pointed out the need of improving the computation-efficiency when designing deep models, and 
different strategies, including filter pruning \cite{li2016pruning}, weight sparsification \cite{sun2016sparsifying}, vector quantization \cite{agustsson2017soft}, and dynamic routing \cite{wang2018skipnet}, etc., have been proposed. In this paper, we propose an efficient model, Eclipse, the first model that performs dynamic inference and adaptive computation adjustment for video QA, which leverages an effective glimpse policy with the guidance of text and visual context information for both glimpse frame selection and computation granularity determination.

\section{TrafficQA Dataset}\label{section:dataset}

Our dataset contains 62,535 QA pairs and 10,080 videos of traffic scenes. Below we first propose 6 challenging traffic-related reasoning tasks, and then introduce the QA collection process and the dataset statistics. 

{\bf Basic understanding.} This task evaluates the ability of the models in perceiving and understanding traffic scenarios at the basic level, which consists of multiple sub-tasks including feature-query (e.g., vehicle type, road situation, and environment description), event-query (e.g., accident existence, pedestrian action analysis, and events temporal relation), event classification (e.g., accident type), and counting (e.g., road-agent number). 

{\bf Event forecasting.} This task requires a model to infer future events based on observed videos, and the forecasting questions query about the outcome of the current situation. 

{\bf Reverse reasoning.} This task is to ask about the events that have happened before the start of a video segment.


{\bf Counterfactual inference.} This task queries the consequent outcomes of certain hypothesis (e.g., what if the blue sedan had not accelerated?). The hypothetical conditions do not occur in the video, so the model needs to reason about the imagined events under the designated condition.

{\bf Introspection.} This task is to test if models are able to provide preventive advice (e.g., what could the pedestrian have done to avoid the collision with the car?). 
The candidate answers list actions that could have been taken to avoid traffic accidents or congestion.

{\bf Attribution.} This task seeks the explanation about the causes of traffic events (e.g., what are the reasons of the rear-end crash?), so as to check if models are able to infer the underlying factors leading to the event. 

We define all the above reasoning tasks as multiple-choice questions without limiting the number of candidate answers for each question. The number of candidate answers varies from 2 to 12 for different questions. We then sample among the candidate answers to balance the dataset and limit the occurrence of the same correct answers within each task to minimize language biases. As summarized in Table \ref{table:dataset_comparison}, by introducing these challenging tasks, our dataset complements existing datasets, and facilitates the exploration of video QA in complex traffic scenarios.

\subsection{QA Collection}

{\bf Videos.} We collected videos by using a combination of online harvesting and offline capturing, to cover various real-world traffic scenarios.
As for online video collection, a variety of video sharing platforms, based in different countries, are used to increase the diversity, including but not limited to YouTube, LiveLeak, Twitter, and Bilibili. More than nine thousand videos were thus collected from these online sources.
As for offline video capturing, a set of videos were captured via handheld cameras by volunteers, while another set were fetched from car-mounted video recorders. These two sets of offline videos were then examined and trimmed into around one thousand video clips.

After combing the online videos and offline captured ones, we obtain a total of  videos containing diversities in various aspects, including: a) different weathers (sunny/rainy/windy/snowy); b) different time (daytime/night); c) diverse road situations (congested/sparse, urban/rural roads); d) various traffic events (accidents, vehicle turning, pedestrian behaviors, traffic lights, etc.); e) different video perspectives (surveillance camera perspective/car-mounted video perspective/hand-held camera perspective); and f) various clip lengths (from  to  seconds).


{\bf QA pairs.} As the first step, the 6 tasks were explained to annotators. To ensure they fully understand the design principle of the tasks, multiple example questions were prepared for them to identify which task the questions belong to. Afterwards, each annotator was presented with batches of video folders, where each folder contains 100 clips that were randomly selected from the full video set. Annotators were asked to create at least 3 questions of the reasoning tasks for each video.
We did not impose constraints on question formats to encourage annotators to keep their QA pairs diversified. Specifically, we encourage them to rephrase similar questions or candidate answers to push the models to learn underlying semantics of QA pairs rather than superficial language correlation. 
To ensure the quality of QA pairs, we cross-checked QA annotations on a weekly basis. 
In addition, we kept monitoring the distribution of different tasks in QA collection to maintain the balance and diversity of our dataset.

\subsection{Dataset Statistics}

\begin{figure}[h]
\begin{center}
    \includegraphics[width=1\linewidth]{stats_2.pdf}
\end{center}
\vspace{-0.4cm}
\caption{Statistics of TrafficQA. More in supplementary.
}
\label{fig:Dataset_stats}
\vspace{-0.2cm}
\end{figure}

In this part, we present the statistics of our TrafficQA dataset. Figure \ref{fig:Dataset_stats} (a) demonstrates the distribution of question length measured by number of words. The average length of questions is  words. Figure \ref{fig:Dataset_stats} (b) shows the various question types categorized by their beginning words, which implies the diversity of questions in our dataset.
Figure \ref{fig:Dataset_stats} (c) presents the split of questions in terms of reasoning tasks.  Our dataset covers a broad range of traffic-related reasoning tasks requiring various levels of spatio-temporal understanding and causal reasoning in videos. 
\section{Eclipse Network}



To deal with video reasoning, a common solution is to watch the full video and analyze the whole event information carefully. In this manner, generally, a fixed computation architecture \cite{lei2018tvqa,jang2017tgif,arad2018compositional,Le_2020_CVPR} can be applied over the whole video for tackling each question. However, using a fixed network architecture for handling the video QA task is often computation-heavy and energy-consuming \cite{strubell2019energy,schwartz2019green}, because the video sequence used for reasoning can be very long and contain plenty of frames for processing.




Recalling that, as humans,
to analyze events in a video, we may not be patient enough to scrutinize the frames in the whole video,
instead, we may adopt an adaptive information ``foraging'' strategy  \cite{fitzsimmons2014skim,duggan2011skim}.
Concretely, we may 
use a ``dynamic'' inference manner 
to skip forth and back over the sequence to progressively infer and select some useful frames based on the task. Moreover, for the picked frames, we may examine a few of them very carefully while glancing over others. 
Such a dynamic and adaptive perception habit \cite{fitzsimmons2014skim,duggan2011skim} frees us from watching the whole video thoroughly, and often enables fast yet still very accurate video reasoning at a very small frame usage.


Motivated by this, we aim to explore the direction of efficient and dynamic reasoning in complex traffic scenarios. Thus, we propose an \textbf{E}ffi\textbf{c}ient g\textbf{li}m\textbf{pse} (\textbf{Eclipse}) network for video QA, as illustrated in Figure \ref{fig:Eclipse}.
Instead of using a fixed computation architecture over each video and question, our network learns to dynamically skip to and select a useful video frame at each inference step. Moreover, our network adaptively decides the feature computation granularity (i.e., coarse or fine) of the selected frame. To perform such a process, at each inference step, our network takes advantage of the guidance information including the QA pair, the currently selected frame and the historical cues for dynamic reasoning.


Specifically, as shown in Figure \ref{fig:Eclipse},
in our network, to provide QA information for the Interaction Module, the QA Bank stores the representation of the QA pairs. 
At each inference step, to assist the dynamic reasoning of selecting the frame and the corresponding feature granularity, the Interaction Module leverages the QA information, the currently selected frame and the information from historically observed frames to derive an expressive representation, 
which then serves as the input of the dynamic reasoning process performed by the downstream modules, including the Prediction Module for outputting the reasoning result, and
Glimpse-Determination Module for dynamically determining which frame to be observed at next step. 
Besides, the Exit-Policy Module also uses this representation to adaptively decide whether we can exit the reasoning process at current inference step. 
Via such a dynamic and recurring reasoning process, our network can derive reliable answer predictions with notable computation efficiency w.r.t. both the frame usage and feature computation granularity. We elaborate the network modules in detail below.

\begin{figure}[tbp]
\begin{center}
    \includegraphics[width=1\linewidth]{Eclipse_1_v1_3.pdf}
\end{center}
\vspace{-0.5cm}
\setlength\abovecaptionskip{-2cm}   
\setlength\belowcaptionskip{-2cm}
\caption{Architecture of Eclipse for dynamic causal reasoning.}
\label{fig:Eclipse}
\vspace{-0.4cm}
\end{figure}

{\bf QA Bank.} To provide the QA information as the guidance for dynamic reasoning, our QA Bank encodes the representation of the question via a bi-directional LSTM. 
By concatenating hidden states of the BiLSTM, the question representation can be denoted as , where  is the number of words in the question,  denotes the dimension of LSTM hidden state. Similarly, we encode all the candidate answers as
,
where ,
 is the number of candidate answers, 
and  is the number of words in the answer .


{\bf Interaction Module.} To assist the subsequent dynamic reasoning with rich guidance information, we design an Interaction Module to fuse different kinds of available inputs. This module, consisting of a context-query sub-module and an interaction LSTM, recurrently interacts with a small number of frames selected from the full video sequence containing  frames. As shown in Figure \ref{fig:Eclipse}, at each inference step , this module fuses the QA information ( and ),
the currently selected frame (), and the historical cues ()
to produce an expressive representation () that can be used for dynamic reasoning. 

More formally, at the  inference step, to first fuse the textual QA information ( and ) with the currently selected visual frame feature (),
we use a context-query sub-module to perform such a fusion process. We implement this sub-module by following the context-matching module of \cite{lei2018tvqa}, which can effectively fuse the visual feature sequence with the textual sequence to produce a combined representation. Since the original method in \cite{lei2018tvqa} takes a sequence of visual features as an input, and we have only the selected frame feature  here, we treat  as a visual feature sequence containing a single element. Hence the fusion process in this sub-module can be formulated as:

where  represents element-wise product.  and  are obtained by computing the similarity between the visual frame feature () and the textual features ( and ). More details of this process are referred to \cite{lei2018tvqa} and also our supplementary.
For simplicity, we use  to represent the concatenated , as shown in Figure \ref{fig:Eclipse}. The 
output , that incorporates the information of currently selected frame and the QA embedding, can be fed into the following interaction LSTM for more information interaction, as introduced below.

Besides fusing the QA information and the currently selected frame (), to guide the dynamic reasoning at current inference step, we also incorporate the historical cues from 
past inference steps. Thus we design an interaction LSTM that takes the  as the new input to interact with the historical cues  as:

where  are parameters of LSTM. The generated hidden state  encodes rich information of all available inputs and historical cues, and thus it can serve as an expressive representation to be fed into the downstream modules for dynamic reasoning as follows.

{\bf Prediction Module.} This module is used to generate the reasoning result (i.e. the probability distribution over candidate answers) at the current inference step . This module computes the reasoning result as: , where  represents the probability scores for all candidate answers.  can be implemented with one fully-connected (FC) layer followed by a  classifier.


{\bf Glimpse-Determination Module.} At each inference step , conditioned on , this module performs dynamic reasoning by making two decisions simultaneously. The first decision is to select which frame to be observed at next step, and the second is to decide whether to compute fine-grained features or coarse features for this selected frame. 
Corresponding to these two decisions, we design the following two branches within this module.

The skip-policy branch selects the frame that we need to skip to at next inference step via the following process: , where the output  indicates the decision of the next frame location. Note that our network can skip forth and back over the entire video sequence, which is conceptually similar to the human reasoning process where we need to not only jump forward to find future informative frames but also go back to examine past information. 


Besides determining next frame, this module also has a granularity-policy branch that adaptively decides the feature computation granularity for the next selected frame, formulated as:
. The output , 
denotes the decision of feature granularity. In our implementation, we provide two kinds of feature granularity, namely, coarse features computed by a lightweight CNN; fine-grained features computed by a more representative yet computation-heavier CNN, to be chosen from. In the Glimpse-Determination module, both  and  are implemented with a FC layer. 




{\bf Exit-Policy Module.} To estimate when we can exit the reasoning process to achieve adaptive inference, we design the Exit-Policy Module. At each inference step , this module decides if we can exit the reasoning process at current step based on the guidance information () as: , where the output  denotes
the confidence score of terminating the reasoning at current step. By training the exit-policy, 
our network can achieve adaptive inference, such that only a small and flexible number of frames are selected and computed on a per-video basis to derive reliable reasoning result.

{\bf Optimization.} To optimize the above modules in our network, we introduce several loss functions. Specifically, at each inference step , a cross-entropy loss  is used to train the classifier of the Prediction Module:

where  is the ground-truth one-hot label vector for the candidate answers and  is the predicted score for the  answer.
As for Glimpse-Determination Module, to push the skip-policy branch to select a useful frame at each inference step, a simple yet effective loss is used:

where  is the margin between the predicted probability of the correct answer (indexed by ) and the largest probability of other candidate answers. We can simply infer that a larger  indicates a more confident and accurate reasoning. Therefore, we use  to encourage the margin to keep growing over the inference steps, which indicates that at each step, we aim to select a useful frame to benefit our dynamic reasoning, considering that the confidence of our network for the correct answer increases when seeing the selected frames.


Meanwhile, to further save computation cost and prevent the granularity-policy branch from constantly using the computation-heavy fine features, we penalize the feature granularity policy
when the fine feature is computed at each inference step  as follows:

where  represents that the policy chooses to extract computation-heavy fine features for next step, while  means it switches to extract computation-cheap coarse features for next step.
To optimize our whole Glimpse-Determination Module, we incorporate the above two loss functions,    and , into a combined loss:


Last but not least, we need to train the Exit-Policy to make a reliable decision if we can exit the reasoning process at current inference step. However, there are no ground-truth labels providing feedback on when our network can exit reasoning.
Therefore, we leverage  
to generate dynamic labels to train Exit-Policy. Recalling that  is the probability margin between the correct answer and the largest one of other candidate answers at each step, and  is optimized to keep increasing 
under the constraint of Eqn. \ref{eq.increment_loss}. 
Therefore, the gap between  at different inference steps can be used to estimate the information gain of seeing more frames in our dynamic reasoning process. Given the pre-defined largest reasoning step , the gap between  (at the current step) and  (of the final step) can estimate the value of remaining information gain by continuing reasoning till the end. Thus the gap can be used to determine whether our network can stop inference at  step. When  is very close to , this means the information gain by observing more frames is small, and thus we can exit reasoning in advance to reduce computation without incurring decrease in prediction accuracy.


In particular, at each step , if , which means  is close to , i.e., the estimated remaining information gain is small enough, we set the label, , as 1, indicating our model can exit reasoning at the  inference step. Otherwise, the label is set to 0, representing we need to seek more frames. Here  controls how close  should be to  when the network exits inference. Conditioned on the estimated labels, , training Exit-Policy can be seen as a binary classification problem. Thus we train this module by minimizing a binary cross-entropy loss:

By combining Eqns (\ref{eq.cross-entropy loss}), (\ref{eq.glimpse_loss}),
and (\ref{eq.bce_loss}), the total loss function for each step  can be formulated as:

where  is the weight of the combined loss function for optimizing the Glimpse-Determination Module.
In our experiments, we compute the sum:  from all inference steps as the final optimization objective. 

Note that Eqn. (\ref{eq.final_loss}) cannot be optimized directly with gradient descent, since the involved decisions of selecting frames and determining feature granularity in our dynamic reasoning are discrete, and sampling from discrete distribution makes the network non-differentiable. To address this issue, we introduce an effective joint Gumbel-Softmax operation.

{\bf Joint Gumbel Softmax.} The original Gumbel-Softmax Sampling \cite{jang2016categorical} is an effective way to transform the original non-differentiable sample from a discrete distribution, to a differentiable decision from a corresponding Gumbel-Softmax distribution. In our task, to sample from the aforementioned two discrete distributions (namely, selecting frames and determining granularity) simultaneously, we here design an effective joint Gumbel-Softmax operation. 

In particular, in the Glimpse-Determination Module, at each step , we first derive the logits  by feeding the hidden state  into a fully-connected layer. 
Then we use  to obtain a categorical distribution  from :
.
With the Gumbel-Max trick \cite{jang2016categorical}, the discrete sample from the categorical distribution  can be defined as follows:

where  denotes the Gumbel noise, and  is the i.i.d. samples drawn from . We can further relax the non-differentiable operation  with  to facilitate gradient-based optimization:

where  is the temperature parameter, which controls the smoothness of the sampling mechanism. When , the sampling approximates the  operation in Eqn. \ref{eq.argmax}. The output  incorporates the output of two decisions: the first dimension of  denotes the decision of selecting the frame (i.e, the output  of the skip-policy branch) and the second dimension of  denotes the decision of the feature granularity of the selected frame (i.e, the output  of the granularity-policy branch).

By using the outputs ( and ) of the joint Gumbel-Softmax operation, our network manages to dynamically select the frame for next inference step and specify the feature granularity for the selected frame at each step. 
Therefore, by introducing the joint Gumbel-Softmax, our network can learn the two discrete policy decisions jointly in a fully differentiable way.

{\bf Training and testing.} During training, we optimize our network within a fixed number of steps, which means the exit-policy is trained together with other modules but the exit decisions 
are not used. However, at the testing phase, if the exit-policy decides to stop reasoning at the  inference step, we exit the model and use the current prediction result, , as the final reasoning result. In such a manner, our model achieves dynamic causal reasoning.

\section{Experiments} \label{sec:exp}
Given that the number of candidate answers for each question is not fixed in our dataset, we evaluate the performance of our network using binary and multi-choice setups. In binary case (denoted as \textbf{Setting-1/2}), the input to the model is a question with an answer, and the model needs to predict the correctness of this answer. In multi-choice setup (denoted as \textbf{Setting-1/4}), models are expected to select the correct answer from 4 candidate answers (i.e, 3 of them are incorrect). These two experiment setups can be treated as binary and four-class classification problems.

{\bf Implementation Details.} 
We compute features from the penultimate layer of a pretrained ResNet-101 model \cite{he2016deep} as the fine-grained frame feature,
and a pretrained MobileNetv2 \cite{sandler2018mobilenetv2} is used as the lightweight CNN to extract coarse features. In the QA Bank, we use Glove \cite{Pennington14glove:global} to embed QA text, and then use a BiLSTM with 150-Dimension hidden states to encode the textual sequence.  As for the Interaction LSTM, the dimension of hidden states is 300.
We implement the framework using Pytorch and adopts Adam \cite{kingma2014adam} with a learning rate of 3e-4 and a weight-decay of 1e-5. The  in the Exit-Policy is set to 0.1 and  is set to 0.01 in the loss function. We follow \cite{jang2016categorical} and set the initial temperature  to 5, and gradually anneal it with an exponential decay factor of -0.045 in every epoch. According to evaluation statistics, our network shows a very fast \textbf{inference speed} of  per testing video on a Nvidia RTX 2080Ti GPU.

We compare our network with the following baselines.
{\bf Text-only models.} These models only relying on text information without visual input, are relatively weak baselines used to assess language biases in our TrafficQA. {\bf Q-type (random)} randomly selects an answer from the answer space. {\bf QE-LSTM} uses Glove \cite{Pennington14glove:global} to embed the input question and then encode it with LSTM \cite{hochreiter1997long}. The final LSTM hidden state is passed to a MLP for predicting the correct answer. Different from {\bf QE-LSTM} using questions only, {\bf QA-LSTM} uses LSTM to encode both question embedding and answer embedding, and the final hidden states are used for predicting the answer.

{\bf Text+video models.} We evaluate the following models that require both video and text inputs. 
{\bf VIS+LSTM} \cite{ren2015exploring} uses LSTM to encode image representation and textual features. Since the original method takes a single image as input, we adapt this method by averaging features of all sampled frames in a video as the visual input. 
{\bf Avgpooling} uses each frame with the encoded QA features to compute a prediction for each frame.
We then perform mean pooling over all the frame predictions to obtain the final result. 
{\bf CNN+LSTM} uses two LSTMs to encode both the video sequence and the QA text respectively. The two final hidden states are concatenated to predict the correct answer.
{\bf I3D+LSTM} uses I3D network \cite{carreira2017quo} to extract video motion features, and then fuse with QA textual features encoded by LSTM to compute the model prediction.
{\bf TVQA} \cite{lei2018tvqa} is a multi-stream network to fuse input features from different modalities to answer the question. 
{\bf HCRN} \cite{Le_2020_CVPR} adopts a Hierarchical Conditional Relation Networks to model sophisticated structure for reasoning over videos.
{\bf BERT-VQA} \cite{Yang_2020_WACV} uses BERT \cite{devlin2018bert} 
to encode the visual and language information jointly to predict the answer. 
{\bf }




\subsection{Results and Analysis}

\begin{figure*}[t]
\setlength\abovecaptionskip{-5cm}
\setlength\belowcaptionskip{-5cm}
\begin{center}
    \includegraphics[width=1\linewidth]{stats_1_compressed.pdf}
\end{center}
\vspace{-0.3cm}
    \caption{A qualitative example. The numbers above the selected frames show the order of the sequence selected by our network. It shows that our model selects informative frames dynamically and allocates large computation budget using fine features to most relevant frames for causal reasoning. More examples in supplementary.}
\label{fig:policy_example}
\vspace{-0.3cm}
\end{figure*}

\begin{table}[t]
\vspace{-0.2cm}
\caption{Results on TrafficQA dataset. 
}
\setlength\abovecaptionskip{-2cm}
\setlength\belowcaptionskip{-2cm}
\setlength{\tabcolsep}{10pt}
\scriptsize
\begin{center}
\begin{tabular}{l|ccc}
\hline
Models & Setting-1/4 & Setting-1/2 & GFLOPs  \\\hline
Q-type (random)& 25.00 & 50.00 & - \\ 
QE-LSTM & 25.21 & 50.45 & - \\ 
QA-LSTM & 26.65 & 51.02 & -  \\ \hline
Avgpooling & 30.45& 57.50 & 252.69 \\ 
CNN+LSTM & 30.78 & 57.64 & 252.95  \\ 
I3D+LSTM & 33.21 & 54.67 & 108.72  \\
VIS+LSTM \cite{ren2015exploring} & 29.91 & 54.25 & 252.80\\
BERT-VQA \cite{Yang_2020_WACV} & 33.68 & 63.50 & 266.77 \\ 
TVQA \cite{lei2018tvqa} & 35.16 & 63.15 & 252.11  \\ 
HCRN \cite{Le_2020_CVPR} & 36.49 & 63.79 & 2051.04 \\ 
\textbf{Eclipse}  & \textbf{37.05} & \textbf{64.77} & \textbf{28.14} \\ \hline
\textit{Human}    &  95.43    & 96.78 & -\\ \hline
\end{tabular}
\end{center}
\label{table:baseline_comparison}
\vspace{-0.5cm}
\end{table}

The results in Table \ref{table:baseline_comparison}
demonstrate the minimal language biases in TrafficQA, as the text-only baselines perform almost the same as the random choice. In contrast, the models using video input achieve obviously higher accuracy than text-only baselines. This demonstrates that to solve the reasoning tasks in our dataset, the model needs to associate visual content with linguistic cues to infer correct answers. For a fair computation-efficiency comparison, we pick models requiring video input to compute GFLOPs per video, as the visual feature extraction consumes much computation budget, and the metric of GFLOPs is independent of hardware configurations. The results show our Eclipse achieves state\text{-}of\text{-}the\text{-}art reasoning accuracy 
with significantly improved computation efficiency. This verifies that compared to conventional video QA  methods, through dynamic causal reasoning, our model effectively exploits the spatio-temporal and logical structure of video events to infer correct answers with much smaller frame usage and efficient feature computation.
In addition, three volunteers who did not see the videos and questions before, were invited to pick correct answers, and we use the average prediction accuracy as \textit{Human} performance. The discrepancy between neural networks and the human performance demonstrates the challenging nature of our dataset and the necessity of further research in video reasoning area.


\begin{table}[tbp]
\caption{Results of removing granularity-policy in Eclipse.
}
\vspace{-0.2cm}
 \setlength{\tabcolsep}{0.5pt}
\scriptsize
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
Models & Coarse Features Only & Fine Features Only & ~Eclipse~(dynamic granularity) \\\hline
Accuracy & 34.35 & 37.16 & 37.05 \\ \hline
GFLOPs & 10.61 & 133.75 & 28.14 \\ \hline
\end{tabular}
\end{center}
\label{table:granularity-policy}
\vspace{-0.7cm}
\end{table}

\begin{table}[t]
\caption{Results of removing skip-policy in Eclipse. 
Uniform- means uniformly sampling  frames from the video for reasoning. 
}
\vspace{-0.1cm}
\setlength{\tabcolsep}{2pt}
\scriptsize
\begin{center}
\setlength\abovecaptionskip{-2cm}   
\setlength\belowcaptionskip{-2cm} 
\begin{tabular}{c|ccc|c}
\hline
Models & ~~Uniform-10~~ &~~ Uniform-20~~ & ~~Uniform-40~~ & ~Eclipse~(skip-policy) \\\hline
Accuracy & 34.16 & 35.49 & 36.48 & 37.05 \\ \hline
GFLOPs & 32.17 & 51.19 & 68.41 & 28.14 \\ \hline
\end{tabular}
\end{center}
\label{table:skipping-policy}
\vspace{-0.7cm}
\end{table}

\begin{table}[tbp]
\caption{Results of removing exit-policy in Eclipse. 
Final-Step Inference refers to that we remove the exit-policy and infer until the final step. 
}
\vspace{-0.1cm}
\setlength{\tabcolsep}{10pt}
\scriptsize
\begin{center}
\begin{tabular}{c|c|c}
\hline
Models & Final-Step Inference & ~Eclipse~(exit-policy)\\\hline
Accuracy & 37.11  & 37.05 \\ \hline
GFLOPs & 36.92 & 28.14 \\ \hline
\end{tabular}
\end{center}
\label{table:exit-policy}
\vspace{-0.9cm}
\end{table}


{\bf Ablation Study.} As shown in Table \ref{table:granularity-policy}, compared with the model variant of using fine features only, by adopting the granularity-policy to adaptively decide feature granularity for the selected frame at each step, our network achieves nearly the same accuracy yet using much lower computation cost. 
Our network also achieves obviously higher reasoning accuracy than the method of using coarse features only. These results show the effectiveness of our granularity-policy by choosing fine features for most useful frames and coarse features for less important frames. 
Furthermore, we remove the skip-policy and simply uniformly sample frames at each step. 
As shown in Table \ref{table:skipping-policy}, our Eclipse performs the best yet at small computation cost. 
This shows that our skip-policy effectively reduces the computation cost by selecting useful frames for dynamic reasoning.
Moreover, we present the frame location distributions for the first three inference steps of our network in Figure \ref{fig:Freq}. 
As shown, our network selects frames dynamically for different videos as we expect. 
We also investigate the exit-policy module by comparing it with the method of reasoning until the final inference step. The result in Table \ref{table:exit-policy} shows that our model achieves best accuracy-to-computation ratio via adaptive inference. In Figure \ref{fig:policy_example}, we further present a qualitative example from our dataset to show how Eclipse performs dynamic and efficient reasoning. 


\begin{figure}[htbp]
\vspace{-0.3cm}
\begin{center}
   \includegraphics[width=0.8\linewidth]{freq_v2.pdf}
\end{center}
\vspace{-0.4cm}
   \caption{Distributions of frame location (Left to right:step1, 2, 3).}
\label{fig:Freq}
\vspace{-0.4cm}
\end{figure}



\section{Conclusion}
We create a new video QA dataset, TrafficQA, focusing on video reasoning over traffic events. In our dataset, we introduce 6 reasoning tasks requiring various levels of causal reasoning. Besides, we propose the Eclipse network for video QA. By learning the dynamic glimpse policy and adaptive exit policy, our network achieves superior performance with significant computation efficiency.

{\bf Acknowledgement.} We would like to thank Yutian Lin, Renhang Liu, Yingjie Qiao, Xun Long Ng, Tran Nguyen Bao Long, Koh Kai Ting and Christabel Dorothy for their help in dataset collection and running baseline models. This work is supported by SUTD Projects PIE-SGP-Al2020-02 and SRG-ISTD-2020-153.

{\small
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{agustsson2017soft}
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu
  Timofte, Luca Benini, and Luc~V Gool.
\newblock Soft-to-hard vector quantization for end-to-end learning compressible
  representations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1141--1151, 2017.

\bibitem{10.3115/1073012.1073017}
Michele Banko and Eric Brill.
\newblock Scaling to very very large corpora for natural language
  disambiguation.
\newblock In {\em Proceedings of the 39th Annual Meeting on Association for
  Computational Linguistics}, ACL '01, page 26–33, USA, 2001. Association for
  Computational Linguistics.

\bibitem{Bhardwaj_2019_CVPR}
Shweta Bhardwaj, Mukundhan Srinivasan, and Mitesh~M. Khapra.
\newblock Efficient video classification using fewer frames.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2019.

\bibitem{carreira2017quo}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In {\em proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6299--6308, 2017.

\bibitem{chandra2020forecasting}
Rohan Chandra, Tianrui Guan, Srujan Panuganti, Trisha Mittal, Uttaran
  Bhattacharya, Aniket Bera, and Dinesh Manocha.
\newblock Forecasting trajectory and behavior of road-agents using spectral
  clustering in graph-lstms.
\newblock {\em IEEE Robotics and Automation Letters}, 5(3):4882--4890, 2020.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{duggan2011skim}
Geoffrey~B Duggan and Stephen~J Payne.
\newblock Skim reading by satisficing: evidence from eye tracking.
\newblock In {\em Proceedings of the SIGCHI Conference on Human Factors in
  Computing Systems}, pages 1141--1150, 2011.

\bibitem{fan2019heterogeneous}
C.~Fan et al.
\newblock Heterogeneous memory enhanced multimodal attention model for video
  question answering.
\newblock In {\em CVPR}, 2019.

\bibitem{jang2016categorical}
J.~Eric et al.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock {\em arXiv}, 2016.

\bibitem{Le_2020_CVPR}
Le.~Minh et al.
\newblock Hierarchical conditional relation networks for video question
  answering.
\newblock In {\em CVPR}, 2020.

\bibitem{li2019beyond}
X.~Li et al.
\newblock Beyond rnns: Positional self-attention with co-attention for video
  question answering.
\newblock In {\em AAAI}, 2019.

\bibitem{jang2017tgif}
Y.~Jang et al.
\newblock Tgif-qa: Toward spatio-temporal reasoning in visual question
  answering.
\newblock In {\em CVPR}, 2017.

\bibitem{zadeh2019social}
Z.~Amir et al.
\newblock Social-iq: A question answering benchmark for artificial social
  intelligence.
\newblock In {\em CVPR}, 2019.

\bibitem{fan2020adaptive}
Zhipeng Fan and YW Jun~Liu.
\newblock Adaptive computationally efficient network for monocular 3d hand pose
  estimation.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, 2020.

\bibitem{figurnov2017spatially}
Michael Figurnov, Maxwell~D Collins, Yukun Zhu, Li Zhang, Jonathan Huang,
  Dmitry Vetrov, and Ruslan Salakhutdinov.
\newblock Spatially adaptive computation time for residual networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1039--1048, 2017.

\bibitem{fitzsimmons2014skim}
Gemma Fitzsimmons, Mark~J Weal, and Denis Drieghe.
\newblock Skim reading: an adaptive strategy for reading on the web.
\newblock In {\em Proceedings of the ACM Conference on Web Science}, pages
  211--219, 2014.

\bibitem{garcia2020knowledge}
Noa Garcia and Yuta Nakashima.
\newblock Knowledge-based video question answering with unsupervised scene
  descriptions.
\newblock {\em arXiv preprint arXiv:2007.08751}, 2020.

\bibitem{garcia2020knowit}
Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima.
\newblock Knowit vqa: Answering knowledge-based questions about videos.
\newblock In {\em Proceedings of the Thirty-Fourth AAAI Conference on
  Artificial Intelligence}, 2020.

\bibitem{35179}
Alon Halevy, Peter Norvig, and Fernando Pereira.
\newblock The unreasonable effectiveness of data.
\newblock {\em IEEE Intelligent Systems}, 24:8--12, 2009.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{arad2018compositional}
Drew~Arad Hudson and Christopher~D. Manning.
\newblock Compositional attention networks for machine reasoning.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Jiang_2020_CVPR}
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei
  Chen.
\newblock In defense of grid features for visual question answering.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2020.

\bibitem{Kim_2020_CVPR}
Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, and Chang~D. Yoo.
\newblock Modality shifting attention network for multi-modal video question
  answering.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2020.

\bibitem{Kim_2018_ECCV}
Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and Byoung-Tak Zhang.
\newblock Multimodal dual attention memory for video story question answering.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem{ijcai2017-280}
Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang.
\newblock Deepstory: Video story qa by deep embedded memory networks.
\newblock In {\em Proceedings of the Twenty-Sixth International Joint
  Conference on Artificial Intelligence, {IJCAI-17}}, pages 2016--2022, 2017.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{korbar2019scsampler}
Bruno Korbar, Du Tran, and Lorenzo Torresani.
\newblock Scsampler: Sampling salient clips from video for efficient action
  recognition.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 6232--6242, 2019.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lei2018tvqa}
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara~L Berg.
\newblock Tvqa: Localized, compositional video question answering.
\newblock {\em arXiv preprint arXiv:1809.01696}, 2018.

\bibitem{lei-etal-2020-tvqa}
Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal.
\newblock {TVQA}+: Spatio-temporal grounding for video question answering.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 8211--8225, Online, July 2020. Association
  for Computational Linguistics.

\bibitem{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{liang2018focal}
Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, and Alexander~G Hauptmann.
\newblock Focal visual-text attention for visual question answering.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6135--6143, 2018.

\bibitem{lou2019veri}
Yihang Lou, Yan Bai, Jun Liu, Shiqi Wang, and Lingyu Duan.
\newblock Veri-wild: A large dataset and a new method for vehicle
  re-identification in the wild.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3235--3243, 2019.

\bibitem{maharaj2017dataset}
Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher
  Pal.
\newblock A dataset and exploration of models for understanding video data
  through fill-in-the-blank question-answering.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6884--6893, 2017.

\bibitem{Mu_2019_CVPR}
Guodong Mu, Di Huang, Guosheng Hu, Jia Sun, and Yunhong Wang.
\newblock Led3d: A lightweight and efficient deep approach to recognizing
  low-quality 3d faces.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2019.

\bibitem{mun2017marioQA}
Jonghwan Mun, Paul~Hongsuck Seo, Ilchae Jung, and Bohyung Han.
\newblock Marioqa: Answering questions by watching gameplay videos.
\newblock In {\em ICCV}, 2017.

\bibitem{Pennington14glove:global}
Jeffrey Pennington, Richard Socher, and Christopher~D. Manning.
\newblock Glove: Global vectors for word representation.
\newblock In {\em In EMNLP}, 2014.

\bibitem{ren2015exploring}
Mengye Ren, Ryan Kiros, and Richard Zemel.
\newblock Exploring models and data for image question answering.
\newblock In {\em Advances in neural information processing systems}, pages
  2953--2961, 2015.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  91--99, 2015.

\bibitem{10.1007/s11263-016-0987-1}
Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal,
  Hugo Larochelle, Aaron Courville, and Bernt Schiele.
\newblock Movie description.
\newblock {\em Int. J. Comput. Vision}, 123(1):94–120, May 2017.

\bibitem{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem{schwartz2019green}
Roy Schwartz, Jesse Dodge, Noah~A Smith, and Oren Etzioni.
\newblock Green ai.
\newblock {\em arXiv preprint arXiv:1907.10597}, 2019.

\bibitem{emrvqasongMM18}
Xiaomeng Song, Yucheng Shi, Xin Chen, and Yahong Han.
\newblock Explore multi-step reasoning in video question answering.
\newblock In {\em Proceedings of the ACM International Conference on Multimedia
  (ACM MM)}, 2018.

\bibitem{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock {\em arXiv preprint arXiv:1906.02243}, 2019.

\bibitem{sukhbaatar2015end}
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et~al.
\newblock End-to-end memory networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2440--2448, 2015.

\bibitem{sun2016sparsifying}
Yi Sun, Xiaogang Wang, and Xiaoou Tang.
\newblock Sparsifying neural network connections for face recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4856--4864, 2016.

\bibitem{tapaswi2016movieqa}
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel
  Urtasun, and Sanja Fidler.
\newblock Movieqa: Understanding stories in movies through question-answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4631--4640, 2016.

\bibitem{tsai2019GSTEG}
Yao-Hung~Hubert Tsai, Santosh Divvala, Louis-Philippe Morency, Ruslan
  Salakhutdinov, and Ali Farhadi.
\newblock Video relationship reasoning using gated spatio-temporal energy
  graph.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{wang2018movie}
Bo Wang, Youjiang Xu, Yahong Han, and Richang Hong.
\newblock Movie question answering: Remembering the textual cues for layered
  visual contents.
\newblock {\em arXiv preprint arXiv:1804.09412}, 2018.

\bibitem{wang2018skipnet}
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph~E Gonzalez.
\newblock Skipnet: Learning dynamic routing in convolutional networks.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 409--424, 2018.

\bibitem{NEURIPS2019_bd853b47}
Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry~S Davis.
\newblock Liteeval: A coarse-to-fine framework for resource efficient video
  recognition.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem{wu2019adaframe}
Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and Larry~S Davis.
\newblock Adaframe: Adaptive frame selection for fast video recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1278--1287, 2019.

\bibitem{xu2017video}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In {\em ACM Multimedia}, 2017.

\bibitem{Yang_2020_WACV}
Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo
  Takemura.
\newblock Bert representations for video question answering.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision (WACV)}, March 2020.

\bibitem{yi2019clevrer}
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba,
  and Joshua~B Tenenbaum.
\newblock Clevrer: Collision events for video representation and reasoning.
\newblock {\em arXiv preprint arXiv:1910.01442}, 2019.

\bibitem{you2016image}
Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo.
\newblock Image captioning with semantic attention.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4651--4659, 2016.

\bibitem{you2020traffic}
Tackgeun You and Bohyung Han.
\newblock Traffic accident benchmark for causality recognition.
\newblock In {\em Proceedings of the European Conference on Computer Vision},
  2020.

\bibitem{Yu_2018_ECCV}
Youngjae Yu, Jongseok Kim, and Gunhee Kim.
\newblock A joint sequence fusion model for video question answering and
  retrieval.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem{yu2017end}
Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim.
\newblock End-to-end concept word detection for video captioning, retrieval,
  and question answering.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3165--3173, 2017.

\bibitem{yu2019activityqa}
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng
  Tao.
\newblock Activitynet-qa: A dataset for understanding complex web videos via
  question answering.
\newblock In {\em AAAI}, pages 9127--9134, 2019.

\bibitem{tensoremnlp17}
Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe
  Morency.
\newblock Tensor fusion network for multimodal sentiment analysis.
\newblock In {\em Empirical Methods in Natural Language Processing, EMNLP},
  2017.

\bibitem{zeng2016leveraging}
Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan~Carlos
  Niebles, and Min Sun.
\newblock Leveraging video descriptions to learn video question answering,
  2016.

\bibitem{5959985}
J. {Zhang}, F. {Wang}, K. {Wang}, W. {Lin}, X. {Xu}, and C. {Chen}.
\newblock Data-driven intelligent transportation systems: A survey.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems},
  12(4):1624--1639, 2011.

\bibitem{8654010}
Z. {Zhao}, Z. {Zhang}, X. {Jiang}, and D. {Cai}.
\newblock Multi-turn video question answering via hierarchical attention
  context reinforced networks.
\newblock {\em IEEE Transactions on Image Processing}, 28(8):3860--3872, 2019.

\bibitem{zhu2017uncovering}
Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexander~G Hauptmann.
\newblock Uncovering the temporal context for video question answering.
\newblock {\em International Journal of Computer Vision}, 124(3):409--421,
  2017.

\end{thebibliography}
 \bibliographystyle{ieee_fullname}
}

\end{document}
