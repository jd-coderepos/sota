\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{color}
\usepackage{times}
\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,urlcolor={gray},citecolor={gray}, bookmarks=false]{hyperref}
\usepackage{url}
\usepackage{color,xcolor}
\usepackage{epsfig}
\usepackage{graphicx}

\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{float,wrapfig}
\usepackage{hhline}
\usepackage{multirow}

\usepackage{bm}
\usepackage{nicefrac}
\usepackage{microtype}

\usepackage{changepage}
\usepackage{extramarks}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage{soul}
\usepackage{xspace}


\usepackage{algorithm, algorithmic}
\usepackage{enumerate}

 
\newcommand{\changeurlcolor}[1]{\hypersetup{urlcolor=#1}}

\definecolor{Gray}{gray}{0.5}
\definecolor{nicergreen}{rgb}{0.13, 0.54, 0.13}
\definecolor{nicered}{rgb}{0.83, 0.16, 0.16}
\definecolor{Highlight}{HTML}{39b54a}  \newcommand\showdiff[1]{\textbf{\textcolor{nicergreen}{#1}}}
\newcommand\showdiffn[1]{\textcolor{black}{#1}}
\newcommand{\cgap}[2]{
\fontsize{6pt}{1em}\selectfont{({#2})}
}
\newcommand{\cgaphl}[2]{
\fontsize{6pt}{1em}\selectfont{\textcolor{nicergreen}{(\textbf{#2})}}
}

\newcommand{\header}[1]{\vspace{0.05in}\noindent\textbf{#1}}
\newcommand{\textred}[1]{\textcolor{red}{#1}}
\newcommand{\citep}[1]{\cite{#1}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Contrastive Multiview Coding}

\author{Yonglong Tian\\
MIT CSAIL\\
{\tt\small yonglong@mit.edu}
\and
Dilip Krishnan\\
Google Research\\
{\tt\small dilipkay@google.com}
\and
Phillip Isola\\
MIT CSAIL\\
{\tt\small phillipi@mit.edu}
}

\maketitle

\begin{abstract}

\vspace{-1pt}

Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ``dog" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: \small{\url{http://github.com/HobbitLong/CMC/}}.


\end{abstract} 
\vspace{-4mm}
\section{Introduction}

A foundational idea in coding theory is to learn compressed representations that nonetheless can be used to reconstruct the raw data. This idea shows up in contemporary representation learning in the form of autoencoders~\cite{salakhutdinov2009deep} and generative models~\cite{kingma2013auto,goodfellow2014generative}, which try to represent a data point or distribution as losslessly as possible. Yet lossless representation might not be what we really want, and indeed it is trivial to achieve -- the raw data itself is a lossless representation. What we might instead prefer is to keep the ``good" information (signal) and throw away the rest (noise). How can we identify what information is signal and what is noise?

To an autoencoder, or a max likelihood generative model, a bit is a bit. No one bit is better than any other. Our conjecture in this paper is that some bits \emph{are} in fact better than others. Some bits code important properties like semantics, physics, and geometry, while others code attributes that we might consider less important, like incidental lighting conditions or thermal noise in a camera's sensor.

We revisit the classic hypothesis that the good bits are the ones that are shared between multiple \emph{views} of the world, for example between multiple sensory modalities like vision, sound, and touch~\cite{smith2005development}. Under this perspective ``presence of dog" is good information, since dogs can be seen, heard, and felt, but ``camera pose" is bad information, since a camera's pose has little or no effect on the acoustic and tactile properties of the imaged scene. This hypothesis corresponds to the inductive bias that the way you view a scene should not affect its semantics. There is significant evidence in the cognitive science and neuroscience literature that such view-invariant representations are encoded by the brain (e.g., \cite{smith2005development,den2012prediction,hohwy2013predictive}). In this paper, we specifically study the setting where the different views are different image channels, such as luminance, chrominance, depth, and optical flow. The fundamental supervisory signal we exploit is the \emph{co-occurrence}, in natural data, of multiple views of the same scene. For example, we consider an image in Lab color space to be a paired example of the co-occurrence of two views of the scene, the  view and the  view: .

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/teaser_eccv2020.pdf}
\caption{\small{Given a set of sensory views, a deep representation is learnt by bringing views of the \emph{same} scene together in embedding space, while pushing views of \emph{different} scenes apart. Here we show and example of a 4-view dataset (NYU RGBD~\cite{Silberman:ECCV12}) and its learned representation. The encodings for each view may be concatenated to form the full representation of a scene.}}
\label{fig:method_fig}
\end{figure}
 
Our goal is therefore to learn representations that capture information shared between multiple sensory channels but that are otherwise compact (i.e. discard channel-specific nuisance factors). To do so, we employ contrastive learning, where we learn a feature embedding such that views of the same scene map to nearby points (measured with Euclidean distance in representation space) while views of different scenes map to far apart points. In particular, we adapt the recently proposed method of Contrastive Predictive Coding (CPC)~\cite{oord2018representation}, except we simplify it -- removing the recurrent network -- and generalize it -- showing how to apply it to arbitrary collections of image channels, rather than just to temporal or spatial predictions. In reference to CPC, we term our method \emph{Contrastive Multiview Coding} (CMC), although we note that our formulation is arguably equally related to Instance Discrimination \citep{wu2018unsupervised}. 
The contrastive objective in our formulation, as in CPC and Instance Discrimination, can be understood as attempting to maximize the mutual information between the representations of multiple views of the data.

We intentionally leave ``good bits" only loosely defined and treat its definition as an empirical question. Ultimately, the proof is in the pudding: we consider a representation to be good if it makes subsequent problem solving easy, on tasks of human interest. For example, a useful representation of images might be a feature space in which it is easy to learn to recognize objects. We therefore evaluate our method by testing if the learned representations transfer well to standard semantic recognition tasks. On several benchmark tasks, our method achieves results competitive with the state of the art, compared to other methods for self-supervised representation learning. We additionally find that the quality of the representation improves as a function of the number of views used for training. Finally, we compare the contrastive formulation of multiview learning to the recently popular approach of cross-view prediction, and find that in head-to-head comparisons, the contrastive approach learns stronger representations.

The core ideas that we build on: contrastive learning, mutual information maximization, and deep representation learning, are not new and have been explored in the literature on representation and multiview learning for decades \cite{sa2004sensory,li2018survey,xu2013survey,arora2019theoretical}. Our main contribution is to set up a framework to extend these ideas to \emph{any number of views}, and to empirically study the factors that lead to success in this framework. A review of the related literature is given in Section \ref{sec:related_works}; and Fig. \ref{fig:method_fig} gives a pictorial overview of our framework. Our main contributions are:
\begin{itemize}[noitemsep]
    \item We apply contrastive learning to the multiview setting, attempting to maximize mutual information between representations of different views of the same scene (in particular, between different image channels).
    \item We extend the framework to learn from \emph{more than two} views, and show that the quality of the learned representation improves as number of views increase. Ours is the first work to explicitly show the benefits of multiple views on representation quality.
    \item We conduct controlled experiments to measure the effect of mutual information estimates on representation quality. Our experiments show that the relationship between mutual information and views is a subtle one.
    \item Our representations rival state of the art on popular benchmarks.
    \item We demonstrate that the contrastive objective is superior to cross-view prediction.
\end{itemize}
 \section{Related work}\label{sec:related_works}
\vspace{-5pt}

Unsupervised representation learning is about learning transformations of the data that make subsequent problem solving easier \citep{bengio2013representation}.
This field has a long history, starting with classical methods with well established algorithms, such as principal components analysis (PCA \citep{jolliffe2011principal}) and independent components analysis (ICA \citep{hyvarinen2004independent}). These methods tend to learn representations that focus on low-level variations in the data, which are not very useful from the perspective of downstream tasks such as object recognition.  

Representations better suited to such tasks have been learnt using deep neural networks, starting with seminal techniques such as Boltzmann machines \citep{smolensky1986information,salakhutdinov2009deep}, autoencoders \citep{hinton2006reducing}, variational autoencoders \citep{kingma2013auto}, generative adversarial networks \citep{goodfellow2014generative} and autoregressive models \citep{oord2016pixel}. Numerous other works exist, for a review see \citep{bengio2013representation}. A powerful family of models for unsupervised representations are collected under the umbrella of ``self-supervised" learning \citep{sa2004sensory,isola2015learning,zhang2017split,zhang2016colorful,wang2015unsupervised,pathak2016context,zhang2019aet}. In these models, an input  to the model is transformed into an output , which is supposed to be close to another signal  (usually in Euclidean space), which itself is related to  in some meaningful way. Examples of such  pairs are: luminance and chrominance color channels of an image \citep{zhang2017split}, patches from a single image \citep{oord2018representation}, modalities such as vision and sound \citep{owens2016visually} or the frames of a video \citep{wang2015unsupervised}. Clearly, such examples are numerous in the world, and provides us with nearly infinite amounts of training data: this is one of the appeals of this paradigm. Time contrastive networks \citep{sermanet2017time} use a triplet loss framework to learn representations from aligned video sequences of the same scene, taken by different video cameras. Closely related to self-supervised learning is the idea of multi-view learning, which is a general term involving many different approaches such as co-training \citep{blum1998combining}, multi-kernel learning \citep{cortes2009learning} and metric learning \citep{bellet2012similarity,zhuang2019local}; for comprehensive surveys please see \citep{xu2013survey,li2018survey}. Nearly all existing works have dealt with one or two views such as video or image/sound. However, in many situations, many more views are available to provide training signals for any representation. 

The objective functions used to train deep learning based representations in many of the above methods are either reconstruction-based loss functions such as Euclidean losses in different norms e.g. \citep{isola2017image}, adversarial loss functions \citep{goodfellow2014generative} that learn the loss in addition to the representation, or contrastive losses e.g. \citep{hadsell2006dimensionality,ye2019unsupervised,sohn2016improved,gutmann2010noise,hjelm2018learning,oord2018representation,arora2019theoretical,henaff2019data,ji2019invariant} that take advantage of the co-occurence of multiple views. 

Some of the prior works most similar to our own (and inspirational to us) are Contrastive Predictive Coding (CPC) \citep{oord2018representation}, Deep InfoMax \citep{hjelm2018learning}, and Instance Discrimination \citep{wu2018unsupervised}. These methods, like ours, learn representations by contrasting between congruent and incongruent representations of a scene. CPC learns from two views -- the past and future -- and is applicable to sequential data, either in space or in time. Deep Infomax \citep{hjelm2018learning} considers the two views to be the input to a neural network and its output. Instance Discrimination learns to match two sub-crops of the same image. CPC and Deep InfoMax have recently been extended in \citep{henaff2019data} and \citep{bachman2019learning} respectively. These methods all share similar mathematical objectives, but differ in the definition of the views. Our method differs from these works in the following ways: we extend the objective to the case of \emph{more than two} views, and we explore a different set of view definitions, architectures, and application settings. In addition, we contribute a unique empirical investigation of this paradigm of representation learning. 


The idea of contrastive learning has also started to spread over many other tasks in various other domains~\cite{sun2019contrastive,zhuang2019unsupervised,piergiovanni2019evolving,tschannen2019self,miech2019end,kawakami2020learning,Tian2020Contrastive}.
 
 \vspace{-10pt}
  \section{Method}

Our goal is to learn representations that capture information shared between multiple sensory views without human supervision. We start by reviewing previous predictive learning (or reconstruction-based learning) methods, and then elaborate on contrastive learning within two views. We show connections to mutual information maximization and extend it to scenarios including more than two views. We consider a collection of  views of the data, denoted as . For each view , we denote  as a random variable representing samples following . 

\subsection{Predictive Learning}

Let  and  represent two views of a dataset. For instance,  might be the luminance of a particular image and  the chrominance. We define the \emph{predictive learning} setup as a deep nonlinear transformation from  to  through latent variables , as shown in Fig.~\ref{fig:pred_contrast}. Formally,  and , where  and  represent the encoder and decoder respectively and  is the prediction of  given . The parameters of the encoder and decoder models are then trained using an objective function that tries to bring  ``close to" . Simple examples of such an objective include the  or  loss functions. Note that these objectives assume independence between each pixel or element of  given , i.e., , thereby reducing their ability to model correlations or complex structure. The predictive approach has been extensively used in representation learning, for example, colorization~\citep{zhang2016colorful,zhang2017split} and predicting sound from vision~\citep{owens2016visually}. \begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/contrastive_vs_predictive1.png}
\caption{\small{Predictive Learning vs Contrastive Learning. Cross-view prediction (\textbf{Top}) learns latent representations that predict one view from another, with loss measured in the \emph{output} space. Common prediction losses, such as the  and  norms, are \emph{unstructured}, in the sense that they penalize each output dimension independently, perhaps leading to representations that do not capture all the shared information between the views. In contrastive learning (\textbf{Bottom}), representations are learnt by contrasting congruent and incongruent views, with loss measured in \emph{representation} space. The red dotted outlines show where the loss function is applied.}}
\label{fig:pred_contrast}

\end{figure} \subsection{Contrastive Learning with Two Views}
\label{sec:contrast_two}

The idea behind contrastive learning is to learn an embedding that separates (contrasts) samples from two different distributions.
Given a dataset of  and  that consists of a collection of samples , we consider contrasting congruent and incongruent pairs, i.e. samples from the joint distribution  or , which we call \emph{positives}, versus samples from the product of marginals,  or , which we call \emph{negatives}.

We learn a ``critic" (a discriminating function)  which is trained to achieve a high value for positive pairs and low for negative pairs. Similar to recent setups for contrastive learning \citep{oord2018representation,gutmann2010noise,mnih2013learning}, we train this function to correctly select a single positive sample  out of a set  that contains  negative samples:

To construct , we simply fix one view and enumerate positives and negatives from the other view, allowing us to rewrite the objective as:

where  is the number of negative samples  for a given sample . In practice,  can be extremely large (e.g., 1.2 million in ImageNet), and so directly minimizing Eq. \ref{eqn:contrastive_loss} is infeasible. In Section \ref{sec:app_softmax}, we show two approximations that allow for tractable computation.

\paragraph{Implementing the critic}
We implement the critic  as a neural network. To extract compact latent representations of  and , we employ two encoders  and  with parameters  and  respectively. The latent representions are extracted as , . We compute their cosine similarity as score and adjust its dynamic range by a hyper-parameter :


Loss  in Eq.~\ref{eqn:contrastive_loss} treats view  as anchor and enumerates over . Symmetrically, we can get  by anchoring at . We add them up as our two-view loss:

After the contrastive learning phase, we use the representation , , or the concatenation of both, , depending on our paradigm. This process is visualized in Fig. \ref{fig:method_fig}.

\paragraph{Connecting to mutual information}
The optimal critic  is proportional to the density ratio between the joint distribution  and the product of marginals  (proof provided in supplementary material):

This quantity is the pointwise mutual information, and its expectation, in Eq. \ref{eqn:contrastive_loss}, yields an estimator related to mutual information. A formal proof is given by~\cite{oord2018representation,poole2019variational}, which we recapitulate in supplement, showing that:

where, as above,  is the number of negative pairs in sample set . Hence minimizing the objective  maximizes the lower bound on the mutual information , which is bounded above by  by the data processing inequality. The dependency on  also suggests that using more negative samples can lead to an improved representation; we show that this is indeed the case (see supplement). We note that recent work \citep{mcallester2018formal} shows that the bound in Eq. \ref{eqn:mi_lb} can be very weak; and finding better estimators of mutual information is an important open problem. \begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/info_diagram2.pdf}
\caption{\small{Graphical models and information diagrams~\citep{infdiag} associated with the core view and full graph paradigms, for the case of  views, which gives a total of  learning objectives. The numbers within the regions show how much ``weight" the total loss places on each partition of information (i.e. how many of the 6 objectives that partition contributes to). A region with no number corresponds to  weight. For example, in the full graph case, the mutual information between all  views is considered in all  objectives, and hence is marked with the number .}}
\label{fig:info_diagram}
\vspace{-10pt}
\end{figure} \subsection{Contrastive Learning with More than Two Views}
\label{sec:method_more_view}

We present more general formulations of Eq. \ref{eqn:contrastive_loss} that can handle any number of views. We call them the ``core view" and ``full graph" paradigms, which offer different tradeoffs between efficiency and effectiveness. These formulations are visualized in Fig.~\ref{fig:info_diagram}.

Suppose we have a collection of  views . The ``core view" formulation sets apart one view that we want to optimize over, say , and builds pair-wise representations between  and each other view , by optimizing the sum of a set of pair-wise objectives:

A second, more general formulation is the ``full graph" where we consider all pairs , and build 
relationships in all. By involving all pairs, the objective function that we optimize is:

Both these formulations have the effect that information is prioritized in proportion to the number of views that share that information. This can be seen in the information diagrams visualized in Fig.~\ref{fig:info_diagram}. The number in each partition of the diagram indicates how many of the pairwise objectives, , that partition contributes to. Under both the core view and full graph objectives, a factor, like ``presence of dog", that is common to all views will be preferred over a factor that affects fewer views, such as ``depth sensor noise".

The computational cost of the bivariate score function in the full graph formulation is combinatorial in the number of views. However, it is clear from Fig.~\ref{fig:info_diagram} that this enables the full graph formulation to capture more information between different views, which may prove useful for downstream tasks. For example, the mutual information between  and  or  and  is completely ignored in the core view paradigm (as shown by a  count in the information diagram). Another benefit of the full graph formulation is that it can handle missing information (e.g. missing views) in a natural manner. \subsection{Implementing the Contrastive Loss}
\label{sec:app_softmax}
Better representations using  in Eqn.~\ref{eqn:contrastive_loss} are learnt by using many negative samples. In the extreme case, we include every data sample in the denominator for a given dataset. However, computing the full softmax loss is prohibitively expensive for large dataset such as ImageNet. One way to approximate this full softmax distribution, as well as alleviate the computational load, is to use Noise-Contrastive Estimation~\cite{gutmann2010noise,wu2018unsupervised} (see supplement). Another solution, which we also adopt here, is to randomly sample  negatives and do a simple (+)-way softmax classification. This strategy is also used in~\cite{bachman2019learning,henaff2019data,he2019momentum} and dates back to~\cite{sohn2016improved}.

\textbf{Memory bank.}
Following~\cite{wu2018unsupervised}, we maintain a memory bank to store latent features for each training sample. Therefore, we can efficiently retrieve  negative samples from the memory buffer to pair with each positive sample without recomputing their features. The memory bank is dynamically updated with features computed on the fly. The benefit of a memory bank is to allow contrasting against more negative pairs, at the cost of slightly stale features. 

  \section{Experiments}
\label{sec:results}
We extensively evaluate Contrastive Multiview Coding (CMC) on a number of datasets and tasks. We evaluate on two established image representation learning benchmarks: ImageNet~\cite{deng2009imagenet} and STL-10~\cite{coates2011analysis} (see supplement). We further validate our framework on video representation learning tasks, where we use image and optical flow modalities, as the two views that are jointly learned. The last set of experiments extends our CMC framework to more than two views and provides empirical evidence of its effectiveness.

\subsection{Benchmarking CMC on ImageNet}\label{sec:imagenet}

Following~\cite{zhang2016colorful}, we evaluate task generalization of the learned representation by training 1000-way \emph{linear} classifiers on top of different layers. This is a standard benchmark that has been adopted by many papers in the literature.

\header{Setup.} Given a dataset of RGB images, we convert them to the \emph{Lab} image color space, and split each image into \emph{L} and \emph{ab} channels, as originally proposed in SplitBrain autoencoders \cite{zhang2017split}. During contrastive learning, L and ab from the same image are treated as the positive pair, and ab channels from other randomly selected images are treated as a negative pair (for a given L). Each split represents a view of the orginal image and is passed through a separate encoder. As in SplitBrain, we design these two encoders by evenly splitting a given deep network, such as AlexNet \citep{krizhevsky2012imagenet}, into sub-networks across the channel dimension. By concatenating representations layer-wise from these two encoders, we achieve the final representation of an input image. As proposed by previous literature \citep{oord2018representation,hjelm2018learning,arora2019theoretical,zhuang2019local,wu2018unsupervised}, the quality of such a representation is evaluated by freezing the weights of encoder and training linear classifier on top of each layer.

\header{Implementation.} Unless otherwise specified, we use PyTorch~\cite{paszke2019pytorch} default data augmentation. Following~\cite{wu2018unsupervised}, we set the temperature  as 0.07 and use a momentum 0.5 for memory update. We use 16384 negatives. The supplementary material provides more details on our hyperparameter settings.




\header{CMC with AlexNet.} As many previous unsupervised methods
are evaluated with AlexNet~\cite{krizhevsky2012imagenet} on ImageNet~\cite{deng2009imagenet,krahenbuhl2015data,doersch2015unsupervised,zhang2016colorful,noroozi2016unsupervised,donahue2016adversarial,zhang2017split,noroozi2017representation,gidaris2018unsupervised,caron2018deep,zhang2019aet}, we also include the the results of CMC using this network. Due to the space limit, we present this comparison in supplementary material.

\begin{table}[t]
    \setlength{\tabcolsep}{5pt}
	\centering
    \begin{tabular}{c|ccc}
    \multirow{2}{*}{Setting} & ResNet-50 & ResNet-50 & ResNet-50 \\
     & x0.5 & x1 & x2 \\
    \shline
     &        57.5 / 80.3 & 64.0 / 85.5 & 68.3 / 88.2\\
     &      58.4 / 81.2 & 64.8 / 86.1 & 69.0 / 88.9\\
     + RA & \textbf{60.0 / 82.3} & \textbf{66.2 / 87.0} & \textbf{70.6 / 89.7}\\
    \end{tabular}
    \caption{\small{Top-1 / Top-5 \emph{Single} crop classification accuracy (\%) on ImageNet with a supervised logistic regression classifier. We evaluate CMC using ResNet50 with different width as encoder for \emph{each} of the two views (e.g., \emph{L} and \emph{ab}). ``RA'' stands for RandAugment~\cite{cubuk2019randaugment}.}}
    \vspace{-5pt}
    \label{tbl:resnet}
\end{table}
 \header{CMC with ResNets.} We verify the effectiveness of CMC with larger networks such as ResNets~\cite{he2016deep}. 
We experiment on learning from luminance and chrominance views in two colorspaces,  and  (see sec.~\ref{sec:color_space} for validation of this choice), and we vary the width of the ResNet encoder for each view. We use the feature after the global pooling layer to train the linear classifier, and the results are shown in Table~\ref{tbl:resnet}.  achieves  top-1 single crop accuracy with ResNet50x2 for each view, and switching to  further brings about  improvement. On top of it, strengthening data augmentation with RandAugment~\cite{cubuk2019randaugment} yields better or comparable results to other state-of-the-art methods~\cite{kolesnikov2019revisiting,wu2018unsupervised,zhuang2019local,he2019momentum,misra2019self,donahue2019large,henaff2019data,bachman2019learning}.

 \subsection{CMC on videos}\label{exp:video}
We apply CMC on videos by drawing insight from the two-streams hypothesis \citep{schneider1969two,goodale1992separate}, which posits that human visual cortex consists of two distinct processing streams: the ventral stream, which performs object recognition, and the dorsal stream, which processes motion. In our formulation, given an image  that is a frame centered at time , the ventral stream associates it with a neighbouring frame , while the dorsal stream connects it to optical flow  centered at . Therefore, we extract ,  and  from two modalities as three views of a video; for optical flow we use the TV-L1 algorithm \citep{zach2007duality}. Two separate contrastive learning objectives are built within the ventral stream  and within the dorsal stream . For the ventral stream, the negative sample for  is chosen as a random frame from another randomly chosen video; for the dorsal stream, the negative sample for  is chosen as the flow corresponding to a random frame in another randomly chosen video. 

\header{Pre-training.} We train CMC on UCF101 \citep{soomro2012ucf101} and use two CaffeNets~\citep{krizhevsky2012imagenet} for extracting features from images and optical flows, respectively. In our implementation,  represents 10 continuous flow frames centered at . We use batch size of 128 and contrast each positive pair with 127 negative pairs. 

\begin{table}
	\centering
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{l|ccc}
Method & \# of Views &UCF-101 & HMDB-51\\
        \shline
		Random & - & 48.2 & 19.5 \\
		ImageNet & - & 67.7 & 28.0 \\
		\hline
		VGAN*~\citep{vondrick2016generating}  & 2 & 52.1 & - \\
		LT-Motion*~\citep{luo2017unsupervised} & 2 &53.0 & - \\
		\hline
		TempCoh~\citep{mobahi2009deep} & 1 & 45.4 & 15.9 \\
		Shuffle and Learn~\citep{misra2016shuffle} & 1 & 50.2 & 18.1\\
		Geometry~\citep{gan2018geometry} & 2 & 55.1 & 23.3 \\
		OPN~\citep{lee2017unsupervised}  & 1 & 56.3 & 22.1\\
		ST Order~\citep{buchler2018improving} & 1 & 58.6 & 25.0 \\
		Cross and Learn~\citep{sayed2018cross} & 2 & 58.7 & \textbf{27.2} \\
		\hline
		CMC (V) & 2 & 55.3 & - \\
		CMC (D) & 2 & 57.1 & - \\
		CMC (V+D)& 3 & \textbf{59.1} & 26.7 \\ 
	\end{tabular}
	\caption{\small{Test accuracy (\%) on UCF-101 which evaluates \emph{task} transferability and on HMDB-51 which evaluates \emph{task} and \emph{dataset} transferability. Most methods either use single RGB view or additional optical flow view, while VGAN explores sound as the second view. * indicates different network architecture.}}
	\vspace{-10pt}
	\label{table:actionrecognition}
\end{table} 
\header{Action recognition.} We apply the learnt representation to the task of action recognition. The spatial network from~\citep{simonyan2014two} is a well-established paradigm for evaluating pre-trained RGB network on action recognition task. We follow the same spirit and evaluate the transferability of our RGB CaffeNet on UCF101 and HMDB51 datasets. We initialize the action recognition CaffeNet up to conv5 using the weights from the pre-trained RGB CaffeNet. The averaged accuracy over three splits is present in Table~\ref{table:actionrecognition}. Unifying both ventral and dorsal streams during pre-training produces higher accuracy for downstream recognition than using only single stream. Increasing the number of views of the data from  to  (using both streams instead of one) provides a boost for UCF-101. 
 \subsection{Extending CMC to More Views}
We further extend our CMC learning framework to multiview scenarios. We experiment on the NYU-Depth-V2 \citep{Silberman:ECCV12} dataset which consists of 1449 labeled images. We focus on a deeper understanding of the behavior and effectiveness of CMC. The views we consider are: luminance (L channel), chrominance (ab channel), depth, surface normal~\citep{eigen2015predicting}, and semantic labels.

\header{Setup.} To extract features from each view, we use a  neural network with 5 convolutional layers, and 2 fully connected layers. As the size of the dataset is relatively small, we adopt the sub-patch based contrastive objective (see supplement) to increase the number of negative pairs. Patches with a size of  are randomly cropped from the original images for contrastive learning (from images of size ). For downstream tasks, we discard the fully connected layers and evaluate using the convolutional layers as a representation. 

\subsubsection{Does representation quality improve as number of views increases?}

To measure the quality of the learned representation, we consider the task of predicting semantic labels from the representation of . We follow the \emph{core view paradigm} and use  as the core view, thus learning a set of representations by contrasting different views with .
A UNet style architecture~\citep{ronneberger2015u} is utilized to perform the segmentation task. Contrastive training is performed on the above architecture that is equivalent of the UNet's encoder. After contrastive training is completed, we initialize the encoder weights of the UNet from the  encoder (which are equivalent architectures) and keep them frozen. Only the decoder is trained during this finetuning stage. 

\begin{figure}
  \centering
  \begin{subfigure}[]{0.23\textwidth}
    \includegraphics[width=\textwidth]{fig/n_vs_iou.pdf}
  \end{subfigure}
  \begin{subfigure}[]{0.23\textwidth}
    \includegraphics[width=\textwidth]{fig/n_vs_pa.pdf}
  \end{subfigure}
  \caption{\small{We show the Intersection over Union (IoU) (left) and Pixel Accuracy (right) for the NYU-Depth-V2 dataset, as CMC is trained with increasingly more views from 1 to 4. As more views are added, both these metrics steadily increase. The views are (in order of inclusion): L, ab, depth and surface normals.}}
  \label{fig:iou_pa}
\end{figure} 
Since we use the patch-based contrastive loss, in the 1 view setting case, CMC coincides with DIM~\citep{hjelm2018learning}.  
The 2-4 view cases contrast L with ab, and then sequentially add depth and surface normals. The semantic labeling results are measured by mean IoU over all classes and pixel accuracy, shown in Fig.~\ref{fig:iou_pa}. We see that the performance steadily improves as new views are added. We have tested different orders of adding the views, and they all follow a similar pattern.

\begin{table}[t]
	\centering
	\setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{l|cc}
     & Pixel Accuracy (\%) & mIoU (\%) \\
    \shline
    Random & 45.5 & 21.4  \\
    CMC (core-view) & 57.1 & 34.1 \\
    CMC (full-graph) & 57.0 & 34.4 \\
    Supervised & \textbf{57.8} & \textbf{35.9} \\
    \end{tabular}
    \caption{\small{Results on the task of predicting semantic labels from \textbf{L channel} representation which is learnt using the patch-based contrastive loss and all  views. We compare CMC with \emph{Random} and \emph{Supervised} baselines, which serve as lower and upper bounds respectively. Th core-view paradigm refers to Fig. \ref{fig:info_diagram}(a), and full-view Fig. \ref{fig:info_diagram}(b).}}
    \label{tbl:nyu_L}
\vspace{-10pt}
\end{table} 
We also compare CMC with two baselines. First, we randomly initialize and freeze the encoder, and we call this the \emph{Random} baseline; it serves as a lower bound on the quality since the representation is just a random projection. Rather than freezing the randomly initialized encoder, we could train it jointly with the decoder. This end-to-end \emph{Supervised} baseline serves as an upper bound. The results are presented in Table~\ref{tbl:nyu_L}, which shows our CMC produces high quality feature maps even though it's unaware of the downstream task.

\subsubsection{Is CMC improving all views?}

A desirable unsupervised representation learning algorithm operating on multiple views or modalities should improve the quality of representations for all views. We therefore investigate our CMC framwork beyond L channel. To treat all views fairly, we train these encoders following the \emph{full graph paradigm}, where each view is contrasted with all other views.

We evaluate the representation of each view  by predicting the semantic labels from only the representation of , where  is L, ab, depth or surface normals. This uses the full-graph paradigm.
As in the previous section, we compare CMC with \emph{Random} and \emph{Supervised} baselines. As shown in Table~\ref{tbl:nyu_all}, the performance of the representations learned by CMC using full-graph significantly outperforms that of randomly projected representations, and approaches the performance of the fully supervised representations. Furthermore, the full-graph representation provides a good representation learnt for all views, showing the importance of capturing different types of mutual information across views.

\begin{table}[t]
    \setlength{\tabcolsep}{4.5pt}
	\centering
    \begin{tabular}{l|c|cccc}
     & Metric (\%) & L & ab & Depth & Normal \\
    \shline
    \multirow{2}{*}{Random}      & mIoU & 21.4 & 15.6 & 30.1 & 29.5 \\
                             & pix. acc.& 45.5 & 37.7 & 51.1 & 50.5 \\
    \hline
    \multirow{2}{*}{CMC}         & mIoU & 34.4 & 26.1 & 39.2 & 37.8 \\
                             & pix. acc.& 57.0 & 49.6 & \textbf{59.4} & 57.8 \\
    \hline
    \multirow{2}{*}{Supervised}  & mIoU & \textbf{35.9} & \textbf{29.6} & \textbf{41.0} & \textbf{41.5} \\
                             & pix. acc.& \textbf{57.8} & \textbf{52.6} & 59.1 & \textbf{59.6} \\
    \end{tabular}
    \caption{\small{Performance on the task of using single view  to predict the semantic labels, where  can be L, ab, depth or surface normal. Our CMC framework improves the quality of unsupervised representations towards that of supervised ones, for all of views investigated. This uses the full-graph paradigm Fig. 3(b).}}
    \label{tbl:nyu_all}
\end{table} 
\subsubsection{Predictive Learning vs. Contrastive Learning}
While experiments in section~\ref{sec:imagenet} show that contrastive learning outperforms predictive learning~\citep{zhang2017split} in the context of Lab color space, it's unclear whether such an advantage is due to the natural inductive bias of the task itself. To further understand this, we go beyond chrominance (ab), and try to answer this question when geometry or semantic labels are present.

We consider three view pairs on the NYU-Depth dataset: (1) L and depth, (2) L and surface normals, and (3) L and segmentation map. For each of them, we train two identical encoders for L, one using contrastive learning and the other with predictive learning. We then evaluate the representation quality by training a linear classifier on top of these encoders on the STL-10 dataset. 

\begin{table}[t]
\centering
	\centering
    \begin{tabular}{l|cc}
    & \multicolumn{2}{c}{Accuracy on STL-10 (\%)} \\
    \shline
    Views & Predictive  & Contrastive \\
    \hline
    L, Depth & 55.5 & \textbf{58.3} \\
    L, Normal & 58.4 & \textbf{60.1} \\
    L, Seg. Map & 57.7 & \textbf{59.2} \\
    \hline
    Random & \multicolumn{2}{c}{25.2} \\
    Supervised & \multicolumn{2}{c}{65.1} \\
    
    \end{tabular}
    
    \caption{\small{We compare predictive learning with contrastive learning by evaluating the learned encoder on unseen dataset and task. The contrastive learning framework consistently outperforms predictive learning.}}
    \label{tbl:nyu_compare}
\end{table} 
\begin{figure*}[t]
\centering
\includegraphics[width=0.48\linewidth]{fig/mi_acc.pdf}
\includegraphics[width=0.48\linewidth]{fig/crop_mi_acc.pdf}
\caption{\small{How does mutual information between views relate to representation quality? (Left) Classification accuracy against estimated MI between channels of different color spaces; (Right) Classification accuracy vs  estimated MI between patches at different distances (distance in pixels is denoted next to each data point). MI estimated using MINE~\citep{belghazi2018mine}.}}
\label{fig:mi_vs_acc}
\end{figure*} 
The comparison results are shown in Table \ref{tbl:nyu_compare}, which shows that contrastive learning consistently outperforms predictive learning in this scenario where both the task and the dataset are unknown. We also include ``random'' and ``supervised'' baselines similar to that in previous sections. Though in the unsupervised stage we only use 1.3K images from a dataset much different from the target dataset STL-10, the object recognition accuracy is close to the supervised method, which uses an end-to-end deep network directly trained on STL-10.

Given two views  and  of the data, the predictive learning approach approximately models . Furthermore, losses used typically for predictive learning, such as pixel-wise reconstruction losses usually impose an independence assumption on the modeling: . On the other hand, the contrastive learning approach by construction does not assume conditional independence across dimensions of . In addition, the use of random jittering and cropping between views allows the contrastive learning approach to benefit from spatial co-occurrence (contrasting in space) in addition to contrasting across views. We conjecture that these are two reasons for the superior performance of contrastive learning approaches over predictive learning.



 \subsection{How does mutual information affect representation quality?}\label{sec:color_space}


Given a fixed set of views, CMC aims to maximize the mutual information between representations of these views. We have found that maximizing information in this way indeed results in strong representations, but it would be incorrect to infer that information maximization (infomax) is the key to good representation learning. In fact, this paper argues for precisely the opposite idea: that cross-view representation learning is effective because it results in a kind of information minimization, \emph{discarding} nuisance factors that are not shared between the views.

The resolution to this apparent dilemma is that we want to maximize the ``good" information -- the \emph{signal} -- in our representations, while minimizing the ``bad" information -- the \emph{noise}. The idea behind CMC is that this can be achieved by doing infomax learning on two views that share signal but have independent noise. This suggests a ``Goldilocks principle" \cite{kidd2012goldilocks}: a good collection of views is one that shares some information but not too much. Here we test this hypothesis on two domains: learning representations on images with different colorspaces forming the two views; and learning representations on pairs of patches extracted from an image, separated by varying spatial distance. 

In patch experiments we randomly crop two RGB patches of size 64x64 from the same image, and use these patches as the two views. Their relative position is fixed. Namely, the two patches always starts at position  and  with  being randomly sampled. While varying the distance , we start from  to avoid overlapping. There is a possible bias that with an image of relatively small size (e.g., 512x512), a large  (e.g., 384) will always push these two patches around boundary. To minimize this bias, we use high resolution images (e.g. ) from DIV2K~\citep{agustsson2017ntire} dataset.

Fig. \ref{fig:mi_vs_acc} shows the results of these experiments. The left plot shows the result of learning representations on different colorspaces (splitting each colorspace into two views, such as (L, ab), (R, GB) etc). We then use the MINE estimator \cite{belghazi2018mine} to estimate the mutual information between the views. We measure representation quality by training a linear classifier on the learned representations on the STL-10 dataset \cite{coates2011analysis}. The plots clearly show that using colorspaces with minimal mutual information give the best downstream accuracy (For the outlier HSV in this plot, we conjecture the representation quality is harmed by the periodicity of H. Note that the H in HED is not periodic.). On the other hand, the story is more nuanced for representations learned between patches at different offsets from each other (Fig. \ref{fig:mi_vs_acc}, right). Here we see that views with too little or too much MI perform worse; a sweet spot in the middle exists which gives the best representation. That there exists such a sweet spot should be expected. If two views share \emph{no} information, then, in principle, there is no incentive for CMC to learn anything. If two views share all their information, no nuisances are discarded and we arrive back at something akin to an autoencoder or generative model, that simply tries to represent all the bits in the multiview data.

These experiments demonstrate that the relationship between mutual information and representation quality is meaningful but not direct. Selecting optimal views, which just share relevant signal, may be a fruitful direction for future research.  \section{Conclusion}
We have presented a contrastive learning framework which enables the learning of unsupervised representations from multiple views of a dataset. The principle of maximization of mutual information enables the learning of powerful representations. A number of empirical results show that our framework performs well compared to predictive learning and scales with the number of views.  
\vspace{10pt}
\noindent \textbf{Acknowledgements} Thanks to Devon Hjelm for providing implementation details of Deep InfoMax, Zhirong Wu and Richard Zhang for helpful discussion and comments. This material is based on resources supported by Google Cloud. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{CMC}
}

\newpage

\appendix
\section{ImageNet-100 Proposed in this Paper}
\changeurlcolor{purple}
\begin{table}[t]
\caption{\small{The list of classes from ImageNet-100, which are randomly sampled from the original ImageNet-1k dataset~\cite{deng2009imagenet}. This list can also be downloaded at:
\small{\url{http://github.com/HobbitLong/CMC/blob/master/imagenet100.txt}}}
}
\setlength{\tabcolsep}{3.5pt}
\small
\vspace{-10pt}
\label{tab:imagenet100_list}
\begin{center}
\begin{small}
\begin{tabular}{ccccc}
\toprule
\multicolumn{5}{c}{List of ImageNet-100 classes} \\
\midrule
\midrule

n02869837 & n01749939 & n02488291 & n02107142 & n13037406  \\
n02091831 & n04517823 & n04589890 & n03062245 & n01773797  \\
n01735189 & n07831146 & n07753275 & n03085013 & n04485082  \\
n02105505 & n01983481 & n02788148 & n03530642 & n04435653  \\
n02086910 & n02859443 & n13040303 & n03594734 & n02085620  \\
n02099849 & n01558993 & n04493381 & n02109047 & n04111531  \\
n02877765 & n04429376 & n02009229 & n01978455 & n02106550  \\
n01820546 & n01692333 & n07714571 & n02974003 & n02114855  \\
n03785016 & n03764736 & n03775546 & n02087046 & n07836838  \\
n04099969 & n04592741 & n03891251 & n02701002 & n03379051  \\
n02259212 & n07715103 & n03947888 & n04026417 & n02326432  \\
n03637318 & n01980166 & n02113799 & n02086240 & n03903868  \\
n02483362 & n04127249 & n02089973 & n03017168 & n02093428  \\
n02804414 & n02396427 & n04418357 & n02172182 & n01729322  \\
n02113978 & n03787032 & n02089867 & n02119022 & n03777754  \\
n04238763 & n02231487 & n03032252 & n02138441 & n02104029  \\
n03837869 & n03494278 & n04136333 & n03794056 & n03492542  \\
n02018207 & n04067472 & n03930630 & n03584829 & n02123045  \\
n04229816 & n02100583 & n03642806 & n04336792 & n03259280  \\
n02116738 & n02108089 & n03424325 & n01855672 & n02090622  \\

\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\vspace{-5pt}
\end{table} In this paper, we proposed a subset of ImageNet that contains randomly selected 100 classes for ablation study as well as hyper-parameter tuning. To ease a relevant study in the future, we release the list of these categories that we consistently used throughout all our experiments, as summarized in Table~\ref{tab:imagenet100_list}.

\section{Contrastive Loss}

\subsection{NCE approximation for high-dimensional softmax corss-entropy}

In addition to the subsampled (+)-way softmax cross-entropy, Noise-Contrastive Estimation (NCE~\citep{gutmann2010noise}) is another way to approximate the -way ( is the dataset size) softmax cross entropy full softmax in Eqn 2. Compared with the (+)-way softmax cross-entropy, NCE is computationally faster and may result in slightly worse performance in standard linear evaluation. It has been used in~\cite{mnih2013learning,wu2018unsupervised}\footnote{Confusingly, the literature has previously referred to Eqn.2 as ``InfoNCE"~\citep{oord2018representation}. Our NCE approximation \emph{does not} refer to the allusion to NCE in the name ``InfoNCE". Rather we are here describing an NCE approximation to the ``InfoNCE" softmax objective.}. We depict its general idea as below.

Given an anchor  from , the probablity that an atom  from  is the best match of , using the score  is given by:

where the normalization factor  is expensive to compute for large . 

NCE~\citep{gutmann2010noise} is an effective way to estimate \emph{unnormalized} statistical models. NCE fits a density model  to data distributed as (unknown) distribution , by using a binary classifier to distinguish it from noise samples distributed as . To learn , we use a binary classifier, which treats  as the data (or positive) sample when given . The noise distribution  we choose here is a uniform distribution over all atoms from , i.e., . If we sample  noise samples to pair with each data sample, the posterior probability that a given atom  comes from the data distribution is: 

and we estimate this probability by replacing  with our \emph{unnormalized} model distribution , where  is a constant estimated from the first batch. Minimizing the negative log-posterior probability of correct labels  over data and noise samples yields our final objective, which is the NCE-based approximation of Eq. 2 ( is the empirical data distribution):


\subsection{Contrasting Sub-patches}\label{app:sub-patch}

Instead of contrasting features from the last layer, patch-based method~\citep{hjelm2018learning} contrasts feature from the last layer with features from previous layers, hence increasing the number of negative pairs. For instance, we use features from the last layer of  to contrast with feature points from feature maps produced by the first several conv layers of . This is equivalent to contrast between global patch from one view with local patches from the other view. In this fashion, we directly perform  way softmax classification, the same as~\citep{oord2018representation,hjelm2018learning} for a fair comparison in Sec.~\ref{exp:stl-10}.

Such patch-based contrastive loss is computed within each mini-batch and does not require a memory bank. Therefore, deploying it in parallel training schemes is easy and flexible. However, patch-based contrastive loss usually yields suboptimal results compared to NCE-based contrastive loss, according to our experiments.


\section{Proofs}
\label{sec:app_MI}
We prove that: (a) the optimal score function  is proportional to density ratio between the joint distribution  and product of marginals , as shown in Eq. 5; (b) Minimizing the contrastive loss  maxmizes a lower bound on the mutual information between two views, as shown in Eq. 6.

We will use the most general formula of contrastive loss  shown in Eq. 1 for our derivation. But we note that replacing  with  is straightforward. The overall proof follows a similar derivation introduced in ~\citep{oord2018representation}.

\subsection{Score function as density ratio estimator}
\label{sec:app:score_function}

We first show that the optimal score function  that minimizes Eq. 1 is proportional to  the density ratio between joint distribution and product of marginals, shown as Eq. 5. For notation convenience, we denote  as data distribution  and  as noise distribution . The loss in Eq. 1 is indeed a cross-entropy loss of classifying the correct positive pair out from the given set . Without loss of generality, we assume the first pair  in  is positive or congruent and all others  are negative or incongruent. The optimal probability for the loss, , should depict the fact that  comes from the data distribution  while all other pairs come from the noise distribution . Therefore,

where we plug in the definition of  and , and divide  for both the numerator and denominator. By comparing above equation with the loss function in Eq. 1, we can see that the optimal score function  is proportional to the density ratio . The above derivation is agnostic to which layer the score function starts from, e.g.,  can be defined on either the raw input  or the latent representation . As we care more about the property of the latent representation, for the following derivation we will use , which is proportional to .

\subsection{Maximizing lower bound on MI}
\label{sec:app:mi}

Now we substitute the score function in Eq. 1 with the above density ratio, and the optimal loss objective  becomes:

Therefore, for any two views  and , we have . 
As the  increases, the approximation step becomes more accurate. Given any , minimizing  maximizes the lower bound on the mutual information . We should note that increasing  to infinity does not always lead to a higher lower bound. While  increases with a larger , the optimization problem becomes harder and  also increases.


\section{Additional Experiments}




\subsection{CMC on STL-10}\label{exp:stl-10}
STL-10~\citep{coates2011analysis} is an image recognition dataset designed for developing unsupervised or self-supervised learning algorithms. It consists of  unlabeled training  RGB image samples and  labeled samples for each of the  classes. 

\header{Setup.} We adopt the same  data augmentation strategy and network architecture as those in DIM~\citep{hjelm2018learning}. A variant of AlexNet takes as input  images, which are randomly cropped and horizontally flipped from the original  size images. For a fair comparison with DIM, we also train our model in a patch-based contrastive fashion during unsupervised pre-training. With the weights of the pre-trained encoder frozen, a two-layer fully connected network with 200 hidden units is trained on top of different layers for 100 epochs to perform 10-way classification. We also investigated the strided crop strategy of CPC~\citep{oord2018representation}. Fixed sized overlapping patches of size  with an overlap of  pixels are cropped and fed into the network separately. This ensures that features of one patch contain minimal information from neighbouring patches; and increases the available number of negative pairs for the contrastive loss. Additionally, we include NCE-based contrastive training and linear classifier evaluation.

\header{Comparison.} We compare CMC with the state of the art unsupervised methods in Table~\ref{tbl:stl10}. Three columns are shown: the conv5 and fc7 columns use respectively these layers of AlexNet as the encoder (again remembering that we split across channels for L and ab views). For these two columns we can compare against the all methods except CPC, since CPC does not report these numbers in their paper \citep{hjelm2018learning}. In the Strided Crop setup, we only compare against the approaches that use contrastive learning, DIM and CPC, since this method was only used by those works. We note that in Table~\ref{tbl:stl10} for all the methods except SplitBrain, we report numbers are shown in the original paper. For SplitBrain, we reimplemented their model faithfully and report numbers based on our reimplementation (we verified the accuracy of our SplitBrain  code by the fact that we get very similar results with our reimpementation as in the original paper \citep{zhang2017split} for ImageNet experiments, see below).

The family of contrastive learning methods, such as DIM, CPC, and CMC, achieve higher classification accuracy than other methods such as SplitBrain that use predictive learning; or BiGAN that use adversarial learning. CMC significantly outperforms DIM and CPC in all cases. We hypothesize that this outperformance results from the modeling of cross-view mutual information, where view-specific noisy details are discarded. Another head-to-head comparison happens between CMC and SplitBrain, both of which modeling images as seprated L and ab streams; we achieve a nearly  absolute improvement for conv5 and  improvement for fc5. Finally, we notice that the predictive learning methods suffer from a big drop in performance when the encoding layer is switched from conv5 to fc7. On the other hand, the contrastive learning approaches are much more stable across layers, suggesting that the mutual information maximization paradigm learns more semantically meaningful representations shared by the different views. From a practical perspective, this is a significant advantage as the selection of specific layers should ideally not change downstream performance by too much.

In this experiments we used AlexNet as backbone. Switching to more powerful networks such as ResNets is likely to further improve the representation quality.

\begin{table}[t]
    \setlength{\tabcolsep}{5pt}
	\centering
	\small
    \begin{tabular}{l|c|cc|c}
    
    Method & classifier & conv5 & fc7 & Strided Crop \\
    \shline
    AE & \multirow{4}{*}{MLP} &62.19 & 55.78 & - \\
    NAT~\citep{bojanowski2017unsupervised} &  & 64.32 & 61.43 & - \\
    BiGAN~\citep{donahue2016adversarial} &  & 71.53 & 67.18 & - \\
    SplitBrain~\citep{zhang2017split} &  & 72.35 & 63.15 & - \\
    \hline
    DIM~\citep{hjelm2018learning} & \multirow{2}{*}{MLP} & 72.57 & 70.00 & 78.21 \\
    CPC~\citep{oord2018representation} & & - & - & 77.81 \\
    \hline
    CMC(Patch) & Linear & 76.65 & 79.25 & 82.58 \\
    CMC(Patch) & MLP & 80.14 & 80.11 & \textbf{83.43} \\
    CMC(NCE) & Linear & 83.28 & 86.66 & - \\
    CMC(NCE) & MLP & \textbf{84.64} & \textbf{86.88} & - \\
    \hline
    Supervised & \multicolumn{4}{c}{68.70} \\
    \end{tabular}
    \caption{\small{Classification accuracies on STL-10 by using a two layer MLP as classifier for evaluating the representations learned by a small \textbf{AlexNet}. For all methods we compare against, we include the numbers that are reported in the DIM~\citep{hjelm2018learning} paper, except for SplitBrain, which is our reimplementation. Methods marked with  have half the number of parameters because of splitting.}}
    \label{tbl:stl10}
\end{table} 
\subsection{CMC on ImageNet with AlexNet}

ImageNet~\citep{deng2009imagenet} consists of 1000 image classes and is frequently considered as a testbed for unsupervised representation learning algorithms.

To compare with other methods, we adopt standard AlexNet and split it into two encoders. Because of splitting, each layer only connects to half of the neurons in the previous layer, and therefore the number of parameters in our model halves. We remove local response layer and add batch normalization to each layer. For the memory-based CMC model, we adopt ideas from \cite{wu2018unsupervised} for computing and storing a memory. We retrieve  negative pairs from the memory bank to contrast each positive pair (the effect of the number of negatives is shown in Sec.~\ref{sec:imgnet}). The training details are present in Sec.~\ref{app:imagenet}.

\begin{table}[htp]
\setlength{\tabcolsep}{3.5pt}
\centering
\begin{tabular}{l|ccccc}
& \multicolumn{5}{c}{ImageNet Classification Accuracy} \\
Method  & conv1 & conv2 & conv3 & conv4 & conv5\\
\shline
ImageNet-Labels & 19.3 & 36.3 & 44.2 & 48.3 & 50.5 \\
\hline
Random & 11.6 & 17.1 & 16.9 & 16.3 & 14.1\\
Data-Init~\citep{krahenbuhl2015data} & 17.5 & 23.0 & 24.5 & 23.2 & 20.6\\
\hline
Context~\citep{doersch2015unsupervised} & 16.2 & 23.3 & 30.2 & 31.7 & 29.6 \\
Colorization~\citep{zhang2016colorful} & 13.1 & 24.8 & 31.0 & 32.6 & 31.8\\
Jigsaw~\citep{noroozi2016unsupervised} & 19.2 & 30.1 & 34.7 & 33.9 & 28.3 \\
BiGAN~\citep{donahue2016adversarial} & 17.7 & 24.5 & 31.0 & 29.9 & 28.0 \\
SplitBrain~\citep{zhang2017split} & 17.7 & 29.3 & 35.4 & 35.2 & 32.8\\
Counting~\citep{noroozi2017representation} & 18.0 & 30.6 & 34.3 & 32.5 & 25.7 \\
Inst-Dis~\citep{wu2018unsupervised} & 16.8 & 26.5 & 31.8 & 34.1 & 35.6 \\
RotNet~\citep{gidaris2018unsupervised} & 18.8 & 31.7 & 38.7 & 38.2 & 36.5 \\
DeepCluster~\citep{caron2018deep} & 12.9 & 29.2 & 38.2 & 39.8 & 36.1 \\
AET \citep{zhang2019aet} & \textbf{19.3} & 32.8 & \textbf{40.6} & 39.7 & 37.7 \\
\hline
CMC() & 18.3 & \textbf{33.7} & 38.3 & \textbf{40.5} & \textbf{42.8}\\
\end{tabular}
\caption{\small
Top-1 classification accuracy on 1000 classes of ImageNet \cite{deng2009imagenet} with \emph{single} crop. We compare
our CMC method with other unsupervised representation learning approaches by training 1000-way logistic regression classifiers on top of the feature maps of each layer, as proposed by~\cite{zhang2016colorful}. Methods marked with  only have half the number of parameters compared to others, because of splitting. 
}
\label{exp:cls_imagenet}
\end{table} 
Table~\ref{exp:cls_imagenet} shows the results of comparing the CMC against other models, both predictive and contrastive. Our CMC is the best among all these methods; futhermore CMC tends to perform better at higher convolutional layers, similar to another contrasting-based model Inst-Dis \citep{wu2018unsupervised}.

\subsection{Number of negatives}\label{sec:imgnet}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/nce_k.pdf}
\caption{\small{We plot the number of negative examples  in NCE-based contrastive loss against the accuracy for 100 randomly chosen classes of Imagenet 100. It is seen that the accuracy steadily increases with .}}
\label{fig:num_nce}
\end{figure} 
\header{Effect of the number of negative samples.}
\label{sec:num_neg}
We investigate the relationship between the number of negative pairs  in NCE-based loss and the downstream classification accuracy on a randomly chosen subset of  classes of Imagenet (the same set of classes is used for any number of negative pairs). We train a 100-way linear classifier using CMC pre-trained features with varying number of negative pairs, starting from  pairs upto  (in multiples of ). Fig. \ref{fig:num_nce} shows that the accuracy of the resulting classifier steadily increases but saturates at around  with  samples. We used AlexNet and the NCE approximation in this study ((+1)-way softmax cross entropy, a.k.a. InfoNCE, also follow a similar trend). 

\subsection{Compatibility with other methods}

\begin{table}[t]
\setlength{\tabcolsep}{5pt}
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
Method & \# of Params  & Top-1 Acc \\
\midrule 
\midrule 
MoCo & 24M & 73.5 \\
PIRL & 24M & 75.1 \\
CMC  & 12M & 75.6 \\
\midrule 
CMC + MoCo  & 12M & 77.2~\showdiff{(+1.6)} \\
CMC + PIRL  & 12M & 78.0~\showdiff{(+2.4)} \\
CMC + MoCo + PIRL  & 12M & 79.3~\showdiff{(+3.7)} \\
CMC + MoCo + PIRL + RA  & 12M & 81.5~\showdiff{(+5.9)} \\
\bottomrule
\end{tabular}
\caption{\small{Compatibility of CMC with other mthods. We pre-train and evaluate on ImageNet-100 subset with top-1 accuracy reported. Specifically, we combine CMC with MoCo (\ie, switching from memory bank to momentum encoder), PIRL (\ie, applying a second JigSaw branch), or both. Consistent improvement is observed with a ResNet-50. RA stands for RandAugment.}}
\label{tab:compatibility}
\end{small}
\end{center}
\end{table} 
To test the compatibility of CMC with mechanisms proposed in other self-supervised learning methods, we consider combining CMC with MoCo~\cite{he2019momentum} (\ie, switching from memory bank~\cite{wu2018unsupervised} to momentum encoder) and PIRL~\cite{misra2019self} (\ie, applying a second JigSaw branch). In this setup, we perform both unsupervised pre-training and linear evaluation on the same ImageNet-100 subset as above. We use the same contrastive loss objective function and training recipe for all methods, to ensure a head-to-head comparison. Specifically, we pre-train for 240 epochs with learning rate initialized as 0.03 and decayed with cosine annealing schedule.

Table~\ref{tab:compatibility} summarizes the results. We observe that combining CMC with the MoCo mechanism or JigSaw branch in PIRL can consistently improve the performance, verifying that they are compatible.








\section{Implementation Details}
\subsection{STL-10}\label{app:stl}
For a fair comparison with DIM~\citep{hjelm2018learning} and CPC~\citep{oord2018representation}, we adopt the same architecture as that used in DIM and split it into two encoders, each shown as in Table~\ref{tab:alexnet-stl-cmc}. For the implementation of the score function, we adopt similar ``encoder-and-dot-product'' strategy, which is tantamount to a bilinear model.  

\begin{table}[t]
\centering
\vspace{-.1in}
\begin{tabular}{c c c c c c }
\specialrule{.1em}{.1em}{.1em}
\multicolumn{6}{c}{\textbf{Half of AlexNet\citep{krizhevsky2012imagenet} for STL-10}} \\
\specialrule{.1em}{.1em}{.1em}
\specialrule{.1em}{.1em}{.1em}
\textbf{Layer} & \textbf{X} & \textbf{C} & \textbf{K} & \textbf{S} & \textbf{P} \\ \hline
\textbf{data} & 64 & * & -- & -- & -- \\
\textbf{conv1} & 64 & 48 & 3 & 1 & 1 \\
\textbf{pool1} & 31 & 48 & 3 & 2 & 0 \\
\textbf{conv2} & 31 & 96 & 3 & 1 & 1 \\
\textbf{pool2} & 15 & 96 & 3 & 2 & 0 \\
\textbf{conv3} & 15 & 192 & 3 & 1 & 1 \\
\textbf{conv4} & 15 & 192 & 3 & 1 & 1 \\
\textbf{conv5} & 15 & 96 & 3 & 1 & 1 \\
\textbf{pool5} & 7 & 96 & 3 & 2 & 0 \\
\textbf{fc6} & 1 & 2048 & 7 & 1 & 0 \\
\textbf{fc7} & 1 & 2048 & 1 & 1 & 0 \\
\textbf{fc8} & 1 & 64 & 1 & 1 & 0 \\
\specialrule{.1em}{.1em}{.1em}
\end{tabular}
\caption{\small{\textbf{The variant of AlexNet architecture used in our CMC for STL-10 (only half is present here due to splitting)}. \textbf{X} spatial resolution of layer, \textbf{C} number of channels in layer; \textbf{K} \texttt{conv} or \texttt{pool} kernel size; \textbf{S} computation stride; \textbf{P} padding; * channel size is dependent on the input source, e.g. 1 for L channel and 2 for ab channel.}}
\vspace{-.1in}
\label{tab:alexnet-stl-cmc}
\end{table}

In the patch-based contrastive learning stage, we use Adam optimizer with an initial learning rate of , , . We train for a total of  epochs with learning rate decayed by  after  and  epochs. In the non-linear classifier evaluation stage, we use the same optimizer setting. For the NCE-based contrastive learning stage, we train for 320 epochs with the learning rate initialized as  and further decayed by 10 for every  epochs after the first  epochs. The temperature  is set as . In general,  works reasonably well.

\subsection{ImageNet} \label{app:imagenet}
For patch-based contrastive loss, we use the same optimizer setting as in Sec.~\ref{app:stl} except that the learning rate is initialized as 0.01.

For NCE-basd contrastive loss in both full ImageNet and ImageNet100 experiments present in Sec.~\ref{sec:imgnet}, the encoder architecture used for either L or ab channels
is shown in Table~\ref{tab:alexnet-ImageNet-cmc}. In the unsupervised learning stage of AlexNet, we use SGD to train the network for a total of  epochs. The temperature  is set as  by following previous work~\citep{wu2018unsupervised}. The learning rate is initialized as  with a decay of 10 for every  epochs after the first  epochs. Weight decay is set as  and momentum is kept as . For the linear classification stage, we train for  epochs. The learning rate is initialized as  and decayed by  every  epochs after the first  epochs. We set weight decay as  and momentum as . 

For ResNets in CMC stage, instead of using step decay, we choose cosine annealing to gradually decrease the learning rate. In the linear evaluation stage, we train for  epochs. The learning rate is initialized as  for ResNet-50 and ResNet-101, and  for ResNet-50 x2. It is decayed by  every  epochs after the first  epochs. We set weight decay as  and momentum as .

\begin{table}[ht]
\centering
\vspace{-0pt}
\begin{tabular}{c c c c c c}
\specialrule{.1em}{.1em}{.1em}
\multicolumn{6}{c}{\textbf{Half of AlexNet\citep{krizhevsky2012imagenet} for ImageNet}} \\
\specialrule{.1em}{.1em}{.1em}
\specialrule{.1em}{.1em}{.1em}
\textbf{Layer} & \textbf{X} & \textbf{C} & \textbf{K} & \textbf{S} & \textbf{P} \\ \hline
\textbf{data} & 224 & * & -- & -- & -- \\
\textbf{conv1} & 55 & 48 & 11 & 4 & 2 \\
\textbf{pool1} & 27 & 48 & 3 & 2 & 0 \\
\textbf{conv2} & 27 & 128 & 5 & 1 & 2 \\
\textbf{pool2} & 13 & 128 & 3 & 2 & 0 \\
\textbf{conv3} & 13 & 192 & 3 & 1 & 1 \\
\textbf{conv4} & 13 & 192 & 3 & 1 & 1 \\
\textbf{conv5} & 13 & 128 & 3 & 1 & 1 \\
\textbf{pool5} & 6  & 128 & 3 & 2 & 0 \\
\textbf{fc6} & 1 & 2048 & 6 & 1 & 0 \\
\textbf{fc7} & 1 & 2048 & 1 & 1 & 0 \\
\textbf{fc8} & 1 & 128 & 1 & 1 & 0 \\
\specialrule{.1em}{.1em}{.1em}
\end{tabular}
\caption{\small{\textbf{AlexNet architecture used in CMC for ImageNet (only half is present here due to splitting)}. \textbf{X} spatial resolution of layer, \textbf{C} number of channels in layer; \textbf{K} \texttt{conv} or \texttt{pool} kernel size; \textbf{S} computation stride; \textbf{P} padding; * channel size is dependent on the input source, e.g. 1 for L channel and 2 for ab channel.}}
\label{tab:alexnet-ImageNet-cmc}
\end{table}

\subsection{UCF101 and HMDB51}

Following previous work~\citep{misra2016shuffle,lee2017unsupervised,sayed2018cross,buchler2018improving},
we use CaffeNet for the video experiments. We tailor the network and use features from the fc6 layer for contrastive learning. Dropout of  is used to alleviate overfitting.

\subsection{NYU Depth-V2}

While experimenting with different views on NYU Depth-V2 dataset, we encode the features from patches with a size of . The detailed architecture is shown in Table~\ref{tab:nyudepth-cmc}. In the unsupervised training stage, we use Adam optimizer with an initial learning rate of , , . We train for a total of  epochs with learning rate decayed by  after , , and  epochs. For the downstream semantic segmentation task, we use the same optimizer setting but train for fewer epochs. We only train  epochs for CMC pre-trained models, and train  epochs for the \emph{Random} and \emph{Supervised} baselines until convergence. For the classification task evaluated on STL-10, we use the same optimizer setting as in Sec.~\ref{app:stl} to report numbers.


\begin{table}[ht]
\centering
\begin{tabular}{c c c c c c}
\specialrule{.1em}{.1em}{.1em}
\multicolumn{6}{c}{\textbf{Encoder Architecture on NYU}} \\
\specialrule{.1em}{.1em}{.1em}
\specialrule{.1em}{.1em}{.1em}
\textbf{Layer} & \textbf{X} & \textbf{C} & \textbf{K} & \textbf{S} & \textbf{P} \\ \hline
\textbf{data} & 128 & * & -- & -- & -- \\
\textbf{conv1} & 64 & 64 & 8 & 2 & 3 \\
\textbf{pool1} & 32 & 64 & 2 & 2 & 0 \\
\textbf{conv2} & 16 & 128 & 4 & 2 & 1 \\
\textbf{conv3} & 8 & 256 & 4 & 2 & 1 \\
\textbf{conv4} & 8 & 256 & 3 & 1 & 1 \\
\textbf{conv5} & 4 & 512 & 4 & 2 & 1 \\
\textbf{fc6} & 1 & 512 & 4 & 1 & 0 \\
\textbf{fc7} & 1 & 256 & 1 & 1 & 0 \\
\specialrule{.1em}{.1em}{.1em}
\end{tabular}
\caption{\small{\textbf{Encoder architecture used in our CMC for playing with different views on NYU Depth-V2}. \textbf{X} spatial resolution of layer, \textbf{C} number of channels in layer; \textbf{K} \texttt{conv} or \texttt{pool} kernel size; \textbf{S} computation stride; \textbf{P} padding; * channel size is dependent on the input source, e.g. 1 for L, 2 for ab, 1 for depth, 3 for surface normal, and 1 for segmentation map.}}
\vspace{-.1in}
\label{tab:nyudepth-cmc}
\end{table}

\section{Change Log}
{\bf {arXiv v2}} Added references to Time Contrastive Networks \citep{sermanet2017time} and Local Aggregation \citep{zhuang2019local}. Fixed Typos.

{\bf{arXiv v3}} Added analysis of effect of mutual information, updated results, and rearranged the contents. 

{\bf{arXiv v4}} Added reference to the original k-pair loss~\cite{sohn2016improved} and other related work. Cited some recent works to reflect progresses after our v1 version. We also removed Fast AutoAugment and rearranged the contents. 

{\bf{arXiv v5}} Added the list of ImageNet-100 proposed in this paper, and the compatibility of CMC with other methods.  
\end{document}
