\documentclass[11pt]{article}


\usepackage{amssymb,amsmath,amsthm,sectsty,url}
\usepackage[letterpaper,hmargin=1.0in,vmargin=1.0in]{geometry}
\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{color}


\usepackage[pdftex,colorlinks,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue]{hyperref}
\usepackage{cleveref}

\usepackage[margin=20pt,font=small,labelfont=bf]{caption}


\pagestyle{plain}



\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\lVert#1\rVert}


\def\E{\mathop{\mathbb E}}
\newcommand{\Exp}{\mathop{\mathrm E}\displaylimits} \newcommand{\Var}{{\bf Var}}
\newcommand{\Cov}{{\bf Cov}}



\newcommand{\U}{\mathbf U}
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\D}{\mathbb D}
\newcommand{\F}{\mathbb F}
\newcommand{\GF}{\mathbb{GF}}
\newcommand{\B}{\{ 0,1 \}}
\newcommand{\BM}{\{ -1,1 \}}

\newcommand{\FA}{\mathcal{A}}
\newcommand{\FB}{\mathcal{B}}
\newcommand{\FC}{\mathcal{C}}
\newcommand{\FD}{\mathcal{D}}
\newcommand{\FP}{\mathcal{P}}
\newcommand{\FQ}{\mathcal{Q}}
\newcommand{\FS}{\mathcal{S}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\FH}{\mathcal{H}}
\newcommand{\FG}{\mathcal{G}}

\newcommand{\BAD}{\text{BAD}}
\newcommand{\leps}{\log{\frac{1}{\varepsilon}}}
\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\parensqr}[1]{\left[#1\right]}
\newcommand{\Prp}[1]{\Pr\left[#1\right]}
\newcommand{\Ep}[1]{\E\left[#1\right]}
\newcommand{\maxparen}[1]{\max{\left\{#1\right\}}}
\newcommand{\logchoose}[2]{\log{ {#1 \choose #2}}}
\newcommand{\logp}[1]{\log{\paren{#1}}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{question}[theorem]{Question}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{example}[theorem]{Example}

\newtheorem*{thm:LowerBound}{\Cref{LowerBoundNumber} (Restated)}

\begin{document}

\title{Tight Bounds for Sliding Bloom Filters\footnote{Research supported in part by a grant from the I-CORE Program of the Planning and Budgeting Committee, the Israel Science Foundation and the Citi Foundation.}}
\author{
    Moni Naor\footnotemark[\value{footnote}]\thanks{Incumbent of the Judith
        Kleeman Professorial Chair. Research supported in part by a grant from the
        Israel Science Foundation. Department of Computer Science and
                Applied Mathematics, Weizmann Institute of Science, Rehovot 76100,
                Israel. Email: \texttt{moni.naor@weizmann.ac.il}.}
  \and
    Eylon Yogev\thanks{Department of Computer Science and
        Applied Mathematics, Weizmann Institute of Science, Rehovot 76100,
        Israel. Email: \texttt{eylon.yogev@weizmann.ac.il}.}
}


\pagestyle{plain}

\maketitle

\begin{abstract}
A Bloom filter is a method for reducing the space (memory) required for representing a set by allowing a small error probability. In this paper we consider a Sliding Bloom Filter: a data structure that, given a stream of elements, supports membership queries of the set of the last  elements (a sliding window), while allowing a small error probability and a slackness parameter. 

The problem of sliding Bloom filters has appeared in the literature in several communities, but this work is the first theoretical investigation of it. 

We formally define the data structure and its relevant parameters and analyze the time and memory requirements needed to achieve them. We give a low space construction that runs in  time per update with high probability (that is, for all sequences with high probability all operations take constant time) and provide an almost matching lower bound on the space that shows that our construction has the best possible space consumption up to an additive lower order term.
\end{abstract}

\section{Introduction}
Given a stream of elements, we consider the task of determining whether an element has appeared in the last  elements of the stream. To accomplish this task, one must maintain a representation of the last  elements at each step. One issue, is that the memory required to represent them might be too large and hence an approximation is used. We formally define this approximation and completely characterize the space and time complexity needed for the task.

In 1970 Bloom~\cite{Bloom70} suggested an efficient data structure, known as the `\emph{Bloom filter}', for reducing the space required for representing a set  by allowing a small error probability on membership queries. The problem is also known as the approximate membership problem (however, we refer to any solution simply as a `Bloom filter'). A solution is allowed an error probability of  for elements not in  (false positives), but no errors for members of . In this paper, we consider the task of efficiently maintaining a Bloom filter of the last  elements (called `the sliding window') of a stream of elements.

We define an -\emph{Sliding Bloom Filter} as the task of maintaining a Bloom filter over the last  elements. The answer on these elements must always be `Yes', the  elements that appear prior to them have no restrictions ( is a slackness parameter) and for any other element the answers must be `Yes' with probability at most . In case  is infinite, all elements prior to the current window have no restrictions. In this case we write for short -Sliding Bloom Filter.

The problem was studied in several communities and various solutions were suggested. In this paper, we focus on a theoretical analysis of the problem and provide a rigorous analysis of the space and time needed for solving the task. We construct a Sliding Bloom Filter with  query and update time, where the running time is worst case with high probability (see the theorems in \Cref{sec:our} for precise definitions) and has near optimal space consumption. We prove a matching space lower bound that is tight with our construction up to an additive lower order term. Roughly speaking, our main result is figuring out the first two terms of the space required by a Sliding Bloom Filter: 

A simple solution to the task is to partition the window into blocks of size  and for each block maintain its own Bloom filter. This results in maintaining  Bloom filters. To determine if an element appeared or not we query all the Bloom filters and answer `Yes' if any of them answered positively. There are immediate drawbacks of this solution, even assuming the Bloom filters are optimal in space and time:
\begin{itemize}
\item Slow query time:  Bloom filter lookups.
\item High error probability: since an error can occur on each block, to achieve an effective error probability of  we need to set each Bloom filter to have error , which means that the total space used has to grow (relative to a simple Bloom filter) by roughly  bits (see \Cref{related}).
\item Sub-optimal space consumption for large : the first two drawbacks are acute for small , but when  is large, say , then each block is large which results in a large portion of the memory being `wasted' on old elements.
\end{itemize}
We overcome all of the above drawbacks: the query time is always constant and for \emph{any}  the space consumption is nearly optimal.

Sliding Bloom Filters can be used in a wide range of applications and we discuss two settings where they are applicable and have been suggested. In one setting, Bloom filters are used to quickly determine whether an element is in a local web cache \cite{FanCAB00}, instead of querying the cache which may be slow. Since the cache has limited size, it usually stores the least recently used items (LRU policy). A Sliding Bloom Filter is used to represent the last  elements used and thus, maintain a representation of the cache's contents at any point in time.

Another setting consists of the task of identifying duplicates in streams. In many cases, we consider the stream to be unbounded, which makes it impractical to store the entire data set and answer queries precisely and quickly. Instead, it may suffice to find duplicates over a sliding window while allowing some errors. In this case, a Sliding Bloom Filter (with  set to infinity) suffices and in fact, we completely characterize the space complexity needed for this problem.

\subsection{Problem Definition}
Given a stream of elements  from a finite universe  of size , parameters ,  and , such that , we want to approximately represent a sliding window of the  most recent elements of the stream. An algorithm  is given the elements of the stream one by one, and does not have access to previous elements that were not stored explicitly. Let  be the first  elements of the stream  and let  be the last  elements of the stream . At any step  the current window is  and the  elements before them are . If  then define . Denote  the result of the algorithm on input  given the stream . We call  an -\emph{Sliding Bloom Filter} if for any  the following two conditions hold:
\begin{enumerate}
\item For any : 
\item For any 
\end{enumerate}
where the probability is taken over the internal randomness of the algorithm . Notice that for an element  the algorithm may answer arbitrarily (no restrictions). See Figure 1.


\begin{figure}[ht!]
\centering
\includegraphics[scale=0.35]{Figure1.pdf}
\caption{The sliding window of the last  and  elements}
\label{overflow}
\end{figure}

An algorithm  for solving the problem is measured by its memory consumption, the time it takes to process each element and answer a query. We denote by  the maximum number of bits used by  at any step. The model we consider is the unit cost RAM model in which the elements are taken from a universe of size , and each element can be stored in a single word of length  bits. Any operation in the standard instruction set can be executed in constant time on -bit operands. This includes addition, subtraction, bitwise Boolean operations, left and right bit shifts by an arbitrarily number of positions, and multiplication. The unit cost RAM model is considered the standard model for the analysis of the efficiency of data structures.

An element not in  on which the data structure accepts is called a false positive. At any point in time, the fraction of false positives in  is called the false positive rate.

\subsection{Our Contributions}
\label{sec:our}
We provide tight upper and lower bounds to the -problem. In fact, we achieve space optimality up to the second term. Our first contribution is a construction of an efficient Sliding Bloom Filter: it has query time  worst case and update time  worst case with high probability, for the entire sequence. For  the space consumption is near optimal: the two leading terms are optimal in constants.

\begin{theorem}\label{UpperBoundNumber}
For any , and sufficiently large  there exists an -Sliding Bloom Filter having the following space and time complexity on a unit cost RAM:

\begin{description}
\item \textbf{Time}: Query time is  worst case. For any polynomial  and sequence of at most  operations, with probability at least , over the internal randomness of the data structure, all insertions are performed in time  worst case.
\item
\textbf{Space}: the space consumption is: .\\
In particular, for constant error  we get that the space consumption is: . Otherwise, for sub-constant  that satisfies  we get that:
\begin{enumerate}
\item If   then the space consumption is: 
\item If  then the space consumption is: 
\end{enumerate}
\end{description}

\end{theorem}

The challenge we face is achieving constant time operations while space consumption remains very tight. In designing our algorithm we assemble ideas from several previous works along with new ones. The basic skeleton of the algorithm shares ideas with the work of Zhang and Guan~\cite{ZhangG08}, however, their algorithm is based on the traditional Bloom filter and has immediate drawbacks: running time is super-constant and the space is far from optimal. To get an error probability of  they use  bits, and moreover this is assuming the availability of truly random hash functions.

Thorup~\cite{Thorup11} considered a similar data structure of hash tables with timeouts based on linear probing. He did not allow any error probability nor any slackness (i.e.  and  in our terminology). The query time, as is general for linear probing, is only constant in expectation, and the space is only optimal within a constant factor.

Pagh, Pagh and Rao~\cite{PaghPR05} showed that the traditional construction of a Bloom filter can be replaced with a construction that is based on dictionaries. The dictionary based Bloom filter has the advantage that its running time and space consumption are completely determined by the dictionary itself, and it does not assume availability of truly random functions. Given the developments in succinct dictionaries, using this alternative has become more appealing.

Our algorithm is conceptually similar to the work of Zhang and Guan. However, we replace the traditional implementation of the Bloom filter with a dictionary based one. As the underlying dictionary, we use the state of the art dictionary given by Arbitman, Naor and Segev~\cite{ArbitmanNS10}, known as Backyard Cuckoo Hashing. Then we apply a similar method of lazy deletions as used by Thorup on the Backyard Cuckoo Hashing dictionary. Moreover, we introduce a slackness parameter  and instead of storing the exact index of each element we show a trade-off parameter  between the accuracy of the index stored and the number of elements we store in the dictionary. Optimizing  along with the combined methods described gives us the desired result: constant running time, space consumption of nearly  which is optimal in both leading constants and no assumption on the availability of truly random functions. We inherit the implementation complexity of the dictionary, and given an implementation of one, it is relatively simple to complete the algorithm's implementation.

Our second contribution, and technically the more involved one, is a matching space lower bound. We prove that if  then any Sliding Bloom Filter must use space that is within an additive low order term of the space of our construction, regardless of its running time.

\begin{theorem}\label{LowerBoundNumber}
Let  be an -Sliding Bloom Filter where , then
\begin{enumerate}
\item If  then 
\item If  then 
\end{enumerate}
\end{theorem}

From \Cref{UpperBoundNumber,LowerBoundNumber} we conclude that making  larger than  does not make sense: one gets the same result for any value in .
When  is small (less than ), then the dominant expression in both the upper and lower bounds is .

The lower bound is proved by an encoding argument which is a common way of showing lower bounds in this area (see for example \cite{PaghSW12}). Specifically, the idea of the proof is to use  to encode a set  and a permutation  on the set corresponding to the order of the elements in the set. We consider the number of steps from the point an element is inserted to  to the first point where  answers `No' on it, and we define  to be the sum of  such lengths. If  is large, then there is a point where  represents a large portion of , which benefits in the encoding of . If  is small, then  can be used as an approximation of , thus encoding  precisely requires a small amount of bits. In either case, the encoding must be larger than the entropy lower bound\footnote{The entropy lower bound is base 2 logarithm of the size of the set of all possible inputs. In our case, all possible pairs .} which yields a bound on the size of . The optimal value of the trade-off between representing a larger set or representing a more accurate ordering is achieved by our construction. In this sense, our upper bound and lower bound match not only by `value' but also by `structure'.

\subsection{Related Work and Background}\label{related}
The data structure for the approximate set membership as suggested by Bloom in 1970 \cite{Bloom70} is relatively simple: it consists of a bit array which is initiated to `0' and  random hash functions. Each element is mapped to  locations in the bit array using the hash functions. To insert an element set all  locations to 1. On lookup return `Yes' if all  locations are 1. To achieve an error probability of  for a set of size  Bloom showed that if  then the length of the bit array should be roughly  (where the 1.44 is an approximation of ). Since its introduction Bloom filters have been investigated extensively and many variants, implementations and applications have been suggested. We call any data structure that implements the approximate set membership a `Bloom filter'. A comprehensive survey (for its time) is Broder and Mitzenmacher~\cite{BroderM03}.

A lot of attention was devoted for determining the exact space and time requirements of the approximate set membership problem. Carter et al.~\cite{CarterFGMW78} proved an entropy lower bound of , when the universe  is large. They also provided a reduction from approximate membership to {\em exact} membership, which we use in our construction. The retrieval problem associates additional data with each element of the set. In the static setting, where the elements are fixed and given in advance, Dietzfelbinger and Pagh propose a reduction from the retrieval problem to approximate membership \cite{DietzfelbingerP08}. Their construction gets arbitrarily close to the entropy lower bound.

In the dynamic case, Lovett and Porat~\cite{LovettP10} proved that the entropy lower bound cannot be achieved for any {\em constant} error rate. They show a lower bound of  where  depends only on . Pagh, Segev and Wieder~\cite{PaghSW12} showed that if the size  is not known in advance then at least  bits of space must be used. The Sliding Bloom Filter is in particular also a Bloom Filter in a dynamic setting, thus the \cite{LovettP10} and~\cite{PaghSW12} bounds are applicable.

As discussed, Pagh, Pagh and Rao \cite{PaghPR05} suggested an alternative construction for the Bloom filter. They used the reduction of Carter et al.\ to improve the traditional Bloom filter in several ways: Lookup time becomes  independent of , has succinct space consumption, uses explicit hash functions and supports deletion. In the dynamic setting for a constant  we do not know what is the leading term in the memory needed, however, for any sub-constant  we know that the leading term is : Arbitman, Naor and Segev present a solution, called `Backyard Cuckoo Hashing', which is optimal up to an additive lower order term (i.e., it is a succinct representation) \cite{ArbitmanNS10}. Thus, in this paper we focus on sub-constant .

The model of sliding windows was first introduced by Datar et al.~\cite{DatarGIM02}. They consider maintaining an approximation of a statistic over a sliding window. They provide an efficient algorithm along with a matching lower bound.

Data structures for problems similar to the Sliding Bloom Filters have been studied in the literature quite extensively over the past years. The simple solution using  consists of two large Bloom filters which are used alternatively. This method known as \emph{double buffering} was proposed for classifying packets caches \cite{ChangLF04}. Yoon~\cite{Yoon10} improved this method by using the two buffers simultaneously to increase the capacity of the data structure. Deng and Rafiei~\cite{DengR06} introduced the Stable Bloom filter and used it to approximately detect duplicates in stream. Instead of a bit array they use an array of counters and to insert an element they set all associated counters to the maximal value. At each step, they randomly choose counters to decrease and hence older element have higher probability of being decreased and eventually evicted over time. Metwally et al.~\cite{MetwallyAA05} showed how to use Bloom filters to identify duplicates in click streams. They considered three models: Sliding Windows, Landmark Windows and Jumping Windows and discuss their relations. A comprehensive survey including many variations is given by Tarkoma et al.~\cite{TarkomaRL12}. However, as far as we can tell, no formal definition of a Sliding Bloom Filter as well as a rigorous analysis of its space and time complexity, appeared before.

\section{The Construction of a Succinct Sliding Bloom Filter}\label{sec:contruction}

Our algorithm uses a combination of transforming the approximate membership problem to the exact membership problem plus a solution to the retrieval problem. On an input , we store , for some hash function , in a dynamic dictionary and in addition store some information on the last time where  appeared. We consider the stream to be divided into generations of size  each, where  is a parameter that will be optimized later. The first  elements are generation 1, the next  elements are generation 2 etc. The current window contains the last  elements and consists of at most  different generations. Therefore, at each step, we maintain a set  that represents the last  generations (that is, at most  elements) and count the generations mod . In addition to storing , we associate  bits indicating the generation of . Every  steps, we delete elements associated with the oldest generation. We adjust  to optimize the space consumption while requiring .

In this section, we describe the algorithm in more detail. We first present the transformation from approximate to exact membership (Section 2.1). We define a dynamic dictionary and the properties we need from it in order to implement our algorithm (Section 2.3). Then, we describe the algorithm in two stages, using any dictionary as a black box. The memory consumption is merely the memory of the dictionary and therefore we use one with succinct representation. At first, in Section 2.3, the running time will not be optimal and depend on  (which is not a constant), even if we use optimal dictionaries. Then, in Section 2.4, we describe how to eliminate the dependency on  as well as deamortizing the algorithm, making the running time constant for each operation. This includes augmenting the dictionary, and thus it can no longer be treated as a black box. We prove correctness and analyze the resulting memory consumption and running time.

\subsection{Approximate Membership and Exact Membership}\label{sec:approxToExact}
Carter et al.~\cite{CarterFGMW78} showed a transformation from approximate membership to exact membership that works as follows. We want to represent a set  of size  and support membership queries in the following manner: For a query on  we answer `Yes' and for  we answer `Yes' with probability at most . Choose a hash function  from a universal family of hash functions mapping . Then for any  of size at most  it holds that for any :

where the first inequality comes from a union bound and the second from the definition of a universal hash family. This implies that storing  suffices for solving the approximate membership problem. This dictionary-based construction and the traditional construction can be viewed as lying on a spectrum - the former writes many bits in one location, whereas the latter writes one bit in many locations.

To store  we use an exact dictionary , which supports \texttt{insert} (including associated data), \texttt{delete} and \texttt{update} procedures (the update procedure can be simulated by a delete followed by an insert). While most dictionaries support these basic procedures, we require  to additionally support the ability of \emph{scanning}. We further discuss these properties in the next section.\\

\noindent
\textbf{Number of false positives}:
We note that in addition to the error bound on each element, we can bound the total number of false positives in the universe. Any hash family  divides the universe to  `bins', and the number of false positives is the total number of elements in any bin containing an element from . If  divides  to (roughly) equally sized bins, each of size at most , then the total number of false positives is at most . A simple example of such a hash family can be obtained by choosing a prime  then defining  to be , where  is a random integer modulo  with  \cite{CarterW79}. In this case, the bound holds with certainty for \emph{any} function . This property is not guaranteed by the traditional construction of Bloom, and we further discuss it in \Cref{sec:lowebound}.

\subsection{Succinct Dynamic Dictionary}
The information-theoretic lower bound on the minimum number of bits needed to represent a set  of size  out of  different elements is . A succinct representation is one that uses  bits \cite{Dem07}. A significant amount of work was devoted for constructing dynamic dictionaries over the years and most of them are appropriate for our construction. Some have good theoretical results and some emphasize the actual implementation. In order for the reduction to compete with the Bloom filter construction (in terms of memory consumption) we must use a dynamic dictionary with succinct representation. There are several different definitions in the literature for a \emph{dynamic} dictionary. A static dictionary is a data structure storing a finite subset of a universe , supporting only the \texttt{member} operation. In this paper, we refer to a dynamic dictionary where only an upper bound  on the size of  is given in advance and it supports the procedures \texttt{member}, \texttt{insert} and \texttt{delete}. The memory of the dictionary is measured with respect to the bound .

In addition to storing , we assume  supports associating data with each element. Specifically, we want to store -bits of data with each element, where  is fixed and known in advance. Finally, we assume the dictionary supports \emph{scanning}, that is, the ability to go over the associated data of all elements of the dictionary, and delete the element if needed. Using the scanning process, we scan the generations stored in the dictionary and delete elements of specific generations.

Several dynamic dictionaries can be used in our construction of a Sliding Bloom Filter. The running time and space consumption are directly inherited from the dictionary, making it an important choice. We use the `Backyard Cuckoo Hashing' construction of \cite{ArbitmanNS10} (but other alternative are possible). It supports \texttt{insert} and \texttt{delete} in  worst case with high probability while having a succinct representation. Implicitly in their work, they support associating any fixed number of bits and scanning. When -bits of data are associated with each , the representation lower bound becomes  bits. For concreteness, the memory consumption of their dictionary is , where the  hides the expression .

\subsection{An Algorithm with Dependency on }\label{Algorithm1}
Initiate a dynamic dictionary  of size  as described above. Let  be a family of universal hash functions and pick  at random. At each step maintain a counter  indicating the current generation and a counter  indicating the current element in the generation. At every step  is increased and every  steps  is reset back to 0 and  is increased mod .

To insert an element  check if  exists in . If not then insert  (insert  associated with ) into . If  is in , then update the associated data of  to . Finally, update the counters  and . If  has increased (which happens every  steps) then \emph{scan}  and delete all elements with associated data equal to the new value of .

To query the data structure on an element , return whether  is in . See Algorithm 1 for pseudo-code of the insert and lookup procedures.

\begin{algorithm}
\caption {Pseudo-code of the Insert and Lookup procedures}
\texttt{Insert():}
\begin{algorithmic}[1]

\If { is a member of }
	\State update  to have data 
\Else
	\State insert  into 
\EndIf
\State maintain counters  and 
\If {the value of  has changed}
	\State scan  and delete elements of generation 
\EndIf
\newline
\end{algorithmic}

\texttt{Lookup():}
\begin{algorithmic}[1]
\Procedure {member}{}
\If { is a member of }
	\State return `Yes'
\Else
	\State return `No'
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\noindent
\textbf{Correctness}:
We first notice that  is used correctly and never represents a set of size larger than . In each step we either insert an element to generation  or move an existing element to generation . In any case, each generation consists of at most  elements in . Each  we evict a whole generation, assuring no more than  generations are present in the dictionary at once. Thus, at most  elements are represented at any given step.

Next we prove that for any time  the three conditions in \Cref{UpperBoundNumber} hold. The first condition follows directly from the algorithm. Assume  is inserted with associated generation . Notice that its associated generation can only increase.  will be deleted only when  completes a full cycle and its value is  again, which takes at least  steps. Thus, for any ,  is in  and the algorithm will always answer `Yes'.

For the second condition assume that  and notice that  is at least  generations. Assume w.l.o.g.\ that  ( could have less than  elements) is the set of elements represented in  at time . Then  for all . Therefore, using a union bound we get that the total false positive probability is



\noindent
\textbf{Memory consumption:}
The bulk of memory is used for storing . In addition, we need to store two counters  and  and the hash function , which together take  bits.  stores  elements out of  while associating each with  bits. Using the `Backyard Cuckoo Hashing' dictionary yields a total space of

We minimize this expression, as a function of , and get that the minimum is at the solution to . An approximate solution is  and requiring that  yields that  and the total space is

as required. As mentioned, the  hides the term , therefore if  then the product of  with  is . If  then the product of  with  is  as well. Thus, we can write the space consumption as:

Otherwise, if  then we can write it as:

If  then  and  and we can write it as:

\noindent
\textbf{Running time:}
Assume that  supports  running time worst case for all procedures. The lookup procedure performs a single query to  and hence always runs in . In the insert procedure, every  steps, the value of  is updated and we scan all elements in  deleting old elements. For any other step, the running time is . Therefore, the total running time for  steps is , which is  amortized running time. If  then  and the running time is , otherwise it is , which in both cases is not constant. We now show how to eliminate the large step, making the running time  worst case. Using the `Backyard Cuckoo Hashing' dictionary we get that the total running time including the dictionary's operations is  worst case with high probability (over internal randomness of the dictionary).

\subsection{Reducing the Running Time to Constant}


The main load of the algorithm of \Cref{Algorithm1} stems from the need to scan the entire dictionary to delete old elements. The issue we have to deal with in order to reduce the time of each step to be constant (independent of ) is that the scanning operation is done too many times: each  steps we scan the dictionary which is a total of  operations over  steps. Another drawback of that algorithm is that the scanning process is all done in one step, hence we can get only an amortized result. We modify the algorithm to solve these two issues simultaneously: only one scanning is performed every  steps and the scanning processes is deamortized and spread over these  steps.

The first modification is to extend the range of the generations counter  to loop between  and  (instead of between 0 and ). This lets us distinguish between elements of the last  generation and enables a window of size  to delete old elements before the counter overrides them with a new generation. At any moment, only the  recent generations are considered active and the rest slated to be deleted.

The second modification is to combine many scanning processes to one, which is spread over a sequence of  steps. The scanning process needs to support running in small steps while allowing other operations to run concurrently. We should be able to save its state, then allow other operations to run and finally resume its state and continue the scanning process. Instead of scanning all the  elements in one step, we scan two elements at each step and save the scanning index such that we are able to continue from that point. Thus, after  steps all  elements of the dictionary are scanned.

These modifications raise two new problems with which we need to deal. First, the dictionary is initialized to be of size  and since we do not delete old elements immediately, there might be more than  elements present in the dictionary. Notice, however, that the number of \emph{active} elements present will never exceed . Second, since the scanning is done in small steps concurrently with other operations, it might miss elements the have been moved by other operations. In any case the dictionary has been modified, the scanning process should succeed in scanning all elements nevertheless.

To solve this, we need the dictionary to be able to consider non-active elements as deleted such that they do not interfere with other operations: whenever a non-active element is encountered it is simply deleted. Supporting this requires some additional properties from the dictionary. Later, for concreteness, we describe how to modify the `Backyard Cuckoo Hashing' dictionary to support these properties.

It is not clear whether all dictionaries can be modified to support this property, since the dictionary might have some implicit representation of various elements using the same memory space. However, the property can be supported assuming each element has a unique memory space in which it is  represented, called a `cell'; we do not assume that the dictionary is `systematic', i.e. that the string encodes the element directly, but rather that as in `traditional' hash tables the content of the cell plus its location and some other easily accessible information determine the element uniquely. We assume that given a cell, we can figure out the associated data with the element of the cell and delete the element of this cell from the dictionary. An insert or delete procedure may modify a constant number of cells. Elements of cells which were accessed are called the accessed elements. We assume the cells have some order in which we can scan them and save an index indicating the state of the scanning process using  bits of memory (actually it is ).

Assuming the dictionary supports these properties, we can modify its lookup and insert procedures to check whether any accessed element needs to be deleted. For example, an insert procedure may move an element from one cell to another, which was already scanned. Thus, before moving or changing a cell we scan it and delete it if it's old. This way, each element is scanned either by the scanning process or by an insert or lookup procedure. Moreover, we change the Lookup procedure to return `Yes' on input  only if  exists in  \textbf{and} its associated generation is active.

A cell occupied by an old element, will be deleted whenever accessed, thus effectively not occupying space in dictionary. Since elements might be deleted only after  steps, it could be the case that more than  elements are present in the dictionary. However, this way, the old elements do not interfere: when an old item is encountered during the insertion it is deleted, as described above. Hence, effectively when an item is inserted the data structure has at most  elements and it will have a valid place.

We discuss implementing these requirements in the `Backyard Cuckoo Hashing' construction (see pseudo-code in Figure 2 of their paper). Their hashing scheme is based on two-level hashing, the first level consists of an array  of bins of size  and the second level consists of Cuckoo hashing which includes two arrays,  and  and a queue, . The cells are the  cells in each bin of , the cells of ,  and . Each element is implicitly stored in a unique cell in one of the components.

Scanning the cells is achieved by going over the cells of each component and saving an index of the current component and cell within the component. The lookup and delete procedures are simple and does not involve moving cells. The insert procedure is more involved and may move cells from one component to another, e.g. a cell from  might be moved to . Since the running time is constant, so is the number of accessed elements. The procedure can be easily modified such that before \emph{any} cell is accessed it is first scanned, and deleted if old. If there are less than  \emph{active} elements in the dictionary, then an insert operation will succeed, removing old elements if required. After these modifications, the `Backyard Cuckoo Hashing' dictionary supports all needed requirements for our construction of an -Sliding Bloom Filter.

We analyze the running time of the modified construction. At each step, we scan two elements and delete them if necessary. The delete operation always takes constant time. The insert procedure was modified to delete old accessed elements when encountered. Since the insert operation takes constant time in worst case with high probability, then with the same probability, it will access only a constant number of cells. Hence, deleting accessed elements will increase the running time, but it will remain a constant. Similarly, the modified lookup procedure also remains constant. Overall, all operations remain constant in the worst case, where the insert operation has constant running time, with high probability. This completes the proof of \Cref{UpperBoundNumber}.


\section{A Tight Space Lower Bound}\label{sec:lowebound}
In this section we present a matching space lower bound to our construction. For simplicity, we first introduce what we call the `absolute false positive assumption'. We define it and use it in the proof of \Cref{sec:lower}, and in \Cref{RemoveAssumption} we show how to get the same lower bound without it.

Recall that at any point in time, the false positive rate is the fraction of false positive elements. According to the definition of a Sliding Bloom Filter, we are not assured that there are no `bad' points in time where the false positive rate is much higher than its expectation, and in fact it could get as high as .

We call the property that throughout the lifetime of the data structure at {\em all} points in time the false positive rate is at most  the \emph{absolute false positive assumption}. This assumption is a desirable property from a Sliding Bloom Filter and reasonable constructions, including ours\footnote{See discussion at \Cref{sec:approxToExact}} in \Cref{sec:contruction}, enjoy it.

An (artificial) example of a Sliding Bloom Filter for which the assumption does not hold can be obtained by taking any -Sliding Bloom Filter and modifying it such that it chooses a random index  and at step  of the stream it always answers `Yes'. This results in an -Sliding Bloom Filter in which there will \emph{always} be some point at which the false positive rate is high.

\subsection{Proof Under the Absolute False Positive Assumption}\label{sec:lower}


\begin{theorem}\label{LoweBoundAssumptionNumber}
Let  be an -Sliding Bloom Filter where . If for any stream  it holds that

then
\begin{enumerate}
\item If  then 
\item If  then 
\end{enumerate}
\end{theorem}

\begin{proof}
Let  be an algorithm satisfying the requirements in the statement of the theorem. The main idea of the proof is to use  to encode and decode a set  and a permutation  on the set (i.e. an ordered set). Giving  to  as a stream, ordered by , creates an encoding of an approximation of  and :  is approximated by the set of all the elements for which  answers `Yes' (denoted by ), and  is approximated by the number of elements needed to be added to the stream in order for  to "release" each of the elements in  (that is, to answer `No' on it). Then, to get an exact encoding, we encode only the elements of  from within the set . To get an exact encoding of  we encode only the difference between the location  of each element and the actual location it has been released. The key is to find the point where  best approximates  and  \emph{simultaneously}.

Denote by  the algorithm with fixed random string  and let . We show that w.l.o.g.\ we can consider  to be deterministic. Let  be the set of all sequences of  \emph{distinct} elements, and let  be the subset of inputs such that  for all . Since we assumed that for any  we have that  then there must exist an  such that . Thus, we can assume that  is deterministic and encode only sequences from . Then the encoding lower bound changes from  to . This loss of 1 bit is captured by the lower order term  in the lower bound, and hence can be ignored.

Notice that  need not be explicitly specified in the encoding since the decoder can compute it using the description of the algorithm  (which may be part of its fixed program). From now on, we assume that  is deterministic (and remove the  notation) and assume that for any  we have that .

We now make an important definition:

 is the minimum number of elements needed to be added to  such that  answers `No' on . Notice that  can be computed for any set  given the representation of .

We encode any set  of size  and a permutation  using . After encoding  we compare the encoding length to the entropy lower bound of . Consider applying  on (some canonical order of) the elements of  and let  be the resulting elements of  ordered by . For any  let , then for any  define the sequence . Let  and define 
Notice that, given , ,  and  one can compute the position  of the element . Define



If  (or ) then , otherwise 
\begin{lemma}\label{foranyk}
Let  then

\end{lemma}
\begin{proof}
Instead of summing over , we sum over  and count the number of  such that . For  we know that  and by the definition of  we get that . For  we know that . Therefore:


\end{proof}

By averaging, we get that for any  there exist some  such that . Let  be such that , then we know that there exist some  such that . Note that  satisfies  which is in the range of indices of the false positive assumption.

We include the memory representation of  in the encoding. The decoder uses this to compute the set , which by the absolute false positive definition we know that . Since , we need only  bits to encode  elements of  out of them. The remaining  elements are encoded explicitly using  bits. This completes the encoding of .

To encode  we need the decoder to be able to extract  for each . For any  the decoder uses  and computes . Now, in order for the decoder to exactly decode  we need to encode all the 's. Since  we can encode all the 's using  bits (balls and sticks method), and the remaining elements' positions will be explicitly encoded using  bits. Denote by  the number of bits used by the algorithm . Comparing the encoding length to the entropy lower bound we get


and therefore

Consider two possible cases for . If  then we get

The minimum of this expression, as a function of , is achieved at . If  then the minimum can be achieved and we get that

Otherwise, if  then  and minimum value will be achieved at  which yields the required lower bound:

If  then . Thus, we get that


the minimum of this expression, as a function of  between the given range is achieved at  which yields

as required.
\end{proof}

In the proof, we encoded a sequence of length  and we assumed that the false positive assumption holds for any such sequence. However, the only property used was the number of bits required for encoding any possible sequence. Since the lower bound includes a  term, we conclude that the theorem holds even for smaller sets of sequences resulting in a larger constant hidden in the  term. In particular, we get the following corollary, which we use to prove \Cref{LowerBoundNumber}:
\begin{corollary}\label{corollary1}
Let  be a subset of sequences of length  such that . Let  be an -Sliding Bloom Filter where . If for any  it holds that

then
\begin{enumerate}
\item If  then 
\item If  then 
\end{enumerate}
\end{corollary}

\subsection{Removing the Absolute False Positive Assumption}\label{RemoveAssumption}
We show how we can remove the `absolute false positive' assumption while maintaining the same lower bound as in the original theorem. Towards this end, we construct a new data structure  which uses multiple instances of . The new data structure  will work only on a specific subset of all inputs, however, we show that the number of such inputs is approximately the same and hence the same entropy lower bound holds, up to a larger constant hidden in the  term. Moreover, we show that on these inputs, the absolute false positive assumption holds and thus we can apply \Cref{corollary1} on . We restate and prove the theorem.

\begin{thm:LowerBound}
Let  be an -Sliding Bloom Filter where , then
\begin{enumerate}
\item If  then 
\item If  then 
\end{enumerate}
\end{thm:LowerBound}
\begin{proof}
In order to prove the result we need to reduce the probability of having many false positives to roughly . To obtain this sort of bound, we partition the sequence into several subsequences on which we apply the original Sliding Bloom Filter independently. The motivation of using multiple instances of  is to introduce independence between different sets of inputs.

Let  be a universe composed of  copies of , where  will be determined later. We denote each copy by  and we call it a world. Each world is of size  and  is of size . We consider only sequences such that each chunk of  elements in the sequence contain exactly one element from each world. These are the only sequences the are valid for , and we denote them by . Let  be  independent instances of the algorithm  with parameters .

 works by delegating each input to the corresponding instance of . On input  we insert  into , and on query  we query  and return its answer. 
\begin{claim}
Let  and . Then,  is an -Sliding Bloom Filter for any sequence .
\end{claim}
\begin{proof}
We show that the two properties hold for any time  in the sequence. Let , let  such that , and let  be the sequence  limited to elements in .  answers by  and thus . We analyze  in the different cases.

Since each chuck of  elements contain one element from each world, each data structure will contain at most  elements of the current window. Moreover, each element of the window will be present in one of the 's. If  is in the current window, then since each  has no false negatives we have .

Now suppose  is not in the current window and not in the  element beforehand. If  then, by the false positive probability of , we have that . Otherwise,  but , and therefore at least  elements have arrived after . Thus, each  has received at least  elements, and so . Since each  is an -Sliding Bloom Filter we have that 
\end{proof}

We have shown that  satisfies that properties of an -Sliding Bloom Filter for sequences of . Now, we show that the false positives assumption holds for  under sequences of .

For any sequence , for all , let  be a random variable indicating the number of false positives of  in . Let  be the total number of false positives of . We bound the probability that  is too high.
\begin{claim}
For  we have that 
\end{claim}
\begin{proof}
Define  and . Since  we get that

To bound , we use Azuma's inequality (in the form of \cite[Theorem 7.2.1]{AlonS04}). First, note that  is a martingale:

Moreover, we have . Thus, by Azuma's inequality we get



which holds for .
\end{proof}

\begin{claim}
The false positive assumption holds for  for valid sequences. Namely, for any sequence  it holds that

\end{claim}
\begin{proof}
By the previous claim, we know that for any : . Using a union bound we get that

\end{proof}

We have shown that the false positive assumption holds for all sequences in . To apply \Cref{corollary1} we are left to show that the entropy lower bound is large enough, namely:
\begin{claim}

\end{claim}
\begin{proof}
We count the number of possible sequences in  of length . First we need to choose  elements from each of the  worlds. Then, for each  chosen elements, we divide them between the  chucks of the sequence, and finally we count all possible orderings of each chuck. Altogether we get:


The entropy lower bound is:


\end{proof}

Let  be the memory consumption of . We can now apply \Cref{corollary1} on  with the set of sequences  and parameters  and get

Since  we get that there exist some  such that


Since  is an -Sliding Bloom Filter we get the desired lower bound.
\end{proof}

\section{Acknowledgments}
We thank Ilan Komargodski, Tal Wagner and the anonymous referees for many useful comments.

\bibliographystyle{amsalpha}
\bibliography{SlidingBloomFilter}


\end{document} 