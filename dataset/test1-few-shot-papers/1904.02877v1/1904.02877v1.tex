\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{multirow}
\usepackage{bbm}
\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{amsmath}
\usepackage{color}
\newcommand\rd[1]{{\color{blue}{#1}}} 

\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
\title{Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours}
\titlerunning{Single-Path NAS}
\author{Dimitrios Stamoulis\inst{1} \and
Ruizhou Ding\inst{1} \and
Di Wang\inst{2} \and
Dimitrios Lymberopoulos\inst{2} \and
Bodhi Priyantha\inst{2} \and
Jie Liu\inst{3} \and
Diana Marculescu\inst{1}}
\authorrunning{D. Stamoulis et al.}
\institute{Department of ECE, Carnegie Mellon University, Pittsburgh, PA, USA \and
Microsoft, Redmond, WA, USA \and
Harbin Institute of Technology, Harbin, China\\
\email{dstamoul@andrew.cmu.edu}}

\maketitle              \begin{abstract}
Can we automatically design a Convolutional Network (ConvNet) with the
highest image classification accuracy under the latency constraint 
of a mobile device? Neural architecture search (NAS) has revolutionized
the design of hardware-efficient ConvNets by automating this process.
However, the NAS problem remains challenging due to the combinatorially large 
design space, causing a significant searching time (at least 200 GPU-hours). 
To alleviate this complexity, we propose \textit{Single-Path NAS}, a novel 
differentiable NAS method for designing hardware-efficient ConvNets 
in \textbf{less than 4 hours}.  Our contributions are as follows: 
1.~\textbf{Single-path search space}: Compared to previous differentiable 
NAS methods, \textit{Single-Path NAS} uses one single-path over-parameterized 
ConvNet to encode all architectural decisions with shared convolutional
kernel parameters, hence drastically decreasing the number of
trainable parameters and the search cost down to few epochs. 
2.~\textbf{Hardware-efficient ImageNet classification}: 
\textit{Single-Path NAS} achieves  top-1 accuracy on 
ImageNet with 79ms latency on a Pixel 1 phone, which is 
state-of-the-art accuracy compared to NAS methods 
with similar inference latency constraints (). 3.~\textbf{NAS efficiency}: 
\textit{Single-Path NAS} search cost is only 
\textbf{8 epochs} (30 TPU-hours), which is up to \textbf{5,000 faster}
compared to prior work. 4.~\textbf{Reproducibility}:
Unlike all recent mobile-efficient NAS methods which only 
release pretrained models, we open-source our entire codebase at:
\url{https://github.com/dstamoulis/single-path-nas}.

\keywords{Neural Architecture Search  \and Hardware-aware ConvNets.}
\end{abstract}


\section{Introduction}

``\textit{Is it possible to reduce the considerable search cost of Neural 
Architecture Search (NAS) down to only \textit{few hours}?}''
NAS has revolutionized the design of Convolutional Networks
(ConvNets)~\cite{zoph2017learning},
yielding state-of-the-art results in several deep learning applications~\cite{real2018regularized}.
NAS methods already have a profound impact on the design of hardware-efficient 
ConvNets for computer vision tasks under the constraints (\textit{e.g.}, 
inference latency) imposed by mobile devices~\cite{tan2018mnasnet}.


Despite the recent breakthroughs, NAS remains an intrinsically costly 
optimization problem. Searching for 
which convolution operation to use per ConvNet layer, gives rise to a 
combinatorially large search space: \textit{e.g.}, for a mobile-efficient 
ConvNet with 22 layers, choosing among five candidate operations
yields  possible ConvNet architectures. 
To traverse this design space, earlier NAS methods guide the exploration
via reinforcement learning (RL)~\cite{tan2018mnasnet}. Nonetheless, 
training the RL controller poses prohibitive computational challenges, 
and thousands of candidate ConvNets need to be trained~\cite{wu2018fbnet}. 


\begin{figure}[ht!]
  \centering
  \includegraphics[width=1.0\columnwidth]{key-idea.png}
  \caption{\textit{Single-Path NAS} directly 
  optimizes for the subset of convolution kernel weights and searches
  over an over-parameterized ``\textbf{superkernel}'' in each ConvNet layer
  (right). This \textbf{novel view} of the design space
  eliminates the need for maintaining separate paths for each candidate 
  operation, as in previous \textit{multi-path} approaches (left). 
  Our \textbf{key insight} drastically reduces the NAS search cost by up to 
  \textbf{5,000} with state-of-the-art accuracy 
  on ImageNet for the same mobile latency setting, compared to prior work.}
  \label{fig:key_idea}
\end{figure}


\textbf{Inefficiencies of \textit{multi-path} NAS}:
Recent NAS literature has seen a shift towards one-shot differentiable
formulations~\cite{liu2018darts,pham2018efficient,xie2018snas}
which search over a supernet that encompasses all candidate 
architectures. Specifically, current NAS methods relax 
the combinatorial optimization problem of finding the optimal ConvNet 
architecture to an operation/path selection problem: first, 
an over-parameterized, \textit{multi-path} supernet is constructed, 
where, for each layer, every candidate operation is added as a 
\textit{separate} trainable path, as illustrated in Figure~\ref{fig:key_idea} (left).
Next, NAS formulations solve for the (distributions of) paths of the 
\textit{multi-path} supernet that yield the optimal architecture.

As expected, naively branching out all paths is inefficient 
due to an intrinsic limitation: the number of trainable 
parameters that need to be maintained and updated 
during the search grows linearly with respect to the number 
of candidate operations per layer~\cite{bender2018understanding}.
To tame the memory explosion introduced by the \textit{multi-path} supernet,
current methods employ creative ``workaround'' solutions:
\textit{e.g.}, searching on a proxy dataset (subset of ImageNet~\cite{wu2018fbnet}), 
or employing a memory-wise scheme with only a subset of paths being updated during 
the search~\cite{cai2018proxylessnas}. Nevertheless, these techniques 
remain considerably costly, with an overall computational demand of
at least 200 GPU-hours. 


In this paper, we propose \textit{Single-Path NAS}, a novel NAS method for designing
hardware-efficient ConvNets in \textbf{less than 4 hours}. Our \textbf{key insight} 
is illustrated in Figure~\ref{fig:key_idea} (right). We build upon the 
observation that different candidate convolutional operations in NAS 
can be viewed as subsets of a \textbf{single ``superkernel''}. Without having to 
choose among different paths/operations as in \textit{multi-path} methods, we instead 
solve the NAS problem as \textit{finding which subset of kernel weights to use 
in each ConvNet layer}. By sharing the convolutional kernel weights, 
we encode all candidate NAS operations into a single \textbf{``superkernel''}, 
\textit{i.e.}, with a single path, for each layer of the one-shot NAS supernet. 
This novel encoding of the design space yields a drastic reduction to 
the number of trainable parameters/gradients, allowing our NAS method to use 
batch sizes of , a four-fold increase compared to prior art's 
search efficiency.

Our contributions are as follows:
\begin{enumerate}

\item \textbf{Single-path NAS}: We propose a novel 
view of the one-shot, supernet-based design space, hence drastically 
decreasing the number of trainable parameters. To the best of our knowledge,
this is the \textit{first} work to formulate the NAS problem 
as finding the subset of kernel weights in each ConvNet layer.

\item  \textbf{State-of-the-art results}: \textit{Single-Path NAS} achieves 
 top-1 accuracy on ImageNet with 79ms latency on a Pixel 1, 
\textit{i.e.}, a  improvement over the current best
hardware-aware NAS~\cite{tan2018mnasnet} under .

\item  \textbf{NAS efficiency}: The overall search cost is only 
\textbf{8 epochs}, \textit{i.e.}, \textbf{3.75 hours} on TPUs  
(30 TPU-hours), up to \textbf{5,000 faster} compared to prior work. 

\item  \textbf{Reproducibility}: Unlike recent hardware-efficient 
NAS methods which release pretrained models only, we open-source and fully document
our method at: \url{https://github.com/dstamoulis/single-path-nas}.

\end{enumerate}

\section{Related Work}

\textbf{Hardware-efficient ConvNets}: While complex ConvNet designs
have unlocked unprecedented performance levels in computer vision tasks,
the accuracy improvement has come at the cost of higher computational 
complexity, making the deployment of state-of-the-art ConvNets to 
mobile devices challenging~\cite{stamoulis2018designing}. To this end, 
a significant body of prior work aims to co-optimize
for the inference latency of ConvNets. Earlier approaches focus on human expertise
to introduce hardware-efficient
operations~\cite{howard2017mobilenets,sandler2018mobilenetv2,zhang1707shufflenet}.
Pruning~\cite{chin2018layer} and quantization~\cite{ding2017lightnn} methods 
share the same goal to improve the efficiency of ConvNets.

\textbf{Neural Architecture Search (NAS)}: NAS aims at 
automating the process of designing ConvNets, giving rise to 
methods based on reinforcement learning (RL), evolutionary 
algorithms, or gradient-based 
methods~\cite{liu2018darts,pham2018efficient,real2018regularized,zoph2016neural,zoph2017learning}.
Earlier approaches train an agent (\textit{e.g.}, RNN controller)
by sampling candidate architectures over a cell-based 
design space, where the same cell is repeated in all layers
and the focus is on searching the cell architecture~\cite{zoph2017learning}. 
Nonetheless, training the controller
over different architectures makes the search costly. 

\textbf{Hardware-aware NAS}: Earlier NAS methods focused 
on maximizing accuracy under FLOPs constraints~\cite{xie2018snas,zhou2018resource}, 
but low FLOP count does not necessarily translate to hardware 
efficiency~\cite{dong2018dpp,stamoulis2018hyperpower}.
More recent methods incorporate hardware terms (\textit{e.g.}, runtime, power)
into cell-based NAS formulations~\cite{dong2018dpp,hsu2018monas}, but 
cell-based implementations are not hardware friendly~\cite{wu2018fbnet}.
Breaking away from cell-based assumptions in the search space encoding, recent work 
employs NAS over a generalized MobileNetV2-based design space 
introduced in~\cite{tan2018mnasnet}.

\textbf{Hardware-aware Differentiable NAS}: 
Recent NAS literature has seen a shift towards one-shot 
NAS formulations~\cite{liu2018darts,pham2018efficient,xie2018snas}. 
Gradient-based NAS in particular has gained increased popularity and 
has achieved state-of-the-art results~\cite{brock2017smash}. One-shot-based 
methods use an over-parameterized super-model network, where, for each layer, every 
candidate operation is added as a separate trainable path.
Nonetheless, \textit{multi-path} 
search spaces have an intrinsic limitation: the number of trainable 
parameters that need to be maintained and updated with gradients 
during the search grows linearly with respect to the number of different
convolutional operations per layer, resulting in memory 
explosion~\cite{bender2018understanding,cai2018proxylessnas}.

To this end, state-of-the-art approaches employ different novel ``workaround'' solutions.
FBNet~\cite{wu2018fbnet} searches on a ``proxy'' dataset (\textit{i.e.},
subset of the ImageNet dataset). Despite the decreased search cost thanks 
to the reduced number of training images, these approaches do not address the fact 
that the entire supermodel needs to be maintained in memory during search, 
hence the efficiency is limited due to inevitable use of smaller batch sizes. 
ProxylessNAS~\cite{cai2018proxylessnas} has employed a memory-wise one-shot model 
scheme, where only a set of paths is updated during the search. However, such implementation-wise 
improvements do not address a second key suboptimality of one-shot approaches, 
\textit{i.e.}, the fact that separate gradient steps are needed to update the 
weights and the architectural decisions interchangeably~\cite{liu2018darts}. Although the 
number of trainable parameters, with respect to the memory cost, is kept to the same 
level at any step, the way that \textit{multi-path}-based methods traverse the 
design space remains inefficient. 


\section{Proposed Method: \textit{Single-Path} NAS}

In this Section, we present our proposed 
method. First, we discuss our novel \textit{single-path} view 
(Subsection~\ref{subsec:view}) of the search space. Next, 
we encode the NAS problem as finding the 
subset of convolution weights over the \textit{over-parameterized} 
\textbf{``superkernel''} (Subsection~\ref{subsec:single-path-kernel}), and we discuss 
how it compares to existing \textit{multi-path}-based NAS 
(Subsection~\ref{subsec:comparison-vs-multi}). Last, we formulate 
the hardware-aware NAS objective function, where we incorporate 
an accurate inference latency model of ConvNets executing on the
Pixel~1 smartphone (Subsection~\ref{subsec:hw-loss}).


\begin{figure}[ht!]
  \centering
  \includegraphics[width=1.0\columnwidth]{hierarchy.png}
  \caption{\textbf{\textit{Single-path} search space}: Our method builds upon
  \textit{hierarchical} MobileNetV2-like search 
  spaces~\cite{sandler2018mobilenetv2,tan2018mnasnet}, where the goal
  is to identify the type of mobile inverted bottleneck 
  convolution (MBConv)~\cite{sandler2018mobilenetv2} per layer.
  Our \textit{one-shot supernet} encapsulates all possible NAS architectures 
  in the search space, without the need for appending each candidate operation
  as a separate path. \textit{Single-Path} NAS directly searches over the weights of 
  a \textbf{searchable ``superkernel''} that encodes all MBConv types.}
  \label{fig:design_space}
\end{figure}


\subsection{Mobile ConvNets Search Space: A Novel View}
\label{subsec:view}

\textbf{Background - Mobile ConvNets}: State-of-the-art  
NAS builds upon a fixed ``backbone'' ConvNet~\cite{cai2018proxylessnas} inspired 
by the MobileNetV2 design~\cite{sandler2018mobilenetv2}, illustrated in 
Figure~\ref{fig:design_space} (top). Specifically, in this fixed macro-architecture, 
except for the head and stem layers, all ConvNet layers are grouped into 
blocks based on their filter sizes. The filter numbers per block 
follow the values in~\cite{wu2018fbnet}, \textit{i.e.}, we use 
seven blocks with up to four layers each. Each layer of these blocks 
follows a mobile inverted bottleneck convolution MBConv~\cite{sandler2018mobilenetv2}
micro-architecture, which consists of a point-wise () convolution, a  
depthwise convolution, and a linear  convolution (Figure~\ref{fig:design_space}, middle). 
Unless the layer has a stride 
value of two, a skip path is introduced to provide a residual 
connection from input to output. 

Each MBConv layer is parameterized by , \textit{i.e.}, the kernel size
of the depthwise convolution, and by expansion ratio , \textit{i.e.}, 
the ratio between the output and input of the first 
 convolution. Based on this parameterization, we denote 
each MBConv as MBConv--.
Mobile-efficient NAS aims to choose each MBConv-- layer, 
by selecting among different  and  values~\cite{cai2018proxylessnas,wu2018fbnet}. 
In particular, we consider MBConv layers with kernel sizes  and 
expansion ratios . NAS also considers a special 
\textit{skip-op} ``layer'', which ``zeroes-out'' the kernel and feeds 
the input directly to the output, \textit{i.e.}, the entire layer is dropped. 


\textbf{Novel view of design space}: 
Our \textit{key insight} is illustrated in Figure~\ref{fig:design_space}. 
We build upon the observation that different candidate convolutional operations in NAS 
can be viewed as subsets of the weights of an over-parameterized 
\textbf{single ``superkernel''} (Figure~\ref{fig:design_space}, bottom). 
This observation allows us to view the NAS combinatorial problem as 
\textit{finding which subset of kernel weights to use in each MBConv layer}. 
This observation is important since it allows
sharing the kernel parameters across different MBConv
architectural options. As shown in Figure~\ref{fig:design_space},
we encode all candidate NAS operations to this single 
\textbf{``superkernel''}, \textit{i.e.}, with a \textbf{single path}, for each
layer of the one-shot NAS supernet. 


\subsection{Proposed Methodology: Single-Path NAS formulation}
\label{subsec:single-path-kernel}

\textbf{Key idea - Relaxing NAS decisions over an over-parameterized kernel}: 
To simplify notation and to illustrate the key idea, 
without loss of generality, we show the case of choosing between a 
 or a  kernel for an MBConv layer.
Let us denote the weights of the two candidate kernels as 
 and , respectively. 
As shown in Figure~\ref{fig:key_idea_2} (left), we observe that 
the weights of the  kernel can be viewed as 
the \textit{inner} core of the weights of the  kernel, 
while ``zeroing'' out the weights of the ``\textit{outer}'' shell.
We denote this (\textit{outer}) subset of weights (that does not contribute 
to output of the  kernel but only to the  kernel), 
as .
Hence, the NAS architectural choice of using 
the  convolution corresponds to using both 
the \textit{inner}  weights and the \textit{outer} shell, 
\textit{i.e.},  (Figure~\ref{fig:key_idea_2}, left). 

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\columnwidth]{key-idea-2.png}
  \caption{Encoding NAS decisions into the \textbf{superkernel}: 
  We formulate all candidate convolution operations (\textit{i.e.},
  different kernel size (left) and expansion ratio (right) values) directly 
  into the \textbf{searchable superkernel}.}
  \label{fig:key_idea_2}
\end{figure}

We can therefore encode the NAS decision directly into the 
\textbf{superkernel} of an MBConv layer as a function of kernel weights
as follows:

where  is the indicator function that encodes the 
architectural NAS choice, \textit{i.e.}, if  then 
, else  then  
.

\textbf{Trainable indicator/condition function}: While the indicator function
encodes the NAS decision, a critical choice is how to formulate the condition over 
which the  is evaluated. Our intuition is that, for an 
indicator function that represents whether to use the subset of weights,
its condition should be \textit{directly a function of the subset's weights}. 
Thus, our goal is to define an ``importance'' signal of the subset
weights that intrinsically captures their contribution to the overall ConvNet
loss. We draw inspiration from weight-based conditions that have been 
successfully used for quantization-related 
decisions~\cite{ding2019flightnns} and we use 
the \textit{group Lasso term}. Specifically, for the indicator related to
the  ``outer shell'' decision, 
we write the following condition:

where  is a latent variable that controls the decision (\textit{e.g.},
a threshold value) of selecting kernel . The threshold will be compared to 
the Lasso term to determine if the \textit{outer}
 weights are used to the overall convolution.
It is important to notice that, instead of picking the thresholds (\textit{e.g.}, ) 
by hand, we seamlessly 
treat them as trainable parameters to learn via gradient descent. 
To compute the gradients for thresholds, we relax the indicator 
function  to a 
sigmoid function, , when computing gradients, \textit{i.e.}, 
. 


\textbf{Searching for expansion ratio and skip-op}: Since the result of the 
kernel-based NAS decision  (Equation~\ref{eq:proposed-form-2}) is a 
convolution kernel itself, we can in turn apply our formulation to also encode 
NAS decisions for the expansion ratio of the  kernel.
As illustrated in Figure~\ref{fig:key_idea_2} (right), the channels of the 
depthwise convolution in an MBConv-- layer with expansion ratio  
can be viewed as using one half of the channels of an 
MBConv-- layer with expansion ratio , while ``zeroing'' 
out the second half of channels .
Finally, by ``zeroing'' out the first half of the output filters as well, 
the entire \textbf{superkernel} contributes nothing if added to the 
residual connection of the MBConv layer: \textit{i.e.}, by deciding if ,
we can encode the NAS decision of using, or not, only the ``skip-op'' path. 
For both decisions over  kernel, we write:

Hence, for  input , the output of the -th MBConv layer of the network is: 


\textbf{Searchable MBConv kernels}: Each MBConv uses  convolutions for 
the point-wise (first) and linear stages, while the kernel-size decisions 
affect only the (middle)  depthwise convolution (Figure~\ref{fig:design_space}). 
To this end, we use our \textbf{searchable}  depthwise 
kernel at this middle stage. In terms of number of channels, the depthwise 
kernel depends on the point-wise  output, which allows us to 
directly encode the expansion ratio  at the middle stage as well: 
by setting the point-wise  output to 
the maximum candidate expansion ratio, we can instead solve for which 
of them not to ``zero'' out at the depthwise (middle) state.
In other words, we directly use our \textbf{searchable} depthwise convolution 
\textbf{superkernel} to effectively encode the NAS decision for the expansion ratio.
Hence, our \textit{single-path}, convolution-based formulation can sufficiently capture 
any MBConv type (\textit{e.g.}, MBConv--, MBConv--,
\textit{etc.}) in the MobileNetV2-based design space (Figure~\ref{fig:design_space}).


\subsection{Single-Path vs. Existing Multi-Path Assumptions}
\label{subsec:comparison-vs-multi}

\textbf{Comparison with multi-path over-parameterized network}:
We briefly illustrate how our \textit{single-path} formulation compares to multi-path 
NAS approaches. In existing methods~\cite{cai2018proxylessnas,liu2018darts,wu2018fbnet},
the output of each layer  is a (weighted) sum defined over the output of  
different paths, where each path  corresponds to a different candidate 
kernel . The weight of each path 
 corresponds to the probability that this path 
is selected over the parallel paths:

It is easy to see how our novel \textit{single-path} view is advantageous, 
since the output of the convolution at layer  of our search space 
is \textit{directly a function of the weights of our single over-parameterized kernel} 
(Equation~\ref{eq:effective-kernel-output}): 



\textbf{Comparison with multi-path NAS optimization}: 
Multi-path NAS methods solve for the optimal architecture parameters 
 (path weights), such that the weights 
 of the corresponding -architecture have minimal 
loss :

However, solving Equation~\ref{eq:bilevel} gives rise to a challenging \textit{bi-level} 
optimization problem~\cite{liu2018darts}. Existing methods interchangeably 
update the 's while freezing the 's and vice versa, leading to more gradient steps. 


In contrast, with our \textit{single-path} formulation, the overall network loss
is directly a function of the \textbf{``superkernel''} weights, where the learnable
kernel- and expansion ratio-related threshold variables, 
and , are directly derived as a function (norm) of
the kernel weights . Consequently, \textit{Single-Path NAS} formulates 
the NAS problem as solving \textit{directly over the weight kernels  
of a single-path, compact neural network}. Formally, the NAS problem becomes:


\textbf{Efficiency of \textit{Single-Path NAS}}: 
Unlike the bi-level optimization problem in prior work, solving
our NAS formulation in Equation~\ref{eq:sp-nas} is as expensive as
training the weights of a single-path, \textbf{branchless}, compact neural network
with vanilla gradient descent. Therefore, our formulation eliminates the need 
for separate gradient steps between the ConvNet
weights and the NAS parameters. Moreover, the reduction of the trainable 
parameters  per se, further leads to a drastic reduction of
the search cost down to \textbf{just a few epochs},
as our experimental results show later in Section~\ref{sec:results}.
Our NAS problem formulation allows us to efficiently 
solve Equation~\ref{eq:sp-nas} with batch sizes of 1024, 
a four-fold increase compared to prior art's search efficiency.


\subsection{Hardware-Aware NAS with Differentiable Runtime Loss}
\label{subsec:hw-loss}

To design hardware-efficient ConvNets, the differentiable objective in Equation~\ref{eq:sp-nas}
should reflect both the accuracy of the searched architecture and its inference latency
on the target hardware. Hence, we use a latency-aware 
formulation~\cite{cai2018proxylessnas,wu2018fbnet}:

The first term  corresponds to the cross-entropy loss of the
single-path model. The hardware-related term  is the 
runtime in milliseconds () of the searched NAS model on the target 
mobile platform. Finally, the coefficient  modulates
the trade-off between cross-entropy and runtime.

\textbf{Runtime model over the single-path design space}: To preserve the
differentiability of the objective, another critical choice is the formulation of
the latency term . Prior art has showed that the total network latency of a 
mobile ConvNet can be modeled as the sum of each -th layer's 
runtime , since the runtime of each operator
is independent of other operators~\cite{cai2017neuralpower,cai2018proxylessnas,wu2018fbnet}:


For our approach, we adapt the per-layer runtime model as a function of the 
NAS-related decisions .
We profile the target mobile platform (Pixel 1)
and we record the runtime for each candidate kernel operation per layer , 
\textit{i.e.}, , , ,
and . We denote the runtime of layer  by following the notation 
in Equation~\ref{eq:proposed-form-3}. Specifically, 
the runtime of layer  is defined first as a
function of the expansion ratio decision:

Next, by incorporating the kernel size decision, the total runtime is:

As in Equation~\ref{eq:proposed-form-2}, we relax the indicator 
function to a sigmoid function  when computing gradients.
By using this model, the runtime term in the loss function remains 
differentiable with respect to layer-wise NAS choices.
As we show in our results, the model is accurate, with an average 
prediction error of .


\section{Experiments}
\label{sec:results}

\subsection{Experimental Setup}

\textbf{Dataset and target application}:
We use \textit{Single-Path NAS} to design ConvNets for image classification 
on ImageNet~\cite{deng2009imagenet}. We use Pixel 1 as the target 
mobile platform. The choice of this experimental setup is important,
since it allows for a representative comparison with prior 
hardware-efficient NAS methods that optimize for the same
Pixel 1 device around a target latency of 
~\cite{cai2018proxylessnas,tan2018mnasnet}.

\textbf{Implementation and deployment}: We implement our NAS 
framework in TensorFlow (\texttt{TF} version 1.12).
During both search and training stages, we use TPUs 
(version 2)~\cite{jouppi2017datacenter}. To this end,  
we build on top of the \texttt{TPUEstimator} classes following the 
TPU-related documentation of the MnasNet 
repository\footnote{\url{https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet}}. 
Last, all models (ours and prior work) are deployed with 
TensorFlow TFLite to the mobile device. On the device, we profile 
runtime using the Facebook AI Performance Evaluation 
Platform (\texttt{FAI-PEP})\footnote{\url{https://github.com/facebook/FAI-PEP}}
that supports profiling for \texttt{tflite} models with detailed 
per-layer runtime breakdown.

\textbf{Implementing the custom ``superkernels''}: We use \texttt{Keras} 
to implement our trainable ``superkernels.'' Specifically, we define a custom
\texttt{Keras}-based depthwise convolution kernel where the output is a function
of both the weights and the threshold-based decisions 
(Equations~\ref{eq:proposed-form-2}-\ref{eq:proposed-form-3}). Our custom 
layer also returns the effective runtime of the layer 
(Equations~\ref{eq:runtime-layer-e}-\ref{eq:runtime-layer}). 
We document our implementation in our project GitHub 
repository: \url{https://github.com/dstamoulis/single-path-nas},
with detailed steps on how to reproduce the results.  


\subsection{State-of-the-art Runtime-Constrained ImageNet Classification}

We apply our method to design ConvNets for the Pixel 1 phone
with an overall target latency of . We train the derived \textit{Single-Path} 
NAS model for 350 epochs, following the MnasNet 
training schedule~\cite{tan2018mnasnet}. We compare our method with
mobile ConvNets designed by human experts and state-of-the-art 
NAS methods in Table~\ref{tab:imagenet-sota}, in terms of classification 
accuracy and search cost. In terms of hardware efficiency, prior work has 
shown that low FLOP count does 
not necessarily translate to high hardware efficiency~\cite{dong2018dpp}, 
we therefore evaluate the various NAS methods with respect to the inference
runtime on Pixel 1 (). 


\textbf{Enabling a representative comparison}:
While we provide the original values from the 
respective papers, our goal is to ensure a fair comparison. 
To this end, we retrain the baseline models following the same 
schedule (in fact, we find that the MnasNet-based training schedule 
improves the top1 accuracy compared to what is reported in several previous 
methods). Similarly, we profile the models on the same Pixel 1 device.
For prior work that does not optimize for Pixel 1, we retrain and profile their 
model closest to the MnasNet baseline (\textit{e.g.}, the FBNet-B and ChamNet-B 
networks~\cite{dai2018chamnet,wu2018fbnet}, since the authors use these
ConvNets to compare against the MnasNet model). Finally, to enable a representative 
comparison of the search cost per method, we directly report 
the number of epochs reported per method, hence canceling out the 
effect of different hardware systems (GPU vs TPU hours).


\textbf{ImageNet classification}:
Table~\ref{tab:imagenet-sota} shows that our \textit{Single-Path} NAS achieves 
top-1 accuracy of , which is the new state-of-the-art ImageNet accuracy
among hardware-efficient NAS methods. More specifically, 
\textbf{our method achieves better top-1 accuracy than ProxylessNAS
by} , while maintaining on par target latency of  on the 
same target mobile phone. \textit{Single-Path} NAS outperforms methods in 
this mobile latency range, \textit{i.e.}, better than MnasNet (), 
FBNet-B (), and MobileNetV2 (). 


\begin{table}[t!]
\caption{\textit{Single-Path} NAS achieves state-of-the-art accuracy (\%) on ImageNet 
for similar mobile latency setting compared to previous 
NAS methods ( on Pixel 1), with up to 
 reduced search cost in terms of number of epochs. *The search cost
in epochs is estimated based on the claim~\cite{cai2018proxylessnas}
that ProxylessNAS is  faster than MnasNet. ChamNet does not detail
the model derived under runtime constraints~\cite{dai2018chamnet} 
so we cannot retrain or measure the latency.}
\centering
\scalebox{0.955}{
\begin{tabular}{l|cccc}
\hline
\multirow{2}{*}{Method} & Top-1 & Top-5 & Mobile & Search \\ 
  & Acc (\%) & Acc (\%) & Runtime (ms) & Cost (epochs)  \\ \hline \hline
  
MobileNetV1~\cite{howard2017mobilenets} & 70.60 & 89.50 & 113 & \multirow{3}{*}{-} \\
MobileNetV2 1.0x~\cite{sandler2018mobilenetv2} & 72.00 & 91.00 & 75.00 &  \\
MobileNetV2 1.0x (our impl.) & 73.59 & 91.41 & 73.57 &  \\\hline

Random search     & 73.78  0.85 & 91.42  0.56 & 77.31  0.9 ms & - \\\hline



MnasNet 1.0x~\cite{tan2018mnasnet} & 74.00 & 91.80 & 76.00 & \multirow{2}{*}{40,000}  \\
MnasNet 1.0x (our impl.)  & 74.61 & 91.95 & 74.65 &   \\\hline
ChamNet-B~\cite{dai2018chamnet}   & 73.80 & -- & -- & 240  \\\hline
ProxylessNAS-R~\cite{cai2018proxylessnas} & 74.60 & 92.20 & 78.00 & \multirow{2}{*}{200*}  \\
ProxylessNAS-R (our impl.)  & 74.65 & 92.18 & 77.48 &   \\\hline
FBNet-B~\cite{wu2018fbnet} & 74.1 & - & - & \multirow{2}{*}{90}  \\
FBNet-B (our impl.)  & 73.70 & 91.51 & 78.33 &   \\\hline
\hline
\textit{Single-Path} NAS (\textbf{proposed}) & \textbf{74.96} & \textbf{92.21} & 79.48 & \textbf{8} (\textbf{3.75 hours})  \\\hline
\end{tabular}
}
\label{tab:imagenet-sota}
\end{table}



\textbf{NAS search cost}: \textit{Single-Path} NAS has \textbf{orders of magnitude 
reduced search cost} compared to all previous hardware-efficient NAS methods.
Specifically, MnasNet reports that the controller uses 8k sampled models, each 
trained for 5 epochs, for a total of 40k train epochs. In turn, ChamNet
trains an accuracy predictor on 240 samples, which assuming an aggressively 
fast training schedule of five epochs per sample (same as in MnasNet),
corresponds to a total search cost of 1.2k epochs.
ProxylessNAS reports  search cost improvement over MnasNet,
hence the overall cost is the TPU-equivalent of 200 epochs. 
Finally, FBNet reports 90 epochs of training on a proxy dataset (10\%
of ImageNet). While the number of images per epoch is reduced, we found
that a TPU can accommodate a FBNet-like supermodel with maximum batch size
of 128, hence the number of steps per FBNet epoch are still 
more compared to the steps per epoch in our method.


\begin{figure}[h!]
  \centering
  \includegraphics[width=.5\columnwidth]{spnas_progress_ce.png}~
  \includegraphics[width=.5\columnwidth]{spnas_progress_runtime.png}
  \caption{\textit{Single-Path NAS} search progress: Progress of both objective terms, 
  \textit{i.e.}, cross entropy  (left) and runtime  (right) during NAS search.}
  \label{fig:progress}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\columnwidth]{spnet.png}
  \caption{Hardware-efficient ConvNet found by \textit{Single-Path} NAS, with 
  top-1 accuracy of  on ImageNet and inference time of  
  on Pixel 1 phone.}
  \label{fig:spnet}
\end{figure}

In comparison, \textit{Single-Path NAS} has a total cost of eight epochs, which 
is  faster than MnasNet,  faster than ProxylessNAS, 
and  faster than FBNet. In particular, we use an aggressive 
training schedule similar to the few-epochs schedule used in MnasNet to 
train the individual ConvNet samples~\cite{tan2018mnasnet}. Due to space 
limitations, we provide implementation details (\textit{e.g.}, label smoothing, learning rates,
 value, \textit{etc.}) in our project repository. Overall, we visualize the search 
efficiency of 
our method in Figure~\ref{fig:progress}, where we show the progress of both 
 and  terms of Equation~\ref{eq:sp-nas}. Earlier during our search (first six epochs), 
we employ \textit{dropout} across the different subsets of the kernel weights
(Figure~\ref{fig:progress}, right).
Dropout is a common technique in NAS methods to prevent the supernet from 
learning as an ensemble. Unlike prior art that employs this technique over 
the separate paths of the \textit{multi-path} supernet, we directly 
drop randomly the subsets of the superkernel in our
\textit{single-path} search space. 
We search for \textbf{ steps} (8 epochs with a batch size of ),
which corresponds to total wall-clock time of \textbf{3.75 hours} on a TPUv2.
In particular, given than a TPUv2 has 2 chips with 4 cores each, this 
corresponds to a total of 30 TPU-hours. 


\textbf{Visualization of \textit{Single-Path NAS} ConvNet}: Our derived 
ConvNet architecture is shown in Figure~\ref{fig:spnet}. Moreover,
to illustrate how the \textbf{searchable superkernels} effectively 
capture NAS decisions across subsets of kernel weights,
we plot the standard deviation of weight values in 
Figure~\ref{fig:kernels} (shown in log-scale, with lighter colors 
indicating smaller values). Specifically, we compute the standard deviation 
of weights across the channel-dimension for all \textbf{superkernels}. 
For various layers shown in Figure~\ref{fig:kernels} (per -th ConvNet's 
layer from Figure~\ref{fig:spnet}), we observe
that the \textit{outer} 
``shells'' reflect the NAS architectural choices: for
layers where the entire  is selected, 
the  
values drastically vary across the channels. On the contrary, 
for all layers where  convolution is selected, 
the \textit{outer} shell values do not vary significantly. 


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\columnwidth]{filters.png}
  \caption{Visualization of kernel-based architectural contributions. The 
  \textit{standard deviation} of \textbf{superkernel} values across the kernel
  channels is shown in log-scale, with lighter colors indicating smaller values.}
  \label{fig:kernels}
\end{figure}


\textbf{Comparison with random search}: We find surprising that 
mobile-efficient NAS methods lack a comparison against random search. 
To this end, we randomly sample ten ConvNets based on our design space; we employ 
sampling by rejection, where we keep samples with predicted runtime
from  to . The average accuracy and runtime of the random samples
are reported in Table~\ref{tab:imagenet-sota}. We observe that,
while random search does not outperform NAS methods, the overall accuracy
is comparable to MobileNetV2. This highlights that the effectiveness of
NAS methods heavily relies upon the properties of the MobileNetV2-based 
design space. Nonetheless, the search cost of random search is not 
representative: to avoid training all ten samples,
we would follow a selection process similar to MnasNet, by training each 
sample for few epochs and picking the one with highest accuracy. 
Hence, the actual search cost for random search 
is not negligible, and for  samples it is in fact comparable to 
automated NAS methods.


\begin{figure}[t]
    \centering
    \begin{tabular}{l  r}
    \begin{minipage}{0.46\linewidth}
        \includegraphics[width=\linewidth]{runtime_lut.png}
        \caption{The runtime model (Equation~\ref{eq:runtime-network}) is accurate,
        with an average prediction error of .}
        \label{fig:runtime_lut}
    \end{minipage}
    \qquad
    \begin{minipage}{0.46\linewidth}
        \includegraphics[width=\linewidth]{depth_mult_figure.png}
        \caption{\textit{Single-Path} NAS outperforms MobileNetV2 and 
        MnasNet across various channel size scales.}
        \label{fig:depth_mult_figure}
    \end{minipage}
    \end{tabular}
\end{figure}



\textbf{Different channel size scaling}:
Next, we follow a typical analysis~\cite{cai2018proxylessnas,wu2018fbnet},
by rescaling the networks using a width multiplier~\cite{sandler2018mobilenetv2}.
As shown in Figure~\ref{fig:depth_mult_figure}, we observe 
that our model consistently outperforms prior methods under varying runtime 
settings. For instance, Single-Path NAS with  is 1.56 faster
than the MobileNetV2 scaled model of similar accuracy.


\textbf{Runtime model}: To train the runtime model, we record the  
runtime per layer (MBConv operations breakdown) by profiling ConvNets with 
different MBConv types, \textit{i.e.}, we obtain the 
, , ,
and  runtime values per MBConv layer   
(Equations~\ref{eq:runtime-layer-e}-\ref{eq:runtime-layer}).
To evaluate the runtime-prediction accuracy of the model, 
we generate 100 randomly designed ConvNets and we measure their runtime
on the device. As illustrated in Figure~\ref{fig:runtime_lut},
our model can accurately predict the actual runtimes: 
the Root Mean Squared Error (RMSE) is , which corresponds to an 
average  prediction error. 


\begin{table}[t!]
\caption{Searching across subsets of kernel weights: 
ConvNets with weight values trained over subsets 
of the kernels ( as subset of )
achieve performance (top-1 accuracy) similar to ConvNets 
with individually trained kernels.}
\centering
\label{my-label}
\scalebox{0.9}{
\begin{tabular}{l|cc}
\hline
Method & Top-1  Acc (\%)  & Top-5  Acc (\%)   \\\hline\hline
  
Baseline ConvNet -  kernels & 73.59 & 91.41   \\
Baseline ConvNet -  kernels & 74.10 & 91.67   \\ \hline
\textit{Single-Path ConvNet} - inference w/  kernels  & 73.43 & 91.42  \\
\textit{Single-Path ConvNet} - inference w/  kernels & 73.86 & 91.72 \\
\hline
\end{tabular}
}
\label{tab:levels}
\end{table}


\subsection{Ablation Study: Kernel-based Accuracy-Efficiency Trade-off}

\textit{Single-Path NAS} searches over subsets of
the convolutional kernel weights. Hence, we conduct experiments to 
highlight how kernel-weight subsets can capture 
accuracy-efficiency trade-off effectively. To this end,
we use the MobileNetV2 macro-architecture as a backbone (we maintain the 
location of stride-2 layers as default). As two baseline networks, 
we consider the default MobileNetV2 with MBConv-- blocks 
(\textit{i.e.},  kernels for all depthwise convolutions), 
and a network with MBConv-- blocks (\textit{i.e.},  
kernels).

Next, to capture the subset-based training of weights during a \textit{Single-Path} 
NAS search, we consider a \textit{ConvNet} with MBConv-- blocks, where we 
compute the loss of the model over two subsets, 
(i) the inner  weights, and (ii) by also using the 
remaining  
weights. For each loss computed over these subsets, we accumulate back-propagated 
gradients and update the respective weights, \textit{i.e.}, gradients are being 
applied separately to the inner and to the entire kernel per layer. We
follow training steps similar to the ``switchable'' 
training across channels as in~\cite{yu2018slimmable} (for the remaining 
training hyper-parameters we use the same setup as the default MnasNet).
As shown in Table~\ref{tab:levels}, we observe the final accuracy
across the kernel granularity, \textit{i.e.}, with the 
inner  and the 
entire  kernels, 
follows an accuracy change relative to ConvNets with
individually trained kernels. 

Such finding is significant in the context of NAS, since choosing over 
subsets of kernels can effectively capture the accuracy-runtime trade-offs 
similar to their individually trained counterparts. We therefore conjecture that
our efficient \textbf{superkernel}-based design search can be flexibly adapted 
and benefit the guided search space exploration in other RL-based
NAS methods. Beyond the NAS literature, our finding is closely 
related to Slimmable networks~\cite{yu2018slimmable}. SlimmableNets
limit however their analysis across the channel dimension, and 
our work is the first to study trade-offs across the NAS kernel dimension.


\section{Conclusion}

In this paper, we proposed \textit{Single-Path NAS}, a NAS method
that reduces the search cost for designing hardware-efficient ConvNets 
to \textbf{less than 4 hours}. The key idea is to revisit the 
one-shot \textbf{supernet} design space with a novel
\textit{single-path} view, by formulating the NAS problem as 
\textit{finding which subset of kernel weights
to use} in each ConvNet layer. \textit{Single-Path NAS} achieved 
 top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 
phone, which is state-of-the-art accuracy with latency on-par 
with previous NAS methods (). More importantly, 
we reduced the search cost of hardware-efficient NAS down 
to only \textbf{8 epochs} (30 TPU-hours), which is up to 
\textbf{5,000 faster} compared to prior work. 
\textbf{Impact beyond differentiable NAS}: 
While we used a differentiable NAS formulation, our novel design 
space encoding can be flexibly incorporated into other NAS methodologies. 
Hence, \textit{Single-Path NAS} could enable future work that 
builds upon the efficiency of our \textit{single-path}, one-shot 
design space for RL- or evolutionary-based NAS methods. 


\section*{Acknowledgements}
This research was supported in part by National Science Foundation CSR 
Grant No. 1815780 and National Science Foundation CCF Grant No. 1815899. 
Dimitrios Stamoulis also acknowledges support from the Qualcomm Innovation 
Fellowship (QIF) 2018 and the TensorFlow Research Cloud programs.



\bibliographystyle{splncs04}
\bibliography{singlepathnas}


\end{document}
