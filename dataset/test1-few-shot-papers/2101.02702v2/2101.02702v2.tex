\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xparse}
\usepackage{color}
\usepackage{tabulary}
\usepackage{multirow}
\usepackage{overpic}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{soul}

\usepackage{array}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}


\ExplSyntaxOn
\cs_new:Nn\__avercalc_plus:n{
  + ( #1 )
}
\NewExpandableDocumentCommand{\avercalc}{O{1}+m}{\fp_eval:n {
    round(
      ( 0 \clist_map_function:nN { #2 } \__avercalc_plus:n ) / max(1, \clist_count:n { #2 })
      , #1
    )
  }
}
\ExplSyntaxOff

\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\newcommand{\minus}{\scalebox{0.75}[1.0]{}}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\replaced}[2]{\textcolor{blue}{#1 \st{#2}}}

\newcommand{\lau}[1]{{\color{magenta}[Lau: #1]}}
\newcommand{\ch}[1]{{\color{orange}[ch: #1]}}
\newcommand{\alex}[1]{{\color{blue}[alex: #1]}}

\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}




\definecolor{citecolor}{RGB}{34,139,34}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,
citecolor=citecolor,bookmarks=false]{hyperref}

\iccvfinalcopy

\def\iccvPaperID{2115} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\ificcvfinal\pagestyle{empty}\fi
 
\newif\ifarxiv
\arxivtrue

\unless\ifarxiv
  \usepackage{xr}
  \externaldocument{supp}
\fi

\def\titlename{TrackFormer: Multi-Object Tracking with Transformers}

\begin{document}

\title{\titlename}

\author{
	Tim Meinhardt\textsuperscript{1}\footnotemark \qquad
	Alexander Kirillov\textsuperscript{2} \qquad
	Laura Leal-TaixÃ©\textsuperscript{1} \qquad
	Christoph Feichtenhofer\textsuperscript{2} \vspace{.8em}\\
	\textsuperscript{1}Technical University of Munich \qquad \qquad
	\textsuperscript{2}Facebook AI Research (FAIR)
}

\maketitle

\ificcvfinal
	\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
	\setcounter{footnote}{1}
	\footnotetext{Work done during an internship at Facebook AI Research.}
	\renewcommand*{\thefootnote}{\arabic{footnote}}
	\setcounter{footnote}{0}
\fi \begin{abstract}
    The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatiotemporal trajectories.
We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end MOT approach based on an encoder-decoder Transformer architecture.
Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence.
The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the new concept of identity preserving track queries.
Both decoder query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization and matching or modeling of motion and appearance.
TrackFormer represents a new tracking-by-attention paradigm and yields state-of-the-art performance on the task of multi-object tracking (MOT17) and segmentation (MOTS20).
\ificcvfinal
        The code is available at~\url{https://github.com/timmeinhardt/trackformer}
    \else
        Code will be made publicly available.
    \fi

\end{abstract} \section{Introduction}

Humans need to focus their \textit{attention} to track objects in space and time, for example, when playing a game of tennis, golf, or pong.
This challenge is only increased when tracking not one, but \textit{multiple} objects, in crowded and real world scenarios.
Following this analogy, we demonstrate the effectiveness of Transformer~\cite{attention_is_all_you_need} attention for the task of multi-object tracking (MOT) in videos.

The goal in MOT is to follow the trajectories of a set of objects, \eg, pedestrians, while keeping their identities discriminated as they are moving throughout a video sequence.
With progress in image-level object detectors~\cite{rennips2015,DETR}, most approaches follow the~\textit{tracking-by-detection} paradigm which consists of two-steps: (i) detecting objects in individual video frames, and (ii) associating sets of detections between frames, thereby creating individual object tracks over time.
Traditional tracking-by-detection methods associate detections via temporally sparse~\cite{MHT_DAM,lealiccv2011} or dense~\cite{jCC, FWT} graph optimization, or apply convolutional neural networks to predict matching scores between detections~\cite{MOTDT,lealcvprw2016}.


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/teaser_reduced.pdf}
    \caption{
        TrackFormer performs joint object detection and tracking by attention with Transformers.
Object and autoregressive \textit{track queries} reason about track initialization, identity, and occlusion of spatiotemporal trajectories.}
    \label{fig:teaser}
\end{figure} 
Recent works~\cite{tracktor,mot_neural_solver_2020_CVPR,GSM} suggest a variation of the traditional paradigm, coined~\textit{tracking-by-regression}~\cite{MOTCHallenge}.
In this approach, the object detector not only provides frame-wise detections, but replaces the data association step with a continuous regression of each track to the changing position of its object.
These approaches achieve track association implicitly, but achieve top performance only by relying either on additional graph optimization~\cite{mot_neural_solver_2020_CVPR,GSM} or motion and appearance models~~\cite{tracktor}.
This is largely due to the isolated and local bounding box regression which lacks any notion of object identity or global communication between tracks.


In this work, we introduce a \textit{tracking-by-attention} paradigm which formulates MOT as a set prediction problem.
Our paradigm not only applies attention for data association~\cite{tracking_dual_matching, spatial_temporal_attention}, but performs tracking and detection in a unified way.
As shown in~\figref{fig:teaser}, this is achieved by evolving a set of track predictions from frame to frame forming trajectories over time.



We present \mbox{TrackFormer}, an end-to-end trainable Transformer~\cite{attention_is_all_you_need} encoder-decoder architecture which encodes frame-level features from a convolutional neural network (CNN)~\cite{he2016deep} and decodes queries into bounding boxes associated with identities.
The data association is performed through the nove and simple concept of~\textit{track queries}.
Each query represents an object and follows it in space and time over the course of a video sequence in an autoregressive fashion.
New objects entering the scene are detected by static object queries as in~\cite{DETR,deformable_detr} and subsequently transform to future track queries.
At each frame, the encoder-decoder processes the input image features, as well as the track and object queries, and outputs bounding boxes with assigned identities.
Thereby, TrackFormer achieves detection and data association jointly via tracking-by-attention without relying on any additional track matching, graph optimization, or explicit modeling of motion and appearance.
Our model is trained end-to-end and extends the recently proposed set prediction objective for object detection~\cite{stewart2016end,DETR,deformable_detr} to multi-object tracking.

We evaluate TrackFormer on the MOT17~\cite{MOT16} benchmark where it achieves state-of-the-art performance for public detections.
Furthermore, we demonstrate the flexibility of our model with an additional mask prediction head and show state-of-the-art results on the Multi-Object Tracking and Segmentation (MOTS20) challenge~\cite{MOTS}.

In summary, we make the following {contributions}:

\begin{itemize}
	\item An end-to-end multi-object tracking approach which achieves detection and data association in a  tracking-by-attention paradigm.

	\item The  concept of autoregressive track queries which embed an object's spatial position and identity, thereby tracking it in space and time.



	\item State-of-the-art results on two challenging multi-object tracking (MOT17) and segmentation (MOTS20) benchmarks.
\end{itemize}
 \section{Related work}

In light of the recent trend to look beyond data association of given detections, we categorize and review methods according to their respective tracking paradigm.

\paragraph{Tracking-by-detection} approaches form trajectories by associating a given set of detections over time.

\textit{Graphs} have been used for track association and long-term re-identification by formulating the problem as a maximum flow (minimum cost) optimization~\cite{berclaztpami2011} with distance based~\cite{jiangcvpr2007, pirsiavashcvpr2011, zhangcvpr2008} or learned costs~\cite{lealcvpr2014}.
Other methods use association graphs~\cite{eHAF}, learned models~\cite{MHT_DAM}, and motion information~\cite{jCC}, general-purpose solvers~\cite{yucvpr2007}, multi-cuts~\cite{TangAAS17}, weighted graph labeling~\cite{FWT}, edge lifting~\cite{lifted_disjoint_paths_2020_ICML}, or trainable graph neural networks~\cite{mot_neural_solver_2020_CVPR}.
However, graph-based approaches suffer from expensive optimization routines, limiting their practical application for online tracking.

\textit{Appearance} driven methods capitalize on increasingly powerful image recognition backbones to track objects by relying on similarity measures given by twin neural networks \cite{lealcvprw2016}, learned reID features~\cite{ristanicvpr2018}, detection candidate selection~\cite{MOTDT} or affinity estimation~\cite{famnet}.
Just like for re-identification, appearance models struggle in crowded scenarios with many object-object-occlusions.

\textit{Motion} can be modelled for trajectory prediction~\cite{lealiccv2011, alahicvpr2016, robicqueteccv2016} using a constant velocity assumption (CVA)~\cite{choieccv2010, andriyenkocvpr2011} or the social force model~\cite{scovannericcv2009, pellegriniiccv2009, yamaguchicvpr2011, lealiccv2011}.
Learning a motion model from data~\cite{lealcvpr2014} can also accomplish track association between frames~\cite{TT}.
However, the projection of non-linear 3D motion into the 2D image domain still poses a challenging problem for many models.

\paragraph{Tracking-by-regression} refrains from associating detections between frames but instead accomplishes tracking by regressing past object locations to the new positions in the current frame.
Previous efforts~\cite{feichtenhofer2017detect,tracktor} use regression heads on region-pooled object features.
In~\cite{center_track}, objects are represented as center points which allow for an association by a distance-based greedy matching algorithm.
To overcome their lacking notion of object identity and global track reasoning, additional re-identification and motion models~\cite{tracktor}, as well as traditional~\cite{GSM} and learned~\cite{mot_neural_solver_2020_CVPR} graph methods have been necessary to achieve top performance.


\paragraph{Tracking-by-segmentation} not only predicts object masks but leverages the pixel-level information to mitigate issues from crowdedness and ambiguous background areas.
Prior attempts have used category-agnostic image segmentation~\cite{Osep18ICRA}, applied Mask R-CNN~\cite{he2017mask} with 3D convolutions~\cite{MOTS} and mask pooling layers~\cite{MOTSNet}, or represented objects as unordered point clouds~\cite{pointtrack}.
However, the scarcity of annotated MOT segmentation data makes modern approaches still rely on bounding box predictions.

\paragraph{Attention for image recognition} correlates each element of the input with respect to the others and is used in Transformers~\cite{attention_is_all_you_need} for image generation~\cite{image_transformer} and object detection~\cite{DETR, deformable_detr}.
For MOT, attention has only been used to associate a given set of object detections~\cite{tracking_dual_matching, spatial_temporal_attention}, not tackling the detection and tracking problem jointly.

In contrast, TrackFormer casts the entire tracking objective into a single set prediction problem, applying attention not only as a post-processing matching step.
It jointly reasons about track initialization, identity, and spatiotemporal trajectories.
This allows us to refrain from any additional graph optimization, appearance or motion model by only relying on feature-level global attention.
 \section{TrackFormer}

We present TrackFormer, an end-to-end multi-object tracking (MOT) approach based on an encoder-decoder Transformer~\cite{attention_is_all_you_need} architecture.
This section describes how we cast MOT as a set prediction problem and introduce the~\textit{tracking-by-attention} paradigm. We then introduce the concept of~\textit{track queries}, and how these are trained for frame-to-frame data association.



\subsection{MOT as a set prediction problem}

\begin{figure*}[ht]
    \centering
    \vspace{-10pt}
    \includegraphics[width=1\textwidth]{figures/method_reduced.pdf}
    \caption{
        \textbf{TrackFormer} casts multi-object tracking as a set prediction problem performing joint detection and~\textbf{tracking-by-attention}.
The architecture consists of a CNN for image feature extraction, a Transformer~\cite{attention_is_all_you_need} encoder for image feature encoding and a Transformer decoder which applies self- and encoder-decoder attention to produce output embeddings with bounding box and class information.
At frame , the decoder transforms  object queries (white) to output embeddings either initializing new autoregressive~\textbf{track queries} or predicting the background class (crossed).
On subsequent frames, the decoder processes the joint set of  queries to follow or remove (blue) existing tracks as well as initialize new tracks (purple).
        }
    \label{fig:method}
\end{figure*} 
Given a video sequence with  individual object identities, MOT describes the task of generating ordered tracks \mbox{} with bounding boxes  and track identities .
The subset  of total frames  indicates the time span between an object entering and leaving the the scene.
These include all frames for which an object is occluded by either the background or other objects.

In order to cast MOT as a set prediction problem, we leverage an encoder-decoder Transformer architecture.
Our model performs tracking online and yields per-frame object bounding boxes and class predictions associated with identities in four consecutive steps:

\begin{enumerate}[label=(\roman*)]
    \item Frame-level feature extraction with a common CNN backbone, \eg, ResNet~\cite{he2016deep}.
    \item Encoding of frame features with self-attention in a Transformer encoder~\cite{attention_is_all_you_need}.
    \item Decoding of queries with self- and encoder-decoder attention in a Transformer decoder.
    \item Mapping of queries to box and class predictions using multilayer perceptrons (MLP). \label{item:detr_iv}
\end{enumerate}

Objects are implicitly represented in the decoder \textit{queries}, which are embeddings used by the decoder to output bounding box coordinates and class predictions.
The decoder alternates between two types of attention: (i) self-attention over all queries, which allows for joint reasoning about the objects in a scene and (ii) encoder-decoder attention, which gives queries global access to the visual information of the current frame.
The output embeddings accumulate bounding box and class information over multiple consecutive decoding layers.
The permutation invariance of Transformers requires additive positional and object encodings for the frame features and decoder queries, respectively.


\subsection{Tracking with decoder queries} \label{sec:mot_with_trackformer}

The total set of output embeddings is initialized with two types of query encodings: (i) static object queries, which allow the model to initialize tracks at any frame of the video, and (ii) autoregressive track queries, which are responsible for tracking objects across frames.


The simultaneous Transformer decoding of object and track queries allows our model to perform detection and tracking in a unified way, and thereby introduces a new~\textit{tracking-by-attention} paradigm.
A detailed architecture overview where we illustrate the integration of track and object queries into the Transformer decoder is shown in appendix~\ref{sec:impl_details_enc_dec}.

\paragraph{Track initialization.}
New objects that appear in the scene are detected by a fixed number of  output embeddings each initialized with a static and learned object encoding referred to as~\textit{object queries}~\cite{DETR}.
Intuitively, each object query learns to predict objects with certain spatial properties, such as bounding box size and position.
The decoder self-attention relies on the object encoding to avoid duplicate detections and to reason about spatial and categorical relations of objects.
The number of object queries is ought to exceed the maximum number of objects per frame.

\paragraph{Track queries.}
In order to achieve frame-to-frame track generation, we introduce the concept of \textit{track queries} to the decoding step.
Track queries follow objects through a video sequence carrying over their identity information while adapting to their changing position in an autoregressive manner.


For this purpose, each new object detection initializes a track query with the corresponding output embedding of the previous frame.
The Transformer encoder-decoder performs attention on current frame features and decoder queries~\textit{continuously updating} the instance-specific representation of object identity and location in each track query embedding.
Self-attention over the joint set of both query types allows for the detection of new objects while simultaneously avoiding re-detection of already tracked objects.
TrackFormer thereby achieves implicit multi-frame attention over past frames.

In~\figref{fig:method}, we provide a visual illustration of the track query concept.
The initial detection in frame  spawns new track queries following their corresponding object to frame  and beyond.
To this end,  object queries (white) are decoded to output embeddings for potential track initializations.
Each successful object detection  with a classification score above , \ie, output embedding not predicting the background class (crossed), initializes a new track query embedding.
As not all objects in a sequence might already appear on the first frame, the track identities  only represent a subset of all .
For the decoding step at any frame , each track query initializes an additional output embedding associated to a different identity (colored).
The joint set of  output embeddings is initialized by (learned) object and (temporally adapted) track queries, respectively.


The Transformer decoder transforms the entire set of output embeddings at once and yields bounding box and class predictions for frame .
The number of track queries  changes between frames as new objects are detected or tracks are removed.
Existing tracks followed by a track query can be removed either if their classification score drops below  or by non-maximum suppression (NMS) with an IoU threshold of . The application of a comparatively high  only removes strongly overlapping duplicate bounding boxes which we found to be  not resolvable by the decoder self-attention.


\paragraph{Track query re-identification.}

The ability to decode an arbitrary number of track queries allows for an attention-based short-term re-identification process.
We keep decoding previously removed track queries for a maximum number of  frames.
During this \textit{patience window}, track queries are considered to be inactive and do not contribute to the trajectory until a classification score higher than  triggers a re-identification.
The spatial information embedded into each track query prevents their application for long-term occlusions with large object movement, but, nevertheless, allows for a short-term recovery from track loss.
This is possible without any dedicated re-identification training; and furthermore, cements \mbox{TrackFormer}'s holistic approach by relying on the same attention mechanism as for track initialization, identity preservation and trajectory forming through short-term \textit{occlusions}.

\subsection{TrackFormer training} \label{sec:trackformer_training}

For track queries to follow objects to the next frame and work in interaction with object queries, TrackFormer requires dedicated frame-to-frame tracking training.
This is accomplished by training on two adjacent frames, as indicated in~\figref{fig:method}, and optimizing the entire MOT objective at once.
The set prediction loss for frame  measures the set prediction of all output embeddings  with respect to the ground truth objects in terms of class prediction and bounding box similarity.

The set prediction loss is computed in two steps:
\begin{enumerate}[label=(\roman*)]
	\itemsep0em
	\item Object detection on frame  with  object queries (see  in~\figref{fig:method}). \label{item:training_i}
	\item Tracking of objects from~\ref{item:training_i} and detection of new objects on frame  with all  queries. \label{item:training_ii}
\end{enumerate}
The number of track queries  depends on the number of successfully detected objects in frame .
During training, the MLP predictions  of the output embeddings from step~\ref{item:detr_iv} are each assigned to one of the ground truth objects  or the background class.
Each  represents a bounding box , object class  and identity .

\paragraph{Bipartite matching.}
The mapping  from ground truth objects  to the joint set of object and track query predictions  is determined either via track identity or costs based on bounding box similarity and object class.
For the former, we denote the subset of ground truth track identities at frame  with .
Each detection from step~\ref{item:training_i} is assigned to its respective ground truth track identity  from the set .
The corresponding output embeddings, \ie track queries, inherently carry over the identity information to the next frame.
The two ground truth track identity sets describe a hard assignment of the  track query outputs to the ground truth objects in frame :
\begin{itemize}
    \setlength{\itemindent}{5em}
    \item[:] Match by track identity .
    \item[:] Match with background class.
    \item[:] Match by minimum cost mapping.
\end{itemize}
The second set of ground truth track identities  includes tracks which either have been occluded or left the scene at frame .
The last set  of previously not yet tracked ground truth objects remains to be matched with the  object queries.
To achieve this, we follow~\cite{DETR} and search for the injective minimum cost mapping  in the following assignment problem,


with index  and pair-wise costs  between ground truth  and prediction .
The problem is solved with a combinatorial optimization algorithm as in~\cite{stewart2016end}.
Given the ground truth class labels  and predicted class probabilities  for output embeddings , the matching cost  is defined as

In~\cite{DETR}, a class cost term without logarithmic probabilities yielded  better empirical performance.
The  term penalizes bounding box differences by a linear combination of a  distance and a generalized intersection over union (IoU)~\cite{giou} cost ,

with weighting parameters .
The scale-invariant IoU term provides similar relative errors for different box sizes and mitigates inconsistency of the  distance.
The optimal cost mapping  determines the corresponding assignments in .

\paragraph{Set prediction loss.} \label{sec:set_loss}
The final MOT set prediction loss is computed over all  output predictions:

The output embeddings which were not matched via track identity or  are not part of the mapping  and will be assigned to the background class .
We indicate the ground truth object matched with prediction  by  and define the loss per query

The bounding box loss  is computed in the same fashion as~\eqref{eq:box_cost}, but we differentiate its notation as the cost term  is generally not required to be differentiable.


\paragraph{Track augmentations.}
The two-step loss computation (see \ref{item:training_i} and~\ref{item:training_ii}) for training track queries represents only a limited range of possible tracking scenarios.
Therefore, we propose the following augmentations to enrich the set of potential track queries during training.
These augmentations will be verified in our experiments.
We use three types of augmentations similar to~\cite{center_track} which lead to perturbations of object location and motion, missing detections, and simulated occlusions.

\begin{enumerate}
    \item
        The frame  for step~\ref{item:training_i} is sampled from a range of frames around frame , thereby generating challenging frame pairs where the objects have moved substantially from their previous position.
Such a sampling allows for the simulation of camera motion and low frame rates from usually benevolent sequences. \label{item:temporal_stride}

    \item
        We sample false negatives with a probability of  by removing track queries before proceeding with step~\ref{item:training_ii}.
The corresponding ground truth objects in frame  will be matched with object queries and trigger a new object detection.
Keeping the ratio of false positives sufficiently high is vital for a joined training of both query types.


    \item
        To improve the removal of tracks by assigning the background class in occlusion scenarios, we complement the set of track queries with additional false positives.
These queries are sampled from output embeddings of frame  that were classified as background.
Each of the original track queries has a chance of  to spawn an additional false positive query.
We chose these with a large likelihood of occluding with the respective spawning track query.
\end{enumerate}

Another common augmentation for improved robustness, is to applying spatial jittering to input bounding boxes or center points~\cite{center_track}.
The nature of track queries, which encode spatial object information implicitly, does not allow for such an explicit perturbation in the spatial domain.
We believe our randomization of the temporal range provides a more natural augmentation from video data.
 \section{Experiments}

In this section, we present tracking results for TrackFormer on two MOTChallenge benchmarks, namely, MOT17~\cite{mot17} and MOTS20~\cite{MOTS}.
Furthermore, we verify individual contributions in an ablation study.

\subsection{MOT benchmarks and metrics}
\paragraph{Datasets.}

The {MOT17}~\cite{mot17} benchmark consists of a train and test set, each with 7 sequences and pedestrians annotated with full-body bounding boxes.
To evaluate the tracking (data association) robustness independently, three sets of public detections with varying quality are provided, namely, DPM~\cite{dpmpami2009}, Faster R-CNN~\cite{rennips2015} and SDP~\cite{sdpYangcvpr2016}.

MOTS20~\cite{MOTS} provides mask annotations for 4 train and test sequences of MOT17.
The corresponding bounding boxes are not full-body, but based on the visible segmentation masks, and only large objects are annotated.

\paragraph{Metrics.}  \label{sec:metrics}
Different aspects of MOT are evaluated by a number of individual metrics~\cite{clear_mot}.
The community focuses on two compound metrics, namely, Multiple Object Tracking Accuracy (MOTA) and Identity F1 Score (IDF1)~\cite{metrics}.
While the former focuses on object coverage, the identity preservation of a method is measured by the latter.
For MOTS, we report MOTSA which evaluates predictions with a ground truth matching based on mask IoU.

\paragraph{Public detections.}
The MOT17~\cite{MOT16} benchmark is evaluated in a private and public detection setting.
The latter allows for a comparison of tracking methods independent of the underlying object detection performance.
MOT17 provides three sets of public detections with varying quality.
In contrast to classic tracking-by-detection methods, TrackFormer is not able to directly produce tracking outputs from detection inputs.
Therefore, we report the results of TrackFormer and CenterTrack~\cite{center_track} in~\tabref{tab:mot_eval} by filtering the initialization of tracks with a minimum IoU requirement.
For more implementation details and a discussion on the fairness of such a filtering, we refer to~\ref{sec:impl_details}. 

\subsection{Implementation details} \label{sec:imp_details}

TrackFormer follows the ResNet50~\cite{he2016deep} CNN feature extraction and Transformer encoder-decoder architecture presented in Deformable DETR~\cite{deformable_detr}.
However, the Focal loss~\cite{lin2017focal} applied in~\cite{deformable_detr} emphasizes only the track queries, and ignores new object detections during training.
Therefore, we resort to the original cross-entropy loss of DETR~\cite{DETR} as in~\secref{sec:trackformer_training}.
Deformable DETR~\cite{deformable_detr}  substantially reduces training time and improves detection performance for small objects which are very prominent in the MOT17 dataset.
It~\cite{deformable_detr}  achieves this by replacing the original attention over single scale feature maps with multi-scale deformable attention modules.
For track queries, the deformable reference points for the current frame  are dynamically adjusted to the previous frame bounding box centers.

\paragraph{Queries and the background class.}
By design, \mbox{TrackFormer} can only detect a maximum of  objects.
To detect the maximum number of 52 objects per frame in MOT17~\cite{MOT16}, we train TrackFormer with ~300 learned object queries.
The number of possible track queries is adaptive and only practically limited by the ability of the decoder to discriminate them.
For optimal performance, the total number of queries must exceed the number of ground truth objects per frame by a large margin.
To mitigate the resulting class imbalance, we follow~\cite{DETR} and downweigh the class prediction loss for background class queries by a factor of 0.1.
To facilitate the training of track removal, we do not apply downweighting for false positive track augmentations.

\paragraph{Simulate MOT from single images.}
The encoder-decoder multi-level attention mechanism requires substantial amounts of training data.
Hence, we follow a similar approach as in~\cite{center_track} and simulate MOT data from the CrowdHuman~\cite{crowdhuman} person detection dataset.
The adjacent training frames  and  are generated by applying random spatial augmentations to a single image.
To simulate high frame rates as in MOT17~\cite{MOT16}, we only randomly resize and crop of up to  with respect to the original image size.

\paragraph{Training procedure.}
We follow~\cite{deformable_detr} and pretrain the model for 50 epochs without track queries on COCO~\cite{COCO}.
The tracking capabilities are learned by training on MOT17 frame pairs or simulating adjacent MOT frames from single images.
As in~\cite{deformable_detr}, the backbone and encoder-decoder are trained with individual learning rates of 0.00001 and 0.0001, respectively.
The MOT17 public detections model is trained for a total of 40 epochs with a learning rate drop by a factor of 10 after the first 10 epochs.
The private detections model is pretrained for 50 epochs on CrowdHuman and then fine-tuned on MOT17 with reduced learning rates for additional 20 epochs.
Excluding the COCO pretraining, we train the public detections model for around 2 days on 7 16GB RTX GPUs.

\paragraph{Mask training.}
TrackFormer predicts instance-level object masks with a segmentation head as in~\cite{DETR} by generating spatial attention maps from the encoded image features and decoder output embeddings.
Subsequent upscaling and convolution operations yield mask predictions for all output embeddings.
Since the two datasets have several sequences in common, we adopt the private detection training pipeline from MOT17 including the pretraining on CrowdHuman.
However, for our MOTS20 model, we retrain TrackFormer with the original DETR~\cite{DETR} attention.
This is due to the reduced memory consumption for single scale feature maps and inferior segmentation masks from sparse deformable attention maps.
Furthermore, the beneficial effect of deformable attention vanishes on MOTS20 as it excludes small objects from segmentation.
After training on MOT17, we freeze the model and train only the segmentation head on all COCO images containing persons.
Finally, we fine-tune the entire model on the MOTS20 dataset.


\begin{table}[ht]
\tablestyle{1.2pt}{1.05}
    \begin{center}
    \begin{tabular}[t]{llH cc >{\scriptsize}c>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c}
\toprule
        & Method  & AD & MOTA  & IDF1  & MT  & ML  & FP  & FN  & ID Sw.  \\

        \midrule
        \multicolumn{10}{c}{Private} \\
        \midrule

        \parbox[t]{3mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Online}}}
        & TubeTK~\cite{tube_tk}                     &       & 63.0 & 58.6 & 735  & 468 & 27060 & 177483 & 4137 \\
        & CTracker~\cite{chained_tracker}     &               & 66.6 & 57.4 & 759  & 570 & 22284 & 160491 & 5529 \\
        & CenterTrack~\cite{center_track}           &       & \textbf{67.8} & \textbf{64.7} & 816  & 579 & \textbf{18498} & 160332 & \textbf{3039}  \\
        & \textbf{TrackFormer}                      &       & 65.0 & 63.9 & \textbf{1074} & \textbf{324} & 70443 & \textbf{123552} & 3528  \\

        \midrule
        \multicolumn{10}{c}{Public} \\
        \midrule

        \parbox[t]{3mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Offline}}}
& jCC~\cite{jCC}            &                      & 51.2 & 54.5 & 493 & 872 &  25937 &  247822 & 1802  \\
& FWT~\cite{FWT}             &                     & 51.3 & 47.6 & 505 & 830 &  24101 &  247921 & 2648  \\
& eHAF~\cite{eHAF}            &                    & 51.8 & 54.7 & 551 & 893 &  33212 &  236772 & 1834 \\
& TT~\cite{TT}                 &                   & 54.9 & 63.1 & 575 & 897 &  20236 &  233295 & 1088  \\
& MPNTrack~\cite{mot_neural_solver_2020_CVPR}   &  & 58.8 & 61.7 & \textbf{679} & \textbf{788} & 17413 & 213594 & \textbf{1185}  \\
& Lif\_T~\cite{lifted_disjoint_paths_2020_ICML} &  & \textbf{60.5} & \textbf{65.6} & 637 & 791 & \textbf{14966} &  \textbf{206619} & 1189  \\


        \midrule


\parbox[t]{3mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Online}}}
& FAMNet~\cite{famnet}        &           & 52.0 & 48.7 & 450 & 787 & 14138 & 253616 & 3072  \\
& Tracktor++~\cite{tracktor}   &          & 56.3 & 55.1 & 498 & 831 & \textbf{8866} & 235449 & 1987  \\
& GSM~\cite{GSM}      &         & 56.4 & 57.8 & 523 & 813 & 14379 & 230174 & \textbf{1485} \\
& CenterTrack ~\cite{center_track} &         & 60.5 & 55.7 & 580 & 777 & 11599 & 208577 & 2540  \\




& \textbf{TrackFormer}               &                     & \textbf{62.5} & \textbf{60.7} & \textbf{702} & \textbf{632} & 32828 & \textbf{174921} & 3917  \\
\bottomrule
    \end{tabular}
    \end{center}
\caption{
        Comparison of modern multi-object tracking methods evaluated on the~\textbf{MOT17}~\cite{MOT16} test set.
We report private as well as public detections results and separate between online and offline approaches.
TrackFormer achieves state-of-the-art results in terms of MOTA among all public tracking methods.
Both TrackFormer and CenterTrack filter track initializations by requiring a minimum IoU with public detections.
For a detailed discussion on the fairness of such a filtering, we refer to~\ref{sec:results_public} and ~\tabref{tab:public_and_private}. }
\label{tab:mot_eval}
\end{table} \begin{table}
\tablestyle{1.2pt}{1.05}
    \begin{center}
        \begin{tabular}[t]{lHH ccc}
            \toprule
            Method                  & MOT17         & CrowdHuman    & AP  & FP  & FN \\

            \midrule

            DPM~\cite{dpmpami2009}                     &       &       & 0.61 & 42308 & 36557\\
            FRCNN~\cite{rennips2015}                   &       &               & 0.72 & 10081 & 25963\\
            SDP~\cite{sdpYangcvpr2016}                     &       &               & 0.81 & 7599 & 18865\\

            \midrule
            CenterTrack~\cite{center_track}             &       &               & 0.77 & 7662 & 9900\\
            TrackFormer             &       &               & 0.73 & 13178 & 15441\\

            \bottomrule
        \end{tabular}
    \end{center}
\caption{
        Detection performance on the MOT17 test set.
Both TrackFormer and CenterTrack were pretrained on CrowdHuman~\cite{crowdhuman} and evaluated only for single frame detection.
    }
\label{tab:ablation_detection}
\end{table} 
\subsection{Benchmark results}

\paragraph{MOT17.}
Following the training procedure described in~\secref{sec:imp_details}, we evaluate TrackFormer on the MOT17~\cite{MOT16} test set and report results in~\tabref{tab:mot_eval}.
For private detections, we achieve results comparable with modern state-of-the-art methods.
This is due to the detection performance of~\cite{DETR,deformable_detr}, and thereby TrackFormer, which still lacks behind modern object detectors.


As shown in~\tabref{tab:ablation_detection}, TrackFormer applied for single-frame detection of pedestrians, \ie, many small objects with object-object occlusions, is merely on par with Faster R-CNN.
Note, the superior CenterTrack detection performance, which only translates to 2.8 points higher MOTA compared to our method.
This demonstrates TrackFormer as a powerful approach for tracking.

To further isolate the tracking performance, we compare results in a public detection setting in the lower two sections of~\tabref{tab:mot_eval}.
In that case, TrackFormer achieves state-of-the-art results both in terms of MOTA and IDF1 for online methods without pretraining on CrowdHuman~\cite{crowdhuman}.
Our identity preservation performance is only surpassed by offline methods which benefit from the processing of entire sequences at once.


TrackFormer achieves top performance via global attention between encoded input pixels and decoder queries without relying on additional motion~\cite{tracktor,famnet} or appearance models~\cite{tracktor,MOTDT,famnet}.
Furthermore, the frame to frame association with track queries avoids any post-processing with heuristic greedy matching procedures~\cite{center_track} or additional graph optimization~\cite{GSM}.


\paragraph{MOTS20.}
In addition to object detection and tracking, TrackFormer is able to predict instance-level segmentation masks.
As reported in~\tabref{tab:mots_eval}, we achieve state-of-the-art MOTS results in terms of object coverage (MOTSA) and identity preservation (IDF1).
All methods are evaluated in a private setting.
A MOTS20 test set submission is only recently possible, hence we also provide the 4-fold cross-validation evaluation established in~\cite{MOTS} and report the mean best epoch results over all splits.
\mbox{TrackFormer} surpasses all previous methods without relying on a dedicated tracking formulation for segmentation masks as in~\cite{pointtrack}.
In~\figref{fig:mots_vis}, we present a qualitative comparison of TrackFormer and Track R-CNN~\cite{MOTS} on two test sequences.


\begin{figure*}[ht]
    \centering
    \vspace{-15pt}
    \includegraphics[width=1\textwidth]{figures/mots_vis_reduced.pdf}
    \caption{
        We compare TrackFormer segmentation results with the popular Track R-CNN~\cite{MOTS} on selected MOTS20~\cite{MOTS} test sequences.
The superiority of TrackFormer in terms of MOTSA in~\tabref{tab:mots_eval} can be clearly observed by the difference in pixel mask accuracy.
        }
    \label{fig:mots_vis}
\end{figure*} \begin{table}
\tablestyle{1.2pt}{1.05}
    \begin{center}
\begin{tabular}[t]{lc ccH >{\scriptsize}c>{\scriptsize}c>{\scriptsize}c}
        \toprule
        Method & TbD & sMOTSA  & IDF1  & MOTSA  & FP  & FN  & ID Sw.   \\

        \midrule
        \multicolumn{8}{c}{Train set (4-fold cross-validation)} \\
        \midrule
MHT\_DAM~\cite{MHT_DAM}                  &  & 48.0 & -- & 62.7 & -- & -- & -- \\
FWT~\cite{FWT}                          &  & 49.3 & -- & 64.0 & -- & -- & -- \\
MOTDT~\cite{MOTDT}                      &  & 47.8 & -- & 61.1 & -- & -- & -- \\
jCC~\cite{jCC}                          &  & 48.3 & -- & 63.0 & -- & -- & --  \\

        TrackRCNN~\cite{MOTS}                   &  & 52.7 & -- & 66.9 & -- & -- & -- \\
MOTSNet~\cite{MOTSNet}                  &  & 56.8 & -- & 69.4 & -- & -- & --  \\
PointTrack~\cite{pointtrack}            &  & 58.1 & -- & 70.6 & -- & -- & --  \\
TrackFormer                &  & \textbf{\avercalc{51.62, 51.09, 64.81, 67.20}} & -- & \textbf{72.0} & -- & -- & --  \\
\midrule
        \multicolumn{8}{c}{Test set} \\
        \midrule
Track R-CNN~\cite{MOTS}                   &  & 40.6 & 42.4 & 55.2 &  \textbf{1261} &  12641 &  567  \\
TrackFormer                             &  & \textbf{54.9} & \textbf{63.6} & \textbf{69.9} &  2233 & \textbf{7195} &  \textbf{278}  \\
        \bottomrule
    \end{tabular}
\end{center}
\caption{
        Comparison of modern multi-object tracking and segmentation methods evaluated on the~\textbf{MOTS20}~\cite{MOTS} train and test sets.
Methods indicated with~\textit{TbD} originally perform tracking-by-detection without segmentation.
Hence, they are evaluated on SDP~\cite{SDP} public detections and predict masks with an additional Mask R-CNN~\cite{he2017mask} fine-tuned on MOTS20.
TrackFormer achieves state-of-the-art results in terms of MOTSA and IDF1 on both sets.
}
\label{tab:mots_eval}
\end{table} 
\subsection{Ablation study} \label{sec:ablation_study}

\begin{table}
\tablestyle{1.2pt}{1.05}
     \vspace{-10pt}
    \begin{center}
\begin{tabular}[t]{l cccc}
            \toprule
            Method                                & \multicolumn{1}{c}{MOTA } &  & \multicolumn{1}{c}{IDF1 } & \\

            \midrule

            TrackFormer                             & 51.4 & & 55.3 & \\

            --------------- w\textbackslash o --------------- & \multicolumn{4}{c}{-----------------------------------}\\
            Pretraining on CrowdHuman                            & 42.8 & -8.6 & 45.2 & -10.1\\

            Track query re-identification                           & 42.7 & -0.1   & 43.6 & -1.6 \\



            Track augmentations (FP)                    & 40.1 & -2.6   & 42.9 & -0.7 \\

            Track augmentations (Range)                 & 38.1 & -2.0    & 41.0 & -1.9 \\



Track queries                                 & 37.8 & -0.3 & 27.4 & -13.6 \\

            \bottomrule
        \end{tabular}
         \vspace{-5pt}
\end{center}
\caption{
        Ablation study on individual TrackFormer components.
We report mean best epoch results in a private setting on a 7-fold split on the MOT17~\cite{MOT16} training set.
For the last row without (w\textbackslash o) all components, we train only for object detection and associate tracks based on output embedding distance.
    }
\vspace{-10pt}
    \label{tab:ablation_study}
\end{table}
 
The ablation study on the MOT17 and MOTS20 training sequences are evaluated in a private detection setting with a 7- and 4-fold cross-validation split, respectively.
\paragraph{TrackFormer components.}
We ablate the impact of different~\mbox{TrackFormer} components on the tracking performance in~\tabref{tab:ablation_study}.
Our full system including pretraining on the CrowdHuman dataset provides a MOTA and IDF1 of 51.4 and 55.3, respectively.
The baseline without (w\textbackslash o) pretraining reduces this by -8.6 and -10.1 points which demonstrates the limitations of the MOT17 dataset.
The attention-based~\textit{track query re-identification} has a negligible effect on MOTA but improves IDF1 by 1.6 points.


If we further ablate our false positives (FP) and frame range~\textit{track augmentations}, we see another drop of -4.6 MOTA and ~\mbox{-2.6} IDF1 points.
Both augmentations provide the training which rich tracking scenarios and prevent an early overfitting.
The false negative track augmentations are indispensable for a joint training of object and track queries, hence we refrain from ablating these.


Our final baseline is without (w\textbackslash o) any tracking components, not using~\textit{track queries} and is only trained for object detection.
Data association is performed with a greedy center distance matching as in~\cite{center_track}.
This leads to a dramatic drop of -13.6 in IDF1, as shown in the last row of~\tabref{tab:ablation_study}.
The version represents previous post-processing and matching methods and demonstrates the strength of jointly addressing track initialization, identity and trajectory forming in a unified TrackFormer configuration.














\begin{table}
    \tablestyle{1.2pt}{1.05}
         \vspace{-10pt}
    \begin{center}
        \begin{tabular}[t]{lc cc}
            \toprule
            Method    &  Mask training                          & MOTA  & IDF1  \\

            \midrule
            \multirow{2}{*}{TrackFormer}    &          & 61.9 & 56.0 \\
                                            &                           & 61.9 & 54.8 \\


            \bottomrule
        \end{tabular}
    \end{center}
\caption{
        We demonstrate the effect of jointly training for tracking and segmentation on a 4-fold split on the MOTS20~\cite{MOTS} train set.
We evaluate with regular MOT metrics, \ie, matching to ground truth with bounding boxes instead of masks.
    }
     \vspace{-10pt}
\label{tab:ablation_mots}
\end{table} 
\paragraph{Mask information improves tracking.}
This final ablation is studying if  segmentation mask prediction can improve tracking performance.
\tabref{tab:ablation_mots} shows that  a unified segmentation and tracking training procedure ican improve IDF1 by +1.2. 
In contrast to~\cite{DETR}, we trained the entire model including the mask head and evaluate its bounding box tracking performance.
The additional mask information did not improve track coverage (MOTA) but resolved ambiguous occlusion scenarios during training, thereby improving identity preservation (IDF1).
 \section{Conclusion}

We have presented a unified tracking-by-attention paradigm for detection and multi-object tracking with Transformers.
Our end-to-end TrackFormer architecture introduces track query embeddings which follow objects over a sequence in an autoregressive manner.
An encoder-decoder architecture transforms each track query to the changing position of its corresponding object.
\mbox{TrackFormer} jointly tackles track initialization, identity and spatiotemporal trajectory forming solely by attention operations and does not rely on any additional matching, graph optimization, motion or appearance modeling.
Our approach achieves state-of-the-art results for multi-object tracking as well as segmentation.
We hope that  this paradigm will foster future work in detection and multi-object tracking.  
\ificcvfinal
    \paragraph{Acknowledgements:} We are grateful for discussions with Jitendra Malik, Karttikeya Mangalam, and David Novotny.
\fi



\ificcvfinal\thispagestyle{empty}\fi

\ifarxiv
    \newcount\cvprrulercount
\appendix

\def\suppabstract{This section provides additional material for the main paper:
\S\ref{sec:impl_details} contains further implementation details for TrackFormer (\S\ref{sec:trackformer_details}), a visualization of the Transformer encoder-decoder architecture  (\S\ref{sec:impl_details_enc_dec}), and parameters for multi-object tracking (\S\ref{sec:impl_details_params}).  \S\ref{sec:results} contains a discussion related to public detection evaluation (\S\ref{sec:results_public}), and detailed per-sequence results for MOT17 and MOTS20 (\S\ref{sec:results_sequences}).}

\ifarxiv
    \section*{Appendix}
    \suppabstract
\fi

\newcommand{\sref}[1]{Sec.~\ref{#1}}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}

\setcounter{figure}{0}
\renewcommand{\thefigure}{A.\arabic{figure}}
\thispagestyle{empty}

\unless\ifarxiv
    \begin{abstract}
        \suppabstract
    \end{abstract}
\fi

\section{Implementation details} \label{sec:impl_details}
\subsection{Backbone and training} \label{sec:trackformer_details}

We provide additional hyperparameters for \mbox{TrackFormer}. This supports our implementation details reported in~\secref{sec:imp_details} of the main paper.
The Deformable DETR~\cite{deformable_detr} encoder and decoder both apply 6 individual layers of feature-width 256.
Each attention layer applies multi-headed self-attention~\cite{attention_is_all_you_need} with 8 attention heads.
We do not use the ``DC5'' (dilated conv) version of the backbone as this will incur a large memory requirement related to the larger resolution of the last residual stage.
We expect that using ``DC5'' or any other heavier, or higher-resolution, backbone to provide better accuracy and leave this for future work.

Our training hyperparameters mostly follow the original DETR~\cite{DETR}.
The weighting parameters of the individual box cost  and loss  are set to  and .
The probabilities for the track augmentation at training time are  and 

\subsection{Dataset splits}

All experiments evaluated on dataset splits (ablation studies and MOTS20 training set in~\tabref{tab:mots_eval}) apply the same training pipeline presented in~\secref{sec:imp_details} to each split.
We average validation metrics over all splits and report the results from a single epoch (which yields the best mean MOTA / MOTSA) over all splits, \ie, we do not take the best epoch for each individual split.
For our ablation on the MOT17~\cite{MOT16} training set, we separate the 7 sequences into 7 splits each with a single sequence as validation set.
Before training each of the 4 MOTS20~\cite{MOTS} splits, we pre-train the model on all MOT17 sequences excluding the corresponding split of the validation sequence.



\begin{figure}
    \begin{center}
        \includegraphics[width=0.45\textwidth]{figures/encoder_decoder_tikz_reduced}
    \end{center}
    \caption{
        The TrackFormer encoder-decoder architecture. We indicate the tensor dimensions in squared brackets.}
    \label{fig:encoder_decoder}
\end{figure} 
\subsection{Transformer encoder-decoder architecture} \label{sec:impl_details_enc_dec}
To foster the understanding of TrackFormer's integration of track queries within the decoder self-attention block, we provide a simplified visualization of the encoder-decoder architecture in~\figref{fig:encoder_decoder}.
In comparison to the original illustration in~\cite{DETR}, we indicate \textit{track identities} instead of spatial encoding with~\textit{color-coded} queries.
The frame features (indicated in grey) are the final output of the CNN feature extractor and have the same number of channels as both query types.
The entire Transformer architecture applies  and  independently supervised encoder and decoder layers, with spatial positional and object encoding as in~\cite{DETR}.
Track queries are fed \textit{autoregressively} from the \textit{previous frame} output embeddings of the last decoding layer (before the final feed-forward class and bounding box networks (FFN)).
The object encoding is achieved by re-adding the object queries to the corresponding embeddings in the decoder key (K) and query (Q).

\subsection{Multi-object tracking parameters} \label{sec:impl_details_params}

In~\secref{sec:mot_with_trackformer}, we explain the process of track initialization and removal over a sequence.
The corresponding hyperparameters were optimized by a grid search on the MOT17 training set cross-validation splits.
The grid search yielded track initialization and removal thresholds of  and , respectively.
The lower  score prevents tracks from being removed too early and improves identity preservation performance.
~\mbox{TrackFormer} benefits from an NMS operation for the removal of strong occlusion cases with an intersection over union larger than .


For the track query re-identification, our search proposed an optimal inactive patience and score of  and , respectively.

\section{Experiments} \label{sec:results}

\subsection{Public detections and track filtering} \label{sec:results_public}
    TrackFormer implements a new tracking-by-attention paradigm which requires track initializations to be filtered for an evaluation with public detections.
Here, we provide a discussion on the comparability of TrackFormer with earlier methods and different filtering schemes.

    \begin{table}
    \begin{center}
    \resizebox{1.0\columnwidth}{!}{\begin{tabular}[t]{l ccc cc >{\footnotesize}H>{\footnotesize}H>{\footnotesize}H>{\footnotesize}H>{\footnotesize}H H}
        \toprule
        Method  & IN & IoU & CD & MOTA  & IDF1  & MT  & ML  & FP  & FN  & ID Sw.  &  Hz \\

        \midrule
        \multicolumn{8}{c}{Offline} \\
        \midrule

        MHT\_DAM~\cite{MHT_DAM}           &       &    &    & 50.7 & 47.2 & 491 & 869 &  22875 &  252889 & 2314 & 0.9\\
jCC~\cite{jCC}                    &       &    &    & 51.2 & 54.5 & 493 & 872 &  25937 &  247822 & 1802 & 1.8 \\
FWT~\cite{FWT}                    &       &    &    & 51.3 & 47.6 & 505 & 830 &  24101 &  247921 & 2648 & 0.2 \\
eHAF~\cite{eHAF}                  &       &    &    & 51.8 & 54.7 & 551 & 893 &  33212 &  236772 & 1834 & 0.7 \\
TT~\cite{TT}                      &       &    &    & 54.9 & 63.1 & 575 & 897 &  20236 &  233295 & 1088 & 2.5 \\
MPNTrack~\cite{mot_neural_solver_2020_CVPR}     &       &    &    & 58.8 & 61.7 & 679 & 788 & 17413 & 213594 & 1185 & 6.5 \\
Lif\_T~\cite{lifted_disjoint_paths_2020_ICML}   &       &    &    & 60.5 & 65.6 & 637 & 791 & 14966 &  206619 & 1189 & 0.5 \\


        \midrule
        \multicolumn{8}{c}{Online} \\
        \midrule

MOTDT~\cite{MOTDT}                     &       &   &     & 50.9 & 52.7 & 413 & 841 & 24069 & 250768 & 2474 & \textbf{18.3}\\
FAMNet~\cite{famnet}                   &       &     &   & 52.0 & 48.7 & 450 & 787 & 14138 & 253616 & 3072 & -- \\
Tracktor++~\cite{tracktor}             &       &    &    & 56.3 & 55.1 & 498 & 831 & \textbf{8866} & 235449 & 1987 & 1.5 \\
GSM\_Tracktor~\cite{GSM}               &       &   &     & 56.4 & 57.8 & 523 & 813 & 14379 & 230174 & \textbf{1485} & 8.7 \\
\cmidrule(r){1-1} \cmidrule(lr){2-9}
CenterTrack~\cite{center_track}                            & &       &    & 60.5 & 55.7 & 580 & 777 & 11599 & 208577 & 2540 \\
TrackFormer                            & &       &        & 62.5 & 60.7 & 702 & 632 & 32828 & 174921 & 3917 \\

        \cmidrule(r){1-1} \cmidrule(lr){2-9}

        CenterTrack~\cite{center_track}        &    &   &        & 61.5 & 59.6 & 621 & 752 & 14076 & 200672 & 2583 & 17.0 \\

        TrackFormer                            &    &   &        & 63.4 & 60.0 & 795 & 829 & 45721 & 156062 & 4958 & \\
        \bottomrule
    \end{tabular}}
    \end{center}
\caption{
        Comparison of modern multi-object tracking methods evaluated on the~\textbf{MOT17}~\cite{MOT16} test set for different~\textbf{public detection processing}.
Public detections are either directly processed as input (IN) or applied for filtering of track initializations by center distance (CD) or intersection over union (IoU).
We report mean results over the three sets of public detections provided by~\cite{MOT16} and separate between online and offline approaches.
The arrows indicate low or high optimal metric values.
    }
\label{tab:public_and_private}
\end{table} 
    Common tracking-by-detection methods directly process the MOT17 public detections and report their mean tracking performance over all three sets.
This is only possible for methods that perform data association on a bounding box level.
However, TrackFormer and point-based methods such as CenterTrack~\cite{center_track} require a procedure for filtering track initializations by public detections in a comparable manner.
Unfortunately, MOT17 does not provide a standardized protocol for such a filtering.
The authors of CenterTrack~\cite{center_track} filter detections based on bounding box center distances (CD).
Each public detection can possibly initialize a single track but only if its center point falls in the bounding box area of the corresponding track.

    In~\tabref{tab:public_and_private}, we revisit our MOT17 test set results but with this public detections center distance (CD) filtering, while also inspecting the CenterTrack per-sequence results in~\tabref{tab:mot_eval_centertrack_seqs_center_distance}.
We observe that this filtering does not reflect the quality differences in each set of public detections, \ie, DPM~\cite{dpmpami2009} and SDP~\cite{sdpYangcvpr2016} results are expected to be the worst and best, respectively, but their difference is small.


    We hypothesize that a center distance filtering is not in accordance with the common public detection setting and propose a filtering based on Intersection over Union (IoU).
For IoU filtering, public detections only initialize a track if they have an IoU larger than 0.5.
The results in~\tabref{tab:public_and_private}, show that for TrackFormer and CenterTrack, using IoU filtering performs worse compared to the CD filtering which is expected as this is a more challenging evaluation protocol.
We believe IoU-based filtering (instead of CD-based) provides a fairer comparison to previous MOT methods which directly process public detections as inputs (IN).
This is validated by the per-sequence results in~\tabref{tab:mot_eval_trackformer_seqs_min_iou}, where IoU filtering shows differences across detectors that are more meaningfully correlated with detector performance, compared to the relatively uniform performance across detections with the CD based method in~\tabref{tab:mot_eval_centertrack_seqs_center_distance} (where DPM, FRCNN and SDP show \emph{very similar} performance).


    Consequently, we follow the IoU-based filtering protocol to compare with CenterTrack in our main paper.
While our gain over CenterTrack seems similar across the two filtering techniques for MOTA (see ~\tabref{tab:public_and_private}), the gain in IDF1 is significantly larger under the more challenging IoU-based protocol, which suggests that CenterTrack benefits from the less challenging CD-based filtering protocol, while TrackFormer does not rely on the filtering for achieving its high IDF1 tracking accuracy.

\subsection{MOT17 and MOTS20 sequence results} \label{sec:results_sequences}

    \begin{table}
    \tablestyle{1.2pt}{1.05}
    \begin{center}
    \begin{tabular}[t]{lH ccc ccc H}
        \toprule
        Sequence & TD & sMOTSA  & IDF1  & MOTSA  & FP  & FN  & ID Sw.  & Hz  \\

        \midrule
        MOTS20-01 & 01 & 59.8 & 68.0 & 79.6 & 255 & 364 & 16 \\
        MOTS20-06 & 06 & 63.9 & 65.1 & 78.7 & 595 & 1335 & 158 \\
        MOTS20-07 & 07 & 43.2 & 53.6 & 58.5 & 834 & 4433 & 75 \\
        MOTS20-12 & 12 & 62.0 & 76.8 & 74.6 & 549 & 1063 & 29 \\
        \midrule
        ALL & ALL & 54.9 & 63.6 & 69.9 & 2233 & 7195 & 278 \\
        \bottomrule
    \end{tabular}
    \end{center}
\caption{
        We present TrackFormer tracking and segmentation results on each individual sequence of the~\textbf{MOTS20}~\cite{MOTS} test set.
MOTS20 is evaluated in a private detections setting.
The arrows indicate low or high optimal metric values.
    }
\label{tab:mots_eval_trackformer_seqs}
\end{table}     \begin{table*}


    \begin{center}
    \resizebox{0.75\textwidth}{!}{\begin{tabular}[t]{ll cc rrrrr H}
        \toprule
        Sequence         & Public detection                        & MOTA  & IDF1  & MT  & ML  & FP  & FN  & ID Sw.  &  Hz \\

        \midrule

        MOT17-01 & DPM~\cite{dpmpami2009} & 41.6 & 44.2 & 5 & 8 & 496 & 3252 & 22 \\
        MOT17-03 & DPM & 79.3 & 71.6 & 94 & 8 & 1142 & 20297 & 191 \\
        MOT17-06 & DPM & 54.8 & 42.0 & 54 & 63 & 314 & 4839 & 175 \\
        MOT17-07 & DPM & 44.8 & 42.0 & 11 & 16 & 1322 & 7851 & 147 \\
        MOT17-08 & DPM & 26.5 & 32.2 & 11 & 37 & 378 & 15066 & 88 \\
        MOT17-12 & DPM & 46.1 & 53.1 & 16 & 45 & 207 & 4434 & 30 \\
        MOT17-14 & DPM & 31.6 & 36.6 & 13 & 78 & 636 & 11812 & 196 \\

        \midrule

        MOT17-01 & FRCNN~\cite{rennips2015} & 41.0 & 42.1 & 6 & 9 & 571 & 3207 & 25 \\
        MOT17-03 & FRCNN & 79.6 & 72.7 & 93 & 7 & 1234 & 19945 & 180 \\
        MOT17-06 & FRCNN & 55.6 & 42.9 & 57 & 59 & 363 & 4676 & 190 \\
        MOT17-07 & FRCNN & 45.5 & 41.5 & 13 & 15 & 1263 & 7785 & 156 \\
        MOT17-08 & FRCNN & 26.5 & 31.9 & 11 & 36 & 332 & 15113 & 89 \\
        MOT17-12 & FRCNN & 46.1 & 52.6 & 15 & 45 & 197 & 4443 & 30 \\
        MOT17-14 & FRCNN & 31.6 & 37.6 & 13 & 77 & 780 & 11653 & 202 \\

        \midrule

        MOT17-01 & SDP~\cite{sdpYangcvpr2016} & 41.8 & 44.3 & 7 & 8 & 612 & 3112 & 27 \\
        MOT17-03 & SDP & 80.0 & 72.0 & 93 & 8 & 1223 & 19530 & 181 \\
        MOT17-06 & SDP & 55.5 & 43.8 & 56 & 61 & 354 & 4712 & 181 \\
        MOT17-07 & SDP & 45.2 & 42.4 & 13 & 15 & 1332 & 7775 & 147 \\
        MOT17-08 & SDP & 26.6 & 32.3 & 11 & 36 & 350 & 15067 & 91 \\
        MOT17-12 & SDP & 46.0 & 53.0 & 16 & 45 & 221 & 4426 & 30 \\
        MOT17-14 & SDP & 31.7 & 37.1 & 13 & 76 & 749 & 11677 & 205 \\

        \midrule

        \multicolumn{2}{c}{All} & 61.5 & 59.6 & 621  & 752 & 14076 & 200672 & 2583 \\

        \bottomrule
    \end{tabular}
    }
    \end{center}
\caption{
        We report the original per-sequence~\textbf{CenterTrack}~\cite{center_track} MOT17~\cite{MOT16} test set results with ~\textbf{Center Distance (CD)} public detection filtering.
The results do not reflect the varying object detection performance of DPM, FRCNN and SDP, respectively.
The arrows indicate low or high optimal metric values.
    }
	\vspace{15pt}
\label{tab:mot_eval_centertrack_seqs_center_distance}
\end{table*}     \begin{table*}


    \begin{center}
    \resizebox{0.75\textwidth}{!}{\begin{tabular}[t]{ll cc rrrrr H}
        \toprule
        Sequence         & Public detection                        & MOTA  & IDF1  & MT  & ML  & FP  & FN  & ID Sw.  &  Hz \\

        \midrule

        MOT17-01 & DPM~\cite{dpmpami2009} & 48.2 & 38.4 & 5 & 8 & 266 & 3012 & 60 \\
        MOT17-03 & DPM & 73.8 & 70.4 & 90 & 14 & 6102 & 21083 & 236 \\
        MOT17-06 & DPM & 55.6 & 54.6 & 58 & 76 & 499 & 4533 & 195 \\
        MOT17-07 & DPM & 53.6 & 46.3 & 11 & 17 & 686 & 7047 & 112 \\
        MOT17-08 & DPM & 35.0 & 34.9 & 12 & 28 & 544 & 13024 & 164 \\
        MOT17-12 & DPM & 49.9 & 57.6 & 21 & 36 & 508 & 3789 & 44 \\
        MOT17-14 & DPM & 39.2 & 42.4 & 19 & 57 & 947 & 9958 & 338 \\

        \midrule

        MOT17-01 & FRCNN~\cite{rennips2015} & 49.5 & 40.7 & 8 & 7 & 363 & 2831 & 66 \\
        MOT17-03 & FRCNN & 75.5 & 71.4 & 91 & 12 & 6490 & 18907 & 243 \\
        MOT17-06 & FRCNN & 59.0 & 56.7 & 64 & 50 & 644 & 3962 & 224 \\
        MOT17-07 & FRCNN & 52.8 & 45.2 & 11 & 16 & 867 & 6980 & 131 \\
        MOT17-08 & FRCNN & 34.2 & 35.0 & 13 & 30 & 552 & 13201 & 142 \\
        MOT17-12 & FRCNN & 48.0 & 56.5 & 18 & 38 & 532 & 3932 & 40 \\
        MOT17-14 & FRCNN & 38.8 & 42.9 & 20 & 50 & 1596 & 9238 & 485 \\

        \midrule

        MOT17-01 & SDP~\cite{sdpYangcvpr2016} & 55.7 & 43.0 & 8 & 5 & 391 & 2396 & 69 \\
        MOT17-03 & SDP & 77.5 & 71.3 & 103 & 12 & 7159 & 16063 & 302 \\
        MOT17-06 & SDP & 58.5 & 56.5 & 78 & 58 & 724 & 3950 & 214 \\
        MOT17-07 & SDP & 55.8 & 46.1 & 14 & 14 & 958 & 6370 & 141 \\
        MOT17-08 & SDP & 36.4 & 35.6 & 15 & 26 & 720 & 12525 & 193 \\
        MOT17-12 & SDP & 50.7 & 59.8 & 21 & 31 & 666 & 3559 & 44 \\
        MOT17-14 & SDP & 42.4 & 42.8 & 22 & 47 & 1614 & 8561 & 474 \\

        \midrule

        \multicolumn{2}{c}{All} & 62.5 & 60.7 & 702 & 632 & 32828 & 174921 & 3917 \\

        \bottomrule
    \end{tabular}
    }
    \end{center}
\caption{
        We report~\textbf{TrackFormer} results on each individual sequence and set of public detections evaluated on the MOT17~\cite{MOT16} test set.
We apply our minimum~\textbf{Intersection over Union (IoU)} public detection filtering.
The arrows indicate low or high optimal metric values.
    }
\label{tab:mot_eval_trackformer_seqs_min_iou}
\end{table*} 
    In~\tabref{tab:mot_eval_trackformer_seqs_min_iou}, we provide per-sequence MOT17~\cite{MOT16} test set results for public detection filtering via Intersection over Union (IoU).
Futhermore, we present per-sequence TrackFormer results on the MOTS20~\cite{MOTS} test set in~\tabref{tab:mots_eval_trackformer_seqs}.


    \paragraph{Evaluation metrics}
    In~\secref{sec:metrics} we explained two compound metrics for the evaluation of MOT results, namely, Multi-Object Tracking Accuracy (MOTA) and Identity F1 score (IDF1).~\cite{clear_mot}
However, the~\href{https://motchallenge.net/}{MOTChallenge} benchmark implements all CLEAR MOT~\cite{clear_mot} evaluation metrics.
In addition to MOTA and IDF1, we report the following additional CLEAR MOT metrics:

    \begin{enumerate}[align=parleft, labelwidth=5em, labelindent=16pt, leftmargin=60pt]
        \item[MT:] Ground truth tracks covered for at least 80\%.
        \item[ML:] Ground truth tracks covered for at most 20\%.
        \item[FP:] False positive bounding boxes not corresponding to any ground truth.
        \item[FN:] False negative ground truth boxes not covered by any bounding box.
        \item[ID Sw.:] Bounding box switching the corresponding ground truth identity.
        in the previous frame.
        \item[sMOTSA:] Mask-based Multi-Object Tracking Accuracy (MOTA) which counts true positives instead of only masks with IoU larger than 0.5.
    \end{enumerate}

    %
 \fi

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
