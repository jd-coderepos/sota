
\documentclass{article} \usepackage{arxiv,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\DeclareMathOperator{\diver}{\text{div}}
 
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{doi}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{url}
\usepackage{clipboard}
\usepackage{amsthm, bm}
\usepackage{wrapfig,lipsum}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{dblfloatfix}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{Conjecture}[theorem]{Conjecture}
\newcommand{\xhdr}[1]{{\noindent\bfseries #1}.}


\def\eigvec{{\bm{\phi}}}
\def\eigvecnorm{\hat{{\bm{\phi}}}}

\title{Directional Graph Networks}
\renewcommand{\undertitle}{
Anisotropic aggregation in graph neural networks via directional vector fields
}



\author{Dominique Beaini\thanks{equal contribution}\\
Valence Discovery\\
Montreal, QC, Canada\\
\texttt{dominique@valencediscovery.com} \\
\And
Saro Passaro\\
 University of Cambridge \\
Cambridge, United Kingdom \\
\texttt{sp976@cam.ac.uk} \\
\And
Vincent LÃ©tourneau\\
Valence Discovery\\
Montreal, QC, Canada\\
\AND
William L. Hamilton \\
McGill University, MILA \\
Montreal, QC, Canada \\
\And
Gabriele Corso \\
University of Cambridge \\
Cambridge, United Kingdom \\
\And
Pietro Li\`{o} \\
 University of Cambridge \\
Cambridge, United Kingdom \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}

The lack of anisotropic kernels in graph neural networks (GNNs) strongly limits their expressiveness, contributing to well-known issues such as over-smoothing. To overcome this limitation, we propose the first globally consistent anisotropic kernels for GNNs, allowing for graph convolutions that are defined according to topologicaly-derived directional flows.
First, by defining a vector field in the graph, we develop a method of applying directional derivatives and smoothing by projecting node-specific messages into the field. 
Then, we propose the use of the Laplacian eigenvectors as such vector field.
We show that the method generalizes CNNs on an -dimensional grid and is provably more discriminative than standard GNNs regarding the Weisfeiler-Lehman 1-WL test.
We evaluate our method on different standard benchmarks and see a relative error reduction of 8\% on the CIFAR10 graph dataset and 11\% to 32\% on the molecular ZINC dataset, and a relative increase in precision of 1.6\% on the MolPCBA dataset. 
An important outcome of this work is that it enables graph networks to embed directions in an unsupervised way, thus allowing a better representation of the anisotropic features in different physical or biological problems. 

\end{abstract}


\begin{figure*}[ht]
\centering
 \includegraphics[width=\textwidth]{full-method.pdf}
 \vspace{-20pt}
 \caption{Overview of the steps required to aggregate messages in the direction of the eigenvectors.} 
\label{fig:full-method}
\end{figure*}


\section{Introduction}

One of the most important distinctions between convolutional neural networks (CNNs) and graph neural networks (GNNs) is that CNNs allow for any convolutional kernel, while most GNN methods are limited to symmetric kernels (also called isotropic kernels) \cite{kipf2016gcn,gilmer2017mpnn}. There are some implementations of asymmetric kernels using gated mechanisms \cite{bresson2017gatedGCN, velikovic2017gat}, motif attention \cite{peng_graph_2019_motif}, edge features \cite{gilmer2017mpnn}, port numbering \cite{sato2019approximation} or the 3D structure of molecules \cite{klicpera_directional_2019_dimenet}.

However, to the best of our knowledge, there are currently no methods that allow asymmetric graph kernels that are dependent on the full graph structure or directional flows. They either depend on local structures or local features. This is in opposition to images, which exhibit canonical directions: the horizontal and vertical axes. The absence of an analogous concept in graphs makes it difficult to define directional message passing and to produce an analogue of the directional frequency filters (or Gabor filters) widely present in image processing \cite{olah_overview_2020}. In fact, there is numerous evidence that directional filtering is fundamental image processing \cite{kang_deep_2017, antoine_two-dimensional_1996, yue_lu_finer_2005}.

We propose a novel idea for GNNs: use vector fields in the graph to define directions for the propagation of information. An overview of this framework is presented in figure \ref{fig:full-method}. Using this approach, the usual message-passing structure of a GNN is projected onto globally-defined directions so that the contribution of each neighbouring node  is weighted by its alignment with the vector fields at the receiving node . This enables our method to propagate information via directional derivatives or smoothing of the features.

In order to define globally consistent directional fields over general graphs, we propose to use the gradients of the low-frequency eigenvectors  of the graph Laplacian, since they are known to capture key information about the global structure of graphs \cite{chavel1984eigenvalues,chung1997spectral,Grebenkov_2013}. In particular, these eigenvectors can be used to define optimal partitions of the nodes in a graph, to give a natural ordering \cite{levy_laplace_beltrami_2006}, and to find the dominant directions of the graph diffusion process \cite{chung_discrete_2000,saerens2004principal}. Further, we show that they generalize the horizontal and vertical directional flows in a grid (see figure \ref{fig:eig_vec_grad}), allowing them to guide the aggregation and mimic the asymmetric and directional kernels present in computer vision. In fact, we demonstrate mathematically that our work generalizes CNNs, by reproducing all convolutional kernels of radius  in an -dimensional grid, while also bringing the powerful data augmentation capabilities of reflection, rotation or distortion of the directions. Additionally, we also prove that our directional graph networks (DGNs) are more discriminative than standard GNNs in regards to the Weisfeiler-Lehman 1-WL test, confirming an increase of expressiveness.

We further show that our DGN model theoretically and empirically allows for efficient message passing across distant communities, which counteracts the well-known problem of over-smoothing in GNNs. Alternative methods reduce the impact of over-smoothing by using skip connections \cite{luan2019break}, global pooling \cite{alon_bottleneck_2020}, or randomly dropping edges during training time \cite{rong_dropedge_2020}, but without solving the underlying problem. 

Our method distinguishes itself from other spectral GNNs since the literature usually uses the low frequencies to estimate local Fourier transforms in the graph \cite{levie_cayleynets_2018, xu_graph_2019}. Instead, we do not try to approximate the Fourier transform, but only to define a directional flow at each node and guide the aggregation.


We tested our method on 5 standard datasets from \cite{dwivedi2020benchmarking} and \cite{hu2020open}, using two types of architectures, and either using or ignoring edge features. In all cases, we observed state-of-the-art results from the proposed DGN, with relative improvements of 8\% on CIFAR10, 11-32\% on ZINC, 0.8\% on MolHIV and 1.6\% on MolPCBA. Most of the improvement is attributed to the directional derivative aggregator, highlighting our method's ability of capturing directional high-frequency signals in graphs.





\begin{figure*}[ht]
\centering
 \includegraphics[height=5.7cm]{Gradient-of-eigenvectors.png}
 \vspace{-5pt}
 \caption{Possible directional flows in different types of graphs. The node coloring is a potential map and the edges represent the gradient of the potential with the arrows in the direction of the flow. The first 3 columns present the arcosine of the normalized eigenvectors () as node coloring, and their gradients represented as edge intensity. The last column presents examples of inductive bias introduced in the choice of direction. (a) The eigenvectors 1 and 2 are the horizontal and vertical flows of the grid. (b) The eigenvectors 1 and 2 are the flow in the longest and second-longest directions. (c) The eigenvectors 1, 2 and 3 flow respectively in the South-North, suburbs to the city center and West-East directions. We ignore  since it is constant and has no direction.} 
\label{fig:eig_vec_grad}
\end{figure*}



\section{Theoretical development}

\subsection{Intuitive overview}

One of the biggest limitations of current GNN methods compared to CNNs is the inability to do message passing in a specific direction such as the horizontal one in a grid graph. In fact, it is difficult to define directions or coordinates based solely on the shape of the graph. 

The lack of directions strongly limits the discriminative abilities of GNNs to understand local structures and simple feature transformations. 
Most GNNs are invariant to the permutation of the neighbours' features, so the nodes' received signal is not influenced by swapping the features of two neighbours. Therefore, several layers in a deep network will be employed to understand these simple changes instead of being used for higher level features, leading to problematic phenomena such as a  over-squashing \cite{alon_bottleneck_2020}. 



In the first part of the theoretical development, we develop the mathematical theory for general vector fields . Intuitively, defining a vector field over a graph corresponds to assigning a scalar weight to edges corresponding to the magnitude of the flow in that direction. Note that  has the same shape as the adjacency matrix and the same zero entries. As an example a left-to-right flow in a grid corresponds to a matrix with positive values over all left-to-right edges, negative over the right-to-left edges and 0 on the vertical edges.

In the second part, we set  to be the gradient of the low-frequency eigenvectors of the Laplacian. Using this directional field, we show that the expressiveness of GNNs can be improved, while providing an intuitive directional flows over a variety of graphs (see figure \ref{fig:eig_vec_grad}). For example, we prove that in grid-shaped graphs some of these eigenvectors correspond to the horizontal and vertical flows. Again, we observe in the Minnesota map that the first 3 non-constant eigenvectors produce logical directions, namely South/North, suburb/city, and West/East.





Another important contribution---also noted in figure \ref{fig:eig_vec_grad}---is the ability to define any kind of directional flow based on prior knowledge of the problem. Hence, instead of relying on eigenvectors to find directions in a map, we can simply use the cardinal directions or the rush-hour traffic flow.


\subsection{Overview of the theoretical contributions}

\xhdr{Vector fields in a graph}
Using directions in a graph is novel and not intuitive, so our first step is to define a simple nomenclature where we use a vector field to define a directional flow at each node.

\xhdr{Directional smoothing and derivatives}
To make use of vector fields over graphs, we define aggregation matrices that can either smooth the signal (low pass filter) or compute its derivative (high pass filter) according to the directions specified by the vector field.

\xhdr{Gradient of the Laplacian eigenvectors} We show that using the gradient of the low-frequency eigenvectors of the graph Laplacian generates interpretable vector fields that counteract the over-smoothing problem.

\xhdr{Generalization of CNNs} We demonstrate that, when applied to a grid graph, the eigenvector-based directional aggregation generalizes convolutional neural networks.

\xhdr{Comparison to the Weisfeiler-Lehman (WL) test} We prove that the proposed DGN is more expressive than the 1-WL test, and thus more expressive than ordinary GNNs.




\subsection{Vector fields in a graph}
This section presents the ideas of differential geometry applied to graphs, with the goal of finding proper definitions of scalar products, gradients and directional derivatives. For reference see for example \cite{bronstein_geometric_2017,Grebenkov_2013,discrete_calculus_2010}.



Let  be a graph with  the set of vertices and  the set of edges. The graph is undirected meaning that  iff . Define the vector spaces  and  as the set of maps  and  with  and  and scalar products



Think of  as the ``tangent space" to  and of  as the set of ``vector fields'' on the space  with each row  representing a vector at the -th node, and the element  being the component of the vector going from node  to  through edge . Note that with  the number of nodes in , any  can be represented as an  coordinates vector and  can be represented as an  matrix.

Define the pointwise scalar product as the map  taking 2 vector fields and returning their inner product at each point of ,  at the node  is defined by \eqref{eq:inner-product}.



In \eqref{eq:grad_div}, we define the gradient  as a mapping  and the divergence  as a mapping , thus leading to an analogue of the directional derivative in \eqref{eq:directional derivative}.


\begin{definition}
The directional derivative of the function  on the graph  in the direction of the vector field  where each vector is of unit-norm is 

\end{definition}

 will denote the absolute value of  and  the -norm of the -th row of . We also define the forward/backward directions as the positive/negative parts of the field .

\begin{comment}
Let  be a vector field in the graph, meaning that it is an anti-symmetric weighted aggregation matrix,  We define the operators  as the absolute value of  and  as the -norm of the -th row of . The variable  is an arbitrarily small positive real number used to avoid floating-point errors. We also define the forward/backward directions as the positive/negative parts of the field .
\end{comment}

\subsection{Directional smoothing and derivatives}
\label{sec:B_av-B_dx}

Next, we show how the vector field  is used to \textit{guide} the graph aggregation by projecting the incoming messages. Specifically, we define the weighted aggregation matrices  and  that allow to compute the directional smoothing and directional derivative of the node features, as presented visually in figure \ref{fig:full-method}-d.

\paragraph{The directional average matrix }  is the weighted aggregation matrix such that all weights are positives and all rows have an -norm equal to 1, as shown in \eqref{eq:Bav_simple} and theorem \ref{th:dir_smooth}, with a proof in the appendix \ref{app:proof:dir_smooth}.

The variable  is an arbitrarily small positive number used to avoid floating-point errors.
The -norm denominator is a local row-wise normalization.
The aggregator works by assigning a large weight to the elements in the forward or backward direction of the field, while assigning a small weight to the other elements, with a total weight of 1.



\begin{theorem}[Directional smoothing]
\label{th:dir_smooth}
    \Copy{th:dir_smooth}{
    The operation  is the directional average of , in the sense that  is the mean of , weighted by the direction and amplitude of .
    }
\end{theorem}

With  the features at the nodes  neighbouring , and  the directional smoothing at node .

\paragraph{The directional derivative matrix } is defined in (\ref{eq:Bdx_simple}) and theorem \ref{th:dir_dx}, with the proof in appendix \ref{app:proof:dir_dx}. Again, the denominator is a local row-wise normalization but can be replaced by a global normalization.  is a square, diagonal matrix with diagonal entries given by . The aggregator works by subtracting the projected forward message by the backward message (similar to a center derivative), with an additional diagonal term to balance both directions.




\begin{theorem}[Directional derivative]
\label{th:dir_dx}
    \Copy{th:dir_dx}{
    Suppose  have rows of unit  norm. The operation  is the centered directional derivative of  in the direction of , in the sense of equation \ref{eq:directional derivative}, i.e.
    
    }
\end{theorem}


These aggregators are directional, interpretable and complementary, making them ideal choices for GNNs. We discuss the choice of aggregators in more details in appendix \ref{app:agg_choices}, while also providing alternative aggregation matrices such as the center-balanced smoothing, the forward-copy, the phantom zero-padding, and the hardening of the aggregators using softmax/argmax on the field. We further provide a visual interpretation of the  and  aggregators in figure \ref{fig:directional-agg}. Interestingly, we also note in appendix \ref{app:simple_agg_examples} that  and  yield respectively the mean and Laplacian aggregations when  is a vector field such that all entries are constant .

\begin{figure}[h]
\centering
 \includegraphics[width=\textwidth]
 {directional-aggregation.pdf}
 \vspace{-8pt}
 \caption{Illustration of how the directional aggregation works at a node , with the arrows representing the direction and intensity of the field .}
\label{fig:directional-agg}
\end{figure}


\subsection{Gradient of the Laplacian eigenvectors as interpretable vector fields}
\label{sec:grad_eig}

In this section we give theoretical support for the choice of gradients of the eigenfunctions of the Laplacian as sensible vectors along which to do directional message passing since they are interpretable and allow to reduce the over-smoothing. This section gives a theoretical ground to the intuitive directions presented in figure \ref{fig:eig_vec_grad}, and is the motivation behind steps (b-c) in figure \ref{fig:full-method}.

As usual the combinatorial, degree-normalized and symmetric normalized Laplacian are defined as

The eigenvectors of these matrices are known to capture many essential properties of graphs, making them a natural foundation for directional message passing.
For example, the Laplacian eigenvectors corresponding to the smallest eigenvalues (i.e., the low frequency eigenvectors) effectively capture the community structure of a graph, and these eigenvectors also play the role of Fourier modes in graph signal processing \cite{hamilton_2020}.
Indeed, the Laplacian eigenvectors hold such rich information about graph structure that their study is the focus of the mathematical subfield of spectral graph theory
\cite{chung1997spectral}.

In order to illustrate the utility of these eigenvectors in the context of GNNs, we show that the low-frequency eigenvectors provide a natural direction that allows us to pass messages between distant nodes in a graph. 
In particular, we show in theorem \ref{th:reduced-diffusion-distance} (proved in appendix \ref{app:proof:reduced-diffusion-distance}) that by passing information in the direction of , the eigenvector associated to the lowest non-trivial frequency of , DGNs can efficiently share information between distant nodes of the graph by reducing the diffusion distance between them. This idea is reflected in figure \ref{fig:eig_vec_grad}, where we see that the eigenvectors of the Laplacian give directions that correspond to a natural notion of distance on real-world graphs. 





In the next paragraphs, we will prove that following the gradient of the eigenvectors allows to effectively reduce the heat-kernel distance between pairs of nodes.



Consider the transition matrix . Its entries can be used to define a random walk with probability to move from node  to node  equal to  if  and  are neighbors and  if not. Notice that the probability to transition from  to  in  steps is given by the  entry of the matrix . This matrix is also called the discrete heat kernel . 
Given a Markov process  defined by the transition matrices , , we can define a continuous time random walk on the same graph in the following way. Let  be a mean 1 Poisson random variable, the continuous time random variable is defined by  with transition probability .

In \cite{barlow_2017}, the following identity is shown 

Or in matrix form . This transition probability is also called the continuous time heat kernel because it satisfies the continuous time heat equation on graphs . In \cite{COIFMAN20065} the following distance is defined
\begin{definition}[Diffusion distance]
The diffusion distance at time  between the nodes  is 

\end{definition}
The diffusion distance is small when there is high probability that two random walks starting at  and  meet at time . The diffusion distance is used as a model of how the data at a node  influences a node  in a GNN. The symmetrisation of the heat kernel in the diffusion distance and the use of continuous time are slight departure from the actual process of information diffusion in a GNN but allow us to describe the important phenomenons with much simpler statements.


\begin{definition}[Gradient step]
\label{def:gradient-step}
\Copy{def:gradient-step}{Suppose the two neighboring nodes  and  are such that  is maximal among the neighbors of , then we will say  is obtained from  by taking a step in the direction of the gradient .
}
\end{definition}
\begin{theorem}[Gradient steps reduce diffusion distance]
\label{th:reduced-diffusion-distance}
\Copy{th:reduced-diffusion-distance}{
Let  be nodes such that . Let  be the node obtained from  by taking one step in the direction of , then there is a constant  such that for  we have 

With the reduction in distance being proportional to .
}
\end{theorem}
From this theorem, we see that moving from node  to node  by following the gradient of the eigenvector  is guaranteed to reduce the heat kernel distance with a destination node .
While the theorem always holds for , it should be true for higher frequency eigenvectors if the graph has added structure for example if it is an approximation of a surface or a higher dimensional manifold.



In the context of GNNs, Theorem \ref{th:reduced-diffusion-distance} also has implications for the well-known problems of {\em over-smoothing} and {\em over-squashing} \cite{alon_bottleneck_2020, hamilton_2020}. 
In most GNN models, node representations become over-smoothed after several rounds of message passing, as the representations tend to reach a mean-field equilibrium equivalent to the stationary distribution of a random walk \cite{hamilton_2020}. 
Researchers have also highlighted the related issue of over-squashing, which reflects the inability for GNNs to propagate informative signals between distant nodes in a graph \cite{alon_bottleneck_2020}.

Both these problems are related to the fact that the influence of one node's input on the final representation of another node in a GNN is correlated with the diffusion distance between the nodes \cite{xu2018representation}.
Theorem \ref{th:reduced-diffusion-distance} highlights how the DGN approach can alleviate these issues. In particular, the Laplacian eigenfunctions reveal directions that can counteract over-smoothing and over-squashing by allowing efficient propagation of information between distant nodes instead of following a diffusion process. 



Finally it is interesting to note that by selecting different eigenvectors as basis of directions, our method further aligns with a theorem that multiple independent aggregators are needed to distinguish neighbourhoods of nodes with continuous features \cite{corso2020principal}.


\subsection{Choosing a basis of the Laplacian eigenspace}

When using eigenvectors of the Laplacian  to define directions in a graph, we need to keep in mind that there is never a single eigenvector associated to an eigenvalue, but a whole eigenspace.
If an eigenvalue has multiplicity of , the associated eigenspace has dimension  and any collection of  orthogonal vectors could be chosen as basis of that space and as vectors for the definitions of the aggregation matrices  defined in the previous sections.

\xhdr{Disconnected graphs} When a graph is disconnected, then the eigenfunctions will simply be the combination of the eigenfunctions of each connected components. Hence, one must consider  as the -th eigenvector of each component when taken separately.


\xhdr{Normalizing the eigenvectors} For an eigenvalue of multiplicity 1, there are always two unit norm eigenvectors of opposite sign, which poses a problem during the directional aggregation. We can make a choice of sign and later take the absolute value (i.e.  in \eqref{eq:Bav_simple}).
An alternative that applies to multiplicities higher than 1 is to take samples of orthonormal bases of the eigenspace and use each choice to augment the training (see section \ref{sec:data-augmentation}). 

\xhdr{Multiplicities greater than 1} Although multiplicities higher than one do happen for low-frequencies (square grids have a multiplicity 2 for ) this is not common in ``real-world graphs'' since it suggests symmetries in the graph which are uncommon. Furthermore,  we found no  multiplicity greater than 1 in the ZINC and PATTERN datasets. We further discuss these rare cases and how to deal with them in appendix \ref{app:eig_mult}.

\xhdr{Orthogonal directions} Although all  are orthogonal, their gradients, used to define directions, are not always \textit{locally} orthogonal (e.g. there are many horizontal flows in the grid).
This concern is left to be addressed in future work.



\subsection{Generalization of the convolution on a grid} \label{sec:generalization_cnn}

In this section we show that our method generalizes CNNs by allowing to define any radius- convolutional kernels in grid-shaped graphs. The radius- kernel at node  is a convolutional kernel that takes the weighted sum of all nodes  at a distance .

Consider the lattice graph  of size  where each vertices are connected to their direct non-diagonal neighbour. We know from Lemma \ref{lemma:cosine_eigvec} that, for each dimension, there is an eigenvector that is only a function of this specific dimension. For example, the lowest frequency eigenvector  always flows in the direction of the longest length. Hence, the Laplacian eigenvectors of the grid can play a role analogous to the axes in Euclidean space, as shown in figure \ref{fig:eig_vec_grad}. 

With this knowledge, we show in theorem \ref{th:general_grid_radius2} (proven in \ref{app:proof:general_grid_radius2}), that we can generalize all convolutional kernels in an n-dimensional grid.
This is a strong result since it demonstrates that our DGN framework generalizes CNNs when applied on a grid, thus closing the gap between GNNs and the highly successful CNNs on image tasks.


\begin{theorem}[Generalization radius- convolutional kernel in a lattice]
\label{th:general_grid_radius2}
    \Copy{th:general_grid_radius2}{
    For an -dimensional lattice, any convolutional kernel of radius  can be realized by a linear combination of directional aggregation matrices and their compositions.
}
\end{theorem}


As an example, figure \ref{fig:cnn-gnn-generalization} shows how a linear combination of the first and -th aggregators  realize a kernel on an  grid, where  and .

Note that when the size of a given dimension is an integer multiple of another direction, e.g.  or , then you will find a multiplicity of 2 for the  eigenvector. Hence, the eigenvector used to define the direction is not unique. This does not void theorem \ref{th:general_grid_radius2} since the eigenvectors flowing in the horizontal/vertical directions are still valid choices.






\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{GNN-CNN-generalization.pdf}
    \vspace{-10pt}
    \caption{Realization of a radius-1 convolution using the proposed aggregators.  is the input feature map,  the convolutional operator,  the convolution result, and .}
    \label{fig:cnn-gnn-generalization}
\end{figure}

\subsection{Extending the radius of the aggregation kernel}

Having aggregation kernels for neighbours of distance 2 or 3 is important to improve the expressiveness of GNNs, their ability to understand patterns, and to reduce the number of layers required. However, the lack of directions in GNNs strongly limits the radius of the kernels since, given a graph of regular degree , a mean/sum aggregation at a radius- will result in a heavy over-squashing of  messages. Using the directional fields, we can enumerate different paths, thus assigning a different weight for different -distant neighbours. This method, proposed in appendix \ref{app:radius-r-kernel}, avoids the over-squashing. (Empirical results on this extension are left for future work.)

\subsection{Comparison with Weisfeiler-Lehman (WL) test}

We also compare the expressiveness of the Directional Graph Networks with the classical WL graph isomorphism test which is often used to classify the expressivity of graph neural networks \cite{xu2018gin}. In theorem \ref{th:wl-test} (proven in appendix \ref{app:proof:wl-test}) we show that DGNs are capable of distinguishing pairs of graphs that the 1-WL test (and so ordinary GNNs) cannot differentiate.

\begin{theorem}[Comparison with 1-WL test]
\label{th:wl-test}
    \Copy{th:wl-test}{
    DGNs using the \textit{mean} aggregator, any directional aggregator of the first Laplacian eigenvector and injective degree-scalers are strictly more powerful than the 1-WL test.
}
\end{theorem}

\subsection{Data augmentation}\label{sec:data-augmentation}

Another theoretical result is that the directions in the graph allow to replicate some of the most common data augmentation techniques used in computer vision, namely reflection, rotation and distortion. The main difference is that, instead of modifying the image (such as a  rotation), the proposed transformation is applied on the vector field defining the aggregation kernel (thus rotating the kernel by  without changing the image). This offers the advantage of avoiding to pre-process the data since the augmentation is done directly on the kernel at each iteration of the training.

The simplest augmentation is the vector field flipping, which is done changing the sign of the field , as stated in definition \ref{def:field_flip}. This changes the sign of , but leaves  unchanged.

 \begin{definition}[Reflection of the vector field]
\label{def:field_flip}
For a vector field , the reflected field is .
\end{definition}

Let   be vector fields in a graph, with  and  being the field normalized such that each row has a unitary -norm. Define the angle vector  by . The vector field  is the normalized component of  perpendicular to . The equation below defines . The next equation defines the angle 


Notice that we then have the decomposition .


\begin{definition}[Rotation of the vector fields]
\label{def:field_rotation}
    \Copy{def:field_rotation}{
    For  and  non-colinear vector fields with each vector of unitary length, their rotation by the angle  in the plane formed by  is     
    
}
\end{definition}



Finally, the following augmentation has a similar effect to a wave distortion applied on images.

\begin{definition}[Random distortion of the vector field]
\label{def:field_distortion}
    \Copy{def:field_distortion}{
    For vector field  and anti-symmetric random noise matrix , its randomly distorted field is .
    }
\end{definition}






\section{Implementation}
\label{sec:implementation}

We implemented the models using the DGL and PyTorch libraries and we provide the code at the address
\href{https://github.com/Saro00/DGN}{https://github.com/Saro00/DGN}. 
We test our method on standard benchmarks from \cite{dwivedi2020benchmarking} and \cite{hu2020open}, namely ZINC, CIFAR10, PATTERN, MolHIV and MolPCBA with more details on the datasets and how we enforce a fair comparison in appendix \ref{app:benchmarks}.

For the empirical experiments we inserted our proposed aggregation method in two different type of message passing architectures used in the literature: a \textit{simple} convolutional architecture similar to the one present in GCN (equation \ref{eq:simple}) \cite{kipf2016gcn} and a more \textit{complex} and general one typical of MPNNs (\ref{eq:complex}) \cite{gilmer2017mpnn} with or without edge features . The time complexity of our approach is , which is identical to PNA \cite{corso2020principal}, where  is the number of edges and  the number of aggregators, with an additional  to pre-compute the -first eigenvectors, as explained in the appendix \ref{app:computation-complexity}.

     \label{eq:simple}
    X_i^{(t+1)} = 
    U \Bigg(
    \underset{(j,i) \in E}{\bigoplus} 
    X_j^{(t)} \Bigg)
  
    \label{eq:complex}
    X_i^{(t+1)} = 
    U \Bigg( X_i^{(t)}, 
    \underset{(j,i) \in E}{\bigoplus} 
    M \Big( X_i^{(t)}, X_j^{(t)}, 
    \underbrace{e_{ji}}_{\text{\tiny optional}}
    \Big) \Bigg)
  
Here,  is an operator which concatenates the results of multiple aggregators,  is the node features,  is a linear transformation and  a multiple layer perceptron (MLP). This \textit{simple} architecture of equation \ref{eq:simple} is observed visually in steps (f-g) of figure \ref{fig:full-method}.


We further use degree scalers  defined below to scale the aggregation results according to each node's degree, as proposed by the PNA model \cite{corso2020principal}. Here,  is the degree of a given node,  is the average node degree in the training set, and  is a parameter set to  for degree-attenuation and  for degree amplification. Note that each degree scaler is applied to the result of each aggregator, and the results are concatenated.




We tested the directional aggregators across the datasets using the gradient of the first  eigenvectors  as the underlying vector fields. Here,  is a hyperparameter, usually 1 or 2, but could be bigger for high-dimensional graphs. To deal with the arbitrary sign of the eigenvectors, we take the absolute value of the result of \eqref{eq:Bdx_simple}, making it invariant to a reflection of the field. In case of a disconnected graph,  is the -th eigenvector of each connected component. Despite the numerous aggregators proposed in appendix \ref{app:agg_choices}, only  and  are tested empirically.

The metrics used to measure the performance of a model depend are enforced for each dataset and provided by \cite{dwivedi2020benchmarking} and \cite{hu2020open}. In particular, we use the mean absolute error (MAE), the accuracy (acc), the area under the receiver operating curve (ROC-AUC), and the average precision (AP).


\section{Results and discussion}


\subsection{Directional aggregation}

Using the benchmarks introduced in section \ref{sec:implementation}, we present in figure \ref{fig:results_table} a fair comparison of various aggregation strategies using the same parameter budget and hyperparameters. We see a consistent boost in the performance for \textit{simple}, \textit{complex} and \textit{complex with edges} models using directional aggregators compared to the \textit{mean-aggregator} baseline.


With our theoretical analysis in mind, we expected to perform well on PATTERN since the flow of the first eigenvectors are meaningful directions in a stochastic block model (i.e., these eigenvectors tend to correlate with community membership).
The results match our expectations, outperforming all the previous models.


In particular, we see a significant improvement in the molecular datasets (ZINC, MolHIV and MolPCBA) when using the directional aggregators, especially for the derivative aggregation  (noted \textit{dx\textsubscript{1}} in figure \ref{fig:results_table}). 
We believe this is due to the capacity to efficiently move  messages  across opposite parts of the molecule and to better understand the role of atom pairs. We further believe that the derivative aggregator is better able to capture high-frequency directional signals, similarly to the Gabor filters in computer vision.

Further, the thesis that DGNs can bridge the gap between CNNs and GNNs is supported by the clear improvements on CIFAR10 over the baselines. 

In the work by \cite{dwivedi2020benchmarking}, they proposed the use of positional encoding of the eigenvectors. However, our experiments with the positional encoding of the first 2 non-trivial eigenvectors, noted \textit{pos\textsubscript{1}, pos\textsubscript{2}} in figure \ref{fig:results_table}, showed no clear improvement on most datasets. In fact, Dwivedi et al. noted that many eigenvectors and high network depths are required for improvements, yet we outperform their results with fewer parameters, less depth, and only 1-2 eigenvectors, further motivating their use as directional flows instead of positional encoding.




\subsection{Comparison to the literature}

In order to compare our model with the literature, we fine-tuned it on the various datasets and we report its performance in figure \ref{fig:resultscomparison}. We observe that DGN provides significant improvement across all benchmarks, highlighting the importance of anisotropic kernels that are dependant on the graph topology.

Note that the results in Figure \ref{fig:resultscomparison} are better those in Figure \ref{fig:results_table} since the latter uses a more exhaustive parameter search, and uses the \textit{min/max} aggregators proposed in PNA \cite{corso2020principal} alongside the directional aggregators.


\begin{figure*}[ht]
\centering
\includegraphics[height=4.3cm]{results_table.pdf}
\vspace{-8pt}
\caption{Test set results using a parameter budget of  with the same hyperparameters as \cite{corso2020principal}, except MolPCBA with a budget of . The low-frequency Laplacian eigenvectors are used to define the directions, except for CIFAR10 that uses the coordinates of the image. For brevity, we denote \textit{dx}\textsubscript{i} and \textit{av}\textsubscript{i} as the directional derivative  and smoothing  aggregators of the -th direction. We also denote \textit{pos}\textsubscript{i} as the -th eigenvector used as positional encoding for the mean aggregator.}
\label{fig:results_table}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[height=4.3cm]{comparison_table.pdf}
\vspace{-8pt}
\caption{Fine-tuned results of the DGN model against models from \cite{dwivedi2020benchmarking} and \cite{hu2020open}: GCN \cite{kipf2016gcn}, GraphSage \cite{hamilton2017inductive}, GIN \cite{xu2018gin}, GAT \cite{velikovic2017gat}, MoNet \cite{monti2017moNet}, GatedGCN \cite{bresson2017gatedGCN} and PNA \cite{corso2020principal}. All the models use  parameters, except those with * who use  to . In ZINC the DGN aggregators are \textit{\{mean, dx, max, min\}}, in PATTERN \textit{\{mean, dx, av\}}, in CIFAR10 \textit{\{mean, dx, dx, max\}}, in MolHIV \textit{\{mean, dx, av, max, min\}}, in MolPCBA \textit{\{mean, sum, max, dx\}}. Mean and uncertainty are taken over 4 runs for ZINC, PATTERN and CIFAR10 and 10 runs for MolHIV and MolPCBA.
}
\label{fig:resultscomparison}
\end{figure*}



\subsection{Preliminary results of data augmentation}

To evaluate the effectiveness of the proposed augmentation, we trained the models on a reduced version of the CIFAR10 dataset. 
The results in figure \ref{fig:results_augmentation} show clearly a higher expressive power of the \textit{dx} aggregator, enabling it to fit well the training data. For a small dataset, this comes at the cost of overfitting and a reduced test-set performance, but we observe that randomly rotating or distorting the kernels counteracts the overfitting and improves the generalization. 

As expected, the performance decreases when the rotation or distortion is too high since the augmented graph changes too much. In computer vision images similar to CIFAR10 are usually rotated by less than  \cite{shorten_survey_2019, ogara_comparing_2019}. Further, due to the constant number of parameters across models, less parameters are attributed to the mean aggregation in the directional models, thus it cannot fit well the data when the rotation/distortion is too strong since the directions are less informative. We expect large models to perform better at high angles.


\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{augmentation_cifar.pdf}
\vspace{-18pt}
\caption{ Accuracy of the various models using data augmentation with a \textit{complex} architecture of  parameters and trained on 10\% of the CIFAR10 training set (4.5k images). An angle of  corresponds to a rotation of the kernel by a random angle sampled uniformly in  using definition \ref{def:field_rotation} with  being the gradient of the horizontal/vertical coordinates. A noise of  corresponds to a distortion of each eigenvector with a random noise uniformly sampled in  where  is the average absolute value of the eigenvector's components. The \textit{mean} baseline model is not affected by the augmentation since it does not use the underlining vector field. }
\label{fig:results_augmentation}
\end{figure*}





\section{Conclusion}

The proposed DGN method allows to address many problems of GNNs, including the lack of anisotropy, the low expressiveness, the over-smoothing and over-squashing. For the first time in graph networks, we generalize the directional properties of CNNs and their data augmentation capabilities. Based on the intuitive idea that the low-frequency eigenvectors of the graph Laplacian gives an interpretable directional flow, we backed our work by a set of strong theoretical results showing that these eigenvectors are important in connecting nodes that are far away and improving the expressiveness in regards to the WL-test. 

The work being also supported by strong empirical results, we believe it will give rise to a new family of directional GNNs. In fact, we introduce in the appendix different avenues for future work, including the hardening of the aggregators \ref{app:agg:hardening}, the introduction of a zero-padding at the boundaries \ref{app:agg:zero-padding}, the implementation of radius- kernels \ref{app:radius-r-kernel}, and the full study of directional data augmentation. Future methods could also improve the choice of multiple directions beyond the selection of the \textit{k-lowest} frequencies.

\xhdr{Broader Impact}
This work will extend the usability of graph networks to all problems with engineering and physically defined directions, thus making GNN a new laboratory for signal processing, physics, material science and molecular and cell biology.
In fact, the anisotropy present in a wide variety of systems could be expressed as vector fields (spinor, tensor) compatible with the DGN framework, without the need of eigenvectors. One example is magnetic anisotropicity in metals, alloys and organic molecules that is dependant on the relative orientation to the magnetic field.
Other examples are the response of materials to high electromagnetic fields; all kind of field propagation in crystals lattices (vibrations, heat, shear and frictional force, young modulus, light refraction, birefringence); multi-body or liquid motion; magnons and solitons in different media, fracture propagation, traffic modelling; developmental biology and embryology, and design of novel materials and constrained structures. Finally applications based on neural operators for ODE/PDE may benefit as well.










\bibliography{citations_arxiv}
\bibliographystyle{plain}

\clearpage
\newpage
\onecolumn

\appendix

\title{Directional Graph Networks}
\renewcommand{\undertitle}{
Anisotropic aggregation in graph neural networks via directional vector fields
}



\section{Appendix - Choices of directional aggregators}
\label{app:agg_choices}

This appendix helps understand the choice of  and  in section \ref{sec:B_av-B_dx} and presents different directional aggregators that can be used as an alternative to the ones proposed. 

A simple alternative to the directional smoothing and directional derivative operator is to simply take the \textit{forward/backward} values according to the underlying positive/negative parts of the field , since it can effectively replicate them. However, there are many advantage of using . First, one can decide to use either of them and still have an interpretable aggregation with half the parameters. Then, we also notice that  regularize the parameter by forcing the network to take both forward and backward neighbours into account at each time, and avoids one of the neighbours becoming too important. Lastly, they are robust to a change of sign of the eigenvectors since  is sign invariant and  will only change the sign of the results, which is not the case for \textit{forward/backward} aggregations.


\subsection{Retrieving the mean and Laplacian aggregations}
\label{app:simple_agg_examples}
It is interesting to note that we can recover simple aggregators from the aggregation matrices  and . Let  be a vector field such that all edges are equally weighted  for all edges . Then, the aggregator  is equivalent to a mean aggregation:

Under the condition , the differential aggregator is equivalent to a Laplacian operator  normalized using the degree 


\subsection{Global field normalization}

The proposed aggregators are defined with a row-wise normalized field 

meaning that all the vectors are of unit-norm and the aggregation/message passing is done only according to the direction of the vectors, not their amplitude. However, it is also possible to do a global normalization of the field  by taking a matrix-norm instead of a vector-norm. Doing so will modulate the aggregation by the amplitude of the field at each node. One needs to be careful since a global normalization might be very sensitive to the number of nodes in the graph.


\subsection{Center-balanced aggregators}

A problem arises in the aggregators  and  proposed in equations \ref{eq:Bav_simple} and \ref{eq:Bdx_simple} when there is an imbalance between the positive and negative terms of . In that case, one of the directions overtakes the other in terms of associated weights.

An alternative is also to normalize the forward and backward directions separately, to avoid having either the backward or forward direction dominating the message.



The same idea can be applied to the derivative aggregator \eqref{eq:Bdx_center} where the positive and negative parts of the field  are normalized separately to allow to project both the \textit{forward} and \textit{backward} messages into a vector field of unit-norm.  is the out-going field at each node and is used for the \textit{forward} direction, while  is the in-going field used for the \textit{backward} direction. By averaging the \textit{forward} and \textit{backward} derivatives, the proposed matrix  represents the centered derivative matrix.






\subsection{Hardening the aggregators}
\label{app:agg:hardening}

The aggregation matrices that we proposed, mainly  and  depend on a smooth vector field . At any given node, the aggregation will take a weighted sum of the neighbours in relation to the direction of . Hence, if the field  at a node  is \textit{diagonal} in the sense that it gives a non-zero weight to many neighbours, then the aggregator will compute a weighted average of the neighbours. 

Although there are clearly good reasons to have this weighted-average behaviour, it is not necessarily desired in every problem. For example, if we want to move a single node across the graph, this behaviour will smooth the node at every step. Instead, we propose below to soften and harden the aggregations by forcing the field into making a decision on the direction it takes.

\paragraph{Soft hardening the aggregation} is possible by using a softmax with a temperature  on each row to obtain the field .


\paragraph{Hardening the aggregation} is possible by using an infinite temperature, which changes the softmax functions into argmax. In this specific case, the node with the highest component of the field will be copied, while all other nodes will be ignored.



An alternative to the aggregators above is to take the \textit{softmin/argmin} of the negative part and the \textit{softmax/argmax} of the positive part.

\subsection{Forward and backward copy}

The aggregation matrices  and  have the nice property that if the field is flipped (change of sign), the aggregation gives the same result, except for the sign of . However, there are cases where we want to propagate information in the forward direction of the field, without smoothing it with the backward direction. In this case, we can define the strictly forward and strictly backward fields below, and use them directly with the aggregation matrices.



Further, we can use the hardened fields in order to define a forward copy and backward copy, which will simply copy the node in the direction of the highest field component.




\subsection{Phantom zero-padding}
\label{app:agg:zero-padding}

Some recent work in computer vision has shown the importance of zero-padding to improve CNNs by allowing the network to understand it's position relative to the border \cite{islam_how_2020}. In contrast, using boundary conditions or reflection padding makes the network completely blind to positional information. In this section, we show that we can mimic the zero-padding in the direction of the field  for both aggregation matrices  and .

Starting with the  matrix, in the case of a missing neighbour in the forward/backward direction, the matrix will compensate by adding more weights to the other direction, due to the denominator which performs a normalization. Instead, we would need the matrix to consider both directions separately so that a missing direction would result in zero padding. Hence, we define  below, where either the  or  will be 0 on a boundary with strictly in-going/out-going field.



Following the same argument, we define  below, where either the forward or backward term is ignored. The diagonal term is also removed at the boundary so that the result is a center derivative equal to the subtraction of the forward term with the 0-term on the back (or vice-versa), instead of a forward derivative.




\subsection{Extending the radius of the aggregation kernel}
\label{app:radius-r-kernel}

We aim at providing a general radius- kernel  that assigns different weights to different subsets of nodes  at a distance  from the center node .

First, we decompose the matrix  into positive and negative parts  representing the forward and backward steps aggregation in the field . 


Thus, defining , we can find different aggregation matrices by using different combinations of walks of radius . First demonstrated for a grid in theorem \ref{th:general_grid_radius2}, we generalize it in \eqref{eq:radius-kernel} for any graph .

\begin{definition}[General radius  n-directional kernel]
\label{def:radius}
Let  be the group of permutations over  elements with a set of directional fields .

\end{definition}

In this equation,  is the number of directional fields and  is the desired radius.  represents all the choices of walk  in the direction of the fields . For example,  has a radius , with 3 steps \textit{forward} of , 1 step \textit{forward} of , and 2 steps \textit{backward} of . The sign of each  is dependant to the sign of , and the power  is the number of aggregation steps in the directional field . The full equation is thus the combination of all possible choices of paths across the set of fields , with all possible permutations. Note that we are restricting the sum to  having only a possible sign; although matrices don't commute, we avoid choosing different signs since it will likely self-intersect a lower radius walk. The permutations  are required since, for example, the path \textit{up  left} is different (in a general graph) than the path \textit{left  up}.

This matrix  has a total of  parameters, with a high redundancy since some permutations might be very similar, e.g. for a grid graph we have that \textit{up  left} is identical to \textit{left  up}. Hence, we can replace the permutation  by a reverse ordering, meaning that . Doing so does not perfectly generalize the radius- kernel for all graphs, but it generalizes it on a grid and significantly reduces the number of parameters to .


\subsection{Arcsine of the eigenvectors}
Since the eigenvectors  are equivalent to the Fourier basis and represent the waves in the graphs, then it is expected that they behave similarity to sine/cosine waves when the graph is similar to a grid. This is further highlighted by the proof that the eigenvectors of a grid are all sines/cosines in appendix \ref{app:proof:cosine_eigvec}.

Hence, when we define the field  as , we must realize that the gradient will be lower near the minima/maxima of the eigenvector, as it is the case with sine/cosine waves. In the paper, we cope with this problem by dividing by the norm of the field  in equations \ref{eq:Bav_simple} and \ref{eq:Bdx_simple}.

Another solution is to use the arcsine of the eigenvectors so that the function eigenvectors become similar to triangle functions and the gradient is almost uniform. However, since the arcsine function works only in the range , then we must first normalize the eigenvector by it's maximum, as given by equation \ref{eq:F_asin}.





\section{Appendix - Implementation details}

\subsection{Benchmarks and datasets}
\label{app:benchmarks}
We use a variety of benchmarks proposed by \cite{dwivedi2020benchmarking} and \cite{hu2020open} to test the empirical performance of our proposed methods. In particular, to have a wide variety of graphs and tasks we chose: 
\begin{enumerate}
    \item ZINC, a graph regression dataset from molecular chemistry. The task is to predict a score that is a subtraction of computed properties , with  being the computed octanol-water partition coefficient, and  being the synthetic accessibility score \cite{jin_junction_2018}.
    \item CIFAR10, a graph classification dataset from computer vision \cite{krizhevsky_CIFAR10}. The task is to classify the images into 10 different classes, with a total of 5000 training image per class and 1000 test image per class. Each image has  pixels, but the pixels have been clustered into a graph of  super-pixels. Each super-pixel becomes a node in an \textit{almost} grid-shaped graph, with 8 edges per node. The clustering uses the code from \cite{knyazev2019understanding}, and results in a different number of super-pixels per graph. 
    \item PATTERN, a node classification synthetic benchmark generated with Stochastic Block Models, which are widely used to model communities in social networks. The task is to classify the nodes into 2 communities and it tests the fundamental ability of recognizing specific predetermined subgraphs.
    \item MolHIV, a graph classification benchmark from molecular chemistry. The task is to predict whether a molecule inhibits HIV virus replication or not. The molecules in the training, validation and test sets are divided using a scaffold splitting procedure that splits the molecules based on their two-dimensional structural frameworks.
    \item MolPCBA, a graph classification benchmark from molecular chemistry. It consists of measured biological activities of small molecules generated by high-throughput screening. The dataset consists of a total of 437,929 molecules divided using a scaffold slitting procedure and a set of 128 properties to predict for each.
\end{enumerate}

For the results in figure \ref{fig:results_table}, our goal is to provide a fair comparison to demonstrate the capacity of our proposed aggregators. Therefore, we compare the various methods on both types of architectures using the same hyperparameters tuned in previous works \cite{corso2020principal} for similar networks. The models vary exclusively in the aggregation method and the width of the architectures to keep a set parameter budget. Following the indication of the benchmarks' authors, we averaged the performances of the models on 4 runs with different initialization seeds for the benchmarks from \cite{dwivedi2020benchmarking} (ZINC, PATTERN and CIFAR10) and 10 runs for the ones from \cite{hu2020open} (MolHIV and MolPCBA\footnote{For MolPCBA, due to the computational cost of running models in the large dataset and the relatively low variance, we only used 1 run for the results in figure \ref{fig:results_table}, but 10 runs in those for figure \ref{fig:resultscomparison}}).

For the results in figure \ref{fig:resultscomparison}, we took the fine tuned results of other models from the corresponding public leaderboards by \cite{dwivedi2020benchmarking} and \cite{hu2020open}. For the DGN results we fine tuned the model taking the lowest validation loss across runs with the following hyperparameters (you can also find the fine tuned commands in the documentation of the \href{https://github.com/Saro00/DGN}{code repository}):

\begin{enumerate}
    \item ZINC: weight decay , aggregators , , , ,  

    \item CIFAR10: weight decay , dropout , aggregators , , , , 

    \item PATTERN: weight decay , architecture , aggregators , ,  

    \item MolHIV: aggregators , , , , , , dropout , L 
    
    \item for MolPCBA, given we did not start from any previously tuned architecture, we performed a line search with the following hyperparameters:  mix of aggregators , dropout , L , weight decay , batch size , learning rate , learning rate patience , learning rate reduce factor , architecture type , edge features dimension 

\end{enumerate}

In CIFAR10 it is impossible to numerically compute a deterministic vector field with eigenvectors due to the multiplicity of  being greater than 1. This is caused by the symmetry of the square image, and is extremely rare in real-world graphs. Therefore, we used as underlying vector field the gradient of the coordinates of the image. Note that these directions are provided in the nodes' features in the dataset and available to all models, that they are co-linear to the eigenvectors of the grid as per lemma \ref{lemma:cosine_eigvec}, and that they mimic the inductive bias in CNNs.

\subsection{Implementation and computational complexity}
\label{app:computation-complexity}
Unlike several more expressive graph networks \cite{kondor2018covariant, maron2018invariant}, our method does not require a computational complexity superlinear with the size of the graph. The calculation of the first  eigenvectors during pretraining, done using Lanczos method \cite{lanczos1950iteration} and the sparse module of Scipy, has a time complexity of  where  is the number of edges. During training the complexity is equivalent to a -aggregator GNN  \cite{corso2020principal} for the aggregation and  for the MLP.


To all the architectures we added residual connections \cite{he2016deep}, batch normalization \cite{ioffe2015batch} and graph size normalization \cite{dwivedi2020benchmarking}.

For some of the datasets with non-regular graphs, we combine the various aggregators with logarithmic degree-scalers as in \cite{corso2020principal}.

An important thing to note is that, for dynamic graphs, the eigenvectors need to be re-computed dynamically with the changing edges. Fortunately, there are random walk based algorithms that can estimate  quickly, especially for small changes to the graph \cite{doshi_fiedler_2020}. In the current empirical results, we do not work with dynamic graphs.

To evaluate the difficulty of computing the eigenvectors on very large graphs, we decided to load the COLLAB dataset comprising of a single graph with 235k nodes and 2.35M edges \cite{dwivedi2020benchmarking}. Computing it's first 6 eigenvectors using the scipy \textit{eigsh} function with machine precision took 25.5 minutes on an Intel\textregistered \space Xeon\textregistered \space CPU @ 2.20GHz. This is acceptable, knowing that a general training time can take hours, and that the result can be cached and reused during debugging and hyper-parameter optimization.

\subsection{Running time} \label{app:running_time}

The precomputation of the first four eigenvectors for all the graphs in the datasets takes  for ZINC,  for PATTERN and  for MolHIV on CPU. Table \ref{tab:running_time} shows the average running time on GPU for all the various model from figure \ref{fig:results_table}. On average, the epoch running time is 15\% slower for the DGN compared to the mean aggregation, but a faster convergence for DGN means that the total training time is on average 2\% faster for DGN.

\begin{table}[h]
\caption{Average running time for the non-fine tuned models from figure \ref{fig:results_table}. Each entry represents average time per epoch / average total training time. For the first four datasets, each of the models has a parameter budget  and was run on a Tesla T4 (15GB GPU). The \textit{avg increase} row is the average of the relative running time of all rows compared to the \textit{mean} row, with a negative value meaning a faster running time.  }
\label{tab:running_time}
\begin{center}
\begin{tabular}{c|ccc|cc}
\hline
                     & \multicolumn{3}{c}{\textbf{ZINC}}                       & \multicolumn{2}{|c}{\textbf{PATTERN}} \\ \hline
\textbf{Aggregators} & \textbf{Simple} & \textbf{Complex} & \textbf{Complex-E} & \textbf{Simple}  & \textbf{Complex}  \\ \hline
mean                 & 3.29s/1505s     & 3.58s/1584s      & 3.56s/1654s        & 153.1s/10154s    & 117.8s/9031s      \\
mean dx             & 3.86s/1122s     & 3.77s/1278s      & 4.22s/1371s        & 144.9s/8109s     & 127.2s/8417s      \\
mean dx dx         & 4.23s/1360s     & 4.55s/1560s      & 4.63s/1680s        & 153.3s/8057s     & 167.9s/9326s      \\
mean av             & 3.68s/1297s     & 3.84s/1398s      & 3.92s/1272s        & 128.0s/8680s     & 88.1s/7456s       \\
mean av av         & 3.95s/1432s     & 4.03s/1596s      & 4.07s/1721s        & 134.2s/8115s     & 170.4s/11114s     \\
mean dx av         & 3.89s/1079s     & 4.09s/1242s      & 4.58s/1510s        & 118.6s/6221s     & 144.2s/9112s      \\
\hline
avg increase         & +19\%/-16\%     & +13\%/-11\%      & +20\%/-9\%        & -11\%/-23\%     & +18\%/+1\%      \\
\hline
\end{tabular}

\vspace{10pt}

\begin{tabular}{c|cc|c|cc}
\hline
                     & \multicolumn{2}{c|}{\textbf{CIFAR10}} & \textbf{MolHIV} & \multicolumn{2}{c}{\textbf{MolPCBA}}\\ \hline
\textbf{Aggregators} & \textbf{Simple}  & \textbf{Complex}  & \textbf{Simple} & \textbf{Complex} & \textbf{Complex-E} \\ \hline
mean                 & 83.6s/10526s     & 78.7s/10900s      & 11.4s/2189s     & 279s/30128s & 356s/38126s \\
mean dx             &                  &                   & 12.6s/2348s    & 304s/34129s & 461s/43419s \\
mean dx dx         & 98.4s/8405s      & 100.9s/5191s      & 14.1s/2345s    & 314s/36581s & 334s/38363s \\
mean av             &                  &                   & 12.2s/2177s    & 297s/30316s & 436s/54545s  \\
mean av av         & 117.1s/12834s    & 89.5s/14481s      & 13.9s/2150s    & 315s/42297s & 333s/36641s \\
mean dx av         &                  &                   & 14.0s/2070s    & 326s/37523s & 461s/59109s \\ 
\hline
avg increase         & +29\%/+1\%     & +21\%/-10\%      & +17\%/+1\%  & +12\%/+20\%  &  +14\%/+22\%\\
\hline
\end{tabular}

\end{center}
\end{table}

\subsection{Eigenvector multiplicity} \label{app:eig_mult}

The possibility to define equivariant directions using the low-frequency Laplacian eigenvectors is subject to the uniqueness of those vectors.
When the dimension of the eigenspaces associated with the lowest eigenvalues is , the eigenvectors are defined up to a constant factor. In section \ref{sec:grad_eig}, we propose the use of unit vector normalization and an absolute value to eliminate the scale and sign ambiguity. When the dimension of those eigenspaces is greater than , it is not possible to define equivariant directions using the eigenvectors. 

Fortunately, it is very rare for the Laplacian matrix to have repeated eigenvalues in real-world datasets.
We validate this claim by looking at ZINC and PATTERN datasets where we found no graphs with repeated Fiedler vector and only one graph out of 26k with multiplicity of the second eigenvector greater than 1.

When facing a graph that presents repeated Laplacian eigenvalues, we propose to randomly shuffle, during training time, different eigenvectors randomly sampled in the eigenspace. This technique will act as a data augmentation of the graph during training time allowing the network to train with multiple directions at the same time. 


\section{Appendix - Mathematical proofs}

\subsection{Proof for theorem \ref{th:dir_smooth} (\nameref{th:dir_smooth})}
\label{app:proof:dir_smooth}
\Paste{th:dir_smooth}

\begin{proof}
This should be a simple proof, that if we want a weighted average of our neighbours, we simply need to multiply the weights by each neighbour, and divide by the sum of the weights. Of course, the weights should be positive.

\end{proof}


\subsection{Proof for theorem \ref{th:dir_dx} (\nameref{th:dir_dx})}
\label{app:proof:dir_dx}
\Paste{th:dir_dx}

\begin{proof}
Since  rows have unit  norm, . The -th coordinate of the vector  is 


\end{proof}









































\subsection{Proof of theorem \ref{th:reduced-diffusion-distance} (\nameref{th:reduced-diffusion-distance})}
\label{app:proof:reduced-diffusion-distance}

\Paste{th:reduced-diffusion-distance}


Recall that  is the discrete heat kernel at step ,  is the continuous heat kernel at time .
In \cite{barlow_2017}, it is shown that the continuous heat kernel is computed by 
.
Following \cite{COIFMAN20065} we can diagonalise  to get the identity

The inequality  is equivalent to

The term on the left is bounded above by 

and this last term is in turn bounded above by

Inequality \ref{eq:inegalitÃ©-distance-diffusion} will then hold if

and this is equivalent to

if we take  to be larger than the term on the left the inequality we get .
\paragraph{}
The constant  in the statement is the constant on the left side of the inequality. It is also interesting to note that  is expected to be positive since the term  is negative and the argument of the  will most likely be .

\begin{comment}
We need the following inequalities that follow directly from equation \ref{eq:diffusion-distance}

The first inequality follows from the fact that we assume the  to be normalized and the second one from the ordering of the eigenvalues.
    
If we can show 

the proof will be done but it is equivalent to 

\end{comment}
\begin{comment}
As a reminder, the definition of a gradient step is given in the definition \ref{def:gradient-step}, copied below.

\Paste{def:gradient-step}

In  \cite{chung2000greenfunction}, it is shown the hitting time  is given by the equation

With  and  being the -th eigenvalues and eigenvectors of the symmetric normalized Laplacian ,  the sum of the degrees of all nodes,  the degree of node  and  Green's function for the graph


Since the sign of the eigenvector is not deterministic, the choice  is used to simplify the argument without having to consider the change in sign.

Supposing , the first term of the sum of  has much more weight than the following terms. With  obtained from  by taking a step in the direction of the gradient of  we have


We want to show that the following inequality holds

this is equivalent to the following inequality

By the hypothesis , we can approximate  so the last inequality is equivalent to

Removing all equal terms from both sides, the inequality is equivalent to

But showing this last inequality is not easy. We know that  and from the choice of  being a step in the direction of , we know it is less likely to be on the border of the graph so we believe . Thus we also believe that the conjecture should hold in general.

We believe this should be true even without the assumption on  and  and for more eigenvectors than .
\end{comment}

\subsection{Proof for Lemma \ref{lemma:cosine_eigvec} (\nameref{lemma:cosine_eigvec})}
\label{app:proof:cosine_eigvec}

Consider the lattice graph  of size , that has vertices  and the vertices  and  are connected by an edge iff  for one index  and  for all other indices. Note that there are no diagonal edges in the lattice. The eigenvector of the Laplacian of the grid  are given by .

\begin{lemma}[Cosine eigenvectors]
\label{lemma:cosine_eigvec}
    \Copy{lemma:cosine_eigvec}{
    The Laplacian of  has an eigenvalue  with the associated eigenvector  that depends only the variable in the -th dimension and is constant in all others, with ,
    and 
}
\end{lemma}


\begin{proof}
First, recall the well known result that the path graph on  vertices  has eigenvalues 

with associated eigenvector  with -th coordinate


The Cartesian product of two graphs  and  is defined as  with  and  iff either  and  or  and . It is shown in \cite{fiedler1973algebraic} that if  and  are the eigenvalues of  and  respectively, then the eigenvalues of the Cartesian product graph  are  for all possible eigenvalues  and . Also, the eigenvectors associated to the eigenvalue  are  with  an eigenvector of the Laplacian of  associated to the eigenvalue  and  an eigenvector of the Laplacian of  associated to the eigenvalue . 

Finally, noticing that a lattice of shape  is really the Cartesian product of path graphs of length  up to , we conclude that there are eigenvalues . Denoting by  the vector in  with only ones as coordinates, then the eigenvector associated to the eigenvalue  is 

where  is the eigenvector of the Laplacian of  associated to its first non-zero eigenvalue. .
\end{proof}


\subsection{Radius 1 convolution kernels in a grid}
In this section we show any radius 1 convolution kernel can be obtained as a linear combination of the  and  matrices for the right choice of Laplacian eigenvectors . First we show this can be done for 1-d convolution kernels.


\begin{theorem}\label{th:1D_convolution_kernel}\Copy{th:1D_convolution_kernel}{
On a path graph, any 1D convolution kernel of size 3  is a linear combination of the aggregators  and the identity .}
\end{theorem}

\begin{proof}
Recall from the previous proof that the first non zero eigenvalue of the path graph  has associated eigenvector . Since this is a monotone decreasing function in , the -th row of  will be 

with  and . We are trying to solve 

with  in positions  and . This simplifies to solving

with , which always has a solution because . 
\end{proof}

\begin{theorem}[Generalization radius-1 convolutional kernel in a grid]
\label{th:general_grid_radius1}
    \Copy{th:general_grid_radius1}{
    Let  be the -dimensional lattice as above and let  be the eigenvectors of the Laplacian of the lattice as in theorem \ref{lemma:cosine_eigvec}. Then any radius 1 kernel  on  is a linear combination of the aggregators ) and .
}
\end{theorem}

\begin{proof}
This is a direct consequence of \ref{th:1D_convolution_kernel} obtained by adding  1-dimensional kernels, with each kernel being in a different axis of the grid as per Lemma \ref{lemma:cosine_eigvec}. See figure \ref{fig:cnn-gnn-generalization} for a visual example in 2D. 

\end{proof}


\subsection{Proof for theorem \ref{th:general_grid_radius2} (\nameref{th:general_grid_radius2})}
\label{app:proof:general_grid_radius2}
\Paste{th:general_grid_radius2}

\begin{proof}
For clarity, we first do the 2 dimensional case for a radius 2, then extended to the general case. Let  be the radius 2 kernel on a grid represented by the matrix

since we supposed the  grid was such that , by theorem \ref{lemma:cosine_eigvec}, we have that  is depending only in the first variable  and is monotone in . Recall from \ref{lemma:cosine_eigvec} that 

The vector  will be denoted by  in the rest. Notice all entries of  are  or . Denote by  the gradient vector  where  is the eigenvector given by theorem \ref{lemma:cosine_eigvec} that is depending only in the second variable  and is monotone in  and recall

For a matrix , let  the positive/negative parts of , ie matrices with positive entries such that .
Let  be a matrix representing the radius 1 kernel with weights
 

The matrix  can be obtained by theorem \ref{th:general_grid_radius1}. Then the radius 2 kernel  is defined by all the possible combinations of 2 positive/negative steps, plus the initial radius-1 kernel.

with  the sign function  if  and  if . The matrix  then realises the kernel .

We can further extend the above construction to  dimension grids and radius  kernels 


with  , the eigenvector with lowest eigenvalue only dependent on the -th variable and given in theorem \ref{lemma:cosine_eigvec} and  is the matrix multiplication.  represents all the choices of walk  in the direction of the fields . For example,  has a radius , with 3 steps \textit{forward} of , 1 step \textit{forward} of , and 2 steps \textit{backward} of .

\end{proof}


\subsection{Proof for theorem \ref{th:wl-test} (\nameref{th:wl-test})}
\label{app:proof:wl-test}
\Paste{th:wl-test}

\begin{proof}

We will show that (1) DGNs are at least as powerful as the 1-WL test and (2) there is a pair of graphs which are not distinguishable by the 1-WL test which DGNs can discriminate. 

Since the DGNs include the mean aggregator combined with at least an injective degree-scaler, \cite{corso2020principal} show that the resulting architecture is at least as powerful as the 1-WL test.

\begin{figure}[ht]
\centering
 \includegraphics[width=0.6\textwidth]
 {weisfeiler-lehman.pdf}
 \caption{Illustration of an example pair of graphs which the 1-WL test cannot distinguish but DGNs can. The table shows the node feature updates done at every layer. MPNN with mean/sum aggregators and the 1-WL test only use the updates in the first row and therefore cannot distinguish between the nodes in the two graphs. DGNs also use directional aggregators that, with the vector field given by the first eigenvector of the Laplacian matrix, provides different updates to the nodes in the two graphs.}
\label{fig:wl}
\end{figure}

Then, to show that the DGNs are strictly more powerful than the 1-WL test it suffices to provide an example of a pair of graphs that DGNs can differentiate and 1-WL cannot. Such a pair of graphs is illustrated in figure \ref{fig:wl}. 

The 1-WL test (as any MPNN with, for example, sum aggregator) will always have the same features for all the nodes labelled with \textit{a} and for all the nodes labelled with \textit{b} and, therefore, will classify the graphs as isomorphic. DGNs, via the directional smoothing or directional derivative aggregators based on the first eigenvector of the Laplacian matrix, will update the features of the \textit{a} nodes differently in the two graphs (figure \ref{fig:wl} presents also the aggregation functions) and will, therefore, be capable of distinguishing them.

\end{proof}


\end{document}
