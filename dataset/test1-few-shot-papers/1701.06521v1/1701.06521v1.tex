\documentclass[11pt,twocolumn]{article}
\pdfoutput=1

\usepackage[utf8]{inputenc}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{acl2017}
\usepackage{times}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{esvect}
\usepackage{cleveref}
\crefformat{section}{\S#2#1#3}



\newcommand\given[1][]{\:#1\vert\:}
\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand{\checkme}[1]{\textcolor{red}{#1}}
\newcommand{\white}[1]{\textcolor{White}{#1}}
\newcommand{\whitebf}[1]{\textbf{\textcolor{White}{#1}}}
\newcommand{\red}[1]{\textcolor{Red}{#1}}
\newcommand{\redbf}[1]{\textbf{\textcolor{Red}{#1}}}
\newcommand{\blue}[1]{\textcolor{Blue}{#1}}
\newcommand{\bluebf}[1]{\textbf{\textcolor{Blue}{#1}}}
\newcommand{\green}[1]{\textcolor{OliveGreen}{#1}}
\newcommand{\greenbf}[1]{\textbf{\textcolor{OliveGreen}{#1}}}

\renewcommand{\labelenumi}{(\roman{enumi})}

\aclfinalcopy \def\aclpaperid{***} 

\title{Incorporating Global Visual Features into\\Attention-Based Neural Machine Translation}

\author{Iacer Calixto\\
	    ADAPT Centre\\
	    Dublin City University\\
	    Glasnevin, Dublin 9\\
	    {\tt\small iacer.calixto@adaptcentre.ie}
	  \And
	Qun Liu\\
	ADAPT Centre\\
  	Dublin City University\\
  	Glasnevin, Dublin 9\\
\And
	Nick Campbell\\
	ADAPT Centre\\
  	Trinity College Dublin\\
  	College Green, Dublin 2\\
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
 We introduce multi-modal, attention-based neural machine translation (NMT) models
 which incorporate visual features into different parts of both the encoder and the decoder.
 We utilise global image features extracted using a pre-trained convolutional neural network and incorporate them \textit{(i)} as words in the source sentence, \textit{(ii)} to initialise the encoder hidden state, and \textit{(iii)} as additional data to initialise the decoder hidden state.
In our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best.
 We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models.
We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated.
 To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.
\end{abstract}

\setlength{\belowdisplayskip}{1.0pt} \setlength{\belowdisplayshortskip}{1.0pt}
\setlength{\abovedisplayskip}{1.0pt} \setlength{\abovedisplayshortskip}{1.0pt}
\setlength{\intextsep}{1.5pt}

\section{Introduction}
\label{sec:intro}

Neural Machine Translation (NMT) has recently been proposed as an instantiation of the \emph{sequence to sequence} (\emph{seq2seq}) learning problem~\cite{KalchbrennerBlunsom2013,Choetal2014,SutskeverVinyalsLe2014}.
In this problem, each training example consists of one source and one target variable-length sequence, with no prior information regarding the alignments between the two.
A model is trained to \emph{translate} sequences in the source language into corresponding sequences in the target.
This framework has been successfully used in many different tasks, such as
handwritten text generation~\cite{Graves2013},
image description generation~\cite{Hodoshetal2013,Kirosetal2014b,Maoetal2014,Elliottetal2015,KarpathyFeiFei2015,Vinyalsetal2014},
machine translation~\cite{Choetal2014,SutskeverVinyalsLe2014}
and
video description generation~\cite{Donahueetal2015,Venugopalanetal2015}.

Recently, there has been an increase in the number of natural language generation models that explicitly use \emph{attention-based decoders}, i.e. decoders that model an \emph{intra-sequential} mapping between source and target representations.
For instance,~\newcite{Xuetal2015} proposed an attention-based model
for the task of image description generation where the model learns to \emph{attend to}
specific parts of an image (the source)
as it generates its description (the target).
In MT,
one can intuitively interpret this attention mechanism as inducing
an \emph{alignment} between source and target sentences,
as first proposed by~\newcite{BahdanauChoBengio2015}.
The common idea is to explicitly frame a learning task in which
the decoder learns to attend to the relevant parts of the source sequence
when generating each part of the target sequence.

We are inspired by recent successes in using attention-based models in both image description generation and NMT.
Our main goal in this work is to propose end-to-end multi-modal NMT models which effectively incorporate visual features in different parts of the attention-based NMT framework.
The main contributions of our work are:
\begin{itemize}
 \item We propose novel attention-based multi-modal NMT models which incorporate visual features into the encoder and the decoder.
 
 \item We discuss the impact that adding synthetic multi-modal and multilingual data brings to multi-modal NMT.
 
 \item We show that images bring useful information to an NMT model and report state-of-the-art results.
\end{itemize}

One additional contribution of our work is that
we corroborate previous findings
by~\newcite{Vinyalsetal2014} that suggested that
using image features directly as additional context to update the hidden state of the decoder
(at each time step) leads to overfitting, ultimately preventing learning.

The remainder of this paper is structured as follows.
In~\cref{sec:background} we briefly discuss relevant previous related work.
We then revise the attention-based NMT framework and further expand it into different multi-modal NMT models (\cref{sec:attention-based-neural-machine-translation}).
In~\cref{sec:dataset} we introduce the data sets we use in our experiments. In~\cref{sec:experiments} we detail the hyperparameters, parameter initialisation and other relevant details of our models.
Finally, in~\cref{sec:conclusion} we draw conclusions and provide some avenues for future work.


\subsection{Related work}
\label{sec:background}

Attention-based encoder-decoder models for MT have been actively investigated in recent years.
Some researchers have studied how to improve attention mechanisms~\cite{Luongetal2015,Tuetal2016} and how to train attention-based models to translate between many languages~\cite{Dongetal2015,Firatetal2016}.


There has been some previous related work on using images in tasks involving multilingual and multi-modal natural language generation.
\newcite{Calixtoetal2012} studied how the visual context of a textual description can be helpful in the disambiguation of Statistical MT (SMT) systems.
\newcite{Hitschleretal2016} used image features for re-ranking translations of image descriptions generated by an SMT model and reported significant improvements.
\newcite{Elliottetal2015} generated multilingual descriptions of images
by learning and transferring features between
two independent, non-attentive neural image description models.
\newcite{Luongetal2016} proposed a multi-task learning approach and incorporated neural image description as an auxiliary task to sequence-to-sequence NMT and improved translations in the main translation task.

Multi-modal MT has recently been addressed by the MT community in the form of a shared task~\cite{Speciaetal2016}.
We note that in the official results of this first shared task no submissions based on a purely neural architecture could improve on the phrase-based SMT (PBSMT) baseline.
Nevertheless, researchers have proposed to include global visual features in re-ranking -best lists generated by a PBSMT system or directly in a purely NMT framework with some success~\cite{Caglayanetal2016,CalixtoElliottFrank2016,Libovickyetal2016,Shahetal2016}.
The best results achieved by a purely NMT model in this shared task are those of \newcite{Huangetal2016}, who proposed to use global and regional image features extracted with the VGG19 network.

Similarly to one model we propose,\footnote{This idea has been developed independently by both research groups.} they extract global features for an image, project these features into the vector space of the source words and then add it as a word in the input sequence.
Their best model improves over a strong NMT baseline and is comparable to results obtained with a PBSMT model trained on the same data.
For that reason, their models are used as baselines in our experiments.
Next, we point out some key differences between their models and ours.

\paragraph{Architecture}
Their implementation is based on the attention-based model of~\newcite{Luongetal2015}, which has some differences to that of~\newcite{BahdanauChoBengio2015}, used in our work (\cref{sec:text-only-attention-based-neural-machine-translation}).
Their encoder is a single-layer unidirectional LSTM and they use the last hidden state of the encoder to initialise the decoder's hidden state, therefore indirectly using the image features to do so.
We use a bi-directional recurrent neural network (RNN) with GRU~\cite{Choetal2014b} as our encoder, better encoding the semantics of the source sentence.

\paragraph{Image features}
We include image features separately either as a word in the source sentence~(\cref{sec:image-as-word}) or \emph{directly} for encoder~(\cref{sec:image-encoder-init}) or decoder initialisation (\cref{sec:image-decoder-init}), whereas \newcite{Huangetal2016} only use it as a word.
We also
show it is better to include an image exclusively for the encoder \emph{or} the decoder initialisation (Tables~\ref{tbl:evaluation-translational-flickr30k} and~\ref{tbl:evaluation-backtranslated-flickr30k}).

\paragraph{Data}
\newcite{Huangetal2016} use object detections obtained with the RCNN of \newcite{Girshicketal2014} as additional data, whereas
we study the impact that additional back-translated data brings.

\paragraph{Performance}
All our models outperform \newcite{Huangetal2016}'s according to all metrics evaluated, even when they use additional object detections.
If we use additional back-translated data, the difference becomes even larger.

\section{Attention-based NMT}
\label{sec:attention-based-neural-machine-translation}

In this section, we briefly revise the attention-based NMT framework (\cref{sec:text-only-attention-based-neural-machine-translation}) and expand it into a multi-modal NMT framework (\cref{sec:multimodal-neural-machine-translation}).

\subsection{Text-only attention-based NMT}
\label{sec:text-only-attention-based-neural-machine-translation}

We follow the notation of~\newcite{BahdanauChoBengio2015}
and~\newcite{Firatetal2016} throughout this section.
Given a source sequence  and
its translation ,
an NMT model aims at building a single neural network that translates  into 
by directly learning to model .
Each  is a row index in a source lookup matrix

(the \emph{source word embeddings matrix})
and each  is an index in a target lookup matrix

(the \emph{target word embeddings matrix}).
 and  are source and target vocabularies
and  and  are source and target word embeddings dimensionalities,
respectively.

A bidirectional RNN with GRU is used as the encoder.
A forward RNN 
reads  word by word, from left to right,
and generates a sequence of \emph{forward annotation vectors}

at each encoder time step .
Similarly, a backward RNN 
reads 
from right to left,
word by word, and generates a sequence of \emph{backward annotation vectors}
, as in~(\ref{eq:bidirectional-rnn}):

The final annotation vector for a given time step  is the concatenation of forward and backward vectors .


In other words, each source sequence  is encoded into a sequence of annotation vectors , which are in turn used by the decoder: essentially a neural language model (LM)~\cite{Bengioetal2003} conditioned on the previously emitted words and the source sentence via an attention mechanism.

At each time step  of the decoder, we compute a \emph{time-dependent} context vector  based on the annotation vectors , the decoder's previous hidden state  and the target word  emitted by the decoder in the previous time step.\footnote{At training time, the correct previous target word  is known and therefore used instead of .
At test or inference time,  is not known and  is used instead.
\newcite{Bengioetal2015} discussed problems that may arise from this difference between training and inference distributions.
}



We follow~\newcite{BahdanauChoBengio2015}
and use a single-layer feed-forward network
to compute an \emph{expected alignment} 
between each source annotation vector 
and the target word to be emitted at the current time step , as in~(\ref{eq:expected-alignment}):


In Equation~(\ref{eq:normalised-expected-alignment}),
these expected alignments are further normalised and converted into
probabilities:


\noindent
where  are called the model's \emph{attention weights},
which are in turn used in computing the time-dependent context vector
.
Finally, the context vector  is used in computing the decoder's hidden state  for the current time step , as shown in Equation~(\ref{eq:decoder}):


\noindent
where  is the decoder's previous hidden state,
 is the embedding of the word emitted in the previous time step,
and  is the updated time-dependent context vector.
In Figure~\ref{fig:attention-mechanism}
we illustrate the computation of
the decoder's hidden state .


\begin{figure}[t!]
 \centering
 \includegraphics[width=0.35\textwidth]{img/attention-mechanism.png}
 \caption{Computation of the decoder's hidden state 
using the attention mechanism.}
\label{fig:attention-mechanism}
\end{figure}

We use a single-layer feed-forward neural network to initialise the decoder's hidden state  at time step  and feed it the concatenation of the last hidden states of the encoder's forward RNN  () and backward RNN (), as in~(\ref{eq:decoder-initial-state}):

\noindent
where  and  are model parameters.
Since RNNs normally better store information about recent inputs in comparison to more distant ones~\cite{HochreiterSchmidhuber1997,BahdanauChoBengio2015}, we expect to initialise the decoder's hidden state with a strong source sentence representation, i.e. a representation with a strong focus on both the first and the last tokens in the source sentence.



\subsection{Multi-modal NMT (MNMT)}
\label{sec:multimodal-neural-machine-translation}

Our models can be seen as
expansions of the attention-based NMT framework described in~\cref{sec:attention-based-neural-machine-translation} with
the addition of a \emph{visual component} to incorporate image features.

Simonyan and Zisserman~\shortcite{SimonyanZisserman2014} trained and evaluated an extensive set of deep convolutional neural network (CNN) models for classifying images into one out of the  classes in ImageNet~\cite{Russakovskyetal2014}.
We use their 19-layer VGG network (VGG) to extract image feature vectors for all images in our dataset.
We feed an image to the pre-trained VGG network and use the D activations of the penultimate fully-connected layer
FC\footnote{We use the activations of the FC layer, which encode information about the entire image, of the VGG network (configuration E) in~\newcite{SimonyanZisserman2014}'s paper.}
as our \emph{image feature vector}, henceforth referred to as .

We propose three different methods to incorporate images into the attentive NMT framework:
using an image as words in the source sentence (\Cref{sec:image-as-word}), using an image to initialise the source language encoder (\cref{sec:image-encoder-init}) and the target language decoder (\cref{sec:image-decoder-init}).

We also evaluated a fourth mechanism to incorporate images into NMT, namely to use an image as one of the different contexts available to the decoder at each time step of the decoding process.
We add the image features directly as an additional context,
in addition to ,  and ,
to compute the hidden state  of the decoder at a given time step .
We corroborate previous findings by~\newcite{Vinyalsetal2014} in that adding the image features as such causes the model to overfit, ultimately preventing learning.\footnote{For comparison, translations for the translated Multi30k test set (described in \cref{sec:dataset}) achieve just 3.8 BLEU~\cite{Papinenietal2002}, 15.5 METEOR~\cite{DenkowskiLavie2014} and 93.0 TER~\cite{Snoveretal2006}.}

\begin{comment}
\begin{figure}[t!]
 \centering
 \includegraphics[width=0.45\textwidth]{img/image-as-context_3.png}
 \caption{Using image features as an additional context
 at each time step  of the decoder.}
 \label{fig:image-as-context}
\end{figure}
\end{comment}

\subsubsection{Images as source words: \texorpdfstring{IMG}{}}
\label{sec:image-as-word}

One way we propose to incorporate images into the encoder is to project an image feature vector into the space of the words of the source sentence.
We use the projected image as the first and/or last word of the source sentence and let the attention model learn when to attend to the image representation.
Specifically, given the global image feature vector ,
we compute~(\ref{eq:image-projection}):

\noindent
where  and  are image transformation matrices,  and  are bias vectors, and  is the source words vector space dimensionality, all trained with the model.
We then directly use  as words in the source words vector space:
as the first word only (model IMG), and
as the first and last words of the source sentence (model IMG).

\begin{figure}[t!]
 \centering
 \includegraphics[width=0.32\textwidth]{img/image-as-word.png}
 \caption{An encoder bidirectional RNN that uses image features
 as words in the source sequence.}
 \label{fig:image-as-word}
\end{figure}

An illustration of this idea is given in Figure~\ref{fig:image-as-word}, where a source sentence that originally contained  tokens, after including the image as source words will contain
 tokens (model IMG) or
 tokens (model IMG).
In model IMG, the image is projected as the first source word only (solid line in Figure~\ref{fig:image-as-word});
 in model IMG, it is projected into the source words space as both first and last words (both solid and dashed lines in Figure~\ref{fig:image-as-word}).
 
Given a source sequence ,
we concatenate the transformed image vector  to  and apply the forward and backward encoder RNN passes, generating hidden vectors as in Figure~\ref{fig:image-as-word}.
When computing the context vector  (Equations~(\ref{eq:expected-alignment}) and~(\ref{eq:normalised-expected-alignment})), we effectively make use of the transformed image vector, i.e. the  attention weight parameters will use this information to attend or not to the image features.

By including images into the encoder in models
IMG and IMG, our intuition is that
\textit{(i)} by including the image as the \emph{first word}, we propagate image features into the source sentence vector representations when applying the forward RNN  (vectors ), and
\textit{(ii)} by including the image as the \emph{last word}, we propagate image features into the source sentence vector representations when applying the backward RNN  (vectors ).


\subsubsection{Images for encoder initialisation: \texorpdfstring{IMG}{}}
\label{sec:image-encoder-init}

In the original attention-based NMT model described in~\cref{sec:attention-based-neural-machine-translation}, the hidden state of the encoder is initialised with the zero vector .
Instead, we propose to use two new single-layer feed-forward neural networks to compute the initial states of the forward RNN  and the backward RNN , respectively, as illustrated in Figure~\ref{fig:image-encoder-init}.

Similarly to~\cref{sec:image-as-word}, given a global image feature vector , we compute a vector  using Equation~(\ref{eq:image-projection}), only this time the parameters  and  project the image features into the same dimensionality as the textual encoder hidden states.

\begin{figure}[t!]
 \centering
 \includegraphics[width=0.4\textwidth]{img/image-encoder-init.png}
 \caption{Using an image to initialise the encoder hidden states.}
 \label{fig:image-encoder-init}
\end{figure}

The feed-forward networks used to initialise the encoder hidden state are computed as in (\ref{eq:encoder-initial-state-multimodal}):

\noindent
where  and  are multi-modal projection matrices that project the image features  into the encoder forward and backward hidden states dimensionality, respectively, and  and  are bias vectors.



\subsubsection{Images for decoder initialisation: \texorpdfstring{IMG}{}}
\label{sec:image-decoder-init}

To incorporate an image into the decoder, we introduce a new single-layer feed-forward neural network to be used instead of the one described in Equation~\ref{eq:decoder-initial-state}.
Originally, the decoder's initial hidden state was computed using the concatenation of the last hidden states of the encoder forward RNN  () and backward RNN (), respectively  and .

\begin{figure}[t!]
 \centering
 \includegraphics[width=0.4\textwidth]{img/image-decoder-init.png}
 \caption{Image as additional data to initialise the decoder
 hidden state .}
 \label{fig:image-decoder-init}
\end{figure}

Our proposal is that we include the image features as additional input to initialise the decoder hidden state at time step , as in~(\ref{eq:decoder-initial-state-multimodal}):

\noindent
where  is a multi-modal projection matrix that projects the image features  into the decoder hidden state dimensionality and  and  are the same as in Equation~(\ref{eq:decoder-initial-state}).

Once again we compute  by applying Equation~(\ref{eq:image-projection}) onto a global image feature vector \mbox{},
only this time the parameters  and  project the image features into the same dimensionality as the decoder hidden states.
We illustrate this idea in Figure~\ref{fig:image-decoder-init}.


\section{Data set}
\label{sec:dataset}

Our multi-modal NMT models need bilingual sentences accompanied by one or more images as training data.
The original Flickr30k data set contains k images and  English sentence descriptions for each image~\cite{Youngetal2014}.
We use the translated and the comparable Multi30k datasets \cite{ElliottFrankSimaanSpecia2016}, henceforth referred to as M30k and M30k, respectively, which are multilingual expansions of the original Flickr30k.

For each of the 30k images in the Flickr30k, the M30k has one of its English descriptions manually translated into German by a professional translator. Training, validation and test sets contain 29k, 1014 and 1k images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation).
For each of the 30k images in the Flickr30k, the M30k has five descriptions in German collected independently of the English descriptions. Training, validation and test sets contain 29k, 1014 and 1k images, respectively, each accompanied by five sentences in English and five sentences in German.

We use the scripts in the Moses SMT Toolkit~\cite{Koehnetal2007} to normalise, truecase and tokenize English and German descriptions
and we also convert space-separated tokens into subwords~\cite{Sennrichetal2016}.
All models use a common vocabulary of 83,093 English and 91,141 German subword tokens.
If sentences in English or German are longer than 80 tokens, they are discarded.

We use the entire M30k
training set for training, its validation set for model selection with BLEU, and its test set to evaluate our models.
In order to study the impact that additional training data brings to the models,
we use the baseline model described in~\cref{sec:attention-based-neural-machine-translation} trained on the textual part of the M30k data set (GermanEnglish) without the images to build a back-translation model~\cite{Sennrichetal2016a}.
We back-translate the k German descriptions in the M30k into English and include the triples (synthetic English description, German description, image) as additional training data.

We train models to translate from English into German and report evaluation of cased, tokenized sentences with punctuation.

\section{Experimental setup}
\label{sec:experiments}

Our encoder is a bidirectional RNN with GRU
(one 1024D single-layer forward RNN and one 1024D single-layer backward RNN).
Source and target word embeddings are 620D each
and both are trained jointly with our model.
All non-recurrent matrices are initialised by sampling from
a Gaussian distribution ,
recurrent matrices are orthogonal and
bias vectors are all initialised to zero.
Our decoder RNN also uses GRU and is a neural LM~\cite{Bengioetal2003} conditioned on its previous emissions and the source sentence by means of the source attention mechanism.

Image features are obtained by feeding images to the pre-trained VGG network of Simonyan and Zisserman~\shortcite{SimonyanZisserman2014} and using the activations of the penultimate fully-connected layer FC.
We apply dropout with a probability of  in both source and target word embeddings and with a probability of  in the image features (in all MNMT models), in the encoder and decoder RNNs inputs and recurrent connections, and before the readout operation in the decoder RNN.
We follow~\newcite{Gal2015} and apply dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps.

Our models are trained using
stochastic gradient descent with Adadelta~\cite{Zeiler2012} and minibatches of size 40, where each training instance consists of one English sentence, one German sentence and one image.
We apply early stopping for model selection based on BLEU scores, so that if a model does not improve on BLEU in the validation set for more than 20 epochs, training is halted.

We evaluate our models' translation quality quantitatively in terms of BLEU~\cite{Papinenietal2002}, METEOR~\cite{DenkowskiLavie2014}, TER~\cite{Snoveretal2006}, and chrF3 scores\footnote{We specifically compute character 6-gram F3 scores.}~\cite{Popovic2015} and we report statistical significance for the three first metrics using approximate randomisation computed with \mbox{MultEval}~\cite{Clarketal2011}.

As our main baseline we train an attention-based NMT model (\cref{sec:attention-based-neural-machine-translation}) in which only the textual part of M30k is used for training.
We also train a PBSMT model built with Moses on the same data.
The LM is a 5--gram LM with modified Kneser-Ney smoothing~\cite{KneserNey1995} trained on the German side of the  M30k dataset.
We use minimum error rate training~\cite{Och2003} for tuning the model parameters for BLEU scores.
Our third baseline is the best comparable multi-modal model by \newcite{Huangetal2016} and also their best model with additional object detections: respectively models \texttt{m1} (image at head) and \texttt{m3} in the authors' paper.


\subsection{Results}
\label{sec:results}


\begin{table}[t!]
  \centering
  \resizebox{\linewidth}{!} {
  \begin{tabular}{lllll}
   \toprule
   & BLEU & METEOR & TER & chrF3 \\
   \toprule
   PBSMT &
   32.9 &
   \underline{54.1} &
   \underline{45.1} &
   \underline{67.4} \\
   
   NMT &
   \underline{33.7} &
   52.3 &
   46.7 &
   64.5 \\
   
   \midrule
   
Huang
   & 35.1 & 52.2 & --- & --- \\
   
   + RCNN
& 36.5 & 54.1 & --- & --- \\
   
   \midrule
   
   IMG &
   37.1 \small\greenbf{( 3.4)} &
   54.5\white{} \small\greenbf{( 0.4)} &
   42.7 \small\greenbf{( 2.4)} &
   66.9 \small\redbf{( 0.5)}\\
   
   IMG &
   36.9 \small\greenbf{( 3.2)} &
   54.3\white{} \small\greenbf{( 0.2)} &
   \textbf{41.9} \small\greenbf{( 3.2)} &
   66.8 \small\redbf{( 0.6)} \\
   
   IMG &
   37.1 \small\greenbf{( 3.4)} &
   55.0 \small\greenbf{( 0.9)} &
   43.1 \small\greenbf{( 2.0)} &
   67.6 \small\greenbf{( 0.2)}\\
   
   IMG &
   \textbf{37.3} \small\greenbf{( 3.6)} &
   \textbf{55.1} \small\greenbf{( 1.0)} &
   42.8 \small\greenbf{( 2.3)} &
   \textbf{67.7} \small\greenbf{( 0.3)}\\
   
   IMG &
   35.7 \small\greenbf{( 2.0)} &
   53.6\white{} \small\redbf{( 0.5)} &
   43.3 \small\greenbf{( 1.8)} &
   66.2 \small\redbf{( 1.2)}\\
   
   IMG &
   37.0 \small\greenbf{( 3.3)} &
   54.7\white{} \small\greenbf{( 0.6)} &
   42.6 \small\greenbf{( 2.5)} &
   67.2 \small\redbf{( 0.2)}\\
   
   \bottomrule
  \end{tabular}
  }
  \caption{
  BLEU, METEOR, chrF3 (higher is better) and TER scores (lower is better) on the M30k
  test set for the two text-only baselines PBSMT and NMT, the two multi-modal NMT models by \newcite{Huangetal2016}
  and our MNMT models that:
  \textit{(i)} use images as words in the source sentence (IMG, IMG),
  \textit{(ii)} use images to initialise the encoder (IMG), and
  \textit{(iii)} use images as additional data to initialise the decoder (IMG).
  Best text-only baselines are underscored and best overall results appear in bold.
  We highlight in parentheses the improvements brought by our models compared to the best corresponding text-only baseline score.
  Results differ significantly from PBSMT baseline () or NMT baseline () with .
  }
  \label{tbl:evaluation-translational-flickr30k}
\end{table}



The Multi30K dataset contains images and bilingual descriptions.
Overall, it is a small dataset with a small vocabulary whose sentences have simple syntactic structures and not much ambiguity~\cite{ElliottFrankSimaanSpecia2016}.
This is reflected in the fact that even the simplest baselines perform fairly well on it, i.e. the smallest BLEU score of 32.9 is that of the PBSMT model, which is still good for translating into German.

From Table~\ref{tbl:evaluation-translational-flickr30k} we see that our multi-modal models perform well, with models IMG and IMG improving on both baselines according to all metrics analysed.
We also note that all models but IMG perform consistently better than the strong multi-modal NMT baseline of \newcite{Huangetal2016}, even when this model has access to more data (+RCNN features).\footnote{In fact, model IMG still improves on the multi-modal baseline of \newcite{Huangetal2016} when trained on the same data.}
Combining image features in the encoder and the decoder at the same time (last two entries in Table~\ref{tbl:evaluation-translational-flickr30k}) does not seem to improve results compared to using the image features in only the encoder or the decoder.
To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model in all metrics on this data set.

Arguably, the main downside of applying multi-modal NMT in a real-world scenario is the small amount of publicly available training data (k), which restricts its applicability.
For that reason, we back-translated the German sentences in the M30k and created additional k synthetic triples (synthetic English sentence, original German sentence and image).

In Table~\ref{tbl:evaluation-backtranslated-flickr30k}, we present results for some of the models evaluated in Table~\ref{tbl:evaluation-translational-flickr30k} but when also trained on the additional data.
In order to add more data to the PBSMT baseline, we simply added the German sentences in the M30k as additional data to train the LM.\footnote{Adding the synthetic sentence pairs to train the baseline PBSMT model, as we did with all neural MT models, deteriorated the results.}
Both our models IMG and IMG that use global image features to initialise the encoder and the decoder, respectively, improve significantly according to BLEU, METEOR and TER with the additional back-translated data,
and also achieved better chrF3 scores.
Model IMG, that uses images as words in the source sentence, does not significantly differ in BLEU, METEOR or TER (), but achieves a lower chrF3 score than the comparable PBSMT model.
Although model IMG trained on only the original data has the best TER score (), both models IMG and IMG perform comparably with the additional back-translated data ( and , respectively), though the difference between the latter and the former is still not statistically significant ().

We see in Tables~\ref{tbl:evaluation-translational-flickr30k} and~\ref{tbl:evaluation-backtranslated-flickr30k} that our models that use images directly to initialise either the encoder or the decoder are the only ones to consistently outperform the PBSMT baseline according to the chrF3 metric, a character-based metric that includes both precision and recall, and has a recall bias.
That is also a noteworthy finding, since chrF3 is the only character-level metric we use, and it has shown a high correlation with human judgements~\cite{Stanojevicetal2015}.

In Table~\ref{tbl:translation-examples} we see translations for two entries in the test M30k set.
In the first entry,
although the reference translation is incorrect---there is just one dog in the image---, the multi-modal models translated it correctly.
In the second entry, the last three multi-modal models extrapolate the reference+image and describe ``ceremony'' as a ``wedding ceremony'' (IMG) and as an ``Olympics ceremony'' (IMG and IMG).
This could be due to the fact that the training set is small, depicts a small variation of different scenes and contains different forms of biasses~\cite{Miltenburg2015}.

We note that the idea of using images as words in the source sentence, also entertained by~\newcite{Huangetal2016}, does not perform as well as directly using the images in the encoder or decoder initialisation.
The fact that multi-modal NMT models can benefit from back-translated data
is also an interesting finding.


\begin{table}[t!]
  \centering
  \resizebox{\linewidth}{!} {
  \begin{tabular}{lllll}
   \toprule
   & BLEU & METEOR & TER & chrF3 \\
   \toprule
   
   \multicolumn{5}{l}{original training data}\\
   \midrule
   
   IMG &
   36.9 & 54.3 & 41.9 & 66.8 \\
   
   IMG &
   37.1 & 55.0 & 43.1 & 67.6 \\
   
   IMG &
   37.3 & 55.1 & 42.8 & 67.7 \\
   
   \midrule
   \multicolumn{5}{l}{+ back-translated training data}\\
   \midrule
   PBSMT & 34.0             & \underline{55.0} & 44.7 & \underline{68.0} \\
   NMT & \underline{35.5} & 53.4             & \underline{43.3} & 65.3 \\
   \midrule
   
   IMG &
   36.7
   \small\greenbf{( 1.2)} &
   54.6\white{}
   \small\redbf{( 0.4)} &
   42.0
   \small\greenbf{( 1.3)} &
   66.8 \small\redbf{( 1.2)} \\
   
   IMG &
   \textbf{38.5}
   \small\greenbf{( 3.0)} &
   55.7
   \small\greenbf{( 0.9)} &
   \textbf{41.4}
   \small\greenbf{( 1.9)} &
   68.3 \small\greenbf{( 0.3)} \\
   
   IMG &
   \textbf{38.5}
   \small\greenbf{( 3.0)} &
   \textbf{55.9}
   \small\greenbf{( 1.1)} &
   41.6
   \small\greenbf{( 1.7)} &
   \textbf{68.4} \small\greenbf{( 0.4)} \\
   
   \midrule
   \multicolumn{4}{l}{\textbf{Improvements} (original vs. + back-translated)}\\
   \midrule
   
   IMG&
   {\small\red{ 0.2}} &
   {\small\green{ 0.1}} &
   {\small\red{ 0.1}} &
   {\small\green{ 0.0}} \\
   
   IMG&
   {\small\greenbf{ 1.4}}&
   {\small\green{ 0.7}}&
   {\small\greenbf{ 1.8}}&
   {\small\greenbf{ 0.7}}\\

   IMG&
   {\small\green{ 1.2}}&
   {\small\greenbf{ 0.8}}&
   {\small\green{ 1.2}}&
   {\small\greenbf{ 0.7}}\\
   
   \bottomrule
  \end{tabular}
  }
  \caption{BLEU, METEOR, TER and chrF3 scores on the M30k test set for models trained on original and additional back-translated data.
  Best text-only baselines are underscored and best overall results in bold.
  We highlight in parentheses the improvements brought by our models compared to the best baseline score.
  Results differ significantly from PBSMT baseline () or NMT baseline () with .
  We also show the improvements each model yields in each metric when only trained on the original M30k training set vs. also including additional back-translated data.
  }
  \label{tbl:evaluation-backtranslated-flickr30k}
\end{table}

\begin{table}[t!]
  \centering
  \resizebox{\linewidth}{!} {
    \begin{tabular}{ll}
      \toprule


      \multicolumn{2}{c}{\includegraphics[height=130pt]{img/2367139509.jpg}}\\
      \midrule
      
      \textbf{ref.} & \redbf{ein brauner und ein schwarzer Hund} laufen auf \\
      & einem Pfad im Wald. \\
      
      SMT & \redbf{ein braun und schwarzer} Hund läuft auf einem \\
      & Pfad im Wald. \\
      
      NMT & ein \redbf{brauner Hund steht} an einem \redbf{Sand Strand}. \\
      
      IMG & ein braun-schwarzer Hund läuft auf einem Pfad im Wald. \\
      
      IMG & ein braun-schwarzer Hund läuft im Wald auf einem Pfad. \\
      
      IMG & ein braun-schwarzer Hund läuft im Wald auf einem Pfad. \\
      
      IMG & ein braun-schwarzer Hund läuft im Wald auf einem Pfad. \\
      \midrule
      
      \multicolumn{2}{c}{\includegraphics[height=130pt]{img/4610973875.jpg}}\\
      \midrule
      
      \textbf{ref.} & eine Frau mit langen Haaren bei einer Abschluss Feier. \\
      
      SMT & eine Frau mit langen Haaren steht \redbf{an einem Abschluss} \\
      
      NMT & eine Frau mit langen Haaren \redbf{ist an einer StaZeremonie}. \\
      
      IMG & eine Frau mit langen Haaren \redbf{ist an einer warmen} \\
      & \redbf{Zeremonie teil}. \\
      
      IMG & eine Frau mit langen Haaren steht bei einer \bluebf{Hochzeit Feier}. \\
      
      IMG & eine \bluebf{lang haarige} Frau bei einer \bluebf{olympischen} Zeremonie. \\
      
      IMG & eine \bluebf{lang haarige} Frau bei einer \bluebf{olympischen} Zeremonie. \\
      
















      \bottomrule
    \end{tabular}
  }
  \caption{Some translations for the M30k test set.}
  \label{tbl:translation-examples}
\end{table}

\section{Conclusions}
\label{sec:conclusion}

We have introduced different ideas to incorporate images into state-of-the-art attention-based NMT,
by using images as words in the source sentence, to initialise the encoder's hidden state and
as additional data in the initialisation of the decoder's hidden state.
We corroborate previous findings in that using image features directly at each time step of the decoder causes the model to overfit and prevents learning.
The intuition behind our effort is to use global image feature vectors to visually \emph{ground} translations and consequently increase translation quality.
Extensive experiments show that adding global image features into attention-based NMT is useful and improves over NMT and PBSMT as well as a strong multi-modal NMT baseline, according to all metrics evaluated.

In future work we will
conduct a more systematic study on the impact that synthetic back-translated data can have on multi-modal NMT,
and also investigate how to incorporate local, spatial-preserving image features.

\bibliography{phd}
\bibliographystyle{acl_natbib}

\end{document}