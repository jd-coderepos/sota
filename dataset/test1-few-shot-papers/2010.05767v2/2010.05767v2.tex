

\documentclass{article}

\usepackage{microtype}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[capitalise]{cleveref}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{textcomp}
\usepackage[group-separator={,}]{siunitx}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2021}

\makeatletter
\renewcommand{\Notice@String}{}
\makeatother

\icmltitlerunning{Smaller World Models for Reinforcement Learning}


\DeclareMathOperator{\Cat}{Cat}
\DeclareMathOperator{\Softmax}{Softmax}

\begin{document}

\twocolumn[
\icmltitle{Smaller World Models for Reinforcement Learning}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jan Robine}{hhu}
\icmlauthor{Tobias Uelwer}{hhu}
\icmlauthor{Stefan Harmeling}{hhu}
\end{icmlauthorlist}

\icmlaffiliation{hhu}{Department of Computer Science, Heinrich Heine University Düsseldorf, Düsseldorf, Germany}

\icmlcorrespondingauthor{Jan Robine}{jan.robine@hhu.de}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
 Sample efficiency remains a fundamental issue of reinforcement learning.
Model-based algorithms try to make better use of data by simulating the
environment with a model. We propose a new neural network architecture for
world models based on a vector quantized-variational autoencoder (VQ-VAE)
to encode observations and a convolutional LSTM to predict the next embedding
indices. A model-free PPO agent is trained purely on simulated experience from
the world model. We adopt the setup introduced by \citet{simple}, which only
allows  interactions with the real environment. We apply our method on
 Atari environments and show that we reach comparable performance to their
\mbox{SimPLe} algorithm, while our model is significantly smaller.
\end{abstract}

\section{Introduction}

Reinforcement learning is a generally applicable framework for finding actions
in an environment that maximizes the sum of rewards. From a probabilistic
perspective, the following probabilities lay the foundations of all
reinforcement learning problems:

The \textit{dynamics} , i.e., the conditional
joint probability of the next reward and state given the current state and
action. This probability defines the environment and is usually unknown. In most
real-world applications the underlying states  are not observable, but
instead the environment produces observations , which might not contain all
state information. In this case we can only observe
.

The \textit{policy} , i.e., the conditional probability
of choosing an action given the current observation, \mbox{where } denotes the
parameters of some model, indicating that this probability is not provided by
the environment, but learned. Reinforcement learning methods provide means to
optimize the policy in the sense that the actions that maximize the future sum
of rewards have the highest probability.

\textit{Model-free} algorithms try to optimize the policy
 based on real experience from the environment, without
any knowledge of the underlying dynamics. They have shown great success in a
wide range of environments, but usually require a large amount of training data,
i.e., many interactions with the environment. This low sample efficiency makes
them inappropriate for real-world applications in which data collection is
expensive.

\textit{Model-based} algorithms approximate the dynamics
,
where  denotes some learned parameters, thus building a model of the
environment, which we will call \textit{world model} to differentiate it from
other models. The process of improving the policy using the world model is
called \textit{planning}, but there are two types: first, we can generate
training data by sampling from  and apply a
model-free algorithm to this \textit{simulated experience}. Second, we can try
to \textit{look ahead} into the future using the world model, starting from the
current observation, in order to dynamically improve the action selection during
runtime.

\paragraph{Contributions.}
In this work we follow a model-based approach and consider the first type of
planning. The ability to generate new experience without acting in the real
environment does effectively increase the sample efficiency. Our main
contributions and insights can be summarized as follows:
\begin{itemize}
  \item VQ-VAE based world models require fewer parameters than previous
    approaches (see \cref{tab:parameters} and \cref{tab:parameters-detail}).
  \item Learning a latent space world model and training an agent on it is
    possible with only 100K interactions (see \cref{tab:results-mean}).
  \item A two-dimensional discrete latent representation combined with a
    dynamics network built from convolutional LSTMs (see
    \cref{fig:architecture}) is sufficient for model-based training in Atari
    games.
\end{itemize}

\clearpage

\section{Related Work}

\paragraph{World Models \citep{world-models}.}
Modeling environments with complex visual observations is a hard task, but
luckily predicting observations on pixel level is not necessary. The authors
introduce latent variables by encoding the high-dimensional observations 
into lower-dimensional, latent representations . For this purpose they use
a variational autoencoder \citep{vae}, which they call the ``vision''
component, that extracts information from the observation at the current time
step. They use an LSTM combined with a mixture density network \citep{mdn} to
predict the next latent variables stochastically. They call this component the
``memory'', that can accumulate information over multiple time steps.

They also condition the policy on the latent variable, which enables them to
stay in latent space, so that decoding back into pixels is not required (except
for learning the representations). This makes simulation more efficient and can
reduce the effect of accumulating errors. In \cref{sec:latent-variables} we
describe in more detail how to integrate latent variables into the dynamics.

They successfully evaluate their architectures on two environments, but it
involves some manual fine-tuning of the policy. They use an evolution strategy
to optimize the policy, which is not suitable for bigger networks. They also use
a non-iterative training procedure, i.e., they randomly collect real experience
only once and then train the world model and the policy. This implies that the
improved policies cannot be used to obtain new experience, and a random policy
has to ensure sufficient exploration, which makes the approach inappropriate for
more complex environments.

\paragraph{Simulated Policy Learning \citep{simple}.}
The authors introduce a new model-based algorithm (SimPLe) and successfully
apply it to Atari games. They use an iterative training procedure, that
alternates between collecting real experience, training the world model, and
improving the policy using the world model.

Another novelty is that they restrict the algorithm to about 
interactions with the real environment, which is considerably less than the
usual  to  interactions.

They train the policy using the model-free PPO algorithm \citep{ppo} instead of
an evolution strategy. They use a video prediction model similar to SV2P
\citep{sv2p} and incorporate the input action in the decoding process to predict
the next frame. The latent variable is discretized into a bit vector, that is
predicted autoregressively using an LSTM during inference time. The policy gets
frames as input, which means that decoding the latent variables back into
pixel-level is required.

\newpage
They get very good results on a lot of Atari environments, considering the low
number of interactions.

\paragraph{Dreamer \citep{dreamer} and DreamerV2 \citep{dreamerv2}.}
The architecture of DreamerV2 is closely related to ours. The authors train a
world model with latent representations, such that the agent can operate
directly on the latent variables. One of the main improvements of DreamerV2 over
the model of Dreamer is the discretization of the latent space, as it uses
categorical instead of Gaussian latent variables.

They show that their agent beats model-free algorithms in many Atari games after
 interactions. This is a quite different goal from our work, where we
attempt to learn as much as possible from only  interactions. Furthermore,
we base our discrete latent world model on a VQ-VAE and thus discretize the
latent variables using vector quantization.

\section{Discrete Latent Space World Models}

The goal of this work is to extend the idea of \citet{world-models} by using
more sophisticated neural network architectures and evaluating them on Atari
environments with the model-free PPO algorithm instead of evolution strategies.
In particular, we discretize the latent space, but have a fundamentally
different architecture compared to DreamerV2 \citep{dreamerv2}, since our latent
space is two-dimensional. Moreover, we adopt the limitation to 
interactions, the iterative training scheme and some other crucial ideas from
\citet{simple}, which will be explained in later sections.

\subsection{Latent Variables} \label{sec:latent-variables}

\begin{figure}[t]
  \vskip 0.05in
	\centering
	\resizebox{\columnwidth}{!}{
		\begin{tikzpicture}[yscale=0.7]
		\tikzstyle{vertex}=[draw,circle,inner sep=0pt,minimum size=0.8cm]
		\node[vertex] (x) at (0,1.5) {};
		\node[vertex] (x2) at (4,1.5) {};
		\node[vertex] (x3) at (8,1.5) {};

		\node[vertex] (z) at (0,0) {};
		\node[vertex] (z2) at (4,0) {};
		\node[vertex] (z3) at (8,0) {};

		\node[vertex] (r) at (2.5,0) {};
		\node[vertex] (r2) at (6.5,0) {};

		\node[vertex] (y) at (2.5,-1.5) {};
		\node[vertex] (y2) at (6.5,-1.5) {};

		\node[vertex] (a) at (0.75,-1.5) {};
		\node[vertex] (a2) at (4.75,-1.5) {};

		\node[vertex] (h) at (0,-3) {};
		\node[vertex] (h2) at (4,-3) {};
		\node[vertex] (h3) at (8,-3) {};

		\draw[->,dotted] (-0.55,-0.55) -- (z);
		\draw[->,dotted] (-0.75,-3) -- (h);
		\node at (-1.4,-1.5) {};

		\draw[->] (h) -- (h2);
		\draw[->] (h2) -- (h3);

		\draw[->] (x) -- (z);
		\draw[->] (x2) -- (z2);
		\draw[->] (x3) -- (z3);

		\draw[->] (z) -- (h);
		\draw[->] (z2) -- (h2);
		\draw[->] (z3) -- (h3);

		\draw[->] (z) -- (y);
		\draw[->] (z2) -- (y2);

		\draw[->] (z) -- (a);
		\draw[->] (z2) -- (a2);

		\draw[->] (a) -- (h);
		\draw[->] (a2) -- (h2);

		\draw[->] (a) -- (y);
		\draw[->] (a2) -- (y2);

		\draw[->] (h) -- (y);
		\draw[->] (h2) -- (y2);

		\draw[->] (y) -- (r);
		\draw[->] (y2) -- (r2);

		\draw[->] (y) -- (z2);
		\draw[->] (y2) -- (z3);

		\draw[->,dotted] (h3) -- (8.75,-3);
		\draw[->,dotted] (z3) -- (8.375,-0.75);
		\draw[->,dotted] (8.375,-2.25) -- (h3);

		\node at (8.75,-1.5) {};
		\end{tikzpicture}
	}
  \vskip 0.1in
	\caption{Graphical model of the world model and the policy, which arises from
		inserting the latent variables and from our independence assumptions.}
	\label{fig:graphical-model}
  \vskip -0.1in
\end{figure}

Similar to \citet{world-models} we approximate the dynamics with the help of
latent variables. The sum rule of probability allows us to introduce a latent
variable ,

where we have made multiple independence assumptions. Especially, we want an
observation encoding model, , that does not depend on the
action (analogous to the ``vision'' component of \citet{world-models}). Second,
we want to predict the next latent variable based on the previous latent
variable and action, i.e., , independent of the
observation . Furthermore, the reward and next latent variable should not
depend on each other, which allows to predict them using two heads and compute
them in a single neural network pass.

In contrast to \citet{simple}, the policy is conditioned on the latent
variables, so that no decoding into high-dimensional observations is necessary,


\subsection{Recurrent Dynamics}

Predicting the next latent variable and reward can be improved by introducing a
recurrent variable  to the dynamics model, similar to \citet{world-models},

We have made independence assumptions analogous to \cref{eq:latent-expectation}.
In particular, the observation encoder and decoder do not depend on the
recurrent variable, but the latent and reward dynamics do. For notation
purposes, we introduced an intermediate representation  in
\cref{eq:recurrent-expectation}, which is a deterministic function of ,
, and . The resulting graphical model can be seen in
\cref{fig:graphical-model}.

We can also condition the policy on the recurrent variable, which adds a
dependency from  to ,


From \cref{eq:recurrent-expectation}, \cref{eq:policy-expectation}, and
\cref{eq:recurrent-policy} it becomes clear that several models have to be
learned. We denote the parameters of the world model by , and the
parameters of the policy by . \cref{tab:models} provides an overview of
all models that need to be learned.

\begin{table}[h]
  \vskip -0.083in
	\caption{Summary of the learned models. Note that  is a deterministic
		function of , , and .}
	\label{tab:models}
	\vskip 0.15in
	\begin{center}
		\resizebox{\columnwidth}{!}{
		\begin{tabular}{lll}
			\toprule
			& Observation encoder &  \2pt]
			World model & Reward dynamics &  \2pt]
			& Observation decoder &  \\
			\midrule
			Agent & Policy &  or  \\
			\bottomrule
		\end{tabular}
    }
	\end{center}
  \vskip -0.15in
\end{table}

\subsection{Architecture}

\paragraph{Latent Representations.}
We use a vector quantized-variational autoencoder \cite{vq-vae} for the
observation encoder and decoder, so each latent variable  is a matrix
filled with discrete embedding indices. The observations  are the last four
frames of the Atari game, stacked, scaled down to  and converted
from RGB to grayscale. Frame stacking allows the observation encoder to
incorporate short-time information, e.g., the velocity of objects, into the
otherwise stationary latent representations.

The encoder uses convolutions with batch normalization, while the decoder uses
deconvolutions without batch normalization. All convolutions and deconvolutions
are followed by leaky ReLU nonlinearities (after the batch normalization). We
model the outputs of the decoder with continuous Bernoulli distributions
\cite{continuous-bernoulli} with independence among the stacked frames and
pixels, so the last deconvolution outputs the logits of 
distributions. See \cref{fig:vqvae} for a visualization of the model.

We employ a two-dimensional representation because it can also express local
spatial correlations. Thus, it is better suited to predict the representation at
the next time step, especially when combined with convolutional operations.

\begin{figure*}[t]
  \includegraphics[height=5cm]{figures/architectures/state-encoding.pdf}
\vskip -1.2in
  \hfill
  \includegraphics[height=5cm]{figures/architectures/state-decoding.pdf}
\caption{Visualization of the observation encoder architecture (top) and
		decoder architecture (bottom).}
	\label{fig:vqvae}
\end{figure*}

\paragraph{Dynamics.}
For the recurrent dynamics we use a two-cell convolutional LSTM
\citep{conv-lstm} with layer normalization. The input consists of a
 tensor, where the first  channels are the embedding
vectors looked up in the codebook of the VQ-VAE using the indices of the
 state representation. The last  channels contain one-hot
encodings of the actions, repeated along the spatial dimensions. By doing this,
we condition the dynamics on the action. The action encodings are also
concatenated to the output of each convolutional LSTM cell, since the action
information might get lost during the forward pass. After the last
convolutional LSTM cell, this corresponds to the intermediate representation
 from \cref{fig:graphical-model}, which means that we actually drop its
direct dependence on . Then, there are two prediction heads, one for the
next latent variable,  consisting of one convolutional layer, and
one for the reward,  consisting of a convolutional layer and two
fully-connected layers. The convolutional layers are followed by layer
normalization and leaky ReLU nonlinearities. For a detailed depiction of the
model see \cref{fig:architecture}.

The output of  is a  tensor
() that contains the unnormalized scores for the
embedding indices that get normalized via the softmax function,

We suppose that the discretization of the latent space stabilizes the dynamics
model, since it has to predict scores for a predefined set of categories instead
of real values, especially considering that the target is moving, i.e., the
latent representations change during training.

The rewards are discretized into three categories  by clipping them
into the interval  and rounding them to the nearest integer. The output
of  is a -dimensional vector containing the scores for each
reward category, which are also normalized via the softmax function,

The support of this distribution is , so we have to map the
rewards accordingly () when we compute the likelihood.

\begin{figure*}[t]
	\centering
  \vskip 0.1in
	\includegraphics[width=0.8\linewidth]
	 {figures/architectures/dynamics.pdf}
	\caption{A visualization of the architecture of the dynamics network. After
    the second convolutional LSTM cell the network splits into the reward
    prediction head  at the top and the next latent prediction head
     at the bottom. The recurrent states  of the
    LSTM are not visualized for clarity.}
	\label{fig:architecture}
\end{figure*}

\paragraph{Policy.}
The input of the policy network are the embedding vectors and it processes them
using two convolutional layers with layer normalization, followed by a fully
connected layer. Unlike \cref{eq:recurrent-policy} the policy does not depend on
the recurrent variable  from the world model in our experiments, but
this could be useful for more complex environments. The output of the network
 is again a vector of unnormalized scores of a categorical
distribution for the  possible actions,


\begin{table}[t]
	\centering
  \vskip -0.083in
	\caption{Number of parameters of the world model compared with \citet{simple}
    (their number is approximate).}
	\label{tab:parameters}
	\vskip 0.15in
  \resizebox{0.42\columnwidth}{!}{
	\begin{tabular}{lr}
		\toprule
		Model & \# parameters \\
		\midrule
		Ours & \num{10332740} \\
		SimPLe & \num{74000000} \\
		\bottomrule
	\end{tabular}
  }
  \vskip -0.1in
\end{table}

\paragraph{Number of Parameters.} \cref{tab:parameters} shows the number of
parameters of our model compared with the model by \citet{simple}, which
uses about seven times as many parameters. \cref{tab:parameters-detail} shows
the number of parameters of our models in detail. At training time all models
are used, but at test time only the encoder and the policy network are required.

\subsection{Training}

The distributions in \cref{eq:latent-dynamics} and \cref{eq:reward-dynamics} are
trained using maximum likelihood. The policy is optimized on simulated
experience using proximal policy optimization \cite{ppo}, a model-free
reinforcement learning algorithm. We approximate the expectations in
\cref{eq:latent-expectation} and \cref{eq:policy-expectation} with single Monte
Carlo samples. While we simulate experience, we use the same sample
 for both the dynamics, \cref{eq:recurrent-expectation},
and the policy, \cref{eq:policy-expectation}. At inference time, when the policy
is applied to a real environment, \cref{eq:policy-expectation} still needs
access to the observation encoder  in order to sample the
latent variables.

\begin{table}[t]
	\centering
  \vskip -0.084in
	\caption{Number of parameters of our models in detail. The encoder and decoder
  both need access to the embedding vectors, therefore the sum of their
  individual number of parameters is slightly higher than for the total VQ-VAE.}
  \label{tab:parameters-detail}
	\vskip 0.15in
  \resizebox{0.81\columnwidth}{!}{
	\begin{tabular}{lr}
		\toprule
		Model & \# parameters \\
		\midrule
		World model & \num{10332740} \\
		VQ-VAE & \num{4089313} \\
		Encoder & \num{2014720} \\
		Decoder & \num{2078689} \\
		Dynamics network & \num{6243427} \\
		Policy network & \num{1297415} \\
		\midrule
		World model + policy (training) & \num{11630155} \\
		Encoder + policy (inference) & \num{3312135} \\
		\bottomrule
	\end{tabular}
  }
\vskip -0.1in
\end{table}

\paragraph{Episodic Environments.}
Atari games are episodic, therefore the world model needs to predict terminal
states, for instance by predicting a binary variable that indicates the end of
the episode. This prediction has to be reliable, since an incorrect prediction
of ``true'' can have a severe impact on the simulated experience and thus on
the policy.

We follow \citet{simple} and end all episodes after a fixed number of steps
(e.g., 50), so the world model does not have to terminate episodes.

Furthermore, we adopt the idea of randomly selecting the initial observation of
a simulated episode from the collected real data. This enables the policy to
learn from experience from any stage of the environment, although the number of
time steps is limited. On the downside, this prevents the policy to learn from
effects that are longer than the fixed number of steps \cite{simple}.

\paragraph{Iterative Training.}
We adopt the iterative training procedure from \citet{simple} and alternate
between interacting with the real environment, training the world model, and
training the policy.

We also adopt the number of interactions per iteration,
\num{6400}, and the number of iterations, \num{15}. The authors state that they
perform additional \num{6400} interactions prior to the first iteration. Thus,
we perform \num{12800} interactions in the first iteration, resulting in the
same number of total interactions, .
In the first iteration we use a random uniform policy.

\paragraph{Warming Up Latent Representations.}
After collecting the first batch of data, we train the VQ-VAE separately for
\num{50} epochs with a higher learning rate. We want to give the dynamics model
a better starting point, with representations that already contain useful
information. This cannot be done in later training stages, as the dynamics
model would not be able to keep up with the representations.

\paragraph{Fixed Representations.}
After warming up, we even slow down the state representation training by
updating the parameters of the VQ-VAE only in every second training step, so the
targets of the dynamics model are moving slower.

\paragraph{Reward Loss.}
If the gradients coming from the reward prediction head have a high magnitude,
they have a degrading effect on the performance of the next latent prediction
head. We solve this issue by scaling down the cross-entropy loss of the rewards
to reduce its influence on the entire model, but using a higher learning rate
for the reward prediction head to compensate for the smaller gradients.

\paragraph{Constant KL Term.}
\citet{simple} state that the weight of the KL divergence loss of a variational
autoencoder is game dependent, which makes VAEs impractical to apply on all
Atari games without fine-tuning. The VQ-VAE does not suffer from this problem,
since the KL term is constant and depends on the number of embeddings.

\begin{figure*}
  \vskip 0.02in
	\begin{subfigure}[t]{0.33\linewidth}
		\centering
		\includegraphics[width=\columnwidth,trim={0 0 0 0.5cm},clip]
		{figures/plots/world-models-freeway.pdf}
		\caption{Freeway}
		\label{fig:training-plot-freeway}
	\end{subfigure}
	\begin{subfigure}[t]{0.33\linewidth}
		\centering
		\includegraphics[width=\columnwidth,trim={0 0 0 0.5cm},clip]
		{figures/plots/world-models-gravitar.pdf}
		\caption{Gravitar}
		\label{fig:training-plot-gravitar}
	\end{subfigure}
	\begin{subfigure}[t]{0.33\linewidth}
		\centering
		\includegraphics[width=\columnwidth,trim={0 0 0 0.5cm},clip]
		{figures/plots/world-models-hero.pdf}
		\caption{Hero}
		\label{fig:training-plot-hero}
	\end{subfigure}
	\caption{Mean episode reward across five training runs for three Atari
		environments. The x-axis shows the number of interactions with the real
		environment, and does not reflect the number of parameter updates that were
		performed in between. For SimPLe \citep{simple} we only know the final score
		which is depicted by a straight line.}
	\label{fig:training-plots}
\end{figure*}

\section{Evaluation}

\begin{figure}[t]
  \centering
  \vspace{0.055in}
  \begin{subfigure}[t]{0.48\columnwidth}
    \centering
    \includegraphics[height=3.5cm]
    {figures/frames/hero-latent-1.png}
    \includegraphics[height=3.5cm]
    {figures/frames/hero-latent-2.png}
    \includegraphics[height=3.5cm]
    {figures/frames/hero-latent-3.png}
    \caption{Hero}
    \label{fig:latent-frame-hero}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\columnwidth}
    \centering
    \includegraphics[height=3.5cm]
    {figures/frames/krull-latent-1.png}
    \includegraphics[height=3.5cm]
    {figures/frames/krull-latent-2.png}
    \includegraphics[height=3.5cm]
    {figures/frames/krull-latent-3.png}
    \caption{Krull}
    \label{fig:latent-frame-krull}
  \end{subfigure}
  \caption{Visualization of original game frames (top row), the encoded
 	 discrete latent variables (middle row), and the reconstructions
 	from the VQ-VAE (bottom row). We assign colors to the embedding indices
 	and draw a colored square for each entry of the latent matrix. A square does
  not necessarily correspond to the same area in the frame as they have larger
  receptive fields.}
 \label{fig:latent-frames}
\end{figure}

We compare our method with the variant of \citet{simple} (stochastic discrete,
50 steps, ) that comes closest to our model in terms of
hyperparameters (discount rate, batch size etc.) and number of parameter
updates.

This implies that the results of \citet{simple} that are shown are not
necessarily the best across all of their variants, but the best for a fairer
comparison with our method. We do not know the performance difference to
\citet{simple} in terms of training time and test time, but considering the
model size our method should be a lot faster.

We restrict our agent to  interactions with the environment and average
the results over five training runs. For every run we evaluate the latest policy
in each iteration by rolling out  episodes in the real environment and
computing the mean of the (cumulative) episode rewards. In
\cref{tab:results-mean} we report the mean final episode rewards (i.e., the mean
episode reward after the final iteration; averaged over five runs) for 
Atari environments. Our method achieves a higher value than SimPLe \cite{simple}
in  out of  environments (when also considering no frame stacking, as
explained below).

\paragraph{Learning Curves.}
\cref{fig:training-plots} shows three cases of learning curves that were typical
for our model. First, \cref{fig:training-plot-freeway} shows an example of an
environment in which the agent's performance successfully increases over the
course of training. Secondly, \cref{fig:training-plot-gravitar} shows an
example of an environment in which the agent's performance \textit{decreases}
over the course of training. This can have various reasons, e.g., when the
agent reaches a new area of the state space and the environment dynamics change
drastically. Finally, \cref{fig:training-plot-hero} shows an example of an
environment in which the agent has comparably high performance starting from
the first iteration, which is most likely due to model bias.

\paragraph{Latent Representations.}
In \cref{fig:latent-frames} we can see that the state representation model is
able to encode almost all information into the latent representation. However,
the embedding indices are tuned for the decoder, so the dynamics model still has
a hard task. We picked these two environments to show that changes in the scene
(\cref{fig:latent-frame-hero}) or even switching scenes
(\cref{fig:latent-frame-krull}) can be represented, although this can cause some
loss of details (e.g., see the left reconstruction in
\cref{fig:latent-frame-krull}).

\paragraph{No Frame Stacking.} Our default model stacks the last four
frames to incorporate short-time information into the state
representations. On the downside, this introduces complexity, since the
same frame can get a different representation, depending on the three
other frames in the stack. This in turn will make it harder for the
dynamics model to predict the next representation. The results show that no
frame stacking improves the performance in some environments, as can be seen in
\cref{tab:results-mean}.

\paragraph{Exploration.}
In our experiments we observe the same phenomenon as \citet{simple}, namely that
the results can vary drastically for different runs with the same
hyperparameters but different random seeds. The main reasons might be that
the world model cannot infer dynamics of regions of the environment's state
space that it has never seen before, and that the algorithm is very sensitive to
the exploration-exploitation trade-off, as the number of interactions is low.

\clearpage

\begin{table*}[t]
	\centering
  \vspace{-0.1in}
	\caption{Comparison of our method (with and without frame stacking) with
    SimPLe \citep{simple} and model-free PPO \citep{ppo} trained with 
    steps. Our scores are the mean final episode reward, averaged over five runs
     standard deviation. The PPO scores are taken from \citet{simple}.}
  \label{tab:results-mean}
  \vskip 0.1in
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lrlrlrlrl}
    \toprule
    & \multicolumn{2}{c}{Ours (4 frames)}
    & \multicolumn{2}{c}{Ours (1 frame)}
    & \multicolumn{2}{c}{SimPLe (SD, )}
    & \multicolumn{2}{c}{PPO } \\
    \midrule
    Game & Mean & Std. Dev. & Mean & Std. Dev. & Mean & Std. Dev.  & Mean & Std. Dev. \\
    \midrule
    Alien
    &  & 
    &  & 
    &  & 
    &  &  \\
    Amidar
    &  & 
    &  & 
    &  & 
    &  &  \\
    Assault
    &  & 
    &  & 
    &  & 
    &  &  \\
    Asterix
    &  & 
    &  & 
    &  & 
    &  &  \\
    Asteroids
    &  & 
    &  & 
    &  & 
    &  &  \\
    Atlantis
    &  & 
    &  & 
    &  & 
    &  &  \\
    BankHeist
    &  & 
    &  & 
    &  & 
    &  &  \\
    BattleZone
    &  & 
    &  & 
    &  & 
    &  &  \\
    BeamRider
    &  & 
    &  & 
    &  & 
    &  &  \\
    Bowling
    &  & 
    &  & 
    &  & 
    &  &  \\
    Boxing
    &  & 
    &  & 
    &  & 
    &  &  \\
    Breakout
    &  & 
    &  & 
    &  & 
    &  &  \\
    ChopperCommand
    &  & 
    &  & 
    &  & 
    &  &  \\
    CrazyClimber
    &  & 
    &  & 
    &  & 
    &  &  \\
    DemonAttack
    &  & 
    &  & 
    &  & 
    &  &  \\
    FishingDerby
    &  & 
    &  & 
    &  & 
    &  &  \\
    Freeway
    &  & 
    &  & 
    &  & 
    &  &  \\
    Frostbite
    &  & 
    &  & 
    &  & 
    &  &  \\
    Gopher
    &  & 
    &  & 
    &  & 
    &  &  \\
    Gravitar
    &  & 
    &  & 
    &  & 
    &  &  \\
    Hero
    &  & 
    &  & 
    &  & 
    &  &  \\
    IceHockey
    &  & 
    &  & 
    &  & 
    &  &  \\
    Jamesbond
    &  & 
    &  & 
    &  & 
    &  &  \\
    Kangaroo
    &  & 
    &  & 
    &  & 
    &  &  \\
    Krull
    &  & 
    &  & 
    &  & 
    &  &  \\
    KungFuMaster
    &  & 
    &  & 
    &  & 
    &  &  \\
    MsPacman
    &  & 
    &  & 
    &  & 
    &  &  \\
    NameThisGame
    &  & 
    &  & 
    &  & 
    &  &  \\
    Pong
    &  & 
    &  & 
    &  & 
    &  &  \\
    PrivateEye
    &  & 
    &  & 
    &  & 
    &  &  \\
    Qbert
    &  & 
    &  & 
    &  & 
    &  &  \\
    Riverraid
    &  & 
    &  & 
    &  & 
    &  &  \\
    RoadRunner
    &  & 
    &  & 
    &  & 
    &  &  \\
    Seaquest
    &  & 
    &  & 
    &  & 
    &  &  \\
    UpNDown
    &  & 
    &  & 
    &  & 
    &  &  \\
    YarsRevenge
    &  & 
    &  & 
    &  & 
    &  &  \\
    \bottomrule
  \end{tabular}
  }
  \vskip -0.1in
\end{table*}

\clearpage


\paragraph{LSTM architecture.}
Instead of a convolutional LSTM, we also tried follow-up architectures like the
spatio-temporal LSTM \citep{st-lstm} or causal LSTM \citep{causal-lstm}, but
for our problem the performance gain was not significant enough, compared with
the imposed additional training time and number of parameters.


\section{Limitations and Future Work}

Currently, our dynamics model samples the indices in the latent representation
independently. This might be disadvantageous because conditional dependencies
between the indices, which correspond to certain areas of the video frames, are
ignored. So in the future it would be interesting to predict them
autoregressively, e.g., with a conditional PixelCNN \citep{conditional-pixelcnn}
conditioned on , to see whether this solves prediction errors like
duplication or incoherent movement of objects. Nevertheless, an autoregressive
model might have a negative impact on the training times (when training the
dynamics model and when simulating experience), and it has to be seen whether
the resulting training times are acceptable.

Another line of research should improve exploration in order to enable even
higher sample efficiency. Furthermore, we would like the world model to predict
terminal states and move away from terminating episodes after a fixed number of
steps, since this introduces a new hyperparameter that needs to be tuned, and
prevents the agent from learning beyond this time horizon.

\section{Conclusion}

In this paper we demonstrate that a generative model with a discrete latent
space can be used to strongly decrease the size of world models. We employ a
VQ-VAE with a discrete two-dimensional latent space. We show that this powerful
model is able to effectively encode the complex visual observations of Atari
games. The chosen model and latent space have produced representations that are
more stable and expressive at the same time. Our experiments show that acting
entirely in latent space is possible, which speeds up training since no decoding
into high-dimensional frames is required.

\bibliography{bibliography}
\bibliographystyle{icml2021}



\end{document}
