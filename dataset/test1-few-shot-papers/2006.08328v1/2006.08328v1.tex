\documentclass[sigconf]{acmart}
\settopmatter{authorsperrow=4}
\usepackage{url}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor,colortbl}


\hyphenation{op-tical net-works semi-conduc-tor Wikipe-dia}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}





\acmConference[CIKM2020]{CIKM2020: 29th ACM International Conference on Information and Knowledge Management}{October 19--23, 2020}{Galway, IE}
\acmBooktitle{CIKM2020: 29th ACM International Conference on Information and Knowledge Management, October 19--23, 2020, Galway, IE}







\begin{document}




\title{ETHOS: an Online Hate Speech Detection Dataset}\author{Ioannis Mollas}
\affiliation{\institution{Dept. of Informatics}
  \streetaddress{Aristotle University of Thessaloniki}
  \city{Thessaloniki}
  \state{Greece}
  \postcode{54636}}
\email{iamollas@csd.auth.gr}

\author{Zoe Chrysopoulou}
\affiliation{\institution{Dept. of Informatics}
  \streetaddress{Aristotle University of Thessaloniki}
  \city{Thessaloniki}
  \state{Greece}
  \postcode{54636}}
\email{zoichryso@gmail.com}

\author{Stamatis Karlos}
\affiliation{\institution{Dept. of Informatics}
  \streetaddress{Aristotle University of Thessaloniki}
  \city{Thessaloniki}
  \state{Greece}
  \postcode{54636}}
\email{stkarlos@csd.auth.gr}

\author{Grigorios Tsoumakas}
\affiliation{\institution{Dept. of Informatics}
  \streetaddress{Aristotle University of Thessaloniki}
  \city{Thessaloniki}
  \state{Greece}
  \postcode{54636}}
\email{greg@csd.auth.gr}
\renewcommand{\shortauthors}{Mollas, Chrysopoulou, Karlos and Tsoumakas}


\begin{abstract}
Online hate speech is a newborn problem in our modern society which is growing at a steady rate exploiting weaknesses of the corresponding regimes that characterise several social media platforms. Therefore, this phenomenon is mainly cultivated through such comments, either during users' interaction or on posted multimedia context. Nowadays, giant companies own platforms where many millions of users log in daily. Thus, protection of their users from exposure to similar phenomena for keeping up with the corresponding law, as well as for retaining a high quality of offered services, seems mandatory. Having a robust and reliable mechanism for identifying and preventing the uploading of related material would have a huge effect on our society regarding several aspects of our daily life. On the other hand, its absence would deteriorate heavily the total user experience, while its erroneous operation might raise several ethical issues. In this work, we present a protocol for creating a more suitable dataset, regarding its both informativeness and representativeness aspects, favouring the safer capture of hate speech occurrence, without at the same time restricting its applicability to other classification problems. Moreover, we produce and publish a textual dataset with two variants: binary and multi-label, called `ETHOS', based on YouTube and Reddit comments validated through figure-eight crowdsourcing platform. Our assumption about the production of more compatible datasets is further investigated by applying various classification models and recording their behaviour over several appropriate metrics.
\end{abstract}


\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010258.10010259</concept_id>
<concept_desc>Computing methodologies~Supervised learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003347.10003353</concept_id>
<concept_desc>Information systems~Sentiment analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003462.10003480.10003482</concept_id>
<concept_desc>Social and professional topics~Hate speech</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003227.10003351</concept_id>
<concept_desc>Information systems~Data mining</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}


\ccsdesc[500]{Computing methodologies~Supervised learning}
\ccsdesc[500]{Information systems~Sentiment analysis}
\ccsdesc[500]{Information systems~Data mining}
\ccsdesc[500]{Social and professional topics~Hate speech}
\keywords{Hate Speech Detection, Multi-Label Classification, Crowdsourcing, Dataset Manipulation}



\maketitle

\section{Introduction}
Hate speech (HS) is a form of insulting public speech directed at specific individuals or groups of people on the basis of characteristics, such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation or gender identity~\cite{wikiHate}. This phenomenon is manifested either verbally or physically (e.g.  speech, text, gestures) promoting the emergence of racism and ethnocentrism. Because of the social cost arising out of HS, several countries consider HS as an illegal act, particularly when it encourages violence or hatred~\cite{euCHS,canadaHS}. Although a fundamental human right, freedom of speech, it is actually in conflict with laws that protect people from HS. Therefore, almost every country has responded to this fact by drawing up corresponding legal frameworks, while the research which is related with mechanisms that try to remedy such phenomena have recently been really attractive to Data Mining (DM) and Machine Learning (ML) communities~\cite{DBLP:journals/ethicsit/UllmannT20}.

Another important issue is that the occurrence of HS phenomena is actually emerging in the social media's environment. The anonymity of social media is the main reason for favouring its spread avoiding deliberately the corresponding legislation. Big companies, like Google and Facebook, are therefore obliged to detect and remove such kind of content from their platforms. As a result, Artificial Intelligence (AI) methodologies are required to detect and remove (semi-)automatically such content in real-time level, or even prevent users from publishing similar context with appropriate warnings or bans. The solution of quarantining in an online fashion has recently been demonstrated~\cite{DBLP:journals/ethicsit/UllmannT20}, trying to smooth the censorship and the possible harmful consequences of HS attacks. Naturally, AI methodologies require balanced, reliable and unbiased datasets in order to achieve high performances in real-life tasks. However, this requirement seldomly holds without applying proper manipulation stages~\cite{DBLP:conf/cikm/ChenMLZM19}. This is the direction towards our work aims to highly contribute, motivated mainly by the use case of HS, but offering a generic-based protocol which could be applied to a broad range of learning applications.

To be more specific, there are currently a lot of HS datasets available at the related literature~\cite{waseem-hovy, DBLP:conf/naacl/ZampieriMNRFK19}. However, since the majority of them were not manually constructed, phenomena of extremely imbalanced classes or redundant information may occur frequently, violating thus the previously mentioned desired requirements, leading to solutions that are characterised by low variance and/or high bias. Moreover, most of them concern binary classification or multi-class classification problems, ignoring the more realistic case of Multi-label classification (MLC), since an online comment may correspond to more than one existing labels at the same time rather than being restricted to exactly one outcome. 

A simple application that would utilise the multi-label information provided by this dataset could be a support system for human personnel reviewing comments on social media platforms that may or may not contain HS. This would make it easier for the reviewer to determine whether the message included HS content provided such additional information. For example, if a comment is presented as targeting people with disabilities, is directed at an individual and promotes violence, it would be more useful for the reader in order to conclude and condemn it for containing HS, than to be presented with a single label, such as `may contain HS':\{`yes',`no'\}.

Regarding also the ethical issues that arise in case of HS, a proper manipulation protocol seems necessary for validating several defects that are met on the initially collected data, as other works have suggested in ML community, examining either wider topics of research or more targeted, like news articles or nutritional labels, respectively~\cite{DBLP:conf/cikm/SunAJHS19, DBLP:conf/cikm/HoangVN18}. Similar directions have recently been investigated in the framework of HS detection~\cite{DBLP:journals/corr/abs-2004-14454}.

In this work, we present the process of creating a multi-labelled dataset with a step-by-step narrative, to avoid the implications that usually occur in similar attempts with data that come from social platforms, as well as increase the chance of mining better and more informative insights from the corresponding instances. Although the nature of the proposed protocol is broad enough, without deterring us from applying it over diverse datasets' fields, at the present, we focus on tackling the HS scenario through exploiting an available dataset mined from popular social media platforms and obtaining later feedback from a well-known crowdsourcing platform. The effects of the proposed protocol are commented in depth and visualised through explanatory methods. Afterwards, a set of experiments are taking place in order to find a baseline performance of this specific dataset using a variety of state-of-the-art techniques, ranging from conventional ML algorithms and ensemble learners to Deep Neural Networks (DNNs) with and without embeddings information, in both binary and multi-label scope, influenced mainly by other published works that also demonstrate approaches of producing appropriate datasets~\cite{hateTweets,almeida2013towards}. 

Our ultimate ambition by describing the total procedure and providing the corresponding dataset is to foster any interested researchers and businesses to take into consideration an approach that attempts towards transforming the existing insulting environment of social media into a non-hate inclusive online society. Adoption of the proposed protocol into different scientific fields may also be proved quite beneficial, especially when the knowledge acquired by oracles during annotation may be disambiguous.

The remainder of this paper is organised as follows: Section 2 contains some well-documented attempts of facing HS problem with instances that were collected from related sources. The proposed protocol is described next, while some extended single/multi-label classification experiments are placed in Section 4, revealing the discriminating ability of several examined algorithms. Finally, Section 5 discusses the more crucial assets of the proposed protocol, regarding also the recorded experiments, reporting later some remarkable future points that could be further investigated.




\section{Related Datasets} 
In this section, we present datasets related to HS, along with their formulation as well as some useful information about their structure and/or the manner under their composition took place. The last paragraph describes the Hatebusters' data that we utilise as a seed data through the proposed protocol to produce the final structure of data, named {\em ETHOS} (onlinE haTe speecH detectiOn dataSet).

A collection of 16,914 hate speech tweets was introduced in a study of how different features improve the identification of users that use analogous language online~\cite{waseem-hovy}. Out of the total number of messages, 3,383 concerned sexism and were sent by 613 users, 1,972 concerned racism and were sent by 9 users, and 11,559 did not include hate speech and were sent by 614 users. The corpus was generated by a manual tweet search, using the public Twitter API, containing popular slurs and terms related to sexual, religious, gender and ethnic minorities in order to include samples that are not offensive regardless of the inclusion of such words. A drawback of this dataset is that the text of the tweets is not directly accessible, but only through the Twitter API.

Another dataset~\cite{hateTweets} contains 24,783 tweets, manually classified as hate speech (1,430), offensive but not hate speech (19,190), and neither hate nor offensive speech (4,163) by Figure Eight's (formerly CrowdFlower) members. The data was gathered via the Twitter API, filtering tweets containing HS words that Internet users submitted to Hatebase.org. The outcome was a sample of tweets from 33,548 Twitter users. 85.4 million tweets were collected from the accounts of all users. A random sample of this collection is the final dataset of 24,783 tweets. Nevertheless, this dataset lacks diversity in terms of HS content. For example, the gender-based hate speech tweets are biased towards women, while the greatest number of hate speech tweets contain ethnicity content.

Research focusing on the identification of misogynistic language on Twitter uses a dataset called Automatic Misogyny Identification (AMI)~\cite{fersini2018overview} with 4.000 annotated comments and their labels that define if they are misogynous or not. Apart from this binary labelling, every comment is defined by two extra fields. The first one concerns the type of misogynistic behaviour and takes the following values: stereotype, dominance, derailing, sexual harassment, discredit or none if the tweet is not misogynous. The second field concerns the subject of the misogynistic tweet and takes the following values: active, when it attacks a specific target (individual), passive, when it denotes potential receivers (generic), and again none, if there is no misogyny in the tweet.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.75\textwidth]{CreationStagesV2.png}
    \caption{Dataset creation stages flowchart}
    \label{fig:dcreation}
\end{figure*}

The largest online community of white nationalists, called Stormfront, was used as the source of the dataset in~\cite{whitesupremacist}. The content in this forum revolves around discussions of race, with various degrees of offensiveness, included. The annotation of the samples is at the sentence level, which is a technique that keeps the smallest unit containing hate speech and reduces noise. The dataset
contains 10,568 sentences that are classified as hate speech (1,119 comments) or not (8,537 comments), as well as two supplementary classes, {\em relation} for sentences that express hate speech only when related to each other and {\em skip} for sentences which are not in English or do not contain any information as to be classified as hate speech or not. Furthermore, information like the post identifier and the sentence’s position in the post, a user identifier and a sub-forum identifier, as well as the number of previous posts the annotator had to read before making a decision over the sentence’s category are also recorded. The samples were picked randomly from 22 sub-forums covering diverse topics and nationalities.

The dataset introduced in~\cite{foxnews} consists of 1,528 Fox News user comments (435 hateful), which were acquired from 10 discussion threads of 10 widely read Fox News articles published during August 2016. Context information is considered extremely important, so details such as the screen name of the user, all the comments in the same thread and the original article, are also included.

Another research direction is to investigate the detection of HS by mining data from non-English languages, either for developing monolingual ML models regarding their source languages, or for proposing multilingual solutions~\cite{DBLP:conf/lrec/SanguinettiPBPS18, DBLP:conf/emnlp/OusidhoumLZSY19}. For the former case, more in-depth analysis of specialised idioms that govern low-resource languages is needed~\cite{DBLP:conf/ifip12/2019aiaiw, DBLP:journals/corr/abs-2003-07459}, while the latter case seems more challenging, favouring the exploitation of correlations that appear between distinct languages which may reveal useful insights, or even enable the face of code switching~\cite{DBLP:conf/coling/GaleryC18}. Reviewing the recently demonstrated multilingual work~\cite{DBLP:conf/emnlp/OusidhoumLZSY19}, a trilingual dataset was created, including English, French and Arabic tweets, trying initially to mine similar expressions of 15 common phrases over the examined languages. After some modifications of the selected phrases, different sources of obscene phrases were searched, such as more sensitive topic-based discussions based on locality criteria. After the resolvement of some linguistic challenges that occur per separate language, and a strict rule set that was posed to human annotators from the Amazon Mechanical Turk platform to ensure trustworthy feedback, a pilot test set was provided. Having gathered the necessary evaluations, another one reconstruction of the label set was applied, before the final formulation of 5,647 English, 4,014 French and 3,353 Arabic tweets was reached, annotated over 5 separate tasks. Apart from the directness of each tweet, which was a binary label and was tackled better by single task language models, the rest 4 tasks included at least 5 label gradiations and their classification via multi task single/multi language or single/multi multilingual models was clearly boosted.

Finally, a small collection of 454 YouTube comments annotated as hate speech (120) or not (334) was introduced by the creators of the Hatebusters Platform~\cite{hatebusters}, which aims to establish an online inclusive community of volunteers actively reporting illegal hate speech content on YouTube. This dataset, through semi-supervised learning, was evolving in the Hatebusters Platform improving the classification accuracy of the ML models. However, this unpremeditated expansion of the dataset led to a more redundant variant of its original form. We use the initial collection of Hatebusters data as a seed to the protocol that we propose in the following section.


\section{ETHOS Dataset Creation}
In order to overcome the key weaknesses of the existing HS datasets, we introduce a small, yet fairly, informative dataset, ETHOS, that does not suffer from issues such as imbalanced or biased labels (e.g. gender), produced appropriately following the proposed protocol. Considering the aforementioned popular approaches of mining similar datasets for tackling with HS problem, we assume that an appropriate pre-process of such kind of initially collected data could improve in general their overall utilisation under ML or AI learning products, improving the total fitness of data quality, blending data mining techniques related with the field of Active Learning (AL)~\cite{bonwell1991active}, such as query strategy and crowdsourcing platforms. The overview of the proposed protocol is visualised through a flow chart in Figure~\ref{fig:dcreation}. More specialised comments are placed in the following subsections. The finally obtained dataset is the outcome of a 3-stage process, which we describe shortly in the current Section.

\subsection{Stage 1---Initial Dataset Creation and Manual Annotation}
The first three procedures, mentioned as ``Platform Selection \& Data Collection'', ``Data Prediction'' and ``Manual Data Annotation'', could be seen as the initial stage which is executed until a stopping criterion is satisfied regarding the cardinality of the collected instances, based on the original available HS dataset which operates as the input. This stage works like a ``stream'', specifically for groups of comments that we have already collected, annotating their weak labels' predictions through a predefined ML classifier, before an active selection and manually annotation takes place over some unlabelled  mined examples.

\subsubsection{Platform Selection \& Data Collection}
To create this dataset (), initially , a data collection protocol has been designed. We chose the platforms of Hatebusters\footnote{\url{{https://hatebusters.org}}}~\cite{hatebusters} and Reddit through the Public Reddit Data Repository\footnote{\url{{https://files.pushshift.io/reddit/comments/}}} to collect our data. 

Hatebusters Platform collects new data every day via the YouTube Data v3 API. After these new data have been collected, the Hatebusters Platform performs the classification process. The locally retained pre-trained ML model predicts the class of each comment, exporting a `hate' score. Currently, this model is a Support Vector Machine (SVM)~\cite{SVM} model with a linear kernel embedded with the well-known vectorization technique of the term frequency-inverse document frequency (TF-IDF)~\cite{TFIDFVec}.

Based on the input data, this first part was to query the Hatebusters database for comments already annotated by the corresponding users, without spending any monetisation resources. These comments seemed to be accurate, and they were the first group of comments to be manually annotated. The second part concerns the enrichment of the initially gathered comments, by querying Hatebusters' database with a specific frequency (e.g. daily) for a time period---in our case this was equal to two months---with various queries. However, based on the data we obtained each previous day, the applied queries have been updated concerning only the last day. For example, when we received a sufficient amount for all categories of HS, except for one category, the queries in the Hatebusters database were updated to make comments specific to the residual category. Later on, we will show the categories and the amount of comments we have received.

\iffalse
\begin{figure}[ht]
\centering
\begin{BVerbatim}
SELECT text
FROM comment
WHERE date >= `2019-04-20'
\end{BVerbatim}
\caption{Collecting every new comment.}
\label{fig:easySQL}
\end{figure}

\begin{figure}[ht]
\centering
\begin{BVerbatim}
SELECT text
FROM comment 
  inner join comment_interests on 
  comment.id = comment_interests.cid
  inner join interest on 
  comment_interests.iid = interests.id
WHERE date >= `2019-04-20' 
  AND ( inter = `disabled' )
\end{BVerbatim}
\caption{Collecting new comments from specific categories.}
\label{fig:neasySQL}
\end{figure}
\fi


The final part of the data collection process was based on a public Reddit data archive, which provides batches of files regarding Reddit comments on a monthly basis. The files of this directory were processed through a JSON crawler for selecting comments from specific subreddits for particular time periods. The discovery of subreddits incorporating different HS contents has been investigated~\cite{wikiIncels,wikiDonald}, we distinguished the next entities:
\begin{itemize}
    \item \textbf{Incels}: This subreddit became known as a place where men blamed women for their unintended celibacy, often promoting rape or other abuse. Those posts had a misogynistic and sometimes racist content. This subreddit was terminated on 7 November 2017.
    \item \textbf{TheRedPill}: TheRedpill is a subreddit devoted to the rights of men, containing material of a misogynous character.
    \item \textbf{The\_Donald}: The\_Donald is a subreddit where the participants create discussions and memes supportive of U.S. President Donald Trump. This subreddit has been described as hosting conspiracy theories and content that is racist, misogynous, Islamophobic, and antisemitic.
    \item \textbf{RoastMe}: In this subreddit, reddit users can ask subreddit followers to `roast' them, namely to insult them.
\end{itemize}

While some of these subreddits were suspended and shut down by Reddit at the end of 2017 due to their context, it was possible to access comments from these subreddits by selecting files from the archive for October 2017 and earlier.

\subsubsection{Data Prediction}
The second process of the Stage 1 is the ``Data Prediction''. For each batch of comments extracted from the first stage, the assignment of some useful labels to the available unlabelled set () is triggered through an ML model trained on an expanded version () of the Hatebusters' dataset () and the new data annotated on Stage 3 (). In every iteration of Stage 1, we were performing a grid search among a bunch of classification methods in the currently expanded dataset, for obtaining the best algorithm examining its efficacy through a typical 10-fold-CV process so as to be selected as the annotator of the ().

Among these grid searches, we evaluated various ML models, such as SVMs, Random Forests (RF), Logistic Regression (LR), as well as some simple or more complex architectures of Neural Networks (NNs). In addition to the classifier tuning, several vectorization techniques were also examined in this search, such as TF-IDF vectorization~\cite{TFIDFVec} with word or char -grams ( from 1 to 13).

\subsubsection{Manual Data Annotation} 
By the end of the ``Data Prediction'' phase, the ``Data Annotation'' process is initiated. In the sense of AL concept, using a combination of Query Strategies, similar to Uncertainty Sampling and Maximum Relevance with predefined ranges of accepted confidence values based on the expected labels of the classifier we trained in the previous step, we pick informative comments for manual annotation~\cite{DBLP:journals/kbs/PupoAV18}. More specifically, we were selecting for annotation the comments within the  probability range, while we were examining few comments in the ranges  to detect any major misclassification. Eventually, only comments with specific labels and content were added to the new dataset () in order to preserve both the balance of the labels and the diversity of the comments per label. At the end of this process, if the number of comments collected is not more than 1000, we will update the , and Stage 1 will be repeated to request new unlabelled comments. Otherwise, Stage 2 will be triggered.

\subsection{Stage 2---Data Validation via Figure-Eight Platform}
The second stage will begin when the 1000 comments have been collected. Moreover, the Hatebusters' dataset is discarded, since it does not further contribute to our protocol. After a number of different experiments on the Figure-Eight Platform, we settled on the following technique.
Firstly, given a specific comment, we ask the contributors to identify whether that comment \textit{contains HS or not}. If the comment contains HS, then we raise 3 more questions: i) whether the comment \textit{incites violence}, defining violence as ``the use of physical force to injure, abuse, damage, or destroy'', ii) whether the comment includes \textit{directed} or \textit{generalized} hate speech. The case of targeting a single person or a small group of people is defined as directed HS, whereas the case of targeting a class or a large group of people is described as generalised HS. Finally, we ask the contributors to pick \textit{one} or \textit{more} from the following \textit{hate speech categories}, which, according to their opinion, better reflect the content of the comments. The categories of HS concern gender, race, national origin, disability, religion and sexual orientation.

\iffalse
The types of HS are as follows:
\begin{itemize}
    \item \textit{Gender}: When a comment is targeting gender (ex. All men should die).
    \item \textit{Race}: When a comment is targeting a specific race (ex. Black people are apes).
    \item \textit{National Origin}: When a comment is targeting a specific nation (ex. Chinese people should be deported).
    \item \textit{Disability}: When a comment contains HS about one of these disability categories:
        \begin{itemize}
        \item Autism Spectrum Disorder
        \item Deaf-blindness
        \item Developmental Delay
        \item Hearing Impairment, Including Deafness
        \item Infant/Toddler with a Disability
        \item Intellectual Disability
        \item Orthopaedic Impairment
        \item Specific Learning Disability (SLD)
        \item Serious Emotional Disability
        \item Speech or Language Impairment
        \item Traumatic Brain Injury (TBI)
        \end{itemize}
    \item \textit{Religion}: When a comment is targeting a specific religion (ex. Christians are true devils, we should get rid of them).
    \item \textit{Sexual Orientation}: When a comment is targeting a specific sexual preference (ex. I hate gay people).
\end{itemize}{}
\fi
\begin{figure}[ht]
\centering
\includegraphics[width=0.36\textwidth]{figure-eight-design.png}
\caption{Example of Figure-Eight design on the platform}
\label{fig:figureEightExample}
\end{figure}

Through Figure~\ref{fig:figureEightExample}, an example of the designed query template on the crowdsourcing platform is presented, depicting the types of questions and potential responses. With respect to the proposed template, we introduced a set of 40 questions for validating the trustworthiness of human annotators, as well as a test run of 100 unlabelled comments, with 3 different contributors determining the labels for each one of these. The feedback we collected helped us to finalise some design and instruction issues. Most importantly, we have fixed some naive and few serious mistakes on the validation questions, such as the following:

\textit{[naive mistake]}: For the comment ``Jews are lower class pigs.'' we had `Yes' on question `Does this comment incites violence?', while we gave as an explanation ``It spreads hatred, but not violence''.

\textit{[serious mistake]}: We misclassified the comment ``Queers are an abomination and need to be helped to go straight to Hell!'' to the wrong category. By receiving the feedback we fixed it.

Then, we executed the task for the whole , collecting 5,360 judgements. Almost every comment was therefore annotated by five different annotators. The level of expertise of the annotators was the 3rd, on a scale of 3 levels. ``The 3rd level annotators are the smallest group of the most experienced, most accurate, contributors'' according to the Figure-Eight System.

\subsection{Stage 3---Dataset Configuration}
The third and final stage is the configuration of the dataset. Taking as input the results from the second stage, the dataset takes its final form. Examining the annotated data one last time manually, we checked for any mistake or misclassification. Few errors occurred on some of the most disambiguous examples, assuring us about the quality of the annotators that participated in our task. 


\subsection{ETHOS Dataset Overview}

Two datasets\footnote{\url{https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset}} were the product of the above operation. The first one, ``Ethos\_Dataset\_Binary.csv'', includes 998 comments and a label on the presence or absence of hate speech content (`\textit{isHate}'). The second file, called ``Ethos\_Dataset\_Multi\_Label.csv'', includes 433 hate speech messages along with the following 8 labels: (`\textit{violence}', `\textit{directed\_vs\_generalized}', `\textit{gender}', `\textit{race}', `\textit{national\_origin}', `\textit{disability}', `\textit{sexual\_orientation}', `\textit{religion}'). 

For every comment ,  annotators voted for the labels that we set. The label `\textit{isHate}' was the result of summing up the positive votes  of the contributors, divided by , so its values are within the range of . We measured the `\textit{violence}' label by summarising the positive votes of the contributors  to the question: ``Does this comment incite violence?'', which was divided by  to be normalised to . Likewise, the value of the label `\textit{directed\_vs\_generalized}' was determined by summarising the annotators replied `directed'  to the question, ``Is this comment targeting a specific individual (directed) or a group/class of people (generalized)?'', divided by . Finally, we accumulated the votes of the  contributors for each of the 6 hate speech categories, and dividing them by , we obtained six independent labels.

\begin{figure}[ht]
\centering
\includegraphics[width=0.36\textwidth]{ration_of_labels.png}
\caption{Ratio of labels}
\label{fig:isHateLabels}
\end{figure}

This dataset achieves to create labels with balanced classes. In particular, it maintains balance between the two classes of `isHate' label almost perfect balance between the 6 labels of hate speech categories, while it has a fair ratio between the other labels too (Figure~\ref{fig:isHateLabels}). In Table~\ref{tab:correlTable}, the balance between hate speech categories (last column) and their correlation with violence and directed/generalized labels is further portrayed.

\begin{table}[ht]
\centering
\resizebox{0.375\textwidth}{!}{\begin{tabular}{c|c|c|c|c|c}
\cline{2-5}
 & V-D & nV-D & V-G & nV-G & \\ \hline
\multicolumn{1}{|c|}{Gender} & 14 & 22 & 13 & 37 & \multicolumn{1}{c|}{86} \\ \hline
\multicolumn{1}{|c|}{Race} & 4 & 13 & 12 & 47 & \multicolumn{1}{c|}{76} \\ \hline
\multicolumn{1}{|c|}{National Origin} & 5 & 11 & 18 & 40 & \multicolumn{1}{c|}{74} \\ \hline
\multicolumn{1}{|c|}{Disability} & 12 & 15 & 8 & 18 & \multicolumn{1}{c|}{53} \\ \hline
\multicolumn{1}{|c|}{Religion} & 11 & 8 & 24 & 38 & \multicolumn{1}{c|}{81} \\ \hline
\multicolumn{1}{|c|}{Sexual Orientation} & 11 & 15 & 11 & 36 & \multicolumn{1}{c|}{73} \\ \hline
 & 57 & 84 & 86 & 216 & \multicolumn{1}{c|}{443} \\ \cline{2-6}
\end{tabular}}
\caption{Correlation of HS categories with violence (V)/not violence (nV) and directed (D)/generalized (G) labels}
\label{tab:correlTable}
\end{table}

\renewcommand{\thetable}{3}
\begin{table*}[t]
\centering
\resizebox{0.85\textwidth}{!}{\begin{tabular}{|
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |}
\hline
{\color[HTML]{000000} \textbf{}} & {\color[HTML]{000000} \textbf{F1 Score}} & {\color[HTML]{000000} \textbf{F1 Hate}} & {\color[HTML]{000000} \textbf{Accuracy}} & {\color[HTML]{000000} \textbf{Precision}} & {\color[HTML]{000000} \textbf{Sensitivity}} & {\color[HTML]{000000} \textbf{Recall}} & {\color[HTML]{000000} \textbf{Recall Hate}} & {\color[HTML]{000000} \textbf{Specificity}} & {\color[HTML]{000000} \textbf{TN}} & {\color[HTML]{000000} \textbf{FP}} & {\color[HTML]{000000} \textbf{FN}} & {\color[HTML]{000000} \textbf{TP}} \\ \hline
{\color[HTML]{000000} \textbf{MultinomialNB}} & {\color[HTML]{000000} 63.78} & {\color[HTML]{000000} 59.14} & {\color[HTML]{000000} 64.73} & {\color[HTML]{000000} 64.06} & {\color[HTML]{000000} 58.82} & {\color[HTML]{000000} 63.96} & {\color[HTML]{000000} 59.45} & {\color[HTML]{000000} 69.2} & {\color[HTML]{000000} 391} & {\color[HTML]{000000} 173} & {\color[HTML]{000000} 179} & {\color[HTML]{000000} 255} \\ \hline
{\color[HTML]{000000} \textbf{BernoulliNB}} & {\color[HTML]{000000} 47.78} & {\color[HTML]{000000} 44.52} & {\color[HTML]{000000} 48.3} & {\color[HTML]{000000} 48.23} & {\color[HTML]{000000} 47.81} & {\color[HTML]{000000} 48.16} & {\color[HTML]{000000} 41.65} & {\color[HTML]{000000} 48.51} & {\color[HTML]{000000} 275} & {\color[HTML]{000000} 290} & {\color[HTML]{000000} 226} & {\color[HTML]{000000} 207} \\ \hline
{\color[HTML]{000000} \textbf{Logistic Regression}} & {\color[HTML]{000000} 66.5} & {\color[HTML]{000000} 64.35} & {\color[HTML]{000000} 66.94} & {\color[HTML]{000000} 66.94} & {\color[HTML]{000000} 68.78} & {\color[HTML]{000000} 67.07} & {\color[HTML]{000000} 60.46} & {\color[HTML]{000000} 65.36} & {\color[HTML]{000000} 370} & {\color[HTML]{000000} 195} & {\color[HTML]{000000} 135} & {\color[HTML]{000000} 298} \\ \hline
{\color[HTML]{000000} \textbf{SVM}} & {\color[HTML]{000000} 66.07} & {\color[HTML]{000000} 63.77} & {\color[HTML]{000000} 66.43} & {\color[HTML]{000000} 66.47} & {\color[HTML]{000000} 68.08} & {\color[HTML]{000000} 66.7} & {\color[HTML]{000000} 59.96} & {\color[HTML]{000000} 65.32} & {\color[HTML]{000000} 368} & {\color[HTML]{000000} 197} & {\color[HTML]{000000} 138} & {\color[HTML]{000000} 295} \\ \hline
{\color[HTML]{000000} \textbf{Ridge}} & {\color[HTML]{000000} 65.47} & {\color[HTML]{000000} 61.64} & {\color[HTML]{000000} 66.24} & {\color[HTML]{000000} 65.71} & {\color[HTML]{000000} 62.51} & {\color[HTML]{000000} 65.8} & {\color[HTML]{000000} 60.79} & {\color[HTML]{000000} 69.09} & {\color[HTML]{000000} 390} & {\color[HTML]{000000} 174} & {\color[HTML]{000000} 163} & {\color[HTML]{000000} 271} \\ \hline
{\color[HTML]{000000} \textbf{Decision Trees}} & {\color[HTML]{000000} 61.04} & {\color[HTML]{000000} 56.51} & {\color[HTML]{000000} 61.81} & {\color[HTML]{000000} 61.48} & {\color[HTML]{000000} 57.18} & {\color[HTML]{000000} 61.52} & {\color[HTML]{000000} 55.85} & {\color[HTML]{000000} 65.86} & {\color[HTML]{000000} 369} & {\color[HTML]{000000} 195} & {\color[HTML]{000000} 186} & {\color[HTML]{000000} 248} \\ \hline
{\color[HTML]{000000} \textbf{Random Forests}} & {\color[HTML]{000000} 64.41} & {\color[HTML]{000000} 60.07} & {\color[HTML]{000000} 65.04} & {\color[HTML]{000000} 64.69} & {\color[HTML]{000000} 60.61} & {\color[HTML]{000000} 64.68} & {\color[HTML]{000000} 59.54} & {\color[HTML]{000000} 68.75} & {\color[HTML]{000000} 387} & {\color[HTML]{000000} 179} & {\color[HTML]{000000} 170} & {\color[HTML]{000000} 262} \\ \hline
{\color[HTML]{000000} \textbf{AdaBoost}} & {\color[HTML]{000000} 63.78} & {\color[HTML]{000000} 59.5} & {\color[HTML]{000000} 64.52} & {\color[HTML]{000000} 64.18} & {\color[HTML]{000000} 60.06} & {\color[HTML]{000000} 64} & {\color[HTML]{000000} 58.94} & {\color[HTML]{000000} 67.95} & {\color[HTML]{000000} 384} & {\color[HTML]{000000} 181} & {\color[HTML]{000000} 173} & {\color[HTML]{000000} 260} \\ \hline
{\color[HTML]{000000} \textbf{Gradient Boosting}} & {\color[HTML]{000000} 63.55} & {\color[HTML]{000000} 59.21} & {\color[HTML]{000000} 64.33} & {\color[HTML]{000000} 64.34} & {\color[HTML]{000000} 59.67} & {\color[HTML]{000000} 64.2} & {\color[HTML]{000000} 58.76} & {\color[HTML]{000000} 68.73} & {\color[HTML]{000000} 384} & {\color[HTML]{000000} 182} & {\color[HTML]{000000} 174} & {\color[HTML]{000000} 258} \\ \hline
{\color[HTML]{000000} \textbf{CNN+Attention+FT+GV}} & {\color[HTML]{000000} 74.41} & {\color[HTML]{000000} 70.46} & {\color[HTML]{000000} 75.15} & {\color[HTML]{000000} 74.92} & {\color[HTML]{000000} 68.36} & {\color[HTML]{000000} 74.35} & {\color[HTML]{000000} 72.73} & {\color[HTML]{000000} \textbf{80.35}} & {\color[HTML]{000000} 454} & {\color[HTML]{000000} 111} & {\color[HTML]{000000} 137} & {\color[HTML]{000000} 296} \\ \hline
{\color[HTML]{000000} \textbf{CNN+LSTM+GV}} & {\color[HTML]{000000} 72.13} & {\color[HTML]{000000} 68.6} & {\color[HTML]{000000} 72.94} & {\color[HTML]{000000} 73.47} & {\color[HTML]{000000} 68.14} & {\color[HTML]{000000} 72.4} & {\color[HTML]{000000} 69.07} & {\color[HTML]{000000} 76.65} & {\color[HTML]{000000} 433} & {\color[HTML]{000000} 132} & {\color[HTML]{000000} 138} & {\color[HTML]{000000} 295} \\ \hline
{\color[HTML]{000000} \textbf{LSTM+FT+GV}} & {\color[HTML]{000000} 72.85} & {\color[HTML]{000000} 69.42} & {\color[HTML]{000000} 73.43} & {\color[HTML]{000000} 73.37} & {\color[HTML]{000000} 69.5} & {\color[HTML]{000000} 72.97} & {\color[HTML]{000000} 69.33} & {\color[HTML]{000000} 76.44} & {\color[HTML]{000000} 432} & {\color[HTML]{000000} 133} & {\color[HTML]{000000} 132} & {\color[HTML]{000000} 301} \\ \hline
{\color[HTML]{000000} \textbf{FF+LSTM+CNN+FT+GV}} & {\color[HTML]{000000} 74.08} & {\color[HTML]{000000} 71.37} & {\color[HTML]{000000} 74.65} & {\color[HTML]{000000} 74.85} & {\color[HTML]{000000} 72.82} & {\color[HTML]{000000} 74.44} & {\color[HTML]{000000} 69.97} & {\color[HTML]{000000} 76.07} & {\color[HTML]{000000} 430} & {\color[HTML]{000000} 135} & {\color[HTML]{000000} 118} & {\color[HTML]{000000} 315} \\ \hline
{\color[HTML]{000000} \textbf{BiLSTM+FT+GV}} & {\color[HTML]{000000} 76.85} & {\color[HTML]{000000} 74.15} & {\color[HTML]{000000} \textbf{77.45}} & {\color[HTML]{000000} 77.99} & {\color[HTML]{000000} 74.55} & {\color[HTML]{000000} 77.1} & {\color[HTML]{000000} \textbf{73.76}} & {\color[HTML]{000000} 79.66} & {\color[HTML]{000000} 450} & {\color[HTML]{000000} 115} & {\color[HTML]{000000} 110} & {\color[HTML]{000000} 323} \\ \hline
{\color[HTML]{000000} \textbf{BiLSTM+Attention+FT}} & {\color[HTML]{000000} 76.8} & {\color[HTML]{000000} 74.01} & {\color[HTML]{000000} 77.34} & {\color[HTML]{000000} 77.76} & {\color[HTML]{000000} 74.37} & {\color[HTML]{000000} 77} & {\color[HTML]{000000} 73.66} & {\color[HTML]{000000} 79.63} & {\color[HTML]{000000} 450} & {\color[HTML]{000000} 115} & {\color[HTML]{000000} 111} & {\color[HTML]{000000} 322} \\ \hline
{\color[HTML]{000000} \textbf{BERT}} & {\color[HTML]{000000} \textbf{78.83}} & {\color[HTML]{000000} \textbf{74.45}} & {\color[HTML]{000000} 76.64} & {\color[HTML]{000000} \textbf{79.17}} & {\color[HTML]{000000} \textbf{78.43}} & {\color[HTML]{000000} \textbf{78.43}} & {\color[HTML]{000000} 70.85} & {\color[HTML]{000000} 74.31} & {\color[HTML]{000000} 425} & {\color[HTML]{000000} 139} & {\color[HTML]{000000} 94} & {\color[HTML]{000000} 340} \\ \hline
\end{tabular}}
\caption{Performance of selected models on binary HS classification}
\label{tab:results1}
\end{table*}

\section{Dataset Evaluation}
In order to evaluate ETHOS, after pre-processing the data, we used a variety of different algorithms in binary and multi-label scope to present the performance of the baseline models in this dataset. For the purpose of providing the unbiased performance of each algorithm, we performed nested-CV~\cite{nestedCrossValidation} evaluation for every algorithm except NNs, where we applied 10-fold-CV~\cite{crossValidation}. In addition, we binarise the values of each label, which are initially discrete in a range of [0,1], to the \{0,1\} classes using the rule \textit{``If value  Else value ''}.  More in-depth details are provided next.

\subsection{Data Pre-processing}
The pre-processing methodology used in our case begins with lowercasing transformation, contraction transformations (Table~\ref{tab:my-phrase}), removal of punctuation marks, stemming and lemmatization via Snow-ball stemmer~\cite{snowball} and WordNet lemmatizer~\cite{wordnet}. 

\renewcommand{\thetable}{2}
\begin{table}[ht]
\centering
\resizebox{0.47\textwidth}{!}{\begin{tabular}{|ccc|ccc|ccc|}
\hline
\multicolumn{9}{|c|}{Phrases and words transformations}                          \\ \hline
“what's”  & to & “what is”  & “'ll”    & to & “ will”  & “'s”    & to & “ is”    \\
“don't”   & to & “do not”   & “i'm”    & to & “i am”    & “'ve”   & to & “ have”  \\
“doesn't” & to & “does not” & “he's”   & to & “he is”  & “isn't” & to & “is not” \\
“that's”  & to & “that is”  & “she's”  & to & “she is” & “'re”   & to & “ are”   \\
“aren't”  & to & “are not”  & “it's”   & to & “it is”  & “'d”    & to & “ would” \\
“\%”      & to & “ percent” & “e-mail” & to & “e mail” &         &    &          \\ \hline
\end{tabular}}
\caption{Phrases and words transformations}
\label{tab:my-phrase}
\end{table}

\subsection{Vectorization and Word Embeddings}
Before we proceed to the classification experiments, we have to transform the pre-processed textual data into word vectors. The applied methodologies for these transformations are the TF-IDF Vectorizer and the Text-to-Sequences. Particularly, for the former, several parameter tuples of (n\_gram, max\_features, stopwords existence) were examined, while on the latter, we set the corresponding number of maximum features at 50,000. Furthermore, 4 pre-trained models that concern computation of embeddings were included: FastText (FT)~\cite{fastText}, GloVe (GV)~\cite{glove}, a combination of FastText and Glove, as well as Bert Language Model (BERT)~\cite{bert}.

\iffalse
\begin{itemize}
    \item FastText (FT)~\cite{fastText}: 2 million word vectors trained on Common Crawl with 600B tokens\footnote{\url{https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip}}
    \item GloVe (GV)~\cite{glove}: Common Crawl with 42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download\footnote{\url{http://nlp.stanford.edu/data/glove.42B.300d.zip}}
    \item FastText \& GloVe concatenated
    \item Bert (BERT)~\cite{bert}: pre-trained language model ``bert\_u-ncased\_L-12\_H-768\_A-12'' trained on English Wikipedia and the BookCorpus
\end{itemize}
\fi

\subsection{Binary Classification}
A lot of applications approach the problem of HS detection through a binary scope. It is therefore necessary to present the performance of state-of-the-art algorithms on the binary version of this dataset before proceeding with the multi-label classification experiments.

\subsubsection{Algorithms}
We used the following algorithms for our experiments in the binary level: Multinomial and Bernoulli variations of Naive Bayes (MNB and BNB, respectively)~\cite{NB1,NB2}, LR and Ridge Regression as well, SVMs, Decision Trees (DTs)~\cite{DTrees}, RF, AdaBoost (Ada)~\cite{ABoost} and Gradient Boosting (Grad)~\cite{GBoost}. Moreover, we used six different NN architectures, as other similar works attempt~\cite{DBLP:journals/corr/abs-2003-07459}. The first one utilises convolutional NNs (CNNs)~\cite{CNNnets} with an attention~\cite{attentionLayers} layer. The second is a compilation of CNNs and long short-term memory NNs (LSTMs)~\cite{LSTMnets}. A single LSTM-based NN constitutes the third architecture. The fourth model is an NN with multiple parallel layers, which contain CNNs, LSTMs and FeedForward layers (FFs). The last two architectures are Bidirectional LSTMs (BiLSTMs) without and with attention layers, respectively. We combined these NNs with FT and GV. Lastly, we used BERT language model, which was fine-tuned in our classification task.

\renewcommand{\thetable}{4}
\begin{table*}[t]
\centering
\resizebox{0.8\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 & \textbf{\begin{tabular}[c]{@{}c@{}}F1\\ Example\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}F1 \\ Macro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}F1 \\ Micro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}P \\ Example\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}P \\ Macro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}P \\ Micro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}R\\ Example\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}R\\ Macro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}R\\ Micro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}AP\\ Macro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}AP\\ Micro\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Subset\\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Hamming\\ Loss\end{tabular}} \\ \hline
\textbf{MLkNN} & 48.01 & 53.04 & 53.74 & 55.27 & 71.29 & 69.95 & 46.28 & 45.04 & 43.98 & 46.63 & 42.79 & 26.53 & 0.1566 \\ \hline
\textbf{MLARAM} & 18.47 & 6.06 & 18.71 & 21.44 & 3.78 & 21.44 & 17.69 & 16.25 & 18.27 & 20.79 & 21.55 & 7.15 & 0.2948 \\ \hline
\textbf{BR} & 48.59 & 52.49 & 56.76 & 57.69 & 79.74 & 79.37 & 45.30 & 42 & 44.37 & 47.66 & 47.04 & 26.28 & 0.1395 \\ \hline
\textbf{CC} & 56.51 & 59.24 & 58.23 & 62.49 & 69.08 & 63.44 & 56.54 & 56.22 & 53.99 & 49.74 & 44.07 & 31.4 & 0.1606 \\ \hline
\textbf{NNBR} & \textbf{70.95} & \textbf{73.95} & \textbf{72.26} & \textbf{76.43} & \textbf{80.3} & 76.21 & \textbf{71.76} & \textbf{71.14} & \textbf{68.92} & \textbf{64.82} & \textbf{59.3} & \textbf{41.79} & \textbf{0.1097} \\ \hline
\textbf{NNCC} & 54.19 & 56.94 & 59.12 & 63.96 & 76.61 & \textbf{82.31} & 50.51 & 49.03 & 46.76 & 54.47 & 49.71 & 31.82 & 0.1320 \\ \hline
\end{tabular}}
\caption{Performance of selected models on multi-label HS classification (P: Precision, R: Recall, AP: Average Precision)}
\label{tab:mllresults}
\end{table*}

\subsubsection{Metrics}
The metrics that we have chosen are accuracy and precision, recall and F1-score with macro indication, as well as the confusion matrix. Furthermore, we calculate specificity , which measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of comments without HS content who are correctly identified as not containing HS) as well as sensitivity , which measures the proportion of actual positives that are correctly identified as such (e.g., the percentage of HS comments who are correctly identified as containing HS). However, in applications like HS monitoring where human interference is essential to ensure that users' rights are not abused on the grounds of incorrect HS charges, we must rely on metrics such as high recall and precision of HS category that they could guarantee to not overwhelm the human personnel on checking redundant content.

\subsubsection{Results}
The handling of textual data is a thoroughly researched task and has a dedicated category, NLP, which stands for natural language processing. We used common and widely accepted techniques to process our textual data, utilising TF-IDF vectorizers, pre-trained embeddings and language models. In Table~\ref{tab:results1}, we are showcasing the results of the selected evaluation processes per each classifier. The best performance per metric is highlighted in bold format. The NNs seem to outperform the conventional ML techniques. It is worth mentioning that DTs, as well as the MNB and BNB, have the lowest performance in terms of almost every metric. Tree-ensembles seem to have similar performance between them, but lower compared to the SVMs and the rest linear models.

Between the examined NNs, those who achieved the highest performance using embeddings were the architectures using BiLSTMs. BiLSTMs with FT + GV embeddings achieved the highest recall on hate category, as well as high overall accuracy. Finally, BERT outperformed every other model in almost any metric. BERT achieved this performance using fine-tuning on the dataset, validating its superior performance on similar tasks~\cite{DBLP:conf/fire/RanasingheZH19}.

\subsection{Multi-Label Classification}
Providing a dataset with multi-label information about HS, we accomplish to uncover new possible applications. HS is indeed an ML task that cannot be completely studied just through the binary aspect. Indeed, it is a multi-dimensional task involving, for example, the presence of violence in the HS Act. Another dimension may be whether the target of the act is directed or generalised, as well as the HS category to which it is oriented.

\subsubsection{Algorithms}
The algorithms handling multi-label data can be either problem transformation techniques or adaptation approaches~\cite{tsoum}. MlkNN~\cite{zhang2007ml} and MLARAM \cite{MLARAM} adaptation methods, as well as Binary Relevance (BR) and Classifier Chains (CC)~\cite{read2009classifier} transformation methods with base classification models like LR, SVMs and RF, are utilised. We used FT embeddings for our NNs architectures and designed models inspired by classic MLC systems, such as BR and CC. Specifically, NNBR is an NN containing BiLSTMs, an attention layer, two FFs and an output layer with 8 outputs in a BR fashion. The NNCC architecture is inspired by the CC technique and is quite similar to the NNBR except for the output where each label is given as input for the next label prediction.

\subsubsection{Metrics}
In the evaluation of MLC systems, a very common measure is the Hamming loss which measures the symmetric difference between the ground truth labels and the predicted ones. Furthermore, subset accuracy measuring symmetric similarity, as well as precision, recall and F1-score, are contained here. These evaluation metrics are instance-based. Moreover, it is necessary to compute some label-based metrics like -macro and -micro, where F1, Precision, Recall.

\subsubsection{Results}
After testing this multi-label dataset using nested CV for MLkNN, MLARAM, CC and BR models, and 10-fold-CV for NNs, we are presenting the performance of each approach in Table~\ref{tab:mllresults}. We observe again the superior performance of neural-based approaches compared to classical ML techniques. Specifically, NNBR achieves the highest score in 12 out of 13 metrics, while we kept the same highlighting strategy as in Table 2.

\section{Discussion}

The provision of a new well-designed dataset to the public on a specific subject is always considered to be a significant contribution~\cite{DBLP:conf/cikm/SunAJHS19, DBLP:conf/cikm/HoangVN18}. In this sense, this multi-labelled dataset on HS, which we call ETHOS, collected from various social media, could be reused by other researchers or even industries for a wide variety of tasks regarding binary/multi-class and/or multi-label classification. 

In addition, the exported dataset can be combined with other similar HS datasets for evaluation reasons, or even for the development of hybrid Weakly supervised HS detection models, merging Semi-supervised and AL strategies under common frameworks, alleviating the human intervention based on decisions over the gathered unlabelled instances that come solely from the side of a robust learner~\cite{DBLP:journals/mlc/YuFXQ19, DBLP:conf/iisa/KarlosKAFK19}. Online HS detection and prevention tools such as Hatebusters~\cite{hatebusters}, among others, would be able to use this dataset to build better performing classifiers and use the category labels of this dataset to better assign comments to HS categories.

Another application, which appears to be beneficial to reviewers on social media platforms, might use the multi-labelled nature of this dataset to provide suggestions for comments containing HS, the level of violence, the target of comments and the categories of HS that are present in each comment. All this information could be very helpful to the reviewers, assisting them to conclude, without a doubt, the existence of HS in a comment.

However, this is not a multi-purpose HS detection dataset, as the data on which the comments are based are found in social media. This means that the corpora will have relatively small sentences. Thus, models trained on this dataset may fail to detect HS in documents of a larger scale without segmentation. On the other hand, the general structure of the proposed protocol could be applied to a variety of learning tasks , especially when they are based on large databases, since the proper exploitation of human expertise or crowdsourcing options could benefit them from more accurate predictions and less intensive annotation~\cite{DBLP:journals/biomedsem/DrameMD16}.

\begin{acks}
(co)winning CrowdFlower’s AI for Everyone Challenge for Q4'17\footnote{\url{https://cutt.ly/kyVoeNa}}.
\end{acks}

\bibliographystyle{ACM-Reference-Format}



\begin{thebibliography}{50}



\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{#1}\fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       {\relax}        \fi
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[\protect\citeauthoryear{against Racism and Intolerance}{against Racism
  and Intolerance}{2016}]{euCHS}
\bibfield{author}{\bibinfo{person}{European~Commission against Racism} {and}
  \bibinfo{person}{Intolerance}.} \bibinfo{year}{2016}\natexlab{}.
\newblock \bibinfo{title}{Code of conduct on countering illegal hate speech
  online}.
\newblock
\newblock
\urldef\tempurl \url{https://cutt.ly/RyV8fpj}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Almeida, Hidalgo, and Silva}{Almeida
  et~al\mbox{.}}{2013}]{almeida2013towards}
\bibfield{author}{\bibinfo{person}{Tiago Almeida}, \bibinfo{person}{Jos{\'e}
  Mar{\'\i}a~G{\'o}mez Hidalgo}, {and} \bibinfo{person}{Tiago~Pasqualini
  Silva}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Towards sms spam filtering: Results under a new
  dataset}.
\newblock \bibinfo{journal}{\emph{International Journal of Information Security
  Science}} \bibinfo{volume}{2}, \bibinfo{number}{1} (\bibinfo{year}{2013}),
  \bibinfo{pages}{1--18}.
\newblock


\bibitem[\protect\citeauthoryear{Anagnostou, Mollas, and Tsoumakas}{Anagnostou
  et~al\mbox{.}}{2018}]{hatebusters}
\bibfield{author}{\bibinfo{person}{Antonios Anagnostou},
  \bibinfo{person}{Ioannis Mollas}, {and} \bibinfo{person}{Grigorios
  Tsoumakas}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Hatebusters: A Web Application for Actively
  Reporting YouTube Hate Speech}. In \bibinfo{booktitle}{\emph{Proceedings of
  the Twenty-Seventh International Joint Conference on Artificial Intelligence,
  {IJCAI-18}}}. \bibinfo{publisher}{International Joint Conferences on
  Artificial Intelligence Organization}, \bibinfo{address}{Stockholm, Sweden},
  \bibinfo{pages}{5796--5798}.
\newblock
\urldef\tempurl \url{https://doi.org/10.24963/ijcai.2018/841}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Bahdanau, Cho, and Bengio}{Bahdanau
  et~al\mbox{.}}{2015}]{attentionLayers}
\bibfield{author}{\bibinfo{person}{Dzmitry Bahdanau},
  \bibinfo{person}{Kyunghyun Cho}, {and} \bibinfo{person}{Yoshua Bengio}.}
  \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{Neural Machine Translation by Jointly Learning to
  Align and Translate}. In \bibinfo{booktitle}{\emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, May 7-9, 2015,
  Conference Track Proceedings}}. \bibinfo{address}{San Diego, California,
  USA}.
\newblock
\urldef\tempurl \url{http://arxiv.org/abs/1409.0473}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Benites and Sapozhnikova}{Benites and
  Sapozhnikova}{2015}]{MLARAM}
\bibfield{author}{\bibinfo{person}{F. Benites} {and} \bibinfo{person}{E.
  Sapozhnikova}.} \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{HARAM: A Hierarchical ARAM Neural Network for
  Large-Scale Text Classification}. In \bibinfo{booktitle}{\emph{2015 IEEE
  International Conference on Data Mining Workshop (ICDMW)}}.
  \bibinfo{publisher}{IEEE Computer Society}, \bibinfo{address}{USA},
  \bibinfo{pages}{847--854}.
\newblock
\showISSN{2375-9259}
\urldef\tempurl \url{https://doi.org/10.1109/ICDMW.2015.14}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Bonwell and Eison}{Bonwell and Eison}{1991}]{bonwell1991active}
\bibfield{author}{\bibinfo{person}{Charles~C Bonwell} {and}
  \bibinfo{person}{James~A Eison}.} \bibinfo{year}{1991}\natexlab{}.
\newblock \showarticletitle{Active Learning: Creating Excitement in the
  Classroom. ERIC Digest.}
\newblock \bibinfo{journal}{\emph{ASHE-ERIC Higher Education Reports}}
  (\bibinfo{year}{1991}).
\newblock


\bibitem[\protect\citeauthoryear{Breiman, Friedman, Olshen, and Stone}{Breiman
  et~al\mbox{.}}{1984}]{DTrees}
\bibfield{author}{\bibinfo{person}{Leo Breiman}, \bibinfo{person}{Jerome~H
  Friedman}, \bibinfo{person}{Richard~A Olshen}, {and}
  \bibinfo{person}{Charles~J Stone}.} \bibinfo{year}{1984}\natexlab{}.
\newblock \showarticletitle{Classification and regression trees. Belmont, CA:
  Wadsworth}.
\newblock \bibinfo{journal}{\emph{International Group}}  \bibinfo{volume}{432}
  (\bibinfo{year}{1984}), \bibinfo{pages}{151--166}.
\newblock


\bibitem[\protect\citeauthoryear{Chen, Mao, Liu, Zhang, and Ma}{Chen
  et~al\mbox{.}}{2019}]{DBLP:conf/cikm/ChenMLZM19}
\bibfield{author}{\bibinfo{person}{Jia Chen}, \bibinfo{person}{Jiaxin Mao},
  \bibinfo{person}{Yiqun Liu}, \bibinfo{person}{Min Zhang}, {and}
  \bibinfo{person}{Shaoping Ma}.} \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{TianGong-ST: {A} New Dataset with Large-scale
  Refined Real-world Web Search Sessions}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 28th {ACM} International
  Conference on Information and Knowledge Management, {CIKM} 2019, November
  3-7, 2019}}. \bibinfo{publisher}{{ACM}}, \bibinfo{address}{Beijing, China},
  \bibinfo{pages}{2485--2488}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1145/3357384.3358158}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{contributors}{contributors}{2019}]{wikiHate}
\bibfield{author}{\bibinfo{person}{Wikipedia contributors}.}
  \bibinfo{year}{2019}\natexlab{}.
\newblock \bibinfo{title}{Hate speech --- {Wikipedia}{,} The Free
  Encyclopedia}.
\newblock
  \bibinfo{howpublished}{\url{https://en.wikipedia.org/w/index.php?title=Hate_speech&oldid=911114999}}.
\newblock


\bibitem[\protect\citeauthoryear{Davidson, Warmsley, Macy, and Weber}{Davidson
  et~al\mbox{.}}{2017}]{hateTweets}
\bibfield{author}{\bibinfo{person}{Thomas Davidson}, \bibinfo{person}{Dana
  Warmsley}, \bibinfo{person}{Michael Macy}, {and} \bibinfo{person}{Ingmar
  Weber}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Automated Hate Speech Detection and the Problem of
  Offensive Language}. In \bibinfo{booktitle}{\emph{Proceedings of the 11th
  International AAAI Conference on Web and Social Media}}
  \emph{(\bibinfo{series}{ICWSM '17})}. \bibinfo{publisher}{{AAAI} Press},
  \bibinfo{address}{Montreal, Canada}, \bibinfo{pages}{512--515}.
\newblock


\bibitem[\protect\citeauthoryear{de~Gibert, Perez, García-Pablos, and
  Cuadros}{de~Gibert et~al\mbox{.}}{2018}]{whitesupremacist}
\bibfield{author}{\bibinfo{person}{Ona de Gibert}, \bibinfo{person}{Naiara
  Perez}, \bibinfo{person}{Aitor García-Pablos}, {and} \bibinfo{person}{Montse
  Cuadros}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Hate Speech Dataset from a White Supremacy Forum}.
\newblock \bibinfo{journal}{\emph{Proceedings of the 2nd Workshop on Abusive
  Language Online (ALW2)}} (\bibinfo{year}{2018}).
\newblock
\urldef\tempurl \url{https://doi.org/10.18653/v1/w18-5102}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Devlin, Chang, Lee, and Toutanova}{Devlin
  et~al\mbox{.}}{2018}]{bert}
\bibfield{author}{\bibinfo{person}{Jacob Devlin}, \bibinfo{person}{Ming-Wei
  Chang}, \bibinfo{person}{Kenton Lee}, {and} \bibinfo{person}{Kristina
  Toutanova}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Bert: Pre-training of deep bidirectional
  transformers for language understanding}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:1810.04805}}
  (\bibinfo{year}{2018}).
\newblock


\bibitem[\protect\citeauthoryear{Dram{\'{e}}, Mougin, and Diallo}{Dram{\'{e}}
  et~al\mbox{.}}{2016}]{DBLP:journals/biomedsem/DrameMD16}
\bibfield{author}{\bibinfo{person}{Khadim Dram{\'{e}}}, \bibinfo{person}{Fleur
  Mougin}, {and} \bibinfo{person}{Gayo Diallo}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Large scale biomedical texts classification: a kNN
  and an ESA-based approaches}.
\newblock \bibinfo{journal}{\emph{J. Biomedical Semantics}}
  \bibinfo{volume}{7} (\bibinfo{year}{2016}), \bibinfo{pages}{40}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1186/s13326-016-0073-1}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Fersini, Rosso, and Anzovino}{Fersini
  et~al\mbox{.}}{2018}]{fersini2018overview}
\bibfield{author}{\bibinfo{person}{Elisabetta Fersini}, \bibinfo{person}{Paolo
  Rosso}, {and} \bibinfo{person}{Maria Anzovino}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Overview of the Task on Automatic Misogyny
  Identification at IberEval 2018.}. In \bibinfo{booktitle}{\emph{IberEval@
  SEPLN}}. \bibinfo{pages}{214--228}.
\newblock


\bibitem[\protect\citeauthoryear{Freund and Schapire}{Freund and
  Schapire}{1997}]{ABoost}
\bibfield{author}{\bibinfo{person}{Yoav Freund} {and} \bibinfo{person}{Robert~E
  Schapire}.} \bibinfo{year}{1997}\natexlab{}.
\newblock \showarticletitle{A decision-theoretic generalization of on-line
  learning and an application to boosting}.
\newblock \bibinfo{journal}{\emph{Journal of computer and system sciences}}
  \bibinfo{volume}{55}, \bibinfo{number}{1} (\bibinfo{year}{1997}),
  \bibinfo{pages}{119--139}.
\newblock


\bibitem[\protect\citeauthoryear{Friedman}{Friedman}{1999}]{GBoost}
\bibfield{author}{\bibinfo{person}{JH Friedman}.}
  \bibinfo{year}{1999}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{Stochastic gradient boosting. Department of
  Statistics}}.
\newblock \bibinfo{type}{{T}echnical {R}eport}. \bibinfo{institution}{Stanford
  University, Technical Report, San Francisco, CA}.
\newblock


\bibitem[\protect\citeauthoryear{Fukushima}{Fukushima}{1980}]{CNNnets}
\bibfield{author}{\bibinfo{person}{Kunihiko Fukushima}.}
  \bibinfo{year}{1980}\natexlab{}.
\newblock \showarticletitle{Neocognitron: A self-organizing neural network
  model for a mechanism of pattern recognition unaffected by shift in
  position}.
\newblock \bibinfo{journal}{\emph{Biological cybernetics}}
  \bibinfo{volume}{36}, \bibinfo{number}{4} (\bibinfo{year}{1980}),
  \bibinfo{pages}{193--202}.
\newblock


\bibitem[\protect\citeauthoryear{Galery and Charitos}{Galery and
  Charitos}{2018}]{DBLP:conf/coling/GaleryC18}
\bibfield{author}{\bibinfo{person}{Thiago Galery} {and}
  \bibinfo{person}{Efstathios Charitos}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Aggression Identification and Multi Lingual Word
  Embeddings}. In \bibinfo{booktitle}{\emph{Proceedings of the First Workshop
  on Trolling, Aggression and Cyberbullying, TRAC@COLING 2018, August 25,
  2018}}. \bibinfo{publisher}{Association for Computational Linguistics},
  \bibinfo{address}{Santa Fe, New Mexico, USA}, \bibinfo{pages}{74--79}.
\newblock
\urldef\tempurl \url{https://www.aclweb.org/anthology/W18-4409/}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Gao and Huang}{Gao and Huang}{2017}]{foxnews}
\bibfield{author}{\bibinfo{person}{Lei Gao} {and} \bibinfo{person}{Ruihong
  Huang}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Detecting Online Hate Speech Using Context Aware
  Models}. In \bibinfo{booktitle}{\emph{RANLP}}.
\newblock


\bibitem[\protect\citeauthoryear{Geisser}{Geisser}{1993}]{crossValidation}
\bibfield{author}{\bibinfo{person}{Seymour Geisser}.}
  \bibinfo{year}{1993}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{Predictive inference}}.
  Vol.~\bibinfo{volume}{55}.
\newblock \bibinfo{publisher}{CRC press}.
\newblock


\bibitem[\protect\citeauthoryear{Hoang, Vo, and Nejdl}{Hoang
  et~al\mbox{.}}{2018}]{DBLP:conf/cikm/HoangVN18}
\bibfield{author}{\bibinfo{person}{Tuan{-}Anh Hoang}, \bibinfo{person}{Khoi~Duy
  Vo}, {and} \bibinfo{person}{Wolfgang Nejdl}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{{W2E:} {A} Worldwide-Event Benchmark Dataset for
  Topic Detection and Tracking}. In \bibinfo{booktitle}{\emph{Proceedings of
  the 27th {ACM} International Conference on Information and Knowledge
  Management, {CIKM} 2018, Torino, Italy, October 22-26, 2018}}.
  \bibinfo{publisher}{{ACM}}, \bibinfo{pages}{1847--1850}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1145/3269206.3269309}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Hochreiter and Schmidhuber}{Hochreiter and
  Schmidhuber}{1997}]{LSTMnets}
\bibfield{author}{\bibinfo{person}{Sepp Hochreiter} {and}
  \bibinfo{person}{J{\"u}rgen Schmidhuber}.} \bibinfo{year}{1997}\natexlab{}.
\newblock \showarticletitle{Long short-term memory}.
\newblock \bibinfo{journal}{\emph{Neural computation}} \bibinfo{volume}{9},
  \bibinfo{number}{8} (\bibinfo{year}{1997}), \bibinfo{pages}{1735--1780}.
\newblock


\bibitem[\protect\citeauthoryear{Joulin, Grave, Bojanowski, Douze, Jégou, and
  Mikolov}{Joulin et~al\mbox{.}}{2016}]{fastText}
\bibfield{author}{\bibinfo{person}{Armand Joulin}, \bibinfo{person}{Edouard
  Grave}, \bibinfo{person}{Piotr Bojanowski}, \bibinfo{person}{Matthijs Douze},
  \bibinfo{person}{Hérve Jégou}, {and} \bibinfo{person}{Tomas Mikolov}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \bibinfo{title}{FastText.zip: Compressing text classification
  models}.
\newblock
\newblock
\showeprint[arxiv]{cs.CL/1612.03651}


\bibitem[\protect\citeauthoryear{Karlos, Kanas, Aridas, Fazakis, and
  Kotsiantis}{Karlos et~al\mbox{.}}{2019}]{DBLP:conf/iisa/KarlosKAFK19}
\bibfield{author}{\bibinfo{person}{Stamatis Karlos},
  \bibinfo{person}{Vasileios~G. Kanas}, \bibinfo{person}{Christos~K. Aridas},
  \bibinfo{person}{Nikos Fazakis}, {and} \bibinfo{person}{Sotiris Kotsiantis}.}
  \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{Combining Active Learning with Self-train algorithm
  for classification of multimodal problems}. In \bibinfo{booktitle}{\emph{10th
  International Conference on Information, Intelligence, Systems and
  Applications, {IISA} 2019, Patras, Greece, July 15-17, 2019}}.
  \bibinfo{pages}{1--8}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1109/IISA.2019.8900724}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{MacIntyre, Maglogiannis, Iliadis, and
  Pimenidis}{MacIntyre et~al\mbox{.}}{2019}]{DBLP:conf/ifip12/2019aiaiw}
\bibfield{editor}{\bibinfo{person}{John MacIntyre}, \bibinfo{person}{Ilias
  Maglogiannis}, \bibinfo{person}{Lazaros~S. Iliadis}, {and}
  \bibinfo{person}{Elias Pimenidis}} (Eds.). \bibinfo{year}{2019}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{Artificial Intelligence Applications and
  Innovations - {AIAI} 2019 {IFIP} {WG} 12.5 International Workshops: {MHDW}
  and 5G-PINE 2019, Hersonissos, Crete, Greece, May 24-26, 2019, Proceedings}}.
  \bibinfo{series}{{IFIP} Advances in Information and Communication
  Technology}, Vol.~\bibinfo{volume}{560}. \bibinfo{publisher}{Springer}.
\newblock
\showISBNx{978-3-030-19908-1}
\urldef\tempurl \url{https://doi.org/10.1007/978-3-030-19909-8}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Maron}{Maron}{1961}]{NB1}
\bibfield{author}{\bibinfo{person}{Melvin~Earl Maron}.}
  \bibinfo{year}{1961}\natexlab{}.
\newblock \showarticletitle{Automatic indexing: an experimental inquiry}.
\newblock \bibinfo{journal}{\emph{Journal of the ACM (JACM)}}
  \bibinfo{volume}{8}, \bibinfo{number}{3} (\bibinfo{year}{1961}),
  \bibinfo{pages}{404--417}.
\newblock


\bibitem[\protect\citeauthoryear{McCallum, Nigam, et~al\mbox{.}}{McCallum
  et~al\mbox{.}}{1998}]{NB2}
\bibfield{author}{\bibinfo{person}{Andrew McCallum}, \bibinfo{person}{Kamal
  Nigam}, {et~al\mbox{.}}} \bibinfo{year}{1998}\natexlab{}.
\newblock \showarticletitle{A comparison of event models for naive bayes text
  classification}. In \bibinfo{booktitle}{\emph{AAAI-98 workshop on learning
  for text categorization}}, Vol.~\bibinfo{volume}{752}. Citeseer,
  \bibinfo{pages}{41--48}.
\newblock


\bibitem[\protect\citeauthoryear{Miller}{Miller}{1995}]{wordnet}
\bibfield{author}{\bibinfo{person}{George~A Miller}.}
  \bibinfo{year}{1995}\natexlab{}.
\newblock \showarticletitle{WordNet: a lexical database for English}.
\newblock \bibinfo{journal}{\emph{Commun. ACM}} \bibinfo{volume}{38},
  \bibinfo{number}{11} (\bibinfo{year}{1995}), \bibinfo{pages}{39--41}.
\newblock


\bibitem[\protect\citeauthoryear{Ousidhoum, Lin, Zhang, Song, and
  Yeung}{Ousidhoum et~al\mbox{.}}{2019}]{DBLP:conf/emnlp/OusidhoumLZSY19}
\bibfield{author}{\bibinfo{person}{Nedjma Ousidhoum}, \bibinfo{person}{Zizheng
  Lin}, \bibinfo{person}{Hongming Zhang}, \bibinfo{person}{Yangqiu Song}, {and}
  \bibinfo{person}{Dit{-}Yan Yeung}.} \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{Multilingual and Multi-Aspect Hate Speech
  Analysis}. In \bibinfo{booktitle}{\emph{Proceedings of the 2019 Conference on
  Empirical Methods in Natural Language Processing and the 9th International
  Joint Conference on Natural Language Processing, {EMNLP-IJCNLP} 2019,
  November 3-7, 2019}}. \bibinfo{publisher}{Association for Computational
  Linguistics}, \bibinfo{address}{Hong Kong, China},
  \bibinfo{pages}{4674--4683}.
\newblock
\urldef\tempurl \url{https://doi.org/10.18653/v1/D19-1474}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Pennington, Socher, and Manning}{Pennington
  et~al\mbox{.}}{2014}]{glove}
\bibfield{author}{\bibinfo{person}{Jeffrey Pennington},
  \bibinfo{person}{Richard Socher}, {and} \bibinfo{person}{Christopher~D.
  Manning}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{GloVe: Global Vectors for Word Representation}. In
  \bibinfo{booktitle}{\emph{Empirical Methods in Natural Language Processing
  (EMNLP)}}. \bibinfo{address}{Doha, Qatar}, \bibinfo{pages}{1532--1543}.
\newblock
\urldef\tempurl \url{http://www.aclweb.org/anthology/D14-1162}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Pitenis, Zampieri, and Ranasinghe}{Pitenis
  et~al\mbox{.}}{2020}]{DBLP:journals/corr/abs-2003-07459}
\bibfield{author}{\bibinfo{person}{Zeses Pitenis}, \bibinfo{person}{Marcos
  Zampieri}, {and} \bibinfo{person}{Tharindu Ranasinghe}.}
  \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Offensive Language Identification in Greek}.
\newblock \bibinfo{journal}{\emph{CoRR}}  \bibinfo{volume}{abs/2003.07459}
  (\bibinfo{year}{2020}).
\newblock
\showeprint[arxiv]{2003.07459}
\urldef\tempurl \url{https://arxiv.org/abs/2003.07459}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Porter}{Porter}{2001}]{snowball}
\bibfield{author}{\bibinfo{person}{Martin~F. Porter}.}
  \bibinfo{year}{2001}\natexlab{}.
\newblock \bibinfo{title}{Snowball: A language for stemming algorithms}.
\newblock \bibinfo{howpublished}{Published online}.
\newblock
\urldef\tempurl \url{http://snowball.tartarus.org/texts/introduction.html}
\showURL{\tempurl}
\newblock
\shownote{Accessed 11.03.2008, 15.00h.}


\bibitem[\protect\citeauthoryear{Pupo, Altalhi, and Ventura}{Pupo
  et~al\mbox{.}}{2018}]{DBLP:journals/kbs/PupoAV18}
\bibfield{author}{\bibinfo{person}{Oscar Gabriel~Reyes Pupo},
  \bibinfo{person}{Abdulrahman~H. Altalhi}, {and}
  \bibinfo{person}{Sebasti{\'{a}}n Ventura}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Statistical comparisons of active learning
  strategies over multiple datasets}.
\newblock \bibinfo{journal}{\emph{Knowl. Based Syst.}}  \bibinfo{volume}{145}
  (\bibinfo{year}{2018}), \bibinfo{pages}{274--288}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1016/j.knosys.2018.01.033}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Ranasinghe, Zampieri, and
  Hettiarachchi}{Ranasinghe et~al\mbox{.}}{2019}]{DBLP:conf/fire/RanasingheZH19}
\bibfield{author}{\bibinfo{person}{Tharindu Ranasinghe},
  \bibinfo{person}{Marcos Zampieri}, {and} \bibinfo{person}{Hansi
  Hettiarachchi}.} \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{{BRUMS} at {HASOC} 2019: Deep Learning Models for
  Multilingual Hate Speech and Offensive Language Identification}. In
  \bibinfo{booktitle}{\emph{Working Notes of {FIRE} 2019 - Forum for
  Information Retrieval Evaluation, December 12-15, 2019}}
  \emph{(\bibinfo{series}{{CEUR} Workshop Proceedings})},
  Vol.~\bibinfo{volume}{2517}. \bibinfo{publisher}{CEUR-WS.org},
  \bibinfo{address}{Kolkata, India}, \bibinfo{pages}{199--207}.
\newblock
\urldef\tempurl \url{http://ceur-ws.org/Vol-2517/T3-3.pdf}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Read, Pfahringer, Holmes, and Frank}{Read
  et~al\mbox{.}}{2009}]{read2009classifier}
\bibfield{author}{\bibinfo{person}{Jesse Read}, \bibinfo{person}{Bernhard
  Pfahringer}, \bibinfo{person}{Geoff Holmes}, {and} \bibinfo{person}{Eibe
  Frank}.} \bibinfo{year}{2009}\natexlab{}.
\newblock \showarticletitle{Classifier chains for multi-label classification}.
  In \bibinfo{booktitle}{\emph{Joint European Conference on Machine Learning
  and Knowledge Discovery in Databases}}. Springer,
  \bibinfo{publisher}{Springer}, \bibinfo{address}{Bled, Slovenia},
  \bibinfo{pages}{254--269}.
\newblock


\bibitem[\protect\citeauthoryear{Rosenthal, Atanasova, Karadzhov, Zampieri, and
  Nakov}{Rosenthal et~al\mbox{.}}{2020}]{DBLP:journals/corr/abs-2004-14454}
\bibfield{author}{\bibinfo{person}{Sara Rosenthal}, \bibinfo{person}{Pepa
  Atanasova}, \bibinfo{person}{Georgi Karadzhov}, \bibinfo{person}{Marcos
  Zampieri}, {and} \bibinfo{person}{Preslav Nakov}.}
  \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{A large-scale semi-supervised dataset for offensive
  language identification}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2004.14454}}
  (\bibinfo{year}{2020}).
\newblock


\bibitem[\protect\citeauthoryear{Sanguinetti, Poletto, Bosco, Patti, and
  Stranisci}{Sanguinetti et~al\mbox{.}}{2018}]{DBLP:conf/lrec/SanguinettiPBPS18}
\bibfield{author}{\bibinfo{person}{Manuela Sanguinetti}, \bibinfo{person}{Fabio
  Poletto}, \bibinfo{person}{Cristina Bosco}, \bibinfo{person}{Viviana Patti},
  {and} \bibinfo{person}{Marco Stranisci}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{An Italian Twitter Corpus of Hate Speech against
  Immigrants}. In \bibinfo{booktitle}{\emph{Proceedings of the Eleventh
  International Conference on Language Resources and Evaluation, {LREC} 2018,
  May 7-12, 2018}}. \bibinfo{publisher}{European Language Resources Association
  {(ELRA)}}, \bibinfo{address}{Miyazaki, Japan}.
\newblock
\urldef\tempurl \url{http://www.lrec-conf.org/proceedings/lrec2018/summaries/710.html}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Sparck~Jones}{Sparck~Jones}{1972}]{TFIDFVec}
\bibfield{author}{\bibinfo{person}{Karen Sparck~Jones}.}
  \bibinfo{year}{1972}\natexlab{}.
\newblock \showarticletitle{A statistical interpretation of term specificity
  and its application in retrieval}.
\newblock \bibinfo{journal}{\emph{Journal of documentation}}
  \bibinfo{volume}{28}, \bibinfo{number}{1} (\bibinfo{year}{1972}),
  \bibinfo{pages}{11--21}.
\newblock


\bibitem[\protect\citeauthoryear{Sun, Asudeh, Jagadish, Howe, and
  Stoyanovich}{Sun et~al\mbox{.}}{2019}]{DBLP:conf/cikm/SunAJHS19}
\bibfield{author}{\bibinfo{person}{Chenkai Sun}, \bibinfo{person}{Abolfazl
  Asudeh}, \bibinfo{person}{H.~V. Jagadish}, \bibinfo{person}{Bill Howe}, {and}
  \bibinfo{person}{Julia Stoyanovich}.} \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{MithraLabel: Flexible Dataset Nutritional Labels
  for Responsible Data Science}. In \bibinfo{booktitle}{\emph{Proceedings of
  the 28th {ACM} International Conference on Information and Knowledge
  Management, {CIKM} 2019, Beijing, China, November 3-7, 2019}}.
  \bibinfo{publisher}{{ACM}}, \bibinfo{address}{Beijing, China},
  \bibinfo{pages}{2893--2896}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1145/3357384.3357853}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Tsoumakas and Katakis}{Tsoumakas and
  Katakis}{2007}]{tsoum}
\bibfield{author}{\bibinfo{person}{Grigorios Tsoumakas} {and}
  \bibinfo{person}{Ioannis Katakis}.} \bibinfo{year}{2007}\natexlab{}.
\newblock \showarticletitle{Multi-label classification: An overview}.
\newblock \bibinfo{journal}{\emph{International Journal of Data Warehousing and
  Mining (IJDWM)}} \bibinfo{volume}{3}, \bibinfo{number}{3}
  (\bibinfo{year}{2007}), \bibinfo{pages}{1--13}.
\newblock


\bibitem[\protect\citeauthoryear{Ullmann and Tomalin}{Ullmann and
  Tomalin}{2020}]{DBLP:journals/ethicsit/UllmannT20}
\bibfield{author}{\bibinfo{person}{Stefanie Ullmann} {and}
  \bibinfo{person}{Marcus Tomalin}.} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Quarantining online hate speech: technical and
  ethical perspectives}.
\newblock \bibinfo{journal}{\emph{Ethics Inf. Technol.}} \bibinfo{volume}{22},
  \bibinfo{number}{1} (\bibinfo{year}{2020}), \bibinfo{pages}{69--80}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1007/s10676-019-09516-z}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Vapnik}{Vapnik}{2000}]{SVM}
\bibfield{author}{\bibinfo{person}{Vladimir~Naumovich Vapnik}.}
  \bibinfo{year}{2000}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{The Nature of Statistical Learning Theory,
  Second Edition}}.
\newblock \bibinfo{publisher}{Springer}.
\newblock
\showISBNx{978-0-387-98780-4}


\bibitem[\protect\citeauthoryear{Varma and Simon}{Varma and Simon}{2006}]{nestedCrossValidation}
\bibfield{author}{\bibinfo{person}{Sudhir Varma} {and} \bibinfo{person}{Richard
  Simon}.} \bibinfo{year}{2006}\natexlab{}.
\newblock \showarticletitle{Bias in error estimation when using
  cross-validation for model selection}.
\newblock \bibinfo{journal}{\emph{BMC bioinformatics}} \bibinfo{volume}{7},
  \bibinfo{number}{1} (\bibinfo{year}{2006}), \bibinfo{pages}{91}.
\newblock


\bibitem[\protect\citeauthoryear{Waseem and Hovy}{Waseem and Hovy}{2016}]{waseem-hovy}
\bibfield{author}{\bibinfo{person}{Zeerak Waseem} {and} \bibinfo{person}{Dirk
  Hovy}.} \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Hateful Symbols or Hateful People? Predictive
  Features for Hate Speech Detection on Twitter}. In
  \bibinfo{booktitle}{\emph{Proceedings of the NAACL Student Research
  Workshop}}. \bibinfo{publisher}{Association for Computational Linguistics},
  \bibinfo{address}{San Diego, California}, \bibinfo{pages}{88--93}.
\newblock
\urldef\tempurl \url{http://www.aclweb.org/anthology/N16-2013}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{{Wikipedia contributors}}{{Wikipedia
  contributors}}{2019a}]{canadaHS}
\bibfield{author}{\bibinfo{person}{{Wikipedia contributors}}.}
  \bibinfo{year}{2019}\natexlab{a}.
\newblock \bibinfo{title}{Hate speech laws in Canada --- {Wikipedia}{,} The
  Free Encyclopedia}.
\newblock
\newblock
\urldef\tempurl \url{https://en.wikipedia.org/w/index.php?title=Hate_speech_laws_in_Canada&oldid=927068115}
\showURL{\tempurl}
\newblock
\shownote{[Online; accessed 24-November-2019].}


\bibitem[\protect\citeauthoryear{{Wikipedia contributors}}{{Wikipedia
  contributors}}{2019b}]{wikiIncels}
\bibfield{author}{\bibinfo{person}{{Wikipedia contributors}}.}
  \bibinfo{year}{2019}\natexlab{b}.
\newblock \bibinfo{title}{Incel}.
\newblock
  \bibinfo{howpublished}{\url{https://en.wikipedia.org/w/index.php?title=Incel&oldid=910373786}}.
\newblock
\newblock
\shownote{[Online; accessed 26-August-2019].}


\bibitem[\protect\citeauthoryear{{Wikipedia contributors}}{{Wikipedia
  contributors}}{2019c}]{wikiDonald}
\bibfield{author}{\bibinfo{person}{{Wikipedia contributors}}.}
  \bibinfo{year}{2019}\natexlab{c}.
\newblock \bibinfo{title}{R/The Donald --- {Wikipedia}{,} The Free
  Encyclopedia}.
\newblock
  \bibinfo{howpublished}{\url{https://en.wikipedia.org/w/index.php?title=R/The_Donald&oldid=912336074}}.
\newblock
\newblock
\shownote{[Online; accessed 26-August-2019].}


\bibitem[\protect\citeauthoryear{Yu, Fu, Xu, and Qin}{Yu et~al\mbox{.}}{2019}]{DBLP:journals/mlc/YuFXQ19}
\bibfield{author}{\bibinfo{person}{Dingguo Yu}, \bibinfo{person}{Bin Fu},
  \bibinfo{person}{Guandong Xu}, {and} \bibinfo{person}{Aihong Qin}.}
  \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{Constrained nonnegative matrix factorization-based
  semi-supervised multilabel learning}.
\newblock \bibinfo{journal}{\emph{Int. J. Machine Learning {\&} Cybernetics}}
  \bibinfo{volume}{10}, \bibinfo{number}{5} (\bibinfo{year}{2019}),
  \bibinfo{pages}{1093--1100}.
\newblock
\urldef\tempurl \url{https://doi.org/10.1007/s13042-018-0787-8}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Zampieri, Malmasi, Nakov, Rosenthal, Farra,
  and Kumar}{Zampieri et~al\mbox{.}}{2019}]{DBLP:conf/naacl/ZampieriMNRFK19}
\bibfield{author}{\bibinfo{person}{Marcos Zampieri}, \bibinfo{person}{Shervin
  Malmasi}, \bibinfo{person}{Preslav Nakov}, \bibinfo{person}{Sara Rosenthal},
  \bibinfo{person}{Noura Farra}, {and} \bibinfo{person}{Ritesh Kumar}.}
  \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{Predicting the Type and Target of Offensive Posts
  in Social Media}. In \bibinfo{booktitle}{\emph{Proceedings of the 2019
  Conference of the North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN,
  USA, June 2-7, 2019, Volume 1 (Long and Short Papers)}}.
  \bibinfo{pages}{1415--1420}.
\newblock
\urldef\tempurl \url{https://doi.org/10.18653/v1/n19-1144}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Zhang and Zhou}{Zhang and Zhou}{2007}]{zhang2007ml}
\bibfield{author}{\bibinfo{person}{Min-Ling Zhang} {and}
  \bibinfo{person}{Zhi-Hua Zhou}.} \bibinfo{year}{2007}\natexlab{}.
\newblock \showarticletitle{ML-KNN: A lazy learning approach to multi-label
  learning}.
\newblock \bibinfo{journal}{\emph{Pattern recognition}} \bibinfo{volume}{40},
  \bibinfo{number}{7} (\bibinfo{year}{2007}), \bibinfo{pages}{2038--2048}.
\newblock


\end{thebibliography}
 




\end{document}
