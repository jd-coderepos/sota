\documentclass{article}
\usepackage{iclr2020_conference,times}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{subfigure}

\usepackage{hyperref}
\usepackage{url}

\newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}

\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}  \def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}  \def\cf{\emph{cf.}}  \def\Cf{\emph{Cf.}}   

\newcommand{\mat}[1]{\bm{#1}}

\newtheorem{theorem}{Theorem}

\title{Deep Graph Matching Consensus}

\author{Matthias Fey\\
\And \hspace{-0.4cm}
Jan E.~Lenssen\\
\And \hspace{-0.4cm}
Christopher Morris\\
\And \hspace{-0.4cm}
Jonathan Masci\\
\And \hspace{-0.4cm}
Nils M.~Kriege\\
\And \vspace{-0.5cm}
\\
TU Dortmund University\\
\,\,\,Dortmund, Germany\\
\And \vspace{-0.5cm}
\\
NNAISENSE\\
\,\,\,Lugano, Switzerland\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 

\begin{document}

\maketitle

\footnotetext[3]{Correspondence to \href{mailto:matthias.fey@udo.edu}{\url{matthias.fey@udo.edu}}}
\footnotetext[4]{Work done during an internship at NNAISENSE}

\begin{abstract}
This work presents a two-stage neural architecture for learning and refining structural correspondences between graphs.
First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes.
Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs.
We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process.
Our purely local and sparsity-aware architecture scales well to large, real-world inputs while still being able to recover global correspondences consistently.
We demonstrate the practical effectiveness of our method on real-world tasks from the fields of computer vision and entity alignment between knowledge graphs, on which we improve upon the current state-of-the-art.
Our source code is available under \url{https://github.com/rusty1s/deep-graph-matching-consensus}.
\end{abstract}

\section{Introduction}\label{sec:introduction}

Graph matching refers to the problem of establishing meaningful \emph{structural correspondences} of nodes between two or more graphs by taking both node similarities and pairwise edge similarities into account \citep{Wang/etal/2019}.
Since graphs are natural representations for encoding relational data, the problem of graph matching lies at the heart of many real-world applications.
For example, comparing molecules in cheminformatics \citep{Kriege/etal/2019}, matching protein networks in bioinformatics \citep{Sharan/Ideker/2006,Singh/etal/2008}, linking user accounts in social network analysis \citep{Zhang/Philip/2015}, and tracking objects, matching 2D/3D shapes or recognizing actions in computer vision \citep{Vento/Foggia/2012} can be formulated as a graph matching problem.

The problem of graph matching has been heavily investigated in theory \citep{Grohe/etal/2018} and practice \citep{Conte/etal/2004}, usually by relating it to domain-agnostic distances such as the \emph{graph edit distance} \citep{Stauffer/etal/2017} and the \emph{maximum common subgraph} problem \citep{Bunke/Shearer/1998}, or by formulating it as a \emph{quadratic assignment problem} \citep{Yan/etal/2016}.
Since all three approaches are -hard, solving them to optimality may not be tractable for large-scale, real-world instances.
Moreover, these purely combinatorial approaches do not adapt to the given data distribution and often do not consider continuous node embeddings which can provide crucial information about node semantics.

Recently, various neural architectures have been proposed to tackle the task of graph matching \citep{Zanfir/Sminchisescu/2018,Wang/etal/2019,Zhang/Lee/2019,Xu/etal/2019a,Xu/etal/2019c,Derr/etal/2019,Zhang/etal/2019a,Heimann/etal/2018} or graph similarity \citep{Bai/etal/2018,Bai/etal/2019,Li/etal/2019} in a data-dependent fashion.
However, these approaches are either only capable of computing similarity scores between whole graphs \citep{Bai/etal/2018,Bai/etal/2019,Li/etal/2019}, rely on an inefficient global matching procedure \citep{Zanfir/Sminchisescu/2018,Wang/etal/2019,Xu/etal/2019a,Li/etal/2019}, or do not generalize to unseen graphs \citep{Xu/etal/2019c,Derr/etal/2019,Zhang/etal/2019a}.
Moreover, they might be prone to match neighborhoods between graphs inconsistently by only taking localized embeddings into account \citep{Zanfir/Sminchisescu/2018,Wang/etal/2019,Zhang/Lee/2019,Xu/etal/2019a,Derr/etal/2019,Heimann/etal/2018}.

Here, we propose a fully-differentiable graph matching procedure which aims to reach a data-driven \emph{neighborhood consensus} between matched node pairs without the need to solve any optimization problem during inference.
In addition, our approach is \emph{purely local}, \ie{}, it operates on fixed-size neighborhoods around nodes, and is \emph{sparsity-aware}, \ie{}, it takes the sparsity of the underlying structures into account.
Hence, our approach scales well to large input domains, and can be trained in an end-to-end fashion to adapt to a given data distribution.
Finally, our approach improves upon the state-of-the-art on several real-world applications from the fields of computer vision and entity alignment on knowledge graphs.

\section{Problem Definition}\label{sec:problem_definition}

A \emph{graph}  consists of a finite set of \emph{nodes} , an \emph{adjacency matrix} , a \emph{node feature} matrix , and an optional (sparse) \emph{edge feature} matrix  .
For a subset of nodes ,  denotes the \emph{subgraph} of  induced by .
We refer to  as the \emph{-hop neighborhood} around node , where  denotes the shortest-path distance in .
A \emph{node coloring} is a function  with arbitrary codomain .

The problem of \emph{graph matching} refers to establishing node correspondences between two graphs.
Formally, we are given two graphs, a \emph{source graph}  and a \emph{target graph} , w.l.o.g. , and are interested in finding a \emph{correspondence matrix}  which minimizes an objective subject to the one-to-one mapping constraints  and .
As a result,  infers an injective mapping  which maps each node in  to a node in .

Typically, graph matching is formulated as an edge-preserving, quadratic assignment problem \citep{Anstreicher/2003,Gold/Rangarajan/1996,Caetano/etal/2009,Cho/etal/2013}, \ie,

subject to the one-to-one mapping constraints mentioned above.
This formulation is based on the intuition of finding correspondences based on \emph{neighborhood consensus} \citep{Rocco/etal/2018}, which shall prevent adjacent nodes in the source graph from being mapped to different regions in the target graph.
Formally, a neighborhood consensus is reached if for all node pairs  with , it holds that for every node  there exists a node  such that .

In this work, we consider the problem of supervised and semi-supervised matching of graphs while employing the intuition of neighborhood consensus as an inductive bias into our model.
In the supervised setting, we are given pair-wise ground-truth correspondences for a set of graphs and want our model to generalize to unseen graph pairs.
In the semi-supervised setting, source and target graphs are fixed, and ground-truth correspondences are only given for a small subset of nodes.
However, we are allowed to make use of the complete graph structures.

\section{Methodology}\label{sec:methodology}

In the following, we describe our proposed end-to-end, deep graph matching architecture in detail.
See Figure~\ref{fig:overview} for a high-level illustration.
The method consists of two stages: a \emph{local feature matching procedure} followed by an \emph{iterative refinement strategy} using synchronous message passing networks.
The aim of the feature matching step, see Section~\ref{sub:feature_matching}, is to compute initial correspondence scores based on the similarity of local node embeddings.
The second step is an iterative refinement strategy, see Sections~\ref{sub:parallel_message_passing_for_neighborhood_consensus} and~\ref{sub:relation_to_the_graduated_assignment}, which aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism.
Finally, in Section~\ref{sub:scaling_to_large_input}, we show how to scale our method to large, real-world inputs.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/overview.pdf}
  \caption{High-level illustration of our two-stage neighborhood consensus architecture.
    Node features are first locally matched based on a graph neural network , before their correspondence scores get iteratively refined based on neighborhood consensus.
    Here, an injective node coloring of  is transferred to  via , and distributed by  on both graphs.
    Updates on  are performed by a neural network  based on pair-wise color differences.
    }\label{fig:overview}
\end{figure}

\subsection{Local Feature Matching}\label{sub:feature_matching}

We model our \emph{local feature matching procedure} in close analogy to related approaches \citep{Bai/etal/2018,Bai/etal/2019,Wang/etal/2019,Zhang/Lee/2019,Wang/Solomon/2019} by computing similarities between nodes in the source graph  and the target graph  based on node embeddings.
That is, given latent node embeddings  and  computed by a shared neural network  for source graph  and target graph , respectively, we obtain initial \emph{soft} correspondences as

Here,  normalization is applied to obtain \emph{rectangular doubly-stochastic} correspondence matrices that fulfill the constraints  and  \citep{Sinkhorn/Knopp/1967,Adams/Zemel/2011,Cour/etal/2006}.

We interpret the -th row vector  as a discrete distribution over potential correspondences in  for each node .
We train  in a dicriminative, supervised fashion against ground truth correspondences  by minimizing the negative log-likelihood of correct correspondence scores .

We implement  as a \emph{Graph Neural Network} (GNN) to obtain localized, permutation equivariant vectorial node representations \citep{Bronstein/etal/2017,Hamilton/etal/2017,Battaglia/etal/2018,Goyal/Ferrara/2018}.
Formally, a GNN follows a \emph{neural message passing scheme} \citep{Gilmer/etal/2017} and updates its node features  in layer  by aggregating localized information via

where  and  denotes a multiset.
The recent work in the fields of \emph{geometric deep learning} and \emph{relational representation learning} provides a large number of operators to choose from \citep{Kipf/Welling/2017,Gilmer/etal/2017,Velickovic/etal/2018,Schlichtkrull/etal/2018,Xu/etal/2019b}, which allows for precise control of the properties of extracted features.

\subsection{Synchronous Message Passing for Neighborhood Consensus}\label{sub:parallel_message_passing_for_neighborhood_consensus}

Due to the purely local nature of the used node embeddings, our feature matching procedure is prone to finding false correspondences which are locally similar to the correct one.
Formally, those cases pose a violation of the neighborhood consensus criteria employed in Equation~\eqref{eq:unsupervised_formulation}.
Since finding a global optimum is -hard, we aim to detect violations of the criteria in local neighborhoods and resolve them in an iterative fashion.

We utilize graph neural networks to detect these violations in a neighborhood consensus step and iteratively refine correspondences , , starting from .
Key to the proposed algorithm is the following observation: The soft correspondence matrix  is a map from the node function space  to the node function space .
Therefore, we can use  to pass node functions ,  along the soft correspondences by

to obtain functions ,  in the other domain, respectively.

Then, our consensus method works as follows: Using , we first map node indicator functions, given as an injective node coloring  in the form of an identity matrix , from  to .
Then, we distribute this coloring in corresponding neighborhoods by performing synchronous message passing on both graphs via a shared graph neural network , \ie,

We can compare the results of both GNNs to recover a vector  which measures the neighborhood consensus between node pairs .
This measure can be used to perform trainable updates of the correspondence scores

based on an  .
The process can be applied  times to iteratively improve the consensus in neighborhoods.
The final objective  with  combines both the feature matching error and neighborhood consensus error.
This objective is fully-differentiable and can hence be optimized in an end-to-end fashion using stochastic gradient descent.
Overall, the consensus stage distributes global node colorings to resolve ambiguities and false matchings made in the first stage of our architecture by only using purely local operators.
Since an initial matching is needed to test for neighborhood consensus, this task cannot be fulfilled by  alone, which stresses the importance of our two-stage approach.

The following two theorems show that  is a good measure of how well local neighborhoods around  and  are matched by the soft correspondence between  and .
The proofs can be found in Appendix~\ref{sec:proof_for_theorem_1} and~\ref{sec:proof_for_theorem_2}, respectively.

\begin{theorem}\label{theorem:theorem1}
  Let  and  be two isomorphic graphs and let  be a permutation equivariant GNN, \ie,  for any permutation matrix .
  If  encodes an isomorphism between  and , then  for all .
\end{theorem}

\begin{theorem}\label{theorem:theorem2}
  Let  and  be two graphs and let  be a permutation equivariant and -layered GNN for which both  and  are injective for all .
  If , then the resulting submatrix  is a permutation matrix describing an isomorphism between the -hop subgraph  around  and the -hop subgraph  around .
  Moreover, if  for all , then  denotes a full isomorphism between  and .
\end{theorem}

Hence, a GNN  that satisfies both criteria in Theorem~\ref{theorem:theorem1} and~\ref{theorem:theorem2} provides equal node embeddings  and  if and only if nodes in a local neighborhood are correctly matched to each other.
A value  indicates the existence of inconsistent matchings in the local neighborhoods around  and , and can hence be used to refine the correspondence score .

Note that both requirements, permutation equivariance and injectivity, are easily fulfilled: (1) All common graph neural network architectures following the message passing scheme of Equation~\eqref{eq:message_passing} are equivariant due to the use of permutation invariant neighborhood aggregators.
(2) Injectivity of graph neural networks is a heavily discussed topic in recent literature.
It can be fulfilled by using a GNN that is as powerful as the \citet{Weisfeiler/Lehman/1968} (WL) heuristic in distinguishing graph structures, \eg{}, by using  aggregation in combination with s on the multiset of neighboring node features, \cf{} \citep{Xu/etal/2019b,Morris/etal/2019}.

\subsection{Relation to the Graduated Assignment Algorithm}\label{sub:relation_to_the_graduated_assignment}

Theoretically, we can relate our proposed approach to classical graph matching techniques that consider a doubly-stochastic relaxation of the problem defined in Equation~\eqref{eq:unsupervised_formulation}, \cf{} \citep{Lyzinski/etal/2016} and Appendix~\ref{sec:related_work_i} for more details.
A seminal work following this method is the \emph{graduated assignment algorithm} \citep{Gold/Rangarajan/1996}.
By starting from an initial feasible solution , a new solution  is iteratively computed from  by approximately solving a linear assignment problem according to

where  denotes the gradient of Equation~\eqref{eq:unsupervised_formulation} at .\footnote{For clarity of presentation, we closely follow the original formulation of the method for simple graphs but ignore the edge similarities and adapt the constant factor of the gradient according to our objective function.}
The  operator is implemented by applying  normalization on rescaled inputs, where the scaling factor grows in every iteration to increasingly encourage integer solutions.
Our approach also resembles the approximation of the linear assignment problem via  normalization.

Moreover, the gradient  is closely related to our neighborhood consensus scheme for the particular simple, non-trainable GNN instantiation .
Given  and , we obtain  by substitution.
Instead of updating  based on the similarity between  and  obtained from a fixed-function GNN , we choose to update correspondence scores via trainable neural networks  and  based on the difference between  and .
This allows us to interpret our model as a deep parameterized generalization of the graduated assignment algorithm.
In addition, specifying node and edge attribute similarities in graph matching is often difficult and complicates its computation \citep{Zhou/DeLaTorre/2016,Zhang/etal/2019}, whereas our approach naturally supports continuous node and edge features via established GNN models.
We experimentally verify the benefits of using trainable neural networks  instead of  in Appendix~\ref{sec:comparison_to_the_graduated_assignment_algorithm}.

\subsection{Scaling to Large Input}\label{sub:scaling_to_large_input}

We apply a number of optimizations to our proposed algorithm to make it scale to large input domains.
See Algorithm~\ref{alg:algorithm} in Appendix~\ref{sec:optimized_graph_matching_consensus_algorithm} for the final optimized algorithm.

\paragraph{Sparse correspondences.}

We propose to sparsify initial correspondences  by filtering out low score correspondences before neighborhood consensus takes place.
That is, we sparsify  by computing top  correspondences with the help of the \textsc{KeOps} library \citep{Charlier/etal/2019} without ever storing its dense version, reducing its required memory footprint from  to .
In addition, the time complexity of the refinement phase is reduced from  to , where  and  denote the number of edges in  and , respectively.
Note that sparsifying initial correspondences assumes that the feature matching procedure ranks the correct correspondence within the top  elements for each node .
Hence, also optimizing the initial feature matching loss  is crucial, and can be further accelerated by training only against sparsified correspondences with ground-truth entries .

\paragraph{Replacing node indicators functions.}

Although applying  on node indicator functions  is computationally efficient, it requires a parameter complexity of .
Hence, we propose to replace node indicator functions  with randomly drawn node functions , where  with , in iteration .
By sampling from a continuous distribution, node indicator functions are still guaranteed to be injective \citep{DeGroot/Schervish/2012}.
Note that Theorem~\ref{theorem:theorem1} still holds because it does not impose any restrictions on the function space .
Theorem~\ref{theorem:theorem2} does not necessarily hold anymore, but we expect our refinement strategy to resolve any ambiguities by re-sampling  in every iteration .
We verify this empirically in Section~\ref{sub:ablation_study_on_synthetic_graphs}.

\paragraph{Softmax normalization.}

The  normalization fulfills the requirements of rectangular doubly-stochastic solutions.
However, it may eventually push correspondences to inconsistent integer solutions very early on from which the neighborhood consensus method cannot effectively recover.
Furthermore, it is inherently inefficient to compute and runs the risk of vanishing gradients  \citep{Zhang/etal/2019b}.
Here, we propose to relax this constraint by only applying row-wise  normalization on , and expect our supervised refinement procedure to naturally resolve violations of  on its own by re-ranking false correspondences via neighborhood consensus.
Experimentally, we show that row-wise normalization is sufficient for our algorithm to converge to the correct solution, \cf{} Section~\ref{sub:ablation_study_on_synthetic_graphs}.

\paragraph{Number of refinement iterations.}

Instead of holding  fixed, we propose to differ the number of refinement iterations  and , , for training and testing, respectively.
This does not only speed up training runtime, but it also encourages the refinement procedure to reach convergence with as few steps as necessary while we can run the refinement procedure until convergence during testing.
We show empirically that decreasing  does not affect the convergence abilities of our neighborhood consensus procedure during testing, \cf{} Section~\ref{sub:ablation_study_on_synthetic_graphs}.

\section{Experiments}\label{sec:experiments}

We verify our method on three different tasks.
We first show the benefits of our approach in an ablation study on synthetic graphs (Section~\ref{sub:ablation_study_on_synthetic_graphs}), and apply it to the real-world tasks of supervised keypoint matching in natural images (Sections~\ref{sub:supervised_keypoint_matching_in_natural_images} and~\ref{sub:supervised_geometric_keypoint_matching}) and semi-supervised cross-lingual knowledge graph alignment (Section~\ref{sub:semi-supervised_cross-lingual_knowledge_graph_alignment}) afterwards.
All dataset statistics can be found in Appendix~\ref{sec:dataset_statistics}.

Our method is implemented in \textsc{PyTorch} \citep{Paszke/etal/2017} using the \textsc{PyTorch Geometric} \citep{Fey/Lenssen/2019} and the \textsc{KeOps} \citep{Charlier/etal/2019} libraries.
Our implementation can process sparse mini-batches with parallel GPU acceleration and minimal memory footprint in all algorithm steps.
For all experiments, optimization is done via \textsc{Adam} \citep{Kingma/Ba/2015} with a fixed learning rate of .
We use similar architectures for  and  except that we omit dropout \citep{Srivastava/etal/2014} in .
For all experiments, we report Hits@ to evaluate and compare our model to previous lines of work, where Hits@ measures the proportion of correctly matched entities ranked in the top .

\subsection{Ablation Study on Synthetic Graphs}\label{sub:ablation_study_on_synthetic_graphs}

In our first experiment, we evaluate our method on synthetic graphs where we aim to learn a matching for pairs of graphs in a supervised fashion.
Each pair of graphs consists of an undirected \citet{Erdos/Renyi/1959} graph  with  nodes and edge probability , and a target graph  which is constructed from  by removing edges with probability  without disconnecting any nodes \citep{Heimann/etal/2018}.
Training and evaluation is done on  graphs each for different configurations .
In Appendix~\ref{sec:robustness_towards_node_addition_or_removal}, we perform additional experiments to also verify the robustness of our approach towards node addition or removal.

\paragraph{Architecture and parameters.}

We implement the graph neural network operators  and  by stacking three layers () of the  operator  \citep{Xu/etal/2019b}

due to its expressiveness in distinguishing raw graph structures.
The number of layers and hidden dimensionality of all s is set to  and , respectively, and we apply  activation \citep{Glorot/etal/2011} and Batch normalization \citep{Ioffe/Szegedy/2015} after each of its layers.
Input features are initialized with one-hot encodings of node degrees.
We employ a \emph{Jumping Knowledge} style concatenation  \citep{Xu/etal/2018} to compute final node representations .
We train and test our procedure with  and  refinement iterations, respectively.

\paragraph{Results.}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/legend.pdf}
  \subfigure[, ]{
    \includegraphics[height=2.86cm]{figures/syn50.pdf}\label{fig:syn50}
  }
  \hspace{-0.4cm}
  \subfigure[, ]{
    \includegraphics[height=2.86cm]{figures/syn100.pdf}\label{fig:syn100}
  }
  \hspace{-0.4cm}
  \subfigure[, ]{
    \includegraphics[height=2.86cm]{figures/synL.pdf}\label{fig:synL}
  }
  \hspace{-0.4cm}
  \subfigure[, ]{
    \includegraphics[height=2.86cm]{figures/synTop.pdf}\label{fig:synTop}
  }
  \caption{The performance of our method on synthetic data with structural noise.}\label{fig:syn}
\end{figure}

Figures~\ref{fig:syn50} and~\ref{fig:syn100} show the matching accuracy Hits@1 for different choices of  and .
We observe that the purely local matching approach via  starts decreasing in performance with the structural noise  increasing.
This also holds when applying global  normalization on .
However, our proposed two-stage architecture can recover \emph{all} correspondences, independent of the applied structural noise .
This applies to both variants discussed in the previous sections, \ie, our initial formulation , and our optimized architecture using random node indicator sampling and row-wise normalization .
This highlights the overall benefits of applying matching consensus and justifies the usage of the enhancements made towards scalability in Section~\ref{sub:scaling_to_large_input}.

In addition, Figure~\ref{fig:synL} visualizes the test error  for varying number of iterations .
We observe that even when training to non-convergence, our procedure is still able to converge by increasing the number of iterations  during testing.

Moreover, Figure~\ref{fig:synTop} shows the performance of our refinement strategy when operating on sparsified top  correspondences.
In contrast to its dense version, it cannot match all nodes correctly due to the poor initial feature matching quality.
However, it consistently converges to the perfect solution of Hits@1  Hits@ in case the correct match is included in the initial top  ranking of correspondences.
Hence, with increasing , we can recover most of the correct correspondences, making it an excellent option to scale our algorithm to large graphs, \cf{} Section~\ref{sub:semi-supervised_cross-lingual_knowledge_graph_alignment}.

\subsection{Supervised Keypoint Matching in Natural Images}\label{sub:supervised_keypoint_matching_in_natural_images}

We perform experiments on the \textsc{PascalVOC} \citep{Everingham/etal/2010} with Berkeley annotations \citep{Bourdev/etal/2009} and \textsc{WILLOW-ObjectClass} \citep{Cho/etal/2013} datasets which contain sets of image categories with labeled keypoint locations.
For \textsc{PascalVOC}, we follow the experimental setups of \citet{Zanfir/Sminchisescu/2018} and \citet{Wang/etal/2019} and use the training and test splits provided by \citet{Choy/etal/2016}.
We pre-filter the dataset to exclude difficult, occluded and truncated objects, and require examples to have at least one keypoint, resulting in  and  annotated images for training and testing, respectively.
The \textsc{PascalVOC} dataset contains instances of varying scale, pose and illumination, and the number of keypoints ranges from  to .
In contrast, the \textsc{WILLOW-ObjectClass} dataset contains at least 40 images with consistent orientations for each of its five categories, and each image consists of exactly 10 keypoints.
Following the experimental setup of peer methods \citep{Cho/etal/2013,Wang/etal/2019}, we pre-train our model on \textsc{PascalVOC} and fine-tune it over 20 random splits with 20 per-class images used for training.
We construct graphs via the Delaunay triangulation of keypoints.
For fair comparison with \citet{Zanfir/Sminchisescu/2018} and \citet{Wang/etal/2019}, input features of keypoints are given by the concatenated output of \texttt{relu4\_2} and \texttt{relu5\_1} of a pre-trained \textsc{VGG16} \citep{Simonyan/Zisserman/2014} on \textsc{ImageNet} \citep{Deng/etal/2009}.

\paragraph{Architecture and parameters.}

We adopt \textsc{SplineCNN} \citep{Fey/etal/2018} as our graph neural network operator

whose trainable B-spline based kernel function  is conditioned on edge features  between node-pairs.
To align our results with the related work, we evaluate both isotropic and anisotropic edge features which are given as normalized relative distances and 2D Cartesian coordinates, respectively.
For \textsc{SplineCNN}, we use a kernel size of  in each dimension, a hidden dimensionality of , and apply  as our non-linearity function .
Our network architecture consists of two convolutional layers (), followed by dropout with probability , and a final linear layer.
During training, we form pairs between any two training examples of the same category, and evaluate our model by sampling a fixed number of test graph pairs belonging to the same category.

\paragraph{Results.}

\begin{table}[t]
  \centering
  \caption{Hits@1 (\%) on the \textsc{PascalVOC} dataset with Berkeley keypoint annotations.}\label{tab:pascal}
  \renewcommand{\arraystretch}{1.1}
  \setlength{\tabcolsep}{2pt}
  \resizebox{\linewidth}{!}{\begin{tabular}{clccccccccccccccccccccc}
    \toprule
      \mc{2}{l}{\textbf{Method}} & \textbf{Aero} & \textbf{Bike} & \textbf{Bird} & \textbf{Boat} & \textbf{Bottle} & \textbf{Bus} & \textbf{Car} & \textbf{Cat} & \textbf{Chair} & \textbf{Cow} & \textbf{Table} & \textbf{Dog} & \textbf{Horse} & \textbf{M-Bike} & \textbf{Person} & \textbf{Plant} & \textbf{Sheep} & \textbf{Sofa} & \textbf{Train} & \textbf{TV} & \textbf{Mean} \\
    \midrule
        \mc{2}{l}{} & 31.1 & 46.2 & 58.2 & 45.9 & 70.6 & 76.5 & 61.2 & 61.7 & 35.5 & 53.7 & 58.9 & 57.5 & 56.9 & 49.3 & 34.1 & 77.5 & 57.1 & 53.6 & 83.2 & 88.6 & 57.9 \\
        \mc{2}{l}{} & 40.9 & 55.0 & \textbf{65.8} & 47.9 & 76.9 & 77.9 & 63.5 & 67.4 & 33.7 & \textbf{66.5} & 63.6 & \textbf{61.3} & 58.9 & \textbf{62.8} & 44.9 & 77.5 & 67.4 & 57.5 & 86.7 & \textbf{90.9} & 63.8 \\
    \midrule
    \midrule
      \multirow{3}{1.88cm}{\centering{ \\ isotropic}}
      &   & 34.7 & 42.6 & 41.5 & 50.4 & 50.3 & 72.2 & 60.1 & 59.4 & 24.6 & 38.1 & 86.2 & 47.7 & 56.3 & 37.6 & 35.4 & 58.0 & 45.8 & 74.8 & 64.1 & 75.3 & 52.8 \\
      &  & 45.8 & 58.2 & 45.5 & 57.6 & 68.2 & 82.1 & 75.3 & 60.2 & 31.7 & 52.9 & \textbf{88.2} & 56.2 & 68.2 & 50.7 & 46.5 & 66.3 & 58.8 & 89.0 & 85.1 & 79.9 & 63.3 \\
      &  & 45.3 & 57.1 & 54.9 & 54.7 & 71.7 & 82.6 & 75.3 & 65.9 & 31.6 & 50.8 & 86.1 & 56.9 & 67.1 & 53.1 & 49.2 & 77.3 & 59.2 & 91.7 & 82.0 & 84.2 & 64.8 \\
    \midrule
      \multirow{3}{1.88cm}{\centering{ \\ isotropic}}
      &   & 44.3 & 62.0 & 48.4 & 53.9 & 73.3 & 80.4 & 72.2 & 64.2 & 30.3 & 52.7 & 79.4 & 56.6 & 62.3 & 56.2 & 47.5 & 74.0 & 59.8 & 79.9 & 81.9 & 83.0 & 63.1 \\
      &  & 46.5 & 63.7 & 54.9 & 60.9 & 79.4 & \textbf{84.1} & 76.4 & 68.3 & \textbf{38.5} & 61.5 & 80.6 & 59.7 & 69.8 & 58.4 & 54.3 & 76.4 & 64.5 & \textbf{95.7} & \textbf{87.9} & 81.3 & 68.1 \\
      &  & \textbf{50.1} & \textbf{65.4} & 55.7 & \textbf{65.3} & \textbf{80.0} & 83.5 & \textbf{78.3} & \textbf{69.7} & 34.7 & 60.7 & 70.4 & 59.9 & \textbf{70.0} & 62.2 & \textbf{56.1} & \textbf{80.2} & \textbf{70.3} & 88.8 & 81.1 & 84.3 & \textbf{68.3} \\
    \midrule
    \midrule
      \multirow{3}{1.88cm}{\centering{ \\ anisotropic}}
      &   & 34.3 & 45.9 & 37.3 & 47.7 & 53.3 & 75.2 & 64.5 & 61.7 & 27.7 & 40.5 & 85.9 & 46.6 & 50.2 & 39.0 & 37.3 & 58.0 & 49.2 & 82.9 & 65.0 & 74.2 & 53.8 \\
      &  & 44.6 & 51.2 & 50.7 & 58.5 & 72.3 & 83.3 & 76.6 & 65.6 & 31.0 & 57.5 & 91.7 & 55.4 & 69.5 & 56.2 & 47.5 & 85.1 & 57.9 & 92.3 & 86.7 & 85.9 & 66.0 \\
      &  & \textbf{48.7} & 57.2 & 47.0 & 65.3 & 73.9 & 87.6 & 76.7 & 70.0 & 30.0 & 55.5 & \textbf{92.8} & 59.5 & 67.9 & 56.9 & 48.7 & 87.2 & 58.3 & 94.9 & 87.9 & 86.0 & 67.6 \\
    \midrule
      \multirow{3}{1.88cm}{\centering{ \\ anisotropic}}
      &   & 42.1 & 57.5 & 49.6 & 59.4 & 83.8 & 84.0 & 78.4 & 67.5 & 37.3 & 60.4 & 85.0 & 58.0 & 66.0 & 54.1 & 52.6 & 93.9 & 60.2 & 85.6 & 87.8 & 82.5 & 67.3 \\
      &  & 45.5 & \textbf{67.6} & 56.5 & 66.8 & \textbf{86.9} & 85.2 & 84.2 & \textbf{73.0} & \textbf{43.6} & 66.0 & 92.3 & \textbf{64.0} & \textbf{79.8} & 56.6 & 56.1 & 95.4 & 64.4 & \textbf{95.0} & 91.3 & 86.3 & 72.8 \\
      &  & 47.0 & 65.7 & 56.8 & \textbf{67.6} & \textbf{86.9} & \textbf{87.7} & \textbf{85.3} & 72.6 & 42.9 & \textbf{69.1} & 84.5 & 63.8 & 78.1 & 55.6 & \textbf{58.4} & \textbf{98.0} & \textbf{68.4} & 92.2 & \textbf{94.5} & 85.5 & \textbf{73.0} \\
    \bottomrule
  \end{tabular}}
\end{table}

\begin{table}[t]
  \centering
  \caption{Hits@1 (\%) with standard deviations on the \textsc{WILLOW-ObjectClass} dataset.}\label{tab:willow}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\linewidth}{!}{\begin{tabular}{lclccccc}
    \toprule
      \mc{3}{l}{\textbf{Method}} & \textbf{Face} & \textbf{Motorbike} & \textbf{Car} & \textbf{Duck} & \textbf{Winebottle} \\
    \midrule
      \mc{3}{l}{\textsc{GMN} \citep{Zanfir/Sminchisescu/2018}} & 99.3 & 71.4 & 74.3 & 82.8 & 76.7 \\
      \mc{3}{l}{\textsc{PCA-GM} \citep{Wang/etal/2019}} & \textbf{100.0} & 76.7 & 84.0 & \textbf{93.5} & 96.9 \\
    \midrule
    \midrule
      \mr{3}{} & \mr{3}{isotropic}
        &   &  98.07  0.79 & 48.97  4.62 & 65.30  3.16 & 66.02  2.51 & 77.72  3.32 \\
      & &  & \textbf{100.00  0.00} & 67.28  4.93 & 85.07  3.93 & 83.10  3.61 & 92.30  2.11 \\
      & &  & \textbf{100.00  0.00} & 68.57  3.94 & 82.75  5.77 & 84.18  4.15 & 90.36  2.42 \\
    \midrule
      \mr{3}{} & \mr{3}{isotropic}
        &   &  99.62  0.28 & 73.47  3.32 & 77.47  4.92 & 77.10  3.25 & 88.04  1.38 \\
      & &  & \textbf{100.00  0.00} & \textbf{92.05  3.49} & 90.05  5.10 & 88.98  2.75 & \textbf{97.14  1.41} \\
      & &  & \textbf{100.00  0.00} & \textbf{92.05  3.24} & \textbf{90.28  4.67} & 88.97  3.49 & \textbf{97.14  1.83} \\
    \midrule
    \midrule
      \mr{3}{} & \mr{3}{anisotropic}
        &   & 98.47  0.61 & 49.28  4.31 & 64.95  3.52 & 66.17  4.08 & 78.08  2.61 \\
      & &  & \textbf{100.00  0.00} & 76.28  4.77 & 86.70  3.25 & 83.22  3.52 & 93.65  1.64 \\
      & &  & \textbf{100.00  0.00} & 76.57  5.28 & 89.00  3.88 & 84.78  2.73 & 95.29  2.22 \\
    \midrule
      \mr{3}{} & \mr{3}{anisotropic}
        &   &  99.96  0.06 & 91.90  2.30 & 91.28  4.89 & 86.58  2.99 & 98.25  0.71 \\
      & &  & \textbf{100.00  0.00} & 98.80  1.58 & \textbf{96.53  1.55} & 93.22  3.77 & \textbf{99.87  0.31} \\
      & &  & \textbf{100.00  0.00} & \textbf{99.40  0.80} & 95.53  2.93 & 93.00  2.71 & 99.39  0.70 \\
    \bottomrule
  \end{tabular}}
\end{table}

We follow the experimental setup of \citet{Wang/etal/2019} and train our models using negative log-likelihood due to its superior performance in contrast to the \emph{displacement loss} used in \citet{Zanfir/Sminchisescu/2018}.
We evaluate our complete architecture using isotropic and anisotropic GNNs for , and include ablation results obtained from using  for the local node matching procedure.
Results of Hits@1 are shown in Table~\ref{tab:pascal} and~\ref{tab:willow} for \textsc{PascalVOC} and \textsc{WILLOW-ObjectClass}, respectively.
We visualize qualitative results of our method in Appendix~\ref{sec:qualitative_keypoint_matching_results}.

We observe that our refinement strategy is able to significantly outperform competing methods as well as our non-refined baselines.
On the  dataset, our refinement stage at least reduces the error of the initial model () by half across all categories.
The benefits of the second stage are even more crucial when starting from a weaker initial feature matching baseline (), with overall improvements of up to  percentage points on .
However, good initial matchings do help our consensus stage to improve its performance further, as indicated by the usage of task-specific isotropic or anisotropic GNNs for .

\subsection{Supervised Geometric Keypoint Matching}\label{sub:supervised_geometric_keypoint_matching}

We also verify our approach by tackling the \emph{geometric feature matching problem}, where we only make use of point coordinates and no additional visual features are available.
Here, we follow the experimental training setup of \citet{Zhang/Lee/2019}, and test the generalization capabilities of our model on the  dataset \citep{Ham/etal/2016}.
For training, we generate a synthetic set of graph pairs: We first randomly sample 30--60 source points uniformly from , and add Gaussian noise from  to these points to obtain the target points.
Furthermore, we add 0--20 outliers from  to each point cloud.
Finally, we construct graphs by connecting each node with its -nearest neighbors ().
We train our unmodified anisotropic keypoint architecture from Section~\ref{sub:supervised_keypoint_matching_in_natural_images} with input  until it has seen  synthetic examples.

\paragraph{Results.}

We evaluate our trained model on the  dataset \citep{Ham/etal/2016} which consists of  image pairs within 20 classes, with the number of keypoints ranging from 4 to 17.
Results of Hits@1 are shown in Table~\ref{tab:pascal_pf}.
Overall, our consensus architecture improves upon the state-of-the-art results of \citet{Zhang/Lee/2019} on almost all categories while our  baseline is weaker than the results reported in \citet{Zhang/Lee/2019}, showing the benefits of applying our consensus stage.
In addition, it shows that our method works also well even when not taking any visual information into account.

\begin{table}[t]
  \centering
  \caption{Hits@1 (\%) on the \textsc{PascalPF} dataset using a synthetic training setup.}\label{tab:pascal_pf}
  \renewcommand{\arraystretch}{1.1}
  \setlength{\tabcolsep}{2pt}
  \resizebox{\linewidth}{!}{\begin{tabular}{llccccccccccccccccccccc}
    \toprule
      \mc{2}{l}{\textbf{Method}} & \textbf{Aero} & \textbf{Bike} & \textbf{Bird} & \textbf{Boat} & \textbf{Bottle} & \textbf{Bus} & \textbf{Car} & \textbf{Cat} & \textbf{Chair} & \textbf{Cow} & \textbf{Table} & \textbf{Dog} & \textbf{Horse} & \textbf{M-Bike} & \textbf{Person} & \textbf{Plant} & \textbf{Sheep} & \textbf{Sofa} & \textbf{Train} & \textbf{TV} & \textbf{Mean} \\
    \midrule
      \mc{2}{l}{\citep{Zhang/Lee/2019}} & 76.1 & 89.8 & 93.4 & 96.4 & 96.2 & 97.1 & 94.6 & 82.8 & 89.3 & \textbf{96.7} & 89.7 & 79.5 & 82.6 & 83.5 & 72.8 & 76.7 & 77.1 & 97.3 & 98.2 & \textbf{99.5} & 88.5 \\
    \midrule
      \mr{3}{\textbf{Ours}~~~} &   & 69.2 & 87.7 & 77.3 & 90.4 & 98.7 & 98.3 & 92.5 & 91.6 & 94.7 & 79.4 & 95.8 & 90.1 & 80.0 & 79.5 & 72.5 & 98.0 & 76.5 & 89.6 & 93.4 & 97.8 & 87.6 \\
      &                           & \textbf{81.3} & \textbf{92.2} & 94.2 & 98.8 & \textbf{99.3} & 99.1 & 98.6 & \textbf{98.2} & \textbf{99.6} & 94.1 & \textbf{100.0} & \textbf{99.4} & \textbf{86.6} & \textbf{86.6} & \textbf{88.7} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & 99.3 & \textbf{95.8} \\
      &                           & 81.1 & 92.0 & \textbf{94.7} & \textbf{100.0} & \textbf{99.3} & \textbf{99.3} & \textbf{98.9} & 97.3 & 99.4 & 93.4 & \textbf{100.0} & 99.1 & 86.3 & 86.2 & \textbf{87.7} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & \textbf{100.0} & 99.3 & 95.7 \\
    \bottomrule
  \end{tabular}}
\end{table}

\subsection{Semi-supervised Cross-lingual Knowledge Graph Alignment}\label{sub:semi-supervised_cross-lingual_knowledge_graph_alignment}

We evaluate our model on the \textsc{DBP15K} datasets \citep{Sun/etal/2017} which link entities of the Chinese, Japanese and French knowledge graphs of \textsc{DBpedia} into the English version and vice versa.
Each dataset contains exactly  links between equivalent entities, and we split those links into training and testing following upon previous works.
For obtaining entity input features, we follow the experimental setup of \citet{Xu/etal/2019a}: We retrieve monolingual \textsc{fastText} embeddings \citep{Bojanowski/etal/2017} for each language separately, and align those into the same vector space afterwards \citep{Lample/etal/2018}.
We use the sum of word embeddings as the final entity input representation (although more sophisticated approaches are just as conceivable).

\paragraph{Architecture and parameters.}

Our graph neural network operator mostly matches the one proposed in \citet{Xu/etal/2019a} where the direction of edges is retained, but not their specific relation type:

We use  followed by dropout with probability  as our non-linearity , and obtain final node representations via .
We use a three-layer GNN () both for obtaining initial similarities and for refining alignments with dimensionality  and , respectively.
Training is performed using negative log likelihood in a semi-supervised fashion: For each training node  in , we train  sparsely by using the corresponding ground-truth node in , the top  entries in  and  randomly sampled entities in .
For the refinement phase, we update the sparse top  correspondence matrix  times.
For efficiency reasons, we train  and  sequentially for  epochs each.

\paragraph{Results.}

\begin{table}[t]
  \centering
  \caption{Hits@1 (\%) and Hits@10 (\%) on the \textsc{DBP15K} dataset.}\label{tab:dbp15k}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\linewidth}{!}{\begin{tabular}{llcccccccccccc}
    \toprule
      \mc{2}{l}{\mr{2}{\textbf{Method}}} & \mc{2}{c}{\textbf{ZH\raisebox{1pt}{}EN}} & \mc{2}{c}{\textbf{EN\raisebox{1pt}{}ZH}} & \mc{2}{c}{\textbf{JA\raisebox{1pt}{}EN}} & \mc{2}{c}{\textbf{EN\raisebox{1pt}{}JA}} & \mc{2}{c}{\textbf{FR\raisebox{1pt}{}EN}} & \mc{2}{c}{\textbf{EN\raisebox{1pt}{}FR}} \\
      & & \textbf{@1} & \textbf{@10} & \textbf{@1} & \textbf{@10} & \textbf{@1} & \textbf{@10} &\textbf{@1} & \textbf{@10} & \textbf{@1} & \textbf{@10} &  \textbf{@1} & \textbf{@10} \\
    \midrule
      \mc{2}{l}{\textsc{GCN} \citep{Wang/etal/2018}} & 41.25 & 74.38 & 36.49 & 69.94 & 39.91 & 74.46 & 38.42 & 71.81 & 37.29 & 74.49 & 36.77 & 73.06 \\
      \mc{2}{l}{\textsc{BootEA} \citep{Sun/etal/2018}} & 62.94 & 84.75 & & & 62.23 & 85.39 & & & 65.30 & 87.44 & & \\
      \mc{2}{l}{\textsc{MuGNN} \citep{Cao/etal/2019}} & 49.40 & 84.40 & & & 50.10 & 85.70 & & & 49.60 & 87.00 & & \\
      \mc{2}{l}{\textsc{NAEA} \citep{Zhu/etal/2019}} & 65.01 & 86.73 & & & 64.14 & 87.27 & & & 67.32 & 89.43 & & \\
      \mc{2}{l}{\textsc{RDGCN} \citep{Wu/etal/2019}} & 70.75 & 84.55 & & & 76.74 & 89.54 & & & 88.64 & 95.72 & & \\
      \mc{2}{l}{\textsc{GMNN} \citep{Xu/etal/2019a}} & 67.93 & 78.48 & 65.28 & 79.64 & 73.97 & 87.15 & 71.29 & 84.63 & 89.38 & 95.25 & 88.18 & 94.75 \\
    \midrule
       &  & 58.53 & 78.04 & 54.99 & 74.25 & 59.18 & 79.16 & 55.40 & 75.53 & 76.07 & 91.54 & 74.89 & 90.57 \\
    \midrule
      \mr{2}{\textbf{Ours} (sparse)} &  & 67.59 & \textbf{87.47} & 64.38 & \textbf{83.56} & 71.95 & \textbf{89.74} & 68.88 & \textbf{86.84} & 83.36 & \textbf{96.03} & 82.16 & \textbf{95.28} \\
      &  & \textbf{80.12} & \textbf{87.47} & \textbf{76.77} & \textbf{83.56} & \textbf{84.80} & \textbf{89.74} & \textbf{81.09} & \textbf{86.84} & \textbf{93.34} & \textbf{96.03} & \textbf{91.95} & \textbf{95.28} \\
    \bottomrule
  \end{tabular}}
\end{table}

We report Hits@1 and Hits@10 to evaluate and compare our model to previous lines of work, see Table~\ref{tab:dbp15k}.
In addition, we report results of a simple three-layer  which matches nodes purely based on initial word embeddings, and a variant of our model without the refinement of initial correspondences ().
Our approach improves upon the state-of-the-art on all categories with gains of up to  percentage points.
In addition, our refinement strategy consistently improves upon the Hits@1 of initial correspondences by a significant margin, while results of Hits@10 are shared due to the refinement operating only on sparsified top  initial correspondences.
Due to the scalability of our approach, we can easily apply a multitude of refinement iterations while still retaining large hidden feature dimensionalities.

\section{Limitations}\label{sec:limitations}

Our experimental results demonstrate that the proposed approach effectively solves challenging real-world problems.
However, the expressive power of GNNs is closely related to the WL heuristic for graph isomorphism testing \citep{Xu/etal/2019b,Morris/etal/2019}, whose power and limitations are well understood \citep{Arvind/etal/2015}.
Our method generally inherits these limitations.
Hence, one possible limitation is that whenever two nodes are assigned the same color by WL, our approach may fail to converge to one of the possible solutions.
For example, there may exist two nodes  with equal neighborhood sets .
One can easily see that the feature matching procedure generates equal initial correspondence distributions , resulting in the same mapped node indicator functions  from  to nodes  and , respectively.
Since both nodes share the same neighborhood,  also produces the same distributed functions .
As a result, both column vectors  and  receive the same update, leading to non-convergence.
In theory, one might resolve these ambiguities by adding a small amount of noise to .
However, the general amount of feature noise present in real-world datasets already ensures that this scenario is unlikely to occur.

\section{Related Work}\label{sec:related_work}

Identifying correspondences between the nodes of two graphs has been studied in various domains and an extensive body of literature exists.
Closely related problems are summarized under the terms \emph{maximum common subgraph} \citep{Kriege/etal/2019}, \emph{network alignment} \citep{Zhang/Tong/2016}, \emph{graph edit distance} \citep{Chen/etal/2019} and \emph{graph matching} \citep{Yan/etal/2016}.
We refer the reader to the Appendix~\ref{sec:related_work_i} for a detailed discussion of the related work on these problems.
Recently, graph neural networks have become a focus of research leading to various proposed \emph{deep graph matching techniques} \citep{Wang/etal/2019,Zhang/Lee/2019,Xu/etal/2019a,Derr/etal/2019}.
In Appendix~\ref{sec:related_work_ii}, we present a detailed overview of the related work in this field while highlighting individual differences and similarities to our proposed graph matching consensus procedure.

\section{Conclusion}\label{sec:conclusion}

We presented a two-stage neural architecture for learning node correspondences between graphs in a supervised or semi-supervised fashion.
Our approach is aimed towards reaching a neighborhood consensus between matchings, and can resolve violations of this criteria in an iterative fashion.
In addition, we proposed enhancements to let our algorithm scale to large input domains.
We evaluated our architecture on real-world datasets on which it consistently improved upon the state-of-the-art.

\subsubsection*{Acknowledgements}

This work has been supported by the \emph{German Research Association (DFG)} within the Collaborative Research Center SFB 876 \emph{Providing Information by Resource-Constrained Analysis}, projects A6 and B2.

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\newpage

\appendix

\section{Optimized Graph Matching Consensus Algorithm}\label{sec:optimized_graph_matching_consensus_algorithm}

Our final optimized algorithm is given in Algorithm~\ref{alg:algorithm}:

\begin{algorithm}[H]
  \caption{Optimized graph matching consensus algorithm}\label{alg:algorithm}
  \begin{algorithmic}
    \State{\textbf{Input}: , , hidden node dimensionality , sparsity parameter , number of consensus iterations , number of random functions }
\State{\textbf{Output}: Sparse soft correspondence matrix  with  non-zero entries}
    \State{--------------------------------------------------------------------------------------------------------------------} \State{} \Comment{Compute node embeddings }
    \State{} \Comment{Compute node embeddings }
      \State{} \Comment{Local feature matching}
      \State{} \Comment{Sparsify to top  candidates }
    \For{ in } \Comment{}
      \State{} \Comment{Normalize scores }
      \State{} \Comment{Sample random node function }
      \itemsep=0.5ex
      \State{} \Comment{Map random node functions  from  to }
      \itemsep=0ex
      \State{} \Comment{Distribute function  on }
      \State{} \Comment{Distribute function  on }
      \State{} \Comment{Compute neighborhood consensus measure}
      \State{} \Comment{Perform trainable correspondence update}
    \EndFor \itemsep=0.5ex
      \State{} \Comment{Normalize scores }
    \itemsep=0ex
    \State{\textbf{return} }
  \end{algorithmic}
\end{algorithm}

\section{Proof for Theorem 1}\label{sec:proof_for_theorem_1}

\begin{proof}
  Since  is permutation equivariant, it holds for any node feature matrix  that .
  With  and , it follows that
  
  Hence, it shows that  for any node , resulting in .
\end{proof}

\section{Proof for Theorem 2}\label{sec:proof_for_theorem_2}

\begin{proof}
  Let be .
  Then, the -layered GNN  maps both -hop neighborhoods around nodes  and  to the same vectorial representation:
  
  Because  is as powerful as the WL heuristic in distinguishing graph structures \citep{Xu/etal/2019b,Morris/etal/2019} and is operating on injective node colorings , it has the power to distinguish \emph{any} graph structure from , \cf{} \citep{Murphy/etal/2019}.
  Since  holds information about every node in , it necessarily holds that  in case , where  denotes the labeled graph isomorphism relation.
  Hence, there exists an isomorphism  between  and  such that
  
  With  being the identity matrix, it follows that .
  Furthermore, it holds that  when reducing  to its column-wise non-zero entries.
  It follows that  is a permutation matrix describing an isomorphism.

  Moreover, if  for all , it directly follows that  is holding submatrices describing isomorphisms between any -hop subgraphs around  and .
  Assume there exists nodes  that map to the same node .
  It follows that  which contradicts the injectivity requirements of  and  for all .
  Hence,  must be itself a permutation matrix describing an isomorphism between  and .
\end{proof}

\section{Comparison to the Graduated Assignment Algorithm}\label{sec:comparison_to_the_graduated_assignment_algorithm}

As stated in Section~\ref{sub:relation_to_the_graduated_assignment}, our algorithm can be viewed as a generalization of the graduated assignment algorithm \citep{Gold/Rangarajan/1996} extending it by trainable parameters.
To evaluate the impact of a trainable refinement procedure, we replicated the experiments of Sections~\ref{sub:supervised_keypoint_matching_in_natural_images} and~\ref{sub:semi-supervised_cross-lingual_knowledge_graph_alignment} by implementing  via a non-trainable, one-layer GNN instantiation .

The results in Tables~\ref{tab:pascal_ga} and~\ref{tab:dbp15k_ga} show that using trainable neural networks  consistently improves upon the results of using the fixed-function message passing scheme.
While it is difficult to encode meaningful similarities between node and edge features in a fixed-function pipeline, our approach is able to \emph{learn} how to make use of those features to guide the refinement procedure further.
In addition, it allows us to choose from a variety of task-dependent GNN operators, \eg, for learning geometric/edge conditioned patterns or for fulfilling injectivity requirements.
The theoretical expressivity discussed in Section~\ref{sec:limitations} could even be enhanced by making use of higher-order GNNs, which we leave for future work.

\begin{table}[t]
  \centering
  \caption{Hits@1 (\%) on the \textsc{PascalVOC} dataset with Berkeley keypoint annotations.}\label{tab:pascal_ga}
  \renewcommand{\arraystretch}{1.1}
  \setlength{\tabcolsep}{2pt}
  \resizebox{\linewidth}{!}{\begin{tabular}{llccccccccccccccccccccc}
    \toprule
      \mc{2}{l}{\textbf{Method}} & \textbf{Aero} & \textbf{Bike} & \textbf{Bird} & \textbf{Boat} & \textbf{Bottle} & \textbf{Bus} & \textbf{Car} & \textbf{Cat} & \textbf{Chair} & \textbf{Cow} & \textbf{Table} & \textbf{Dog} & \textbf{Horse} & \textbf{M-Bike} & \textbf{Person} & \textbf{Plant} & \textbf{Sheep} & \textbf{Sofa} & \textbf{Train} & \textbf{TV} & \textbf{Mean} \\
    \midrule
      isotropic &   & 44.3 & 62.0 & 48.4 & 53.9 & 73.3 & 80.4 & 72.2 & 64.2 & 30.3 & 52.7 & 79.4 & 56.6 & 62.3 & 56.2 & 47.5 & 74.0 & 59.8 & 79.9 & 81.9 & 83.0 & 63.1 \\
    \midrule
      \mr{2}{}
      &  & 45.9 & 60.5 & 49.0 & 59.7 & 72.8 & 80.9 & 77.4 & 67.2 & 34.1 & 56.3 & 80.4 & 59.5 & 68.6 & 53.9 & 48.6 & 75.5 & 60.8 & 91.5 & 84.8 & 80.3 & 65.4 \\
      &  & 44.7 & 61.5 & 53.0 & 63.1 & 73.6 & 81.2 & 75.2 & 68.1 & 33.9 & 57.1 & 80.5 & 59.7 & 66.5 & 54.4 & 51.6 & 74.9 & 63.6 & 85.4 & 79.6 & 82.3 & 65.5 \\
    \midrule
      \mr{2}{}
      &  & 46.5 & 63.7 & 54.9 & 60.9 & 79.4 & \textbf{84.1} & 76.4 & 68.3 & \textbf{38.5} & \textbf{61.5} & \textbf{80.6} & 59.7 & 69.8 & 58.4 & 54.3 & 76.4 & 64.5 & \textbf{95.7} & \textbf{87.9} & 81.3 & 68.1 \\
      &  & \textbf{50.1} & \textbf{65.4} & \textbf{55.7} & \textbf{65.3} & \textbf{80.0} & 83.5 & \textbf{78.3} & \textbf{69.7} & 34.7 & 60.7 & 70.4 & \textbf{59.9} & \textbf{70.0} & \textbf{62.2} & \textbf{56.1} & \textbf{80.2} & \textbf{70.3} & 88.8 & 81.1 & \textbf{84.3} & \textbf{68.3} \\
    \bottomrule
  \end{tabular}}
\end{table}

\begin{table}[t]
  \centering
  \caption{Hits@1 (\%) on the \textsc{DBP15K} dataset.}\label{tab:dbp15k_ga}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\linewidth}{!}{\begin{tabular}{llccccccc}
    \toprule
      \mc{2}{l}{\textbf{Method}} & \textbf{ZH\raisebox{1pt}{}EN} & \textbf{EN\raisebox{1pt}{}ZH} & \textbf{JA\raisebox{1pt}{}EN} & \textbf{EN\raisebox{1pt}{}JA} & \textbf{FR\raisebox{1pt}{}EN} & \textbf{EN\raisebox{1pt}{}FR} \\
    \midrule
                                                    &   & 67.59 & 64.38 & 71.95 & 68.88 & 83.36 & 82.16 \\
            &  & 71.61 & 68.52 & 77.18 & 76.53 & 85.69 & 85.96 \\
       &  & \textbf{80.12} & \textbf{76.77} & \textbf{84.80} & \textbf{81.09} & \textbf{93.34} & \textbf{91.95} \\
    \bottomrule
  \end{tabular}}
\end{table}

\section{Robustness Towards Node Addition or Removal}\label{sec:robustness_towards_node_addition_or_removal}

To experimentally validate the robustness of our approach towards node addition (or removal), we conducted additional synthetic experiments in a similar fashion to \citet{Xu/etal/2019c}.
We form graph-pairs by treating an Erd\H{o}s \& R\'{e}nyi graph with  nodes and edge probability  as our source graph .
The target graph  is then constructed by first adding  noisy nodes to the source graph, \ie, , and generating edges between these nodes and all other nodes based on the edge probability  afterwards.
We use the same network architecture and training procedure as described in Section~\ref{sub:ablation_study_on_synthetic_graphs}.

Figure~\ref{fig:syn_add} visualizes the Hits@1 for different choices of ,  and .
As one can see, our consensus stage is extremely robust to the addition or removal of nodes while the first stage alone has major difficulties in finding the right matching.
This can be explained by the fact that unmatched nodes do not have any influence on the neighborhood consensus error since those nodes do not obtain a color from the functional map given by .
Our neural architecture is able to detect and gradually decrease any false positive influence of these nodes in the refinement stage.

\begin{figure}[t]
  \centering
  \includegraphics[height=0.4cm]{figures/legend_add.pdf}\\
  \subfigure[, ]{
    \includegraphics[height=2.86cm]{figures/syn50_add.pdf}\label{fig:syn50_add}
  }
  \hspace{1cm}
  \subfigure[, ]{
    \includegraphics[height=2.86cm]{figures/syn100_add.pdf}\label{fig:syn100_add}
  }
  \caption{The performance of our method on synthetic data with node additions.}\label{fig:syn_add}
\end{figure}

\section{Related Work I}\label{sec:related_work_i}

Identifying correspondences between the nodes of two graphs is a problem arising in various domains and has been studied under different terms.
In graph theory, the combinatorial \emph{maximum common subgraph isomorphism} problem is studied, which asks for the largest graph that is contained as subgraph in two given graphs.
The problem is -hard in general and remains so even in trees \citep{Garey/Johnson/1979} unless the common subgraph is required to be connected \citep{Matula/1978}.
Moreover, most variants of the problem are difficult to approximate with theoretical guarantees \citep{Kann/1992}.
We refer the reader to the survey by \citet{Kriege/etal/2019} for a overview of the complexity results noting that exact polynomial-time algorithms are available for specific problem variants only that are most relevant in cheminformatics.

Fundamentally different techniques have been developed in bioinformatics and computer vision, where the problem is commonly referred to as \emph{network alignment} or \emph{graph matching}.
In these areas large networks without any specific structural properties are common and the studied techniques are non-exact.
In graph matching, for two graphs of order  with adjacency matrix  and , respectively, typically the function

is to be minimized, where  with  the set of  permutation matrices and  denotes the squared Frobenius norm.
Since the first two terms of the right-hand side do not depend on , minimizing Equation~\eqref{eq:gm} is equivalent in terms of optimal solutions to the problem of Equation~\eqref{eq:unsupervised_formulation}.
We briefly summarize important related work in graph matching and refer the reader to the recent survey by \citet{Yan/etal/2016} for a more detailed discussion.
There is a long line of research trying to minimize Equation~\eqref{eq:gm} for  by a Frank-Wolfe type algorithm \citep{Jaggi/2013} and finally projecting the fractional solution to  \citep{Gold/Rangarajan/1996,Zaslavskiy/etal/2009,Leordeanu/etal/2009,Egozi/etal/2013,Zhou/DeLaTorre/2016}.
However, the applicability of relaxation and projection is still poorly understood and only few theoretical results exist \citep{Aflalo/etal/2015,Lyzinski/etal/2016}.
A classical result by \citet{Tinhofer/1991} states that the WL heuristic distinguishes two graphs  and  if and only if there is no fractional  such that the objective function in Equation~\eqref{eq:gm} takes .
\citet{Kersting/etal/2014} showed how the Frank-Wolfe algorithm can be modified to obtain the WL partition.
\citet{Aflalo/etal/2015} proved that the standard relaxation yields a correct solution for a particular class of asymmetric graphs, which can be characterized by the spectral properties of their adjacency matrix.
Finally, \citet{Bento/Ioannidis/2018} studied various relaxations, their complexity and properties.
Other approaches to graph matching exist, \eg, based on spectral relaxations \citep{Umeyama/1988,Leordeanu/Hebert/2005} or random walks \citep{Gori/etal/2005}.
The problem of graph matching is closely related to the notoriously hard quadratic assignment problem (QAP) \citep{Zhou/DeLaTorre/2016}, which has been studied in operations research for decades.
Equation~\eqref{eq:unsupervised_formulation} can be directly interpreted as \emph{Koopmans-Beckmann's QAP}.
The more recent literature on graph matching typically considers a weighted version, where node and edge similarities are taken into account. This leads to the formulation as \emph{Lawler's QAP}, which involves an affinity matrix of size  and is computational demanding.
\citet{Zhou/DeLaTorre/2016} proposed to factorize the affinity matrix into smaller matrices and incorporated global geometric constraints.
\citet{Zhang/etal/2019} studied \emph{kernelized graph matching}, where the node and edge similarities are kernels, which allows to express the graph matching problem again as Koopmans-Beckmann's QAP in the associated Hilbert space.
Inspired by established methods for Maximum-A-Posteriori (MAP) inference in conditional random fields, \citet{Swoboda/etal/2017} studied several Lagrangean decompositions of the graph matching problem, which are solved by dual ascent algorithms also known as \emph{message passing}.
Specific message passing schedules and update mechanisms leading to state-of-the-art performance in graph matching tasks have been identified experimentally.
Recently, \emph{functional representation} for graph matching has been proposed as a generalizing concept with the additional goal to avoid the construction of the affinity matrix \citep{Wang/etal/2019b}.

\paragraph{Graph edit distance.}

A related concept studied in computer vision is the \emph{graph edit distance}, which measures the minimum cost required to transform a graph into another graph by adding, deleting and substituting vertices and edges.
The idea has been proposed for pattern recognition tasks more than 30 years ago \citep{Sanfeliu/Fu/1983}.
However, its computation is -hard, since it generalizes the maximum common subgraph problem \citep{Bunke/1997}.
Moreover, it is also closely related to the quadratic assignment problem \citep{Bougleux/etal/2017}.
Recently several elaborated exact algorithms for computing the graph edit distance have been proposed \citep{Gouda/Hassaan/2016,Lerouge/etal/2017,Chen/etal/2019}, but are still limited to small graphs.
Therefore, heuristics based on the assignment problem have been proposed \citep{Riesen/Bunke/2009} and are widely used in practice \citep{Stauffer/etal/2017}.
The original approach requires cubic running time, which can be reduced to quadratic time using greedy strategies \citep{Riesen/etal/2015b,Riesen/etal/2015}, and even linear time for restricted cost functions \citep{Kriege/etal/2019a}.

\paragraph{Network alignment.}

The problem of \emph{network alignment} typically is defined analogously to Equation~\eqref{eq:unsupervised_formulation}, where in addition a similarity function between pairs of nodes is given.
Most algorithms follow a two step approach: First, an  node-to-node similarity matrix  is computed from the given similarity function and the topology of the two graphs.
Then, in the second step, an alignment is computed by solving the assignment problem for .
\citet{Singh/etal/2008} proposed \textsc{IsoRank}, which is based on the adjacency matrix of the product graph  of  and , where  denotes the Kronecker product.
The matrix  is obtained by applying \textsc{PageRank} \citep{Page/etal/1999} using a normalized version of  as the \textsc{Google} matrix and the node similarities as the personalization vector.
\citet{Kollias/etal/2012} proposed an efficient approximation of \textsc{IsoRank} by decomposition techniques to avoid generating the product graph of quadratic size.
\citet{Zhang/Tong/2016} present an extension supporting vertex and edge similarities and propose its computation using non-exact techniques.
\citet{Klau/2009} proposed to solve network alignment by linearizing the quadratic optimization problem to obtain an integer linear program, which is then approached via Lagrangian relaxation.
\citet{Bayati/etal/2013} developed a message passing algorithm for sparse network alignment, where only a small number of matches between the vertices of the two graphs are allowed.

The techniques briefly summarized above aim to find an optimal correspondence according to a clearly defined objective function.
In practical applications, it is often difficult to specify node and edge similarity functions.
Recently, it has been proposed to \emph{learn} such functions for a specific task, \eg, in form of a cost model for the graph edit distance \citep{Cortes/etal/2019}.
A more principled approach has been proposed by \citet{Caetano/etal/2009} where the goal is to learn correspondences.

\section{Related Work II}\label{sec:related_work_ii}

The method presented in this work is related to different lines of research.
Deep graph matching procedures have been investigated from multiple perspectives, \eg{}, by utilizing local node feature matchings and cross-graph embeddings \citep{Li/etal/2019}.
The idea of refining local feature matchings by enforcing neighborhood consistency has been relevant for several years for matching in images \citep{Sattler/etal/2009}.
Furthermore, the functional maps framework aims to solve a similar problem for manifolds \citep{Halimi/etal/2019}.

\paragraph{Deep graph matching.}

Recently, the problem of graph matching has been heavily investigated in a deep fashion.
For example, \citet{Zanfir/Sminchisescu/2018,Wang/etal/2019,Zhang/Lee/2019} develop supervised deep graph matching networks based on displacement and combinatorial objectives, respectively.
\citet{Zanfir/Sminchisescu/2018} model the graph matching affinity via a differentiable, but unlearnable spectral graph matching solver \citep{Leordeanu/Hebert/2005}.
In contrast, our matching procedure is fully-learnable.
\citet{Wang/etal/2019} use node-wise features in combination with dense node-to-node cross-graph affinities, distribute them in a local fashion, and adopt  normalization for the final task of linear assignment.
\citet{Zhang/Lee/2019} propose a compositional message passing algorithm that maps point coordinates into a high-dimensional space.
The final matching procedure is done by computing the pairwise inner product between point embeddings.
However, neither of these approaches can naturally resolve violations of inconsistent neighborhood assignments as we do in our work.

\citet{Xu/etal/2019c} tackles the problem of graph matching by relating it to the Gromov-Wasserstein discrepancy \citep{Peyre/etal/2016}.
In addition, the optimal transport objective is enhanched by simultaneously learning node embeddings which shall account for the noise in both graphs.
In a follow-up work, \citet{Xu/etal/2019d} extend this concept to the tasks of multi-graph partioning and matching by learning a Gromov-Wasserstein barycenter.
Our approach also resembles the optimal transport between nodes, but works in a supervised fashion for sets of graphs and is therefore able to generalize to unseen graph instances.

In addition, the task of network alignment has been recently investigated from multiple perspectives.
\citet{Derr/etal/2019} leverage s \citep{Zhu/etal/2017} to align  embeddings \citep{Grover/Leskovec/2016} and find matchings based on the nearest neighbor in the embedding space.
\citet{Zhang/etal/2019a} design a deep graph model based on global and local network topology preservation as auxiliary tasks.
\citet{Heimann/etal/2018} utilize a fast, but purely local and greedy matching procedure based on local node embedding similarity.

Furthermore, \citet{Bai/etal/2019} use shared graph neural networks to approximate the graph edit distance between two graphs.
Here, a (non-differentiable) histogram of correspondence scores is used to fine-tune the output of the network.
In a follow-up work, \citet{Bai/etal/2018} proposed to order the correspondence matrix in a breadth-first-search fashion and to process it further with the help of traditional CNNs.
Both approaches only operate on local node embeddings, and are hence prone to match correspondences inconsistently.

\paragraph{Intra- and inter-graph message passing.}

The concept of enhanching intra-graph node embeddings by inter-graph node embeddings has been already heavily investigated in practice \citep{Li/etal/2019,Wang/etal/2019,Xu/etal/2019a}.
\citet{Li/etal/2019} and \citet{Wang/etal/2019} enhance the GNN operator by not only aggregating information from local neighbors, but also from similar embeddings in the other graph by utilizing a cross-graph matching procedure.
\citet{Xu/etal/2019a} leverage alternating GNNs to propagate local features of one graph throughout the second graph.
\citet{Wang/Solomon/2019} tackle the problem of finding an unknown rigid motion between point clouds by relating it to a point cloud matching problem followed by a differentiable SVD module.
Intra-graph node embeddings are passed via a Transformer module before feature matching based on inner product similarity scores takes place.
However, neither of these approaches is designed to achieve a consistent matching, due to only operating on localized node embeddings which are alone not sufficient to resolve ambiguities in the matchings.
Nonetheless, we argue that these methods can be used to strengthen the initial feature matching procedure, making our approach orthogonal to improvements in this field.

\paragraph{Neighborhood consensus for image matching.}

Methods to obtain consistency of correspondences in local neighborhoods have a rich history in computer vision, dating back several years \citep{Sattler/etal/2009,Sivic/Zisserman/2003,Schmid/Mohr/1997}.
They are known for heavily improving results of local feature matching procedures while being computational efficient.
Recently, a deep neural network for neighborhood consensus using 4D convolution was proposed \citep{Rocco/etal/2018}.
While it is related to our method, the 4D convolution can not be efficiently transferred to the graph domain directly, since it would lead to applying a GNN on the product graph with  nodes and  edges.
Our algorithm also infers errors for the (sparse) product graph but performs the necessary computations on the original graphs.

\paragraph{Functional maps.}

The functional maps framework was proposed to provide a way to define continuous maps between function spaces on manifolds and is commonly applied to solve the task of 3D shape correspondence \citep{Ovsjanikov/etal/2012,Litany/etal/2017,Rodola/etal/2017,Halimi/etal/2019}.
Recently, a similar approach was presented to find functional correspondences between graph function spaces \citep{Wang/etal/2019b}.
The functional map is established by using a low-dimensional basis representation, \eg, the eigenbasis of the graph Laplacian as generalized Fourier transform.
Since the basis is usually truncated to the  vectors with the largest eigenvalues, these approaches focus on establishing global correspondences.
However, such global methods have the inherent disadvantage that they often fail to find partial matchings due to the domain-dependent eigenbasis.
Furthermore, the basis computation has to be approximated in order to scale to large inputs.

\section{Dataset Statistics}\label{sec:dataset_statistics}

\begin{table}[t]
  \centering
  \begin{minipage}[c]{0.4\linewidth}
  \centering
  \caption{Statistics of the \textsc{WILLOW-ObjectClass} dataset.}\label{tab:willow_stats}
  \vspace{0.1cm}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\linewidth}{!}{\begin{tabular}{lrrr}
    \toprule
      \textbf{Category} & \textbf{Graphs} & \textbf{Keypoints} & \textbf{Edges} \\
    \midrule
      \textbf{Face} & 108 & 10 &  \\
      \textbf{Motorbike} & 40 & 10 &  \\
      \textbf{Car} & 40 & 10 &  \\
      \textbf{Duck} & 50 & 10 &  \\
      \textbf{Winebottle} & 66 & 10 &  \\
    \bottomrule
  \end{tabular}}
  \end{minipage}
  \hfill
  \begin{minipage}[c]{0.49\linewidth}
  \centering
  \caption{Statistics of the \textsc{DBP15K} dataset.}\label{tab:dbp15k_stats}
  \vspace{0.1cm}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\linewidth}{!}{\begin{tabular}{clrrr}
    \toprule
      \mc{2}{c}{\textbf{Datasets}} & \textbf{Entities} & \textbf{Relation types} & \textbf{Relations} \\
    \midrule
      \mr{2}{\textbf{ZH\raisebox{1pt}{}EN}} & Chinese  & 19\,388 & 1\,701 & 70\,414 \\
                                                                  & English  & 19\,572 & 3\,024 & 95\,142 \\
    \midrule
      \mr{2}{\textbf{JA\raisebox{1pt}{}EN}} & Japanese & 19\,814 & 1\,299 & 77\,214 \\
                                                                  & English  & 19\,780 & 2\,452 & 93\,484 \\
    \midrule
      \mr{2}{\textbf{FR\raisebox{1pt}{}EN}} & French   & 19\,661 & 903    & 105\,998 \\
                                                                  & English  & 19\,993 & 2\,111 & 115\,722 \\
    \bottomrule
  \end{tabular}}
  \end{minipage}
\end{table}

\begin{table}[t]
  \centering
  \caption{Statistics of the \textsc{PascalVOC} dataset with Berkeley annotations.}\label{tab:pascal_stats}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\linewidth}{!}{\begin{tabular}{lrrrrlrrrr}
    \toprule
      \textbf{Category} & \textbf{Train graphs} & \textbf{Test graphs} & \textbf{Keypoints} & \textbf{Edges} & \textbf{Category} & \textbf{Train graphs} & \textbf{Test graphs} & \textbf{Keypoints} & \textbf{Edges} \\
    \midrule
      \textbf{Aeroplane} & 468 & 136 &  &  & \textbf{Diningtable} &  27 &   5 &   &   \\
      \textbf{Bicycle}   & 210 &  53 &  &  & \textbf{Dog}         & 608 & 147 &  &  \\
      \textbf{Bird}      & 613 & 117 &  &  & \textbf{Horse}       & 217 &  45 &  &  \\
      \textbf{Boat}      & 411 &  88 &  &  & \textbf{Motorbike}   & 234 &  60 &  &  \\
      \textbf{Bottle}    & 466 & 120 &   &  & \textbf{Person}      & 539 & 156 &  &  \\
      \textbf{Bus}       & 288 &  52 &   &  & \textbf{Pottedplant} & 429 &  99 &   &  \\
      \textbf{Car}       & 522 & 160 &  &  & \textbf{Sheep}       & 338 &  73 &  &  \\
      \textbf{Cat}       & 415 & 101 &  &  & \textbf{Sofa}        &  73 &   8 &  &  \\
      \textbf{Chair}     & 298 &  63 &  &  & \textbf{Train}       & 166 &  43 &   &  \\
      \textbf{Cow}       & 257 &  55 &  &  & \textbf{TV Monitor}  & 374 &  90 &   &  \\
    \bottomrule
  \end{tabular}}
\end{table}

\begin{table}[t]
  \centering
  \caption{Statistics of the \textsc{PascalPF} dataset.}\label{tab:pascal_pf_stats}
  \renewcommand{\arraystretch}{1.1}
  \resizebox{\linewidth}{!}{\begin{tabular}{lrrrrlrrrr}
    \toprule
      \textbf{Category} & \textbf{Graphs} & \textbf{Pairs} & \textbf{Keypoints} & \textbf{Edges} & \textbf{Category} & \textbf{Graphs} & \textbf{Pairs} & \textbf{Keypoints} & \textbf{Edges} \\
    \midrule
      \textbf{Aeroplane} &  69 &  69 &   &  & \textbf{Diningtable} &  38 &  38 &    &   \\
      \textbf{Bicycle}   & 133 & 133 &  &   & \textbf{Dog}         & 106 & 106 &   &  \\
      \textbf{Bird}      &  50 &  50 &    &   & \textbf{Horse}       &  39 &  39 &   &  \\
      \textbf{Boat}      &  28 &  28 &    &   & \textbf{Motorbike}   & 120 & 120 &   &   \\
      \textbf{Bottle}    &  42 &  42 &    &   & \textbf{Person}      &  56 &  56 &  &  \\
      \textbf{Bus}       & 140 & 140 &    &   & \textbf{Pottedplant} &  35 &  35 &    &   \\
      \textbf{Car}       &  84 &  84 &   &   & \textbf{Sheep}       &   6 &   6 &    &   \\
      \textbf{Cat}       & 119 & 119 &   &  & \textbf{Sofa}        &  59 &  59 &   &   \\
      \textbf{Chair}     &  59 &  59 &   &   & \textbf{Train}       &  88 &  88 &    &   \\
      \textbf{Cow}       &  15 &  15 &   &  & \textbf{TV Monitor}  &  65 &  65 &    &   \\
    \bottomrule
  \end{tabular}}
\end{table}

We give detailed descriptions of all datasets used in our experiments, \cf{} Tables~\ref{tab:willow_stats},~\ref{tab:dbp15k_stats},~\ref{tab:pascal_stats} and~\ref{tab:pascal_pf_stats}.

\section{Qualitative Keypoint Matching Results}\label{sec:qualitative_keypoint_matching_results}

Figure~\ref{fig:willow_examples} visualizes qualitative examples from the task of keypoint matching on the  dataset.
Examples were selected as follows:
Figure~\ref{fig:best_motorbike},~\subref{fig:best_car} and~\subref{fig:best_duck} show examples where the initial feature matching procedure fails, but where our refinement procedure is able to recover \emph{all} correspondences succesfully.
Figure~\ref{fig:worst_duck} visualizes a rare failure case.
However, while the initial feature matching procedure maps most of the keypoints to the same target keypoint, our refinement strategy is still able to succesfully resolve this violation.
In addition, note that the target image contains wrong labels, \eg{}, the eye of the duck, so that some keypoint mappings are mistakenly considered to be wrong.

\begin{figure}[t]
  \centering
  \subfigure[Motorbike]{
    \includegraphics[height=4.4cm]{figures/best_motorbike.pdf}\label{fig:best_motorbike}
  }
  \hfill
  \subfigure[Car]{
    \includegraphics[height=4.4cm]{figures/best_car.pdf}\label{fig:best_car}
  }
  \subfigure[Duck]{
    \includegraphics[height=4.4cm]{figures/best_duck.pdf}\label{fig:best_duck}
  }
  \hfill
  \subfigure[A rare failure case]{
    \includegraphics[height=4.4cm]{figures/worst_duck.pdf}\label{fig:worst_duck}
  }
  \caption{
    Qualitative examples from the  dataset.
    Images on the left represent the source, whereas images on the right represent the target.
    For each example, we visualize both the result of the initial feature matching procedure  (top) and the result obtained after refinement  (bottom).
  }\label{fig:willow_examples}
\end{figure}

\end{document}
