

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{subfig}
\usepackage{hyperref}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage[hang,flushmargin]{footmisc}  \usepackage{amsfonts,amsmath,amssymb}
\usepackage[T1]{fontenc}  \usepackage{bold-extra}   \usepackage{microtype}    \usepackage{graphicx}
\usepackage{multirow}
\usepackage{bm}           \usepackage{soul}         \usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{array,booktabs}
\usepackage{stfloats}  

\usepackage{enumitem}
\setenumerate[1]{leftmargin=*}    \setitemize[1]{leftmargin=*}      



\aclfinalcopy 



\newcommand{\textsec}[1]{\textsection\ref{#1}}
\newcommand\LN{\linebreak\noindent}
\newcommand\AAA{\texttt{AA}}  \newcommand\EE{\texttt{EE}}
\newcommand\SC{\texttt{SC}}
\newcommand\CM{\texttt{CM}}

\title{Revealing the Myth of Higher-Order Inference in Coreference Resolution}

\author{Liyan Xu \\
  Computer Science \\
  Emory University, Atlanta, GA \\
  \texttt{liyan.xu@emory.edu} \\\And
  Jinho D. Choi \\
  Computer Science \\
  Emory University, Atlanta, GA \\
  \texttt{jinho.choi@emory.edu} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution.
HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning.
To make a\LN comprehensive analysis, we implement an end-to-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods.
We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task.
Our best model using cluster merging shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.
\end{abstract}
 \section{Introduction}
\label{sec:intro}

Coreference resolution has always been considered one of the unsolved NLP tasks due to its challenging aspect of document-level understanding \cite{wiseman-etal-2015-learning,wiseman-etal-2016-learning,clark-manning-2015-entity,clark-manning-2016-improving,lee-etal-2017-end}.
Nonetheless, it has made a tremendous progress in recent years by adapting contextualized embedding encoders such as ELMo \cite{lee-etal-2018-higher,fei-etal-2019-end} and BERT \cite{kantor-globerson-2019-coreference,joshi-etal-2019-bert,spanbert-joshi}.
The latest state-of-the-art model shows the improvement of 12.4\% over the model introduced 2.5 years ago, where the major portion of the improvement is derived by representation learning (Figure~\ref{fig:coref-history}).

Most of these previous models have also adapted higher-order inference (HOI) for the global optimization of coreference links, although HOI clearly has not been the focus of those works, for the fact that gains from HOI have been reported marginal.
This has inspired us to analyze the impact of HOI on modern coreference resolution models in order to envision the future direction of this research.

\noindent To make thorough ablation studies among different approaches, we implement an end-to-end coreference system in PyTorch (Sec~\ref{subsec:baseline}),
and two HOI approaches proposed by previous work, attended antecedent and entity equalization (Sec~\ref{subsec:previous}), along with two of our original approaches, span clustering and cluster merging (Sec~\ref{subsec:new}).
These approaches are experimented with two Transformer encoders, BERT and SpanBERT, to assess how effective HOI is even when coupled with those high-performing encoders (Sec~\ref{sec:exp}).
To the best of our knowledge,\LN this is the first work to make a comprehensive analysis on multiple HOI approaches side-by-side for the task of coreference resolution.\footnote{Source codes and models are available at \url{https://github.com/lxucs/coref-hoi}.}

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.9\columnwidth]{img/coref-history.pdf}
\caption{Performance breakdown of the recent state-of-the-art models on the CoNLL 2012 shared task. W-16: \citet{wiseman-etal-2016-learning}, C-16: \citet{clark-manning-2016-improving}, L-17: \citet{lee-etal-2017-end}, L-18: \citet{lee-etal-2018-higher}, F-19: \citet{fei-etal-2019-end}, K-19: \citet{kantor-globerson-2019-coreference}, J-19: \citet{joshi-etal-2019-bert}, J-20: \citet{spanbert-joshi}.}
\label{fig:coref-history}
\end{figure}










%
 \vspace{-1.5ex}
\section{Related Work}
\label{sec:related}

Most neural network-based coreference resolution models have adapted antecedent-ranking \cite{wiseman-etal-2015-learning,clark-manning-2015-entity,lee-etal-2017-end,lee-etal-2018-higher,joshi-etal-2019-bert,spanbert-joshi}, which relies on the local decisions between each mention and its antecedents.
To achieve deeper global optimization,\LN \citet{wiseman-etal-2016-learning,clark-manning-2016-improving,yu-etal-2020-cluster} built entity representations in the ranking process, whereas \citet{lee-etal-2018-higher,kantor-globerson-2019-coreference} refined the mention representation by aggregating its antecedents' information.

It is no secret that the integration of contextualized embeddings has played the most critical role in this task. While the following are based on the same end-to-end coreference model \cite{lee-etal-2017-end},
\citet{lee-etal-2018-higher,fei-etal-2019-end} reported 3.3\% improvement by adapting ELMo in the encoders \cite{peters-etal-2018-deep}.
\citet{kantor-globerson-2019-coreference,joshi-etal-2019-bert} gained additional 3.3\% by adapting BERT \cite{devlin-etal-2019-bert}.
\citet{spanbert-joshi} introduced SpanBERT that gave another 2.7\% improvement over \citet{joshi-etal-2019-bert}.

Most recently, \citet{wu-etal-2020-corefqa} proposes a new model that adapts question-answering framework on coreference resolution, and achieves state-of-the-art result of 83.1 on the CoNLL'12 shared task.




 \section{Approach}
\label{sec:approach}




\subsection{End-to-End Coreference System}
\label{subsec:baseline}

We reimplement the end-to-end \textit{c2f-coref} model introduced by \citet{lee-etal-2018-higher} that has been adapted by every coreference resolution model since then. It detects mention candidates through span enumeration and aggressive pruning.
For each candidate span , the model learns the distribution over its antecedents :

where  is the local score involving two parts: how likely the spans  and  are valid mentions, and how likely they refer to the same entity:

 are the span embeddings of  and ,  is the meta-information (e.g., speakers, distance), and  are the mention and coreference scores, respectively (\texttt{FFNN}: feedforward neural network).

We use different Transformers-based encoders, and follow the ``\textit{independent}'' setup for long documents as suggested by \citet{joshi-etal-2019-bert}.







\subsection{Span Refinement}
\label{subsec:previous}

Two HOI methods presented by recent coreference work are based on span refinement that aggregates non-local features to enrich the span representation with more ``global'' information.
The updated span representation  can be derived as in Eq.~\ref{eq:refine}, where  is the interpolation between the current and refined representation  and , and  is the gate parameter. 
 is used to perform another round of antecedent-ranking in replacement of .

The following two methods share the same updating process for , but with different ways to obtain the refined span representation .


\paragraph{Attended Antecedent (\texttt{AA})} 

takes the antecedent information to enrich  \citep{lee-etal-2018-higher,fei-etal-2019-end,joshi-etal-2019-bert,spanbert-joshi}. 
The refined span  is the attended antecedent representation over the current antecedent distribution , where  is the antecedent representation:



\paragraph{Entity Equalization (\texttt{EE})}

takes the clustering relaxation as in Eq.~\ref{eq:ee} to model the entity distribution \citep{kantor-globerson-2019-coreference}, where  is the probability of the span  referring to an entity  in which the span  is the first mention.  is the current antecedent distribution.

The refined span  is the attended entity representation, where  is the entity representation to which the span  belongs till the span :





\subsection{HOI with Clustering}
\label{subsec:new}

This section introduces two new HOI methods for a more extensive study in HOI.



\paragraph{Span Clustering (\texttt{SC})}

is also based on span refinement, and it
constructs the actual clusters and obtains the ``true'' predicted entities using  instead of modeling the ``soft'' entity clusters through the relaxation as in \texttt{EE} (Section~\ref{subsec:previous}).
This way, although we lose the differentiable property, the obtaining of true entities with the same empirical inference time as \texttt{EE} has made \texttt{SC} desirable.

\noindent The entity representation  for an entity cluster  is given by the attended spans in this cluster:

The entity clusters  are constructed in the same way as in the final cluster prediction. 
The refined span  is then equal to the representation of entity  to which it belongs ().

\paragraph{Cluster Merging (\texttt{CM})}

performs sequential antecedent ranking combining both antecedent and entity information to gradually build up the entity clusters, which is distinguished from span refinement methods that simply re-rank antecedents.
Algorithm~\ref{algo:cm} describes the ranking process for \texttt{CM}. 
 is the 'th span,  is the indices of 's antecedents, and  is the cluster that  belongs to.
The ranking score   consists of both antecedent score  (see Eq.~\ref{eq:antecedent_score}) and cluster score .
To avoid overlapping between  and , we set  as  if the cluster is the initial cluster (L6). 
Thus,  becomes the consultation such that when , the span  is likely to match the cluster , and vice versa. 
 is computed by \texttt{FFNN} similar to , and  is the meta-feature such as the cluster size.

\begin{algorithm}
  \caption{Antecedent Ranking for \CM}\label{algo:cm}
  \begin{algorithmic}[1]
    \small
    \Procedure{ranking}{}
    \State 
    \State  ranking\_order()
    \For{}
        \For{} \Comment{Parallelized}
            \State  if 
            \State 
        \EndFor
        \State 
        \If{}
            \State merge  and 
        \EndIf
    \EndFor
    \State \textbf{return} 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\noindent Two simple configurations can be tuned for \texttt{CM}. We can have the sequential left-to-right ranking order or the easy-first order (L3) whose sequence is ordered by each span's max antecedent score, building the most confident clusters first \citep{ng-cardie-2002-improving,clark-manning-2016-improving}. There can be element-wise mean or max-reduction among the spans in the two merging clusters (L10).

Distinguished from \citet{wiseman-etal-2016-learning}, clusters in \texttt{CM} are searched and merged in training without the use of oracle clusters, closing the gap between training and test time.

 \begin{table*}[htbp!]
\small
\centering
\begin{tabular}{>{\quad}cccccccccccccc}
\toprule
 & \multicolumn{3}{c}{MUC} & & \multicolumn{3}{c}{B\textsuperscript{3}} & & \multicolumn{3}{c}{CEAF\textsubscript{}}\\
 \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}
 & P & R & F1 & & P & R & F1 & & P & R & F1 & Avg. F1 & Avg-M \\
\midrule
\tt L-17 & 78.4 & 73.4 & 75.8 & & 68.6 & 61.8 & 65.0 & & 62.7 & 59.0 & 60.8 & 67.2 & - \\
\tt L-18 & 81.4 & 79.5 & 80.4 & & 72.2 & 69.5 & 70.8 & & 68.2 & 67.1 & 67.6 & 73.0 & - \\
\tt F-19 & 85.4 & 77.9 & 81.4 & & 77.9 & 66.4 & 71.7 & & 70.6 & 66.3 & 68.4 & 73.8 & - \\
\tt K-19 & 82.6 & 84.1 & 83.4 & & 73.3 & 76.2 & 74.7 & & 72.4 & 71.1 & 71.8 & 76.6 & - \\
\tt J-19 & 84.7 & 82.4 & 83.5 & & 76.5 & 74.0 & 75.3 & & 74.1 & 69.8 & 71.9 & 76.9 & - \\
\tt J-20 & 85.8 & 84.8 & 85.3 & & 78.3 & 77.9 & 78.1 & & 76.4 & 74.2 & 75.3 & 79.6 & - \\
\midrule
\tt BERT & 85.0 & 82.5 & 83.8 & & 77.3 & 74.0 & 75.6 & & 74.9 & 70.7 & 72.8 & 77.4 & 77.3 (0.1) \\
\tt SpanBERT & 85.7 & 85.3 & 85.5 & & 78.6 & 78.6 & 78.6 & & \bf 76.8 & 74.8 & 75.8 & 79.9 & 79.7 (0.1) \\
+ \AAA & \bf 86.1 & 84.8 & 85.4 & & \bf 79.3 & 77.3 & 78.3 & & 76.0 & 74.7 & 75.4 & 79.7 & 79.4 (0.2) \\
+ \EE  & 85.7 & 84.5 & 85.1 & & 78.5 & 77.4 & 77.9 & & 76.7 & 73.4 & 75.0 & 79.4 & 78.9 (0.4) \\
+ \SC  & 85.5 & 85.2 & 85.4 & & 78.4 & 78.5 & 78.4 & & 76.5 & 74.1 & 75.2 & 79.7 & 79.2 (0.3) \\
+ \CM  & 85.9 & \bf 85.5 & \bf 85.7 & & 79.0 & \bf 78.9 & \bf 79.0 & & 76.7 & \bf 75.2 & \bf 75.9 & \bf 80.2 & \textbf{79.9} (0.2) \\
\bottomrule
\end{tabular}
\caption{Best results on the test set of the CoNLL'12 English shared task. The averaged F1 of MUC, B\textsuperscript{3}, CEAF\textsubscript{} is the main evaluation metric. Avg-M: the mean Avg-F1 and its standard deviation from five developments. The mean and stdev of other metrics are provided in Appendix~\ref{append:results}. See Figure~\ref{fig:coref-history} for acronyms of the previous works.}
\label{table:results}
\end{table*}



%
 
\section{Experiments}
\label{sec:exp}

For our experiments, the CoNLL 2012 English shared task dataset is used \citep{pradhan-etal-2012-conll}.
Given the end-to-end coreference system in Section~\ref{subsec:baseline}, six models are developed as follows:\footnote{Appdendix~\ref{append:experiments} provides details of our experimental settings.} 

\begin{itemize}
\small\setlength\itemsep{0em}
\item \texttt{BERT}: BERT \cite{devlin-etal-2019-bert} as the encoder
\item \texttt{SpanBERT}: SpanBERT \cite{spanbert-joshi} as the encoder
\item \texttt{+AA}: \texttt{SpanBERT} with attended antecedent (\textsec{subsec:previous})
\item \texttt{+EE}: \texttt{SpanBERT} with entity equalization (\textsec{subsec:previous})
\item \texttt{+SC}: \texttt{SpanBERT} with span clustering (\textsec{subsec:new})
\item \texttt{+CM}: \texttt{SpanBERT} with cluster merging (\textsec{subsec:new})
\end{itemize}

\noindent Note that \noindent \texttt{BERT} and \texttt{SpanBERT} completely rely on only local decisions without any HOI. 
Particularly, \texttt{+AA} is equivalent to \citet{spanbert-joshi}.


\subsection{Results}
\label{subsec:results}

Table~\ref{table:results} shows the best results in comparison to previous state-of-the-art systems. We also report the mean scores and standard deviations from 5 repeated developments, which we could not find from the previous works.


The impact of SpanBERT over BERT is clear, showing 2.4\% improvement on average.
However, none of the HOI models shows a clear advantage over \texttt{SpanBERT} which adapts no HOI.
In fact, all HOI models except for \CM\ show negative impact.
The best result is achieved by \CM\ with the Avg-F1 of 80.2, surpassing the previous best result of 79.6 based on \textit{c2f-coref} reported by \citet{spanbert-joshi}.







\subsection{Impact Analysis of HOI}
\label{subsec:impact}

Three HOI methods based on span refinement, \texttt{AA}, \texttt{EE}, and \texttt{SC}, show negative impact upon local decisions. 
We suspect that error propagation from antecedent-ranking may downgrade the quality of refinement. 
On the other hand, \texttt{CM} shows marginal improvement, suggesting that maintaining entity clusters can be superior to span refinement, at the cost of more inference time from the sequential ranking process.
To analyze the direct impact of HOI, we take the trained models of each HOI method and evaluate them on the test set while turning off HOI, making it compatible to \texttt{SpanBERT}.

The averaged performance drop w.r.t Avg-F1 after turning off HOI is less than 0.2 for all methods\LN (Appendix~\ref{append:analysis}), implying that none of the HOI method has a significantly direct impact to the final performance of the model using SpanBERT.

\begin{table}[htbp!]
\small
\centering
\begin{tabular}{c|cccc}
\multicolumn{1}{c|}{} & \multicolumn{1}{c}{W2C} & \multicolumn{1}{c}{C2W} & \multicolumn{1}{c}{C2C} & \multicolumn{1}{c}{W2W} \\
\midrule
 + \texttt{AA} & 240.8 (1.3) & 241.2 (1.3) & 16262.2 & 2168.4 \\
 + \texttt{EE} & 244.1 (1.3) & 245.3 (1.3) & 16183.3 & 2136.3 \\
 + \texttt{SC} & 248.2 (1.3) & 262.0 (1.4) & 16184.4 & 2146.0 \\
 + \texttt{CM} & 226.4 (\textbf{1.2}) & 235.0 (\textbf{1.2}) & 16446.0 & 2180.0 \\
\end{tabular}
\caption{Averaged statistics on the test set prediction of four HOI approaches. W2C represents the number of mentions that are linked to a \textbf{W}rong antecedent before HOI and are linked to a \textbf{C}orrect antecedent after HOI; vice versa for C2W. C2C/W2W is the number of mentions that are both linked to \textbf{C}orrect/\textbf{W}rong antecedents before and after HOI. Parentheses indicate the percentage of corresponding numbers per row.}
\label{table:link_change}
\vspace{-3ex}
\end{table}

In further investigation, we examine the change of coreferent links w.r.t their correctness. Specifically, Table~\ref{table:link_change} shows the four types of link changes before and after HOI. It demonstrates that the benefits from HOI is diminished because the effects are two-sided: there are roughly same amounts of links (about 1\%) becoming correct or wrong after HOI, therefore neither HOI method leads to much improvement overall.

It is worth mentioning that the impact of HOI is not limited to only global decisions.
HOI implicitly\LN serves as a way of regularization that impacts local decisions as well, since HOI and local ranking are mutually dependent during training.
Such indirect influence of HOI makes it difficult to assess its true impact, which we will explore more in the future.






\subsection{Analysis of Pronoun Resolution}
\label{subsec:pronoun}

\begin{table}[htbp!]
\small
\centering
\begin{tabular}{c|cc|cc|c}
\multicolumn{1}{c|}{} & \multicolumn{1}{c}{SP} & \multicolumn{1}{c|}{PS} & \multicolumn{1}{c}{FL} & \multicolumn{1}{c|}{WL} & \multicolumn{1}{c}{BC} \\
\midrule
\tt BERT     & 2.3 & 6.5 & 213.8 & 186.3 & 48.8 (3.5) \\
\tt SpanBERT & 2.8 & 6.6 & 218.3 & 168.0 & \textbf{43.8} (2.7) \\
 + \texttt{AA} & \bf 1.8 & 8.8 & 214.2 & \bf 159.4 & 44.8 (2.4) \\
 + \texttt{EE} & \bf 1.8 & \bf 5.5 & 210.0 & 165.3 & 44.0 (2.5) \\
 + \texttt{SC} & 3.8 & 7.2 & 223.6 & 170.0 & 45.4 (3.0) \\
 + \texttt{CM} & 3.0 & 6.6 & \bf 208.0 & 162.2 &\textbf{43.8} (2.6) \\
\end{tabular}
\caption{Averaged statistics on the test set prediction of different approaches. SP is the number of coreferent links from \textbf{S}ingular to \textbf{P}lural personal pronouns; vice versa for PS. FL (False Link) and WL (Wrong Link) is the number of conreferent link errors that involve two personal pronouns. BC is the number of clusters that contain both singular and plural pronouns, and the parentheses indicate the numbers of BC that contain ambiguous pronouns such as ``you''.}
\label{table:analysis}
\vspace{-3ex}
\end{table}


\paragraph{Direct Inference}

For the error analysis, we examine the direct inference between two personal pronouns.\footnote{Ambiguous pronouns such as ``you'' are excluded in direct inference analysis, and included in indirect inference analysis.} 
SP/PS in Table~\ref{table:analysis} shows the numbers of links that one pronoun incorrectly selects another pronoun with different plurality as its antecedent. 
We find that adapting HOI shows slightly higher impact than switching to a more advanced encoder. 
\texttt{AA} can reinforce the pronoun representation to bias towards singularity and lead to lower SP error and higher PS error, while the difference between \texttt{BERT} and \texttt{SpanBERT} is trivial on SP/PS.

\noindent We also look at the general types of coreferent errors involving two pronouns. 
False Link (FL) falsely links a non-anaphoric pronoun to another pronoun as antecedent; Wrong Link (WL) links an anaphoric pronoun to another wrong pronoun as antecedent. Table~\ref{table:analysis} shows that \texttt{EE} and \texttt{CM} reduce FL errors by 4+\%, suggesting that the aggregation of non-local features indeed leads to more conservative linking decisions. 
However, adapting an advanced encoder shows higher impact on WL errors, as \texttt{SpanBERT} reduces almost 10\% compared to \texttt{BERT}, implying that representation learning is still more important for semantic matching.

\paragraph{Indirect Inference}

The plurality of ambiguous pronouns such as \textit{you} depends on the context. Two indirect links of (\textit{he}, \textit{you}) and (\textit{you}, \textit{they}) can be common to induce incorrect clusters that contain both singular and plural pronouns \citep{wiseman-etal-2016-learning,lee-etal-2018-higher}. Table~\ref{table:analysis} shows the numbers of these erroneous clusters in prediction. Surprisingly, very few of these clusters contain ambiguous pronouns in either approach. This observation moderates the long-standing movitation of HOI.

Additionally, the change of representation from BERT to SpanBERT has far more impact that reduces 10\% of these erroneous clusters, while the four HOI methods fail to show significant difference compared to \texttt{SpanBERT}.





 \section{Conclusion}
\label{sec:conclusion}

We implement the end-to-end coreference resolution model and investigate four higher-order inference methods, including two of our own methods. 
Our best model shows the new result of 80.2 on the CoNLL 2012 dataset. 
We thoroughly analyze the empirical effectiveness of HOI and demonstrate why it fails to boost performance on the CoNLL 2012 dataset compared to the improvement from encoders. We show that current HOI does not meet up with the original motivation, suggesting that a new perspective of HOI is needed for this task in the era of deep learning-based NLP. \section*{Acknowledgments}

We gratefully acknowledge the support of the AWS Machine Learning Research Awards (MLRA).
Any contents in this material are those of the authors and do not necessarily reflect the views of AWS. 
\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}

\cleardoublepage\appendix
\begin{table*}[hb]
\small
\centering
\begin{tabular}{>{\quad}cccccccc}
\toprule
 & \multicolumn{1}{c}{MUC} & & \multicolumn{1}{c}{B\textsuperscript{3}} & & \multicolumn{1}{c}{CEAF\textsubscript{}}\\
 \cmidrule{2-2} \cmidrule{4-4} \cmidrule{6-6}  
 & F1 & & F1 & & F1 & & Avg. F1\\
\midrule
\tt BERT & 83.7 ( 0.1) & & 75.5 ( 0.1) & & 72.6 ( 0.1) && 77.3 ( 0.1) \\
\tt SpanBERT & 85.3 ( 0.1) & & 78.4 ( 0.1) & & 75.5 ( 0.3) && 79.7 ( 0.1) \\
+ \AAA & 85.2 ( 0.2) & & 78.1 ( 0.2) & & 75.0 ( 0.2) && 79.4 ( 0.2) \\
+ \EE & 85.0 ( 0.1) & & 77.7 ( 0.2) & & 74.7 ( 0.2) && 78.9 ( 0.4) \\
+ \SC & 85.1 ( 0.2) & & 77.9 ( 0.3) & & 74.7 ( 0.3) && 79.2 ( 0.3) \\
+ \CM & \textbf{85.5} ( 0.2) & & \textbf{78.5} ( 0.3) & & \textbf{75.6} ( 0.2) && \textbf{79.9} ( 0.2) \\
\bottomrule
\end{tabular}
\caption{Results on the test set of the CoNLL'12 English shared task data. Macro-average is reported for each F1 score from 5 repeated developments of each approach. See Section~\ref{sec:exp} for the approaches.}
\label{table:results_avg}
\end{table*} 
\section{Appendices}
\label{sec:appendix}

\subsection{Experimental Settings}
\label{append:experiments}

We implement the experimented models using PyTorch. BERT\textsubscript{Large} and SpanBERT\textsubscript{Large} are used as encoders. For each experiment, the best performed model on the development set is selected and evaluated on the test set.


\paragraph{Hyperparameters and Implementation}
Similar to \citet{joshi-etal-2019-bert,spanbert-joshi}, documents are split into independent segments with maximum 384 word pieces for BERT\textsubscript{Large} and 512 for SpanBERT\textsubscript{Large}. In our final setting, BERT-parameters and task-parameters have separate learning rates ( and  respectively), separate linear decay schedule, and separate weight decay rates ( and  respectively). Models are trained 24 epochs with dropout rate 0.3.

The implementation of \texttt{EE} is based on the Tensorflow implementation from \citet{kantor-globerson-2019-coreference} which requires  memory with  being the number of extracted spans, while other HOI approaches only requires  memory \footnote{The maximum number of antecedents for all models is set to 50 which is constant.}. To keep the GPU memory usage within 32GB, we limit the maximum number of span candidates for \texttt{EE} to be 300, which may have a negative impact on the performance.

Experiments are conducted on Nvidia Tesla V100 GPUs with 32GB memory. The average training time is around 7 hours for \texttt{BERT} and \texttt{SpanBERT} without HOI, and ranges from 9 - 15 hours with HOI methods.

\subsection{Results}
\label{append:results}

Table~\ref{table:results_avg} reports the macro-average F1 scores out of 5 repeated developments of each approach. \texttt{CM} still has the best performance with 79.9 averaged F1 score. Span refinement-based HOI approaches, \texttt{AA}, \texttt{EE}, and \texttt{SC}, still have lower F1 scores than the local-only \texttt{SpanBERT}.

We do not find different configurations for \texttt{CM} make any huge impact to the performance. The final configuration for \texttt{CM} is sequential order and max reduction (Algorithm~\ref{algo:cm}).


\subsection{Analysis}
\label{append:analysis}

\begin{table}[htbp!]
\centering
\begin{tabular}{c|c}
\toprule
\AAA & -0.02 ( 0.06) \\
\EE & 0.03 ( 0.07) \\
\SC & 0.11 ( 0.10) \\
\CM & 0.04 ( 0.04) \\
\bottomrule
\end{tabular}
\caption{Performance drop on CoNLL'12 English test set after turning off the corresponding HOI in trained models.}
\label{table:impact}
\end{table}

Table~\ref{table:impact} shows the averaged performance drop and its standard deviations w.r.t Avg-F1 after turning off the corresponding HOI in trained models, to see the direct performance impact of HOI over local decisions.

\paragraph{Pronoun Resolution}

In our analysis, the following personal pronouns are regarded as ambiguous pronouns: ``you'', ``your'', ``yours''.

 
\end{document}
