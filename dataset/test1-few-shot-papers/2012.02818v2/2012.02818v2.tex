\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}   
\usepackage{xcolor}
\usepackage{color}
\usepackage[skip=0.5ex]{caption}
\usepackage{microtype}
\usepackage{booktabs}       \usepackage{multirow}
\usepackage{wrapfig}
\usepackage{stmaryrd} 
\usepackage[normalem]{ulem}
\usepackage{soul}
\usepackage{comment}
\usepackage{flushend}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphbox}
\usepackage[export]{adjustbox}
\usepackage{widetext}

\usepackage{paralist}
\usepackage{array}
\usepackage{placeins}
\usepackage{epstopdf}
\usepackage[titletoc,title]{appendix}
\usepackage[T1]{fontenc}
\usepackage{diagbox}
\newcommand{\rone}[1]{\textcolor{MidnightBlue}{#1}}
\newcommand{\rtwo}[1]{\textcolor{orange}{#1}}
\newcommand{\rthree}[1]{\textcolor{violet}{#1}}
\definecolor{myteal}{rgb}{0.25,0.5,0.5}
\newcommand{\relative}[1]{\textcolor{myteal}{#1}}

\usepackage{xr}


\makeatletter
\newcommand*{\addFileDependency}[1]{\typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{\externaldocument{#1}\addFileDependency{#1.tex}\addFileDependency{#1.aux}}

\myexternaldocument{supplementary}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{3490} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\definecolor{orange}{rgb}{0.99,0.29,0.07}
\newcommand\Sev{\textcolor{black}}
\newcommand\emi{\textcolor{black}}
 \newcommand{\emir}[1]{\textcolor{magenta}{\sout{#1}}}
\newcommand\Gianni{\textcolor{black}}
\newcommand\Isa{\textcolor{black}}
  

\newcommand{\ab}[1]{\textcolor{black}{#1}}


\newcommand{\abc}[1]{\textcolor{violet}{[AB: \em #1]}}
\newcommand{\todo}[1]{\textcolor{purple}{ToDo: #1}}



\newcommand{\centered}[1]{\begin{tabular}{@{}l@{}} #1 \end{tabular}}




\newcommand{\real}{\mathbb{R}}

\newcommand{\vomega}{\boldsymbol{\mathbf{\omega}}}
\newcommand{\vTheta}{\boldsymbol{\Theta}}
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\vphi}{\boldsymbol{\phi}}
\newcommand{\vpsi}{\boldsymbol{\psi}}
\newcommand{\vmu}{\boldsymbol{\mathbf{\mu}}}
\newcommand{\vsigma}{\boldsymbol{\mathbf{\sigma}}}
\newcommand{\vSigma}{\boldsymbol{\mathbf{\Sigma}}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vY}{\mathbf{Y}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vx}{\mathbf{x}}

\newcommand{\gdec}{g^{\scriptstyle \text{dec}}_{\scriptstyle \psi}}
\newcommand{\genc}{g^{\scriptstyle \text{enc}}_{\scriptstyle \phi}}
\newcommand{\ThetaLP}{\Theta^{\scriptscriptstyle \text{LP-BNN}}}

\newcommand{\qenc}{Q_{\scriptstyle \phi}}
\newcommand{\pdec}{P_{\scriptstyle \psi}}



\newcommand{\losslpbnn}{\mathcal{L}_{\scriptscriptstyle \text{LP-BNN}}}
\newcommand{\lossbnn}{\mathcal{L}_{\scriptscriptstyle \text{BNN}}}


\newcommand{\parag}[1]{\smallskip\noindent\textbf{#1}~~}
\newcommand{\paragnoskip}[1]{\noindent\textbf{#1}~~}



\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\norm}[1]{\left\|{#1}\right\|}


\newcommand{\noise}{\varepsilon}
\newcommand{\mcd}{MC Dropout\xspace}
\newcommand{\de}{Deep Ensembles\xspace}
\newcommand{\method}{LP-BNN\xspace}
\newcommand{\be}{BatchEnsemble\xspace}
 
\listfiles

\begin{document}


\title{Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification}


\author{
	Gianni Franchi\textsuperscript{1} \ \ \ 
	Andrei Bursuc\textsuperscript{2} \ \ \ 
	Emanuel Aldea\textsuperscript{2} \ \ \ 
	S\'{e}verine Dubuisson\textsuperscript{4} \ \ \ 
	Isabelle Bloch\textsuperscript{1} \\
	\small \textsuperscript{1} Institut Polytechnique de Paris 
	\ \ \ \ \ \textsuperscript{2}valeo.ai
	\ \ \ \ \ \textsuperscript{3}Universit\'{e} Paris-Saclay
    \ \ \ \ \ \textsuperscript{4}Aix Marseille University
}



\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Bayesian Neural Networks (BNNs) have been long considered an ideal, yet unscalable solution for improving the robustness and the predictive uncertainty of deep neural networks. While they 
could capture more accurately the posterior distribution of the network parameters, most BNN approaches are either limited to small networks or rely on constraining assumptions,
e.g., parameter independence. These drawbacks have enabled prominence of simple, but computationally heavy approaches such as Deep Ensembles, whose training and testing costs increase linearly with the number of networks. In this work we aim for efficient deep BNNs amenable to complex computer vision architectures, e.g., ResNet50 DeepLabV3+, and tasks, e.g., semantic segmentation, with fewer assumptions on the parameters. We achieve this by leveraging variational autoencoders (VAEs) to learn the interaction and the latent distribution of the parameters at each network layer. Our approach, Latent-Posterior BNN (LP-BNN), is compatible with the recent BatchEnsemble method, leading to highly efficient ({in terms of computation and} memory during both training and testing) ensembles. LP-BNNs attain competitive results across multiple metrics in several challenging benchmarks for image classification, semantic segmentation and out-of-distribution detection.
\end{abstract}

\section{Introduction}



\begin{figure}[t!]
  \renewcommand{\figurename}{Figure}
\renewcommand{\captionfont}{\small}
  \centering
\includegraphics[width=0.95\linewidth]{images/bnn-overview.png}
  \caption{
In a standard NN each weight has a fixed value. In 
\ab{most} BNN\ab{s}
all weights follow Gaussian distributions and are assumed to be mutually independent: each weight is factorized by a Gaussian distribution. \ab{For \method  in each layer, weights follow a multivariate Gaussian distribution with a latent weight space composed of independent Gaussian distributions.} This enables computing expressive weight distributions in a lower dimensional space.
}
  \label{fig:teaser}
\vspace{-4mm}
\end{figure}



\ab{
Most top-performing approaches for predictive uncertainty estimation with Deep Neural Networks (DNNs)~\cite{lakshminarayanan2017simple, ashukha2020pitfalls,maddox2019simple,franchi2019tradi} are essentially based on ensembles, in particular Deep Ensembles (DE)~\cite{lakshminarayanan2017simple}, which have been shown to display many strengths: stability, mode diversity, good calibration, \etc.~\cite{fort2019deep}. In addition, through the Bayesian lens, ensembles enable a more straightforward separation and quantification of the sources and forms of uncertainty~\cite{gal2016phd, lakshminarayanan2017simple, malinin2018predictive}, which in turn allows a better communication of the decisions to humans~\cite{bhatt2020uncertainty, kompa2021second} or to connected modules in an autonomous system~\cite{mcallister2017concrete}. This is crucial for real-world decision making systems. Although originally introduced as simple and scalable alternative to Bayesian Neural Networks (BNNs)~\cite{mackay1992practical, neal1995bayesian}, DE still have notable drawbacks in terms of computational cost for both training and testing often make them prohibitive in practical applications. 
}


\ab{In this work we address uncertainty estimation with BNNs, the departure point of DE. BNNs propose an intuitive and elegant formalism suited for this task by estimating the posterior distribution over the parameters of a network conditioned on training data. Performing exact inference BNNs is intractable and most approaches require approximations. The most common one is the mean-field assumption~\cite{jordan1999introduction}, i.e., the weights are assumed to be independent of each other and factorized by their own distribution, usually Gaussian~\cite{hinton1993keeping, graves2011practical, blundell2015weight, hernandez2015probabilistic, gal2016dropout, mishkin2018slang}.
However, this approximation can be damaging~\cite{mackay1992practical, foong2020expressiveness} as a more complex organization can emerge within network layers, and that higher level correlations contribute to better performance and generalization~\cite{bengio2017deep,salimans2016weight,srivastava2014dropout}. Yet, even under such settings, BNNs are challenging to train at scale on modern DNN architectures~\cite{ovadia2019can, dusenberry2020efficient}.
In response, researchers have looked into structured-covariance approximations~\cite{louizos2016structured, sun2017learning, zhang2018noisy, mishkin2018slang}, however they further increase memory and time complexity over the original mean-field approximation.
}

\ab{Here, we revisit BNNs in a pragmatic manner. We propose an approach to estimate the posterior of a BNN with layer-level inter-weight correlations, in a stable and computationally efficient manner, compatible with modern DNNs and complex computer vision tasks, e.g., semantic segmentation. 
We advance a novel deep BNN model, dubbed \emph{Latent Posterior BNN} (LP-BNN), where the posterior distribution of the weights at each
layer is encoded with a variational autoencoder (VAE)~\cite{kingma13vae} into a lower-dimensional latent space that follows a Gaussian distribution (see Figure~\ref{fig:teaser}). We switch from the inference of the posterior in the high dimensional space of the network weights to a lower dimensional space which is easier to learn and already encapsulates weight interaction information. LP-BNN is naturally compatible with the recent BatchEnsemble (BE) approach~\cite{wen2020batchensemble} that enables learning a more diverse posterior from the weights of the BE sub-networks. Their combination outperforms most of related approaches across a breadth of benchmarks and metrics. In particular, \method is competitive with DE and has significantly lower costs for training and prediction.
}






















\parag{Contributions.} \ab{To summarize, the contributions of our
work are: \textbf{(1)} We introduce a scalable approach for BNNs to implicitly capture \emph{layer-level weight correlations} enabling more expressive posterior approximations, by foregoing the limiting mean-field assumption 
LP-BNN scales to high capacity DNNs (e.g., 50+ layers and 30M parameters for DeepLabv3+), while still training on a single V100 GPU. \textbf{(2)} We propose to leverage VAEs for computing the posterior distribution of the weights by projecting them in the latent space. This improves significantly training stability while ensuring diversity of the sampled weights. \textbf{(3)} We extensively evaluate our method on a range of computer vision tasks and settings: image classification for \emph{in-domain uncertainty},
\emph{out-of-distribution (OOD) detection}, 
\emph{robustness to distribution shift}, 
and semantic segmentation ( high-resolution images, strong class imbalance) for \emph{OOD detection}.
We demonstrate that LP-BNN achieves similar performances with high-performing Deep Ensembles, while being substantially more efficient computationally.  } \section{Background}\label{section:background}
In this section, we present the chosen formalism for this work and \ab{offer} a short background on BNNs.

\subsection{Preliminaries}
We consider a training dataset  with  samples and labels, corresponding to two random variables  and . 
Without loss of generality we represent  as a vector, and 
as a scalar label. We process the input data  with a neural network  with parameters , that outputs a classification or regression prediction. We view the neural network as a probabilistic model with . In the following, when there are no ambiguities, we discard the random variable from notations. For classification,  is a categorical distribution over the set of classes over {the domain of} , typically corresponding to the cross-entropy loss function, while for regression   is a Gaussian distribution of real values  over {the domain of}  when using the squared loss function.
For simplicity 
we unroll our reasoning for the classification task.


In supervised learning, we leverage gradient descent for learning  that minimizes the cross-entropy loss, which is equivalent to finding the parameters that maximize the likelihood estimation (MLE)  over the training set , or equivalently minimize the following loss function: 



The Bayesian approach enables adding prior information on the parameters , by placing a prior distribution  upon them. This prior represents some expert knowledge about the dataset and the model. Instead of maximizing the likelihood, we can now find the maximum a posteriori (MAP) weights for 
to compute , \ie to minimize the following loss function:

inducing a specific distribution over the functions computed by the network and a regularization of the weights. For a Gaussian prior, Eq.~\eqref{eq:loss_MAP} reads as  regularization (weight decay).


\subsection{Bayesian Neural Networks}
In 
most neural networks only the  weights computed during training are kept for predictions. Conversely, in BNNs we aim to find the posterior distribution  of the parameters given the training dataset, not only the values corresponding to the MAP. Here we can make {a prediction } on a new sample  by computing the expectation of the predictions from an infinite ensemble corresponding to different configurations of the weights sampled from the posterior distribution:

which is also known as Bayes ensemble. The integral in Eq.~\eqref{eq:marginalization}, {which is calculated over the domain of }, is intractable,  and in practice it is approximated by averaging predictions from a limited set  of  weight configurations sampled from the posterior distribution:



Although BNNs are elegant and easy to formulate, their inference is non-trivial and has been subject to extensive research across the years~\cite{hinton1993keeping,mackay1992practical,neal1995bayesian}. Early approaches relied on Markov chain Monte Carlo variants for inference, while progress in variational inference (VI)~\cite{jordan1999introduction} has enabled a recent revival of BNNs~\cite{graves2011practical, blundell2015weight, hernandez2015probabilistic}. VI turns posterior inference into an optimization problem. In detail, VI finds the parameters  of a distribution  on the weights that approximates the true Bayesian posterior distribution of the weights  through KL-divergence minimization. This is equivalent to minimizing the following loss function, also known as expected lower bound (ELBO) loss~\cite{blundell2015weight, kingma13vae}:

The loss function  is composed of two terms: the KL term depends on the weights and the prior , while the likelihood term is data dependent. This function strives to simultaneously capture faithfully the complexity and diversity of the information from data , while 
preserving the simplicity of the prior . To optimize this loss function, Blundell \etal~\cite{blundell2015weight} proposed leveraging the \emph{re-parameterization trick}~\cite{kingma13vae,rezende2014stochastic}, foregoing the expensive MC estimates.

\paragraph{Discussion.} BNNs are particularly appealing for uncertainty quantification thanks to the ensemble of predictions from multiple weight configurations sampled from the posterior distribution. However this brings an increased computational and memory cost. For instance, the simplest variant of BNNs with fully factorized Gaussian approximation distributions~\cite{blundell2015weight,graves2011practical}, 
{\ie each weight consists of a Gaussian mean and variance, carries a double amount of parameters.} In addition, 
\ab{recent works~\cite{ ovadia2019can, dusenberry2020efficient}} point out that BNNs often underfit, and need multiple tunings to stabilize training dynamics involved by the loss function and the variance from weight samplings at each forward pass. 
Due to computational limitations, most BNN approaches assume that parameters are not correlated. This hinders their effectiveness~\cite{foong2020expressiveness}, as empirical evidence has shown that encouraging weight collaboration improves training stability and generalization~\cite{qiao2019weight, salimans2016weight,srivastava2014dropout}. 

In order to calculate a tractable weight correlation aware posterior distribution, we propose to leverage a VAE to compute compressed latent distributions {from which we can sample new weight configurations.}
We rely on the recent BatchEnsemble (BE) method~\cite{wen2020batchensemble} to further improve the parameter-efficiency of BNNs. We now proceed to describe BE and then derive our approach.












\begin{figure}[!t]
\renewcommand{\captionfont}{\small}
\centering
\includegraphics[width=0.70\linewidth]{images/be-layer-v2.png}
\caption{\ab{\textbf{Diagram of a BatchEnsemble layer} that generates for an ensemble of size , the ensemble weights  from shared weights  and fast weights , with }.}
\label{fig:be-layer}
\end{figure}

\subsection{BatchEnsemble}

Deep Ensembles (DEs)~\cite{lakshminarayanan2017simple} {are a popular and pragmatic alternative to BNNs. While DEs} boast outstanding accuracy and predictive uncertainty, 
their training and testing cost increases linearly with the number of networks. This drawback has motivated the emergence of a recent stream of works proposing efficient ensemble methods~\cite{ashukha2020pitfalls, franchi2019tradi, maddox2019simple, mehrtash2020pep, wen2020batchensemble}. One of the most promising ones is \be~\cite{wen2020batchensemble}, which mimics in a parameter-efficient manner one of the main strengths of DE, \ie diverse  predictions~\cite{fort2019deep}. 



In a nutshell, BE builds up an ensemble from a single base network (shared among ensemble members) and a set of layer-wise weight matrices specific to each member. At each layer, the weight of each ensemble member is generated from the Hadamard product between a weight shared among all ensemble members, called \emph{``slow weight''}, and a Rank-1 matrix that varies among all members, called \emph{``fast weight''}. Formally, let  be the slow weights in a neural network layer with input dimension  and with  outputs. Each member  from an ensemble of size  owns a fast weight matrix .  is a Rank-1 matrix computed from a tuple of trainable vectors  and , with . BE generates from them a family of ensemble weights as follows: , where  is the Hadamard product. Each  member of the ensemble is essentially a Rank-1 perturbation of the shared weights  (see Figure~\ref{fig:be-layer}).
The sequence of operations during the forward pass reads:

where  is an activation function and  the output activations. 

The operations in BE can be efficiently vectorized, enabling each member to process in parallel the corresponding subset of samples from the mini-batch.  is trained in a standard manner over all samples in the mini-batch. A BE network  is parameterized by an extended set of parameters 
.


With its multiple sub-networks parameterized by a reduced set of weights, BE is a practical method that can potentially improve the scalability of BNNs. We take advantage of the small size of the fast weights to capture efficiently the interactions between units and to compute a latent distribution of the weights. We detail our approach below.



 
\section{Efficient Bayesian Neural Networks (BNNs) }\label{section:approach}




\subsection{Encoding the posterior weight distribution of a BNN}

Most BNN variants assume full independence between weights, both inter- and intra-layer. Modeling precisely weight correlations in modern high capacity DNNs with thousands to millions of parameters per layer~\cite{he2016deep} is however a daunting endeavor due to computational intractability. Yet, multiple strategies aiming to boost weight collaboration in one way or another, \eg Dropout~\cite{srivastava2014dropout}, WeightNorm~\cite{salimans2016weight}, Weight Standardization~\cite{qiao2019weight}, have proven to improve training speed, stability and generalization. Ignoring weight correlations might partially explain the shortcomings of BNNs in terms of underfitting~\cite{ovadia2019can, dusenberry2020efficient}. This motivates us to find a scalable way to compute the posterior distribution of the weights without discarding their correlations.


Li \etal~\cite{li2018intrinsic} have recently found that the \emph{intrinsic} dimension of DNNs can be in the order of hundreds to a few thousands. The good performances of BE, that builds on weights from a low-rank subspace, further confirm this finding. For efficiency, we leverage the Rank-1 subspace decomposition in BE and estimate here the distribution of the weights, leading to a novel form of BNNs. Formally, instead of computing the posterior distribution , we aim now for .

A first approach would be to compute Rank-1 weight distributions by using  and  as variational layers, place priors on them and compute their posterior distributions in a similar manner to~\cite{blundell2015weight}. Dusenberry \etal~\cite{dusenberry2020efficient} show that these Rank-1 BNNs stabilize training by reducing the variance of the sampled weights, due to sampling only from Rank-1 variational distributions instead of full weight matrices. However this raises the memory cost significantly, as 
\ab{training} is performed simultaneously over all  sub-networks: on CIFAR-10 for ResNet-50 with , the authors use  TPUv2 cores with mini-batches of size  per core.



We argue that a more efficient way of computing the posterior distribution of the fast weights would be to 
\ab{learn} instead the posterior distribution of the lower dimensional latent variables of . This can be efficiently done with a VAE~\cite{kingma13vae} that can find a variational approximation  to the intractable posterior distribution .  VAEs can be seen as a generative model that can deal with complicated dependencies between input dimensions via a probabilistic encoder that projects the input into a latent space following a specific prior distribution. For simplicity and clarity, from here onward we derive our formalism only for  at a single layer and consider weights  to be deterministic. Here the input to the VAE are the weights  and we rely on it  to learn the dependencies between weights and encode them into the latent representation.



In detail, for each layer of the network  we introduce a VAE composed of a one layer encoder 
 with variational parameters  and a one layer decoder 
 with parameters . Let the prior over the latent variables be a centered isotropic Gaussian distribution . Like common practice, we let the variational approximate posterior distribution  be a multivariate Gaussian with diagonal covariance. The encoder takes as input a mini-batch of size  (the size of the ensemble) composed of all the  weights of this layer and outputs as activations . We sample a latent variable  and feed it to the decoder, which in turn outputs the reconstructed weights 
. In other words, at each forward pass, we sample new fast weights  from the latent posterior distribution to be further used for generating the ensemble. The weights of each member of the ensemble  are now random variables depending on ,  and .
Note that while in practice we sample  weight configurations, this approach allows us to generate larger ensembles by sampling multiple times from the same latent {distribution}. 
We illustrate an overview of an~\method~ layer in Figure~\ref{fig:LPBNN}.


The VAE modules are trained in the standard manner with the ELBO loss function~\cite{kingma13vae} jointly with the rest of the network. The final loss function is:

where  \Gianni{and  the number of layers}. The loss function is applied to all  members of the ensemble. 



At a first glance, the loss function  bears some similarities with  (Eq.~\ref{eq:loss-BNN}). Both functions include likelihood and KL terms. The likelihood in , \ie the cross-entropy loss, depends on input data  and on the parameters  sampled from , while  {depends} on the latent variables  sampled from  that lead to the fast weights . It guides the weights towards useful values for the main task. The KL term in  enforces the per-weight prior, while in  it preserves the consistency and simplicity of the common latent distribution of the weights . In addition,  has an input weight reconstruction loss (last term in Eq.~\ref{eq:loss-lpbnn}) ensuring that the generated weights  are still compatible with the rest of parameters of the network and do not cause high variance and instabilities during training, as typically occurs in standard BNNs~\cite{dusenberry2020efficient}.


At test time, we generate the LP-BNN ensemble on the fly by sampling the weights  from the encodings of  to compute . For the final prediction we compute the empirical mean of the likelihoods of the ensemble:





\begin{figure}[!t]
\renewcommand{\captionfont}{\small}
\centering
 \includegraphics[width=\linewidth]{images/lp-bnn-layer-v3.png}
\caption{\ab{\textbf{Diagram of a \method layer} that generates for an ensemble of size , ensemble weights  from shared weights  and fast weights  and , the latter sampled and decoded from the corresponding latent projection  of , with .}} 
         
\label{fig:LPBNN}
\end{figure}

\begin{comment}


\subsection{The utility of Rank-1 Priors }

\ab{One could ask why using the Rank-1 formalism, instead of simply feeding the weights a layer to the VAE to infer the latent distribution. Rank-1 prior reduce significantly the number of weights to train the VAE over, due to the decomposition of the fast weights into  and . This further allows us to consider multiple such weights, , enabling faster training of the VAE as its training samples are more numerous and more diverse.} 

Next, we establish connections between the cardinality  of the ensemble and the posterior covariance matrix.
{Our prior distribution allows for the introduction of correlations between weights, which is a desirable property due to its superior expressiveness~\cite{dusenberry2020efficient} but which can be otherwise difficult to approximate.} Also, the covariance matrix of our prior is a Rank-1 matrix. Thanks to the Eckart-Young theorem \cite{wang2012geometric} (Theorem 5.1), we can quantify the error of approximating the covariance by a Rank-1 matrix, based on the second up to the last singular values.


Let us denote by  the  weights trained by our algorithm,    and . The differences and the sum in the previous equations are calculated element-wise on all the weights of the DNNs. 
Then, for each new data sample {}, the prediction of the DNN   is equivalent to the average of the DNNs   applied on   :



with  where the  is computed over all weights.
The proof can {be found} in Section 3.5 of \cite{izmailov2018averaging}.
It follows that in fact we do not learn a Rank-1, but an up to Rank- covariance matrix, if all the   are independent. 
Hence the choice of  acts as an approximation factor of the covariance matrix. 
\ab{Wen \etal\cite{wen2020batchensemble} tested 
different values of  and found} that  was the best compromise, which we will also use \ab{here}.


 
\subsection{Computational complexity}



Recent efforts~\cite{fort2019deep,wilson2020bayesian} 
studied the weight modes 
computed by Deep Ensembles within BNNs, yet this line of research is computationally intractable at the scale required for \ab{practical} computer vision tasks. Dusenberry \etal~\cite{dusenberry2020efficient} propose 
a \ab{more} scalable solution for image classification, which is nonetheless prone to high instabilities due to the important number of parameters and to the fact that  and  are the latent variables of the variational distribution. In comparison, our 
approach requires less memory resources since we encode  in a lower dimensional space (we found empirically that a latent space of size \ab{only}  provides \ab{an appealing} compromise between accuracy and compactness). The only additional cost in terms of parameters and memory used 
\ab{w.r.t.} BE is related to the compact VAEs associated with each layer. 



\ab{Besides the lower number of parameters, LP-BNN training is more stable than for Rank-1 BNN due to the reconstruction term   which regularizes the 
loss in Eq.~\eqref{eq:loss-lpbnn} by controlling the variances of the sampled weights. BNNs usually need a range of heuristics, e.g. clipping, initialization from truncated Normal, extra weight regularization to stabilize training~\cite{dusenberry2020efficient}. For \method~training is overall straightforward even on complex and deep models, \eg DeepLabV3+, thanks to the VAE module that is stable.
}
\end{comment}

\subsection{Discussion on \method}



\ab{We discuss here the quality of the uncertainty from \method. The predictive uncertainty of a DNN stems from two main types of uncertainty~\cite{hora1996aleatory}: \emph{aleatoric uncertainty} and \emph{epistemic uncertainty}.}
The former is related to randomness, typically due to the noise in the data. The latter 
concerns 
\ab{finite size training datasets.}
\ab{The epistemic uncertainty captures the uncertainty in the DNN parameters and their lack of knowledge on the model that generated the training data.}





In BNN approaches, through likelihood marginalization over weights, the prediction is computed by integrating the outputs from different DNNs weighted by the posterior distribution \ab{(Eq.~\ref{eq:marginalization}), allowing us to conveniently capture both types of uncertainties~\cite{malinin2018predictive}.} 
\ab{The quality of the uncertainty estimates depends on the diversity of predictions and views provided by the BNN.}
\ab{DE~\cite{lakshminarayanan2017simple} achieve excellent diversity~\cite{fort2019deep} by mimicking BNN ensembles through training of multiple individual models.} 
Recently, Wilson \ab{and Izmailov}~\cite{wilson2020bayesian} proposed to 
\ab{combine DE}
and \ab{BNNs} towards improving diversity \ab{further}. However, as 
\ab{DE are already computationally demanding}, we argue \ab{that} BE is a more 
pragmatic choice for increasing \ab{the} diversity of our BNN\ab{, leading to better uncertainty quantification}.

Figure~\ref{fig:diversityall} shows 
\ab{a qualitative comparison of the prediction diversity from different methods.}
\ab{We compare \method, BE, and DE based on WRN-28-10~\cite{zagoruyko2016wide} trained on CIFAR-10~\cite{krizhevsky2009learning} and analyze predictions on CIFAR-10, CIFAR10-C~\cite{hendrycks2018benchmarking}, and SVHN~\cite{Netzer2011} test images. SVHN contains digits which have a different distribution from the training data, i.e., predominant epistemic uncertainty, while CIFAR10-C displays a distribution shift via noise corruption, i.e., more aleatoric uncertainty.
The expected behavior is that individual DNNs in an ensemble would predict different classes for OOD images and have higher entropy on the corrupted ones, reducing the confidence score of the ensemble.}
We can see that the diversity of BE is 
\ab{lower} for CIFAR10-C and SVHN, leading to \ab{poorer} results in Table~\ref{table:outofditribution}. 

\ab{In the supplementary we include additional discussions on our posterior covariance matrix ({\S}A.1), the link between the size of the ensemble and the covariance matrix approximation ({\S}A.2), computational complexity ({\S}A.3), training stability of \method ({\S}A.4), and diversity of \method ({\S}A.5).}


\begin{figure}[t!]
\renewcommand{\figurename}{Figure}
\renewcommand{\captionfont}{\small}
     \centering
        \begin{subfigure}[b]{0.16\linewidth}
        \caption*{{\textbf{Input}}}
        \includegraphics[width=\textwidth]{images/ID_batch_ensemble_1.jpg}
\end{subfigure}\;
        \begin{subfigure}[b]{0.22\linewidth}
        \caption*{{\textbf{LP-BNN}}}
        \includegraphics[width=\textwidth]{images/ID_VAEBNN_1_histo.pdf}
\end{subfigure}\;
        \begin{subfigure}[b]{0.22\linewidth}
        \caption*{{\textbf{BE}}}
        \includegraphics[width=\textwidth]{images/ID_batch_ensemble_1_histo.pdf}
\end{subfigure}\;
         \begin{subfigure}[b]{0.22\linewidth}
        \caption*{{\textbf{DE }}}
        \includegraphics[width=\textwidth]{images/ID_DE_1_histo.pdf}
\end{subfigure}\;


        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/ID_corrupt_batch_ensemble_1.jpg}
\end{subfigure}\;
        \begin{subfigure}[b]{0.22\linewidth}
\includegraphics[width=\textwidth]{images/ID_corrupt_VAEBNN_1_histo.pdf}
\end{subfigure}\;
        \begin{subfigure}[b]{0.22\linewidth}
\includegraphics[width=\textwidth]{images/ID_corrupt_batch_ensemble_1_histo.pdf}
\end{subfigure}\;
         \begin{subfigure}[b]{0.22\linewidth}
\includegraphics[width=\textwidth]{images/ID_corrupt_DE_1_histo.pdf}
\end{subfigure}\;

        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/OOD_batch_ensemble_4.jpg}
\end{subfigure}\;
        \begin{subfigure}[b]{0.22\linewidth}
\includegraphics[width=\textwidth]{images/OOD_VAEBNN_4_histo.pdf}
\end{subfigure}\;
        \begin{subfigure}[b]{0.22\linewidth}
\includegraphics[width=\textwidth]{images/OOD_batch_ensemble_4_histo.pdf}
\end{subfigure}\;
         \begin{subfigure}[b]{0.22\linewidth}
\includegraphics[width=\textwidth]{images/OOD_DE_4_histo.pdf}
\end{subfigure}\;
\vspace{2mm}      
\caption{\ab{\textbf{Diversity of predictions of different ensemble methods.} \emph{Col.} : in top-down order images from the test set of CIFAR-10,  CIFAR-10-C, and SVHN; \emph{Col.} : outputs of different sub-models of the three ensemble techniques:
\method, BE~\cite{wen2020batchensemble}, and DE~\cite{lakshminarayanan2017simple}. For all methods we set the number of models to . }}
\label{fig:diversityall}
\end{figure}




 \section{Related work}\label{section:related}



\parag{Bayesian Deep Learning.} \ab{Bayesian approaches and neural networks have a long joint history~\cite{mackay1992bayesian,mackay1992practical, neal1995bayesian}. Early approaches relied on Markov chain Monte
Carlo variants for inference on BNNs, which was later replaced by variational inference (VI)~\cite{jordan1999introduction} in the context of deeper networks. Most of the modern approaches make use of VI with the mean-field approximation~\cite{hinton1993keeping, graves2011practical, blundell2015weight, hernandez2015probabilistic, gal2016dropout, mishkin2018slang} which conveniently makes posterior inference tractable. However this limits the expressivity of the posterior~\cite{mackay1992practical, foong2020expressiveness}. 
This drawback became subject of multiple attempts for structured-covariance approximations using matrix variate Gaussian priors~\cite{louizos2016structured, sun2017learning} or natural gradients~\cite{zhang2018noisy, mishkin2018slang}. However they further increase memory and time complexity over the original mean-field approximation. Recent methods proposed more simplistic BNNs by performing inference with structured priors only over the first and last layer~\cite{pearce2020structured} or just the last layer~\cite{riquelme2018deep, ovadia2019can}. Full covariance can be computed for shallow networks thanks to a meta-prior in a low-dimensional space where the VI can be performed~\cite{karaletsos2018probabilistic, karaletsos2020hierarchical}. 
Most BNNs are still challenging to train, underfit and are difficult to scale to big DNNs~\cite{ovadia2019can, dusenberry2020efficient}, while the issue of finding a proper prior is still open~\cite{wenzel2020good, fortuin2021bayesian}. Our approach builds upon the low dimensional fast weights from BE and on the stability of the VAEs, foregoing many of the shortcomings of BNN training.}






\parag{Ensemble Approaches.}
\ab{Ensembles 
mimick and attain, to some extent, properties of BNNs~\cite{fort2019deep}. Deep Ensembles~\cite{lakshminarayanan2017simple} train multiple DNNs with different random initializations leading to excellent uncertainty quantification scores. The major inherent drawback in terms of computational and memory overhead has been subsequently addressed through multi-head networks~\cite{lee2015m}, snapshot-ensembles from intermediate training checkpoints~\cite{huang2017snapshot, garipov2018loss}, efficient computation of the posterior distribution from weight trajectories during training~\cite{maddox2019simple,franchi2019tradi}, use of multiple Dropout masks at test time~\cite{gal2016dropout}, multiple random perturbations to the weights of a pre-trained network~\cite{franchi2019tradi, mehrtash2020pep, atanov2018uncertainty}, multiple perturbation of the input image~\cite{ashukha2020pitfalls}, multiple low-rank weights tied to a backbone network~\cite{wen2020batchensemble}, simultaneous processing of multiple images by the same DNN~\cite{havasi2020training}. Most approaches still have a significant computational overhead for training or for prediction, while struggling with diversity~\cite{fort2019deep}.}

\parag{Dirichlet Networks (DNs).} 
\ab{DNs~\cite{malinin2018predictive,malinin2019reverse,sensoy2018evidential, charpentier2020posterior,tsiligkaridis2021information} bring a promising line of approaches that estimate uncertainty from a single network by parameterizing a Dirichlet distribution over its predictions. However, most of these methods~\cite{malinin2018predictive, malinin2019reverse} use OOD samples during training, which may be unrealistic in many applications~\cite{charpentier2020posterior}, or do not scale to bigger DNNs~\cite{joo2020being}.DNs have been developed only for classification tasks and extending them to regression requires further adjustments~\cite{malinin2020regression}, unlike \method that can be equally used for classification and regression.}







\begin{comment}


\abc{Plan:
\begin{itemize}
    \item BNNs from old to new
    \item Ensembles and pseudo-ensembles ( swag+swa+temporal per gaussian pertubation +tradi + ovnni+)
\item Other works using Bayesian ideas for uncertainty: DUQ, Prior Networks, last-layer approaches, etc.
\end{itemize}
Take away message: BNNs are nice but rather impractical. There are several approaches for uncertainty quantification but Deep Ensembles are still on top since they have very good predictive uncertainty and diversity. We go for more scalable BNNs that would enable performance close do Deep Ensembles but with better parameter-efficiency.
} 
\end{comment} 

\section{Experiments and results}\label{section:experiments}


\subsection{Implementation details}
We {evaluate} the performance of LP-BNN in assessing  {the uncertainty of its predictions.} 
For our benchmark, we evaluate \method on different scenarios against several 
\ab{strong} baselines with different advantages in terms of performance, training or runtime: \ab{BE~\cite{wen2020batchensemble}, DE~\cite{lakshminarayanan2017simple}, Maximum Class Probability (MCP)~\cite{hendrycks2016baseline},} MC Dropout~\cite{gal2016dropout}, TRADI~ \cite{franchi2019tradi}, \Gianni{EDL \cite{sensoy2018evidential}, DUQ \cite{van2020uncertainty}, and MIMO \cite{havasi2020training}.} 





First, we evaluate the predictive performance in terms of accuracy for image classification and mIoU~\cite{everingham2015pascal} for semantic segmentation, respectively.
Secondly, we evaluate the quality of the confidence scores provided by the DNNs by means of Expected Calibration Error (ECE)~\cite{guo2017calibration}. For ECE we use -bin histograms of confidence scores and accuracy, and compute the average of  bin-to-bin differences between the two histograms. Similarly to~\cite{guo2017calibration} we set . 
\ab{To evaluate the robustness to dataset shift via corrupted images, }
we first train the DNNs on CIFAR-10~\cite{krizhevsky2009learning} or CIFAR-100~\cite{krizhevsky2009learning} and then test on the corrupted versions of these datasets~\cite{hendrycks2018benchmarking}. The corruptions include different types of noise, 
blurring, and some other transformations that alter the quality of the images. 
For this scenario, similarly to~\cite{wen2020improving}, we use as evaluation measures the Corrupted Accuracy (cA) and Corrupted Expected Calibration Error (cE), {that offer} a better understanding of the behavior of our DNN when facing \ab{shift of data distribution and} aleatoric uncertainty. 


In order to evaluate the epistemic uncertainty, we propose to assess the {OOD} {detection} performance. This scenario typically consists in training a DNN over a dataset following a given distribution, and testing it on data coming from this distribution and data from another distribution \ab{, not seen during training}. We 
\ab{quantify} the confidence of the DNN predictions in this setting 
\ab{through their} prediction scores, i.e., output softmax values. We use the same indicators of the accuracy of detecting OOD data as in~\cite{hendrycks2016baseline}: AUC, AUPR, and the FPR-95\%-TPR.
{These indicators measure 
whether the DNN model lacks knowledge regarding some specific data and how reliable are its predictions.} 

\begin{table*}[!t]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
\begin{center}
\scalebox{0.65}
{
\begin{tabular}{l|ccccccc|cccc}
\toprule
 &   \multicolumn{7}{c|}{CIFAR-10}   &  \multicolumn{4}{c}{CIFAR-100}        \\ 
Method                  & Acc    & AUC   & AUPR    & FPR-95-TPR    & ECE     & cA    & cE       &  Acc    & ECE      & cA    & cE      \\ 
\midrule
MCP + cutout~\cite{hendrycks2016baseline}            & 96.33 & 0.9600 & 0.9767 & 0.115 &0.0207 & 32.98 & 0.6167  & 80.19 & 0.1228 & 19.33 & 0.7844 \\ 
\midrule
MC dropout~\cite{gal2016dropout}              & 95.95 & 0.9126 & 0.9511 & 0.282 & 0.0172 & 32.32 & 0.6673  & 75.40 & 0.0694 & 19.33 & 0.5830 \\ 
\midrule
MC dropout +cutout~\cite{gal2016dropout}     & 96.50 & 0.9273 & 0.9603 & 0.242 & 0.0117 & 32.35 & 0.6403  & 77.92 & 0.0672 & 27.66 & 0.5909 \\ 
\midrule
DUQ~\cite{van2020uncertainty}  & 87.48 & 0.7083 & 0.8114 & 0.698 & 0.3983 & 64.89 & 0.2542  & - & - & - & - \\
\midrule 
DUQ Resnet18~\cite{van2020uncertainty}  & 93.36 & 0.8994 & 0.9213 & 0.1964& 0.0131 & 69.01 & 0.5059  & - & - & - & - \\ 
\midrule 
EDL~\cite{sensoy2018evidential}  & 85.73 & 0.9002 & 0.9198 & 0.247 & 0.0904 & 59.54 & 0.3412  & - & - & - & - \\ 
\midrule
MIMO~\cite{havasi2020training} & 94.96 & 0.9387 & 0.9648 & 0.175 & 0.0300 & \textbf{69.99} & 0.1846  & 0.7869 & 0.1018 & 0.4735 & 0.2832 \\ 
\midrule
Deep Ensembles + cutout~\cite{lakshminarayanan2017simple} & \textbf{96.74} & \textbf{0.9803} & \textbf{0.9896} & \textbf{0.071} & \textbf{0.0093} & 68.75 & 0.1414  & \textbf{83.01} & \textbf{0.0673} & 47.35 & \textbf{0.2023} \\ 
\midrule
BatchEnsembles  + cutout~\cite{wen2020batchensemble}   & 96.48 & 0.9540 & 0.9731 & 0.132 & 0.0167 & \textbf{71.67} & 0.1928  & 81.27 & 0.0912 & 47.44 & 0.2909 \\ \midrule
LP-BNN (ours) + cutout        & 95.02 & \textbf{0.9691} & \textbf{0.9836} & \textbf{0.103} & \textbf{0.0094} & \textbf{69.51} &\textbf{ 0.1197}  & 79.3 & 0.0702 & \textbf{48.40} & \textbf{0.2224} \\ 
\bottomrule
\end{tabular}
} \end{center}
\vspace{-2mm}
\caption{\ab{\textbf{Comparative results for image classification tasks}. We evaluate on CIFAR-10 and CIFAR-100 for the tasks: in-domain classification, out-of-distribution detection with SVHN (CIFAR-10 only), robustness to distribution shift (CIFAR-10-C, CIFAR-100-C)}. We run all methods ourselves in similar settings using publicly available code for related methods. Results are averaged over three seeds. \ab{: We did not manage to scale these methods to WRN-28-10 on CIFAR-100. A similar finding for EDL was reported in ~\cite{joo2020being}.  DUQ does not scale on CIFAR-100 and it does not perfectly scale to WRN-28-10 on CIFAR-10 so we train it with Resnet 18 \cite{he2016deep} architecture like in the original paper. }}
\label{table:tab1}
\vspace{-4mm}
\end{table*}




\begin{figure}[!t]
\renewcommand{\captionfont}{\small}
\centering
 \includegraphics[width=0.80\linewidth]{images/ECE_histo.pdf}
\caption{ 
\ab{\textbf{Calibration at different levels of corruption.} We report ECE scores for \method, BE~\cite{wen2020batchensemble}, and DE~\cite{lakshminarayanan2017simple} on CIFAR-10-C.}
}
\label{fig:ECELBNN}
\end{figure}


        

\subsection{Image classification with CIFAR-10/100~\cite{krizhevsky2009learning}}\label{subsection:Experiments:cifar}
\noindent\textbf{Protocol.}
Here we train on CIFAR-10~\cite{krizhevsky2009learning} composed of 10 classes. For  CIFAR-10 we consider as OOD the SVHN dataset~\cite{Netzer2011}. Since SVHN is a color image dataset of digits, it guarantees that the OOD data comes from a distribution different from those of CIFAR-10.
We use 
\ab{WRN-28-10~\cite{zagoruyko2016wide}} for all methods, a popular architecture for this dataset, and evaluate on CIFAR-10-C~\cite{hendrycks2018benchmarking}. 
For CIFAR-100~\cite{krizhevsky2009learning} we use again 
\ab{WRN-28-10}
and 
\ab{evaluate} on the test sets of CIFAR-100 and 
CIFAR-100-C~\cite{hendrycks2018benchmarking}.
Note that for all DNNs, even for DE, 
\ab{we average results over three random seeds} for statistical relevance. We use cutout~\cite{devries2017improved} as data augmentation, as commonly used for these datasets. {Please find in the supplementary the hyperparameters for this experiment.}


\noindent\textbf{Discussion.} We illustrate results for this experiment in Table~\ref{table:outofditribution}. We notice that DE with cutout outperforms other methods on most of the metrics except ECE, cA, and cE on CIFAR-10, and cA on CIFAR-100, where \method~ achieves state of the art results. This means that \method~ is competitive for aleatoric uncertainty estimation. In fact, ECE is calculated on the test set of CIFAR-10 and CIFAR-100, so it mostly measures the reliability of the confidence score in the training distribution. cA and cE are evaluated on corrupted versions of CIFAR-10 and CIFAR-100, which amounts to quantifying the aleatoric uncertainty. We can see that for this kind of uncertainty, \method~ achieves state of the art performance.
 On the other hand, for epistemic uncertainty, we can see that DE always attain best results. Overall, our \method~ is more computationally efficient 
while providing better results for the aleatoric uncertainty.  
\ab{Computation wise, DE takes  hours to train on CIFAR-10, while \method needs 2 times less,  hours and  minutes.}
In Figure~\ref{fig:ECELBNN} and Table~\ref{table:tab1}, we observe that our method exhibits 
\ab{top ECE score on CIFAR-10-C, as well as for the stronger corruptions.}












    



 \begin{table}[t!]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
\centering
 \scalebox{0.6}
 {
\begin{tabular}{l l | r r r r r}
\toprule
  &     & Vanilla & BatchEnsemble      & Deep Ensembles  & TRADI    & LP-BNN     \\
\midrule
\multirow{4}{*}{Training}&  Time (s)  &  1,506    & 1,983    & 6,026  & 1,782  & 1,999     \\
  &  for 1 epochs  &   \relative{1}   &  \relative{1.31}   &  \relative{4.0} &  \relative{1.18} &  \relative{1.33}       \\
  & Memory (MiB) &   8,848 & 9,884 & 35,392   &9,040  & 9,888  \\
  & &   \relative{1}   &  \relative{1.11}   &  \relative{4.0}  &  \relative{1.02}&  \relative{1.11}     \\
\midrule
\multirow{4}{*}{Testing} & Time (s) &  0.21   & 0.56   & 0.84 & 0.84   & 0.57     \\
  & on 1 image  &  \relative{1}  & \relative{2.67}   & \relative{4.0} & \relative{4.0} &  \relative{2.71}    \\ 
  &  Memory (MiB) &  1,884 & 4,114 & 7,536 & 7,536    & 4,114     \\
  &  & \relative{1}   &  \relative{2.18}  & \relative{4.0}  & \relative{4.0} &   \relative{2.18}     \\
\bottomrule
\end{tabular}
}
\vspace{-1mm}
\caption{\ab{\textbf{Runtime and memory analysis.} Numbers correspond to StreetHazards images processed with DeepLabv3+ ResNet-50 with PyTorch on a PC: Intel Core i9-9820X and  GeForce RTX 2080Ti. Colored numbers are relative to vanilla approach. Mini-batch size for training is  and for testing .}
}
\vspace{-6mm}
\label{table:time}
\end{table}

\subsection{Semantic segmentation}

Next, we evaluate semantic segmentation, a task of interest for autonomous driving, 
\ab{where} high capacity DNNs are used for processing high resolution images with complex urban scenery \ab{with strong class imbalance.}

\noindent\textbf{StreetHazards~\cite{hendrycks2019anomalyseg}.}
StreetHazards is a large-scale dataset that consists of different sets of synthetic images of street scenes. More precisely, this dataset is composed of  images for training and  test images.
The training dataset contains pixel-wise annotations for  classes. The test dataset comprises  training classes and  OOD classes, unseen in the training set, making it possible to test the robustness of the algorithm when facing a diversity of possible scenarios. 
For this experiment, we used DeepLabv3+~\cite{chen2018encoder} with a ResNet-50 encoder~\cite{he2016deep}. Following the implementation in~\cite{hendrycks2019anomalyseg}, most papers use PSPNet~\cite{zhao2017pyramid} that aggregates predictions over multiple scales, an ensembling that can obfuscate in the evaluation the uncertainty contribution of a method. This can partially explain the excellent performance of MCP on the original settings~\cite{hendrycks2019anomalyseg}. We propose using DeepLabv3+ instead, as it enables a clearer evaluation of the predictive uncertainty.
 We propose two DeepLabv3+ variants as follows. DeepLabv3+ is composed of an encoder network and a decoder network; in the first version, we change the decoder by replacing all the convolutions with our new version of \method~convolutions and leave the encoder unchanged. In the second variant we use weight standardization~\cite{qiao2019rethinking} on the convolutional layers of the decoder, replacing batch normalization \cite{ioffe2015batch} in the decoder with group normalization \cite{wu2018group}\ab{, to better balance mini-batch size and ensemble size.} We denote the first version LP-BNN and the second one LP-BNN + GN.



                




 
\begin{table*}[htbp]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
 \vspace{-2mm}
 \begin{center}
 \scalebox{0.65}
 {
 \begin{tabular}{c l  c c c c c }
 \toprule
 Dataset & OOD method  & mIoU  & AUC   & AUPR  & FPR-95-TPR  & ECE  \\ 
 \midrule
\multirow{6}{*}{\shortstack[c]{\textbf{StreetHazards} \\ DeepLabv3+ \\ ResNet50}}  & Baseline (MCP)~\cite{hendrycks2016baseline} & 53.90  & 0.8660& 0.0691 & 0.3574 & 0.0652 \\ 
             & TRADI      \cite{franchi2019tradi} &  52.46	& 0.8739 & 0.0693 & 0.3826 &0.0633\\
           & Deep Ensembles  \cite{lakshminarayanan2017simple}& 55.59 & 0.8794 & \textbf{0.0832} & 0.3029 & 0.0533 \\
            & MIMO  \cite{havasi2020training} & 55.44 &  0.8738 &  0.0690 & 0.3266 & \textbf{0.0557} \\ 
                  & BatchEnsemble \cite{wen2020batchensemble}  &\textbf{56.16}  & 0.8817 & 0.0759 & 0.3285 & 0.0609 \\ 
                                 & LP-BNN (ours)  &  54.50 & \textbf{0.8833} & 0.0718 & 0.3261 & \textbf{0.0520}  \\
           & LP-BNN + GN (ours)  & \textbf{56.12} & \textbf{0.8908} & 0.0742 &\textbf{0.2999} & \textbf{0.0593}  \\
\midrule 
  \multirow{6}{*}{\shortstack[c]{\textbf{BDD-Anomaly} \\ DeepLabv3+ \\ ResNet50}}                               & Baseline (MCP)~\cite{hendrycks2016baseline} &  47.63  & 0.8515  & 0.0450  & 0.2878  & 0.1768 \\ 

    & TRADI   \cite{franchi2019tradi}   & 44.26	& 0.8480	& 0.0454	& 0.3687	& 0.1661 \\ 
 & Deep Ensembles  \cite{lakshminarayanan2017simple}& \textbf{51.07}  & 0.8480  & 0.0524  & \textbf{0.2855 } &\textbf{0.1419 } \\
  & MIMO  \cite{havasi2020training}& 47.20  &  0.8438  & 0.0432  & 0.3524 &0.1633  \\

                    & BatchEnsemble  \cite{wen2020batchensemble}  &  48.09  & 0.8427  & 0.0449  & 0.3017  & 0.1690  \\ 
                                  & LP-BNN  (ours)   & {49.01}  &\textbf{0.8532 } & 0.0452  & 0.2947  & 0.1716  \\ 
                                  &LP-BNN + GN (ours)   & 47.15  & \textbf{0.8553}  & \textbf{0.0577}  & 0.2866  & 0.1623 \\ 
\bottomrule
 \end{tabular}
 } \end{center}
 \vspace{-3mm}
 \caption{\ab{\textbf{Comparative results on the OOD task for semantic segmentation.}  We run all methods ourselves in similar settings using publicly available code for related methods. Results are averaged over three seeds.}\label{table:outofditribution}}
 \vspace{-2mm}
 \end{table*}
 
   

\begin{figure*}[t!]
\renewcommand{\figurename}{Figure}
\renewcommand{\captionfont}{\small}
     \centering
        \begin{subfigure}[b]{0.16\linewidth}
        \caption*{\textbf{Input image}}
        \includegraphics[width=\textwidth]{images/1e232ffe-b4db708bLPBNN.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
        \caption*{\textbf{MCP}}
        \includegraphics[width=\textwidth]{images/confiance_1e232ffe-b4db708bMCP.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
        \caption*{\textbf{BE}}
        \includegraphics[width=\textwidth]{images/confiance_1e232ffe-b4db708bBE.png}
\end{subfigure}\;
         \begin{subfigure}[b]{0.16\linewidth}
        \caption*{\textbf{LP-BNN}}
        \includegraphics[width=\textwidth]{images/confiance_1e232ffe-b4db708bLPBNN.png}
\end{subfigure}\;

        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/1e232ffe-b4db708bLPBNN_GT.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/1e232ffe-b4db708bMCP_pred.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/1e232ffe-b4db708b8BE_pred.png}
\end{subfigure}\;
         \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/1e232ffe-b4db708bLPBNN_pred.png}
\end{subfigure}\;
   
        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/42f17eef-9158dc4d_LPBNN.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/confiance_42f17eef-9158dc4d_MCP.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/confiance_42f17eef-9158dc4d_BE.png}
\end{subfigure}\;
         \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/confiance_42f17eef-9158dc4d_LPBNN.png}
\end{subfigure}\;

        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/42f17eef-9158dc4d_LPBNN_GT.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/42f17eef-9158dc4d_MCP_pred.png}
\end{subfigure}\;
        \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/42f17eef-9158dc4d_BE_pred.png}
\end{subfigure}\;
         \begin{subfigure}[b]{0.16\linewidth}
\includegraphics[width=\textwidth]{images/42f17eef-9158dc4d_LPBNN_pred.png}
\end{subfigure}\;
 \caption{
\ab{\textbf{Visual assessment on two BDD-Anomaly test images containing a motorcycle (OOD class).} For each image: \emph{on the first row}, input image and confidence maps from MCP~\cite{hendrycks2016baseline}, BE~\cite{wen2020batchensemble}, and LP-BNN; \emph{on the second row}, ground-truth segmentation and segmentation maps from MCP, BE, and LP-BNN. LP-BNN is less confident on the OOD objects.}
 }
  \label{fig:accuconfidance}
\vspace{-5mm}
\end{figure*}

       
     



   




\noindent\textbf{BDD-Anomaly~\cite{hendrycks2019anomalyseg}.}
BDD-Anomaly is a subset of the BDD100K dataset~\cite{yu2020bdd100k}, composed of  street scenes for training and  for the test set. The training set contains pixel-level annotations for  classes, and the test dataset is composed of the  training classes and  OOD classes: motor-cycle and train.
For this experiment, we use DeepLabv3+~\cite{chen2018encoder} with the experimental protocol from~\cite{hendrycks2019anomalyseg}. As previously we use ResNet50 encoder~\cite{he2016deep}. For this experiment, we use  the LP-BNN and LP-BNN + GN variants.

\noindent\textbf{Discussion.} 
We emphasize that the semantic segmentation is more challenging than the CIFAR classification since {images are bigger} and their content is more complex. The larger input size constrains to use smaller \ab{mini-}batches.  This is crucial since the fast weights of the ensemble layers are trained just on one \ab{mini-}batch slice. In this experiment, we could use \ab{mini-}batches of size  and train the fast weights on slices of size . Yet, despite these computational difficulties, with our technique, we achieve state-of-the-art results for most metrics. We can see in Table~\ref{table:outofditribution} that our strategies {achieve} state-of-the-art performance in detecting OOD data and are well calibrated. We can also see in Figure~\ref{fig:accuconfidance}, where the OOD class is the motorcycle, that our DNN is less confident 
\ab{on} this class. Hence LP-BNN allows us to have a more reliable DNN which is essential for real-world applications.

\Gianni{Table~\ref{table:time} shows the computational cost of LP-BNN \ab{and related methods}. \ab{For training,} LP-BNN
\ab{takes only  more time than a vanilla approach, in contrast} to} 
\ab{DE} 
\ab{that take much longer, }
\ab{while their} performances are equivalent in most cases. 
\ab{In the same time, LP-BNN enables implicit modeling of weight correlations at every layer with limited overhead as it does not explicitly computes the covariances.}
\ab{To the best of our knowledge, LP-BNN is the first approach with the posterior distribution computed with variational inference successfully trained and applied for semantic segmentation.}


 
 



   
%
 
\section{Conclusion}
We propose a new BNN framework able to quantify uncertainty in the context of deep learning. Owing to each layer of the network being tied to and regularized by a VAE, LP-BNNs are stable, efficient, and therefore easy to train compared to existing BNN models. The extensive empirical comparisons on multiple tasks show that LP-BNNs reach state-of-the-art levels with substantially lower computational cost. We hope that our work will open new research paths on effective training of BNNs. In the future we intend to explore new strategies for plugging more sophisticated VAEs in our models along with more in-depth theoretical studies.

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage



\clearpage
\begin{widetext}
\begin{center}
\textbf{\large Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification (Supplementary material)}
\end{center}
\end{widetext}

\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}


\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{\Alph{section}}



\section{Discussion}

\subsection{Covariance Priors of Bayesian Neural Network}
\label{subsection1appendix}

We consider a data sample , with  and .
We process the input data  with a MLP network  with parameters  composed of one hidden layer of  neurons. We detail the composing operations of the function  associated to this network: , where  is an element-wise activation function. For simplicity, we ignore the biases in this example.  is the weight matrix associated with the first fully connected layer and  the weights of the second layer. 
In BNNs,  and  represent random variables, 
while for classic DNNs, they are simply singular realizations of the distribution sought by BNN inference.
Most works exploiting in some way the statistics of the network parameters assume, for tractability reasons, that all weights of   and  follow independent Gaussian distributions.  Hence this will lead to the following covariance matrix for : 



where  is the variance of the coefficient 
\ab{} of matrix . Similarly, for  we will have a diagonal matrix.

Now, let us assume that  and    have a latent representation 
\ab{}
and  
\ab{}, respectively, such that  for every coefficient 
\ab{}  of  and  there exist real weights 
\ab{ and  such that:  and , respectively.}
\Gianni{In the case of LP-BNN, we consider that each coefficient of   and  represent an independent random variable.} 
\ab{Thus, in contrast to approaches based on the mean-field approximation  \Gianni{directly on the weights of the DNN},} we can have for each layer a non-diagonal covariance matrix with the following variance and covariance term\emi{s} for :




This allows us to leverage the lower-dimensional parameters of the distributions of  and  for estimating the higher-dimensional distributions of  and . In this manner, in LP-BNN we model an \textbf{implicit covariance} of weights at each layer.


We note that several approaches for modeling correlation between weights have been proposed under certain settings and assumptions. For instance, Karaletesos and Bui~\cite{karaletsos2020hierarchical} model correlations between weights within a layer and across layers thanks to a Gaussian process-based approach working in the function space via hierarchical priors instead of directly on the weights. Albeit elegant, this approach is still limited to relatively shallow MLPs (e.g., one hidden layer with 100 units~\cite{karaletsos2020hierarchical}) and cannot scale up yet to deep architectures considered in this work (e.g., ResNet-50).
Other approaches~\cite{louizos2016structured, sun2017learning} 
model layer-level weight correlations through Matrix Variate Gaussian (MVG) prior distributions, increasing the expressiveness of the inferred posterior distribution at the cost of further increasing the computational complexity w.r.t. mean-field approximated BNNs~\cite{blundell2015weight, graves2011practical}.
In contrast, LP-BNN does not \textbf{explicitly model the covariance} by conveniently leveraging fully connected layers to project weights in a low-dimensional latent space and performing the inference of the posterior distribution there. This strategy leads to a lighter BNN that is competitive in terms of computation and performance for complex computer vision tasks.






\subsection{The utility of Rank-1 perturbations }
\label{subsection2appendix}

\ab{One could ask why using the Rank-1 perturbation formalism from BE~\cite{wen2020batchensemble}, instead of simply feeding the weights of a layer to the VAE to infer the latent distribution. Rank-1 perturbations significantly reduce the number of weights upon which we train the VAE, due to the decomposition of the fast weights into  and . This further allows us to consider multiple such weights, , at each forward pass enabling faster training of the VAE as its training samples are more numerous and more diverse.} 

Next, we establish connections between the cardinality  of the ensemble and the posterior covariance matrix.
\ab{The mechanism of placing a prior distribution over the latent space enables an implicit modeling of correlations between weights in their original space. This is a desirable property due to its superior expressiveness~\cite{foong2020expressiveness, karaletsos2020hierarchical} but which can be otherwise computationally intractable or difficult to approximate.} 
The covariance matrix of our prior \ab{in the original weight space} is a Rank-1 matrix. Thanks to the Eckart-Young theorem (Theorem 5.1 in~\cite{wang2012geometric}), we can quantify the error of approximating the covariance by a Rank-1 matrix, based on the second up to the last singular values.


Let us denote by  the  weights trained by our algorithm,    and . The differences and the sum in the previous equations are calculated element-wise on all the weights of the DNNs. 
Then, for each new data sample {}, the prediction of the DNN   is equivalent to the average of the DNNs   applied on   :

with .
\ab{The  norm is computed over all weights.}
\ab{We refer the reader to the proof in \S3.5 of~\cite{izmailov2018averaging}.}
It follows that in fact we do not learn a Rank-1 matrix, but an up to Rank- covariance matrix, if all the   are independent. 
Hence the choice of  acts as an approximation factor of the covariance matrix. 
\ab{Wen \etal\cite{wen2020batchensemble} tested 
different values of  and found} that  was the best compromise, which we also use \ab{here}.


 
\subsection{Computational complexity}
\label{subsection3appendix}

Recent 
\ab{works}~\cite{fort2019deep,wilson2020bayesian} 
studied the weight modes 
computed by Deep Ensembles \ab{under a BNN lens}, yet these \ab{approaches are} computationally \ab{prohibitive} at the scale required for \ab{practical} computer vision tasks. \ab{Recently, Dusenberry et al.~\cite{dusenberry2020efficient} proposed a more scalable approach for BNNs, which can still be subject to high instabilities as the ELBO loss is applied over a high-dimensional parameter space, all BatchEnsemble parameters. Increased stability can be achieved by leveraging large mini-batches that bring more robust feature and gradient statistics, at significantly higher computational cost (large virtual mini-batches are obtained through distributed training over multiple TPUs).}
In comparison, our 
approach 
\ab{has a smaller memory overhead} since we encode  in a lower dimensional space (we found empirically that a latent space of size {only}  provides {an appealing} compromise between accuracy and compactness). \ab{The ELBO loss here is applied over this lower-dimensional space which is easier to optimize.} The only additional cost in terms of parameters and memory used 
\ab{w.r.t.} BE is related to the compact VAEs associated with each layer. 

\ab{In addition to} the lower number of parameters, LP-BNN training is more stable than for Rank-1 BNN~\cite{dusenberry2020efficient} due to the reconstruction term   which regularizes the 
loss in Eq.~(7) \Isa{of the main paper} by controlling the variances of the sampled weights.  
\ab{In practice, to train BNNs successfully, a range of carefully crafted heuristics are necessary}, e.g., clipping, initialization from truncated Normal \Isa{distributions}, extra weight regularization to stabilize training~\cite{dusenberry2020efficient}. For \method, training is overall straightforward even on complex and deep models, e.g., DeepLabV3+, thanks to the VAE module that is stable \ab{and trains faster}.













\begin{table}[t!]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
\begin{center}
\scalebox{0.8}
{
\begin{tabular}{l|c|c|c|c|c}
\toprule
\textbf{Learning Rate }                                              & 0.2   & 0.1   & 0.05  & 0.01  & 0.005 \\
\midrule
\begin{tabular}[c]{@{}l@{}}\textbf{BNN }\\ accuracy\end{tabular}         &22.48 & 44.60 & 49.83 & 48.70 & 56.69 \\
\midrule
\begin{tabular}[c]{@{}l@{}}\textbf{BNN }\\ epoch div\end{tabular}       &3     & 25    & 65    & None  & None  \\
 \midrule
\begin{tabular}[c]{@{}l@{}}\textbf{LP-BNN }\\ accuracy\end{tabular}        & 20.02 & 55.04 & 59.68 & 63.73 & 64.41 \\
 \midrule
\begin{tabular}[c]{@{}l@{}}\textbf{LP-BNN }\\ epoch div\end{tabular}      &3     & None  & None  & None  & None \\ 

\bottomrule
\end{tabular}
}
\end{center}
\caption{\Gianni{\textbf{Stability analysis of BNNs.}} Stability experiment with LeNet 5 architecture and 80 epoch\ab{s} on CIFAR-10. 
\ab{On the epoch divergence row, \emph{None}} means that the DNN does not diverge. }\label{table:stability}
\vspace{-3pt}
\end{table}


\subsection{Stability of Bayesian Neural Networks}
In this section, we experiment on CIFAR-10 to evaluate the stability of LP-BNN versus a 
\ab{classic} BNN. \emi{For} this experiment, we use \emi{the} LeNet-5 architecture and choose a weight decay of   \emi{along with} a \ab{mini-}batch size of . Our goal is to see \emi{whether} both techniques are stable when we vary the learning rate.  Both DNNs were trained under the \Gianni{exact} same conditions for  epochs.
In Table~\ref{table:stability}, we present two metrics for both DNNs. The first metric is the accuracy.  \Gianni{The second metric is the epoch \emi{during which} the training loss of the DNN \ab{explodes, i.e.,} is equal to infinity. This phenomenon may occur if the DNN is highly unstable to train. 
}
\ab{We argue that LP-BNN is visibly more stable and standard BNNs during training.}
We can see from Table~\ref{table:stability} that LP-BNN is more stable than the 
\ab{standard} BNN \emi{as it does not diverge for a wide range of learning rates}. \emi{Moreover, its accuracy is higher than that of a standard BNN implemented on the same architecture, a property that we attribute to the VAE regularization.} 




\subsection{LP-BNN diversity}


\begin{table}[t!]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
\begin{center}
\scalebox{0.75}
{
\begin{tabular}{l|c|c|c}
\toprule
 &   \textbf{ratio errors  }  & \textbf{Q-statistic  }   & \textbf{correlation coefficient  } \\ 
\midrule
\textbf{DE}        &\textbf{0.9825 }     & \textbf{0.9877 }   & \textbf{0.6583} \\ 
\midrule
\textbf{BE}         &0.5915     & 0.9946     & 0.7634  \\ 
 \midrule
\textbf{LP-BNN }      & 0.8390     & \textbf{0.9842}    & \textbf{0.6601 }      \\ 
\bottomrule
\end{tabular}
}
\end{center}
\caption{\Gianni{\textbf{Comparative results of diversity \ab{scores} for image classification 
\ab{on the CIFAR-10 dataset.}
}}
}
\label{table:diversity}
\vspace{-3pt}
\end{table}

\begin{table}[t!]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
\begin{center}
\scalebox{0.75}
{
\begin{tabular}{l|c|c|c}
\toprule
 &   \textbf{ratio errors  }  & \textbf{Q-statistic  }   & \textbf{correlation coefficient  } \\  
\midrule
\textbf{DE}        &0.4193      & 0.9690    & 0.7568 \\ 
\midrule
\textbf{BE}         &0.2722     & 0.9874    & 0.8352  \\ 
 \midrule
\textbf{LP-BNN }      & \textbf{0.4476 }    & \textbf{0.9595}    & \textbf{0.7332}       \\ 
\bottomrule
\end{tabular}
}
\end{center}
\caption{\Gianni{\textbf{Comparative results of diversity \ab{scores} for image classification 
\ab{on the CIFAR-10-C dataset}.}} 
}\label{table:diversity_c}
\vspace{-3pt}
\end{table}

\ab{At test-time, }
\Gianni{
BNNs and DE aggregate the different predictions. For BNNs, these predictions come from the different  realizations of the posterior distribution, while, for the DE, these predictions \emi{are provided by} several DNNs trained in parallel.
As proved in \cite{fort2019deep,rame2021dice}, the diversity \emi{among} these different predictions is 
key to quantify the uncertainty of a DNN. Indeed, we want the different DNN predictions to have a high variance when the 
\ab{model} is not accurate. 
\emi{Figure~\ref{fig:diversityall} highlights this desirable property of high variance on out-of-distribution samples exhibited by LP-BNN.} Also, as in~\cite{rame2021dice}, we evaluate the ratio-error introduced in~\cite{aksela2003comparison}. The ratio-error between two classifiers is the  number of data \ab{samples} on which 
\ab{only} one classifier is wrong divided by the number of 
\ab{samples} on \emi{which} they are both wrong. A higher value means that the two classifiers are less likely to make the same errors. 
We also evaluated the Q-statistics \cite{aksela2003comparison}, which measures the diversity between two classifiers. The value of the Q-statistics \emi{is} between \ab{ and } and is defined 
\ab{as:}

where  and  are the numbers of data \emi{on which} both classifiers are correct and incorrect\emi{, respectively}.   and   are the number of data where just one of the two classifier\emi{s} is wrong. 
If the two classifiers are always wrong or right for all data\emi{, then}  and , \emi{while} if both classifiers always make errors on different inputs, then . The maximum diversity comes when  is minimum.} 

\Gianni{Finally, we evaluated the correlation coefficient~\cite{aksela2003comparison}, which assesses \Isa{the correlation between the error vectors of the two classifiers}. Tables \ref{table:diversity} and \ref{table:diversity_c} illustrate that, for the normal case (CIFAR-10), LP-BNN 
\ab{displays similar diversity with DE,} while in the corrupted case (CIFAR-10-C) LP-BNN 
\ab{achieves better diversity scores.} 
\emi{We conclude that} 
\ab{in terms of diversity metrics,} LP-BNN has \emi{indeed} the behavior that one \emi{would} expect \emi{for uncertainty quantification purposes}.
}







\label{subsection5appendix}
\begin{figure*}[!t]
\renewcommand{\captionfont}{\small}

    \centering
    \scalebox{0.85}
{
\begin{tabular}{cccccccc}
\raisebox{7mm}{\large{Input}}
& 
\includegraphics[width=0.10\linewidth]{images/ID_batch_ensemble_3.jpg}\label{image1} &
\includegraphics[width=0.10\linewidth]{images/ID_batch_ensemble_4.jpg}\label{image2} &
\includegraphics[width=0.10\linewidth]{images/ID_corrupt_batch_ensemble_2.jpg}\label{image4} &
\includegraphics[width=0.10\linewidth]{images/ID_corrupt_batch_ensemble_4.jpg}\label{image4} &
\includegraphics[width=0.10\linewidth]{images/OOD_batch_ensemble_2.jpg}\label{image4} &
\includegraphics[width=0.10\linewidth]{images/OOD_batch_ensemble_3.jpg}\label{image4} &
\\

\raisebox{7mm}{\large{LP-BNN}} &
\includegraphics[width=0.15\linewidth]{images/ID_VAEBNN_3_histo.pdf}\label{dist1} &
 \includegraphics[width=0.15\linewidth]{images/ID_VAEBNN_4_histo.pdf}\label{dist2} &
\includegraphics[width=0.15\linewidth]{images/ID_corrupt_VAEBNN_2_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/ID_corrupt_VAEBNN_4_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/OOD_VAEBNN_2_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/OOD_VAEBNN_3_histo.pdf}\label{dist4} &
\\
\raisebox{7mm}{\large{BE}} &
\includegraphics[width=0.15\linewidth]{images/ID_batch_ensemble_3_histo.pdf}\label{dist1} &
 \includegraphics[width=0.15\linewidth]{images/ID_batch_ensemble_4_histo.pdf}\label{dist2} &
\includegraphics[width=0.15\linewidth]{images/ID_corrupt_batch_ensemble_2_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/ID_corrupt_batch_ensemble_4_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/OOD_batch_ensemble_2_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/OOD_batch_ensemble_3_histo.pdf}\label{dist4} &
\\
\raisebox{7mm}{\large{DE}} & 
\includegraphics[width=0.15\linewidth]{images/ID_DE_3_histo.pdf}\label{dist1} &
 \includegraphics[width=0.15\linewidth]{images/ID_DE_4_histo.pdf}\label{dist2} &
\includegraphics[width=0.15\linewidth]{images/ID_corrupt_DE_2_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/ID_corrupt_DE_4_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/OOD_DE_2_histo.pdf}\label{dist4} &
 \includegraphics[width=0.15\linewidth]{images/OOD_DE_3_histo.pdf}\label{dist4} &
\end{tabular}
 }
\caption{\textbf{Diversity of predictions of different ensemble methods.}  The first row contains in order {two} images from the test set of CIFAR-10, of  CIFAR-10-C  and of SVHN, respectively. The next three rows represent the corresponding outputs of the different sub models for the three ensembling algorithms being considered: \method, BatchEnsemble and Deep Ensembles. }\label{fig:diversityall}
\end{figure*}


\section{Implementation details}



\ab{For our implementation, we use PyTorch~\cite{paszke2019pytorch} and will release the code after the review in order to facilitate reproducibility and further progress. In the following we share the hyper-parameters for our experiments on image classification and semantic segmentation.}

\subsection{Semantic segmentation experiments}
{In Table \ref{table:tab3}, we summarize the \ab{hyper-parameters} used in the StreetHazards~\cite{hendrycks2019anomalyseg} and BDD-Anomaly~\cite{hendrycks2019anomalyseg} experiments. }



\begin{table}[t!]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
\begin{center}
\scalebox{0.85}
{
\begin{tabular}{l|c|c}
\toprule
  \ab{\textbf{Hyper-parameter}} &   \textbf{StreetHazards} & \textbf{BDD-Anomaly}        \\ 
\midrule
Ensemble size           &4 & 4 \\ 
\midrule
learning rate         &0.1 &0.1 \\ 
 \midrule
batch size        &4 & 4 \\ 
 \midrule
number of train epochs  & 25 & 25 \\ 
\midrule
 weight decay for      weights  &0.0001 & 0.0001 \\ 
 \midrule
  weight decay for     weights   &0.0 & 0.0 \\ 
 \midrule
 cutout         & True& True \\ 
 \midrule
 SyncEnsemble BN        & False & False \\ 
  \midrule
Group Normalisation      & True & True \\ 
  \midrule
 Size of the latent space         & 32 & 32 \\ 
\bottomrule
\end{tabular}
}
\end{center}
\caption{
\ab{\textbf{Hyper-parameter configuration used in the semantic segmentation experiments (\S5.3).
}}
}\label{table:tab3}
\end{table}


\begin{table}[t!]
\renewcommand{\figurename}{Table}
\renewcommand{\captionlabelfont}{\bf}
\renewcommand{\captionfont}{\small}
\begin{center}
\scalebox{0.85}
{
\begin{tabular}{l|c|c}
\toprule
 \ab{\textbf{Hyper-parameter}} &   \textbf{CIFAR-10}  & \textbf{CIFAR-100} \\ 
\midrule
Ensemble size           &4 & 4 \\ 
\midrule
 initial learning rate         &0.1 &0.1 \\ 
 \midrule
batch size        &128 & 128 \\ 
 \midrule
lr decay ratio     &0.1 & 0.1 \\ 
 \midrule
 lr decay epochs         &[80, 160, 200] & [80, 160, 200] \\ 
 \midrule
number of train epochs  &250 & 250 \\ 
\midrule
 weight decay for      weights  &0.0001 & 0.0001 \\ 
 \midrule
  weight decay for     weights   &0.0 & 0.0 \\ 
 \midrule
 cutout         & True& True \\ 
 \midrule
 SyncEnsemble BN        & False & False \\ 
  \midrule
 Size of the latent space         & 32 & 32 \\ 
\bottomrule
\end{tabular}
}
\end{center}
\caption{
\ab{\textbf{Hyper-parameter configuration used in the classification experiments (\S5.2).}}
} 
\label{table:tab2}
\vspace{-3pt}
\end{table}


\subsection{Image classification experiments}

In Table \ref{table:tab2}, we summarize the \ab{hyper-parameters} used in the CIFAR-10~\cite{krizhevsky2009learning} and CIFAR-100~\cite{krizhevsky2009learning} experiments. 




\section{Notations}

In Table \ref{table:tab1}, we summarize the {main} notations used in the paper. Table \ref{table:tab1} should facilitate the understanding of Section~2 (the preliminaries) and Section~3 (the presentation of our approach) of the main paper.








\begin{table*}[!t]
\renewcommand{\figurename}{Table}
\renewcommand{\captionfont}{\small}
\centering
\scalebox{0.90}
{
\begin{tabular}{ll}
\toprule
\textbf{Notations}                      & \textbf{Meaning}  \\ 
\midrule
 & the set of  data samples and the corresponding labels \\ 
\midrule
                      & the set of weights of a DNN            \\ \midrule
& the prior distribution over the weights of a DNN    \\ 
\midrule
\Gianni{}& \Gianni{the variational prior distribution over the weights of a DNN used in standard BNNs}\ab{~\cite{blundell2015weight}}   
\\ 
\midrule
\Gianni{}& \ab{the parameters of the variational prior distribution over the weights of a DNN used in standard BNNs~\cite{blundell2015weight}
}   \\ 
\midrule
     & the likelihood that DNN outputs  \emi{following a} prediction over input image    \\ 
\midrule
   & the number of ensembling DNNs  \\ 
\midrule
\ab{}   & \ab{the shared ``slow''} weights of the network   \\ 
\midrule
  & \ab{the set of individual ``fast''} weights of BatchEnsemble for ensembling of  networks \\
\midrule
     & \ab{the  set of fast weights of \method~ for ensembling of  networks.} \\ &  \ab{ are sampled from the latent weight space of weights .} \\
\midrule
   & the parameters of the VAE for \ab{computing the low dimensional latent distribution} of   \\ \midrule
\ab{} & the encoder of the VAE applied on   \\ \midrule
\ab{} &  \ab{the decoder of the VAE for reconstructing  from latent code of } \\ \midrule 
 
 & the variational distribution over the weights  to approximate the intractable posterior    \\ \midrule
  & \ab{encoder output that parameterize a multivariate Gaussian with diagonal covariance} \\ \midrule
  &  sampling a latent code  from the latent distribution \\
\midrule
 with  & the prior distribution on   \\ 
\midrule
 &  the reconstruction of  from its latent distribution \ab{, \ie the variational fast weights}  \\ 
\midrule 
\Gianni{} & \ab{the weight of a BatchEnsemble network  computed from slow and fast weights}  \\
& where  is the Hadamard product and  \ab{} the inner product between these two vectors.\\
\midrule
\Gianni{}&  \ab{the weight  of \method~ network network  computed from slow and variational fast weights} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Summary of the main notations of the paper.}}
\label{table:tab1}
\end{table*}













 

\end{document}
