\begin{table*}
\centering
\scalebox{1}{
\begin{tabular}{l|ccc|cc}
\toprule
  & \multicolumn{3}{|c|}{Samples} & \multicolumn{2}{|c}{Reconstructions} \\
 Model & Image & IS () & FID () & Image & Rel.  Error \% () \\
\midrule
  Base
      & Figure~\ref{fig:samples_128} & 24.10 & 30.14
      & Figure~\ref{fig:more_recons_128} & 70.54
  \\
  Light Augmentation
      & Figure~\ref{fig:samples_i128lightaug} & 27.09 & 20.96
      & Figure~\ref{fig:recons_i128lightaug} & 72.53
  \\
  High Res  (256) 
      & Figure~\ref{fig:samples_highrese} & 24.91 & 26.56
      & Figure~\ref{fig:recons_highrese} & 70.60
  \\
  High Res  (256)
      & Figure~\ref{fig:samples_256} & 25.73 & 37.21
      & Figure~\ref{fig:recons_256} & 77.70
  \\
\bottomrule
\end{tabular}
}
 \caption{
  Links to \method{} samples and reconstructions with associated metrics.
 }
 \label{sample_info}
\end{table*}

In this Appendix we present \method{} samples and reconstructions from several variants of the method.
Table~\ref{sample_info} includes pointers to samples and reconstruction images, as well as relevant metrics.
The samples were selected by best FID vs. training set statistics,
and we show the IS and FID along with sample images at that point.
The reconstructions were selected by best (lowest) relative pixel-wise  error, the error metric presented in Table~\ref{sample_info}, computed as:

where  and  are independent data samples, and  serves as a ``baseline'' reconstruction error relative to a ``random'' input.
For example, with a random initialization of  and , we have .
This relative metric penalizes degenerate reconstructions, such as the mean image, which would sometimes achieve low absolute reconstruction error despite having no perceptual similarity to the inputs.
despite that the resulting images having no perceptual similarity to the inputs.
In practice, given  data samples  (we use  50K),
we estimate the denominator by comparing each sample  with a single neighbor , computing:


\paragraph{Iterated reconstruction}
To further explore the behavior of a BigBiGAN (or any other model capable of approximately reconstructing its input),
we can ``iterate'' the reconstruction operation.
In particular, let  be defined for non-negative integers  and input images  as:

In Figure~\ref{fig:iterrecons} we show the results of up to 500 steps of this process for a few sample images.
Qualitatively, the first several steps of this process often appear to retain some semantics of the input image .
After dozens or hundreds of iterations, however, little content from the original input apparently remains intact.

\begin{figure}
\centering
 \includegraphics[width=1\textwidth]{samples_base_best_fid.jpg}

 \caption{
   samples  from an unsupervised \method{} generator , trained using the \textit{Base} method from Table~\ref{ablations}.
}
 \label{fig:samples_128}
\end{figure}

\begin{figure}
\centering

 \includegraphics[trim=0 1290 0 0,clip,width=1\textwidth]{recons_base_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 1032 0 258,clip,width=1\textwidth]{recons_base_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 774 0 516,clip,width=1\textwidth]{recons_base_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 516 0 774,clip,width=1\textwidth]{recons_base_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 258 0 1032,clip,width=1\textwidth]{recons_base_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 0 0 1290,clip,width=1\textwidth]{recons_base_6x9.jpg}
 \caption{
   reconstructions from an unsupervised \method{} model, trained using the \textit{Base} method from Table~\ref{ablations}.
  The top rows of each pair are real data , and bottom rows are generated reconstructions computed by .
}
 \label{fig:more_recons_128}
\end{figure}

\begin{figure}
\centering




 \includegraphics[width=1\textwidth]{samples_i128df.jpg}
 \caption{
   samples  from an unsupervised \method{} generator , trained using the lighter augmentation from~\cite{zurichfewer} with generation results reported in Table~\ref{uncond_gen_results}.
 }
 \label{fig:samples_i128lightaug}
\end{figure}

\begin{figure}
\centering

 \includegraphics[trim=0 1290 0 0,clip,width=1\textwidth]{recons_i128df_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 1032 0 258,clip,width=1\textwidth]{recons_i128df_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 774 0 516,clip,width=1\textwidth]{recons_i128df_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 516 0 774,clip,width=1\textwidth]{recons_i128df_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 258 0 1032,clip,width=1\textwidth]{recons_i128df_6x9.jpg}
 \\ \vspace{0.1cm}
 \includegraphics[trim=0 0 0 1290,clip,width=1\textwidth]{recons_i128df_6x9.jpg}
 \caption{
   reconstructions from an unsupervised \method{} model, trained using the lighter augmentation from~\cite{zurichfewer} with generation results reported in Table~\ref{uncond_gen_results}.
  The top rows of each pair are real data , and bottom rows are generated reconstructions computed by .
}
 \label{fig:recons_i128lightaug}
\end{figure}

\begin{figure}
\centering



 \includegraphics[width=1\textwidth]{samples_highrese.jpg}
 \caption{
   samples  from an unsupervised \method{} generator , trained using the \textit{High Res  (256)} configuration from Table~\ref{ablations}.}
 \label{fig:samples_highrese}
\end{figure}

\begin{figure}
\centering



 \includegraphics[trim=0 1542 0 0,clip,width=\textwidth]{recons_highresE_4x6_0_qual_80.jpg}
 \\ \vspace{0.2cm}
 \includegraphics[trim=0 1028 0 514,clip,width=\textwidth]{recons_highresE_4x6_0_qual_80.jpg}
 \\ \vspace{0.2cm}
 \includegraphics[trim=0 514 0 1028,clip,width=\textwidth]{recons_highresE_4x6_0_qual_80.jpg}
 \\ \vspace{0.2cm}
 \includegraphics[trim=0 0 0 1542,clip,width=\textwidth]{recons_highresE_4x6_0_qual_80.jpg}

 \caption{
   reconstructions of  encoder input images from an unsupervised \method{} model, trained using the \textit{High Res  (256)} configuration from Table~\ref{ablations}.
  Reconstructions are upsampled from  to  for visualization.
  The top rows of each pair are real data , and bottom rows are generated reconstructions computed by .
}
 \label{fig:recons_highrese}
\end{figure}

\begin{figure}
\centering




 \includegraphics[width=1\textwidth]{samples_256_qual_80.jpg}
 \caption{
   samples  from an unsupervised \method{} generator , trained with a high-resolution  and  (\textit{High Res  (256)} from Table~\ref{ablations}).
 }
 \label{fig:samples_256}
\end{figure}

\begin{figure}
\centering

 \includegraphics[trim=0 1542 0 0,clip,width=\textwidth]{recons_256_qual_80.jpg}
 \\ \vspace{0.2cm}
 \includegraphics[trim=0 1028 0 514,clip,width=\textwidth]{recons_256_qual_80.jpg}
 \\ \vspace{0.2cm}
 \includegraphics[trim=0 514 0 1028,clip,width=\textwidth]{recons_256_qual_80.jpg}
 \\ \vspace{0.2cm}
 \includegraphics[trim=0 0 0 1542,clip,width=\textwidth]{recons_256_qual_80.jpg}
 \caption{
   reconstructions from an unsupervised \method{} model, trained with a high-resolution  and  (\textit{High Res  (256)} from Table~\ref{ablations}).
  The top rows of each pair are real data , and bottom rows are generated reconstructions computed by .
}
 \label{fig:recons_256}
\end{figure}

\setlength{\tabcolsep}{2pt}
\begin{figure}
\centering
\begin{tabular}{cccccccc}
 \includegraphics[trim=0 2560 0    0,clip,width=0.14\textwidth]{resnet_ims90pctres_2_qual65.jpg} &
 \includegraphics[trim=0    0 0 2560,clip,width=0.14\textwidth]{resnet_ims90pctres_2_qual65.jpg} &
 &
 \includegraphics[trim=0 2560 0    0,clip,width=0.14\textwidth]{resnet_ims90pctres_3_qual65.jpg} &
 \includegraphics[trim=0    0 0 2560,clip,width=0.14\textwidth]{resnet_ims90pctres_3_qual65.jpg} &
 &
 \includegraphics[trim=0 2560 0    0,clip,width=0.14\textwidth]{resnet_ims90pctres_11_qual65.jpg} &
 \includegraphics[trim=0    0 0 2560,clip,width=0.14\textwidth]{resnet_ims90pctres_11_qual65.jpg} \\
  &  & &
  &  & &
  &  \\
 \multicolumn{2}{c}{Image 1} & &
 \multicolumn{2}{c}{Image 2} & &
 \multicolumn{2}{c}{Image 3} \\
\end{tabular}
 \caption{
  Iterated reconstructions from an unsupervised \method{} model, trained using the \textit{ResNet ( LR)} method from Table~\ref{ablations},
  computed by recursively running the reconstruction operation  on its own output as described in Appendix~\ref{appendix_samples}.
  In each pair of columns, the left column shows a real input image  at the top, and  through  in the remaining rows,
  the results of iterating reconstruction one to nine times,
  The right column shows the result of up to 500 iterations sampled at longer intervals, displaying
  ,
  ,
  ,
  ,
  ,
  ,
  ,
  ,
  , and
  .
}
 \label{fig:iterrecons}
\end{figure}
\setlength{\tabcolsep}{6pt}
