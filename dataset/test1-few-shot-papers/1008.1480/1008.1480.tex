\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{epic,eepic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvips]{graphicx}
\usepackage[dvips]{color}
\usepackage{chngpage}

\setlength{\textwidth}{7.0in}
\setlength{\evensidemargin}{-0.25in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9.3in}
\setlength{\textfloatsep}{10pt}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 2pt}

\renewcommand{\textfraction}{.01}
\renewcommand{\topfraction}{.99}
\renewcommand{\bottomfraction}{.99}

\renewcommand{\baselinestretch}{0.99}

\setlength{\baselineskip}{1\baselineskip}
\newcommand{\reals}{{\mathbb R}}

\def\Ot{\tilde{O}}
\def\Lovasz{Lov{\'a}sz }

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Property}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{property}{Property}

\setlength{\parindent}{0pt}
\def\subsubsubsection#1{{\bf #1.}\ }

\def\cal{\mathcal}
\def\ceil#1{{\lceil {#1} \rceil}}

\newcommand{\Proof}{\noindent{\bf Proof.}\ \ }

\newcommand{\lvl}{\mbox{lvl}}
\newcommand{\lca}{\mbox{lca}}
\newcommand{\pnt}{\mbox{pnt}}
\newcommand{\poly}{\mbox{poly}}
\newcommand{\DIJ}{\mbox{\bf Dijkstra}}
\newcommand{\apBHSP}{\mbox{\bf apBLSP}}
\newcommand{\SEARCH}{\mbox{\bf Search}}
\newcommand{\SP}{\mbox{\bf SP}}
\newcommand{\hdel}{\hat{\delta}}
\newcommand{\eps}{\varepsilon}
\newcommand{\del}{\delta}


\newcommand{\Top}{\mbox{\tiny{TOP}}}
\newcommand{\bottom}{\mbox{\tiny{BOTTOM}}}
\newcommand{\lleft}{\mbox{\tiny{LEFT}}}
\newcommand{\parent}{\mbox{\footnotesize{P}}}

\newcommand{\ml}{min}
\newcommand{\Ml}{max}

\newcommand{\ignore}[1]{}

\newcommand{\ALGORITHM}{{\tt algorithm}}
\newcommand{\WHILE}{{\tt while}}
\newcommand{\FOR}{{\tt for}}
\newcommand{\IF}{{\tt if}}
\newcommand{\FOREACH}{{\tt foreach}}
\newcommand{\RETURN}{{\tt return}}

\newcommand{\ARTCOVER}{\mbox{\bf artificial-cover}}
\newcommand{\NEXT}{\mbox{\bf next}}
\newcommand{\etal}{{\em et al.\ }}

\newenvironment{proof}{{\bf Proof:\ }}{\hfill\medskip}


\begin{document}
\title{Fast, precise and dynamic distance queries}
\author{
\and Yair Bartal\thanks{
 Hebrew University.
 \texttt{\scriptsize yair@cs.huji.ac.il}
}
\and Lee-Ad Gottlieb\thanks{
 Weizmann Institute of Science.
 \texttt{\scriptsize lee-ad.gottlieb@weizmann.ac.il}
}
\and Tsvi Kopelowitz\thanks{
 Bar Ilan University.
 \texttt{\scriptsize \{kopelot,moshe,liamr\}@cs.biu.ac.il}
}
\and Moshe Lewenstein\footnotemark[3]
\and Liam Roditty\footnotemark[3]
}


\date{}
\maketitle
\thispagestyle{empty}

\begin{abstract}\noindent
We present an approximate distance oracle for a point set  with  points 
and doubling dimension . For every , the oracle supports 
-approximate distance queries in (universal) constant time, occupies 
space , and can be 
constructed in  expected time. This improves upon the best previously known 
constructions, presented by Har-Peled and Mendel~\cite{HaPe06}. Furthermore, 
the oracle can be made fully dynamic with expected  query time and only 
 
update time. This is the first fully dynamic -distance oracle.

\end{abstract}

\newpage
\setcounter{page}{1}

\section{Introduction}
A distance oracle for a set of  points  under some distance 
function , is a preprocessed data structure that given two 
points  returns their distance without needing to query the 
distance function. Distance oracles are of interest when the distance 
function is too large to store (for example when the function is a distance 
matrix storing all  interpoint distances) or when querying the 
distance function is expensive (for example when the distance function is 
defined by graph-induced distances).

Distance oracles were introduced in a seminal paper of Thorup and Zwick
\cite{ThZw05}. For a weighted undirected graph, they gave an -approximate 
distance oracle with query time , for . Immediate from these
runtimes is the question of reduced query time,
and in fact Mendel and Naor~\cite{MeNa06} recently presented a 
-approximate distance oracle for general metrics with  query time.
Another direction for improvement is in the approximation guarantee, and 
distance oracles with -approximation () 
have been achieved for planar graphs \cite{Klein02,Thorup04},
geometric graphs \cite{GuLeNaSm08} and doubling spaces \cite{HaPe06}.

A further question in this field is that of dynamic distance oracles. In this
setting, the point set  is updated with the removal or addition of points,
and the distance oracle must be updated accordingly. A similar paradigm was considered
by \cite{RZ04}, who gave a distance oracle for an unweighted undirected graph under the 
removal of edges. Here, the distance function is the shortest path metric of the 
underlying graph, which must be consulted during an oracle update.

In this paper we consider a metric space with doubling dimension ,
and present a -approximate distance that answers
queries in (universal) constant time. The distance oracle occupies near-optimal space
, and
can be constructed in 
  expected  time.
This improves upon the best previously known constructions in this setting, presented by 
Har-Peled and Mendel~\cite{HaPe06}.
Furthermore, this oracle can be made fully dynamic with expected  query time. In this case, 
the update time is only

time per point. 


\paragraph{Related work.}
Thorup and Zwick \cite{ThZw05} demonstrated that a weighted undirected graph can be
preprocessed to create an oracle that can answer -approximate
distance queries between any two vertices in  time. The structure is
of size , and the randomized preprocessing takes
 time (where  is the number of vertices and  is the
number of edges). Roditty, Thorup and Zwick~\cite{RoThZw05} gave a
deterministic preprocessing algorithm that builds the distance oracle in
 time. Baswana and Sen~\cite{BaSe06} and Baswana and
Kavitha~\cite{BaKa06} improved the deterministic preprocessing time to
. Mendel and Naor~\cite{MeNa06} showed
that for any metric space there exists an -approximate distance
oracle of size  that supports queries in constant time
independent of .

Turning to lower bounds, Thorup and Zwick~\cite{ThZw05} proved that any
-approximate distance oracle must have size at least
. Very recently, Sommer, Verbin, and
Yu~\cite{SoVeYu09} extended a technique of P\v{a}tra\c{s}cu~\cite{Pa08} to
prove that a -approximate distance oracle preprocessed in  time must
occupy  space.

While the previous results apply to arbitrary metric space, distance
oracles have also been studied for more restricted settings.
Klein~\cite{Klein02} and Thorup~\cite{Thorup04} considered planar graphs,
and showed how to build a -distance oracle with  space and  query time. (Thorup~\cite{Thorup04}
presented an oracle for directed planar graphs.) Gudmundsson, Levcopoulos,
Narasimhan and Smid~\cite{GuLeNaSm08} considered geometric (Euclidean)
graphs that are -spanners for some constant . (A graph 
is said to be a -spanner for , if for any point pair ,
there exists in  a path connecting  and , and the length of this
path is at most  times the true distance between  and .) They
showed how to construct a -approximate distance oracle of size
. Their oracle can be constructed in
 time, and answers distance queries
in  time. Har-Peled and Mendel~\cite{HaPe06} considered  metric spaces with low doubling dimension. They presented two data structures both of size  (which attains the lower-bound on the space required for this task).
Their first data structure can be constructed in  time and answers -approximate distance queries in  time. Their second data structure can be constructed in polynomial time and answers -approximate distance queries in  time.

\paragraph{Our contribution.}
Our result improves the query time from  in the construction of Har-Peled and Mendel~\cite{HaPe06} to constant time,
while also providing the first fully dynamic oracle construction.
As in Har-Peled and Mendel \cite{HaPe06}, an immediate application of our static oracle for 
is a -approximate distance oracle for every graph which is a
-spanner of . Our static oracle is a dramatic improvement over those 
of Gudmundsson \etal \cite{GuLeNaSm08} reducing the query time to constant in several aspects, 
while the space is smaller by a factor of  and the setting is more general. 

To obtain our improved bounds, we present contributions in several distinct areas, including 
dynamic embeddings and dynamic tree structures. We present two probabilistic dynamic 
embeddings for doubling spaces: The first is into a tree metric, and the second is a 
snowflake embedding into Euclidean space (see Section~\ref{sec:dyn-embed}). In both cases, we 
are interested in the probability of low distortion, as opposed to expectation. This seems to 
be the first consideration of dynamic embeddings (although the related notion of on-line 
embeddings recently appeared in \cite{InMa10}). We also present a powerful dynamic tree 
structure that allows a binary search over centroid paths in our setting (see 
Section~\ref{sec:backup1}). Our oracle framework and the tools used in this paper further 
imply other distance oracles with various tradeoffs. A brief summary of these results is 
found in Table~\ref{tab:misc}.

\renewcommand{\arraystretch}{1.2}
\begin{table}
\small
\centering
\begin{tabular}{||c|c|c|c|c||}
\hline
 & Reference & Static construction & Query time & Space  \\
\hline\hline 
1& \cite{HaPe06} & [ &  
&  \\
\hline
2& \cite{HaPe06} &  &  &  \\
\hline
3& Section~\ref{sec:backup1} &  & 
 &  \\
\hline
4& Section~\ref{sec:misc} & * & 
 &  \\
\hline
5& Section~\ref{sec:misc} & * &  &  \\
\hline
6& Theorem~\ref{thm:static-oracle} & * &  &  \\
\hline\hline
 & Reference & Dynamic updates & Query time & Space  \\
\hline\hline 
7& Theorem~\ref{thm:backup2} &  & 
 &  \\
\hline
8& Theorem~\ref{thm:backup1} &  &  &  \\
\hline
9& Section~\ref{sec:misc} &  &  & 
 \\
\hline
10& Theorem~\ref{thm:dynamic-oracle} &  &  &  \\
\hline
\end{tabular}
\caption{A summary of  distance oracles. *In expectation.}\label{tab:misc}
\end{table}
\normalsize


\vspace{-10pt}
\paragraph{Paper outline.}
The rest of this paper is organized as follows. We first present preliminary points
in the next section. We then describe (in Section~\ref{sec:backbone}) the basic 
structure that forms the backbone of most of our constructions. We proceed to present 
the central contribution of this paper, the  query time oracles 
(both static and dynamic) in Section~\ref{sec:main}. The dynamic oracle requires two 
dynamic backup oracles, which are separate constructions of independent interest
(presented in Section~\ref{sec:backup}), and both oracles require several technical 
contributions (presented in Section~\ref{sec:technical}).


\section{Preliminaries}\label{sec:prelim}

Here we review some preliminary definitions and results that are
required in order to present our new ideas.

\paragraph{\bf Lowest common ancestor query.}
A lowest common ancestor (LCA) query on tree  provides two nodes  of ,
and asks for the node  that is ancestral to  and , and is minimal in the
sense that no descendant of  is ancestral to both  and .
LCA queries can be answered in  time in the word
RAM model, using a linear size data structure \cite{CoHa05}.

In the dynamic setting, Cole and Hariharan \cite{CoHa05} gave
a linear size data structure that supports LCA queries under insertions and
deletions of leaves and internal nodes to the tree. The query and update times are
all  under the word RAM model. We can extend their structure to also identify in
 time the two children of  that are ancestors of  and 
(Lemma \ref{lem:lca-child}).

\paragraph{\bf Level ancestor query.} A level ancestor query on tree 
provides a node  and level , and asks for the node  that is both an
ancestor of  and is  nodes removed from the root of . There exists a
linear size structure that supports level ancestor queries in  time.

In the dynamic setting, there exists a structure that supports level ancestor
queries in  search and update time under insertions of leaves into .
However, the insertion of internal nodes is not supported by this structure
\cite{AlHo00,KoLe07}. For the purposes of this paper, we must maintain a tree under the
insertions of internals nodes, hence we are unable to utilize standard level
ancestor query structures in our dynamic setting.

\ignore{
\paragraph{\bf Membership query.}
Given a set of numbers , there exists a linear data structure that
preprocesses the data in linear time, and can determine in  time
whether a given number  is a member of the set ~\cite{FrKoSz84}.
For our purposes, we will not require a dynamic version of this structure.

\paragraph{Predecessor query.}
The predecessor query problem is defined as follows: Given a set of
numbers  of size  over a universe  of size ,
preprocess  so that given a query number , the largest number
in  less than  can be efficiently computed. There exists a linear
space data structure that can be constructed in  time and
which answers predecessor queries in time  in the word RAM
model \cite{}.

For our purposes, we will not require a dynamic version of this structure.
}

\paragraph{Doubling dimension.}

For a metric , let  be the smallest value such that every
ball in  can be covered by  balls of half the radius. The {\em
doubling dimension} of  is dim. A metric is {\em doubling}
when its doubling dimension is constant. Note that while low Euclidean
dimension implies low doubling dimension (Euclidean metrics of dimension
 have doubling dimension  \cite{GuKrLe03}), low doubling
dimension is more general than low Euclidean dimension.
The following property can be demonstrated via a repetitive application of
the doubling property.

\begin{property}[Packing property]\label{prop:doubling}
For set  with doubling dimension , if the minimum
interpoint distance in  is at least , and the diameter
of  is at most , then .
\end{property}


\paragraph{Hierarchical Partitions.}\label{sec-hier}

Similar to what was described in \cite{GaoGuiNgu04,KrLe04}, a subset of
points  is an -discrete center set (or {\em net} in
the terminology of \cite{KrLe04}) of  () if it satisfies the
following properties:
\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{enumerate}
\item Packing: For every , .
\item Covering: Every point  is strictly within distance  of
some point : .
\end{enumerate}
The previous conditions require that the points of  be spaced out, yet
nevertheless cover all points of .

A hierarchical partition for a set  is a hierarchy of discrete center
sets, where each level of the hierarchy is a discrete center set of the
level beneath it. Krauthgamer and Lee \cite{KrLe04} gave a fully dynamic
hierarchy that can be updated in time 
( is the aspect ratio of ), where a single update to  can result in
 updates to the hierarchy. Cole and
Gottlieb \cite{CoGo06} presented a semi-dynamic hierarchy, where a single insertion into
 can result in the insertion of  points into the hierarchy.
However, points cannot be removed from within the hierarchy, and after many deletions the
hierarchy is rebuilt in the background.\footnote{It suffices, if the hierarchy holds 
nodes (included those nodes storing deleted points), to start
rebuilding after  deletions, and to complete the
rebuilding over the next  insertions and deletions;
that is, for each update to the point set 7 updates are performed
on the background structure. The completed hierarchy
will then contain at least  points, including at
most  deleted points.}

Our constructions can make use of either hierarchy, but our descriptions will
assume the hierarchy of \cite{CoGo06}.
The bottom level of this hierarchy is the set  that contains all points, and the top level  contains only a single point.
(For ease of presentation, we assume throughout
that the minimum interpoint distance in  is 1.) Each intermediate
level  is represented by a set , which is a
-discrete center set for .
The {\em radius} of level  is defined to be . A point  -covers a point  if ,
and the covering property states that each points in the hierarchy is
-covered by some point one level up. It can be shown by a
repeated application of the covering property that each point is
-covered by some point in every higher level.

The hierarchy may be augmented with neighbor links: Each point  records what points of  are within distance  of  -- these are the -neighbors of . By the packing property, a point may have
 such neighbors. To save space in the hierarchy, points
that have no -neighbors (where ) and also cover only one point
in the next level may be represented implicitly. This compression scheme
ensures that a hierarchy and neighbor links can be stored with
 space.


\paragraph{\bf Snowflake embedding.}
Assouad's \cite{Assouad83} snowflake embedding -- as improved by Gupta \etal
\cite{GuKrLe03} -- takes an arbitrary metric  of
doubling dimension  (where  is the metric's distance
function), and embeds the {\em snowflake}  into -dimensional Euclidean space with  distortion.
That is, the embedding into Euclidean space achieves low
dimension and distortion, but has the `side effect' that every
interpoint distance in the metric is replaced by its square root, with
distortion to the square root at most . Har-Peled and Mendel
\cite{HaPe06} used this embedding in the context of distance oracles.

Although \cite{GuKrLe03} did not give an exact run time
for their static embedding, the following analysis holds: The static
embedding can be achieved by first building a point hierarchy that records
-neighbors; this can be done in
 time
\cite{KrLe04,HaPe06,CoGo06}. The analysis in \cite{GuKrLe03} requires a
constructive application of the \Lovasz Local Lemma \cite{MoTa10}, which in this case
can be done in  expected time.
Given these constructions, the image for each point can easily be computed in
 time, but a more careful analysis
shows that  work per
hierarchical point is sufficient. It follows that the entire construction can be done in

expected time. The construction of \cite{GuKrLe03} is static, and so for our purposes we
will need to create a dynamic version of the embedding (see
Section~\ref{sec:dyn-snow}).


\section{Construction backbone}\label{sec:backbone}

The backbone of our distance oracles is a point hierarchy, and we shall employ the semi-dynamic 
hierarchy of Cole and Gottlieb \cite{CoGo06} augmented with storage of -neighbor pairs, and 
the distance between the neighbors in a pair. We will show below that  
() is an appropriate choice. On top of this hierarchy, we define a 
parent-child relationship as follows: every point  is a child of some point  that -covers . This implies that points that are siblings must 
be -neighbors. The parent-child relationship immediately 
defines an ancestor-descendant relationship as well. (Note that some other constructions in 
this paper, such as the dynamic embeddings of Section~\ref{sec:dyn-embed}, will require a 
different definition of the parent-child relationship.) We have the following property:

\begin{property}[Hereditary property]\label{prop:hereditary}
If two points  are -neighbors
then their respective parents  are -neighbors as well.
\end{property}

That is, if two points  have the property that
, then their respective parents  have the property that
. 
This means that  and all their ancestors up to their lowest common ancestor are all
found explicitly in the hierarchy.

The hierarchical tree  is extracted from the hierarchy.  has one node per 
hierarchical point, and the points of hierarchical level  all have 
corresponding nodes in tree level . The parent-child relationship among points in 
the hierarchy defines the same parent-child relationship among the corresponding tree 
nodes. We will refer to the distance between two tree nodes, by which we mean the 
distance between their corresponding points. Further, we compress all nodes whose 
points are only implicit in the hierarchy; this results in the contraction of some 
unary paths. This tree will allow us to navigate the hierarchy.

\paragraph{Structural lemmas.} Here we present lemmas that will be used to prove 
correctness of our oracles. The key observation motivating our constructions is 
captured by the following lemma, a variant of which was central for the construction 
of low stretch spanners \cite{GaoGuiNgu04,Ro07,GoRo08a,GoRo08b}. While we state the 
lemmas in term of general , we are actually interested in the two cases where  
and .

\begin{theorem}\label{thm:approx}
Let  be a pair that are not -neighbors, and let  in some
level  be the lowest respective ancestors of  and  that are -neighbors. Then
 is a -approximation to .
\end{theorem}

\begin{proof}
The fact that  and  are not -neighbors implies that , while the fact
that  and  are -neighbors implies that . The parent-child
relationship implies that
.
It follows that
.
Also,
.
\end{proof}

When  we have that  is a -approximation for 
, and when  we have that  is a -approximation for 
. Hence, the problem of finding a -approximation for  can be 
solved by finding the lowest ancestral -neighbors of  and  in the hierarchy. 
Later, we will also make use of the following corollary.

\begin{corollary}\label{cor:approx}
A -approximation for  implies a -approximation for
, and vice versa.
\end{corollary}

We have proved that finding the lowest ancestral -neighbors of  and  in the
hierarchy will provide a -approximation to . The following lemma demonstrates
a close relationship between the value of , and the level in which 
the lowest ancestral -neighbors  of  are found.

\begin{lemma}\label{lem:level}
Let  be such that  . Let  and , be the
respective ancestors of  and  in level .
\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{enumerate}
\item
If , then  and  are not -neighbors.
\item
If ,  and  must be either -neighbors or the same
point.
\end{enumerate}
\end{lemma}

\begin{proof}
\noindent (i) We have that
.
Note that for values
,
we have that
,
and  and  cannot be -neighbors.

\smallskip
\noindent (ii) We have that
.
Note that for values
,
we have that
,
and  and  must be -neighbors or the same point.
\end{proof}

Lemma~\ref{lem:level} implies that the level of the lowest ancestral
-neighbors  is in the
range ,
and this range is of size  irrespective of the value of .
Crucially, this means that the levels of the lowest ancestral -neighbors
of  and the lowest ancestral -neighbors of  differ by a fixed
value (), up to an additive constant. This implies that
finding the lowest common -neighbors of 
is a useful tool to find their lowest common -neighbors, and therefore
a -approximation to . A further consequence of
Lemma~\ref{lem:level} is that a -approximation for  (or in
fact for any descendants of ) is
sufficient to pinpoint the level of the lowest ancestral -neighbors
 to a range of  possible levels.

\paragraph{Deletions.} In closing this section, we note that for all dynamic 
structures
presented in this paper, deletions are handled by rebuilding in the background
(as was described in Section~\ref{sec:prelim} in the context of dynamic hierarchies):
Deleted points are kept in the structure, and when a large number of points have been deleted,
we begin to rebuild the structure in the background. This has no effect on the asymptotic
runtimes of our constructions.



\section{Oracle queries in  time}\label{sec:main}
In this section we present -approximate distance oracles with 
query time and size . The
first oracle we present is static, and the second is a dynamic version of the
static construction. In Section~\ref{sec:misc}, we briefly discuss variants of these 
constructions that appear in Table~\ref{tab:misc}


\subsection{Static oracle}\label{sec:main-static}

In this section we prove the following theorem:
\begin{theorem}\label{thm:static-oracle}
There exists a static  approximate distance oracles with 
query time and size . The
oracle can be updated in expected time
.
\end{theorem}

Given points  and , the oracle finds in  time the lowest ancestral
-neighbors  of . As a consequence of Lemma~\ref{lem:level}, the
level of the lowest ancestral -neighbors of  gives the level of the
lowest ancestral -neighbors of  to within an additive constant. 
The level of the -neighbors can then be found using a constant number of level
ancestor queries.

The oracle locates  in three steps, each of which can be implemented in 
time: In the first step we compute an  approximation to .
As a consequence of Lemma \ref{lem:level}, this approximation restricts the
candidate level of  to a range of  possible levels.
The second step then derives an  approximation to ,
which further restricts the candidate level to  possible
levels. The third step locates .


\paragraph{Step 1.}
The first step provides an -approximation for , which implies a 
approximation for  (by Corollary~\ref{cor:approx}). First note that for
doubling metrics, there exists a -stretch spanner with  edges that can
be constructed in time  \cite{GaoGuiNgu04, GoRo08b}. Given this
spanner, we can construct the oracle of Mendel and Schwab \cite[Theorem 2(2)]{MeSc09} with
parameter , which yields an -approximate distance oracle of size
 that supports distance queries in  time, with expected construction time
. We construct the oracle in the preprocessing stage, and derive
an -approximation for  -- and therefore for  -- in  time.

\paragraph{Step 2.} 
The second step gives an -approximation to , assuming an -approximation is already known. If , this step is unnecessary and is 
skipped. We therefore assume that .

Recall that the  approximation to  restricts the candidate levels of
 to a range of  levels (Lemma~\ref{lem:level}). The ancestors of
 and  in the top level of this range can be located via a level ancestor query on 
and the desired number of levels below  (assuming that we have recorded for every
node in  its distance from the root). But the task of locating the ancestors of  in
the bottom level of this range is frustrated by the fact that some ancestors of  below 
 may be
compressed (if these nodes are below the lowest ancestral -neighbors of ); these
uncompressed nodes will be ignored by the level ancestor query, which will therefore return an
incorrect level. To solve this problem, we preprocess a -jump tree for  (see
Section~\ref{sec:jump-tree}). A series of -jump queries locate in  time
explicit ancestors of  that are at most  levels below , which will
suffice for our purposes. Call these ancestors  -- By Corollary~\ref{cor:approx}, an
-approximation to  yields an -approximation to 
.

Now, for every node , , (including implicit nodes) let
the neighbor set  contain all explicit nodes that are descendants of  and 's
-neighbors, in  levels below . We preprocess the snowflake embedding
for each non-empty neighborhood, which can be done in total time  (since each explicit node participates in  
neighborhoods). Since , the target dimension of the
snowflake embedding is . Since the aspect ratio of each
neighborhood is  and the embedding has distortion ,
each coordinate can be stored in  bits.
Therefore . It follows from Lemma \ref{lem:bit-dist} (see 
Section~\ref{sec:bit-trick}) that each vector
may be stored in  words, and the embedding distance between two vectors returned in
 time. Squaring the embedding distance gives a  approximation to the true
distance.

It remains only to locate a neighborhood containing both  and , for which it
suffices to locate 's ancestor in the lowest level  above the
candidate range. A pointer to this ancestor can be preprocessed in time  per
node. Given the correct neighborhood, a -approximation for  can be
found in  time, and this yields a -approximation for 
.


\paragraph{Step 3.}
The third step locates  in  time,
under the assumption that a -approximation to  is known.

As in Step 2, the  approximation to  restricts the candidate
levels of  to a range of  levels. The top level of this range 
is found using a level ancestor query, and then a constant number of -jump
queries locate explicit ancestors of  that are at most  levels below
. Call these ancestors , and let their level (or the level of the lower one)
be . Note that 
.

In the preprocessing stage, we find for each explicit node  all nodes in 
levels  through  whose distance to  is . 
For each such pair, we preprocess their lowest ancestral -neighbors, which can all be 
done in time  time per point. Now, given  and , 
their lowest ancestral -neighbors can be located in  time.



\subsection{Dynamic oracle}\label{sec:main-dynamic}

In this section we give a dynamic version of the oracle. We prove the following theorem:
\begin{theorem}\label{thm:dynamic-oracle}
There exists a dynamic -approximate distance oracle with expected  and
worst-case

query time, and size
.
The oracle can be maintained dynamically in 

time per point update.
\end{theorem}

The dynamic oracle is given points  and  as a query, and runs the two backup oracles of
Section~\ref{sec:backup} in the background. Between them, these oracles locate the
lowest ancestral -neighbors of  in

time (Theorems~\ref{thm:backup1} and \ref{thm:backup2}).

The oracle itself searches for the lowest ancestral -neighbors  of . After
locating these nodes, a -jump query can be used to descend in the tree to within a
constant number of levels of the lowest ancestral -neighbors of . As before, The
oracle locates  in three steps, each of which can be implemented in  time: In
the first step we use the probabilistic dynamic tree embedding of Section~\ref{sec:dyn-tree}
to compute a -approximation to  and therefore to . In the second
step we use the probabilistic snowflake embedding of Section~\ref{sec:dyn-snow} to compute
an -approximation to . In the third step we locate .


\paragraph{Step 1.}
The first step provides a -approximation to , which implies a 
approximation to . We utilize the dynamic tree embedding of
Lemma~\ref{lem:tree-embed} with parameter . The probability that the
embedding fails to give the desired distortion  is given as . Since the
backup oracles
run in time , the event of failure does not affect the target expected runtime of
.


\paragraph{Step 2.}
The second step gives a -approximation to , assuming an
-approximation is already known. If , this step is skipped.
We therefore assume that .

Recall that the  approximation to  restricts the candidate levels of
 to a range of  levels (Lemma~\ref{lem:level}). The top level in this range
is provided by an LCA query on the dynamic tree of Section~\ref{sec:dyn-tree}.
Then a series of -jump queries locate in  time
explicit ancestors of  that are at most  levels below , which will
suffice for our purposes. Call these ancestors  -- an -approximation to
 yields an -approximation to .

Similar to what was done before, we preprocess the dynamic snowflake embedding of
Section~\ref{sec:dyn-snow} for each non-empty neighborhood. Our target dimension for the
embedding is , so it follows from Theorem~\ref{thm:snowflake} that the
embedding achieves an -approximation with probability of failure only ,
which does not affect the expected  runtime of the oracle. Since the aspect ratio of
each neighborhood is  and the embedding has distortion , each
coordinate can be stored in  bits. Therefore , and it
follows from Lemma \ref{lem:bit-dist} that each vector may be stored in  words, and
the embedding distance between two vectors returned in  time. Squaring the embedding
distance gives a  approximation to the true distance.

We then locate a neighborhood containing both  and , for which it suffices to
locate 's ancestor in the lowest level  above the candidate range.
A pointer to this ancestor can be recorded dynamically in time  per node
insertion into . Given the correct neighborhood, a -approximation to
 can be found in  time, and this yields a -approximation
to .

\paragraph{Step 3.}
The third step provides a constant factor approximation to  in  time,
under the assumption that we are provided a  approximation to .
The  approximation to  restricts the candidate
levels of  to a range of  levels. We can ascend to the bottom
level of this range via pointers from  and , and these pointers can be maintained
dynamically in  time per insertion into .
The rest of the construction for this step is identical to the third step of the static
oracle, and can be done in  time and space per node insertion
into .

\subsection{Variant constructions.}\label{sec:misc}
Here, we briefly discuss three variant constructions that appear in 
Table~\ref{tab:misc}. We show that these constructions can find the lowest ancestral 
-neighbors of , after which the lowest ancestral -neighbors of  can be 
found easily by using a level ancestor query or a -jump query.

Construction 4 is achieved by first running the -approximate 
oracle of \cite{MeSc09}. As mentioned in Section~\ref{sec:main-static}, this 
approximation restrics the candidate levels of the lowest ancestral -neighbors to 
 levels. Using level ancestor queries, a binary search finds the correct 
level in  query time.

Construction 5 is achieved by running the static construction until the end of Step 2.
At the end of Step 2, the range of candidate levels in , and a binary 
search on this range finds the correct level in  query time.

Construction 9 runs the dynamic construction until the end of Step 2, at which
point the range of candidate levels is reduced to 
. 
A binary search on these levels can be executed using at most  
different -jump trees, resulting in query time
.

\section{Backup oracles}\label{sec:backup}

In this section, we present two dynamic oracles that find the lowest ancestral -neighbors of 
points . The maintenance of both oracles is bounded by the time to maintain a hierarchy. The 
first oracle answers query in time , and the second in time . 
While we have presented these constructions as backup oracles, it should be noted that they are 
contributions of independent interest.

\subsection{Dynamic oracle queries in  time}\label{sec:backup2}

In this section, we give a dynamic oracle that given  and , finds their lowest 
ancestral -neighbors in the hierarchy, thereby deriving a 
-approximation to . We prove the following theorem:

\begin{theorem}\label{thm:backup2}
There exists a dynamic oracle that given  returns a 
-approximation to
 in  time, and supports updates in time
.
\end{theorem}

An overview of the construction is as follows. Given hierarchy tree , we
create a forest of  distinct trees. The difference
between these trees lies solely in their parent-child relationship.
We then show that in at least one of these trees,  and  have their lowest
common ancestor at level , or within  levels of this
level. By Lemma~\ref{lem:level} this level is within  levels of the
lowest ancestral -neighbors of .


\paragraph{Construction.}
We create a forest of distinct trees  in a manner similar to the creation of .
Each tree is built on top of the point hierarchy, so all
trees share the same nodes and tree level sets. However,
we ignore every odd level of the hierarchy, so the
trees of  only have non-odd levels. Each point in
 (for non-odd ) corresponds to a unique node in tree
level  of each tree .

It remains to describe the parent-child assignments for the trees of
. A node  is assigned a single parent
 which covers . Crucially, ties among
candidate parents are not broken arbitrarily (as they were for
tree ). Rather, each tree  possesses a distinct
set of {\em dominant} nodes in each tree level. Given a group of
candidate parent nodes, the dominant node in the group always
takes the child. We stipulate that the distance between dominant
nodes  must be greater than , so that two
dominant nodes cannot vie for the same child. Further, we stipulate
that a node in  is dominant in exactly one tree of .
Clearly, a forest of size  can
obey these stipulations.

The dominance assignment can be implemented as follows: When a
point  is added to hierarchical level , a corresponding
node  is added to  for each . In one
of these trees,  is chosen to be dominant. (Note that by the
packing property of doubling spaces, there must be at least one
tree in which  is not within distance  of any other
dominant node in the same level.) In each tree ,
 is assigned as a child of the dominant node in  that
covers , or of an arbitrary node of  if there is no dominant one.
Note that once a parent-child assignment is made, the assignment
cannot be reversed. Hence, a newly inserted dominant node does
not become the parent of previously inserted nodes that it covers.
(A reassignment would necessitate a cut-link operation on the tree,
which is not supported by either \cite{CoGo06} or \cite{CoHa05}.)
The entire forest  can be maintain in time  per node
insertion into .
The distance between  and its ancestor  is less than
.

\paragraph{Oracle query}
Let  and  be two points such that
.
We prove the following lemma:

\begin{lemma}\label{L-bad-approximation}
\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{enumerate}
\item For all , the LCA of  and  in 
is in tree level  or higher.

\item There exists at least one  for which the
LCA of  and  in  is in level  or lower.
\end{enumerate}
\end{lemma}

\begin{proof}
\noindent (i) Consider an arbitrary tree , and
nodes  and , the respective ancestors of  and  in
. We have that .
We further have that
.
Note that for ,
(or equivalently, ) we have that
, and  and  cannot be
siblings. Hence, the LCA of  and  cannot be found in level
 or lower and can be found in level  or higher.

\smallskip
\noindent (ii) Consider an arbitrary tree , and
nodes  and , the respective ancestors of  and  in
tree level . Assume that  was inserted before . 
There must exist some covering point  for which
.
If there exist more than one point satisfying this condition, let
 be the first inserted point satisfying the condition. Also recall that
.
We have that
.
Now let  denote the tree in which  is dominant, and let nodes
 and  be the respective ancestors of  and  in .
Note that for values  (or equivalently, for values ), we have that , and so
 and  are both children of  in  (or are in fact
the same point). This implies that  and  must be descendants
of . Hence,  and  must have a common ancestor in level
 or below.
\end{proof}

The query proceeds by executing an LCA query for  and  in each tree of . We
select the lowest node among the ancestors returned from these LCA queries, say .
By Lemma~\ref{lem:level}, this level is within a constant number of levels of the 
lowest ancestral -neighbors of .
Given , the ancestors of  in  can be located in  time, and a
-jump query then locates nodes within a constant number of levels of the lowest
ancestral -neighbors of .


\subsection{Dynamic oracle queries in  time}\label{sec:backup1}

In this section, we give a dynamic oracle that given  and , finds their lowest 
ancestral -neighbors in the hierarchy, thereby deriving a 
-approximation to . We prove the following theorem:

\begin{theorem}\label{thm:backup1}
There exists a dynamic oracle that given  returns a 
-approximation to
 in  time, and supports updates in time
.
\end{theorem}

We begin by presenting a solution for the static version of the problem
and later show how to adapt this solution to the dynamic environment. 
We will make use of the point set  and tree .

\paragraph{Static construction.}
Recall that given  and  it is sufficient to find the lowest
ancestral -neighbors of  and  in order to answer the query. 
This problem could be solved by a simple traversal, in parallel, on the 
paths in  upwards from  and . At each level we check 
whether , the ancestor of , and , the
ancestor of , are -neighbors, and the first encountered -neighbors are
the lowest ancestral -neighbors. (Note though that some
ancestors may not be explicit in certain levels.)
This method may require  time.

To improve this runtime, we note that the hereditary property, 
Property \ref{prop:hereditary} implies that a binary search can
be used. This binary search can be implemented via
level ancestor queries on  (see Section~\ref{sec:prelim}),
and reduces the query time to .

To further improve the query time we use a centroid path decomposition 
of . A {\em centroid path decomposition} partitions the tree  into a
collection of {\em centroid paths} in the following way. The size of a
node  () is the number of nodes in the subtree rooted at . Each
centroid path has an associated power of 2, say , and all nodes on
the path have size . A node  is on the same
centroid path as its parent if their sizes are both between  and
 for some .

A well-known property of centroid path decompositions is
that for any node , the path from  to the tree root
traverses at most  centroid-paths (along their prefixes). To
utilize this we create a centroid path tree that contains a node for each
centroid path. The centroid path tree has an edge from centroid-path-node  to
centroid-path-node  if , the head of the path , is a child (in
) of a node on . It follows from the path-decomposition property
that the height of the centroid-path tree is .

To speed up the queries we first perform a binary search along the path from -to-root 
considering only the  heads of the centroid paths on the -to-root path. 
This is done by using the centroid path tree and level ancestor queries on the centroid 
path tree. The nodes evaluated are compared with to counterparts (in the same level) in 
the -to-root path in , to see if they are -neighbors. The node on the 
-to-root path on the appropriate level can be found using a level ancestor query (in 
the tree ). This search determines which pair of centroid paths (one overlapping the 
path of -to-root and one overlapping the path of -to-root) contains the nodes that 
constitute the lowest ancestral -neighbors. However, these paths themselves may be of 
size . Therefore, we preprocess the following information: We create a {\em 
centroid path graph} with the same node-set as the centroid path tree and an edge 
between two centroid-path-nodes if their paths contain any nodes that are -neighbors. 
The edges are weighted with the lowest level on which there exist -neighbors on these 
paths. Trivially, the centroid path graph is not larger than , and can be
preprocessed in the same time. Once this graph exists, the extraction of the 
lowest ancestral -neighbors is immediate.

\paragraph{Static query time.}
The time to binary search the centroid path tree is  as the height of any path (in the centroid path tree) is . Note that although we binary search on both paths, these searches are
done one after the other and, therefore, the time is still . Once the two centroid paths that contain the lowest ancestral
-neighbors are found then we in  we can obtain the lowest
ancestral -neighbors because of the preprocessing.

\paragraph{Dynamic construction.}
Now consider the dynamic version of the query problem. A dynamic version
of the above search encounters the following problems
(1) level ancestor queries are not supported in this setting, and
(2) the centroid paths, centroid path tree and graph must be maintained.

Recall that the level ancestor query was used twice, upon  and upon
the centroid path tree. We will show how to remove the query on  and how to
circumvent the level ancestor query upon the centroid path tree.

First, we consider the problem of a dynamic centroid path decomposition.
We will use the method from \cite{CoGo06,CoHa05,KoLe07}.
The general idea of the method is a lazy approach
achieved by changing the size constraints of the centroid paths to have
nodes of size between  and . This gives the necessary
time to (lazily) update the centroid path decomposition with worst case
 time per change.

Consider the centroid path tree. Define a directed edge from a leaf to an
ancestor to be an {\em ancestor edge}. We change the centroid path as
follows. The node set, i.e. a node per centroid path, remains the same.
However, the edge set is changed to be the collection of all ancestor
edges. We note that it follows directly from the lazy approach method for
the centroid paths that maintaining the ancestor edges under the dynamic
changes is possible with the same lazy approach. Hence each update can be
implemented in  time. Unfortunately, the number of edges in the
centroid path tree blows up to  instead of the original .
However, this can be corrected by binarizing the tree  and using
indirection on the tree in a method described in \cite{CoHa05,KoLe07}. The
idea follows along the following lines.

The tree  is partitioned into a collection of trees  of size
 such that every node of  is in  and an edge of  is
in  if it connects two nodes in the same tree in . The property of
this partition is that each tree in  has at most two other children
trees of . A skeleton tree  containing the roots of the
-trees as nodes and children-parent edges according to the  tree
relationship are created. See \cite[Section 6]{KoLe07}, for
details of this skeleton tree and its dynamic handling. Obviously the
size of the skeleton tree is . We will use a centroid path
decomposition on the skeleton tree and create accordingly a centroid path
tree. The centroid path tree can now handle the dynamic changes and
searches and maintain a size of .

A change needs to be made to the
centroid path graph as well. Note that the centroid path graph, as opposed to
the centroid path decomposition and centroid path tree, is unique to this problem.
Beforehand, two centroid paths had an edge
between them if there was a -neighbor pair. We slightly change this
definition such that two nodes (both in the skeleton tree) will be {\em
-pseudo-neighbors} if one of them is a -neighbor of a node in the 
tree of the other. In the centroid path graph two centroid paths will be
neighbors if there are a pair of nodes that are -pseudo-neighbors. The
weight of the edge, similar to before, will be the level of the lowest
level for which we have a pair of -pseudo-neighbors (the level is defined according to the node
with the lower level).

Finally, we need to replace the level ancestor query which we used
upon . This query was done when we had an ancestor of  which was the
head of a centroid path  on some level, say , and we needed to find
its counterpart, i.e. the ancestor of  on level , to see if they are
-neighbors. The replacement will be a binary search on the path from
 to root in  along the heads of the centroid paths. This is done
until we are in the position where we have two centroid paths  and
 on the -to-root path, where  is the son of  in the
centroid path tree and where the level of the head of the path of  is
 and the level of the head of  is . It can be verified
that the counterpart of the ancestor of  is in a  tree whose root
is on the centroid path  and hence if the ancestor of  and it's counterpart are -neighbors then
the ancestor of  and the root of the  tree (containing the counterpart) are -pseudo-neighbors.
Hence, there is an edge  in the centroid path graph. Conversely,
if there is an edge  because the level of the head of  is
lower than the head of  it follows from the hereditary property that
the mentioned ancestor of  and its counterpart must be -neighbors.
Finding the lowest ancestral -neighbors is done by finding the lowest
pair of nodes (which are  tree roots that are -pseudo-neighbors). Then
one needs to extract the appropriate node from the  tree of one which
is on the level of the root of the other. This can be done with a simple
scan in the -tree.

\paragraph{Dynamic query time.}
A binary search on the path of  can be done in  as in the static
case. However, for each step in the binary search on the path of , we must
execute a binary search over the path of , in order to locate the ancestor of  in
the correct level. Now, there is the additional step of moving from -pseudo-neighbors to
-neighbors in order to find the lowest ancestral -neighbors may cost
 time because of the size of the  tree. However, if we
recurse the above-described method partitioning each of the  trees
then we will have small- trees of size  and extracting
the appropriate node will take only another  steps.




\ignore{
\section{Oracle for -spanner}\label{sec-t-spanner}

Here we consider the construction of a (static) oracle for a point set 
in a graph. More precisely, the points set  lies in an ambient space
 of doubling dimension , where the function
 defines the {\em ambient distance}. The input is further
extended with a set of edges , and the length of an edge connecting
 is defined to be . The edge set  implies a graph
space , where the {\em graph distance}  is
defined to be the shortest path distance between  and  in the graph.
We are given that the input graph is a -stretch spanner for :
.

We show how to construct an oracle for the graph space . Let
us ignore for the moment the edge set  and focus on the ambient space
. Using the techniques developed above, we build a
hierarchy and  approximate distance oracle for the
ambient space . (Here, .) A distance query on point  and  returns a pair of
hierarchical points  in level  with the property that they
are ancestors of  and  in the hierarchy, and that they are minimal
in the sense that no lower ancestors of  and  are -neighbors
(Theorem \ref{T-spanner}).

Now we turn to the graph space . We prove
the following lemma:

\begin{lemma}
 is a  approximation of .
\end{lemma}

\begin{proof}
First note that

and also

Further, the minimality of the pair  and  implies that their
children are not -neighbors in the ambient space. Hence,
.
Noting that
 completes the proof.
\end{proof}

We however do not know the value of . To determine
, we will precompute the graph distances between all
hierarchical points that are ambient -neighbors, or rather graph
-neighbors. It suffices to run Dijkstra's algorithm on each
hierarchical point , and terminate when the radius of the
cloud is . A naive analysis gives a run time bound of  to discover all -neighbors (worst-case run time  for each of  searches). A more careful analysis notes
that a point  is touched by at most 
searches of level , since each search is rooted at a point of
 and terminates at ambient distance . Hence, a level can be
searched in  time, and all levels in  time. }




\section{Technical contributions}\label{sec:technical}

In this section we present technical constructions utilized by the distance
oracles.


\subsection{Euclidean distance oracle}\label{sec:bit-trick}
The following lemma utilizes atomic word operations to find the exact distance
between (sparse) Euclidean points in  time.

\begin{lemma}\label{lem:bit-dist}
Let  be a dynamic set of -dimensional vectors, where each coordinate is a
-bit number. If , then there exists a
vector representation of points in  that
\begin{enumerate}
\item
Constructs each vector of  words in  time.
\item
Allows the  distance between any point pair  to be computed
in  time.
\end{enumerate}
\end{lemma}
\begin{proof}
Let  be the -th coordinate of -dimensional point , and
recall that the  distance between two points  is
defined as
.
It suffices to show
that there exists a vector representation for all points 
that occupies  words per point and
allows the sum  to be computed in  time.

Assume without loss of generality that  is a power of 2, and
for the sake of simplicity, assume that , so that all operations below can be done on a
single word. We pad each vector with  additional coordinates (each of  bits all set
to ), resulting in -dimensional vectors.

For each point , we create two vectors  and . Vector
 is constructed as follows. Every coordinate of  is
stored at the rightmost position of a range of  bits, with coordinate
 stored in bits  for all 
(numbered from the right end as usual), with all unused bits set to . Vector  is constructed as
follows. Every coordinate in  is stored in the
rightmost position in a range of  bits, with coordinate  stored
in bits , with all unused bits set to .

Now take points , and compute in  time the
product . Note that for all ,  is found in
 consecutive bits beginning at position  of . Set to 
all bits of  that do not correspond to a product , that is
all bits not in the range  for all . (This can be done using bitwise AND with a fixed number.) We are left
with vector  that contains exactly one copy of each product .
It remains only to sum these entries in  time.

To this end, let  be a vector that has a
 in the -th bit for every 
and  elsewhere. Let . The sum of the entries of  is
found in  bits beginning at position  of .
\end{proof}

\subsection{Dynamic jump tree}\label{sec:jump-tree}

In this section, we will describe a dynamic structure that supports jump queries.
The compressed hierarchy tree  was described in Section~\ref{sec:backbone}. We now describe
-jump queries on the tree .

\begin{definition}
A -jump query  on compressed hierarchy tree  provides two explicit tree
nodes,  and its ancestor . Let  be the largest value less
than  which is a multiple of . The query requests the node
 that is ancestral to ; if  is implicit then its lowest
explicit ancestor is requested instead.
\end{definition}

The existence of a dynamic structure supporting jump queries would allow us to descend 
via jumps. 

\begin{lemma}\label{lem:jump}
For fixed , a structure that supports -jump queries of hierarchy tree  can
be maintained along with  work per insertion to  and  space.
\end{lemma}

Before presenting a proof of Lemma~\ref{lem:jump}, we first need a preliminary lemma
that extends the dynamic LCA structure of Cole and Hariharan \cite{CoHa05}.

\begin{lemma}\label{lem:lca-child}
For any tree , there exists an LCA query structure that supports insertion of
leaves and internal nodes to  in  time, and answers the following query in
 time: given nodes , return  as well as the
children  of  that are the respective ancestors of  and .
\end{lemma}

\begin{proof}
Given tree , we create a new tree  as follows. Let  be the root of 
and let  be 's ordered children. The root of the tree  is .
's left child is , and for all nodes  we have that 
is the right child of . We then recursively build the subtrees of each child
node . This tree can be maintained in  time for each update to .
Now consider nodes  that have , and consider the nodes
 that are children of  and the respective ancestors of .
Assume that  precedes  in the ordering of the children of . Then by
construction, an LCA query on  returns .

It remains to identify . To this end, we create tree  as follows:
The root of the tree  is . 's left child is , and for
 we have that  is the right child of . We then
recursively create the subtrees of 's children .
Now consider nodes  mentioned above. By construction, an LCA query
on  returns .
\end{proof}

We can now proceed in the proof of Lemma~\ref{lem:jump}.

\begin{proof}
Let tree  preserve every -th level of . We build  from  as follows: Level
 contains a copy of every uncompressed node of level ,
. Further,  contains a copy of every compressed node  in  that has its lowest uncompressed ancestor  in some level below , and  is given a pointer to . This can easily be done in  time per
tree update. (Note that the compression scheme implies that  is the only descendant of
 in level .) The ancestor-descendant relationship in  is defined by
the anscestor-descendant relationship in .

Now given a -jump query for nodes , we first locate the lowest respective
ancestors  of  whose tree level is divisible by . (This information can be
maintained for each node in  time.) The LCA query of Lemma~\ref{lem:lca-child} on
, where  is a child of  which is not an ancestor of , returns  as well as the child  of .  (or if it
is compressed, its lowest uncompressed ancestor) is the desired node.
\end{proof}



\subsection{Dynamic embeddings}\label{sec:dyn-embed}

Here we present two randomized dynamic embeddings for an -point metric space 
with doubling dimension . Both embeddings store  interpoint distances
and each can be maintained in time
 per update (where 
is a parameter specific to each embedding).
\begin{itemize}
\item
The first embedding is into a tree metric, with . Let  be the target space
of the embedding. Given two points , we show that  (that is,
the embedding is non-contractive), and that  with
probability at most  (for any positive integer ).
\item
The second embedding is a snowflake embedding into , with . Let  be the
target space of the embedding. Given two points , we show that
 (that is, the embedding is non-expansive to
the snowflake), and that 
with high probability.
\end{itemize}

Both embeddings are build upon the hierarchy of \cite{CoGo06}, after a new
assignment of parent-child relationships to the hierarchical points.

\paragraph{Parent-child assignment.}
We restrict ourselves to consider each -th level in
the hierarchy. (For ease of presentation, we
will henceforth assume that  is a power of 5.) With regards to this
{\em restricted hierarchy}, a repeated application of the covering property
gives that every point in level  is within
distance  of some point in level , and this
constitutes the covering property for the restricted hierarchy.

Let  be a newly inserted point occurrence in the hierarchy.
As in \cite{AbBaNe08}, we associate with  a radius
, where  is a random
variable sampled from a truncated exponential density function:
The density function is 
with parameter  when
, and is  elsewhere. (This is the construction presented
in \cite{AbBaNe08} with parameter .)
Then  is the parent of all subsequently inserted point occurrences in level
 within distance  of , unless those points are within
the radius  of a point  that was inserted before .
This defines the parent-child relationship in the restricted hierarchy.

The hierarchy stores  interpoint distances, and can be maintained in
 update
time.



\subsubsection{Tree embedding}\label{sec:dyn-tree}

Here, we present a dynamic embedding of  into a tree metric. We use
the hierarchy and parent-child relationships delineated above, with
. We extract a randomized tree from the hierarchy as follows:
For each point occurrence in the restricted hierarchy, there exists a
single corresponding node in the tree. Hence, the parent-child
relationship among the restricted hierarchical points
immediately defines a parent-child relationship in the corresponding tree,
where an edge connect a parent to its child.
From the randomized tree, we extract a tree metric  by
assigning a length to each edge: An edge rooted at level  is
assigned length . We have the following lemma:

\begin{lemma}\label{lem:tree-embed}
For any two points  and positive integer , where ,
\begin{itemize}
\item
.
\item
.
\end{itemize}
\end{lemma}

\begin{proof}
Consider any two points , or equivalently the corresponding point occurrences
. We first show that the tree embedding is non-contractive: If  and 
have their least common ancestor in level  of the tree, then by construction
, while
.
Hence, the embedding is non-contractive.

Next, we derive a probabilistic upper bound on the expansion of the embedding: Let
. Then the true distance between the hierarchical ancestors
of  in level , , of the restricted hierarchy is less than
.
By the covering property of the restricted hierarchy,  is covered by some point
 for which , and so a simple
computation gives 's covering point  also falls within the radius
 of :
.
Now, the probability that the respective ancestors of  and  in level 
do not share the same parent is bounded by

\cite{AbBaNe08}. Hence, the probability that  and  have their lowest common
ancestor at
level  is bounded by , in which case
.
\end{proof}


\subsubsection{Snowflake embedding}\label{sec:dyn-snow}

In this section we give a dynamic Assouad style embedding \cite{Assouad83}, in which
for a given metric space  we embed the snowflake  of the metric
into  space. Our theorem can be viewed as a dynamic version of the theorem
of \cite{GuKrLe03,AbBaNe08}. (A similar embedding holds for  with  and for general target space .) For simplicity we focus on the
probabilistic version of the theorem which bounds the distortion with constant
probability.

\begin{theorem}\label{thm:snowflake}
For any  point metric space  with doubling dimension , there
exists a non-expansive probabilistic embedding ,
, that realizes the snowflake : For every pair
:

Moreover, this construction can be computed dynamically with storage of  interpoint
distances and  update time.
\end{theorem}

Our embedding uses the same hierarchy and parent-child relationship presented above,
with . Let -cluster  be composed of all descendants of , and call  the center of this cluster. It follows that each point is
found in  clusters, one cluster for each level of the hierarchy. Let
 denote the -cluster containing .

As usual for the construction of snowflake embeddings, we shall construct the
embedding function  by defining for each integer  a function
, and then letting
.
Fix , , and in what follows we will define :
For each restricted hierarchical level  we define a
function , and for each point
, let .
Let  be i.i.d.\ symmetric
-valued Bernoulli random variables.
Let .
The embedding is defined as follows: for each ,
\begin{itemize}
\item For each , let ,
\end{itemize}
where  is a function which computes the distance from 
to the boundary of . This can be computed as follows. Let
 be the center of  and let  be the set of
-cluster centers within distance  of  which were
inserted into  before the insertion of . For  in 
let  denote its associated radius. Then:
\begin{itemize}
\item .
\end{itemize}
The function  replaces the
expression  used in embedding of
\cite{AbBaNe08}. (Note that  is not affected by the insertion of new points
into the hierarchy, and can be evaluated in time .) The following
properties are needed to show that it can be replaced in their analysis:

\begin{claim}
\label{claim:distance-to-boundary}
For every :

\begin{itemize}
\item If  then .
\item If  then .
\item  with constant probability.
\end{itemize}
\end{claim}
\begin{proof}
\begin{itemize}
\item To prove the first claim is clear from assume that  is
minimized for some , then: . If  is
minimized for  a similar argument applies. Similarly, .

\item We prove the second claim by contrary assumption that . It follows that  which
implies that . Also for each
, we have  which implies
that  but together these
inequalities imply that  which is a contradiction.

\item As a consequence of the analysis of \cite{AbBaNe08},
we have with constant probability that  and also
for every , . It follows that
 with constant probability.
\end{itemize}
\end{proof}

Given Claim~\ref{claim:distance-to-boundary} the analysis of
\cite{AbBaNe08} implies the following:

\begin{lemma}
\label{lemma:embedding-upper-assouad} For any  and
:

\end{lemma}

\begin{lemma}
\label{lemma:embedding-lower-assouad} For any , with
probability at least :

\end{lemma}
\begin{proof}
It follows from the Assouad-type argument that with probability at
least :

The lemma follows from applying a Chernoff bound.
\end{proof}

The theorem follows from an appropriate scaling of the embedding
so to achieve a contractive embedding with the required
properties.


\paragraph{Acknowledgements.} We thank Richard Cole, Robi Krauthgamer,
Manor Mendel and Michiel Smid for helpful conversations.

\bibliographystyle{plain}
\bibliography{oracle-dd}




\end{document}
