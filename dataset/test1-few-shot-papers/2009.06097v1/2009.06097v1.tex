

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{booktabs} 
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amsfonts}       \usepackage{nicefrac}
\usepackage{comment}
\usepackage{enumitem}
\aclfinalcopy 



\usepackage{mathtools}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\vars}{\texttt}
\newcommand{\func}{\textrm}

\newcommand\BibTeX{B\textsc{ib}\TeX}

\newcommand{\edit}[1]{{\color{Red}{#1}}}
\newcommand{\head}[1]{\noindent\textbf{#1}}


\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\wrt}{\textit{w.r.t. \xspace}}
\newcommand{\defeq}{\vcentcolon=}

\title{Cluster-Former: Clustering-based Sparse Transformer \\
for Long-Range Dependency Encoding}
\author{Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen,\\  \textbf{Yuwei Fang}, \textbf{Siqi Sun}, \textbf{Yu Cheng}, \textbf{Jingjing Liu} \\
  Microsoft Dynamics 365 AI Research \\
  \small{ \texttt{\{shuowa,luowei.zhou,zhe.gan,yen-chun.chen\}@microsoft.com} }\\
  \small{ \texttt{\{yuwfan,siqi.sun,yu.cheng,jingjl\}@microsoft.com} }}

\date{}

\newcommand{\jj}[1]{\textcolor{blue}{\small{\bf [JJ: #1 ]}}}
\newcommand{\shuohang}[1]{\textcolor{cyan}{\small{\bf [Shuohang: #1 ]}}}
\newcommand{\luowei}[1]{\textcolor{green}{\small{\bf [Luowei: #1 ]}}}
\newcommand{\yenchun}[1]{\textcolor{red}{\small{\bf [Yen-Chun: #1 ]}}}

\begin{document}
\maketitle
\begin{abstract}
Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. 
However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically
\textit{w.r.t.} the sequence length.
Therefore, long sequences are often encoded by Transformer in chunks using a sliding window.
In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences.
Our proposed method allows information integration beyond local windows, which is especially beneficial for question answering (QA) and language modeling tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.

\end{abstract}



\section{Introduction}
Long-range contextual understanding is critical for many natural language processing (NLP) tasks.
For example, the relevant information needed to correctly answer an open-domain question or generate the next word (\emph{i.e.}, language modeling) can arch over thousands of words. 
Due to limitations on time and GPU memory, encoding long sequences through neural networks is challenging and expensive.
Traditional sequence modeling methods~\cite{hochreiter1997long} encode long sequences in a chronological order, hence suffering from the high latency issue. On the other hand, recent models such as Transformer~\cite{transformer} avoid sequential encoding via simultaneous self-attention over the entire input, and have been successfully adopted in many NLP tasks such as textual entailment~\cite{bert}, dependency parsing~\cite{zhou2019head}, and summarization~\cite{bart}. A caveat with Transformer is that building full connections over long sequences leads to quadratic growth on memory demand and computational complexity with respect to the sequence length.



\begin{figure*}[t!]
\centering
\includegraphics[width=6.3in]{cformer.png}
\caption{Illustration of different methods for processing long sequences. Each square represents a hidden state. The black-dotted boxes are Transformer layers. (a) is the sliding-window-based method to chunk a long sequence into short ones with window size 3 and stride 2. (b) builds cross-sequence attention based on sliding window over pre-selected positions (red-dotted boxes). (c) hashes the hidden states into different buckets by randomly-initialized vectors. (d) is our proposed approach to cluster the hidden states. Our final model is a combination of (a) and (d) that processes both local and global context. }
\label{fig:comp}
\end{figure*} 

To process long sequences, a widely adopted solution is to first chunk a sequence into much shorter ones with a sliding window, then build connections between the shorter sequences.
For example, \citet{sparsetransf}, \citet{longformer} and \citet{zaheer2020big} apply sparse attention to chunked sequences in hand-designed patterns in order to gather information from the chunks.
\citet{choi2017coarse} and \citet{wang2019multi} first use a simpler model to filter chunked sequences, then process selected sequences with fully-connected self-attention.
\citet{rae2019compressive} makes use of the shared memory of chunked sequences to build connections between them.
However, the above
methods cannot encode long-range dependencies with as much flexibility or accuracy as fully-connected self-attention, due to the dependency on hand-designed patterns or the lack of critical information.

Recently, several works~\cite{reformer,tay2020sparse} have proposed to further improve the sparse attention mechanism by hashing or sorting the hidden states into different buckets.
However, these works mainly explore tasks with relatively short sequences, such as sentence-level Machine Translation (MT), where the number of hashing vectors is relatively small (less than 16 in~\citet{reformer}) and randomly initialized hashing vectors are good enough to hash hidden states into correct buckets.
In this paper, we further explore the potential of hashing-based attention
in the context of long sequences (thousands of words).



Our proposed framework for processing long sequences combines the benefits of both sliding-window and hashing-based methods on local and long-range dependency encoding.
It consists of two types of encoding layer.
The first one (noted as a \textit{Sliding-Window Layer}) focuses on local information within a sliding window. It applies Transformer to the hidden states of each chunked sequence independently, as shown in Figure~\ref{fig:comp}(a).
The other one (noted as a \textit{Cluster-Former Layer}) encodes global information beyond the initial chunked sequences.
Specifically, we first apply clustering to the input hidden states
so that similar hidden states are assigned to the same cluster, as shown in Figure~\ref{fig:comp}(d). The clustered and sorted input is then divided uniformly into chunks, each encoded by a Transformer layer. 
Note that to make model training more efficient, the cluster centroids are not computed online but updated periodically (every epoch or a few epochs).
We accumulate the hidden states from the layer prior to the Cluster-Former layer in a memory bank, and apply the K-Means algorithm to form cluster centroids during each update cycle.
Compared to previously discussed sparse attention based on pre-selected positions (Figure~\ref{fig:comp}(b)) or randomly-initialized hashing vectors (Figure~\ref{fig:comp}(c)), experimental results show that our method can encode dependency across chunked sequences more effectively.


Our contributions can be summarized as follows.
() We propose Cluster-Former, a novel approach to capturing long-range dependencies more effectively than the locality-sensitive hashing method.
() We propose a new Transformer-based framework to process long sequences by combining Sliding-Window and Cluster-Former layers to extract both local and global contextual information. 
() Our model achieves the best performance on question answering datasets of Natural Questions (long answer), SearchQA, and Quasar-T. 
() We provide fair comparison between different methods on multiple language modeling tasks, and demonstrate that our clustering-based method makes use of contextual information beyond sliding windows effectively.
 
\section{Related Work}
\paragraph{Long Sequence in Language Modeling}
Language modeling is one of the benchmark tasks to test models' ability on handling long sequences.
As words from the same long article are likely related, a model should have the ability to detect long-range dependencies for sequence generation.
\citet{sundermeyer2012lstm} first used LSTM to address long-range dependencies beyond N-grams.
With the availability of more computational resources, more complex models are proposed to encode long sequences. 
\citet{merity2016pointer} released the WikiText dataset that composes of full Wiki articles and facilitates the study of long context modeling. They proposed to use self-attention mechanism for encoding long-range dependencies.
\citet{grave2016improving} proposed to save a long range of hidden states in continuous cache, which can be used for next word generation later on.

Recently, most of the best-performing models are based on Transformer~\cite{transformer}.
\citet{rae2019compressive} proposed to make use of the compressed memory from Transformer for long-range sequence modeling.
To enable the Transformer itself to handle long sequences without additional memory or continuous cache, \citet{sparsetransf,longformer} introduced Sparse Attention, where attention is applied to sparse pre-selected positions across sliding windows to encode long-range dependencies.
\citet{reformer} proposed to re-order the hidden states by hashing similar states into the same buckets, and then apply Sparse Attention across the buckets.
Our proposed method is in line with this direction. 
However, instead of using randomly initialized hashing vectors, we propose to cluster the hidden states based on the cluster centroids from our hidden-state memory bank, which leads to better performance on language modeling.


\paragraph{Long Sequence in Question Answering}
For tasks such as open-domain question answering~\cite{chen2017reading}, a large volume of documents or paragraphs are usually retrieved to infer the answer, yielding extremely long context content.
Despite that state-of-the-art NLP models are capable of extracting answers amid complex context,
they still struggle with extremely long input sequence.
Recent advances that advocate the use of large-scale pre-trained models~\citep{bart,roberta,lan2019albert} for question answering
make this problem more prominent, due to tremendous memory consumption.
Therefore, to process a long sequence, the most widely-used method is to first use a lightweight model to filter out redundant text, and then use sliding-window-based approaches to encode the remaining sequences with a more sophisticated model.
\citet{chen2017reading} integrated bi-gram features into Information Retrieval (IR) methods to retrieve the related documents more accurately.
\citet{wang2018r} trained a paragraph selector using whether the entire system can obtain the correct answer or not as the reward.
\citet{lin2018denoising} proposed to use a paragraph ranking model to curate data that are required for training reading comprehension models.
\citet{wang2019multi} trained a ranker to merge paragraphs for multi-passage reasoning.
\citet{asai2019learning} trained a recurrent retriever to select paragraphs for multi-hop question answering.
However, all these paragraph ranking or filtering methods may risk losing important information for question answering.
In this paper, we focus on directly training a large model on long sequences without any intermediate method for text filtering. 



\begin{comment}
\paragraph{Transformer architecture optimization}
There are two popular ways  to optimize the structure of Transformer. 
One is to optimize the self-attention mechanism.
The other is to compress the large pre-trained model into smaller one by knowledge distillation.


Even though some NLP tasks may have longer context to process, they still rely heavily on the local information.
In this way, our model consists of two strategies, which focus on the local information in the sequence windows nearby and global information beyond the nearby windows respectively, to encode long sequence in different Transformer layers.
For the first strategy, after chunking the long sequence by sliding window, we will not encode the sequence in window sides independently, but merge the overlap of hidden states in the same positions as the input of the next layer.
Thus the information from nearby sequences in windows can always be shared.
For the second strategy, we will cluster the hidden states and run Transformer layer on each cluster separately, so that each hidden state may be enriched by the similar information beyond nearby windows.
Note that our cluster centers are not online computed for each sequence, but based on the hidden states in the memory bank of each layer.
The method is close to Reformer~\citet{reformer} by replacing the random hashing vectors with our cluster centers in memory bank.
However, as we are processing longer sequence than Reformer~\citet{reformer}, we need more clusters to assign the hidden state more accurately.
And we will greedily sort the cluster centers by the nearest neighbour, so that the hidden states in nearby buckets can compensate each other for making the sequences in the same size.
\end{comment} \section{Model}
\begin{figure*}[t]
\centering
\includegraphics[width=6.2in]{kmatt.png}
\caption{An overview of two types of Transformer layer. (a): Sliding-window layer over a sequence. The question is omitted here for simplicity. (b) Cluster-Former layer over clustered hidden states from the output of (a). The cluster centroids are periodically updated based on the memory bank of the hidden states in the corresponding layer. Note that the sequence inputs in (a) and (b) usually come from two different samples.}
\label{fig:model}
\end{figure*}
The proposed framework to handle long sequences is centered on two types of Transformer layers: () Sliding-Window Layer, and () Cluster-Former Layer. 
The former layer focuses on encoding local sequence information, while the latter is on encoding global context and always built on top of the former layer.
An overview of the two layers is illustrated in Figure~\ref{fig:model}.

\subsection{Sliding-Window Layer}
Despite that our focus is on capturing long-range dependencies for global context, local information also plays a critical role for knowledge propagation.
Therefore, in the lower part of our network, we adopt the traditional sliding-window encoding mechanism.
A sliding window segments a long sequence  into short, overlapping ones with window size  and stride , as illustrated in Figure~\ref{fig:model}(a). 
Note that for question answering tasks, we concatenate the question  with each sequence chunked from the document ( is not applicable in language modeling tasks). Then, we have

where  denotes question embeddings given a QA task,  is the number of tokens in the question, and  is the embeddings for all the context.
 is the id of the chunked sequence,  is the window size,  is the stride of the sliding window.
 indicates selecting rows between the index of  and  of the matrix.
 means concatenating the matrices along the row.
As we expect the neighbouring sequences to share useful information in hidden states as well, we always set  to allow overlapping between sequences.
We use the mean values of the Transformer hidden states at the overlapped tokens as their final output.

where  is to add matrices in-place and  is to divide a matrix by a scaler value in-place. 
The output of the -th sequence in the -th layer is  , which merges the hidden states from both the -th and -th sequences.
If the next layer is Cluster-Former, the output hidden states in this layer  will be saved into memory bank for computing the cluster centroids.


\subsection{Cluster-Former Layer}
\label{sec:trans_clusters}
We introduce a new method, Cluster-Former,
to add global representational power to Transformer beyond sliding windows.
An in-depth visualization of the layer is illustrated in Figure~\ref{fig:model}(b).

The input of the Cluster-Former layer comes from the hidden states of the prior layer (in our case a Sliding-Window layer).
After merging the overlaps between sequence chunks, the input of this layer is defined as:

where  is the hidden states to cluster,  is the number of tokens in the context.

As the hidden states with larger cosine similarity are more likely to have higher attention weights, we build sparse self-attention only on the hidden states in the same cluster.
In this work, we use K-Means as the chosen clustering method for simplicity. More advanced clustering algorithms have the potential of yielding better performance.
Since running K-Means on the fly in each training iteration is computationally expensive, we decide to re-compute the cluster centroids with low frequency (every epoch or every few epochs).

\begin{algorithm}[t]
\small
\caption{Cluster Centroids Update}
\label{alg:alg}
\begin{algorithmic}[1]
\State Initialize  Queue()
\State  \\


\Function{train}{}
  \For{ 1, 2,\ldots, IterationNum}
  \State  \func{Sliding-Transformer}([])
\State \vars{Memory}.add(\vars{States})
  \While{len(\vars{Memory})  M}
  \State \vars{Memory}.pop()
  \EndWhile
\If{ \% ClusterUpdateFrequency  0} \;
\State )
  \EndIf
  \State  cluster  by 
  \State )
  \EndFor
\EndFunction \\

\Function{GetCentroids}{}
  \State )
  \State  List()
  \State [1] [1]
 \For{ 2, 3,\ldots, ClusterNum}
\State{}
\EndFor
  \State \Return 
\EndFunction
\end{algorithmic}
\end{algorithm}

Besides, to avoid dramatic changes in the cluster centroids due to limited hidden state inputs, we maintain a memory bank for the most recent hidden states. The entire procedure is depicted in Algorithm~\ref{alg:alg}.
Once we have the cluster centroids, we can directly use them for hidden state clustering as follows:

where  are the cluster centroids for layer , and  is the pre-defined number of clusters.
The function argmax() performs on the last dimension and assigns all the input hidden states into different clusters based on the max value of cosine similarity between the hidden states and cluster centroids.
 is the assigned cluster IDs of all the input hidden states.

As the number of hidden states in different clusters can vary substantially, padding them to the  maximum length to run Transformer will significantly increase the computational time.
To make global context gathering more efficient, we greedily pick the cluster centroids based on the nearest neighbour (measured by cosine similarity) as shown in the function \textsc{GetCentroids} in Algorithm~\ref{alg:alg}.
Thus, the hidden states with similar cluster IDs are also close to each other.
Then, we can directly sort the cluster IDs of hidden states and uniformly chunk the hidden states (same window size and stride ):

where the function argsort() is to obtain the indexes of input values sorted in order (same values sorted by the corresponding position of hidden states).
 is the chunked indexes of the hidden states.
Note that the function  on  is specifically designed for the language modeling task to mask words, and can be ignored for QA tasks.
 is the -th clustered hidden states, and we will run Transformer on top of it to build the connection beyond the words in the initial sliding window as follows:

After updating the hidden states, we will map them back to the order before clustering:

where  is the final output hidden state of this layer and has the same word order as the input .



\begin{comment}
where  is the hidden state in the n-th layer.
 is the norm2 value of each row of the input matrix. 
The function argmax(\cdot) is to obtain the position of the max value in each row.
 are the assigned cluster IDs for all the  hidden states.
The function sort(\cdot) sorts the values of the input vector.
 re-orders the hidden states  based on the index order of .
 is the re-ordered hidden states and will be chunked into shorter sequences as follows:


where  is the window size of each chunk and  is the index of the chunk.
Each chucked sequence will be encoded by Transformer into another Transformer layer . These hidden states will be concatenated and mapped back to the original order before clustering based on the sorted index
.
\end{comment} \section{Experiments}
In this section, we introduce our experimental setting and detailed analysis of results.
\begin{table}[t]
\begin{tabular}{lcccc}
\toprule
                 & \#train & \#test & med  & max  \\
\midrule
Quasar-T         & 29k     & 3k     & 2.8k    & 8.2k    \\
SearchQA         & 100k    & 27k    & 2.5k    & 4.9k    \\
Natural Questions & 292k    & 8k     & 6.3k    & 128k    \\
\bottomrule
\end{tabular}
\caption{Statistics on Question Answering datasets. \#train: number of questions in the training set. \#test: number of questions in the test set. med: median length of the context. max: max length of the context. }
\label{tbl:sts}
\end{table}

\begin{table*}[t]
\centering
\begin{tabular}{lcccc}
\toprule
                           & Quasart-T & SearchQA  & NQ(long) & NQ(short) \\
                           & EM/F1     & EM/F1     & F1       & F1        \\
\midrule                
R3~\cite{wang2018r}                         & 35.3/41.7 & 49.0/55.3 & -        & -         \\
DECAPROP~\cite{tay2018densely} &38.6/46.9 & 62.2/70.8 &- &-\\
DS-QA~\cite{lin2018denoising}                      & 42.2/49.3 & 58.8/64.5 & -        & -         \\
Multi-passage BERT~\cite{wang2019multi}         & 51.1/59.1 & 65.1/70.7 & -        & -         \\
DrQA~\cite{chen2017reading}                       & 37.7/44.5 & 41.9/48.7 & 46.1     & 35.7      \\
DecAtt + DocReader~\cite{kwiatkowski2019natural}         & -         & -         & 54.8     & 31.4      \\
BERT~\cite{alberti2019bert}                  & -         & -         & 64.7     & 52.7      \\
BERT + SQuAD 2~\cite{pan2019frustratingly} & -         & -         & 68.2    & \textbf{57.2}       \\
\midrule
Sliding window   & 52.9/62.8 & 65.8/73.2 & 75.3     & 56.4\\
Sparse Attention~\cite{sparsetransf}               & 52.1/62.0 & 64.7/71.7 & 74.5     & 56.1\\
Locality-Sensitive Hashing~\cite{reformer}       & 53.2/62.9 & 66.0/73.5 & 75.5     & 56.4\\
\midrule
Cluster-Former (\#C=64)        & 53.3/63.3 & 67.0/74.2 & 76.3     & 56.7\\
Cluster-Former (\#C=256)       & 53.6/63.5 & 67.5/74.5 & 76.3     & 56.7\\
Cluster-Former (\#C=512)       & \textbf{54.0}/\textbf{63.9} & \textbf{68.0}/\textbf{75.1} & \textbf{76.5}     & 57.1\\
\bottomrule
\end{tabular}
\caption{Experimental results on Question Answering datasets. \#C: number of clusters.}
\label{tbl:qa}
\end{table*}

\begin{table}[t]
\begin{tabular}{lcc}
\toprule
                           & Wikitext & Enwik8\\
                           & ppl         & bpc    \\
\midrule

Sliding window       & 20.8        & 1.34   \\
Sparse Attention    & 20.5        & 1.29   \\
Locality-Sensitive Hashing   & 20.8        & 1.33   \\
\midrule
Cluster-Former (\#C=64)    & 20.5        & 1.28   \\
Cluster-Former (\#C=256)   & 20.3        & 1.24   \\
Cluster-Former (\#C=512)   & \textbf{20.2}        & \textbf{1.22}  \\
\bottomrule
\end{tabular}
\caption{Experimental results on Language Modeling.  \#C: number of clusters;  Wikitext: Wikitext-103.}
\label{tbl:lm}
\end{table}


\subsection{Datasets}
We evaluate our proposed approach on two main tasks: question answering and language modeling.
For question answering, we use the following datasets, and summarize the statistics in Table~\ref{tbl:sts}.

\vspace{5pt}
\noindent\textbf{Quasar-T\footnote{https://github.com/bdhingra/quasar}}~\cite{dhingra2017quasar}: The goal of this task is to answer open-domain questions from Trivia Challenge. All the passages harvested through information retrieval can be used to answer questions. The task requires the model to generate answers in phrases. The evaluation metric on this dataset is based on Exact Match and F1 score of the bag-of-words matching. Our evaluation tool\footnote{https://rajpurkar.github.io/SQuAD-explorer/} comes from the SQuAD dataset.

\vspace{5pt}
\noindent\textbf{SearchQA\footnote{https://github.com/nyu-dl/dl4ir-searchQA}}~\cite{dunn2017searchqa}: The setting of this dataset is the same as Quasar-T, except that the questions are sourced from Jeopardy!\footnote{http://j-archive.com/} instead.

\vspace{5pt}
\noindent \textbf{Natural Questions\footnote{https://ai.google.com/research/NaturalQuestions}}~\cite{kwiatkowski2019natural}: This task aims to answer questions based on a given Wikipedia document, and has two settings.
() Long answer: select a paragraph that can answer the question based on the Wikipedia document if any.
() Short answer: extract an answer phrase from the document if the document contains the answer.
As the given document may not contain answer, we can either predict an answer or predict no answer. 
The evaluation metric on this dataset is the F1 score, where
true positives are exactly correct answers, false positives are incorrect answer predictions, and false negatives are incorrect ``no answer" predictions.
As the test set is hidden, we split 5\% of the training set for validation, and use the original validation set for testing.
We use the official tool from the dataset to evaluate our models.

To demonstrate Cluster-Former's ability to detect long-range dependencies, we also evaluate on two language modeling tasks:\footnote{For both datasets, we follow the dataset split from https://github.com/salesforce/awd-lstm-lm.}

\vspace{5pt}
\noindent \textbf{Wikitext-103}~\cite{merity2016pointer}: The dataset is extracted from the set of verified \emph{Good and Featured} articles on Wikipedia. We use the dataset to train word-level language modeling and use perplexity as the evaluation metric.

\vspace{5pt}
\noindent \textbf{Enwik8}~\cite{mahoney2011large}: This also comes from Wikipedia. We train character-level language modeling on this dataset and use bit per character (bpc) as the evaluation metric.

\subsection{Implementation Details}
All the models are trained on 8 Nvidia V100 GPUs. 
We use cluster centroids that perform the best on the validation set for test set experiments.
The number of hidden states kept in memory bank is 100K.

\vspace{5pt}
\noindent \textbf{Question Answering}: We initialize our models with RoBERTa-large~\cite{roberta} that has 24 Transformer layers, 16 heads per layer and hidden state dimension of 1024.
As the number of position embeddings of RoBERTa is limited to 512, we cannot assign different position embeddings to all tokens.
Instead, we assign the same position embeddings to each chunked sequence.
The majority of our model is made up of Sliding-Window Layers, as the local information is essential for QA tasks. We adopt the proposed Cluster-Former Layer in the randomly selected layers 15 and 20 to further capture long-range information.
We set the sliding window size  to 256, stride  to 224, and change the number of clusters in \{64, 256, 512\} to analyze its impact on the final performance.
We prepend a special token to the beginning of all the given/retrieved paragraphs and directly concatenate all the paragraphs as the final context sequence.

For Quasar-T and SearchQA, we predict the start and end positions of the answer.
For Natural Question, we first identify whether the question can be answered or not based on the given document, before predicting the answer.
Then, we classify the mean values of the first hidden state of all the chunked sequences to identify whether the question has short or long answers. 
We rank the hidden states of all the special tokens for long answer selection and predict the start and end positions of short answers.
Due to memory constraints, we set the max length to be 5000 during training and 10000 during inference.
During dataset finetuning, we use Adam~\cite{kingma2014adam} to optimize the model. We set warm-up updates to 2,220, maximal updates to 22,200, dropout rate to 0.1, learning rate to , and batch size to 160.
The model will converge in one day for all the QA datasets.

\vspace{5pt}
\noindent \textbf{Language Modeling}: All the models are trained from scratch.
We set the number of layers to 16, with 8 heads per layer.
Our Cluster-Former layer is used in layers 11 and 15. 
We segment long input into short sequences of 3072 tokens, set sliding window size  to 256, and stride  to 128.
SGD is used for optimizing the models.
We set clip threshold of gradients to 0.1, warm-up updates to 16,000, maximal updates to 286,000, dropout rate to 0.3, learning rate to 0.1, and batch size to 16.
The model will converge in 3 days for all the LM datasets.

\subsection{Baseline}
We compare our models with several strong baselines according to their published results.
\begin{itemize}[itemsep=-5pt,topsep=2pt,leftmargin=12pt]
\item \textit{R3}~\cite{wang2018r} proposes to use reinforcement learning to jointly train passage ranker and reader, considering no gold label for the passage.

\item \textit{DS-QA}~\cite{lin2018denoising} proposes to first use paragraph selection to filter the noisy data, so that the final answer extraction model can be trained on denoised data.

\item \textit{Multi-passage BERT}~\cite{wang2019multi} proposes to filter the passages and then merge multiple useful passages into one sequence, which can be encoded by BERT.

\item \textit{DrQA}~\cite{chen2017reading} makes use of attention mechanism across the question and the document for answer phrase extraction.

\item \textit{DecAtt and DocReader}~\cite{kwiatkowski2019natural} is based on a pipeline approach that first uses a simpler model to select long answers and then a reading comprehension model to extract short answers from the long answers.

\item \textit{BERT}~\cite{alberti2019bert} jointly trains short and long answer extraction in a single model rather than using a pipeline approach.

\item \textit{BERT+SQuAD2}~\cite{pan2019frustratingly} makes use of multi-task learning to further boost performance.
\end{itemize}
We also re-implement several strong baselines which have not been explored to process long context in question answering tasks.
To make a fair comparison among different methods on long-range information collection, we replace several layers of the sliding window baseline with Sparse Attention, Locality-Sensitive Hashing and Cluster-Former.

\begin{itemize}[itemsep=-5pt,topsep=2pt,leftmargin=12pt]
\item \textit{Sliding Window}: This method is fully made up of Sliding-Window Layers and can only attend to local information.

\item \textit{Sparse Attention}~\cite{sparsetransf}: This method replaces several layers in the previous baseline by training a Transformer layer across sequences on pre-selected positions. We run this sparse Transformer on all the hidden states in the same position across sequences, so that the output of sparse Transformer can merge the information from different sequences.

\item \textit{Locality-Sensitive Hashing}~\cite{reformer}:  This method hashes hidden states into different buckets determined by randomly-initialized hashing vectors.
A Transformer layer is then applied across buckets to build Sparse Attention across the whole sequence.
Note that this method cannot be directly used for question answering without adding Sliding-Window layer, as our QA model is initialized by RoBERTa that only has 512 position embeddings.
\end{itemize}

\begin{table*}[]
\small
\begin{tabular}{lp{13cm}}
\toprule
Question         & Where did the underground railroad start and finish ?\\
Context & The Underground Railroad by artist Charles T. Webber , 1893 Date Late 1700s - 1865 Location Northern United States with routes to Canada , Mexico ...\\
\midrule
Special token & \textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater Island island in the colonies city\textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater With in the in . the \textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater the South The Underground Railroad was the Railroad \textless{}s\textgreater The Underground Railroad   \\
Time & did start and finish 1893 Date 1700 1865 Location Participants Outcome Deaths 19 1763 83 17 1821 formed in the late 1700s  1850 1860 1850 via did start and finish 1821 1700 1850 1860 1850 - 1872 1793      \\
Stopwords & the the , the , , , , to , , , , the American runaway slaves of free states the , , , it to , a the the to , , , there the the the , - , , the , they a , there , to , it the the , the Federal for Deep cotton as the the , , , \\
Entity & Canada Mexico Canada is applied Florida Spanish Railroad Railroad Railroad British Canada Ontario were said Numerous are documented in the book  Congress  \\
\midrule
Positions         &   49, 50, 51, 52, 53, 54, 55, 115, 116, 168, 273, 394, ..., 5567, 5577, 5704, 5722, 5740, 5742, 5760, 5778, 5831, 5850, 5851, 5890, 5891, 5989, 6022, 6040, 6042, 6060, 6094, 6095, 6096\\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\caption{An example from Natural Question dataset. The rows in the middle section show the corresponding words of the clustered hidden states, and the bottom row shows the positions of the clustered hidden states. ``\textless{}s\textgreater"  refers to start token of long answer candidate.}
\label{tbl:cluster}
\vspace{-0.4cm}
\end{table*}
\subsection{Experimental Results}
Experiments on question answering and language modeling are presented in Table~\ref{tbl:qa} and \ref{tbl:lm}.

\vspace{2pt}
\noindent \textbf{State-of-the-Art Results on QA}: Our proposed method outperforms several strong baselines, thanks to its ability to encode both local and global information.
Cluster-Former with 512 clusters achieves new state-of-the-art results on Quasar-T, SearchQA and Natural Question (long answer).

\vspace{2pt}
\noindent \textbf{Effect of Cluster-Former}: We also test the ability of Cluster-Former on modeling long-range dependencies.
NOte that  Sparse Attention~\cite{sparsetransf} and Locality-Sensitive Hashing~\cite{reformer} have never been tested on question answering tasks with long context.
To make a fair comparison, we replace the same layers in our baseline (sliding window only) with these methods and also our Cluster-Former.
As can be seen, although Sparse Attention can boost the performance of language modeling, it hurts the performance of question answering.
The loss may come from the noise introduced by pre-selected positions, the corresponding words of which may not be related.
We set the number of hashing vectors in Locality-Sensitive Hashing (LSH) to 64, the same as the number of clusters in Cluster-Former.
LSH outperforms the baseline slightly on QA and consistently underperforms our Cluster-Former (\#C=64).
Overall, our Cluster-Former performs the best on detecting long-range dependencies.

\vspace{2pt}
\noindent \textbf{Effect of Number of Cluster Centroids}: 
We also test the effect of different numbers of cluster centroids () on model performance. 
With the increase of , the hidden states with dependencies tend to be assigned to the same cluster.
Based on our experiments on both QA and LM, we observe that the model with 512 clusters works significantly better than the model with 64 clusters on most of the tasks.
However, for Natural Questions, the improvement is marginal.


\vspace{2pt}
\noindent \textbf{Potential Improvement on LM}: 
Although we have proven the effectiveness of our Cluster-Former on the task of language modeling, our best performance is still lagging behind state of the art~\cite{adaptspan}.
We will further adapt our Cluster-Former to more different frameworks on language modeling in the future.
\subsection{Qualitative Analysis}

We perform qualitative analysis on how the hidden states are clustered in Table~\ref{tbl:cluster},
by visualizing the corresponding words and positions of the hidden states.
From the first row of the table, we can see that the special tokens ``s" tend to belong to the same cluster. 
Note that the special token ``s" is the start token of each long answer candidate, and its hidden state is used for final long answer ranking.
Therefore, the Transformer on this cluster can compare across the candidates and help make the final prediction.

We further observe that the same types of tokens are more likely to appear in the same cluster. 
For example, words from the second row to the forth row cover the topics of time, stopwords, and organization \& geopolitical entities.

Finally, we randomly sample a cluster and list positions of clustered hidden states in the last row. 
We find that states in long distance (over 6000 tokens apart) can be in one cluster, which demonstrates the ability of Cluster-Former to detect long-range dependencies. Besides, we observe that states tend to cluster in phrases. 
For example, we see consecutive positions like ``49, 50, 51, 52, 53, 54, 55", which likely result from the sliding-window encoding. 
\section{Conclusion}
In this paper, we present Cluster-Former, a new method to encode global information for long sequence modeling. 
We achieve new state of the art on three question answer datasets: Quasar-T, SearchQA, and Natural Questions.
Further, we observe that a larger number of clusters in Cluster-Former can lead to better performance on question answering and language modeling tasks. Cluster-Former is a generic approach, and we believe that it can also potentially benefit other tasks that rely on long-range dependencies.
\bibliographystyle{acl_natbib}
\bibliography{emnlp2020}

\end{document}
