



\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{dsfont}
\usepackage{color}
\usepackage{lettrine}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{tikz}
\usepackage{multirow}

\usepackage{xspace}
\usepackage{bbm} 
\usepackage{fixltx2e}
\usepackage{contour}
\usepackage{colortbl}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref} 

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\knn}{\mathit{kNN}}

\usepackage{tikzextern}
\newcommand{\extfig}[2]{\tikzsetnextfilename{fig/extern/#1}{#2}}
\newcommand{\extdata}[1]{}
 

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCV16SubNumber{825}  


\title{CNN Image Retrieval Learns from BoW:\\ Unsupervised Fine-Tuning with Hard Examples}

\titlerunning{CNN Image Retrieval Learns from BoW} 

\authorrunning{F. Radenovi{\'c}, G. Tolias, and O. Chum} 

\newcommand{\namespace}{\hspace{5mm}} \author{Filip Radenovi{\'c} \namespace Giorgos Tolias \namespace Ond{\v r}ej Chum} 

\institute{CMP, Faculty of Electrical Engineering, Czech Technical University in Prague \\ \email{ \{filip.radenovic,giorgos.tolias,chum\}@cmp.felk.cvut.cz}} 

\maketitle

\def\ie{\emph{i.e.}\xspace}
\def\eg{\emph{e.g.}\xspace}
\def\wrt{\emph{w.r.t.}\xspace}
\def\etal{\emph{et al.}\xspace}

\definecolor{darkred}{rgb}{0.8,0,0}

\newcommand{\real}{\mathbb{R}}
\newcommand{\realnn}{{\mathbb{R}^{+}_{0}}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\natzero}{{\mathbb{N}_{0}}}

\newcommand{\loss}{\mathcal{L}}

\newcommand{\cX}{\mathcal{X}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cM}{\mathcal{M}}

\newcommand{\bG}{\mathbb{G}}
\newcommand{\cG}{\boldsymbol{\mathcal{G}}}


\newcommand{\vf}{\mathbf{f}}
\newcommand{\f}{\mathrm{f}}
\newcommand{\mac}{\bar{\vf}}

\def\l2{}

\xspaceaddexceptions{+}
\def\cpl2{L\textsubscript{w}\xspace}
\def\pcawhiten{PCA\textsubscript{w}\xspace}

\def\cropI{\xspace}
\def\cropA{\xspace}

\renewcommand{\paragraph}[1]{{\medskip \noindent \bf #1}}
\newcommand{\pari}[1]{{\medskip \noindent \it #1}}
\newcommand{\equ}[1]{Equation~(\ref{#1})\xspace}

\newcommand{\alert}[1]{{\color{red}{#1}}}
\newcommand{\todo}[1]{{\color{blue}{#1}}}

\renewcommand{\b}[1]{\textbf{#1}}
\newcommand{\w}[1]{\color{blue}{#1}}
\newcommand{\ww}[1]{\textbf{\color{blue}{#1}}}

\newcommand{\nb}[1]{\textbf{\color{darkred}{#1}}}
\renewcommand{\sb}[1]{{\color{black}{\contour{darkred}{#1}}}}
\newcommand{\ob}[1]{\textbf{#1}}
\newcommand{\bo}{\cellcolor{gray!15}}

\def\sssp{\hspace{1pt}}
\def\ssp{\hspace{3pt}}
\def\msp{\hspace{5pt}}
\def\bsp{\hspace{8pt}}
 
\begin{abstract}
Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner.
We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes.
\keywords{CNN fine-tuning, unsupervised learning, image retrieval}
\end{abstract}

\section{Introduction}
\lettrine{I}{mage} retrieval has received a lot of attention since the advent of invariant local features, such as SIFT~\cite{L04}, and since the seminal work of Sivic and Zisserman~\cite{SZ03} based on Bag-of-Words (BoW). 
Retrieval systems have reached a higher level of maturity by incorporating large visual codebooks~\cite{PCISZ07,AK12}, spatial verification~\cite{PCISZ07,SLBW14} and query expansion~\cite{CMPM11,DGBQG11,TJ14}. 
These ingredients constitute the state of the art on particular object retrieval.
Another line of research focuses on compact image representations in order to decrease memory requirements and increase the search efficiency. 
Representative approaches are Fisher vectors~\cite{PLSP10}, VLAD~\cite{JPDSPS11} and alternatives~\cite{RJC15,AZ13,TFJ14}.
Recent advances~\cite{BSCL14,RSMC14} show that Convolutional Neural Networks (CNN) offer an attractive alternative for image search representations with small memory footprint.

CNNs attracted a lot of attention after the work of Krizhevsky \etal~\cite{KSH12}. Their success is mainly due to the computational power of GPUs and the use of very large annotated datasets~\cite{RDSK+15}. 
Generation of the latter comes at the expense of costly manual annotation. Using CNN layer activations as off-the-shelf image descriptors~\cite{DJVO+13,RASC14} appears very effective and is adopted in many tasks~\cite{GDDM14,IMKG+14,GWGL14}. 
In particular for image retrieval, Babenko \etal~\cite{BSCL14} and Gong \etal~\cite{GWGL14} concurrently propose the use of Fully Connected (FC) layer activations as descriptors, while convolutional layer activations are later shown to have superior performance~\cite{RSMC14,BL15,KMO15,TSJ16}. 

Generalization to other tasks \cite{ARSM+14} is attained by CNN activations, at least up to some extent.
However, initialization by a pre-trained network and re-training for another task, a process called \emph{fine-tuning}, significantly improves the adaptation ability~\cite{ZDGD14,OBLS14}. 
Fine-tuning by training with classes of particular objects, \eg building classes in the work of Babenko \etal~\cite{BSCL14}, is known to improve retrieval accuracy. 
This formulation is much closer to classification than to the desired properties of instance retrieval. 
Typical architectures for metric learning, such as siamese~\cite{CHL05,HCL06,HLT14} or triplet networks~\cite{WSLT+14,SKP15,HA15} employ \emph{matching} and \emph{non-matching} pairs to perform the training and better suit to this task.
In this fashion, Arandjelovic \etal~\cite{AGTPS15} perform fine-tuning based on geo-tagged databases and, similar to our work, they directly optimize the the similarity measure to be used in the final task. 
In contrast to them, we dispense with the need of annotated data or any assumptions on the training dataset. 
A concurrent work~\cite{GARL16} bears resemblance to ours but their focus is on boosting performance through end-to-end learning of a more sophisticated representation, while we target to reveal the importance of hard examples and of training data variation. 

A number of image clustering methods based on local features have been introduced~\cite{CM10a,WL13,PSZ11}. 
Due to the spatial verification, the \emph{clusters} discovered by these methods are reliable. 
In fact, the methods provide not only clusters, but also a matching graph or sub-graph on the cluster images. 
These graphs are further used as an input to a Structure-from-Motion (SfM) pipeline to build a 3D model~\cite{SRCF15}. 
The SfM filters out virtually all mismatched images, and also provides camera positions for all matched images in the cluster. 
The whole process from unordered collection of images to 3D reconstructions is fully automatic.

In this paper, we address an unsupervised fine-tuning of CNN for image retrieval. 
We propose to exploit 3D reconstructions to select the training data for CNN. We show that compared to previous supervised approaches, the variability in the training data from 3D reconstructions delivers superior performance in the image retrieval task. 
During the training process the CNN is trained to learn what a state-of-the-art retrieval system based on local features and spatial verification would match. 
Such a system  has large memory requirements and high query times, while our goal is to mimic this via CNN-based representation.
We derive a short image representation and achieve similar performance to such state-of-the-art systems.

In particular we make the following contributions.
(1) We exploit SfM information and enforce not only hard non-matching (\emph{negative}) but also hard matching (\emph{positive}) examples to be learned by the CNN. This is shown to enhance the derived image representation.
(2) We show that the whitening traditionally performed on short representations~\cite{JC12} is, in some cases, unstable and we rather propose to learn the whitening through the same training data. Its effect is complementary to fine-tuning and it further boosts performance. 
(3) Finally, we set a new state-of-the-art based on compact representations for Oxford Buildings and Paris datasets by re-training well known CNNs, such as AlexNet~\cite{KSH12} and VGG~\cite{SZ14}.
Remarkably, we are on par with existing 256D compact representations even by using 32D image vectors.

 \section{Related work}
A variety of previous methods apply CNN activations on the task of image retrieval~\cite{GWGL14,RSMC14,BL15,KMO15,TSJ16,ZZWWT16}.
The achieved accuracy on retrieval is evidence for the generalization properties of CNNs. 
The employed networks were trained for image classification using ImageNet dataset, optimizing classification error.
Babenko \etal~\cite{BSCL14} go one step further and re-train such networks with a dataset that is closer to the target task.
They perform training with object classes that correspond to particular landmarks/buildings. 
Performance is improved on standard retrieval benchmarks.
Despite the achievement, still, the final metric and utilized layers are different to the ones actually optimized during learning.

Constructing such training datasets requires manual effort. 
The same stands for attempts on different tasks~\cite{RASC14,TSJ16} that perform fine-tuning and achieve increase of performance.
In a recent work, geo-tagged datasets with timestamps offer the ground for weakly supervised fine-tuning of a triplet network~\cite{AGTPS15}. 
Two images taken far from each other can be easily considered as non-matching, while matching examples are picked by the most similar nearby images. 
In the latter case, similarity is defined by the current representation of the CNN.
This is the first approach that performs end-to-end fine-tuning for image retrieval and in particular for the task of geo-localization.
The employed training data are now much closer to the final task.
We differentiate by discovering matching and non-matching image pairs in an unsupervised way.
Moreover, we derive matching examples based on 3D reconstruction which allows for harder examples, compared to the ones that the current network identifies. 
Even though hard negative mining is a standard process~\cite{GDDM14,AGTPS15}, this is not the case with hard positive examples. 
Large intra-class variation in classification tasks requires the positive pairs to be sampled carefully;  forcing the model to learn  extremely hard positives may result in over-fitting.
Another exception is the work Simo-Serra \etal~\cite{STFKM14} where they mine hard positive patches for descriptor learning. 
They are also guided by 3D reconstruction but only at patch level.

Despite the fact that one of the recent advances is the triplet loss~\cite{WSLT+14,SKP15,HA15}, note that also Arandjelovic \etal~\cite{AGTPS15} use it, there are no extenstive and direct comparisons to siamese networks and the contrastive loss. 
One exception is the work of Hoffer and Ailon~\cite{HA15}, where triplet loss is shown to be marginally better only on MNIST dataset.
We rather employ a siamese architecture with the contrastive loss and find it to generalize better and to converge at higher performance than the triplet loss. \section{Network architecture and image representation}
\label{sec:network}
In this section we describe the derived image representation that is based on CNN and we present the network architecture used to perform the end-to-end learning in a siamese fashion.
Finally, we describe how, after fine-tuning, we use the same training data to learn projections that appear to be an effective post-processing step.

\subsection{Image representation}
\vspace{-2pt}
We adopt a compact representation that is derived from activations of convolutional layers and is shown to be effective for particular object retrieval~\cite{ARSM+14,TSJ16}.
We assume that a network is fully convolutional~\cite{PKS15} or that all fully connected layers are discarded.
Now, given an input image, the output is a 3D tensor  of  dimensions, where  is the number of feature maps in the last layer. 
Let  be the set of all  activations for feature map \}.
The network output consists of  such sets of activations.
The image representation, called Maximum Activations of Convolutions (MAC)~\cite{RSMC14,TSJ16}, is simply constructed by max-pooling over all dimensions per feature map and is given by
\vspace{-2pt}

\noindent
The indicator function  takes care that the feature vector  is non-negative, as if the last network layer was a Rectified Linear Unit (ReLU).
The feature vector finally consists of the maximum activation per feature map and its dimensionality  is equal to .
For many popular networks this is equal to 256 or 512, which makes it a compact image representation.
MAC vectors are subsequently \l2-normalized and similarity between two images is evaluated with inner product. 
The contribution of a feature map to the image similarity is measured by the product of the corresponding MAC vector components. 
In Figure~\ref{fig:mac_matches} we show the image patches in correspondence that contribute most to the similarity. 
Such implicit correspondences are improved after fine-tuning. Moreover, the CNN fires less to ImageNet classes, \eg cars and bicycles. 
\begin{figure}[t]
\centering

\def\queryone{11}
\def\dbimageone{3885}
\def\querytwo{42}
\def\dbimagetwo{211}

\setlength{\tabcolsep}{0pt}
\hspace{-20pt}
\begin{tabular}{ccc}
\raisebox{8pt}{\includegraphics[height=65pt]{fig/correspondences/q\queryone_db\dbimageone_net0_1} }  &
\includegraphics[height=72pt]{fig/correspondences/q\queryone_db\dbimageone_net0_2}  &
\raisebox{30pt}{
	\begin{tabular}{c}
		\multicolumn{1}{c}{VGG off-the-shelf} \\	
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\queryone_db\dbimageone_net0_p\patch_1.png}
		\hspace{-8pt} 
		}\\
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\queryone_db\dbimageone_net0_p\patch_2.png} 
		\hspace{-8pt} 
		}\\
	\end{tabular}
}
\\
\includegraphics[height=60pt]{fig/correspondences/q\queryone_db\dbimageone_net1_1} & \includegraphics[height=58pt]{fig/correspondences/q\queryone_db\dbimageone_net1_2} ~~~&
\raisebox{30pt}{
	\begin{tabular}{c}
		\multicolumn{1}{c}{VGG ours} \\	
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\queryone_db\dbimageone_net1_p\patch_1.png}
		\hspace{-8pt} 
		}\\
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\queryone_db\dbimageone_net1_p\patch_2.png} 
		\hspace{-8pt} 
		}\\
	\end{tabular}
}
\\
\includegraphics[height=70pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net0_1} &
\raisebox{8pt}{\includegraphics[height=70pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net0_2}} &
\raisebox{30pt}{
	\begin{tabular}{c}
		\multicolumn{1}{c}{VGG off-the-shelf} \\	
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net0_p\patch_1.png}
		\hspace{-8pt} 
		}\\
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net0_p\patch_2.png} 
		\hspace{-8pt} 
		}\\
	\end{tabular}
}
\\
\includegraphics[height=66pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net1_1} &
~\includegraphics[height=65pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net1_2} &
\raisebox{30pt}{
	\begin{tabular}{c}
		\multicolumn{1}{c}{VGG ours} \\	
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net1_p\patch_1.png}
		\hspace{-8pt} 
		}\\
		\foreach \patch in {1,2,3,4,5,6,7,8,9,10}  {
		\includegraphics[height=19pt]{fig/correspondences/q\querytwo_db\dbimagetwo_net1_p\patch_2.png} 
		\hspace{-8pt} 
		}\\
	\end{tabular}
}
\\
\end{tabular}
\vspace{-10pt}
\caption{Visualization of patches corresponding to the MAC vector components that have the highest contribution to the pairwise image similarity. Examples shown use CNN before (top) and after (bottom) fine-tuning of VGG. The same color corresponds to the same vector component (feature map) per image pair. The patch size is equal to the receptive field of the last pooling layer.
\label{fig:mac_matches}
\vspace{-15pt}}
\end{figure}
%
 \vspace{-6pt}
\subsection{Network and siamese learning}
The proposed approach is applicable to any CNN that consists of only convolutional layers. 
In this paper, we focus on re-training (\ie fine-tuning) state-of-the-art CNNs for classification, in particular AlexNet and VGG. 
Fully connected layers are discarded and the pre-trained networks constitute the initialization for our convolutional layers.
Now, the last convolutional layer is followed by a MAC layer that performs MAC vector computation (\ref{equ:mac}).
The input of a MAC layer is a 3D tensor of activation and the output is a non-negative vector. 
Then, an \l2-normalization block takes care that output vectors are normalized. 
In the rest of the paper, MAC corresponds to the \l2-normalized vector .

We adopt a siamese architecture and train a two branch network. 
Each branch is a clone of the other, meaning that they share the same parameters. 
Training input consists of image pairs  and labels  declaring whether a pair is non-matching (label 0) or matching (label 1). 
We employ the contrastive loss~\cite{CHL05} that acts on the (non-)matching pairs and is defined as

where  is the \l2-normalized MAC vector of image , and  is a parameter defining when non-matching pairs have large enough distance in order not to be taken into account in the loss.
We train the network using Stochastic Gradient Descent (SGD) and a large training set created automatically (see Section~\ref{sec:dataset}). 
\subsection{Whitening and dimensionality reduction}
\label{ref:projections}
\vspace{-5pt}
In this section, the post-processing of fine-tuned MAC vectors is considered. 
Previous methods~\cite{BL15,TSJ16} use PCA of an independent set for whitening and dimensionality reduction, that is the covariance matrix of all descriptors is analyzed. We propose to take advantage of the labeled data provided by the 3D models and use linear discriminant projections originally proposed by Mikolajczyk and Matas~\cite{MM07}. The projection is decomposed into two parts, whitening and rotation. 
The whitening part is the inverse of the square-root of the intraclass (matching pairs) covariance matrix , where 
\vspace{-6pt}

The rotation part is the PCA of the interclass (non-matching pairs) covariance matrix in the whitened space , where 
\vspace{-6pt}

The projection  is then applied as , where  is the mean MAC vector to perform centering. To reduce the descriptor dimensionality to  dimensions, only eigenvectors corresponding to  largest eigenvalues are used.
Projected vectors are subsequently \l2-normalized. \section{Training dataset}
\label{sec:dataset}
In this section we briefly summarize the tightly-coupled BoW and SfM reconstruction system~\cite{SRCF15,RSJFCM16} that is employed to automatically select our training data. 
Then, we describe how we exploit the 3D information to select harder matching pairs and hard non-matching pairs with larger variability. 

\subsection{BoW and 3D reconstruction}
The retrieval engine used in the work of Schonberger \etal~\cite{SRCF15} builds upon BoW with fast spatial verification~\cite{PCISZ07}. 
It uses Hessian affine local features~\cite{MTSZMSKG05}, \mbox{RootSIFT} descriptors~\cite{AZ12}, and a fine vocabulary of 16M visual words~\cite{MPCM13}.
Then, query images are chosen via min-hash and spatial verification, as in~\cite{CM10a}. 
Image retrieval based on BoW is used to collect images of the objects/landmarks.
These images serve as the initial matching graph for the succeeding SfM reconstruction, which is performed using state-of-the-art SfM~\cite{FGGJR10,AFSS+11}. Different mining techniques, \eg zoom in, zoom out~\cite{MCM13,MRCM14}, sideways crawl~\cite{SRCF15}, help to build larger and complete model. 

In this work, we exploit the outcome of such a system. 
Given a large unannotated image collection, images are clustered and a 3D model is constructed per cluster.
We use the terms \emph{3D model}, \emph{model} and \emph{cluster} interchangeably.
For each image, the estimated camera position is known, as well as the local features registered on the 3D model. 
We drop redundant (overlapping) 3D models, that might have been constructed from different seeds.
Models reconstructing the same landmark but from different and disjoint viewpoints are considered as non-overlapping.

\subsection{Selection of training image pairs}

A 3D model is described as a bipartite visibility graph ~\cite{LSH10}, where images  and points  are the vertices of the graph. 
Edges of this graph are defined by visibility relations between cameras and points, \ie if a point  is visible in an image , then there exists an edge . 
The set of points observed by an image  is given by
 


We create a dataset of tuples , where  represents a query image,  is a positive image that matches the query, and  is a set of negative images that do not match the query.
These tuples are used to form training image pairs, where each tuple corresponds to  pairs. 
For a query image , a pool  of candidate positive images is constructed based on the camera positions in the cluster of .
It consists of the  images with closest camera centers to the query.
Due to the wide range of camera orientations, these do not necessarily depict the same object. 
We therefore propose three different ways to sample the positive image.
The positives examples are fixed during the whole training process for all three strategies.


\paragraph{Positive images: MAC distance.} 
The image that has the lowest MAC distance to the query is chosen as positive, formally
 
This strategy is similar to the one followed by Arandjelovic \etal~\cite{AGTPS15}. 
They adopt this choice since only GPS coordinates are available and not camera orientations.
Downside of this approach is that the chosen matching examples already have low distance, thus not forcing network to learn much out of the positive samples.


\paragraph{Positive images: maximum inliers.} 
In this approach, the 3D information is exploited to choose the positive image, independently of the CNN descriptor. In particular, the image that has the highest number of co-observed 3D points with the query is chosen.
That is, 
 
This measure corresponds to the number of spatially verified features between two images, a measure commonly used for ranking in BoW-based retrieval. As this choice is independent of the CNN representation, it delivers more challenging positive examples.


\begin{figure}[t]
\centering

\def\imheight{1.1}
\def\qnumone{1021}
\def\qnumtwo{1441}
\def\qnumthree{2461}
\def\qnumfour{6511}
\def\qnumfive{3481}
\def\qnumsix{3991}
\def\qnumseven{1111}
\def\qnumeight{5311}

\def\vnum{5}
\def\raisenum{0}

\setlength{\fboxsep}{0pt}\setlength{\fboxrule}{2pt}

\setlength\tabcolsep{1.5mm}

\begin{tabular}{lclc}
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumone.jpg}} &
\includegraphics[height=\imheight cm]{fig/positives/q\qnumone_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumone_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumone_p3.jpg}
&
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumtwo.jpg}} &
\includegraphics[height=\imheight cm]{fig/positives/q\qnumtwo_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumtwo_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumtwo_p3.jpg}
\\
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumthree.jpg}} & 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumthree_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumthree_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumthree_p3.jpg}
&
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumfour.jpg}} & 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumfour_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumfour_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumfour_p3.jpg}
\\
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumfive.jpg}} & 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumfive_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumfive_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumfive_p3.jpg}
&
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumsix.jpg}} & 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumsix_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumsix_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumsix_p3.jpg}
\\
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumseven.jpg}} & 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumseven_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumseven_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumseven_p3.jpg}
&
\fcolorbox{green}{black}{\includegraphics[height=\imheight cm]{fig/positives/q\qnumeight.jpg}} & 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumeight_p1.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumeight_p2.jpg} 
\includegraphics[height=\imheight cm]{fig/positives/q\qnumeight_p3.jpg}
\\

\end{tabular}
\caption{Examples of training query images (green border) and matching images selected as positive examples by methods (from left to right) , , and .
\label{fig:positives}}
\end{figure}
 

\paragraph{Positive images: relaxed inliers.}
Even though both previous methods choose positive images depicting the same object as the query, the variance of viewpoints is limited.
Instead of using a pool of images with similar camera position, the positive example is selected at random from a set of images that co-observe enough points with the query, but do not exhibit too extreme scale change. 
The positive example in this case is  
 
where  is the scale change between the two images.
This method results in selecting harder matching examples which are still guaranteed to depict the same object. Method  chooses different image than  on 86.5\% of the queries.
In Figure~\ref{fig:positives} we present examples of query images and the corresponding positives selected with the three different methods. The relaxed method increases the variability of viewpoints. 


\paragraph{Negative images.} 
Negative examples are selected from clusters different than the cluster of the query image, as the clusters are non-overlaping. 
Following a well-known procedure, we choose hard negatives~\cite{STFKM14,GDDM14}, that is, non-matching images with the most similar descriptor. Two different strategies are proposed. In the first, , k-nearest neighbors from all non-matching images are selected. In the other, , the same criterion is used, but at most one image per cluster is allowed. While  often leads to multiple, and very similar, instances of the same object,  provides higher variability of the negative examples, see Figure~\ref{fig:negatives}. While positives examples are fixed during the whole training process, hard negatives depend on the current CNN parameters and are re-mined multiple times per epoch. 

\setlength{\fboxsep}{0pt}\setlength{\fboxrule}{2pt}

\begin{figure}[t]
\centering

\def\imheight{1cm}
\def\qnumone{253}
\def\qnumtwo{1611}
\def\qnumthree{1609}

\hspace{-10pt}

\setlength\tabcolsep{2mm}


\begin{tabular}{cccc}



\fcolorbox{green}{black}{\includegraphics[height=\imheight]{fig/negatives/q\qnumone.jpg}} &
\includegraphics[height=\imheight]{fig/negatives/q\qnumone_m2_n1.jpg} &
\foreach \neg in {2,3}  { 
	\includegraphics[height=\imheight]{fig/negatives/q\qnumone_m2_n\neg.jpg}
} & 
\foreach \neg in {4,5}  { 
	\includegraphics[height=\imheight]{fig/negatives/q\qnumone_m1_n\neg.jpg}
}\\



\fcolorbox{green}{black}{\includegraphics[height=\imheight]{fig/negatives/q\qnumtwo.jpg}} &
\includegraphics[height=\imheight]{fig/negatives/q\qnumtwo_m2_n1.jpg} &
\foreach \neg in {3,4}  { 
	\includegraphics[height=\imheight]{fig/negatives/q\qnumtwo_m2_n\neg.jpg}
} & 
\foreach \neg in {2,5}  { 
	\includegraphics[height=\imheight]{fig/negatives/q\qnumtwo_m1_n\neg.jpg}
}\\



\fcolorbox{green}{black}{\includegraphics[height=\imheight]{fig/negatives/q\qnumthree.jpg}} &
\includegraphics[height=\imheight]{fig/negatives/q\qnumthree_m2_n1.jpg} &
\foreach \neg in {2,3,4}  { 
	\includegraphics[height=\imheight]{fig/negatives/q\qnumthree_m2_n\neg.jpg}
} & 
\foreach \neg in {2,3,4}  { 
	\includegraphics[height=\imheight]{fig/negatives/q\qnumthree_m1_n\neg.jpg}
}\\

 &  &  &  \\



\end{tabular}\-2ex]
    \hline
    \multicolumn{13}{|c|}{Compact representations} \\ \hline 
    mVoc/BoW~\cite{RJC15}&                  & 128 & 48.8         & --           & 41.4         & --           & --           & --           & --           & --           & 65.6         & --        \\
    Neural codes~\cite{BSCL14} 
                           &(\ebf\ebA)      & 128 & --           & \ob{55.7}    & --           & \ob{52.3}    & --           & --           & --           & --           & \ob{78.9}    & --        \\
    MAC         &(\ebV)          & 128 & 53.5         & \ob{55.7}    & 43.8         & 45.6         & 69.5         & \ob{70.6}    & 53.4         & \ob{55.4}    & 72.6         & \ob{56.7} \\
    CroW~\cite{KMO15}      &(\ebV)          & 128 & \ob{59.2}    & --           & \ob{51.6}    & --           & \ob{74.6}    & --           & \ob{63.2}    & --           & --           & --        \\ 
    \our MAC               &(\ebf\ebV)      & 128 & \bo\nb{75.8} & \bo\nb{76.8} & \bo\nb{68.6} & \bo\nb{70.8} & \bo77.6      & \bo78.8      & \bo68.0      & \bo69.0      & 73.2         & \bo58.8      \\
    \our R-MAC             &(\ebf\ebV)      & 128 & \bo72.5      & \bo76.7      & \bo64.3      & \bo69.7      & \bo\nb{78.5} & \bo\nb{80.3} & \bo\nb{69.3} & \bo\nb{71.2} & \bo\nb{79.3} & \bo\nb{65.2} \\
    \hline
    MAC         &(\ebV)          & 256 & 54.7         & 56.9         & 45.6         & 47.8         & 71.5         & 72.4    & 55.7         & \ob{57.3}    & 76.5         & \ob{61.3} \\
    SPoC~\cite{BL15}       &(\ebV)          & 256 & --           & 53.1         & --           & \ob{50.1}    & --           & --           & --           & --           & 80.2         & --        \\
    R-MAC~\cite{TSJ16}     &(\ebA)          & 256 & 56.1         & --           & 47.0         & --           & 72.9         & --           & 60.1         & --           & --           & --        \\    
    CroW~\cite{KMO15}      &(\ebV)          & 256 & \ob{65.4}    & --           & \ob{59.3}    & --           & \ob{77.9}    & --           & \ob{67.8}    & --           & 83.1         & --        \\ 
    NetVlad~\cite{AGTPS15} &(\ebV)          & 256 & --           & 55.5         & --           & --           & --           & 67.7         & --           & --           & \sb{86.0}    & --        \\ 
    NetVlad~\cite{AGTPS15} &(\ebf\ebV)      & 256 & --           & \ob{63.5}    & --           & --           & --           & \ob{73.5}         & --           & --           & 84.3         & --        \\
    \our MAC               &(\ebf\ebA)      & 256 & 62.2         & \bo65.4      & 52.8         & \bo58.0      & 68.9         & 72.2         & 54.7         & \bo58.5      & 76.2         & \bo63.8      \\
    \our R-MAC             &(\ebf\ebA)      & 256 & 62.5         & \bo68.9      & 53.2         & \bo61.2      & 74.4         & \bo76.6      & 61.8         & \bo64.8      & 81.5         & \bo\nb{70.8} \\
    \our MAC               &(\ebf\ebV)      & 256 & \bo\nb{77.4} & \bo\nb{78.2} & \bo\nb{70.7} & \bo\nb{72.6} & \bo80.8      & \bo81.9      & \bo72.2      & \bo73.4      & 77.3         & \bo62.9      \\
    \our R-MAC             &(\ebf\ebV)      & 256 & \bo74.9      & \bo\nb{78.2} & \bo67.5      & \bo72.1      & \bo\nb{82.3} & \bo\nb{83.5} & \bo\nb{74.1} & \bo\nb{75.6} & 81.4         & \bo69.4      \\
    \hline
    MAC         &(\ebV)          & 512 & 56.4         & \ob{58.3}    & 47.8         & \ob{49.2}    & 72.3         & \ob{72.6}    & 58.0         & \ob{59.1}    & 76.7         & \ob{62.7} \\
    R-MAC~\cite{TSJ16}     &(\ebV)          & 512 & 66.9         & --           & 61.6         & --           & \ob{83.0}    & --           & \ob{75.7}    & --           & --           & --        \\     
    CroW~\cite{KMO15}      &(\ebV)          & 512 & \ob{68.2}    & --           & \ob{63.2}    & --           & 79.6         & --           & 71.0         & --           & 84.9         & --        \\    
    \our MAC               &(\ebf\ebV)      & 512 & \bo\nb{79.7} & \bo80.0      & \bo\nb{73.9} & \bo\nb{75.1} & 82.4         & \bo82.9      & 74.6         & \bo75.3      & 79.5         & \bo67.0      \\
    \our R-MAC             &(\ebf\ebV)      & 512 & \bo77.0      & \bo\nb{80.1} & \bo69.2      & \bo74.1      & \bo\nb{83.8} & \bo\nb{85.0} & \bo\nb{76.4} & \bo\nb{77.9} & 82.5         & \bo\nb{71.5} \\
    \hline 
    \multicolumn{13}{c}{~} \-2ex]
    \hline
    \multicolumn{13}{|c|}{Re-ranking (R) and query expansion (QE)} \\ \hline 
    BoW(1M)+QE~\cite{CMPM11}      &             & --  & 82.7         & --           & 76.7         & --           & 80.5         & --           & 71.0         & --           & --           & --   \\
    BoW(16M)+QE~\cite{MPCM13}     &        & -- & 84.9         & --           & 79.5         & --           & 82.4         & --           & 77.3         & --           & --           & --   \\
    HQE(65k)~\cite{TJ14}           &             & -- & \sb{88.0}    & --           & \sb{84.0}    & --           & 82.8         & --           & --           & --           & --           & --   \\
    R-MAC+R+QE~\cite{TSJ16}  &(\ebV)       & 512 & 77.3         & --           & 73.2         & --           & \sb{86.5}    & --           & \sb{79.8}    & --           & --           & --   \\ 
    CroW+QE~\cite{KMO15}      &(\ebV)       & 512 & 72.2         & --           & 67.8         & --           & 85.5         & --           & 79.7         & --           & --           & --   \\       
    \our MAC+R+QE            &(\ebf\ebV)   & 512 & 85.0         & \bo\nb{85.4} & 81.8         & \bo\nb{82.3} & \bo\nb{86.5} & \bo\nb{87.0} & 78.8         & \bo79.6      & --           & --   \\
    \our R-MAC+R+QE          &(\ebf\ebV)   & 512 & 82.9         & \bo84.5      & 77.9         & \bo80.4      & 85.6         & \bo86.4      & 78.3         & \bo\nb{79.7} & --           & --   \\
    \hline
\end{tabular}
\end{center}
\vspace{-5pt}
\scriptsize
:~Full images are used as queries making the results not directly comparable on Oxford and Paris. \\
:~Our evaluation of MAC with \pcawhiten as in~\cite{TSJ16} with the off-the-shelf network.
\end{table}
 \paragraph{Comparison with the state of the art.}
We extensively compare our results with the state-of-the-art performance on compact image representations and extremely short codes. The results for MAC and R-MAC with the fine-tuned networks are summarized together with previously published results in Table~\ref{tab:stateofart}. The proposed methods outperform the state of the art on Paris and Oxford datasets, with and without distractors with all 16D, 32D, 128D, 256D, and 512D descriptors. On Holidays dataset, the Neural codes~\cite{BSCL14} win the extreme short code category, while off-the-shelf NetVlad performs the best on 256D and higher. 

We additionally combine MAC and R-MAC with recent localization method for re-ranking~\cite{TSJ16} to further boost the performance. Our scores compete with state-of-the-art systems based on local features and query expansion. These have much higher memory needs and larger query times. 

Observations on the recently published NetVLAD~\cite{AGTPS15}:
(1) After fine-tuning, NetVLAD performance drops on Holidays, while our training improves off-the-shelf results on all datasets.
(2) Our 32D MAC descriptor has comparable performance to 256D NetVLAD  on Oxford5k (ours  vs NetVLAD ), and on Paris6k (ours  vs NetVLAD ).
 
\vspace{-2pt}
\section{Conclusions}
We addressed fine-tuning of CNN for image retrieval. The training data are selected from an automated 3D reconstruction system applied on a large unordered photo collection. The proposed method does not require any manual annotation and yet outperforms the state of the art on a number of standard benchmarks for wide range (16 to 512) of descriptor dimensionality. The achieved results are reaching the level of the best systems based on local features with spatial matching and query expansion, while being faster and requiring less memory. Training data, fine-tuned networks and evaluation code are publicly available\footnote{\href{http://cmp.felk.cvut.cz/~radenfil/projects/siamac.html}{http://cmp.felk.cvut.cz/\~{}radenf{}i{}l/projects/siamac.html}}.

\clearpage

\paragraph{Acknowledgment}. Work was supported by the MSMT LL1303 ERC-CZ grant.

\bibliographystyle{splncs}
\bibliography{egbib}
\end{document}
