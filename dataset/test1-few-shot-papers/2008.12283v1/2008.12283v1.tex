
\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{graphics, amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{fixltx2e}
\usepackage{bbm,color}
\usepackage{verbatim}
\usepackage{bm}
\usepackage{url}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{pgffor}

\newcommand{\hlc}[2][yellow]{{\colorlet{foo}{#1}\sethlcolor{foo}\hl{#2}}}


\aclfinalcopy 



\def\shownotes{1}  \ifnum\shownotes=1
\newcommand{\authnote}[2]{[#1: #2]}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand{\tnote}[1]{{\color{red}\authnote{TM}{{#1}}}}
\newcommand{\gnote}[1]{{\color{magenta}\authnote{G}{{#1}}}}
\newcommand{\jnote}[1]{{\color{blue}\authnote{J}{{#1}}}}
\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Entity and Evidence Guided Relation Extraction for DocRED}



 
\author{Kevin Huang, Guangtao Wang, Tengyu Ma, Jing Huang \\
JD AI Research, Stanford University \\
\texttt{\{kevin.huang3, guangtao.wang, jing.huang\}@jd.com}\\ \texttt{tengyuma@stanford.edu}} 
  
\date{}

\begin{document}
\maketitle
\begin{abstract}


Document-level relation extraction is a challenging task which requires reasoning over multiple sentences in order to predict relations in a document. 
In this paper, we propose a joint training framework {\em E2GRE} (Entity and Evidence Guided Relation Extraction) for this task.
First, we introduce entity-guided sequences as inputs to a pretrained language model (e.g. BERT, RoBERTa).
These entity-guided sequences help a pretrained language model (LM) to focus on areas of the document related to the entity.
Secondly, we guide the fine-tuning of the pretrained language model by using its internal attention probabilities as additional features for evidence prediction.
Our new approach encourages the pretrained language model to focus on the entities and supporting/evidence sentences. 
We evaluate our {\em E2GRE} approach on DocRED, a recently released large-scale dataset for relation extraction. 
Our approach is able to achieve state-of-the-art results on the public leaderboard across all metrics, showing that our {\em E2GRE} is both effective and synergistic on relation extraction and evidence prediction.

\end{abstract}

\begin{comment}
\begin{figure}[t]
\framebox{
\parbox{0.45\textwidth}{
\small
\textbf{DocRED Example}  \newline
\textbf{Context:}  {\textcolor{brown}{[0]}} \textbf{\textcolor{red}{Bad Astronaut}} is an American indie / alternative rock band founded in 2000 by Joey Cape , singer from Lagwagon.
{\textcolor{brown}{[1]}} In \textbf{\textcolor{red}{Bad Astronaut}} , Joey Cape explores a style of alternative rock , with lyrics often about deep and intricate personal matters .
{\textcolor{brown}{[2]}} The band released its debut album , " Acrophobe " in 2001 , followed by in 2002 on " Honest Don 's Records . "
{\textcolor{brown}{[3]}} The band released its third and final album , Twelve Small Steps , One Giant Disappointment on November 14 , 2006 on Fat Wreck Chords .
{\textcolor{brown}{[4]}} Upon the album 's release , Joey Cape announced , " without \textbf{\textcolor{blue}{Derrick}} , there is no \textbf{\textcolor{red}{Bad Astronaut}}" on the band 's Myspace page , deciding the resulting record would be the last for \textbf{\textcolor{red}{Bad Astronaut}}.
{\textcolor{brown}{[5]}} ( Drummer \textbf{\textcolor{blue}{Derrick Plourde}} committed suicide in March 2005 . )
{\textcolor{brown}{[6]}} Joey Cape expressed plans on releasing a b - sides album sometime in the future .
{\textcolor{brown}{[7]}} \textbf{\textcolor{red}{Bad Astronaut}} reformed to play their first ever live shows in July 2010 .
{\textcolor{brown}{[8]}} They played 4 shows in California , with Mike Hale of In the Red and Joey Cape doing a solo act as the openers .
{\textcolor{brown}{[9]}} On December 2 , 2016 Fat Wreck Chords announced that Erik Herzog died .
\newline
\textbf{Head Entity:} \textbf{\textcolor{red}{Bad Astronaut}} \newline
\textbf{Tail Entity:} \textbf{\textcolor{blue}{Derrick}} \newline
\textbf{Relation:} ``has part of'' \newline
\textbf{Evidence Sentences:} 0,4,5}
}
\caption{An exemplar document in DocRED datasets where a head and tail entity pair span across multiple sentences.}
} 
\label{fig:exp}
\end{figure}
\end{comment}

\begin{figure}[h]
\framebox{
\parbox{0.45\textwidth}{
\small
\textbf{\textcolor{blue}{Relation Example} } \newline
\textbf{Document:}  {\textcolor{brown}{[0]}} \textbf{\textcolor{red}{The Legend of Zelda}} : The Minish Cap ( ) is an action - adventure game and the twelfth entry in \textbf{\textcolor{red}{The Legend of Zelda}} series.
{\textcolor{brown}{[1]}} Developed by Capcom and Flagship , with Nintendo overseeing the development process , it was released for the Game Boy Advance handheld game console in Japan and Europe in 2004 and in North America and Australia the following year .
{\textcolor{brown}{[2]}} In June 2014 , it was made available on the Wii U Virtual Console .
{\textcolor{brown}{[3]}} The Minish Cap is the third Zelda game that involves the legend of the Four Sword , expanding on the story of and .
{\textcolor{brown}{[4]}} A magical talking cap named Ezlo can shrink series protagonist \textbf{\textcolor{blue}{Link}} to the size of the Minish , a bug - sized race that live in Hyrule .
{\textcolor{brown}{[5]}} The game retains some common elements from previous Zelda installments , such as the presence of Gorons , while introducing Kinstones and other new gameplay features .
{\textcolor{brown}{[6]}} The Minish Cap was generally well received among critics .
{\textcolor{brown}{[7]}} It was named the 20th best Game Boy Advance game in an IGN feature , and was selected as the 2005 Game Boy Advance Game of the Year by GameSpot .\newline
\textbf{Head Entity:} \textbf{\textcolor{red}{The Legend of Zelda}} \newline
\textbf{Tail Entity:} \textbf{\textcolor{blue}{Link}} \newline
\textbf{Relation:} ``Publisher'' \newline
\textbf{Evidence Sentences:} 0,3,4}
}
\caption{An exemplar document in DocRED datasets where a head and tail entity pair span across multiple sentences.}
\label{fig:exp}
\end{figure}


\section{Introduction}


Relation Extraction (RE), the problem of extracting relations between pairs of entities in plain text, has received increasing research attention in recent years~\cite{zhang2017tacred,pmlr-v101-zhao19a,guo-etal-2019-attention}. It has important downstream applications to many other Natural Language Processing (NLP) tasks, such as Knowledge Graph Construction \cite{trisedya-etal-2019-neural}, Information Retrieval, Question Answering \cite{yu-etal-2017-improved} and Dialogue Systems \cite{Young2018aaai}. 


The majority of existing RE datasets focus on predicting \textit{intra-sentence} relations, i.e., extracting relations between entity pairs in the same sentence. For example, SemEval-2010 Task 8~\cite{hendrickx-etal-2010-semeval}, and TACRED~\cite{zhang2017tacred} are two popular RE datasets with intra-sentence relations.
These datasets have facilitated much research progress in this area such as~\cite{wang-etal-2016-relation,Alt2019,pmlr-v101-zhao19a} on SemEval-2010 Task 8 and \cite{zhang2017tacred,guo-etal-2019-attention,baldini-soares-etal-2019-matching,spanBERT2019} on TACRED. 
However, in real world applications, the majority of relations are expressed across sentences.
Figure~\ref{fig:exp} shows an example from the DocRED dataset~\citep{yao-etal-2019-docred}, which requires reasoning over three evidence sentences to predict the relational fact that ``The Legend of Zelda", is the publisher of ``Link". 

In this paper, we focus on the \textit{document-level} relation extraction problem and design a method to facilitate document-level reasoning.
We work on the DocRED~\cite{yao-etal-2019-docred}, a recent large-scale \textit{document-level} relation extraction dataset.
This dataset is annotated with a set of named entities and relations, as well as a set of supporting/evidence sentences for each relation. 
Over 40\% of the relations in DocRED require reasoning over multiple sentences. And supporting/evidence sentences can be used to provide an auxiliary task for explainable relation extraction. 


A natural attempt to solve this problem is to fine-tune the large pretrained Language Models (LMs) (e.g., GPT~\cite{radford2019GPT}, BERT~\cite{devlin-bert}, XLnet~\cite{xlnet}, RoBERTa~\cite{anonymous2020roberta}), a paradigm that has proven to be extremely successful for many NLP tasks. 
For example, all recent papers on DocRED have used BERT as an encoder to obtain the state-of-the-art results~\cite{tang2020hin,acl2020latentreasoning}. 
However, naively adapting pretrained LMs for document-level RE faces a key issue that limits its performance. Due to the length of a given document, there are more entities pairs with meaningful relations in document-level relation extraction than in the intra-sentence relation extraction.
A pretrained LM has to simultaneously encode information regarding all pairs of entities for relation extraction.
Therefore, attention values that the pretrained LM gives over all the tokens are more uniform for document-level RE compared to intra-sentence RE. 
This problem of having more uniform attention values limits the model's ability to extract information from relevant tokens from the document, limiting the effectiveness of the pretrained LM. 

In order to mitigate this problem, we propose our novel Entity and Evidence Guided Relation Extraction ({\em E2GRE}).
For each entity in a document, we generate a new input sequence by appending the entity to the beginning of a document, and then feed it into the pretrained LM. 
Thus, for each document with  entities, we generate  entity-guided input sequences for training.
By introducing these new training inputs, we encourage the pretrained LM to focus on the entity that is appended to the start of the document. 
We further exploit the pretrained LM by directly using internal attention probabilities as additional features for evidence prediction. 
The joint training of relation extraction and evidence prediction helps the model locate the correct semantics that are required for relation extraction. 
Both of these ideas take advantage of pretrained LMs in order to make full use of pretrained LMs for our task. 
Our main contribution is to propose the {\em E2GRE} approach, which consists of the two main ingredients below:

\begin{enumerate}

    \item  For every document, we generate multiple new inputs to feed into a pretrained language model: we concatenate every entity with the document and feed it as an input sequence to the language model. This allows the fine-tuning of the internal representations from the pretrained LM to be guided by the entity. 
\item We further propose to use internal BERT attention probabilities as additional features for the evidence prediction.
This allows the fine-tuning of the internal representations from the pretrained LM to be also guided by evidence/supporting sentences. 


\end{enumerate}

Each of these ideas give a significant boost in performance and by combining them, we are able to achieve the state-of-the-art results on DocRED leaderboard. 


\section{Related Work}
\subsection{Relation Extraction}
Relation Extraction is a long standing problem in NLP that has garnered significant research attention. 
Early work attempts to solve this problem used statistical methods with different types of feature engineering \cite{zelenko-kernel,bunescu-mooney-2005-shortest}. 
Afterwards, neural models have shown better performance at capturing semantic relationship between entities.
These methods include CNN-based approaches \cite{zeng-etal-2014-relation,wang-etal-2016-relation} and LSTM approaches \cite{cai-etal-2016-bidirectional}.


On top of using CNNs/LSTM encoders, previous models add more layers to take advantage of these embeddings.
For example, \citet{han-etal-2018-hierarchical} introduced using hierarchical attentions in order to generate relational information from coarse-to-fine semantic ideas; \citet{zhang2017tacred} applied GCN over the pruned dependency trees, and \citet{guo-etal-2019-attention} introduced Attention Guided Graph Convolutional Networks (AG-GCNs) over dependency trees. These models have shown good performance on intra-sentence relation extraction, however, some of them are not easily adapted for inter-sentence document-level RE.




\citet{Li2016bio,quirk-poon-2017-distant,PengTACL2017} were among the early work on cross sentences and document-level relation extraction.
Most approaches for document-level RE are graph-based neural network methods.
\citet{quirk-poon-2017-distant} first introduced a document graph being used for document-level RE;
\citet{PengTACL2017} proposed a graph-structured LSTM for cross-sentence n-ary relation extraction; and \cite{song-etal-2018-n} further extended the approach to graph-state LSTM. 
In \cite{jia-etal-2019-document}, an entity-centric, multi-scale representation learning on entity/sentence/document-level LSTM model was proposed for document-level n-ary RE task. \citet{christopoulou-etal-2019-connecting} recently proposed a novel edge-oriented graph model that deviates from existing graph models.
\citet{acl2020latentreasoning} proposed an induced latent graph to perform document-level relation extraction on DocRED.
These graph models generally focus on constructing unique nodes and edges, 
and have the advantage of connecting different granularity of information and aggregate them together.



\subsection{Pretrained Language Models}
Pretrained Language Models (LMs) are powerful tools which emerged in recent years. 
Recent pretrained LMs \cite{radford2019GPT,devlin-bert,xlnet,anonymous2020roberta} are Transformer-based \cite{Vaswani17transformers}, and trained with enormous amounts of data. \cite{devlin-etal-2019-bert} was the first large pretrained transformer-based LM to be released, and immediately get the state-of-the-art performance on a number of NLP tasks. 
New pretrained LM models such as XLNet \cite{xlnet} and RoBERTa~\cite{anonymous2020roberta} further increase the performance on the most NLP tasks. 

In order to take advantage of the large amounts of text that these models have seen, we finetune all of the weights inside the model. 
Finetuning on large pretrained LMs has been shown to be effective on relation extraction~\cite{wadden-etal-2019-entity}. 
Generally, large pretrained LMs are used to encode a sequence and then generate the representation of a head/tail entity pair to learn a classification \cite{SpBERT, docred}. \citet{baldini-soares-etal-2019-matching} introduced a new concept similar to BERT called ``matching-the-black'' and pretrained a Transformer-like model for relation learning. 
The models were fine-tuned on SemEval-2010 Task 8 and TACRED achieved state-of-the-art results.
Our method aims to improve the effectiveness of a pretrained LMs, and directly influence the finetuning of the pretrained LMs with our entity and evidence guided approach.


\section{Methods}

In this section, we introduce our {\em E2GRE} method. We first describe how to generate entity-guided inputs in Section~\ref{subsec:inputs}. Then, we present the entity-guided RE (Relation Extraction) in Section~\ref{subsec:RE}. 
Finally, we describe the entity and evidence-guided joint training for RE in Section~\ref{subsec:evidencePred}.
We use BERT as an embodiment of a pretrained LM, and use BERT when describing our methods.\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Model7.png}
    \caption{Diagram of our {\em E2GRE} framework. As shown in the diagram, we pass an input sequence consisting of an entity and document into BERT. We extract head and tails for relation extraction. We have learned relation vector weights shown in green. We also extract out sentence, relation vectors, and BERT attention probabilities for evidence predictions.}

    \label{fig:model}
\end{figure*}
\subsection{Entity-Guided Input Sequences}\label{subsec:inputs}



The relation extraction task is to predict the {\em relation} between each pair of {\em head} entity and {\em tail} entity in a given {\em document}. 

We design the entity-guided inputs to give BERT more guidance towards the entities when finetuning.
Each training input is organized by concatenating the tokens of the first mention of a single entity, denoted by  (named \textit{Concatenated Head Entity}), together with the document tokens , to form: ``[CLS]''+  + ``[SEP]'' +  + ``[SEP]'', which is then fed into BERT. 
We generate such input sequences for each entity in the given document. 
Therefore, for a document with  entities,  new entity-guided input sequences are generated and fed into BERT separately.


Due to BERT's sequence length constraint of 512 tokens, if the length of the training input is longer than 512, we make use of a sliding window approach over the document:
we separate the input into multiple sequences. The first sequence is the original input sequence up to 512 tokens. The second sequence is the same as the first sequence, with an offset to the document, such that it can reach the end.
This is shown as ``[CLS]''+  + ``[SEP]'' + [offset:end] + ``[SEP]''. We combine these two input sequences in our model by averaging the embeddings, and compute the BERT attention probabilities of the tokens twice in the model.

\subsection{Entity-Guided Relation Extraction}\label{subsec:RE}









For a given training input, we have one head entity, which corresponds with the concatenated entity  in the input, and  different tail entities, which are located within the document .
Our method predicts  different relations for each training input, corresponding to  head/tail entity pairs. 

After passing a training input through BERT, we extract out the {\em head entity} embedding and a set of {\em tail entity} embeddings from the BERT output.
We average the embeddings over the concatenated head entity tokens to obtain the head entity embedding .
This is shown as the \textit{Head Extraction} in Fig. \ref{fig:model}. 
In order to extract the -th tail entity embedding , we locate the indices of the tokens of -th tail entity, average the output embeddings of BERT at these indices to get  (i.e., \textit{Tail Extraction} in Fig. \ref{fig:model}).

After obtaining the head entity embedding  and all tail entity embeddings \} in a entity-guided sequence, where , we feed them into a bilinear layer with the sigmoid activation function to predict the probability of -th relation between the head entity  and the -th tail entity , denoted by , as follows

where  is the sigmoid function,  and  are the learnable parameters corresponding to -th relation, where , and  is the number of relations. 

Finally, we finetune BERT with a multi-label cross-entropy loss as follow:



During inference, the goal of relation extraction is to predict a relation for each pair of head/tail entity within a document.
For a given entity-guided input sequence of ``[CLS]''+ entity + ``[SEP]'' + document + ``[SEP]'', the output of our model is a set of  relation predictions.
We combine the predictions from every sequence generated from the same document and with different head entity, in order to obtain all relation predictions over the document. 


\subsection{Evidence Guided Relation Extraction}
\label{subsec:evidencePred}


\subsubsection{Evidence Prediction}
\label{subsec:evid}



Evidence/supporting sentences are the sentences containing important supporting facts for predicting the correct relationships between head and tail entities. Therefore, evidence prediction is a good auxiliary task to relation extraction and also provides explainability for the model. 

The objective of evidence prediction is to predict whether a given sentence is evidence/supporting sentence for a given relation.
Let  be the number of sentences in the document. We first obtain the sentence embedding  by averaging all the embeddings of the words in  (i.e., \textit{Sentence Extraction} in Fig. \ref{fig:model}). 
These word embeddings are derived from the BERT output embeddings. 

Let  be the relation embedding of -th relation (), which is initialized randomly and learnable in our model. 
We employ a bilinear layer with sigmoid activation function to predict the probability of the -th sentence  being a supporting sentence w.r.t. the given -th relation  as follows.






where  represents the embedding of th sentence,  and  are the learnable parameters w.r.t. -th relation.
We define the loss of evidence prediction under the given -th relation as follows:

where , and   means that sentence  is an evidence for inferring -th relation.
It should be noted that in the training stage, we use the embedding of true relation in Eq. \ref{eq:sentPred}. In testing/inference stage, we use the embedding of the relation predicted by the relation extraction model in Section \ref{subsec:RE}.

\subsubsection{Evidence-guided Finetuning with BERT Attention Probabilities}\label{subsec:evid-bert}



Internal attention probabilities of BERT help locate the areas within a document where the BERT model focuses on. 
Therefore, these probabilities can guide the language model to focus on relevant areas of the document for relation extraction (See the attention visualization in Section \ref{sec:attn_viz}). 
In fact, we find that the areas with higher attention values are usually come from the supporting sentences. Therefore, we believe these attention probabilities can be helpful for evidence prediction. 
For each pair of head  and tail , we make use of the attention probabilities extracted from the last  internal BERT layers for evidence prediction.

Let  be the query and  be the key of the multi-head self attention layer,  be the number of attention heads as described in \cite{Vaswani17transformers},  be the length of the input sequence (i.e., the length of entity-guided sequence defined in Section \ref{subsec:RE}) and  being the embedding dimension.
We first extract the output of multi-headed self attention (MHSA)  from a given layer in BERT as follows. 
These extraction outputs are shown as ``Attention Extractor'' in Fig. \ref{fig:model}. 
For a given pair of head  and tail , we extract the attention probabilities corresponding to head and tail tokens to help relation extraction. Specifically, we concatenate the MHSAs for the last  BERT layers extracted by Eq. \ref{eq:layerAttn} to form an attention probability tensor as: .

Then, we calculate the attention probability representation of each sentence under a given head-tail entity pair as follows. 
\begin{enumerate}
    \item We first apply maximum pooling layer along the attention head dimension (i.e., second dimension) over . The max values are helpful to show where a specific attention head might be looking at. Afterwards we apply mean pooling over the last  layers. 
We obtain ,   from these two steps.
    
    \item We then extract the attention probability tensor from the head and tail entity tokens according to the start and end positions of in the document.
    We average the attention probabilities over all the tokens for the head and tail embeddings to obtain .

    \item Finally, we generate sentence representations from  by averaging over the attentions of each token in a given sentence from the document to obtain 



\end{enumerate}



Once we get the attention probabilities , we combine  with the evidence prediction result  of sentence  from Eq.~\ref{eq:sentPred} to form the new sentence representation and feed it into a bilinear layer with sigmoid for evidence sentence prediction as follows:

where  is the vector of fused representation of sentence embeddings and relation embeddings for a given head/tail entity pair.

Finally, we define the loss of evidence prediction under a given -th relation based on attention probability representation as follows: 

where  is the -th value of  computed by Eq. \ref{eq:attenSentPred}.

\subsubsection{Joint Training with Evidence Prediction}
We combine the relation extraction loss and attention probability guided evidence prediction loss as the final objective function for the joint training:

where  is the weight factor to make trade-offs between two losses, which is data dependent.

\section{Experiments}
We present the experimental results of our model {\em E2GRE} and compare with previously established baselines and published results, as well as the public leaderboard results on DocRED.

\subsection{Dataset}

\noindent DocRED \cite{docred} is a large document-level data set for the tasks of relation extraction and evidence sentence prediction. 
It consists of  documents,  entities, and  relations mined from Wikipedia articles. 
For each (head, tail) entity pair, there are  different relation types as the candidates to predict. 
The first relation type is an ``NA'' relation between two entities, and the rest of them corresponds to a WikiData relation name.
Each of the head/tail pair that contain valid relations also include a set of supporting/evidence sentences. 

We follow the same setting in~\cite{docred} to split the data into Train/Validation/Test for model evaluation to make a fair comparison. The number of documents in Train/Validation/Test is //, respectively.

The dataset is evaluated with the metrics of relation extraction \textbf{RE F1}, and evidence \textbf{Evi F1}.
There are also instances where relational facts may occur in the validation and train set, and so we also evaluate on the \textbf{Ign RE F1}, which removes these relational facts.



\subsection{Experimental Setup}
\noindent\textbf{hyper-parameter Setting.} The configuration for the BERT-base model follows the setting in \cite{devlin-bert}. We set the learning rate as 1e-5,  as 1e-4, the hidden dimension of the relation vectors as , and extract internal attention probabilities from last three BERT layers. 

We conduct most of our experiments by fine-tuning the BERT-base model. The implementation is based on the PyTorch~\cite{paszke2017automatic} implementation of BERT\footnote{https://github.com/huggingface/pytorch-pretrained-BERT}. We run our model on a single V100 GPU for 60 epochs, resulting in approximately one day of training. The DocRED baseline and our {\em E2GRE} model have ~115M parameters\footnote{We will release the code after paper review.}. 


\noindent\textbf{Baseline Methods}. We compare our model with the following published models.

\noindent1. \textit{Context Aware BiLSTM}. \citet{docred} introduced the original baseline to DocRED in their paper. They used a context-aware BiLSTM (+ additional features such as entity type, coreference and distance) to encode the document. Head and tail entities are then extracted for relation extraction.

\noindent2. \textit{BERT Two-Step}. \citet{twostepBert} introduced finetuning BERT in a two-step process, where the model first does predicts the NA relation, and then predicts the rest of the relations.\footnote{BERT Two-Step is an arxiv preprint}.

\noindent3. \textit{HIN}. \citet{tang2020hin} introduced using a hierarchical inference network to help aggregate the information from entity to sentence and further to document-level in order to obtain semantic reasoning over an entire document.

\noindent4. \textit{BERT+LSR}. \citet{acl2020latentreasoning} introduced using an induced latent graph structure to help learning how the information should flow between entities and sentences within a document.


\subsection{Main Results}
As shown in Table \ref{table:leaderboard}, our method {\em E2GRE} is the current state-of-the-art model on the public leaderboard for DocRED. 

Table~\ref{table:re} compares our method with the baseline models. From Table~\ref{table:re}, we observe that our {\em E2GRE} method is not only competitive to the previous best methods on the development set, but also holds the following advantages over previous models.


\begin{table}[!h]
\begin{small}
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline \bf User & \bf RE Ign F1 & \bf Re F1(\%) & \bf Evi F1(\%)\\ \hline
BigOrange & 60.1 & 62.3 & - \\
nttmac  & 60.2 & 63.3 & - \\
Ours  & \bf 60.3 & \bf 62.5 & \bf 50.5\\ 
\hline
\end{tabular}
\end{center}
\end{small}
\caption{\label{font-table} Top public leaderboard numbers on DocRED. Our {\em E2GRE} method uses RoBERTa-large.
}
\label{table:leaderboard}
\end{table}

\begin{table}[!h]
\begin{small}
\begin{center}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{|l|l|l|l|}
\hline \bf Model & \bf Ign F1(\%) & \bf RE F1(\%) & \bf Evi F1 \\ \hline
\shortstack[l]{Context-Aware\\~\cite{docred}} & 48.94 & 51.09 & - \\
\hline
\shortstack[l]{BERT Two-Step\\~\cite{twostepBert}}& - & 54.42 & - \\
\hline
\shortstack[l]{HIN-BERT\\~\cite{tang2020hin}}& 54.29 & 56.31 & - \\
\hline
\shortstack[l]{BERT + LSR\\~\cite{acl2020latentreasoning}}& 52.43 & \bf 59.00 & - \\
\hline
E2GRE(Ours) &\bf 55.22 & 58.72 & \bf 47.12\\ \hline


\end{tabular}
}
\end{center}
\end{small}
\caption{\label{font-table} Results of relation extraction on the supervised setting of DocRED. Shown above are comparisons between {\em E2GRE}, and other published models on the validation set with BERT-base as the pretrained language model.}
\label{table:re}
\end{table}

\noindent Our {\em E2GRE} method is not only competitive to the previous best methods on the development set, but also holds the following advantages over previous models.
\begin{itemize}
\item Our method is more intuitive and simpler in design compared to the HIN model and BERT+LSR model. In addition, our method provides interpretable relation extraction with supporting evidence prediction.





\item Our method is also better than all other models on the \textbf{Ign RE F1} metric.
This shows that our model does not memorize relational facts between entities, but rather examine relevant areas in the document to generate the correct relation extraction.
\end{itemize}
Compared to the original BERT baseline, our training time is slightly longer, due to the multiple new entity-guided input sequences. We examined with the idea of generating new sequences based on each head and tail entity pair, but such a method would scale quadratically with the number of entities in the document. Using our entity-guided approach strikes a balance between performance and the training time.

\begin{table}[!h]
\begin{small}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline \bf Model & \bf Rec(\%) & \bf Prec(\%) & \bf F1(\%)\\ \hline
\bf Relation Extraction & & & \\ \hline
BERT + Joint Training & 53.33 & 55.79 & 54.54 \\
BERT-entity-guided  & 54.07 & \bf 60.43 & 57.08\\ + Evidence Guided  &  \bf 59.09 &  56.95 &  \bf 58.72\\ \hline
\bf Evidence Prediction & & & \\ 
\hline
BERT + Joint Training & 43.48 & 41.54 & 42.49 \\
BERT-entity-guided  & 43.10 & \bf 49.66 & 46.15\\ + Evidence Guided  &  \bf 48.26 &  49.47 &  \bf 47.14 \\ 
\hline
\end{tabular}
\end{center}
\end{small}
\caption{\label{font-table} Ablation study on the entity-guided vs evidence-guided RE. BERT+Joint Training is the BERT baseline with joint training of RE and evidence prediction. Results are evaluated on the validation set.
}
\label{table:ablation}
\end{table}

\subsection{Ablation Study}
\noindent\textbf{Analysis of Method Components}
Table \ref{table:ablation} shows the ablation study of our method on the effectiveness of entity-guided and evidence-guided training. The baseline here is the joint training model of relation extraction and evidence prediction with BERT-base.

We see that the entity-guided BERT improves the over this baseline by , and evidence-guided training further improve the method by .
This shows that both parts of our method are important to the overall {\em E2GRE} method.
Our {\em E2GRE} method not only obtains improvement on the relation extraction F1, but it also obtains significant improvement on evidence prediction compared to this baseline. 
This further shows that our evidence-guided finetuning method is effective.



\begin{table}[!h]
\begin{small}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline \bf Model & \bf Rec(\%) & \bf Prec(\%) & \bf F1(\%)\\ \hline
\bf Relation Extraction & & & \\ \hline 
BERT-entity-guided  & 54.07 & 60.43 & 57.08\\ 3 Layers  & 56.50 & \bf 60.13 & \bf 58.71\\ 
6 Layers  &  \bf 61.87 &  54.14 &  58.51\\ \hline
\bf Evidence Prediction & & & \\ \hline
BERT-entity-guided  & 43.10 & \bf 49.66 & 46.15\\ 3 Layers  & 45.33 & 49.07 & \bf 47.12\\
6 Layers  & \bf 46.34 & 48.19 & 46.90\\
\hline
\end{tabular}
\end{center}
\end{small}
\caption{\label{font-table} Ablation study on different numbers of layers of attention probabilities from BERT that are used for evidence prediction. Results are evaluated on the validation set.
}
\label{table:ablation_layers}
\end{table}

\noindent\textbf{Analysis of Number of BERT Layers.} 
We also conduct experiments to analyze the impact of the number of BERT layers used for obtaining attention probability values, see the results in Table \ref{table:ablation_layers}.
From this table, we observe that using more layers is not necessarily better for relation extraction. One possible reason may be that the BERT model encodes more syntactic information in the middle layers \cite{Clark2019WhatDB}.





\begin{comment}


\begin{figure}[h]
\framebox{
\parbox{0.45\textwidth}{
\small
\textbf{\textcolor{blue}{DocRED Example} } \newline
\textbf{Context:}  {\textcolor{brown}{[0]}} \textbf{\textcolor{red}{The Legend of Zelda}} : The Minish Cap ( ) is an action - adventure game and the twelfth entry in \textbf{\textcolor{red}{The Legend of Zelda}} series.
{\textcolor{brown}{[1]}} Developed by Capcom and Flagship , with Nintendo overseeing the development process , it was released for the Game Boy Advance handheld game console in Japan and Europe in 2004 and in North America and Australia the following year .
{\textcolor{brown}{[2]}} In June 2014 , it was made available on the Wii U Virtual Console .
{\textcolor{brown}{[3]}} The Minish Cap is the third Zelda game that involves the legend of the Four Sword , expanding on the story of and .
{\textcolor{brown}{[4]}} A magical talking cap named Ezlo can shrink series protagonist \textbf{\textcolor{blue}{Link}} to the size of the Minish , a bug - sized race that live in Hyrule .
{\textcolor{brown}{[5]}} The game retains some common elements from previous Zelda installments , such as the presence of Gorons , while introducing Kinstones and other new gameplay features .
{\textcolor{brown}{[6]}} The Minish Cap was generally well received among critics .
{\textcolor{brown}{[7]}} It was named the 20th best Game Boy Advance game in an IGN feature , and was selected as the 2005 Game Boy Advance Game of the Year by GameSpot .\newline
\textbf{Head Entity:} \textbf{\textcolor{red}{The Legend of Zelda}} \newline
\textbf{Tail Entity:} \textbf{\textcolor{blue}{Link}} \newline
\textbf{Relation:} ``Publisher'' \newline
\textbf{Evidence Sentences:} 0,3,4}
}
\caption{An example of a head/tail entity pair that spans across multiple sentences from DocRED's validation set. Our model is able to highlight the specific areas of the document that is necessary to correctly predict this relation.} 
\label{fig:exp0}
\end{figure}
\end{comment}

\subsection{Attention Visualizations}\label{sec:attn_viz}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{baseline_attention.png}
    \caption{Baseline BERT attention heatmap over the tokenized document of a DocRED example.}
\label{fig:baseline_attn}
\end{figure}


Fig.~\ref{fig:exp} shows an example from the validation set of our model. In this example, the relation between ``The Legend of Zelda" and ``Link" relies on information across multiple sentences in the given document. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{on_attention3.png}
    \caption{E2GRE's attention heatmap over the tokenized document of a DocRED example.}
\label{fig:on_attn}
\end{figure}

Fig. \ref{fig:baseline_attn} shows the attention heatmap of naively applying BERT for relation extraction. 
This heatmap shows the attention of each word receives from `The Legend of Zelda'' and ``Link''.
We observe that the model is able to locate the relevant areas of ``Link'' and ``Legend of Zelda series'', but the attention values over the rest of the document are very small. Therefore, the model has trouble in extracting out information within the document to generate a correct relation prediction. 
 
In contrast, Fig.~\ref{fig:on_attn} shows that our {\em E2GRE} model highlights the evidence sentences, particularly in the areas where it finds relevant information. 
Phrases related to ``Link'' and ``The Legend of Zelda series'' are assigned with the higher weights. Words (such as``protagonist'' or ``involves'') linking these phrases together are also highly weighted. Moreover, the scale of the attention probabilities for E2GRE is also much larger for E2GRE compared to the baseline.
All of these phrases and bridging words are located within the evidence sentences, and make our model better at evidence prediction as well.



\section{Conclusion}
In order to more effectively exploit pretrained LMs for document-level RE, we propose a new approach called {\em E2GRE} (Entity and Evidence Guided Relation Extraction). We first generate new entity-guided sequences to feed into a LM, focusing the model on the relevant areas in the document. Then we utilize the internal attentions extracted from the last  layers to help guide an LM to focus on relevant areas of the document. Our {\em E2GRE} method improves performance on both RE and evidence prediction on DocRED dataset, and achieves the state-of-the-art performance on the DocRED public leaderboard.


For future work, we plan to incorporate our ideas on using attention-guided multi-task learning to other NLP tasks with evidence sentences. Combining our approach with graph-based models for NLP tasks is another interesting direction to explore.


\bibliography{emnlp2020}                     
\bibliographystyle{acl_natbib}
\end{document}
