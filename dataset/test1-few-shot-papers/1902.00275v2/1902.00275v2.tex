\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{footmisc}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{amsmath,amssymb,mathtools,amsthm,amsfonts,bm}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Ea}[1]{\E\left[#1\right]}
\newcommand{\Eb}[2]{\E_{#1}\left[#2\right]}
\newcommand{\kl}[2]{D_{\mathrm{KL}}\left(#1 ~ \| ~ #2\right)}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bh}{\mathbf{h}}

\newcommand{\cH}{\mathcal{H}}

\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bphi}{\mathbf{\phi}}
\newcommand{\btheta}{\mathbf{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand{\pt}{p_\mathbf{\theta}}
\newcommand{\qt}{q_\mathbf{\theta}}
\newcommand{\bff}{\mathbf{f}}

\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\standardnormal}{\mathcal{N}(\bzero, \bI)}


\usepackage[accepted]{icml2019}

\usepackage{cleveref}

\icmltitlerunning{Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design}

\begin{document}

\twocolumn[
\icmltitle{Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jonathan Ho}{equal,ucb}
\icmlauthor{Xi Chen}{equal,ucb,covariant}
\icmlauthor{Aravind Srinivas}{ucb}
\icmlauthor{Yan Duan}{covariant}
\icmlauthor{Pieter Abbeel}{ucb,covariant} 
\end{icmlauthorlist}

\icmlaffiliation{ucb}{UC Berkeley, Department of Electrical Engineering and Computer Science}
\icmlaffiliation{covariant}{covariant.ai}

\icmlcorrespondingauthor{Jonathan Ho}{jonathanho@berkeley.edu}
\icmlcorrespondingauthor{Aravind Srinivas}{aravind\_srinivas@berkeley.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. 
Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models.
In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers.
Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at: \url{https://github.com/aravindsrinivas/flowpp}.

\end{abstract}


\section{Introduction}
Deep generative models -- latent variable models in the form of variational autoencoders \citep{kingma2013auto}, implicit generative models in the form of GANs \citep{goodfellow2014generative}, and exact likelihood models like PixelRNN/CNN \citep{oord2016pixel, oord2016conditional}, Image Transformer \citep{parmar2018image}, PixelSNAIL \citep{chen2017pixelsnail}, NICE, RealNVP, and Glow \citep{dinh2014nice, dinh2016density, kingma2018glow} -- have recently begun to successfully model high dimensional raw observations from complex real-world datasets, from natural images and videos, to audio signals and natural language \citep{karras2017progressive, kalchbrenner2016video, oord2016wavenet, kalchbrenner2016language, vaswani2017attention}.
 
Autoregressive models, a certain subclass of exact likelihood models, achieve state-of-the-art density estimation performance on many challenging real-world datasets, but generally suffer from slow sampling time due to their autoregressive structure \citep{oord2016pixel,salimans2017pixelcnn++,chen2017pixelsnail, parmar2018image}. Inverse autoregressive models can sample quickly and potentially have strong modeling capacity, but they cannot be trained efficiently by maximum likelihood \citep{kingma2016improving}. Non-autoregressive flow-based models (which we will refer to as ``flow models''), such as NICE, RealNVP, and Glow, are efficient for sampling, but have so far lagged behind autoregressive models in density estimation benchmarks \citep{dinh2014nice,dinh2016density,kingma2018glow}.

In the hope of creating an ideal likelihood-based generative model that simultaneously has fast sampling, fast inference, and strong density estimation performance, we seek to close the density estimation performance gap between flow models and autoregressive models. In subsequent sections, we present our new flow model, Flow++, which is powered by an improved training procedure for continuous likelihood models and a number of architectural extensions of the coupling layer defined by \citet{dinh2014nice,dinh2016density}.

\section{Flow Models}

A flow model  is constructed as an invertible transformation that maps observed data  to a standard Gaussian latent variable , as in nonlinear independent component analysis \citep{bell1995information, hyvarinen2004independent, hyvarinen1999nonlinear}.
The key idea in the design of a flow model is to form  by stacking individual simple invertible transformations \citep{dinh2014nice,dinh2016density,kingma2018glow,rezende2015variational, kingma2016improving, louizos2017multiplicative}. Explicitly,  is constructed by composing a series of invertible flows as , with each  having a tractable inverse and a tractable Jacobian determinant. This way, sampling is efficient, as it can be performed by computing  for , and so is training by maximum likelihood, since the model density

is easy to compute and differentiate with respect to the parameters of the flows .

\section{Flow++}
\label{sec:flowpp}

In this section, we describe three modeling inefficiencies in prior work on flow models: (1) uniform noise is a suboptimal dequantization choice that hurts both training loss and generalization; (2) commonly used affine coupling flows are not expressive enough; (3) convolutional layers in the conditioning networks of coupling layers are not powerful enough.
Our proposed model, Flow++, consists of a set of improved design choices:
(1) variational flow-based dequantization instead of uniform dequantization; (2) logistic mixture CDF coupling flows; (3) self-attention in the conditioning networks of coupling layers. 

\subsection{Dequantization via variational inference}
Many real-world datasets, such as CIFAR10 and ImageNet, are recordings of continuous signals quantized into discrete representations. Fitting a continuous density model to discrete data, however, will produce a degenerate solution that places all probability mass on discrete datapoints \citep{uria2013rnade}. A common solution to this problem is to first convert the discrete data distribution into a continuous distribution via a process called ``dequantization,'' and then model the resulting continuous distribution using the continuous density model \citep{uria2013rnade,dinh2016density,salimans2017pixelcnn++}.

\subsubsection{Uniform dequantization}
Dequantization is usually performed in prior work by adding uniform noise to the discrete data over the width of each discrete bin: if each of the  components of the discrete data  takes on values in , then the dequantized data is given by , where  is drawn uniformly from . \citet{theis2015note} note that training a continuous density model  on uniformly dequantized data  can be interpreted as maximizing a lower bound on the log-likelihood for a certain discrete model  on the original discrete data :

The argument of \citet{theis2015note} proceeds as follows. Letting  denote the original distribution of discrete data and  denote the distribution of uniformly dequantized data, Jensen's inequality implies that

Consequently, maximizing the log-likelihood of the continuous model on uniformly dequantized data cannot lead to the continuous model degenerately collapsing onto the discrete data, because its objective is bounded above by the log-likelihood of a discrete model.

\subsubsection{Variational dequantization}
\label{sec:variationaldequant}
While uniform dequantization successfully prevents the continuous density model  from collapsing to a degenerate mixture of point masses on discrete data, it asks  to assign uniform density to unit hypercubes  around the data . It is difficult and unnatural for smooth function approximators, such as neural network density models, to excel at such a task. To sidestep this issue, we now introduce a new dequantization technique based on variational inference.

Again, we are interested in modeling -dimensional discrete data  using a continuous density model , and we will do so by maximizing the log-likelihood of its associated discrete model . Now, however, we introduce a dequantization noise distribution , with support over . Treating  as an approximate posterior, we have the following variational lower bound, which holds for all :

We will choose  itself to be a conditional flow-based generative model of the form , where  is Gaussian noise. In this case, , and thus we obtain the objective

which we maximize jointly over  and . When  is also a flow model  (as it is throughout this paper), it is straightforward to calculate a stochastic gradient of this objective using the pathwise derivative estimator, as  is differentiable with respect to the parameters of  and .

Notice that the lower bound for uniform dequantization --  \crefrange{eq:unif_bound1}{eq:unif_bound3} -- is a special case of our variational lower bound -- \crefrange{eq:vlb1}{eq:vlb3}, when the dequantization distribution  is a uniform distribution that ignores dependence on . Because the gap between our objective \labelcref{eq:vlb3} and the true expected log-likelihood  is exactly , using a uniform  forces  to unnaturally place uniform density over each hypercube  to compensate for any potential looseness in the variational bound introduced by the inexpressive . Using an expressive flow-based , on the other hand, allows  to place density in each hypercube  according to a much more flexible distribution . This is a more natural task for  to perform, improving both training and generalization loss.

\subsection{Improved coupling layers}
Recent progress in the design of flow models has involved carefully constructing flows to increase their expressiveness while preserving tractability of the inverse and Jacobian determinant computations. One example is the invertible  convolution flow, whose inverse and Jacobian determinant can be calculated and differentiated with standard automatic differentiation libraries \citep{kingma2018glow}. Another example, which we build upon in our work here, is the affine coupling layer \citep{dinh2016density}. It is a parameterized flow  that first splits the components of  into two parts , and then computes , given by

Here,  and  are outputs of a neural network that acts on  in a complex, expressive manner, but the resulting behavior on  always remains an elementwise affine transformation -- effectively,  and  together form a data-parameterized family of invertible affine transformations. This allows the affine coupling layer to express complex dependencies on the data while keeping inversion and log-likelihood computation tractable. Using  and  to respectively denote elementwise multiplication and exponentiation, the affine coupling layer is defined by:

The splitting operation  and merging operation  are usually performed over channels or over space in a checkerboard-like pattern \citep{dinh2016density}.

\subsubsection{Expressive coupling transformations with continuous mixture CDFs}
\label{sec:mixlogistic}
We found in our experiments that density modeling performance of these coupling layers could be improved by augmenting the data-parameterized elementwise affine transformations by more general nonlinear elementwise transformations. For a given scalar component  of , we apply the cumulative distribution function (CDF) for a mixture of  logistics -- parameterized by mixture probabilities, means, and log scales  -- followed by an inverse sigmoid and an affine transformation parameterized by  and :

where

The transformation parameters  for each component of  are produced by a neural network acting on . This neural network must produce these transformation parameters for each component of , hence it produces vectors  and  and tensors  (with last axis dimension ). The coupling transformation is then given by:

where the formula for computing  operates elementwise.

The inverse sigmoid ensures that the inverse of this coupling transformation always exists: the range of the logistic mixture CDF is , so the domain of its inverse must stay within this interval. The CDF itself can be inverted efficiently with bisection, because it is a monotonically increasing function. Moreover, the Jacobian determinant of this transformation involves calculating the probability density function of the logistic mixtures,
which poses no computational difficulty.


\subsubsection{Expressive conditioning architectures with self-attention}
In addition to improving the expressiveness of the elementwise transformations on , we found it crucial to improve the expressiveness of the conditioning on  -- that is, the expressiveness of the neural network responsible for producing the elementwise transformation parameters . Our best results were obtained by stacking convolutions and multi-head self attention into a gated residual network \citep{mishra2018simple,chen2017pixelsnail}, in a manner resembling the Transformer \citep{vaswani2017attention} with pointwise feedforward layers replaced by  convolutional layers. Our architecture is defined as a stack of blocks. Each block consists of the following two layers connected in a residual fashion, with layer normalization \citep{ba2016layer} after each residual connection:

where  refers to a  convolution that doubles the number of channels, followed by a gated linear unit \citep{dauphin2016language}.
The convolutional layer is identical to the one used by \mbox{PixelCNN++} \citep{salimans2017pixelcnn++}, and the multi-head self attention mechanism we use is identical to the one in the Transformer \citep{vaswani2017attention}. (We always use 4 heads in our experiments, since we found it to be effective early on in our experimentation process.)

With these blocks in hand, the network that outputs the elementwise transformation parameters is simply given by stacking blocks on top of each other, and finishing with a final convolution that increases the number of channels to the amount needed to specify the elementwise transformation parameters.

\section{Experiments}

Here, we show that Flow++ achieves state-of-the-art density modeling performance among non-autoregressive models on CIFAR10 and 32x32 and 64x64 ImageNet. We also present ablation experiments that quantify the improvements proposed in \cref{sec:flowpp}, and we present example generative samples from Flow++ and compare them against samples from autoregressive models.


Our experiments employed weight normalization and data-dependent initialization \citep{salimans2016weight}. We used the checkerboard-splitting, channel-splitting, and downsampling flows of \citet{dinh2016density}; we also used before every coupling flow an invertible 1x1 convolution flows of \citet{kingma2018glow}, as well as a variant of their ``actnorm'' flow that normalizes all activations independently (instead of normalizing per channel). Our CIFAR10 model used 4 coupling layers with checkerboard splits at 32x32 resolution, 2 coupling layers with channel splits at 16x16 resolution, and 3 coupling layers with checkerboard splits at 16x16 resolution; each coupling layer used 10 convolution-attention blocks, all with 96 filters. More details on architectures, as well as details for the other experiments, are in our source code release.


\subsection{Density modeling results}

In \cref{table:cifar10}, we show that Flow++ achieves state-of-the-art density modeling results out of all non-autoregressive models, and it is competitive with autoregressive models: its performance is on par with the first generation of PixelCNN models \citep{oord2016pixel}, and it outperforms Multiscale PixelCNN \citep{reed2017parallel}. Our results are reported using 16384 importance samples in our CIFAR experiment and 1 sample in our ImageNet experiments \citep{burda2015importance}. With 1 sample only, our CIFAR model attains 3.12 bits/dim.
Our listed ImageNet 32x32 and 64x64 results are evaluated on a NVIDIA DGX-1; they are worse by 0.01 bits/dim when evaluated on a NVIDIA Titan X GPU.

\begin{table*}[t]
\small
\caption{Unconditional image modeling results in bits/dim}
\label{table:cifar10}
\begin{center}
\begin{tabular}{r|cccc}
\multicolumn{1}{c}{\bf Model family} & \multicolumn{1}{c}{\bf Model} &\multicolumn{1}{c}{\bf CIFAR10} &\multicolumn{1}{c}{\bf ImageNet 32x32}
&\multicolumn{1}{c}{\bf ImageNet 64x64}
\\ \hline \\
Non-autoregressive & RealNVP \citep{dinh2016density} & 3.49 & 4.28 & -- \\
 & Glow \citep{kingma2018glow} & 3.35 & 4.09 & 3.81 \\
 & IAF-VAE \citep{kingma2016improving} & 3.11 & -- & -- \\
 & {\bf Flow++ (ours)} & {\bf 3.08} & {\bf 3.86} & {\bf 3.69} \\
\\ \hline \\
Autoregressive & Multiscale PixelCNN \citep{reed2017parallel} & -- & 3.95 & 3.70 \\
 & PixelCNN \citep{oord2016pixel} & 3.14 & -- & -- \\ 
 & PixelRNN \citep{oord2016pixel} & 3.00 & 3.86 & 3.63 \\ 
 & Gated PixelCNN \citep{oord2016conditional} & 3.03 & 3.83 & 3.57 \\
 & PixelCNN++ \citep{salimans2017pixelcnn++} & 2.92 & -- & --\\
 & Image Transformer \citep{parmar2018image} & 2.90 & 3.77 & -- \\
 & PixelSNAIL \citep{chen2017pixelsnail} & 2.85 & 3.80 & 3.52 \\
\end{tabular}
\end{center}
\end{table*}



\begin{table}[h]
\small
\caption{CIFAR10 ablation results after 400 epochs of training. Models not converged for the purposes of ablation study.}
\label{table:ablations}
\begin{center}
\begin{tabular}{ccc}
\multicolumn{1}{c}{\bf Ablation}  &\multicolumn{1}{c}{\bf bits/dim} &\multicolumn{1}{c}{\bf parameters} 
\\ \hline \\
uniform dequantization & 3.292 & 32.3M \\
affine coupling & 3.200 & 32.0M \\
no self-attention & 3.193 & 31.4M \\
{\bf Flow++ (not converged for ablation)} & {\bf 3.165} & {\bf 31.4M}
\end{tabular}
\end{center}
\end{table}

\subsection{Ablations}

We ran the following ablations of our model on unconditional CIFAR10 density estimation:
variational dequantization vs. uniform dequantization; logistic mixture coupling vs. affine coupling; and stacked self-attention vs. convolutions only. As each ablation involves removing some component of the network, we increased the number of filters in all convolutional layers (and attention layers, if present) in order to match the total number of parameters with the full Flow++ model.

In \cref{fig:ablations} and \cref{table:ablations}, we compare the performance of these ablations relative to Flow++ at 400 epochs of training, which was not enough for these models to converge, but far enough to see their relative performance differences. 
Switching from our variational dequantization to the more standard uniform dequantization costs the most: approximately  bits/dim. The remaining two ablations both cost approximately  bits/dim: switching from our logistic mixture coupling layers to affine coupling layers, and switching from our hybrid convolution-and-self-attention architecture to a pure convolutional residual architecture. Note that these performance differences are present despite all networks having approximately the same number of parameters: the improved performance of Flow++ comes from improved inductive biases, not simply from increased parameter count.

The most interesting result is probably the effect of the dequantization scheme on training and generalization loss. At 400 epochs of training, the full Flow++ model with variational dequantization has a train-test gap of approximately 0.02 bits/dim, but with uniform dequantization, the train-test gap is approximately 0.06 bits/dim. This confirms our claim in \Cref{sec:variationaldequant} that training with variational dequantization is a more natural task for the model than training with uniform dequantization.


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.47\textwidth]{figures/ablations_plot.pdf}
\end{center}
\caption{Ablation training (light) and validation (dark) curves on unconditional CIFAR10 density estimation. These runs are not fully converged, but the gap in performance is already visible.}
\label{fig:ablations}
\end{figure}



\begin{figure*}[h]
\subfigure[PixelCNN]{\label{fig:pixecnn_cifar}\includegraphics[width=0.5\linewidth]{__figures/cifar10/pixelcnn.png}}\hspace{2.5mm}
\subfigure[Flow++]{\label{fig:flow_cifar}\includegraphics[width=0.45\linewidth]{figures/cifar10/cifar_samples_100.png}}
\caption{CIFAR 10 Samples. Left: samples from  \citet{oord2016pixel}. Right: samples from Flow++, which captures local dependencies well and generates good samples at the quality level of PixelCNN, but with the advantage of efficient sampling.
}
\label{fig:cifar-samples-1}
\end{figure*}


\subsection{Samples}


\begin{figure*}[h!]
\subfigure[PixelCNN]{\label{fig:pixelcnn_imagenet32}\includegraphics[width=0.5\linewidth]{__figures/imagenet32/pixelcnn.png}}\hspace{3mm}
\subfigure[Flow++]{\label{fig:flow_imagenet32}\includegraphics[width=.45\linewidth]{figures/imagenet32/imagenet32-samples-finals-smaller.png}}
\caption{32x32 ImageNet Samples. Left: samples from \citet{oord2016pixel}. Right: samples from Flow++. Note that diversity of samples from Flow++ matches the diversity of samples from an autoregressive model on this dataset, which is much larger than CIFAR10.}
\label{fig:imagenet-samples-32}
\end{figure*}

We present the samples from our trained density models of Flow++ on CIFAR10, 32x32 ImageNet, 64x64 ImageNet, and 5-bit CelebA in \cref{fig:cifar-samples-1,fig:imagenet-samples-32,fig:imagenet-samples-64,fig:celeba-samples}. The Flow++ samples match the perceptual quality of PixelCNN samples, showing that Flow++ captures both local and global dependencies as well as PixelCNN and is capable of generating diverse samples on large datasets. Moreover, sampling is fast: our CIFAR10 model takes approximately 0.32 seconds to generate a batch of 8 samples in parallel on one NVIDIA 1080 Ti GPU, making it more than an order of magnitude faster than PixelCNN++ with sampling speed optimizations \citep{ramachandran2017fast}.
More samples are available in the supplementary.


\begin{figure}[!h]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/celebA64/5bit/celeba5bit-64px-samples-final.png}
\end{center}
\caption{Samples from Flow++ trained on 5-bit 64x64 CelebA, without low-temperature sampling.}
\label{fig:celeba-samples}
\end{figure}


\begin{figure}[h]
\centering
\subfigure[Multi-Scale PixelRNN]{\label{fig:pixelcnn_imagenet64}\includegraphics[width=0.9\linewidth]{figures/imagenet64/pixrnn.png}}\\
\subfigure[Flow++]{\label{fig:flow_imagenet64}\includegraphics[width=0.9\linewidth]{figures/imagenet64/imagenet64-samples-final-cropped.png}}
\caption{64x64 ImageNet Samples. Top: samples from Multi-Scale PixelRNN \citep{oord2016pixel}. Bottom: samples from Flow++. The diversity of samples from Flow++ matches the diversity of samples from PixelRNN with multi-scale ordering.}
\label{fig:imagenet-samples-64}
\end{figure}


\section{Related Work}

Likelihood-based models constitute a large family of deep generative models. One subclass of such methods, based on variational inference, allows for efficient approximate inference and sampling, but does not admit exact log likelihood computation \citep{kingma2013auto, rezende2014stochastic, kingma2016improving}.
Another subclass, which we called exact likelihood models in this work, does admit exact log likelihood computation. These exact likelihood models are typically specified as invertible transformations that are parameterized by neural networks \citep{deco1995higher,larochelle2011neural,uria2013rnade,dinh2014nice,germain2015made,oord2016pixel,salimans2017pixelcnn++,chen2017pixelsnail}.

There is prior work that aims to improve the sampling speed of deep autoregressive models. The Multiscale PixelCNN \citep{reed2017parallel} modifies the PixelCNN to be non-fully-expressive by introducing conditional independence assumptions among pixels in a way that permits sampling in a logarithmic number of steps, rather than linear. Such a change in the autoregressive structure allows for faster sampling but also makes some statistical patterns impossible to capture, and hence reduces the capacity of the model for density estimation. WaveRNN \citep{kalchbrenner2018efficient} improves sampling speed for autoregressive models for audio via sparsity and other engineering considerations, some of which may apply to flow models as well.

There is also recent work that aims to improve the expressiveness of coupling layers in flow models.
\citet{kingma2018glow} demonstrate improved density estimation using an invertible 1x1 convolution flow, and demonstrate that very large flow models can be trained to produce photorealistic faces. 
\citet{huang2018neural} show how to design elementwise transformations which themselves are neural networks. \citet{muller2018neural} introduce piecewise polynomial couplings that are similar in spirit to our mixture of logistics couplings and found them to be more expressive than affine couplings, but reported little performance gains in density estimation. We leave a detailed comparison between our coupling layer and these other types of coupling layers for future work.

\section{Conclusion}

We presented Flow++, a new flow-based generative model that begins to close the performance gap between flow models and autoregressive models. Our work considers specific instantiations of design principles for flow models -- dequantization, flow design, and conditioning architecture design -- and we hope these principles will help guide future research in flow models and likelihood-based models in general. 


\section*{Acknowledgements}
We thank Evan Lohn for discovering that our ImageNet models attain slightly different performance on different GPU hardware.
This work was funded in part by ONR PECASE N000141612723, Huawei, Amazon AWS, and Google Cloud.

\bibliography{flowpp_icml2019_arxiv}
\bibliographystyle{icml2019}


\clearpage{}{
\onecolumn
\icmltitle{Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design -- Supplementary Material}
In subsequent pages, we present more Flow++ samples for CIFAR 10, 32 x 32 Imagenet, 64 x 64 Imagenet, 64 x 64 Imagenet with 5-bits per color channel, 64 x 64 CelebA HQ with 3-bits per color channel and 64 x 64 CelebA HQ with 5-bits per color channel.

Our implementation can be found at \url{https://github.com/aravindsrinivas/flowpp}.
}

\newpage

\begin{figure*}[]
\begin{center}
\includegraphics[width=.8\textwidth]{figures/cifar10/cifar_samples_225.png}
\end{center}
\caption{Samples from Flow++ trained on CIFAR10}
\end{figure*}


\begin{figure*}[]
\begin{center}
\includegraphics[width=1.\textwidth]{figures/imagenet32/imagenet32-samples-finals.png}
\end{center}
\caption{Samples from Flow++ trained on 32x32 ImageNet}
\end{figure*}

\begin{figure*}[]
\begin{center}
\includegraphics[width=1.\textwidth]{figures/imagenet64/imagenet64-samples.png}
\end{center}
\caption{Samples from Flow++ trained on 64x64 ImageNet}
\end{figure*}

\begin{figure*}[]
\begin{center}
\includegraphics[width=1.\textwidth]{figures/imagenet64-5bit/imagenet64-5bit-samples-final.png}
\end{center}
\caption{Samples from Flow++ trained on 5-bit 64x64 ImageNet}
\end{figure*}

\begin{figure*}[]
\begin{center}
\includegraphics[width=1.\textwidth]{figures/celebA64/5bit/celeba5bit-64px-samples-final.png}
\end{center}
\caption{Samples from Flow++ trained on 5-bit 64x64 CelebA}
\end{figure*}

\begin{figure*}[]
\begin{center}
\includegraphics[width=1.\textwidth]{figures/celebA64/3bit/celeba3bit-64px-samples-final.png}
\end{center}
\caption{Samples from Flow++ trained on 3-bit 64x64 CelebA}
\end{figure*}
\clearpage{}

\end{document}
