\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{microtype}
\usepackage{tabularx}
\usepackage{booktabs}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{property}{Property}
\newtheorem{remark}{Remark}[section]
\newtheorem{notation}{Notation}[section]
\newtheorem{example}{Example}[section]
\newtheorem{algorithm}{Algorithm}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}[conjecture]{Question}

\providecommand{\abs}[1]{\lvert#1\rvert} \providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\providecommand{\myvec}[1]{\ensuremath{\mathbf{#1}}}

\newcommand{\dmax}{d_{\mathsf{max}}}
\newcommand{\dmin}{d_{\mathsf{min}}}
\newcommand{\deltamin}{\delta_{\mathsf{min}}}
\newcommand{\fn}{f}
\newcommand{\wmin}{w_{\mathsf{min}}}
\newcommand{\myin}{\mathsf{in}}
\newcommand{\myout}{\mathsf{out}}
\newcommand{\row}{\mathrm{row}}
\newcommand{\ball}{\mathsf{ball}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\bad}{\mathrm{bad}}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vvv}{\mathbf{v'}}
\newcommand{\f}{\mathbf{F}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\LL}{\mathbf{\cal L}}
\newcommand{\cst}{{c}}
\newcommand{\XX}{{\widetilde{X}}}
\newcommand{\XXX}{{\bar{X}}}

\renewcommand{\nu}{\ensuremath{\mathbf{g}}}

\def\polylog{\operatorname{polylog}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\mytriangle}{\triangle}

\newcommand{\eps}{\varepsilon}
\title{Spectral Concentration and Greedy -Clustering}
\author{
Tamal K. Dey\thanks{
    Dept.~of Computer Science and Engineering, and Dept.~of Mathematics, The Ohio State University. Columbus, OH, 43201.
    \texttt{tamaldey@cse.ohio-state.edu}
}
\and
Pan Peng\thanks{
    Department of Computer Science, University of Sheffield.
    \texttt{p.peng@sheffield.ac.uk}
}
\and
Alfred Rossi\thanks{
    Dept.~of Computer Science and Engineering, The Ohio State University. Columbus, OH, 43201.
    \texttt{rossi.49@osu.edu}
}
\and
Anastasios Sidiropoulos\thanks{
    Dept.~of Computer Science and Engineering, and Dept.~of Mathematics, The Ohio State University. Columbus, OH, 43201.
    \texttt{sidiropoulos.1@osu.edu}
}
}
\date{}
\begin{document}
\maketitle

\begin{abstract}
A popular graph clustering method is to consider the embedding of an input graph
 into  induced by the first  eigenvectors of its Laplacian, and
to partition the graph via geometric manipulations on the resulting metric
space. Despite the practical success of this methodology, there is limited
understanding of several heuristics that follow this framework. We provide
theoretical justification for one such natural and computationally efficient
variant.

Our result can be summarized as follows. A partition of a graph is called {\em
strong} if each cluster has small external conductance, and large internal
conductance. We present a simple greedy spectral clustering algorithm which
returns a partition that is provably close to a suitably strong partition,
provided that such a partition exists. A recent result shows that strong
partitions exist for graphs with a sufficiently large spectral gap between the
-th and -st eigenvalues. Taking this together with our main theorem
gives a spectral algorithm which finds a partition close to a strong one for
graphs with large enough spectral gap. We also show how this simple greedy
algorithm can be implemented in near-linear time for any fixed  and error
guarantee. Finally, we evaluate our algorithm on some real-world and synthetic
inputs.
\end{abstract}



 \section{Introduction}
Spectral clustering of graphs is a fundamental technique in data analysis that
has enjoyed broad practical usage because of its efficacy and simplicity. The
technique maps the vertex set of a graph into a Euclidean space 
 where a classical clustering algorithm (such as -means, -center) is
applied to the resulting embedding~\cite{DBLP:journals/sac/Luxburg07}. The
coordinates of the vertices in the embedding are computed from  eigenvectors
of a matrix associated with the graph. The exact choice of matrix depends on the
specific application but is typically some weighted variant of , for a
 graph with degree matrix  and adjacency matrix . Despite widespread usage, theoretical
understanding of the technique remains limited. For example, it is generally not
clear for which classes of graphs spectral clustering works well, or what the
structure of the subgraph induced by vertices that correspond to embedded points
from the same cluster is. Although the case for  (two clusters) is well
understood, the case of general  is not yet settled and a growing body of
work seeks to address the practical success of spectral clustering methods
\cite{DBLP:conf/nips/BalakrishnanXKS11,DBLP:journals/jacm/BiswalLR10,
DBLP:journals/siamcomp/Kelner06, DBLP:conf/nips/NgJW01,
DBLP:conf/compgeom/SpielmanT96, DBLP:journals/sac/Luxburg07}.
 
In this paper we present a simple greedy spectral clustering algorithm which is
guaranteed to return a high quality partition, provided that one of sufficient
quality {\em exists}. It first chooses  clusters along with their centers
greedily from the vertices spectrally embedded in an Euclidean space. Any left
over vertex is assigned to one of the computed clusters whose center it is
closest to. The resulting partition is close in symmetric difference to the high
quality one. Our results can be viewed as providing further theoretical
justification for popular clustering algorithms such as in
\cite{DBLP:conf/nips/BalakrishnanXKS11} and \cite{DBLP:conf/nips/NgJW01}.

\paragraph*{Measuring partition quality} Intuitively, a good -clustering
of a graph is one where there are few edges between vertices residing in
different clusters and where each cluster is well-connected as an induced
subgraph. Such a qualitative definition of clusters can be appropriately
characterized by vertex sets with small external conductance and large internal
conductance, which has been first formalized by Oveis Gharan and
Trevisan~\cite{DBLP:conf/soda/GharanT14}.

Let  be an undirected unweighted graph. Let  be the degree of a vertex . For a subset , the \emph{external
conductance} and \emph{internal conductance} are defined to be

respectively, where  (called the \emph{volume}
of ),  denotes the set of edges between  and , and
 denotes the subgraph of  induced on . For an {\em isolated} vertex
 in , we assume  and  by
definition. Let . It follows that if
, then  cannot have any isolated vertex. When  is
understood from context we sometimes write  in place of
.

We define a \emph{-partition} of a graph  to be a partition  of  into  disjoint subsets. We say that  is \emph{-strong}, for some
, if for all , we
have


Thus a high quality partition is one where  is large and
 is small.

\paragraph*{Our contribution} We present a simple spectral algorithm which
computes a partition provably close to {\em any} -strong -partition if there is large gap between
 and  (see Theorem~\ref{thm:main} for formal
statement). We emphasize the fact that the algorithm's output approximates any
good existing clustering in the input graph. The algorithm consists of a simple
greedy clustering procedure performed on the embedding into 
induced by the first  eigenvectors. We further show how to implement this
algorithm in near-linear time for any fixed  and error guarantee (see
Theorem~\ref{thm:algorithm2}).

In the analysis of our algorithm, we show some interesting spectral properties
of graphs that admit strong -partitions: each of the (rescaled) first 
eigenvectors of the Laplacian matrix of the graph is close to some vector that
is constant on each cluster; the image of each cluster concentrates around some
point in the spectral embedding, and all these points are well separated.

\paragraph*{Related work} The discrete version of Cheeger's inequality
asserts that a graph admits a bipartition into two sets of small external
conductance if and only if the second eigenvalue is small
\cite{DBLP:journals/combinatorica/Alon86, DBLP:journals/jct/AlonM85, cheeger,
DBLP:conf/focs/Mihail89, DBLP:journals/iandc/SinclairJ89}. In fact, such a
bipartition can be efficiently computed via a simple algorithm that examines the
second eigenvector. Generalizations of Cheeger's inequality have been obtained
by Lee, Oveis Gharan, and Trevisan \cite{DBLP:conf/stoc/LeeGT12}, and Louis
\etal~\cite{DBLP:conf/stoc/LouisRTV12}. They showed that spectral algorithms can
be used to find  disjoint subsets, each with small external conductance,
provided that the -th eigenvalue is small. An improved version of Cheeger's
inequality has been obtained by Kwok \etal~\cite{DBLP:conf/stoc/KwokLLGT13} for
graphs with large -th eigenvalue.

Even though the clusters given by the above spectral partitioning methods have
small external conductance, they are not guaranteed to have large internal
conductance. In other words, for a resulting cluster , the induced graph
 might admit further partitioning into sub-clusters of small conductance.
Kannan, Vempala and Vetta proposed quantifying the quality of a partition by
measuring the internal conductance of clusters
\cite{DBLP:journals/jacm/KannanVV04}. Allen Zhu, Lattanzi and
Mirrokni~\cite{DBLP:conf/icml/ZhuLM13} and Orecchia and Allen Zhu~\cite{DBLP:conf/soda/OrecchiaZ14}
studied local algorithms for extracting subsets with small external conductance
under the assumption that subsets with small external conductance and high
(internal) connectivity exist.

One may wonder under what conditions a graph admits a partition which provides
guarantees on both internal and external conductance. Oveis Gharan and Trevisan,
improving on a result of Tanaka~\cite{Tanaka:2011vk}, showed that graphs which
have a sufficiently large spectral gap between the -th and -st
eigenvalues (denoted as  and , respectively) of its
Laplacian admit a strong -partition~\cite{DBLP:conf/soda/GharanT14} (see
Theorem~\ref{thm:OT_clustering}). Czumaj et al.~\cite{DBLP:conf/stoc/CzumajPS15} recently
proposed a sublinear algorithm for testing if a graph with bounded maximum
degree has an -strong partition in the
framework of property testing, assuming there is some gap between
.

\paragraph*{Follow-up work} Subsequent to the original ArXiv submission
\cite{DBLP:journals/corr/DeyRS14} of this paper, Peng, Sun, and Zanetti
\cite{DBLP:conf/colt/PengSZ15}, Awasthi et al.~\cite{DBLP:conf/innovations/AwasthiCKS16}
and  Sinop~\cite{DBLP:conf/soda/Sinop16} have derived spectral algorithms with weaker
assumption on the gap between  and  (or some
related gap, e.g.,  and ) to cluster the
vertices of the graph. The clustering algorithm analyzed in this paper remains
distinct from this body of work. For example, in
\cite{DBLP:conf/colt/PengSZ15} the authors applied -means clustering to
the spectral embedding by the first  eigenvectors; and show that the
resulting algorithm is able to find  sets each of which is close to one
cluster of a strong -partition and has bounded small external conductance,
under the assumption that . (In
contrast, our assumption is  for
graphs with maximum degree at most ; see Theorem~\ref{thm:main}.) Their
error guarantee ultimately depends on the approximation factor afforded by a
-means algorithm. Unfortunately, -means is very sensitive to the initial
choice of  centers and it is NP-hard to approximate to within some constant
factor~\cite{DBLP:conf/compgeom/AwasthiCKS15}. They also gave a heat-kernel
based algorithm that runs in near-linear time, which seems to be unappealing for
implementation. In \cite{DBLP:conf/innovations/AwasthiCKS16}, the authors proposed an algorithm
that iteratively applies the -means clustering on the resistive embedding
projected onto the first  eigenvectors, and outputs a -partition such that
each part is close to one set in a target partition, under the assumption that
the ratio between the algebraic expansion of the clusters and 
is .   Sinop~\cite{DBLP:conf/soda/Sinop16} gave another spectral algorithm assuming
that . In each case,
\cite{DBLP:conf/innovations/AwasthiCKS16,DBLP:conf/colt/PengSZ15,DBLP:conf/soda/Sinop16} use either a
different measure of the difference of the output partition and the target
partition, or a different definition of conductance (see our remark below
Theorem~\ref{thm:main}).

\paragraph*{Outline} Section~\ref{sec:greedy-k-clustering} contains a
description of the greedy -clustering algorithm and the statement of our main
theorem. In Section~\ref{sec:concentration} we show a spectral concentration
property for any graph that admits a high quality partition. Building on this
property, we argue that the image of each cluster concentrates around some point
in the spectral embedding and these points are well separated. The complete proof of the main theorem is then given in Section~\ref{sec:clustering}.
In Section~\ref{fast-algo}, we give a randomized version of the algorithm which runs
in time  for any error parameter .
Finally, we present some experimental results in \ref{sec:experiments}.

\section{Greedy \texorpdfstring{}{k}-Clustering}\label{sec:greedy-k-clustering} Let  be an
undirected unweighted graph with  vertices, and let  be its normalized Laplacian, where
 is the adjacency matrix of  and  is a diagonal
matrix with the entries  equal to the degree of vertex
. Let  be the
eigenvalues of , and  a
corresponding collection of orthonormal left eigenvectors\footnote{We will use
 to denote a row vector and  to denote a column vector.}. Note that
by the variational characterization of eigenvalues,
 for 
(see~\cite{Chu97:spectral}).

In this paper we consider a simple geometric clustering operation on the
embedding  which carries a vertex  to a point given by a rescaling of
the first  eigenvectors of ,

For any , let  denote all the embedded points corresponding to
vertices in , that is, . For any set , let . For any point  and real number , let .

\paragraph{Intuitive description}
The algorithm takes as input a graph , and a desired number of clusters, .
The algorithm uses the bottom  eigenvectors  of  to compute the embedding  of  into .
Next, it begins an iterative process of searching for regions of the embedding containing many points from , and removing them to form clusters. To do so, it first computes a distance threshold , where  and  represent an upper bound of the maximum degree.
Using this threshold, the algorithm looks for a point  such that the number of near-by points of  (points of  which fall within a radius of  of ) is maximized.
The vertices corresponding to these points (including ) are made into a cluster, and  is remembered as the location of the cluster in the embedding. Next,  and its near-by points are removed from .
This iterative process continues either for  iterations, or until there are no points of  left in the embedding.
Afterward, any remaining points of  are thought of as ``outliers'', and each has its corresponding vertex assigned to a nearest cluster.

A more formal description of the algorithm appears in Figure \ref{fig:algorithm}. 
In Section~\ref{fast-algo}, we show how the algorithm can be implemented in time  for any error parameter , where  denotes the number of edges of the graph and  hides  factors.



\begin{figure}
\begin{center}
\begin{tabularx}{\textwidth}{l}
\toprule
\textbf{Algorithm: Greedy Spectral -Clustering}\\
\textbf{Input: -vertex graph }\\
\textbf{Output: Partition  of }\\
\midrule
Let  be the  first eigenvectors of . \\
Let , where . \\
 \\
 \\
for  \\
~~~~~~~~\\
~~~~~~~~~~~~ \\
~~~~~~~~ \\
~~~~~~~~ \\
Let ,  if  is the smallest index \\
~~~~~~~~~~~~satisfying  for all . \\
Return  \\
\bottomrule
\end{tabularx}
\end{center}
\caption{The greedy spectral -clustering algorithm takes an -vertex graph
 with maximum degree at most  as input, and
outputs a partition  of .
\label{fig:algorithm}}
\end{figure}

To measure the performance of our algorithm, we introduce the following notion
of symmetric difference between two collections, each of  sets, that
generalizes the symmetric difference between two sets.

\paragraph*{A distance on -partitions} For two sets , their
symmetric difference is given by  Let  be a finite set, , and let ,  be collections of
disjoint subsets of . Then, we define a distance function between ,
, by

where  ranges over all permutations  on .

\subsection{Main Theorem}

\begin{theorem}[Spectral partitioning via greedy -clustering]\label{thm:main}
Let  be an -vertex graph with maximum degree at most . Let , and let  be any -strong
-partition of  with . Then, on input , the algorithm in Figure
\ref{fig:algorithm} outputs a partition  such that

\end{theorem}

Due to the dependency on , the above result is mainly interesting for bounded degree graphs. We remark that even for some special classes of bounded degree graphs, analyzing the performance of spectral clustering algorithms is already interesting and challenging. For example, in their seminal work Spielman and Teng gave the first rigorous analysis of the performance of spectral clustering methods which use the second eigenvector of the matrix  on bounded degree planar graphs and finite element meshes~\cite{ST2007}. This result was further generalized to graphs with bounded degree and bounded genus by Kelner~\cite{DBLP:journals/siamcomp/Kelner06} and excluded-minor graphs by Biswal, Lee and Rao~\cite{DBLP:journals/jacm/BiswalLR10}. Our result holds for arbitrary bounded degree graphs that admit a good quality -partition and demonstrates the effectiveness of the spectral clustering algorithms that use only the first  eigenvectors (cf. Alpert and Yao~\cite{DBLP:conf/dac/AlpertY95}).

We further remark that while  does not appear explicitly in the error
term in Theorem~\ref{thm:main}, it does implicitly bound the error through a
higher order Cheeger inequality~\cite{DBLP:conf/stoc/LeeGT12}. In particular,
 and thus when
 is small there is strong agreement
between  and . In addition, the dependency on the upper bound  of maximum vertex degree seems unavoidable since we are measuring
the \emph{size} of the symmetric difference 
rather than its \emph{volume} or \emph{total degree}. (Note that the definition of conductance is volume-based, while the error is measured with respect to the size of clusters. Such an inconsistency seems to cause the dependency on .) In contrast, the latter
measurement was used in~\cite{DBLP:conf/colt/PengSZ15}, which allows the
authors to derive an error term that is independent of the maximum degree. On the other hand, such a dependency also does not
appear in~\cite{DBLP:conf/innovations/AwasthiCKS16} and~\cite{DBLP:conf/soda/Sinop16}, as the authors are
studying the size-based definition of conductance (i.e.,
) instead of the
volume-based definition as in our paper. We also note that the algorithms in some follow-up work (e.g.,~\cite{DBLP:conf/innovations/AwasthiCKS16,DBLP:conf/colt/PengSZ15}) can output some partition with an approximate guarantee
for individual clusters, while our algorithm can only have approximate guarantee over the all  clusters. 

\paragraph*{Application of main theorem} Oveis Gharan and Trevisan
\cite{DBLP:conf/soda/GharanT14} (see also \cite{Tanaka:2011vk}) showed that, if
the \emph{gap} between  and  is large enough, then
there exists a partition into  clusters, each having small external
conductance and large internal conductance.

\begin{theorem}[\cite{DBLP:conf/soda/GharanT14}]\label{thm:OT_clustering} There
exist universal constants , , and , such that for any
graph  with , there is a
-strong -partition of .\label{OT-thm}
\end{theorem}

The same paper \cite{DBLP:conf/soda/GharanT14} also shows how to compute a
partition with slightly worse quantitative guarantees, using an iterative
combinatorial algorithm with polynomial running time. More specifically, they have shown the following theorem.
\begin{theorem}[\cite{DBLP:conf/soda/GharanT14}]\label{thm:OT_clustering_algorithm}
There is a polynomial time algorithm that takes as input a
graph  with  for any , outputs an
-partition that is -strong, for some .
\end{theorem}


Let  be a number satisfying , where  are the constants given
in Theorem \ref{thm:OT_clustering}. Let  be a graph with . By applying Theorem~\ref{thm:main} on  with parameters , which satisfies that , we obtain the following corollary. \begin{corollary}\label{cor:main}
Let . Let  be an -vertex graph with maximum degree at most , and , where  and  is defined as above. Let  be the -strong partition of 
guaranteed by Theorem \ref{thm:OT_clustering}. Then, on input , the algorithm
in Figure \ref{fig:algorithm} outputs a partition  such that

\end{corollary}
In comparison with the algorithm from Theorem~\ref{thm:OT_clustering_algorithm} that finds a partition that is a -strong partition, our algorithm finds a partition that is \emph{close to} some -strong partition. It is not clear how to find in polynomial time a partition (without error) that is -strong.  

\section{Spectral Concentration}\label{sec:concentration} In this section, we
prove that for any graph with some strong -partition and any eigenvector  (), the rescaled vector
 is close (with respect to the  norm) to
some vector  that is constant on each cluster. We slightly
abuse the notation by also using  to denote the  matrix that
corresponds to our spectral embedding (i.e., with row vectors , for all
). It is useful to note that .

\begin{lemma}\label{lem:uniformity}
Let  be a graph with maximum degree at most .
Let  denote the first  eigenvectors of .
For , let  be any -strong
-partition of .
For any , if
, then there exists , such that,
\begin{description}
  \item{(i)}  , and
  \item{(ii)}  is constant on the clusters of , i.e.~for any , , we have .
\end{description}
\end{lemma}

Before laying out the proof, we provide some explanation of the statement of the
theorem. First, note that the -distance between  and its
uniform approximation  depends linearly on the ratio
, which, as noted above, is bounded from above by
. Second, the partition-wise uniform vector
 which minimizes the left hand side of (i) is constructed by
taking the mean values of  on each partition. This, together with the
bound in (i), means that  assumes values in each partition close to their
mean. In summary, if there is a sufficiently large gap between the external
conductance  and internal conductance  of the
clusters, the values taken by each vector  have  prominent modes over
 partitions.

We need the following result that is a slight restatement of a lemma in
\cite{DBLP:conf/stoc/CzumajPS15} to prove Lemma \ref{lem:uniformity}. (For completeness, a
proof of Lemma~\ref{lem:clusterable-eigenvector} is included in~\ref{appendix:eigenvector}.)

\begin{lemma}
\label{lem:clusterable-eigenvector}
Let  be any undirected graph and let  be any subset with
. Then for every , ,
, the following holds:

\end{lemma}

We remark that in the above Lemma, there is a linear dependency on , which directly causes our result and the following analysis to depend on . Now we are ready to prove Lemma~\ref{lem:uniformity}.
\begin{proof}[Proof of Lemma~\ref{lem:uniformity}]
Let , and . By precondition of the lemma,  is an -strong
-partition. Now we apply  and  in
Lemma~\ref{lem:clusterable-eigenvector} to get

where the second inequality follows from our assumption that the maximum degree is .
Let  if . Note that  is constant on each cluster. On the other hand, by the definition of , we have


Therefore,

This completes the proof of the lemma.
\end{proof}

\section{From Spectral Concentration to Spectral Clustering}\label{sec:clustering}

In this section we prove Theorem \ref{thm:main}. We begin by showing that if
there exists strong -partition with high quality in the graph , then in
the spectral embedding defined by  for any , one can find  well-separated
center points in  such that the balls (of some appropriately
chosen radius) centered at these center points are disjoint and the collection
of these balls is close to any strong -partition.

\begin{lemma}\label{lem:sep}
Let  be an -vertex graph of maximum degree at most .
Let  be any -strong -partition of  with . Let  be
the spectral embedding of  given by (\ref{eq:embedding}). Let
. Then there exists  points
 and a family, , of  subsets of , given by  for , such that the following conditions are satisfied:
\begin{description}
\item{(i)} For , .
\item{(ii)} The elements of  are pairwise disjoint.
\item{(iii)} .
\end{description}
\end{lemma}

To prove Lemma~\ref{lem:sep}, we first give some definitions and introduce some useful tools. 
For any symmetric matrix , let  denote the th
largest eigenvalue of . For any (not necessarily square) matrix
, let  denote the th row vector of .
We will make use of the following pair of facts which are proved in~\ref{sec:facts}:

\begin{fact}\label{fact:eigenvalue-A-B}
For any two  symmetric matrices , if ,  then for any , .
\end{fact}

\begin{fact}\label{fact:rowdifference}
For any two  matrices , if , and , then .
\end{fact}

Now we prove Lemma~\ref{lem:sep}.
\begin{proof}[Proof of Lemma~\ref{lem:sep}]
Recall that for any , ,  and
 denotes the vector that  if . Now for each , define

Further recall that  is the matrix corresponding
to our spectral embedding and that . Let
.
Note that for any , the row vector corresponding to  of 
is . Let .
We have the following claim about the eigenvalues of matrix .

\begin{claim}\label{lem:eigen_lower}
All eigenvalues of  are at least
.
\end{claim}

\begin{proof}
Let  be the  matrix with th column , for each
. Thus, . Now we note that all the
eigenvalues of  are at least . This is true since for any
,

where the second to last inequality follows from the fact that
 for
any vector  and diagonal matrix , which is true since
; and the last equation follows from the observation that .

Now note that for each ,
since , and , it holds that
. On the other hand, by
Lemma~\ref{lem:uniformity}, it holds that


By Fact~\ref{fact:rowdifference}, we have that
.
Then by Fact~\ref{fact:eigenvalue-A-B}, for each ,
. Since all the eigenvalues of
 are at least , it follows that that
all eigenvalues of  are at least
.
\end{proof}

On the other hand, we prove in the following claim that if there exists two vectors
 that are close, then  has at least
one small eigenvalue.

\begin{claim}\label{lem:eigen_upper}
If there exists  such that ,
then  has an eigenvalue at most
.
\end{claim}
\begin{proof}Let  denote the  matrix obtained from  by
replacing each row vector that equals  by vector . Note that
 is singular, and thus has eigenvalue .

Now note that
 since the
absolute value of each entry in  is at most , and also note that
for any ,


Then by Fact~\ref{fact:rowdifference},
.
Now by Fact~\ref{fact:eigenvalue-A-B} and the fact that  has
eigenvalue , we know that at least one eigenvalue of  is at
most .
\end{proof}

Now we prove Item~(i) of the lemma. By the
assumption , it holds
that , which implies
that
.
On the other hand, since , we have that
. Therefore,
by Claim~\ref{lem:eigen_lower} and Claim~\ref{lem:eigen_upper}, we have reached
a contradiction regarding the minimum eigenvalue of .
This implies Item (i) of the lemma, that is, for all ,
. Now for each , define  as required by Item (ii). Let . By Item (i)  are disjoint, proving
Item (ii).

Finally, we prove Item (iii) of the lemma. By
Lemma~\ref{lem:uniformity},

On the other hand, if we let , then

Therefore,
. 

Now we observe that 


Note that for any , it holds that  for some . Since , it holds that , which implies that . This further implies that . Since all  are disjoint, we have that .

Thus, it holds that 

which proves the Item (iii) of the lemma.
\end{proof}

We are now ready to prove our main theorem.

\begin{proof}[Proof of Theorem \ref{thm:main}]
Let , , , and  be as in
Lemma \ref{lem:sep}. Let
    . Let  be the ordered collection of
pairwise disjoint subsets of  output by
the greedy spectral -clustering algorithm
in Figure~\ref{fig:algorithm}. Let  where  is the subset, called \emph{group}, found by the algorithm at the th iteration for .
The set of vertices not covered by any of the clusters in 
plays a special role in our argument which we denote as
. Clearly,
.

We say that a cluster  is {\em touched}
if the algorithm, while computing the centers,
considers a group  with
.
For a cluster , let  be the group
in  that touches  for the {\em first time} in the
algorithm if it is touched at all.
Let  be the support
of , that is,  exists if and only if .
Let .
By permuting the indices of the clusters in , we may assume w.l.o.g.~that .

First we observe that  is a bijection on .
This is because the group , , can intersect at most one
cluster in . The reason is that every cluster in  is
contained inside some ball of radius , the distance between any two centers
of such balls is more than , and each  is contained inside some
ball of radius .

In case , we have
.
This is because  cannot intersect any other cluster in 
but . On the other hand, it holds that , since otherwise the algorithm could have made a better choice by taking the
entire  while computing . Such a choice can be made by taking
 to be all the yet unclustered points that are inside a ball of
radius  centered at any point in ; since  is in a ball of radius
, it follows by the triangle inequality that  will be contained inside
. Therefore, for every , .

In the other case when , we claim that the cluster  can have
at most  vertices. Suppose not. Since  is a bijection on  and
 is a proper subset of , there is a group  with
, which does not intersect any cluster in  for the
first time. Then, it has the only option of intersecting a cluster in  beyond the first time and/or intersect . Since  for all ,  can have at most  vertices. But, the algorithm could have made a better choice
by selecting  while computing  because  by our
assumption. We reach a contradiction.

Let  be the set of indices  such that  does not intersect any cluster
in  for the first time. For any ,
 can only intersect set  and/or set  for
some  such that . This then gives that
.

Using the bijectivity of  on the set ,
we have


Now since the vertices in  are distributed to
the clusters in  to create the output clusters , we have that
.
Observe that by the above analysis,

It follows that .
The following concludes the proof:

\end{proof}

\section{Implementation in Practice}\label{fast-algo}
In this section, we show how to efficiently implement the greedy algorithm in
Figure~\ref{fig:algorithm}. To this end, we discuss how to quickly compute the
first  eigenvectors and how to speed up the step of finding centers by random
sampling.

\subsection{Eigenvectors Computation} In general, the eigenvectors cannot be
computed exactly in polynomial time as the entries may be irrational. However,
in our application it is sufficient to have a set of vectors that well
approximate the eigenvectors. To see this, we note that only the orthonormal
property of eigenvectors  and the fact that for ,  are needed for
all our previous results and analysis. Therefore, if we have a set of 
orthonormal vectors  with
 for , then our previous results still hold if we replace  by
. On the other hand, such set of  orthonormal vectors can be
computed efficiently as shown in the following folklore lemma, the proof of
which follows from a repeated application of the near-linear time algorithm for
computing the second eigenvector given by Spielman and Teng~\cite{DBLP:journals/siammax/SpielmanT14}
and the variational characterization of eigenvalues (see e.g., Corollary 7.6.4
in \cite{Ove14:rounding}).

\begin{lemma}[\bf{folklore}]\label{lem:approx-eigen}
There exists a procedure, \textup{\textbf{ApproxEigen}}, that takes an
-vertex, -edge graph , and an integer , and returns 
orthonormal vectors  such that
, for . The running time of the procedure is .
\end{lemma}

\subsection{A Faster Algorithm}\label{sec:fast-algorithm}
To further speed up the running time, we note that in each iteration  such that , the greedy algorithm in Figure~\ref{fig:algorithm} has to consider all vertices in  to determine the best center.
This may cause the total time in these iterations to be as large as , which is slow in practice since  can be much larger than .
We show how to speed up this step via random sampling.
The main observation is that we can get good center candidates by only computing the number of near-by vertices in the embedding, for vertices from a randomly chosen subset  of , of size about , for any error parameter .
This will reduce the computation time for finding the best centers from  to .
The procedure is summarized in Figure \ref{fig:algorithm2}.
The performance of the algorithm is given in the following theorem (that is similar to Theorem~\ref{thm:main}).

\begin{figure}
\begin{center}
\begin{tabularx}{\textwidth}{l}
\toprule
\textbf{Algorithm: Fast Spectral -Clustering}\\
\textbf{Input: -vertex graph }\\
\textbf{Output: Partition  of }\\
\midrule
Let  be the returned vectors of the procedure \textbf{ApproxEigen}. \\
Let , where . \\
 \\
 \\
for  \\
~~~~~~~~Sample uniformly with repetition a subset , . \\
~~~~~~~~ \\
~~~~~~~~~~~~ \\
~~~~~~~~ \\
~~~~~~~~ \\
Let ,  where  is the smallest index \\
~~~~~~~~~~~~satisfying  for all . \\
Return  \\
\bottomrule
\end{tabularx}
\end{center}
\caption{The fast spectral -clustering algorithm takes an -vertex graph
     with maximum degree at most , and
    an  as input, and returns a partition 
    of . \label{fig:algorithm2}}
\end{figure}

\begin{theorem}\label{thm:algorithm2}
Let . Let  be an -vertex -edge graph with maximum
degree at most . Let , and let
 be any -strong
-partition of  with  for some
sufficiently large constant . Then, on input , with high probability,
the algorithm in Figure \ref{fig:algorithm2} outputs a partition  such
that

Furthermore, the running time of the algorithm is .
\end{theorem}

\begin{proof}[Proof sketch]
Note that , and
that by Lemma~\ref{lem:approx-eigen} the vectors  returned
by \textbf{ApproxEigen} are orthonormal and satisfy
, for . Thus, we can apply the argument of the proof of Lemma~\ref{lem:sep} on
vectors  to find a collection  of pairwise disjoint subsets of , such that
. Since  is sufficiently large
and , we can guarantee that . Let .
Similar to the proof of Theorem~\ref{thm:main}, we define a function
 that maps clusters in  to
clusters in , where  are the ordered
collection of pairwise disjoint subsets of  output by the greedy spectral
-clustering algorithm in Figure~\ref{fig:algorithm2}. Now note that for any
, a vertex from a cluster  of size at least  will be
sampled out with probability at least . This implies that with
high probability the following holds: for each , if , then  exists, since the vertices in the set 
of the th iteration of the algorithm are sampled uniformly at random. The
rest of the argument for the correctness of the algorithm is identical to the
proof of Theorem \ref{thm:main}, and eventually, we can guarantee that .

For the running time of the algorithm, note that by
Lemma~\ref{lem:approx-eigen}, the procedure \textbf{ApproxEigen} takes time
.
In each iteration, we need to sample  vertices, and for each sampled vertex we need to determine the number of near-by vertices in the embedding, which takes  time, as each vertex corresponds to a point in .
This means that the computation in all the  iterations
takes time . Therefore, the total running time of the
algorithm is thus .
\end{proof}

In practice it is common to work with fixed . We note that the fast
randomized algorithm runs in near-linear time for , provided
that the user specifies .

\section{Conclusion}
In this paper we have presented a very simple spectral clustering algorithm that provably approximates any good partition of an input graph, provided that one exists. Further, our preliminary experimental results given in \ref{sec:experiments} indicate that the algorithm gives meaningful output even when the  spectral gap condition is much smaller than what our theorems require.
This provides some evidence that stronger theoretical guarantees may be obtainable, possibly by weakening the gap condition. It is also natural to wonder how small of a separation is necessary for good performance of the algorithm. We currently know of no such lower bounds.
We believe that these are interesting research directions.

One interesting remark is that a qualitatively similar result can be obtained for weighted graphs by introducing the input graph's minimum edge weight, , as a lower bound for the (now weighted) degree in Lemma~\ref{lem:clusterable-eigenvector} and in Claim~\ref{lem:eigen_lower}.
This results in an additional factor of  in the corresponding analog of Theorem~\ref{thm:main}, provided that  of Figure~\ref{fig:algorithm} is also scaled by a factor of .

Unfortunately, due to the appearance of  in the denominator, the result of the previous paragraph is unstable under perturbation of the input graph by a low-weight edge. This instability appears to be an artifact of the analysis. For instance we can show that given any -strong partition , there exists a choice of  such that the resulting clustering is stable under perturbation by a low-weight edge. Such a result essentially follows by replacing  with , the minimum weighted vertex degree among all induced graphs  for . The obstacle with turning this into an algorithm is that to obtain the corresponding error guarantee we must scale  (as it appears in  Figure~\ref{fig:algorithm}) by a factor of , which is not known a priori. It remains an open problem to give a similar algorithm for weighted graphs which is stable under perturbation by a low-weight edge.


\section*{Acknowledgments}
The authors wish to thank James R.~Lee for bringing to their attention a result from the latest version of \cite{DBLP:conf/stoc/LeeGT12}.
This work was partially supported by the NSF grants
CCF 1318595 and CCF 1423230. The second author acknowledges the
support of ERC grant No. 307696 and a 973 program Grant No. 2014CB340302.

\bibliographystyle{plain}
\bibliography{bibfile}
\appendix

\section{Missing Proofs from Section~\ref{sec:concentration}}\label{appendix:eigenvector}

\begin{proof}[Proof of Lemma~\ref{lem:clusterable-eigenvector}]
\label{lem:clusterable-eigenvector-proof}
For any ,

This further gives that

since .
Let us recall a known result (see, e.g., \cite[(1.5), p.~5 and (1.14), p.~13]{Chu97:spectral}) that for any graph ,\footnote{We remark that in \cite{Chu97:spectral}, the summation in the denominator is over all unordered pairs of vertices, while in our context, the summation is over all possible  vertex pairs. Therefore, a multiplicative factor  appears in the numerator in Equation~(\ref{ineq:from-Chung}) compared to the form in \cite[(1.5), p.~5]{Chu97:spectral}.}

where  denotes the second smallest eigenvalue of the normalized Laplacian of . Let us consider the induced subgraph  on .
Since ,
Cheeger's inequality
yields . Therefore, if we apply this bound to inequality (\ref{ineq:from-Chung}), then,

Combining this with the fact that  where the last inequality follows from inequality (\ref{eqn:lambdaphiout}), we have that

Next, since  implies that  for all . Using the bound above we obtain:

This completes the proof of the lemma.
\end{proof}


\section{Missing Proofs from Section~\ref{sec:clustering}}
\label{sec:facts}

\begin{proof}[Proof of Fact~\ref{fact:eigenvalue-A-B}]
Since ,
then the Frobenius norm  of  is at
most , and therefore, the induced 2-norm
 of  is at most
. By Weyl's inequality~\cite{Tao12:randommatrix}, for any
, .
\end{proof}

\begin{proof}[Proof of Fact~\ref{fact:rowdifference}]
For simplicity, let  and
. Note that the th entry of  and  are 
and , respectively, and that

Therefore,
. \end{proof}

\section{Experimental Evaluation}\label{sec:experiments}
Results from our greedy -clustering implementation are shown
in Figures~\ref{experiment1}, \ref{experiment2}, \ref{experiment3}.
Cluster assignments for graphs are shown as colored nodes.
In the case where the graph comes from a triangulated surface, we have extended
the coloring to a small surface patch in the vicinity of the node. Each
experiment includes a plot of the eigenvalues of the normalized Laplacian (y-axis), by eigenvector number (x-axis).
A small rectangle on each plot highlights the corresponding spectral gap
between  and .

\paragraph*{Multiple spectral gaps}
Recall that graphs
which have a sufficiently large spectral gap between the -th and -st
eigenvalues admit a strong clustering~\cite{DBLP:conf/soda/GharanT14}.
Figure~\ref{experiment1} shows the result of our algorithm on a graph
with two prominent spectral gaps,  (left) and  (right).

This graph is sampled from the following generative model.
Let  be disjoint vertex sets of equal size, depicted as circles.
Every edge with both endpoints in the same  appears with probability ,
every edge between  and  appears with
probability , and every other edge appears with probability , for
some . The resulting graph admits a strong -partition,
for any . This fact is reflected in the output of our algorithm.

We remark that to achieve a sufficiently large gap at  many intra-circle
edges are necessary, which makes the resulting figures too dense.
To make the plots readable, we have displayed only a subsampling of these edges.

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\includegraphics[width=0.47\textwidth]{circleplot-k2.jpg} &
\includegraphics[width=0.47\textwidth]{circleplot-k5.jpg}
\end{tabular}
\end{center}
\caption{Clustering a graph with multiple spectral gaps. The graph has 
nodes, with edges chosen according to our model below. Clustering is performed
with  for  and .}
\label{experiment1}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\includegraphics[width=0.45\textwidth]{pathbased.jpg} &
\includegraphics[width=0.45\textwidth]{pathbased-kmeans.jpg} \\
\includegraphics[width=0.45\textwidth]{zahn.jpg} &
\includegraphics[width=0.45\textwidth]{zahn-kmeans.jpg} \\
\includegraphics[width=0.45\textwidth]{aggregation.jpg} &
\includegraphics[width=0.45\textwidth]{aggregation-kmeans.jpg} \\
\end{tabular}
\end{center}
\caption{Visual comparison of cluster assignments between Greedy -Center
and -means. Clustering was performed on graphs with radius .
The scores in the titles indicate percent correct classification with respect
to the ground truth.}
\label{experiment2}
\end{figure}

\paragraph*{Comparison with -means}
In Figure~\ref{fig:expresults} we compare the greedy approach to -means on
the spectral embedding. Our experiments build a graph on two and
three-dimensional euclidean point-cloud data by selecting a threshold value,
, and connecting any two points which are no further than . Additionally,
any singletons are removed. A key feature of the chosen point-cloud data is that
it comes with ground truth labeling which we lift to the corresponding
graph.

\begin{figure}
\begin{center}
\begin{tabular}{ l | c c c c c }
  Data set     &  n   & k & D   & Greedy -C & -means	\\
\hline
  pathbased   & 297  & 3 & 2   & 85.185     & 85.522        \\
  jain        & 373  & 2 & 3   & 100        & 100           \\
  zahn        & 399  & 6 & 3   & 65.414     & 71.471        \\
  aggregation & 788  & 7 & 2   & 99.746     & 96.063        \\
  LSun        & 400  & 3 & 1   & 93.250     & 93.250        \\
  Tetra       & 400  & 4 & 1   & 100        & 100           \\
  Hepta       & 212  & 7 & 5   & 44.811     & 57.849        \\
  Chainlink   & 1000 & 2 & 1   & 80.600     & 83.196        \\
  EngyTime\footnotemark
              & 4082 & 2 & 0.5 & 96.546     & 96.423        \\
  TwoDiamonds & 800  & 2 & 1   & 100        & 100           \\
\end{tabular}
\end{center}
\caption{Percentage agreement to ground truth for Greedy -Clustering and
-means. The results reported in the -means column are the mean percent
agreement over  trials where -means was initialized by random selection
of  points in the embedding. The  parameter for Greedy -Clustering was
given by the formula . The first four input
graphs are based on data sets of the same name from \cite{dataset:shapesets},
the rest are based on similarly named point-cloud data from FCPS.
\cite{dataset:FCPS}. \label{fig:expresults}}
\end{figure}
\footnotetext{Only the largest component was used for clustering.}

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\includegraphics[width=0.4\textwidth]{human-k-6.jpg} &
\includegraphics[width=0.4\textwidth]{human-kmeans-k-6.jpg} \\
\end{tabular}
\end{center}
\caption{A  clustering performed on the -skeleton of triangulated mesh
(), using . Here the two algorithms agree
on  of the vertices.}
\label{experiment3}
\end{figure}

\end{document}
