\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}
\usepackage{makecell}
\usepackage[font=small]{caption}
\usepackage{overpic}

\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{subcaption}
\usepackage{color}
\usepackage{diagbox}
\usepackage{array}
\usepackage{overpic}
\usepackage[table]{xcolor}

\makeindex

\graphicspath{{./figs/}}

\usepackage{pifont}
\usepackage{overpic}
\usepackage{url}

\newcommand{\nameofatten}{Re-attention}

\newcommand{\sArt}{state-of-the-art~}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\AddImg}[2]{\subfigure[#1]{\includegraphics[height=.2\linewidth]{#2}}}
\newcommand{\AddFigure}[4]{{\subfigure[#1]{\label{#2}\includegraphics[width=#3\linewidth]{#4}}}}

\newcommand{\addFig}[1]{}
\newcommand{\addFigs}[1]{}

\definecolor{mygreen}{RGB}{0,100,0}
\definecolor{myred}{RGB}{150,0,0}
\definecolor{myblue}{RGB}{60,60,250}

\newcommand{\myPara}[1]{\vspace{.05in}\noindent\textbf{#1}}
\newcommand{\QB}[1]{\textcolor{blue}{[QB: #1]}}
\newcommand{\BY}[1]{\textcolor{olive}{[BY: #1]}}



\usepackage{hyperref}
\hypersetup{pagebackref=false,colorlinks=true,linkcolor=myred,citecolor=myblue,bookmarks=false,urlcolor=black}

\iccvfinalcopy 

\def\iccvPaperID{4046} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Depthwise Scalable Vision Transformer on ImageNet-1k Without Extra Dataset}
\title{Does a Deep Vision Transformer perform Better?}
\title{Two Vision Transformers Blocks are Larger Than One? }
\title{DeepViT: Towards Depth-Scalable Vision Transformer}
\title{DeepViT: Towards Deeper Vision Transformer}



\author{Daquan Zhou\textsuperscript{1},
Bingyi Kang\textsuperscript{1},
Xiaojie Jin\textsuperscript{2},
Linjie Yang\textsuperscript{2},\\
Xiaochen Lian\textsuperscript{2},
Zihang Jiang\textsuperscript{1},
Qibin Hou\textsuperscript{1},
Jiashi Feng\textsuperscript{1} \\
\textsuperscript{1}National University of Singapore,\quad \textsuperscript{2}ByteDance US AI Lab  \\
\texttt{\small {\{zhoudaquan21, xjjin0731, lianxiaochen, yljatthu, andrewhoux\}}@gmail.com}
\\
\texttt{\small{jzihang, kang, elefjia}@nus.edu.sg}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Vision transformers (ViTs) have been successfully applied in image classification tasks recently.
In this paper, we show that, unlike convolution neural networks (CNNs) that can be improved by
stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper.
More specifically, we empirically observe that such scaling difficulty is caused by the \emph{attention collapse} issue: as the transformer goes deeper, 
the attention maps gradually become similar and even much the same after certain layers. 
In other words, the feature maps tend to be identical in the top layers of
deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain.
Based on above observation, we propose a simple yet effective method, named \emph{\nameofatten{}}, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. 
The proposed method makes it feasible to train deeper ViT models with consistent performance improvements
via minor modification to existing ViT models.
Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification
accuracy  can be improved by 1.6\% on ImageNet. 
Code is publicly available at \url{https://github.com/zhoudaquan/dvit_repo}.
   
   
\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \centering
    \small
\begin{overpic}[width=\linewidth]{figures/teaser.pdf}
\put(36.5, 55.4){\cite{dosovitskiy2020image}}
    \end{overpic}
    \caption{Top-1 classification performance of vision transformers (ViTs) \cite{dosovitskiy2020image} on ImageNet with different network depth 
    \{12, 16, 24, 32\}.
    Directly scaling the depth of ViT by stacking more transformer blocks cannot monotonically  increase the performance. Instead, the model performance  saturates when  going deeper. In contrast, with the proposed \nameofatten, 
    our DeepViT model   successfully achieves better  performance when it goes deeper. 
    }
    \label{fig:depth_vs_acc}
\end{figure}

Recent studies \cite{dosovitskiy2020image,touvron2020training} have demonstrated that transformers
\cite{vaswani2017attention} can be successfully applied to vision tasks \cite{krizhevsky2012imagenet} with competitive
performance compared with convolutional neural networks
(CNNs)~\cite{he2016deep,tan2019efficientnet}.
Different from CNNs that aggregate global information by stacking multiple  convolutions (\eg, )
\cite{he2016deep,he2016identity}, 
vision transformers (ViTs) \cite{dosovitskiy2020image} take  advantages of the self-attention (SA) mechanism \cite{vaswani2017attention} to capture spatial patterns and  non-local dependencies.
This allows ViTs to aggregate rich global information without handcrafting layer-wise local feature extractions as  CNNs and thus achieves better performance. 
For example, as shown in \cite{touvron2020training}, a 12-block ViT model with 22M learnable
parameters achieves better results than the ResNet-101 model which has more than 30 bottleneck
convolutional blocks in ImageNet classification. 


The recent progress of deep CNN models is largely driven by training  very deep models with a large number of layers which is enabled by novel model architecture designs \cite{he2016deep,xie2017aggregated,srinivas2021bottleneck,liu2020improving,zhang2020resnest}. 
This is because a deeper CNN can learn richer and more complex representations for the input images and provide better performance on vision tasks \cite{bengio2013representation,zhang2018network,rebuffi2017icarl}. Thus, how to effectively scale CNNs to be deeper is an important theme in recent deep learning fields, which stimulates the   techniques like residual learning \cite{he2016deep}. 

Considering the remarkable    performance of shallow ViTs, a natural question arises: 
\emph{can we further improve performance of  ViTs by making it deeper, just like  CNNs?} Though it  seems to be  straightforward at the first glance, the answer may not be trivial since ViT is essentially different from CNNs in its heavy reliance on the self-attention mechanism. 
To settle the question, we  investigate in detail the scalability of ViTs along depth in this work.



We start with a pilot study on ImageNet to investigate how the performance of ViT changes with increased model depth. In Fig.~\ref{fig:depth_vs_acc}, we show the performance of ViTs \cite{dosovitskiy2020image}
with different block numbers (green line), ranging from 12 to 32.
As shown, as the number of transformer blocks increases, the model performance does not
improve accordingly. To our surprise, the ViT model with 32 transformer blocks performs even worse than the one with
24 blocks.
This means that directly stacking more transformer blocks as performed in CNNs~\cite{he2016deep}
is inefficient at enhancing ViT models.
We then dig into the cause of this phenomenon. We empirically observed that as the depth of ViTs increases,
the attention maps, used for aggregating the features for each transformer block,  tend to be overly similar after certain layers, which makes the representations stop evolving after certain layers. We name this specific issue as \emph{attention collapse}.
This indicates that as the ViT goes deeper, the self-attention mechanism becomes less effective for generating diverse attentions to capture rich representations. 



To resolve the attention collapse issue and effectively scale the vision transformer to be deeper, 
We present a simple yet effective self-attention mechanism, named as \nameofatten{}.
Our \nameofatten{} takes advantage of the multi-head self-attention(MHSA) structure and regenerates attention maps by exchanging the information from different attention heads in a learnable manner.
Experiments show that, Without any extra augmentation and regularization policies, 
simply replacing the MHSA module in ViTs with \nameofatten{} allows us to 
train very deep vision transformers with even 32 transformer blocks with consistent improvements
as shown in Fig.~\ref{fig:depth_vs_acc}.
In addition, we also provide ablation analysis to help better understand of the role
of \nameofatten{} in scaling vision transformers.


To sum up, our contributions are as follows:
\begin{itemize}
    \setlength\itemsep{0em}
    \item We deeply study the behaviour of vision transformers
    and observe that they cannot continuously benefit from stacking more layers as CNNs. We further identify the underlying reasons behind such a counter-intuitive phenomenon and conclude it as \emph{attention collapse} for the first time.


    \item We present \nameofatten{}, a simple yet effective attention mechanism
    that considers information exchange among different attention heads.


    \item To the best of our knowledge, we are the first to successfully train a 32-block ViT
    on ImageNet-1k from scratch with consistent performance improvement.
    We show that by replacing the self-attention module with our \nameofatten{}, 
    new state-of-the-art results can be achieved on the ImageNet-1k dataset 
    without any pre-training on larger datasets.
\end{itemize} \section{Related Work}
\subsection{Transformers for Vision Tasks}

Transformers \cite{vaswani2017attention} are initially used for machine translation which replace the recurrence and convolutions entirely with self-attention mechanisms \cite{ramachandran2019stand,hu2019local,zhao2020exploring,ho2019axial,wang2020axial,jiang2020convbert} and achieve outstanding performance. Later,  transformers become the dominant models   for  various  natural language processing (NLP) tasks \cite{brown2020language,radford2019language,devlin2018bert,liu2019roberta}. Motivated by  their success  on the NLP tasks, recent  researchers attempted to combine the self-attention mechanism into CNNs   for computer vision tasks \cite{wang2018non,carion2020end,chen2020pre,sun2019videobert,lu2019vilbert,zheng2020rethinking,zhao2020point}.. Those achievements also stimulate interests of the community in building purely transformer-based models (without convolutions and inductive bias) for vision tasks.  
The vision transformer (ViT)~\cite{dosovitskiy2020image} is among the first attempt that uses the pure transformer architecture to achieve competitive performance with CNNs on the image classification task. However, due to the large model complexity, ViT needs to be pre-trained on larger-scale  datasets (e.g., JFT300M) for  performing  well on the ImageNet-1k dataset. To solve the data efficiency issue, DeiT \cite{touvron2020training}   deploys knowledge distillation to   train the model with a larger pre-trained teacher model. In this manner, vision transformer can perform well on ImageNet-1k without the need of pre-training on larger dataset. Differently, in this work, we target at a different problem with ViT, \ie, how to effectively scale ViT to be deeper. We propose  a new design for the self-attention mechanism so that it can perform well on vision tasks without the need of extra data, teacher networks,
and the domain specific inductive bias.


\subsection{Depth Scaling  of CNNs   }


Increasing the network depth   of a CNN model is deemed to be an effective way to improve the model performance
\cite{simonyan2014very,szegedy2015going,szegedy2016rethinking,he2016identity,huang2017densely}. 
However, very deep CNNs are generally harder to train to perform significantly better   than the shallow ones in the past \cite{glorot2010understanding,zagoruyko2016wide}.  How to effectively scale up the CNNs in depth was a long-standing and challenging problem \cite{huang2018gpipe}. 
The recent progress of CNNs largely benefits from novel architecture design strategies 
that make training deep CNNs more effective~\cite{he2016deep, tan2019efficientnet, howard2019searching, hou2021coordinate, tan2019mixconv,zhou2020rethinking}.
Transformer-alike models have modularized architectures and thus can be easily made deeper by
repeating the basic transformer blocks or
using larger embedding dimensions \cite{brown2020language,lepikhin2020gshard}. However, those straightforward scaling strategies only work well   with larger datasets and stronger augmentation policies \cite{zhong2020random,zhang2017mixup,yun2019cutmix} to alleviate the brought training difficulties.
In this paper, we observed that with the same dataset, 
the performance of vision transformers do saturate as the network depth rises.
We rethink the self-attention mechanism and present a simple but effective approach
to address the difficulties in scaling vision transformers. 


 \section{Revisiting Vision Transformer} \label{sec:revisit_vit}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/comps.pdf}
\caption{Comparison between the (a) original ViT with  transformer blocks and (b) our proposed DeepViT model. Different from ViT, DeepViT replaces the self-attention layer within  the transformer block  with the proposed \nameofatten{} which effectively addresses the attention collapse issue and enables training deeper ViTs. More details are given in Sec. \ref{subsec:head_gen}.
    }
    \label{fig:diagram}
\end{figure}

A vision transformer (ViT) model \cite{touvron2020training,dosovitskiy2020image}, as depicted in Fig.~\ref{fig:diagram}(a),
is composed of three main components:
a linear layer for patch embedding (\ie, mapping the high-resolution input image
to a low-resolution feature map), a stack of transformer blocks with multi-head self-attention
and feed-forward layers for feature encoding,
and a linear layer for classification score prediction.
In this section, we first review its unique transformer blocks,
in particular the self-attention mechanism, and then we provide studies on 
the collapse problem of self-attention. 


\subsection{Multi-Head Self-Attention}
Transformers \cite{vaswani2017attention} were extensively used in natural language  
for encoding a sequence of input word tokens into a sequence of embeddings. 
To comply with such sequence-to-sequence  learning structure when processing images, 
ViTs first divide an input image into multiple  patches uniformly and
encode each patch into a token embedding. 
Then, all these tokens, together with a class token,  are fed into a stack of transformer blocks. 

Each transformer block consists of a multi-head self-attention (MHSA) layer 
and a feed-forward  multi-layer perceptron (MLP). The MHSA   generates a trainable associate memory with a query () and a pair of key ()-value () pairs to an output
via linearly transforming the input.  
Mathematically, the output of a MHSA is calculated by:

where  is a scaling factor based on the depth of the network. The output of the MHSA is then normalized and   fed into the MLP to generate the input to the next block. 
In the above self-attention,  and  are multiplied to generate the attention map, which represents the correlation
between all the tokens within each layer. It is used to retrieve and combine the embeddings in the value  . In the following, we particularly analyze the role of the attention map in   scaling the ViT. For convenience,  we use  to denote the attention map, with  being the number of SA heads and  the number of tokens. For the -th SA head, the attention map is computed as   with  and  from the corresponding head.
When the context is clear, we omit the subscript .


\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/combo_similarity_similarity_vs_depth_inter_head_similarity.png}
\vspace{-20pt}
\end{center}
  \caption{(a) The similarity ratio of the generated self-attention maps across different layers. The visualization is based on ViT models with 32 blocks pre-trained on ImageNet. For visualization purpose, we plot the ratio of token-wise attention vectors with similarity in Eqn.~\eqref{eqn:cross_layer_similarity} larger than   the average similarity within nearest   transformer blocks. As can be seen, the similarity ratio is   larger than 90\% for blocks after the 17th one. (b) The ratio  of similar blocks to the total number of blocks increases when the depth of the ViT model increases.   (c) Similarity of attention maps from different heads within the same block. The similarity between different heads within the blocks is all lower than 30\% and they present sufficient diversity.}
  \vspace{-10pt}
\label{fig:atten_similarity}
\end{figure*}

\subsection{Attention Collapse}



Motivated by the success of deep CNNs \cite{he2016deep,simonyan2014very, tan2019efficientnet, tan2019mixconv}, we conduct   systematic study in the changes of the performance of ViTs as depth increases.
Without loss of generality, we first fix the hidden dimension  and 
the number of heads to 384 and 12 respectively\footnote{Similar phenomenon can also
be found when we vary the hidden dimension size according to our experiments.}, following the common  practice in~\cite{touvron2020training}.
Then we stack different number of transformer blocks (varying from 12 to 32) to build multiple ViT models corresponding to different depths. The overall performances for image classification are evaluated on ImageNet~\cite{krizhevsky2012imagenet} and summarized in Fig.~\ref{fig:depth_vs_acc}.
As evidenced by the performance curve, we surprisingly find that the classification accuracy improves slowly and saturates fast as the model goes deeper. More specifically, we can observe that the improvement stops after employing 24 transformer blocks. This phenomenon demonstrates that  existing ViTs have difficulty in gaining benefits from deeper architectures. 


Such a problem is quite counter-intuitive and worth exploration, as similar issues (\ie, how to effectively train a deeper  model) have also been observed for CNNs at its early development stage~\cite{he2016deep}, but properly solved later \cite{he2016deep,he2016identity}. By taking a deeper look into the transfromer architecture, we would like to highlight that the self-attention mechanism plays a key role in ViTs, which makes it significantly different from CNNs. Therefore,  we  start with  investigating how the self-attention, or more concretely, the generated attention map  varies as the model goes deeper. 


To measure the evolution of the attention maps over layers, we compute the following
cross-layer similarity between the attention maps from different layers:

where  is the cosine similarity matrix between the attention map of layers  and . Each element  measures the similarity of attention for head  and token . {Consider one specific self-attention layer and its -th head,  is a -dimensional vector representing how much the input token  contributes to each of the  output tokens. 
, thus, provides an appropriate measurement on how the contribution of one token varies from layer  to . When  equals one, it means that token  plays exactly the same role for self-attention in layers  and .}




Given Eqn.~\eqref{eqn:cross_layer_similarity}, we then train a ViT model with 32 transformer blocks on ImageNet-1k and  investigate the above similarity  among all the attention maps. As shown in Fig.~\ref{fig:atten_similarity}(a), the ratio of similar attention maps in   after the 17th block is larger than 90\% .
This indicates that the learned attention maps afterwards are similar and the transformer block may degenerate to an MLP. As a result, further stacking such degenerated MHSA
may introduce the model rank degeneration issue (\ie, the rank of  the model parameter tensor  from multiplying the layer-wise parameters together will decrease) and limits the model learning capacity. This is also validated by our analysis on the degeneration of  learned features as shown below.
Such observed attention collapse could be one of the reasons for the observed performance saturation of ViTs. 
To further validate the existence of  this phenomenon for ViTs with different depths, we  conduct the same experiments on ViTs with 12, 16, 24 and 32 transformer blocks respectively and calculate the number of blocks with similar attention maps. The results  shown in Fig.~\ref{fig:atten_similarity}(b) clearly demonstrate the  ratio of the number of similar attention map blocks to the total number of blocks increases when adding more transformer blocks. 

\begin{figure}[t] 
\small
\begin{center}
\includegraphics[width=0.49\linewidth]{figures/feature_atten_similarity.png}
\includegraphics[width=0.49\linewidth]{figures/feature_atten_similarity_reatten.png}
\end{center}
\vspace{-15pt}
  \caption{
  (\textbf{Left}): Cross layer similarity of attention map and features for ViTs. The black dotted line shows the cosine similarity between feature maps of the last block and each of the previous blocks. The red dotted line shows the ratio of similar attention maps of adjacent blocks. The visualization is based on a 32-block ViT model   pre-trained on ImageNet-1k. (\textbf{Right}): Feature map cross layer cosine similarity for both the original ViT model and ours. As can be seen, replacing
  the original self-attention with \nameofatten{} could reduce the feature map similarity significantly. }
\label{fig:feature_similarity}
\end{figure}

To understand how the  attention collapse may hurt the ViT model performance, we further  study how it affects feature learning of the deeper layers. 
{For a specific 32-block ViT model, we compare the final output features  with the outputs of  each intermediate transformer block  by investigating their cosine similarity. The results in Fig.~\ref{fig:feature_similarity}  demonstrate that the similarity is quite high and the  learned features stop evolving 
after the 20th block. There is a close correlation between the increase of attention similarity and feature similarity.} 
This observation indicates that attention collapse is responsible for the non-scalable issue of ViTs.











\section{\nameofatten{} for Deep ViT}

As revealed above, one major obstacle in scaling up ViT to a deeper one is the attention
collapse problem. In this section, we present two solution approaches, one is to increase the hidden dimension for computing self-attention and the other one is a novel re-attention mechanism. 

\subsection{Self-Attention in Higher Dimension Space}
One intuitive solution to conquer attention collapse is to increase the embedding dimension of each token. This will augment the representation capability of each token embedding to encode more information. As such, the resultant attention maps can be more diverse and the similarity between each block's attention map could be reduced. {Without loss of generality, we verify this approach empirically by conducting a set of experiments based on ViT models with 12 blocks for quick experiments.} 
Following previous transformer based works \cite{vaswani2017attention,dosovitskiy2020image}, four embedding dimensions are selected,  ranging from 256 to 768.  The detailed configurations and the results are shown in Tab.~\ref{tab:increasing_dim}. 

\begin{figure}[t] 
\begin{center}
\includegraphics[width=0.8\linewidth]{figures/similarity_vs_embedding_v3.png}
\end{center}
\vspace{-15pt}
  \caption{Impacts of embedding dimension on the similarity of generated self-attention map across layers. As can be seen, the number of similar attention maps decreases with increasing embedding dimension. However, the model size also increases rapidly. 
}
\label{fig:similar_block_vs_embedding}
\end{figure}



\begin{table}[t]
\footnotesize
\caption{Top-1 accuracy on ImageNet-1k dataset of vision transformer with different embedding dimensions. The number of model parameters increase quadratically with the embedding dimension. The number of similar attention map blocks with different embedding dimensions are shown in Figure \ref{fig:similar_block_vs_embedding}}. 
\label{tab:increasing_dim}
\centering
\setlength\tabcolsep{1.7mm}
\renewcommand\arraystretch{1.0}
\begin{tabular}{ccccc}
\toprule
\bf Model
&\bf \#Blocks
&\bf Embed Dim.
&\bf \#Param. (M)
&\bf Top-1 Acc.(\%)
\\ \midrule ViT  & 12 & 256 & 8.15 & 74.6 \\
 ViT  & 12 & 384 & 18.51 & 77.86 \\
 ViT  & 12 & 512 & 33.04 & 78.8 \\
 ViT  & 12 & 768 & 86 & 79.3 \\
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}



From Fig.~\ref{fig:similar_block_vs_embedding} and Tab.~\ref{tab:increasing_dim}, one can see that the number of blocks with similar attention maps is  reduced and the attention collapse is alleviated by increasing the  embedding dimension. Consequently, 
the model performance is also increased accordingly. This validates our core hypothesis\textemdash the attention collapse is the main bottleneck for scaling ViT. Despite its effectiveness, increasing the embedding dimension also increases the computation cost   significantly and the brought performance  improvement tends to diminish. Besides, a larger model (with higher embedding dimension) typically needs  more data for training, suffering  the   over-fitting risk  and   decreased  efficiency. 





\subsection{\nameofatten{}}
\label{subsec:head_gen}

It has been demonstrated in Sec.~\ref{sec:revisit_vit} that the similarity between
attention maps across different transformer blocks is high, especially for deep
layers.
However, we find  the similarity of attention maps from different heads of
the same transformer block is quite small, as shown in Fig.~\ref{fig:atten_similarity}(c).
Clearly, different heads from the same self-attention layer 
focus on different aspects of the input tokens.
Based on this observation, we propose to establish cross-head communication 
to re-generate the attention maps and train deep ViTs to perform better. 




Concretely, we  use the attention maps from the heads as basis and generate a new set of attention maps by dynamically aggregating them. To achieve this, we define a learnable transformation matrix  and then use it to mix the multi-head attention maps into  re-generated new ones,  before being multiplied with . Specifically, the \nameofatten{} is implemented by:

where transformation matrix  is multiplied to the self-attention map  along the head dimension. Here  is a normalization
function used to reduced the layer-wise variance.  is end-to-end learnable. 




\myPara{Advantages:} 
The advantages of the proposed \nameofatten{} are two-fold.
First of all, compared with other  possible attention augmentation methods,
such as randomly dropping some elements of the attention map or tuning  SoftMax temperature,
our \nameofatten{} exploits the interactions among different
attention heads  to collect their complementary information  and better improves the attention map diversity. This is  also verified by our  following experiments.
Furthermore, our \nameofatten{} is effective and easy to implement.
It needs only a few lines of code and negligible computational overhead
compared to the original self-attention. Thus it is much more efficient than the approach of increasing embedding dimension. 








 \section{Experiments}
\label{exp:experiments}



In this section, we first conduct experiments to further demonstrate the attention collapse
problem. Then, we give extensive ablation analysis to show the advantages of the proposed \nameofatten{}. By incorporating \nameofatten{} into the transformers, we design two modified version of vision transformers and name them as deep vision transformers (DeepViT). Finally, we compare the proposed DeepViT models against the latest state-of-the-arts (SOTA).

\subsection{Experiment Details}





To make a fair comparison, we first tuned a set of parameters 
for training the ViT base model and then use the same set of hyper-parameters for all the ablation experiments.
Specifically, we use AdamW optimizer \cite{loshchilov2017decoupled} and cosine  learning rate decay policy with an
initial learning rate of 0.0005. We use 8 Telsa-V100 GPUs and train the model for 300 epochs using Pytorch \cite{paszke2019pytorch} library. 
The batch size is set to 256.  
We use 3 epochs for learning rate warm-up \cite{loshchilov2016sgdr}. We also use some  augmentation techniques such as mixup \cite{zhang2017mixup} and random augmentation \cite{cubuk2020randaugment} to boost the performance of baseline models following \cite{zhang2020resnest}. 
When comparing with other methods, we adopt the same set of hyper-parameters as used by the target models. 
We report results on the ImageNet dataset \cite{krizhevsky2012imagenet}. For all experiments, the image size is set to be 224224. 
To study the scaling capability of current transformer blocks,
we set the embedding dimension to 384 and the expansion ratio 3
for the MLP layers. We use 12 heads for all the models. More detailed configurations are shown in Tab. \ref{tab:ablation_depth}.

\begin{table}[h]
\footnotesize
\caption{Baseline model specifications. All ablation experiments are based on the ViT models with different number of blocks. The `\#B' in `ViT-\#B' denotes the number of transformer blocks in the model.}
\label{tab:ablation_depth}
\centering
\begin{tabular}{lcccc}
\toprule
\bf Model
&\bf \#Blocks
&\bf \#Embeddings

&\bf MLP Size
&\bf Params. (M)
\\ \midrule ViT-16B & 16 & 384 & 1152 & 24.46 \\
 ViT-24B & 24 & 384 & 1152 & 36.26 \\
 ViT-32B & 32 & 384 & 1152 & 48.09 \\
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}


\subsection{More Analysis on Attention Collapse}

In this section, we show more  analysis on the attention map similarity  and study how the collapsed attention maps affect the model performance. 


\myPara{Attention reuse:}
As discussed above, when the model goes deeper, the attention maps of the deeper blocks 
become highly similar. This implies that adding more blocks on a deep ViT model may not improve  the model performance.
To further verify this claim, we design an experiment to reuse the attention maps computed 
at an early block of ViT to replace the ones after it. 
Specifically, we run experiments on the ViT models with 24 blocks and 32 blocks
but share the  and   values (and the resulted attention maps) of the  last ``unique'' block   to  all the blocks afterwards.
The ``unique'' block is defined as the block whose  attention map's similarity ratio
with adjacent layers is smaller than 90\%. More implementation details can be found in the supplementary material. 
The results are shown in Tab.~\ref{tab:ablation_sharing}. 
Surprisingly, for a ViT model with 32 transformer blocks, when we use the same   and  values for 
the last 15 blocks, the performance degradation is negligible.
This implies the attention collapse problem indeed exists
and reveals the inefficacy in adding more blocks when the model is deep.  

\begin{table}[h]
\footnotesize
\caption{ImageNet top-1 accuracy of the ViT models with shared self-attention maps. `\#Shared blocks' denotes the number of the transformer blocks that share the same attention map.  }
\label{tab:ablation_sharing}
\centering
\begin{tabular}{ccccc}
\toprule
\bf \#Blocks
&\bf \#Embeddings
&\bf \#Shared blocks
&\bf Top-1 Acc. (\%)
\\ \midrule 24 & 384 & 0 & 79.3 \\
24 &  384 & 11 & 78.7 \\ 
\midrule
32 &  384   & 0 & 79.2 \\ 
32 &  384   & 15 & 79.2 \\ 
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}

\myPara{Visualization:} To more intuitively  understand   the attention map collapse across layers, we visualize the learned attention maps from different blocks of the original ViT \cite{dosovitskiy2020image}.
We take a 32-block ViT model as an example and pre-train it on ImageNet. The visualization of the attention maps with original MHSA and \nameofatten{}  are shown 
in Fig.~\ref{fig:vis_atten_map}. 
{It can be observed that the original MHSA learns the local relationship  among the adjacent 
  patches in  the shallow blocks and the attention maps tend to expand to cover more patches gradually. In the deep blocks, the MHSA learns   nearly uniform global attention maps with high similarity. Differently, after implementing \nameofatten{}, the attention maps at deep blocks keep the diversity and have small similarities from adjacent blocks.}

\renewcommand{\addFig}[1]{\includegraphics[width=0.135\linewidth]{figures/atten_visualize/#1}}
\renewcommand{\addFigs}[1]{\addFig{layer_#1_head_mean_bs_mean.jpg}}
\newcommand{\addFigsRe}[1]{\addFig{layer_#1_head_mean_bs_mean_re.jpg}}

\begin{figure*}[t]
  \centering
  \footnotesize
  \setlength\tabcolsep{0.2mm}
  \renewcommand\arraystretch{1.0}
  \begin{tabular}{cccccccc}
    \rotatebox{90}{~Self-attention} & 
    \addFigs{0} & 
    \addFigs{4} & 
    \addFigs{10} & 
    \addFigs{18} & 
    \addFigs{22} &
    \addFigs{28} &
\addFigs{29} \\
    \rotatebox{90}{~~Re-attention} & 
    \addFigsRe{0} & 
    \addFigsRe{4} &
    \addFigsRe{10} & 
    \addFigsRe{18} & 
    \addFigsRe{22} &
    \addFigsRe{28} &
\addFigsRe{29} \\
     & Block 1 & Block 4 & Block 11 & Block 18  &Block 23 & Block 29 
     & Block 30
     \\
  \end{tabular}
  \vspace{3pt}
  \caption{Attention map visualization of the selected blocks of the baseline  ViT model with 32 transformer blocks. The first row is based on original Self-attention module and the second is based on \nameofatten{}. As can be seen, the model   only learns local patch relationship at its  shallow blocks with the rest of attention values  near to zero. Though their the scope  increases gradually as the block goes deeper,  the attention maps tend to become   nearly uniform and thus lose diversity. After adding \nameofatten{}, the originally similar attention maps are changed to be diverse as shown in the second row. Only at the last block's attention map, a nearly uniform attention map is learned. }
  \label{fig:vis_atten_map}
\end{figure*}






\subsection{Analysis on \nameofatten{}}
\label{exp:add_variations}

{In this subsection, we present two   straightforward  modifications to the current self-attention mechanism  as baselines.  We then conduct a series of comparison experiments to show the advantages
of our proposed \nameofatten{}.}


\begin{figure}[t]
    \centering
    \includegraphics[width  =\linewidth]{figures/reatt.pdf}
    \caption{(\textbf{Left}): The original self-attention mechanism; (\textbf{Right}): Our proposed re-attention mechanism. As shown, the original attention map is mixed via a learnable matrix  before multiplied with values.}
    \label{fig              :reatt}
\end{figure}

\myPara{Re-attention v.s. Self-attention:}
We first evaluate the effectiveness of \nameofatten{} by comparing to the pure ViT models
using the same set of training hyper-parameters.
We directly replace the self-attention module in ViT with \nameofatten{} and show the results
in Tab.~\ref{tab:head_regen} with different number of transformer blocks.
As can be seen, the vanilla ViT architecture suffers   performance saturation
when adding more transformer blocks. 
This phenomenon coincides with our observations that the number of blocks with similar attention maps increases with the depth as shown in Fig. \ref{fig:atten_similarity}(b). 
Interestingly, when replacing the   self-attention with
our proposed \nameofatten{}, the number of similar blocks are all reduced to be zero and the performance rises consistently
as the model depth increases.
The performance gain is especially significant for deep ViT with 32 blocks. This might be explained by the fact that the 32 block ViT model has the largest number of blocks with similar attention maps and the improvements should be proportional to the number similar blocks in the model.
These experiments demonstrate that the proposed \nameofatten{}
can indeed solve the attention collapse problem
and thus enables training a very deep vision transformer without extra datasets or augmentation policies.

\begin{table}[h]
\footnotesize
\caption{ImageNet Top-1 accuracy of deep ViT (DeepViT) models with \nameofatten{} and different number of transformer blocks. }
\label{tab:head_regen}
\centering
\begin{tabular}{lcccc}
\toprule
\bf Model
&\bf \#Similar Blocks
&\bf Param. (M)
&\bf Top-1 Acc. (\%)
\\ \midrule ViT-16B \cite{dosovitskiy2020image} & 5 & 24.5 & 78.9 \\
 DeepViT-16B & 0  & 24.5 & 79.1 (+0.2) \\ 
\midrule
 ViT-24B \cite{dosovitskiy2020image} & 11 & 36.3 & 79.4 \\
 DeepViT-24B & 0  & 36.3 & 80.1 (+0.7) \\ 
 \midrule
 ViT-32B \cite{dosovitskiy2020image} & 16 & 48.1 & 79.3 \\
 DeepViT-32B & 0  & 48.1 & 80.9 (+1.6) \\ 
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}








\myPara{Comparison to adding temperature in self-attention:}
The most intuitive way to mitigate the over-smoothing phenomenon is to sharpen  the distribution of the elements in the attention map of MHSA. We could achieve this by assigning a   temperature   to the Softmax layer of MHSA:

As the attention collapse is observed to be severe on deep layers (as shown in Fig.~\ref{fig:atten_similarity}),
we design two sets of experiments on a ViT model with 32 transformer blocks: (a)   linearly decaying the temperature    in each block such that the attention map distribution is sharpened and (b) making the  temperature learnable and optimized together with the model training. We first check the impact of the SoftMax temperature on reducing the attention map similarity. As shown in Fig. \ref{fig:fc_weights_heatmap}(a), the number of similar blocks are still large. Correspondingly, the feature similarity among blocks are also large as shown in Fig. \ref{fig:fc_weights_heatmap}(b). Thus, adding a temperature to the SoftMax only reduces the attention map similarity by a small margin.   The classification results on ImageNet are shown in Tab.~\ref{tab:ablation_temperature}. As shown, using a learnable temperature could improve the performance but the improvement is marginal. 




\begin{figure*}[h]
\begin{center}
\includegraphics[width=1.\linewidth]{figures/combo_similarity_split_transform_heatmap_v4.png}
\end{center}
\vspace{-20pt}
  \caption{(a) Adjacent block attention map similarity with different methods. As can be seen, our proposed \nameofatten{} achieves low cross layer attention map similarity. 
  (b) Cosine similarity between the feature map of the last block and each of the previous block. 
  (c) Visualization of transformation matrix of the last block.}
\label{fig:fc_weights_heatmap}
\end{figure*}

\begin{table}[h]
\footnotesize
\caption{ImageNet Top-1 accuracy of the ViT models with  SoftMax temperature  and the drop attention. The embedding dimension of all the models is set as 384. }
\label{tab:ablation_temperature}
\centering
\begin{tabular}{lcccc}
\toprule
\bf \# Blocks
&\bf \# Similar Blocks
&\bf Model
&\bf Acc. (\%)
\\ \midrule 32 &  16  & Vanilla & 79.3 \\ 
32 &  13  & Linearly decayed  & 79.0  \\ 
32 &  10  & Learnable   & 79.5  \\ 
32 &  8  & drop attention & 79.5  \\ 
32 & 0   & \nameofatten{} & 80.9 \\
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}

\paragraph{Comparison to dropping attentions:}
Another baseline we have attempted to differentiate the self-attention maps across layers is to use random dropout on the attention maps . 
As the dropout will mask out    different   positions on the attention maps for different blocks, the similarity between attention maps could be reduced.
The impacts on the attention maps and the output features of each block are shown in Fig. \ref{fig:fc_weights_heatmap}(a-b). It is observed that dropping attention does reduce the cross layer similarity of the attention maps. However, the similarity among features are not reduced by much. This is because the difference between attention maps comes from the zero positions in the generated mask. Those zero values do reduce the similarity between attention maps but not contribute to the features. Thus, the improvement is still not significant as shown in Tab.~\ref{tab:ablation_temperature}.











\myPara{Advantages of \nameofatten{}:}
Our proposed \nameofatten{} brings  more significant improvements over the temperature-tuning and the attention-dropping  methods. This is because both adding temperature and dropping  attention are regularizing the distribution of the originally over-smoothed self-attention maps, without explicitly encouraging them to be diverse. However, our proposed \nameofatten{} mechanism
 uses different heads (whose attention maps are dissimilar) as basis and  re-generate the attention maps via the transformation matrix . This process incorporates the inter-head information communication and the generated attention maps can encode richer information. It is worth noting that the original MHSA design can be thought as a special case of \nameofatten{} with an identity  transformation matrix. By making  learnable for each block, an optimized pattern could be learned end to end.  As shown in Fig.~\ref{fig:fc_weights_heatmap}(c), the learned transformation matrix assigns a diverse set of weights for each newly generated head. It clearly shows that the combination for each new heads takes different weights from the original heads in the re-attention process and thus reduces the   similarity between their attention maps. As shown in Fig.~\ref{fig:fc_weights_heatmap}(a), our proposed \nameofatten{} achieves the lowest cross layer attention map similarity. Consequently, it also reduces the feature map similarity across layers as shown in Fig.~\ref{fig:fc_weights_heatmap}(b).  
 









\subsection{Comparison with other SOTA models}






With \nameofatten{}, we design two ViT variants, \ie,  DeepViT-S and DeepViT-L,  based on the ViT with 16 and 32 transformer blocks respectively. For both   models, we use \nameofatten{} to replace the   self-attention. To have a similar number of parameters with other ViT models, we adjust the embedding dimension accordingly. The hidden dimensions of  DeepViT-S and DeepViT-L models are set as 396 and 408 respectively. More details on the model configuration are given in the supplementary material.
 Besides, motivated by \cite{yuan2021tokens}, we add   three CNN layers for extracting  the token embeddings, using the same configurations as     \cite{yuan2021tokens}.  It is worth noting that we do not use the optimized training recipes and the repeated augmentation as  \cite{touvron2020training} for training our models.  The results are shown in Tab.~\ref{tab:sota_comparison}.  Clearly, our DeepViT model achieves higher accuracy with less  parameters than the recent CNN  and ViT based models. Notably, without any complicated architecture change as made by T2T-ViT \cite{yuan2021tokens} (adopting a deep-narrow architecture)  or  DeiT \cite{touvron2020training} (introducing token distillation), simply using the \nameofatten{} makes our DeepViT-L outperforms them  by 0.4 points  with even smaller model size (55M vs.\ 64M \& 86 M). 





\begin{table}[t]
\footnotesize
\caption{Top-1 accuracy comparison with other SOTA models on ImageNet. * denotes our reproduced results.  denotes our model trained with training recipes used in DeiT \cite{touvron2020training}. }
\label{tab:sota_comparison}
\centering
\begin{tabular}{lcccc}
\toprule
\bf Model
&\bf Params. (M)
&\bf MAdds (G)
&\bf Acc. (\%)
\\ \midrule ResNet50 \cite{he2016deep} & 25 & 4.0 & 76.2 \\
 ResNet50* & 25 & 4.0 & 79.0 \\
 RegNetY-8GF \cite{radosavovic2020designing} & 40 & 8.0 & 79.3 \\
 Vit-B/16 \cite{dosovitskiy2020image} & 86 & 17.7 & 77.9 \\
 Vit-B/16* & 86 & 17.7 & 79.3 \\
 T2T-ViT-16 \cite{yuan2021tokens} & 21 & 4.8 & 80.6 \\
 DeiT-S \cite{touvron2020training} & 22 & -- & 79.8 \\
\midrule
DeepVit-S (Ours) & 27  & 6.2 & 81.4 \\ 
  DeepVit-S (Ours) & 27  & 6.2 & 82.3 \\ 
\midrule
 ResNet152 \cite{he2016deep} & 60 & 11.6 & 78.3 \\
 ResNet152* & 60 & 11.6 & 80.6 \\
 RegNetY-16GF \cite{radosavovic2020designing} & 54 & 15.9 & 80.0 \\
 Vit-L/16 \cite{dosovitskiy2020image} & 307 & -- & 76.5 \\
 T2T-ViT-24 \cite{yuan2021tokens} & 64 & 12.6 & 81.8 \\
 DeiT-B \cite{touvron2020training} & 86 & -- & 81.8 \\
 DeiT-B* & 86 & 17.7 & 81.5 \\
\midrule
DeepVit-L (Ours) & 55  & 12.5 & 82.2 \\ 
 DeepVit-L (Ours) & 58  & 12.8 & 83.1 \\ 
 DeepVit-L 384 (Ours) & 58  & 12.8 & 84.3 \\ 
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}




%
 




\section{Conclusion}

In this work, we found the attention collapse problem of vision transformers as they go deeper and
propose a novel \nameofatten{} mechanism to solve it with minimum amount of computation and memory overhead. 
With our proposed \nameofatten, we are able to maintain an increasing performance when increasing the depth of ViTs. 
We hope our observations and methods could facilitate the development of vision transformers
in future.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage\null\thispagestyle{empty}

\appendix

\section{Experiment Implementation Details}
\label{supp:impl_details}

\paragraph{Attention reuse:} As shown in Fig. 3(b) and Tab. 3 in the main paper, the vision transformers with 24 blocks and 32 blocks have 11 and 15 blocks with similar attention maps, respectively. 
To verify the effectiveness of the attention maps from those blocks, 
we directly force those blocks to share the same attention map 
as the last `unique' block as defined in Sec. 5.2. 
Specifically, we take the attention map of the last `unique' block and 
denote it as .
For all the following blocks, the attention output is calculated by:

where  is used to simulate the small variance between attention maps across layers since they are not identical. Norm is batch normalization used to adjust the variance across layers. As shown in Tab. 3, for a ViT with 32 blocks, forcing the top 15 blocks to share the same attention map causes negligible degradation on the classification accuracy on ImageNet. This proves that adding those blocks do not contribute to the accuracy improvement.


\paragraph{Training loss:} We use the cross-entropy (CE) loss as the training loss for all experiments. To minimize the similarity of the attention maps across layers, we add the cosine similarity between layers into the loss function when training the model. 
 
 
where  denotes the cosine similarity between layer  and  and  denotes the attention map of layer . B denotes the number of bottom blocks used for regularization and is a hyper-parameter. We set B to 4, 8 and 12 for training ViT models with 16, 24 and 32 blocks respectively.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/acc_vs_split_ratio_v3.png}
\end{center}
  \caption{ViT classification accuracy with \nameofatten{} applied on different number of blocks. The black dotted line denotes the cosine similarity ratio between adjacent blocks of the original ViT model with 16 blocks. The red dotted line denotes the top-1 classification accuracy on ImageNet. The accuracy of the model with blocks index k denotes that the \nameofatten{} is applied on top  blocks.}
\label{fig:supp_split_ratio}
\end{figure}

\begin{table}[h]
\footnotesize
\caption{Structural hyper-parameter of DeepViT-S and DeepViT-L. Note that the embedding dimension is slightly larger than the baseline models. This is to adjust the size of the model to have a comparable size with other methods for a fair comparison.}
\label{tab:supp_sota_comparison_arch}
\centering
\begin{tabular}{lccccc}
\toprule
\bf Model
&\bf \#Blocks
&\bf \#Embedding
&\bf MLP size
&\bf Split ratio
\\ \midrule DeepViT-S & 16 & 396 & 1188  & 11-5 \\
 DeepViT-L &  32 & 420 & 1260  & 20-12 \\ 
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}

\section{DeepViT architecture design}
As observed in Fig. 3(a), the attention maps of the transformer blocks become similar only at the top blocks. Thus, it is not necessary to apply re-attention to all blocks. To study the optimal number of blocks with re-attention, we conduct a set of experiments on a ViT model with 16 transformer blocks. For each experiment, we only apply re-attention on the top  blocks where  ranges from 5 to 15. The rest of the blocks are using the original transformer block structure. We train each model on ImageNet with the same set of training hyper-parameters as those for baseline models as detailed in Sec. 5 in the main paper. The results are shown in Fig. \ref{fig:supp_split_ratio}. 

It is observed that, as the number of re-attention blocks varies, the top-1 classification accuracy changes correspondingly. The highest accuracy appears at the position where the number of re-attention blocks is the same as the number of similar attention map blocks. Based on this observation, we define the architecture of DeepViT-S and DeepViT-L with 5 and 12 re-attention blocks respectively. Detailed configurations are shown in Tab. \ref{tab:supp_sota_comparison_arch}. Note that we adjust the embedding dimension to have a comparable size with other methods.



\section{Impacts of hyper-parameters}
In the main paper, all experiments are run with the same set of training hyper-parameters as the one used for reproducing ViT models.
However, as shown in \cite{touvron2020training}, an improved training recipe could improve the performance of ViT models significantly. In Tab. \ref{tab:supp_comaprison_deit}, we present the performance of DeepViT-S and DeepViT-L with the same set of training recipes as DeiT except that we do not use repeated augmentation. 
In Tab. \ref{tab:supp_comaprison_deit}, it is clearly shown that the performance of DeepViT could be further improved with optimized training hyper-parameters. 

\begin{table}[h]
\footnotesize
\caption{DeepViT model with different training recipes.  denotes the model trained with DeiT \cite{touvron2020training} training recipes. }
\label{tab:supp_comaprison_deit}
\centering
\begin{tabular}{lcccc}
\toprule
\bf Model
&\bf Params. (M)
&\bf MAdds (G)
&\bf Acc. (\%)
\\ \midrule DeiT-S \cite{touvron2020training} & 22 & -- & 79.8 \\
 DeiT-S (KD) \cite{touvron2020training} & 22 & -- & 81.2 \\
DeepVit-S (Ours) & 27  & 6.2 & 81.4 \\ 
  DeepVit-S (Ours) & 27  & 6.2 & 82.3 \\ 
 \midrule
 DeiT-B \cite{touvron2020training} & 86 & 17.7 & 81.8 \\
 DeiT-B (KD) \cite{touvron2020training} & 86 & 17.7 & 83.4 \\
DeepViT-L (Ours) & 55  & 12.5 & 82.2 \\ 
 DeepViT-L (Ours) & 58  & 12.8 & 83.1 \\ 
\bottomrule
\vspace{-5mm}
\end{tabular}
\end{table}

\section{Similarity calculation}

\myPara{Cosine similarity between layers} To measure the similarity between the attention maps, we define the similarity  between the attention maps of two layers,  and , as the ratio of the number of similar vector pairs to the total number of pairs between two attention maps:

where  is a hyper-parameter and used as a threshold for deciding similar vectors\footnote{0.5 is selected as a threshold for visualization purpose in this paper}. 

\myPara{Definition of similar blocks} A block is counted as a similar block if the similarity between it's attention map and the adjacent block's attention map is larger than 80\%. To measure the block similarity for a ViT model with  blocks, we take the ratio of the number of similar blocks to the total number of blocks as a measurement. 

 
\end{document}
