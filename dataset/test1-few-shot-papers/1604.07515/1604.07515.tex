\subsection{PR-Nibble}\label{sec:pr-nibble}
Andersen, Chung, and Lang~\cite{Andersen2006} give an improved local
clustering algorithm, \defn{PageRank-Nibble (PR-Nibble)}, with  and work .
Their algorithm generates an approximate PageRank vector based on
repeatedly pushing mass from vertices that have enough
residual. Again a sweep cut is applied to the resulting vector
to give a partition. 


The PR-Nibble algorithm~\cite{Andersen2006} takes as input
an error parameter , a teleportation parameter , and a
seed vertex . The algorithm maintains two vectors,  (the PageRank
vector) and  (the residual vector).  At the end of the algorithm,
 is returned to be used in the sweep cut procedure. Initially,
 is set to  everywhere (i.e., the sparse set representing it is
empty), and  is set to store a mass of  on  and  everywhere
else (i.e., the sparse set contains just the entry ).

On each iteration, a vertex  with  is chosen
to perform a \defn{push} operation.  Following the description
in~\cite{Andersen2006}, a push operation on vertex  will perform
the following three steps:
\begin{enumerate}[itemsep=0pt,topsep=3pt]
\item 
\item For each  such that : \par
 \hspace{4ex} 
\item 
\end{enumerate}
The PR-Nibble simply repeatedly applies the push operation on a vertex
until no vertices satisfy the criterion , at
which point it returns  and terminates. The work of the algorithm
has been shown to be  in~\cite{Andersen2006}.

Again, our sequential implementation simply follows the above
procedure, and uses sparse sets to represent  and  to obtain
the local work bound. As described in~\cite{Andersen2006}, we use a
queue to store the vertices with , and
whenever we apply a push on , we check if any of its neighbors
satisfy the criterion, and if so we add it to the back of the
queue. We repeatedly push from  until it is below the threshold.

\myparagraph{An Optimization} We implement an optimization to speed
up the code in practice.  
In particular, we use a more
aggressive implementation of the push procedure as follows:
\begin{enumerate}[itemsep=0pt,topsep=3pt]
\item 
\item For each  such that : \par
 \hspace{4ex} 
\item 
\end{enumerate}
This rule can be shown to approximate the same linear system as the
original rule~\cite{Andersen2006}, and the solution generated can be
shown to have the same asymptotic conductance guarantees as the
original PR-Nibble algorithm.
The work of this
modified algorithm can be shown to also be  by
using the same proof as in Lemma 2 of~\cite{Andersen2006} and
observing that at least  mass is pushed from  per iteration.



Figure~\ref{fig:acl-opt} shows the normalized running times of the
original PR-Nibble algorithm versus our modified version with the optimized update rule for  and  on various input graphs.  In
the experiment, both versions return clusters with the same
conductance for the same input graph.  We see that the optimized
version always improves the running time, and by a factor of
--x for the graphs that we experimented with.



\begin{figure}[!t]
\centering
\vspace{-10pt}
\includegraphics[width=0.8\columnwidth]{figs/acl-opt_data.pdf}
\vspace{-4pt}
\caption{Running times of the original version of sequential PR-Nibble
  versus the optimized version with  and
  . The running times are normalized to that of
  original PR-Nibble. See Section~\ref{sec:exp} for more information about the graphs and machine specifications.}
\label{fig:acl-opt}
\vspace{-3pt}
\end{figure}




We also tried using a priority queue instead of a
regular queue to store the vertices, where the priority of a vertex
 is the value of  when it is first inserted into the
queue (with a higher value corresponding to a higher priority). We did
not find this to help much in practice, and sometimes performance was
worse due to the overheads of priority queue operations.





\myparagraph{Parallel Implementation} 
The PR-Nibble algorithm as described above is sequential because each
iteration performs a push on only one vertex. To add parallelism,
instead of selecting a single vertex in an iteration, we select
\emph{all} vertices  where , and perform
pushes on them in parallel. This idea was described by Perozzi et
al.~\cite{Perozzi2014}, who implemented it for the distributed setting. Unfortunately their algorithm does not
have a local running time since it does work proportional to at least the
number of vertices in the graph. Here we develop a work-efficient parallel algorithm (and thus with local running time) in the
shared-memory setting based on this idea.

In our parallel algorithm, we
maintain an additional vector , which is set to  at
the beginning of an iteration, and during the iteration vertices read
values from  and write values to . At the end of the iteration,
 is set to . Thus, the pushes use information in the
 vector computed from previous iterations, and do not take into
account updates that occur within the current iteration. We also tried
an asynchronous version which only maintains a single  vector,
with updates always going to that vector, but found that 
mass would leak when running in parallel due to race conditions, and
it was unclear what the meaning of the solution at the end
was. Developing an asynchronous version that preserves mass and works
well in practice is a direction for future work.


We implement a parallel version of PR-Nibble with the original update
rule (Figure~\ref{alg:pr-nibble}) as well as a version with
the optimized update rule, which requires changing only the update
functions passed to \emap{} (Figure~\ref{alg:pr-nibble2}).  The
implementations are very similar to that of Nibble---the main differences
are in the update rules 
and the fact that PR-Nibble runs until the size of the frontier
becomes empty whereas Nibble will stop after at most  iterations.


\begin{figure}[!t]
\vspace{-6pt}
\scriptsize
\begin{algorithmic}[1] 
\State 
\State 
\State 
\Procedure{UpdateNgh}{, } \Comment{passed to \emap{}}
\State 
\EndProcedure
\smallskip
\Procedure{UpdateSelf}{} \Comment{passed to \vmap{}}
\State 
\State 
\EndProcedure

\smallskip
\Procedure{PR-Nibble}{, , , }
\State 
\State  \Comment{seed vertex}
\While {}
\State 
\State 
\State 
\State  \Comment{using filter}
\EndWhile
\State \algorithmicreturn{} 
\EndProcedure
\end{algorithmic}
\caption{Pseudocode for parallel PR-Nibble with the original update rule.} \label{alg:pr-nibble}
\end{figure}

\begin{figure}[!t]
\scriptsize
\begin{algorithmic}[1] 
\Procedure{UpdateNgh}{, } \Comment{passed to \emap{}}
\State 
\EndProcedure
\smallskip
\Procedure{UpdateSelf}{} \Comment{passed to \vmap{}}
\State 
\State 
\EndProcedure
\end{algorithmic}
\caption{Update functions for the optimized version of parallel PR-Nibble. The rest of the code is the same as in Figure~\ref{alg:pr-nibble}.} \label{alg:pr-nibble2}
\vspace{-4pt}
\end{figure}

Unlike Nibble, the amount of work performed in the parallel versions
of PR-Nibble can differ from the sequential version as
the parallel version pushes from all vertices above the threshold with
their residual at the \emph{start of an iteration}.  In particular,
the sequential version selects and pushes a single vertex based
on the most recent value of  whereas the parallel version selects
and pushes vertices based on the value of  before any of the
vertices in the same iteration have been processed.  The residual of
the vertex when it is pushed in the parallel version can be lower than
when it is pushed in the sequential version, causing less
progress to be made towards termination, and leading to more pushes
overall. However, the following theorem shows that the asymptotic work
complexity of the parallel versions match that of the sequential
versions.


\begin{theorem}
The work of the parallel implementations of PR-Nibble using either
update rule is  with high probability.
\end{theorem}
\vspace{-7pt}
\begin{proof}
The proof is very similar to how the work is bounded in the sequential
algorithm~\cite{Andersen2006} and involves showing that the  norm
(sum of entries) of the residual vector  sufficiently decreases
in each iteration. This idea is also used in~\cite{Perozzi2014}. 

Our proof will be for PR-Nibble using the optimized update rule, but
the proof is similar when using the original rule.  Denote the
residual vector  at the beginning of iteration  by . In
the pseudocode, each iteration will generate  by moving
mass from  into . Denote the set of active
vertices in iteration  by .  Any vertex  processed in the
iteration will have  (as the frontier is
defined by vertices that satisfy this property) and do  work
to perform the push. It will also contribute  work towards
the filter on Line 17.  The work of an iteration is thus , and the total work of the algorithm is
 if it runs for  iterations.


A push from vertex  will first set  to 0, and then
add a total of  mass to its neighbors' entries
in the  vector.  Thus the total contribution to
 of vertex  is
.  Parallel
updates to  will be correctly reflected due to the use of
the atomic fetch-and-add function, and so all vertices  that are on the
frontier in iteration  will contribute at least  to the difference .

We know that  and ,
and so
. Rearranging, we have
.  Thus, the total
work of the algorithm is . Note that this bound is independent
of .
\end{proof}
\vspace{-7pt}

We also note that both versions of parallel PR-Nibble can be shown to satisfy the same asymptotic
conductance guarantees as the sequential algorithm
of~\cite{Andersen2006}. 


In practice we found that the parallel versions do more work than the
corresponding sequential versions, but benefit from a fewer number of
iterations, each of which can be
parallelized. Table~\ref{table:pr-nibble} shows the number of pushes
for both the sequential and parallel versions of PR-Nibble with the
optimized update rule on several real-world graphs. The table also
shows the number of iterations required for parallel PR-Nibble (the
number of iterations for sequential PR-Nibble is equal to the number
of pushes). We see that the number of pushes of the parallel version
is higher by at most a factor of x and usually much less. The
number of iterations is significantly lower than the number of pushes,
indicating that on average there are many pushes to do in an
iteration, so parallelism is abundant.


{\begin{table}
\small
\centering
\begin{tabular}[!t]{c|r|r|r}
{\bf Input Graph} & Num. Pushes & Num. Pushes & Num. Iterations\\
& (sequential) & (parallel) & (parallel) \\
\hline
soc-LJ & 475,815  & 535,418 & 66 \\
cit-Patents & 4,154,752 & 5,323,710 & 139\\
com-LJ & 763,213 & 917,798 & 77\\
com-Orkut & 516,787  & 803,038 & 166 \\
Twitter & 125,478 & 127,625 & 44\\
com-friendster & 1,731,670 & 2,086,491 & 65\\
Yahoo & 740,410 & 830,457& 96 \\
\end{tabular}
\caption{Number of pushes and number of iterations for PR-Nibble on
  several graphs using  and . More
  information about the graphs can be found in
  Table~\ref{table:inputs} of
  Section~\ref{sec:exp}.}\label{table:pr-nibble}
\end{table}
}

We also implemented a parallel version which in each iteration
processes the top -fraction () of the vertices
in the set  with the highest
 values. The  parameter trades off between additional work
and parallelism.  We found that this optimization helped in practice
for certain graphs, but not by much. Furthermore the best value of
 varies among graphs. We do not report the details of the
performance of this variant in this paper due to space constraints.



