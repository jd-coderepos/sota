\documentclass[12pt, a4paper, abstracton]{scrreprt}
\usepackage[T1]{fontenc}
\pdfoutput=1
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{microtype}
\usepackage{lmodern}
\usepackage{units}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{subfigure}
\usepackage{paralist}
\usepackage{schuster}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[sort,comma,square,numbers]{natbib}
\usepackage{scrpage2}
\usepackage[compact]{titlesec}
\usepackage[figure]{hypcap}

\renewcommand{\cite}{~\citep}

\hypersetup{breaklinks,a4paper,linktocpage,bookmarksopen, bookmarksopenlevel=2, pdftitle={A Quadratic-Vertex Problem Kernel for s-Plex Cluster Vertex Deletion}, pdfauthor={Ren\'e van Bevern}, pdfsubject={Graph-Modeled Data Clustering}, pdfkeywords={data clustering, graph algorithms, problem kernel, fixed-parameter algorithmics},pdfdisplaydoctitle,colorlinks,linkcolor=blue,citecolor=blue}

\setkomafont{sectioning}{\normalfont\bfseries}
\setkomafont{descriptionlabel}{\normalfont\bfseries}
\setkomafont{title}{\normalfont}
\setcapindent{1em}
\newcommand{\ns}{\!\!\!\!\!\!\!\!}
\newcommand{\premv}[1]{Let~ be a solution. For each vertex~, let~ be the set constructed by \autoref{#1}.}
\newcommand{\prem}[1]{Let~ be a solution. Let~ be the set constructed by \autoref{#1}.}
\newcommand{\N}{\mathbb{N}}
\newcommand{\FPT}{\text{FPT}}
\newcommand{\DP}{\text{P}}
\newcommand{\md}[1]{\mbox{-module}}
\newcommand{\nx}[1]{N(#1)\cap X}
\newcommand{\ind}{}
\newcommand{\col}{\text{col}}
\newcommand{\NP}{\text{NP}}
\newcommand{\PSPACE}{\text{PSPACE}}
\newcommand{\EXPSPACE}{\text{EXPSPACE}}
\newcommand{\IS}{\text{IS}}
\newcommand{\VC}{\text{VC}}
\newcommand{\PH}{\text{PH}}
\newcommand{\W}[1]{\text{W}[#1]}
\newcommand{\WNS}[1]{\text{WNS}[#1]}
\newcommand{\name}{\textsc}
\newcommand{\hneg}{\mathcal H_0(X,M)}
\newcommand{\hpos}{\mathcal H_1(X,M)}
\newcommand{\hv}{\mathcal H(X)}
\newcommand{\hp}{H}
\newcommand{\pol}[1]{\text{pol}(#1)}
\newcommand{\fall}[1]{\textbf{Fall #1.}}
\newcommand{\pvd}[1]{\name{\mbox{-Plex} Cluster Vertex Deletion}}
\newcommand{\cvd}{\name{Cluster Vertex Deletion}}
\newcommand{\hs}[1]{\name{\mbox{-Hitting} Set}}
\newcommand{\pl}[1]{\mbox{-plex}}
\newcommand{\pcg}[1]{\pl #1 cluster graph}
\newcommand{\FISG}{\textsc{Fisg}}
\newcounter{theorem}
\newcommand{\myqed}{}
\DeclareMathOperator{\poly}{poly}

\newcommand{\decprob}[3]{\begin{flushright}
\addtolength{\linewidth}{-1em}
\begin{minipage}{\linewidth}
\textsc{#1}
\begin{compactdesc}
\item[\hspace{\parindent}Instance:] #2
\item[\hspace{\parindent}Question:] #3
\end{compactdesc}  
\end{minipage}
\addtolength{\linewidth}{1em}
\end{flushright}
}

\newtheorem{satz}{Theorem}[chapter]
\newtheorem{korollar}{Corollary}[chapter]
\newtheorem{lemma}{Lemma}[chapter]

\theoremstyle{definition}
\newtheorem{rul}{Reduction Rule}[chapter]
\newtheorem{asu}{Assumption}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{proc}{Algorithm}[chapter]
\newtheorem{bsp}{Example}[chapter]

\theoremstyle{remark}
\newtheorem{rema}{Remark}[chapter]

\def\remaautorefname{Re\-mark}
\def\claimautorefname{Claim}
\def\chapterautorefname{Chapter}
\def\procautorefname{Algo\-rithm}
\def\satzautorefname{Theorem}
\def\asuautorefname{Assumption}
\def\korollarautorefname{Corollary}
\def\subfigureautorefname{Figure}
\def\rulautorefname{Reduction Rule}
\def\definitionautorefname{Definition}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\lemmaautorefname{Lemma}

\begin{document}
\mathtoolsset{centercolon}
\begin{titlepage}
\thispagestyle{scrheadings}
\cfoot{}
\setfootsepline{0.5pt}
\ifoot{\vspace{1cm}\\
Cite this work as: René van Bevern. \emph{A quadratic-vertex problem kernel for -plex cluster vertex deletion}. Studienarbeit, Department of Mathematics and Computer Science, Friedrich Schiller University of Jena, Germany, 2009.}
\centering
\noindent \rule{\textwidth}{0.5pt}

\vspace{1em}
\Large Graph-Based Data Clustering:\\\Huge A Quadratic-\!Vertex Problem Kernel for\\-Plex Cluster Vertex Deletion

\normalsize\vspace{1.12\topsep}\large
by

\normalsize\vspace{\topsep}\Large
\textsc{Ren\'e van Bevern}\\
\rule{\textwidth}{0.5pt}

\vfill
\Large Studienarbeit\\
\normalsize 14. September 2009

\vspace{\topsep}
  Betreuung: Hannes Moser,\\
  Rolf Niedermeier      

\vfill
\normalfont
\includegraphics[width=5cm,trim=0cm 4cm 0cm 0cm,clip=true]{hanfried}

\vspace{\topsep}
Friedrich-Schiller-Universität Jena\\
  Institut für Informatik\\
  Theoretische Informatik I / Komplexitätstheorie
\end{titlepage}

\begin{abstract}
  We introduce the \pvd s problem. Like the \cvd{} problem, it is NP-hard and motivated by graph-based data clustering. While the task in \cvd{} is to delete vertices from a graph so that its connected components become \emph{cliques}, the task in \pvd s is to delete vertices from a graph so that its connected components become \emph{\pl ses}. An \pl s is a graph in which every vertex is nonadjacent to at most~ other vertices; a clique is an \pl1. In contrast to \cvd, \pvd s allows to balance the number of vertex deletions against the sizes and the density of the resulting clusters, which are \pl ses instead of cliques.

  The focus of this work is the development of provably efficient and effective data reduction rules for \pvd s. In terms of fixed-parameter algorithmics, these yield a so-called \emph{problem~kernel}. A similar problem, \textsc{-Plex Editing}, where the task is the insertion or the deletion of edges so that the connected components of a graph become \pl ses, has also been studied in terms of fixed-parameter algorithmics. Using the number of allowed graph modifications as parameter, we expect typical parameter values for \pvd s to be significantly lower than for \textsc{-Plex Editing}, because one vertex deletion can lead to a high number of edge deletions. This holds out the prospect for faster fixed-parameter algorithms for \pvd s.
\end{abstract}

\tableofcontents

\chapter{Introduction}\label{introsec}
Data clustering problems are of great importance in the disciplines of machine learning, pattern recognition, and data mining\cite{Berkhin2006}.  Given a data set, one can define a measure of similarity on data pairs. The goal in data clustering is to partition the data set into \emph{clusters} so that the elements within a cluster are similar, while there are less similarities between vertices in different clusters. Mapping clustering tasks into graph-theoretic models allows the usage of the broad variety of graph algorithms to process and cluster data\cite{DBLP:journals/csr/Schaeffer07}. Usually, the similarity between data records is mapped to a graph~ as follows: each vertex in~ corresponds to a data record, and an edge between two vertices in~ exists if and only if the similarity of the corresponding data records exceeds a certain threshold. This threshold is specific to the actual clustering problem. An obvious possible postulation on clusters is for each data pair in one cluster to be similar. A cluster can therefore be interpreted as a complete graph, also called \emph{clique}. Subject to our goal that there shall be only few similarities between vertices in different clusters, the graph~ constructed from our data would ideally consist of isolated cliques only. Such a graph is called a \emph{cluster graph}. For real-world data, it is unrealistic to expect~ to be a cluster graph. We could modify~ to become a cluster graph, but because we want to avoid excessive perturbation of the input data, the graph should be modified only modestly. One way to model this task is \cvd\cite{HKMN09TOCS}.  \decprob{\cvd}{An undirected graph~ and a natural number~.}{Is there a vertex set~ with~ such that deleting all vertices in~ from~ results in a graph where each connected component forms a clique?}

\noindent This problem corresponds to discarding at most ~data records in order to find a plausible data clustering. We can regard the discarded data records as outliers. Although \cvd{} is a very intuitive model of graph-based data clustering, it is very restrictive as it requires \emph{every} data pair in a cluster to be similar.  \cvd{} offers no option to relax this requirement, so that we could allow for a few dissimilarities within the resulting clusters. Obviously, it is desirable to balance the amount of discarded data against the number of dissimilarities within a cluster. Also, inaccuracies in the data could render finding satisfactory clustering results using \cvd{} impossible, yielding too many or too small clusters. Therefore, we weaken the requirement for every connected component to form a clique. Seidman and Foster\cite{SF78} have introduced one generalization of the clique concept in 1978:

\begin{definition}
  For~, an \emph{-plex} is a graph~ such that every
  vertex in~ is adjacent to at least ~other vertices in~.
\end{definition}

\noindent For example, a clique is an \pl1. By modeling clusters using \pl{s}es instead of cliques, we allow each data record to be dissimilar to  other data records within the same cluster.  Although the \pl s concept has already been introduced in 1978, it has only recently become subject to algorithmic research\cite{BBH09,DBLP:conf/aaim/GuoKNU09,MH09,MNS09,WP07}. In this work, we introduce the \pvd s~problem.

\decprob{\pvd s}{An undirected
  graph~ and a natural number~.}{Is there a vertex
  set~ with~ such that deleting all vertices
  in~ from~ results in a graph where each connected component
  forms an \pl s?}

\noindent In the following, we will call a graph that has only \pl ses
as connected components an \emph{\pcg s}.  For each~, the \pvd s
problem yields a different clustering model. In each model,
~determines the ``density'' of the resulting clusters and with that
the dissimilarities that are allowed within each cluster.

\paragraph{Fixed-Parameter Algorithmics.}
In this work, we study the \pvd s problem in terms of fixed-parameter algorithmics. Fixed-parameter algorithmics aims at a multivariate complexity analysis of problems without giving up the demand for finding optimal solutions\cite{DF99,Flu06,Nie06}. A \emph{parameterized} problem is a language~, where~~is a finite alphabet. The second component is called the \emph{parameter} of the problem. The \pvd s problem is a parameterized problem with the input~ and the parameter~. A parameterized problem~ is \emph{fixed-parameter tractable} if it can be determined in ~time whether~, where~ is a computable function only depending on~. The corresponding complexity class is called \emph{FPT}.

Given a parameterized problem instance~, \emph{reduction to a problem kernel} or \emph{kernelization} means to transform~ into an instance~ in polynomial time, such that the size of~ is bounded from above by some function only depending on~, ~, and~ is a yes-instance if and only if~ is a yes-instance. We refer to~ as \emph{problem kernel}. Kernelization enables us to develop provably efficient and effective \emph{data reduction rules}. Refer to Guo and Niedermeier\cite{GN07SIGACT} for a survey on problem kernelization. In this work, we present a kernelization for \pvd s.

\paragraph{Terminology.}
\label{sec:prelim}
We only consider \emph{undirected} graphs~, where~ is the set of vertices and~ is the set of edges. Throughout this work, we use  and~. We call two vertices~ \emph{adjacent} or \emph{neighbors} if~. The \emph{neighborhood}~ of a vertex~ is the set of vertices that are adjacent to~.  For a vertex set~, we set~. We call a vertex~ \emph{adjacent} to~ if ~has a neighbor in~. Analogously, we extend this definition and call a vertex set~ \emph{adjacent} to a vertex set~ with~ if~. A \emph{path} in~ from~ to~ is a sequence~ of vertices with~ for~. We call two vertices~~and~ \emph{connected} in~ if there exists a path from~ to~ in~.  For a set of vertices~, the \emph{induced subgraph}~ is the graph over the vertex set~ with the edge set~.  For~, we use  as an abbreviation for~.

\paragraph{Related Work.}
The two ``sister problems'' of \pvd s, namely \name{-Plex Editing} and \cvd, have been subject to recent research\cite{DBLP:conf/aaim/GuoKNU09,HKMN09TOCS}. The goal of the \name{-Plex Editing} problem is to transform a graph into an~-plex cluster graph by insertion or removal of at most ~\emph{edges}. For \cvd, Hüffner et al.\cite{ HKMN09TOCS} have developed fixed-parameter algorithms using the recent iterative compression\cite{GMN2009} technique introduced by Reed et al.\cite{RSV04}. Their algorithm solves \cvd{} in ~time, where~ is the number of allowed vertex deletions.  Guo~et~al.\cite{DBLP:conf/aaim/GuoKNU09} have shown a problem kernel with ~vertices for \name{-Plex Editing}, where~ is the number of allowed edge modifications. They also have developed the following forbidden induced subgraph characterization for \pcg ss.

\begin{satz}[Guo et al.\cite{DBLP:conf/aaim/GuoKNU09}]\label{fisg-char}
  Let~ be a graph. Let  be the set of all connected graphs with at most  vertices that contain a vertex that is nonadjacent to~ other vertices. The graph~ is an \pcg s if and only if it does not contain any graph from  as induced subgraph.
\end{satz}
\begin{figure}
  \centering
    \includegraphics{forbidden}
    \caption{Minimal forbidden induced subgraphs for .}
  \label{forbidden}
\end{figure}

\noindent
Guo et al.\cite{DBLP:conf/aaim/GuoKNU09} have also shown the stronger result that, for each natural number~, there exists a natural number  such that if a graph~ is not an \pcg s, then~ contains a forbidden induced subgraph (\FISG{}) with at most~ vertices. They present an algorithm that, if~ is not an \pcg s, finds such a \FISG{} in~ in~~time. If~ and if~ is not a \pcg 2, then their algorithm always finds one of the three \FISG{}s shown in \autoref{forbidden}.  We can solve \pvd s by repeatedly finding a \FISG{} with at most ~vertices in ~time and then branching into all possibilities of deleting one of its vertices. This yields a trivial search tree algorithm to solve \pvd s in ~time. Algorithms with a lower exponential time term can be obtained employing the \hs d problem:

\decprob{\hs d}{A set , a collection of subsets  and a natural number~.} {Is there a \emph{hitting set}~ with~ such that each set in  contains an element of~?}

\noindent We obtain a \hs d instance~ from an \pvd s instance~ as follows: we use the vertex set of~ as~; for each \FISG{}~ containing at most~ vertices from~, we add the vertex set of~ to~. Because each element in~ corresponds to a \FISG{} with at most~ vertices, we have~. Because this bound is exponential in~, it is practically infeasible to transform an \pvd s instance into a \hs d instance without prior data reduction. We can solve \hs d using a trivial -time search tree algorithm; we repeatedly choose a set from the collection  and branch into all possibilities of adding one of its vertices to a hitting set. Faster algorithms for \hs d are known\cite{Nie06}. For example, consider the special case~. The \FISG{}s for \pvd 2 are shown in \autoref{forbidden}. The trivial search tree algorithm for \pvd 2 (as discussed above) runs in  time. We can solve an equivalent \hs 4 instance in  time by combining Wahlström's -time algorithm for \hs 3\cite{Wah07} with iterative compression, as discussed by Dom et al.\cite{DGHNT09}.

The forbidden induced subgraph characterization by Guo~et~al.\cite{DBLP:conf/aaim/GuoKNU09} implies that every induced subgraph of an~-plex cluster graph is again an~-plex cluster graph. The property of being an~-plex cluster graph is thus \emph{hereditary}. Lewis and Yannakakis\cite{LY80} have shown that vertex deletion problems for hereditary graph properties are NP-hard. Because it can be verified in polynomial time whether a graph contains a \FISG{} for \pcg ss, \pvd s is in NP.  As a consequence, we can conclude that \pvd s is NP-complete. Further, Lund and Yannakakis\cite{CM93} have shown that vertex deletion problems for hereditary graph properties are constant-factor approximable and MAX SNP-hard, if the graph property admits a characterization by a finite number of \FISG{}s. Because \pcg ss are characterized by a finite number of \FISG{}s, finding a minimum solution for \pvd s is constant-factor approximable and MAX SNP-hard.

\paragraph{Our contributions.}
We show a problem kernel with ~vertices for \pvd2, which can be found in ~time. We then generalize this kernelization algorithm to show a problem kernel with ~vertices for \pvd s, which can be found~in~~time.

\chapter{Kernelization for 2-Plex Cluster Vertex Deletion}
\label{2pvd}
In this chapter, we transform a \pvd2 instance~ into a problem kernel~. To this end, we present a series of data reduction rules that remove vertices from~ so that the maximum number of vertices in the resulting graph~ depends only on the parameter~. These data reduction rules also compute the new parameter~. For each data reduction rule, we show that it can be carried out in polynomial time and that it is \emph{correct}, that is, we show that  is a yes-instance if and only if~ is a yes-instance.


Assume that we are given a \pvd2 instance~. We want to apply a series of data reduction rules to~ so that we can bound the size of~ by a function only depending on the parameter~. To structure the graph~, we first search for a constant-factor approximate solution~ so that each connected component in~ is a \pl2. This partitions the graph as shown in \autoref{packingfig}. To bound the overall size of~ by a function only depending on the parameter~, we independently bound the sizes of~ and~ by functions only depending on~.

\begin{figure}[h]
  \centering
  \includegraphics{packing}
  \caption{Constant-factor approximate solution  and the graph~.}
  \label{packingfig}
\end{figure}

\noindent To bound the size of~, we use that~ is a constant-factor approximate solution. If~ is a yes-instance, then~ can be transformed into a \pcg 2 by at most~ vertex deletions. This implies that the size of~ is at most~ for some constant factor~. In particular, the maximum size of~ only depends on~. If~ contains more than~ vertices, we stop our kernelization algorithm and output that~ is a no-instance.

It is left to bound the size of~ by a function only depending on the parameter~. To this end, we present data reduction rules to independently bound the \emph{number} and the \emph{sizes} of the connected components in~ by functions only depending on~. Bounding the sizes of the connected components is the most sophisticated part of our kernelization algorithm. To this end, we employ graph separators and introduce a generalization of the graph module concept\cite{Gal67,DBLP:journals/dm/McConnellS99} in \autoref{redundant}.

Summarizing, we obtain a problem kernel for a \pvd 2 instance~ by executing the following steps:
\begin{enumerate}
\item Find a constant-factor approximate solution~ such that~ is a \pcg 2. This is the subject of \autoref{greedy}. Because~ is a constant-factor approximate solution, the size of~ is bounded by a function only depending on the parameter~. 
\item Bound the \emph{number} of connected components in~ by a function only depending on the parameter~. To this end, we use data reduction rules presented in \autoref{mark}.
\item Bound the \emph{sizes} of the connected component in~ by a function only depending on the parameter~. To this end, we use data reduction rules presented in \autoref{redundant}.
\end{enumerate}
In \autoref{result}, we show that the remaining graph (consisting of the vertices in~ and the connected components in~ to which all data reduction rules have been applied) contains  vertices. Together with the new parameter computed by our data reduction rules, this graph constitutes our problem kernel.

In the following, we write \emph{solution} for a vertex set~ such that~ is a \pcg 2. If we intend to refer to a solution containing at most~ vertices, then we state it explicitly.

\section{An Approximate Solution}
\label{greedy}
In this section, we present an algorithm that greedily computes an approximate solution for \pvd 2. Given a graph~, Guo~et~al.\cite{DBLP:conf/aaim/GuoKNU09} have shown that if~ is not an \pcg s, an -vertex \FISG{} in~ can be found in  time. For the case~, this algorithm finds the \FISG{}s shown in \autoref{forbidden}. We apply their algorithm for~ to construct an initial solution:

\begin{proc}\label{find-X}
  Given a graph~, we start with  and . We repeatedly apply the algorithm by Guo et~al.\cite{DBLP:conf/aaim/GuoKNU09} to find a \FISG{} in~, we add its vertices to~, and remove them from~. If no \FISG{} can be found, then the algorithm stops and returns~.
\end{proc}

\noindent \autoref{packingfig} illustrates the separation of~
into~ and~.
\begin{lemma}\label{find-X-time}
  \autoref{find-X} computes a factor-4 approximate solution for \pvd 2. It can be carried out in ~time.
\end{lemma}
\begin{proof}
  First, we show the running time.
  In each step, a \FISG{} can be found in  time. Because in each step of \autoref{find-X} four vertices are removed from~, we apply it at most ~times. Therefore, \autoref{find-X} runs in ~time.

  It is left to show that the set~ computed by \autoref{find-X} is a factor-4 approximate solution.  \autoref{find-X} stops when no more \FISG{}s can be found in~. Thus,  must be a 2-plex cluster graph and  is a solution.

  Let  be the set of all \FISG{}s found by \autoref{find-X}. Because each \FISG{} is deleted from~ when it is discovered, the graphs in~ are pairwise vertex-disjoint. Any solution must contain at least one vertex of each \FISG{} in~. Therefore, the size of a solution is at least~. Each \FISG{} found by the algorithm of Guo et al.\cite{DBLP:conf/aaim/GuoKNU09} contains four vertices. It follows that the solution  computed by \autoref{find-X} contains~ vertices, which is at most four times the number of vertices in an optimal solution.
\end{proof}
\begin{korollar}\label{X-boundary}
  Let  be a yes-instance. Then, \autoref{find-X} computes a solution for~ that contains at most~ vertices.
\end{korollar}
\noindent Many of the following observations and data reduction rules require an initial solution~. In those observations, we make no assumptions about~ other than ~being a solution. For practical considerations, a heuristic search for an initial solution might be superior to employing \autoref{find-X}. Heuristic search might not only be faster, but might also find a smaller solution. This is desirable because the size of our problem kernel is proportional to the size of the initial solution. However, to conclude a problem kernel with~ vertices, we require an initial constant-factor approximate solution.

\section{Bounding the Number of Connected Components}
\label{mark}
Let~ be a solution for~. In this section we bound the number of connected components in~ by a function only depending on the parameter~. To this end, we employ a data reduction rule that resembles Buss and Goldsmith's\cite{BG93} kernelization of the \name{Vertex Cover} problem.

\begin{lemma}\label{high-occurence}
  Let~ be a \pvd 2 instance and let  be a set of \FISG{}s pairwisely intersecting only in the vertex  of~. If , then ~is a yes-instance if and only if ~is a yes-instance.
\end{lemma}

\begin{proof}
  If ~is a yes-instance, then there exists a solution~ with~ such that ~is a 2-plex cluster graph. The set ~is a solution for~.  If~ does not contain~, then it contains at least one vertex for every \FISG{} in~. Because there are more than~ \FISG{}s in~, this contradicts~.  Therefore,  and~ contains at most~ vertices. This shows that~ is a yes-instance.

  If ~is a yes-instance, then ~admits a solution~ of size~.  The set  is a solution for~ that contains at most ~vertices. Thus, ~is a yes-instance.
\end{proof}

\noindent In \autoref{mark-vertices}, we introduce the concept of \emph{peripheral} sets. Given a solution~, peripheral sets help us in \autoref{unmarked-bounds} to bound the number of connected components in~ and help us in \autoref{redundant} to bound their sizes. We present an algorithm that constructs a peripheral set efficiently and enables us to give a lower bound on the number of vertices that pairwisely intersect only in a single vertex~. If more than~ \FISG{}s intersect only in~, then we can remove~ from~ according to \autoref{high-occurence}.

\subsection{Peripheral Sets}\label{mark-vertices}
In this section, we present an algorithm that, for each vertex~ in a solution~, constructs a vertex set~ that allows us to give a lower bound on the number of \FISG{}s that pairwisely intersect only in the vertex~. If this lower bound shows that more than~ \FISG{}s pairwisely intersect only in~, then we can remove~ from~ according~to~\autoref{high-occurence}.

As a side effect, we construct the sets~ so that their union~ helps us to bound the number and the sizes of the connected components in~: informally speaking, if we remove~ from~, then we want each vertex~ to be adjacent to only one large connected component in~. As a result, there will be at most~ large connected components in~ adjacent to~. Further, if a vertex~ has a neighbor in a connected component in~, then we want the vertex  to be adjacent to almost all of that connected component's vertices. This will help us in \autoref{redundant} to bound the sizes of the connected components in~. We later formalize these properties and capture them under the concept~of~a~\emph{peripheral}~set.

We will see that we can easily bound the size of~ by a function only depending on the parameter~. Thus, the graph~ can be thought of as the ``core'' of our kernelization problem, for which we must provide further data reduction rules. In contrast, the vertices in~ are only of peripheral interest.

Given a solution~ for~, we now construct the set~ for each vertex~. We start with~. Then, we repeatedly search for a \FISG{}~ in~ that contains~ but no vertices from~ and add the vertices of~ to~. This ensures that we only find \FISG{}s that pairwisely intersect only in~. To find such \FISG{}s, we present three observations on the connected components in~. Each observation will lead to a phase of an algorithm that constructs the sets~.

\begin{definition}
  Let~ be the vertex set of~ and let~ be a solution. We define the collection  of the vertex sets of the connected components in~.
\end{definition}
\noindent Because each set in~ induces a connected component in~ and because~ is a solution, each set in  induces a 2-plex.

\begin{figure}[t]
  \centering
    \subfigure[\FISG{}s that will be found in Phase 1.]{
      \includegraphics{alg-m-pass1}
      \label{fig:pass1}}\hfill
    \subfigure[\FISG{}s that will be found in Phase 2.]{
      \includegraphics{alg-m-pass2}
      \label{fig:pass2}}
    \subfigure[A \FISG{} that will be found in Phase 3.]{
      \includegraphics{alg-m-pass3}
      \label{fig:pass3}}\hfill
    \subfigure[\FISG{}s that will not be found.]{
      \includegraphics{alg-m-no-hit}
      \label{fig:no-hit}}
    \caption{Each figure shows the graph~ with a solution~ and \FISG{}s that are found in the different phases of \autoref{mark-neighbors}. Also compare these \FISG{}s with the \FISG{}s shown in \autoref{forbidden}. The vertices~ and~ as used in the algorithm are shown. The big circles represent connected components in~, that is, they are \pl2es and their vertex sets are sets in~. Squares are vertices in the set~ for some vertex~, that is, they are vertices of \FISG{}s that have already been found.}
\end{figure}
We now turn to our first out of three observations. Let~ be a vertex with three neighbors~, , and~. Assume that~ is nonadjacent to~ and~, as shown in \autoref{fig:pass1}. Then, ~is a connected graph, but~ is nonadjacent to \emph{two} vertices~ and~. According to \autoref{fisg-char}, ~is a \FISG{}.

\begin{proc}[Phase 1]\label{mark-neighbors}
  Given a graph~ and a solution~, initialize  for each~. For each~, as long as there are vertices~ such that ~is neither adjacent to~ nor~, add the vertices~ and~ to~.
\end{proc}\setcounter{proc}{1}

\noindent Now, for each vertex~ in the solution~, let  be the set constructed by Phase~1 of \autoref{mark-neighbors}. For a vertex~, assume that there exists a set~ such that~ is adjacent to a vertex~ but nonadjacent to \emph{two} vertices~. This situation is shown in \autoref{fig:pass2}. The graph~ is an induced subgraph of~. Thus, it is a 2-plex with three vertices, implying that it is connected. Because~~is adjacent to~, the vertex~ is connected but nonadjacent to the \emph{two} vertices~ and~. By \autoref{fisg-char}, ~is a \FISG{}. We continue \autoref{mark-neighbors} as follows:

\begin{proc}[Phase 2]
  \begin{samepage}
    For each~, as long as there is a set~ such~that
    \begin{compactenum}
    \item the vertex  is adjacent to a vertex~ and
    \item the vertex  is nonadjacent to two vertices~,
    \end{compactenum}
    add the vertices~ and~ to~.
  \end{samepage}
\end{proc}\setcounter{proc}{1}  

\noindent Now, for each vertex~ in a solution~, let  be the set constructed by Phase~1 and Phase~2 of \autoref{mark-neighbors}. Assume that for a vertex~, there exist two sets~ such that there exist two neighbors~ and~ of~. This situation is shown in \autoref{fig:pass3}. Assume that~ or~ contains at least three vertices. Without loss of generality, assume that . Then, ~is a connected 2-plex. Therefore, there exists a neighbor~ of~. The vertex~ is nonadjacent to~ and , because~ is in another set in~. Because ~is connected, ~is a \FISG{} according to \autoref{fisg-char}.

\begin{proc}[Phase 3]
  \begin{samepage}
  For each vertex~, as long as there are two vertex
  sets~ such that
  \begin{compactenum}
  \item the vertex~ has neighbors~ and  and
  \item there is a neighbor~ of either~ or~,
  \end{compactenum}
  add the vertices~ and~ to~. Finally, return  for all vertices .
  \end{samepage}
\end{proc}

\noindent This concludes the description of \autoref{mark-neighbors}.  For a solution~, we now inspect the union~ of the sets~ constructed by \autoref{mark-neighbors}. Informally speaking, we show that if we remove~ from~, then each vertex~ is adjacent to the vertices of at most one large connected component in~. As a result, there are at most~ large connected components in~ containing neighbors of~.  Further, we show that if~ is adjacent to vertices of a connected component in~, then it is adjacent to almost all of its vertices. This helps us in \autoref{redundant} to bound the sizes of the connected components in~. To formalize these properties, we introduce the concept of a \emph{peripheral set}:

\begin{definition}\label{per} 
  Let~ be a solution. We call a vertex set~ with the following properties \emph{peripheral with respect to }:
  \begin{compactenum}
  \item\label{per1} For each vertex~, there are at most two sets~ such that~ is adjacent~to~.
  \item\label{per2} If there is a vertex~ and a set~ such that~ is adjacent to~, then~ is nonadjacent to at most one vertex in~.
  \item\label{per3} For each vertex~, if there is more than one set~ such that~ is adjacent to~, then each such set~ satisfies .
  \end{compactenum}
\end{definition}

\begin{figure}[t]
  \centering
  \includegraphics{peripheral}
  \caption{An example for a peripheral set~, which contains the vertices drawn as squares. Shown is the graph~ with a solution~. The circles represent sets in~, which induce connected components in~.
  \label{per-fig}}
\end{figure}

\noindent For an example, refer to \autoref{per-fig}.  In this figure, no vertex in~ is adjacent to the three sets~, , and~, as required by \autoref{per}(\ref{per1}). The vertex~ is adjacent to~ and~. There is only one vertex in~ that is nonadjacent to~, as required by \autoref{per}(\ref{per2}). As required by \autoref{per}(\ref{per3}), the sets~ and~ each contain at most two vertices. The vertex~ is only adjacent to~. Because~ contains more than two vertices,  is only adjacent to~, as required by \autoref{per}(\ref{per3}).

\begin{lemma}\label{two-sets}
  \prem{mark-neighbors} The set~ is peripheral with respect to~.
\end{lemma}

\begin{proof}
  We do not directly prove that for each vertex~, the set  satisfies the properties in \autoref{per}. Instead, we show for each vertex~ that the set~ satisfies them. Because~ for all~, this is sufficient. We show the properties separately.

  (\ref{per1}) Assume that there exists a vertex  and three sets~ such that~ has the neighbors~ and .  This case is illustrated for the vertex~ in \autoref{fig:pass1}. Because the vertices ~and~ come from different connected components in~, they are pairwise nonadjacent. Phase~1 of \autoref{mark-neighbors} would have added~~and~ to~.  This contradicts the assumption that~ and . This shows the first property.

  (\ref{per2}) Assume that there exists a set~ such that the vertex~ is adjacent to the vertex~ and~ is nonadjacent to the vertices~ and~. This is illustrated in \autoref{fig:pass2}. Phase 2 of \autoref{mark-neighbors} would have added the vertices~ and~ to~. This contradicts the assumption that~. This shows the second property.

  (\ref{per3}) Assume that there exist two sets~ such that a vertex~ has the neighbors~ and~. Without loss of generality, assume that~.  This situation is shown in \autoref{fig:pass3}.  Because , the 2-plex~ is connected. Therefore, the vertex ~has a neighbor~. Phase 3 of \autoref{mark-neighbors} would have added~~and~ to~. This contradicts the assumption that~ and~. To fully prove the third property, one can show~ analogously.
\end{proof}

\noindent In the following, we provide a more detailed view on the execution steps of \autoref{mark-neighbors} and also analyze its running time. The following lemma enables us to execute Phase~3 of \autoref{mark-neighbors} quickly.

\begin{lemma}\label{two-neighbors}
  Let~ be a solution. For each vertex~, let~ be the set constructed by Phase 1 of \autoref{mark-neighbors}. If there exists a vertex~ and two sets~ such that~ and~ are adjacent to~, then .
\end{lemma}

\begin{proof}
  Assume that the vertex~ has three neighbors~, as shown in \autoref{fig:pass1}. According to the proof of \autoref{two-sets}, there are at most two sets~ such that~ is adjacent to~ and~. Without loss of generality, assume that  and~. The vertices~ and  are neighbors of~ and ~is neither adjacent to~~nor~. Phase 1 of \autoref{mark-neighbors} would have added~~and~ to~. This contradicts the assumption that~.
\end{proof}

\begin{lemma}\label{mark-neighbors-time}
  Given a solution~, \autoref{mark-neighbors} can be carried out in ~time.
\end{lemma}

\begin{proof}
  Given a graph~ and a solution~, we first compute the graph~ in ~time. We can then compute the collection~ of vertex sets of the connected components in~. This can be done in ~time using breadth-first search. During the construction of~, we construct a table~ that stores, for each vertex~, the set~ with . We assume that set membership can be tested in constant time and that elements can be added to sets in constant time.  For each vertex~, we now execute the three phases:

  In Phase 1, we construct the set~ in ~time. For each vertex , we scan the set~ again to find two vertices nonadjacent to~. Therefore, Phase 1 runs in ~time for each vertex~.

  In Phase 2, for each~, we can (using the table~) find~ with~ in constant time. If~ or~, then we proceed with the next~. Otherwise, in ~time, we scan~ for two vertices that are nonadjacent to~. The running time for one vertex~ is thus , resulting in a running time of~ for each~.

  In Phase 3, we first construct the set  in ~time. According to \autoref{two-neighbors}, if we have , then there is at most one set~ such that~ is adjacent to~. Thus, we continue with the next~. Otherwise, let~. In constant time, we check (using the table~) if the vertices ~and~ are in different sets in~. If so, we scan the neighborhoods of~ and~ for a vertex~ in ~time. Thus, the total running time of Phase~3 is~ for each~. \autoref{mark-neighbors} has a worst-case running time of~.
\end{proof}

\noindent Note that, given a vertex~ of a solution~, \autoref{mark-neighbors} only finds a \FISG{}~ containing~ if the vertices in~ are neighbors of~ or if at least two vertices of~ are in distinct connected components in~. This is not the case for the \FISG{}s shown in \autoref{fig:no-hit}. Thus, \autoref{mark-neighbors} does not necessarily find them. We could search for these \FISG{}s, but this would presumably increase the asymptotic running time of \autoref{mark-neighbors}. It would not improve the worst-case size of our problem kernel.

\subsection{Reducing the Number of Connected
  Components}\label{unmarked-bounds}
In this section, given a solution~ for the graph~, we present data reduction rules to bound the number of connected components in~ by a function only depending on the parameter~. To this end, we bound the size of the peripheral set constructed by \autoref{mark-neighbors} using the following data reduction rule, which is based on \autoref{high-occurence}.

\begin{rul}\label{high-occurence-fast}
  \premv{mark-neighbors} If there exist a vertex~ such that~, then delete~ from~ and~ and decrement~ by one.
\end{rul}

\begin{lemma}\label{high-occurence-time}
  \autoref{high-occurence-fast} is correct. Given a solution~ and the set~ constructed by \autoref{mark-neighbors} for each vertex~, we can exhaustively apply \autoref{high-occurence-fast} in ~time.
\end{lemma}

\begin{proof}
  If \autoref{mark-neighbors} adds vertices to~ for a vertex~, then it has found a \FISG{} that contains no vertices from~. That is, apart from~, this \FISG{} does not contain vertices from previously found \FISG{}s. Thus, if , then ~contains vertices of more than ~\FISG{}s that pairwisely intersect only in the vertex~. According to \autoref{high-occurence}, we can delete~ from~ and decrement the parameter~~by one. For each vertex~, the elements in~ can be counted in ~time. The deletion of all vertices~ with~ is possible in ~time.
\end{proof}

\noindent Observe that for each vertex~ in a solution , \autoref{high-occurence-fast} does not change the set~ constructed by \autoref{mark-neighbors}. Also, the graph~ is invariant under \autoref{high-occurence-fast}; so is the set~. We can conclude that, after we have applied \autoref{high-occurence-fast} to~ and~, the proof of \autoref{two-sets} is still valid and shows that the set  is still peripheral by \autoref{per}. Therefore, \autoref{high-occurence-fast} does not only reduce the size of~ and~; we also obtain a smaller peripheral set. This is because after the exhaustive application of \autoref{high-occurence-fast}, for each vertex~, the set~ contains at most~ vertices.

\begin{korollar}\label{size-M}
  Let~ be a solution for~. For each vertex~, let~ be the set constructed by \autoref{mark-neighbors}. After exhaustively applying \autoref{high-occurence-fast} to~ and~, the peripheral set  contains at most  vertices.
\end{korollar}

\noindent 
Now that we have bounded the size of the peripheral set, we can, given a solution~, bound the number of connected components in~. First, we remove connected components from~, which are induced by the vertex sets in~, according to the following data reduction rule. Then, we use a peripheral set to show a bound on the number of the remaining connected components.

\begin{rul}\label{isolated}
  Let~ be a solution. If there exists a set~ that is nonadjacent to~, then remove the vertices in~ from~.
\end{rul}

\begin{lemma}\label{isolated-time}
  \autoref{isolated} is correct. Given a solution~, we can exhaustively apply \autoref{isolated} in ~time.
\end{lemma}

\begin{proof}
  Let~ be the set of vertices chosen for removal by \autoref{isolated} and let~. To prove the correctness of \autoref{isolated}, we have to show that ~is a yes-instance if and only if ~is a yes-instance. If ~is a yes-instance, then there exists a solution~ with~ for~. Since ~is a \pcg 2, ~is a \pcg 2 as well. Thus, ~is a yes-instance.

  If ~is a yes-instance, then there exists a solution~ with~ for~. Because \autoref{isolated} chooses to remove the vertices in~ from~, the set~ is nonadjacent to the solution . Therefore, ~induces an isolated 2-plex in~. It can therefore not contain vertices of a \FISG{}. Thus, also ~is a 2-plex cluster graph and ~is a yes-instance.

  Considering the running time, we can obtain the set~ in ~time. During the construction of~, we use a table~ to store for each vertex~ the set~ with~. We have already used this technique in the proof of \autoref{mark-neighbors-time}. We construct a further table~ as follows: for each vertex~ and for each vertex~, we set~. This can be done in~ time. Then, the sets~ with~ are known to have no neighbor in~. These can be removed from~ in~ time.
\end{proof}

\begin{figure}
  \centering

  \subfigure[A set~ with~ nonadjacent to~]{ \hspace{1cm}\includegraphics{h0}
    \label{fig:h0}\hspace{1cm}}\hspace{1cm}
  \subfigure[A set~ with~ adjacent to~]{ \hspace{1cm} \includegraphics{h1}\hspace{1cm}
    \label{fig:h1}}
  \caption{A solution~ and a vertex set~. The big circles represent sets in~, or connected components in~, likewise.}
  \label{teilung-h1h0}
\end{figure}
\noindent 
Given a solution~ and a vertex set~, there are two possible scenarios for a connected component in~. Consider the vertex set~ of such a connected component. As shown in \autoref{fig:h0}, it might be the case that the edges between the set~ and the solution~ separate the vertices in~ from the vertices in~. That is, the set~ might be nonadjacent to~. As shown in \autoref{fig:h1}, it might also be the case that for a set~, the set~ is adjacent to~. According to \autoref{per}(\ref{per1}), if~ is peripheral, then there are at most~ sets~ such that~ is adjacent to~. To bound the total number of connected components in~, it is left to bound the number of sets~ such that~ is nonadjacent to~.

\begin{lemma}\label{sets-in-hm}
  Let~ be a solution and let~ be a vertex set. After applying \autoref{isolated}, there are at most ~sets~ such that~ is nonadjacent to~.
\end{lemma}

\begin{proof}
  Let~ such that~ is nonadjacent to the solution~. Because \autoref{isolated} has been applied, the set  must be adjacent to~. Otherwise, \autoref{isolated} would have removed~. Because the set~ is nonadjacent to~, the set  must contain a vertex from~ that is adjacent to~. Because a vertex in~ can be contained in only one set in~, there can be at most~~sets  such that~ is nonadjacent to~.
\end{proof}

\noindent Given a solution~ and a peripheral set~, we conclude from \autoref{per}(\ref{per1}) and \autoref{sets-in-hm} that the number of the connected components in~ is at most~.

\section{Bounding the Sizes of Connected Components}
\label{redundant}
In this section, given a solution~ for~, we bound the \emph{sizes} of the connected components in~ by functions only depending on the parameter~. Because we have already bounded the size of~ and the \emph{number} of connected components in~, this will finally lead to a problem kernel, as we have discussed in the beginning of \autoref{2pvd}. In \autoref{strong}, we present a generalization of the \emph{module} concept\cite{Gal67,DBLP:journals/dm/McConnellS99}. Based on this, we develop a data reduction rule to reduce the sizes of the connected components in~. \autoref{ddot-mu} deals with the efficient execution of this data reduction rule and uses a peripheral set~ to bound the sizes of the connected components.  In \autoref{weak}, we present an additional data reduction rule that is only applicable to connected components induced by sets~ such that~ is nonadjacent to~. We have already specially handled this type of connected components in \autoref{unmarked-bounds}, where we bounded the \emph{number} of connected components in~. We use the fact that the edges between the set~ and the solution~ separate the vertices in~ from the vertices in~, as shown in \autoref{fig:h0}. We will see that the additional data reduction rule presented in \autoref{weak} is necessary to obtain an~-vertex problem kernel.

\subsection{Data Reduction Based on Modules}
\label{strong}
Given a solution~, we now develop a characterization of vertices that can be removed from the connected components in~. This characterization is based on so-called \emph{modules}\cite{Gal67,DBLP:journals/dm/McConnellS99}. For a graph with the vertex set~, a vertex subset~ is called a \emph{module}, if any two vertices~ satisfy~. That is, a vertex not in~ is adjacent to either to all or to no vertices in~. For example, the two vertices~ and~ in \autoref{fig:redundant1} form a module. Modules also serve as the base of the \emph{critical clique} concept introduced by Guo\cite{Guo09} to kernelize the \name{Cluster Editing} problem.

Given a vertex set~, we generalize the module concept and introduce the \emph{\md W}. We call a vertex set~ a \emph{\md{W}}, if any two vertices~ satisfy~. That is, a vertex in~ is either adjacent to all or to no vertices in~. \autoref{fig:redundant} shows examples for~-modules. Observe that if~ is a -module, then~ is a module. Every subset of a -module is again a -module.

For a graph~ and a solution~, we use the fact that the vertices in an \md X are equivalent with respect to their neighborhood in~. The idea is, informally, to represent a large \md X by one of its subsets and to replace the \md X by its representative.
\begin{figure}[t]
  \centering
    \subfigure[Graph prior to reduction.]{
     \hspace{0.5cm} \includegraphics{redundant31}\hspace{0.5cm}
      \label{fig:redundant1}}\hspace{1cm}
    \subfigure[Removed~ and~: valid data reduction.]{
      \hspace{0.5cm}\includegraphics{redundant2}\hspace{0.5cm}
      \label{fig:redundant2}}\hspace{1.0cm}
    \subfigure[Removed~: wrong data reduction.]{
      \includegraphics{redundant1}
      \label{fig:redundant3}}
    \caption{In each displayed graph, the vertices drawn as squares
      form an \md X.}
    \label{fig:redundant}
\end{figure}
Consider the following example, which also shows that we cannot choose an arbitrary subset of an \md X as representative: the graph shown in \autoref{fig:redundant1}, call it~, requires one vertex deletion to transform it into a \pcg 2. The vertices~ and~ are part of an~-module. Observe that also for~ shown in \autoref{fig:redundant2}, one vertex deletion is required to transform it into a \pcg2. It follows that  is a yes-instance if and only if~~is. Therefore, it is valid to remove~ and~ from~ to obtain the graph shown in \autoref{fig:redundant2}. In contrast, the graph~ shown in \autoref{fig:redundant3} is a \pcg2. Because  can be transformed into a \pcg 2 with less vertex deletions than~, we may not remove  from~. To circumvent this problem, we give a constraint on the vertices that may be removed from an \md X in~. Recall that the connected components in~ are induced by vertex sets in~.

\begin{definition}\label{redundant-def}
  Let~ be a solution. For , let~ be an \md X. We call~ \emph{redundant} if there exists an \md X  with  that contains all vertices from~ that are nonadjacent to a vertex in~.
\end{definition}

\begin{rul}\label{rul:goodconnected}
  Let~ be a solution, let  and let  be a redundant subset of~. If , then choose an arbitrary vertex from~ and remove it from~.
\end{rul}

\noindent In \autoref{ddot-mu} we construct a redundant set  for each vertex set~ so that we can give a bound on the size of~. Using \autoref{rul:goodconnected}, we can then bound the size of~.  To prove the correctness of the above data reduction rule, we assume that \autoref{rul:goodconnected} chooses to remove a vertex~ from~ and show that~ is a yes-instance if and only if~ is a yes-instance. To this end, we need three further observations, which we present in the following lemmas.

\begin{lemma}\label{FISG-has-v}
  Let~ be an arbitrary graph and let ~be a vertex of~. If  but not~~is a 2-plex cluster graph, then ~contains a \FISG{} including the vertex~.
\end{lemma}

\begin{proof}
  Because ~is not a 2-plex cluster graph, it contains a \FISG{}.  If all \FISG{}s in~ did not contain~, then no \FISG{} could be destroyed by removing ~from~. Thus, ~would not be a 2-plex cluster graph, contradicting our assumption.
\end{proof}

\noindent Additionally to the assumption that \autoref{rul:goodconnected} chooses to remove a vertex~ from~, we now assume that~ is a yes-instance and show two further lemmas. Finally, we prove the correctness of \autoref{rul:goodconnected}.

\begin{asu}\label{working-asu}
  Let~ be a solution and let ~be a redundant subset of . Assume that \autoref{rul:goodconnected} chooses to remove the vertex  from~. Further, assume that~ is a yes-instance, that is, that there exists a solution~ with~ for the graph~.
\end{asu}

\noindent In the following, we write ~for~. Because we assume that \autoref{rul:goodconnected} chooses to remove~ from~, the set  must contain more than~ vertices, which implies .
Because~ is a \pl 2,  is a \pl 2 containing at least three vertices. We can conclude that ~is connected. The graph  is connected for the same reason.

\begin{lemma}\label{connected}
  Under \autoref{working-asu}, let ~contain a \FISG{}~ including~. Then in~, the vertices of~ are connected to all vertices in~.
\end{lemma}
\begin{proof}
  Let ~be a vertex of~. Because ~is connected, there exists a path in~, connecting ~to~. This path has to use a neighbor~ of~ (possibly, ). We now distinguish between the two cases~ and~.
\begin{figure}[t]
  \centering\hspace{0.25cm}\subfigure[Case~. Because  contains three vertices, the vertices in  are connected.]{ \hspace{1.5cm} \includegraphics{proof-connected-indh}\hfill
    \label{fig:connected-indh}} \hspace{1cm} \subfigure[Case~. Because~ is an \md X and~ is adjacent to~, all vertices in~ are adjacent to~.]{ \hspace{1.25cm} \includegraphics{proof-connected-outdh}\hspace{1.25cm}
    \label{fig:connected-outdh}}
  \caption{The vertices~, and  are named as in the proof of \autoref{connected}. Note that in either case, ~is connected to all vertices in~ even if ~is removed. Also note that the vertex ~is not necessarily~in~.}
\end{figure}

According to \autoref{working-asu}, ~is connected.  So if , as shown in \autoref{fig:connected-indh}, then ~is connected to every other vertex in~. That is,  connects~ to the vertices in~ even when~ is removed.

Because ~is in~, we have . That is, if , then . Because~ is adjacent to~ and because there are no edges between distinct sets in~, we have~, as shown in \autoref{fig:connected-outdh}. Because  is the neighbor of~ and~ is in the \md X~, it follows that all vertices in~ are neighbors of~ in~. So  connects~ to the vertices in~ even when~ is removed.
\end{proof}

\begin{lemma}\label{in-hi}
  Under \autoref{working-asu}, let  be an \md X with  and let~ be a \FISG{} in~ including~. If a vertex~ of~ is nonadjacent to a vertex~, then~.
\end{lemma}

\begin{proof}
\begin{figure}
  \centering
  \includegraphics{proof-in-hi}
  \caption{Because ~is nonadjacent to the vertex~ of the \md X , the vertex ~can not be adjacent to any vertex in~. These are more than three vertices. But  is connected to all vertices in~, including~.}
  \label{fig:in-hi}
\end{figure}
Assume that a vertex~ of~ is nonadjacent to the vertex~. This situation is shown in \autoref{fig:in-hi}. Because~ is in~, we have  and therefore~. We first show that~ is nonadjacent to the \md X~.

Assume that~ is adjacent to the \md X~. This implies~, because there are no edges between distinct sets in  and~. Because~ is in the \md X~ and because~ is adjacent to , the vertex  must also be adjacent to~. This is by our assumption not the case, so  is nonadjacent to the \md X~. In particular,  is nonadjacent to its subset~.

According to \autoref{connected}, the vertex ~is connected to all vertices in~ in~.  By \autoref{working-asu}, there are at least three vertices in~. These are connected but nonadjacent to~ in~. By \autoref{fisg-char}, this implies that there exists a \FISG{} in~, contradicting \autoref{working-asu}.
\end{proof}

\begin{lemma}\label{goodconnected-correct}
  \autoref{rul:goodconnected} is correct.
\end{lemma}

\begin{proof}
  Assume that \autoref{rul:goodconnected} chooses to remove a vertex~ from~. Let  denote the graph . We have to show that~ is a yes-instance if and only if~ is a yes-instance. If  is a yes-instance, then there exists a solution~ with~ such that ~is a 2-plex cluster graph. Then, also ~is a 2-plex cluster graph and ~is a yes-instance.

  If ~is a yes-instance, then there exists a solution~ with~ such that ~is a 2-plex cluster graph, implying that \autoref{working-asu} is true. Assume that ~contains a \FISG{}. By \autoref{FISG-has-v}, there exists a \FISG{}~ in~ containing the vertex~.  Because~ is a \FISG{}, it contains a vertex~ that is connected but nonadjacent to two vertices  in~.

  If~, then \autoref{connected} shows that the vertices~ are connected to all vertices in~ in~. Thus, the vertices~ and~ would exist in~ and would be connected. That contradicts ~being a 2-plex cluster graph, because~ is nonadjacent but connected to the vertices~ and~. Thus, ~must be one of~ or~.

  First, assume that . That is, the vertex  is nonadjacent to the vertices~ and~. From \autoref{in-hi}, we can conclude that . Because also~, this contradicts the graph ~being a 2-plex. So~ must either be~ or~.

  Without loss of generality, assume that~. That is, the vertex  is nonadjacent to~. By \autoref{in-hi}, we have . By \autoref{redundant-def}, there exists an \md X  with~ and , because the vertex~ is nonadjacent to the vertex~. But then, because the vertex~ is nonadjacent to~, the vertex ~must also be in~ by \autoref{in-hi}. This again contradicts~ being a 2-plex. We conclude that ~must be a 2-plex cluster graph. Thus, ~is a yes-instance.
\end{proof}

\subsection{Constructing Redundant Sets}
\label{ddot-mu}
In this section, we show how to efficiently find redundant sets as defined in \autoref{redundant-def}. Our goal is, given a solution~ and the vertex set~ of a connected component in~, to construct a redundant subset~ so that the size of~ is bounded by a function only depending on the parameter~. Then, we can apply \autoref{rul:goodconnected} to~ to bound the overall size of~.

To this end, we employ a peripheral set~. Using \autoref{size-M}, we can bound the size of~ by . Thus, for each~, we only need to bound the size of the set~. \autoref{per}(\ref{per2}) for peripheral sets guarantees that if a vertex~ is adjacent to~, then there is at most one vertex in~ that is nonadjacent to the vertex~. Thus, the number of vertices in~ that are nonadjacent to a vertex in~ cannot exceed~. The size of~ is in turn bounded by~ in \autoref{X-boundary}. It follows that we only have to bound the number of vertices in~ that are adjacent to \emph{all} vertices in~. We show that we can obtain a redundant set from such vertices by employing the following~algorithm:

\begin{proc}\label{construct-D}
  Given a set~ that is peripheral with respect to a solution~, for each~, first find all vertices belonging to~ and . Then, construct the sets
  
  Return~, where .
\end{proc}

\begin{lemma}\label{construct-D-correct}
  Given a set~ that is peripheral with respect to a solution~, for~, let~ be the set constructed by \autoref{construct-D}. The set~ is redundant.
\end{lemma}
 
\begin{proof}
  According to \autoref{redundant-def}, we have to show that there exists an \md X  with  that contains all vertices in~ that are nonadjacent to a vertex in~. Because~ is a \pl2, we could choose~. But with \pl ses in mind, we present a proof that does not rely on the fact that~~is~a~\pl2.

Consider the set . For any two vertices , we have that . Thus, the set~ is an \md X.  To show that a vertex~ is in~, it is sufficient to show~ and . The opposite inclusion  follows directly from~.

  We first show that~. Because~, every vertex in~ is in~. Because~, for a vertex~, each vertex~ is adjacent to~. Otherwise, ~would be in~. From this, we can conclude that~. This implies~.
 
  Now assume that there exists a vertex ~and a vertex~ such that~ and~ are nonadjacent. From  follows that . Otherwise, ~would be in~. Because , for a vertex~, the vertex ~is adjacent to~. Otherwise, ~and therefore~. Thus, we have  and~.
\end{proof}

\begin{lemma}\label{construct-D-time}
  Given a set~ that is peripheral with respect to a solution~, \autoref{construct-D} can be carried out in ~time.
\end{lemma}

\begin{proof}
  Observe that we can construct the set~ in ~time. During the construction of~, we use a table~ to store for each vertex~ the set~ with~.  We now scan each~ in four passes, classifying each vertex~ as follows:

  The first pass constructs the sets~ and~. If , we memorize the vertex~ to belong to~. If , we memorize its neighbors in~ to belong to~. Finding~'s neighbors in~ can take~ time.

  The second pass constructs the sets~ and~ with the results from the first pass as follows: if the vertex~ is nonadjacent to a vertex in~, then add~ to~. This works in ~time.
If the vertex~ is nonadjacent to a vertex in~, which can be checked in ~time, then add  to~. 

  The third pass is similar to the second pass and constructs  from~ in ~time. In a final pass, we add all vertices~ that are not in~ or~ to~. This can be done in constant time for each vertex~.

  Finally, we encounter at most~ vertices scanning through each~, yielding a total running time of~.
\end{proof}

\begin{lemma}\label{goodconnected-time}
  Given a set~ that is peripheral with respect to a solution~, we can exhaustively apply \autoref{rul:goodconnected} in ~time.
\end{lemma}
\begin{proof}
  We first, for all~, use \autoref{construct-D} on the sets~ and~ to construct the sets~ in ~time (\autoref{construct-D-time}). According to \autoref{construct-D-correct}, these sets are redundant. Thus, \autoref{rul:goodconnected} can be applied.

  Observe that after \autoref{rul:goodconnected} removes a vertex~ from~, the set  is still redundant. Thus, we can remove a whole subset of~ from~ without constructing new redundant sets between vertex deletions.

  For each~, we can count the number of vertices in  in )~time. Removing a set of vertices works in ~time.
\end{proof}

\noindent Given an instance~ and a solution~ for~, we can now bound the sizes of the connected components in~ by a function that only depends on the parameter~.

\begin{lemma}\label{h1-vertices}
  Let the set~ be peripheral with respect to a solution~.  For a set , let~ be the redundant subset constructed by \autoref{construct-D}. After exhaustively applying \autoref{rul:goodconnected} using~, the number of vertices in~ is at most .
\end{lemma}

\begin{proof}
  To prove the above lemma, we study the sets constructed in \autoref{construct-D}. By construction of~, we have~. Observe that because~, we also have . Because ~is a 2-plex, there exists at most one vertex~ for every vertex~ such that ~and~ are nonadjacent. Thus, we have . Because~ is peripheral, we can conclude from \autoref{per}(\ref{per2}) that for each vertex~, there is at most one vertex~ such that~ and~ are nonadjacent. If~, then~.  Thus, we have . Now, again because~ is a \pl 2, there exists at most one vertex~ for every vertex~ such that~ and~ are nonadjacent.  Thus, we have .  This shows that the number of vertices in~ cannot exceed . To get the total number of vertices in~, we must add~. \autoref{rul:goodconnected} bounds~ to~.
\end{proof}

\subsection{Data Reduction Based on Separators}
\label{weak}
In the previous section, we have, given a solution~, bounded the sizes of the connected components in~. Given a peripheral set~, we now present an additional data reduction rule to further reduce the sizes of the connected components induced by vertex sets from the collection . The vertices in a set~ are \emph{separated} from the vertices in the solution~ by the edges between~ and~, as shown in \autoref{fig:h0}. \autoref{fig:h1} shows an example for a vertex set that is \emph{not} in .  The following observation makes clear why an additional data reduction rule for sets in ~is~necessary.

According to \autoref{size-M}, if~ is our parameter, we can employ \autoref{high-occurence-fast} to obtain a peripheral set~ containing at most~ vertices. By \autoref{sets-in-hm}, exhaustively applying \autoref{isolated} gives us a bound of  on the number of sets in . Since we have , if we bound the size of each set in~ by a function linear in~, then the total number of vertices in sets in  is~. To conclude an~-vertex problem kernel, we have to provide a data reduction rule additionally to \autoref{rul:goodconnected}.

For each connected component in~ that is induced by a set~, we now bound~ by a function linear in~. Thus, we effectively bound the total number of vertices in sets in  by~.  Observe that since~ is a solution, every \FISG{} that contains a vertex from a set~ must also contain a vertex from~. Because  is nonadjacent to~, the \FISG{} ~must also contain a vertex from~. The following data reduction rule is based on the idea that if~ is too large and contains vertices of \FISG{}s, then we can find a small solution containing the vertices in~.

\begin{rul}\label{ruecken}
  Let~ be a solution and let~. Given a vertex set~
  such that~ is nonadjacent
  to~, if , then choose
  a vertex from~ and remove it from~.
\end{rul}

\noindent To prove the correctness of this data reduction rule, we need a series of observations. To this end, we use the following definition:

\begin{definition}
  For two vertex sets~ and~, we introduce the set~ of edges between~ and~. That is, .  We say that a solution \emph{destroys} an edge~, if the solution contains a vertex incident to~.
\end{definition}

\noindent For a solution~ and the vertex set~ of a connected component in~, the edges in~ \emph{separate} the vertices in~ from the vertices in~. This is shown in \autoref{ruecken-proof}. If a solution~ destroys all edges in~, then  is an isolated~2-plex.

\begin{lemma}\label{ruecken-lemma}
  Let~ and~ be solutions. Assume that there is a vertex set~
  and a set~ such that~ is nonadjacent
  to~. If~ does not destroy all edges in~, then it
  contains~ vertices from~.
\end{lemma}
\begin{proof}
  Because the solution  does not destroy all edges in~,
  there must exist an edge~.
\begin{figure}
  \centering
  \includegraphics{ruecken-proof}
  \caption{Empty squares are the vertices in the set~. Filled squares are in the solution~. Dashed edges are destroyed by~. Note that there are no edges from  to vertices in~. Shown are \FISG{}s that result if a solution~ does not destroy an edge~ and if~ does not contain all but one~vertex~in~.}
\label{ruecken-proof}
\end{figure}
Now assume that  does not contain two distinct
  vertices~, as shown in
  \autoref{ruecken-proof}. Because~ and because~ is
  nonadjacent to~, the vertex  incident to the
  edge~ cannot be adjacent to the vertices~.  But~ contains at least
  three vertices:~ and at least one vertex from~. Thus,
  the vertex ~is connected but nonadjacent to~ and~. We can
  conclude from \autoref{fisg-char} that they are part of a
  \FISG{}. This contradicts  being a solution.
\end{proof}

\begin{lemma}\label{exist-sol}
  Let~ and~ be solutions.  Assume that there is a vertex set~
  and a set~ such that~ is nonadjacent
  to~. If , then there exists a solution~
  with~ that destroys all edges in~.
\end{lemma}

\begin{proof}
  Assume that~ does not destroy all edges in~. From
  \autoref{ruecken-lemma} and from , we can conclude that there are at least 
  vertices from  in~. The set~ destroys all edges in~. Because~
  contains at least  vertices from~
  and~ instead contains~, the set  is not larger
  than~. The set ~is a solution,
  because~ is a 2-plex cluster graph and because
  ~is ~with the additional connected component formed
  by the \pl2~.
\end{proof}

\begin{lemma}\label{ruecken-time}
  \autoref{ruecken} is correct. Given a vertex set~ and a solution~, we can exhaustively apply \autoref{ruecken} in ~time.
\end{lemma}

\begin{proof}
  Let~ be the vertex chosen by \autoref{ruecken} and let~.
  We have to show that~ is a yes-instance if and only
  if~ is a yes-instance. If~ is a yes-instance, then there
  exists a solution~ with~ for~. Since~ is
  a 2-plex cluster graph,  is a \pcg2 as well. Thus, ~is a
  yes-instance.

  If ~is a yes-instance, then there exists a solution~
  with~ for~. From \autoref{exist-sol}, we can without
  loss of generality assume that the solution~ destroys all edges
  in~.  Now assume that~ is not a 2-plex cluster
  graph. From \autoref{FISG-has-v}, we can conclude that  contains
  a \FISG{}~ including~. The \FISG{}~ also contains a vertex~, because~ is a solution. However, the vertices~
  and~ are not connected in~, because ~destroys all edges
  in~. Therefore, ~cannot exist in~ and ~must be
  a solution for~. Because~, it follows that ~is
  a yes-instance.

  To prove the running time, recall that we can construct the set~ in  time. Then, in~ time, we construct a table  so that for every neighbor~ of~, we have~.  For each~, we now count the number of vertices in~ and  in ~time. If in the counting process, we find a vertex~ with~, then  is adjacent to~. This implies that \autoref{ruecken} is not applicable for~; we continue with the next set in . The removal of vertices works in ~time.
\end{proof}

\begin{korollar}\label{h0-vertices}
  Let~ be a solution. Assume that there is a vertex set~ and a
  set~ such that~ is nonadjacent to~.
  After exhaustively applying \autoref{ruecken} given~, the set~ contains at most  vertices.
\end{korollar}

\section{Kernel Size}
\label{result}
In this section, we count the total number of vertices remaining in a graph~ after all data reduction rules have been applied. To this end, we assume that we have a solution~ and a set~ that is peripheral with reference to~. Then, we count the vertices in~, the vertices in~ and the vertices in the connected components in~ that are not in~.

Observe that to bound the sizes of the connected components in~, which are induced by sets in~, we have presented \emph{two} data reduction rules in \autoref{redundant}. \autoref{rul:goodconnected} is applicable to all sets in~. The additional \autoref{ruecken} is only applicable to sets in the collection  is nonadjacent~to~. Thus, we independently count the vertices in the sets in~ and the vertices in the sets in~ is adjacent~to~. \autoref{fig:h0} shows an example for a set in , \autoref{fig:h1} shows an example for a set in . We have already made this distinction when we bounded the number of sets in~ in \autoref{unmarked-bounds}; it is not the only distinction~we~make:

\autoref{per}(\ref{per3}) for peripheral sets ensures that if there is more than one set  such that a vertex~ is adjacent to~, then each such set~ satisfies~. To allow for a tighter worst-case analysis, we count the vertices in such sets independently. To this end, we use the following lemma:

\begin{lemma}\label{nx-sum}
Let the set~ be peripheral with respect to a solution~.
For the sets
  
the following relations hold:
  
\end{lemma}

\begin{proof}
  Let  be a set such that~ is only adjacent to vertices in~.  For a vertex~ that is adjacent to~, there is by definition of~ no other set~ such that~ is adjacent to~. Thus, if we count the number of vertices in~ for all~, then we count every vertex~ exactly once. This proves the first relation.

  For each~, there is by definition of  at least one vertex~ such that~ is adjacent to~. Thus, .

  According to \autoref{per}(\ref{per1}), there are at most two sets~ such that~ is adjacent to~. The set  only contains sets~ such that a vertex in~ is adjacent to~. This yields .
\end{proof}

\noindent Given a solution~ and a set~ that is peripheral with respect to~, we now assume that all data reduction rules have been exhaustively applied to our input graph~ and count the vertices in the connected components in~ that are not in~.

\begin{lemma}\label{unmarked-boundary}
  Let~ be a solution and let the set~ be peripheral with respect to~. After exhaustively applying \autoref{isolated}, \autoref{rul:goodconnected} and \autoref{ruecken}, it holds that
  
\end{lemma}

\begin{proof}Let~ and~ be as
  defined in \autoref{nx-sum}. We can conclude from
  \autoref{h1-vertices} and \autoref{h0-vertices} that
   is upper-bounded by
  
  We can interpret this term as a function in~
  and~ with fixed~ and~. Subject to the
  constraint~, it is maximal for~ and~. This
  yields the desired result.
\end{proof}

\begin{satz}\label{kern-size}
  \pvd 2 has a problem kernel containing 
  vertices. It can be found in ~time.
\end{satz}

\begin{proof}
  Given a \pvd 2 instance~, we first compute a constant-factor approximate solution~ using \autoref{find-X}. Then, we compute a set that is peripheral with respect to~ using \autoref{mark-neighbors}. We apply \autoref{high-occurence-fast}, from which we obtain a new parameter~ and a peripheral set~ with~ according to \autoref{size-M}. Finally, we apply \autoref{isolated}, \autoref{rul:goodconnected}, and \autoref{ruecken} to~. The so-obtained graph and the new parameter~ constitute our problem kernel.

  We first show that after applying all data reduction rules to~, the size of~ only depends on the parameter~. To this end, we count the vertices in the solution~, the vertices in the peripheral set~ and the vertices in~ that are not in the peripheral set~. If~ is a yes-instance, then \autoref{X-boundary} gives an upper bound of~ on the number of vertices in the constant-factor approximate solution~. If~ is larger, we terminate our kernelization algorithm and output that~ is a no-instance. By applying \autoref{high-occurence-fast}, we obtain a peripheral set~ that contains at most~ vertices according to \autoref{size-M}.  By exhaustively applying \autoref{isolated}, \autoref{rul:goodconnected}, and \autoref{ruecken} to~, we can use \autoref{unmarked-boundary} to give a bound of  on the number of vertices in~ that are not in the peripheral set~. Adding~ and~, we conclude that~ contains at most~~vertices.

The correctness of \autoref{high-occurence-fast}, \autoref{isolated}, \autoref{rul:goodconnected}, and \autoref{ruecken}, has been shown in \autoref{high-occurence}, \autoref{isolated-time}, \autoref{goodconnected-correct}, and \autoref{ruecken-time}, respectively.

Finally, we show the running time of our kernelization algorithm. When we construct an approximate solution~ using \autoref{find-X}, we can stop after finding more than~ pairwise vertex-disjoint \FISG{}s, because this implies that~ is a no-instance. Analog\-ously to the proof of \autoref{find-X-time}, it follows that we can construct~ in ~time. \autoref{mark-neighbors}, \autoref{high-occurence-fast}, \autoref{isolated}, \autoref{rul:goodconnected}, and \autoref{ruecken} run in~ time according to \autoref{mark-neighbors-time}, \autoref{high-occurence-time}, \autoref{isolated-time}, \autoref{goodconnected-time}, and \autoref{ruecken-time}, respectively.
\end{proof}

\noindent To solve a \pvd 2 instance, we can compute a problem kernel with~ vertices and reduce this problem kernel to a \hs 4 instance with~ sets, as discussed in \autoref{introsec}. Then, we can solve this \hs 4 instance by combining Wahlström's algorithm for \hs 3\cite{Wah07} with iterative compression, as discussed by Dom et al.\cite{DGHNT09}.

\begin{korollar}\label{interleave}
Using \hs 4, we can solve \pvd 2 in  time.
\end{korollar}

\paragraph{Concluding Remarks.} Peripheral sets played a central role in all stages of our kernelization algorithm. After constructing a peripheral set~ with respect to a solution~ using \autoref{mark-neighbors}, the peripheral set~ helps us to bound the number of the connected components in~ in \autoref{unmarked-bounds}. For a connected component in~, in \autoref{ddot-mu} we use the peripheral set~ to bound the number of vertices that are \emph{not} in the redundant set constructed by \autoref{construct-D}. Then, we remove vertices from that redundant set to bound the overall size of the connected component. In \autoref{weak}, we use the set of edges between~ and~ as a separator to develop an additional data reduction rule to further reduce the sizes of the connected components in~.

To construct a set~ that is peripheral with respect to a solution~, we employ \autoref{mark-neighbors}. We could also construct~ by enumerating all minimal \FISG{}s in~, which are shown in \autoref{forbidden}. Then, for each vertex~, we could pick an inclusion-maximal set of \FISG{}s that pairwisely intersect only in~. However, because each minimal \FISG{} contains four vertices, the total number of minimal \FISG{}s in a graph with~ vertices is~. In contrast, \autoref{mark-neighbors} finds at most~ \FISG{}s for each vertex~. It runs in~ time. Therefore, the running time of enumerating all minimal \FISG{}s in a graph might be significantly worse that of \autoref{mark-neighbors}.

\chapter[Kernelization for \pvd s]{Kernelization for \boldmath-Plex Cluster Vertex Deletion}
In this chapter, we generalize the problem kernel for \pvd 2 to \pvd s. We will see that many definitions and lemmas that we have worked out for the case~ also work for general  if we modify them slightly. In \autoref{per-sec}, we first show how to find an approximate solution~ for a graph~, so that~ is an \pcg s. Then, we generalize our concept of a peripheral set and show how to find one. In \autoref{bounds-s}, we revise our data reduction rules to bound the number and the sizes of the connected components in~.  In \autoref{result-s}, we conclude a problem kernel with~ vertices for \pvd s.

We now turn our attention to the main difference between \pvd 2 and \pvd s.  For \pvd 2, we used the fact that a \pl 2 containing at least three vertices is connected. We used this fact to construct a peripheral set using \autoref{mark-neighbors}, in the correctness proof of \autoref{rul:goodconnected}, and in the correctness proof of \autoref{ruecken}. To generalize these proofs, we need the following result:
 
\begin{lemma}\label{disconnected-size}
  An \pl s containing at least  vertices is a connected graph.
\end{lemma}
 
\begin{proof}
  Let  be an \pl s with more than one connected component. Because  is an \pl s, a vertex in  is nonadjacent to at most  other vertices in~.

  Let~ be the vertex set of a connected component of~. Because a vertex in~ is nonadjacent to all vertices in~, we have that~ and~. Therefore, it holds that~. Thus, if an \pl s contains at least~ vertices, it must be a connected graph.
\end{proof}
 
\noindent Note that the bound given in \autoref{disconnected-size} is tight. Consider two cliques with  vertices
  each. These two cliques can still be considered as one single~-plex with  vertices.

\section{Approximate Solutions and Peripheral Sets}\label{per-sec}
Given an \pvd s instance , in this section we first show how to
find an approximate solution~ for~. We then generalize our
concept of peripheral sets and construct a set that is peripheral with
respect to the solution~.

Similarly to the case~, we can easily find a constant-factor approximate solution for \pvd s using the algorithm by Guo et al.\cite{DBLP:conf/aaim/GuoKNU09}, which finds an -vertex \FISG{} in  time if we apply it to a graph that is not a \pcg{s}. In particular, if~ is the maximum integer satisfying , then Guo et al.\cite{DBLP:conf/aaim/GuoKNU09} show that their algorithm finds a \FISG{} with at most~ vertices. Similarly to \autoref{find-X-time}, we can show that \autoref{find-X} computes a constant-factor approximate solution for \pvd s.
 
\begin{lemma}\label{find-X-time-s}
  There is a factor- approximate solution for \pvd s and it can be found in ~time.
\end{lemma}
\begin{korollar}\label{X-boundary-s}
  Let  be a yes-instance and let  be a factor- approximate solution for~. Then,  contains ~vertices.
\end{korollar}

\noindent We now construct a set that is peripheral with respect to a
solution~. To this end, we modify \autoref{per}.

\begin{definition}\label{per-s}Let~ be a solution. We call a vertex
  set~ with the following properties \emph{peripheral (with
    respect to )}:
  \begin{compactenum}
  \item\label{per-s1} For each vertex~, there are at most  sets~ such that~ is adjacent~to~.
  \item\label{per-s2} If there is a vertex~ and a set~ such that~ is adjacent to~, then~ is nonadjacent to at most  vertices in~.
  \item\label{per-s3} For each vertex~, if there is more than one set~ such that~ is adjacent to~, then each such set~ satisfies~.
  \end{compactenum}
\end{definition}

\noindent To construct a peripheral set, we proceed analogously to
\autoref{mark}: for each vertex~ in a given solution~, we find a
\FISG{}~ including~ that contains no vertices from~. Then, we add the vertices of~ to~. We find such
\FISG{}s by three observations, each leading to one of three phases of
an algorithm that constructs the sets~.

We now turn to our first observation. Given a solution~, assume that there exists a vertex~ and a set~ of~ neighbors of~ such that~ contains a vertex~ that is nonadjacent to~. Then, the vertex~ is connected to every vertex in~, because the vertices in~ are neighbors of~. The vertex~ is nonadjacent to the~ vertices in~. By \autoref{fisg-char}, the graph~ is a \FISG{}.
 
\begin{proc}[Phase 1]\label{mark-neighbors-s}
  \begin{samepage}
    Given a graph~ and a solution~, for each vertex~, let~. For each~, as long as there is a set~ such~that
    \begin{compactenum}

    \item  and
    \item there exists a vertex~ that is nonadjacent to~,
    \end{compactenum}
    add the vertices in~ to~.
  \end{samepage}
\end{proc}\setcounter{proc}{0}

\noindent Now, for each vertex~ in the solution~, let  be
the set constructed by Phase~1 of \autoref{mark-neighbors-s}. For a
vertex~, assume that there exists a set~ such
that~ is adjacent to a vertex~. Further,
assume that the vertex~ is nonadjacent to a set~ of  vertices. Then, the graph~
is an induced subgraph of the \pl s~ and contains 
vertices. According to \autoref{disconnected-size}, it is
connected. The vertex~ is, because it is a neighbor of~ and because~ is adjacent to~, connected but nonadjacent to
the~ vertices in~. By \autoref{fisg-char}, the
graph~ is a \FISG{}. We continue
\autoref{mark-neighbors-s} as follows:
 
\begin{proc}[Phase 2]
  \begin{samepage}
    For each~, as long as there is a set~ such~that
    \begin{compactenum}
    \item the vertex~ is adjacent to a vertex~ and
    \item the vertex~ is nonadjacent to a set~ of vertices with~,
    \end{compactenum}
    add the vertex  and the vertices in~ to~.
  \end{samepage}
\end{proc}\setcounter{proc}{0}

\noindent Now, for each vertex~ in the solution~, let  be
the set constructed by Phase~1 and Phase~2 of
\autoref{mark-neighbors-s}. Assume that for a vertex~, there
are two sets~ such that~ is adjacent to the
vertices~ and~. Further,
assume that~ contains at least  vertices. Then,
 is a 2-plex containing at least~
vertices. According to \autoref{disconnected-size}, it is
connected. The vertex~ is nonadjacent to
at least~ vertices in~, but~ is
connected. According to \autoref{fisg-char}, it is a \FISG{}.
 
\begin{proc}[Phase~3]
  For each~, as long as there are~~such~that
  \begin{compactenum}
  \item  and
  \item the vertex~ has neighbors~ and~,
  \end{compactenum}
add the vertices~, ,
  and~ other vertices from~ to~.
\end{proc}
 
\noindent Note that in contrast to \autoref{mark-neighbors}, Phase~2 and Phase~3 of \autoref{mark-neighbors-s} do not necessarily find minimal \FISG{}s. That is, there exist \FISG{}s found by Phase~2 and Phase~3 such that we could remove a vertex from them and they would still be \FISG{}s. For running time considerations, we construct \FISG{}s from parts of \pl{s}es that contain enough vertices to derive their connectedness from \autoref{disconnected-size}. Thus, we do not have to explicitly check whether the subgraphs that we find are connected.
 
\begin{lemma}\label{s-sets}
 \prem{mark-neighbors-s} The set~ is peripheral with respect
  to~.
\end{lemma}

\begin{proof}
  The proof of this lemma is analogous to the proof of
  \autoref{two-sets}. For each vertex~, the set~ satisfies
  all properties in \autoref{per-s}. This follows directly from the
  description of \autoref{mark-neighbors}.
\end{proof}
 
\begin{lemma}\label{mark-neighbors-time-s}
  Given a solution~, \autoref{mark-neighbors-s} can be carried out in ~time.
\end{lemma}
 
\begin{proof}
  The running times of Phase~1 and Phase~2 of \autoref{mark-neighbors-s} can be proven in the same way as for \autoref{mark-neighbors} in \autoref{mark-neighbors-time}. We only prove the running time of the modified Phase~3. First, we construct for each vertex~ the set~. The proof of \autoref{mark-neighbors-time} shows how this can be done in~ time. For each vertex~, we can determine the set~ with~ in constant time, as seen in the proof of \autoref{mark-neighbors-time}. Counting the elements in~ takes at most~ time.  This yields a running time of~ for Phase~3 of \autoref{mark-neighbors-s}.
\end{proof}
 
\section{Adapted Data Reduction Rules and Bounds}\label{bounds-s}
\noindent Given an \pvd s instance  and a solution~ for~,
we now bound the number and the sizes of the connected
components in~. To this end, we first revise
\autoref{high-occurence-fast} as shown below.

\begin{rul}\label{high-occurence-fast-s}
  \premv{mark-neighbors-s} If there exists a vertex~ such that~, then delete~ from~ and~ and decrement~ by one.
\end{rul}

\begin{lemma}\label{high-occurence-time-s}
  \autoref{high-occurence-fast-s} is correct. Given a solution~ and the set~ constructed by \autoref{mark-neighbors-s} for each vertex~, we can exhaustively apply \autoref{high-occurence-fast-s} in ~time.
\end{lemma}
 
\begin{proof}
  For each vertex~ in a solution~, \autoref{mark-neighbors-s} adds at most~ vertices to~ for each found \FISG{}. If a vertex~ satisfies , then more than~ \FISG{}s pairwisely intersect only in~. According to \autoref{high-occurence}, we can delete~ from~ and decrement~ by one. The running time can be shown analogously to \autoref{high-occurence-time}.
\end{proof}

\begin{korollar}\label{size-M-s}
  Let~ be a solution for~. For each~, let~ be the
  set constructed by \autoref{mark-neighbors-s}. After exhaustively
  applying \autoref{high-occurence-fast} to~ and~, the peripheral set
   contains at most  vertices.
\end{korollar}

\noindent Given a graph~ and a solution~, we can apply
\autoref{isolated} without any changes compared to the case~. As
we have seen in \autoref{unmarked-bounds}, \autoref{sets-in-hm} and
\autoref{per} then bound the number of connected components
in~. It is left to bound their sizes.  To this end, we only need
to slightly change \autoref{rul:goodconnected} and
\autoref{ruecken}. Recall that the connected components in~ are
induced by sets in the collection~. We start with a revision of
\autoref{rul:goodconnected}:
 
\begin{rul}\label{rul:goodconnected-s}
  Let~ be a solution, let  and let  be a
  redundant subset of~ as defined in \autoref{redundant-def}. If , choose an arbitrary vertex
  from~ and remove it from~.
\end{rul}

\noindent For the correctness proof of
\autoref{rul:goodconnected-s}, observe that \autoref{connected} and \autoref{in-hi} are still valid if we prove them under the following assumption instead of proving them under \autoref{working-asu}:

\begin{asu}\label{working-asu-s}
  Let~ be a solution and let ~be a redundant subset of . Assume that \autoref{rul:goodconnected-s} chooses to remove  from~.  Further, assume that there exists a solution~ with  for the graph~.
\end{asu}

\noindent \autoref{working-asu-s} implies that ; otherwise, \autoref{rul:goodconnected-s} could not have been applied. Because~ is a 2-plex, ~is connected. The graph  is connected for the same reason.  In the following, we write ~for~. 

\begin{lemma}\label{goodconnected-correct-s}
  \autoref{rul:goodconnected-s} is
  correct.
\end{lemma}

\begin{proof}
  We have to show that~ is a yes-instance if and only
  if~ is a yes-instance. If  is a yes-instance, then there
  exists a solution~ with~ such that ~is an \pcg
  s. Clearly, then also ~is an \pcg s and ~is a
  yes-instance.

  If ~is a yes-instance, then there exists a solution~ with~ such that ~is an \pcg s, implying that \autoref{working-asu-s} is true. Assume that ~contains a \FISG{}. By \autoref{FISG-has-v}, there exists a \FISG{}~ in~ containing the vertex~.  Because~ is a \FISG{}, it contains a vertex~ that is connected but nonadjacent to a set~ of  other vertices in~.

  If~, then \autoref{connected} shows that the
  vertices in~ are connected to all vertices
  in~. Thus, the vertices in~
  would exist in~ and would be connected. That contradicts
  ~being an \pcg s, because~ is nonadjacent but connected to
  the~ vertices in~. Thus, we have .

  First, assume that . That is, the vertex~ is
  nonadjacent to~. From \autoref{in-hi}, we can conclude
  that~. Because also~,
  this contradicts the graph  being an \pl s. Thus,
  we have~.

  Because~, we have that~ is nonadjacent to~. From \autoref{in-hi}, we conclude that . By \autoref{redundant-def}, there exists an \md X  with~ and~, because the vertex~ is nonadjacent to the vertex~. But then, because the vertex~ is nonadjacent to~, the vertices in~ must also be in~ by \autoref{in-hi}. Because also~ is in , this again contradicts~ being an \pl s. We conclude that ~must be a \pcg s.  Thus, ~is a yes-instance.
\end{proof}

\noindent We employ \autoref{construct-D} to construct redundant
sets. For a solution~, the bound on the number
of vertices in a connected component in~ then changes as follows:
 
\begin{lemma}\label{h1-vertices-s}
  Let the set~ be peripheral with respect to a solution~.  For
  a vertex set~, let~ be the redundant subset constructed by
  \autoref{construct-D}. After exhaustively applying
  \autoref{rul:goodconnected-s} using~, the number of vertices
  in~ is .
\end{lemma}
 
\begin{proof}
  To prove the above lemma, we study the sets constructed in \autoref{construct-D}. By construction of~, we have~. Observe that because~, we also have~. Because~ is an \pl s, there exist at most~ vertices~ for every vertex~ such that~ and~ are nonadjacent. Thus, we have .  Because~ is peripheral, we can conclude from \autoref{per-s} that for each vertex~, there are at most ~vertices~ such that~ and~ are nonadjacent. Thus, we have .  Now, again because ~is an \pl s, there exist at most ~vertices~ for every vertex~ such that~ and~ are nonadjacent. Thus, we have .  This shows that the number of vertices in~ is .  After applying \autoref{rul:goodconnected}, the number of vertices in~ is .
\end{proof}

\noindent Let~ be peripheral with respect to a solution~. We now revise \autoref{ruecken} to reduce the sizes of the connected components in~ that are induced by sets~ such that~ is nonadjacent to~. Refer to \autoref{fig:h0} for an example.

\begin{rul}\label{ruecken-s}
  Let~ be a solution and let~. Given a vertex set~ such that~ is nonadjacent to~, if , then choose a vertex from  and remove it from~.
\end{rul}

\begin{lemma}\label{ruecken-time-s}
  \autoref{ruecken-s} is correct. Given a vertex set~, we can exhaustively apply \autoref{ruecken-s} in ~time.
\end{lemma}

\begin{proof}
  Let~ be a solution. First, observe that analogous to the proof of \autoref{ruecken-lemma}, we can show that if~ does not destroy all edges between vertices in~ and~, then it must contain at least~ vertices from~. If~, then we can analogously to the proof of \autoref{exist-sol} find a solution~ with~ that destroys all edges between vertices in~ and~. From this, \autoref{ruecken-time-s} follows analogously to \autoref{ruecken-time}.
\end{proof}

\begin{korollar}\label{h0-vertices-s}
  Let~ be a solution. Assume that there is a vertex set~ and a set~ such that~ is nonadjacent to~.  After exhaustively applying \autoref{ruecken-s} given~, the number of vertices in~ is .
\end{korollar}
 
\section{Kernel Size}
\label{result-s}
Given an \pvd s instance , we now give a bound on the number of vertices in~ after all data reduction rules have been applied. Given a solution~, recall that for a connected component in~ that is induced by a set~, the set~ is nonadjacent to~ (cf.\,\autoref{fig:h0}); for a vertex set~, the set~ is adjacent to~ (cf.\,\autoref{fig:h1}). We handle connected components induced by vertex sets in~ and~ separately.

\begin{lemma}\label{nx-sum-s}
Let the set~ be peripheral with respect to a solution~.
For the sets
  
  the following relations hold:
  
\end{lemma}
 
\begin{proof}
  This follows analogously to the proof of \autoref{nx-sum} with \autoref{per-s} for peripheral sets.
\end{proof}

\begin{satz}
  \pvd s has a problem kernel with~ vertices. It can be found in ~time.
\end{satz}

\begin{proof}
  Given an \pvd s instance , we first find a constant-factor approximate solution~ for~ using \autoref{find-X}. If~ is a yes-instance, we have~ according to \autoref{X-boundary-s}. In this case, we find~ in  time according to \autoref{find-X-time-s}, because if we find more than~ \FISG{}s using \autoref{find-X}, then we can stop and output that~ is a no-instance. After constructing the constant-factor approximate solution~, we construct a set~ that is peripheral with respect to~. According to \autoref{mark-neighbors-time-s}, this can be done in~ time using \autoref{mark-neighbors-s}. According to \autoref{size-M-s}, we can use \autoref{high-occurence-fast-s} to reduce the size of~ to at most  vertices. This can be done in~ time according to \autoref{high-occurence-time-s}. We then apply \autoref{isolated} in~~time as shown in \autoref{isolated-time}, followed by \autoref{rul:goodconnected-s}. Analogously to the proof of \autoref{goodconnected-time}, we can show that this works in~~time.  Finally, we apply \autoref{ruecken-s}, which runs in~ time; this follows analogously to the proof of \autoref{ruecken-time}.

  We now count the vertices that remain in~. The graph~ contains vertices from~, vertices from~, and vertices from the connected components in~ that are not in~. As shown above, we have~ and~. It is left to count the vertices in . Let~ and~ be as defined in \autoref{nx-sum-s}. We can conclude from \autoref{h1-vertices-s} and \autoref{h0-vertices-s} that the size of  is
  
By \autoref{nx-sum-s}, this is . Using  and adding the vertices in~ and~, this is . Thus, the total number of vertices in~ is~.
\end{proof}

\chapter{Conclusion and Outlook}
We have shown an -vertex problem kernel for \pvd s. This result is comparable with the~-vertex problem kernel for \name{-Plex Editing} shown by Guo et al.\cite{DBLP:conf/aaim/GuoKNU09}: in an~-vertex graph, one vertex deletion can lead to~ edge deletions. Under the assumption that input graphs for clustering problems are typically dense, this suggests that typical parameter values for \name{-Plex Editing} are at least quadratic in parameter values for \pvd s; the parameter is the number of allowed graph modifications. Seen from this angle, our result seems consistent with the result that \name{-Plex Editing} has a problem kernel with~~vertices.

It is open whether \pvd s has an -vertex problem kernel for some constant~. It is also open to improve the -factor in the number of vertices in our problem kernel. This factor results from the size of the constant-factor approximate solution shown in \autoref{X-boundary-s}, from the size of the peripheral set shown in \autoref{size-M-s}, and from the way we construct redundant sets in \autoref{h1-vertices-s}. The most promising approach to improve on the~-factor seems to be the construction of larger redundant sets so that more vertices can be removed by \autoref{rul:goodconnected-s}.

In \autoref{introsec}, we discussed how to solve \pvd s using \hs d for a natural number~. For \hs d, problem kernels containing~ or~ elements are known\cite{DBLP:conf/wads/Abu-Khzam07,DBLP:conf/stacs/Kratsch09,Flu06}. This bound is exponential in~ and  is in turn bounded by a function linear in~. This yields an upper bound on the number of elements in a \hs d kernel that is exponential in~. From this angle, it is remarkable that problem kernels for \pvd s as well as for \name{-Plex Editing} exist whose number of vertices is bounded by a polynomial in~ as~well~as~in~.

It might be hard to find a search tree for \pvd s that is smaller than the search tree for an equivalent \hs d instance. However, a \hs d instance obtained from our \pvd s problem kernel contains~ sets. This bound is exponential in~. Thus, constructing a \hs d instance from an \pvd s instance might be practically infeasible. It is open to find faster algorithms for \pvd s that do not rely on \hs d.

The most promising approach to faster algorithms for \pvd s seems to be iterative compression\cite{GMN2009} introduced by Reed et al.\cite{RSV04}.  Hüffner et al.\cite{ HKMN09TOCS} have successfully applied it to \cvd. Using iterative compression, we can solve \pvd s by solving multiple instances of the following problem:

\decprob{\textsc{Disjoint} \pvd s}{A graph~, a non-negative number~, and a solution~ with~ such that~ is an \pcg s.}{Is there an alternative solution~ with~ and~ such that~ is an \pcg s?}

\noindent Fellows et al.\cite{DBLP:conf/mfcs/FellowsGMN09} have shown that while the analog problem for \cvd{} is in~, \textsc{Disjoint} \pvd s is -hard.  After some initial observations on the case~, we guess that \textsc{Disjoint} \pvd 2 can be solved using a size- search tree. In combination with our kernelization algorithm, we could then solve \pvd 2 in~ time for some constant~.

\bibliographystyle{plain}
\bibliography{splex-notizen}{}

\end{document}
