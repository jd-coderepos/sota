\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs} \usepackage{multirow} \usepackage{siunitx} 

\usepackage{overpic}
\usepackage{bbm}
\usepackage{dsfont}

\newcommand\orl[1]{\textcolor{blue}{Or: #1}}
\newcommand\rqi[1]{\textcolor{red}{Charles: #1}}
\newcommand\leo[1]{\textcolor{green}{Leo: #1}}
\newcommand\kaiming[1]{\textcolor{cyan}{Kaiming: #1}}
\newcommand\todo[1]{\textcolor{magenta}{TODO: #1}}

\newcommand\boxnet{BoxNet}
\newcommand\votenet{VoteNet}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{1861} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Deep Hough Voting for 3D Object Detection in Point Clouds}

\author{Charles R. Qi~~~~Or Litany~~~~Kaiming He~~~~Leonidas J. Guibas \vspace{0.2cm}\\
Facebook AI Research~~~~Stanford University
}


\maketitle


\begin{abstract}
   








Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds.
In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible.
However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step.
To address the challenge, we propose \votenet{}, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting.
Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, \votenet{} outperforms previous methods by using purely geometric information without relying on color images.   \end{abstract}

\section{Introduction}




The goal of 3D object detection is to localize and recognize objects in a 3D scene. More specifically, in this work, we aim to estimate oriented 3D bounding boxes as well as semantic classes of objects from point clouds.


Compared to images, 3D point clouds provide accurate geometry and robustness to illumination changes. On the other hand, point clouds are irregular.
thus typical CNNs are not well suited to process them directly.


\begin{figure}\centering
    \vspace{14pt}
    \begin{overpic}
    [trim=0cm 0cm 0cm 0cm,clip,width=0.95\linewidth]{fig/teaser.pdf}
    \put(2,43){\small Voting from input point cloud}
    \put(60,43){\small 3D detection output}
    \end{overpic}
\caption{\textbf{3D object detection in point clouds with a deep Hough voting model.} Given a point cloud of a 3D scene, our \votenet{} votes to object centers and then groups and aggregates the votes to predict 3D bounding boxes and semantic classes of objects. Our code is open sourced at \url{https://github.com/facebookresearch/votenet}
    }
    \label{fig:teaser}
\end{figure}

To avoid processing irregular point clouds, current 3D detection methods heavily rely on 2D-based detectors in various aspects. For example, \cite{song2016deep,hou20183d} extend 2D detection frameworks such as the Faster/Mask R-CNN~\cite{ren2015faster,he2017mask} to 3D. They voxelize the irregular point clouds to regular 3D grids and apply 3D CNN detectors, which fails to leverage sparsity in the data and suffer from high computation cost due to expensive 3D convolutions.
Alternatively,~\cite{cvpr17chen,zhou2018voxelnet} project points to regular 2D bird's eye view images and then apply 2D detectors to localize objects. This, however, sacrifices geometric details which may be critical in cluttered indoor environments. More recently,~\cite{lahoud20172d,qi2018frustum} proposed a cascaded two-step pipeline by firstly detecting objects in front-view images and then localizing objects in frustum point clouds extruded from the 2D boxes, which however is strictly dependent on the 2D detector and will miss an object entirely if it is not detected in 2D.



In this work we introduce a \emph{point cloud focused} 3D detection framework that directly processes raw data and does not depend on any 2D detectors neither in architecture nor in object proposal. Our detection network, \emph{\votenet{}}, is based on recent advances in 3D deep learning models for point clouds, and is inspired by the generalized Hough voting process for object detection~\cite{leibe2004combined}.

We leverage PointNet++~\cite{qi2017pointnetplusplus}, a hierarchical deep network for point cloud learning, to mitigates the need to convert point clouds to regular structures. By directly processing point clouds not only do we avoid information loss by a quantization process, but we also take advantage of the sparsity in point clouds by only computing on sensed points.

While PointNet++ has shown success in object classification and semantic segmentation ~\cite{qi2017pointnetplusplus}, few research study how to detect 3D objects in point clouds with such architectures.
A na\"ive solution would be to follow common practice in 2D detectors and perform dense object proposal~\cite{liu2016ssd,ren2015faster}, i.e. to propose 3D bounding boxes directly from the sensed points (with their learned features). However, the inherent sparsity of point clouds makes this approach unfavorable. In images there often exists a pixel near the object center, but it is often not the case in point clouds. As depth sensors only capture surfaces of objects, 3D object centers are likely to be in empty space, far away from any point. As a result, point based networks have difficulty aggregating scene context in the vicinity of object centers. Simply increasing the receptive field does not solve the problem because as the network captures larger context, it also causes more inclusion of nearby objects and clutter.


To this end, we propose to endow point cloud deep networks with a voting mechanism similar to the classical \emph{Hough voting}.  
By \emph{voting} we essentially generate new points that lie close to objects centers, which can be \textit{grouped and aggregated} to generate box \textit{proposals}. 



In contrast to traditional Hough voting with multiple separate modules that are difficult to optimize jointly, \emph{\votenet{}} is end-to-end optimizable. Specifically, after passing the input point cloud through a backbone network, we sample a set of seed points and generate votes from their features. Votes are targeted to reach object centers. As a result, vote clusters emerge near object centers and in turn can be aggregated through a learned module to generate box proposals. The result is a powerful 3D object detector that is purely geometric and can be applied directly to point clouds.



We evaluate our approach on two challenging 3D object detection datasets: SUN RGB-D~\cite{song2015sun} and ScanNet~\cite{dai2017scannet}. On both datasets \votenet{}, \emph{using geometry only}, significantly outperforms prior arts that use both RGB and geometry or even multi-view RGB images.
Our study shows that the voting scheme supports more effective context aggregation, and verifies that \votenet{}~offers the largest improvements when object centers are far from the object surface (e.g. tables, bathtubs, etc.).




In summary, the contributions of our work are:
\begin{itemize}
    \setlength\itemsep{0.1em}
    \item A reformulation of Hough voting in the context of deep learning through an end-to-end differentiable architecture, which we dub \votenet{}.
    \item State-of-the-art 3D object detection performance on SUN RGB-D and ScanNet.
    \item An in-depth analysis of the importance of voting for 3D object detection in point clouds.
\end{itemize}



\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{./fig/votingnet4.pdf}
    \caption{\textbf{Illustration of the \votenet~architecture} for 3D object detection in point clouds. Given an input point cloud of  points with XYZ coordinates, a backbone network (implemented with PointNet++~\cite{qi2017pointnetplusplus} layers) subsamples and learns deep features on the points and outputs a subset of  points but extended by -dim features. This subset of points are considered as seed points. Each seed independently generates a vote through a voting module. Then the votes are grouped into clusters and processed by the proposal module to generate the final proposals. The classified and NMSed proposals become the final 3D bounding boxes output. Image best viewed in color.}
    \label{fig:votingnet}
\end{figure*}
 
\section{Related Work}
\paragraph{3D object detection.}
Many previous methods were proposed to detect 3D bounding boxes of objects.
Examples include: \cite{lin2013holistic} where a pair-wise semantic context potential helps guide the proposals objectness score;  template-based methods~\cite{li2015database, Indoor2012, litany2015asist}; Sliding-shapes \cite{song2014sliding} and its deep learning-based successor \cite{song2016deep}; Clouds of Oriented Gradients (COG)~\cite{ren2016three}; and the recent 3D-SIS \cite{hou20183d}.


Due to the complexity of directly working in 3D, especially in large scenes, many methods resort to some type of projection.   For example in MV3D~\cite{cvpr17chen} and VoxelNet \cite{zhou2018voxelnet}, the 3D data is first reduced to a bird's-eye view before proceeding to the rest of the pipeline. A reduction in search space by first processing a 2D input was demonstrated in both Frustum PointNets \cite{qi2018frustum} and \cite{lahoud20172d}. Similarly, in \cite{kim2013accurate} a segmentation hypothesis is verified using the 3D map. 
More recently, deep networks on point clouds are used to exploit sparsity of the data by GSPN~\cite{yi2018gspn} and PointRCNN~\cite{shi2018pointrcnn}.



\paragraph{Hough voting for object detection.}
Originally introduced in the late 1950s, the Hough transform \cite{hough1959machine} translates the problem of detecting simple patterns in point samples to detecting peaks in a parametric space. The Generalized Hough Transform~\cite{ballard1981generalizing} further extends this technique to image patches as indicators for the existence of a complex object. Examples of using Hough voting include the seminal work of \cite{leibe2008robust} which introduced the implicit shape model, planes extraction from 3D point clouds~\cite{borrmann20113d}, and 6D pose estimation~\cite{sun2010depth} to name a few. 


Hough voting has also been previously combined with advanced learning techniques. In ~\cite{maji2009object} the votes were assigned with weights indicating their importance, which were learned using a max-margin framework.  
Hough forests for object detection were introduced in~\cite{gall2011hough,gall2013class}. More recently, \cite{kehl2016deep} demonstrated improved voting-based 6D pose estimation by using deep features extracted to build a codebook. Similarly \cite{milletari2017hough} learned deep features to build codebooks for segmentation in MRI and ultrasiounds images. In~\cite{huan2017vehicle} the classical Hough algorithm was used to extract circular patterns in car logos, which were then input to a deep classification network.
\cite{novotny2018semi} proposed the semi-convolutional operator for 2D instance segmentation in images, which is also related to Hough voting.

There have also been works using Hough voting for 3D object detection~\cite{woodford2014demisting,knopp2010orientation,velizhev2012implicit,knopp2011scene}, which adopted a similar pipeline as in 2D detectors.


























\paragraph{Deep learning on point clouds.}
Recently we see a surge of interest in designing deep network architectures suited for point clouds~\cite{qi2017pointnet, qi2017pointnetplusplus, su2018splatnet, atzmon2018point, li2018pointcnn, graham20183d, wang2017cnn, tatarchenko2017octree,tatarchenko2018tangent,le2018pointgrid,klokov2017escape,yang2018foldingnet,xu2018spidercnn,wang2018dynamic,xie2018attentional}, which showed remarkable performance in 3D object classification, object part segmentation, as well as scene segmentation. In the context of 3D object detection, VoxelNet~\cite{zhou2018voxelnet} learn voxel feature embeddings from points in voxels, while in~\cite{qi2018frustum} PointNets are used to localize objects in a frustum point cloud extruded from a 2D bounding box. However, few methods studied how to directly propose and detect 3D objects in raw point cloud representation.











 
\section{Deep Hough Voting}
\label{sec:deep_hough_voting}


A traditional Hough voting 2D detector~\cite{leibe2008robust} comprises an offline and an online step. First, given a collection of images with annotated object bounding boxes, a codebook is constructed with stored mappings between image patches (or their features) and their offsets to the corresponding object centers.  
At inference time, interest points are selected from the image to extract patches around them. These patches are then compared against patches in the codebook to retrieve offsets and compute votes. As \textit{object} patches will tend to vote in agreement, clusters will form near object centers. Finally, the object boundaries are retrieved by tracing cluster votes back to their generating patches. 



We identify two ways in which this technique is well suited to our problem of interest.
First, voting-based detection is more compatible with sparse sets than region-proposal networks (RPN)~\cite{ren2015faster} is. For the latter, the RPN has to generate a proposal near an object center which is likely to be in an empty space, causing extra computation.
Second, it is based on a bottom-up principle where small bits of partial information are accumulated to form a confident detection. Even though neural networks can potentially aggregate context from a large receptive field, it may be still beneficial to aggregate in the vote space. 



However, as traditional Hough voting comprises multiple separated modules, integrating it into state-of-the-art point cloud networks is an open research topic. To this end, we propose the following adaptations to the different pipeline ingredients.
\smallskip











\noindent \textbf{Interest points}
are described and selected by deep neural networks instead of depending on hand-crafted features.

\smallskip


\noindent\textbf{Vote} generation is learned by a network instead of using a codebook. Levaraging larger receptive fields, voting can be made less ambiguous and thus more effective. In addition, a vote \emph{location} can be augmented with a \emph{feature vector} allowing for better aggregation. 



\smallskip
\noindent\textbf{Vote aggregation} is realized through point cloud processing layers with trainable parameters. Utilizing the vote features, the network can potentially filter out low quality votes and generate improved proposals.



\smallskip
\noindent\textbf{Object proposals} in the form of: location, dimensions, orientation and even semantic classes can be directly generated from the aggregated features, mitigating the need to trace back votes' origins.




In what follows, we describe how to combine all the aforementioned ingredients into a single end-to-end trainable network named as \votenet{}.




%
 
\section{VoteNet Architecture}


Fig.~\ref{fig:votingnet} illustrates our end-to-end detection network (\emph{\votenet{}}).
The entire network can be split into two parts: one that processes \emph{existing} points to generate votes; and the other part that operates on \emph{virtual} points -- the votes -- to propose and classify objects.


\subsection{Learning to Vote in Point Clouds}
\label{sec:votingnet:vote}


From an input point cloud of size , with a 3D coordinate for each of the  points, we aim to generate  votes, where each vote has both a 3D coordinate and a high dimensional feature vector. There are two major steps: point cloud feature learning through a backbone network and learned Hough voting from seed points.





\smallskip\noindent\textbf{Point cloud feature learning.}
Generating an accurate vote requires geometric reasoning and contexts. Instead of relying on hand-crafted features, we leverage recently proposed deep networks~\cite{qi2017pointnetplusplus,graham20183d,su2018splatnet,li2018pointcnn} on point clouds for point feature learning. While our method is not restricted to any point cloud network, we adopt PointNet++~\cite{qi2017pointnetplusplus} as our backbone due to its simplicity and demonstrated success on tasks ranging from normal estimation~\cite{guerrero2018pcpnet}, semantic segmentation~\cite{landrieu2018large} to 3D object localization~\cite{qi2018frustum}.

The backbone network has several set-abstraction layers and feature propagation (upsampling) layers with skip connections, which outputs a subset of the input points with XYZ and an enriched -dimensional feature vector. The results are  \emph{seed points} of dimension . Each seed point generates one vote\footnote{The case of more than one vote is discussed in the appendix.}.









\smallskip\noindent\textbf{Hough voting with deep networks.} Compared to traditional Hough voting where the votes (offsets from local keypoints) are determined by look ups in a pre-computed codebook, we generate votes with a deep network based voting module, which is both more efficient (without kNN look ups) and more accurate as it is trained jointly with the rest of the pipeline.

Given a set of seed points  where  with  and , a shared \emph{voting module} generates votes from each seed independently. Specifically, the voting module is realized with a multi-layer perceptron (MLP) network with fully connected layers, ReLU and batch normalization. The MLP takes seed feature  and outputs the Euclidean space offset  and a feature offset  such that the vote  generated from the seed  has  and .

The predicted 3D offset  is explicitly supervised by a regression loss


\noindent
where  indicates whether a seed point  is on an object surface and  is the count of total number of seeds on object surface.  is the ground truth displacement from the seed position  to the bounding box center of the object it belongs to. 



Votes are the same as seeds in tensor representation but are no longer grounded on object surfaces. A more essential difference though is their position -- votes generated from seeds on the same object are now closer to each other than the seeds are, which makes it easier to combine cues from different parts of the object. Next we will take advantage of this semantic-aware locality to aggregate vote features for object proposal.







\subsection{Object Proposal and Classification from Votes}
\label{sec:votingnet:pooling}



The votes create canonical ``meeting points'' for context aggregation from different parts of the objects. After clustering these votes we aggregate their features to generate object proposals and classify them. 

\smallskip\noindent\textbf{Vote clustering through sampling and grouping.}
While there can be many ways to cluster the votes, we opt for a simple strategy of uniform sampling and grouping according to spatial proximity. Specifically, from a set of votes , we sample a subset of  votes using farthest point sampling based on  in 3D Euclidean space, to get  with . Then we form  clusters by finding neighboring votes to each of the 's 3D location:  for . Though simple, this clustering technique is easy to integrate into an end-to-end pipeline and works well in practice.

\smallskip\noindent\textbf{Proposal and classification from vote clusters.}
As a vote cluster is in essence a set of high-dim points, we can leverage a generic point set learning network to aggregate the votes in order to generate object proposals. Compared to the back-tracing step of traditional Hough voting for identifying the object boundary, this procedure allows to propose \textit{amodal} boundaries even from partial observations, as well as predicting other parameters like orientation, class, etc.  

In our implementation, we use a \emph{shared} PointNet~\cite{qi2017pointnet} for vote aggregation and proposal in clusters. Given a vote cluster  with  and its cluster center , where  with  as the vote location and  as the vote feature. To enable usage of local vote geometry, we transform vote locations to a local normalized coordinate system by . Then an object proposal for this cluster  is generated by passing the set input through a PointNet-like module:



\noindent
where votes from each cluster are independently processed by a  before being max-pooled (channel-wise) to a single feature vector and passed to  where information from different votes are further combined. We represent the proposal  as a multidimensional vector with an objectness score, bounding box parameters (center, heading and scale parameterized as in~\cite{qi2018frustum}) and semantic classification scores.













\smallskip\noindent\textbf{Loss function.}
The loss functions in the proposal and classification stage consist of objectness, bounding box estimation, and semantic classification losses.



We supervise the objectness scores for votes that are located either close to a ground truth object center (within  meters) or far from any center (by more than  meters). We consider proposals generated from those votes as \textit{positive} and \textit{negative} proposals, respectively. Objectness predictions for other proposals are not penalized. Objectness is supervised via a cross entropy loss normalized by the number of non-ignored proposals in the batch. For positive proposals we further supervise the bounding box estimation and class prediction according to the closest ground truth bounding box. Specifically, we follow ~\cite{qi2018frustum} which decouples the box loss to center regression, heading angle estimation and box size estimation. For semantic classification we use the standard cross entropy loss. In all regression in the detection loss we use the Huber (smooth-~\cite{ren2015faster}) loss. Further details are provided in the appendix. 













\subsection{Implementation Details}
\label{sec:votingnet:implmentation}

\smallskip\noindent\textbf{Input and data augmentation.} Input to our detection network is a point cloud of  points randomly sub-sampled from either a popped-up depth image () or a 3D scan (mesh vertices, ).
In addition to  coordinates, we also include a height feature for each point indicating its distance to the floor. The floor height is estimated as the  percentile of all points' heights.
To augment the training data, we randomly sub-sample the points from the scene points on-the-fly. We also randomly flip the point cloud in both horizontal direction, randomly rotate the scene points by  around the upright-axis, and randomly scale the points by .

\smallskip\noindent\textbf{Network architecture details.} The backbone feature learning network is based on PointNet++~\cite{qi2017pointnetplusplus}, which has four set abstraction (SA) layers and two feature propagation/upsamplng (FP) layers, where the SA layers have increasing receptive radius of , ,  and  in meters while they sub-sample the input to , ,  and  points respectively. The two FP layers up-sample the 4th SA layer's output back to  points with -dim features and 3D coordinates (more details in the appendix).








The voting layer is realized through a multi-layer perceptron with FC output sizes of , where the last FC layer outputs XYZ offset and feature residuals.


The proposal module is implemented as a set abstraction layer with a post processing  to generate proposals after the max-pooling. The SA uses radius  and  with output sizes of . The max-pooled features are further processed by  with output sizes of  where the output consists of  objectness scores,  center regression values,  numbers for heading regression ( heading bins) and  numbers for box size regression ( box anchors) and  numbers for semantic classification.

\smallskip\noindent\textbf{Training the network.} We train the entire network end-to-end and from scratch with an Adam optimizer, batch size 8 and an initial learning rate of . The learning rate is decreased by  after 80 epochs and then decreased by another  after 120 epochs. Training the model to convergence on one Volta Quadro GP100 GPU takes around 10 hours on SUN RGB-D and less than 4 hours on ScanNetV2.

\smallskip\noindent\textbf{Inference.} Our \votenet{} is able to take point clouds of the entire scenes and generate proposals in one forward pass. The proposals are post-processed by a 3D NMS module with an IoU threshold of . The evaluation follows the same protocol as in~\cite{song2016deep} using mean average precision.
 
\section{Experiments}
In this section, we firstly compare our Hough voting based detector with previous state-of-the-art methods on two large 3D indoor object detection benchmarks (Sec.~\ref{sec:exp:sota}). We then provide analysis experiments to understand the importance of voting, the effects of different vote aggregation approaches and show our method's advantages in its compactness and efficiency (Sec.~\ref{sec:exp:analysis}). Finally we show qualitative results of our detector (Sec.~\ref{sec:qualitative}). More analysis and visualizations are provided in the appendix.







\begin{table*}[t!]
\small
\setlength{\tabcolsep}{4.8pt}
\begin{center}
\begin{tabular}{l|c|x{25}x{25}x{25}x{25}x{25}x{25}x{25}x{25}x{25}x{25}|c}
\toprule
          & Input & bathtub & bed & bookshelf & chair & desk & dresser & nightstand & sofa & table & toilet & mAP \\ \midrule
DSS~\cite{song2016deep} & Geo + RGB & 44.2 & 78.8 & 11.9 & 61.2 & 20.5 & 6.4 & 15.4 & 53.5 & 50.3 & 78.9 & 42.1    \\
COG~\cite{ren2016three} & Geo + RGB & 58.3 & 63.7 & 31.8 & 62.2 & \textbf{45.2} & 15.5 & 27.4 & 51.0 & \textbf{51.3} & 70.1 & 47.6 \\
2D-driven~\cite{lahoud20172d} & Geo + RGB & 43.5 & 64.5 & 31.4 & 48.3 & 27.9 & 25.9 & 41.9 & 50.4 & 37.0 & 80.4 & 45.1  \\
F-PointNet~\cite{qi2018frustum} & Geo + RGB & 43.3 & 81.1 & \textbf{33.3} & 64.2 & 24.7 & \textbf{32.0} & 58.1 & 61.1 & 51.1 & \textbf{90.9} & 54.0 \\ \midrule
\votenet~(ours) & \textbf{Geo only} & \textbf{74.4} & \textbf{83.0} & 28.8 & \textbf{75.3} & 22.0 & 29.8 & \textbf{62.2} & \textbf{64.0} & 47.3 & 90.1 & \textbf{57.7} \\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{3D object detection results on SUN RGB-D val set.} Evaluation metric is average precision with 3D IoU threshold 0.25 as proposed by~\cite{song2015sun}. Note that both COG~\cite{ren2016three} and 2D-driven~\cite{lahoud20172d} use room layout context to boost performance. To have fair comparison with previous methods, the evaluation is on the SUN RGB-D V1 data.
}
\label{tab:sunrgbd}
\end{table*}

\begin{table}[t!]
\small
\setlength{\tabcolsep}{3.2pt}
\begin{center}
\begin{tabular}{l |c| c c}
\toprule
    & Input & mAP@0.25 & mAP@0.5 \\ \hline    
    DSS~\cite{song2016deep,hou20183d} & Geo + RGB & 15.2 & 6.8  \\
    MRCNN 2D-3D~\cite{he2017mask,hou20183d} & Geo + RGB & 17.3 & 10.5 \\
    F-PointNet~\cite{qi2018frustum,hou20183d} & Geo + RGB & 19.8 & 10.8 \\
    GSPN~\cite{yi2018gspn} & Geo + RGB & 30.6 & 17.7 \\ \midrule
    3D-SIS \cite{hou20183d} & Geo + 1 view & 35.1 & 18.7 \\ 
    3D-SIS \cite{hou20183d} & Geo + 3 views & 36.6 & 19.0 \\
    3D-SIS \cite{hou20183d} & Geo + 5 views & 40.2 & 22.5 \\ \midrule
    3D-SIS \cite{hou20183d} & Geo only & 25.4 & 14.6 \\
\votenet~(ours) & Geo only & \textbf{58.6} & \textbf{33.5} \\ \bottomrule 
\end{tabular}
\end{center}
\caption{\small \textbf{3D object detection results on ScanNetV2 val set.} DSS and F-PointNet results are from~\cite{hou20183d}. Mask R-CNN 2D-3D results are from~\cite{yi2018gspn}. GSPN and 3D-SIS results are up-to-date numbers provided by the original authors.
}
\label{tab:scannet}
\end{table}

\subsection{Comparing with State-of-the-art Methods}
\label{sec:exp:sota}




\noindent\textbf{Dataset.}
SUN RGB-D \cite{song2015sun} is a single-view RGB-D dataset for 3D scene understanding. It consists of 5K RGB-D training images annotated with amodal oriented 3D bounding boxes for  object categories. To feed the data to our network, we firstly convert the depth images to point clouds using the provided camera parameters. We follow a standard evaluation protocol and report performance on the  most common categories.


ScanNetV2 \cite{dai2017scannet} is a richly annotated dataset of 3D reconstructed meshes of indoor scenes. It contains 1.2K training examples collected from hundreds of different rooms, and annotated with semantic and instance segmentation for  object categories. Compared to partial scans in SUN RGB-D, scenes in ScanNetV2 are more complete and cover larger areas with more objects on average.
We sample vertices from the reconstructed meshes as our input point clouds.  Since ScanNetV2 does not provide amodal or oriented bounding box annotation, we aim to predict axis-aligned bounding boxes instead, as in~\cite{hou20183d}.

\smallskip
\noindent\textbf{Methods in comparison.} We compare with a wide range of prior art methods. Deep sliding shapes (DSS)~\cite{song2016deep} and 3D-SIS~\cite{hou20183d} are both 3D CNN based detectors that combine geometry and RGB cues in object proposal and classification, based on the Faster R-CNN~\cite{ren2015faster} pipeline. Compared with DSS, 3D-SIS introduces a more sophisticated sensor fusion scheme (back-projecting RGB features to 3D voxels) and therefore is able to use multiple RGB views to improve performance. 2D-driven~\cite{lahoud20172d} and F-PointNet~\cite{qi2018frustum} are 2D-based 3D detectors that rely on object detection in 2D images to reduce the 3D detection search space. Cloud of gradients~\cite{ren2016three} is a sliding window based detector using a newly designed 3D HoG-like feature. MRCNN 2D-3D is a na\"ive baseline that directly projects Mask-RCNN~\cite{he2017mask} instance segmentation results into 3D to get a bounding box estimation. GSPN~\cite{yi2018gspn} is a recent instance segmentation method using a generative model to propose object instances, which is also based on a PointNet++ backbone.

\smallskip
\noindent\textbf{Results}
are summarized in Table \ref{tab:sunrgbd} and \ref{tab:scannet}. \votenet{} outperforms all previous methods by at least \textbf{3.7} and \textbf{18.4} mAP increase in SUN RGB-D and ScanNet respectively. Notably, we achieve such improvements when we \emph{use geometric input (point clouds) only} while they used both geometry and RGB images. Table~\ref{tab:sunrgbd} shows that in the category ``chair'' with the most training samples, our method improves upon previous state of the art by more than \textbf{11 AP}.
Table~\ref{tab:scannet} shows that when taking geometric input only, our method outperforms 3D CNN based method 3D-SIS by more than \textbf{33 AP}.
A per-category evaluation for ScanNet is provided in the appendix. Importantly, the same set of network hyper-parameters was used in both datasets. 





\subsection{Analysis Experiments}
\label{sec:exp:analysis}



\paragraph{To Vote or Not To Vote?}






\begin{table}[t!]
    \small
    \begin{center}
    \begin{tabular}{c|cc}
    \toprule
        Method & \multicolumn{2}{c}{mAP@0.25} \\
        & SUN RGB-D & ScanNet \\ \midrule
\boxnet~(ours) & 53.0 & 45.4 \\ \midrule
        \votenet~(ours) & \textbf{57.7} & \textbf{58.6} \\
        \bottomrule
    \end{tabular}
    \end{center}
    \caption{\textbf{Comparing \votenet{} with a no-vote baseline.} Metric is 3D object detection mAP. \votenet~estimate object bounding boxes from vote clusters. \boxnet~ proposes boxes directly from seed points on object surfaces without voting.}
    \label{tab:vote_or_not}
\end{table}

A straightforward baseline to \votenet{} is a network that directly proposes boxes from sampled scene points. Such a baseline -- which we refer to as \emph{\boxnet{}} -- is essential to distill the improvement due to voting. The \boxnet~has the same backbone as the \votenet{} but instead of voting, it directly generates boxes from the seed points (more details in appendix). Table~\ref{tab:vote_or_not} shows voting boosts the performance by a significant margin of 5 mAP on SUN RGB-D and 13 mAP on ScanNet.



In what ways, then, does voting help? We argue that since in sparse 3D point clouds, existing scene points are often far from object centroids, a direct proposal may have lower confidence and inaccurate amodal boxes. Voting, instead, brings closer together these lower confidence points and allows to reinforce their hypothesis though aggregation. We demonstrate this phenomenon in Fig.~\ref{fig:voting_vs_novote} on a typical ScanNetV2 scene. We overlay the scene with only those seed points which, if sampled, would generate an accurate proposal. As can be seen, \votenet{} (right) offers a much broader coverage of ``good'' seed points compared to \boxnet{} (left), showing its robustness brought by voting.

We proceed with a second analysis in Fig.~\ref{fig:perclass} showing on the same plot (in separate scales), for each SUN RGB-D category: (in blue dots) gains in mAP between \votenet{} and \boxnet{}, and (in red squares) closest distances between object points (on their surfaces) and their amodal box centers, averaged per category and normalized by the mean class size (a large distance means the object center is usually far from its surface). Sorting the categories according to the former, we see a strong correlation. Namely, when object points tend to be further from the amodal box center, voting helps much more.






\begin{figure}[t!]
    \centering
    \begin{overpic}
    [trim=0cm 0cm 0cm 0cm,clip,width=0.8\linewidth]{fig/voteOrNot.pdf}
    \put(8,73){\small \boxnet{} (no voting)}
    \put(65,73){\small \votenet{}}
    \end{overpic}
\caption{\textbf{Voting helps increase detection contexts.} Seed points that generate good boxes (\boxnet), or good votes (\votenet) which in turn generate good boxes, are overlaid (in blue) on top of a representative ScanNet scene. As the voting step effectively increases context, \votenet{} demonstrates a much denser cover of the scene, therefore increasing the likelihood of accurate detection.}
    \label{fig:voting_vs_novote}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/acc_vs_dist.pdf}
    \caption{\textbf{Voting helps more in cases where object points are far from object centers}. We show for each category: voting accuracy gain (in blue dots) of \votenet{} w.r.t our direct proposal baseline \boxnet{}; and (in red squares) average object-center distance, normalized by the mean class size.}
    \label{fig:perclass}
\end{figure}

\paragraph{Effect of Vote Aggregation}
\label{sec:aggregation_variants}
Aggregation of votes is an important component in \votenet{} as it allows communication between votes. Hence, it is useful to analyze how different aggregation schemes influence performance. 





In Fig.~\ref{fig:vote_pooling} (right), we show that vote aggregation with a learned Pointnet and max pooling achieves far better results than manually aggregating the vote features in the local regions due to the existence of clutter votes (i.e. votes from non-object seeds). We test 3 types of those aggregations (first three rows): max, average, and RBF weighting (based on vote distance to the cluster center). In contrast to aggregation with Pointnet (Eq.~\ref{eq:vote_aggregation}), the vote features are directly pooled, e.g. for avg. pooling: ).

In Fig.~\ref{fig:vote_pooling} (left), we show how vote aggregation radius affects detection (tested with Pointent using max pooling). As the aggregation radius increases, \votenet{} improves until it peaks at around  radius. Attending to a larger region though introduces more clutter votes thus contaminating the good votes and results in decreased performance.










\begin{figure}[t!]
\begin{minipage}[c]{0.52\linewidth}
\begin{center}
\includegraphics[width=\linewidth]{./fig/vote_pooling2.pdf}
\end{center}
\end{minipage}\begin{minipage}[c]{0.48\linewidth}
\begin{center}
{
\fontsize{8pt}{1em}\selectfont
\begin{tabular}{c|c}
\toprule
  Aggregation method & mAP \\
\midrule
Feature avg. & 47.2 \\ 
Feature max & 47.8 \\
Feature RBF avg. & 49.0 \\
\midrule
Pointnet (avg.) & 56.5 \\
Pointnet (max) & 57.7 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{minipage}
\vspace{0.1in}
    \caption{\textbf{Vote aggregation analysis.} \emph{Left:} mAP@0.25 on SUN RGB-D for varying aggregation radii when aggregating via Pointnet (max). \emph{Right:} Comparisons of different aggregation methods (radius =  for all methods). Using a learned vote aggregation is far more effective than manually pooling the features in a local neighborhood.}
    \label{fig:vote_pooling}
\end{figure}


\paragraph{Model Size and Speed}
Our proposed model is very efficient since it leverages sparsity in point clouds and avoids search in empty space. Compared to previous best methods (Table~\ref{tab:size_speed}), our model is more than  smaller than F-PointNet (the prior art on SUN RGB-D) in size and more than  times faster than 3D-SIS (the prior art on ScanNetV2) in speed. Note that the ScanNetV2 processing time by 3D-SIS is computed as averaged time in offline batch mode while ours is measured with sequential processing which can be realized in online applications.

\begin{table}[b!]
\small
    \begin{center}
    \begin{tabular}{l|c|c|c}
    \toprule
        Method & Model size & SUN RGB-D & ScanNetV2 \\ \midrule
        F-PointNet~\cite{qi2018frustum} & MB &  & -\\
        3D-SIS~\cite{hou20183d} & MB & - & s \\ \midrule
        \votenet~(ours) & 11.2MB &  & s\\ \bottomrule
    \end{tabular}
    \end{center}
    \caption{\textbf{Model size and processing time (per frame or scan).} Our method is more than  more compact in model size than~\cite{qi2018frustum} and more than  faster than~\cite{hou20183d}.}
    \label{tab:size_speed}
\end{table}



\begin{figure*}[t!]
    \centering
    \begin{overpic}
    [trim=0cm 0cm 0cm 0cm,clip,width=0.86\linewidth]{fig/scannet_results_for_paper.pdf}
    \put(15,50){\small \votenet{} prediction}
    \put(70,50){\small Ground truth}
    \end{overpic}
    \caption{\textbf{Qualitative results of 3D object detection in ScanNetV2.} Left: our \votenet{}, Right: ground-truth. See Section \ref{sec:qualitative} for details.}
    \label{fig:qualitative_results}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.86\linewidth]{fig/sunrgbd_results.jpg}
\caption{\textbf{Qualitative results on SUN RGB-D.} Both left and right panels show (from left to right): an image of the scene (not used by our network), 3D object detection by \votenet{}, and ground-truth annotations. See Section \ref{sec:qualitative} for details.}
    \label{fig:qualitative_results2}
\end{figure*}



\subsection{Qualitative Results and Discussion}
\label{sec:qualitative}
Fig.~\ref{fig:qualitative_results} and Fig.~\ref{fig:qualitative_results2} show several representative examples of \votenet{} detection results on ScanNet and SUN RGB-D scenes, respectively. As can be seen, the scenes are quite diverse and pose multiple challenges including clutter, partiality, scanning artifacts, etc. Despite these challenges, our network demonstrates quite robust results. See for example in Fig.~\ref{fig:qualitative_results}, how the vast majority of chairs were correctly detected in the top scene. Our method was able to nicely distinguish between the attached sofa-chairs and the sofa in the bottom left scene; and predicted the complete bounding box of the much fragmented and cluttered desk at the bottom right scene.

There are still limitations in our method though. Common failure cases include misses on very thin objects like doors, windows and pictures denoted in black bounding boxes in the top scene (Fig.~\ref{fig:qualitative_results}). As we do not make use of RGB information, detecting these categories is almost impossible. Fig.~\ref{fig:qualitative_results2} on SUN RGB-D also reveals the strengths of our method in partial scans with single-view depth images. For example, it detected more chairs in the top-left scene than were provided by the ground-truth. In the top-right scene we can see how \votenet{} can nicely hallucinate the amodal bounding box despite seeing only part of the sofa. A less successful amodal prediction is shown in the bottom right scene where an extremely partial observation of a very large table is given. 
 
\section{Conclusion}
In this work we have introduced \votenet{}: a simple, yet powerful 3D object detection model inspired by Hough voting. The network learns to vote to object centroids directly from point clouds and learns to aggregate votes through their features and local geometry to generate high-quality object proposals. Using only 3D point clouds, the model showed significant improvements over previous methods that utilize both depth and colored images.


In future work we intend to explore how to incorporate RGB images into our detection framework and to utilize our detector in downstream application such as 3D instance segmentation. We believe that the synergy of Hough voting and deep learning can be generalized to more applications such as 6D pose estimation, template based detection etc. and expect to see more future research along this line. 
\paragraph{Ackownledgements.}
This work was supported in part by ONR MURI grant N00014-13-1-0341, NSF grant IIS-1763268 and a Vannevar Bush Faculty Fellowship. We thank Daniel Huber, Justin Johnson, Georgia Gkioxari and Jitendra Malik for valuable discussions and feedback.





{\small
\bibliographystyle{ieee_fullname}
\bibliography{pcl}
}

\newpage
\appendix
\section{Appendix}
\label{sec:overview}
This appendix provides additional details on the network architectures and loss functions (Sec.~\ref{sec:arch_details}), more analysis experiment results (Sec.~\ref{sec:more_analysis}), per-category results on ScanNet (Sec.~\ref{sec:percat_scannet}), and finally more visualizations (Sec.~\ref{sec:more_vis}).


\subsection{Details on Architectures and Loss Functions}
\label{sec:arch_details}

\paragraph{\votenet{} architecture details.} As mentioned in the main paper, the \votenet{} architecture composes of a backbone point feature learning network, a voting module and a proposal module.

The backbone network, based on the PointNet++ architecture~\cite{qi2017pointnetplusplus}, has four set abstraction layers and two feature up-sampling layers. The detailed layer parameters are shown in Table~\ref{tab:votingnet_detail}. Each set abstraction (SA) layer has a receptive field specified by a ball-region radius , a MLP network for point feature transform  where  is output channel number of the -th layer in the MLP. The SA layer also subsamples the input point cloud with farthest point sampling to  points. Each SA layer is specified by  as shown in the Table~\ref{tab:votingnet_detail}. Compared to~\cite{qi2017pointnetplusplus}, we also normalize the XYZ scale of points in each local region by the region radius.

Each set feature propagation (FP) layer upsamples the point features by interpolating the features on input points to output points (each output point's feature is weighted average of its three nearest input points' features). It also combines the skip-linked features through a MLP (interpolated features and skip-linked features are concatenated before fed into the MLP). Each FP layer is specified by  where  is the output of the -th layer in the MLP.

The voting module as mentioned in the main paper is a MLP that transforms seeds' features to votes including a XYZ offset and a feature offset. The seed points are outputs of the fp2 layer. The voting module MLP has output sizes of  for its fully connected layers. The last fully connected layer does not have ReLU or BatchNorm.

The proposal module as mentioned in the main paper is a SA layer followed by another MLP after the max-pooling in each local region. We follow~\cite{qi2018frustum} on how to parameterize the oriented 3D bounding boxes. The layer's output has  channels where  is the number of heading bins (we predict a classification score for each heading bin and a regression offset for each bin -- relative to the bin center and normalized by the bin size),  is the number of size templates (we predict a classification score for each size template and 3 scale regression offsets for height, width and length) and  is the number of semantic classes. In SUN RGB-D: , in ScanNet: . In the first  channels, the first two are for objectness classification and the rest three are for center regression (relative to the vote cluster center). 

\begin{table*}[]
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        layer name & input layer & type & output size &  layer params\\ \hline
        sa1 & raw point cloud & SA & (2048,3+128) & (2048,0.2,[64,64,128]) \\ 
        sa2 & sa1 & SA & (1024,3+256) & (1024,0.4,[128,128,256])\\
        sa3 & sa2 & SA & (512,3+256) & (512,0.8,[128,128,256]) \\
        sa4 & sa3 & SA & (256,3+256) & (256,1.2,[128,128,256]) \\
        fp1 & sa3, sa4 & FP & (512,3+256) & [256,256] \\
        fp2 & sa2, sa3 & FP & (1024,3+256) & [256,256] \\ \hline
    \end{tabular}
    \end{center}
    \caption{Backbone network architecture: layer specifications.}
    \label{tab:votingnet_detail}
\end{table*}

\paragraph{\votenet{} loss function details.}
The network is trained end-to-end with a multi-task loss including a voting loss, an objectness loss, a 3D bounding box estimation loss and a semantic classification loss. We weight the losses such that they are in similar scales with ,  and .



Among the losses, the vote regression loss is as defined in the main paper (with L1 distance). For ScanNet we compute the ground truth votes as offset from the mesh vertices of an instances to the centers of the axis-aligned tight bounding boxes of the instances. Note that since the bounding box is not amodal, they can vary in sizes due to scan completeness (e.g. a chair may have a floating bounding box if its leg is not recovered from the reconstruction). For SUN RGB-D since the dataset does not provide instance segmentation annotations but only amodal bounding boxes, we cannot compute a ground truth vote directly (as we don't know which points are on objects). Instead, we consider any point inside an annotated bounding box as an object point (required to vote) and compute its offset to the box center as the ground truth. In cases that a point is in multiple ground truth boxes, we keep a set of up to three ground truth votes, and consider the minimum distance between the predicted vote and any ground truth vote in the set during vote regression on this point.

The objectness loss is just a cross-entropy loss for two classes and the semantic classification loss is also a cross-entropy loss of  classes.

The box loss follows~\cite{qi2018frustum} (but without the corner loss regularization for simplicity) and is composed of center regression, heading estimation and size estimation sub-losses. In all regression in the box loss we use the robust -smooth loss. Both the box and semantic losses are only computed on positive vote clusters and normalized by the number of positive clusters. We refer readers to~\cite{qi2018frustum} for more details.




One difference though is that, instead of a naive regression loss, we use a \emph{Chamfer loss}~\cite{fan2017point} for  (between regressed centers and ground truth box centers). It requires that each positive proposal is close to a ground truth object and each ground truth object center has a proposal near it. The latter part also influences the voting in the sense that it encourages non-object seed points near the object to also vote for the center of the object, which helps further increase contexts in detection.


\paragraph{BoxNet architecture details.}
Our baseline network without voting, BoxNet, shares most parts with the \votenet{}. They share the same backbone architecture. But instead of voting from seeds, the BoxNet directly proposes bounding boxes and classifies object classes from seed points' features. To make the BoxNet and \votenet{} have similar capacity we also include a SA layer for the proposal in BoxNet. However this SA layer takes \emph{seed clusters} instead of \emph{vote clusters} i.e. it samples seed points and then combines neighboring seeds with MLP and max-pooling. This SA layer has exactly the same layer parameters with that in the \votenet{}, followed by the same .

\paragraph{BoxNet loss function details.}
BoxNet has the same loss function as \votenet{}, except it is not supervised by vote regression. There is also a slight difference in how objectness labels (used to supervise objectness classification) are computed. As seed points (on object surfaces) are often far from object centroids, it no longer works well to use the distances between seed points and object centroids to compute the objectness labels. In BoxNet, we assign positive objectness labels to seed points that are on objects (those belonging to the semantic categories we consider) and negative labels to all the other seed points on clutter (e.g. walls, floors).



\begin{table*}[t!]
\begin{center}
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|cccccccccccccccccc|c}
\toprule
& cab & bed & chair & sofa & tabl & door & wind & bkshf & pic & cntr & desk & curt & fridg & showr & toil & sink & bath & ofurn & mAP \\ \midrule
3DSIS 5views~\cite{hou20183d} & 19.76 & 69.71 & 66.15 & 71.81 & 36.06 & 30.64 & 10.88 & 27.34 & 0.00 & 10.00 & 46.93 & 14.06 & 53.76 & 35.96 & 87.60 & 42.98 & 84.30 & 16.20 & 40.23 \\
3DSIS Geo~\cite{hou20183d} & 12.75 & 63.14 & 65.98 & 46.33 & 26.91 & 7.95 & 2.79 & 2.30 & 0.00 & 6.92 & 33.34 & 2.47 & 10.42 & 12.17 & 74.51 & 22.87 & 58.66 & 7.05 & 25.36 \\
\votenet{} {ours} & 36.27 & 87.92 & 88.71 & 89.62 & 58.77 & 47.32 & 38.10 & 44.62 & 7.83 & 56.13 & 71.69 & 47.23 & 45.37 & 57.13 & 94.94 & 54.70 & 92.11 & 37.20 & 58.65 \\
\bottomrule
\end{tabular}
\end{center}
\caption{3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.25 IoU.}
\label{tab:perclassscannet025}
\end{table*}

\begin{table*}[t!]
\begin{center}
\footnotesize
\setlength{\tabcolsep}{3.2pt}
\begin{tabular}{l|cccccccccccccccccc|c}
\toprule
& cab & bed & chair & sofa & tabl & door & wind & bkshf & pic & cntr & desk & curt & fridg & showr & toil & sink & bath & ofurn & mAP \\ \midrule
3DSIS 5views~\cite{hou20183d} & 5.73 & 50.28 & 52.59 & 55.43 & 21.96 & 10.88 & 0.00 & 13.18 & 0.00 & 0.00 & 23.62 & 2.61 & 24.54 & 0.82 & 71.79 & 8.94 & 56.40 & 6.87 & 22.53 \\
3DSIS Geo~\cite{hou20183d} & 5.06 & 42.19 & 50.11 & 31.75 & 15.12 & 1.38 & 0.00 & 1.44 & 0.00 & 0.00 & 13.66 & 0.00 & 2.63 & 3.00 & 56.75 & 8.68 & 28.52 & 2.55 & 14.60 \\
\votenet{} (ours) & 8.07 & 76.06 & 67.23 & 68.82 & 42.36 & 15.34 & 6.43 & 28.00 & 1.25 & 9.52 & 37.52 & 11.55 & 27.80 & 9.96 & 86.53 & 16.76 & 78.87 & 11.69 & 33.54 \\
\bottomrule
\end{tabular}
\end{center}
\caption{3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.5 IoU.}
\label{tab:perclassscannet050}
\end{table*}

\subsection{More Analysis Experiments}
\label{sec:more_analysis}

\paragraph{Average precision and recall plots}
Fig.~\ref{fig:ap_ar} shows how average precision (AP) and average recall (AR) change as we increase the number of proposals. The AP and AR are both averaged across 10 categories on SUN RGB-D. We report two ways of using the proposals: joint and per-class. For the joint proposal we propose  objects' bounding boxes for all the 10 categories, where we consider each proposal as the semantic class it has the largest confidence in, and use their objectness scores to rank them. For the per-class proposal we duplicate the  proposal 10 times thus have  proposals per class where we use the multiplication of semantic probability for that class and the objectness probability to rank them. The latter way of using proposals gives us  a slight improvement on AP and a big boost on AR.

We see that with as few as 10 proposals our \votenet{} can achieve a decent AP of around  while having 100 proposals already pushes the AP to above . With a thousand proposals, our network can achieve around  recall with joint proposal and around  recall with per-class proposal.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./fig/ap_ar.pdf}
    \caption{\textbf{Number of proposals per scene v.s. Average Precision (AP) and Average Recall (AR) on SUN RGB-D.} The AP and AR are averaged across the 10 classes. The recall is maximum recall given a fixed number of detection per scene. The ``joint proposal'' means that we assign each proposal to a single class (the class with the highest classification score); The ``per-class proposal'' means that we assign each proposal to all the 10 classes (the objectness score is multipled by the semantic classification probability).}
    \label{fig:ap_ar}
\end{figure}

\paragraph{Context of voting}
One difference of a deep Hough voting scheme with the traditional Hough voting is that we can take advantage of deep features, which can provide more context knowledge for voting. In Table~\ref{tab:vote_context} we show how features from different levels of the PointNet++ affects detection performance (from SA2 to FP3, the network has increasing contexts for voting). FP3 layer is extended from the FP2 with a MLP of output sizes 256 and 256 with 2048 output points (the same set of XYZ as that output by SA1).

It is surprising to find that voting from even SA2 can achieve reasonable detection results (mAP ) while voting from FP2 achieves the best performance. Having larger context (e.g. FP3) than FP2 does not show further improvements on the performance.

\begin{table}[h]
    \begin{center}
    \begin{tabular}{l|cccccc}
    \toprule
         Seed layer & SA2 & SA3 & SA4 & FP1 & FP2 & FP3 \\ \midrule
         mAP & 51.2 & 56.3 & 55.1 & 56.6 & \textbf{57.7} & 57.1 \\ \bottomrule
    \end{tabular}
    \end{center}
    \caption{\textbf{Effects of seed context for 3D detection.} Evaluation metric is mAP@0.25 on SUN RGB-D.}
    \label{tab:vote_context}
\end{table}



\paragraph{Multiple votes per seed}
In default we just generate one vote per seed since we find that with large enough context there is little need to generate more than one vote to resolve ambiguous cases. However, it is still possible to generate more than one vote with our network architecture. Yet to break the symmetry in multiple vote generation, one has to introduce some bias to different votes to prevent then from pointing to the same place.

In experiments, we find that one vote per seed achieves the best results, as shown in Table~\ref{tab:vote_num}. We ablate by using a vote factor of , where the voting module generates  votes per seed with a MLP layer spec: ). In computing the vote regression loss on a seed point, we consider the minimum distance between any predicted votes to the ground truth vote (in case of SUN RGB-D where we may have a set of ground truth votes for a seed, we compute the minimum distance among any pair of predicted vote and ground truth vote).


\begin{figure}[t!]
    \centering
\begin{overpic}
    [trim=0cm 0cm 0cm 0cm,clip,width=\linewidth]{fig/votes_scene.pdf}
\end{overpic}
    \caption{\textbf{Vote meeting point.} \emph{Left:} ScanNet scene with votes coming from object points. \emph{Right:} vote offsets from source seed-points to target-votes. Object votes are colored green, and non-object ones are colored red. See how object points from all-parts of the object vote to form a cluster near the center. Non-object points, however, either vote ``nowhere'' and therefore lack structure, or are near object and have gathered enough context to also vote properly. }
    \label{fig:showvotes}
\end{figure}


To break symmetry, we generate  random numbers and inject them to the second last features from the MLP layer. We show results both with and without this procedure which shows no observable difference.  

\begin{table}[]
    \begin{center}
    \begin{tabular}{c|c|c|c}
    \toprule
         Vote factor & 1 & 3 & 3 \\ \hline
         Random number & N & N & Y \\ \hline
         mAP & \textbf{57.7} & 55.8 & 55.8 \\ \bottomrule
    \end{tabular}
    \end{center}
    \caption{\textbf{Effects of number of votes per seed.} Evaluation metric is mAP@0.25 on SUN RGB-D. If random number is on, we concatenate a random number to the seed feature before voting, which helps break symmetry in the case of multiple votes per seed.}
    \label{tab:vote_num}
\end{table}

\paragraph{On proposal sampling}
In the proposal step, to generate  proposals from the votes, we need to select  vote clusters. How to select those clusters is a design choice we study here (each cluster is simply a group of votes near a center vote). In Table~\ref{tab:sampling}, we report mAP results on SUN RGB-D with 256 proposals (joint proposal) using cluster sampling strategies of vote FPS, seed FPS and random sampling, where FPS means farthest point sampling. From  vote clusters, vote FPS samples  clusters based on votes' XYZ. Seed FPS firstly samples on seed XYZ and then finds the votes corresponding to the sampled seeds -- it enables a direct comparison with BoxNet as it uses the same sampling scheme, making the two techniques similar up to the space in which the points are grouped: \votenet{} groups votes according to vote XYZ, while BoxNet groups seeds according to seed XYZ. Random sampling simply selects a random set of  votes and take their neighborhoods for proposal generation. Note that the results from Table~\ref{tab:sampling} are from the same model trained with vote FPS to select proposals.

We can see that while seed FPS gets the best number in mAP, the difference caused by different sampling strategies is small, showing the robustness of our method.

\begin{table}[]
    \begin{center}
    \begin{tabular}{l|c}
    \toprule
        Proposal sampling & mAP \\ \midrule
        Random sampling & 57.5\\ Farthest point sampling on votes & 57.2\\ Farthest point sampling on seeds & \textbf{57.7}\\ 
        \bottomrule
    \end{tabular}
    \end{center}
    \caption{\textbf{Effects of proposal sampling.} Evaluation metric is mAP@0.25 on SUN RGB-D.  proposals are used for all evaluations. Our method is not sensitive to how we choose centers for vote groups/clusters.}
    \label{tab:sampling}
\end{table}

\paragraph{Effects of the height feature}
In point clouds from indoor scans, point height is a useful feature in recognition. As mentioned in the main paper, we can use  of the  values (-axis is up-right) of all points from a scan as an approximate as the floor height , and then compute the a point 's height as . In Table~\ref{tab:height} we show how this extra height feature affect detection performance. We see that adding the height feature consistently improves performance in both SUN RGB-D and ScanNet.

\begin{table}[]
    \centering
    \begin{tabular}{l|c|c}
    \toprule
         Dataset & with height & without height \\ \midrule
         SUN RGB-D & 57.7 & 57.0 \\ \hline
         ScanNet & 58.6 & 58.1 \\ \bottomrule
    \end{tabular}
    \caption{\textbf{Effects of the height feature.} Evaluation metric is mAP@0.25 on both datasets.}
    \label{tab:height}
\end{table}

\subsection{ScanNet Per-class Evaluation}
\label{sec:percat_scannet}

Table~\ref{tab:perclassscannet025} and Table~\ref{tab:perclassscannet050} report per-class average precision on 18 classes of ScanNetV2 with  and  box IoU thresholds respectively. Relying on purely geonetric data, our method excels (esp. with mAP@0.25) in detecting objects like bed, chair, table, desk etc. where geometry is a strong cue for recognition; and struggles with objects best recognized by texture and color like pictures.


\subsection{Visualization of Votes}
\label{sec:more_vis}
Fig.~\ref{fig:showvotes} shows (a subset of) votes predicted from our \votenet{} in a typical ScanNet scene. We clearly see that seed points on objects (bed, sofa etc.) vote to object centers while clutter points vote either to object center as well (if the clutter point is close to the object) or to nowhere due to lack of structure in the clutter area (e.g. a wall).
 
\end{document}
