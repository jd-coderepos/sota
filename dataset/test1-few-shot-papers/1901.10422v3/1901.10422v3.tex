\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[final]{neurips_2019}




\usepackage[utf8]{inputenc} 
\usepackage[english]{babel}

 

\usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{slashbox}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{times}
\usepackage{epsfig}
\usepackage{placeins}
\usepackage{colortbl}
\usepackage{rotating}
\usepackage{array}
\usepackage{color}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{units}
\usepackage{textcomp}
\usepackage{bbding}
\usepackage{mwe}
\usepackage{graphicx}
\usepackage{subcaption}

\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}
\definecolor{lightgray}{gray}{0.4}
\definecolor{verylightgray}{gray}{0.7}
\definecolor{veryverylightgray}{gray}{0.9}

\definecolor{darkgreen}{rgb}{0, 0.5, 0}

\newcommand{\anna}[1]{\textcolor{blue}{Anna: #1}}
\newcommand{\dan}[1]{\textcolor{brown}{Dan: #1}}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \newcommand{\bvec}[1]{\displaystyle {#1}}

\captionsetup[table]{skip=5pt} 
\captionsetup[figure]{skip=5pt} 
\newtheorem{thm}{Theorem}
\newtheorem{pro}{Proposition}
\newtheorem{lma}{Lemma}
\newcommand{\beginsupplement}{\setcounter{table}{0}
	\renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0}
	\renewcommand{\thefigure}{S\arabic{figure}}\setcounter{section}{0}
	\renewcommand{\thepage}{S\arabic{page}} 
	\renewcommand{\thesection}{S\arabic{section}}  
	\setcounter{equation}{0}
	\renewcommand{\theequation}{S\arabic{equation}}
}

\makeatletter
\setlength{\parskip}{0.4pc}
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{-2pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{-2pt}

\makeatother


\title{Progressive Augmentation of GANs}

\vspace{0em}
\author{Dan~Zhang\\
	Bosch Center for Artificial Intelligence\\
\texttt{dan.zhang2@bosch.com} \\
\And
	Anna~Khoreva \\
	Bosch Center for Artificial Intelligence\\
\texttt{anna.khoreva@bosch.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
	Training of Generative Adversarial Networks (GANs) is notoriously fragile, 
requiring to maintain a careful balance between the generator and the discriminator in order to perform well. To mitigate this issue we introduce a new regularization technique -  \emph{progressive augmentation of GANs (PA-GAN)}. The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input or feature space, thus enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective, does not compromise the discriminator's optimality and encourages a healthy competition between the generator and discriminator, leading to the better-performing generator. We experimentally demonstrate the effectiveness of PA-GAN across different architectures and 
on multiple benchmarks for the image synthesis task, on average achieving  point improvement of the FID score.

	
\end{abstract}

\section{\label{sec:Introduction}Introduction}
Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} are a recent development in the field of deep learning, that have attracted a lot of attention in the research community~\cite{Radford2016UnsupervisedRL,SalimansNIPS2016,Arjovsky2017WGAN,karras2018progressive}. The GAN framework can be formulated as a competing game between the generator and the discriminator. Since both the generator and the discriminator are typically parameterized as deep convolutional neural networks with millions of parameters, optimization is notoriously difficult in practice~\cite{Arjovsky2017WGAN,gulrajani_NIPS2017,miyato2018spectral}.

The difficulty lies in maintaining a healthy competition between the generator and discriminator. A commonly occurring problem arises when the discriminator overshoots, leading to escalated gradients and oscillatory GAN behaviour~\cite{MeschederICML2018, Brock2019}. Moreover, the supports of the data and model distributions typically lie on low dimensional manifolds and are often disjoint~\cite{Arjovsky2017TowardsPM}. Consequently, there exists a nearly trivial discriminator that can perfectly distinguish real data samples from synthetic ones. Once such a discriminator is produced, its loss quickly converges to zero and the gradients used for updating parameters of the generator become useless. For improving the training stability of GANs regularization techniques~\cite{Roth_NIPS2017,gulrajani_NIPS2017} can be used to constrain the learning of the discriminator. But as shown in~\cite{Brock2019,Kurach2018GANlandscape} they also impair the generator and lead to the performance degradation.

In this work we introduce a new regularization technique to alleviate this problem  - \emph{progressive augmentation of GANs (PA-GAN)} - that helps to control the behaviour of the discriminator and thus improve the overall training.\footnote{\url{https://github.com/boschresearch/PA-GAN}} The key idea is to progressively augment the input of the discriminator network or its intermediate feature layers with auxiliary random bits in order to gradually increase the discrimination task difficulty (see Fig.~\ref{fig:Data_augmentation}). In doing so, the discriminator can be prevented from becoming over-confident, enabling continuous learning of the generator. As opposed to standard augmentation techniques (e.g. rotation, cropping, resizing), the proposed progressive augmentation does not directly modify the data samples or their features, but rather structurally appends to them. Moreover, it can also alter the input class. For instance, in the single-level augmentation the data sample or its features  are combined with a random bit  and both are provided to the discriminator. The class of the augmented sample  is then set based on the combination  with , resulting in real and synthetic samples contained in both classes, see Fig.~\ref{fig:Data_augmentation}-(a). This presents a more challenging task for the discriminator, as it needs to tell the real and synthetic samples apart plus additionally learn how to separate  back into  and  and understand the association rule. We can further increase the task difficulty of the discriminator by progressively augmenting its input or feature space, gradually increasing the number of random bits during the course of training as depicted in Fig.~\ref{fig:Data_augmentation}-(b).

We prove that PA-GAN preserves the original GAN objective and, in contrast to prior work~\cite{Arjovsky2017TowardsPM,Sonderby2016AmortisedMI,SalimansNIPS2016}, does not bias the optimality of the discriminator (see Sec.~\ref{sec:PAGAN}). Aiming at minimum changes we further propose an integration of PA-GAN into existing GAN architectures (see Sec.~\ref{subsec:implement}) and experimentally showcase its benefits (see Sec.~\ref{sec:PA-diff}). Structurally augmenting the input or its features and mapping them to higher dimensions not only challenges the discrimination task, but, in addition, with each realization of the random bits alters the loss function landscape, potentially providing a different path for the generator to approach the data distribution.

Our technique is orthogonal to existing work, it can be successfully employed with other regularization strategies~\cite{Roth_NIPS2017,gulrajani_NIPS2017,SalimansNIPS2016,JMLR:v15:srivastava14a,ChenSS2019} and different network architectures \cite{miyato2018spectral,Zhang_SAGAN18}, which we demonstrate in Sec.~\ref{subsec:Exp-Regulariz}. We experimentally show the effectiveness of PA-GAN for unsupervised image generation tasks on multiple benchmarks (Fashion-MNIST~\cite{xiao2017}, CIFAR10 \cite{Cifar10_Krizhevsky09learningmultiple}, CELEBA-HQ~\cite{karras2018progressive}, and Tiny-ImageNet~\cite{imagenet_cvpr09}), on average improving the FID score around  points. For PA combination with SS-GAN~\cite{ChenSS2019} we achieve the best FID of  for the unsupervised setting on CIFAR10, which is on par with the results achieved by large scale BigGAN training~\cite{Brock2019} using label supervision.  


\begin{figure}[t!]\vspace{-1em}
	\centering
	\begin{subfigure}{0.4\textwidth}
			\centering
\includegraphics[width=\linewidth]{figures/fig1_v2_6.pdf} \caption{Discriminator task}
	\end{subfigure}
\begin{subfigure}{0.45\textwidth}
		\centering
\includegraphics[width=\linewidth]{figures/fig1_v2_3_3.pdf} \caption{Input space augmentation}
\end{subfigure}
	\caption{\label{fig:Data_augmentation}
		Visualization of progressive augmentation.
		At level  (no augmentation) the discriminator  aims at classifying the samples  and  , respectively drawn from the data  and generative model  distributions, into true (green) and fake (blue). At single-level augmentation () the class of the augmented sample is set based on the combination  and  with , resulting in real and synthetic samples contained in both classes and leading to a harder task for .
		With each extra augmentation level () the decision boundary between two classes becomes more complex and the discrimination task difficulty gradually increases. This prevents the discriminator from easily solving the task and thus leads to meaningful gradients for the generator updates.}
		\vspace{-1em}
\end{figure}
 \section{\label{sec:Related}Related Work}	

Many recent works have focused on improving the stability of GAN training and the overall visual quality of generated samples \cite{Roth_NIPS2017,miyato2018spectral,Zhang_SAGAN18, Brock2019}.
The unstable behaviour of GANs is partly attributed to a dimensional mismatch or non-overlapping support between the real data and the generative model distributions \cite{Arjovsky2017TowardsPM}, 
resulting in an almost trivial task for the discriminator. Once the performance of the discriminator is maxed out, it provides a non-informative signal to train the generator. To avoid vanishing gradients, the original GAN paper \cite{goodfellow2014generative} proposed to modify the min-max based GAN objective to a non-saturating loss.
However, even with such a re-formulation the generator updates tend to get worse over the course of training and optimization becomes massively unstable \cite{Arjovsky2017TowardsPM}.

Prior approaches tried to mitigate this issue by using heuristics to weaken the discriminator, e.g. decreasing its learning rate, adding label noise or directly modifying the data samples.
\cite{SalimansNIPS2016} proposed a one-sided label smoothing to smoothen the classification boundary of the discriminator, thereby preventing it from being overly confident, but at the same time biasing its optimality. \cite{Arjovsky2017TowardsPM,Sonderby2016AmortisedMI} tried to ensure a joint support of the data and model distributions to make the job of the discriminator harder by adding Gaussian noise to both generated and real samples. However, adding high-dimensional noise introduces significant variance in the parameter estimation, slowing down the training and requiring multiple samples for counteraction \cite{Roth_NIPS2017}. Similarly, \cite{SajParMehSch18} proposed to blur the input samples and gradually remove the blurring effect during the course of training. These techniques perform direct modifications on the data samples.


Alternatively, several works focused on regularizing the discriminator.
\cite{gulrajani_NIPS2017} proposed to add a soft penalty on the gradient norm which ensures a 1-Lipschitz discriminator. Similarly, \cite{Roth_NIPS2017} added a zero-centered penalty on the weighted gradient-norm of the discriminator, showing its equivalence to adding input noise.
On the downside, regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training, and results in increased runtime due to additional gradient norm computation \cite{Kurach2018GANlandscape}. Most recently, \cite{Brock2019} also experimentally showed that the gradient penalty may lead to the performance degradation, which corresponds to our observations as well (see Sec. \ref{subsec:Exp-Regulariz}) 
In addition to the gradient penalty, \cite{Brock2019} also exploited the dropout regularization \cite{JMLR:v15:srivastava14a} on the final layer of the discriminator and reported its similar stabilizing effect.
\cite{miyato2018spectral} proposed another way to stabilize the discriminator by normalizing its weights and limiting the spectral norm of each layer to constrain the Lipschitz constant. This normalization technique does not require intensive tuning of hyper-parameters and is computationally light.
Moreover, \cite{Zhang_SAGAN18} showed that spectral normalization is also beneficial for the generator, preventing the escalation of parameter magnitudes and avoiding unusual gradients. 

Several methods have proposed to modify the GAN training methodology in order to further improve stability, e.g. by considering multiple discriminators \cite{Durugkar2016GenerativeMN}, growing both the generator and discriminator networks progressively \cite{karras2018progressive} or exploiting different learning rates for the discriminator and generator \cite{heuselttur2017}. Another line of work resorts to objective function reformulation, e.g. by using the Pearson  divergence~\cite{MaoLXLW16}, the Wasserstein distance \cite{Arjovsky2017WGAN}, or f-divergence \cite{Nowozin2016fGANTG}.

In this work we introduce a novel and orthogonal way of regularizing GANs by progressively increasing the discriminator task difficulty. In contrast to other techniques, our method does not bias the optimality of the discriminator or alter the training samples.
Furthermore, the proposed augmentation is complementary to prior work. It can be employed with different GAN architectures and combined with other regularization techniques (see Sec. \ref{sec:Experiments}).

 \section{Progressive Augmentation of GANs} \label{sec:PAGAN-all}
\subsection{Theoretical Framework of PA-GAN} \label{sec:PAGAN}
The core idea behind the GAN training~\cite{goodfellow2014generative} is to set up a competing game between two players, commonly termed discriminator and generator. The discriminator aims at distinguishing the samples  respectively drawn from the data distribution  and generative model distribution , i.e. performing binary classification . 
\footnote{ aims to learn the probability of  being true or fake, however, it can also be regarded as the sigmoid response of classification with cross entropy loss.} 
The aim of the generator, on the other hand, is to make synthetic samples into data samples, challenging the discriminator. In this work,  represents a compact metric space such as the image space  of dimension . Both  and  are defined on . The model distribution  is induced by a function  that maps a random vector  to a synthetic data sample, i.e. . Mathematically, the two-player game is formulated as

As being proved by~\cite{goodfellow2014generative}, the inner maximum equals the Jensen-Shannon (JS) divergence between  and , i.e., . Therefore, the GAN training attempts to minimize the JS divergence between the model and data distributions.



\begin{lma}\label{Mlma1}
Let  denote a random bit with uniform distribution , where  is the Kronecker delta. Associating  with , two joint distributions of  are constructed as 

Their JS divergence is equal to 
Taking (\ref{MPQ0}) as the starting point and with  being a sequence of i.i.d. random bits of length , the recursion of constructing the paired joint distributions of  

results into a series of JS divergence equalities for , i.e.,

\end{lma}


\begin{thm}\label{Mthm}
The min-max optimization problem of GANs~\cite{goodfellow2014generative} as given in (\ref{Mmin-max}) is equivalent to

where the two joint distributions, i.e.,  and , are defined in (\ref{MPQL}) and the function  maps  onto . For a fixed , the optimal  is

whereas the attained inner maximum equals  for .
\end{thm}

According to Theorem~\ref{Mthm}, solving (\ref{Mmin-max}) is interchangeable with solving (\ref{Mmin-max2}). In fact, the former can be regarded as a corner case of the latter by taking  as the absence of the auxiliary bit vector . As the length  of  increases, the input dimension of the discriminator grows accordingly. Furthermore, two classes to be classified consist of both the data and synthetic samples as illustrated in Fig.~\ref{fig:Data_augmentation}-(a).  Note that, the mixture strategy of the distributions of two independent random variables in Lemma~\ref{Mlma1} can be extended for any generic random variables (see Sec.~\ref{Ssub:lemma1_gen} in the supp. material). 

When solving (\ref{Mmin-max}),  and  are parameterized as deep neural networks and SGD (or its variants) is typically used for the optimization, updating their weights in an alternating or simultaneous manner, with no guarantees on global convergence. 
Theorem~\ref{Mthm} provides a series of JS divergence estimation proxies by means of the auxiliary bit vector  that in practice can be exploited as a regularizer to improve the GAN training (see Sec. \ref{sec_toy_example} for empirical evaluation). First, the number of possible combinations of the data samples with  grows exponentially with , thus helping to prevent the discriminator from overfitting to the training set. Second, the task of the discriminator gradually becomes harder with the length . The input dimensionality of  becomes larger and as the label of  is altered based on the new random bit  the decision boundary becomes more complicated (Fig.~\ref{fig:Data_augmentation}-b). Given that, progressively increasing  can be exploited during training to balance the game between the discriminator and generator whenever the former becomes too strong. Third, when the GAN training performance saturates at the current augmentation level, adding one random bit changes the landscape of the loss function and may further boost the learning.



 \subsection{Implementation of PA-GAN} \label{subsec:implement}

\begin{figure}
	\vspace{-1em}
	\begin{center}
		\includegraphics[width=.7\textwidth]{figures/fig2_v5_5.pdf}
	\end{center}
	\vspace{-0.5em}
	\caption{\label{fig:approach} PA-GAN overview. With each level of progressive augmentation  the dimensionality of  is enlarged from 1 to , . The task difficulty of the discriminator gradually increases as the length of  grows.}
	\vspace{-1em}
\end{figure}
The min-max problem in (\ref{Mmin-max2}) shares the same structure as the original one in (\ref{Mmin-max}), thus we can exploit the standard GAN training for PA-GAN, see Fig.~\ref{fig:approach}. The necessary change only concerns the discriminator. It involves 1) using checksum principle as a new classification criterion, 2) incorporating  in addition to  as the network input and 3) enabling the progression of  during training.


\textbf{Checksum principle.}
The conventional GAN discriminator assigns TRUE (0) / FAKE (1) class label based on  being either data or synthetic samples. In contrast, the discriminator  in (\ref{Mmin-max2}) requires  along with  to make the decision about the class label. Starting from , the two class distributions in (\ref{MPQ0}) imply the label- for ,  and label- for , . The real samples are no longer always in the TRUE class, and the synthetic samples are no longer always in the FAKE class, see Fig.~\ref{fig:Data_augmentation}-(a). To detect the correct class we can use a simple checksum principle.
Namely, let the data and synthetic samples respectively encode bit  and  followed by associating the checksum  of the pair  with TRUE(FAKE). \footnote{By checksum we mean the XOR operation over a bit sequence.} For more than one bit,  and  are recursively constructed according to (\ref{MPQL}). Based on the checksum principle for the single bit case, we can recursively show its consistency for any bit sequence length , . This is a desirable property for progression. With the identified checksum principle, we further discuss a way to integrate a sequence of random bits  into the discriminator network in a progressive manner.


\textbf{Progressive augmentation.}\label{subsec:network-implement}
With the aim of maximally reusing existing GAN architectures we propose two augmentation options. The first one is \emph{input space augmentation}, where  is directly concatenated with the sample  and both are fed as input to the discriminator network. The second option is \emph{feature space augmentation},  where  is concatenated with the learned feature representations of  attained at intermediate hidden layers. For both cases, the way to concatenate  with  or its feature maps is identical.
Each entry  creates one augmentation channel, which is replicated to match the spatial dimension of  or its feature maps. Depending on the augmentation space, either the input layer or the hidden layer that further processes the feature maps will additionally take care of the augmentation channels along with the original input. In both cases, the original layer configuration (kernel size, stride and padding type) remains the same except for its channel size being increased by . All the other layers of the discriminator remain unchanged. When a new augmentation level is reached, one extra input channel of the filter is instantiated to process the bit . 

These two ways of augmentation are beneficial as they make the checksum computation more challenging for the discriminator, i.e., making the discriminator unaware about the need of separating  and  from the concatenated input. 
We note that in order to take full advantage of the regularization effect of progressive augmentation,  needs to be involved in the decision making process of the discriminator either through input or feature space augmentation. Augmenting  with the output  makes the task trivial, thereby disabling the regularization effect of the progressive augmentation.
In this work we only exploit  by concatenating it with either the input or the hidden layers of the network. However, it is also possible to combine it with other image augmentation strategies, e.g. using  as an indicator for the rotation angle, as in \cite{ChenSS2019}, or the type of color augmentation that is imposed on the input  and encouraging  to learn the type through the checksum principle.

\textbf{Progression scheduling.} \label{subsec:PA-schedule}
To schedule the progression we rely on the kernel inception distance (KID) introduced by~\cite{Binkowski2016MMDGAN} to decide if the performance of  at the current augmentation level saturates or even starts degrading (typically happens when  starts overfitting or becomes too powerful). Specifically, after  discriminator iterations, we evaluate KID between synthetic samples and data samples drawn from the training set. If the current KID score is less than  of the average of the two previous evaluations attained at the same augmentation level, the augmentation is leveled up, i.e. . To validate the effectiveness of this scheduling mechanism we exploit it for the learning rate adaptation as in~\cite{Binkowski2016MMDGAN} and compare it with progressive augmentation in the next section. % \section{\label{sec:Experiments}Experiments}



\emph{\textbf{Datasets:}}  We consider four datasets: Fashion-MNIST~\cite{xiao2017}, CIFAR10~\cite{Cifar10_Krizhevsky09learningmultiple}, CELEBA-HQ ~\cite{karras2018progressive} and Tiny-ImageNet (a simplified version of ImageNet~\cite{imagenet_cvpr09}), with the training set sizes equal to \unit{k}, \unit{k}, \unit{k} and \unit{k} plus the test set sizes equal to \unit{k}, \unit{k}, \unit{k}, and \unit{k}, respectively. Note that we focus on unsupervised image generation and do not use class label information.

\emph{\textbf{Networks:}} 
We employ ~\cite{miyato2018spectral} and ~\cite{Zhang_SAGAN18}, both using spectral normalization (SN)~\cite{miyato2018spectral} in the discriminator for regularization.  exploits the ResNet architecture with a self-attention (SA) layer~\cite{Zhang_SAGAN18}. Its generator additionally adopts self-modulation BN (sBN)~\cite{chen2018on} together with SN. We exploit the implementations provided by~\cite{Kurach2018GANlandscape,Zhang_SAGAN18}. Following~\cite{miyato2018spectral,Zhang_SAGAN18}, we train  and ~\cite{Zhang_SAGAN18} with the non-saturation (NS) and hinge loss, respectively. 


\emph{\textbf{Evaluation metrics:}} 
We use Fr{\'e}chet inception distance (FID)~\cite{FID} as the main evaluation metric. Additionally, we also report inception score (IS)~\cite{Theis2016a} and kernel inception distance (KID)~\cite{Binkowski2016MMDGAN} in Sec.~\ref{kid-is}. All measures are computed based on the same number of the test data samples and synthetic samples, following the evaluation framework of~\cite{LucicEqualGANs,Kurach2018GANlandscape}. By default all reported numbers correspond to the median of five independent runs with \unit{k}, \unit{k}, \unit{k} and \unit{k} training iterations for Fashion-MNIST, CIFAR10, CELEBA-HQ, and Tiny-ImageNet, respectively. 

\emph{\textbf{Training details:}} 
We use uniformly distributed noise vector , the mini-batch size of , and Adam optimizer~\cite{adamopt}. The two time-scale update rule (TTUR)~\cite{heuselttur2017} is considered when choosing the learning rates for  and . For progression scheduling KID\footnote{FID is used as the primary metric, KID is chosen for scheduling to avoid over-optimizing towards FID.} is evaluated using samples from the training set every \unit{k} iterations, except for Tiny-ImageNet with \unit{k} given its approximately  larger training set. More details are provided in Sec.~\ref{sec:networks}. 



\subsection{PA Across Different Architectures and Datasets}\label{sec:PA-diff}

\begin{table}[t!]
	\vspace{-1em}
	\setlength{\tabcolsep}{0.2em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{FID improvement of  across different datasets and network architectures. 
		We experiment with augmenting the input and feature spaces, see Sec.\ref{sec:PA-diff} for details.} \label{table:fid_overview} \begin{tabular}{l|c|cccc|c} 
		\rowcolor{verylightgray}
		\footnotesize{}{\text{}}	& \footnotesize{}{\text{}}& 
		\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}}&\footnotesize{}{\text{}} \tabularnewline 
		

		\multirow{3}{*}{\text{~\cite{miyato2018spectral}}} & \footnotesize{}{\xmark}& \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize-} & \multirow{3}{*}{\text{\footnotesize}} \tabularnewline 	
		& \text{} & \text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize-} \tabularnewline 
		& \text{} & \text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize-} \tabularnewline 
		
		\arrayrulecolor{gray}	\hline \arrayrulecolor{verylightgray}
		
		\multirow{3}{*}{\text{~\cite{Zhang_SAGAN18}}}	& \footnotesize{}{\xmark}& \text{\footnotesize-} & \text{\footnotesize} & \footnotesize{}{} &\footnotesize{}{} &\multirow{3}{*}{\text{\footnotesize}}   \tabularnewline 
		& \text{} & \text{\footnotesize-}  & \text{\footnotesize} & \footnotesize{}{} & \text{\footnotesize}  \tabularnewline 
		& \text{} & \text{\footnotesize -}  & \text{\footnotesize} & \footnotesize{}{} & \text{\footnotesize} 
\end{tabular}	
	\vspace{-0.5em}
\end{table}


Table~\ref{table:fid_overview} gives an overview of the FID performance achieved with and without applying the proposed progressive augmentation () across different datasets and networks. We observe consistent improvement of the FID score achieved by  with both the input  and feature  space augmentation (see Sec.~\ref{Ssubsec:abl aug level} for augmentation details and ablation study on the augmentation space). From  to the ResNet-based  the FID reduction preserves approximately around  points, showing that the gain achieved by  is complementary to the improvement on the architecture side. In comparison to input space augmentation, augmenting intermediate level features does not overly simplify the discriminator task, paralysing . In the case of  on CELEBA-HQ, it actually outperforms the input space augmentation. Overall, a stable performance gain of , independent of the augmentation space choice, showcases high generalization quality of  and its easy adaptation into different network designs.\footnote{We also experiment with using PA for WGAN-GP~\cite{Arjovsky2017WGAN}, improving FID from  to  on CIFAR10, see Sec.~\ref{Ssubsec:dropout} in the supp. material.} 

Lower FID values achieved by  can be attributed mostly to the improved sample diversity. By looking at generated images in Fig.~\ref{fig:images} (and Fig.~\ref{Sfig:images} in the supp. material), we observe that  increases the variation of samples while maintaining the same image fidelity. This is expected as  being a regularizer does not modify the GAN architecture, as in PG-GAN~\cite{karras2018progressive} or BigGAN~\cite{Brock2019}, to directly improve the visual quality. Specifically, Fig.~\ref{fig:images} shows synthetic images produced by  and  with and without , on Fashion-MNIST and CELEBA-HQ. By polar interpolation between two samples  and , from left to right we observe the clothes/gender change.  improves sample variation, maintaining representative clothes/gender attributes and achieving smooth transition between samples (e.g. hair styles and facial expressions). For further evaluation, we also measure the diversity of generated samples with the MS-SSIM score \cite{odena17a}. We use \unit{k} synthetic images generated with  on CELEBA-HQ. Employing  reduces MS-SSIM from  to , while  PG-GAN~\cite{karras2018progressive} achieves , and MS-SSIM of \unit{k} real samples is . 



\begin{figure*}
	\captionsetup[subfigure]{labelformat=empty}
	\vspace{0em}
	\centering
	\begin{subfigure}{0.48\textwidth}
\includegraphics[width = \linewidth]{figures/fashion_sndcgan.png}
		\caption{\text{}:  without }
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
\includegraphics[width = \linewidth]{figures/fashion_pa_sndcgan.png} 
		\caption{\text{}:  with }
	\end{subfigure}	
	
	\begin{subfigure}{0.8\textwidth}
		\vspace{1.0em}
		\includegraphics[width = \linewidth]{figures/celebahq_sagan_b.png}\\  
		\includegraphics[width = \linewidth]{figures/celebahq_sagan_a.png} \caption{\text{}:  without }
	\end{subfigure}
\begin{subfigure}{0.8\textwidth}
		\includegraphics[width = \linewidth]{figures/celebahq_pa_sagan_b.png}\\
		\includegraphics[width = \linewidth]{figures/celebahq_pa_sagan_a.png} \caption{\text{}:  with }
	\end{subfigure}	
	
	\caption{Synthetic images generated through latent space interpolation with and without using .  helps to improve variation across interpolated samples, i.e., no close-by images looks alike.}	\label{fig:images}
	\vspace{-1.6em}
\end{figure*}

\textbf{Comparison with SotA on Human Face Synthesis.} Deviating from from low- to high-resolution human face synthesis, the recent work COCO-GAN~\cite{lin2019cocogan} outperformed PG-GAN~\cite{karras2018progressive} on the CELEBA dataset~\cite{Liu_Celeba} via conditional coordinating. At the resolution  of CELEBA,  improves the  FID from  to , being better than COCO-GAN, which achieves FID of  and outperforms PG-GAN at the resolution  (FID of  vs. ).
Thus we conclude that the quality of samples generated by  is comparable to the quality of samples generated by the recent state-of-the-art models~\cite{lin2019cocogan,karras2018progressive} on human face synthesis.

\textbf{Ablation Study.}
In Fig.~\ref{fig:fig_abl_pa_a} and Table~\ref{table:abla_pa_b} we present an ablation study on , comparing single-level augmentation (without progression) with progressive multi-level , showing the benefit of progression. From no augmentation to the first level augmentation, the required number of iterations varies over the datasets and architectures (\unit{k}\unit{k}). Generally the number of reached augmentation levels is less than . Fig.~\ref{fig:fig_abl_pa_a} also shows that single-level augmentation already improves the performance over the baseline . However, the standard deviation of its FIDs across five independent runs starts increasing at later iterations. By means of progression, we can counteract this instability, while reaching a better FID result. Table~\ref{table:abla_pa_b} further compares augmentation at different levels with and without continuing with progression. Both augmentation and progression are beneficial, while progression alleviates the need of case dependent tuning of the augmentation level.

As a generic mechanism to monitor the GAN training, progression scheduling is usable not only for augmentation level-up, but also for other hyperparameter adaptations over iterations. Analogous to \cite{Binkowski2016MMDGAN} here we test it for the learning rate adaptation. From Fig.~\ref{fig:fig_abl_pa_a}, progression scheduling shows its effectiveness in assisting both the learning rate adaptation and  for an improved FID performance.  outperforms learning rate adaptation, i.e. median FID  vs.  across five independent runs. 


\textbf{Regularization Effect of PA.}\label{sec_toy_example}
Fig.~\ref{fig:toy_d_g_fid_loss} depicts the discriminator loss ( loss) and the generator loss ( loss) behaviour as well as the FID curves over iterations. It shows that the discriminator of  very quickly becomes over-confident, providing a non-informative backpropagation signal to train the generator and thus leading to the increase of the  loss.  has a long lasting regularization effect on  by means of progression and helps to maintain a healthy competition between its discriminator and generator. 
Each rise of the  loss and drop of the  loss coincides with an iteration at which the augmentation level increases, and then gradually reduces after the discriminator timely adapts to the new bit.
Observing the behaviour of the  and  losses, we conclude that both  and  can effectively prevent the  discriminator from overfitting, alleviating
the vanishing gradient issue and thus enabling continuous learning of the generator.
At the level one augmentation, both  and  start from the similar overfitting stage, i.e.,  and  respectively at the iteration \unit{k} and \unit{k}. Combining the bit  directly with high-level features eases the checksum computation. As a result, the  loss of  reduces faster, but making its future task more difficult due to overfitting to the previous augmentation level. On the other hand,  let the bits pass through all layers, and thus its adaptation to augmentation progression improves over iterations. In the end, both  and  lead to similar regularization effect and result in the improved FID scores.


\begin{figure}[t!]
\centering
	\begin{minipage}{0.5\textwidth}
\centering
		\includegraphics[width=\linewidth]{figures/sndcgan_pa_over_iters.pdf}
		\caption{FID learning curves on  CIFAR10. The curves show the mean FID with one standard deviation across five random runs.}\label{fig:fig_abl_pa_a}
\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
\setlength{\tabcolsep}{0.36em} 
		\renewcommand{\arraystretch}{1.0}
\centering
		\captionof{table}{Median FIDs of input space augmentation starting from the level  with and without progression on CIFAR10 with .} \label{table:abla_pa_b}
\begin{tabular}{c|cc|c} 
			\rowcolor{verylightgray}
			\footnotesize{}{\text{}} & \multicolumn{2}{c}{\footnotesize{}{\text{}}}   & \tabularnewline 
			\rowcolor{verylightgray}
			\text{\footnotesize}   & 	\footnotesize{}{\text{\xmark }}	& \footnotesize{}{\text{\cmark }} & \multirow{-2}{*}{\footnotesize{}{\text{}}} \tabularnewline 
			\text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} \tabularnewline 
			\arrayrulecolor{verylightgray}	\hline 
			\text{\footnotesize}&\text{\footnotesize}	& \text{\footnotesize} & \text{\footnotesize} \tabularnewline 
			\arrayrulecolor{verylightgray}	\hline 
			\text{\footnotesize} &\text{\footnotesize} & \text{\footnotesize}&  \text{\footnotesize} \tabularnewline 
			\arrayrulecolor{verylightgray}	\hline 
			
			\text{\footnotesize} &\text{\footnotesize} & \text{\footnotesize}&  \text{\footnotesize} \tabularnewline 
			\arrayrulecolor{verylightgray}	\hline 
			
			\text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize} 		&	\text{\footnotesize}
		\end{tabular}			
	\end{minipage}	
\end{figure}


In Fig.~\ref{fig:toy_d_g_fid_loss} we also evaluate the ~\cite{JMLR:v15:srivastava14a} regularization applied on the fourth convolutional layer with the keep rate  (the best performing setting in our experiments). Both  and  resort to random variables for regularization. The former randomly removes features, while the latter augments them with additional random bits and adjusts accordingly the class label. In contrast to ,  has a stronger regularization effect and leads to faster convergence (more rapid reduction of FID scores). In addition, we compare  with the  baseline, where at each scheduled progression all weights are reinitialized with Xavier initialization \cite{GlorotAISTATS2010}. Compared to , using  strategy leads to longer adaptation time (the  loss decay is much slower) and oscillatory GAN behaviour, thus resulting in dramatic fluctuations of FID scores over iterations.

\begin{figure}
	\centering
	\vspace{0em}
		\begin{minipage}{\textwidth}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/d_g_loss_over_iterations.pdf}
		\caption{Discriminator () and generator () loss over iterations}\label{fig:fig_dg_loss}
	\end{subfigure}
	\end{minipage}
\begin{minipage}{.5\textwidth}
\begin{subfigure}{\textwidth}	
		\vspace{0.5em}
		\centering
		\includegraphics[width=\linewidth]{figures/d_g_loss_over_iterations_b.pdf}
		\caption{FID over iterations}\label{fig:fig_fid}
	\end{subfigure}
	\end{minipage}
	\hspace{.6em}
	\begin{minipage}{.45\textwidth}
\caption{\label{fig:toy_d_g_fid_loss} Behaviour of the discriminator loss ( loss) and the generator loss ( loss) as well as FID changes over iterations, using SN DCGAN on CIFAR10.  acts as a stochastic regularizer, preventing the discriminator from becoming overconfident.}
	\end{minipage}	
\end{figure}





\subsection{\label{subsec:Exp-Regulariz}Comparison and Combination with Other Regularizers}
We further compare and combine  with other regularization techniques, i.e., one-sided label smoothing~\cite{SalimansNIPS2016},  from~\cite{gulrajani_NIPS2017}, its zero-centered alternative  from~\cite{Roth_NIPS2017}, ~\cite{JMLR:v15:srivastava14a}, and self-supervised GAN training via auxiliary rotation loss ()~\cite{ChenSS2019}. 

One-sided label smoothing () weakens the discriminator by smoothing its decision boundary, i.e., changing the positive labels from one to a smaller value. This is analogous to introducing label noise for the data samples, whereas  alters the target labels based on the deterministic checksum principle. Benefiting from a smoothed decision boundary,  slightly improves the performance of  ( vs. ), but underperforms in comparison to  () and  (). By applying  on top of  we observe a similar reduction of the FID score ( and  for input and feature space augmentation, respectively).

Both  and  regularize the norms of gradients to stabilize the GAN training. The former aims at a 1-Lipschitz discriminator, and the latter is a closed-form approximation of adding input noise. Table~\ref{table:pa_gp_dropout} shows that both of them are compatible with  but degrade the performance of  alone and its combination with . This effect has been also observed in~\cite{Kurach2018GANlandscape,Brock2019}, constraining the learning of the discriminator improves the GAN training stability but at the cost of performance degradation. Note that, however, with  performance degradation is smaller.

 shares a common stochastic nature with  as illustrated in Fig.~\ref{fig:toy_d_g_fid_loss} and in the supp. material. We observe from Table~\ref{table:pa_gp_dropout} that  and  can be both exploited as effective regularizers.  acts locally on the layer. The layer outputs are randomly and independently subsampled, thinning the network. In contrast,  augments the input or the layer with extra channels containing random bits, these bits also change the class label of the input and thus alter the network decision process.  helps to break-up situations where the layer co-adapts to correct errors from prior layers and enables the network to timely re-learn features of constantly changing synthetic samples.  regularizes the decision process of , forcing  to comprehend the input together with the random bits for correct classification and has stronger regularization effect than , see Fig.~\ref{fig:toy_d_g_fid_loss} and the supp. material. Hence, they have different roles. Their combination further improves FID by  point on average, showing the complementarity of both approaches. It is worth noting that  is sensitive to the selection of the layer at which it is applied. In our experiments (see the supp. material) it performs best when applied at the fourth convolutional layer.

\begin{table}[t!]
\setlength{\tabcolsep}{0.2em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{FID performance of , different regularization techniques and their combinations on CIFAR10, see Sec.~\ref{subsec:Exp-Regulariz} for details.} \label{table:pa_gp_dropout}
	\begin{tabular}{l|c|c|ccccc|c} 
		\rowcolor{verylightgray}
		& & & \footnotesize{}{\text{-}}  &\footnotesize{}{\text{-}} & \footnotesize{}{\text{-}} & \footnotesize{}{\text{-}} &\footnotesize{}{\text{-}}  & \tabularnewline 
		\rowcolor{verylightgray}
		\multirow{-2}{*}{	\footnotesize{}{\text{}}} & \multirow{-2}{*}{\footnotesize{}{\text{}}} & \multirow{-2}{*}{\footnotesize{}{\text{}}} & 	\footnotesize{}{\text{\cite{SalimansNIPS2016}}} &  \footnotesize{}{\cite{gulrajani_NIPS2017}} & \footnotesize{}{ \cite{Roth_NIPS2017}} &
		\footnotesize{}{\cite{JMLR:v15:srivastava14a}} & \footnotesize{}{\cite{ChenSS2019}} &  \multirow{-2}{*}{\footnotesize{}{\text{}}} \tabularnewline  \multirow{2}{*}{\footnotesize{}{\text{~\cite{miyato2018spectral}}}} & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize}  & \text{\footnotesize} &  \text{\footnotesize}  &	\tabularnewline  
			& \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} \tabularnewline 
		& \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize}  &
	\footnotesize\tabularnewline 
		\arrayrulecolor{verylightgray}	\hline 
		\multirow{2}{*}{\footnotesize{}{\text{~\cite{Zhang_SAGAN18}}}}	 & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize} & \tabularnewline  
		& \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize}  & \text{\footnotesize}	\tabularnewline
		& \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} &\text{\footnotesize} &
		\footnotesize\tabularnewline 
		\arrayrulecolor{verylightgray}\hline 
		& \footnotesize{}{\text{}} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} &\text{\footnotesize}\tabularnewline 
	\end{tabular}	
	\vspace{-1em}
\end{table}

Self-supervised training (SS-GAN) in~\cite{ChenSS2019} regularizes the discriminator by encouraging it to solve an auxiliary image rotation prediction task. From the perspective of self-supervision,  presents the discriminator a checksum computation task, whereas telling apart the data and synthetic samples becomes a sub-task. Rotation prediction task was initially proposed and found useful in~\cite{Gidaris2018} to improve feature learning of convolutional networks. The checksum principle is derived from Theorem~\ref{thm}. Their combination is beneficial and achieves the best FID of  for the unsupervised setting on CIFAR10, which is the same score as in the supervised case with large scale BigGAN training~\cite{Brock2019}. 

Overall, we observe that  is consistently beneficial when combining with other regularization techniques, independent of input or feature space augmentation. Additional improvement of the FID score can come along with fine selection of the augmentation space type. 


 \section{\label{sec:Conclusion}Conclusion}
In this work we have proposed progressive augmentation (PA) -  a novel regularization method for GANs. Different to standard data augmentation our approach does not modify the training samples, instead it progressively augments them or their feature maps with auxiliary random bits and casts the discrimination task into the checksum computation. PA helps to entangle the discriminator and thus to avoid its early performance saturation. We experimentally have shown consistent performance improvements of employing PA-GAN across multiple benchmarks and demonstrated that PA generalizes well across different network architectures and is complementary to other regularization techniques. Apart from generative modelling, as a future work we are interested in exploiting PA for semi-supervised learning, generative latent modelling and transfer learning.
 


\bibliography{references} \bibliographystyle{../../iclr2019_template/iclr2019_conference}
\newpage
\begin{center}
	\setlength{\parindent}{0pt}
	\setlength{\parskip}{0pt}
\rule{\linewidth}{2pt} 
\\ [4pt]
	\textbf{\Large Supplemental Materials}: \
\mathbb{P}_{\mathrm{x},\mathrm{s}}(\bvec{\vx},s) \stackrel{\Delta}{=} \frac{\mathbb{P}_{\mathrm{d}}(\bvec{\vx})\delta[s]+\mathbb{P}_{\mathrm{g}}(\bvec{\vx})\delta[s-1]}{2},\label{def_Pxs}

\mathbb{P}_{\mathrm{m}}\stackrel{\Delta}{=}\mathbb{P}_{\mathrm{s}}(s=0)\mathbb{P}_{\mathrm{d}}+\mathbb{P}_{\mathrm{s}}(s=1)\mathbb{P}_{\mathrm{g}}= \frac{\mathbb{P}_{\mathrm{d}}+\mathbb{P}_{\mathrm{g}}}{2}.\label{Pm}
 
I(\bvec{\vx}; s)  &= \frac{E_{\mathbb{P}_{\mathrm{m}}}\left[ p_{\mathrm{d}}(\bvec{\vx})\log p_{\mathrm{d}}(\bvec{\vx})\right] + E_{\mathbb{P}_{\mathrm{m}}}\left[ p_{\mathrm{g}}(\bvec{\vx})\log p_{\mathrm{g}}(\bvec{\vx})\right]}{2}=D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right),\label{mi_js}

	\mathbb{P}_{\mathrm{x},\mathrm{s}}(\bvec{\vx},s) \stackrel{\Delta}{=} \frac{\mathbb{P}_{\mathrm{d}}(\bvec{\vx})\delta[s]+\mathbb{P}_{\mathrm{g}}(\bvec{\vx})\delta[s-1]}{2} ,\quad \mathbb{Q}_{\mathrm{x},\mathrm{s}}(\bvec{\vx},s) \stackrel{\Delta}{=} \frac{\mathbb{P}_{\mathrm{g}}(\bvec{\vx})\delta[s]+\mathbb{P}_{\mathrm{d}}(\bvec{\vx})\delta[s-1]}{2}.\label{PQ0}
	
	D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathrm{s}}\Vert\mathbb{Q}_{\mathrm{x},\mathrm{s}} \right)=D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right).\label{eqjoint}
	
	\begin{array}{ll}
	\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_{l})  \stackrel{\Delta}{=} {\mathbb{P}_{\mathrm{x},\mathbf{s}_{l-1}}(\bvec{\vx},\bvec{\vs}_{l-1})\delta[s_l]/2+\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l-1}}(\bvec{\vx},\bvec{\vs}_{l-1})\delta[s_l-1]/2}\\
	\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_{l}) \stackrel{\Delta}{=} {\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l-1}}(\bvec{\vx},\bvec{\vs}_{l-1})\delta[s_l]/2+\mathbb{P}_{\mathrm{x},\mathbf{s}_{l-1}}(\bvec{\vx},\bvec{\vs}_{l-1})\delta[s_l-1]/2}
	\end{array}\label{PQL}
	
	D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right)
	=D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathbf{s}_1}\Vert \mathbb{Q}_{\mathrm{x},\mathbf{s}_1} \right)=\cdots=
	D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathbf{s}_L}\Vert \mathbb{Q}_{\mathrm{x},\mathbf{s}_L} \right)
	\label{eqjoint2}.
	
\tilde{I}(\bvec{\vx}; s)=D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right).\label{app:Iq}

D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right) = \frac{I(\bvec{\vx}; s)+\tilde{I}(\bvec{\vx}; s)}{2}.

D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right) = \frac{D_{\mathrm{KL}}\left(\mathbb{P}_{\mathrm{x},\mathrm{s}}\Vert\mathbb{P}_{\mathrm{m}}\mathbb{P}_{\mathrm{s}} \right)+D_{\mathrm{KL}}\left(\mathbb{Q}_{\mathrm{x},\mathrm{s}}\Vert\mathbb{P}_{\mathrm{m}}\mathbb{P}_{\mathrm{s}}\right)}{2},\label{js_kl}

\mathbb{P}_{\mathrm{m}}(\bvec{\vx}) \mathbb{P}_{\mathrm{s}}(s) = \frac{\mathbb{P}_{\mathrm{x},\mathrm{s}}(\bvec{\vx},s)+\mathbb{Q}_{\mathrm{x},\mathrm{s}}(\bvec{\vx},s)}{2}

D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right)&=D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathrm{s}}\Vert\mathbb{Q}_{\mathrm{x},\mathrm{s}} \right)\label{eqjoint2}

D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{d}}\Vert\mathbb{P}_{\mathrm{g}} \right)=\cdots=D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathbf{s}_{l-1}}\Vert \mathbb{Q}_{\mathrm{x},\mathbf{s}_{l-1}} \right)=\cdots=D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathbf{s}_L}\Vert \mathbb{Q}_{\mathrm{x},\mathbf{s}_L} \right).\label{eqjoint3}

\min_G \max_{D} 
	\mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\log \left[D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}   + \mathbb{E}_{\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\log\left[1-D(\bvec{\vx},\bvec{\vs}_l)\right]\right\} \quad\forall l\in\{1,2,\dots,L\},\label{min-max2}
	
	D^*(\bvec{\vx},\bvec{\vs}_l)=\frac{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_l)}{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_l)+\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_l)}=\frac{\mathbb{P}_{\mathrm{d}}(\bvec{\vx})}{\mathbb{P}_{\mathrm{d}}(\bvec{\vx})+\mathbb{Q}_{\mathrm{d}}(\bvec{\vx})},
	
D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}\Vert \mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}} \right)=\max_D \mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_l}}\left\{\log \left[D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}+\mathbb{E}_{ \mathbb{Q}_{\mathrm{x},\mathbf{s}_l}}\left\{\log \left[1-D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}\quad \forall l,

D^*(\bvec{\vx},\bvec{\vs}_l)=\frac{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_l)}{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_l)+\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}(\bvec{\vx},\bvec{\vs}_l)}\stackrel{(a)}{=}\frac{\mathbb{P}_{\mathrm{d}}(\bvec{\vx})}{\mathbb{P}_{\mathrm{d}}(\bvec{\vx})+\mathbb{Q}_{\mathrm{d}}(\bvec{\vx})}.

&\min_G \max_D  \mathbb{E}_{\mathbb{P}_{\mathrm{d}}}\left\{\log \left[D(\bvec{\vx})\right]\right\}+\mathbb{E}_{ \mathbb{P}_{\mathrm{g}}}\left\{\log \left[1-D(\bvec{\vx})\right]\right\}\notag\\
&\quad\quad\quad\quad\equiv \min_G \max_D \mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_l}}\left\{\log \left[D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}+\mathbb{E}_{ \mathbb{Q}_{\mathrm{x},\mathbf{s}_l}}\left\{\log \left[1-D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}\quad \forall l.

\begin{array}{ll}
\mathbb{P}_{\mathrm{x},\mathrm{s}}(\bvec{\vx},\bvec{\vs}) =\frac{\mathbb{P}_{\mathrm{d}}(\bvec{\vx}) \mathbb{P}_{\mathrm{s},a}(\bvec{\vs})+\mathbb{P}_{\mathrm{g}}(\bvec{\vx}) \mathbb{P}_{\mathrm{s},b}(\bvec{\vs})}{2} \\
\mathbb{Q}_{\mathrm{x},\mathrm{s}}(\bvec{\vx},\bvec{\vs}) =\frac{\mathbb{P}_{\mathrm{d}}(\bvec{\vx}) \mathbb{P}_{\mathrm{s},b}(\bvec{\vs})+\mathbb{P}_{\mathrm{g}}(\bvec{\vx}) \mathbb{P}_{\mathrm{s},a}(\bvec{\vs})}{2}
\end{array}.\label{propPQ}

D_{\mathrm{JS}}\left(\mathbb{P}_{\mathrm{x},\mathrm{s}}\Vert\mathbb{Q}_{\mathrm{x},\mathrm{s}} \right) = \frac{I(\bvec{\vx}; \bvec{\vs})+\tilde{I}(\bvec{\vx}; \bvec{\vs})}{2}.

\mathrm{conv}(\phi(\bvec{\vx}),s_1,\dots,s_l)=\mathrm{conv}(\phi(\bvec{\vx}))+\sum_l\mathrm{conv}(s_l)\label{aug_conv}

\min_D -\mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\log D(\bvec{\vx},\bvec{\vs}_l)\right\}-\mathbb{E}_{\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\log \left[1-D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}.

\min_G -\mathbb{E}_{\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\log D(\bvec{\vx},\bvec{\vs}_l)\right\}-\mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\log \left[1-D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}.\label{ns-loss}

	\min_D \mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\max \left[0, 1-D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}+\mathbb{E}_{\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}}\left\{\max\left[0, 1+ D(\bvec{\vx},\bvec{\vs}_l)\right]\right\}.

\min_G \mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}}\left\{D(\bvec{\vx},\bvec{\vs}_l)\right\}-\mathbb{E}_{\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}}\left\{ D(\bvec{\vx},\bvec{\vs}_l)\right\}.

\min_G \max_D \mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}}\left\{D(\bvec{\vx},\bvec{\vs}_l)\right\}-\mathbb{E}_{\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}}\left\{ D(\bvec{\vx},\bvec{\vs}_l)\right\}.

&\mathbb{E}_{\mathbb{P}_{\mathrm{x},\mathbf{s}_{l}}}\left\{D(\bvec{\vx},\bvec{\vs}_l)\right\}-\mathbb{E}_{\mathbb{Q}_{\mathrm{x},\mathbf{s}_{l}}}\left\{ D(\bvec{\vx},\bvec{\vs}_l)\right\}\notag\\
&\quad  \approx L_{m}\stackrel{\Delta}{=}\frac{1}{\vert \mathcal{B}_{\mathrm{tr},m}\vert }\sum_{(\bvec{\vx},\bvec{\vs}_l)\in\mathcal{B}_{\mathrm{tr},m}} \hspace{-0.3cm} D(\bvec{\vx},\bvec{\vs}_l) -\frac{1}{\vert \mathcal{B}_{\mathrm{fk},m}\vert }\sum_{(\bvec{\vx},\bvec{\vs}_l)\in\mathcal{B}_{\mathrm{fk},m}} \hspace{-0.3cm} D(\bvec{\vx},\bvec{\vs}_l),\quad m = 1,\dots,M.

The critic  of WGAN-GP is trained to maximize the averaged loss  across the  mini-batches, making use of stochastic model averaging. The generator  is then trained to minimize the maximum of , , i.e. picking the best performing case of the critic, as a good quality of the critic  is important to the optimization process of  in the context of WGAN. With single bit augmentation of  and two draws per minibatch, we can improve WGAN-GP of  on CIFAR10 from  to  FID. 
Here, we boost the diversity of the two draws by choosing them with opposite checksums.

 
\section{Additional Ablation Studies}\label{Ssec:experiments}
In this section, we provide additional ablation studies of PA. Complementary to Table~1 in Sec. 4.1., an ablation study on the choice of augmentation space is conducted in Sec.~\ref{Ssubsec:abl aug level}, evaluating PA across input, low- and high-level feature space augmentation. One important finding in Sec.~4.2. of the main paper is that dropout and PA are complementary and mutually beneficial. In Sec.~\ref{Ssubsec:dropout}, we report our detailed investigation on the dropout regularization followed by evaluation of its combination with PA across the datasets and architectures. The two time-scale update rule (TTUR)~\cite{heuselttur2017}, updating the discriminator and generator with different learning rates, is notoriously helpful to stabilize GAN training. In Sec.~\ref{Ssubsec:ttur}, we examine the performance of PA under different TTURs and then compare it with the adaptive learning rate. 

\subsection{Ablation Study on Augmentation Space}\label{Ssubsec:abl aug level}




\begin{table*}[t!]
\setlength{\tabcolsep}{0.5em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{Median FIDs of input and feature space augmentation across five random runs. We experiment with augmenting input and features at different intermediate layers, e.g.  denotes layer with the spatial dimension , where  is the input image dimension.} \label{table_pa_auglevel}	
	\vspace{0.5em}
	\begin{tabular}{ll|ccccc} 
		\rowcolor{verylightgray}
		& & \multicolumn{5}{c}{\footnotesize{}{\text{}}} \tabularnewline 
		
		\rowcolor{verylightgray}
		
		\multirow{-2}{*}{\footnotesize{}{\text{}}} &	\multirow{-2}{*}{\footnotesize{}{\text{}}}	& \footnotesize{}{\text{\xmark}} &\footnotesize{}{\text{}}  & \footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}}\tabularnewline 
		
	 &  \multirow{1}{*}{\text{}}	& \text{\footnotesize} 
		&\text{\footnotesize} & \text{\footnotesize} &\textbf{\footnotesize} & \textbf{\footnotesize} \tabularnewline 
		
		\multirow{-2}{*}{\text{}} &  \multirow{1}{*}{\text{}}&\text{\footnotesize} 
		&\text{\footnotesize} &\text{\footnotesize} & \footnotesize{\text{}} & \textbf{\footnotesize} \tabularnewline
				\arrayrulecolor{verylightgray}	\hline
				
	 &  	\multirow{1}{*}{\text{}}	& \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize}& \textbf{\footnotesize} & \text{-}\tabularnewline 
		
		
\multirow{-2}{*}{\text{}}	 &  		\multirow{1}{*}{\text{}}	& \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize}& \textbf{\footnotesize} & \textbf{\footnotesize}  \tabularnewline 

\arrayrulecolor{verylightgray}	\hline
		
		
\end{tabular}	
\end{table*}


In the main paper, in Table 1 of Sec. 4.1. we reported the FID scores achieved by PA,
by augmenting either the input - , or its features with spatial dimension  - , where  is the input image dimension (see Sec.~\ref{sec:networks} for the detailed configuration). 
Here, we further perform the ablation study on the choice of the augmentation space across two datasets (CIFAR10 and CELEBA-HQ) and two architectures (SN DCGAN and SA GAN). From Table~\ref{table_pa_auglevel}, we observe the stable performance improvement across all configurations, inline with Table 1 of the main paper. The performance difference across different feature space augmentations is generally small (less than one FID point).



\begin{table}[t!]
	\vspace{-1em}
	\setlength{\tabcolsep}{0.22em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{Median FIDs (across five random runs) of  and  applied on the input layer or intermediate layers with different keep rates on CIFAR10 using SN DCGAN.} \label{table:dropout}\vspace{0.5em}
\begin{tabular}{c|cccc|cccc} 
	\rowcolor{verylightgray}
	&    \multicolumn{4}{c}{\footnotesize{}{\text{}}} &  \multicolumn{4}{c}{\footnotesize{}{\text{}}}  \tabularnewline 
		\rowcolor{verylightgray}
	\multirow{-2}{*}{\footnotesize{}{\text{}} } &\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}}&\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}} \tabularnewline 
	\footnotesize{}{\text{}} & \multicolumn{8}{c}{\text{\footnotesize}} \tabularnewline
	\arrayrulecolor{verylightgray}	\hline 
	\footnotesize{}{\text{}} & \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} & 
									 \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} \tabularnewline 
	\footnotesize{}{\text{}}  & \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} & 
	                                 \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} \tabularnewline 
	\footnotesize{}{\text{}} & \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &
	                                \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} \tabularnewline 
	\footnotesize{}{\text{}} & \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} & 
									\text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize}\tabularnewline 
	\footnotesize{}{\text{}} & \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} & 
	                                \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} \tabularnewline 
	\footnotesize{}{\text{}} & \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} & 
	                                 \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize} &  \text{\footnotesize}  \tabularnewline 
\end{tabular}
\end{table}



\subsection{Ablation Study on Dropout and its Combination with PA}\label{Ssubsec:dropout}
In Sec.~4.2. of the main paper, we have shown the effectiveness of using dropout, particularly, in combination with the proposed PA. In this part we report further ablations for both techniques.


We start from applying dropout at the input layer and different intermediate layers. Note that, in contrast to dropout, we apply PA directly on the input and not on the input layer. In addition, we experiment with different keep rates of the dropout, i.e. . Table~\ref{table:dropout} reports the FID scores achieved with different dropout configurations. In contrast to PA (see Table~\ref{table_pa_auglevel} or Table 1 in the main paper), the performance of dropout is very dependent on the applied layer and the selected keep rate. The feature space with the spatial dimension  together with the keep rate  is the best performing setting on CIFAR10 with SN DCGAN.

We further note that the binary dropout mask is independently drawn for each entry of the input or intermediate layer outputs (each convolution feature map activation is
"dropped-out" independently). In addition, we also experiment with the \emph{spatial} dropout () \cite{Tompson2015EfficientOL}, which randomly drops the entire feature maps instead of individual elements. The results in Tables~\ref{table:dropout} show that the entry-wise dropout outperforms the spatial dropout in the context of GAN training, i.e., FID  vs. . Therefore we only consider the entry-wise dropout for comparison with PA in the main paper.


In Table 3 of the main paper, we have successfully combined dropout at its best setting with PA on CIFAR10 with SN DCGAN and SA GAN. Table~\ref{table:pa_dropout} and~\ref{tab:dropoutpa} additionally report the FID improvements where dropout is applied at different intermediate layers and keep rates. In all configurations, PA provides complementary gains. Note that, for CELEBA-HQ  alone in Table~\ref{table:pa_dropout} only has a marginal performance improvement over the baseline, whereas its combination with  leads to larger performance boost. Overall, Table~\ref{table:pa_dropout},~\ref{tab:dropoutpa} plus Table~3 in the main paper confirms the effectiveness of exploiting both techniques. Adding PA is beneficial independent of the dropout settings (keep rate and applied layer), it helps to reduce the FID sensitivity to the dropout hyperparameter choice.  
\begin{table}[t!]
	\vspace{-0.5em}
	\setlength{\tabcolsep}{0.2em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{Median FIDs (across five random runs) of PA together with dropout applied on different intermediate layers with the keep rate  and on CIFAR10 and CELEBA-HQ.} \label{table:pa_dropout}
	\begin{tabular}{l|c|c|c|ccc|c} 
		\rowcolor{verylightgray}
		& & &  & \multicolumn{3}{c}{\footnotesize{}{\text{-}~\cite{JMLR:v15:srivastava14a}}} & \tabularnewline 
		\rowcolor{verylightgray}
		\multirow{-2}{*}{	\footnotesize{}{\text{}}} & 		\multirow{-2}{*}{	\footnotesize{}{\text{}}}& \multirow{-2}{*}{\footnotesize{}{\text{}}} & \multirow{-2}{*}{\footnotesize{}{\text{}}} & 	
		\footnotesize{}{\text{}} &
		\footnotesize{}{\text{}} &
		\footnotesize{}{\text{}}  &  \multirow{-2}{*}{\footnotesize{}{\text{}}} \tabularnewline  & & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}  	& \text{\footnotesize}\tabularnewline  
	\multirow{-2}{*}{	\footnotesize{}{\text{}}} 	& & \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} \tabularnewline 	 \arrayrulecolor{verylightgray}	 	 \cline{3-8}  \arrayrulecolor{verylightgray}
&	 & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} 	& \text{\footnotesize} \tabularnewline  
		\multirow{-2}{*}{	\footnotesize{}{\text{}}} &\multirow{-4}{*}{	\footnotesize{}{\text{}}}   & \footnotesize{}{\text{}} & \text{\footnotesize}  &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} \tabularnewline 
		\arrayrulecolor{verylightgray}\hline 
		& & 	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} \tabularnewline 
			\multirow{-2}{*}{	\footnotesize{}{\text{}}} &\multirow{-2}{*}{	\footnotesize{}{\text{}}}   & \footnotesize{}{\text{}} & \text{\footnotesize}  &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} \tabularnewline 
			\arrayrulecolor{verylightgray}\hline 
	&	& \footnotesize{}{\text{}} & \text{\footnotesize} &\text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}  &	\tabularnewline 
	\end{tabular}	
\end{table}

\begin{table}[t!]
	\centering
	\caption{Median FIDs (across five random runs) of PA together with dropout applied on different intermediate layers and keep rates on CIFAR10 with SN DCGAN.}\label{tab:dropoutpa}
	\begin{tabular}{c|ccccccccc} 
		\rowcolor{verylightgray}
		&\footnotesize{}{\text{\footnotesize}} &\multicolumn{2}{c}{\footnotesize{}{\text{\footnotesize}}}  & \multicolumn{2}{c}{\footnotesize{}{\text{\footnotesize}}} & \multicolumn{2}{c}{\footnotesize{}{\text{\footnotesize}}} & \multicolumn{2}{c}{\footnotesize{}{\text{\footnotesize}}} \tabularnewline 
		\rowcolor{verylightgray}
		\multirow{-2}{*}{}	&\footnotesize{}{\text{\footnotesize}} & \footnotesize{}{\text{\xmark }}	& \footnotesize{}{\text{\cmark }} & \footnotesize{}{\text{\xmark }}	& \footnotesize{}{\text{\cmark }} & \footnotesize{}{\text{\xmark }}	& \footnotesize{}{\text{\cmark }} & \footnotesize{}{\text{\xmark }}	& \footnotesize{}{\text{\cmark}} \tabularnewline 		
		\multirow{3}{*}{\text{\footnotesize}}	& \footnotesize{}{\text{}} & \text{\footnotesize} & \text{\footnotesize}  &  \text{\footnotesize} &\text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} \tabularnewline 
		& \footnotesize{}{\text{}} & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize} \tabularnewline 
		& \footnotesize{}{\text{}} & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} \tabularnewline 
		\rowcolor{verylightgray}
		\multicolumn{2}{c}{\footnotesize{}{\text{}}} &\multicolumn{2}{c}{\footnotesize{}{\text{}}}  & \multicolumn{2}{c}{\footnotesize{}{\text{}}} & \multicolumn{2}{c}{\footnotesize{}{\text{}}} & \multicolumn{2}{c}{\footnotesize{}{\text{}}}
	\end{tabular}
\end{table}



\begin{table}[t!]
\setlength{\tabcolsep}{0.22em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{Median FIDs (across five random runs) of different learning rates () on CIFAR10 with .
		Italic and bold denotes the best FIDs w/o and with PA respectively, underline denotes the default learning rate setting of .} \label{table:ttur} \vspace{0.5em}
	\begin{tabular}{c|c|cccc|c} 
		\rowcolor{verylightgray}
		\text{\backslashbox{}{}} & \footnotesize{}{\text{}} &  \footnotesize{}{ \text{\footnotesize}} & \footnotesize{}{ \text{\footnotesize}} & \footnotesize{}{ \text{\footnotesize}} & \footnotesize{}{ \text{\footnotesize}}  & \footnotesize{}{\text{}}\tabularnewline 
		
		\multirow{2}{*}{\text{\footnotesize}} &\footnotesize{}{\xmark} &\text{\footnotesize} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \multirow{2}{*}{\text{\footnotesize}}\tabularnewline 	
		&\footnotesize{}{\cmark} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} & \tabularnewline 
		\arrayrulecolor{gray}	\hline \arrayrulecolor{verylightgray}
		
		\multirow{2}{*}{\text{\footnotesize}} &\footnotesize{}{\xmark} &\text{\footnotesize} & \text{\footnotesize\underline{}}  & \text{\footnotesize} & \text{\footnotesize} & \multirow{2}{*}{\text{\footnotesize}} \tabularnewline 	
		&\footnotesize{}{\cmark} & \text{\footnotesize} & \text{\footnotesize\underline{}} & \text{\footnotesize}& \text{\footnotesize}& \tabularnewline 
		\arrayrulecolor{gray}	\hline \arrayrulecolor{verylightgray}
		
		\multirow{2}{*}{\text{\footnotesize}} &\footnotesize{}{\xmark} &\text{\footnotesize} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize}& \multirow{2}{*}{\text{\footnotesize}}\tabularnewline 	
		&\footnotesize{}{\cmark} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize}& \tabularnewline 
		\arrayrulecolor{gray}	\hline \arrayrulecolor{verylightgray}
		
		\multirow{2}{*}{\text{\footnotesize}} &\footnotesize{}{\xmark} &\text{\footnotesize} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &  \multirow{2}{*}{\text{\footnotesize}} \tabularnewline 	
		&\footnotesize{}{\cmark} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize}& \end{tabular}	
	\vspace{-0.7em}
\end{table}



\subsection{Ablation Study on Learning Rates}\label{Ssubsec:ttur}
Table~\ref{table:ttur} compares the performance achieved by using different learning rate configurations. The improvement achieved by  is consistent across different settings ( FID points), showing its robustness to different update rules. Compared to the best performing TTUR,  reduces the FID faster over iterations (see Figure~\ref{fig:scheduling}) without requiring extra hyperparameter search for the best update rule. 

Table~\ref{table:ttur} has also shown a stable FID performance of  with the generator learning rate  and the discriminator learning rate . With this identification, we fix  and reuse the progression scheduling to adaptively reduce  from  to  with the learning rate decay of  (in our experiments the best performing learning rate decay among ). 
Figure~\ref{fig:scheduling} shows the effectiveness of progression scheduling in assisting both the learning rate adaptation and progressive augmentation for an improved performance.  outperforms learning rate adaptation as well as the tuned ~\cite{heuselttur2017} , i.e. FID  vs.  vs. . Its combination with  delivers the best performance in this experiment, i.e., .


\begin{figure*}[t!]
\begin{center}
		\includegraphics[width=0.8\textwidth]{figures/supp_combined_fid_over_iterations.pdf}
	\end{center}
\caption{\label{fig:scheduling}FID learning curves (mean FIDs with one standard deviation across five random runs) of , , adaptive learning rate and  on CIFAR10 with .}
\end{figure*}



\section{Effectiveness of PA as a Regularizer}\label{sup_sec_toy_example}




Here we exploit progressive augmentation on a toy classification task to empirically illustrate its regularization benefits discussed in Sec.~3 of the main paper. Specifically, we focus on binary classification task taking the alike Cat and Dog images from CIFAR10~\cite{Cifar10_Krizhevsky09learningmultiple}, which represent the TRUE (real) and FAKE (synthetic) data samples, and train the discriminator network of SN DCGAN with the cross-entropy loss to tell them apart. Figure~\ref{fig:toy_d_loss} depicts the discriminator loss ( loss) behaviour over iterations on the training and test sets. It shows that the discriminator very quickly becomes over-confident on the training set and that overfitting takes place after \unit{k} iterations.


In order to regularize the discriminator we exploit the proposed progressive augmentation (), augmenting either the input - , or its features with spatial dimension  - , where  is the input image dimension. For a comparison purpose, we also experiment with the ~\cite{JMLR:v15:srivastava14a} regularization applied on  layer with the keep rate  (the best performing rate in our experiments). Both techniques resort to random variables for regularization. The former randomly removes features, while the latter augments them with additional random bits and adjusts accordingly the class label. 
In contrast to ,  exhibits a long lasting regularization effect by means of progression. Each rise of  loss coinciding with an iteration at which the augmentation level increases (every \unit{k} iterations) and then gradually reduces after the discriminator timely adapts to the new bit. At the level one augmentation, both  and  start from the similar overfitting stage. Combining the bit  directly with high-level features eases checksum computation. As a result, the  loss of  reduces faster, but making its future task more difficult due to overfitting to the previous augmentation level. On the other hand,   let the bits pass through all layers, and thus its adaptation to augmentation progression improves over iterations. In the end, both  and  lead to similar regularization effect. In addition, we compare  with the  baseline, where every \unit{k} iterations all weights are reinitialized with Xavier initialization \cite{GlorotAISTATS2010}. Compared to , using  strategy leads to longer adaptation time (the  loss decay is much slower), potentially providing non-informative signal to the generator and thus slowing down the training.


In Figure~\ref{fig:toy_hist} we explore the stochastic nature of  and . Each realization of the dropout mask or the augmentation bit sequence  changes the loss function landscape, varying its gradient with respect to the synthetic sample (i.e. the Dog class in this case). With the same experimental setup, we now assess the correlation of the gradients based on the first four eigenvalues of their correlation matrix - , , i.e. computing the averaged square roots of their ratios . Figure~\ref{fig:toy_hist} depicts the histograms of  among  instances.  has more instances with smaller  in comparison to , indicating a more diverse set of gradients, exploitable by the generator to approach the data distribution. In contrast to , in  the augmentation random bits determine the target class in binary classification and the discriminator is trained to comprehend  together with , leading to the richer loss function landscape. Between input and feature space augmentation, the former yields more diverse gradients than the latter as  is passed through all layers. 

\begin{figure}[t!]
\begin{center}
		\includegraphics[width=\textwidth]{figures/toy_example_d_loss_over_iterations_v2.pdf}
	\end{center}
	\vspace{-1em}
	\caption{\label{fig:toy_d_loss} Behaviour of the discriminator loss ( loss) with and w/o  and in comparison to , using the  architecture of SN DCGAN. See Sec.~\ref{sup_sec_toy_example} for details.} 
	\vspace{-1em}
\end{figure}
\begin{figure}[t!]\centering
	\includegraphics[width=.8\textwidth]{figures/toy_example_eigenratio_hist_v1.pdf}
\caption{Histograms of averaged square roots of eigenvalue ratios computed from gradient correlation matrices for  and . Smaller correlation values indicate a more diverse set of gradients exploitable by the generator to approach the data distribution. See Sec.~\ref{sup_sec_toy_example} for details.}
	\label{fig:toy_hist}
\end{figure} 
\section{Exemplar Synthetic Samples}\label{sec:syn samples}
Figure~\ref{Sfig:images} shows a set of synthetic samples that are outcomes of GAN training with and without .  not only improves sample quality and variation, but also sensibly navigates the image manifold through latent space interpolation.

\begin{figure*}	
	\centering
	\begin{subfigure}{\textwidth}
	\includegraphics[width = \linewidth]{figures/fig_images_fashion.pdf}
	\end{subfigure}	
	\begin{subfigure}{\textwidth}
	\includegraphics[width = \linewidth]{figures/fig_images_celebahq.pdf}
	\end{subfigure}	
	\caption{Synthetic samples from training SN GAN on Fashion-MNIST () and SA GAN (sBN) on CELEBA-HQ () with and without using PA. In all cases, i.e., (a), (b), (c) and (d), the eight images per row are generated through polar-interpolation between two randomly sampled  and .}\label{Sfig:images}
\end{figure*}

 \section{Evaluation with Other Performance Measures} \label{kid-is}
In addition to FID, here we measure the quality of synthetic samples by means of kernel inception distance (KID)~\cite{Binkowski2016MMDGAN} and inception score (IS)~\cite{Theis2016a}, see Tables~\ref{table:kid_is_one} and~\ref{table:kid_is_pa_gp_dropout} which correspond to Tables~1 and~3 in the main paper. The evaluation framework setup is the same as that with FID and follows~\cite{LucicEqualGANs,Kurach2018GANlandscape}. For Fashion-MNIST and CELEBA-HQ, IS computed from the pre-trained Inception network is not meaningful and thus omitted. Overall, the obtained results show consistent observations with those that are made in Sec.~4 of the main paper based on the FID measure.
\begin{table}[t!]
\setlength{\tabcolsep}{0.15em} 
	\renewcommand{\arraystretch}{1}
\centering
	\caption{KID/IS improvements with  across different datasets and network architectures, in accordance with Table~1 in the main paper.} \label{table:kid_is_one} \hspace{-1em}
	\begin{minipage}{0.6\textwidth}
		\caption*{KID}
		\begin{tabular}{l|c|cccc|c} 
		\rowcolor{verylightgray}
		\footnotesize{}{\text{}}	& \footnotesize{}{\text{}}& 
		\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \footnotesize{}{\text{}}&\footnotesize{}{\text{}} \tabularnewline 	
		
		\text{} & \footnotesize{}{\xmark}& \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize-} & \multirow{3}{*}{\text{\footnotesize}} \tabularnewline 	
		\text{} & \text{} & \text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize-} \tabularnewline 
		\footnotesize{}\text{\cite{miyato2018spectral}}  & \text{} & \text{\footnotesize} &  \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize-} \tabularnewline 
		
		\arrayrulecolor{gray}	\hline \arrayrulecolor{verylightgray}
		
		\multirow{1}{*}{\text{}}	& \footnotesize{}{\xmark}& \text{\footnotesize-} & \text{\footnotesize} & \footnotesize{}{} &\footnotesize{}{} &\multirow{3}{*}{\text{\footnotesize}}   \tabularnewline 
		\text{}  & \text{} & \text{\footnotesize-}  & \text{\footnotesize} & \footnotesize{}{} & \text{\footnotesize}  \tabularnewline 
		\footnotesize{}\text{\cite{Zhang_SAGAN18}} & \text{} & \text{\footnotesize -}  & \text{\footnotesize} & \footnotesize{}{} & \text{\footnotesize} 
\end{tabular}	
	\end{minipage}
\hspace{3.5em}
\begin{minipage}{0.3\textwidth}
	\caption*{IS}
	\begin{tabular}{||cc|c} 
	\rowcolor{verylightgray}
	 \footnotesize{}{\text{}} &  \footnotesize{}{\text{}}&\footnotesize{}{\text{}} \tabularnewline 	
	
 \text{\footnotesize} & \text{\footnotesize-} & \multirow{3}{*}{\text{\footnotesize}} \tabularnewline 	
 \text{\footnotesize} &  \text{\footnotesize-} \tabularnewline 
 \text{\footnotesize}  & \text{\footnotesize-} \tabularnewline 
	
	\arrayrulecolor{gray}	\hline \arrayrulecolor{verylightgray}
	
\text{\footnotesize} &\footnotesize{}{} &\multirow{3}{*}{\text{\footnotesize}}   \tabularnewline 
 \text{\footnotesize}  & \text{\footnotesize}  \tabularnewline 
 \text{\footnotesize}  & \text{\footnotesize} 
\end{tabular}	
\end{minipage}
\end{table}


\begin{table}[t!]
	\vspace{-1em}
	\setlength{\tabcolsep}{0.2em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{KIDs/ISs of PA, different regularization techniques and their combinations on CIFAR10, in according with Table 3 in the main paper.} \label{table:kid_is_pa_gp_dropout}
	\begin{minipage}{0.9\textwidth}
		\centering
		\caption*{KID}
	\begin{tabular}{l|c|c|ccccc|c} 		
	\rowcolor{verylightgray}
	& & & \footnotesize{}{\text{-}}  &\footnotesize{}{\text{-}} & \footnotesize{}{\text{-}} & \footnotesize{}{\text{-}} &\footnotesize{}{\text{-}}  & \tabularnewline 
	\rowcolor{verylightgray}
	\multirow{-2}{*}{	\footnotesize{}{\text{}}} & \multirow{-2}{*}{\footnotesize{}{\text{}}} & \multirow{-2}{*}{\footnotesize{}{\text{}}} & 	\footnotesize{}{\text{\cite{SalimansNIPS2016}}} &  \footnotesize{}{\cite{gulrajani_NIPS2017}} & \footnotesize{}{ \cite{Roth_NIPS2017}} &
	\footnotesize{}{\cite{JMLR:v15:srivastava14a}} & \footnotesize{}{\cite{ChenSS2019}} &  \multirow{-2}{*}{\footnotesize{}{\text{}}} \tabularnewline  \footnotesize{}{\text{}} & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize}  & \text{\footnotesize} &  \text{\footnotesize}  &	\tabularnewline  
	\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize}  &
	\multirow{-2}{*}{\footnotesize}\tabularnewline 
	\arrayrulecolor{verylightgray}	\hline 
	\footnotesize{}{\text{}}	 & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize} & \tabularnewline  
	\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} &\text{\footnotesize} &
	\multirow{-2}{*}{\footnotesize}\tabularnewline 
	\arrayrulecolor{verylightgray}\hline 
	& \footnotesize{}{\text{}} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} &\tabularnewline 
\end{tabular}	
	\end{minipage}
\\
	\begin{minipage}{0.9\textwidth}
		\centering
				\caption*{IS}
	\begin{tabular}{l|c|c|ccccc|c} 
		\rowcolor{verylightgray}
		& & & \footnotesize{}{\text{-}}  &\footnotesize{}{\text{-}} & \footnotesize{}{\text{-}} & \footnotesize{}{\text{-}} &\footnotesize{}{\text{-}}  & \tabularnewline 
		\rowcolor{verylightgray}
		\multirow{-2}{*}{	\footnotesize{}{\text{}}} & \multirow{-2}{*}{\footnotesize{}{\text{}}} & \multirow{-2}{*}{\footnotesize{}{\text{}}} & 	\footnotesize{}{\text{\cite{SalimansNIPS2016}}} &  \footnotesize{}{\cite{gulrajani_NIPS2017}} & \footnotesize{}{ \cite{Roth_NIPS2017}} &
		\footnotesize{}{\cite{JMLR:v15:srivastava14a}} & \footnotesize{}{\cite{ChenSS2019}} &  \multirow{-2}{*}{\footnotesize{}{\text{}}} \tabularnewline  \footnotesize{}{\text{}} & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize}  & \text{\footnotesize} &  \text{\footnotesize}  &	\tabularnewline  
		\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize}  &
		\multirow{-2}{*}{\footnotesize}\tabularnewline 
		\arrayrulecolor{verylightgray}	\hline 
		\footnotesize{}{\text{}}	 & 	   	\footnotesize{}{\text{\xmark }} & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize} & \text{\footnotesize} &\text{\footnotesize} & \tabularnewline  
		\footnotesize{}{\text{}} & \footnotesize{}{\text{}} & \text{\footnotesize}  & \text{\footnotesize}  & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} &\text{\footnotesize} &
		\multirow{-2}{*}{\footnotesize}\tabularnewline 
		\arrayrulecolor{verylightgray}\hline 
		& \footnotesize{}{\text{}} & \text{\footnotesize} & \text{\footnotesize}& \text{\footnotesize} & \text{\footnotesize} & \text{\footnotesize} &  \text{\footnotesize} &\tabularnewline 
	\end{tabular}	
\end{minipage}
\end{table}







 \section{Network Architectures and Hyperparameter Settings}\label{sec:networks}
In this work we exploit the implementation provided by~\cite{LucicEqualGANs, Kurach2018GANlandscape}\footnote{\url{https://github.com/google/compare_gan}} and~\cite{Zhang_SAGAN18}\footnote{\url{https://github.com/brain-research/self-attention-gan}}. For the experiments, we run on single GPU (Nvidia Titan X).
\subsection{Network Architectures}\label{subsec:netarch}



\paragraph{SN DCGAN.}\label{subsec:sndcgan}
Following~\cite{miyato2018spectral} for spectral normalization (SN), we adopt the same architecture as in~\cite{Kurach2018GANlandscape} and present its configuration in Table~\ref{tab_sndcgan}. The input and feature (i.e., ,  and ) space augmentations respectively take place at the input of the layers with the index , ,  and . In case of dropout, it is applied to the same intermediate layers plus the output of the layer . For Table~1 in the main paper, we pick the   for all evaluated datasets, whereas Sec.~\ref{Ssubsec:abl aug level} presents an ablation study on the augmentation space.

\paragraph{SA GAN (sBN).}\label{subsec:sagan}
The ResNet-based discriminator and generator architectures tailored for CIFAR10, CELEBA-HQ and T-ImageNet are presented in Table~\ref{tab_sagan} and~\ref{tab_sagantiny}, respectively. Taking the ResNet architecture in~\cite{gulrajani_NIPS2017} for CIFAR10, in~\cite{Kurach2018GANlandscape} for CELEBA-HQ and~\cite{Brock2019} for IMAGENET as the baseline, we adapt them by adding the SN and self-attention as proposed in~\cite{Zhang_SAGAN18}. For the residual and non-local blocks we use the implementation provided by~\cite{Zhang_SAGAN18}. As we target unsupervised GAN, the conditional batch normalization (BN) used by the generator's residual blocks only takes the input noise vector  as the conditioning, namely, self-modulation BN (sBN)~\cite{chen2018on}.

For CIFAR10, we have considered the input and feature (i.e.,  and ) space augmentations which respectively take place at the input of the residual blocks with the index ,  and , see Table~\ref{tab_sagan}-(a). Note that both residual blocks with the index  and  have their feature maps of dimension . We experiment with the feature space augmentation on both of them. They differ little in performance, thereby we only report the result of the feature space augmentation at the residual block  in Table 1 of the main paper.

For CELEBA-HQ, we empirically observe that it is beneficial to start from a convolutional layer rather than a residual block at the discriminator. Apart from input and  space augmentation reported in Table~1 of the main paper, we have also experimented the other feature space augmentations that take place at the input of each residual block, see Table~\ref{tab_sagancelebahq}. At the spatial dimension , we only report the result of input space augmentation, whereas the feature space augmentation at the first residual block delivers a similar performance. Augmenting the input of the last residual block benefits from the first warm-up mechanism presented in Sec.~\ref{sec:warm-up}, otherwise the discriminator can fail after augmentation progression.

For T-ImageNet, we have experimented with the augmentation space at both the input and  (at the input of the rd residual block) and reported their performance in Table~1 of the main paper. It is beneficial to use the second warm-up mechanism introduced in Sec.~\ref{sec:warm-up}. Comparing with the other datasets, the synthesis quality on T-ImageNet is still poor. Single GPU simulation with  samples per batch is not enough in this case. Large-scale simulation as in~\cite{Brock2019}, though demanding a large amount of resources, would be of interest.




\begin{table*}[t!]
	\centering
	\caption{SN DCGAN.\label{tab_sndcgan}}\hspace{-0.5cm}
	\begin{minipage}[t]{.45\textwidth}
	\centering	
	\subcaption{Discriminator}
\begin{tabular}{ll} 	
			\#	& \text{Configuration per Layer} \\ \hline
			0 &	\text{ stride  SN Conv, , lReLu} \\\hline
			1 &	\text{ stride  SN Conv, , lReLu} \\\hline
			2 &	\text{ stride  SN Conv, , lReLu} \\\hline
			3 &	\text{ stride  SN Conv, , lReLu} \\\hline
			4 &	\text{ stride  SN Conv, , lReLu} \\\hline
			5 &	\text{ stride  SN Conv, , lReLu} \\\hline
			6 &	\text{ stride  SN Conv, , lReLu} \\\hline
			7 &	\text{SN Linear  output}   \\
			\bottomrule
		\end{tabular}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[t]{.45\textwidth}	
			\centering
			\subcaption{Generator}
		\begin{tabular}{l} 	
			\text{Configuration per Layer} \\ \hline
			\text{Linear  output, BN, ReLU} \\\hline
			\text{ stride  DeConv, , BN, ReLU}  \\\hline
			\text{ stride  DeConv, , BN, ReLU}  \\\hline
			\text{ stride  DeConv, , BN, ReLU}  \\\hline
			\text{ stride  Deconv, , Tanh}  \\ 
			\bottomrule
		\end{tabular}
\end{minipage}	
\end{table*}


\begin{table*}[t!]
	\centering
	\caption{SA GAN for CIFAR10.\label{tab_sagan}}\hspace{-0.5cm}
	\begin{minipage}[t]{.45\textwidth}
		\centering	
		\subcaption{Discriminator}
		\begin{tabular}{ll} 	
			\#	&	\text{Configuration per Layer} \\ \hline
			0	&	\text{ResBlock, down,  } \\ \hline
			1	&	\text{Non-Local Block ()} \\ \hline
			2	&	\text{ResBlock, down,  }  \\\hline
			3	&	\text{ResBlock,  } \\ \hline
			4	&	\text{ResBlock,  }\\ \hline
			5	&	\text{ReLU, Global sum pooling} \\ \hline
			6	&	\text{SN Linear  output} \\
			\bottomrule
		\end{tabular}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[t]{.45\textwidth}	
		\centering
		\subcaption{Generator}
		\begin{tabular}{l} 	
			\text{Configuration per Layer} \\ \hline
			\text{SN Linear  output} \\\hline
			\text{ResBlock, up, }   \\\hline
			\text{ResBlock, up, }   \\\hline
			\text{Non-local Block ()}  \\\hline
			\text{ResBlock, up, }   \\\hline
			\text{BN, RELU}\\\hline
			\text{ stride  SN Conv. , Tanh}   \\
			\bottomrule
		\end{tabular}
	\end{minipage}	
\end{table*}

\begin{table*}[t!]
	\centering
	\caption{SA GAN for CELEBA-HQ.\label{tab_sagancelebahq}}\hspace{-0.5cm}
	\begin{minipage}[t]{.45\textwidth}
		\centering	
		\subcaption{Discriminator}
		\begin{tabular}{ll} 	
			\#	&	\text{Configuration per Layer} \\ \hline
			0	&	\text{ stride  SN Conv,  } \\ \hline
			1	&	\text{ResBlock, down,  } \\ \hline
			2	&	\text{ResBlock, down,  } \\ \hline
			3	&	\text{Non-Local Block ()} \\ \hline
			4	&	\text{ResBlock, down,  }  \\\hline
			5	&	\text{ResBlock, down,  }  \\\hline
			6	&	\text{ResBlock, down,  } \\ \hline
			8	&	\text{ReLU, Global sum pooling} \\ \hline
			9	&	\text{SN Linear 1 output} \\
			\bottomrule
		\end{tabular}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[t]{.45\textwidth}	
		\centering
		\subcaption{Generator}
	\begin{tabular}{l} 	
		\text{Configuration per Layer} \\ \hline
		\text{SN Linear  output} \\\hline
		\text{ResBlock, up, }   \\\hline
		\text{ResBlock, up, }   \\\hline
		\text{ResBlock, up, }   \\\hline
		\text{Non-local Block ()}  \\\hline
		\text{ResBlock, up, }   \\\hline				
		\text{ResBlock, up, }   \\\hline
		\text{BN, RELU}   \\\hline
		\text{ stride  SN Conv. , Tanh}   \\
		\bottomrule
	\end{tabular}
	\end{minipage}	
\end{table*}


\begin{table*}[t!]
	\centering
\caption{SA GAN for Tiny-IMAGENET.\label{tab_sagantiny}}\hspace{-0.5cm}
	\begin{minipage}[t]{.45\textwidth}
		\centering	
		\subcaption{Discriminator}
	\begin{tabular}{ll} 	
		\#	&	\text{Configuration per Layer} \\ \hline
		1	&	\text{ResBlock, down,  } \\ \hline
		2	&	\text{Non-Local Block ()} \\ \hline
		3	&	\text{ResBlock, down,  } \\ \hline		
		4	&	\text{ResBlock, down,  }  \\\hline
		5	&	\text{ResBlock, down,  }  \\\hline
		6	&	\text{ResBlock,  } \\ \hline
		8	&	\text{ReLU, Global sum pooling} \\ \hline
		9	&	\text{SN Linear 1 output} \\
		\bottomrule
	\end{tabular}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[t]{.45\textwidth}	
		\centering
		\subcaption{Generator}
		\begin{tabular}{l} 	
			\text{Configuration per Layer} \\ \hline
			\text{SN Linear  output} \\\hline
			\text{ResBlock, up, }   \\\hline
			\text{ResBlock, up, }   \\\hline
			\text{ResBlock, up, }   \\\hline
			\text{Non-local Block ()}  \\\hline
			\text{ResBlock, up, }   \\\hline		
			\text{BN, RELU}   \\\hline
			\text{ stride  SN Conv. , Tanh}   \\
			\bottomrule
		\end{tabular}
	\end{minipage}	
\end{table*}


\subsection{Network Training Details}\label{subsec:algsettings}
The training details across the datasets (i.e., F-MNIST, CIFAR10, CELEBA-HQ and T-ImageNet) and architectures (i.e., SN DCGAN, and SA GAN) are summarized in Table~\ref{table:hypersetting4all}. For both architectures, the decay rate of the (s)BNs at the generator is set to . During the evaluation phase, the generator uses the moving averaged mean and variance to produce synthetic samples, thereby being independent of batch size.


\subsection{Other Hyperparameter Settings}\label{subsec:regsettings}

\paragraph{Comparison with SotA on Human Face Synthesis.}
For CELEBA , we used the same network architecture as T-ImageNet. This network is not as tailored as PG-GAN~\cite{karras2018progressive} and COCO-GAN~\cite{lin2019cocogan} for human face synthesis. Unlike the other experiments, we followed the FID evaluation of COCO-GAN~\cite{lin2019cocogan} for the sake of fair comparison. The augmentation space is at  (the input of the th residual block). The hyperparameter setting for the  and  optimizers is: , , , ,  and \unit{m} training iterations.
\paragraph{Regularization Techniques in Table~3}
In Sec.~4 of the main paper, we have experimented with a diverse set of regularization techniques and reported the FIDs in Table~3. Their settings are as follows:

For , we followed the one-side label smoothing presented in~\cite{SalimansNIPS2016} smoothing the positive labels from  to  and leaving the negative ones to  in the binary classification task of the discriminator.

The  from~\cite{gulrajani_NIPS2017} and the zero-centered alternative  from~\cite{Roth_NIPS2017} are implemented by exploiting the publicly available code in \url{https://github.com/igul222/improved_wgan_training} and \url{https://github.com/rothk/Stabilizing_GANs}. The weighting parameter for  and  is respectively set to  and  as suggested by \cite{Kurach2018GANlandscape,Roth_NIPS2017}. 

When combining  with , we adjust its weighting factor whenever kicking off a new augmentation level, namely, gradually increasing the weighting factor from zero to its original value within \unit{k} iterations. This is mainly because the new bit can flip the reference label. Such relaxation on the 1-Lipschitz constraint allows the discriminator to timely cope with the new augmentation bit. Using  instead of  stabilizes the training on SA GAN.

For , we experimented with different keep rates and applied layers. From Table~\ref{table:dropout}, we selected the best performing setting of the  with the keep rate  applied on the feature space with the spatial dimension . 

For , we used the same mini-batch construction as in~\cite{ChenSS2019} for computing the auxiliary rotation loss. The rotation loss is respectively added to the  and  loss with the weighting factors equal to  and  as suggested by~\cite{ChenSS2019}. The augmentation bits does not affect the reference label when constructing the rotation loss.

\paragraph{WGAN-GP}
In Sec.~\ref{subsec:wgan}, we additionally trained CIFAR10 on SN DCGAN with WGAN-GP. The learning rates  and  remain the same as that of NS loss, i.e., , but with two discriminator steps per generator step. The two momentum parameters for the Adam optimizer change to  and . The GP is weighted by one.


\begin{table*}[t!]
\setlength{\tabcolsep}{0.4em} 
	\renewcommand{\arraystretch}{1.1}
\centering
	\caption{Training details for the experiments in this work.}		\vspace{0.5em}
	\label{table:hypersetting4all}
	\begin{tabular}{c|ccc|ccc} 
		\rowcolor{verylightgray}
		
		
		 &  \multicolumn{3}{c|}{\footnotesize{}{\text{}}}&  \multicolumn{3}{c}{\footnotesize{}{\text{}}}  \tabularnewline 	
		
		\rowcolor{verylightgray}
		
		\multirow{-2}{*}{\footnotesize{}{\text{}}} & \footnotesize{}{} & \footnotesize{}{} & \footnotesize{}{}& \footnotesize{}{} & \footnotesize{}{} & \footnotesize{}{}\tabularnewline 	
		\text{}  & \text{} & \text{} & \text{}  & \text{} & \text{} & \text{} \tabularnewline 
		\text{}  &  \text{} & \text{}  &  \text{} & \text{}  &  \text{} &  \text{} \tabularnewline                                                   
		\text{}  &  \text{} & \text{} & \text{}  & \text{} & \text{}& \text{}\tabularnewline          
		\text{}  &  \text{} & \text{} & \text{}& \text{} & \text{}& \text{}\tabularnewline    
		\text{} &  \text{} & \text{} & \text{}  & \text{} & \text{}	& \text{}
\end{tabular}	
	\vspace{-1em}
\end{table*}
 \end{document}
