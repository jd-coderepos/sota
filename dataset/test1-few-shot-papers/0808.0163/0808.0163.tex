\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{lem}{Lemma}
\newtheorem{thm}[lem]{Theorem}
\newtheorem{cor}[lem]{Corollary}

\newcommand{\iex}{\mathbf{i_{\textrm{ext}}}}
\renewcommand{\i}{\mathbf{i}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\cross}{+}
\newcommand{\im}{\mathrm{im}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathbf{Var}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Otilde}{\tilde{O}}
\newcommand{\onevec}{\mathbf{1}}
\newcommand{\R}{\mathbb{R}}






\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{fact}[theorem]{Fact}


\def\softO#1{\widetilde{{O}} \left( #1 \right)}
\def\bigO#1{{{O}}\left( #1 \right) }


\renewcommand{\i}{\mathbf{i}}
\renewcommand{\v}{\mathbf{v}}
\renewcommand{\u}{\mathbf{u}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\lmin}{\lambda_\mathrm{min}}
\newcommand{\lmax}{\lambda_\mathrm{max}}
\newcommand{\psic}{\psi_{\v}}
\newcommand{\deltaL}{\delta_{L}}
\newcommand{\deltaU}{\delta_{U}}
\newcommand{\epsL}{\epsilon_{L}}
\newcommand{\epsU}{\epsilon_{U}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\id}{\mathbf{id}}

\newcommand\ppi{\mathbf{v}}


\def\sp#1{\mathrm{sp} (#1)}

\def\calL{\mathcal{L}}
\def\calG{\mathcal{G}}



\def\trace#1{\mathrm{Tr} \left(#1 \right)}


\def\calC{\mathcal{C}}



\def\calS{\mathcal{S}}
\def\intersect{\cap}
\def\form#1#2{\left\langle #1, #2\right\rangle}



\def\union{\cup}

\def\floor#1{\left\lfloor #1 \right\rfloor}
\def\ceil#1{\left\lceil #1 \right\rceil}
\def\vol#1{\mathrm{vol} (#1)}
\def\bdry#1{\partial (#1)}



\def\abs#1{\left|#1  \right|}

\def\norm#1{\left\| #1 \right\|}

\def\vs#1#2#3{#1_{#2},\ldots , #1_{#3}}

\def\aa{\pmb{\mathit{a}}}
\newcommand\bb{\boldsymbol{\mathit{b}}}
\newcommand\cc{\boldsymbol{\mathit{c}}}
\newcommand\CC{\boldsymbol{\mathit{C}}}
\newcommand\ee{\boldsymbol{\mathit{e}}}
\newcommand\ff{\boldsymbol{\mathit{f}}}
\renewcommand\gg{\boldsymbol{\mathit{g}}}
\renewcommand\ll{\boldsymbol{\mathit{l}}}
\newcommand\KK{\boldsymbol{\mathit{K}}}
\newcommand\pp{\boldsymbol{\mathit{p}}}
\newcommand\qq{\boldsymbol{\mathit{q}}}
\newcommand\bs{\boldsymbol{\mathit{s}}}
\newcommand\nn{\boldsymbol{\mathit{n}}}
\renewcommand\SS{\boldsymbol{\mathit{S}}}
\newcommand\uu{\boldsymbol{\mathit{u}}}
\newcommand\vv{\boldsymbol{\mathit{v}}}
\newcommand\ww{\boldsymbol{\mathit{w}}}
\newcommand\xx{\boldsymbol{\mathit{x}}}
\newcommand\xxs{\boldsymbol{\mathit{x}}^{*}}
\newcommand\xp{\boldsymbol{\mathit{x'}}}
\newcommand\yy{\boldsymbol{\mathit{y}}}
\newcommand\yys{\boldsymbol{\mathit{y}}^{*}}
\newcommand\zz{\boldsymbol{\mathit{z}}}
\def\tt{\boldsymbol{\mathit{t}}}


\def\union{\cup}


\newdimen\pIR
\pIR= -131072sp
\newcommand\StevesR{{\rm I\kern\pIR R}}
\def\Reals#1{\StevesR^{#1}}

\def\pleq{\preccurlyeq}
\def\pgeq{\succcurlyeq}

\def\Span#1{\textbf{Span}\left(#1  \right)}
\def\bvec#1{{\mbox{\boldmath }}}



\def\prob#1#2{\mbox{P}^{#1}\left[ #2 \right]}
\def\pvec#1#2{\vec{\mbox{P}}^{#1}\left[ #2 \right]}
\def\expec#1#2{\mbox{\bf E}_{#1}\left[ #2 \right]}

\def\defeq{\stackrel{\mathrm{def}}{=}}
\def\setof#1{\left\{#1  \right\}}
\def\mand{\mbox{ and }}
\def\mor{\mbox{ or }}


\def\sizeof#1{\left|#1  \right|}
 
\begin{document}


\title{Twice-Ramanujan Sparsifiers
\thanks{
This material is based upon work supported by the National Science Foundation under Grant CCF-0634957.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
}\\} 
 
\author{
Joshua Batson
\thanks{
Department of Mathematics, MIT. Work on this paper performed while at
Yale College.}\\
\and
Daniel A. Spielman
\thanks{
Program in Applied Mathematics and 
Department of Computer Science,
Yale University.}\\
\and
Nikhil Srivastava
\thanks{
Department of Computer Science,
Yale University.}
}

\maketitle
\begin{abstract}
We prove that every graph has a spectral sparsifier with a number
  of edges linear in its number of vertices.
As linear-sized spectral sparsifiers of complete graphs are expanders,
  our sparsifiers of arbitrary graphs 
  can be viewed as generalizations of
  expander graphs.

In particular, we prove that for every  and every
  undirected, weighted graph  on  vertices, there exists
  a weighted graph  with at most  edges
  such that for every ,

where  and  are the Laplacian matrices of 
  and , respectively. 
Thus, 
  approximates  spectrally at least as well as a Ramanujan expander
  with  edges approximates the complete graph.

We give an elementary deterministic polynomial time algorithm for
  constructing .
\end{abstract}

\section{Introduction}\label{sec:intro}
A sparsifier of a graph  is a sparse graph  that is similar to
   in some useful way.
Many notions of similarity have been considered.
For example, Chew's~\cite{PaulChew} spanners have the property that the distance
  between every pair of vertices in  is approximately the same as in .
Benczur and Karger's~\cite{BenczurKarger} cut-sparsifiers have the property
  that the weight of the boundary of every set of vertices is approximately
  the same in  as in .
We consider the spectral notion of similarity introduced
  by Spielman and Teng~\cite{SpielmanTengPrecon,SpielmanTengSparsifier}:
  we say that  is a -approximation of  if for all
  ,

where  and  are the Laplacian matrices of  and .
We recall that

where  is the weight of edge  in .
By considering vectors  that are the characteristic vectors of sets,
  one can see that condition~\eqref{eqn:approximation}
  is strictly stronger than the cut
  condition of Benczur and Karger.

In the case where  is the complete graph, excellent spectral
  sparsifiers are supplied
  by \textit{Ramanujan Graphs} \cite{LPS,Margulis}.
These are -regular graphs  
  all of whose non-zero Laplacian eigenvalues lie between
   and .
Thus, if we take a Ramanujan graph on  vertices and
  multiply the weight of every edge by
  , we obtain a graph
  that
   -approximates
  the complete graph, for 

  
In this paper, we prove that
  every graph can be approximated at least this 
  well\footnote{
Strictly speaking, our approximation constant is only better than the Ramanujan
  bound  in the regime .
  This includes the actual Ramanujan graphs, for which  is an integer greater
  than .}
     by a graph with only twice as many edges 
  as the Ramanujan graph
  (as a -regular graph has  edges).
\begin{theorem}\label{thm:mainthm}
For every , 
  every undirected weighted graph  on  vertices
  contains a weighted subgraph  with 
  edges (i.e., average degree at most ) that satisfies:

\end{theorem}
Our proof provides
  a deterministic greedy algorithm for computing the graph  in 
  time   .

We remark that while the edges of  are a subset of the edges of ,
  the weights of edges in  and  will typically be different.
In fact, there exist unweighted graphs  for which every good spectral 
  sparsifier  must contain edges of 
  widely varying weights~\cite{SpielmanTengSparsifier}.

\subsection{Expanders: Sparsifiers of the Complete Graph}
In the case that  is a complete graph, 
  our construction produces expanders.
However, these expanders are slightly unusual in that their edges have weights,
  they may be irregular, and the weighted degrees of vertices can vary slightly.
This may lead one to ask whether they should really be considered expanders.
In Section~\ref{sec:expanders} we argue that they should be.

As the graphs we produce are irregular and weighted, it is also not immediately
  clear that we should be comparing  with the Ramanujan bound of

It is known\footnote{While lower bounds on the spectral gap of -regular graphs
  focus on showing that the second-smallest eigenvalue is asymptotically
  at most
  , the same proofs by test functions
  can be used to show that the
  largest eigenvalue is at asymptotically least .}
 that no -regular graph of uniform weight
  can -approximate a complete graph for  
  asymptotically better than 
  \eqref{eqn:ramanujanBound}~\cite{Nilli}.
While we believe that no graph of {\em average} degree  can be a
  -approximation of a complete graph for  asymptotically
  better than \eqref{eqn:ramanujanBound}, we are unable to show this at the
  moment and prove
  instead the weaker claim that no such graph can achieve  
  less than



\subsection{Prior Work}\label{sec:prior}
Spielman and Teng~\cite{SpielmanTengPrecon,SpielmanTengSparsifier} 
  introduced the notion of
  sparsification that we consider, and proved that -approxim\-ations
  with  edges could be constructed in  time.
They used these sparsifiers to obtain a nearly-linear time algorithm
  for solving diagonally dominant systems of linear 
  equations~\cite{SpielmanTengPrecon,SpielmanTengLinsolve}.

Spielman and Teng were inspired by the notion of sparsification introduced by Benczur
  and Karger~\cite{BenczurKarger} for cut problems, which only
  required 
  inequality \eqref{eqn:approximation} to hold for all .
Benczur and Karger showed how to construct graphs  meeting this guarantee
  with   edges in  time;
 their cut sparsifiers have been used to obtain faster algorithms
  for cut problems~\cite{BenczurKarger,krv}.


Spielman and Srivastava~\cite{SpielmanSrivastava} proved the
  existence of spectral sparsifiers with 
  edges, and showed how to construct them in  time.
They conjectured that it should be possible to find such sparsifiers with only
   edges.
We affirmatively resolve this conjecture.

Recently, partial progress was made towards this conjecture
  by Goyal, Rademacher and Vempala~\cite{GoyalRademacherVempala},
  who showed how to find graphs  with only  edges that 
  -approximate bounded degree graphs
   under the cut notion of Benczur and Karger.

We remark that all of these constructions were randomized.
Ours is the first deterministic algorithm to achieve the guarantees of any
  of these papers.


\section{Preliminaries}
\subsection{The Incidence Matrix and the Laplacian} \label{sec:incidence}
Let  be a connected weighted undirected graph with  vertices and 
edges and edge weights . If we orient the edges of 
arbitrarily, we can write its Laplacian as , where  is the
{\em signed edge-vertex incidence matrix}, given by

and  is the diagonal matrix with . 
It is immediate that  is positive semidefinite since:

\noindent and that  is connected if and only if
 .



\subsection{The Pseudoinverse}\label{sec:pseudo}
Since  is symmetric we can diagonalize it and write

where  are the nonzero eigenvalues of  and
 are a corresponding set of orthonormal eigenvectors. The {\em
Moore-Penrose Pseudoinverse} of  is then defined as

Notice that  and that

which is simply the projection onto the span of the
nonzero eigenvectors of  (which are also the eigenvectors of ). Thus,  is the identity on
.

\subsection{Formulas for Rank-one Updates}
We use the following well-known theorem from linear algebra, which describes
the behavior of the inverse of a matrix under rank-one updates 
  (see~\cite[Section 2.1.3]{GolubVanLoan}).
\begin{lemma}[Sherman-Morrison Formula] If  is a nonsingular  matrix and  is a vector, then 

\end{lemma}
There is a related formula describing the change in the {\em determinant} of
  a matrix under the same update:
\begin{lemma}[Matrix Determinant Lemma] \label{lem:matrixdet} If  is nonsingular and  is a vector, then
\end{lemma}

\section{The Main Result}
At the heart of this work is the following purely linear algebraic theorem.
We use the notation  to mean that  is positive
  semidefinite, and  to denote the identity operator on a vector space .
\begin{theorem}\label{thm:linalg}
Suppose  and  are vectors in  with

Then there exist scalars  with  so that

\end{theorem}
\noindent The sparsification result for graphs follows quickly from this theorem as shown below.
\begin{proof}[Proof of Theorem \ref{thm:mainthm}]
Assume without loss of generality that  is connected. Write  as in Section \ref{sec:incidence} and fix . 
Restrict attention to  and apply Theorem \ref{thm:linalg} to the columns  of 
 
which are indexed by the edges of  and satisfy

Write the scalars  guaranteed by the theorem in the  diagonal
matrix  and set . Then  is the Laplacian of
the subgraph  of  with edge weights , and  has at most 
edges since at most that many of the  are nonzero. Also,


By the Courant-Fischer Theorem, this is equivalent to:

as desired.
\end{proof}

It is worth mentioning that the above reduction is essentially the same as
  the one in \cite{SpielmanSrivastava}. 
In that paper, the authors consider the
  symmetric projection matrix  whose columns 
  correspond to the edges of .
They show, by a concentration lemma of Rudelson~\cite{rudl},
  that randomly sampling  of the columns with probabilities proportional to
   (where  is the effective
  resistance)
  gives a matrix  that approximates  in the
  spectral norm and corresponds to a graph sparsifier, with high probability.
In this paper, we do essentially the same thing with two modifications:
  we eliminate  in order to simplify notation, since we are no longer
  following the intuition of sampling by effective resistances; and, instead of
  Rudelson's 
  sampling lemma, we use Theorem \ref{thm:linalg} to {\em deterministically}
  select  edges (equivalently, columns of ).

The rest of this section is devoted to proving Theorem \ref{thm:linalg}.
The proof is constructive and yields a deterministic polynomial time algorithm for finding the
  scalars , which can then be used to sparsify graphs, as advertised.

Given vectors , our goal is to choose a small set of coefficients  so that 
   is well-conditioned. 
We will build the matrix  in steps, starting with  and adding one vector
   at a time. 
Before beginning the proof, it will be instructive to study 
  how the eigenvalues and characteristic polynomial of a matrix evolve upon the addition of a vector.
This discussion should provide some intuition for the structure of the proof, and demystify 
  the origin of the `Twice-Ramanujan' number  which 
  appears in our final result.
\subsection{Intuition for the Proof}
It is well known that the eigenvalues of  interlace those of .
In fact, the new eigenvalues can be determined exactly by looking at the 
  characteristic polynomial of , which is computed using 
  Lemma \ref{lem:matrixdet} as follows:

where  are the eigenvalues of  and  are the corresponding eigenvectors.
The polynomial  has two kinds of zeros :
\begin{enumerate}
\item Those for which . These are equal to the eigenvalues
 of  for
  which the added vector  is orthogonal to the corresponding eigenvector
  , and which do not therefore `move' upon adding .
\item Those for which  and 

  These are the eigenvalues which have moved and strictly interlace the old
    eigenvalues. 
  The above equation immediately suggests a simple physical model which gives
    intuition as to where these new eigenvalues are located. 
  \begin{figure}[htp]\label{fig:phys}
  \centering
  \includegraphics[scale=.65]{physm2.pdf}
  \caption{Physical model of interlacing eigenvalues.}
  \end{figure}

  \noindent \textbf{Physical Model.} We interpret the eigenvalues  as charged particles lying on a slope.
  On the slope are  fixed, chargeless barriers located at the initial
    eigenvalues , and each particle is resting against one of the
    barriers under the influence of gravity.
  Adding the vector  corresponds to placing a charge of  on the barrier corresponding to .
  The charges on the barriers repel those on the eigenvalues with a force
    that is proportional to the charge on the barrier and inversely proportional
    to the distance from the barrier --- i.e., the force from barrier  is given
    by 
    a quantity which is positive for  `below' , which are
    pushing the partical `upward', and negative otherwise.
  The eigenvalues move up the slope until they reach an equilibrium in which
    the repulsive forces from the barriers cancel the effect of gravity, which
    we take to be a  in the downward direction.
  Thus the equilibrium condition corresponds exactly to having the total
  `downward pull'  equal to zero.
\end{enumerate}

With this physical model in mind, we begin to consider what happens to the
  eigenvalues of  when we add a {\em random} vector from our set .
The first observation is that for any eigenvector  (in fact for any vector
  at all), the expected projection of a randomly chosen 
  is 

Of course, this does not mean that there is any single vector  in
  our set that realizes this `expected behavior' of equal projections on the
  eigenvectors.
But if we were to add such a vector
\footnote{For concreteness, we remark that this `average' vector would be
  precisely
}
 in our physical model, we would add equal 
  charges of  to each of the barriers, and we would expect all of the
  eigenvalues of  to drift forward `steadily'.
In fact, one might expect that after sufficiently many iterations of this
  process, the eigenvalues would all march forward together, with no eigenvalue
  too far ahead or too far behind, and we would 
  end up in a position where  is bounded.

In fact, this intuition turns out to be correct. 
Adding a vector with equal projections changes the characteristic polynomial 
  in the following manner:

since . 
If we start with , which has characteristic polynomial ,  
  then after  iterations of this process we obtain the polynomial
  
where  is the derivative with respect to .
Fortunately, iterating the operator  for any

  generates a standard family of orthogonal polynomials -- the {\em associated
  Laguerre polynomials} \cite{dette}. 
These polynomials are very well-studied and the locations of their zeros are
  known; in particular, after  iterations the ratio of the largest to the
  smallest zero is known \cite{dette} to be
  
  which is exactly what we want.

To prove the theorem, we will show that we can choose a sequence of actual vectors
  that realizes the expected behavior (i.e. the behavior of repeatedly adding ),
  as long as we are allowed to add 
  arbitrary fractional amounts of the  via the weights .
We will control the eigenvalues of our matrix by maintaining two barriers as 
  in the physical model, and keeping the eigenvalues between them. 
The lower barrier will `repel' the eigenvalues forward; the upper one will make sure
  they do not go too far. 
The barriers will move forward at a steady pace.
By maintaining that the total `repulsion' at every step of this process is
  bounded, we will be able to guarantee that there is always some multiple of 
  a vector to add that allows us to continue the process.

\subsection{Proof by Barrier Functions}
We begin by defining two `barrier' potential
  functions which measure the quality of the eigenvalues of a matrix. 
These potential functions are inspired by the inverse law 
  of repulsion in the physical model discussed in the last section.
\begin{definition}
For  and  a symmetric matrix with eigenvalues
, define:
 

\end{definition}

As long as  and  (i.e.,  and ), these potential functions
  measure how far the eigenvalues of  are from the barriers  and . 
In particular, they blow up as any eigenvalue approaches a
  barrier, since then  (or ) approaches a singular matrix.
Their strength lies in that they reflect the locations of all the
  eigenvalues simultaneously: for instance, 
   implies that
  no  is within distance one of , no 
  's are at distance , no  are at distance , and so on.
In terms of the physical model, the upper potential  is equal to the total repulsion of the eigenvalues of
   from the upper barrier , while  is the analogous quantity for the lower barrier.

To prove the theorem, we will build the sum  iteratively, adding one vector at a time.
Specifically, we will construct a sequence of matrices 

along with positive constants\footnote{On first reading the paper, we suggest the reader follow the proof with
  the assignment , ,
  , , .
  This will provide the bound , and eliminates
  the need to use Claim~\ref{clm:lowercauchy}.
}
  and  which satisfy the following conditions:
\begin{enumerate}
\item [(a)] Initially, the barriers are at  and  and the potentials are 

\item [(b)] Each matrix is obtained by a rank-one update of the previous one ---
specifically by adding a positive multiple of an outer product of some .

\item [(c)] If we increment the barriers  and  by  and  respectively
at each step, then the upper and lower potentials do not increase. For every
,


\item [(d)] No eigenvalue ever jumps across a barrier. For every ,

\end{enumerate}
To complete the proof
 we will choose
  and  so that
after  steps, the condition number of  is bounded by

By construction,  is a weighted sum of at most  of the vectors, as
desired.

The main technical challenge is to show that conditions (b) and (c) can be
satisfied simultaneously --- i.e., that there is always a choice of  to
add to the current matrix which allows us to shift {\em both} barriers up by a 
constant without increasing either potential. We achieve this in the following
three lemmas.

The first lemma concerns shifting the upper barrier. If we shift  forward to
   without changing the matrix , then the upper potential
   decreases since the eigenvalues  do not move and  moves away from
  them.
This gives us room to add some multiple of a vector , which will
  move the  towards  and increase the potential, counteracting the
  initial decrease due to shifting.
The following lemma quantifies exactly how much of a given  we can add
  without increasing the potential beyond its original value before shifting.

\begin{lemma}[Upper Barrier Shift]\label{lem:upperupd} Suppose ,
  and  is any vector. If 

then 

That is, if we add  times  to  and shift the upper barrier
by , then we do not increase the upper potential.
\end{lemma}
\noindent We remark that  is linear in the outer product .
\begin{proof}
Let . By the Sherman-Morrison formula, we can write the updated
potential as:


As ,
  the last term is finite for .
By now substituting any 
   we find
  .
This also tells us that , 
  as if this were not the case, then there would be some positive
   for which .
But, at such a , 
  would blow up, and we have just established that it is finite.
\end{proof}

The second lemma is about shifting the lower barrier.
Here, shifting  forward to  while keeping  fixed has the
opposite effect --- it 
  increases the lower potential  since the barrier  moves 
  towards the eigenvalues .
Adding a multiple of a vector  will move the  forward and away
  from the barrier, decreasing the potential.
Here, we quantify exactly how much of a given  we need to add 
  to compensate for the initial increase from shifting , and return the potential to 
  its original value before the shift. 
\begin{lemma}[Lower Barrier Shift]\label{lem:lowerupd} Suppose ,
  ,
  and  is any vector. If

then 

That is, if we add  times  to  and shift the lower barrier by
, then we do not increase the lower potential.
\end{lemma}
\begin{proof} 
First, observe that  and  
  imply that .
So, for every , .

Now proceed as in the proof for the upper potential. Let .
By Sherman-Morrison, we have:

Rearranging shows that  when .
\end{proof}

The third lemma identifies the conditions under which we can find a single
   which allows us to maintain both potentials while shifting barriers, and thereby continue the
  process.
The proof that such a vector exists is by an averaging argument, so 
  this can be seen as the step in which we relate the behavior of actual vectors
  to the behavior of the expected vector .
Notice that the use of variable weights , from which the eventual
   arise, is crucial to this part of the proof.
\begin{lemma}[Both Barriers]\label{lem:shiftboth} 
If 
, ,
,
, and  and  satisfy

then there exists an  and positive  for which


 \end{lemma}
\begin{proof}
We will show that

 from which the claim will follow by Lemmas~\ref{lem:upperupd} and~\ref{lem:lowerupd}.
We begin by bounding


On the other hand, we have

by Claim~\ref{clm:lowercauchy}. 

Putting these together, we find that 
 as desired.
\end{proof}

\begin{claim}\label{clm:lowercauchy}
If  for all ,
   ,
  and ,
then


\end{claim}
\begin{proof}
We have 

for every .
So, the denominator of the left-most term on the left-hand side is positive,
 and the claimed inequality is equivalent to

which, by moving the first term on the RHS to the LHS, is just

By Cauchy-Schwartz,

and so (\ref{eqn:lowercauchy}) is established.
\end{proof}




\begin{proof}[Proof of Theorem \ref{thm:linalg}] All we need to do now is set ,
and  in a manner that satisfies Lemma \ref{lem:shiftboth} and gives a good
bound on the condition number. Then, we can take  and 
construct  from  by choosing any vector  with
 (such a vector is guaranteed to exist by Lemma
\ref{lem:shiftboth})
and setting  for any  satisfying:


It is sufficient to take


We can check that:

so that (\ref{bothcond}) is satisfied.

The initial potentials are
 and
. After 
steps, we have

as desired. 
\end{proof}

To turn this proof into an algorithm, one must first compute the vectors ,
  which can be done in time .
For each iteration of the algorithm, we must compute 
  ,  ,
  and the same matrices for the lower potential function.
This computation can be performed in time .
Finally, we can decide which edge to add in each iteration by computing
   and  for each edge, which can be done
  in time .
As we run for  iterations, the total time of the algorithm is
  .


\section{Sparsifiers of the Complete Graph}\label{sec:expanders}
Let  be the complete graph on  vertices,
  and let  be a weighted graph of
  average degree  that -approximates
  .
As  for every  orthogonal
  to , it is immediate that every vertex of 
  has weighed degree between  and .
Thus, one should think of  as being an expander graph in which
  each edge weight has been multiplied by .

As  is weighted and can be irregular, it may at first seem
  strange to view it as an expander.
However, it may easily be shown to have the properties that define
   expanders: it has high edge-conductance,
  random walks mix rapidly on  and converge to an almost-uniform distribution,  and it satisfies the
  Expander Mixing Property (see~\cite{AlonChung} 
  or~\cite[Lemma 2.5]{ExpanderSurvey}).
High edge-conductance and rapid mixing would not be so interesting if
  the weighted degrees were not nearly uniform ---
for example, the star graph has both of these properties,
but the random walk on the star graph converges to a very non-uniform
  distribution, and the star does not satisfy the Expander Mixing Property.
For the convenience of the reader, we include a proof that 
  has the Expander Mixing Property below.
\begin{lemma}\label{lem:mixing}
Let  be a graph
  that -approximates ,
  the complete graph on .
Then, for every pair of disjoint sets  and ,

where  denotes the sum of the weights of edges
  between  and .
\end{lemma}
\begin{proof}
We have

so
  we can write 

  where  is a matrix of norm at most 
 .
Let  be the characteristic vector of , and let
   be the characteristic vector of .
We have

As  is the complete graph and  and  are disjoint, we also know

Thus,

The lemma now follows by observing that

\end{proof}


Using the proof of the lower bound on the spectral gap
  of Alon and Boppana (see~\cite{Nilli}) one can show that a -regular
  unweighted graph cannot -approximate a complete graph for
   asymptotically better than \eqref{eqn:ramanujanBound}.
We conjecture that this bound also holds for weighted graphs of average degree .
Presently, we prove the following
  weaker result for such graphs.

\begin{proposition}\label{pro:lowerComplete}
Let  be the complete graph on vertex set ,
  and let  be a weighted graph with  vertices and
  a vertex of degree .
If  -approximates , then

\end{proposition}


\begin{proof}
We use a standard approach.
Suppose  is a -approximation of the complete graph. 
We will construct vectors  and  orthogonal to the  vector so
  that 
  
  is large, and this will give us a lower bound on .

Let  be the vertex of degree , and let
  its neighbors be . 
Suppose  is connected to  by an edge of weight , and the total weight
  of the edges between  and vertices other than  is
  .
We begin by considering vectors  and  with


These vectors are not orthogonal to , but we will take care of that
  later.
It is easy to compute the values taken by the quadratic form at  and :

and 

The ratio in question is thus

Since  is a -approximation, all weighted degrees must lie between 
and , which gives

Therefore,

Let  and  be the projections of  and  respectively orthogonal to
the  vector. Then 

and 

so that as  

Combining (\ref{eqn:testvectors}) and (\ref{eqn:projnorms}), we conclude that asymptotically:

But by our assumption the LHS is at most , so we have

which on rearranging gives

as desired.
\end{proof}
\section{Conclusion}
We conclude by drawing a connection between Theorem \ref{thm:linalg} and an
  outstanding open problem in mathematics, the Kadison-Singer conjecture. 
This conjecture, which dates back to 1959, is equivalent to the well-known
  Paving Conjecture \cite{ak-and, cass} as well as to a stronger form of the restricted invertibility
  theorem of Bourgain and Tzafriri \cite{btz,cass}.
The following formulation is due to Nik Weaver \cite{weaver}.

\begin{conjecture} \label{conj:ks} There are universal constants 
, and  for which the following
statement holds. If 
satisfy  for all  and

  then there is a partition  of  for which

  for every .
\end{conjecture}

Suppose we had a version of Theorem \ref{thm:linalg} which, assuming 
, guaranteed that the
  scalars  were all either  or some constant , and 
  gave a constant approximation factor .
Then we would have

 for , yielding
a proof of Conjecture \ref{conj:ks} with 
     and  since
    
    

As a special case, such a theorem would also imply the existence of {\em unweighted} sparsifiers
  for the complete graph and other (sufficiently dense) edge-transitive graphs. 
It is also worth noting that the  condition when applied to vectors  arising from a graph 
  simply means that the effective resistances
  of all edges are bounded; thus, we would be able to conclude that any graph with sufficiently small resistances
  can be split into two graphs that approximate it spectrally.

\begin{thebibliography}{11}


\bibitem{ak-and}
C.~A. Akemann and J.~Anderson.
\newblock Lyapunov theorems for operator algebras.
\newblock {\em Mem. Amer. Math. Soc.}, 94, 1991.

\bibitem{AlonChung}
Noga Alon and Fan Chung.
\newblock Explicit construction of linear sized tolerant networks.
\newblock {\em Discrete Mathematics}, 72:15--19, 1988.

\bibitem{BenczurKarger}
Andr{\'a}s~A. Bencz{\'u}r and David~R. Karger.
\newblock Approximating s-t minimum cuts in {O}(n{}) time.
\newblock In {\em STOC '96}, pages 47--55, 1996.

\bibitem{btz}
J.~Bourgain and L.~Tzafriri.
\newblock {On a problem of Kadison and Singer}.
\newblock {\em J. Reine Angew. Math.}, 420:1--43, 1991.

\bibitem{cass}
Peter~G. Casazza and Janet~C. Tremain.
\newblock {The Kadison-Singer problem in mathematics and engineering}.
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 103(7):2032--2039, 2006.

\bibitem{PaulChew}
P.~Chew.
\newblock There is a planar graph almost as good as the complete graph.
\newblock In {\em SoCG '86}, pages 169--177, 1986.

\bibitem{dette}
H. Dette and W.~J. Studden.
\newblock {Some new asymptotic properties for the zeros of Jacobi, Laguerre, and Hermite polynomials}.
\newblock {\em Constructive Approximation}, 11(2):227--238, 1995.

\bibitem{GolubVanLoan}
G.~H. Golub and C.~F. {Van Loan}.
\newblock {\em Matrix Computations, 3rd. Edition}.
\newblock The Johns Hopkins University Press, Baltimore, MD, 1996.

\bibitem{GoyalRademacherVempala}
Navin Goyal, Luis Rademacher, and Santosh Vempala.
\newblock Expanders via random spanning trees.
\newblock In {\em SODA '09}, pages 576--585, 2009.

\bibitem{ExpanderSurvey}
Shlomo Hoory, Nathan Linial, and Avi Wigderson.
\newblock Expander graphs and their applications.
\newblock {\em Bulletin of the American Mathematical Society}, 43(4):439--561,
  2006.

\bibitem{krv}
Rohit Khandekar, Satish Rao, and Umesh Vazirani.
\newblock Graph partitioning using single commodity flows.
\newblock In {\em STOC '06}, pages 385--390, 2006.

\bibitem{LPS}
A.~Lubotzky, R.~Phillips, and P.~Sarnak.
\newblock Ramanujan graphs.
\newblock {\em Combinatorica}, 8(3):261--277, 1988.

\bibitem{Margulis}
G.~A. Margulis.
\newblock Explicit group theoretical constructions of combinatorial schemes and
  their application to the design of expanders and concentrators.
\newblock {\em Problems of Information Transmission}, 24(1):39--46, 1988.

\bibitem{Nilli}
Alon Nilli.
\newblock On the second eigenvalue of a graph.
\newblock {\em Discrete Mathematics}, 91(2):207--210, 1991.

\bibitem{rudl}
Mark Rudelson.
\newblock Random vectors in the isotropic position.
\newblock {\em J. of Functional Analysis}, 163(1):60--72, 1999.

\bibitem{SpielmanSrivastava}
Daniel~A. Spielman and Nikhil Srivastava.
\newblock Graph sparsification by effective resistances.
\newblock In {\em STOC '08}, pages 563--568, 2008.

\bibitem{SpielmanTengPrecon}
Daniel~A. Spielman and Shang-Hua Teng.
\newblock Nearly-linear time algorithms for graph partitioning, graph
  sparsification, and solving linear systems.
\newblock In {\em STOC '04}, pages 81--90, 2004.

\bibitem{SpielmanTengLinsolve}
Daniel~A. Spielman and Shang-Hua Teng.
\newblock Nearly-linear time algorithms for preconditioning and solving
  symmetric, diagonally dominant linear systems.
\newblock {\em CoRR}, abs/cs/0607105, 2008.
\newblock Available at \texttt{http://www.arxiv.org/abs/cs.NA/0607105}.

\bibitem{SpielmanTengSparsifier}
Daniel~A. Spielman and Shang-Hua Teng.
\newblock Spectral sparsification of graphs.
\newblock {\em CoRR}, abs/0808.4134, 2008.
\newblock Available at \texttt{http://arxiv.org/abs/0808.4134}.

\bibitem{weaver}
Nik Weaver.
\newblock {The Kadison-Singer problem in discrepancy theory}.
\newblock {\em Discrete Mathematics}, 278(1-3):227 -- 239, 2004.

\end{thebibliography}

\end{document}
