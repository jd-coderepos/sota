\section{Experiments}\label{sec:experiment}


In this section, we compare our MaskTrack R-CNN with several baselines on our new dataset YouTube-VIS. We first present the information of the dataset splits and implementation details of our method. 

\noindent\textbf{Dataset}. We randomly split the YouTube-VIS dataset into  training videos,  validation videos and  test videos. Each of the validation and test set is guaranteed to have more than 4 instances per category. All the methods are trained on the training set and all hyperparameters are cross validated on the validation set. We present results on both the validation set and test set in the results section. 

\noindent\textbf{Implementation}. The backbone of our network is based on the network structure of ResNet-50-FPN in~\cite{he2018maskrcnn} and we use a public implementation~\cite{mmdetection2018} which is pretrained on MS COCO~\cite{lin2014mscoco}. The structure of our new tracking branch is two fully connected layers. The first fully connected layer transforms the  input feature maps to 1-D  dimensions. The second fully connected layer also maps its input to 1-D  dimensions. Our full model is trained end-to-end in 12 epochs. The initial learning rate is set to 0.05 and decays with a factor of 10 at epoch 8 and 11. In testing, our model runs at 20 FPS with a NVIDIA 1080Ti GPU. The hyperparameters ,  and  in Equation~\ref{eq:score} are cross validated and chosen to be 1, 2 and 10 to produce our final results. We downsample original frame sizes to  for all the methods in both training and evaluation.

\subsection{Baselines} 
To our best knowledge, there is no prior work directly applicable to our new task. Therefore we combine ideas from related tasks to propose several new baselines. We incorporate two types of algorithms for the baselines. The first type uses the object masks detected in the first frame of the video as initial guidance and applies video object segmentation algorithms to propagate the masks. We evaluate two recent video object segmentation algorithms OSMN~\cite{Yang2018osmn} and FEELVOS~\cite{voigtlaender2019feelvos}. The second type follows the ``tracking-by-detection" idea which is very popular in the multi-object tracking task. The basic idea of this type of works is using image detection methods on each frame independently and then linking the detection across frames by various tracking methods. In our experiment, all the baselines are given the same per-frame instance segmentation results which are produced by a Mask R-CNN. The Mask R-CNN has the same structure as our network except the tracking branch. To make the evaluation fair, The Mask R-CNN is pretrained on MS COCO and is then finetuned on YouTube-VIS with 12 training epochs. Next we describe different track-by-detect methods in our experiment.

\noindent\textbf{IoUTracker+}. This method computes a score between a new candidate box with each identified instance by using a similar equation as Equation~\ref{eq:score} except without using the first term, \ie the appearance similarity. Therefore the matching does not leverage any visual information. The candidate box is assigned to the instance label with the largest score, with a minimum IoU threshold (30\%). Otherwise it is assigned with a new label. The matching process is similar to IoUTracker~\cite{bochinski2017high}. The difference is that a similar memory as our method is equipped with the baseline to save the information of identified instances. 

\noindent\textbf{OSMN~\cite{Yang2018osmn}}. Given an identified instance mask, OSMN estimates a new mask of the instance at a new frame. The new mask is then used to compute IoU with candidate boxes at the same frame. This is better than IoU directly computed via consecutive frames especially when an instance is occluded or has large motion. The rest of the matching process is the same as IoUTracker+.

\noindent\textbf{DeepSORT~\cite{wojke2017simple}}. DeepSORT is a top-performing tracking method. it uses Kalman filter to predict bounding box location to avoid directly computing IoU of consecutive frames. In addition, it use a deep network to measure the appearance similarity between bounding boxes. Finally the IoU score and the visual appearance score are combined to match tracks by the Hungarian algorithm.

\noindent\textbf{SeqTracker}. This is an offline algorithm following Seq-NMS~\cite{seqnms16}. Given a video and a set of instance segmentation results of every frame, SeqTracker searches all possible tracks to find the one with the largest score, which is computed similarly as IoUTracker+. Then the instance segmentation of the track will be removed from the set and the search process repeats. The method halts until the length of a retrieved track is less than a threshold, which is set to 8 in our experiment. 











\begin{table*}[t]
\centering
\caption{Quantitative evaluation of the proposed algorithm and baselines on the YouTube-VIS validation and test set. The best results are highlighted in bold.}
\label{tab:compare}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Methods}}}  & \multicolumn{5}{c|}{\begin{tabular}[c]{@{}c@{}} \textbf{validation set}\end{tabular}} &
\multicolumn{5}{c|}{\begin{tabular}[c]{@{}c@{}} \textbf{test set}\end{tabular}} \\ \cline{3-12}
\multicolumn{2}{|c|}{} & AP & AP\textsubscript{50} & AP\textsubscript{75} & AR\textsubscript{1} & AR\textsubscript{10}
 & AP & AP\textsubscript{50} & AP\textsubscript{75} & AR\textsubscript{1} & AR\textsubscript{10} \\ \hline
\multirow{2}{*}{\textbf{Mask propagation}}  & OSMN~\cite{Yang2018osmn} & 23.4 & 36.5 & 25.7 & 28.9 &31.1 & 27.3 & 44.4 & 28.0 & 28.8 & 34.0 \\ \cline{2-12}
 & FEELVOS~\cite{voigtlaender2019feelvos} & 26.9 & 42.0& 29.7& 29.9 & 33.4 & 29.6 & 45.4  & 30.7& 33.4 & 36.8 \\ \hline
\multirow{5}{*}{\textbf{Track-by-detect}} & IoUTracker+   & 23.6 & 39.2 & 25.5 & 26.2 & 30.9    & 25.2 & 41.9 & 26.2 &  28.7 & 33.7 \\ \cline{2-12}
& OSMN~\cite{Yang2018osmn}       & 27.5 & 45.1 & 29.1 & 28.6 & 33.1    & 27.3 & 44.4 & 28.0 & 28.8 & 34.0 \\ \cline{2-12}
& DeepSORT~\cite{wojke2017simple}   & 26.1 & 42.9 & 26.1 & 27.8 & 31.3    & 27.2 & 44.0 & 29.2 & 29.1 & 33.3 \\ \cline{2-12}
& SeqTracker    & 27.5 & 45.7 & 28.7 & 29.7 & 32.5    & 29.5 & 48.1 & 31.2 & 32.0 & 34.5 \\ \cline{2-12}
& MaskTrack R-CNN & \textbf{30.3} & \textbf{51.1} & \textbf{32.6} & \textbf{31.0} & \textbf{35.5}    & \textbf{32.3} & \textbf{53.6} & \textbf{34.2} & \textbf{33.6} & \textbf{37.3} \\ \hline
\end{tabular}
\end{table*}

\begin{figure*}[t]\centering
\includegraphics[width=0.95\linewidth]{images/sample.pdf}
\caption{Sample results of MaskTrack R-CNN. Each row have five sampled frames from a video sequence. (a),(b),(c) and (d) show correct predictions while (e) and (f) are failure cases. Objects with same predicated identity have the same color. Object category is shown on top of each bounding box. Zoom in to see details.}
\label{fig:sample}\vspace{-2pt}
\end{figure*}

\subsection{Main Results}
Table~\ref{tab:compare} presents the comparison results. Notably, our method MaskTrack R-CNN achieves the best results under all evaluation metrics and on both the validation and test sets. The main difference between our method and the other track-by-detect baselines is the new tracking branch which is trained end-to-end with the other branches, so that useful information can be shared among multiple tasks. The key for the joint training of tracking with other tasks is that we formulate the instance matching process as a differentiable component, which enables the matching loss to be properly back propagated. 

Next we analyze the performance of the baselines. For mask propagation algorithms, they suffer from a natural disadvantage, that they cannot handle objects appear in the intermediate frames. Also the flawed detections in the frist frames directly degenerate their performance. Even the state-of-the-art video object segmentation algorithm FEELVOS only gains 26.9 AP on validation set. For track-by-detect algorithm, IoUTracker+ does not leverage any visual information which is not surprising to gain weak performance. OSMN predicts the possible location of previously identified instances at new frames, and use the prediction to match instances, which is useful to handle occlusion and fast motion. DeepSORT improves IoUTracker+ on both the IoU matching and usage of visual similarity, achieving better results. SeqTracker does not depend on any visual information and achieves better performance than the other baselines. However, it is an offline method which requires instance segmentation results to be precomputed for all frames. The other methods including MaskTrack R-CNN are online methods, which produce instance tracks sequentially. 



Figure~\ref{fig:sample} shows six sample videos with our predictions. The first four rows ((a),(b), (c) and (d)) are successful predictions and the last two rows are failure cases. In video (a), the frame-level prediction gives incorrect results in the first two frames, where the bear is predicted as ``deer'' and ``earless seal''. The video-level prediction corrects these mistakes by majority voting of all frames. In video (c), the surfboard is occluded by the wave in multiple frames, our algorithm is able to track the surfboard after it disappears and reoccurs. The memory queue in MaskTrack R-CNN is able to keep track of all previous objects even they are disappeared in intermediate frames. In video (d), we show a case that new object enters the video in the intermediate frames. Our algorithm is able to detect the deer in the second frame as new object and add it to the external memory.
Video (e) and (f) shows two challenging cases. In video (e), the dear has quite different appearance in different poses, and our algorithm fail to recognize the same object and consider them as two different objects. In video (f), multiple similar fishes move around the aquarium and occlude each other. Our algorithm groups two fishes as one in the second and third frame, and gets confused with the object identities later on. 

\begin{figure}[t]\centering
\includegraphics[width=1\linewidth]{images/factor.pdf}
\caption{A sample result with different matching cues used. With all four factors, the result is the best.}
\label{fig:factor}\vspace{-2pt}
\end{figure}

\subsection{Ablation Study}


\begin{table}[t]
\small
\centering
\caption{Ablation study of our method on the YouTube-VIS validation set. ``Det", ``IoU", and ``Cat" denote the detection confidence, the bounding box IoU, and the category consistency in Equation~\ref{eq:score} respectively. Numbers in brackets shows the difference compared to the complete score.}
\label{tab:combination}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Det & IoU & Cat & AP      & AP\textsubscript{50} & AP\textsubscript{75} \\ \hline
\xmark         & \xmark        & \xmark         & 21.1(-9.2)   & 37.7(-13.4)   & 23.6(-9.0)\\ \hline
\cmark         & \xmark        & \xmark         & 23.4(-6.9)   & 42.5(-8.6)   & 24.4(-8.2)\\ \hline
\xmark         & \cmark        & \xmark         & 22.7 (-7.6)   & 40.7 (-10.4)   & 25.2 (-7.4)\\ \hline
\cmark         & \cmark        & \xmark         & 24.7 (-5.6)   & 44.3 (-6.8)   & 26.7 (-5.9)\\ \hline
\xmark         & \xmark        & \cmark         & 27.9 (-2.4)   & 47.1 (-4.0)   & 30.5 (-2.1)\\ \hline
\cmark         & \xmark        & \cmark         & 29.2 (-1.1)   & 49.2 (-1.9)   & 31.9 (-0.7)\\ \hline
\xmark         & \cmark        & \cmark         & 29.5 (-0.8)   & 48.7 (-2.4)   & 32.2 (-0.4) \\ \hline
\cmark         & \cmark        & \cmark         & 30.3          & 51.1          & 32.6 \\ \hline
\end{tabular}
\end{table}

We study the importance of three cues used in Equation~\ref{eq:score} to our method. They are the detection score, the bouding box IoU and category consistency. We evaluate our method on the validation set by turning these cues on and off. The results are presented in Table~\ref{tab:combination}. We find that the bounding box IoU and the category consistency are most important to the performance of our method. Without any of them, AP will drop around 5\%. While the detection confidence score only improves our method slightly. Intuitively, the bounding box IoU correlates to the spatial relationship between instances, which is a strong prior in many cases. The category consistency also provides a very strong constraint because the category label of an instance should not change in a video. However, relying too much on the these factors can also cause problems due to the imperfect estimation. Therefore our method uses these cues as soft constraints. To visualize the effect of these three factors, we also generate predictions with the three factors added one by one on one specific sample, which is shown in Figure~\ref{fig:factor}. Note that the first three variants cannot track the identity of the ``green'' motorbike well, while the variant with four different cues is able to track it through the whole video.




\subsection{Oracle Results}

Additionally, we investigate the effectiveness of the two parts in our algorithm: image-level prediction and cross-frame association. We evaluate effectiveness of video-level association by applying ground truth image-level annotations to our algorithm. Specifically, given ground truth image-level predictions including bounding boxes, masks and categories, we compute matching score  using RoIAlign features of ground truth bounding boxes and match objects across frames using combined score . The result is shown in Table~\ref{tab:oracle} with ``Image Oracle''. We also evaluate image-level predictions with ground truth object identities. Towards this end, per-frame predictions are first matched to their closest ground truth image objects, and then video objects are aggregated using ground truth object identities. The result is shown in Table~\ref{tab:oracle} with ``Identity Oracle''. It shows that Image Oracle achieves much better performance than Identity Oracle, which means image-level predictions is critical for better performance on video instance segmentation. Identity Oracle is only marginally better than MaskTrack RCNN, which indicates limited potential of improving over our current method by modifying object tracking method. Improving image-level detection performance by utilizing properly-designed spatial-temporal feature could be a promising direction. Meanwhile, even with image-level ground truth, it is still challenging to associate objects across frames due to object occlusions and fast motion.






\begin{table}[t]
\centering
\caption{Oracle results in two settings on validation set. Image oracle is results with predicted object identity based on ground truth image-level annotations, identity oracle is results with  ground truth object identities based on predicted image-level instances.}
\label{tab:oracle}
\begin{tabular}{|l|l|l|}
\hline
                & AP   & AR\textsubscript{10} \\ \hline
Image Oracle    & 78.7 & 83.7    \\ \hline
Identity Oracle & 31.5 & 34.6    \\ \hline
\end{tabular}
\end{table}