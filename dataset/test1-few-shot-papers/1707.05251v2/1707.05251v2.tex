\documentclass[sigconf]{acmart}

\usepackage{booktabs} \setcopyright{rightsretained}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{breqn}
\usepackage{mathtools}

\newenvironment{packed_itemize}{
    \vspace{-0.15cm}\begin{itemize}
        \setlength{\itemsep}{1pt}
        \setlength{\parskip}{0pt}
        \setlength{\parsep}{0pt}
    }{\end{itemize}}

\newenvironment{packed_enumerate}{
    \begin{enumerate}
        \setlength{\itemsep}{1pt}
        \setlength{\parskip}{0pt}
        \setlength{\parsep}{0pt}
    }{\end{enumerate}}
\pretolerance=150
\setlength{\emergencystretch}{3em}

\newcommand{\argmin}{\arg\!\min}
\newcommand{\trans}[1]{{#1}^{\ensuremath{\mathsf{T}}}} \newcommand{\cavan}[1]{{\color{blue}(cavan: {#1})}} \newcommand{\danny}[1]{{\color{black}(danny: {#1})}} \newcommand{\needtofill}[1]{{\color{black}{#1}}} \newcommand{\ie}{\textit{i.e.,}}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\eg}{\textit{e.g.,}}
\newcommand{\totaluser}{47}




\acmConference[ACMMM'18]{ACM Multimedia conference}{July 2018}{Seoul, Korea}
\acmYear{2018}
\copyrightyear{2018}


\acmArticle{4}
\acmPrice{15.00}



\acmSubmissionID{122}
\begin{document}
\title{Aesthetic-Driven Image Enhancement by Adversarial Learning}






\author{Yubin Deng}
\authornote{@ie.cuhk.edu.hk}
\affiliation{\institution{CUHK-Sensetime Joint Laboratory, \ \ The Chinese University of Hong Kong}
}
\author{Chen Change Loy}
\affiliation{\institution{CUHK-Sensetime Joint Laboratory, \ \ The Chinese University of Hong Kong}
}
\author{Xiaoou Tang}
\affiliation{\institution{CUHK-Sensetime Joint Laboratory, \ \ The Chinese University of Hong Kong}
}





\begin{abstract}
We introduce EnhanceGAN, an adversarial learning based model that performs automatic image enhancement. 
Traditional image enhancement frameworks typically involve training models in a fully-supervised manner, which require expensive annotations in the form of aligned image pairs.  
In contrast to these approaches, our proposed EnhanceGAN only requires weak supervision (binary labels on image aesthetic quality) and is able to learn enhancement operators for the task of aesthetic-based image enhancement. 
In particular, we show the effectiveness of a piecewise color enhancement module trained with weak supervision, and extend the proposed EnhanceGAN framework to learning a deep filtering-based aesthetic enhancer.
The full differentiability of our image enhancement operators enables the training of EnhanceGAN in an end-to-end manner. 
We further demonstrate the capability of EnhanceGAN in learning aesthetic-based image cropping without any groundtruth cropping pairs.
Our weakly-supervised EnhanceGAN reports competitive quantitative results on aesthetic-based color enhancement as well as automatic image cropping, and a user study confirms that our image enhancement results are on par with or even preferred over professional enhancement.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010371.10010382</concept_id>
<concept_desc>Computing methodologies~Image manipulation</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Image manipulation}
\maketitle


\section{Introduction}
\label{sec:introduction}
Image enhancement is considered a skillful artwork that involves transforming or altering a photograph using various methods and techniques to improve the aesthetics of a photo. Examples of enhancement include adjustments of color, contrast and white balance, as shown in Fig.~\ref{fig:fig1}. 
This task is conventionally conducted manually through some professional tools. Manual editing is time-consuming even for a professionally trained artist. 
While there are an increasing number of applications that allow casual users to choose a fixed set of filters and/or to alter the composition of a photo in more convenient ways, human involvement is still inevitable. 
Given the increasing amount of digital photographs captured daily with mobile devices, it is desirable to perform image enhancement with minor human involvement or in a smart and fully automatic manner.
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/cvpr_fig1.pdf}
\end{center}
\vskip -0.4cm
\caption[Caption for LOF]{Examples of image enhancement given original input (a). Can you distinguish which figure is enhanced by human and which by our adversarial learning model? (Answer in footnote\protect\footnotemark. Best viewed in color.)}\color{black}
\vskip -0.5cm
\label{fig:fig1}
\end{figure}
\footnotetext{
\scriptsize
\raggedleft\rotatebox{180}{Row 3:\qquad piecewise color enhancer \qquad  deep filtering\hfill}  \\
\raggedleft\rotatebox{180}{Row 2:\qquad deep filtering \qquad  piecewise color enhancer\hfill}  \\ 
\raggedleft\rotatebox{180}{Row 1:\qquad piecewise color enhancer \qquad  deep filtering\hfill} \\
\raggedleft\rotatebox{180}{\textit{Answer: None of them are the results from human editing}\hfill} }
Previous research efforts have shown some success in automating color enhancement~\cite{bhattacharya2010framework,lee2016automatic,sun2016photo,yan2014learning}, style transfer~\cite{gupta2017characterizing,huang2017arbitrary,johnson2016perceptual} and image cropping~\cite{chen2016automatic,chen2017quantitative,yan2013learning}. However, most of these models require full supervision. 
In particular, we need to provide input and manually-enhanced image pairs to learn the capability of color enhancement~\cite{yan2016automatic} or image re-composition~\cite{huang2015automatic,chen2016automatic,yan2013learning}. Unfortunately, such data is scarce due to the expensive cost of obtaining professional annotations. 
Ignatov et al.~\cite{ignatov2017dslr} has recently demonstrated the possibility of enhancing low-quality photos towards DSLR-quality. The training images, however, have to be captured by specialized time-synchronized hardware to form pairs, and further registered to remove misalignment between image pairs.
In this study, we present a novel approach that trains a weakly-supervised image enhancement model from unpaired images without strong human supervision.
In particular, we attempt to learn image enhancement from images with only binary labels on aesthetic quality, \ie, good or poor quality.
As the edited image should be closer (in the sense of aesthetic quality) to the photo collections by professional photographers compared to the original image with poor aesthetic quality, this notion can be well formulated in an adversarial learning framework~\cite{goodfellow2014generative}. 
Specifically, we have a discriminator  that attempts to distinguish images of poor and good aesthetic quality. Such a network can be trained by an abundant amount of images from existing aesthetic datasets~\cite{murray2012ava,tang2013content}. A generator , on the other hand,  generates a set of manipulation parameters given an image with poor aesthetic quality. The task of  is to fool  so that  confuses the 's outputs (\ie, enhanced images) as images with high quality.

\noindent The main contribution of this study is three-fold:
\begin{packed_enumerate}
  \item EnhanceGAN leverages abundant images that are annotated only with good and poor quality labels. \textit{No knowledge of the groundtruth enhancement action is given to the system}. Image enhancement operators are learned in a weakly-supervised manner through adversarial learning driven by aesthetic judgement.
  \item The framework permits multiple forms of color enhancement. We carefully design a piecewise color enhancement operator and a deep filtering-based enhancer to be fully-differentiable for end-to-end learning in an adversarial learning framework. 
  \item EnhanceGAN is extensible to include further image enhancement schemes (provided that the enhancement operations are fully-differentiable). We explore such capability of EnhanceGAN for aesthetic-based automatic image cropping and present competitive results. 
\end{packed_enumerate}
Owing to the subjective nature of image enhancement, we show the effectiveness of EnhanceGAN for image enhancement in two sets of evaluations. We quantitatively evaluate the performance of color enhancement by multiple state-of-the-art aesthetic evaluators on reserved unseen images and we show quantitative results for automatic image cropping on a standard benchmark dataset. We also perform a blind user study to compare our method with human enhancement. 




\section{Related Work}
\label{sec:related_work}
\noindent\textbf{Aesthetic Quality Assessment.}
The task of aesthetic quality assessment is to distinguish high-quality photos from low-quality ones based on human perceived aesthetics. Previous successful attempts train convolutional neural networks for binary classification of image quality~\cite{lu2014rapid,ma2017lamp} or aesthetic score regression~\cite{kong2016photo}. We refer readers to a comprehensive study~\cite{deng2017image} on the state-of-the-art models on image aesthetic assessment. Although the focus of image enhancement is not on assessing the quality of a given image, our work is closely related to this research domain in the sense that image enhancement aims at improving the aesthetic quality of the given input. 


\begin{figure*}
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/cvpr_fig2_wide.pdf}
\end{center}
\vskip -0.4cm
\caption{\textbf{The architecture of our proposed EnhanceGAN framework}. ResNet module is the feature extractor; in this work, we use the ResNet-101~\cite{he2016deep} and removed the last average pooling layer and the final fc layer. (Best viewed in color.)}
\label{fig:architecture}
\end{figure*}

\noindent\textbf{Automatic Image Enhancement:}
The majority of techniques for image manipulation with the goal to enhance the aesthetic quality of an image can be divided into two genres, namely (1) color enhancement and (2) image re-composition. Pixel-level manipulation and image restoration (\textit{e.g.,} super resolution~\cite{dong2016image}, de-haze~\cite{he2011single} and de-artifacts~\cite{wang2016d3}) are also closely related to image enhancement but is beyond the focus in this work.
\\
\noindent\textbf{Color Enhancement.}
The visual quality of an image can be enhanced by color adjustment, where regression models and ranking models have been trained to map the input image to a corresponding enhanced groundtruth~\cite{faridul2014survey}. Such color mappings are learned~\cite{yan2014learning} from a small set of labeled data by professional editors. To alleviate the lack of sufficient labeled data, recent research efforts formulate the color enhancement problem as the color transfer problem~\cite{hwang2014color,reinhard2001color}. In particular, the popular exemplar-based color transfer approaches~\cite{lee2016automatic,sun2016photo} seek to retrieve the most suitable matching exemplar based on image content and perform color transfer onto the given input. However, they suffer from potential visual artifacts due to erroneous exemplars. 
Ignatov \etal~\cite{ignatov2017dslr} present a fully-supervised approach aided by adversarial learning, where they seek to improve low-quality images towards DSLR quality. However, their method still requires carefully aligned input and groundtruth pairs and the effectiveness of their model is limited to images captured by particular mobile devices.  
Stylistic image enhancement~\cite{yan2016automatic} and creative style transfer~\cite{gupta2017characterizing,huang2017arbitrary,johnson2016perceptual,ulyanov2016instance} are also closely related to color enhancement, but their focus is to transform an input image into an output that matches the artistic style of a given exemplar, instead of focusing on improving the aesthetic quality of the image. 
Aesthetic-oriented evaluation typically does not apply on those methods. 
Unlike style transfer studies~\cite{yan2016automatic,gupta2017characterizing}, our weakly supervised model is able to enhance an image through chrominance and even spatial manipulations driven by content and aesthetic quality of the image. Thus each individual image would experience different manipulations. Unlike exemplar matching~\cite{lee2016automatic,sun2016photo}, our framework does not require finding the correct exemplars for color/style transfer, and hence our model is not limited by the subset of exemplars.
\\
\noindent\textbf{Cropping and Re-targeting.} 
Image cropping and re-targeting aim at finding the most visually significant region based on aesthetic value or human attention focus. Aesthetic-based approaches~\cite{chen2016automatic,islam2016survey,yan2013learning,chen2017learning} evaluate the crop window candidates based on handcrafted low-level features or learned aesthetic features, while attention/composition-based approaches~\cite{fang2014automatic,choi2016object,huang2015automatic,jaiswal2015saliency} rely on image saliency and produce the cropping window encapsulating the most salient region. 
These systems for cropping and re-targeting are mostly based upon a limited amount of labeled cropping data (1000 training image pairs in total), where the cropping problem is modeled as window regression or window candidate classification in a fully-supervised learning manner~\cite{chen2016automatic,chen2017quantitative}. Network fine-tuning with pre-trained convolutional neural network has obtained some success with extensive data augmentation~\cite{deng2017image}.
\color{black}
The non-parametric images/windows retrieval-based method by Chen \etal~\cite{chen2017learning} also features a weakly-supervised model for image cropping. 
\color{black}
In this work, we present an alternative learning-based weakly-supervised attempt and demonstrate the capability of the proposed EnhanceGAN framework in extending the image enhancement tasks to automatic image cropping, and show competitive results on a standard benchmark dataset. 
\\






\section{Aesthetic-Driven Image Enhancement}
\label{sec:methodology}
We formulate the problem of image enhancement in an adversarial learning framework~\cite{goodfellow2014generative}. Specifically, our proposed approach builds upon Wasserstein GAN (W-GAN) by Arjovsky \etal~\cite{arjovsky2017wasserstein}. The full architecture of our proposed framework is shown in Fig.~\ref{fig:architecture}. 


\subsection{Preliminary}
Generative Adversarial Network (GAN)~\cite{goodfellow2014generative} has shown a powerful capability of generating realistic natural images. Typical GANs contain a generator \textit{G} and a discriminator \textit{D}, and it was proven~\cite{goodfellow2014generative} that the minimax game

would reach a global optimum when  converges to the real data distribution , where  is the distribution of the samples  obtained when , and  is a random or encoded vector. 
In this work we follow the practice in~\cite{arjovsky2017wasserstein} and adopt the loss function based on Wasserstein distance,
\begin{dmath}
L = \mathbb{E}_{\mathbf{I} \sim p_{data}}[f_{W}(\mathbf{I})] - \mathbb{E}_{\mathbf{I} \sim p_{gen}}[f_{W}(\mathbf{I})],
\label{eq:w_loss}
\end{dmath}
where  is a -Lipschitz function parameterized by , which is approximated by our discriminator network \textit{D} as detailed in Sec.~\ref{sec:discriminator_network}.

\subsection{Generator Network (Net-\textit{G})}
Different from most existing GAN frameworks, our generator does not generate images by itself. 
Instead, the generator \textit{G} in our EnhanceGAN is responsible for learning the image enhancement operator , according to which the input image will be transformed to the enhanced output: 
 
where  denotes the fully-differentiable transformation applied to the input image parameterized by .  
The base architecture of our generator network is a ResNet-101~\cite{he2016deep} without the last fully-connected layer, and we further remove the last pooling layer to preserve spatial information in the feature maps. As such, this ResNet module acts as a fully-convolutional feature extractor given an input image (see Fig.~\ref{fig:architecture}). 
The 2048 output feature maps produced by the ResNet module has a spatial size  and is subsequently utilized in our enhancement parameter generation modules. 
In this work, we explore multiple forms of image aesthetic enhancement, including two fully-differentiable color enhancement operators and an image editing operator for automatic image cropping. 


\subsubsection{Piecewise Color Enhancer}
\label{subsubsec:piecewise_color_enhancer}
Image brightness and lighting contrast can be adjusted based on the luminance channel of the  image in the CIELab color space, whereas image chrominance resides in the other two channels. Hence, the color enhancement operator  can be applied to an image in a piecewise manner. To this end, the piecewise color enhancement module is designed to learn a set of parameters
,
where  and  denotes respectively the adjustments for better lighting and chrominance. Specifically, we follow the idea behind gamma correction~\cite{pelli1991accurate} to adjust the brightness and contrast of an image in pixel level (\ie, the L channel of image  in the CIELab color space: ) by designing a piecewise transformation function  (see Fig.~\ref{fig:architecture}) defined on each pixel :

where ,  and  to ensure that  is continuous. We further constrain  in order to lighten the dark regions and  to darken the over-exposed region. Similarly, we follow ``The LAB Color Move''\footnote{The LAB Color Move: \url{https://goo.gl/i2ppcw}} and the curve adjustment instructed in~\cite{hosie2011new} and design a similar process to enhance the image color. In particular, the adjustments  and  defined respectively on pixels  and  (\ie, the A and B channels in image , see Fig.~\ref{fig:architecture}) can be formulated as follows:


Under this formulation, the parameter sets  and  can be learned end-to-end. Our piecewise color enhancement operator can therefore be written as
 
where  denotes channel-wise concatenation in the CIELab color space. Directly learning one single set of such parameters may not be optimal as color enhancement prediction is multi-modal to some extents -- an image can have several plausible color enhancement solutions (\eg, an enhancement solution can simply adjust the color saturation of an image, or further tuning lighting contrast from low to high or vice versa). 

With inspirations drawn from attention models~\cite{xu2015show}, our piecewise enhancement operator is built by appending a convolution layer () with kernel size  to the ResNet module.
In particular, the first 6 feature maps correspond to the candidate sets of the color adjustment parameters .
The 7 feature map is an  softmax probability map corresponding to  possible predictions , where . Top-K average pooling~\cite{wang2017untrimmednets} is adopted to aggregate the parameter candidates with the highest probabilities (see Fig.~\ref{fig:architecture}). 
We show in the experiment section that our piecewise color enhancement operator is able to improve both the color and lighting contrast of the image as compared with the input (see Fig.~\ref{fig:fig1}, \ref{fig:architecture}).




\subsubsection{Deep Filtering-based Enhancer}
\label{subsubsec:deep_filtering}
The piecewise color enhancement operator is limited to learning enhancement parameter  for a pre-defined set of transformations  and . We can extend equation~\eqref{eq:piece_wise_eq} to a general form by writing the enhancement operator  to be a linear combination of individual transforming operations  as follows,


In particular,  can be any form of image enhancement transformation provided that it is fully-differentiable. Under this formulation, the weight parameters  can be learned end-to-end by a convolution layer () where the  feature map is also a  softmax probability map for Top-K average pooling similar to that of the piecewise color enhancement operator (see Sec.~\ref{subsubsec:piecewise_color_enhancer}). In this work, we choose three default 
,, color enhancement filters from \textit{Adobe Photoshop}\footnote{The three filters are: the \textit{cooling80} filter, the \textit{waming85} filter and the \textit{underwater} filter. The three filters represent respectively blue-ish (\#), red-ish (\#) and green-ish (\#) effect under default settings.} and approximate each of the filtering operations with a 3-layer convolutional neural network (see Fig.~\ref{fig:architecture}). 
More color filtering operators  with learnable parameter  can be easily extended in this manner provided that the transformations  are differentiable. We show in the experiment section that the deep filtering-based aesthetic enhancer produce smooth and harmonious color improvement as compared to the piecewise color enhancer (see Fig.~\ref{fig:BigFig}, \ref{fig:linzhe}, \ref{fig:visual_results}).


\subsubsection{Image Cropping Operator}
We further extend the image enhancement operation to explore the possibility of learning the task of aesthetic-based image cropping without any cropping labels. The goal of image cropping is to produce a set of cropping coordinates  given an input image. This can be achieved by a convolution layer () where the 5 feature map is the softmax probability map for Top-K average pooling. It is worth mentioning that directly extracting a sub-image from the input given the learned cropping coordinates   is not differentiable. To ensure that the gradients can back-propagate to the cropping parameters via the transformation , bilinear sampling from a sampling grid~\cite{jaderberg2015spatial} is used to sub-sample the input image based on coordinates output  of the cropping module (see Fig.~\ref{fig:architecture}). 









\subsubsection{Generator Loss Function }
\label{sec:generator_loss}
We formulate the loss function for the generator network as a weighted sum of an adversarial loss component  as well as the regularization component . These terms are weighted to ensure that the loss terms are balanced in their scales. This formulation makes the training process more stable and has better performance (see Sec.~\ref{sec:quantitative}).


\noindent\textbf{Adversarial Loss:}
Following Arjovsky \etal~\cite{arjovsky2017wasserstein}, the adversarial gradient to the generator network \textit{G} is computed from the following loss function:


\noindent\textbf{Regularization Loss:}
As regularization, we use the feature reconstruction loss~\cite{johnson2016perceptual} to account for the semantic difference between the enhanced/cropped image and the input as measured by feature similarity:

where  denotes the  feature output from the VGG-16 network~\cite{simonyan2014very} trained for ImageNet. Also, the notion that an edited image should have better aesthetic quality (lower  values) than the original further gives us an intuitive loss for further regularizing the end-to-end training:

where  f_{W}(\mathbf{I}^{\text{output}}_{i}) < f_{W}(\mathbf{I}_{i}) 
\vskip 0.2cm
\noindent We show in the experiment section that the regularization facilitates the aesthetic-driven adversarial learning of the generator network (see Table~\ref{tab:quantitative_aesthetics1}, \ref{tab:quantitative_aesthetics2}).





\subsection{Discriminator Network (Net-\textit{D})}
\label{sec:discriminator_network}
The proposed framework consists of a discriminator network that is able to assess image aesthetic quality. The discriminator network \textit{D} is designed to share the ResNet-101~\cite{he2016deep} base architecture of \textit{G} during pre-training. As shown in Fig.~\ref{fig:architecture}, the last layer for 1000-class classification in the original ResNet-101 is replaced by a 2-neuron fully-connected layer. We pre-train discriminator \textit{D} for binary aesthetic classification with the cross-entropy loss as in~\cite{deng2017image}.
After pre-training, the discriminator network \textit{D} is appended with another 1-neuron fully-connected layer to perform output aggregation as an approximator to  in Eq.~(\ref{eq:w_loss}, \ref{eq:gan_loss}, \ref{eq:per_loss}, \ref{eq:reg_loss}). Deriving from Eq.~\ref{eq:w_loss}, the loss function  in subsequent adversarial training can be written as:

where .


\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/cvpr_img_129_1.pdf}
\includegraphics[width=\linewidth]{figures/cvpr_img_129_2.pdf}
\end{center}
\vskip -0.3cm
\caption{From left to right, top to bottom: (1) input image (2) random combination of pre-trained filters (3) DSLR~\cite{ignatov2017dslr} (4) EnhanceGAN: image cropping (5) EnhanceGAN: piecewise color enhancer (5) EnhanceGAN: deep filtering-based enhancer. (Best viewed in color. More results in the supplementary material.) }
\label{fig:BigFig}
\end{figure}
\begin{table}[h]
\centering
\caption{Quantitative evaluation for color enhancement on the unseen  images by multiple aesthetic evaluators. \textit{AVA-net} and \textit{CUHK-net} denotes the ResNet-based evaluators finetuned for binary image aesthetic assessment with AVA dataset and CUHK-PQ dataset, respectively (see Sec.~\ref{sec:dataset}). The averaged softmax scores for all images in  are shown as final results.
Our weakly supervised EnhanceGAN is reasonably competitive for the task of aesthetic-based color enhancement, as is also validated by additional state-of-the-arts image aesthetic assessment models~\cite{kong2016photo,deng2017image}.
}
\setlength{\tabcolsep}{1.9 pt}
\begin{tabular}{lcccc}
Methods                                               & \textit{AVA-net} & \textit{CUHK-net} & RANK~\cite{kong2016photo} & DAN~\cite{deng2017image} \\ \hline 
Original Input                                                & 0.705     & 0.542       & 0.487       & 0.479                                    \\
DSLR~\cite{ignatov2017dslr}         & 0.624     & 0.449      & 0.476        & 0.359                                \\
DSLR~\cite{ignatov2017dslr}              & 0.524     & 0.385      & 0.475        & 0.295                                    \\
DSLR~\cite{ignatov2017dslr}      & 0.616     & 0.462      & 0.477        & 0.395                                   \\
Photoshop-Auto                                             & 0.687        & 0.530         & 0.481          & 0.447                                     \\
                                                     &            &             &               &                                                  \\                                                   
Piecewise Enhancer                                                     &            &             &               &                                                 \\ \hline
w/o                                   & 0.708     & 0.705      & 0.484        & 0.491                                         \\
w/o                        & 0.751    &  0.755      & 0.492        & 0.527                                       \\
EnhanceGAN                                                 & 0.764     &       & 0.494      & 0.573                                  \\    
                                                     &            &             &               &                                                  \\
Deep Filtering                                                     &            &             &               &                                                 \\ \hline
random weights                               & 0.687     & 0.432      & 0.479          & 0.438                                        \\
EnhanceGAN                                                &      & 0.752      &           &    \\               
\end{tabular}
\label{tab:quantitative_aesthetics1}
\vskip -0.4cm
\end{table}
\section{Experiments}
Our weakly-supervised EnhanceGAN is tasked to learn image enhancement operators based only on binary labels on image aesthetic quality. Specifically, we train the EnhanceGAN with the benchmark datasets used in aesthetic quality assessment and perform quantitative evaluations on reserved unseen data. A user study is also performed to confirm the validity of our quantitative evaluation.

\subsection{Experimental Settings}
\label{sec:dataset}
\noindent\textbf{CUHK-PhotoQuality Dataset (CUHK-PQ)}~\cite{tang2013content}: The dataset contains 4,072 high-quality images and 11,812 low-quality images. This dataset is used to pre-train the feature extractor (ResNet module) of our EnhanceGAN, with  of the images reserved for validation. We follow the training protocol as in~\cite{deng2017image} and pre-train the ResNet-module for binary image aesthetic assessment (see Sec.~\ref{sec:discriminator_network}), obtaining a balanced accuracy~\cite{deng2017image} of  on the validation set. This pre-trained network (denoted as CUHK-Net) is also used as one of the quantitative aesthetic evaluators (see Table~\ref{tab:quantitative_aesthetics1},\ref{tab:quantitative_aesthetics2}).

\noindent\textbf{AVA Dataset}~\cite{murray2012ava}:
The Aesthetic Visual Analysis (AVA) dataset is by far the largest benchmark for image aesthetic assessment. Each of the 255,530 images is labeled with aesthetic scores ranging from 1 to 10. We follow~\cite{murray2012ava} and partition the images into high-quality set and low-quality set based on the average scoring. In this study we select a subset of low-quality images based on the semantic tags provided in the AVA data for analysis\footnote{This corresponds to nine classes in the AVA dataset, \ie, Landscape, Seascape, Cityscape, Rural, Sky, Water, Nature, Animals and Portraiture.}, covering a diverse set of images that require different enhancement operations for quality enhancement. In particular, the ``Real'' input to the discriminator in our EnhanceGAN is chosen from the top  of the high-quality images. The ``Fake'' inputs are the low-quality images that have an average score . A total of  low-quality images are used for training the EnhanceGAN, each paired with k=5 high-quality images sampled from the k-nearest neighbor (k-NN) in the feature space of  (All training images are from the AVA standard training partition). \textit{The binary good/bad quality is the only form of supervision in our training framework. No groundtruth enhancement operation is provided}. 

\noindent\textbf{Test Data 1}:
We keep 100 random images from the standard test partition of AVA (denoted as ) for evaluation.

\noindent\textbf{Test Data 2}:
The \textbf{MIT-Adobe FiveK Dataset}~\cite{bychkovsky2011learning} contains 5,000 images, each is enhanced by 5 experts towards personal quality improvement. The standard test partition of the FiveK dataset as in~\cite{bychkovsky2011learning} is used in for evaluation.
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/cvpr_fig3.pdf}
\end{center}
\vskip -0.3cm
\caption{Visual results on the 5-image evaluation set in~\cite{lee2016automatic}. Column 1: input image; Column 2: output by the state-of-the-art exemplar-based method in Lee \etal~\cite{lee2016automatic}; Column 3: output by the piecewise color enhancer. Column 4: output by the deep filtering-based enhancer. The average aesthetic scores as evaluated by~\cite{deng2017image} are shown below. (Best viewed in color. More results in the supplementary material.)}
\label{fig:linzhe}
\end{figure}
\\
\noindent\textbf{Implementation Details}:
\noindent The generator network \textit{G} in our EnhanceGAN is fully convolutional, allowing for arbitrary-sized input in CIELab color space. We set the image input size to be , resulting in 2048 feature maps with spatial size of  (see Fig.~\ref{fig:architecture}). We found K = 3 in Top-K averaging pooling (Fig.~\ref{fig:architecture}) to be robust in producing parameter candidates. Using a larger K value or global average pooling (\textit{i.e.,} selecting from a large number of parameter candidates) could potentially suffer from noisy parameter predictions, while max pooling only concerns one single prediction and is prone to error. We use RMSprop~\cite{tieleman2012lecture} with a learning rate  for the generator network and  for the discriminator network after pre-training. A batch size of 64 is used to train each of the image enhancement operators.


\begin{table}[t]
\centering
\caption{\color{black}Quantitative evaluation for color enhancement on the MIT-Adobe FiveK Dataset. Top1-Expert (Top2-Expert) is the best (second best) scores among five groundtruth images produced by the experts as in the dataset. Our weakly-supervised EnhanceGAN is reasonably competitive as we have not used any expert labels in this dataset.\color{black}
}
\setlength{\tabcolsep}{2.8pt}
\begin{tabular}{lcccc}
Methods                                               & \textit{AVA-net} & \textit{CUHK-net} & RANK~\cite{kong2016photo} & DAN~\cite{deng2017image} \\ \hline 
Original Input                                                & 0.651     &  0.247       & 0.467       & 0.444                                    \\
Photoshop-Auto                                             & 0.638        & 0.277         & 0.397          & 0.319                                     \\                                             
Top1-Expert                                                  & 0.730        & 0.397         & 0.482          & 0.532                                     \\
Top2-Expert                                                 & 0.675        & 0.319         & 0.471          & 0.480                                     \\
                                             &     &          &        &                                     \\
EnhanceGAN                                                     &            &             &               &                                                 \\ \hline
Piecewise                                 &      &       & 0.476      &                                   \\    
Deep Filtering                               & 0.728     & 0.433      &          & 0.453   \\          
\end{tabular}
\label{tab:quantitative_aesthetics2}
\vskip -0.6cm
\end{table}


\subsection{Evaluations}
\label{sec:quantitative}
\noindent\textbf{Image Aesthetic Assessment:}
Evaluating aesthetic quality of enhanced images quantitatively is non-trivial due to the subjective nature of this task. Inspired by~\cite{dai2017towards} that uses multiple evaluators that are trained discriminatively for assessing the quality of generated image captions, we also prepare multiple aesthetic evaluators for evaluation.
These evaluators are either elaboratively trained, \ie, AVA-Net and CUHK-Net (which have balanced accuracy of 89.1\% and 94.3\% on the CUHK-PQ dataset, respectively); or publicly available, \ie, the RANK~\cite{kong2016photo} and DAN~\cite{deng2017image}.

The results on test images are summarized respectively in Table~\ref{tab:quantitative_aesthetics1} and Table~\ref{tab:quantitative_aesthetics2}.
A higher aesthetic score suggests better aesthetic quality. 
Compared with the fully-supervised DSLR model~\cite{ignatov2017dslr}, our weakly-supervised EnhanceGAN has received consistently better quantitative ratings in terms of aesthetic scores by all aesthetic evaluators (see Table~\ref{tab:quantitative_aesthetics1}). This result is also consistent with the user study described next. Note that the aesthetic-based evaluation may not be fair to DSLR model~\cite{ignatov2017dslr} since their objective function is not to optimize aesthetic quality but focuses more on improving image sharpness, texture details and small color variations.
By contrast, our EnhanceGAN renders perceptually better aesthetic-driven color quality (see Fig.~\ref{fig:BigFig}). 
\color{black}
We further evaluate our EnhanceGAN on MIT-Adobe FiveK Dataset. We observe that our EnhanceGAN produces competitive results compared to the commercial baseline of Photoshop-Auto\footnote{Photoshop ``autoTone+autoContrast+autoColor''} and \textit{Top2-Expert} (see Table~\ref{tab:quantitative_aesthetics2}).
\color{black}
We also evaluate our EnhanceGAN on the 5-image evaluation set in~\cite{lee2016automatic}. As shown in Fig.~\ref{fig:linzhe}, our EnhanceGAN produces consistently more natural color enhancement results while the exemplar-based method~\cite{lee2016automatic} features a more aggressive change in image styles.
We also show an ablation test on different losses in Table~\ref{tab:quantitative_aesthetics1}. We use the piecewise color enhancer in this test. In general, all losses contribute to the performance of EnhanceGAN, with  playing the dominant role.
It is noteworthy that the deep filtering-based operator of EnhanceGAN performs much better than the random weights baseline, as shown in Table~\ref{tab:quantitative_aesthetics1} and Fig.~\ref{fig:BigFig}. The results suggest that EnhanceGAN learns meaningful parameters driven by image aesthetics.
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/Rebutal_Fig.pdf}
\end{center}
\vskip -0.4cm
\caption[]{Examples of image color palettes. (a) first row: Human, enhanced outputs show \textit{Split-complimentary} harmony patterns; (b) second row: Nature, enhanced outputs show \textit{Split-complimentary} or \textit{Triadic} harmony patterns (c) third row: Skyscape, enhanced outputs show \textit{Complimentary} harmony patterns. (Best viewed in color)}
\label{fig:color_harmony}
\end{figure}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/cvpr_fig4.pdf}
\end{center}
\vskip -0.4cm
\caption{The focus of attention as revealed by overlaying the softmax feature map (as in Fig.~\ref{fig:architecture}) onto the input image. (Best viewed in color.)}
\label{fig:cropping_response}
\vskip -0.4cm
\end{figure}
\vspace{0.1cm}
\\
\noindent\textbf{Automatic Image Cropping:} We conduct an additional experiment to demonstrate the extensibility of the proposed method.
We quantitatively evaluate the performance of our EnhanceGAN for image cropping on the popular CUHK Image Cropping Dataset~\cite{yan2013learning}, which contains 950 images and 3 sets of cropping groundtruth by 3 different annotators. 
We perform a 5-fold cross-validation test for all the supervised baselines. Note that these baselines are fine-tuned using the CUHK Image Cropping Dataset, while EnhanceGAN is \textit{NOT} finetuned with any groundtruth cropping labels. Despite the weakly-supervised nature of our approach, EnhanceGAN achieves competitive performance and even surpasses some methods with full supervision\footnote{\color{black}The non-parametric retrieval-based method by Chen \etal~\cite{chen2017learning} included groundtruth crops in their model candidates for crop selection/evaluation, which is not comparable to the reported benchmarks as in~\cite{yan2013learning,chen2017quantitative,deng2017image}.\color{black}}, as shown in Table~\ref{tab:cropping-results-cuhk}. 
\color{black} Still, it is unfair to directly compare EnhanceGAN to the fully-supervised cropping methods. Following the learning scheme in Deng et al.~\cite{deng2017image}, we show that our EnhanceGAN can be further finetuned (FT) towards state-of-the-art performance when groundtruth labels are available (see Table~\ref{tab:cropping-results-cuhk}).
\color{black}
We also observe that the cropping operator of EnhanceGAN has learned to be attentive to specific regions of the input that are relevant to the image content, hence producing reasonable crop-coordinate candidates resided on the corresponding neurons of the feature maps (see Fig.~\ref{fig:cropping_response}). 
\subsection{Color Harmony}
We spotted interesting patterns regarding the enhanced outputs by our proposed model. As shown in Figure~\ref{fig:color_harmony}, EnhanceGAN learns to produce color palettes that approximate certain color harmony schemes~\cite{cohen2006color}, such as \textit{Complementary}, \textit{Split-complementary} and \textit{Triadic} schemes. For example, for input images from human and nature categories, the color palettes from the enhanced outputs show a sign of approximately \textit{Split-complementary} or \textit{Triadic} schemes. For images from sky-and-seascape categories, the color palettes are enhanced towards the \textit{Complementary} scheme. This further shows that the aesthetic enhancement by our weakly-supervised EnhanceGAN is consistent with harmonic color patterns.




\subsection{User Study}
\label{sec:userstudy}
The subjective nature of image enhancement evaluation also calls for a validation through human survey. For the purpose of the user study, we asked a professional editor to enhance each of the 100 images in  in \textit{Adobe Photoshop}. Image editing options including the tools ``Levels'', ``Curves'', ``Auto Tone'',  ``Auto Contrast'' and ``Auto Color'' in \textit{Adobe Photoshop} were available to the professional editor. All enhanced images were stored using sRGB JPEG-format with the highest quality (Quality = 12 in \textit{Adobe Photoshop}). We wrote a ranking software and distributed to a total of \totaluser~participants.
All participants were shown a sequence of 100 image sets, where each image set contained a random ordering of the input image, the image enhanced by the piecewise color enhancer, the image enhanced by the deep filtering-based enhancer, \color{black}the Photoshop-Auto output, \color{black} and the human edited image. Participants were instructed to rank each set by clicking the best-quality image, the excellent-quality image, the good-quality image, the average-quality image, and the poor-quality image on the screen in order. No time constraints were placed.

The results of our user study are shown in Fig.~\ref{fig:userstudy}. Each image in  received \totaluser~ratings, where we assign ``best quality'', \color{black}``excellent quality''\color{black},``good quality'', ``average quality'' and ``poor quality'' to aesthetic scores of  and , respectively. We observe that among the images ranked as the ``best quality'' or ``excellent quality'', the majority of them are from the EnhanceGAN outputs and the human editing. Our piecewise color enhancer and deep filtering-based enhancer obtain mean aesthetic scores of  and , matching  for human edited images and surpassing scores for Photoshop-Auto and the original inputs. Some of the images enhanced by EnhanceGAN receive even higher voting than the ones produced by the professional editor, as shown in Fig.~\ref{fig:visual_results}. The results demonstrate the effectiveness of the proposed EnhanceGAN for automatic image enhancement and confirm our quantitative evaluation results as in Sec.~\ref{sec:quantitative}.
\begin{table}
\centering
\caption{Quantitative evaluation on CUHK Image Cropping Dataset~\cite{yan2013learning}. The first number is average overlap ratio, higher is better. The second number (shown in parenthesis) is average boundary displacement error, lower is better~\cite{yan2013learning}. \color{black}Our EnhanceGAN is by itself competitive compared to weakly-supervised methods, and can be further finetuned (FT) towards state-of-the-art performance\color{black}.}
\color{black}
\setlength{\tabcolsep}{3.0 pt}
\begin{tabular}{lrrrrr}
Full supervision    & \small{Photographer1} &  \small{Photographer2} & \small{Photographer3} \\ \hline
Park \etal \cite{park2012modeling}                           & 0.603 (0.106)      &  0.582 (0.113) & 0.609 (0.110)         \\
Yan \etal \cite{yan2013learning}                              & 0.749 (0.067)        &  0.729 (0.072) & 0.732 (0.072)         \\
\color{black} A2-RL \cite{li2017a2}                                            & 0.793 (0.054)           & 0.791 (0.055) & 0.783 (0.055) \color{black} \\
Deng \etal \cite{deng2017image}                            & 0.806 (0.031)          & 0.775 (0.038) & 0.773 (0.038)  \\
EnhanceGAN (FT)                                                 &           &  &   \\
& & & \\
Weak supervision     & & & \\ \hline
Chen \etal    \cite{chen2017quantitative}                 & 0.664 (0.092)      &  0.656 (0.095) & 0.644 (0.099)         \\
EnhanceGAN                                                 &         &   &          \\
\end{tabular}
\label{tab:cropping-results-cuhk}
\vskip -0.4cm
\end{table}
\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/cvpr_fig_survey_new.pdf}
\end{center}
\vskip -0.4cm
\caption{User study on . Our EnhanceGAN shows competitive performance as compared to human editing.}
\label{fig:userstudy}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/7_fig_visual_7.pdf}
\includegraphics[width=\linewidth]{figures/8_fig_visual_8.pdf}
\includegraphics[width=\linewidth]{figures/17_fig_visual_17.pdf}
\includegraphics[width=\linewidth]{figures/20_fig_visual_20.pdf}
\includegraphics[width=\linewidth]{figures/30_fig_visual_30.pdf}
\includegraphics[width=\linewidth]{figures/33_fig_visual_33.pdf}
\caption{Visual results from our user study. From left to right: (a) Original image; (b) Photoshop-Auto (c) EnhanceGAN - piecewise color enhancer; (d) EnhanceGAN - deep filtering-based enhancer; (e) Human editing. The box plot below shows the ranking for each image, and the amount of dots denotes the number of users who gives a particular rank as in Poor, Average, Good, Excellent, Best. We show the complete results in the supplementary material.}
\label{fig:visual_results}
\end{figure}


\section{Conclusion} 
We have introduced EnhanceGAN for automatic image enhancement. Unlike most existing approaches that require well-aligned and paired images for training, EnhanceGAN only requires weak supervision in the form of binary label on aesthetic quality. We have demonstrated its capability in learning different enhancement operators in an aesthetic-driven manner. 
EnhanceGAN is fully-differentiable and can be trained end-to-end.
We have quantitatively evaluated the performance of EnhanceGAN, and through a user study, we have shown that the high-quality results produced by EnhanceGAN are on par with professional editing.

\bibliographystyle{ACM-Reference-Format}
\bibliography{danny_cvpr18}

\end{document}
