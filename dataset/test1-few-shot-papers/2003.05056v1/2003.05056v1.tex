\documentclass[journal]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{xcolor}
\usepackage{times}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage[nice]{nicefrac}
\usepackage{multirow}
\def\NoNumber#1{{\def\alglinenumber##1{}\State#1}\addtocounter{ALG@line}{-1}}






\begin{document}
\title{Multi-level Context Gating of Embedded Collective Knowledge for Medical Image Segmentation}

\author{Maryam~Asadi-Aghbolaghi, Reza~Azad, Mahmood~Fathy, and~Sergio~Escalera
\thanks{The first two authors contributed equally.}
\thanks{This work is partially supported by the Spanish project TIN2016-74946-P (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPU used for this research. This work is partially supported by ICREA under the ICREA Academia programme.}
\thanks{M. Asadi-Aghbolaghi, and M. Fathy are with the School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran, (e-mail:masadi@ipm.ir, mahfathy@ipm.ir).} 
\thanks{R. Azad is with the Department of Computer Engineering, Sharif University of Technology, Tehran, Iran, (e-mail: rezazad68@gmail.com).}
\thanks{S. Escalera is with the Universitat de Barcelona and Computer Vision Center, Barcelona, Spain. (email: sergio@maia.ub.es)}}

\maketitle

\begin{abstract}
Medical image segmentation has been very challenging due to the large variation of anatomy across different cases. Recent advances in deep learning frameworks have exhibited faster and more accurate performance in image segmentation. Among the existing networks, U-Net has been successfully applied on medical image segmentation.
In this paper, we propose an extension of U-Net for medical image segmentation, in which we take full advantages of U-Net, Squeeze and Excitation (SE) block, bi-directional ConvLSTM (BConvLSTM), and the mechanism of dense convolutions. 
(I) We improve the segmentation performance by utilizing SE modules within the U-Net, with a minor effect on model complexity. These blocks adaptively recalibrate the channel-wise feature responses by utilizing a self-gating mechanism of the global information embedding of the feature maps. 
(II) To strengthen feature propagation and encourage feature reuse, we use densely connected convolutions in the last convolutional layer of the encoding path. (III) Instead of a simple concatenation in the skip connection of U-Net, we employ BConvLSTM in all levels of the network to combine the feature maps extracted from the corresponding encoding path and the previous decoding up-convolutional layer in a non-linear way. 
The proposed model is evaluated on six datasets DRIVE, ISIC 2017 and 2018, lung segmentation, , and cell nuclei segmentation, achieving state-of-the-art performance.

\end{abstract}

\begin{IEEEkeywords}
BConvLSTM, Dense Convolution, Medical Image Segmentation, Squeeze and Excitation, U-Net .
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}


\IEEEPARstart{M}{edical} images play a key role in medical treatment and diagnosis. 
The goal of Computer-Aided Diagnosis (CAD) systems is providing doctors with more precise interpretation of medical images to follow-up of many diseases and have better treatment of a large number of patients. Moreover, accurate and reliable processing of medical images results in reducing the time, cost, and error of human-based processing. A critical step in numerous medical imaging studies is image segmentation. Medical image segmentation is the process of partitioning an image into multiple meaningful regions. Due to the complex geometry and inherent noise value of medical images, segmentation of these images is difficult. 
Interest in medical image segmentation has grown considerably in the last few years. This is due in part to the large number of application domains, like segmentation of blood vessel, skin cancer, lung, and cell nuclei (Figure \ref{fig:app}). 

For instance, segmentation of blood vessels will help to detect and treat many diseases that influence the blood vessels. Width and curves of retinal blood vessel show some symptoms about many diseases. Early diagnosis of many sight-threatening diseases is vital since lots of these diseases like glaucoma, hypertension and diabetic retinopathy cause blindness among working age people. 
Skin lesion segmentation helps to detect and diagnosis the skin cancer in the early stage. One of the most deadly form of skin cancer is melanoma, which is the result of unusual growth of melanocytes. Dermoscopy, captured by the light magnifying device and immersion fluid, is a non-invasive imaging technique providing with a visualization of the skin surface. The detection of melanoma in dermoscopic images by the dermatologists may be inaccurate or subjective. If melanoma is detected in its early stages, the five-year relative survival rate is  \cite{siegel2018jemal}. 




\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{all_intro.pdf}
\caption{Different applications of medical image segmentation.} \label{fig:app}
\vspace*{-\baselineskip}
\end{figure}








 



The first vital step of pulmonary image analysis is identifying the boundaries of lung from surrounding thoracic tissue on CT images, called lung segmentation. It can also be applied to lung cancer segmentation. 
Another application of medical image segmentation is cell nuclei segmentation. All known biological lives include a fundamental unit called cell. By segmentation of nuclei in different situations, we can understand the role and function of the nucleus and the DNA contained in cell in various treatments.



Deep learning networks achieve outstanding results and use to outperform non-deep state-of-the-art methods in medical imaging. 
These networks require a large amount of data to train and provide a good generalization behavior given the huge number of network parameters. A critical issue in medical image segmentation is the unavailability of large (and annotated) datasets. In medical image segmentation, per pixel labeling is required instead of image level label. 
Fully convolutional neural network (FCN) \cite{long2015fully} was one of the first deep networks applied to image segmentation.

Ronneberger et al. \cite{ronneberger2015} extended this architecture to U-Net, achieving good segmentation results leveraging the need of a large amount of training data. Their network consists of encoding and decoding paths. In the encoding path a large number of feature maps with reduced dimensionality are extracted. The decoding path is used to produce segmentation maps (with the same size as the input) by performing up-convolutions.
Many extensions of U-Net have been proposed so far \cite{alom2018,oktay2018,azad2019bi}.
In some of them, the extracted feature maps in the skip connection are first fed to a processing step (e.g. attention gates \cite{oktay2018}) and then concatenated. The main drawback of these networks is that the processing step is performed individually for the two sets of feature maps, and these features are then simply concatenated. 



In this paper,  we propose \textit{Multi-level Context Gating U-Net} (MCGU-Net)
 an extended version of the U-Net, by including BConvLSTM \cite{song2018pyramid} in the skip connection, using SE mechanism in the decoding path, and reusing feature maps with densely convolutions. A VGG backbone is employed in the encoding path to make it possible to use pre-trained weights on large datasets. The feature maps from the corresponding encoding layer have higher resolution while the feature maps extracted from the previous up-convolutional layer contain more semantic information. Instead of a simple concatenation, combining these two kinds of features with non-linear functions in all levels of the network may result in more precise segmentation. Therefore, in this paper we extend the U-Net architecture by adding multi-level BConvLSTM in the skip connection. 


Inspired by the effectiveness of the recently proposed squeeze and excitation modules \cite{hu2018squeeze} on image classification, we modify the U-Net by inserting these blocks in the decoding path. SE modules allow the network to  recalibrate the feature map to have more attention on useful channels by assigning different weights to various channels of feature maps based on to their relationship by employing a context gating mechanism. By using global embedding information, these modules help the network to boost informative and meaningful features, while suppressing weak ones. 
Having a sequence of convolutional layers may help the network to learn more kinds of features; however, in many cases, the network learns redundant features. To mitigate this problem and enhance information flow through the network, we utilize the idea of densely connected convolutions \cite{huang2017densely}. In the last layer of the contracting path, convolutional blocks are connected to all subsequent blocks in that layer via channel-wise concatenation. This strategy helps the method to learn a diverse set of features based on the “collective knowledge” gained by previous layers. Furthermore, we accelerate the convergence speed of the network by employing BN after the up-convolution filters.






We evaluate the proposed MCGU-Net on four different applications retinal blood vessel segmentation (DRIVE dataset), Skin lesion segmentation (three datasets of , ISIC 2017 and 2018), lung nodule segmentation (Lung dataset), and cell nuclei segmentation (Data Science Bowl 2018). The experimental results demonstrate that the proposed network achieves superior performance than state-of-the-art alternatives. \footnote{Source code is available on https://github.com/rezazad68/BCDU-Net.}

\section{Related work}
During the last few years, deep learning-based approaches have outstandingly improved the performance of classical image segmentation strategies. Based on the exploited deep architecture, we divide these approaches into three groups. 

\subsection{Convolutional Neural Network (CNN)}




Cui et al. \cite{cui2016} exploited CNN for automatic segmentation of brain MRI images. The authors first divided the input images into some patches and then utilized these patches for training CNN. To handle an arbitrary number of modalities as the input data, Kleesiek et al. \cite{kleesiek2016} proposed a 3D CNN for brain lesion segmentation. To process MRI data, the network consists of four channels: non-enhanced and contrast-enhanced T1w, T2w and FLAIR contrasts. Roth et al. \cite{roth2015deeporgan} proposed a multi-level deep convolutional networks for pancreas segmentation in abdominal CT scans as a probabilistic bottom-up approach.


\subsection{Fully Convolutional Network (FCN)}
A problem of the CNN models for segmentation is that the spatial information of the image is lost when the convolutional features are fed into the fc layers. To overcome this problem the FCN was proposed \cite{long2015fully}. This network is trained end-to-end and pixels-to-pixels, in which all fc layers of the CNN architecture are replaced with convolutional and deconvolutional to keep the original spatial resolutions. Zhou et al. \cite{zhou2016three} exploited FCN for segmentation of anatomical structures on 3D CT images. An FCN with convolution and de-convolution parts is trained end-to-end, performing voxel-wise multiple-class classification to map each voxel in a CT image to an anatomical label. Drozdzal et al. \cite{drozdzal2016} proposed very deep FCN by using short skip connections. The authors showed that a very deep FCN with both long and short skip connections achieved better result than the original one.
Roth et al. \cite{roth2018application} proposed to employ 3D FCN in a cascaded fashion for segmentation of the organs and vessels in CT images.


U-Net, \cite{ronneberger2015}, is one of the most popular FCNs for medical image segmentation. It has some advantages than the other segmentation-based networks \cite{alom2018}. It works well with few training samples and the network is able to utilize the global location and context information at the same time.
Milletari et al. \cite{milletari2016} proposed V-Net, a 3D extension version of U-Net to predict segmentation of a given volume at once. V-Net is an end-to-end 3D image segmentation network based on a volumetric (MRI volumes). 3D U-Net \cite{cciccek20163d} is proposed for processing 3D volumes instead of 2D images. In which, all 2D operations of U-Net are replaced with their 3D counterparts. 
In \cite{kayalibay2017}, the authors combine multiple segmentation maps that are created at different scales. Moreover, to forward feature maps from one stage of the network to the other one, element-wise summation is utilized.
A dual pathway 3D CNN (with 11 layers) \cite{kamnitsas2017} was proposed for brain lesion segmentation in multi-modal brain MRI. In this model, input images at multiple scales are fed simultaneously to a FCN. 
Li et al. proposed High-Res3DNet \cite{li2017compactness}, which is a high-resolution, compact convolutional network for volumetric image segmentation. 








\subsection{Recurrent Neural Network (RNN)}
One of the most used neural networks for processing a sequence is RNN, which can take into account the temporal data using recurrent connections in hidden layers. It has been successfully applied for modeling short- and long-temporal sequences. These networks are able to model the global contexts and improve semantic segmentation.Different RNN based deep network have been proposed for semantic segmentation. 
Pinheiro et al. \cite{pinheiro2014} proposed a deep network consisting of an RNN that can take into account long range label dependencies in the scenes while limiting the capacity of the model. Visin et al. \cite{visin2016reseg} proposed ReSeg for semantic segmentation. In that network, the input images are processed with a pre-trained VGG-16 model and its resulting feature maps are then fed into one or more ReNet layers. DeepLab architecture \cite{chen2017deeplab} contains a deep convolutional neural network in which all fully connected layers are replaced by convolutional layers and then the feature resolution is increased through atrous convolutional layers.
Alom et al. \cite{alom2018} proposed Recurrent Convolutional Neural Network (RCNN) and Recurrent Residual Convolutional Neural Network (R2CNN) based on U-Net models for medical image segmentation. Gao \cite{gao2018fully} proposed an end to end combination of FCN and RNN with long short-term memory (LSTM) units for 4D segmentation of MRI images.

He et al. \cite{hu2018squeeze} introduced the Squeeze and Excitation (SE) network for image classification which models the explicit relationship between the channels of a feature map. In these modules, the convolutional features are first passed through a squeeze operation in which global average pooling is exploited to produce channel descriptor. The output of the aggregation is then fed to an excitation operation to generate a set of per-channel modulation weights. These weights are utilized to recalibrate the feature map to emphasize on useful channels. 




In this paper, MCGU-Net is proposed as an extension of U-Net, showing better performance than state-of-the-art alternatives. The BConvLSTM is employed in the skip connection to combine features from contracting and expanding paths to learn more discriminative information. The dense convolutions help the network to learn more diverse features. Moreover, BN, utilized in the network, has a significant effect on the convergence speed of the network. In addition, the SE modules are exploited in the decoding path to extract more useful information by considering the interdependecies between channels of features. It is worth mentioning SE blocks are utilized in our network in a different ways than other approaches \cite{rundo2019use,zhu2018anatomynet}. Zhu et al. \cite{zhu2018anatomynet} employed SE residual block in the encoding path, and Rundo et al. utilized these blocks before the concatenation of skip connections while these blocks are inserted in the decoding path of our network.


\section{Proposed Method}
Inspired by U-Net \cite{ronneberger2015}, BConvLSTM \cite{song2018pyramid}, SENet \cite{hu2018squeeze} and dense convolutions \cite{huang2017densely}, we propose the MCGU-Net (Figure \ref{fig:Model}). We detail all parts of the network in the next sub sections.



\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{Model.pdf}
\caption{MCGU-Net with bi-directional ConvLSTM in the skip connections, SE modlues in the decoding path, and densely connected convolution.} \label{fig:Model}
\vspace*{-\baselineskip}
\end{figure*}


\subsection{Encoding Path}
The U-Net consists of a contracting path to extract hierarchically semantic features from the input and capture context information. To improve the performance of the U-Net we utilize the idea of transfer learning by exploiting a pre-trained CNN of VGG family as the encoder \cite{van2014transfer}. To train a complex model with a huge amount of parameters, a large dataset is necessary. However, gathering a vast number of labeled data is very tough. On the other hand, deep learning models are mostly focused on a specific task.  
To overcome the isolated learning paradigm, the idea of transfer learning has been proposed, which leverage knowledge from pre-trained models and use it to solve new problem, which may have less data.  Inspiring by this idea, we design the encoding path like the first four layers of VGG-16 to make it possible to use pre-trained models.
The first two layers includes  two convolutional  filters followed by a  max pooling and ReLU. The number of convolutional filters in the third layer is three with the same filter size followed by the same pooling and ReLU. The number of feature maps are doubled at each step. 





The original U-Net contains a sequence of convolutional layers in the last step of encoding path.
Having a sequence of convolutional layers in a network yields the method learn different kinds of features. Nevertheless, the network might learn redundant features in the successive convolutions. To mitigate this problem, densely connected convolutions are proposed \cite{huang2017densely}. This helps the network to improve its performance by the idea of ``collective knowledge" in which the feature maps are reused through the network. It means feature maps learned from all previous convolutional layers are concatenated with the feature map learned from the current layer and then are forwarded to use as the input to the next convolution.

The idea of densely connected convolutions has some advantages over the regular one \cite{huang2017densely}. First, it helps the network to learn a diverse set of feature maps instead of redundant ones. Moreover, this idea improves the network's representational power by allowing information flow through the network. Furthermore, dense connected convolutions can benefit from all the produced features before it (i.e., collecting knowledge), which prompt the network to avoid the risk of exploding or vanishing gradients. In addition, the gradients are sent to their respective places in the network more quickly in the backward path. 
We employ this idea in the proposed network. To do that, we introduce one block as two consecutive convolutions. There are a sequence of  blocks in the last convolutional layer of the encoding path. These blocks are densely connected. We consider  as the output of the  convolutional block. The input of the  () convolutional block receives the concatenation of the feature maps of all preceding convolutional blocks as its input, i.e.,  , and the output of the  block is . In the remaining part of the paper we use simply  instead of .

  





\subsection{Decoding Path}
Each step in the decoding path starts with an up-sampling function over the output of the previous layer. To improve the representation power of the network, decoding path of the original U-Net is augmented with two important modules of SE block and BConvLSTM. 
SE yield the network to use global information to selectively empathize informative features and suppress less useful ones. This block receives the output of the up-sampling function, which is a collection of feature maps, and encourages the feature maps to be more informative using a weight for each channel based on the interdependencies between all channels. 
The output of the SE module is then passed to an up-sampling function. In the standard U-Net, the corresponding feature maps in the contracting path are concatenated with the output of the up-sampling function. In the proposed network, we employ BConvLSTM to combine these two kinds of feature maps. The output of the BConvLSTM is then fed to a set functions including two convolutional functions, one SE module, and another convolutional filter. A diagram illustrating the structure of the combination of these modules in our network is shown in Figure \ref{fig:BConvLSTM}.

Assume that the set of extracted feature maps from the previous layer in decoding path is  where  is the number of feature maps at layer , and  is the size of each feature map at layer . We have , , and . For simplicity we consider . As it can be seen in Figure \ref{fig:BConvLSTM}, this set of feature maps is first passed through an up-convolutional layer in which an up-sampling function followed by a  convolution are applied, doubling the size of each feature map and halving the number of channels, i.e., producing  . In other words, the expanding path increases the size of the feature maps layer by layer to reach the original size of the input image after the final layer.





\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{SEBConv.pdf}
\caption{BConvLSTM with SE block in the decoding path of MCGU-Net.} \label{fig:BConvLSTM}
\vspace*{-\baselineskip}
\end{figure}


\subsubsection{Squeeze and Excitation Module}
Capturing spatial correlations between features has improved the performance of deep networks, like Inception architectures \cite{szegedy2015going} and spatial attention \cite{jaderberg2015spatial}. 
However, the network produces the same attention to the channels when creating the output feature maps.
The SE mechanism \cite{hu2018squeeze} is proposed to capture explicit relationship between channels of the convolutional layers by a context gating mechanism, which results in improving the representation power of the network. These modules encode feature maps by assigning a weight for each channel (i.e. channel attention) in the feature map. 


The SE block includes two parts squeeze and excitation. The first operation is squeeze. The input feature maps to SE block are aggregated to generate channel descriptor by employing global average pooling (GAP) of the whole context of channels. We have  , where , as the input data to the SE block. The spatial squeeze (GAP) is performed as




\noindent where  are the spatial squeeze function,  is a spatial location of the  channel, and  is the size of this channel. In other words, each two-dimensional feature map is compressed by a global average pooling to produce . 
To capture the channel-wise dependencies, the global information embedded in the first step is then fed to the second step, i.e., Excitation. This function must be able to learn non-mutually-exclusive relationship and nonlinear interaction between channels \cite{hu2018squeeze}. As it is illustrated in Figure \ref{fig:BConvLSTM}, the excitation step consists of two fully connected (FC) layers. The pooled vector is first encoded to shape , and then encoded again to shape  to generate excitation vector as 




\noindent where  is the parameters of the first fc layer, ,  is ReLU, and the  refers to the sigmoid activation. Moreover,  is the reduction ratio. In \cite{hu2018squeeze}, to limit model complexity and aid generalization, a dimensionality-reduction layer with reduction ratio  is used in the first fc layer. In the second fc layer a dimensionality-increasing layer is utilized to set the dimension to the channel one of the transformation output. 
The output of the SE block is generated as . In which , and  is a channel-wise multiplication between the channel attention, the scale factor , and the input feature map.













\subsubsection{Batch Normalization}
After up-sampling,  goes through a BN function and produces . A problem in the intermediate layers in training step is that the distribution of the activations varies. This problem makes the training process very slow since each layer in every training step has to learn to adapt themselves to a new distribution. BN \cite{ioffe2015batch} is utilized to increase the stability of a neural network, which standardizes the inputs to a layer in the network by subtracting the batch mean and dividing by the batch standard deviation. BN affectedly accelerates the speed of training process of a neural network. Moreover, in some cases the performance of the model is improved thanks to the modest regularization effect. More details can be found in \cite{ioffe2015batch}.

\subsubsection{Bi-Directional ConvLSTM}
The output of the BN step () is now fed to a BConvLSTM layer. 
The main disadvantage of the standard LSTM is that these networks does not take into account the spatial correlation since these models use full connections in input-to-state and state-to-state transitions. To solve this problem, ConvLSTM \cite{xingjian2015} was proposed which exploited convolution operations into input-to-state and state-to-state transitions. It consists of an input gate , an output gate , a forget gate , and a memory cell . Input, output and forget gates act as controlling gates to access, update, and clear memory cell. ConvLSTM can be formulated as follows (for convenience we remove the subscript and superscript from the parameters):




\noindent where  and  denote the convolution and Hadamard functions, respectively.  is the input tensor (in our case  and ),  is the hidden sate tensor,  is the memory cell tensor, and,  and  are 2D Convolution kernels corresponding to the input and hidden state, respectively, and , , , and  are the bias terms.





In this network, we employ BConvLSTM \cite{song2018pyramid} to encode  and . BConvLSTM uses two ConvLSTMs to process the input data into two directions of forward and backward paths, and then makes a decision for the current input by dealing with the data dependencies in both directions. In a standard ConvLSTM, only the dependencies of the forward direction are processed. However, all the information in a sequence should be fully considered, therefore, it might be effective to take into account backward dependencies. 
It has been proved that analyzing both forward and backward temporal perspectives enhanced the predictive performance \cite{cui2018deep}.
Each of the forward and backward ConvLSTM can be considered as a standard one. Therefore, we have two sets of parameters for backward and forward states. 
The output of the BConvLSTM is calculated as . 
In which  and  denote the hidden sate tensors for forward and backward states, respectively,  is the bias term, and  indicates the final output considering bidirectional spatio-temporal information. Moreover,  is the hyperbolic tangent which is utilized here to combine the output of both forward and backward states through a non-linear way. We utilize the energy function like the original U-Net to train the network. 














\section{Experimental result}
We evaluate MCGU-Net on six datasets of: DRIVE, ISIC 2017, ISIC 2018, a lung segmentation benchmark, , and a cell nuclei segmentation dataset. The empirical results show that the proposed method outperforms state-of-the-art alternatives for all six datasets. Keras with TenserFlow backend is utilized for implementation. We consider several performance metrics to perform the experimental comparative, including accuracy (AC), sensitivity (SE), specificity (SP), F1-Score, Jaccard similarity (JS), and area under the curve (AUC). We stop the training of the network when the validation loss remains the same in 10 consecutive epochs.





\subsection{DRIVE Dataset}

DRIVE \cite{staal2004} is a dataset for blood vessel segmentation from retina images. It includes 40 color retina images, from which 20 samples are used for training and the remaining 20 samples for testing. The original size of images is  pixels. It is clear that a dataset with this number of samples is not sufficient for training a deep network. Therefore, we use the same strategy as \cite{alom2018} for training our network. The input images are first randomly divided into a number of patches (). In total, around  patches are produced from  training images, from which  patches are used for training, and the remaining  patches are used for validation. 



Some precise and promising segmentation results of the proposed network are shown in Figure \ref{fig:DRIVE_R}. Table \ref{tab:drive} lists the quantitative results obtained by other methods and the proposed network on DRIVE dataset. We evaluate the network with  and  as the number of dense blocks. With  we have one convolutional block without any dense connection in that layer. With  we have three convolutional blocks and two dense connections in that layer. 
It is shown that the MCGU-Net (with both  and ) outperforms w.r.t. the state-of-the-art alternatives for most of the evaluation metrics. Moreover, it can be seen that the network with  works better than . It is worth mentioning that for this dataset, we achieved better result by training the network from scratch rather than using pre-trained weights.

\begin{figure}
\centering
\includegraphics[width=0.38\textwidth]{Retina_Res.pdf}
\caption{Segmentation result of MCGU-Net on DRIVE.} 
\vspace*{-0.5\baselineskip}
\label{fig:DRIVE_R}
\end{figure}




\begin{table}
\centering
    \vspace*{-\baselineskip}
\caption{Performance comparison on DRIVE dataset.}
	\begin{tabular}{cccccc}
		\hline
		\textbf{Methods} & \textbf{F1}&	\textbf{SE}&	\textbf{SP}&	\textbf{AC}&	\textbf{AUC}\\
		\hline
U-net \cite{ronneberger2015} & 0.8142&	0.7537&	0.9820&	0.9531&	0.9755\\
		Deep Model \cite{liskowski2016} & -&	0.7763&	0.9768&	0.9495&	0.9720\\
		Att U-net \cite{oktay2018} & 0.8155&	0.7751&	0.9816&	0.9556&	0.9782\\
		RU-net \cite{alom2018} & 0.8149&	0.7726&	0.\textbf{9820}&	0.9553&	0.9779\\
		R2U-Net \cite{alom2018} &0.8171& 0.7792&	0.9813&	0.9556&	0.9782\\
\hline
		\textbf{MCGU-Net (d=1)} & 0.8222& \textbf{0.8012}& 0.9784 &0.9559 & 0.9788\\
		\textbf{MCGU-Net (d=3)} & \textbf{0.8224} &0.8007& \textbf{0.9786}&	\textbf{0.9560}& \textbf{0.9789}\\
		\hline
	\end{tabular}
	\label{tab:drive}
\end{table}




To ensure the proper convergence of the proposed network, the training and validation accuracy for DRIVE dataset is shown in Figure \ref{fig:converge} (a). It is shown that the network converges very fast, i.e., after the  epoch. We also can see that in the first 15 epochs the validation accuracy is larger than the training one. This fact is mostly because of the small size of dataset since we use a small set of images as the validation set. Moreover, it might be related to the fact that we evaluate the validation set at the end of epoch. To show the overall performance of the MCGU-Net on DRIVE dataset, ROC curves is shown in Figure \ref{fig:ROCs} (a). ROC is the plot of the true positive rate (TPR) against the false positive rate (FPR). 



\begin{figure}[ht]
	\centering
	\begin{tabular}{cc}
\includegraphics[width=0.22\textwidth,height=25mm]{Retina_ACC.pdf}&
		\includegraphics[width=0.22\textwidth,height=25mm]{ISIC17_ACC.pdf} \\
		(a) DRIVE, & (b) ISIC 2017, \\
		\includegraphics[width=0.22\textwidth,height=25mm]{ISIC18_ACC.pdf} &
		\includegraphics[width=0.22\textwidth,height=25mm]{Lung_ACC.pdf} \\
		(c) ISIC 2018,&  (d) Lung Segmentation,  \\
		\includegraphics[width=0.22\textwidth,height=25mm]{Ph_ACC.pdf} &
		\includegraphics[width=0.22\textwidth,height=25mm]{Nuclei_ACC.pdf}  \\
		 (e) ,  & (f) Cell Nuclei Dataset\\
	\end{tabular}
	\caption{Training and validation accuracy of MCGU-Net for six datasets.}
	\vspace*{-\baselineskip}
	\label{fig:converge}
\end{figure}



















\begin{figure}[ht]
	\centering
	\begin{tabular}{cc}
\includegraphics[width=0.22\textwidth,height=25mm]{Retina_ROC.pdf}&
		\includegraphics[width=0.22\textwidth,height=25mm]{ISIC17_ROC.pdf}\\
		(a) DRIVE, & (b) ISIC 2017, \\
		\includegraphics[width=0.22\textwidth,height=25mm]{ISIC18_ROC.pdf} &
		\includegraphics[width=0.22\textwidth,height=25mm]{Lung_ROC.pdf}\\
		(c) ISIC 2018, & (d) Lung Segmentation,\\
		\includegraphics[width=0.22\textwidth,height=25mm]{Ph_ROC.pdf}&
		\includegraphics[width=0.215\textwidth,height=25mm]{Nuclei_ROC.pdf} \\
		 (e) , & (f) Cell Nuclei Dataset\\
	\end{tabular}
	\caption{ROC diagrams of the proposed MCGU-Net for six dataset.}
	\vspace*{-\baselineskip}
	\label{fig:ROCs}
\end{figure}



\subsection{ISIC 2017 Dataset}
The ISIC 2017 dataset \cite{codella2018skin} is taken from the Kaggle competition. We evaluate the proposed method on the provided data for skin lesion segmentation. This dataset provides 2000 skin lesion images as a training set with masks (containing cancer or non-cancer lesions) for segmentation. We use 1250 samples for training, 150 samples as validation data, and the other 600 samples for test. The original size of each sample is . We resize images to . For this dataset, we train the network with pre-trained weights on imageNet. Since the input data is RGB images, the pre-trained weights are good initialization for the network.

Figure \ref{fig:skin_R}(a) shows some segmentation results of the proposed network on ISIC 2017. In Table \ref{tab:isic17}, the results of the MCGU-Net on this dataset are compared with the state-of-the-art approaches. It can be seen that MCGU-Net with both  and  achieves better results (except sensitivity) than the other approaches. Moreover, the result of MCGU-Net with three dense blocks is a bit higher than with one dense block. 
The training and validation accuracy of the proposed network for this dataset is shown in Figure \ref{fig:converge} (b). The network converges very fast for this data (after the  epoch). To show the overall performance of the MCGU-Net on ISIC 2017 dataset, the ROC curves are shown in Figure \ref{fig:ROCs} (b).




\subsection{ISIC 2018 Dataset}
This dataset \cite{codella2019skin} was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images in 2018. It includes 2594 images where like previous approaches\cite{alom2018}, we used 1815 images for training, 259 for validation and 520 for testing.
The original size of each sample is . We resize images to . The training data consists of the original images and corresponding ground truth annotations (containing cancer or non-cancer lesions). Like the ISIC 2017 dataset, the proposed network works better with pre-trained weights. 
For qualitative analysis, Figure \ref{fig:skin_R}(b) shows some example outputs of the proposed MCGU-Net on ISIC 2018. 
Table \ref{tab:isic18} lists the quantitative results obtained by different methods and the proposed network on this dataset. A large improvement is achieved by the MCGU-Net (with both  and ) w.r.t. state-of-the-art alternatives for all of the evaluation metrics. It is clear that the network with  works better than the one with .
It is worth mentioning that there was a challenge on ISIC dataset and the best result achieved by the participants was . Compare to this result, there is a good gap between the  achieved by the MCGU-Net () and the best result of the ISIC challenge.


The training and validation accuracy of the proposed network for ISIC dataset is shown in Figure \ref{fig:converge} (c). The convergence speed of the network for ISIC dataset is fast (after 40 epochs). The validation accuracy over the training process is variable. The reason behind this fact is that the validation set contains some images totally different from the ones in training set, therefore, during the first learning iterations the model has some problems about segmenting those images. To show the overall performance of the MCGU-Net on ISIC dataset, the ROC curves are shown in Figure \ref{fig:ROCs} (c).







\begin{figure}[ht]
	\centering
	\begin{tabular}{ccc}
\includegraphics[width=0.14\textwidth,height=25mm]{Results_isic17.pdf}&
		\includegraphics[width=0.14\textwidth,height=25mm]{Results_isic18.pdf}&
		\includegraphics[width=0.14\textwidth,height=25mm]{Results_ph2.pdf}\\
		(a) ISIC 2017, & (b) ISIC 2018,& (c) 
	\end{tabular}
	\caption{Segmentation result of MCGU-Net on three datasets.}
	\vspace*{-\baselineskip}
	\label{fig:skin_R}
\end{figure}
















\begin{table}
\centering
\caption{Performance comparison on ISIC 2017 dataset.}
	\begin{tabular}{cccccc}
		\hline
		\textbf{Methods} & \textbf{F1}&	\textbf{SE}&	\textbf{SP}&	\textbf{AC}&	\textbf{JS}\\
		\hline
		U-net  \cite{ronneberger2015} & 0.8682 & 0.9479&0.9263&	0.9314&		0.9314\\
		Melanoma det.  \cite{codella2018skin} & -&-&	-&	o.9340	& -\\
		Lesion Analysis  \cite{li2018skin} & - & 0.8250&	0.9750&	0.9340 &		- \\
		R2U-net  \cite{alom2018} & 0.8920 & \textbf{0.9414}&	0.9425 &	0.9424&		0.9421\\
\hline
		\textbf{MCGU-Net (d=1)}& 0.8871  & 0.8305  & \textbf{0.9888} & 0.9555  &  0.9555 \\
		\textbf{MCGU-Net (d=3)}& \textbf{ 0.8927}& \textbf{0.8502}& 0.9855&	\textbf{0.9570}&  \textbf{0.9570}\\
		\hline
	\end{tabular}
	\label{tab:isic17}
\end{table}

\begin{table}
\centering
\caption{Performance comparison on ISIC 2018 dataset.}
	\begin{tabular}{ccccccc}
		\hline
		\textbf{Methods} & \textbf{F1}&	\textbf{SE}&	\textbf{SP}&	\textbf{AC}&\textbf{PC}&	\textbf{JS}\\
		\hline
		U-net  \cite{ronneberger2015} & 0.647 & 0.708 &	0.964 & 0.890&	 0.779& 0.549\\
		Att U-net  \cite{oktay2018} & 0.665&	0.717 &	0.967&	 0.897 &	0.787&	 0.566\\
		R2U-net  \cite{alom2018} & 0.679 &	0.792 & 0.928& 0.880& 0.741& 0.581\\
		Att R2U-Net \cite{alom2018} &0.691&	0.726&	0.971&	0.904&	0.822&	 0.592\\
\hline
		\textbf{MCGU-Net (d=1)}& 0.889 & 0.845 &  0.984 & 0.952 & 0.938 & 0.952\\
		\textbf{MCGU-Net (d=3)}& \textbf{0.895}& \textbf{0.848}& \textbf{0.986}&	\textbf{0.955}& \textbf{0.947}& \textbf{0.955}\\
		\hline
	\end{tabular}
	\label{tab:isic18}
\end{table}


\subsection{Lung Segmentation Dataset}
A lung segmentation dataset is introduced in the Lung Nodule Analysis (LUNA) competition at the Kaggle Data Science Bowl in 2017. This dataset consists of 2D and 3D CT images with respective label images for lung segmentation \cite{lungdata}. We use  of the data as the train set and the remaining  as the test set. The size of each image is . For this dataset, the MCGU-Net works better with training from scratch since the input data is entirely different from images in ImageNet dataset.
Since the lung region in CT images have almost the same Hausdorff value with non-object of interests such as bone and air, it is worth to learn lung region by learning its surrounding tissues. To do that first we extract the surrounding region by applying algorithm \ref{Alg:1} and then make a new mask for the training sets. We train the model on these new masks and on the testing phase,and estimate the lung region as a region inside the estimated surrounding tissues. 

Figure \ref{fig:Lung_R} shows some segmentation outputs of the proposed network for lung dataset. The quantitative results of the proposed MCGU-Net is compared with other methods in Table \ref{tab:lung}. It is clear that the MCGU-Net (with both  and ) outperforms the other methods. Moreover, the network with dense connections works better. The training and validation accuracy for this dataset is shown in Figure \ref{fig:converge} (d). To show the overall performance of the network on this dataset, ROC curves is shown in Figure \ref{fig:ROCs} (d).








 
 
 









\begin{table}
\centering
    \vspace*{-\baselineskip}
\caption{Performance comparison on Lung dataset.}
	\begin{tabular}{cccccc}
		\hline
		\textbf{Methods} & \textbf{F1}&	\textbf{SE}&	\textbf{SP}&	\textbf{AC}&	\textbf{JS}\\
		\hline
		U-net  \cite{ronneberger2015} & 0.9658 & 0.9696&0.9872&	0.9872&		0.9858 \\
		RU-net  \cite{alom2018} &0.9638&	 0.9734&	0.9866&	0.9836&		0.9836 \\
		R2U-Net \cite{alom2018} &0.9832&	\textbf{0.9944}&	0.9832&	0.9918&		0.9918 \\
\hline
\textbf{MCGU-Net (d=1)}& 0.9889& 0.9901&0.9979&	0.9967&0.9967\\
		\textbf{MCGU-Net (d=3)}& \textbf{0.9904} & \textbf{0.9910}& \textbf{0.9982}& \textbf{0.9972}&  \textbf{0.9972}\\
		\hline
	\end{tabular}
	\label{tab:lung}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{Lung_Res.pdf}
	\caption{Segmentation result of MCGU-Net on Lung dataset.} 
	\vspace*{-0.5\baselineskip}
	\label{fig:Lung_R}
\end{figure}

 
\begin{algorithm}
	\caption{Pre-processing over lung dataset.}
	\label{Alg:1}
	\begin{algorithmic}[1]
		\STATE Input =  and \\
		\\
		
		\STATE Output = 
		\STATE  \\
		\\
		\COMMENT{Remove bones and vessels}
		\STATE 
		\COMMENT{Normalize X}
		\STATE 
		\COMMENT{Convert to binary}
		\STATE 
		\STATE 
		\COMMENT{Remove noise}
		\STATE 
	\end{algorithmic}
\end{algorithm}



\subsection{ Dataset}
The  dataset \cite{mendoncca2013ph} is a a dermoscopic image database proposed for segmentation and classification. It contains a total number of 200 melanocytic lesions, including 80 common nevi, 80 atypical nevi, and 40 melanomas. The manual segmentations of the skin lesions are availablee. Each input image is a 8-bit RGB color images with a resolution of  pixels. There are not a pre-defined test and train sets for this dataset. We follow the experimental setting used in \cite{liu2019enhanced}. We randomly split the dataset into two sets of 100 images, and then use one set as the test data,  of the other set for the train, and the remained data for the validation. Since the number of data is not enough for training the network, we exploit the learnt weights of ISIC 2017 as the pre-trained model (like \cite{liu2019enhanced}) and then finetune the network with train data. 

Some segmentation outputs of the proposed network for  dataset are depicted in Figure \ref{fig:skin_R}(c). In Table \ref{tab:ph}, the results of the proposed network are compared with other state-of-the-art approaches. We can see the MCGU-Net results in better performance than other methods. The network has almost the same performance for both  and . The reason behind this fact is the small size of training data since the network with  contains fewer parameters for learning. The training and validation accuracy for this dataset is shown in Figure \ref{fig:converge} (e). The network converges very fast ( epoch) which might be related to the small size of data. The ROC curve is shown in Figure \ref{fig:ROCs} (e).







 \begin{table}
\centering
    \vspace*{-\baselineskip}
\caption{Performance comparison on  dataset.}
	\begin{tabular}{cccccc}
		\hline
		\textbf{Methods} & \textbf{DIC}&	\textbf{SE}&	\textbf{SP}&	\textbf{AC}&	\textbf{JS}\\
		\hline
		FCN \cite{noh2015learning} &  0.8903 & 0.9030 & 0.9402  & 0.9282 & 0.8022  \\
		U-net  \cite{ronneberger2015} & 0.8761  & 0.8163 & 0.9776 &	0.9255 &	0.7795 \\
		SegNet \cite{badrinarayanan2017segnet} & 0.8936 &	 0.8653 &	0.9661&	0.9336 &	0.8077  \\
	    FrCN \cite{al2018skin} & 0.9177&	\textbf{0.9372}   &	0.9565&	0.9508 &	0.8479 \\
		\hline
\textbf{MCGU-Net (d=1)}& 0.9762  &  \textbf{0.8727} & \textbf{0.9925} &	 0.9536 & 0.9536 \\
		\textbf{MCGU-Net (d=3)}& \textbf{0.9763 } & 0.8322 & 0.9714 & \textbf{ 0.9537}& \textbf{0.9537 }\\
		\hline
	\end{tabular}
	\label{tab:ph}
\end{table}

\subsection{Cell Nuclei Dataset}
We evaluate the proposed network on the dataset from 2018 Kaggle Data Science Bowl 2018 \cite{bowl2018}. This data is captured with various situations, like different cell type, illumination status, and image size. Moreover, this dataset contains smaller regions inside images for segmentation for which we want to evaluate the performance of the MCGU-Net. It includes a total number of 670 images. We randomly split the data into  training,  validation, and  test data sets.
Figure \ref{fig:Nuclei_R} shows some segmentation outputs of MCGU-Net. In Table \ref{tab:Nuclei}, the performance of the proposed method is compared with other approaches. We can see there is a high gap between the results of the MCGU-Net and other methods. The network works better with  for this data. The training and validation accuracy for this dataset is shown in Figure \ref{fig:converge} (f). Since the validation and training data are taken from the same set (and validation set is smaller than train set), the validation accuracy is a bit higher than the training. The ROC curve is shown in Figure \ref{fig:ROCs} (f).
 
 
 \begin{figure}
 \centering
 \includegraphics[width=0.4\textwidth]{Nuclei_Res.pdf}
 \caption{Segmentation result of MCGU-Net on cell nuclei Dataset.} 
 \vspace*{-0.5\baselineskip}
 \label{fig:Nuclei_R}
 \end{figure}
 




\begin{table}
\centering
    \vspace*{-\baselineskip}
\caption{Performance comparison on Cell Nuclei dataset.}
	\begin{tabular}{cccccc}
		\hline
		\textbf{Methods} & \textbf{F1}&	\textbf{DIC}&	\textbf{AC}&	\textbf{JS}\\
		\hline
		U-net  \cite{ronneberger2015}  &  0.8994 & 0.9077 & 0.9604  & 0.8310  \\
	    Att U-Net \cite{oktay2018} & 0.8899    & 0.8879 & 	0.9672 &	0.7984 \\
		FocusNet \cite{kaul2019focusnet} & 0.8998 &	 0.8996 &	0.9697 &	0.8176   \\
	    \hline
		\textbf{MCGU-Net (d=1)}& 0.9295  &  0.9882 & 0.9766 &	 0.9766\\
		\textbf{MCGU-Net (d=3)}& \textbf{0.9306} & \textbf{0.9884}& \textbf{0.9771 }& \textbf{ 0.9771}\\
		\hline
	\end{tabular}
	\label{tab:Nuclei}
\end{table}

\subsection{Discussion}
The proposed network has some modifications from the original U-Net. We evaluate each modified part of the network to analyze its influence on the result.

We included BN after each up-convolutional layer to speed up the network learning process. To evaluate the effect of this function, we train the network with and without BN. BN yields the network to converge  times faster. BN manages the variations among data by standardizing data through controlling the mean and variance of distributions of inputs which results in a small regularization and reducing generalization error. 



The last convolutional layer of the encoding path is augmented with dense blocks. The results for the network with  and  dense blocks are reported for all datasets. In Tables \ref{tab:drive} to \ref{tab:Nuclei}, it can be seen that MCGU-Net with  dense block results in better performance.
The key idea of dense convolutions is collecting knowledge by sharing feature maps between blocks through direct connection between convolutional block. Consequently, each dense block receives all preceding layers as input, and therefore, produces more diversified and richer features. Thus, it helps the network to increase the representational power of deeper models.
We have more feature propagation both in backward and forward paths through dense blocks. The network performs a kind of deep supervision in backward path since dense block receives additional supervision from loss function through shorter connections \cite{huang2017densely}. The error signal is propagated to earlier layers more directly, hence, earlier layers can get direct supervision from the final softmax layer, and moreover, it results in decreasing the vanishing-gradient problem. 
In addition, compared to other deep architectures like residual connections, dense convolutions require fewer parameters while improving the accuracy of the network.




In the proposed network, we used multi-level BConvLSTMs to combine encoded and decoded features. The encoded features have higher resolution and therefore contain more local information of the input image, while the decoded features have more semantic information about the input images. The affection of these two features over each other might result in a set of feature maps rich in both local and semantic information. Therefore, instead of a simple concatenation, we utilize BConvLSTM to combine the encoded and decoded features. In BConvLSTM, a set of convolution filters are applied on each kind of features. Therefore each ConvLSTM state, corresponds to one kind of features (e.g. encoded features), ia able to encode relevant information about the other kind of features (e.g. decoded features). The convolutional filters along with the hyperbolic tangent functions help the network to learn complex data structures.
Figure \ref{figConvLSTM} shows the output segmentation mask of the original U-Net and MCGU-Net for two samples of the ISIC 2018 dataset. It shows a more precise and fine segmentation output of the proposed network. 

\begin{figure}
\centering
	\includegraphics[width=0.4\textwidth]{Discussion_UNet.pdf}
	\caption{Visual effect of BConvLSTM in MCGU-Net . From left column 1 input, 2 GT mask, 3 and 4 are the outputs of network without and with ConvLSTM.} 
	 \vspace*{-\baselineskip}
	\label{figConvLSTM}
\end{figure}





In Figure \ref{figSE}, we compare the segmentation output of the MCGU-Net with and without SE blocks for two samples of ISIC 2018 dataset, which demonstrates the power of SE features on semantic segmentation. It can be seen that the SE blocks help the network to produce more precise output by a context gating mechanism. To do that, this block exploits the global information embedded features in different channels and assign different channel attentions. The quality of the segmentation output of a network relies on effective feature learning. These findings reveal that the adaptive feature recalibration of SE blocks result in boosting the representational power of deep networks by focusing on informative features and suppressing weak ones. 

\begin{figure}
\centering
	\includegraphics[width=0.4\textwidth]{Discussion_BCDUNet.pdf}
	\caption{Visual effect of SE blocks in MCGU-Net.  From left column 1 input, 2 GT mask, 3 and 4 are the outputs of network without and with SE blocks. } 
	 \vspace*{-\baselineskip}
	\label{figSE}
\end{figure}












\section{Conclusion}

We proposed MCGU-Net for medical image segmentation. We showed that by including multi-level BConvLSTM in the skip connection, SE blocks in decoding path, inserting a densely connected convolutional blocks, and also employing SE blocks in decoding path, the network is able to capture more discriminative information which resulted in more precise segmentation results. Moreover, we were able to speed up the network by utilizing BN after the up-convolutional layer. The experimental results on six public benchmark datasets showed high gain in semantic segmentation in relation to state-of-the-art alternatives. 


\begin{thebibliography}{00}

\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{siegel2018jemal}
R.~L. Siegel and K.~D. Miller, ``Jemal a (2018) cancer statistics,'' \emph{Ca
  Cancer J Clin}, vol.~68, no.~1, pp. 7--30, 2018.

\bibitem{long2015fully}
J.~Long, E.~Shelhamer, and T.~Darrell, ``Fully convolutional networks for
  semantic segmentation,'' in \emph{CVPR}, 2015, pp. 3431--3440.

\bibitem{ronneberger2015}
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for
  biomedical image segmentation,'' in \emph{International Conference on
  MICCAI}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2015, pp. 234--241.

\bibitem{alom2018}
M.~Z. Alom, M.~Hasan, C.~Yakopcic, T.~M. Taha, and V.~K. Asari, ``Recurrent
  residual convolutional neural network based on u-net (r2u-net) for medical
  image segmentation,'' \emph{arXiv preprint arXiv:1802.06955}, 2018.

\bibitem{oktay2018}
O.~Oktay \emph{et~al.}, ``Attention u-net:
  Learning where to look for the pancreas,'' \emph{arXiv preprint
  arXiv:1804.03999}, 2018.

\bibitem{azad2019bi}
R.~Azad, M.~Asadi-Aghbolaghi, M.~Fathy, and S.~Escalera, ``Bi-directional
  convlstm u-net with densley connected convolutions,'' in \emph{Proceedings of
  the CVPRW}, 2019, pp. 0--0.

\bibitem{song2018pyramid}
H.~Song, W.~Wang, S.~Zhao, J.~Shen, and K.-M. Lam, ``Pyramid dilated deeper
  convlstm for video salient object detection,'' in \emph{Proceedings of the
  ECCV}, 2018, pp. 715--731.

\bibitem{hu2018squeeze}
J.~Hu, L.~Shen, and G.~Sun, ``Squeeze-and-excitation networks,'' in
  \emph{Proceedings of the CVPR}, 2018, pp. 7132--7141.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger, ``Densely connected
  convolutional networks,'' in \emph{Proceedings of the CVPR}, 2017, pp.
  4700--4708.

\bibitem{cui2016}
Z.~Cui, J.~Yang, and Y.~Qiao, ``Brain mri segmentation with patch-based cnn
  approach,'' in \emph{2016 35th Chinese Control Conference (CCC)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2016, pp. 7026--7031.

\bibitem{kleesiek2016}
J.~Kleesiek \emph{et~al.}, ``Deep mri brain extraction: a 3d convolutional neural network for
  skull stripping,'' \emph{NeuroImage}, vol. 129, pp. 460--469, 2016.

\bibitem{roth2015deeporgan}
H.~R. Roth \emph{et~al.}, ``Deeporgan: Multi-level deep convolutional networks for automated
  pancreas segmentation,'' in \emph{International conference on MICCAI}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer, 2015, pp. 556--564.

\bibitem{zhou2016three}
X.~Zhou, T.~Ito, R.~Takayama, S.~Wang, T.~Hara, and H.~Fujita,
  ``Three-dimensional ct image segmentation by combining 2d fully convolutional
  network with 3d majority voting,'' in \emph{Deep Learning and Data Labeling
  for Medical Applications}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2016, pp. 111--120.

\bibitem{drozdzal2016}
M.~Drozdzal, E.~Vorontsov, G.~Chartrand, S.~Kadoury, and C.~Pal, ``The
  importance of skip connections in biomedical image segmentation,'' in
  \emph{Deep Learning and Data Labeling for Medical Applications}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2016, pp. 179--187.

\bibitem{roth2018application}
H.~R. Roth \emph{et~al.}, ``An application of cascaded 3d fully
  convolutional networks for medical image segmentation,'' \emph{Computerized
  Medical Imaging and Graphics}, vol.~66, pp. 90--99, 2018.

\bibitem{milletari2016}
F.~Milletari, N.~Navab, and S.-A. Ahmadi, ``V-net: Fully convolutional neural
  networks for volumetric medical image segmentation,'' in \emph{2016 Fourth
  International Conference on 3D Vision (3DV)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2016, pp. 565--571.

\bibitem{cciccek20163d}
{\"O}.~{\c{C}}i{\c{c}}ek, A.~Abdulkadir, S.~S. Lienkamp, T.~Brox, and
  O.~Ronneberger, ``3d u-net: learning dense volumetric segmentation from
  sparse annotation,'' in \emph{International conference on MICCAI}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2016, pp. 424--432.

\bibitem{kayalibay2017}
B.~Kayalibay, G.~Jensen, and P.~van~der Smagt, ``Cnn-based segmentation of
  medical imaging data,'' \emph{arXiv preprint arXiv:1701.03056}, 2017.

\bibitem{kamnitsas2017}
K.~Kamnitsas \emph{et~al.}, ``Efficient multi-scale 3d cnn with fully
  connected crf for accurate brain lesion segmentation,'' \emph{Medical image
  analysis}, vol.~36, pp. 61--78, 2017.

\bibitem{li2017compactness}
W.~Li, G.~Wang, L.~Fidon, S.~Ourselin, M.~J. Cardoso, and T.~Vercauteren, ``On
  the compactness, efficiency, and representation of 3d convolutional networks:
  brain parcellation as a pretext task,'' in \emph{International Conference on
  Information Processing in Medical Imaging}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2017, pp. 348--360.

\bibitem{pinheiro2014}
P.~O. Pinheiro and R.~Collobert, ``Recurrent convolutional neural networks for
  scene labeling,'' Tech. Rep., 2014.

\bibitem{visin2016reseg}
F.~Visin \emph{et~al.}, ``Reseg: A recurrent neural network-based model for
  semantic segmentation,'' in \emph{Proceedings of the IEEE CVPRW}, 2016, pp.
  41--48.

\bibitem{chen2017deeplab}
L.-C. Chen, G.~Papandreou, I.~Kokkinos, K.~Murphy, and A.~L. Yuille, ``Deeplab:
  Semantic image segmentation with deep convolutional nets, atrous convolution,
  and fully connected crfs,'' \emph{IEEE transactions on PAMI}, vol.~40, no.~4,
  pp. 834--848, 2017.

\bibitem{gao2018fully}
Y.~Gao, J.~M. Phillips, Y.~Zheng, R.~Min, P.~T. Fletcher, and G.~Gerig, ``Fully
  convolutional structured lstm networks for joint 4d medical image
  segmentation,'' in \emph{2018 IEEE 15th ISBI 2018}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2018, pp. 1104--1108.

\bibitem{rundo2019use}
L.~Rundo \emph{et~al.}, ``Use-net: incorporating
  squeeze-and-excitation blocks into u-net for prostate zonal segmentation of
  multi-institutional mri datasets,'' \emph{arXiv preprint arXiv:1904.08254},
  2019.

\bibitem{zhu2018anatomynet}
W.~Zhu  \emph{et~al.}, ``Anatomynet:
  Deep 3d squeeze-and-excitation u-nets for fast and fully automated
  whole-volume anatomical segmentation,'' \emph{bioRxiv}, p. 392969, 2018.

\bibitem{van2014transfer}
A.~Van~Opbroek, M.~A. Ikram, M.~W. Vernooij, and M.~De~Bruijne, ``Transfer
  learning improves supervised image segmentation across imaging protocols,''
  \emph{IEEE transactions on medical imaging}, vol.~34, no.~5, pp. 1018--1030,
  2014.

\bibitem{szegedy2015going}
C.~Szegedy \emph{et~al.},, ``Going deeper with convolutions,'' in
  \emph{Proceedings of the CVPR}, 2015, pp. 1--9.

\bibitem{jaderberg2015spatial}
M.~Jaderberg, K.~Simonyan, A.~Zisserman, ``Spatial transformer
  networks,'' in \emph{Advances in neural information processing systems},
  2015, pp. 2017--2025.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' pp. 448--456, 2015.

\bibitem{xingjian2015}
S.~Xingjian, Z.~Chen, H.~Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo,
  ``Convolutional lstm network: A machine learning approach for precipitation
  nowcasting,'' in \emph{Advances in neural information processing systems},
  2015, pp. 802--810.

\bibitem{cui2018deep}
Z.~Cui, R.~Ke, and Y.~Wang, ``Deep bidirectional and unidirectional lstm
  recurrent neural network for network-wide traffic speed prediction,''
  \emph{arXiv preprint arXiv:1801.02143}, 2018.

\bibitem{staal2004}
J.~Staal, M.~D. Abr{\`a}moff, M.~Niemeijer, M.~A. Viergever, and
  B.~Van~Ginneken, ``Ridge-based vessel segmentation in color images of the
  retina,'' \emph{IEEE transactions on medical imaging}, vol.~23, no.~4, pp.
  501--509, 2004.

\bibitem{liskowski2016}
P.~Liskowski and K.~Krawiec, ``Segmenting retinal blood vessels with deep
  neural networks,'' \emph{IEEE transactions on medical imaging}, vol.~35,
  no.~11, pp. 2369--2380, 2016.

\bibitem{codella2018skin}
N.~C. Codella \emph{et~al.}, ``Skin lesion
  analysis toward melanoma detection: A challenge at the 2017 international
  symposium on biomedical imaging (isbi), hosted by the international skin
  imaging collaboration (isic),'' in \emph{2018 IEEE 15th ISBI 2018}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 168--172.

\bibitem{codella2019skin}
N.~Codella \emph{et~al.}, ``Skin lesion
  analysis toward melanoma detection 2018: A challenge hosted by the
  international skin imaging collaboration (isic),'' \emph{arXiv preprint
  arXiv:1902.03368}, 2019.

\bibitem{li2018skin}
Y.~Li and L.~Shen, ``Skin lesion analysis towards melanoma detection using deep
  learning network,'' \emph{Sensors}, vol.~18, no.~2, p. 556, 2018.

\bibitem{lungdata}
\url{https://www.kaggle.com/kmader/finding-lungs-in-ct-data}.

\bibitem{mendoncca2013ph}
T.~Mendon{\c{c}}a, P.~M. Ferreira, J.~S. Marques, A.~R. Marcal, and J.~Rozeira,
  ``Ph 2-a dermoscopic image database for research and benchmarking,'' in
  \emph{2013 35th EMBC}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2013, pp.
  5437--5440.

\bibitem{liu2019enhanced}
X.~Liu, G.~Hu, X.~Ma, and H.~Kuang, ``An enhanced neural network based on deep
  metric learning for skin lesion segmentation,'' in \emph{2019 Chinese Control
  And Decision Conference (CCDC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2019, pp. 1633--1638.

\bibitem{noh2015learning}
H.~Noh, S.~Hong, and B.~Han, ``Learning deconvolution network for semantic
  segmentation,'' in \emph{Proceedings of the CVPR}, 2015, pp. 1520--1528.

\bibitem{badrinarayanan2017segnet}
V.~Badrinarayanan, A.~Kendall, and R.~Cipolla, ``Segnet: A deep convolutional
  encoder-decoder architecture for image segmentation,'' \emph{IEEE
  transactions on PAMI}, vol.~39, no.~12, pp. 2481--2495, 2017.

\bibitem{al2018skin}
M.~A. Al-Masni, M.~A. Al-antari, M.-T. Choi, S.-M. Han, and T.-S. Kim, ``Skin
  lesion segmentation in dermoscopy images via deep full resolution
  convolutional networks,'' \emph{Computer methods and programs in
  biomedicine}, vol. 162, pp. 221--231, 2018.

\bibitem{bowl2018}
``2018 data science bowl, find the nuclei in divergent images to advance
  medical discovery,'' \url{https://www.kaggle.com/c/data-science-bowl-2018},
  accessed Dec 2019.

\bibitem{kaul2019focusnet}
C.~Kaul, S.~Manandhar, and N.~Pears, ``Focusnet: an attention-based fully
  convolutional network for medical image segmentation,'' in \emph{2019 IEEE
  16th ISBI 2019}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp.
  455--458.

\end{thebibliography}


\end{document}
