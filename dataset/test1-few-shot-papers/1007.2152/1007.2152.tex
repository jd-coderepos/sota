\documentclass[letterpaper,11pt]{article}
\usepackage{fullpage}
\usepackage{mathtools,amsmath,amsthm, amssymb}
\usepackage{paralist}
\usepackage{algpseudocode,algorithm}







\vfuzz2pt \hfuzz2pt \newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{obs}[thm]{Observation}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{Claim}{Claim}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}

\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\N}{\mathbb N}
\newcommand{\F}{\mathbb F}
\newcommand{\E}{\mathbb E}
\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\VAR}{\mathrm{VAR}}
\newcommand{\ALG}{\mathrm{ALG}}
\newcommand{\TOP}{\mathrm{TOP}}
\newcommand{\Greedy}{\mathrm{Greedy}}
\newcommand{\LP}{\mathrm{LP}}
\newcommand{\IP}{\mathrm{IP}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\To}{\longrightarrow}
\newcommand{\M}{\mathcal M}
\newcommand{\PP}{\mathcal P}
\newcommand{\C}{\mathcal C}
\newcommand{\D}{\mathcal D}
\newcommand{\G}{\mathcal G}
\newcommand{\LL}{\mathcal L}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\spa}{span}
\DeclareMathOperator*{\Top}{Top}
\DeclareMathOperator*{\Next}{Next}
\DeclareMathOperator*{\Bin}{Bin}
\DeclareMathOperator*{\Prob}{Pr}
\DeclareMathOperator*{\original}{original}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}




\begin{document}
\title{Matroid Secretary Problem in the Random Assignment Model\footnote{This work was partially supported by NSF
  contract CCF-0829878 and by ONR grant N00014-05-1-0148.}}
\author{Jos\'e A. Soto\thanks{MIT,
Dept.~of Math., Cambridge, MA 02139. \texttt{jsoto@math.mit.edu}.}}
\date{}

\maketitle
\begin{abstract}
In the \emph{Matroid Secretary Problem}, introduced by Babaioff et al.~[SODA 2007], the elements of a given matroid are presented to an online algorithm in random order. When an element is revealed, the algorithm learns its weight and decides whether or not to select it under the restriction that the selected elements form an independent set in the matroid. The objective is to maximize the total weight of the chosen elements. In the most studied version of this problem, the algorithm has no information about the weights beforehand. We refer to this as the \emph{zero information model}. In this paper we study a different model, also proposed by Babaioff et al., in which the relative order of the weights is random in the matroid. To be precise, in the \emph{random assignment model}, an adversary selects a collection of weights that are randomly assigned to the elements of the matroid. Later, the elements are revealed to the algorithm in a random order independent of the assignment.

Our main result is the first constant competitive algorithm for the matroid secretary problem in the random assignment model. This solves an open question of Babaioff et al. Our algorithm achieves a competitive ratio of . It exploits the notion of \emph{principal partition} of a matroid, its decomposition into \emph{uniformly dense minors}, and a -competitive algorithm for uniformly dense matroids we also develop. As additional results, we present simple constant competitive algorithms in the zero information model for various classes of matroids including cographic, low density and the case when every element is in a small cocircuit. In the same model, we also give a -competitive algorithm for -column sparse linear matroids, and a new -competitive algorithm for general matroids of rank  which only uses the relative order of the weights seen and not their numerical value, as previously needed.
\end{abstract}
\thispagestyle{empty}

\newpage
\setcounter{page}{1}
\section{Introduction}
In the simplest form of the \emph{secretary problem}, an employer wants to select the best secretary among  candidates arriving in random order. Once a secretary is interviewed, the employer must decide immediately whether to accept the candidate or not and that decision is final. Lindley \cite{lindley_dynamic_1961} and Dynkin \cite{dynkin_optimum_1963} have shown that sampling the first  candidates and then selecting the first one whose value is higher than all the sampled ones gives a probability of at least  of selecting the best secretary and that no algorithm can beat this constant. An important generalization of this problem with many applications is known as the \emph{multiple choice secretary problem} (see \cite{kleinberg_multiple-choice_2005}). In this problem we wish to select a set of at most~ secretaries from a pool of  applicants and the objective is to select a group of combined value as high as possible.

Babaioff et al.~\cite{babaioff_online_2008} introduce the \emph{generalized secretary problem} as a natural class of extensions of the previous problem in which the set returned by the algorithm must obey some combinatorial restriction. In this setting, a finite set  with hidden nonnegative weights and a collection of subsets  closed under inclusion are given. The collection  describes the sets of elements that can be simultaneously accepted (these are the \emph{feasible sets} or the \emph{domain} of the problem). The elements of  are presented to an online algorithm in random order. When an element is revealed, the algorithm learns its weight and decides whether or not to accept it under the restriction that the set of accepted elements is feasible. This decision is irreversible and it must be taken before the next element is revealed. The objective is to output a feasible set of maximum total weight.

We remark that other lines of generalizations of the multiple choice secretary problem having different objective functions have also been considered. These generalizations include, among others, minimizing the sum of the relative ranks of the selected elements (studied by Ajtai et al.~\cite{ajtai_1995}), the weighted and time discounted secretary problems of Babaioff et al.~\cite{babaioff_secretary_2009}, the -choice -best secretary problem studied by Buchbinder et al.~\cite{buchbinder2010secretary} and the submodular secretary problem of Bateni et al.~\cite{bateni-submodular2010}.


The generalized secretary problem is of interest due to its connection to online auctions. In both the original and multiple choice secretary problems, we can regard the algorithm as an auctioneer having one or many identical items, and the secretaries as agents arriving at random times, each one having a different valuation for the item. The goal of the algorithm is to assign the items to the agents as they arrive while maximizing the total social welfare. In more complex situations, the algorithm may be considered to have access to a collection of goods that it wishes to assign to agents, subject to some restrictions. In many cases, these restrictions can be modeled by matroid constraints. For that reason, the \emph{matroid secretary problem}, in which the feasible sets are the independent sets of a matroid is of special interest (see e.g. a survey of Babaioff et al.~\cite{babaioff_online_2008}).



Notice that the difficulty of the problem changes depending on the information we know beforehand about the weights. We recognize four different models in increasing order of difficulty. \begin{compactitem}
  \item \textbf{Full information model:} The weights are chosen i.i.d. from a known distribution.
  \item \textbf{Partial information model:} The weights are chosen i.i.d. from an unknown distribution.
  \item \textbf{Random assignment model:} An adversary chooses a list of nonnegative weights, which are then assigned to the elements using a uniform random one-to-one correspondence, which is independent of the random order at which the elements are revealed.
  \item \textbf{Zero information model:} An adversary assigns the weights arbitrarily.
\end{compactitem}

The difficulty may also change depending on whether the algorithm learns the actual weight of the elements or just the relative order of the weights seen so far. See the surveys of Freeman~\cite{freeman1983secretary} and Ferguson \cite{ferguson_who_1989} for references and variations of these models in the classical secretary problem. Note that for both the classical problem and the multiple choice problem, the random assignment and the zero information models coincide.

There has been a significant amount of work on the matroid secretary problem under zero information. Constant competitive algorithms are known for partition matroids (this corresponds to the classical~\cite{lindley_dynamic_1961,dynkin_optimum_1963} and multiple choice secretary problem~\cite{kleinberg_multiple-choice_2005,babaioff_knapsack_2007}) and also for graphic and transversal matroids~\cite{babaioff_matroids_2007,dimitrov_competitive_2008,korula2009algorithms,babaioff_secretary_2009}. It is also known~\cite{babaioff_matroids_2007} that if a matroid admits a constant competitive algorithm under zero information, then so do its restrictions and truncations. For general matroids, the best algorithm known so far, due to Babaioff et al.~\cite{babaioff_matroids_2007}, is -competitive, where  is the rank of the matroid.

Non-matroidal domains have also been considered in the literature. Babaioff et al.~\cite{babaioff_knapsack_2007} show a -competitive algorithms for knapsack domains even in the case where both the weights and lengths are revealed online. Korula and Pal~\cite{korula2009algorithms} give constant competitive algorithms for some cases of intersection of partition matroids under zero information, specifically for matchings in graphs and hypergraphs where the edges have constant size.

Not every domain admits constant competitive algorithms. Babaioff et al. \cite{babaioff_matroids_2007} have shown a particular domain for which no algorithm has a competitive ratio smaller than  even in the full information model. However, matroid domains have the following special property: If we are allowed to reject elements which have been previously accepted, while keeping at every moment an independent set, then it is possible to output the optimum independent set no matter in which order the elements are presented. This intuition motivated Babaioff et al.~\cite{babaioff_matroids_2007, babaioff_online_2008} to conjecture that the matroid secretary problem admits a constant competitive algorithm even under zero information. According to these authors, this question is also non-trivial for the random assignment model, and for the model in which the \emph{order} the elements are presented is adversarial, but the weights are randomly assigned from a pool of hidden values.

\begin{paragraph}{Main Result}
In this paper, we answer the last question affirmatively for the random assignment model, exhibiting a -competitive algorithm for any matroid domain. We remark here that our results also apply to the partial and full information settings for which, as far as we know, no previous results existed. Our algorithm uses as building block an algorithm for uniformly dense matroids we also develop.

Uniformly dense matroids are matroids for which the \emph{density} of a set, that is, the ratio of its cardinality to its rank, is at most the density of the entire ground set. The simplest examples of these are precisely the uniform matroids. Uniformly dense matroids and uniform matroids of the same rank over the same ground set behave similarly, in the sense that the distribution of the rank of a random set is similar for both matroids. We use this fact to devise a -competitive algorithm for these matroids in the random assignment model. In order to extend this algorithm to general matroids we exploit some notions coming from the \emph{theory of principal partitions} of a matroid, particularly its \emph{principal sequence}. Roughly speaking, the principal sequence of a matroid  is a decomposition of its ground set into a sequence of parts, each of which is the underlying set of a uniformly dense minor of . By employing independently the previous algorithm in each of these minors, we obtain an algorithm that returns an independent set of , while loosing only an extra factor of  on its competitive ratio.
\end{paragraph}

\begin{paragraph}{Additional Results}
We also develop simple constant competitive algorithms under zero information for various classes of matroids including cographic, low density and the case when every element is in a small cocircuit. Also, we show a -competitive algorithm for the case when the matroid is representable by a matrix in which each column has at most  non-zero entries. This result generalizes the -competitive algorithm for graphic matroids of Korula and Pal~\cite{korula2009algorithms}. Finally, we give a new -competitive algorithm for general matroids. Unlike the previous algorithm of Babaioff et al.~\cite{babaioff_matroids_2007}, our algorithm does not use the numerical value of the weights. It only needs the ability to make comparisons among seen elements.
\end{paragraph}
\section{Matroid Secretary Problem in the Random Assignment Model}\label{section2}

In this paper, we assume familiarity with basic concepts in matroid theory. For an introduction and specific results, we refer to Oxley's book \cite{oxley2006matroid}.

Consider a matroid  with ground set . An adversary selects a set  of  nonnegative weights , which are assigned to the elements of the matroid via a random permutation , i.e., the weight function of the elements  is such that . The elements are then presented to an online algorithm via a random order . When an element is presented, the algorithm must decide whether to add it or not to the solution set under the condition that this set is independent in  at all times. The objective is to output a solution set  whose value  is as high as possible.

We further assume that when the -th element of the stream, , is presented, \emph{the algorithm only learns the relative order of the weight with respect to the previously seen ones}. This is, it can compare  with  for all , but it can not use the actual numerical values of the weights. Without loss of generality, we can assume that there are no ties in , because otherwise we can break them using a new random permutation ; if the comparison between two elements seen gives a tie, then we consider heavier the one having larger -value.

For a given permutation , let  be the the lexicographic first base of  according to the permutation . In other words,  is the set obtained by applying the \emph{greedy procedure} that selects an element if it can be added to the previously selected ones while preserving independence in , on the sequence . Standard matroid arguments imply that  is a maximum independent set with respect to any weight function  for which  In particular, this is true for the weight function  defined before. We will drop the subindex  in  whenever there is no possible confusion.

We say that an online algorithm returning an independent set  is -competitive if for any selection of nonnegative weights  given by the adversary, 


As a particular case, consider the partial and full information models in which elements receive their weights independently from a fixed distribution  over the nonnegative reals. Since it is possible for the expected weight of the optimum and the expected weight of the set returned by the algorithm to be both infinite (for instance, if  has infinite mean), the concept of competitiveness for this scenario has to be slightly modified. We say that an algorithm returning an independent set  is -competitive in the partial or full information models if .

We claim that any algorithm that is -competitive in the random assignment scenario is also -competitive in both the full and the partial information setting. To see this, consider an -competitive algorithm  under the former model, and apply to one of the latter. Note that the distribution of the maximum independent set is the same as the one obtained by first choosing a set  of  sample values from , and then assigning these values to the elements using a uniform random permutation~. For any realization of values of  according to , algorithm  returns a set  such that . By taking expectation over the realizations of  we prove the claim.

\section{Uniformly dense matroids}\label{section3}
Define the \emph{density}  of a \emph{loopless matroid}\footnote{A \emph{circuit} is a minimal non-independent set of a matroid. A \emph{loop} is an element  such that  is a circuit. A loopless matroid is a matroid having all singletons independent. }  with rank function , to be the maximum over all non-empty sets , of the quantity .
The matroid  is \emph{uniformly dense} if  is attained by the entire ground set; that is, if , for every non-empty . Examples of uniformly dense matroids include uniform matroids, the graphic matroid of a complete graph, and all projective geometries . The following property of uniformly dense matroids is important for our analysis.
\begin{lem}\label{lem:randomgreedy}
Let  be a sequence of different elements of a uniformly dense matroid chosen uniformly at random. The probability that element  is selected by the greedy procedure on that sequence is at least , where  is the rank of the matroid.
\end{lem}
\begin{proof}
An element is selected by the greedy procedure only if it is outside the span of the previous elements. Denote by  to the set of the first  elements of the sequence, and let  be the number of elements of the matroid, then:
  
  where the first inequality holds since the matroid is uniformly dense and the second holds because the rank of a set is always at most its cardinality.
\end{proof}

\subsection{A -competitive algorithm for uniformly dense matroids}

Consider the following algorithm for uniform matroids of rank  due to Babaioff et al.~\cite{babaioff_knapsack_2007}: Maintain the set  consisting on the  heaviest elements seen so far (initialize this set with  dummy elements that are considered to be lighter than everyone else). Observe the first  elements without adding them to the output; we refer to this set as the \emph{sample}. An element arriving later will be added to the output only if at the moment it is seen, it enters  and the element that leaves  is either in the sample or a dummy element. Babaioff et al.~have shown that this algorithm returns a set of at most  elements and that by setting  to be , every element of the optimum is in the output of the algorithm with probability at least , making this algorithm -competitive for \emph{uniform} matroids even under zero information.

A slight modification of this algorithm is at least -competitive for uniformly dense matroids in the random assignment model. The full procedure is depicted in Algorithm \ref{algorithm}. The only differences with respect to the algorithm above are that \begin{inparaenum}[(i)]\item The number of elements sampled is given by a binomial distribution  and \item Before adding an element to the output, we test if its addition maintains independence in the matroid\end{inparaenum}.

\begin{algorithm}[h!!!]
\caption{for uniformly dense matroids of  elements and rank  under random assignment.}

\label{algorithm}
\begin{algorithmic}[1]
\State{Maintain a set  containing the heaviest  elements seen so far at every moment (initialize  with  dummy elements that are supposed to be lighter than every element in the stream).}
\State{.}
\State{Choose  from the binomial distribution .}
\State{Observe the first  elements and denote this set as the sample.}
\For{each element  arriving after the first  elements}
\If{ enters  and the element leaving  is in the sample or is a dummy element}
\State{Check if  is independent. If so, add  to .}
\EndIf
\EndFor
\State{Return the set .}
\end{algorithmic}
\end{algorithm}

\begin{thm}\label{thm:algorithm1}
Let  be the set returned by Algorithm \ref{algorithm} when applied to a uniformly dense matroid  of rank . Then

In particular, by setting , we conclude that Algorithm \ref{algorithm} is -competitive for uniformly dense matroids in the random assignment model.
\end{thm}
\begin{proof}

Consider the following offline simulation algorithm. In the first part of the simulation, each weight  in the adversarial list  selects an arrival time  in  uniformly and independently. The algorithm keeps a set  containing the top  \emph{weights} seen at every moment (initially containing  dummy weights of negative value) and processes the weights as they arrive, sampling the ones arriving before time . When a weight arriving after time  is processed, the algorithm \emph{marks it as a candidate} if, first, that weight enters , and second, the one leaving  is either in the sample or a dummy weight.
In the second part of the simulation, the algorithm assigns to each candidate weight a different element of the matroid uniformly at random. Then, it runs the greedy procedure on the sequence of candidates in the order they appeared and returns its answer. Using that the cardinality of the sampled set has binomial distribution with parameters  and , it is not hard to check that the set of elements and weights returned by this simulation has the same distribution as the one returned by Algorithm \ref{algorithm}. For this reason, we focus on the simulation.

We estimate the probability that each one of the top  weights appears in the output set. Focus on one such weight , , and let  be a nonnegative integer strictly smaller than . Define  as the event that exactly  of the top  weights excluding  are sampled.  Given that event  occurs, the corresponding  high weights in the sample enter  as soon as they arrive and never leave it. Since every candidate pushes out either a dummy or a sampled weight of  at the moment it is marked, the previous implies that the number of candidates marked by the simulation algorithm is at most (in fact, exactly) . Event  occurs with probability .


\begin{Claim} For  and ,
.
\end{Claim}

\begin{proof}
  Since  is one of the top  weights, it enters set  at the time it is considered. Thus, it will be marked as a candidate if and only if the weight leaving  at that time is either a dummy or a sampled weight.

Let  be the set of weights seen before  arrives. If this set has less than  elements then the element leaving  at  will be a dummy weight. Consider the case where  has cardinality at least  and let  be the top -th element of this set. Since  is not one of the top  elements in the full adversarial list, its arrival time  is independent of . Therefore,

\end{proof}

The elements of the matroid assigned to the candidate weights form a random set. Conditioned on  and on  being a candidate, Lemma \ref{lem:randomgreedy} implies that no matter what position  takes in the list of at most  candidates, the probability that it gets added to the output is at least ; therefore, the probability that  appears in the output is at least

Theorem \ref{thm:algorithm1} follows easily from here.\qedhere
\end{proof}

We remark that Algorithm \ref{algorithm} does not need to learn the weights of the elements: the algorithm can proceed by only learning the relative order of the weights seen so far. Also, we note that this algorithm is not constant competitive in the zero information model. In fact, if we had such an algorithm  for uniformly dense matroids under zero information we could obtain one for general matroids by using that every matroid  is a restriction of a uniformly dense matroid ~\cite{lai1995every}. The algorithm for  would virtually complete the matroid  by adding a dummy set of zero weight elements and run algorithm  on , simulating the augmented input in such a way that the dummy elements arrive uniformly at random similarly to the real ones.

\section{Principal sequence and general matroids.}\label{section4}

In this section we  need the concept of \emph{principal sequence} of a matroid. This notion is related to the theory of \emph{principal partition} of graphs, matroids and submodular systems, which has applications to connectivity and reliability of networks and to resource allocation problems. The theory of principal partition is extensively analyzed in a monograph by Narayanan~\cite{narayanan1997submodular} and in a recent survey of Fujishige~\cite{fujishige2009theory}.
The definition of principal sequence of a matroid we present was introduced by several authors under different names (See, e.g., ~\cite{Narayanan74phd,tomizawa1976strongly,narayanan1981elementary}).

\begin{thm}[Principal Sequence]\label{thm:ppalsequence}
Let  be a loopless matroid with rank function . There is a sequence of sets  and a sequence of values  satisfying:
\begin{compactenum}
  \item The values  are the only ones for which the submodular set function  admits more than one minimizer.
  \item For every , the unique minimal and unique maximal minimizers of the function  are  and  respectively.
\end{compactenum}
The sequence  is called the \emph{principal sequence} of  and  is the associated sequence of critical values.
\end{thm}

From this definition, it is not hard to obtain the following lemma (it also follows from \cite[Theorem 3.11]{fujishige2009theory} or \cite{catlin1992fractional}).

\begin{lem}\label{lem:Density} Let  be a loopless matroid with principal sequence  and critical values . Then, for every , the matroid  obtained by contracting  and restricting to  is uniformly dense, with density . These matroids are known as the \emph{principal minors} of~.
\end{lem}

The principal sequence of  can be constructed by iteratively finding the maximal densest set  of the current matroid, adding it to the sequence and contracting it on the matroid, until all the elements have been contracted. Polynomial time algorithms to compute the principal sequence of a given matroid can be found in the literature (see, e.g. \cite{narayanan1981elementary} or \cite[chapters 10,11]{narayanan1997submodular}).

We use Lemma \ref{lem:Density} to design an algorithm for the matroid secretary problem under random assignment in a general (not necessarily loopless) matroid . Let  be the sequence of principal minors of the loopless matroid obtained by deleting the set  of loops from . For every , let  and  denote the ground set and rank of  respectively. Note that the family  is a partition of the ground set . Define matroids  and  with ground set  as follows:


Since any independent set in  is, by definition of each , also independent in , Algorithm~\ref{algorithm2}, described below returns an independent set of .

\begin{algorithm}[h!!!!!]
\caption{for General Matroids of  elements and rank  under random assignment.}
\label{algorithm2}
\begin{algorithmic}[1]
\State{Compute the sequence of principal minors  of the matroid obtained by removing the loops of .}
\State{Run Algorithm \ref{algorithm} in parallel on each  and return the union of the answers.}
\end{algorithmic}
\end{algorithm}

\begin{thm}\label{thm:algorithm2}
Algorithm \ref{algorithm2} is -competitive for general matroids in the random assignment model.
\end{thm}

To prove Theorem \ref{thm:algorithm2}, we compare the weight of the set  returned by Algorithm \ref{algorithm2} with the optimum of the partition matroid  defined above. Since both  and  are disjoint union of uniformly dense and uniform matroids over the same ground set and having the same rank, we expect them to behave similarly.
Observe that the random permutation  that is used to assign the weights of the adversary to the elements of the matroid can be viewed as the composition of a random partition of  into blocks of size , and a collection of random permutations inside each block. Conditioned on the random partition, each block  receives a hidden list of weights which are assigned uniformly at random to the elements of the block. Since each  is uniformly dense and the elements of  arrive in random order, Theorem \ref{thm:algorithm1} implies that Algorithm~\ref{algorithm} recovers, in expectation, at least -fraction of the \emph{combined weight of the heaviest  elements of }, where the expectation is over the random permutation of that particular block. Noting that the union of the heaviest  elements of each  is exactly the optimum of the partition matroid  defined above, we conclude, by removing the conditioning, that


To prove that Algorithm~\ref{algorithm2} is constant competitive we only need to show that the optimum of  is only a constant away from the optimum in .

\begin{lem}\label{lem:ineq2} 
\end{lem}

To prove this lemma we note the following fact. For all , let  denote the (random) set of the elements that receives the top  weights. Then, for any matroid:


In order to prove Lemma \ref{lem:ineq2}, we only need to show the following.

\begin{lem}\label{lem:expectedrank}
  For every , 
\end{lem}
\begin{proof}
Since  is a partition matroid with non-trivial parts , we have:


For each ,  is a uniform matroid with the same density  as the corresponding uniformly dense matroid . By Theorem \ref{thm:ppalsequence}, these densities are strictly decreasing with .
Call a part  \emph{dense} if  and \emph{sparse} otherwise. Intuitively, when a part  is dense we expect  to contain at least  elements and thus we expect the rank of  in the partition matroid to be close to . On the other hand, for sparse parts this quantity should be closer to . We formalize this intuition in the following claim.

\begin{Claim} If , then . If on the other hand, , then .
\end{Claim}

\begin{proof}
Focus on a part  and split its elements into  pieces as evenly as possible. To do this, let  and create  pieces of size  and  pieces of size . It is easy to see that both  and  are integers and that the previous construction is indeed a partition of  into  pieces as claimed (note that if  is an integer, this partition consists simply on  pieces of size ).

The rank of any set in  is at least as high as the number of pieces of  this set intersects; therefore,  is at least


The function  is increasing, thus if ,


On the other hand, the function  is decreasing, thus if ,
  
\end{proof}
Since  is a decreasing sequence, there is an index~ such that  is dense if and only if . Recall that  is equal to the set  in the principal sequence of the matroid . Since every set in the principal sequence has the same rank in both  and  we get:

\end{proof}

By combining Lemma \ref{lem:ineq2} with inequality \eqref{ineq} we conclude the proof of Theorem \ref{thm:algorithm2}.

\section{Algorithms for the zero information model}\label{section5}
We give various algorithms for different classes of matroids in the zero information model. In all of them we assume the matroid is \emph{loopless} (in the random assignment model we can not make this assumption since the introduction of loops changes the distribution of the weights of the elements).

\subsection{Cographic Matroids}
In any -edge-connected graph  we can find three spanning trees ,  and , such that the union of their complements covers  (This follows from e.g. Edmonds's Matroid Partitioning Theorem~\cite{edmonds1965minimum}). The sets  are bases in the cographic matroid of . Consider the following algorithm for this matroid. Select  uniformly at random and accept all elements in . Since every edge of  is selected with probability at least , this algorithm is -competitive.

We modify this algorithm to work on the cographic matroid  of any graph . First, remove all the bridges of  since they are loops in . Decompose the edge set of the remaining graph as the direct sum of -edge-connected components.  For each component , let  be the graph obtained by contracting all but one edge in each serial class of its corresponding graphic matroid.\footnote{Two elements are in series in a graphic matroid if and only if they are in parallel in the cographic matroid. A pair of elements  are in parallel in a matroid, if the set  is a circuit. Being in series and being in parallel are equivalence relations, so the serial and parallel class of an element are well defined.
Contracting all but one edge in each serial class of the graphic matroid corresponds to deleting all but one element in each parallel class of the cographic matroid. For the specific case of the cographic matroid of a graph, a set of elements are \emph{in parallel} if each pair of them is a minimal edge cut of the graph.}
Each graph  is then -edge-connected and, as before, we can find three bases ,  and  of the cographic matroid of  covering . The algorithm for  is as follows. Independently for each component , select an index  uniformly at random and run the -competitive algorithm of~\cite{babaioff_knapsack_2007} on the partition matroid that accepts at most one edge of  from each parallel class of  represented in  (discard every element of  not represented in ). Since every element of the optimum base of  is the heaviest of its parallel class and each parallel class of  is selected with probability at least , we conclude the previous algorithm is -competitive.

\begin{thm} For any cographic matroid , the previous algorithms is -competitive. Furthermore, if the graph  associated to  is -edge-connected, the algorithm is -competitive.
\end{thm}
\subsection{Low Density Matroids}
A generalization of the previous algorithm is the following. Given a loopless matroid  of density , the vector  having all its coordinates equal to  is feasible in the matroid polytope. In particular, this vector has a decomposition as convex combination of independent sets of : , which we can find in polynomial time. The algorithm for matroid  will select an independent set  at random, according to probabilities  and accept its elements without looking at their weights. Since every element  is selected with probability , this algorithm is -competitive.

If matroid  contains parallel elements, we could get a better competitive ratio by considering the simple matroid  obtained by removing all but one edge in each parallel class of . By combining the output  of the previous algorithm applied on  with the -competitive algorithm for the partition matroid that selects one element in each parallel class represented in  (similar to what we did for cographic matroids), we obtain a -competitive algorithm.

\begin{thm} For any matroid , the first algorithm described is -competitive, and the second algorithm is -competitive.
\end{thm}

\subsection{Matroids with small cocircuits}
For each element  of a loopless matroid , let  be the size of the smallest cocircuit (i.e. circuits of the dual matroid) containing it, and let . Consider the algorithm that greedily construct an independent set of  selecting elements as they appear without looking at their weights. We claim this algorithm is -competitive. To see this, fix an element  and let  be a cocircuit of minimum size containing it. If  appears before all the other elements of  in the random order then it has to be selected by the algorithm. Otherwise, there would be a circuit  that intersects  only in element , which is a contradiction (See, e.g. \cite[Proposition 2.1.11]{oxley2006matroid}). It is not hard to prove that for every matroid , . This mean that this algorithm is no better than the one for low density matroids. However, this algorithm is much simpler.

\begin{thm} For any matroid , the algorithm described above is -competitive.
\end{thm}
\subsection{Column-sparse linear matroids}
Let  be a linear matroid represented by a matrix  containing at most  non-zero values in each column.
Consider the following algorithm: Randomly permute the rows of  and define for every row , the sets  and , where  denotes the -th coordinate of column  in the permuted matrix. Next, run the secretary algorithm for the partition matroid that accepts at most one element of each . We claim that any set returned by this algorithm is independent in : If this was not the case there would be a circuit  inside the output. Let  be the element belonging to the set  of smallest index~. By definition of , the elements of  are not in ; therefore,  and  intersects only in . This is a contradiction since  is in the cocircuit space of the matroid (Use, e.g. \cite[Proposition 2.1.11]{oxley2006matroid}).

We claim the previous algorithm is -competitive. To see this, construct the bipartite graph  with parts the rows and columns of , where there is an edge  if the corresponding entry of  is non-zero. Assign to each edge a weight equal to the one of its associated column in . Consider the following simulation algorithm: Randomly permute the vertices in the row part of the graph. Delete all the edges, except the ones going from a column vertex to its lowest neighbor (the row having smallest index in the random permutation). Finally, run the secretary algorithm for the partition matroid that accepts for each row vertex, at most one edge incident to it. This returns a matching with the same weight as the set of elements the original algorithm returns.

Since for every independent set of columns, the number of row vertices that this set dominates in  is at least its cardinality, Hall's Theorem implies that there is a matching covering each independent set. In particular the weight of the maximum weight matching  in  is at least the one of the optimum independent set of . On the other hand,  has weight at most the one of the edge set , where  is the maximum weight neighbor of  in . Since each edge  is not deleted with probability   and, given it is not deleted, the simulation selects it with probability , we conclude the original algorithm is -competitive.

\begin{thm} The previous algorithm is -competitive for matroids representable by matrices having only  non-zero elements per column.
\end{thm}

Note that by applying this algorithm to graphic matroids, which are representable by matrices having only 2 ones per column, we recover the  competitive algorithm of Korula and Pal \cite{korula2009algorithms}.


\subsection{A new  competitive algorithm for matroids}

Babaioff et al.~\cite{babaioff_matroids_2007} present an  competitive algorithm for general matroids of rank . This algorithm has many features, including the fact that it does not need to know the matroid beforehand; it only needs to know the number of elements and have access to an oracle that test independence only on subsets of elements it has already seen. Nevertheless, this algorithm makes use of the actual values of the weights being revealed. We present an algorithm having the same features but that only uses the relative order of weights seen and not their numerical value.

If we are given the rank of the matroid, our algorithm is as follows. With probability 1/2, run the classical secretary algorithm that returns the heaviest element of the stream. Otherwise, observe the first  elements of the stream, where  is chosen from the binomial distribution  (as usual, denote this set of elements as the \emph{sample}) and compute the optimum base  (with ) of the sampled elements. Afterwards, select a number  with  uniformly at random, run the greedy procedure on the set of non-sampled elements having weight at least the one of  as they arrive and return its answer (if , run the greedy procedure over the entire set of non-sampled elements). If we are not given the rank of the matroid beforehand, we select  uniformly and use this value in the previous algorithm.

The optimum of the sample is similar to the optimum of the nonsampled part: For any number  the algorithm can choose, there is an independent set of size close to  outside the sample with every element heavier than  (with high probability); therefore, the greedy procedure recovers a weight of roughly . By taking the expectation over the choices of  it is not hard to check that the expected weight returned by the algorithm is at least . We give the formal proof below.

\begin{thm}\label{thm:log}
  The algorithm described above is -competitive for any matroid of rank .
\end{thm}
\begin{proof}
Assume first that the rank  of the matroid is known. Let  with  be the maximum independent set of the matroid and  be the optimum of the sample, i.e., the optimum of the first  elements in the stream (independent of whether the algorithm computes  or not). Note that every element of the matroid is sampled independently with probability , including the elements of the optimum. Therefore,



To simplify our analysis, in the following we assume that for ,  is a dummy element with . Given the number  chosen by the algorithm (if the algorithm reaches that state), the weight of the set returned will be at least  times the number of elements the greedy procedure selects; therefore,  is at least


Let  be the collection of non-sampled elements that are heavier than . If the algorithm chooses the number , it will then execute the greedy procedure on  and return a set of cardinality equal to the rank of . Note that for every~, ; therefore, the rank of  is at least the number of nonsampled elements in . By Chernoff bound, the probability that this last quantity is smaller than  is at most .

In particular, if ,  . Therefore,

Using inequality \eqref{eqn:OPT2}, we get

which implies the algorithm is -competitive.

Suppose now that the rank  is unknown. If  is small, say , then with probability  the algorithm will run the standard secretary algorithm and return the top element of the matroid. This element has weight at least  fraction of the optimum; therefore the algorithm is -competitive for this case.

For the case where  we use a different analysis.  The random variable  denoting the rank of the sampled set could be strictly smaller than . However, the probability that  is small. Indeed, for that event to happen we require that at most  of the elements of  are in the sample. By Chernoff bound, this happens with probability . Noting that  implies that , we deduce that with probability at least 1/4 our algorithm guesses  right; therefore, the competitive ratio of this algorithm is at most 4 times worse than the one that knows the rank beforehand.
\end{proof}
\bibliographystyle{abbrv}

\bibliography{matsec}

\end{document}
