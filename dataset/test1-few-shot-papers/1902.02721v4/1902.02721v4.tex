\documentclass{article} \usepackage{iclr2019_conference}
\usepackage{times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\title{Variational Recurrent Neural Networks for Graph Classification}




\author{Edouard Pineau \& Nathan de Lara \\
Telecom ParisTech, Safran\\
\texttt{\{edouard.pineau,nathan.delara\}@telecom-paristech.fr} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}


\usepackage[english]{babel}
\addto{\captionsenglish}{\renewcommand{\bibname}{References}
}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[noend]{algpseudocode}
\usepackage{fancyhdr,amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{array}
\usepackage{stmaryrd}
\usepackage{amsfonts}


\usepackage{amsfonts,amsmath,amssymb,amsthm,epsfig,psfrag}
\usepackage[T1]{fontenc}
\usepackage{babel}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{systeme}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{xcolor}

\newcommand{\cmmnt}[1]{}

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{N}{@{}m{0pt}@{}}

\setlist[description]{leftmargin=\parindent,labelindent=\parindent}

\begin{document}

\maketitle

\begin{abstract}
    We address the problem of graph classification based only on structural information. Inspired by natural language processing techniques (NLP), our model sequentially embeds information to estimate class membership probabilities. Besides, we experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.
    
\end{abstract}

\section{Introduction}

Many natural or synthetic systems have a natural graph representation where entities are described through their mutual connections: chemical compounds, social or biological networks, for example. Therefore, automatic mining of such structures is useful in a variety of applications.

Graph classification raises several difficulties to leverage standard machine learning algorithms. Indeed, most of these algorithms take vectors of fixed size as inputs. In the case of graphs, usual representations such as edge list or adjacency matrix do not match this constraint. The size of the representations is graph dependent (number of edges in the first case, number of nodes squared in the second) and these representations are index dependent: up to indexing of its nodes, a same graph admits several equivalent representations. In a classification task, the label of a graph is independent from the indices of its nodes, so the model used for prediction should be invariant to node ordering as well. Handling discrete inputs with variable size and ordering is a well known problem in natural language processing (NLP). This is why we adapt NLP techniques to tackle graph classification.

In this paper, we propose a method to sequentially embed graph information in order to perform classification. By construction, this recurrent graph classifier overcomes the common difficulties listed above. Besides, we propose to use an additional node prediction block to help the model to capture the intrinsic structure of the graphs. The complete model is denoted \textit{variational recurrent graph classifier} (VRGC). Experiments show that this leads to better classification results for larger datasets. For clarity of the contribution of our work, we use neither node attributes nor edge attributes.

Related works use either graph kernels \citep{nikolentzos2017kernel, nikolentzos2017matching, nikolentzos2018degeneracy, neumann2016propagation, shervashidze2011weisfeiler, yanardag2015deep}, sequential methods \citep{callut2008classification, xu2012protein, jin2018learning, you2018graphrnn} or graph features \citep{barnett2016feature, DBLP:journals/corr/NarayananCVCLJ17, gomez2017dynamics, dutta2017high}.


\section{Model}
\label{sec:model}

We propose to use a sequential approach to embed graphs with a variable number of nodes and edges into a vector space of a chosen dimension. This latent representation is then used for classification. Node index invariance is approximated through specific pre-processing and aggregation.

Let  be an undirected and unweighted graph with  a set of nodes and  a set of edges. The graph  can be represented, modulo any permutation  over its nodes , by its boolean adjacency matrix  such that  if nodes indexed by  and  are connected in the graph and  otherwise. We use this adjacency matrix as a raw representation of the graph. 

Our VRGC is composed of three main parts: node ordering and embedding, classification and regularization with variational auto-regression (VAR). See Figure \ref{fig:macro} for an illustration. 


\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{archi_macro.png}
    \caption{Macroscopic representation of VRGC.}
    \label{fig:macro}
\end{figure*}



\paragraph{Node ordering and embedding}
\label{subsec:node ordering}

Before being processed by the neural network, the adjacency matrix of a graph is transformed on-the-fly \citep{you2018graphrnn}. First, a node is selected at random and used as root for a breadth first search (BFS) over the graph. The rows and columns of the adjacency matrix are then reordered according to the sequence of nodes returned by the BFS. Next, each row  (corresponding to the  node in the BFS ordering) is truncated to keep only the connections of node  with the  nodes that preceded in the BFS. This way, each node is -dimensional, and each truncated matrix is zero-padded in order to have dimensions . Throughout the rest of the paper, we use the notation  for . 

After node ordering and pre-embedding, each graph is processed as a sequence of -dimensional nodes by a gated recurrent unit (GRU) neural network \citep{cho2014learning}. The GRU is a special RNN able to learn long term dependencies by solving vanishing gradient effect\footnote{The choice of GRU over Long Short Term Memory networks is arbitrary as they have equivalent long-term modeling power \citep{chung2014empirical}.}. In order to help the recurrent network training, we propose to add a simple fully connected network between pre-embedding and recurrent embedding. Therefore, the node will be presented to the GRU in the shape of continuous vectors instead of binary adjacency vectors. 

Finally the GRU sequentially embeds each node  by using  and information contained in a memory cell  that theoretically embeds all previously seen information. The embedded node sequence  then feeds both the VAR and the classifier as discussed in subsequent sections. See top line of Figure \ref{fig:archi}.

\paragraph{Classification}
\label{subsec:classification}

After the embedding step, we use an additional GRU dedicated to classification that takes  as input.
Its last memory cell, denoted , feeds a softmax multilayer perceptron (MLP) which performs class prediction.
Formally, let  be the class index, the classifier is trained by minimizing the cross-entropy loss between ground-truth and  the softmax class membership probability vector for a given graph  that has been sorted by a BFS rooted with node . We call this objective term .
As discriminating patterns might be spread across the whole graph, the network is required to model long-term dependencies. By construction, GRUs have such ability. See middle line of Figure \ref{fig:archi}.

\paragraph{Regularization with variational auto-regression}
\label{subsec:vap}

As the structure of a graph is the concatenation of the interactions between all nodes and their respective neighbors, learning a good representation without using node attributes requires for the model to capture the structure of the graph while classifying. Accordingly, we add an auto-regression block to our model: at each node, the network makes a prediction for the next node adjacency. Multi-task learning is a powerful leverage to learn rich representation \citep{sanh2018hierarchical} in NLP. In particular, such representation for sequence classification has already been used for sentiment analysis \citep{latif2017variational, xu2017variational}. 

We use a variational auto-encoder (VAE) \citep{kingma2013auto} to learn a representation of each node  given . The first layers of the encoder are shared with the classifier and corresponds to the graphs preprocessing (blue part in Figures \ref{fig:macro} and \ref{fig:archi}). The subsequent encoder layers, the latent sampling and the decoder constitute the VAR. For each graph  with embedded nodes , the fully connected variational auto-encoder takes  as input. Let  be the latent random variables for the following model 

 

In practice,  and  are modelled by neural networks parametrized by  and , which require differentiable functions for training. However,  models a binary adjacency vector representing the connections between node  and previously visited nodes . Therefore, we use sigmoid continuous relaxation to train our model, and hard binary sampling at test time. We use a Gaussian variational posterior distribution. Training is done by maximizing the variational lower bound of the log-likelihood of the observation as in Kingma's VAE. The exact loss is displayed in Appendix \ref{app:elbo} and denoted . 

The regularization part is illustrated in the bottom line of Figure \ref{fig:archi}. In the end, the model is trained by minimizing the total loss , where  is a hyper-parameter.


\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{VRGC.png}
    \caption{Architecture for VRGC. Top: node ordering and embedding. Middle: classification. Bottom: regularization with VAR plus final aggregation. FC stands for fully-connected.}
    \label{fig:archi}
\end{figure*}

\paragraph{Aggregation of the results at test time}
\label{sub:aggreg}

The node ordering step introduces randomness to our model. On the one hand, it helps learn more general graph representations during the training phase, but on the other hand, it might produce different outputs for the same graph during the testing phase, depending on the root of the BFS. In order to counter this side effect, we add the following aggregation step for the testing phase\cmmnt{ combining a soft and a hard vote}. Each graph is processed  times by the model with  different roots for BFS ordering. The  class membership probability vectors are extracted and averaged. The average score vector is noted  and computed as follows with an element-wise sum: 




This soft vote is repeated  times resulting in  probability vectors  for each graph . The final class attributed to a graph corresponds to the highest probability among the  vectors.  
This second hard vote enables to choose the batch of votes for which the model is the most confident.


\section{Experiments}
\label{sec:experiments}

\paragraph{Datasets and results}

We evaluated our model against four standard datasets from biology: Mutag (MT), Enzymes (EZ), Proteins Full (PF) and National Cancer Institute (NCI1) \citep{KKMMN2016}. A detailed description of each dataset in provided in Appendix \ref{app:datasets}.

We compare our results to those obtained by Earth Mover's Distance \citep{nikolentzos2017matching} (EMD), Pyramid Match \citep{nikolentzos2017matching} (PM), Feature-Based \citep{barnett2016feature} (FB), Dynamic-Based Features \citep{gomez2017dynamics} (DyF), Stochastic Graphlet Embedding \citep{dutta2017high} (SGE), Truncated Laplacian Spectrum \citep{de2018simple} (TLS) and Family of Graph Spectral Distances \citep{verma2017hunt} (FGSD). All values are directly taken from the aforementioned papers as they use a setup similar to ours. For algorithms presenting results with and without node features, we reported the results without node features. For those presenting results with several sets of hyper-parameters, we reported the results for the parameters that performed best on the largest number of datasets. Results are reported in Table \ref{tab:results}. We obtain state-of-the-art results on three out of the four datasets used for this paper and the second best result on the fourth one. 

\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l|M{1.5cm} M{1.5cm} M{1.5cm} M{1.5cm} M{1.5cm} M{1.5cm}}
           & MT            &      EZ       & PF            & NCI1 \\
      \hline
      EMD  & 86.1          &     36.8      & -             & 72.7 \\
      PM   & 85.6          &     28.2      & -             & 69.7 \\
      FB   & 84.7          &     29.0      & 70.0          & 62.9 \\
      DyF  & 86.3          &     26.6      & 73.1          & 66.6 \\
      SGE  & 87.3          &     40.7      & 71.9          & - \\
      TLS  & 88.4          &     43.7      & 73.6          & 75.2 \\
      FGSD & \textbf{92.1} &     -         & 73.4          & 79.8 \\
      \hline
      RGC  & 89.5          & \textbf{48.7} & 72.5          & 78.1 \\
      VRGC & 86.3          & 48.4          & \textbf{74.8} & \textbf{80.7} \\
      \end{tabular}
  \caption{Experimental results of different models plus our own on four standard molecular datasets. RGC stands for recurrent graph classifier, VRGC for variational RGC. All other acronyms are defined in section \ref{sec:experiments}.}
  \label{tab:results}
\end{table}



\paragraph{Node indexing invariance}
Our model is designed to be independent from node ordering of the graph with respect to different BFS roots. Inputs representing the same graph (up to node ordering) should be close from one another in the latent embedding space. As the preprocessing is performed on each graph at each epoch, a same graph is processed many times by the model during training with different embeddings. This creates a natural regularization for the network. Indeed, as illustrated in Figure \ref{fig:projection}, the projections corresponding to the same graphs form a heap in the low dimensional representation of the latent space.


\paragraph{Contribution of the VAR to classification}
The variational regularization term seems to help the model finding a more meaningful latent representation for classification while graph dataset becomes larger. Note that the extra cost of training the VAR is marginal with respect to the training of the RNNs. We provide an illustration of the output of VAR block in Figure \ref{fig:reconstructions}.

\paragraph{Conclusion and room for improvement}
This paper proposed a recurrent graph classifier with variational regularization. The invariance to node indexing is greedy learned from numerous iterations on randomly rooted BFS-ordered graph. For future work, we should investigate the impact VAR capacity (number and size of hidden layers) on classification accuracy or generalization.


\paragraph{Acknowledgments} We would like to thank Thomas Bonald and Sebastien Razakarivony for their comments and help. We also would like to thank NVIDIA and its GPU Grant Program for providing the hardware we used in our experiments. This work is supported by the company Safran through the CIFRE convention 2017/1317. 



\bibliographystyle{abbrvnat}
\bibliography{biblio}

\clearpage
\appendix


\section{VAR loss}
\label{app:elbo}

The VAE-like loss for VAR regularization is the following:



which is a lower bound of the negative marginal log-likelihood .  and  are the respective densities of  and , whose distribution are parameterized by  and  respectively. KL denotes the Kullback-Leibler divergence,  is the empirical distribution of  and  is the density of the prior distribution of latent variables . We chose the standard Gaussian prior for . 

\section{Dataset characteristics}
\label{app:datasets}
 All graphs represent chemical compounds, nodes are molecular substructures (typically atoms) and edges represent connections between these substructures (chemical bound or spatial proximity). In MT, the compounds are either mutagenic or not mutagenic. EZ contains tertiary structures of proteins from the 6 Enzyme Commission top level classes; it is the only multiclass dataset of this paper. PF is a subset of the Dobson and Doig dataset representing secondary structures of proteins being either enzyme or not enzyme. In NCI1, compounds either have an anti-cancer activity or do not. 

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.4}
    \begin{tabular}{l|M{1.5cm} M{1.5cm} M{1.5cm} M{1.5cm} M{1.5cm} M{1.5cm}}
                          & MT   & EZ   & PF   &  NCI1 \\
        \hline
         graphs       & 188  & 600  & 1113 & 4110 \\
         classes      & 2    & 6    & 2    & 2    \\
        bias              & 0.66 & 0.17 & 0.60 & 0.5  \\
        avg. |V|          & 18   & 33   & 39   & 30 \\
        min |V| / max |V| & 10/28   & 2/125   & 4/620   & 3/106 \\
        avg. |E|          & 39   & 124  & 146  & 64.6 \\
    \end{tabular}
    \caption{Basic characteristics of the datasets. Bias indicates the proportion of the largest class.}
    \label{tab:datasets}
\end{table}



\section{Features of network architecture}
\label{app:architecture}

 MT, EZ, PF and NCI1 are respectively divided into 10 folds such that the class proportions are preserved in each fold for all datasets. These folds are then used for cross-validation i.e., one fold serves as the testing set while the other ones compose the training set. Results are averaged over all testing sets.
Our model is implemented in Pytorch \citep{paszke2017pytorch} and trained with the Adam stochastic optimization method \citep{kingma2014adam} on a NVIDIA TitanXp GPU. 

\begin{table}
\centering
  \begin{center}
  
    \begin{tabular}{l|l}
      \textbf{Step} & \textbf{Architecture}  \\
      \hline
       BFS          & 1-layer FC.  \\
       embedding    & 2-layer GRU.  \\
       \hline
       VAR          & \underline{Encoder} \\
                    & 1-layer FC.  \\
                    &       Gaussian sampling \\
                    & \underline{Predictor} \\ 
                    & 2-layer ReLU FC.  \\
        \hline
       Classifier   & 2-layer GRU.  + DP(0.25) \\
                    & 2-layer ReLU FC.  + SF \\
    \end{tabular}
  \end{center}
  \caption{Generic architecture used in our experiments. ReLU FC stands for fully-connected network with ReLU activation. DP stands for dropout. SF stands for softmax.}
  \label{tab:table1}
\end{table}

The input size  of the recurrent neural network is chosen for each dataset according to the algorithm described in \citep{you2018graphrnn}, namely 11 for MT, 25 for EZ, 80 for PF and 11 for NCI1.  is set to . For training, batch size is set to 64, and the learning rate to , decreased by  at iterations  and . We use the same hyper-parameters for every dataset.

\section{Illustration of experimental results}

The following table presents the hyperparametrization of our model, i.e. the neural network architectures for each part presented in Section \ref{sec:model}.

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{Projection.png}
    \caption{TSNE projection of the latent state preceding classification for five graphs of EZ each initiated with 20 different BFS. Colors and markers represent the respective classes of the graphs.}
    \label{fig:projection}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.245\textwidth]{Original_2.png}
    \includegraphics[width=0.245\textwidth]{Reco_2.png}
    \includegraphics[width=0.245\textwidth]{Original_3.png}
    \includegraphics[width=0.245\textwidth]{Reco_3.png}
    \caption{Left: Representation of the same graph after two differently rooted BFS ordering and truncation. Right: corresponding auto-regressions.}
    \label{fig:reconstructions}
\end{figure}

\end{document}