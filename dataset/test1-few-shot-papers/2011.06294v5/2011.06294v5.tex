\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{multirow,booktabs,makecell}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{1324} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\usepackage{color}
\usepackage{colortbl}

\definecolor{MyRed}{rgb}{0.8,0.2,0}
\definecolor{MyBlue}{rgb}{0,0,1.0}
\def\red#1{\textcolor{MyRed}{#1}}
\def\blue#1{\textcolor{MyBlue}{#1}}
\def\first#1{\red{\textbf{#1}}}
\def\second#1{\blue{\underline{#1}}}

\begin{document}

\title{RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation}

\author{Zhewei Huang
		~~~~
		Tianyuan Zhang
		~~~~
		Wen Heng
		~~~~
		Boxin Shi
		~~~~
		Shuchang Zhou\\
		Megvii Inc~~~~Peking University\\
		{\tt\small \{huangzhewei, zhangtianyuan, hengwen, zsc\}@megvii.com, shiboxin@pku.edu.cn}
		}
\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
   We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video Frame Interpolation (VFI). Most existing flow-based methods first estimate the bi-directional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named IFNet that can directly estimate the intermediate flows from images with much better speed. Based on our proposed leakage distillation loss, RIFE can be trained in an end-to-end fashion. Experiments demonstrate that our method is flexible and can achieve impressive performance on several public benchmarks. The code is available at \url{https://github.com/hzwer/arXiv2020-RIFE}.
\end{abstract}

\section{Introduction}
Video Frame Interpolation (VFI) aims to synthesize intermediate frames between two consecutive frames of a video and is widely used to improve frame rate and enhance visual quality. VFI also supports various applications like slow-motion generation, video compression~\cite{wu2018video}, and novel view synthesis. Moreover, real-time VFI algorithms running on high-resolution videos have many more potential applications, such as increasing the frame rate of video games and live videos, providing video editing services for users with limited computing resources.

\begin{figure}
	\vspace{-2em}
	\centering
	\includegraphics[width=8.2cm]{image/Vimeo/psnr_inference.pdf}
	\vspace{-0.5em}
	\caption{\textbf{Speed and accuracy trade-off by adjusting model size parameters  and .} We compare our models with prior VFI methods including TOFlow~\cite{xue2019video}, SepConv~\cite{Niklaus_ICCV_2017}, MEMC-Net~\cite{bao2019memc}, DAIN~\cite{bao2019depth}, CAIN~\cite{choi2020channel}, SoftSplat~\cite{niklaus2020softmax}, BMBC~\cite{park2020bmbc}, DSepConv~\cite{cheng2020video}, and EDSC~\cite{cheng2020multiple} on the Vimeo90K testing set. }
	\label{fig:intro_fig}\vspace{-0.5em}
\end{figure}

VFI is challenging due to the complex, large non-linear motions and illumination changes in the real world. Recently, flow-based VFI algorithms have offered a framework to address these challenges and achieved impressive results~\cite{jiang2018super, niklaus2018context, xue2019video, bao2019depth}. Common approaches for these methods involve two steps: 1) warping the input frames according to approximated optical flows and 2) fusing and refining the warped frames using Convolutional Neural Networks (CNNs). 

According to the way of warping frames, flow-based VFI algorithms can be classified into forward warping based methods and backward warping based methods. Backward warping is more widely used because forward warping suffers from conflicts when multiple source pixels are mapped to the same location, leading to overlapped pixels and holes. Given the input frames , backward warping based methods need to approximate the intermediate flows  from the perspective of the frame  that we are expected to synthesize. Common practices~\cite{jiang2018super, bao2019depth, xu2019quadratic, liu2020enhanced} first compute bi-directional flows from pre-trained off-the-shelf optical flow models, then reverse and refine them to generate intermediate flows. However, these methods may have flaws on motion boundaries, as the object position changes from frame to frame. Consequently, previous flow-based VFI methods share two major drawbacks:
\begin{enumerate}[1)] \vspace{-0.5em}
	\item Requiring additional components: Image depth~\cite{bao2019depth}, flow refinement~\cite{jiang2018super} and flow reversal layer~\cite{xu2019quadratic} are introduced to compensate for the defects of optical flow reversal. These operations require a large amount computing resources and are not suitable for real-time scenarios. \vspace{-0.5em}
	\item Lacking direct supervision for the approximated intermediate flows: The whole interpolation system is usually trained with only the final reconstruction loss~\cite{park2020bmbc}. There is no other supervision explicitly designed for the flow estimation process, degrading the performance of interpolation. \vspace{-0.5em}
\end{enumerate}

We develop a specialized intermediate flow network named \textbf{IFNet} to directly estimate the intermediate flows. This process is commonly believed to be hard~\cite{jiang2018super, xu2019quadratic}, and few attempts~\cite{liu2017video, bao2019memc} have been made. IFNet adopts a coarse-to-fine strategy~\cite{ilg2017flownet} with progressively increased resolutions: it iteratively updates a flow field via successive IFBlocks. Conceptually, according to the iteratively updated flow fields, we could move corresponding pixels from two input frames to the same location in a 
latent intermediate frame. Unlike most previous optical flow models~\cite{dosovitskiy2015flownet, ilg2017flownet, sun2018pwc, hui2018liteflownet, teed2020raft}, IFBlocks do not contain expensive operators like cost volume or pyramid feature warping and use  convolution as building blocks.



Employing strong intermediate supervision is also found to be important. In fact, when training the IFNet end-to-end with later fusion process using the final reconstruction loss, our method produces worse results than previous methods that use complex pipelines and pre-trained flow models in the intermediate flow estimation process. The picture dramatically changes after we add advanced supervision to IFNet. We design a novel leakage distillation loss which employs an overpowered teacher with access to the intermediate frames during training.

Combining these designs, we propose the Real-time Intermediate Flow Estimation (\textbf{RIFE}). RIFE can achieve satisfactory results when trained from scratch. We illustrate the speed and accuracy trade-off compared with other methods in Figure~\ref{fig:intro_fig}. 

In summary, our contributions are three-fold:\vspace{-0.5em}
\begin{itemize}
	\item We design an efficient IFNet to simplify the flow-based VFI methods. IFNet can be trained from scratch and directly approximate the intermediate flows given two input frames.\vspace{-0.5em}
	\item We provide effective supervision for the intermediate flow estimation, especially a leakage distillation loss, which leads to a more stable convergence and large performance improvement. \vspace{-0.5em}
	\item We use model scaling to obtain models with varying quality and speed trade-offs. Experiments show that RIFE can achieve impressive performance on public benchmarks. \vspace{-0.5em}
\end{itemize}










\begin{figure*}
	\centering
	\includegraphics[width=17cm]{image/MAIN.png}
	\caption{\textbf{Overview of RIFE.} Given two input frames , we directly feed them into our efficient IFNet to approximate intermediate flows . The fusion process takes the warped frames , intermediate flows  and the input frames  as input. Inside the fusion process, a FusionMap and Residual is firstly estimated, then the warped frames are linearly combined according to the FusionMap, and added with the Residual to reconstruct the frame . }\label{fig:main}\vspace{-1em}
\end{figure*} 	\section{Related Works}
We provide a brief overview of the optical flow estimation task, which is the core of most VFI methods. Then, we will review several most related flow-based VFI methods, and cover some inspiring flow-free methods. 
\subsection{Optical Flow}
Optical flow estimation is a long-standing vision task that aims to estimate the per-pixel motion, which are useful in lots of downstream tasks. Since the milestone work of FlowNet~\cite{dosovitskiy2015flownet} based on U-net autoencoder~\cite{ronneberger2015u}, architectures for optical flow models have evolved for several years, yielding more accurate results while being more efficient, such as FlowNet2~\cite{ilg2017flownet}, PWC-Net~\cite{sun2018pwc} and LiteFlowNet~\cite{hui2018liteflownet}. These methods typically adopt an iterative refinement approach and often involve operators like cost volume, pyramidal features, and backward feature warping. Recently Teed~\etal~\cite{teed2020raft} introduce RAFT, which iteratively updates a flow field through a recurrent unit and achieves a remarkable breakthrough in this field. Another important research direction is unsupervised optical flow estimation~\cite{meister2017unflow, jonschkowski2020matters, luo2020upflow} due to the difficulty of optical flow labeling.







\subsection{Video Frame Interpolation}


Liu~\etal.~\cite{liu2017video} propose a fully convolutional network to estimate voxel flow and generate intermediate frames by sampling. Jiang~\etal.~\cite{jiang2018super} propose SuperSlomo using the linear combination of the two bi-directional flows as an initial approximation of the intermediate flows. Then refine them and predict visibility maps that encode occlusion information. Based on SuperSlomo, Reda~\etal.~\cite{reda2019unsupervised} propose to synthesize intermediate frames using unsupervised cycle
consistency. Bao~\etal.~\cite{bao2019depth} propose DAIN using a depth-aware flow projection layer to estimate the intermediate flow as a weighted combination of bidirectional flow. Niklaus~\etal.~\cite{niklaus2020softmax} propose SoftSplat to forward-warp frames and their feature map using softmax splatting. Xu~\etal.~\cite{xu2019quadratic} propose QVI to exploit four consecutive frames and flow reversal filter to get the intermediate flows. Liu~\etal.~\cite{liu2020enhanced} further extend QVI with rectified quadratic flow prediction to EQVI. Among these methods, DAIN has been deployed in many software applications, and SoftSplat leads in many public benchmarks.

Along with flow-based methods, flow-free methods have also achieved remarkable progress in recent years. Meyer~\etal.~\cite{meyer2015phase} utilize phase information to learn the motion relationship for multiple video frame interpolation. Niklaus~\etal.~\cite{niklaus2017video, Niklaus_ICCV_2017} formulate VFI as a spatially-adaptive convolution whose convolution kernel is generated using a convolution network given the input frames. Cheng~\etal. propose DSepConv~\cite{cheng2020video} to extend kernel-based method using deformable separable convolution and further propose EDSC~\cite{cheng2020multiple} to perform multiple interpolation. Choi~\etal.~\cite{choi2020channel} propose an efficient flow-free method named CAIN, which employs the PixelShuffle operator and channel attention to capture the motion information implicitly. 

%
 	\section{Method}

In this section, we first provide an overview of our proposed RIFE. We then describe the efficient design of the major components in RIFE in section~\ref{subsec:architecture}, elaborate on our proposed leakage distillation loss in section~\ref{subsec:leakage}, and explain the training details in section~\ref{subsec:implement}. 

\subsection{Pipeline Overview}
\label{subsec:overview}



We illustrate the overview of our proposed RIFE in Figure~\ref{fig:main}. Given a pair of consecutive RGB frames, , our goal is to synthesize an intermediate frame  at time . We directly estimate the intermediate flows  and  by feeding input frames into IFNet. Then we can get two coarse results  by backward warping the input frames. To remove the artifacts in the warped frames, we feed the input frames, the approximated flow, and warped frames into the fusion process with an encoder-decoder like FusionNet to generate the final result. 

\subsection{Efficient Architecture Design}
\label{subsec:architecture}

RIFE has two major components: 1) efficient \textit{intermediate flow estimation} with the IFNet and 2) \textit{fusion process} of the warped frames.


\begin{figure}[t]
	\centering
	\includegraphics[width=8.3cm]{image/IFFlow.pdf}
	\caption{\textbf{Comparison between previous intermediate estimation approaches~\cite{jiang2018super, xu2019quadratic, bao2019depth, liu2020enhanced} (left) and IFNet (right).} Most existing methods contain two stages: 1) bi-direction flow estimation and 2) flow reversal modules. The flow reversal process is usually cumbersome due to the difficulty of handling the changes of object positions. IFNet can directly estimate the intermediate flows. }\label{fig:IFFlow}
	\vspace{-1em}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=8.3cm]{image/IFNet.pdf}
	\caption{\textbf{Structure of IFNet}. \textbf{Left}: IFNet is composed of three stacked IFBlocks operating at different resolutions. \textbf{Right}: We first warp the two input frames based on current approximated flow . Then the warped frames  and  are fed into the next IFBlock to approximate a flow residual. }\label{fig:IFNet}
\end{figure}

\paragraph{Intermediate flow estimation.}
Some previous intermediate flow estimation methods reverse and refine bi-directional flows~\cite{jiang2018super, xu2019quadratic, bao2019depth, liu2020enhanced} as depicted in Figure~\ref{fig:IFFlow}. 
The role of our IFNet is to directly and efficiently predict  given two consecutive input frames . 

To handle the large motion encountered in intermediate flow estimation, we employ a coarse-to-fine strategy with gradually increasing resolutions, as illustrated in Figure \ref{fig:IFNet}. Specifically, we first compute a rough prediction of the flow on low resolutions, which is believed to capture large motions easier, then iteratively refine the flow fields with gradually increasing resolutions. Following this design, our IFNet has a stacked hourglass structure, where a flow field is iteratively refined via successive IFBlocks operating on increasing resolutions:



where  denotes the current estimation of the intermediate flows from the -th IFBlock,  and  denote the warped input frames using previous approximated flow, and  represents the th IFBlock. We use a total of 3 IFBlocks, and each has a resolution parameter, . To keep our design simple, each IFBlock has a feed-forward structure consisting of serveral convolutional layers and an up-sampling operator. Except for the layer that outputs the optical flow residuals, the fusion map, and the reconstruction residual, we use PReLU~\cite{he2015delving} as the activation function.



\begin{figure}[t]
	\centering
	\begin{minipage}[t]{0.325\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{image/CompareFlow/f1.png}
		\centerline{Inputs~(Overlay)}
	\end{minipage}
	\begin{minipage}[t]{0.325\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{image/CompareFlow/f2.png}
		\centerline{Combination}
	\end{minipage}
	\begin{minipage}[t]{0.325\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{image/CompareFlow/f3.png}
		\centerline{IFNet}
	\end{minipage}
	
	\caption{\textbf{Visual comparison between linearly combined bi-directional flows generated by a pre-trained LiteFlowNet~\cite{hui2018liteflownet} and the intermediate flow approximated by IFNet.} IFNet produces clear motion boundaries.}
	\label{fig:compare_flow}
	\vspace{-1em}
\end{figure}

In Figure~\ref{fig:compare_flow}, we provide visual results of our IFNet and compare them with the linearly combined bi-directional optical flows generated by a pre-trained LiteFlowNet~\cite{hui2018liteflownet}. Our IFNet produces clear and sharp motion boundaries, while linearly combining flow suffers from overlapped pixels and blurring on motion boundaries. 

\begin{table}[t]
	\caption{\textbf{Inference time on 720p video}. Standard flow-based VFI methods run the flow estimation network twice to obtain bi-directional optical flows.}\label{tab:runtime}
	\small
	\begin{tabular}{ccccc}
		\hline
		Method&PWC-Net&LiteFlowNet&FlowNet2&IFNet\\ \hline
		Runtime & ms & ms & ms & \textbf{17ms} \\ \hline
	\end{tabular}
	\vspace{-1em}
\end{table}

We compare the runtime of the current state-of-the-art optical flow estimation networks~\cite{sun2018pwc, hui2018liteflownet, ilg2017flownet} and our IFNet in Table~\ref{tab:runtime}. Current flow-based methods usually need to run their flow models twice to get the bi-directional flows. Therefore the intermediate flow estimation in RIFE runs at a faster speed than previous methods, achieving the acceleration of  times.







\paragraph{Fusion process.} With the estimated intermediate flows  and , we can get the coarse reconstructed frames  and  by performing backward warping on input frames. To reduce the severe artifacts in the warped frames, we perform a refine and fusion process formulated as: 

where  is a soft fusion map used to fuse these two warped frames,  is the reconstruction residual term used to refine the details in images,  is an element-wise multiplier, and .

Following the previous works~\cite{jiang2018super, bao2019depth, niklaus2020softmax}, the fusion process includes a context extractor and a FusionNet with an encoder-decoder architecture similar to U-Net. The context extractor and encoder part of the FusionNet have similar architectures, consisting of four convolutional blocks, and each of them is composed of two  convolutional layers, respectively. The decoder part in the FusionNet has four transpose convolution layers. We use sigmoid function to restrict the outputs of FusionNet.

Specifically, the context extractor first extracts the pyramid contextual features from input frames separately. We denote the pyramid contextual feature as :  and : . We then perform backward warping on these features using estimated intermediate flows to produce aligned pyramid features,  and . The warped frames and intermediate flows are fed into the FusionNet, including an encoder and a decoder. The output of  encoder block is concatenated with the  and  before being fed into the next block. The decoder parts finally produce the fusion map  and the reconstruction residual .






\subsection{Leakage Distillation for IFNet}

\label{subsec:leakage}

Directly approximating the intermediate flows is challenging because of no access to the intermediate frame and the lack of supervision. To address this problem, we add a leakage distillation loss to our IFNet in which the target is the prediction of an overpowered teacher network who has access to the intermediate frame. Specifically, we feed  to a pre-trained optical flow estimation network to get the intermediate flow prediction . And the leakage distillation loss  is defined as follows:

Following the previous work~\cite{teed2020raft}, we apply the leakage distillation loss over the full sequence of predictions generated from the iteratively updating process in our IFNet. 

Our distillation scheme is different from those in semi-supervised learning algorithms~\cite{chen2020big, xie2020self}, where a pre-trained model is used to infer the label of unlabeled data. With the access of the target frame , our teacher model has a different view of the video clip with the student. Conceptually, the overpowered teacher causes a leakage~\cite{kaufman2012leakage} where our flow estimator can have access to the information of the target intermediate frame during training, and in the experiments section, we show that this kind of data (target) leakage is beneficial to the training of our whole system.

















\begin{table*}
\centering
	\caption{
		\textbf{Quantitative comparisons on the UCF101~\cite{soomro2012ucf101}, Vimeo90K~\cite{xue2019video}, Middlebury-\textsc{other} set~\cite{baker2011database}, and HD benchmarks~\cite{bao2019memc}}. The numbers in \first{red} and \second{blue} represent the best and second-best performance. We report the interpolation runtime for a single  video frame. Some methods are unable to run on 1080p videos due to exceeding the 12 gigabytes of memory available on our graphics card (denoted as “OOM”).
	}
	\label{tab:UCF101_Vimeo90K_MB}
	\begin{tabular}{lcccccccccc}
		\toprule
\multirow{2}{*}[-0.28em]{Method}  &\# Parameters&
		Runtime
&\multicolumn{2}{c}{UCF101~\cite{soomro2012ucf101}} &\multicolumn{2}{c}{Vimeo90K~\cite{xue2019video}} & M.B.~\cite{baker2011database} & ~~HD~~\cite{bao2019memc}~\\


		
		\cmidrule(l{7pt}r{7pt}){4-5}
		\cmidrule(l{7pt}r{7pt}){6-7}
		\cmidrule(l{5pt}r{5pt}){8-8}
		\cmidrule(l{5pt}r{5pt}){9-9}
\vspace{0.2em}
		&(Million) & (ms) &PSNR & SSIM 	&PSNR & SSIM & IE & PSNR\\
		\addlinespace[-1pt]
		\midrule
		DVF~\cite{liu2017video}&\second{~1.6}&80&34.12&0.963&31.54&0.946&4.04& - \\
		
		SuperSlomo~\cite{jiang2018super, choi2020channel} & 19.8 & {52} & 34.75 & {0.968} & {33.15} & 0.966 & 2.28 & -
		
		\\\hline \hline
		TOFlow~\cite{xue2019video}&\first{~1.1} &72 & 34.58 & 0.967  &33.73& 0.968 &2.15 &29.37  \\
		
		SepConv~\cite{Niklaus_ICCV_2017} & 21.6 & 51 & 34.78 & 0.967  &33.79& {0.970} &2.27& 30.87
		
		\\MEMC-Net~\cite{bao2019memc} & 70.3 & 120(401) & {35.01} & {0.968} &{34.29}& {0.970}  &{2.12} & 31.39  \\ 

		DAIN~\cite{bao2019depth} & 24.0 & 130(436) & {35.00} & {0.968} &{34.71}& {0.976}  &2.04 & 31.64(OOM)\\ 

		CAIN~\cite{choi2020channel} & 42.8 & \second{38} & 34.98 & {0.969} & 34.65 & 0.973 & ~2.28 & 31.77\\
		
		SoftSplat~\cite{niklaus2020softmax} & ~7.7 & 135 & \first{35.39} & \first{0.970} & \first{36.10} & \first{0.980} & \first{1.81} & - \\
		    
		BMBC~\cite{park2020bmbc} & 11.0 & 1580 & 35.15 & {0.969} & 35.01 & 0.976 & 2.04 & OOM\\
		
		DSepConv~\cite{cheng2020video} & 21.8 & 236 & 35.08 & 0.969 & 34.73 & 0.974 & 2.03 & OOM\\
		
		EDSC~\cite{cheng2020multiple} & {~8.9} & 46 & 35.13 & 0.968 & 34.84 & 0.975 & 2.02 & 31.59\\
		\hline \hline
		
		RIFE (Ours) & {~9.8} & \first{16} & 35.25 & {0.969} & 35.51 & {0.978} & 1.96 & \second{31.99}\\
		
		RIFE-Large (Ours) & 20.9 & 72 & \second{35.29} & 0.969 & \first{36.10} & \first{0.980} & \second{1.94} & \first{32.14}
		\\
		\bottomrule
	\end{tabular} 
	\label{tab:comparison}
	\begin{tablenotes}
		\raggedleft
		\item{
   
    : copy from ~\cite{bao2019depth}, we compile released models and get three times slower on our graphics card.
    
    : get 2.72 using officially released model.
	}
	\end{tablenotes}\vspace{-1em}
\end{table*} 
\subsection{Implement Details}
\label{subsec:implement}

\paragraph{Supervisions.} Given a pair of consecutive frames, , our training loss  is a linear combination of the reconstruction loss , adapted census loss~\cite{meister2017unflow}  and leakage distillation loss  as defined in section~\ref{subsec:leakage}:  

where we set  and .

The reconstruction loss  models the reconstruction quality of the intermediate frame. We denote the synthesized frame by  and the ground-truth frame by .
The reconstruction loss has the formulation of :



As the brightness constancy constraint is often violated in realistic situations, census loss is widely used in unsupervised optical flow estimation~\cite{jonschkowski2020matters} methods to address the illumination changes. We adopt the census loss from unsupervised optical flow learning~\cite{meister2017unflow, jonschkowski2020matters} to robustly handle the illumination changes between consecutive frames.
The census loss is defined as the soft Hamming distance on census-transformed~\cite{zabih1994non} image patches. We optimize the census loss  between census-transformed  and  with the width of patches as 9.

\paragraph{Training dataset.}
We use the Vimeo90K dataset~\cite{xue2019video} to train our model.
The Vimeo90K dataset has 51,312 triplets for training, where each triplet contains three consecutive video frames with a resolution of .
We randomly augment the training data by horizontal and vertical flipping and temporal order reversing during training. We crop every training example to a  patch. In the benchmark experiment of inter-frame interpolation, we train RIFE to predict the middle frame given the frames on both sides. 

\paragraph{Training strategy.}
We train our system from scratch on the Vimeo90K training set. An officially pre-trained LiteFlowNet~\cite{hui2018liteflownet} is used as the overpowered teacher in the leakage distillation.

Our model is optimized by AdamW~\cite{loshchilov2018fixing} with weight decay  for  epochs on the Vimeo90K training set. Our training uses a batch size of . We gradually reduce the learning rate from  to 0 using cosine annealing during the whole training process. Our pipeline is implemented in PyTorch. We train RIFE on four NVIDIA TITAN X (Pascal) GPUs for about 15 hours. The preprocess needs 2 hours in one GPU, and different models can use the same preprocess results. 

 	\section{Experiments}
In this section, we conduct several experiments to validate our method. We first introduce the benchmarks for evaluation. Then we provide variants of our models with different computational costs to meet different needs in section~\ref{sec:model_scaling}. We compare our models with representative state-of-the-art methods, both quantitatively and visually, in section~\ref{sec:comparison}. An ablation study in section~\ref{sec:model_ablation} is carried out to analyze our design. We show the capability of generating multiple frames using our models in section~\ref{sec:model_multi}. Finally, we discuss some limitations of our method in section~\ref{sec:limitation}.

\subsection{Benchmarks and Evaluation Metrics}
We evaluate our models on 
four benchmarks, including Middlebury~\cite{baker2011database}, UCF101~\cite{soomro2012ucf101}, Vimeo90K~\cite{xue2019video} and HD~\cite{bao2019memc}. Following the previous works, we train our models on the Vimeo90K training dataset and directly test it on all these benchmarks. 

\textbf{Middlebury.} The Middlebury (M.B.) benchmark~\cite{baker2011database} is widely used to evaluate VFI methods. The image resolution in this dataset is around . We report the average IE of the Middlebury-\textsc{other} set.

\textbf{Vimeo90K}.
There are 3,782 triplets in the Vimeo90K testing set~\cite{xue2019video} with resolution of .


\textbf{UCF101.}
The UCF101 dataset~\cite{soomro2012ucf101} contains videos with a large variety of human actions. There are 379 triplets with a resolution of .



\textbf{HD.}
Bao~\etal~\cite{bao2019memc} collect 11 high-resolution videos for evaluation. The HD benchmark consists of four 1080p, three 720p and four  videos. The motions in this benchmark are larger than other benchmarks. Following the author of HD benchmark, we use the first 100 frames of each video for evaluation.

We measure the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and interpolation error (IE) for quantitative evaluation. All the methods are tested on a TITAN X (Pascal) GPU. We calculate the average process time for 100 runs after a warm-up process of 100 runs. 



\subsection{Model Scaling}
\label{sec:model_scaling}





\begin{table}[t]
	\caption{\textbf{Increase model complexity by adjusting model size parameters.} \textit{C} denotes the multiplier for the number of channels, and \textit{F} denotes the resolution multiplier. \textit{2F} represents removing the first downsampling layer of IFNet and the first one of FusionNet. The parameter setting of RIFE-Large is \textit{1.5C2F}. }\label{tab:large}
	\centering
	\small
	\begin{tabular}{lcccc}
		\hline
		Scale Setting  & RIFE & \textit{1.5C} & \textit{2F} & RIFE-Large \\ \hline
		UCF101 PSNR & 35.24 & 35.26 & \first{35.30} & \second{35.29} \\
		Vimeo90K PSNR& 35.51 & 35.76 & \second{35.95} & \first{36.10}\\ M.B. IE & 1.96 & 1.96 & \first{1.94} & \first{1.94}
		\\
		HD PSNR& 31.99 & \second{32.04} & 31.91 & \first{32.14} \\ 

		\# Parameters & \first{9.8M} & 20.9M & \first{9.8M} & 20.9M\\ Runtime & \first{35ms} & \second{50ms} & 126ms & 234ms\\
		
		Complexity & \first{200G} & \second{460G} & 790G & 1780G\\
		\hline
	\end{tabular}
	\normalsize
	\begin{tablenotes}
		\raggedleft
		\item{
			*: average runtime of 720p frames ()
		}
	\end{tablenotes}
	\vspace{-1em}
\end{table}

We provide several models with different computation overheads and performance to meet different needs by model scaling. We introduce two hyper-parameters following~\cite{howard2017mobilenets}: width multiplier and resolution multiplier. Upon our base model RIFE, we apply a \textit{1.5} width multiplier on the number of channels uniformly at each layer to produce a model named RIFE-\textit{1.5C}. Meanwhile, we can remove a downsample layer from the headers of IFNet and FusionNet, which doubles the feature map's resolution that produces a model named RIFE-\textit{2F}. Together, we can combine these two modifications to produce a model named RIFE-Large (\textit{1.5C2F}). The performance and runtime for these models is reported in Table~\ref{tab:large} and depicted in Figure~\ref{fig:intro_fig}. We show that our model is flexible, and simply increasing model capacity can effectively improve model performance. A useful trick is that when using RIFE to process 720p video frames in parallel (), the total runtime can further drop to half.

\begin{figure*}[htbp]
  \centering
  \begin{minipage}[t]{0.162\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{image/Vimeo/overlay.png}
  \centerline{Inputs~(Overlay)}
  \end{minipage}
  \begin{minipage}[t]{0.162\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{image/Vimeo/SepConv.png}
  \centerline{SepConv}
  \end{minipage}
\begin{minipage}[t]{0.162\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{image/Vimeo/DAIN.png}
  \centerline{DAIN}
  \end{minipage}
  \begin{minipage}[t]{0.162\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{image/Vimeo/CAIN.png}
  \centerline{CAIN}
  \end{minipage}
  \begin{minipage}[t]{0.162\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{image/Vimeo/Ours.png}
  \centerline{RIFE (Ours)}
  \end{minipage}
  \begin{minipage}[t]{0.162\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{image/Vimeo/GT.png}
  \centerline{GT}
  \end{minipage}
  \caption{\textbf{Qualitative comparison on Vimeo90K~\cite{xue2019video} testing set}. We cut out the objects according to the green boxes and zoom in the results~\cite{choi2020channel}. While other methods cause various artifacts, our method produces best effects on the moving objects.}
\label{fig:Vimeo}
\vspace{-1em}
\end{figure*} 
\subsection{Comparisons with Previous Methods}
\label{sec:comparison}


We report the performance on the benchmarks in Table~\ref{tab:UCF101_Vimeo90K_MB}. These methods are officially trained on Vimeo90K dataset except for DVF~\cite{liu2017video} and SuperSlomo~\cite{jiang2018super}. RIFE runs considerably faster than other methods with comparable performance. Meanwhile, RIFE needs only 3.1 gigabytes of GPU memory to process 1080p videos, while some algorithms exceed 12 gigabytes. We get a larger version of our model (RIFE-Large) by simple model scaling, which runs about two times faster than the previous state-of-the-art method SoftSplat~\cite{niklaus2020softmax} with comparable performance. We provide a visual comparison of video clips with large motions from the Vimeo90K testing set in Figure~\ref{fig:Vimeo}, where SepConv~\cite{Niklaus_ICCV_2017} and DAIN~\cite{bao2019depth} produce ghosting artifacts, and CAIN~\cite{choi2020channel} causes missing-parts artifacts. Overall, our method can produce more reliable results. 

\subsection{Ablation Study}
\label{sec:model_ablation}

\begin{table}[t]
	\caption{\textbf{Ablation study on losses, intermediate flow estimation, and fusion process.}
	}
\label{tab:ablation2}
\small
\begin{tabular}{lccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Setting}} & \multicolumn{1}{c}{Vimeo90K} & \multicolumn{1}{c}{M.B.} & \multicolumn{1}{c}{Runtime} \\ 
\multicolumn{1}{c}{}                         & \multicolumn{1}{c}{PSNR}   & \multicolumn{1}{c}{IE}     & \multicolumn{1}{c}{ms (720p)}            \\ \hline 
w/o  and  & 34.62 & 2.37 & 34\\
w/o  & 34.89 & 2.29 & 34\\
w/o  &   \second{35.38} & \second{1.99} & 34 \\
RIFE & \first{35.51} & \first{1.96} & 34 \\ \hline \hline
linear combination~\cite{jiang2018super} & 34.58 & 2.25 & \second{118} \\
CNN model &  34.89 & 2.15 & 121 \\
reversal layer~\cite{xu2019quadratic} &  \second{35.24} & \second{2.06} & 232 \\
RIFE  & \first{35.51} & \first{1.96} & \first{34} \\ \hline \hline
w/o fusion map & 34.97 & 2.23 &  34 \\
w/o residual & 35.03 & 2.19 &  34 \\
w/o context extractor & \second{35.28} & \second{2.00} & \first{30}\\ 
RIFE & \first{35.51} & \first{1.96} & 34 \\
\hline 
\normalsize
\vspace{-1em}
\end{tabular}
\end{table}


We design an ablation study on losses, intermediate flow estimation, and fusion process, shown in Table~\ref{tab:ablation2}. These experiments use the same hyper-parameter setting and evaluation on Vimeo90K~\cite{xue2019video} and MiddleBury~\cite{baker2011database} benchmark.

\paragraph{Ablation on the losses.} To analyze the contributions of adapted census loss  and leakage distillation loss , we train RIFE models from scratch without these losses. We show that these two losses can improve the performance of RIFE, especially . We also notice that the training of the model will become very unstable without .

\paragraph{IFNet vs. flow reversal.}

To demonstrate the effectiveness of IFNet, we compare it with previous intermediate flow estimation methods used in SuperSlomo~\cite{jiang2018super} and EQVI~\cite{liu2020enhanced}. Specifically, we use PWC-Net~\cite{sun2018pwc} with officially pre-trained parameters to estimate the bi-directional flows. Then we implement three flow reversal methods, including linearly combination~\cite{jiang2018super}, using a hidden convolutional layer with  channels, and the flow reversal method from EQVI~\cite{liu2020enhanced} consisting of a reversal layer and an U-Net filter. The PWC-Net and flow reversal modules are jointly trained with our fusion process. As shown in Table~\ref{tab:ablation2}, IFNet is more efficient and accurate in estimating intermediate flows, leading to better interpolation performance. 

\paragraph{Ablation on the fusion process.}
To study the fusion process design, we remove the fusion map and residual term, resulting in blurry results and performance degradation as in Table~\ref{tab:ablation2}. Moreover, we verify the context extractor can improve performance with a small computational overhead.


\begin{figure*}[t]
	\centering\includegraphics[width=1\linewidth]{image/Multi/merged.png}

	\caption{\textbf{Interpolating multiple frames on the Vimeo90K testing dataset by applying RIFE recursively.} We cut out the moving objects according to the green boxes and zoom in the results. RIFE provides smooth and continuous motions.}\label{fig:multi}
	\vspace{-1em}
\end{figure*}

\subsection{Generating Multiple Frames}
\label{sec:model_multi}
To interpolate multiple intermediate frames at different time , we can apply RIFE recursively. Specifically, given any two consecutive input frames , we apply RIFE once to get intermediate frame  at . We feed  and  to get , and we can repeat this process recursively to  interpolate multiple frames. To demonstrate this ability, we provide the visual results for , ,  settings on images with large motions from the Vimeo90K testing set in Figure~\ref{fig:multi}. We observe that RIFE successfully produces smooth and continuous motions. 


\begin{table}[t]
	\caption{\textbf{Quantitative evaluation for  interpolation on the HD benchmark~\cite{bao2019memc}.} Some methods~\cite{bao2019depth, park2020bmbc, cheng2020multiple} can support interpolation at arbitrary time, while others~\cite{choi2020channel, cheng2020video} can be only applied recursively to get multiple frames. }\label{tab:hd8}
	\centering
	\small
	\begin{tabular}{lccccc}
		\hline
		Method & Recursion && 720p & 1080p\\ \hline
		DAIN~\cite{bao2019depth} & \checkmark & \first{19.32} &     \second{28.81} & OOM \\
		DAIN~\cite{bao2019depth} & - & \second{19.03} &     27.97 & OOM \\
		CAIN~\cite{choi2020channel} & \checkmark & 18.37 & 28.31 & 24.71 \\
		BMBC~\cite{park2020bmbc} & \checkmark & 18.16 & 26.70 & OOM\\
		BMBC~\cite{park2020bmbc} & - & 17.12 & 19.60 & OOM 
	    \\ 
	    DSepconv~\cite{cheng2020video} & \checkmark & 16.80 & 19.57 & OOM\\
		EDSC~\cite{cheng2020multiple} & \checkmark & 18.15 & 28.39 & 24.22\\
		EDSC~\cite{cheng2020multiple} & - & 18.89 & 27.03 & \first{25.49}\\
		\hline
		RIFE (Ours) & \checkmark & 18.89 & \first{28.83} & \second{24.96}\\ \hline
	\end{tabular}
	\normalsize
	\vspace{-1em}
\end{table}

To provide a quantitative comparison for  interpolation, we further extract  of every video from HD benchmark~\cite{bao2019memc} and use them to interpolate other frames. We divide the HD benchmark into three subsets with different resolution to test these methods. We show the quantitative PSNR between generated frames and frames of the origin videos in Table~\ref{tab:hd8}. Note that DAIN~\cite{bao2019depth}, BMBC~\cite{park2020bmbc} and EDSC~\cite{cheng2020video} can generate a frame at an arbitrary time between the input ones. But they do not show obvious advantages over recursive frame interpolation methods. Among these methods, DAIN has the best results. However, DAIN's speed is slower than CAIN~\cite{choi2020channel} and RIFE. Overall, RIFE has stable performance and low overhead in the  interpolation scenario. 

\subsection{Limitations}
\label{sec:limitation}
We show that we can get large efficiency and performance improvement by combining proper representations and supervisions. However, our work has some limitations. First, RIFE does not support directly generating frames at arbitrary time. Using additional training data and following techniques proposed in SuperSlomo~\cite{jiang2018super}, QVI~\cite{xu2019quadratic} may be a feasible approach. Second, RIFE focuses on only using two input frames while multi-frame input is proven to improve the effect of frame interpolation~\cite{xu2019quadratic, liu2020enhanced, kalluri2020flavr}. Third, many previous papers point out that SSIM and PSNR are not consistent with human subjective perception~\cite{niklaus2017video, niklaus2020softmax}, and optimizing perceptual loss~\cite{johnson2016perceptual} and LPIPS~\cite{zhang2018unreasonable} may be essential for training a practical model.  	\section{Conclusion}
In this work, we develop an efficient and flexible algorithm for VFI, named RIFE. With the more accurate intermediate flow estimation and our fusion process, RIFE can effectively process videos of different resolution and interpolate multiple frames between two input frames. The impressive results of the proposed method shed light for future research on real-time flow-based interpolation methods. 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
