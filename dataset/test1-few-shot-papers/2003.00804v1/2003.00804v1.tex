\pdfoutput=1

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\usepackage{algorithm}\usepackage{algpseudocode}\renewcommand{\algorithmicrequire}{\textbf{Require:}} 

\usepackage{graphicx}\usepackage{booktabs}\usepackage{subfig}

\usepackage{color}\usepackage{xcolor} \usepackage{multirow} 


\definecolor{PROTOSS PYLON}{RGB}{0, 168, 255} \definecolor{VANADYL BLUE}{RGB}{0, 151, 230} 

\definecolor{PERIWINKLE}{RGB}{156, 136, 255} \definecolor{MATT PURPLE}{RGB}{140, 122, 230} 

\definecolor{RISE-N-SHINE}{RGB}{251, 197, 49} \definecolor{NANOHANACHA GOLD}{RGB}{225, 177, 44} 

\definecolor{DOWNLOAD PROGRESS}{RGB}{76, 209, 55} \definecolor{SKIRRET GREEN}{RGB}{68, 189, 50} 

\definecolor{SEABROOK}{RGB}{72, 126, 176} \definecolor{NAVAL}{RGB}{64, 115, 158} 

\definecolor{NASTURCIAN FLOWER}{RGB}{232, 65, 24} \definecolor{HARLEY DAVIDSON ORANGE}{RGB}{194, 54, 22} 

\definecolor{LYNX WHITE}{RGB}{245, 246, 250} \definecolor{HINT OF PENSIVE}{RGB}{220, 221, 225} 

\definecolor{BLUEBERRY SODA}{RGB}{127, 143, 166} \definecolor{CHAIN GANG GREY}{RGB}{113, 128, 147} 

\definecolor{MAZARINE BLUE}{RGB}{39, 60, 117} \definecolor{PICO VOID}{RGB}{25, 42, 86} 

\definecolor{BLUE NIGHTS}{RGB}{53, 59, 72} \definecolor{ELECTROMAGNETIC}{RGB}{47,54,64} 

\makeatletter
  \newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\algcaption{\def\@captype{algorithm}\caption}
\makeatother


\usepackage{rotating} 


\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Task Augmentation by Rotating for Meta-Learning}

\author{Jialin~Liu \\
  Cognitive Science Department\\
  Xiamen University\\
  Fujian, P. R. China 361005 \\
  \texttt{jialin@stu.xmu.edu.cn} \\
\and
   Fei Chao \\
   Cognitive Science Department \\
      Xiamen University \\
   Fujian, P. R. China 361005 \\
   \texttt{fchao@xmu.edu.cn} \\
   \and
   Chih-Min Lin\\
    Department of Electrical Engineering\\
   Yuan ze University \\
   Chung-Li, Tao-Yuan 320, Taiwan\\
      \texttt{cml@saturn.yzu.edu.tw} \\
}

\maketitle


\begin{abstract}
  Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. In this paper, we introduce a task augmentation method by rotating, which increases the number of classes by rotating the original images 90, 180 and 270 degrees, different from traditional augmentation methods which increase the number of images.
With a larger amount of classes, we can sample more diverse task instances during training. Therefore, task augmentation by rotating allows us to train a deep network by meta-learning methods with little over-fitting. Experimental results show that our approach is better than the rotation for increasing the number of images and achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. The code is available on \url{www.github.com/AceChuse/TaskLevelAug}.
\end{abstract}

\section{Introduction}

Although the machine learning systems have achieved a human-level ability in many fields with a large amount of data, learning from a few examples is still a challenge for modern machine learning techniques. Recently, the machine learning community has paid significant attention to this problem, where few-shot learning is the common task for meta-learning (e.g., \cite{ravi2017optimization,finn2017model,vinyals2016matching,snell2017prototypical}). The purpose of few-shot learning is to learn to maximize generalization accuracy across different tasks with few training examples. In a classification application of the few-shot learning, tasks are generated by sampling from a conventional classification dataset; then, training samples are randomly selected from several classes in the classification dataset. In addition, a part of the examples is used as training examples and testing examples. Thus, a tiny learning task is formed by these examples. The meta-learning methods are applied to control the learning process of a base learner, so as to correctly classify on testing examples. 

Data augmentation is widely used to improve the training of deep learning models. Usually, data augmentation is regarded as an explicit form of regularization \cite{he2016deep,simonyan2014very,krizhevsky2012imagenet}. Data augmentation aims at artificially generating the training data by using various translations on existing data, such as: adding noises, cropping, flipping, rotation, translation, etc. The general idea of data augmentations is increasing the number of images by change data slightly to be different from original data, but the data still can be recognized by human. The new images involved in the classes are identical to the original data, we call this as Image Aug.

However, the minimum units of meta-learning are tasks rather than data, so we should use rotation operation to augment the number of tasks, which is called as task augmentation (referred to Task Aug). Task Aug means increasing the types of task instances by increasing the data that can be clearly recognized as the different classes as the original data and associating them as the novel classes(we show examples in Figure~\ref{TLAExamples}). This is important for the meta-learning, since meta-learning models require to predict unseen classes during the testing phase, increasing the diverseness of tasks would help models to generate to unseen classes.





In experiments, we compared two cases, 1) the new images are converted to the classes of original images and 2) the new images are associated to the novel classes with the method proposed in \cite{bertinetto2018meta} on CIFAR-FS, FC100, miniImageNet few-shot learning tasks, and showed the second case got better results. Then the proposed method is evaluated by experiments with the state of art meta-learning methods~\cite{snell2017prototypical,lee2019meta,bertinetto2018meta} on CIFAR-FS, FC100, miniImageNet few-shot learning tasks, and compare with the results without the data augmentation by rotating. In the comparative experiments, Task Aug by rotating achieves the better accuracy than the original meta-learning methods. Besides, the best results of our experiments exceed the current state-of-art result over a large margin.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=140mm]{TLAExamples.pdf}
\end{center}
\caption{Examples of the novel created classes.}
\label{TLAExamples}
\end{figure*}

\section{Related Work}\label{Related_Work}
Meta-learning involves two hierarchies learning processes: low-level and high-level. The low-level learning process learns to deal with general tasks, often termed as the ``inner loop''; and the high-level learning process learns to improve the performance of a low-level task, often termed as the ``outer loop''. Since models are required to handle sensory data like images, deep learning methods are often applied for the ``outer loop''. However, the machine learning methods applied for the ``inner loop'' are very diverse. Based on different methods in the ``inner loop'', meta-learning can be applied in image recognition~\cite{fei2006one,santoro2016meta,finn2017model,vinyals2016matching,ravi2017optimization}, image generation~\cite{antoniou2017data,zhang2018metagan,rezende2016one}, reinforce learning~\cite{finn2017model,al2017continuous}, and etc. This work focuses on few-shot learning image recognition based on meta-learning. Therefore, in the experiment, the methods applied in the ``inner loop'' are able to classify data, and they are K-nearest neighbor (KNN), Support Vector Machine (SVM) and ridge regression, respectively \cite{snell2017prototypical,lee2019meta,bertinetto2018meta}.

Previous studies have introduced many popular regularization techniques to few-shot learning from deep learning, such as weight decay, dropout, label smooth~\cite{bertinetto2018meta}, and data augmentation. Common data augmentation techniques for image recognition are usually designed manually and the best augmentation strategies depend on dataset. In natural color image datasets, random cropping and random horizontal flipping are the most common. Since the few-shot learning tasks consist of natural color images, the random horizontal flipping and random cropping are applied in few-shot learning. In addition, color (brightness, contrast, and saturation) jitter is often applied in the works of few-shot learning~\cite{gidaris2018dynamic,qiao2018few}.

Other data augmentation technologies related to few-shot learning include generating samples by few-shot learning and generating samples for few-shot learning. The former tried to synthesize additional examples via transferring, extracting, and encoding to create the data of the new classes, that are intra-class relationships between pairs of reference classes' data instances~\cite{hariharan2017low,schwartz2018delta}. The later tried to apply meta-learning in a few-shot generation to generate samples from other models~\cite{antoniou2017data}.In addition to these two types of studies, the data augmentation technology most closed to the new proposed approach is applied to Omniglot dataset, which consists of handwritten words \cite{lake2015human}. They created the novel classes by rotating the original images 90, 180 and 270 degrees~\cite{santoro2016meta}. However, when this approach is applied  for the natural color image, it would be slightly changed, and we will explain this in Section~\ref{Method}.

\section{Method}\label{Method}



\begin{figure*}
\begin{minipage}{.52\textwidth}
  \begin{algorithm}[H] \algcaption{Task Augmentation by Rotating.}\label{Ari_Task_Aug}
  \begin{algorithmic}[1]
    \Require
      Classes set ;
      Max possibility for Task Aug ;
      The delay to Task Aug ; The current count ; The number of ways, shots and queries , , 
      \State 
      \State 
      \State 
      \State 
      \State  Sample  from 
      \ForAll {}
        \State  Sample  from 
        \State  First  of 
        \State  Last  of 
      \EndFor
      \State  Sample  from 
      \ForAll {}
        \State 
        \State  Sample  from 
        \State 
        \State Rotate all   degrees
        \State  First  of 
        \State  Last  of 
      \EndFor
      \State \Return 
  \end{algorithmic}
  \end{algorithm}
\end{minipage}
\begin{minipage}{.45\textwidth}
\begin{center}
  \includegraphics[width=60mm]{TLA.pdf}
  \put(-144,231){\small\color{ELECTROMAGNETIC}}
  \put(-124,231){\small\color{ELECTROMAGNETIC}}
  \put(-80,231){\small\color{ELECTROMAGNETIC}}
  \put(-142,177){\begin{rotate}{-8}\small\color{PICO VOID}\end{rotate}}
  \put(-106,196){\begin{rotate}{34}\footnotesize\color{PICO VOID}1-\end{rotate}}
  \put(-162,210){\footnotesize\color{PICO VOID}}
  \put(-162,201){\footnotesize\color{PICO VOID}}
  \put(-75,191){\small\color{PICO VOID}- from original classes}
  \put(-170,143){\small\color{PICO VOID} from novel classes}
  \put(-32,43){\color{PICO VOID}}
  \put(-32,13){\color{PICO VOID}}
\end{center}
\figcaption{The process of generating a task instance with Task Aug by rotating.}
\label{TLA}
\end{minipage}
\end{figure*}

\subsection{Problem Definition}
We adopt the formulation purposed by \cite{vinyals2016matching} to describe the -way -shot task. A few-shot task contains many task instances (denoted by ), each instance is a classification problem consisting of the data sampled from  classes. The classes are randomly selected from a classes set. The classes set are split into ,  and  for a training class set , a validation classes set , and a test classes set . In particular, each class does not overlap others (i.e., the classes used during testing are unseen classes during training). Data is randomly sampled from ,  and , so as to create task instances for training meta-set , validation meta-set , and test meta-set , respectively. The validation and testing meta-sets are used for model selection and final evaluation, respectively.

The data in each task instance, , are divided into training examples  and validation examples . Both of them only contains the data from  classes which sampled from the appropriate classes set randomly (for a task instance applied during training, the classes form a subset of the training classes set ). In most settings, the training set  consists of  data instances from each class, this processing usually called as a ``shot''. The validation set, , consists of several other data instances from the same classes, this processing is usually called as a ``query''. An evaluation is provided for generalization performance on the  classification task instance . Note that: the validation set of a task instance  (for optimizing model during ``outer loop'') is different from the held-out validation classes set  and meta-set  (for model selection).


\subsection{Task Augmentation by Rotating}
This work is to increase the size of the training classes set, , by rotating all images within the training classes set with 90, 180, 270 degrees. The size, , is increased for three times. In the Omniglot dataset consisting of handwritten words~\cite{santoro2016meta}, this approach works well, since it can rotate a handwritten word multiple of 90 degrees and treat the new one as another word; in addition, it is really possible that the novel word is similar to some words, which are not included in the training classes but existed. 

For natural images, it is obvious that the images generated by rotating is real enough. But should the new generated images be classified as the novel classes or the original classes? It dependents on the similarity between the new images and the original classes. If the most of they are similar enough, the new images should be classified as the original classes, and vice versa. This logic shows that one of the two methods must be effective. Since there are almost not works merge the new images into the original classes which worked well, we assume that Task Aug by rotating is effective for meta-learning, and we will compare two methods in experiments.





Besides, it is different from in handwritten that we assign the new data smaller weights than the original data, so as to make models prioritize learning the features of the original classes, since the images generated by rotating rarely exist in the original data. This way makes the features of the novel classes as a supplement to prevent the augmented data from taking up large capacity in the model, which is same as other common data augment methods.



The smaller weights are implemented in two ways, 1) lower probability and 2) delaying the probability of selecting the novel classes. For a class in a task instance, the probability of the class coming from the novel classes is , and the probability coming from the original classes is . Besides, the initial  is set to , then linearly rises from  to  for the first  tasks. The max probability  is set lower than the proportion of the novel classes in all classes to make each novel class have a lower probability than each original class. The whole process of Task Aug on a classes set is summarized in Algorithm~\ref{Ari_Task_Aug} and Figure~\ref{TLA}.

\subsection{Ensemble}
In this work, we also compare the methods with the training protocol with ensemble method~\cite{huang2017snapshot} in addition to the standard training protocol, which choosing a model by the validation set. The training protocol with an ensemble method use the models with different training epoch to an ensemble model, in order to better use the models obtained in a single training process, and this approach has been proved to be valid for meta-learning by experiments~\cite{liu2018learning}. We adopt this ensemble method. However, unlike \cite{huang2017snapshot} and \cite{liu2018learning} that we did not use cyclic annealing for learning rate and any methods to select models. We directly took the average of the prediction of all models, which are saved according to an interval of 1 epoch. In Section~\ref{experiments}, the methods with this ensemble approach are marked by ``+ens''.

\begin{figure*}[t]
\begin{center}
\subfloat{
\includegraphics[width=56mm]{CIFAR-FS_y_5shot.pdf}
\label{CIFAR-FS_y_5shot}}
\subfloat{
\includegraphics[width=56mm]{FC100_y_5shot.pdf}
\label{FC100_y_5shot}}
\subfloat{
\includegraphics[width=56mm]{miniImageNet_y_5shot.pdf}
\label{miniImageNet_y_5shot}}

\subfloat{
\includegraphics[width=56mm]{CIFAR-FS_y_1shot.pdf}
\label{CIFAR-FS_y_1shot}}
\subfloat{
\includegraphics[width=56mm]{FC100_y_1shot.pdf}
\label{FC100_y_1shot}}
\subfloat{
\includegraphics[width=56mm]{miniImageNet_y_1shot.pdf}
\label{miniImageNet_y_1shot}}

\subfloat{
\includegraphics[width=56mm]{CIFAR-FS_ens_5shot.pdf}
\label{CIFAR-FS_ens_5shot}}
\subfloat{
\includegraphics[width=56mm]{FC100_ens_5shot.pdf}
\label{FC100_ens_5shot}}
\subfloat{
\includegraphics[width=56mm]{miniImageNet_ens_5shot.pdf}
\label{miniImageNet_ens_5shot}}

\subfloat{
\includegraphics[width=56mm]{CIFAR-FS_ens_1shot.pdf}
\label{CIFAR-FS_ens_1shot}}
\subfloat{
\includegraphics[width=56mm]{FC100_ens_1shot.pdf}
\label{FC100_ens_1shot}}
\subfloat{
\includegraphics[width=56mm]{miniImageNet_ens_1shot.pdf}
\label{miniImageNet_ens_1shot}}
\end{center}
   \caption{The accuracies (\%) on meta-test sets with varying probability  for the novel classes.The 95\% confidence interval is denoted by the shaded region.}
\label{Task_Aug_p}
\end{figure*}

\section{Experiments}\label{experiments}
We evaluate the proposed method on few-shot learning tasks. In order to ensure fair, both the results of baseline and Task Aug were run in our own environment. The comparative experiment is designed to answer the following questions: (1) Image Aug and Task Aug by rotating which is able to improve the performance of meta-learning? (2) How much should the probably for the novel classes be set? (3) Is Task Aug by rotating able to improve the performance of the current popular meta-learning methods?

\subsection{Experimental Configuration}
\subsubsection{Backbone}
Following \cite{lee2019meta,oreshkin2018tadam,mishra2017simple}, we used ResNet-12 network in our experiments. The ResNet-12 network had four residual blocks which contains three  convolution, batch normalization and Leaky ReLU with 0.1 negative slope. One  max-pooling layer is used for reducing the size of the feature map. The numbers of the network channels were 64, 160, 320 and 640, respectively. DropBlock regularization~\cite{ghiasi2018dropblock} is used in the last two residual blocks, the conventional dropout~\cite{hinton2012improving} is used in the first two residual blocks. The block sizes of DropBlock were set to 2 and 5 for CIFAR derivatives and ImageNet derivatives, respectively. In all experiments, the dropout possibility was set to 0.1. The global average pooling was not used for the final output of the last residual block.

\subsubsection{Base Learners}\label{Base_Learners}
We used ProtoNets~\cite{snell2017prototypical}, MetaOptNet-SVM~\cite{lee2019meta} (we write it as M-SVM) and Ridge Regression Differentiable Discriminator (R2-D2)~\cite{bertinetto2018meta} as basic methods to verify the effective of Task Aug.

For ProtoNets, we did not use a higher way for training than testing like \cite{snell2017prototypical}. Instead, the equal number of shot and way were used in both training and evaluation, and its output multiplied by a learnable scale before the softmax following \cite{oreshkin2018tadam,lee2019meta}.

For M-SVM, we set training shot to 5 for CIFAR-FS; 15 for FC100; and 15 for miniImageNet; regularization parameter of SVM was set to 0.1; and a learnable scale was used following \cite{lee2019meta}. We did not use label smoothing like \cite{lee2019meta}, because we did not find that label smoothing can improve the performance in our environment. This was also affirmed from the \cite{lee2019meta} author's message on GitHub, that Program language packages and environment might affect results of the meta-learning method.

For R2-D2, we set the same training shot as for M-SVM, and used a learnable scale and bias following \cite{bertinetto2018meta}. It was different from \cite{bertinetto2018meta} we used a fixed regularization parameter of ridge regression which was set to 50 because \cite{bertinetto2018meta} has confirmed that making it learnable might not be helpful.

Last, for all methods, each class in a task instance contained 6 test (query) examples during training and 15 test (query) examples during testing.

\subsubsection{Training Configuration}
Stochastic gradient descent (SGD) was used. Following \cite{sutskever2013importance}, we set weight decay and Nesterov momentum to 0.0005 and 0.9, respectively. Each mini-batch contained 8 task instances. The meta-learning model was trained for 60 epochs, and 1000 mini-batchs for each epoch. We set the initial learning rate to 0.1, then multiplied it by 0.06, 0.012, and 0.0024 at epochs 20, 40 and 50, respectively, as in \cite{gidaris2018dynamic}. The results, which are marked by ``+ens'' were used the 60 models saved after each epoch to become an ensemble model. For the final training, the training classes set was augmented by the validation classes set. When we only chose one model, we will chose the model at the epoch where we got the best model during training on the training classes set. The results of the final run are marked by ``+val'' in this subsection. Since the base idea of ``+ens'' was proposed by other works and ``+val'' is popular for meta-learning, we do not explain more details about them.

For data augmentation, we adopted random crop, horizontal flip, and color (brightness, saturation, and contrast) jitter data augmentation following the work of \cite{gidaris2018dynamic,qiao2018few}. In the experiments of comparing Task Aug and Image Aug by rotating, R2-D2 was applied, and we set  to 80000. In the evaluation of Task Aug for ProtoNets and M-SVM, we set  to the value getting the best results for R2-D2.



\subsubsection{Dataset}
The \textbf{CIFAR-FS}~\cite{bertinetto2018meta} containing all 100 classes from CIFAR-100~\cite{krizhevsky2010cifar} is proposed as few-shot classification benchmark recently. These classes are randomly divided into training classes, validation classes and test classes. The three types contain 64, 16 and 20 classes, respectively. There are 600 nature color images of size  in each class.

The \textbf{FC100}~\cite{oreshkin2018tadam} are also derived from CIFAR-100~\cite{krizhevsky2010cifar}, and the 100 classes are grouped into 20 superclasses. The training, validation, and testing classes contain 60 classes from 12 superclasses, 20 classes from 4 superclasses, and 20 classes from 4 superclasses, respectively. The target is to minimize the information overlap between classes to make it more challenging than current few-shot classification tasks. Same as CIFAR-FS, there are 600 nature color images of size  in each class.

The \textbf{miniImageNet}~\cite{vinyals2016matching} is one of the most popular benchmark for few-shot classification, which contains 100 classes randomly selected from ILSVRC-2012~\cite{russakovsky2015imagenet}. The classes are randomly divided into training classes, validation classes and test classes, and them contain 64, 16 and 20 classes, respectively. There are 600 nature color images of size  in each class. Since \cite{vinyals2016matching} did not release the class splits, we use the more common split proposed by \cite{ravi2017optimization}.



\subsection{Comparison between Task Aug and Image Aug}
To prove our assumption that rotation multi 90 degrees for Task Aug is better than that for Image Aug, we draw the accuracy curves depending on  for both Task Aug and Image Aug (curves showed in Figure~\ref{Task_Aug_p}). The linear rising of  was also used for Image Aug, and  for both Task Aug and Image Aug. In all the results showed in Figure~\ref{Task_Aug_p}, the training classes set was not augmented by the validation classes set.

As shown in Figure~\ref{Task_Aug_p}, the performance of Task Aug on most of the regimes is better than Image Aug and baseline in general. Besides, we observed that: with the increase of , the accuracy rises at first, reaches the peaks between 0.25 and 0.5, then declines and reaches baseline when  at the end, which is the proportion of the novel classes in all classes. The accuracy of Task Aug on CIFAR-FS, FC100 and miniImagNet reach the peaks at 0.5, 0.25 and 0.25 respectively. At the same time, the rotation multi 90 degrees for Image Aug cannot improve or even cause worse performance. 

\subsection{Evaluation of Task Aug}
In order to further prove the proposed approach can improve the performance of meta-learning, we evaluate Task Aug by rotating on several meta-learning methods in this section.

We choose several currently the state of art base learners for experiments, we detail in Section~\ref{Base_Learners}. Besides, the training protocol with ensemble method can get better results than the standard training protocol, we involve it in the experiments. We think this is important, because the proposed method can only be a contribution if it can further improve performance based on the best method available at present.

\begin{table}
\caption{Comparison to the average accuracies (\%) with 95\% confidence intervals between the methods with and without Task Aug on \textbf{CIFAR-FS 5-way 1-shot}.}
\label{CIFAR-FS_1shot}
\begin{center}
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{Method} & Baseline & Task Aug \\
\hline
ProtoNets~\cite{snell2017prototypical} & 71.880.52 & \textbf{74.150.50}\\
ProtoNets (+ens) & 73.950.51 & \textbf{75.890.48}\\
ProtoNets (+val) & 73.200.51 & \textbf{75.100.49}\\
ProtoNets (+ens+val) & 76.050.49 & \textbf{77.280.47}\\
\hline
M-SVM~\cite{lee2019meta} & 71.520.51 & \textbf{72.950.48}\\
M-SVM (+ens) & 74.120.50 & \textbf{75.850.47}\\
M-SVM (+val) & 72.420.50 & \textbf{73.130.47}\\
M-SVM (+ens+val) & 75.910.48 & \textbf{76.750.46}\\
\hline
R2-D2~\cite{bertinetto2018meta} & 72.270.51 & \textbf{74.420.48}\\
R2-D2 (+ens) & 75.060.50 & \textbf{76.510.47}\\
R2-D2 (+val) & 73.520.50 & \textbf{76.020.47}\\
R2-D2 (+ens+val) & 76.400.49 & \textbf{77.660.46}\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Comparison to the average accuracies (\%) with 95\% confidence intervals between the methods with and without Task Aug on \textbf{CIFAR-FS 5-way 5-shot}.}
\label{CIFAR-FS_5shot}
\begin{center}
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{Method} & Baseline & Task Aug \\
\hline
ProtoNets~\cite{snell2017prototypical} & 84.140.36 & \textbf{85.370.35}\\
ProtoNets (+ens) & 85.720.35 & \textbf{87.330.33}\\
ProtoNets (+val) & 85.290.35 & \textbf{86.530.34}\\
ProtoNets (+ens+val) & 86.880.34 & \textbf{88.240.33}\\
\hline
M-SVM~\cite{lee2019meta} & 84.010.36 & \textbf{85.910.36}\\
M-SVM (+ens) & 85.850.34 & \textbf{87.730.33}\\
M-SVM (+val) & 84.940.36 & \textbf{86.940.34}\\
M-SVM (+ens+val) & 87.150.34 & \textbf{88.380.33}\\
\hline
R2-D2~\cite{bertinetto2018meta} & 84.600.36 & \textbf{86.020.35}\\
R2-D2 (+ens) & 86.110.34 & \textbf{87.630.34}\\
R2-D2 (+val) & 85.390.36 & \textbf{86.730.34}\\
R2-D2 (+ens+val) & 87.040.34 & \textbf{88.330.33}\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Comparison to the average accuracies (\%) with 95\% confidence intervals between the methods with and without Task Aug on \textbf{FC100 5-way 1-shot}.}
\label{FC100_1shot}
\begin{center}
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{Method} & Baseline & Task Aug \\
\hline
ProtoNets~\cite{snell2017prototypical} & 37.530.40 & \textbf{38.890.40
}\\
ProtoNets (+ens) & 40.040.41 & \textbf{42.000.43}\\
ProtoNets (+val) & 43.630.43 & \textbf{44.910.46}\\
ProtoNets (+ens+val) & 47.160.46 & \textbf{48.910.47}\\
\hline
M-SVM~\cite{lee2019meta} & 40.500.39 & \textbf{41.170.40}\\
M-SVM (+ens) & 43.240.42 & \textbf{44.380.42}\\
M-SVM (+val) & 46.720.45 & \textbf{47.390.44}\\
M-SVM (+ens+val) & 49.500.46 & \textbf{49.770.45}\\
\hline
R2-D2~\cite{bertinetto2018meta} & 40.660.41 & \textbf{41.470.40}\\
R2-D2 (+ens) & 43.270.42 & \textbf{44.750.43}\\
R2-D2 (+val) & 47.120.44 & \textbf{48.210.45}\\
R2-D2 (+ens+val) & 49.920.45 & \textbf{51.350.46}\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Comparison to the average accuracies (\%) with 95\% confidence intervals between the methods with and without Task Aug on \textbf{FC100 5-way 5-shot}.}
\label{FC100_5shot}
\begin{center}
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{Method} & Baseline & Task Aug \\
\hline
ProtoNets~\cite{snell2017prototypical} & 51.430.39 & \textbf{53.920.39}\\
ProtoNets (+ens) & 54.240.40 & \textbf{56.550.40}\\
ProtoNets (+val) & \textbf{61.160.42} & 60.860.41\\
ProtoNets (+ens+val) & 63.640.43 & \textbf{65.470.42}\\
\hline
M-SVM~\cite{lee2019meta} & 54.830.40 & \textbf{56.230.40}\\
M-SVM (+ens) & 58.490.41 & \textbf{60.140.41}\\
M-SVM (+val) & 62.990.42 & \textbf{63.640.42}\\
M-SVM (+ens+val) & 66.370.42 & \textbf{67.170.41}\\
\hline
R2-D2~\cite{bertinetto2018meta} & 55.850.39 & \textbf{56.290.40}\\
R2-D2 (+ens) & 58.010.40 & \textbf{59.940.41}\\
R2-D2 (+val) & 63.320.40 & \textbf{64.530.42}\\
R2-D2 (+ens+val) & 65.580.42 & \textbf{67.660.42}\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Comparison to the average accuracies (\%) with 95\% confidence intervals between the methods with and without Task Aug on \textbf{miniImageNet 5-way 1-shot}.}
\label{miniImageNet_1shot}
\begin{center}
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{Method} & Baseline & Task Aug\\
\hline
ProtoNets~\cite{snell2017prototypical} & 58.670.48 & \textbf{60.520.48}\\
ProtoNets (+ens) & 62.120.48 & \textbf{63.690.47}\\
ProtoNets (+val) & 60.130.48 & \textbf{62.220.49}\\
ProtoNets (+ens+val) & 63.840.48 & \textbf{65.040.48}\\
\hline
M-SVM~\cite{lee2019meta} & 60.020.45 & \textbf{62.120.44}\\
M-SVM (+ens) & 63.440.45 & \textbf{64.560.44}\\
M-SVM (+val) & 61.580.45 & \textbf{63.140.45}\\
M-SVM (+ens+val) & 64.740.45 & \textbf{65.380.45}\\
\hline
R2-D2~\cite{bertinetto2018meta} & 60.570.44 & \textbf{62.320.45}\\
R2-D2 (+ens) & 63.720.44 & \textbf{64.790.45}\\
R2-D2 (+val) & \textbf{62.820.45} &62.640.44\\
R2-D2 (+ens+val) & 65.500.45 & \textbf{65.950.45}\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{Comparison to the average accuracies (\%) with 95\% confidence intervals between the methods with and without Task Aug on \textbf{miniImageNet 5-way 5-shot}.}
\label{miniImageNet_5shot}
\begin{center}
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{Method} & Baseline & Task Aug \\
\hline
ProtoNets~\cite{snell2017prototypical} & 75.240.37 & \textbf{77.000.36}\\
ProtoNets (+ens) & 78.110.34 & \textbf{79.770.34}\\
ProtoNets (+val) & 76.980.36 & \textbf{77.590.37}\\
ProtoNets (+ens+val) & 79.540.35 & \textbf{80.600.34}\\
\hline
M-SVM~\cite{lee2019meta} & 77.850.34 & \textbf{78.900.34}\\
M-SVM (+ens) & 80.180.32 & \textbf{81.350.32}\\
M-SVM (+val) & 78.650.34 & \textbf{79.970.33}\\
M-SVM (+ens+val) & 81.390.32 & \textbf{82.130.31}\\
\hline
R2-D2~\cite{bertinetto2018meta} & 77.440.34 & \textbf{78.810.34}\\
R2-D2 (+ens) & 79.900.33 & \textbf{81.080.32}\\
R2-D2 (+val) & 78.610.35 & \textbf{79.580.33}\\
R2-D2 (+ens+val) & 81.340.32 & \textbf{81.960.32}\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}

\textbf{Results.} Table~\ref{CIFAR-FS_1shot}-\ref{miniImageNet_5shot} show the average accuracies (\%) with 95\% confidence intervals of the methods with and without Task Aug, and the best results are highlighted. The tables show that the proposed method can improve the performance in most of cases.

We can observe that: some results without the ensemble approach~\cite{huang2017snapshot} of baseline and Task Aug are close, but the advantage of Task Aug is still obvious on the comparison results with the ensemble approach. We suspect that the scale of backbone limits the performance of the best model. A larger scale backbone is needed for the training process with Task Aug. For the results of ensemble approach, since Task Aug reduces the over-fitting, more models during the training process have good performance, which provide ensemble with models of higher quality.

Last we compare the results of this work with the results proposed by the prior works, in order to show how much this work raises the baselines after combining several prior methods and the proposed method, and they are showed in Table~\ref{CIFAR-FS_soa}, \ref{FC100_soa} and \ref{mini_soa}. The tables show that the highest accuracies of our experiments exceeded the current state-of-art accuracies 2\% to 5\%.

\begin{table}[t]
\caption{The average accuracies (\%) with 95\% confidence intervals on CIFAR-FS. CIFAR-FS results from \cite{bertinetto2018meta}. Result from \cite{lee2019meta}.}
\label{CIFAR-FS_soa}
\begin{center}
\begin{tabular}{lcccc}
\toprule[1pt]
\textbf{Method} & \textbf{1-shot} & \textbf{5-shot} \\
\hline
MAML~\cite{finn2017model} & 58.91.9 & 71.51.0\\
R2-D2~\cite{bertinetto2018meta} & 65.30.2 & 79.40.1\\
ProtoNets~\cite{snell2017prototypical} & 72.20.7 & 83.50.5\\
M-SVM~\cite{lee2019meta} & 72.80.7 & 85.00.5\\
\hline
M-SVM (best) (our) & 76.750.46 & \textbf{88.380.33}\\
R2-D2 (best) (our) & \textbf{77.660.46} & 88.330.33\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{The average accuracies (\%) with 95\% confidence intervals on FC100. FC100 result from \cite{lee2019meta}.}
\label{FC100_soa}
\begin{center}
\begin{tabular}{lcccc}
\toprule[1pt]
\textbf{Method} & \textbf{1-shot} & \textbf{5-shot} \\
\hline
TADAM~\cite{oreshkin2018tadam} & 40.10.4 & 56.10.4\\
ProtoNets~\cite{snell2017prototypical} & 37.50.6 & 52.50.6\\
MTL~\cite{sun2019meta} & 45.11.8 & 57.60.9\\
M-SVM~\cite{lee2019meta} & 47.20.6 & 62.50.6\\
\hline
M-SVM (best) (our) & 49.770.45 & 67.170.41\\
R2-D2 (best) (our) & \textbf{51.350.46} & \textbf{67.660.42}\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{The average accuracies (\%) with 95\% confidence intervals on miniImageNet. Result from \cite{lee2019meta}. Here only list the best results of previous works due to the shortage of space.}
\label{mini_soa}
\begin{center}
\begin{tabular}{lcc}
\toprule[1pt]
\textbf{Method} & \textbf{1-shot} & \textbf{5-shot}\\
\hline
\cite{gidaris2018dynamic} & 56.200.86 & 73.000.64 \\
TADAM~\cite{oreshkin2018tadam} & 58.500.30 & 76.700.30\\
LEO~\cite{rusu2018meta} & 61.760.08 & 77.590.12\\
ProtoNets~\cite{snell2017prototypical} & 59.250.64 & 75.600.48\\
M-SVM~\cite{lee2019meta} & 64.090.62 & 80.000.45\\
\hline
M-SVM (best) (our) & 65.380.45 & \textbf{82.130.31}\\
R2-D2 (best) (our) & \textbf{65.950.45} & 81.960.32\\
\bottomrule[1pt]
\end{tabular}
\end{center}
\end{table}




\section{Conclusion}
We proposed a Task Level Data Augmentation (Task Aug), a data augmentation technique that increased the number of training classes to provide more diverse few-show task instances for meta-learning. We proved that Task Aug was valid for CIFAR-FS, FC100, and miniImageNet, and exceeded the result of the previous works. Task Aug achieved the performance by rotating the images 90, 180 and 270 degrees. This method is simple and cost-effective. With the ensemble method, we exceeded the state-of-the-art result over a large margin.

Future work will focus on searching different network structures for meta-learning, since the training with Task Aug would require larger model. Besides, we will try to apply Task Aug to other few-shot learning tasks to verify its effectiveness. Another interesting topic is to build other approaches for Task Aug, such as swapping channel order, picture blend or even auto augmentation.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{paper}
}

\end{document}
