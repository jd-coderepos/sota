

\documentclass[11pt,letterpaper]{article}
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage[hyperref]{emnlp2017}
\usepackage{graphicx}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}

\usepackage{booktabs}
\usepackage{microtype}
\urlstyle{same}

\usepackage{cleveref}
\Crefformat{equation}{(#2#1#3)}

\newcommand{\ab}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\thetab}{\mathbf{\theta}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\argmax}{\textrm{argmax}}

\newcommand{\eat}[1]{\ignorespaces}
\newcommand{\ocom}[1]{\eat{#1}}
\newcommand{\dcom}[1]{\eat{#1}}
\newcommand{\gcom}[1]{\eat{#1}}
\newcommand{\tcom}[1]{\eat{#1}}
\renewcommand{\ocom}[1]{\textcolor{cyan}{[ #1]}}
\renewcommand{\dcom}[1]{\textcolor{blue}{[ #1]}}
\renewcommand{\gcom}[1]{\textcolor{green}{[ #1]}}
\renewcommand{\tcom}[1]{\textcolor{red}{[ #1]}}

\newcolumntype{L}{>{\arraybackslash}m{7cm}}

\emnlpfinalcopy

\def\emnlppaperid{18}




\title{Neural Paraphrase Identification of Questions\\with Noisy Pretraining}

\author{Gaurav Singh Tomar \ \ Thyago Duque \ \ Oscar T{\"{a}}ckstr{\"{o}}m \\ {\bf Jakob Uszkoreit} \ \ {\bf Dipanjan Das}\\Google Inc.\\ \{\texttt{gtomar, duque, oscart, uszkoreit, dipanjand\}}\texttt{@google.com}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We present a solution to the problem of paraphrase identification of questions.
We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model \cite{parikh.etal.2016} results in accurate performance on this task, while being far simpler than many competing neural architectures.
Furthermore, when the model is pretrained on a noisy dataset of automatically collected question paraphrases, it obtains the best reported performance on the dataset.
\end{abstract}

\section{Introduction}
Question paraphrase identification is a widely useful NLP application.
For example, in question-and-answer (QA) forums ubiquitous on the Web, there are vast numbers of duplicate questions.
Identifying these duplicates and consolidating their answers increases the efficiency of such QA forums.
Moreover, identifying questions with the same semantic content could help Web-scale question answering systems that are increasingly concentrating on retrieving focused answers to users' queries.

Here, we focus on a recent dataset published by the QA website Quora.com containing over 400K annotated question pairs containing binary paraphrase labels.\footnote{See \url{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}.}  We believe that this dataset presents a great opportunity to the NLP research community and practitioners due to its scale and quality; it can result in systems that accurately identify duplicate questions, thus increasing the quality of many QA forums.  We examine a simple model family, the \emph{decomposable attention model} of \newcite{parikh.etal.2016}, that has shown promise in modeling natural language inference and has inspired recent work on similar tasks \cite{ChenZLWJ16,kim:iclr2017}.

We present two contributions.  First, to mitigate data sparsity, we modify the input representation of the decomposable attention model to use sums of character -gram embeddings instead of word embeddings.  We show that this model trained on the Quora dataset produces comparable or better results with respect to several complex neural architectures, all using pretrained word embeddings.  Second, to significantly improve our model performance, we pretrain \textit{all} our model parameters on the noisy, automatically collected question-paraphrase corpus Paralex \cite{paralex}, followed by fine-tuning the parameters on the Quora dataset.  This two-stage training procedure achieves the best result on the Quora dataset to date, and is also significantly better than learning \textit{only} the character -gram embeddings during the pretraining stage.

\section{Related Work}\label{rel-work}
Paraphrase identification is a well-studied task in NLP \cite[\textit{inter alia}]{das2009paraphrase,chang2010discriminative,he-gimpel-lin:2015:EMNLP,wang-mi-ittycheriah:2016:COLING}.  Here, we focus on an instance, that of finding questions with identical meaning.  \newcite{lei-EtAl:2016:N16-1} consider a related task leveraging the AskUbuntu corpus \cite{dossantos-EtAl:2015:ACL-IJCNLP}, but it contains two orders of magnitude less annotations, thus limiting the quality of any model. Most relevant to this work is that of \newcite{wang:2017:ijcai}, who present the best results on the Quora dataset prior to this work. The \emph{bilateral multi-perspective matching} model (\textsc{BiMPM}) of \citeauthor{wang:2017:ijcai} uses a character-based LSTM \cite{hochreiter1997long} at its input representation layer, a layer of bi-LSTMs for computing context information, four different types of multi-perspective matching layers, an additional bi-LSTM aggregation layer, followed by a two-layer feedforward network for prediction.
In contrast, the decomposable attention model uses four simple feedforward networks to (self-)attend, compare and predict, leading to a more efficient architecture.
\textsc{BiMPM} falls short of our best performing model pretrained on noisy paraphrase data and uses more parameters than our best model.

Character-level modeling of text is a popular approach.
While conceptually simple, character -gram embeddings are a highly competitive representation \cite{huang-deep-structured-semantic,wieting-EtAl:2016:EMNLP2016,DBLP:journals/corr/BojanowskiGJM16}.
More complex representations built directly from individual characters have also been proposed \cite{sennrich-haddow-birch:2016:P16-12,luong-manning:2016:P16-1,Kim:2016:CNL:3016100.3016285,chung-cho-bengio:2016:P16-1,ling-EtAl:2015:EMNLP2}.
These representations are robust to out-of-vocabulary items, often producing improved results.  Our pretraining procedure is reminiscent of several recent papers \cite[\textit{inter alia}]{wieting-EtAl:2016:EMNLP2016} who aim for general purpose character -gram embeddings.  In contrast, we pretrain all model parameters on automatic but in-domain paraphrase data.  We employ the same neural architecture as our end task, similar to prior work on multi-task learning \cite[\textit{inter alia}]{sogaard-goldberg:2016:P16-2}, but use a simpler learning setup.

\section{Approach}
Our starting point is the decomposable attention model \cite[\textsc{DecAtt} henceforth]{parikh.etal.2016}, which despite its simplicity and efficiency has been shown to work remarkably well for the related task of natural language inference \cite{bowman2015large}.
We extend this model with character -gram embeddings and noisy pretraining for the task of question paraphrase identification.

\subsection{Problem Formulation}\label{subsec:prob_formulation}
Let  and  be two input texts consisting of  and  tokens, respectively.
We assume that each ,  is encoded in a vector of dimension .
A context window of size  is subsequently applied, such that the input to the model  consists of partly overlapping phrases .
The model is estimated using training data in the form of labeled pairs , where  is a binary label indicating whether  is a paraphrase of  or not.
Our goal is to predict the correct label  given a pair of previously unseen texts .

\subsection{The Decomposable Attention Model}
The \textsc{DecAtt} model divides the prediction into three steps: \emph{Attend}, \emph{Compare} and \emph{Aggregate}. Due to lack of space, we only provide a brief outline below and refer to \newcite{parikh.etal.2016} for further details on each of these steps. 

\paragraph{Attend.}
First, the elements of  and  are aligned using a variant of neural attention \cite{bahdanau2014neural} to decompose the problem into the comparison of aligned phrases.

The function  is a feedforward network.  The aligned phrases are computed as follows:

Here  is the subphrase in  that is (softly) aligned to  and vice versa for .
Optionally, the inputs  and  to \Cref{eq:unnormalized_attention} can be replaced by input representations passed through a ``self-attention" step to capture longer context.
In this optional step, we modify the input representations using ``self-attention" to encode compositional relationships between words within each sentence, as proposed by \cite{cheng-dong-lapata:2016:EMNLP2016}.
Similar to \Cref{eq:unnormalized_attention}, we define

The function  and  are feedforward networks. The self-aligned phrases are then computed as follows:

where  is a learned distance-sensitive bias term. Subsequent steps then use modified input representations defined as  := [,] and  := [,].
\paragraph{Compare.}  Second, we separately compare the aligned phrases  and  using a feedforward network :

where the brackets  denote concatenation. 

\paragraph{Aggregate.} Finally, the sets  and  are aggregated by summation. The sum of two sets is concatenated and passed through another feedforward network followed by a linear layer, to predict the label .

\subsection{Character -Gram Word Encodings}
Parikh~et~al.\nocite{parikh.etal.2016} assume that each token ,  is directly embedded in a vector of dimension ; in practice, they used pretrained word embeddings.
Inspired by prior work mentioned in \Cref{rel-work}, we use an alternative approach and instead represent each token as a sum of its embedded character -grams. This allows for more effective parameter sharing at a small additional computational cost.  As observed in \Cref{sec:experiments}, this leads to better results compared to word embeddings.

\subsection{Noisy Pretraining}
While character -gram encodings help in effective parameter sharing, data sparsity remains an issue.
Pretraining embeddings with a task-agnostic objective on large-scale corpora \cite{pennington2014glove} is a common remedy to this problem.
However, such pretraining is limited in the following ways. First, it only applies to the input representation, leaving subsequent parts of the model to random initialization. Second, there may be a domain mismatch unless embeddings are pretrained on the same domain as the end task (e.g., questions). Finally, since the objective used for pretraining differs from that of the end task (e.g., paraphrase identification), the embeddings may be suboptimal.

As an alternative to task-agnostic pretraining of embeddings on a very large corpus, we propose to pretrain all parameters of the model on a modest-sized corpus of automatically gathered, and therefore noisy examples, drawn from a similar domain.\footnote{Paralex is gathered from WikiAnswers, a QA forum.}
As observed in \Cref{sec:experiments}, such noisy pretraining of the full model results in more accurate performance compared to using pretrained embeddings, as well as compared to only pretraining embeddings on the noisy in-domain corpus.\footnote{The Quora data is similar to the Paralex corpus and we exploit this by pretraining our entire model on the latter.  It can be argued that not all sentence pair modeling tasks may benefit similarly from the Paralex corpus and a detailed empirical study is warranted to investigate that; in this work, we restrict our scope to only the question paraphrase identification task, a very useful NLP application by itself.}

\begin{figure}[!t]
\centering
\includegraphics[width=0.8\columnwidth]{quora-curve}
\caption{\label{fig:learning-curve}
Learning curves for the Quora development set with and without pretraining on Paralex.}
\vspace{-0.2cm}
\end{figure}


\begin{table*}[!th]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{cLLcccc}
\toprule
ID & Question 1 & Question 2 & \textsc{DecAtt} & \textsc{DecAtt} & pt-\textsc{DecAtt} & Gold\\
\midrule
A & How shall I start my preparation for IIT-JEE in class 10? & Should I start preparing for the IIT JEE in class 10 only? & N & \textbf{Y} & \textbf{Y} & \textbf{Y}\0.3cm]
\midrule
C & How does PayPal work in India? & Does PayPal work in India? What features of PayPal are available in India? & Y & Y & \textbf{N} & \textbf{N}\0.3cm]
\midrule
E & How is buying land on the moon a good investment? Why do people buy land on the moon? & At \_\mathrm{glove}_\mathrm{char}_\mathrm{char}_\mathrm{word}_\mathrm{char}_\mathrm{word}_\mathrm{glove}_\mathrm{char}_\mathrm{paralex-char}_\mathrm{word}_\mathrm{char}n\ge\in_\mathrm{word}_\mathrm{char}n_\mathrm{word}_\mathrm{glove}n_\mathrm{char}_\mathrm{paralex-char}n_\mathrm{word}n_\mathrm{char}nn_\mathrm{word}_\mathrm{char}_\mathrm{glove}n_\mathrm{paralex-char}_\mathrm{word}_\mathrm{char}_\mathrm{paralex-char}n_\mathrm{char}_\mathrm{char}_\mathrm{char}_\mathrm{glove}_\mathrm{char}_\mathrm{char}n$-gram embeddings results in significantly better accuracy on this task. Second, we showed that pretraining the full model on automatically labeled noisy, but task-specific data results in further improvements.
Our methods perform better than several complex neural architectures and achieve state of the art. While conceptually simple, we believe that these are two important insights that may be more widely applicable within the field of natural language understanding. We leave investigation of this claim to future work that may involve evaluation on related tasks such as recognizing textual entailment.
\bibliography{emnlp2017}
\bibliographystyle{emnlp_natbib}

\end{document}
