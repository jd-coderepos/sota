\documentclass{article}



\usepackage[preprint,nonatbib]{neurips_2020}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{float}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{wrapfig}

\title{Neural Architecture Search with GBDT}



\author{Renqian Luo\thanks{The work was done when the first author was an intern at Microsoft Research Asia.} \\
  University of Science and Technology of China\\
  \texttt{lrq@mail.ustc.edu.cn} \\
  \And
  Xu Tan\\
  Microsoft Research Asia \\
  \texttt{xuta@microsoft.com} \\
  \And
  Rui Wang\\
  Microsoft Research Asia \\
  \texttt{ruiwa@microsoft.com} \\
  \And
  Tao Qin\\
  Microsoft Research Asia \\
  \texttt{taoqin@microsoft.com} \\
  \And
  Enchong Chen\\
  University of Science and Technology of China \\
  \texttt{cheneh@ustc.edu.cn} \\
  \And
  Tie-Yan Liu \\
  Microsoft Research Asia \\
  \texttt{tyliu@microsoft.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Neural architecture search (NAS) with an accuracy predictor that predicts the accuracy of candidate architectures has drawn increasing interests due to its simplicity and effectiveness. Previous works employ neural network based predictors which unfortunately cannot well exploit the tabular data representations of network architectures. As decision tree-based models can better handle tabular data, in this paper, we propose to leverage gradient boosting decision tree (GBDT) as the predictor for NAS and demonstrate that it can improve the prediction accuracy and help to find better architectures than neural network based predictors. Moreover, considering that a better and compact search space can ease the search process, we propose to prune the search space gradually according to important features derived from GBDT using an interpreting tool named SHAP. In this way, NAS can be performed by first pruning the search space (using GBDT as a pruner) and then searching a neural architecture (using GBDT as a predictor), which is more efficient and effective. Experiments on NASBench-101 and ImageNet demonstrate the effectiveness of GBDT for NAS: (1) NAS with GBDT predictor finds top-10 architecture (among all the architectures in the search space) with  test regret on NASBench-101, and achieves  top-1 error rate on ImageNet; and (2) GBDT based search space pruning and neural architecture search further achieves  top-1 error rate on ImageNet. Code is available at \url{https://github.com/renqianluo/GBDT-NAS}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Neural architecture search~(NAS) has shown its effectiveness in automatically designing neural network architectures and been applied in many tasks such as image classification~\cite{nas,nasnet}, object detection~\cite{nasfpn,detnas}, network pruning~\cite{autoslim} and neural machine translation~\cite{evovledtransformer}. Several kinds of methods have been adopted in NAS, including reinforcement learning based~\cite{nasnet,enas}, evolutionary algorithms based~\cite{amoebanet,darts}, Bayesian methods based~\cite{bayesnas}, gradient based~\cite{nao,darts} and accuracy predictor based methods~\cite{nao,neuralpredictor}. Among them, accuracy predictor based approach is simple but yet effective~\cite{nao,neuralpredictor}, in which an accuracy predictor is used to predict the accuracy of candidate architectures in the search space and saves the huge cost induced by training/evaluating these candidate architectures, as long as the accuracy predictor is well trained. Previous works~\cite{perfpred,PNAS,nao,neuralpredictor} usually employ neural network based models such as RNN, CNN, GCN~(Graph Convolutional Network) to build the predictor. Unfortunately, neural predictor cannot fully exploit the discrete representation of an architecture, which is more like tabular data preferred by tree based models such as decision trees, instead of raw image/text/speech data with spatial or temporal smoothness which is preferred by neural networks. Therefore, neural accuracy predictor may achieve inferior accuracy given limited amount of architecture and accuracy pairs, and thus affect the results of NAS.

In this paper, we build the accuracy predictor based on gradient boosting decision trees (GBDT) to better model the discrete characteristics of the architecture representation. Our proposed NAS algorithm with a GBDT based predictor works as follows: 1) We reformulate the general representation of an architecture into one-hot feature to make it suitable for GBDT. Given an architecture, we denote the presence or absence of an operator as 1 or 0. For examples, we show two architectures and their features in Table~\ref{tbl:tablur}. 2) A GBDT predictor is trained with a few architecture-accuracy pairs. 3) The trained GBDT is used to predict the accuracy of more architectures in the search space and we select architectures with top predicted accuracy for further validation. We name this algorithm as GBDT-NAS.
\begin{wraptable}{r}{5.8cm}
\small
\label{tbl:tablur}
\caption{Two examples of architectures with tabular data representation and the corresponding accuracy. `arch' stands for architecture.}
\centering
\begin{tabular}{l|c|c}
\toprule
Feature & arch 1 & arch 2\\
\midrule
layer 1 is conv1x1 & 1 & 0 \\
layer 1 is conv3x3 & 0 & 1 \\
layer 2 connects layer 1 & 1 & 1 \\
layer 2 is conv1x1 & 1 & 0 \\
layer 2 is conv3x3 & 0 & 1 \\
layer 3 connects layer 1 & 1 & 0 \\
layer 3 connects layer 2 & 0 & 1 \\
layer 3 is conv1x1 & 0 & 0 \\
layer 3 is conv3x3 & 1 & 1 \\
layer 4 connects layer 1 & 0 & 0 \\
layer 4 connects layer 2 & 0 & 1 \\
layer 4 connects layer 3 & 1 & 1 \\
layer 4 is conv1x1 & 0 & 0 \\
layer 4 is conv3x3 & 1 & 1 \\
\midrule
accuracy (\%) & 92.50 & 93.20 \\
\bottomrule
\end{tabular}
\end{wraptable}


In addition to the search method, the search/architecture space itself is an important factor demining the final performance of NAS. A better and compact search space can simplify the search process and help NAS to find better architectures. As GBDT models are easier to tell the importance/contribution of a feature (i.e., the presence or absence of a network operator in candidate architectures), we further propose to leverage some interpretable tools such as SHAP~\cite{unifiedshap} to find not-well-performed operators and prune them from the search space. Consequently, we propose to perform NAS by first pruning the search space with the GBDT predictor and then searching in the pruned search space using the GBDT predictor, leading to a more efficient and effective NAS method which we call GBDT-NAS-3S.

Experiments on NASBench-101 and ImageNet demonstrate the effectiveness of our GBDT based NAS. Specifically, GBDT-NAS achieves  average test regret on NASBench-101, and  top-1 error rate on ImageNet. Moreover, GBDT-NAS-3S achieves  top-1 error rate on ImageNet.

To sum up, our main contributions are listed as following:
\begin{itemize}
    \item We propose to use GBDT as the accuracy predictor to perform architecture search, and show that it leads to better prediction accuracy against neural network based predictors.
    \item We further propose to first prune the search space with GBDT and then conduct architecture search, which makes the overall search process more efficient and effective.
\end{itemize}

\section{Related Works}
\label{sec:rework}
\paragraph{Neural Architecture Search}
\cite{nas} introduces to use reinforcement learning to automatically search neural architecture and brings it to a thriving research area. Lots of works are emerged to explore different search algorithms including reinforcement learning~\cite{nasnet,enas}, evolutionary algorithm~\cite{genetic_cnn,evolvingNN,EA_2017,amoebanet}, Bayesian optimization~\cite{bayesnas}, performance prediction~\cite{perfpred,PNAS,nao,neuralpredictor} and gradient based optimization~\cite{nao,darts,seminas}. Among these algorithms, accuracy predictor based methods have drawn lots of interests due to its simplicity and effectiveness while other algorithms need careful design and tuning. Accordingly, our proposed method is based on accuracy predictor.

\paragraph{Accuracy Predictor in NAS}
Considering that evaluating candidate architectures via training it for several epochs raises extremely high cost for NAS,~\cite{perfpred} proposes to predict the accuracy of a given discrete architecture via RNN to speed up the NAS process. Further, NAO~\cite{nao} builds the accuracy predictor based on LSTM and fully connected layer. Recently, using GCN to model the discrete architecture is proposed~\cite{neuralpredictor}, which achieves some improvements. However, neural network~(e.g., LSTM, GCN, Transformer) based predictors need delicate design for different tasks~\cite{nao,neuralpredictor} and additional human efforts. We propose to utilize GBDT as predictor, which is much simpler and more general and can be easily applied to different tasks without much tuning. More importantly, architectures are commonly represented as sequences of discrete symbols, which are similar to tabular data. Consequently, tree based models~(e.g., GBDT) can better exploit the discrete  (tabular data) representation of architectures compared to neural network based models.

\paragraph{Search Space Search}
Search space plays an essential role in the search phase~\cite{rdarts,evanas}. How to get an appropriate search space is critical in NAS.~\cite{designspace} proposed to progressively prune the large search space via statistical tool~(i.e., empirical distribution function) on a set of randomly sampled architectures to identify the best choice for different factors. Despite the impressive results, the search process heavily relies on human efforts. Specifically the order of pruning disappointing choices is manually decided which is similar to greedy search, and only one factor is considered at each time while different factors may have interactions. As GBDT automatically identifies the importance of different features according to some criterion, it is more interpretable than neural networks. Accordingly it can be used to figure out how different features contribute to the output. In this paper, we propose to use GBDT to automatically prune the search space. Moreover, we can conduct higher-order analysis via combinations of different features during pruning.

\section{GBDT-NAS}
\label{sec:gbdtnas}
In this section, we introduce our GBDT based NAS method. We describe how to use GBDT as accuracy predictor for architecture search, which we call \textbf{GBDT-NAS}. An architecture is more like a tabular data which is preferred by GBDT, rather than raw image/text/speech data with spatial or temporal smoothness which is preferred by neural networks. Therefore, we propose to leverage GBDT as the accuracy predictor, which is simpler and more effective than neural models. To the best of our knowledge, we are the first time to introduce GBDT in NAS. In the following paragraphs, we first describe the design of input feature and output value to train a GBDT model, and then formulate our NAS algorithm that uses GBDT as the accuracy predictor.  

We describe a discrete neural network architecture as a sequence of tokens from bottom layer to top layer~(e.g., `conv 1x1, conv 3x3, conv 1x1, conv 3x3' to describe a 4-layer neural network, where each position represents the categorical choice for a layer) following~\cite{nas,nasnet,enas,amoebanet,nao}. Considering categorical features may not be a good choice since the relative value of the category ID is meaningless, we convert the category feature into one-hot feature with -dimension, where  is the number of candidate operations and the value of the one-hot feature is `' or `'~(representing whether to use this operation or not). For example, if the candidate operations only contain `conv 1x1 and conv 3x3', then the input feature of the 4-layer architecture demonstrated above is `[1,0,0,1,1,0,0,1]'. Examples of cell based architectures where connections are included are demonstrated in Table~\ref{tbl:tablur}. The output of GBDT is the accuracy of an architecture, where the target accuracy is normalized to ease model training. We use two ways for normalization: 1) min-max normalization~\cite{nao}, which rescales the values into , i.e.,  . 2) standardization~\cite{neuralpredictor}, which rescales the accuracy to be zero mean and standard variance, i.e., . The training of GBDT aims to minimize the mean squared error between predicted accuracy and target accuracy. 
We name our GBDT based search algorithm as GBDT-NAS. It contains  iterations and each iteration mainly follows three steps:
\begin{itemize}
    \item \textbf{Train Predictor.} Train the GBDT predictor with  architecture-accuracy pairs.
    \item \textbf{Predict.} Predict the accuracy of  randomly sampled architectures.
    \item \textbf{Validation.} Evaluate  architectures with the top  predicted accuracies. Combine them with the  architecture-accuracy pairs for next iteration.
\end{itemize}
Finally, the architecture with the highest valid accuracy is selected to deploy.


\section{GBDT-NAS Enhanced with Search Space Search}
In this section, we leverage the trained GBDT as a search space pruner, and then formulate our method GBDT-NAS-3S, which leverages GBDT to first search the search space by pruning unpromising candidate operations and then search the architectures by predicting accuracies in the pruned space.

\subsection{GBDT as Search Space Pruner}
\paragraph{Motivation}
Search space is critical for NAS~\cite{random,rdarts}. First, different search spaces have different upper bounds of accuracy that may outweigh the effect of search algorithm. For instance, random search in a good search space may outperform a well-established search algorithm in a bad search space. Second, the size of search space affects the effectiveness of search algorithm. Specifically, large space contains a broader distribution of architectures and potentially contains better architectures, however it is costly to search. Manually designed small space is fast to search but may suffer from lower upper bound which limits the performance of search algorithms and results in marginal differences for different algorithms.

Ideally, when an accuracy predictor is well trained, one may consider using it to predict the accuracy of all the architectures in the search space~(i.e., set  to be the size of search space). This is practicable when the search space is small~(e.g., size of NASBench-101 search space is only ). However, when applying to tasks with large search space, traversing all the architectures is time consuming although predicting the accuracy of a single architecture is negligible for GBDT. For example, a commonly used search space~\cite{proxylessnas,neuralpredictor} based on MobileNet-v2~\cite{mobilenetv2} for ImageNet consists of roughly  architectures\footnote{It consists of multiple stacked stages, and each stage contains multiple layers, yielding  layers in total to search. Candidate operations include mobile inverted bottleneck convolution layers~\cite{mobilenetv2} with various kernel sizes  and expansion ratios , as well as zero-out layer., yielding  candidate operations in total. The search space is roughly .} which would take millions of years for GBDT to predict on a single CPU. A common approach is to randomly sample a small set~(e.g., ) of architectures from the huge search space for prediction. However, considering the normal distribution of architectures~(most architectures have moderate accuracies while few architectures have extremely good and bad accuracies), it is of low probability to find a good architecture using random sampling. 

We come up with a question: Is it possible to search within a sub-space derived from the large one that contains potentially better architectures? Consequently, sampling a small set~(e.g., ) of architectures from this sub-space would potentially get well-performing architecture with higher probability. However, it is challenging to prune a large search space into a relatively small but well-performing one. A straightforward method to prune a search space is to analyze which operation could yield bad architectures based on a number of architecture-accuracy pairs manually and then remove this operation from the search space~\cite{designspace}. However, this pruning process requires human knowledge and explanation on the search space and does not scale well. Considering that GBDT can automatically determine the importance of a feature (the presence or absence of an operation) and can explain the accuracy prediction due to the advantage of tree-based model, in this paper, we leverage the explainable GBDT as the pruner to shrink a search space without human knowledge. A simple way is to use the automatically derived feature importance from the trained GBDT\footnote{The feature importance in GBDT is determined by the average information gain when choosing this feature.}. However, feature importance in GBDT only considers the contribution when training a GBDT model, which may not be entirely consistent with the feature importance in the accuracy of an architecture.




\paragraph{How to Use GBDT for Pruning} In this paper, we leverage SHapley Additive exPlanation (SHAP) value~\cite{unifiedshap}, which can measure the positive or negative contribution of a feature in GBDT prediction~(i.e., architecture accuracy in the GBDT-based accuracy predictor) for each sample\footnote{SHAP value~\cite{unifiedshap} is a unified measure of different Shapley values~\cite{shapleyregression,shapleysampling,quantinputinfluence} which reflects the importance of features on the result considering their cooperation and interaction by solving a combined cooperative game theory problem~\cite{shapleyvalue}. It attributes to each feature a value~(real number) showing how it affects the output~(positively or negatively).}. Accordingly, in each iteration, we can get the average SHAP values for each feature in current search space. Then, we select the one with the lowest and extremely negative SHAP value, which implies the most negative contribution on the predicted accuracy, and then prune the search space according the feature. For example, if the average SHAP value of a feature value, layer\_1\_is\_conv1x1=1 is extremely negative~(e.g., ), then we prune the search space with layer\_1\_is\_conv1x1=1, and then all the architectures in the remaining space have layer\_1\_is\_conv1x1=0. We do this progressively until a certain number of features or all the extremely negative features have been pruned.

Further, since operations in a network may have interactions to cooperatively affect the network accuracy, pruning the search space considering the combinations of several operations is more reasonable and effective. We calculate the interaction SHAP values between any two features, which imply their cooperative contribution to the final accuracy prediction. Then we sort the combinations according to their interaction SHAP values and start from the most negative ones to prune. This can quickly find the most important feature combinations that affect the model output. We name the pruning method that uses SHAP value as first-order pruning and that uses interaction SHAP value as second-order pruning.

\subsection{GBDT-NAS-3S}
In this subsection, we introduce our GBDT-NAS enhanced with search space search (GBDT-NAS-3S for short), which leverages GBDT to first search a good search space (GBDT as a pruner) and then search a good architecture (GBDT as an accuracy predictor, i.e., GBDT-NAS). As shown in Alg.~\ref{alg:GBDT-NAS-3S}, compared to GBDT-NAS, GBDT-NAS-3S additionally uses the trained GBDT predictor  to perform the search space search by pruning unpromising operations. If we remove the pruning process~(line 7), the algorithm degenerates to GBDT-NAS in Sec.~\ref{sec:gbdtnas}.

\begin{algorithm}[ht]
\small
\caption{GBDT-NAS-3S}
\label{alg:GBDT-NAS-3S}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Number of initial architectures . Number of architectures  to predict. Number of top architectures  to evaluate. Number of search iterations . Number of features to prune .
\STATE Pruned feature set .
\STATE Randomly sample  architectures to form .
\STATE Train and evaluate architectures in  and get accuracy numbers .
\FOR {}
\STATE Train GBDT  using  and .
\STATE Prune  features from the search space to get the pruned features , and .
\STATE Randomly sample  architectures with the constraints  and get .
\STATE Predict the accuracy of the architectures in  with  to get .
\STATE Select architectures from  with top  predicted accuracy in  and form .
\STATE Train and evaluate each architecture in  and get .
\STATE .
\ENDFOR
\STATE \textbf{Output}: The architecture within  with the best accuracy.
\end{algorithmic}
\end{algorithm}


\section{Experiments}
We demonstrate the effectiveness of our proposed methods through experiments on two datasets: NASBench-101~\cite{nasbench101} and ImageNet. Since the search space of NASBench-101 is quite small, we only evaluate GBDT-NAS on NASBench-101, and evaluate both GBDT-NAS and GBDT-NAS-3S on ImageNet, which has much larger search space.

\subsection{NASBench-101}
NASBench-101 is a dataset for evaluating NAS algorithms, which eliminates the efforts of evaluating candidate architectures. It defines a narrow search space containing only  CNN architectures. Each architecture has been trained and evaluated on CIFAR-10 for  times following exactly the same pipeline and setting. Thus, one can get architecture-accuracy pairs effortless via querying from the dataset, and use them to quickly evaluate a NAS algorithm and fairly compare it with other algorithms. The search space is cell based following common practice~\cite{nasnet,enas,darts} which involves connections between different leaves besides operations. We follow the encoding guide by the authors to represent the architecture in a sequence~\cite{nasbench101}. For each node, we use its adjacent vector concatenated with its operation to represent it. Standardization is used to rescale the accuracy when training predictors.

\subsubsection{Evaluating Accuracy Predictor}
We first show how the GBDT performs as a pure accuracy predictor. Specifically, we randomly sample  architectures and get their validation accuracies from the dataset to form the architecture-accuracy pairs.  of them are used as training set and the remaining  pairs are used as test set. We train a GBDT model based on LightGBM~\cite{lightgbm}\footnote{https://github.com/microsoft/LightGBM} with  trees and  leaves per tree. We also evaluate LSTM, GCN and Transfomer based accuracy predictors as baselines. For LSTM based predictor, we follow NAO~\cite{nao} to use a single layer LSTM of hidden size  followed by two fully connected layers of hidden size . For GCN based accuracy predictor, we follow~\cite{neuralpredictor} and use a 3-layer GCN of hidden size  followed by a fully connected layer of hidden size . For Transformer based accuracy predictor, we follow~\cite{transformer} and use a 4-layer Transformer model.
\begin{wraptable}{r}{5cm}
\small
\caption{Pairwise accuracy of different predictors.}
\centering
\begin{tabular}{cc}
\toprule
Method  & Pairwise Acc. (\%)\\
\midrule
Transformer  & 65\\
GCN & 80\\
LSTM & 80\\
GBDT & \textbf{82}\\
\bottomrule
\end{tabular}
\label{tbl:nasbench:acc}
\end{wraptable}
All the models are trained on the same training set and tested on the same test set described above. To evaluate the predictors, we compute the pairwise accuracy following~\cite{nao} on the held out test set via , where  is the 0-1 indicator function. We run each setting for  times and report the average results in Table~\ref{tbl:nasbench:acc}. It is shown that Transformer performs the worst among all the methods with a poor accuracy of  which is just better than random guess~(), and GBDT based predictor achieves better prediction accuracy~() than neural network based methods~(LSTM, GCN, Transformer). We empirically find that even an improvement of  of pairwise accuracy is critical to the final accuracy improvement on discovered architecture.

\subsubsection{Evaluating GBDT-NAS}
\paragraph{Setup} 
We use the trained GBDT predictor to conduct architecture search. During search, we get the valid accuracy of an architecture by randomly sampling one from the  runs in NASBench-101 in order to simulate a single run. When the search is finished, we report the mean test accuracy of the  runs of the discovered architecture. From the statistics of the dataset~\cite{nasbench101,neuralpredictor}, the best test accuracy is , while the architectures with the highest validation accuracy~() have an average mean test accuracy of , which we call \textbf{oracle} following~\cite{neuralpredictor}. Considering that only valid accuracy is allowed during the search, it is more reasonable to expect the algorithm to discover the oracle.
We mainly compare our method with several baselines: random search, regularized evolution~\cite{amoebanet}, NAO~\cite{nao} and Neural Predictor~\cite{neuralpredictor}. For all the algorithms, we limit the number of architecture-accuracy pairs can be queried from the dataset to  for fair comparison,  which is equivalent to limiting the training and evaluation cost of architectures. For NAO we use the open source code~\footnote{\url{https://github.com/renqianluo/NAO_pytorch}} and adapt it to NASBench-101 search space. For Neural Predictor, we implement by ourselves since the authors do not release the code. Specifically for Neural Predictor, we train the GCN predictor on  architecture-accuracy pairs queried from the dataset and select  architectures with top predicted accuracies for further validation, where  architecture-accuracy pairs are queried in total. For our proposed GBDT-NAS, we also train GBDT on  architecture-accuracy pairs and select top  architectures with the highest predictions for validation~(i.e., ) yielding  queries of architecture-accuracy pairs in total. Since the search space of NASBench-101 is small, we set  to be the size of the whole search space and search for only 1 iteration~(i.e., ). We run each algorithm for  times and report the average results. Apart from reporting only the final test accuracy, since the search space has an upper bound of accuracy and several well-performing algorithms are reaching the upper bound, we show the test regret (the gap to the best test accuracy  in the dataset) suggested in NASBench-101 publication~\cite{nasbench101} and the ranking of the accuracy among the whole space to better illustrate the improvements of our method.

\paragraph{Results} We list the results in Table~\ref{tbl:nasbench}. Random search achieves  mean test accuracy with a confidence interval of  (alpha=). This implies that even  is a significant difference and there exists a large margin for improvement. We can see that when using the same number of architecture-accuracy pairs, GBDT-NAS significantly outperforms all the baselines with a  test accuracy and corresponding  test regret. It is worth noticing that this is very close to the oracle~() with only  gap, and ranks the -th among the  architectures which is remarkably better than other algorithms.
\begin{table}
\small
\caption{NAS results on NASBench-101. `\#Queries' indicates the number of architecture-accuracy pairs queried from the dataset in total, simulating the architectures required to be trained and evaluated. For Neural Predictor, since the authors do not release the source code, we implement the algorithm ourselves for fair comparison under the same number of queries.}
\label{tbl:nasbench}
\centering
\begin{tabular}{cccccc}
\toprule
Method              & \#Queries & Test Acc. (\%) & SD (\%) & Test Regret (\%) & Ranking \\
\midrule
Random Search       & 2000      & 93.64    & 0.25         & 0.68      & 1749\\
NAO~\cite{nao}     & 2000      & 93.90    & 0.03         & 0.42      & 169\\
RE~\cite{amoebanet}& 2000      & 93.96    & 0.05         & 0.36      & 89\\
Neural Predictor~\cite{neuralpredictor} & 2000 & 94.04  & 0.05 & 0.28 & 34\\
\midrule
GBDT-NAS         & 2000      &\textbf{94.14}   & 0.04   & \textbf{0.18} & \textbf{10}\\
\bottomrule
\end{tabular}
\end{table}

\subsection{ImageNet}
\begin{table}[htbp]
\centering
\small
\caption{Performances of different NAS methods on ImageNet dataset. For NAO, we use the open source code and search on the same search space used in this paper. We run ProxylessNAS by optimizing accuracy without latency for fair comparison. `first-order' and `second-order' indicate using first-order and second order SHAP values for pruning respectively.}
\begin{tabular}{lcccc}
\toprule
Model/Method                      & Top-1 Err. (\%) & Top-5 Err. (\%) & Params (Million) & FLOPS (Million) \\ 
\midrule
MobileNetV2~\cite{mobilenetv2}               & 25.3      & -        & 6.9  & 585 \\
ShuffleNet 2 (v2)~\cite{shufflenet}  & 25.1      & -        &  5 & 591  \\
\midrule
NASNet-A~\cite{nas}               & 26.0      & 8.4       & 5.3       & 564 \\
AmoebaNet-A~\cite{amoebanet}      & 25.5      & 8.0       & 5.1       & 555 \\
MnasNet~\cite{mnasnet}            & 25.2      & 8.0       & 4.4       & 388 \\
PNAS~\cite{PNAS}                  & 25.8      & 8.1       & 5.1       & 588 \\
DARTS~\cite{darts}                & 26.9      & 9.0       & 4.9       & 595 \\
SNAS~\cite{snas}                  & 27.3      & 9.2       & 4.3       & 522\\
P-DARTS~\cite{pdarts}             & 24.4      & 7.4       & 4.9       & 557\\
PC-DARTS~\cite{pcdarts}           & 24.2      & 7.3       & 5.3       & 597\\
Efficienet-B0~\cite{efficientnet} & 23.7     & 6.8       & 5.3       & 390 \\
\midrule
Random Search                    & 25.2    & 8.0     & 5.1   & 578 \\
NAO~\cite{nao}             & 24.5    & 7.8     & 6.5   & 590 \\
ProxylessNAS~\cite{proxylessnas}  & 24.0      & 7.1       & 5.8       & 595\\
Manual Pruning                   & 24.1    & 7.0     & 6.1   & 550 \\
\midrule 
GBDT-NAS                      & 24.2    & 7.1   & 5.8      & 588 \\
GBDT-NAS-3S (first-order)   & 23.8    & 6.9   & 5.6      & 572 \\
GBDT-NAS-3S (second-order)  & \textbf{23.5} & \textbf{6.8}     & 6.4 & 577 \\
\bottomrule
\end{tabular}
\label{tbl:ImageNet}
\end{table}
\paragraph{Setup}
During search, we split out  images from the training set for validation. We adopt weight-sharing method to perform one-shot search~\cite{enas} since training on ImageNet is too costly. We train the supernet containing all the candidates for  steps at each iteration with a batch size of . The GBDT is trained with  trees and  leaves per tree, where the hyperparameters are chosen according to the performance on valid set. Min-max normalization is applied to normalize the accuracy numbers for GBDT. We use  for evaluating both GBDT-NAS and GBDT-NAS-3S as described in Alg.~\ref{alg:GBDT-NAS-3S}, according to preliminary study considering both effectiveness and efficiency. Since the search space contains  candidate operations and  layers, the number of features for an architecture is . Specifically in GBDT-NAS-3S, we prune  features at each iteration to quickly narrow the space. The search runs for  day on 4 V100 GPUs. We limit the FLOPS of the discovered architecture to be less than M for fair comparison with other works~\cite{nasnet,amoebanet,darts,efficientnet,pcdarts,pdarts} and train it for  epochs following~\cite{proxylessnas}. We implement random search as a baseline by randomly sampling  architectures and training them using the supernet. The one with the best validation accuracy is selected for final evaluation. We also implement manual pruning to perform search space search as a baseline where we sequentially prune disappointing operations by doing statistics on architectures with certain operations similar to~\cite{designspace}.
\paragraph{Results} We list the results in Table~\ref{tbl:ImageNet} and all our experiments are conducted for  times. Results of our experiments are averaged across  runs. We can see that our proposed methods all demonstrate promising results. When using GBDT only as accuracy predictor, GBDT-NAS achieves  error rate. Further, when enhanced with search space search, GBDT-NAS-3S achieves more improvements. Second-order pruning with  error rate outperforms first-order pruning with  error rate, demonstrating the effectiveness of considering combinations of feature interactions during search. We will try to use higher-order SHAP value to prune the search space in the future. Compared to other NAS works, our GBDT-NAS-3S achieves better top-1 error rate under the M FLOPS constraints.

\begin{figure}[htbp]
\centering
\subfigure[]{
		\label{fig:shap}
		\includegraphics[width=0.48\columnwidth]{shap.png}
	}
	\subfigure[]{
		\label{fig:acc}
		\includegraphics[width=0.48\columnwidth]{fig1.png}
	}
\caption{(a) SHAP values for several features. `MB3' and `MB6' denote mobile inverted bottleneck convolution layer with an expansion ratio of 3 and 6 respectively. (b) Average valid accuracy of the architectures sampled from search space pruned by different methods evaluated with the shared weights during the search phase.}
\label{fig:studysss}
\end{figure}

\paragraph{Study of Search Space Search}
We conduct some analyses on GBDT-NAS-3S in searching the search space. First, to demonstrate how we perform pruning using SHAP values, we visualize the SHAP values of some features using the official tool\footnote{\url{https://github.com/slundberg/shap}} in Fig.~\ref{fig:shap}. Notice that the colored area contains multiple data points~(architectures). Taking \textbf{`layer 1 is MB3 7x7'} as an example, the SHAP value of this feature is extremely negative when the feature value is `', which indicates that using a `MB3' layer with kernel size  at layer 1 usually has bad accuracy. So we prune this feature and the following sampling process will not sample architectures that use `MB3 7x7' at layer 1. 

Second, to demonstrate the effectiveness of the pruning methods during the search phase, we compare the average valid accuracies of architectures sampled from the search space with different pruning methods at each iteration~(We totally run for  iterations) in Fig.~\ref{fig:acc}. The baseline uses no pruning method~(i.e., GBDT-NAS, as shown in the red bar). We also show the results of pruning according to feature importance determined by GBDT for comparison. Specifically, we sort the features by their feature importance, and then prune the features in sequence. For each feature, we first calculate the average valid accuracies of architectures with and without the feature. Then the feature is pruned if the average valid accuracy of architectures with the feature is lower than the ones without the feature and the gap exceeds a certain margin~(e.g., ). Note that for each single method, the valid accuracy increases with more iterations since the supernet is trained to be better. At each iteration, compared to baseline without pruning, pruned search spaces show higher accuracy. Meanwhile SHAP value based pruning methods outperform the feature importance based method. This demonstrates that our GBDT based pruning indeed finds better sub-space. Moreover, the gap between SHAP value based pruning methods and baseline is increasing, implying that the sub-space after each iteration is becoming better. We provide more studies in the Appendix.

\section{Conclusion}
In this paper, considering the tabular data representation of architectures, we introduce GBDT into neural architecture search and develop two NAS algorithms: GBDT-NAS and GBDT-NAS-3S. In GBDT-NAS, we use GBDT as an accuracy predictor to predict the accuracy of candidate architectures. We further enhance GBDT-NAS with search space search and propose GBDT-NAS-3S, which first uses GBDT to prune the search space and then uses GBDT as an accuracy predictor for architecture search. Experiments on NASBench-101 and ImageNet demonstrate that GBDT-NAS archives better accuracy than previous neural network base predictors and GBDT-NAS-3S achieves even better results with GBDT based space pruning. In our future work, we plan to use GBDT to search in more general search space and on more complicated tasks.

\section*{Acknowledgements}
We sincerely thank Guolin Ke for his valuable comments and suggestions.


\bibliography{main_arxiv}
\bibliographystyle{plain}

\clearpage
\centerline{\Huge{Appendix}}

\setcounter{section}{0}

\section{The SHAP Value based Pruning Algorithm}
In this section, we describe the pruning algorithm using SHAP value. We get the SHAP value of some architecture-accuracy pairs sampled from the search space. Then we sort the features according to the SHAP values. We start pruning from the most negative one. Since the one-hot feature represents using or not using the corresponding operation, we only prune the features with value `1' (indicating that using this operation may lead to inferior prediction). Feature with value `0' will not be pruned since by default the operation is to be sampled. For example, by default `conv1x1' is in the candidate operations and will be sampled. When `layer\_1\_is\_conv1x1=1' has a very negative SHAP value which means using `conv1x1' at `layer\_1' will lead to inferior prediction, we prune the `layer\_1\_is\_conv1x1' from the search space and `conv1x1' will not be sampled to be the operation of `layer\_1'. However, when `layer\_1\_is\_conv1x1=0' has a very negative SHAP value which means not using `conv1x1' at `layer\_1' will lead to inferior prediction, we do nothing since by default `conv1x1' will be sampled to be the operation of `layer\_1' in other cases. We give the first-order pruning algorithm in Alg.~\ref{alg:1pruning} and second-order pruning algorithm in Alg.~\ref{alg:2pruning}.

\begin{algorithm}[ht]
\small
\caption{First-Order Pruning}
\label{alg:1pruning}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Trained GBDT performance predictor . Current architecture pool . One-hot feature set . Number of features to be pruned .
\STATE .
\STATE .
\STATE Sort  according to .
\FOR {}
\STATE .
\STATE .
\STATE .
\IF{}
\STATE .
\ENDIF
\ENDFOR
\STATE \textbf{Output}: The pruned feature set .
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\small
\caption{Second-Order Pruning}
\label{alg:2pruning}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Trained GBDT performance predictor . Current architecture pool . One-hot feature set . Number of features to be pruned .
\STATE .
\STATE .
\STATE .
\STATE Sort  according to .
\FOR {}
\STATE .
\STATE .
\STATE .
\STATE .
\STATE .
\STATE .
\STATE .
\IF{}
\STATE .
\ELSIF{}
\STATE .
\ELSIF{}
\STATE .
\ENDIF
\ENDFOR
\STATE \textbf{Output}: The pruned feature set .
\end{algorithmic}
\end{algorithm}

\section{Ablation Study of Hyperparameters}
In this section, we study the hyperparameters in GBDT-NAS. We mainly study  which respectively stands for the number of architecture-accuracy pairs to train the GBDT predictor, and the number of architectures to select for further validation.

\begin{figure}[htbp]
\centering
\subfigure[]{
		\label{fig:study_n}
		\includegraphics[width=0.48\columnwidth]{study_n.png}
	}
	\subfigure[]{
		\label{fig:study_k}
		\includegraphics[width=0.48\columnwidth]{study_k.png}
	}
\caption{(a) Pairwise accuracy of GBDT predictor under different . (b) Mean test accuracy of discovered architecture on NASBench-101 under different .}
\end{figure}

\subsection{Study of }
Since GBDT predictor trains on  architecture-accuracy pairs, the number  is critical to the effect of the predictor. Small  may result in bad accuracy and large  leads to more resources required. Following the experiments of evaluating the accuracy predictor, we train the GBDT on  architecture-accuracy pairs queried from NASBench-101 dataset, and measure the pairwise accuracy of the GBDT predictor on a held-out set containing  architecture-accuracy pairs. We plot the results in Fig.~\ref{fig:study_n}, from which we can see that the pairwise accuracy of GBDT predictor rises as the  increases. Although larger  leads to better accuracy, we choose  in our final experiments due to the concern of resource required.

\subsection{Study of }
Since the predictor is not  accurate, we cannot completely rely on the prediction to rank all the architectures. Therefore we need to further evaluate the top  architectures by really training and validating them on the valid dataset~(querying the valid accuracy of these architectures in NASBench-101). Small  may potentially miss some well performing architectures that predicted to be bad by the predictor incorrectly, and large  leads to more resource required. We set  and vary the value of  in GBDT-NAS and evaluate on NASBench-101. Results are depicted in Fig.~\ref{fig:study_k}. We can see that, when only a small number of architectures with top predicted accuracy are validated, the final discovered architecture shows a moderate test accuracy. With more architectures are validated, the discovered architecture achieves better test accuracy. This implies that since the predictor is not 100\% accurate, we cannot fully rely on its prediction to return the one with the best predicted accuracy. We need to select a number of architectures with the top predicted accuracies for further evaluation~(training and validation) and return the one with the highest validated accuracy as the discovered one.


\section{Discovered Architectures and Analysis of Pruning the Search Space via GBDT}
In this section, we conduct analysis on the effect of using GBDT for search space pruning. The analysis is conducted on the ImageNet dataset. 

We plot the first tree of the trained GBDT predictor in the file Tree1.pdf~(due to the limited space, we cannot show it here), from which we can see that the most important features determined by the GBDT are close to the root. Instead of manually deciding which feature to prune~\cite{designspace}, we can rely on GBDT to prune the search space staring from the most important features. The pruning method has been described in the previous text. 

Here we mainly focus on using SHAP value to prune the search space. We first plot the architectures for ImageNet discovered by GBDT-NAS, GBDT-NAS-3S (first-order pruning) and GBDT-NAS-3S (second-order pruning) in Fig.~\ref{fig:arch1}, Fig.~\ref{fig:arch2} and Fig.~\ref{fig:arch3} respectively. And we again show the SHAP values for several features here in Fig.~\ref{fig:shap}.
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\columnwidth]{arch1.png}
\caption{Architecture discovered by GBDT-NAS.}
\label{fig:arch1}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\columnwidth]{arch2.png}
\caption{Architecture discovered by GBDT-NAS-3S (first-order pruning).}
\label{fig:arch2}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\columnwidth]{arch3.png}
\caption{Architecture discovered by GBDT-NAS-3S (second-order pruning).}
\label{fig:arch3}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\columnwidth]{shap.png}
\caption{SHAP values for several features.}
\label{fig:shap}
\end{figure}

We have noticed that the SHAP value indicates that using a `MB3 7x7' operation at `layer 1' is not good since `\textbf{layer 1 is MB3 7x7=1}' has the most negative SHAP value among the features. The architecture discovered by GBDT-NAS in Fig.~\ref{fig:arch1} uses `MB3 7x7' at `layer 1'~(`layer 1' starts from the layer right after the first gray bar, while the first two layers `Conv 3x3' and `MB1 3x3' before the bar are fixed as stem layers~\cite{proxylessnas}), which results in the final test error rate of . However, the two architectures discovered by GBDT-NAS-3S in Fig.~\ref{fig:arch2} and Fig.~\ref{fig:arch3} do not choose this operation at `layer 1' as the operation is pruned due to its negative effect to the prediction determined by the SHAP value during the search. And these two architectures show better test error rate~( and ) against the one by GBDT-NAS.

\end{document}
