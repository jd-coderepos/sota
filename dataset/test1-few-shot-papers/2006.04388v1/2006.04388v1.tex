\documentclass{article}





\usepackage[preprint]{neurips_2020}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{authblk}

\usepackage{color}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{bm}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{threeparttable}
\usepackage[dvipsnames]{xcolor}


\newcommand*{\dif}{\mathop{}\!\mathrm{d}}
\renewcommand{\vec}[1]{\ensuremath{\pmb{#1}}}
\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\set}[1]{\ensuremath{\mathscr{#1}}}
\makeatletter
\@tfor\next:=abcdefghijklmnopqrstuvwxyxz\do
 {\begingroup\edef\x{\endgroup
    \noexpand\@namedef{v\next}{\noexpand\vec{\next}}}\x}
\@tfor\next:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\do
 {\begingroup\edef\x{\endgroup
    \noexpand\@namedef{m\next}{\noexpand\mat{\next}}}\x}
\@tfor\next:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\do
 {\begingroup\edef\x{\endgroup
    \noexpand\@namedef{s\next}{\noexpand\set{\next}}}\x}
\makeatother

\title{Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for \\ Dense Object Detection}



\author[]{Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, \\ and Jian Yang\thanks{Corresponding author.}}
\affil{PCALab, Nanjing University of Science and Technology  Momenta Nanjing University \\   Microsoft Research  Tsinghua University}
\affil{\scriptsize \{xiang.li.implus, shuochen, jinhuitang, csjyang\}@njust.edu.cn, wangwenhai362@163.com, \\ xlhu@mail.tsinghua.edu.cn, \{apeterswu,junl.mldl\}@gmail.com}




\begin{document}

\maketitle
\begin{abstract} 
  One-stage detector basically formulates object detection as dense classification and localization (i.e., bounding box regression). The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an \emph{individual} prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the \emph{representations} of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference (i.e., separately trained but compositely used in test) and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty which is often the case in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain \emph{continuous} labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the \emph{continuous} version for successful optimization. On COCO {\tt test-dev}, GFL achieves 45.0\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\%) and ATSS (43.6\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\%, at 10 FPS on a single 2080Ti GPU. Code and pretrained models are available at https://github.com/implus/GFocal.
  








  


















\end{abstract}

\section{Introduction}
\label{intro}

Recently, dense detectors have gradually led the trend of object detection, whilst the attention on the \emph{representation} of bounding boxes and their localization quality estimation leads to the encouraging advancement. Specifically, bounding box \emph{representation} is modeled as a simple Dirac delta distribution \cite{he2019bounding,lin2017focal,zhang2019freeanchor,tian2019fcos,zhang2019bridging}, which is widely used over past years. As popularized in FCOS \cite{tian2019fcos}, predicting an additional localization quality (e.g., IoU score \cite{wu2020iou} or centerness score \cite{tian2019fcos}) brings consistent improvements of detection accuracy, when the quality estimation is combined (usually multiplied) with classification confidence as final scores \cite{jiang2018acquisition,huang2019mask,tian2019fcos,wu2020iou,zhu2019iou} for the rank process of Non-Maximum Suppression (NMS) during inference. Despite their success, we observe the following problems in existing practices: 


\textbf{Inconsistent usage of localization quality estimation and classification score between training and inference:}  (1) In recent dense detectors, the localization quality estimation and classification score are usually trained independently but compositely utilized (e.g., multiplication) during inference \cite{tian2019fcos,wu2020iou} (Fig.~\ref{fig_QFL_drawback_1_cropped}(a));
(2) The supervision of the localization quality estimation is currently assigned for positive samples only \cite{jiang2018acquisition,huang2019mask,tian2019fcos,wu2020iou,zhu2019iou}, which is unreliable as negatives may get chances to have uncontrollably higher quality predictions (Fig.~\ref{fig_QFL_drawback_2_cropped}(a)). These two factors result in a gap between training and test, and would potentially degrade the detection performance, e.g., negative instances with randomly high-quality scores could rank in front of positive examples with lower quality prediction during NMS.




\begin{figure}[t]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/QFL_drawback_1_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{Comparisons between existing separate representation and proposed joint representation of classification and localization quality estimation. (a): Current practices \cite{jiang2018acquisition,tian2019fcos,wu2020iou,zhu2019iou,zhang2019bridging} for the separate usage of the quality branch (i.e., IoU or centerness score) during training and test. (b): Our joint representation of classification and localization quality enables high consistency between training and inference.}
	\label{fig_QFL_drawback_1_cropped}
	\vspace{-16pt}
\end{figure}


\iffalse
\begin{figure}[t]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/QFL_drawback_2_cropped.pdf}}
	\end{center}
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{(a): We demonstrate some background patches (\textcolor{red}{A} and \textcolor{red}{B}) with extremely high predicted quality scores (e.g., IoU score > 0.9), based on the optimized IoU-branch model in Fig.~\ref{fig_QFL_drawback_1_cropped}(a).
	The scatter diagram in (b) denotes the randomly sampled instances with their predicted scores, where the \textcolor{blue}{blue} points clearly illustrate the weak correlation between predicted classification scores and predicted IoU scores for separate representations. The part in \textcolor{red}{red} circle contains many possible negatives with large localization quality predictions, which may potentially rank in front of true positives and impair the performance. Instead, our joint representation (\textcolor{OliveGreen}{green} points) forces them to be equal and thus avoids such risks. (c) denotes the single-model single-scale speed (ms) vs. accuracy (AP) on COCO test-dev among state-of-the-art approaches.}
	\label{fig_QFL_drawback_2_cropped}
	\vspace{-12pt}
\end{figure}
\fi

\begin{wrapfigure}{l}{0.6\textwidth}
  \vspace{-20pt}
  \begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.6\textwidth]{./figs/QFL_drawback_2_real.pdf}}
	\end{center}
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{Unreliable IoU predictions of current dense detector with IoU-branch. (a): We demonstrate some background patches (\textcolor{red}{A} and \textcolor{red}{B}) with extremely high predicted quality scores (e.g., IoU score > 0.9), based on the optimized IoU-branch model in Fig.~\ref{fig_QFL_drawback_1_cropped}(a).
	The scatter diagram in (b) denotes the randomly sampled instances with their predicted scores, where the \textcolor{blue}{blue} points clearly illustrate the weak correlation between predicted classification scores and predicted IoU scores for separate representations. The part in \textcolor{red}{red} circle contains many possible negatives with large localization quality predictions, which may potentially rank in front of true positives and impair the performance. Instead, our joint representation (\textcolor{OliveGreen}{green} points) forces them to be equal and thus avoids such risks. }
  \label{fig_QFL_drawback_2_cropped}
  \vspace{-12pt}
\end{wrapfigure}


\textbf{Inflexible representation of bounding boxes:} The widely used bounding box representation can be viewed as Dirac delta distribution \cite{girshick2015fast,ren2015faster,he2017mask,cai2018cascade,lin2017focal,tian2019fcos,kong2019foveabox,zhang2019bridging} of the target box coordinates. However, it fails to consider the ambiguity and uncertainty in datasets (see the unclear boundaries of the figures in Fig.~\ref{fig_DFL_drawback_22_cropped}). 
Although some recent works \cite{he2019bounding,choi2019gaussian} model boxes as Gaussian distributions, it is too simple to capture the real distribution of the locations of bounding boxes. In fact, the real distribution can be more arbitrary and flexible \cite{he2019bounding}, without the necessity of being symmetric like the Gaussian function.





To address the above problems, we design new representations for the bounding boxes and their localization quality. \textbf{For localization quality representation}, we propose to merge it with the classification score into a single and unified representation: a classification vector where its value at the ground-truth category index refers to its corresponding localization quality (typically the IoU score between the predicted box and the corresponding ground-truth box in this paper). In this way, we unify classification score and IoU score into a joint and single variable (denoted as ``classification-IoU joint representation''),
which can be trained in an end-to-end fashion, whilst directly utilized during inference (Fig.~\ref{fig_QFL_drawback_1_cropped}(b)). As a result, it eliminates the training-test inconsistency (Fig.~\ref{fig_QFL_drawback_1_cropped}(b)) and enables the strongest correlation (Fig.~\ref{fig_QFL_drawback_2_cropped} (b)) between localization quality and classification. Further, the negatives will be supervised with 0 quality scores, thereby the overall quality predictions become more confidential and reliable. It is especially beneficial for dense object detectors as they rank all candidates regularly sampled across an entire image. \textbf{For bounding box representation}, we propose to represent the arbitrary distribution (denoted as ``General distribution'' in this paper) of box locations by directly learning the discretized probability distribution over its continuous space, without introducing any other stronger priors (e.g., Gaussian \cite{he2019bounding, choi2019gaussian}).
Consequently, we can obtain more reliable and accurate bounding box estimations, whilst being aware of a variety of their underlying distributions (see the predicted distributions in Fig.~\ref{fig_DFL_drawback_22_cropped} and Supplementary Materials).

The improved representations then pose challenges for optimization. Traditionally for dense detectors, the classification branch is optimized with Focal Loss \cite{lin2017focal} (FL). FL can successfully handles the class imbalance problem via reshaping the standard cross entropy loss. However, for the case of the proposed classification-IoU joint representation, in addition to the imbalance risk that still exists, we face a new problem with continuous IoU label (01) as supervisions, as the original FL only supports discrete  category label currently. We successfully solve the problem by extending FL from  discrete version to its continuous variant, termed Generalized Focal Loss (GFL). Different from FL, GFL considers a much general case in which the globally optimized solution is able to target at any desired continuous value, rather than the discrete ones. More specifically in this paper, GFL can be specialized into Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), for optimizing the improved two representations respectively: QFL focuses on a sparse set of hard examples and simultaneously produces their \emph{continuous} 01 quality estimations on the corresponding category; DFL makes the network to rapidly focus on learning the probabilities of values around the \emph{continuous} locations of target bounding boxes, under an arbitrary and flexible distribution.






\begin{figure}[t]
\vspace{-6pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/DFL_drawback_22_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-14pt}
	\caption{Due to occlusion, shadow, blur, etc., the boundaries of many objects are not clear enough, so that the ground-truth labels (white boxes) are sometimes not credible and Dirac delta distribution is limited to indicate such issues. Instead, the proposed learned representation of General distribution for bounding boxes can reflect the underlying information by its shape, where a flatten distribution denotes the unclear and ambiguous boundaries (see \textcolor{red}{red} circles) and a sharp one stands for the clear cases. The predicted boxes by our model are marked \textcolor{green}{green}. }
	\label{fig_DFL_drawback_22_cropped}
	\vspace{-14pt}
\end{figure}

We demonstrate three advantages of GFL: (1) It bridges the gap between training and test when one-stage detectors are facilitated with additional quality estimation, leading to a simpler, joint and effective representation of both classification and localization quality; (2) It well models the flexible underlying distribution for bounding boxes, which provides more informative and accurate box locations; (3) The performance of one-stage detectors can be consistently boosted without introducing additional overhead. On COCO {\tt test-dev}, GFL achieves 45.0\% AP with ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\%) and ATSS (43.6\%). Our best model can achieve a single-model single-scale AP of 48.2\% whilst running at 10 FPS on a single 2080Ti GPU. 



















\section{Related Work}
\vspace{-4pt}
\textbf{Representation of localization quality.} Existing practices like Fitness NMS \cite{tychsen2018improving}, IoU-Net \cite{jiang2018acquisition}, MS R-CNN \cite{huang2019mask}, FCOS \cite{tian2019fcos} and IoU-aware \cite{wu2020iou} utilize a separate branch to perform localization quality estimation in a form of IoU or centerness score. As mentioned in Sec.~\ref{intro}, this separate formulation causes the inconsistency between training and test as well as unreliable quality predictions.
Instead of introducing an additional branch, PISA \cite{cao2019prime} and IoU-balance \cite{wu2019iou} assign different weights in the classification loss based on their localization qualities, aiming at enhancing the correlation between the classification score and localization accuracy. However, the weight strategy is of implicit and limited benefits since it does not change the optimum of the loss objectives for classification.



\textbf{Representation of bounding boxes.} Dirac delta distribution \cite{girshick2015fast,ren2015faster,he2017mask,cai2018cascade,lin2017focal,tian2019fcos,kong2019foveabox,zhang2019bridging} governs the representation of bounding boxes over past years. Recently, Gaussian assumption \cite{he2019bounding,choi2019gaussian} is adopted to learn the uncertainty by introducing a predicted variance. Unfortunately, existing representations are either too rigid or too simplified, which can not reflect the complex underlying distribution in real data. In this paper, we further relax the assumption and directly learn the more arbitrary, flexible General distribution of bounding boxes, whilst being more informative and accurate. 





\section{Method}
\vspace{-4pt}
In this section, we first review the original Focal Loss \cite{lin2017focal} (FL) for learning dense classification scores of one-stage detectors. Next, we present the details for the improved representations of localization quality estimation and bounding boxes, which are successfully optimized via the proposed Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), respectively. Finally, we summarize the formulations of QFL and DFL into a unified perspective termed Generalized Focal Loss (GFL), as a flexible extension of FL, to facilitate further promotion and general understanding in the future.

\textbf{Focal Loss (FL)}. The original FL \cite{lin2017focal} is proposed to address the one-stage object detection scenario where an extreme imbalance between foreground and background classes often exists during training. A typical form of FL is as follows (we ignore  in original paper~\cite{lin2017focal} for simplicity):

where  specifies the ground-truth class and  denotes the estimated probability for the class with label .  is the tunable focusing parameter. Specifically, 
FL consists of a standard cross entropy part  and a dynamically scaling factor part , where the scaling factor  automatically down-weights the contribution of easy examples during training and rapidly focuses the model on hard examples.


\textbf{Quality Focal Loss (QFL)}.
To solve the aforementioned inconsistency problem between training and test phases, we present a joint representation of localization quality (i.e., IoU score) and classification score (``classification-IoU'' for short), where its supervision softens the standard one-hot category label and leads to a possible float target 
on the corresponding category (see the classification branch in Fig.~\ref{fig_gfocal_cropped}). Specifically,  denotes the negative samples with 0 quality score, and  stands for the positive samples with target IoU score . Note that the localization quality label  follows the conventional definition as in \cite{wu2020iou,jiang2018acquisition}: IoU score between the predicted bounding box and its corresponding ground-truth bounding box during training, with a dynamic value being 01. Following \cite{lin2017focal,tian2019fcos}, we adopt the multiple binary classification with sigmoid operators  for multi-class implementation. For simplicity, the output of sigmoid is marked as . 

Since the proposed classification-IoU joint representation requires dense supervisions over an entire image and the class imbalance problem still occurs, the idea of FL must be inherited. However, the current form of FL only supports  discrete labels, but our new labels contain decimals. Therefore, we propose to extend the two parts of FL for enabling the successful training under the case of joint representation: (1) The cross entropy part  is expanded into its complete version ; (2) The scaling factor part  is generalized into the absolute distance between the estimation  and its continuous label , i.e.,  (), here  guarantees the non-negativity. 
Subsequently, we combine the above two extended parts to formulate the complete loss objective, which is termed as Quality Focal Loss (QFL):
Note that  is the global minimum solution of QFL. QFL is visualized for several values of  in Fig.~\ref{fig_fcos_atss_reg_cropped}(a) under quality label . Similar to FL, the term  of QFL behaves as a modulating factor: when the quality estimation of an example is inaccurate and deviated away from label , the modulating factor is relatively large, thus it pays more attention to learning this hard example. As the quality estimation becomes accurate, i.e., , the factor goes to 0 and the loss for well-estimated examples is down-weighted, in which the parameter  controls the down-weighting rate smoothly ( works best for QFL in our experiments). 


\begin{figure}[t]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.95\textwidth]{./figs/gfocal_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{The comparisons between conventional methods and our proposed GFL in the head of dense detectors. GFL includes QFL and DFL. QFL effectively learns a joint representation of classification score and localization quality estimation. DFL models the locations of bounding boxes as General distributions whilst forcing the networks to rapidly focus on learning the probabilities of values close to the target coordinates. } \label{fig_gfocal_cropped}
	\vspace{-12pt}
\end{figure}
\begin{figure}[t]
    \vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/fcos_atss_reg_cropped.pdf}}
	\end{center}
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{(a): The illustration of QFL under quality label . (b): Different flexible distributions can obtain the same integral target according to Eq.~\eqref{eq_integral}, thus we need to focus on learning probabilities of values around the target for more reasonable and confident predictions (e.g., (3)). (c): The histogram of bounding box regression targets of ATSS over all training samples on COCO {\tt trainval35k}.}
	\label{fig_fcos_atss_reg_cropped}
	\vspace{-8pt}
\end{figure}


\begin{figure}[t]
\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.9\textwidth]{./figs/QFL_compare_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-10pt}
	\caption{Illustrations of modified versions for separate/implicit and joint representation. The baseline without quality branch is also provided.}
	\label{fig_QFL_compare_cropped}
	\vspace{-12pt}
\end{figure}

\textbf{Distribution Focal Loss (DFL).} Following \cite{tian2019fcos,zhang2019bridging}, we adopt the relative offsets from  the location to the four sides of a bounding box as the regression targets (see the regression branch in Fig. \ref{fig_gfocal_cropped}). Conventional operations of bounding box regression model the regressed label  as Dirac delta distribution , where it satisfies  and is usually implemented through fully connected layers. More formally, the integral form to recover  is as follows:

According to the analysis in Sec.~\ref{intro}, instead of the Dirac delta \cite{ren2015faster,he2017mask,cai2018cascade,tian2019fcos,zhang2019bridging} or Gaussian \cite{choi2019gaussian,he2019bounding} assumptions, we propose to directly learn the underlying General distribution  without introducing any other priors. Given the range of label  with minimum  and maximum  (), we can have the estimated value  from the model ( also meets ):

To be consistent with convolutional neural networks, we convert the integral over the continuous domain into a discrete representation, via discretizing the range  into a set  with even intervals  (we use  for simplicity). Consequently, given the discrete distribution property , the estimated regression value  can be presented as:

As a result,  can be easily implemented through a softmax  layer consisting of  units, with  being denoted as  for simplicity. Note that  can be trained in an end-to-end fashion with traditional loss objectives like SmoothL1 \cite{girshick2015fast}, IoU Loss \cite{tychsen2018improving} or GIoU Loss \cite{rezatofighi2019generalized}. However, there are infinite combinations of values for  that can make the final integral result being , as shown in Fig.~\ref{fig_fcos_atss_reg_cropped}(b), which may reduce the learning efficiency. Intuitively compared against (1) and (2), distribution (3) is compact and tends to be more confident and precise on the bounding box estimation, which motivates us to optimize the shape of  via explicitly encouraging the high probabilities of values that are close to the target . Furthermore, it is often the case that the most appropriate underlying location, if exists, would not be far away from the 
coarse label. Therefore, we introduce the Distribution Focal Loss (DFL) which forces the network to rapidly focus on the values near label , by explicitly enlarging the probabilities of  and  (nearest two to , ). As the learning of bounding boxes are only for positive samples without the risk of class imbalance problem, we simply apply the complete cross entropy part in QFL for the definition of DFL:
Intuitively, DFL aims to focus on enlarging the probabilities of the values around target  (i.e.,  and ). The global minimum solution of DFL, i.e,  (see Supplementary Materials), can guarantee the estimated regression target  infinitely close to the corresponding label , i.e., , which also ensures its correctness as a loss function.




\textbf{Generalized Focal Loss (GFL).} Note that QFL and DFL can be unified into a general form, which is called the Generalized Focal Loss (GFL) in the paper. Assume that a model estimates probabilities for two variables  as  (), with a final prediction of their linear combination being . The corresponding continuous label  for the prediction  also satisfies . Taking the absolute distance  () as modulating factor, the specific formulation of GFL can be written as:

\textbf{Properties of GFL.}  reaches its global minimum with , which also means that the estimation  perfectly matches the continuous label , i.e.,  (see the proof in \textcolor{black}{Supplementary Materials}). Obviously, the original FL \cite{lin2017focal} and the proposed QFL and DFL are all \emph{special cases} of GFL (see Supplementary Materials for details). Note that GFL can be applied to any one-stage detectors. The modified detectors differ from the original detectors in two aspects. First, during inference, we directly feed the classification score (joint representation with quality estimation) as NMS scores without the need of multiplying any \emph{individual} quality prediction if there exists (e.g., centerness as in FCOS \cite{tian2019fcos} and ATSS \cite{zhang2019bridging}). Second, the last layer of the regression branch for predicting each location of bounding boxes now has  outputs instead of  output, which brings \emph{negligible} extra computing cost as later shown in Table~\ref{tab:qfl_dfl_atss}. 
\iffalse

\fi


 









\textbf{Training Dense Detectors with GFL.}
We define training loss  with GFL:

where  is QFL and  is DFL. Typically,  denotes the GIoU Loss as in \cite{tian2019fcos,zhang2019bridging}.  stands for the number of positive samples.  (typically 2 as default, similarly in \cite{chen2019mmdetection}) and  (practically , averaged over four directions) are the balance weights for  and , respectively. The summation is calculated over all locations  on the pyramid feature maps \cite{lin2017feature}.  is the indicator function, being 1 if  and 0 otherwise. Following the common practices in the official codes \cite{chen2019mmdetection,tian2019fcos,zhang2019bridging,li2019learning}, we also utilize the quality scores to weight  and  during training. 




\begin{table*}[t]
	\vspace{10pt}
	\renewcommand\arraystretch{1.2}
	\setlength{\fboxrule}{0pt}
	\begin{center}
		\begin{subtable}[ht]{\textwidth}
\begin{center}
\resizebox{\textwidth}{!}{
		\begin{tabular}{l|ccc|ccc||ccc|ccc}
			\hline
			\multirow{2}{*}{Type}& \multicolumn{6}{c||}{FCOS \cite{tian2019fcos}} & \multicolumn{6}{c}{ATSS \cite{zhang2019bridging}} \\\cline{2-13}
			& AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP \\
			\hline
			\hline
			w/o quality branch & 37.8 & 56.2 & 40.8 & 21.2 & 42.1 & 48.2 & 38.0 & 56.5 & 40.7 & 20.6 & 42.1 & 49.1 
			  \\\hline
			centerness-branch \cite{tian2019fcos} &  38.5 &56.8 &41.6 &22.4 &42.4 &49.1
			& 39.2 & 57.4 & 42.2 & 23.0 & 42.8 & 51.1
			 \\
			IoU-branch \cite{wu2020iou,jiang2018acquisition} & 38.7 & 56.7 & \textbf{42.0} & 21.6 & 43.0 & 50.3 & 39.6 & 57.6 & \textbf{43.0} & \textbf{23.3} & 43.7 & 51.2 
			 \\
			centerness-guided \cite{wu2019iou} & 37.9 & 56.7 & 40.7 & 21.2 & 42.1 & 49.4 & 38.2 & 56.2 & 41.0 & 21.5 & 41.9 & 49.7\\
			IoU-guided \cite{wu2019iou} & 38.2 & 57.0 & 41.1 & \textbf{22.5} & 42.2 & 48.9 & 38.9 &57.4 &41.8 &22.8 &42.4& 50.6	
			 \\\hline
			joint w/ QFL \textbf{(ours)}& \textbf{39.0} & \textbf{57.8} & 41.9 & 22.0 & \textbf{43.1} & \textbf{51.0}
			 & \textbf{39.9} & \textbf{58.5} & \textbf{43.0} &22.4 & \textbf{43.9} & \textbf{52.7}
			 \\
			\hline
		\end{tabular}}
	\vspace{-4pt}
	\captionsetup{font={scriptsize}}
	\caption{\textbf{Comparisons between separate/implicit and joint representation (ours)}: The joint representation optimized by QFL achieves better performance than other counterparts. We also observe that the quality predictions (especially IoU scores) are necessary for obtaining competitive AP.} \label{tab:k}
	\end{center}
\end{subtable}
\vspace{5mm}
\begin{subtable}[ht]{0.63\textwidth}
	\begin{center}
\resizebox{\textwidth}{!}{
		\begin{tabular}{l|ccc|ccc}
			\hline
			Method & AP & AP & AP & AP & AP & AP \\
			\hline\hline
			FoveaBox \cite{kong2019foveabox}& 36.4 & \textbf{55.8} & 38.8 & 19.4 & 40.4 & 47.7 \\ FoveaBox \cite{kong2019foveabox} + joint w/ QFL & \textbf{37.0} & 55.7 & \textbf{39.6} & \textbf{20.2} & \textbf{41.2} & \textbf{48.8}\\\hline RetinaNet \cite{lin2017focal} & 35.6 & 55.5 & 38.1 & 20.1 & 39.4 & 46.8 \\
			RetinaNet \cite{lin2017focal} + joint w/ QFL & \textbf{36.4} & \textbf{56.3} & \textbf{39.1} & \textbf{20.4} & \textbf{40.0} & \textbf{48.7}\\\hline	
			SSD512 \cite{liu2016ssd}  &29.4 & 49.1 & 30.6 & 11.4 & 34.1 & 44.9\\ SSD512 \cite{liu2016ssd} + joint w/ QFL & \textbf{30.2} & \textbf{50.3} & \textbf{31.7} & \textbf{13.3} & \textbf{34.4} & \textbf{45.5}\\ \hline
		\end{tabular}
		}
		\vspace{-4pt}
		\captionsetup{font={scriptsize}}
	    \caption{\textbf{Applying joint representations with QFL to other one-stage detectors}: About 0.6-0.8 \% AP gains are obtained without any additional overhead for inference.
}
	    \label{tab:centerness}
	\end{center}
\end{subtable}
\hspace{10pt}
\begin{subtable}[ht]{0.33\textwidth}
\begin{center}
\resizebox{0.92\textwidth}{!}{
		\begin{tabular}{l|ccc}
			\hline
			 (QFL) & AP & AP & AP \\
			\hline
			\hline
			0 & 37.6 & 55.4 & 40.3 \\ 1 & 39.0 & 58.1 & 41.7 \\
			2 & \textbf{39.9} & \textbf{58.5} & \textbf{43.0} \\
			2.5 & 39.7 & 58.1 & 42.7\\
4 & 38.2 & 55.4 & 41.6 \\
\hline
		\end{tabular}
		}
		\vspace{-4pt}
		\captionsetup{font={scriptsize}}
		\caption{\textbf{Varying  for QFL based on ATSS}:  = 2 performs best. 
		}
		\label{tab:faster_mask}
	\end{center}
\end{subtable}
 	\end{center}
	\captionsetup{font={scriptsize}}
	\vspace{-25pt}
	\caption{Study on QFL (ResNet-50 backbone). All experiments are reproduced in mmdetection \cite{chen2019mmdetection} and validated on COCO {\tt minival}.}
	\vspace{-14pt}
	\label{tab_1}
\end{table*}
\begin{figure}[t]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/unimodal_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{Qualitative comparisons between Dirac delta (a), Gaussian (b) and our proposed General (c) distribution for bounding box regression on COCO {\tt minival}, based on ATSS \cite{zhang2019bridging}. White boxes denote the ground-truth labels, and the predicted ones are marked green.}
	\label{fig_unimodal}
	\vspace{-14pt}
\end{figure}
\section{Experiment}
Our experiments are conducted on COCO benchmark \cite{lin2014microsoft}, where {\tt trainval35k} (115K images) is utilized for training and we use {\tt minival} (5K images) as validation for our ablation study. The main results are reported on {\tt test-dev} (20K images) which can be obtained from the evaluation server. For fair comparisons, all results are produced under mmdetection \cite{chen2019mmdetection}, where the default hyper-parameters are adopted. Unless otherwise stated, we adopt 1x learning schedule (12 epochs) without multi-scale training for the following studies, based on ResNet-50 \cite{he2016deep} backbone. More training/test details can be found in \textcolor{black}{Supplementary Materials}.


\begin{wrapfigure}{r}{0.44\textwidth}
	\vspace{-18pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.42\textwidth]{./figs/sota_time_acc.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{Single-model single-scale speed (ms) vs. accuracy (AP) on COCO test-dev among state-of-the-art approaches. GFL achieves better speed-accuracy trade-off than many competitive counterparts.}
	\label{fig_sota}
	\vspace{-8pt}
\end{wrapfigure}
We first investigate the effectiveness of the QFL (Table~\ref{tab_1}). In Table~\ref{tab_1}(a), we compare the proposed joint representation with its separate or implicit counterparts. Two alternatives for representing localization quality: IoU \cite{wu2020iou,jiang2018acquisition} and centerness \cite{tian2019fcos,zhang2019bridging} are also adopted in the experiments. In general, we construct 4 variants that use separate or implicit representation, as illustrated in Fig.~\ref{fig_QFL_compare_cropped}. According to the results, we observe that the joint representations optimized by QFL consistently achieve better performance than all the counterparts, whilst IoU always performs better than centerness as a measurement of localization quality ({Supplementary Materials}). Table~\ref{tab_1}(b) shows that QFL can also boost the performance of other popular one-stage detectors, and Table~\ref{tab_1}(c) shows that  is the best setting for QFL. We illustrate the effectiveness of joint representation by sampling instances with its predicted classification and IoU scores of both IoU-branch model and ours, as shown in Fig.~\ref{fig_QFL_drawback_2_cropped}(b). It demonstrates that the proposed joint representation trained with QFL can benefit the detection due to its more reliable quality estimation, and yields the strongest correlation between classification and quality scores according to its definition. In fact, in our joint representation, the predicted classification score is equal to the estimated quality score exactly.


\begin{table*}[t]
	\renewcommand\arraystretch{1.2}
	\begin{center}
		\setlength{\fboxrule}{0pt}
		\begin{subtable}[ht]{\textwidth}
\begin{center}
\resizebox{0.96\textwidth}{!}{
		\begin{tabular}{l|ccc|ccc|ccc|ccc}
			\hline
			\multirow{2}{*}{Prior Distribution}& \multicolumn{6}{c|}{FCOS \cite{tian2019fcos}} & \multicolumn{6}{c}{ATSS \cite{zhang2019bridging}} \\\cline{2-13}
			 & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP & AP \\
			\hline
			\hline
			Dirac delta \cite{tian2019fcos,zhang2019bridging} & 38.5 &56.8 &41.6 &22.4 &42.4 &49.1 & 39.2 & \textbf{57.4} & 42.2 & 23.0 & 42.8 & 51.1   \\
			Gaussian \cite{he2019bounding,choi2019gaussian} & 38.6 & 56.5 & 41.6 & 21.7 & 42.5 & 50.0 & 39.3 &57.0 & 42.4 & \textbf{23.6} & 42.9 & 51.0  \\
			General \textbf{(ours)} & 38.8 & 56.6 & 42.0 & 22.5 & 42.9 & 49.8 & 39.3 & 57.1& 42.5& 23.5 & 43.0 & \textbf{51.2}\\
			General w/ DFL \textbf{(ours)} & \textbf{39.0} & \textbf{57.0} & \textbf{42.3} & \textbf{22.6} & \textbf{43.0} & \textbf{50.6} & \textbf{39.5} & 57.3 & \textbf{42.8} & \textbf{23.6} & \textbf{43.2} & \textbf{51.2}
			   \\
\hline
		\end{tabular}
		}
		\vspace{-4pt}
	\captionsetup{font={scriptsize}}
	\caption{\textbf{Performances under different data representation of bounding box regression targets}: the proposed General distribution supervised by DFL improves favorably over the competitive baselines. }
	\label{tab:k}
	\end{center}
\end{subtable}
\vspace{5mm}
\begin{subtable}[t]{0.48\textwidth}
	\begin{center}
\resizebox{\textwidth}{!}{
		\begin{tabular}{c|l|ccc|ccc}
			\hline
			n &  & AP & AP & AP & AP & AP & AP \\
			\hline
			\hline
12 & \multirow{4}{*}{1} & 40.1 & 58.4 & 43.1 & 23.1 & 43.8 & 52.5
			\\
			14 & & \textbf{40.2} & 58.3 & \textbf{43.6} & \textbf{23.3} & 44.2 & 52.2\\
			16 & & \textbf{40.2} & \textbf{58.6} &43.4 &23.0 &\textbf{44.3} &\textbf{53.0}\\
			18 & & 40.1 & 58.1 & 43.1 & 22.6 & 43.9 & 52.6\\
\hline
		\end{tabular}
		}
		\vspace{-4pt}
		\captionsetup{font={scriptsize}}
	    \caption{\textbf{Varying  by fixing  on ATSS (w/ GFL)}: The performance is robust to a range of  according to its target distribution in Fig.~\ref{fig_fcos_atss_reg_cropped}(c). }
	    \label{tab:centerness}
	\end{center}
\end{subtable}
\hspace{10pt}
\begin{subtable}[t]{0.48\textwidth}
	\begin{center}
\resizebox{\textwidth}{!}{
		\begin{tabular}{l|l|ccc|ccc}
			\hline
			n &  & AP & AP & AP & AP & AP & AP\\
			\hline
			\hline
\multirow{4}{*}{16} & 0.5 & \textbf{40.2} & 58.4 & 43.0 & 22.3 & 43.8 & \textbf{53.1} \\
			& 1 & \textbf{40.2} & \textbf{58.6} & \textbf{43.4} & \textbf{23.0} & \textbf{44.3} & 53.0\\
			& 2 & 39.9 & 58.3 & 42.9 & 22.5 & 43.8 & 51.8 \\
			& 4 & 39.8 & 58.5 & 42.8 & 22.8 & 43.4 & 52.3 \\
			\hline
		\end{tabular}
		}
	\vspace{-4pt}
	\captionsetup{font={scriptsize}}
	\caption{\textbf{Varying  by fixing  on ATSS (w/ GFL)}: Small  usually leads to better performance whilst  is good enough for practice.}
	\label{tab:backbone}
	\end{center}
\end{subtable} 	\end{center}
	\captionsetup{font={scriptsize}}
	\vspace{-24pt}
	\caption{Study on DFL (ResNet-50 backbone). All experiments are reproduced in mmdetection \cite{chen2019mmdetection} and validated on COCO {\tt minival}.}
	\label{tab_2}
	\vspace{-16pt}
\end{table*}








\begin{wrapfigure}{l}{0.4\textwidth}
\vspace{-14pt}
  \begin{center}
  \renewcommand\arraystretch{1.2}
        \setlength{\fboxrule}{0pt}
\resizebox{0.4\textwidth}{!}{
			\begin{tabular}{cc|c|ccc}
				\hline
				QFL & DFL & FPS &  AP & AP & AP   \\
				\hline
				\hline
				&& 19.4 & 39.2 & 57.4 & 42.2 \\
				&& 19.4 &{39.9} & {58.5} & {43.0}  \\
				&& 19.4 & 39.5 & 57.3 & 42.8 \\
				&& 19.4& \textbf{40.2} &\textbf{58.6} &\textbf{43.4}   \\
				\hline
			\end{tabular}
		}
		\vspace{-4pt}
		\captionsetup{font={scriptsize}}
		\captionof{table}{\textbf{The effect of QFL and DFL on ATSS}: The effects of QFL and DFL are orthogonal, whilst utilizing both can boost 1\% AP over the strong ATSS baseline, without introducing additional overhead practically. 
		}
		\label{tab:qfl_dfl_atss}
	\end{center}
\vspace{-12pt}
\end{wrapfigure}
Second, we investigate the effectiveness of the DFL (Table~\ref{tab_2}). To quickly select a reasonable value of , we first illustrate the distribution of the regression targets in Fig.~\ref{fig_fcos_atss_reg_cropped}(c). We will show in later experiments, the recommended choice of  for ATSS is 14 or 16. In Table~\ref{tab_2}(a), we compare the effectiveness of different data representations for bounding box regression. We find that the General distribution achieves superior or at least comparable results, whilst DFL can further boost its performance. Qualitative comparisons are depicted in Fig.~\ref{fig_unimodal}. It is observed that the proposed General distribution can provide more accurate bounding box locations than Gaussian and Dirac delta distribution, especially under the case with considerable occlusions (More discussions in \textcolor{black}{Supplementary Materials}). Based on the improved ATSS trained 
by GFL, we report the effect of  and  in DFL by fixing one and varying another in Table~\ref{tab_2}(b) and (c). The results demonstrate that the selection of  is not sensitive and  is suggested to be small (e.g., 1) in practice. To illustrate the effect of General distribution, we plot several representative instances with its distributed bounding box over four directions in Fig.~\ref{fig_DFL_drawback_22_cropped}, where the proposed distributed representation can effectively reflect the uncertainty of bounding boxes by its shape (see more examples in Supplementary Materials).  


\begin{table*}[t]
	\renewcommand\arraystretch{1.2}
\centering
\footnotesize \setlength{\tabcolsep}{4.5pt}
	\vspace{4pt}
	\begin{threeparttable}
		\resizebox{\textwidth}{!}{
			\begin{tabular}{l|l|c|c|c|ccc|ccc|c}
				\hline
				Method  & Backbone & Epoch & MS & FPS & AP &AP &AP &AP &AP &AP & Reference\\
				\hline
				\textit{multi-stage:}  & & & & & & & & & & & \\
Faster R-CNN w/ FPN \cite{lin2017feature}  & R-101 & 24 & & 14.2 & 36.2 & 59.1 & 39.0 & 18.2 & 39.0 & 48.2 & CVPR17 \\
Cascade R-CNN \cite{cai2018cascade}  & R-101 & 18 & & 11.9 &42.8 &62.1 &46.3 &23.7 &45.5 &55.2 & CVPR18\\
				Grid R-CNN \cite{lu2019grid} & R-101 & 20 &  & 11.4 &41.5 & 60.9 & 44.5 & 23.3 & 44.9 & 53.1 & CVPR19 \\
				Libra R-CNN \cite{pang2019libra} & R-101 & 24 & & 13.6 & 41.1 &  62.1 &  44.7 & 23.4 & 43.7 & 52.5 & CVPR19\\
				Libra R-CNN \cite{pang2019libra} & X-101-64x4d & 12 & & 8.5 & 43.0 & 64.0 & 47.0 & 25.3 & 45.6 & 54.6 & CVPR19\\
				RepPoints \cite{yang2019reppoints}   &R-101 & 24 & & 13.3 & 41.0 & 62.9 & 44.3 & 23.6 & 44.1 & 51.7 & ICCV19\\
				RepPoints \cite{yang2019reppoints}   &R-101-DCN& 24 &  & 11.8 & 
				45.0 & 66.1 & 49.0 & 26.6 & 48.6 & 57.5 & ICCV19 \\
				TridentNet \cite{li2019scale}& R-101 & 24 &  & 2.7 & 42.7 & 63.6 & 46.5 & 23.9 & 46.6 & 56.6 & ICCV19 \\
				TridentNet \cite{li2019scale}& R-101-DCN & 36 &  & 1.3 & 46.8 & 67.6 & 51.5 & 28.0 & 51.2 & 60.5 & ICCV19 \\
				TSD \cite{song2020revisiting} & R-101 & 20 &  & 1.1 & 43.2 & 64.0 & 46.9 & 24.0 & 46.3 & 55.8 & CVPR20\\
				\hline
				\hline
				\textit{one-stage:}   & & & & & & & & & & & \\
				CornerNet \cite{law2018cornernet} &HG-104 & 200 &  & 3.1 & 40.6 & 56.4 & 43.2 & 19.1 & 42.8 & 54.3 & ECCV18 \\
				CenterNet \cite{duan2019centernet} &HG-52& 190 &  & 4.4 & 41.6 & 59.4 & 44.2 & 22.5 & 43.1 & 54.1 & ICCV19 \\
				CenterNet \cite{duan2019centernet} &HG-104 & 190 &   & 3.3 &44.9 &62.4 &48.1 &25.6 &47.4 &57.4 & ICCV19 \\
				CentripetalNet \cite{dong2020centripetalnet} & HG-104 & 210 &  & n/a  & 45.8 & 63.0 & 49.3 & 25.0 & 48.2 & 58.7 & CVPR20\\
RetinaNet \cite{lin2017focal} &R-101 &  18 &  & 13.6 &39.1 &59.1 &42.3 &21.8 &42.7 &50.2 & ICCV17 \\
				FreeAnchor \cite{zhang2019freeanchor} & R-101 & 24 &  & 12.8 & 43.1 & 62.2 & 46.4 & 24.5 & 46.1 & 54.8 & NeurIPS19 \\
				FreeAnchor \cite{zhang2019freeanchor} & X-101-32x8d & 24 &  & 8.2 & 44.9 & 64.3 & 48.5 & 26.8 & 48.3 & 55.9 & NeurIPS19 \\
				FoveaBox \cite{kong2019foveabox}& R-101 & 18 &  & 13.1 & 40.6 & 60.1 & 43.5 & 23.3 & 45.2 & 54.5 & -- \\
				FoveaBox \cite{kong2019foveabox}& X-101 & 18 &  & n/a  & 42.1 & 61.9 & 45.2 & 24.9 & 46.8 & 55.6 & -- \\
				FSAF \cite{zhu2019feature} & R-101 & 18 &  & 15.1 & 40.9 & 61.5 & 44.0 & 24.0 & 44.2 & 51.3 & CVPR19\\
				FSAF \cite{zhu2019feature}& X-101-64x4d & 18 &  & 9.1 & 42.9 & 63.8 & 46.3 & 26.6 & 46.2 & 52.7 & CVPR19 \\
				FCOS \cite{tian2019fcos} & R-101 & 24 &  & 14.7 &41.5 & 60.7 & 45.0 & 24.4 & 44.8 & 51.6 & ICCV19\\
				FCOS \cite{tian2019fcos} & X-101-64x4d & 24 &  & 8.9 & 44.7 & 64.1 & 48.4 & 27.6 & 47.5 & 55.6 & ICCV19\\
				SAPD \cite{zhu2019soft}& R-101 & 24 &  & 13.2 & 43.5 & 63.6 & 46.5 & 24.9 & 46.8 & 54.6 & CVPR20 \\
				SAPD \cite{zhu2019soft}& X-101-32x4d & 24 &  & 10.7 & 44.5 & 64.7 & 47.8 & 26.5 & 47.8 & 55.8 & CVPR20 \\
				SAPD \cite{zhu2019soft}&  R-101-DCN & 24 &  & 11.1 & 46.0 & 65.9 & 49.6 & 26.3 & 49.2 & 59.6 & CVPR20 \\
				SAPD \cite{zhu2019soft}& X-101-32x4d-DCN & 24 &  & 8.8 & 46.6 & 66.6 & 50.0 & 27.3 & 49.7 & 60.7 & CVPR20 \\
				ATSS \cite{zhang2019bridging}& R-101 & 24 &  & 14.6 &43.6 &62.1 &47.4 &26.1 &47.0 &53.6 & CVPR20 \\
				ATSS \cite{zhang2019bridging}& X-101-32x8d & 24 &  & 8.9 &45.1 &63.9 &49.1 &27.9 &48.2 &54.6 & CVPR20 \\
				ATSS \cite{zhang2019bridging} &R-101-DCN & 24 &   & 12.7 & 46.3 &64.7 &50.4 &27.7 &49.8 &58.4 & CVPR20\\
				ATSS \cite{zhang2019bridging}& X-101-32x8d-DCN & 24 &  & 6.9 & {47.7} & {66.6} &{52.1} & {29.3} &50.8 & {59.7} & CVPR20 \\
				\hline
				GFL \textbf{(ours)}&R-50 & 24 &  & 19.4 & 43.1 & 62.0 & 46.8 & 26.0 & 46.7 & 52.3 & -- \\
				GFL \textbf{(ours)}&R-101 & 24 &  & 14.6 & 45.0 & 63.7 & 48.9 & 27.2 & 48.8 & 54.5 & -- \\
				GFL \textbf{(ours)}&X-101-32x4d & 24 &  & 12.2 & 46.0 & 65.1 & 50.1 & 28.2 & 49.6 & 56.0 & -- \\
				GFL \textbf{(ours)}&R-101-DCN & 24 &  & 12.7 & 47.3 & 66.3 & 51.4 & 28.0 & {51.1} & 59.2& -- \\
				GFL \textbf{(ours)}&X-101-32x4d-DCN & 24 &   & 10.0 & {48.2} & {67.4} & {52.6} & {29.2} & {51.7} & {60.2}& -- \\
				\hline
			\end{tabular}
		}
	\end{threeparttable}
	\captionsetup{font={scriptsize}}
	\vspace{-4pt}
	\caption{Comparisons between state-of-the-art detectors \emph{(single-model and single-scale results)} on COCO {\tt test-dev}. ``MS'' denotes multi-scale training. FPS values with  are from \cite{zhu2019soft}, while others are measured on the same machine with a single GeForce RTX 2080Ti GPU under the same mmdetection \cite{chen2019mmdetection} framework, using a batch size of 1 whenever possible. ``n/a'' means that both trained models and timing results from original papers are not available. \textbf{R}: ResNet. \textbf{X}: ResNeXt. \textbf{HG}: Hourglass. \textbf{DCN}: Deformable Convolutional Network.}
	\vspace{-16pt}
	\label{tab_3}
\end{table*}

Third, we perform the ablation study on ATSS with ResNet-50 backbone to show the relative contributions of QFL and DFL (Table~\ref{tab:qfl_dfl_atss}). FPS (Frames-per-Second) is measured on the same machine with a single GeForce RTX 2080Ti GPU using a batch size of 1 under the same mmdetection \cite{chen2019mmdetection} framework. We observe that the improvement of DFL is orthogonal to QFL, and joint usage of both (i.e., GFL) improves the strong ATSS baseline by absolute 1\% AP score. Furthermore, according to the inference speeds, GFL brings negligible additional overhead and is considered very practical.



















Finally, we compare GFL (based on ATSS) with state-of-the-art approaches on COCO {\tt test-dev} in Table~\ref{tab_3}. Following previous works \cite{lin2017focal,tian2019fcos}, the multi-scale training strategy and 2x learning schedule (24 epochs) are adopted during training. For a fair comparison, we report the results of single-model single-scale testing for all methods, as well as their corresponding inference speeds (FPS). GFL with ResNet-101 \cite{he2016deep} achieves 45.0\% AP at 14.6 FPS, which is superior than all the existing detectors with the same backbone, including SAPD \cite{zhu2019soft} (43.5\%) and ATSS \cite{zhang2019bridging} (43.6\%). Further, Deformable Convolutional Networks (DCN) \cite{zhu2019deformable} consistently boost the performances over ResNe(X)t backbones, where GFL with ResNeXt-101-32x4d-DCN obtains state-of-the-art 48.2\% AP at 10 FPS. Fig.~\ref{fig_sota} demonstrates the visualization of the accuracy-speed trade-off, where it can be observed that our proposed GFL pushes the envelope of accuracy-speed boundary to a high level. 






\section{Conclusion}
\vspace{-2pt}
To effectively learn qualified and distributed bounding boxes for dense object detectors, we propose Generalized Focal Loss (GFL) that generalizes the original Focal Loss from  discrete formulation to the continuous version. GFL can be specialized into Quality Focal loss (QFL) and Distribution Focal Loss (DFL), where QFL encourages to learn a better joint representation of classification and localization quality, and DFL provides more informative and precise bounding box estimations by modeling their locations as General distributions. Extensive experiments validate the effectiveness of GFL. We hope GFL can serve as a simple yet effective baseline for the community.




\small
\bibliography{neurips_2020}
\bibliographystyle{plain}


\clearpage
\appendix
\section{More Discussions about the Distributions}
Fig.~\ref{fig_distribution_three} depicts the ideas of Dirac delta, Gaussian, and the proposed General distributions, where the assumption goes from rigid (Dirac delta) to flexible (General). 
We also list several key comparisons about these distributions in Table~\ref{tab_distribution}. It can be observed that the loss objective of the Gaussian assumption is actually a dynamically weighted L2 Loss, where its training weight is related to the predicted variance . It is somehow similar to that of Dirac delta (standard L2 Loss) when optimized at the edge level. Moreover, it is not clear how to integrate the Gaussian assumption into the IoU-based Loss formulations, since it heavily couples the expression of the target representation with its optimization objective. Therefore, it can not enjoy the benefits of the IoU-based optimization \cite{rezatofighi2019generalized}, as it is proved to be very effective in practice. In contrast, our proposed General distribution decouples the representation and loss objective, making it feasible for any type of optimizations, including both edge level and box level.

\begin{figure}[h]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.6\textwidth]{./figs/distribution_three_cropped.pdf}}
	\end{center}	
	\captionsetup{font={small}}
	\vspace{-10pt}
	\captionsetup{font={scriptsize}}
	\caption{Illustrations of three distributions, from rigid (Dirac delta) to flexible (General). The proposed General distribution is more flexible as its shape can be arbitrary. In contrast, Dirac delta distribution roots at a fixed point and Gaussian distribution follows a relatively rigid, symmetric expression, e.g., , which both have more limitations in modeling real data distribution.}
	\label{fig_distribution_three}
	\vspace{-8pt}
\end{figure}
\begin{table*}[h]
	\renewcommand\arraystretch{1.2}
	\setlength{\fboxrule}{0pt}
	\begin{center}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{l|c|c|c|c|c}
		\hline
		Type & \multicolumn{2}{c|}{Dirac delta \cite{tian2019fcos,zhang2019bridging}}  & Gaussian \cite{choi2019gaussian,he2019bounding} & \multicolumn{2}{c}{General (ours)} \\ \hline
		Probability Density & \multicolumn{2}{c|}{} &   & \multicolumn{2}{c}{} \\\hline
		Inference Target & \multicolumn{2}{c|}{} &  & \multicolumn{2}{c}{} \\\hline
		Loss Objective (for box part) &  & IoU-based Loss &  &  & IoU-based Loss\\\hline
		Optimization Level & edge &  box & edge & edge &  box \\\hline
		\end{tabular}
	}
	\end{center}
	\captionsetup{font={scriptsize}}
	\vspace{-8pt}
	\caption{Comparisons between three distributions. ``edge'' level denotes optimization over four respective directions, whilst ``box'' level means IoU-based Losses \cite{rezatofighi2019generalized} that consider the bounding box as a whole.}
	\vspace{-6pt}
	\label{tab_distribution}
\end{table*}
\begin{figure}[h]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/DFL_drawback_1_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-10pt}
	\caption{We demonstrate an example in 2D space by fixing the input feature vector and introduce a small disturbance (norm of 0.1) over it. The regression targets are {1.5, 2.5, 3.5} respectively. It is observed that Dirac delta distribution leads to more regression errors after the same disturbance, and the error increases with the growth of regression target. In contrast, our proposed General distribution remains stable and insensitive to the disturbance.  }
	\label{fig_DFL_drawback_1_cropped}
	\vspace{-6pt}
\end{figure}

We also find that the bounding box regression of Dirac delta distribution (including Gaussian distribution based on the analysis from Table \ref{tab_distribution}) behaves more sensitive to feature perturbations, making it less robust and susceptible to noise, as shown in the simulation experiment (Fig.~\ref{fig_DFL_drawback_1_cropped}). It proves that General distribution enjoys more benefits than the other counterparts.


\section{Global Minimum of }
Let's review the definition of :


For simplicity,  can then be expanded as:



Furthermore, given , for arbitrary variable  in the domain of definition, we can have:

Therefore, it is easy to deduce:

where ``'' holds only when .

The global minimum property of GFL somehow explains why the IoU or centerness guided variants in Fig.~\ref{fig_QFL_compare_cropped} would not have obvious advantages. In fact, the weighted guidance does not essentially change the global minimum of the original classification loss (e.g., Focal Loss), whilst their optimal classification targets are still one-hot labels. In contrast, the proposed GFL indeed modifies the global minimum and force the predictions to approach the accurate IoU between the estimated boxes and ground-truth boxes, which is obviously beneficial for the rank process of NMS.

\section{FL, QFL and DFL are special cases of GFL}
In this section, we show how GFL can be specialized into the form of FL, QFL and DFL, respectively.

\textbf{FL}: Letting  and  in GFL, we can obtain FL:


\textbf{QFL}: Having  and  in GFL, the form of QFL can be written as:


\textbf{DFL}: By substituting  in GFL, we can have DFL:








\section{Details of Experimental Settings}
\textbf{Training Details:} The ImageNet pretrained
models \cite{he2016deep} with FPN \cite{lin2017feature} are utilized as the backbones. During training, the input images are resized to keep their shorter side being 800 and their longer side less or equal to 1333. In ablation study, the networks are trained using the Stochastic Gradient Descent (SGD) algorithm for 90K iterations (denoted as 1x schedule) with 0.9 momentum, 0.0001 weight decay and 16 batch size. The initial learning rate is set as 0.01 and decayed by 0.1 at iteration 60K and 80K, respectively.  


\textbf{Inference Details:} During inference, the input image is resized in the same way as in the training phase, and then passed through the whole network to output the predicted bounding boxes with a predicted class. Then we use the threshold 0.05 to filter out a variety of backgrounds, and output top 1000 candidate detections per feature pyramid. Finally, NMS is applied under the IoU threshold 0.6 per class to produce the final top 100 detections per image as results.


\section{Why is IoU-branch always superior than centerness-branch?}
The ablation study in original paper also demonstrates that for FCOS/ATSS, IoU performs consistently better than centerness, as a measurement of localization quality. Here we give a convincing reason why this is the case. We discover the major problem of centerness is that its definition leads to unexpected small ground-truth label, which makes a possible set of ground-truth bounding boxes extremely hard to be recalled (as shown in Fig.~\ref{fig_centerness_vs_iou_cropped}). From the label distributions demonstrated in Fig.~\ref{fig_distribution_atss_centerness_iou_label}, we observe that most of IoU labels is larger than 0.4 yet centerness labels tend to be much smaller (even approaching 0). The small values of centerness labels prevent a set of ground-truth bounding boxes from being recalled, as their final scores for NMS would be potentially small since their predicted centerness scores are already supervised by these extremely small signals.
\begin{figure}[h]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.7\textwidth]{./figs/centerness_vs_iou_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{We demonstrate possible cases of ground-truth/predicted bounding box along with the positive points. The matrix points denote the feature pyramid layer with stride = 8. Centerness label is easier to get very small values by its definition, whilst IoU label is more reliable as the supervisions from bounding boxes will always push it close to 1.0.}
	\label{fig_centerness_vs_iou_cropped}
	\vspace{-10pt}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.6\textwidth]{./figs/distribution_atss_centerness_iou_label.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{Label distributions over all positive training samples on COCO, based on pretrained GFL detector (ResNet-50 backbone).}
	\label{fig_distribution_atss_centerness_iou_label}
	\vspace{-10pt}
\end{figure}




\section{More Examples of Distributed Bounding Boxes}
We demonstrate more examples with General distributed bounding boxes predicted by GFL (ResNet-50 backbone). As demonstrated in Fig.~\ref{fig_more_bad_cropped}, we show several cases with boundary ambiguities: does the slim and almost invisible backpack strap belong to the box of the bag (left top)? does the partially occluded umbrella handle belong to the entire umbrella (left down)? In these cases, our models even produce more reasonable coordinates of bounding boxes than the ground-truth ones. In Fig.~\ref{fig_more_good_cropped}, more examples with clear boundaries and sharp General distributions are shown, where GFL is very confident to generate accurate bounding boxes, e.g., the bottom parts of the orange and skiing woman.


\begin{figure}[h]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/more_bad_cropped.pdf}}
		\fbox{\includegraphics[width=\textwidth]{./figs/moremore_bad_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{Examples with huge boundary ambiguities and uncertainties, where the learned General distributions tend to be flatten. In some cases, we even observe a distribution with two peaks. Interestingly, they do correspond to two different most likely boundaries in the image, e.g., the boundaries of the umbrella whether its heavily occluded  handle is considered. Predictions are marked \textcolor{green}{green} in images, whilst ground-truth boxes are white.}
	\label{fig_more_bad_cropped}
	\vspace{-10pt}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=\textwidth]{./figs/more_good_cropped.pdf}}
		\fbox{\includegraphics[width=\textwidth]{./figs/DFL_goodback_cropped.pdf}}
	\end{center}	
	\captionsetup{font={scriptsize}}
	\vspace{-12pt}
	\caption{Examples with extremely clear boundaries. The learned General distributions are relatively sharp whilst producing very accurate box estimations. Predictions are marked \textcolor{green}{green} in images, whilst ground-truth boxes are white.}
	\label{fig_more_good_cropped}
	\vspace{-10pt}
\end{figure}


\end{document}