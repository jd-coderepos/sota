\documentclass{article}

\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    

\usepackage{url}            \usepackage{color}
\usepackage{soul}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{algorithmicx}
\usepackage{algorithm}

\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{relsize}
\usepackage{soul}


\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath}
\usepackage{enumitem}

\usepackage{subfigure}

\newcommand{\xd}[1]{{\color{red}#1}}
\newcommand{\jb}[1]{{\color{green}#1}}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\RR}[0]{\mathbb{R}}

\newcommand*{\affaddr}[1]{#1} \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}

\begin{document}
\title{Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering}

\author{Junbei Zhang\affmark[1], Xiaodan Zhu\affmark[2], Qian Chen\affmark[1], Lirong Dai\affmark[1], Si Wei\affmark[3] and Hui Jiang\affmark[4]\\
\affaddr{\affmark[1]University of Science and Technology of China}\\
\affaddr{\affmark[2]National Research Council Canada}\\
\affaddr{\affmark[3]iFLYTEK Research, China}\\
\affaddr{\affmark[4]York University}\\
\email{\{zjunbei,cq1231\}@mail.ustc.edu.cn, xiaodan.zhu@nrc-cnrc.gc.ca}\\
\email{lrdai@ustc.edu.cn, siwei@iflytek.com, hj@cse.yorku.ca}\\
}

\maketitle
\begin{abstract}

The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline. 
\end{abstract}

\noindent \section{Introduction}

Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in~\citep{richardson2013mctest,hermann2015teaching,hill2015goldilocks,rajpurkar2016squad,Nguyen2016MS,Berant2014Modeling}. Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs.

The recent availability of relatively large training datasets (see Section~\ref{sec:related} for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective.  

In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines. 


\section{Related Work}
\label{sec:related}
Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. ~\cite{richardson2013mctest} released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset ~\citep{hermann2015teaching} contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) ~\citep{hill2015goldilocks} leverages named entities, common nouns, verbs, and prepositions to test reading comprehension. The Stanford Question Answering Dataset (SQuAD) ~\citep{rajpurkar2016squad} is more recently released dataset, which consists of more than 100,000 questions for documents taken from Wikipedia across a wide range of topics. The question-answer pairs are annotated through crowdsourcing. Answers are spans of text marked in the original documents. In this paper, we use SQuAD to evaluate our models.

Many neural network models have been studied on the SQuAD task. ~\cite{wang2016machine} proposed ~\textit{match LSTM} to associate documents and questions and adapted the so-called~\textit{pointer Network}~\citep{vinyals2015pointer} to determine the positions of the answer text spans. ~\cite{yu2016end} proposed a~\textit{dynamic chunk reader} to extract and rank a set of answer candidates. ~\cite{yang2016words} focused on word representation and presented a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on the properties of words. ~\cite{wang2016multi} proposed a~\textit{multi-perspective context matching} (MPCM) model, which matched an encoded document and question from multiple perspectives. ~\cite{xiong2016dynamic} proposed a dynamic decoder and so-called ~\textit{highway maxout network} to improve the effectiveness of the decoder. The~\textit{bi-directional attention flow} (BIDAF) ~\citep{seo2016bidirectional} used the bi-directional attention to obtain a question-aware context representation.

In this paper, we introduce syntactic information to encode questions with a specific form of recursive neural networks~\citep{zhu2015long,Tai2015,chen2016enhancing,Socher2011Parsing}. More specifically, we explore a tree-structured LSTM ~\citep{zhu2015long,Tai2015} which extends the linear-chain long short-term memory (LSTM)~\citep{hochreiter1997long} to a recursive structure, which has the potential to capture long-distance interactions over the structures. 


Different types of questions are often used to seek for different types of information. For example, a "what" question could have very different property from that of a "why" question, while they may share information and need to be trained together instead of separately. We view this as a "adaptation" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas "i-vector"~\citep{dehak2011front} in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here.


\section{The Approach}
\subsection{The Baseline Model}
Our baseline model is composed of the following typical components: \textit{word embedding}, \textit{input encoder}, \textit{alignment}, \textit{aggregation}, and \textit{prediction}. Below we discuss these components in more details.


\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\linewidth]{squad}
	\caption{A high level view of our basic model.}
	\label{fig:squad}
\end{figure}

\paragraph{Word embedding}
We concatenate embedding at two levels to represent a word: the character composition and word-level embedding. The character composition feeds all characters of a word into a convolutional neural network (CNN)~\citep{kim2014convolutional} to obtain a representation for the word. And we use the pre-trained 300-D GloVe vectors~\citep{pennington2014glove} (see the experiment section for details) to initialize our word-level embedding. Each word is therefore represented as the concatenation of the character-composition vector and word-level embedding. This is performed on both questions and documents, resulting in two matrices: the  for a question and the  for a document, where  is the question length (number of word tokens),  is the document length, and  is the embedding dimensionality.

\paragraph{Input encoding}
The above word representation focuses on representing individual words, and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context. We use bi-directional GRU (BiGRU)~\citep{cho2014properties} for both  documents and questions. 

\vspace{-2mm}
{\fontsize{10pt}{1.0cm}
 	
}
\vspace{-6mm}


A BiGRU runs a forward and backward GRU on a sequence starting from the left and the right end, respectively. By concatenating the hidden states of these two GRUs for each word, we obtain the a representation for a question or document:  for a question and  for a document.

\paragraph{Alignment}
Questions and documents interact closely. As in most previous work, our framework use both soft attention over questions and that over documents to capture the interaction between them. More specifically, in this soft-alignment layer, we first feed the contextual representation matrix  and  to obtain alignment matrix :

Each  represents the similarity between a question word  and a document word . 


\textit{\underline{\smash{Word-level Q-code}}}
Similar as in~\citep{seo2016bidirectional}, we obtain a word-level Q-code. Specifically, for each document word , we find which words in the question are relevant to it. To this end,  is computed with the following equation and used as a soft attention weight:

With the attention weights computed, we obtain the encoding of the question for each document word  as follows, which we call~\textit{word-level Q-code} in this paper:


\textit{\underline{\smash{Question-based filtering}}} To better explore question understanding, we design this~\textit{question-based filtering} layer. As detailed later, different question representation can be easily incorporated to this layer in addition to being used as a filter to find key information in the document based on the question. This layer is  expandable with more complicated question modeling. 

In the basic form of question-based filtering, for each question word , we find which words in the document are associated. Similar to  discussed above, we can obtain the attention weights on document words for each question word :

By pooling , we can obtain a question-based filtering weight :


where the specific pooling function we used include max-pooling and mean-pooling. Then the document softly filtered based on the corresponding question   can be calculated by:



Through concatenating the document representation , word-level Q-code  and question-filtered document , we can finally obtain the alignment layer representation:

where "" stands for element-wise multiplication and "" is simply the vector subtraction.

\paragraph{Aggregation}
After acquiring the local alignment representation, key information in document and question has been collected, and the aggregation layer is then performed to find answers. We use three BiGRU layers to model the process that aggregates local information to make the global decision to find the answer spans. We found a residual architecture~\citep{he2016deep} as described in Figure~\ref{fig:inference} is very effective in this aggregation process:




\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\linewidth]{match}
	\caption{The inference layer implemented with a residual network.}
	\label{fig:inference}
\end{figure}

\paragraph{Prediction}
The SQuAD QA task requires a span of text to answer a question. We use a pointer network~\citep{vinyals2015pointer} to predict the starting and end position of answers as in~\citep{wang2016machine}. Different from their methods, we use a two-directional prediction to obtain the positions. For one direction, we first predict the starting position of the answer span followed by predicting the end position, which is implemented with the following equations:


where  is inference layer output,  is the hidden state of the first step, and all  are trainable matrices. We also perform this by predicting the end position first and then the starting position:


We finally identify the span of an answer with the following equation:


We use the mean-pooling here as it is more effective on the development set than the alternatives such as the max-pooling.

\subsection{Question Understanding and Adaptation}


\subsubsection{Introducing syntactic information for neural question encoding}

The interplay of syntax and semantics of natural language questions is of interest for question representation. We attempt to incorporate syntactic information in questions representation with TreeLSTM~\citep{zhu2015long,Tai2015}. In general a TreeLSTM could perform semantic composition over given syntactic structures. 

Unlike the chain-structured LSTM~\citep{hochreiter1997long}, the TreeLSTM captures long-distance interaction on a tree. The update of a TreeLSTM node is described at a high level with Equation (\ref{equ:TreeLSTM}), and the detailed computation is described in (\ref{equ:begin}--\ref{equ:end}). Specifically, the input of a TreeLSTM node is used to configure four gates: the input gate , output gate , and the two forget gates  for the left child input and  for the right. The memory cell  considers each child's cell vector,  and , which are gated by the left forget gate  and right forget gate , respectively. 

\vspace{-4mm}
{\fontsize{10pt}{1.0cm}
	
}\noindent where  is the sigmoid function,  is the element-wise multiplication of two vectors, and all ,  are trainable matrices.   


To obtain the parse tree information, we use Stanford CoreNLP (PCFG Parser)~\citep{Manning2014The,Klein2003Accurate} to produce a binarized constituency parse for each question and build the TreeLSTM based on the parse tree. The root node of TreeLSTM is used as the representation for the whole question. More specifically, we use it as TreeLSTM Q-code , by not only simply concatenating it to the alignment layer output but also using it as a question filter, just as we discussed in the \textit{\smash{question-based filtering}} section:




where  is the new output of alignment layer, and function  copies  for M times to fit with .

\subsubsection{Question Adaptation}
Questions by nature are often composed to fulfill different types of information needs. For example, a "when" question seeks for different types of information (i.e., temporal information) than those for a "why" question. Different types of questions and the corresponding answers could potentially have different distributional regularity. 

\paragraph{Explicit question-type embedding}
The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. 
In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: \textit{what, how, who, when, which, where, why, be, whose,} and \textit{whom}, in which \textit{be} stands for the questions beginning with different forms of the word~\textit{be} such as~\textit{is, am}, and ~\textit{are}. We explicitly encode question-type information to be an 11-dimensional one-hot vector (the top-10 question types and "other" question type). Each question type is with a trainable embedding vector. We call this explicit question type code, . Then the vector for each question type is tuned during training, and is added to the system with the following equation:



\paragraph{Question adaptation}
As discussed, different types of questions and their answers may share common regularity and have separate property at the same time. We also view this as an adaptation problem in order to let different types of questions share a basic model but still discriminate them when needed. Specifically, we borrow ideas from speaker adaptation~\citep{dehak2011front} in speech recognition, where neural-network-based adaptation is performed among different groups of speakers. 


Conceptually we regard a type of questions as a group of acoustically similar speakers. Specifically we propose a question discriminative block or simply called a \textit{discriminative block} (Figure~\ref{fig:discriminative_block}) below to perform question adaptation. 
The main idea is described below:



For each input question , we can decompose it to two parts: the cluster it belong(i.e., question type) and the diverse in the cluster. The information of the cluster is encoded in a vector . In order to keep calculation differentiable, we compute the weight of all the clusters based on the distances of  and each cluster center vector, in stead of just choosing the closest cluster. Then the discriminative vector  with regard to these most relevant clusters are computed. All this information is combined to obtain the discriminative information. In order to keep the full information of input, we also copy the input question , together with the acquired discriminative information, to a feed-forward layer to obtain a new representation  for the question. 


\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\linewidth]{discriminative_block}
	\caption{The~\textit{discriminative block} for question discrimination and adaptation.}
	\label{fig:discriminative_block}
\end{figure}

More specifically, the adaptation algorithm contains two steps: \textit{adapting} and \textit{updating}, which is detailed as follows:
 

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item \textbf{Adapting}
In the adapting step, we first compute the similarity score between an input question vector  and each centroid vector of  clusters. Each cluster here models a question type. Unlike the explicit question type modeling discussed above, here we do not specify what question types we are modeling but let the system to learn. Specifically, we only need to pre-specific how many clusters, , we are modeling. The similarity between an input question and cluster centroid can be used to compute similarity weight :



We set  equals 50 to make sure only closest class will have a high weight while maintain differentiable. Then we acquire a soft class-center vector :

We then compute a discriminative vector  between the input question with regard to the soft class-center vector:


Note that  here models the cluster information and  represents the discriminative information in the cluster. By feeding ,  and  into feedforward layer with Relu, we obtain :

With  ready, we can apply Discriminative Block to any question code and obtain its adaptation Q-code. In this paper, we use TreeLSTM Q-code as the input vector , and obtain TreeLSTM adaptation Q-code . Similar to TreeLSTM Q-code , we concatenate  to alignment output  and also use it as a question filter:





\item \textbf{Updating}  The updating stage attempts to modify the center vectors of the  clusters in order to fit each cluster to model different types of questions. The updating is performed according to the following formula:

In the equation,  is an updating rate used to control the amount of each updating, and we set it to 0.01. When  is far away from -th cluster center ,  is close to be value 0 and the -th cluster center  tends not to be updated. If  is instead close to the -th cluster center ,  is close to the value 1 and the centroid of the -th cluster  will be updated more aggressively using .
\end{itemize}

\section{Experiment Results}
\subsection{Set-Up}
We test our models on Stanford Question Answering Dataset (SQuAD)~\citep{rajpurkar2016squad}. The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles, and the answer to each question is a span of text in the  Wikipedia articles. Training data includes 87,599 instances and validation set has 10,570 instances. The test data is hidden and kept by the organizer. The evaluation of SQuAD is Exact Match (EM) and F1 score.


We use pre-trained \textit{300-D Glove 840B} vectors~\citep{pennington2014glove} to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. CharCNN filter length is 1,3,5, each is 50 dimensions. All vectors including word embedding are updated during training. The cluster number K in discriminative block is 100. The Adam method~\citep{Kingma2014AdamAM} is used for optimization. And the first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. We will half learning rate when meet a bad iteration, and the patience is 7. Our early stop evaluation is the EM and F1 score of validation set. All hidden states of GRUs, and TreeLSTMs are 500 dimensions, while word-level embedding  is 300 dimensions. We set max length of document to 500, and drop the question-document pairs beyond this on training set. Explicit question-type dimension  is 50. We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5.   

\begin{table*}[t]
	\renewcommand{\arraystretch}{1.3}
\centering
	\begin{tabular}{llll}
		\toprule
		Model    &  EM  &  F1  \\
		\midrule
        Logistic Regression Baseline ~\citep{rajpurkar2016squad} &  40.4  & 51.0 \\
		\midrule
        Match-LSTM with Ans-Ptr (Sentence)~\citep{wang2016machine}  & 54.505 & 67.748 \\
        Match-LSTM with Ans-Ptr (Boundary)~\citep{wang2016machine}  & 60.474 & 70.695 \\
        Dynamic Chunk Reader~\citep{yu2016end} & 62.499 & 70.956 \\
        Fine-Grained Gating~\citep{yang2016words}  & 62.446 & 73.327 \\
        Match-LSTM with Bi-Ans-Ptr (Boundary)~\citep{wang2016machine}  & 64.744 & 73.743 \\
        Multi-Perspective Matching~\citep{wang2016multi} & 65.551 & 75.118 \\ 
        Dynamic Coattention Networks~\citep{xiong2016dynamic}  & 66.233 & 75.896 \\
        BiLSTM [German Research Center for Artificial Intelligence(unpublished)]  & 68.436 & 77.070 \\
        BiDAF~\citep{seo2016bidirectional} & 67.974 & 77.323 \\
        \textbf{jNet(Ours)} & \textbf{68.730} & \textbf{77.393} \\ 
		r-net [Microsoft Research Asia(unpublished)] &  70.062  & 78.782 \\
		\midrule
        Human Performance~\citep{rajpurkar2016squad} &  82.304  & 91.221 \\
		\bottomrule
	\end{tabular}
	\caption{The official leaderboard of single models on SQuAD test set as we submitted our systems (January 20, 2017).}
    \label{tab:result}
\end{table*}


\subsection{Results}



\paragraph{Overall results}
Table~\ref{tab:result} shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73\% EM score and 77.39\% F1 score, which is ranked among the state of the art single models (without model ensembling).  


\begin{table*}[ht]
	\renewcommand{\arraystretch}{1.3}
\centering
	\begin{tabular}{llll}
		\toprule
		Model    &  EM  &  F1 \\
		\midrule
		Baseline &  68.00  & 77.36   \\
        +Explicit question types () & 68.16 & 77.58 \\
        +TreeLSTM () & 68.29 & 77.67 \\
+TreeLSTM adaptation (, K=20) & 68.73 & 77.74 \\
        \textbf{+TreeLSTM adaptation (, K=100)} & \textbf{69.10} & \textbf{78.38} \\
\bottomrule
	\end{tabular}
	\caption{Performance of various Q-code on the development set.}
    \label{tab:ablation}
\end{table*}



Table~\ref{tab:ablation} shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00\% and 77.36\% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16\%(EM) and 77.58\%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types () to be 20, the performance improves to 68.73\%/77.74\% on EM and F1, respectively,  which corresponds to the results of our model reported in Table~\ref{tab:result}.  Furthermore, after submitted our result, we have experimented with a large value of  and found that when , we can achieve a better performance of 69.10\%/78.38\% on the development set.


Figure~\ref{fig:subfig:a} shows the EM/F1 scores of different question types while Figure~\ref{fig:subfig:b} is the question type amount distribution on the development set. In Figure~\ref{fig:subfig:a} we can see that the average EM/F1 of the "when" question is highest and those of the "why" question is the lowest. From Figure~\ref{fig:subfig:b} we can see the "what" question is the major class.




\begin{figure}[h!]
 \centering
 \subfigure[Question Type Results]{
   \label{fig:subfig:a} \includegraphics[width=0.5\linewidth]{QT_new}}
\subfigure[Question Type Amount]{
   \label{fig:subfig:b} \includegraphics[width=0.4\linewidth]{QT2}}
 \caption{Question Type Analysis}
 \label{fig:subfig} \end{figure}


Figure~\ref{fig:result1} shows the composition of F1 score. Take our best model as an example, we observed a 78.38\% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100\%, which means an exact match. This part accounts for 69.10\% of the entire development set. And the other part accounts for 30.90\%, of which the average F1 score is 30.03\%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0\%, which means that predict answer is totally wrong. This part occupies 14.89\% of the total development set. The other part accounts for 16.01\% of the development set, of which average F1 score is 57.96\%. From this analysis we can see that reducing the zero F1 score (14.89\%) is potentially an important direction to further improve the system.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{F1_analysis_new}
	\caption{F1 Score Analysis.}
	\label{fig:result1}
\end{figure}


\section{Conclusions}
Closely modelling questions could be of importance for question answering and machine reading.  In this paper, we introduce syntactic information to help encode questions in neural networks. We view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.  



\bibliographystyle{plainnat}
\bibliography{reference}

\end{document}
