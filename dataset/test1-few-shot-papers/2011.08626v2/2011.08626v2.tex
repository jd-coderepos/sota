\documentclass[11pt,a4paper]{article}
\usepackage[dvipsnames, svgnames, x11names]{xcolor}
\usepackage[hyperref]{emnlp2020}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{comment}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{bookmark}
\usepackage{color}
\usepackage{float}
\usepackage{graphicx}
\usepackage{CJKutf8}
\usepackage{url}
\usepackage{bm}
\usepackage{diagbox}
\usepackage{parskip}
\usepackage{colortbl}
\usepackage{booktabs}       \usepackage{amsmath, amsfonts, amsthm, amssymb}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath, amsfonts}

\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\definecolor{mygray}{RGB}{224,224,224}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  

\usepackage{newtxtext,newtxmath}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Neural Semi-supervised Learning for Text Classification \\
Under Large-Scale  Pretraining 
}
\author{
Zijun Sun, Chun Fan, Xiaofei Sun, Yuxian Meng, Fei Wu and Jiwei Li\\
  Department of Computer Science and Technology, Zhejiang University\\
  Computer Center of Peking University, 
  Peng Cheng Laboratory\\
   Shannon.AI\\
  \{zijun\_sun, xiaofei\_sun, yuxian\_meng, jiwei\_li\}@shannonai.com\\
  fanchun@pku.edu.cn,
  wufei@zju.edu.cn
}




\date{}

\begin{document}
\maketitle

\begin{abstract}
The goal of semi-supervised learning is to utilize the unlabeled, in-domain dataset  to improve models trained on the labeled dataset .   
 Under the context of 
 large-scale language-model (LM) pretraining, 
 how we  can make the best use of
     is poorly understood: 
 Is semi-supervised learning still beneficial 
 with the presence of  large-scale pretraining?
 Should  be used for in-domain LM pretraining or pseudo-label generation?
How should the pseudo-label based semi-supervised model 
  be actually implemented?
How different semi-supervised strategies (e.g., self-learning) affect performances regarding  of different sizes,  of different sizes, etc. 

In this paper, we conduct comprehensive studies  on  semi-supervised learning in the 
task of text classification 
 under the context of 
large-scale
LM pretraining.
Our studies shed
important
 lights on the  behavior of semi-supervised learning methods. 
 We find that: 
(1)  with the presence of  in-domain LM pretraining  on , open-domain LM pretraining \cite{devlin2018bert} 
is unnecessary, and we
are able to achieve better performance with pretraining on  the in-domain dataset ;
(2) both the in-domain pretraining strategy and the pseudo-label based strategy introduce 
significant performance boosts, 
with the former performing better with larger ,  the latter performing better with smaller , and the combination leading to the largest performance gain; 
(3) vanilla self-training (pretraining first on the pseudo-label dataset  and then fine-tuning on ) yields better performances when  is small, while joint training on the combination of 
 and  yields better performances when  is large. 

Using semi-supervised learning strategies, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and 
 a competitive performance of 96.6 with the full  IMDB dataset. 
Our work marks an initial step toward understanding the behavior of semi-supervised learning models under the context of large-scale pretraining.\footnote{Code, models and datasets  can be found at https://github.com/ShannonAI/Neural-Semi-Supervised-Learning-for-Text-Classification}
\end{abstract}
\section{Introduction} 
Because of the fact that
obtaining 
 supervised training labels is costly and time-intensive, 
 and that   unlabeled data is relatively easy to obtain, 
 semi-supervised learning  \cite{10.7551/mitpress/9780262033589.001.0001,zhu2005semi}, which 
utilizes 
in-domain
 unlabeled data  to improve models trained on the labeled dataset , is of growing interest. 
Under the context of large-scale of language model pretraining \citep{devlin2018bert,yang2019xlnet,yinhan2019roberta,lewis2019bart,bao2020unilmv2,danqi2020spanbert},  where a language model is pretrained on an extremely large, open-domain dataset (denoted by {\it largeU}, with {\it |largeU|}  {\it |U|}),
  how we can make the best use of the in-domain unlabeled dataset 
   is poorly understood. 
 There are basically two ways to take advantages of  the unlabeled, in-domain dataset : 
 {\bf in-domain pretraining}\footnote{To note,  
 the pretraining on the in-domain dataset  is distinguished from the pretraining on the large-scale, open-domain dataset {\it largeU}.
 The model for in-domain pretraining can be randomly initialized or 
 taking a pretrained model based on the open-domain dataset {\it largeU} \citep{dontstoppretraining2020}.}, where a language model is pretrained on 
 the in-domain dataset
 , and then   fine-tuned on ; 
{\bf pseudo-label} based approach \citep{lee2013pseudo,reed2015training,iscen2019label,Shi_2018_ECCV,arazo2020pseudo}, where unlabeled data points are assigned with labels predicted by the model trained  on  (referred to as the teacher model), forming a new dataset . 
A new model (referred to as the student model) is trained for final predictions by considering . 

Many important questions regarding the behavior of semi-supervised learning models under the context of large-scale LM pretraining 
 remain unanswered:  
 Is semi-supervised training
 still beneficial with the presence of large scale pretraining on {\it largeU}? 
  Should  be used for in-domain LM pretraining or pseudo-label generation?
 How should  pseudo-label based semi-supervised models 
  be  implemented?
How different semi-supervised strategies (e.g., self learning) affect performances regarding  of different sizes, and  of different sizes, etc. 


In this paper, we conduct comprehensive studies on the behavior of semi-supervised learning in NLP 
with the presence of large-scale language model pretraining.  
We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks.
Our work sheds important lights on the behavior of semi-supervised learning models: 
we find that
(1)  with the presence of  in-domain pretraining LM on , open-domain LM pretraining \cite{devlin2018bert} 
is unnecessary, and we
are able to achieve better performance with pretraining on  the in-domain dataset ;
 (2)
 both
the in-domain pretraining strategy and the pseudo-label based strategy
 lead to significant performance boosts,
with the former performing better with larger , the latter performing better with smaller , and the 
combination 
 of both performing the best;
(3) for pseudo-label based strategies, 
self-training (pretraining first on the pseudo-label dataset  and then fine-tuning on ) yields better performances when  is small, while joint training on the combination of 
 and  yields better performances when  is large.
 
Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and 
 a competitive performance of 96.6 with the full dataset. 
More importantly, our work marks an initial step toward understanding the behavior of semi-supervised learning models in the context of large-scale pretraining. 


The rest of this paper is organized as follows: related work is detailed in Section 2. 
Different strategies for training semi-supervised models are shown in Section 3. 
Experimental results and findings are shown in Section 4, followed by a brief conclusion in Section 5. 

\section{Related Work}
\subsection{Semi-Supervised Learning}
The goal of semi-supervised learning \citep{10.7551/mitpress/9780262033589.001.0001,zhu2005semi} is to use massive
amount of 
 unlabeled data to improve the models trained  on labeled data. 
One widely-used type of semi-supervised method is 
the pseudo-label  based method
 \citep{lee2013pseudo,reed2015training,iscen2019label,Shi_2018_ECCV,arazo2020pseudo}, where unlabeled data points are assigned with labels predicted by a trained model, 
forming a large pseudo labeled dataset to train a model. 
{\bf Self-training} \citep{1053799,riloff2003learning} is a specific type of pseudo-label  based method that is of growing interest. 
Self-training involves training two models: a ``teacher'' used to label unlabeled data, which is  used as an augmented labeled dataset. 
Then  a ``student'' is trained on the newly augmented dataset. 
This process can be iterated to further boost performances. Self-training has been successfully applied in 
different fields such as 
computer vision \citep{yalniz2019billion,babakhin2019semi,xie2020self,chen2020big,zoph2020rethinking}, automatic speech recognition (ASR) \cite{parthasarathi2019lessons}. 
In Vision, \citet{yalniz2019billion} adopted the self-training paradigm in image classification, and achieves the state-of-the-art top-1 result on ImageNet benchmark; \citet{xie2020self} proposed the strategy of Noisy Student Training, a variant of self-training built on top of EfficientNet \citep{tan2020efficientnet}.
In ASR, \citet{parthasarathi2019lessons} trained an ASR model on one million hours of unlabeled speech data  using a teacher-student self-training model.
\citet{park2020improved} proposed the concept of normalized filtering score that filters out low-confident utterance-transcript pairs generated by the teacher to mitigate the noise introduced by the teacher model.


 
 














In the context of natural language processing (NLP), the concept of semi-supervised learning has been adopted in different NLP tasks such as 
  machine translation \citep{cheng-etal-2016-semi,tu2016neural,ramachandran2016unsupervised,edunov2018understanding,clark2018semisupervised}, information extraction \citep{liao2009simple,peters2017semisupervised}, text classification \citep{nigam2006semi,dai2015semi,miyato2016adversarial,howard2018universal,karamanolakis2019leveraging,li2019learning}, and text generation \citep{zang2019semisupervised,qader2019semisupervised,shang2019semisupervised}.
Particularly, \citet{he2019revisiting} studied the efficacy of self-training on sequence generation tasks and  found that self-training can significantly boost performances, particularly when labeled data is scarce. Besides, they also pointed out that the core of self-training for sequence generation tasks is the noise injected into the neural model, which can be interpreted to smooth the latent sequence space.

Word vector models \citep{mikolov2013distributed,mikolov2013efficient,pennington2014glove,mikolov2013efficient,wordvec2014matrixfactor} and language modeling pretraining  \citep{devlin2018bert,yang2019xlnet,yinhan2019roberta,lewis2019bart,bao2020unilmv2,danqi2020spanbert} 
 can also be viewed as a specific type of semi-supervised learning model, by leveraging the information of data in the general domain. 
Other strategies for training semi-supervised models 
 involve co-training \citep{qiao2018deep,DBLP:conf/eccv/ChenZG18,han2018co},  low-density separation \citep{grandvalet2005semi,dai2017good}. 







\subsection{Data Augmentation}
Data augmentation 
aims at 
 increasing the amount of training data by adding slightly modified copies of  existing data points or  created new synthetic data based on existing data points \citep{krizhevsky2012imagenet,paulin2014transformation,laine2016temporal,sajjadi2016regularization,cubuk2018autoaugment,inoue2018data,cubuk2020randaugment}.
 The concept consistency training \citep{rasmus2015semisupervised,sajjadi2016regularization,laine2017temporal,tarvainen2018mean,miyato2018virtual,luo2018smooth,athiwaratkun2019consistent,li2019decoupled,verma2019interpolation,liu2020decoupled} is widely used as a regulation to force the label of modified copies to the same as the original label. 
 



In NLP, 
 modified copies of  existing data points
are generated usually by 
 synonym replacement and text editing \citep{zhang2015character,kobayashi2018contextual,wei2019eda,gao-etal-2019-soft}, back-translation \citep{sennrich2016back-translation,edunov2018understanding,xie2019unsupervised}, noise injection \citep{wang-yang-2015-thats,xie2017data,xie2018noising}, mixup \citep{guo2019augmenting}, generation \citep{anabytavor2019data,wu2019conditional,kumar2020data}.
  Data augmentation has introduced significant performance boost  especially in low-resource scenarios \citep{fadaee2017data,bergmanis2017training,sahin-steedman-2018-data,xia-etal-2019-generalized,shleifer2019low,singh2019xlda}.





\section{Models}
\begin{comment}
This section introduces our designs of  semi-supervised approaches for NLP tasks. 
We introduce two approaches, different in how the augmented dataset is harvested: 
(1) the model-based approach, in which data is harvested based on the prediction of a trained model; and 
(2) semantic-based approach, in which data is harvested based on semantic relatedness. 
\end{comment}

\subsection{Notations}
We use the task of text  classification 
for illustration purposes, in which the goal is to assign a label  to a given  input .  is a sequence of words. 
We have a given labeled set , , where  denotes the number of data points  in . 
In addition, we have an  
in-domain
unlabeled dataset  of size , where  .
The goal of semi-supervised learning is to explore how the unlabeled 
in-domain
dataset  can be leveraged at the training time. 
At test time, inference remains the same as the original setup. 
\subsection{In-domain LM Pretraining}
A direct way to take  advantage of 
 is to  pretrain  a BERT \citep{devlin2018bert}, RoBERTa \citep{yinhan2019roberta} or GPT3 \citep{brown2020language} style language model
on 
 by predicting a masked word given surrounding contexts or 
 predicting
 a subsequent word given proceeding contexts. 
 Pretraining on the in-domain data  
facilitates the learning of in-domain   
semantics and word compositions. 

For training, the LM model can be trained from scratch with random initialization or 
 initialized using an existing BERT or RoBERTa model trained on an open-domain dataset, the latter of which is similar to the idea in \newcite{dontstoppretraining2020}, which continues LM pretraining in different domains and tasks. 
 The LM model trained on  is used as initialization to be further finetuned on . 


\subsection{Pseudo-label Based Approach}
Another way to take advantage of  is to use the model trained on  to assign
pseudo
 labels to data points in . 
Specifically, 
the model trained on  is referred to as the {\it teacher} model. 
The teacher model is used to 
  assign pseudo labels to  or a specific portion  of , forming  the augmented  dataset . 
Let  denote the number of selected examples in  .
There are  different options on how to generate ,  how 
  and  are combined, and how the new model can be trained on the  combination. 
The model trained on the combination is referred to as the {\it student} model. 

The teacher  and the student can share a similar model backbone. 
In the  text classification task, various structures can be used as the backbone such as LSTMs \citep{hochreiter1997long,tang-etal-2016-effective}, 
CNNs \citep{kim-2014-convolutional}, BERT \cite{devlin2018bert,chai2020description}, etc, with the trained model to maximize the probability of 
predicting
the golden label  given . 
The {\it student} model can be randomly initialized or initialized using the {\it teacher} model. 


\subsubsection{Different strategies to construct }
Here we discuss different strategies to generate .
\paragraph{Naive Strategy :}
The teacher model is used to label all instances in  to form . 
The  shortcoming for this strategy is that incorrect and low-confident labels 
  included in 
  can be severely detrimental to the student model. 

\begin{comment}
\paragraph{Top-K Semantic Relatedness}  
To avoid out-of-domain examples, we can select examples in  that are more semantically similar to .
This purpose can be achieved by the following two methods:
(1) for each example , we iterate over all examples in , and obtain  the top-K examples  that are most semantically relevant to .  
We form a new  by 
 combining the top-K related examples for all instances in , with psudo labels assigned by the model trained on ; and 
 (2) we can train a language model on , and select the in-domain data from  with lowest perplexity scores. 
Similarly, the labels for the selected examples are given by the model trained on . 

Apparently, selecting in-domain data can significantly alleviate the issue with out-of-domain data, but might suffer from the incorrect labels. 
\end{comment}
\paragraph{Top-K Model Predictions:} To avoid the negative effect from the 
low-confident 
 labels assigned by the teacher, we can only pick the confident ones. 
The teacher  model  is run on each example in  to obtain the probability of all classes. 
Then for each class , we rank all instances in  based on the corresponding probabilities. The top- instances for each class are selected to form  . 


\begin{comment}
This strategy provides guarantees that the model has  confidence on the selected examples. But given the   fragile nature of neural models\footnote{e.g., predictions of neural models can be flipped with human-indistinguishable perturbations \citep{pmlr-v70-arjovsky17a,mirza2014conditional,ijcai2018-601,liang2017deep,ebrahimi2017hotflip}.}, and the situation where  is extremely large and noisy, it is hard to  
guarantee the correctness of model predictions on these high-confident examples. 
\paragraph{Combing Semantic Relatedness and Model Predictions}
To take the complementary strengths of both above strategies, we can combine them, in which the selected example in  should be both semantic related to the training set and is assigned a relatively confident score by the model trained on . 
\end{comment}

\subsubsection{Different Strategies for Student Training }
Here we discuss how models can be trained given  and . 
\paragraph{Training on :}
This strategy is denoted by .
The most straightforward strategy is to train the student model on the union of  and . 
The potential risk with this naive strategy is that 
the influence from clean labels in  can be diluted if the size of  is large and that 
 incorrect labels in  can exert  negative effects on 
the student model.

\paragraph{Pretraining on  and Fine-tuning on :}
This strategy is denoted by .
To make the model more immune to incorrectly labeled examples in , and be able to take advantage of the large amount of data in  at the same time, we can first 
train the student model
 on the newly collected dataset  to predict the pseudo labels, and then fine-tune the model on the original labeled dataset . 
This strategy 
 ensures that
the final model is fine-tuned on the dataset with clean labels.
This process actually mimics the idea of the self-training \citep{devlin2018bert,xie2020self,chen2020big,grill2020bootstrap} in semi-supervised learning literature. 

\paragraph{Pretraining on  and Fine-tuning on :}
This strategy is denoted by . 
A minor change can be made to the strategy described above, where the model is pretrained on the concatenation of  and , and then fine-tuned on . 
Practically, 
we find that   consistently outperforms 
, which is expected since adding examples with golden labels for pretraining does no harm the performance. 
We thus only report results for , and omit  for brevity. 

\paragraph{Iterative Training}
The process of training the teacher and the student 
 can be iterated: 
the student model trained in the previous iteration 
 can be used 
as  a new teacher model  
to   relabel the unlabeled dataset , 
       from which the top- examples are regenerated to form the new . 
       Based on the new , a new student model is trained. 
       This process is repeated until the pre-defined value of iterations  is reached. 



\subsection{Combining In-domain LM Pretraining and Pseudo-label Based Approach}
The in-domain LM pretraining and the pseudo-label based strategy can be combined: 
an LM model is first pretrained on the in-domain data . Next, the model is used  to initialize the teacher, which will be trained on . 
The teacher model is then used to generate pseudo labels for , which are used to train the student.
In this way,  is used twice, both in LM pretraining and pseudo-label generation for student training.


\section{Experiments}
In this section, we conduct extensive experiments to better understand the behavior of semi-supervised learning. 
We give insights gathered from extensive experimental studies and also discuss several considerations towards obtaining  a successful model.
We use the following two  datasets for detailed explorations: 

(1) The labeled dataset  is the
 IMDB dataset collected by \citet{maas2011learning}. 
This dataset contains an even number of positive and
negative movie reviews. The training and test sets respectively contain 25k and 25k examples.
The task is formalized as a binary classification task to decide the polarity of sentiment for a review. 
To explore models' behavior on training datasets of different sizes, we use 10, 20, 50, 100, 1k, 5k and 25k for training. 

For the unlabeled dataset , we crawled the IMDB and collected about 3.4M movie reviews.
We release this large-scale IMDB movie review dataset  to public. 



(2) The labeled dataset  is the deceptive  opinion spam dataset \citep{ott-etal-2011-finding,li-etal-2014-towards} to 
separate fake hotel reviews generated by Turkers and hotel employees from genuine reviews from real customers.  
The task is formulated as a three-class classification task, and  
 contains 800/280/800 reviews, which respectively denote fake reviews from Turkers, fake reviews from hotel employees and genuine reviews from real customers.
For the unlabeled dataset , we crawled TripAdvisor, an online travel  website and collected about 1M reviews from roughly 5k hotels. 

The sentiment analysis task on movie reviews is a significantly easier task than the deceptive review detection task, 
 since the latter requires to model to identify subtle changes in language usage for spam generation, while the former focuses more on identifying sentiment-indicative tokens. 

\paragraph{Training Details}
For in-domain LM pretraining, we use the 
RoBERTa structure \citep{yinhan2019roberta} as the backbone. 
We use both the small model, a 12-layer transformer with the hidden size of each layer being 768,
and a large model, a 24-layer transformer with the hidden size of each layer being 1,024.
Models are trained using using Adam \citep{kingma2014adam} with , , a polynomial learning rate schedule, warmup for 4K steps and weight decay with . Dropout rate is set to 0.2. 
The pretrained LM model on the in-domain  is then finetuned on .
For training the teacher and the student,  we use Adam for optimization.  Learning rate, batch-size and dropout rate are treated hyperparamters tuned on the dev set. 

For reference purposes,  we also implement the BiLSTM model, where the representation at the last time step from the left-to-right direction is concatenated with the 
the representation at the first time step from the right-to-left direction, which is next fed to the softmax function for golden label prediction. 
Word vectors are initialized using GloVe  \citep{pennington2014glove}. 

\begin{table*}[t]
\renewcommand\arraystretch{1.2}
\centering
\small
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{c|c|cccc|cccc|cccc}
  \toprule
  & T & \multicolumn{4}{c}{T} & \multicolumn{4}{c}{T} & \multicolumn{4}{c}{} \\\hline
  \diagbox[width=1.6cm,height=0.82cm]{}{} & 0 & 1K & 10K & 100K & 1M & 1K & 10K & 100K & 1M & 1K & 10K & 100K & 1M \\\hline
  10 & 53.34 & 54.00 & 51.46 & 53.95 & 53.60 & 55.10 & 51.54 & 52.49 & 53.91 & 55.25 & 55.35 & {\bf 55.47} & 54.60\\
  && (+0.66)  &  (-1.88) & (+0.61)  &  (+0.26) & (+1.76)  &  (-1.80) &  (-0.85) & (+0.57)  &  (+1.91) &  (+2.01) & {\bf (+2.13)}  &(+1.26)\\\hline
  20 & 54.62 &  51.73 & 53.28  & 54.16  & 55.43  & 54.35  & 54.50  &  53.92 & 54.43  & { 58.15}  & 58.63  &{\bf 58.77}  & 55.63\\
  && (-2.89)  &  (-1.34) & (-0.46)  &  (+0.81) & (-0.27)  &  (-0.12) &  (-0.70) & (-0.19)  &  { (+3.53)} &  (+4.01) &  {\bf (+4.15)} &(+1.01)\\\hline
  50 & 80.67 & 56.91  & 72.25  & 82.19  &  81.30 & 73.81  & 77.36  & 83.80  & 81.63  & 81.32  &  84.96 & {\bf 85.86}  & 82.85\\
  && (-23.76)  &  (-8.42) & (+1.52)  &  (+0.63) &(-6.86)  &  (-3.31) &  (+3.13) & (+0.96)  & (+0.65) &  (+4.29) &  {\bf (+5.19)} &(+2.18)\\\hline
  100 & 83.12 &  65.67 &  83.49 & 85.52  & 83.95  & 80.30  & 83.50  & 84.89  & 83.60  & 81.86  & 86.25  & {\bf 87.27}  & 84.40\\
  && (-17.48)  &  (+0.37) & (+2.40)  &  (+0.83) & (-2.82)  &  (+0.38) &  (+1.77) & (+0.48)  &  (-1.26) &  (+3.13) &{\bf (+4.15)}  &(+1.28)\\\hline
  1K & 89.90  &  -- &  84.68 & 89.84  & 90.66  & --  & 90.10  &  90.15 & 91.12  &  -- & 90.45  & 90.78  &{\bf 91.32} \\
  && -- &  (-5.22) & (-0.06)  &  (+0.76) &--  &  (+0.20) &  (+0.25) & (+1.22)  & -- &  (+0.55) & (+0.88)  &{\bf (+1.42)}\\\hline
  5K & 93.16 & --  & 87.09  & 91.43  & 92.98  & --  & 93.02  & 93.57  &  {\bf 93.79} & --  & 93.07  & 93.05  & 93.72\\
  && --  &  (-6.07) & (-1.73)  &  (-0.18) & --  &  (-0.14) &  (+0.41) & {\bf (+0.63)}  & -- &  (-0.09) &  (-0.11) &(+0.56)\\\hline
  25K & 95.20 & --  & --  & 90.56  & 92.75  & --& --  & 95.51  & {\bf 95.80}  &  -- & --  &  95.42 &{ 95.50} \\
  && --  &  -- & (-3.24)  &  (-1.05) & --  &  -- &  (+0.31) & (+{\bf 0.60})  &  -- &  -- &  (+0.22) &{ (+0.30)}\\
  \bottomrule
\end{tabular}
}
\caption{Results for different  strategies for
pseudo-label based approaches
 on the IMDB dataset based on {\bf open-domain pretraining}. ``T'' represents ``Train'' and ``F'' represents ``Fintune''. Brackets represent the used dataset(s), e.g., ``T(D+D')F(D)'' means the model is trained on  and finetuned on . The best result for each row is marked in bold.}
\label{tab:results}
\end{table*}
\begin{figure}[t]
  \centering
  \includegraphics[scale=0.43]{general.pdf}
    \includegraphics[scale=0.43]{general_deceptive.pdf}
  \caption{Teacher perforamnces for BiLSTMs,   In-domain pretraining, 
  Open-domain pretraining, 
  Open-domain+In-domain pretraining  on the IMDB (upper) and the deceptive opinion spam (lower) datasets based on different sizes of the labeled dataset. All pretraining models use RoBERTa small as the backbone.  }
  \label{fig:general}
\end{figure}


\subsection{Teacher Performances}
We first examine  teacher performances
 for BiLSTMs,  {\it In-domain pretraining}, {\it Open-domain pretraining} and {\it Open-domain+In-domain  pretraining} 
on the IMDB and the deceptive  spam  datasets 
regarding different values of , as 
 shown in Figure \ref{fig:general}. 
For {\it In-domain pretraining}, a 
randomly initialized 
LM model is  first pretrained on the in-domain dataset , and then fine-tuned  on  .
For {\it Open-domain pretraining},
we  directly take the pretrained RoBERTa model, which is  pretrained on the open-domain dataset . 
For   {\it open-domain+In-domain  pretraining}, the LM model is first pretrained on  , then on the in-domain dataset , and last fine-tuned  on . 
The pseudo-label based semi-supervised strategy is  applied in none of these setups. 
The accuracy progressively increases 
as we increase the size of the labeled dataset , which is in line with our expectation. 


\paragraph{Is open-domain  pretraining still necessary?}
Take the IMDB dataset as an example. 
As can be seen from Figure \ref{fig:general}, 
the performance of {\it In-domain pretraining} 
(95.87 when )
on the 3.4M unlabeled reviews performs 
nearly 
 the same as {\it open-domain+In-domain  pretraining} (95.82), both of which significantly outperform 
{\it open-domain pretraining} (95.20). 
Similar phenomena are observed for the deceptive spam dataset, where
{\it In-domain pretraining},  {\it open-domain+In-domain  pretraining} and {\it open-domain pretraining} respectively 
obtains 82.5, 82.0 and 78.2 accuracy. 
This demonstrates that with the presence of relatively large in-domain data, the extremely time-intensive training of
 LM on a huge amount of open-domain data is unnecessary. 

\subsection{In-domain Pretraining}
As shown in Figures \ref{fig:general} and \ref{fig:size_d}, 
the  in-domain pretraining strategy has significantly better {\bf few-shot} learning abilities and
requires much smaller amount of  data for training:
the performance 
from in-domain pretraining
drastically improves as   increases from 10 to  50, 
achieving a performance of 93.8 accuracy on the IMDB dataset with only 50 training examples.
This performance is 
higher 
 than BiLSTMs with 25K training examples, and similar to the performance of vanilla RoBERTa with 5K training examples. 
This shows that  LM pretraining on the in-domain dataset  provides the model with the generality to rapidly figure out the necessary task-specific information for predictions. 

Comparing with existing  pretraining models that have few-shot learning ability such as GPT3, the advantages of the semi-supervised in-domain pretraining are obvious: the model
is easier to train, 
 has significantly fewer parameters and does not have to rely on a vast amount of training data.

\subsubsection{Influence from the size of }
It is widely accepted that LM pretraining requires a massive amount of training data. 
Results for  
of different sizes 
for LM pretraining are shown in  Figure \ref{fig-U-pretrain}.
The model is randomly initialized and then trained on the in-domain  until convergence. 
The pretrained LM model is next 
 fine-tuned on . 

As can be seen, the performance of {\it in-domain pretraining} highly relies on the size of the in-domain dataset . 
With smaller sizes of , {\it in-domain pretraining}  underperforms {\it open-domain pretraining}, which is in line with our expectation since small   cannot provide enough evidence for learning in-domain word semantics and compositions.
Specifically, the performance of  is only slightly better than the {\it no-pretraining} setup;
the performance  for {\it in-domain pretraining} outperforms {\it open-domain pretraining} only when the size of  exceeds 1M. 

\begin{figure}[t]
\center
    \includegraphics[scale=0.45]{size_of_U_LM.pdf}
  \caption{The influence of  for in-domain pretraining with .  }
  \label{fig-U-pretrain}
\end{figure}


\begin{figure}[t]
\center
  \includegraphics[scale=0.45]{size_of_U.pdf}
  \caption{The influence of   for pseudo-label based approaches.  }
  \label{fig-U-pseudo}
\end{figure}

\begin{figure*}[!ht]
  \includegraphics[scale=0.38]{size_of_D.pdf}
  \caption{The influence of  on pseudo-label based approaches for different .  }
  \label{fig:size_d}
\end{figure*}



\begin{figure*}[!ht]
  \includegraphics[scale=0.38]{size_of_D_.pdf}
  \caption{The influence of  on pseudo-label based approaches.  is required to be larger than . }
  \label{fig:size_d_}
\end{figure*}


\subsection{Pseudo-label Based Approaches}
Detailed results for 
pseudo-label based approaches 
based on open-domain pretraining
are shown in Table \ref{tab:results}.
The  trends can be summarized as follows: 

(1) Generally, both  and  
perform better than  the vanilla setup .
 underperforms the vanilla setup  when  is small and  is large, 
but outperforms  when  is of medium size. 

(2) The performance boost 
introduced by pseudo-label based methods gradually increases as the size of  increases, and then shrinks.
The explanation is as follows:
with a small , the accuracy of the teacher model is low.
Most predicted labels on  are  thus unreliable. 
Therefore, 
the advantage from the model trained on the noisy  is relatively small;
with a large , the teacher model is already good enough, reaching an accuracy higher than 0.9. 
Though
almost all 
 predicted labels on  are  correct, their improvement upon an already pretty good teacher model is small.
 There is a sweet spot for the size of , where  pseudo-label based methods introduce the largest boost: 
 with a medium-sized , where the teacher model reaches an acceptable accuracy when the correctly labeled examples in 
outweigh incorrectly labeled ones, the student model can take the most advantage of , leading to significant performance boosts of +5. 
Similar trends are observed for , where  underperforms the vanilla setup  both when  is small and  is large, 
but outperforms  when  is of medium size. 


(3) For the three pseudo-label based strategies, ,  and , 
 performs the worst for all .  This is in line with our expectation because for ,  only  is used for final predictions, and thus the influence of the golden-labeled dataset  is diluted;
 works best  
when  is small, while 
 works best when  is large.
Our explanations are as follows: 
when  is small,  the student trained on  is inferior due to the massive amount of incorrect labels in . 
The model thus needs to be further fine-tuned on , making  
perform better than ; 
When  is large, most labels in  are correct and
the student is more immune to the small proportion of incorrect labels in . 
Directly training on the larger  dataset provides the model with more generalization ability,
while fine-tuning only on  dilutes the influence from massive amount of correct labels in , making the performance of 
  worse than  when  is large. 

\subsubsection{Influence from the size of }
With a fixed in-domain dataset , we select top-K examples for each label .
Different values of  lead to different sizes of . 
There is apparently a tradeoff between the size of  and the confidence for examples included in :
larger size of  means that more less-confident examples are selected.

Trends regarding different values of 
are shown in Figure \ref{fig:size_d_}.
We only run setups with   larger than . 
As can be seen, for smaller  (), the final performance first increases as  gets larger, which means the model is taking  advantage of evidence provided by the pseudo labels. Then, the performance decreases as  continues to increase, which means the model starts suffering from the incorrectness of the less-confident examples. 
For larger values of  ( larger than 1k),  performances keep increasing as  gets larger. This is because the teacher model trained on  is good enough to generate confident examples. 



\subsubsection{Influence from the size of }
The influence of the size of  is 
already 
 manifested in the size of , since the size of  should be larger than the size of  ( is selected from ). 
 Performances for different sizes of  are shown in Figure \ref{fig-U-pseudo}, where we randomly sample examples from the 3.4 million IMDB reviews to form  of different sizes. 
 For a given , we plot the performance achieved with the best . 
 Larger  should lead to better performances since more confident examples can be selected.
 
 As shown in Figure \ref{fig-U-pseudo},
for  of different sizes,
the performance first increases as  grows, but then immediately  plateaus. 
Explanations are as follows: 
for larger , the model is already good enough to provide confidently correct labels for points in , and the improvement from 
extra confident  examples is marginal. 

\subsubsection{Iterative Training}
We can iterate the teacher-student pattern 
for pseudo-label based approaches, where the teacher for the current iteration is initialized with the student in the previous iteration.
Results are shown in Figure \ref{Iterative}. As can be seen, additional performance boosts are observed for different  as the iterative process goes on.
For larger , the performance becomes stagnant across the iterative process, while for smaller , the curve keeps rising until convergence.  

\begin{figure}[t]
\center
    \includegraphics[scale=0.45]{iterative.pdf}
  \caption{Iterative Training for Pseudo-label Based Approaches on IMDB based on Open-domain Pretraining.}
  \label{Iterative}
\end{figure}

\subsection{Combing In-domain Pretraining and Pseudo-label Based Approaches}
In-domain pretraining and pseudo-label based approaches can be combined, where the teacher model is initialized with the pretrained model on the in-domain dataset . 
Results
for the combined strategy 
 are shown in Table \ref{combined}.
As can be seen, both strategies lead to progressive performance boosts over Open-domain Pretraining, with a combined boost of +1.1 for the small model, and
+1.0 for the large model. 
 
\begin{table}
\center
\small
\begin{tabular}{ll}\hline
\multicolumn{2}{c}{Small |D|=25k}   \\
Open-domain Pretraining & 95.2 \\
Open-domain+In-domain Pretraining & 95.8 \\
In-domain Pretraining &95.8 \\
In-domain Pretraining + Pseudo-label & 96.3\\\hline
\multicolumn{2}{c}{Large |D|=25k}   \\
Open-domain Pretraining & 95.6 \\
In-domain Pretraining & 96.2 \\
In-domain Pretraining + Pseudo-label & 96.6\\\hline
\end{tabular}
\caption{Results for Combining In-domain Pretraining and Pseudo-label based Approaches}
\label{combined}
\end{table}

\section{Conclusion}
In this paper, we conduct comprehensive 
analysis on semi-supervised learning in NLP under the context of 
large-scale
language model pretraining. 
We find that even with the presence of large-scale LM pretraining,
both the in-domain pretraining strategy and the pseudo-label based strategy introduce 
additional 
significant performance boost, 
with the former performing better with larger ,  the latter performing better with smaller , and the combination leading to the best performance. 
Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and 
 a competitive performance of 96.6 with the full dataset. 
Our work 
sheds light on 
 the behavior of semi-supervised learning models in the context of large-scale pretraining. 



\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}


\end{document}
