


\documentclass[journal,10pt,draftcls,onecolumn]{IEEEtran}

\usepackage{graphics} \usepackage{epsfig} \usepackage{mathptmx} \usepackage{times} \usepackage{amsmath} \usepackage{amssymb}  \usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{latexsym}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}   
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{property}{Property}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}


\title{Partition-based Distributed Kalman Filter with plug and play features}
\author{Marcello Farina\thanks{Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, via Ponzio 34/5, 20133 Milan, Italy}, Ruggero Carli\thanks{Department of Information Engineering University of Padova, Via Gradenigo 6/B, 35131, Padova, Italy} }

\begin{document}

\maketitle

\begin{abstract}                          In this paper we propose a novel partition-based distributed state estimation scheme for non-overlapping subsystems based on Kalman filter. The estimation scheme is designed in order to account, in a rigorous fashion, for dynamic coupling terms between subsystems, and for the uncertainty related to the state estimates performed by the neighboring subsystems. The online implementation of the proposed estimation scheme is scalable, since it involves (\emph{i}) small-scale matrix operations to be carried out by the estimator embedded in each subsystem and (\emph{ii}) neighbor-to-neighbor transmission of a limited amount of data. We provide theoretical conditions ensuring the estimation convergence. Reconfigurability of the proposed estimation scheme is allowed in case of plug and play operations. Simulation tests are provided to illustrate the effectiveness of the proposed algorithm.
\end{abstract}

\section{Introduction}
In many different engineering areas there has been, in the last years, a huge effort to develop algorithms and protocols allowing a number of interconnected, possibly spatially distributed systems, devices, sensors, and actuators, to operate cooperatively and to possess self-organization capabilities. Notable examples include smart grids \cite{Resende11}, environmental monitoring systems \cite{EMMON11}, large-scale irrigation and hydraulic networks \cite{Cantoni07,MaeDoa:12-007}, and multi-robot/vehicle systems \cite{MartinezCortesBullo07,Murray07}.\\
Related research on systems of systems \cite{Samad2011} or cyber-physical systems \cite{Antsaklis2013} is nowadays fostered, pursuing several challenges, including the design of hierarchical and distributed monitoring and control systems with reliability and robustness properties with respect to uncertainties, changing environment, communication failures, etc.\\
In particular, theoretically sound distributed monitoring and state estimation methods are necessary to allow for optimal managing of sensor networks. As also discussed in the survey paper~\cite{Sijs_Lazar_et_al_2008}, two main classes of estimation techniques for distributed smart sensing schemes are presently under investigation. They are generally both referred, in the literature, to as \emph{distributed state-estimation} algorithms. While a widely-considered problem concerns the case where the full state of the system is estimated by all subsystems, e.g., based on consensus and diffusion strategies, e.g., \cite{Saber07CDC,diffusion_d_est,TACFFS09}, in this paper we focus on \emph{partition-based estimation}. The latter consists of estimating, for each sensor, only a part of the state vector of a system, using information transmitted by other neighboring sensors. This problem gives rise to low-order estimation problems solved in a distributed way, and is particularly useful when the observed systems are large scale ones, e.g., power networks~\cite{Siljac78,FP-RC-FB:12}, transport networks~\cite{s1978}, process plants~\cite{Vadigepalli2003}, and robot fleets~\cite{Mutambara_Durrant-Whyte2000}.\\
Concerning linear discrete-time systems, recent contributions include \cite{Vadigepalli2003,Khan2008,Stankovic09,Farina2010,Farina2011b,Negenborn-Kalman13,Haber13,Riverso2013b,Riverso2013e}.\\ Among these, \cite{Vadigepalli2003,Khan2008,Stankovic09,Negenborn-Kalman13} propose Kalman filter-based estimation schemes suitable for systems affected by stochastic noise. The papers \cite{Vadigepalli2003,Stankovic09} propose methods based on local Kalman prediction equations (and neglect the dynamic interconnection terms) and on consensus steps to account for possible overlapping states between pairs of subsystems. The paper \cite{Khan2008} proposes a two-step Kalman filter, where the correction step is performed by each subsystem based on local measurements, while the prediction step is based on
approximating the centralized error process using a distributed iterate-collapse inversion algorithm for -banded matrices \cite{KhanMouraDICI2008}. As in \cite{Vadigepalli2003,Stankovic09}, a consensus step is used to optimally account for overlapping states. Finally, in \cite{Negenborn-Kalman13}, a prediction/corrector-based method for multi-rate systems is proposed. It is worth noting that sufficient convergence conditions are provided just in \cite{Stankovic09} which, in case of non-overlapping subsystems, basically amount to the stability of the original system.\\
The papers \cite{Farina2010,Farina2011b,Riverso2013b,Riverso2013e} assume that the system is affected by bounded noise and guarantee, under suitable conditions, convergence of the estimator and the fulfillment of constraints on local states, e.g., in \cite{Farina2010}, or estimation errors, e.g., in \cite{Farina2011b,Riverso2013b,Riverso2013e}. Finally, \cite{Haber13} proposes an approximated distributed filter based on the moving horizon estimator studied in \cite{ABB03}. A different - cooperative and iterative - approach based on Lagrange decomposition is proposed in \cite{Georges20141451}, where continuous-time systems are considered.\\
The conditions required for convergence of the estimators discussed in all the mentioned papers, where available, (with the notable exception of \cite{Riverso2013e}) require a centralized synthesis/analysis phase which (\emph{i}) limits the application to very large-scale systems and (\emph{ii}) requires a complete re-design in case of configuration changes (e.g., addition/removal of subsystems or sensors). On the other hand, in \cite{Riverso2013e} the design phase (guaranteeing global properties) is distributed, i.e., the state estimator embedded in each subsystem is devoted to solve a local design problem. This has paved the way to a plug-and-play (PnP) implementation \cite{StoustrupEJC,RiversoFarinaGFT_PnP13}, which confers flexibility, reconfigurability, and reliability to the estimation architecture.\\
In this paper we propose a novel partition-based distributed state estimation scheme based on Kalman filter (denoted DKF) for non-overlapping subsystems affected by stochastic noise. The estimation scheme is designed to account for dynamic coupling terms between subsystems, and for the uncertainty related to the state estimates performed by the neighboring subsystems. This is done in a conservative but rigorous way by means of suitable covariance matrix bounds. The online implementation of the proposed estimation scheme is scalable, since it involves (\emph{i}) small-scale matrix operations to be carried out by the estimator embedded in each subsystem and (\emph{ii}) neighbor-to-neighbor transmission of a limited amount of data. Concerning the design/analysis phase, we provide both centralized (both with suitable linear matrix inequalities and with aggregate small gain-type arguments) and distributed scalable conditions to be verified ensuring the estimation convergence. The latter are then used to provide a fully distributed and PnP implementation of DKF. More specifically, distributed reconfigurability conditions are provided in case a subsystem is added to or removed from the network, and also in case PnP operations involve sensors.

The paper is structured as follows. In Section \ref{sec:statement} we introduce and motivate the distributed Kalman filter equations, while in Section \ref{sec:convergence} we provide the main conditions for convergence. In Section \ref{sec:d-design} we discuss how DKF can be designed in a distributed fashion and the resulting application for PnP operations. Finally, in Section \ref{sec:Exs} the algorithm is tested both on an academic example and on a benchmark case study, and in Section \ref{sec:conclusions} some conclusions are drawn. All proofs are postponed to Appendix A for better readability.\\
\noindent
\textbf{Notation}\\
The symbols  and  are used to denote semi-definite positive matrices and definite positive matrices, respectively. The symbols  and  are used for brevity to denote the Riccati equation update and the optimal Kalman predictor gain, respectively, i.e.,

where , , , , and  are matrices of appropriate dimensions. Finally, the cardinality of a set  is denoted with~ and the spectral radius of matrix  is denoted .
\section{The distributed Kalman filter}
\label{sec:statement}
\subsection{Statement of the problem}
Consider  interconnected systems, each described by the following equations:

where  and .
We assume that  and  are zero-mean white noises, for all , and that ,  (with  for all ), and that  for all , and . For , we define with  the set of neighbors (also denoted predecessors in \cite{RiversoFarinaGFT_PnP13}) of subsystem  defined as  while  is the set of successors of subsystem  defined as . In our setup we assume that subsystem  can exchange information with its neighbors. Note that  is in general included in  and .\\
Collectively, if we define the variables , , , and , we can rewrite \eqref{eq:subsystems00} as

where diag, diag, diag, and\\
\\
The optimal centralized Kalman predictor \cite{GoodwinRiccati-84} for system \eqref{eq:system00} is

where  denotes the one-step optimal predictor of . According to the classical Kalman prediction theory, the optimal gain is

where  is the centralized Kalman prediction error covariance matrix and is computed iteratively using the Riccati equation

\subsection{Distributed prediction scheme}
As clear from \eqref{eq:predictor00}-\eqref{eq:riccati_centr00}, the optimal centralized Kalman predictor for system \eqref{eq:system00} is based on the iteration of the Riccati equation \eqref{eq:riccati_centr00}, which requires a global knowledge of the system and, in general, leads to a matrix gain which has not the sparsity properties of the dynamic system (i.e., of matrix ).\\
In contrast, in this paper we seek for a distributed observer implementation, meaning that: (\emph{i}) at most data originated by neighbors are used by the local observers, to reduce the communication load of the scheme; (\emph{ii}) information about the model of the overall system is not required to be stored by each local observer, but at most information concerning the neighboring subsystems; (\emph{iii}) the computational load required by each local filter is scalable.\\
In line with this we propose an estimation scheme of the type

where

The matrices ,  are updated according to the following distributed equation.

where, for all , we have defined , , and , where . Note that \eqref{eq:gain_distributed} and \eqref{eq:riccati_distr01} are equivalent to  and , respectively.

Equation \eqref{eq:subpredictor00} is distributed, i.e.,  only if . Therefore, the computation of  can be done distributedly and communication is required between local state estimators of dynamically interconnected subsystems only. Concerning the scalability of the algorithm observe also that, for , subsystem  permanently stores in memory only the matrices , , ,  and, for , ,  and ; on the other hand, the information which must be transmitted and temporarily stored at each time step consists of , , ,  for all . The DKF algorithm is formally described in Algorithm~1.

\begin{algorithm}
\label{algo:DKF}
\caption{DKF algorithm}
\textbf{Memory requirements}\\
For , subsystem  stores in memory
\begin{algorithmic}
\State - Permanently , , , , ;
\State - Temporarily, for each , ;
\end{algorithmic}
\hrulefill \\
\textbf{On-line implementation}\\
At each time step  subsystem :
\begin{algorithmic}
\State \textbf{1)} Measures 
\State \textbf{2)} Broadcasts to its successors the quantities , , and ;
\State \textbf{3)} Gathers from its neighbors the information ;
\State \textbf{4)} Computes the gains  as in \eqref{eq:gain_distributed};
\State \textbf{5)} Computes the estimate  and the matrix  as in \eqref{eq:subpredictor00} and \eqref{eq:riccati_distr01}, respectively.
\end{algorithmic}
\end{algorithm}
\subsection{Main properties}
Let us define , and the distributed filter estimation error . From \eqref{eq:system00} and \eqref{eq:subpredictor00} we obtain that

where  is the matrix whose block entries are . Let var. From~\eqref{eq:prediction_error_collective} the following is obtained.

The following result can be derived.
\begin{lemma}
\label{lemma:stability}
Assume that the pair  is stabilizable (where ) and that there exist symmetric matrices ,  such that

for all . For all , let
 and let  be the matrix whose block entries are . Then, the matrix  is Schur stable.\hfill{}
\end{lemma}
Thanks to Lemma~\ref{lemma:stability}, a \emph{simplified version} of the DKF Algorithm~1 can be devised: assuming that each subsystem  stores in memory matrix , , with property \eqref{eq:Pi_bar}, then it is sufficient to set  and  for all  to guarantee that the estimation error  is a stationary process. Therefore, the error covariance of this modified scheme is asymptotically convergent to a bounded definite positive matrix, i.e.,  for some positive definite matrix .

In case the Algorithm~1 is implemented, under the assumption that there exist steady-state solutions of \eqref{eq:riccati_distr01} , , the next result can be proved.
\begin{proposition}\label{prop:stability}
Consider the DKF Algorithm 1. Assume that , , are such that there exists  with the property that

Let diag. Then, there exists a positive definite matrix  such that  and . \hfill{}
\end{proposition}
Note that, under the validity of \eqref{eq:Pi_bar_Eq}, then in steady state conditions also \eqref{eq:Pi_bar} is verified. Therefore the DKF Algorithm~1 provides a stationary equation error; also, Proposition~\ref{prop:stability} states that, for , matrix  plays the role of an upper bound of the covariance of the prediction error  in steady state.\\
Observe that Lemma \ref{lemma:stability} and Proposition \ref{prop:stability} require the existence of matrices , , such that either property \eqref{eq:Pi_bar} or property \eqref{eq:Pi_bar_Eq} are satisfied. However, differently from the centralized Kalman filter, these properties are not guaranteed by standard detectability assumptions on the system.\\
In this paper we provide conditions under which these properties can be verified. In particular, in Section \ref{sec:convergence} we discuss the conditions allowing the application of  centralized design procedures while, in Section \ref{sec:d-design}, we provide a distributed design procedure.\\
We conclude this section with a couple of remarks.
\begin{remark}
Consider the DKF algorithm and assume that \eqref{eq:Pi_bar_Eq} holds true.
Let  be the steady-state covariance of the prediction error for the centralized Kalman filter. Obviously, if  then   for all .
\end{remark}
\begin{remark}\label{rem:subopt_costant}
Assume that \eqref{eq:Pi_bar} holds true and consider the \emph{simplified DKF algorithm} described after Lemma \ref{lemma:stability}. Then, also in this case, the asymptotic covariance of the prediction error  is
upper-bounded by the matrix .
\end{remark}

\section{Centralized design}
\label{sec:convergence}
In this section we address the problem of providing () conditions that can be used to guarantee a-priori the validity of properties \eqref{eq:Pi_bar} or \eqref{eq:Pi_bar_Eq} and () practical methods for computing them. First, in Section \ref{subsec:convergence:LMI} we will analyze \eqref{eq:Pi_bar} through a linear matrix inequality approach; secondly, in Section \ref{subsec:convergence:smallgain} we will provide an aggregate design procedure, based on small gain arguments, to guarantee \eqref{eq:Pi_bar_Eq}
\subsection{Design using LMI's}
\label{subsec:convergence:LMI}
In this section we provide a practical method based on LMI's for computing, if possible, matrices  verifying \eqref{eq:Pi_bar}. Then, as already highlighted, if we set  for all  and for all , then it is guaranteed that this simplified version of the DKF algorithm has suitable convergence properties in view of Lemma \ref{lemma:stability}. Also, its suboptimality features are discussed in Remark \ref{rem:subopt_costant}.\\
Using LMI's we aim to compute (see Lemma \ref{lemma:stability}) ,  verifying

Provided that  is non singular for each , the algebraic inequality \eqref{eq:riccati_distr04_alg} is equivalent to

thanks to the application of the matrix inversion lemma. Inequality \eqref{eq:riccati_distr05_alg} can be cast as the following LMI

where  is defined in such a way that  and, for all , . If we define , the latter inequality can be written as

Finally, the equality  can be managed using the recursive approach proposed in \cite{ElGhaoui_ConeComplementarity}.
Indeed, we solve the following LMI

and, at the same time, we minimize the additional cost function
.
The problem can be managed using the recursive cone complementarity linearization algorithm discussed in \cite{ElGhaoui_ConeComplementarity}.
\subsection{Design using small gain arguments}
\label{subsec:convergence:smallgain}
In this section we investigate conditions ensuring the validity of \eqref{eq:Pi_bar_Eq}. In particular,
the following result addresses the offline design issue providing an aggregate and lightweight analytical condition, which relies on small-gain arguments. First, the following assumption is required.
\begin{assumption}
\label{ass:local-in} For subsystem ,  is invertible.\hfill
\end{assumption}
We will also need one of the following assumptions for properly initializing  for the implementation of Algorithm~1.
\begin{assumption}
\label{ass:local-det1}For subsystem 
\begin{itemize}
\item[(i)]  is detectable;
\item[(ii)]  is stabilizable, where  verifies ;
\end{itemize}
\end{assumption}
\begin{assumption}
\label{ass:local-det2}For subsystem 
\begin{itemize}
\item[(i)]  is detectable;
\item[(ii)]  is stabilizable;
\end{itemize}
\end{assumption}
Note that, while Assumption~\ref{ass:local-det1} is required to define, for a given subsystem ,  as the unique semi-positive definite solution to the local Riccati algebraic equation , Assumption~\ref{ass:local-det2} allows to define  as the unique semi-positive definite solution to .

Let us now define full rank  arbitrary transformation matrices , , i.e.,  (introduced for reducing, if possible, the conservativity of the results stated next) and gains , selected in such a way that  is Schur stable.
Define also  and , for all .
Finally we define

Scalars ,  are defined in such a way that . We introduce a further assumption.
\begin{assumption}
For some values of , , (i)  for all , and (ii) .
\label{ass:small-gain-global}
\end{assumption}
Note that, a necessary condition for the existence of matrix  guaranteeing that  is that  is detectable, i.e., Assumption~\ref{ass:local-det2}; therefore, the latter is implicitly required by Assumption~\ref{ass:small-gain-global}.
\begin{theorem}
\label{prop:small-gain}
 If Assumption \ref{ass:local-in} holds for all  and under Assumption \ref{ass:small-gain-global}, there exist  for all  such that,  as  if one of the following initializations is used:
\begin{enumerate}[a.]
\item  for all .
\item  if Assumption~\ref{ass:local-det1} holds for all .
\item  if Assumption~\ref{ass:local-det2} holds for all .\hfill{}
\end{enumerate}
\end{theorem}
Regarding Assumption~\ref{ass:small-gain-global}, provided that Assumption~\ref{ass:local-det2} (i) is verified, it is always possible of find  such that  for all . Note that, in case the system has a cascade topology (i.e., if it admits a lower - or upper - block triangular form,~\cite{Siljak91}),  is block triangular, and therefore Assumption ~\ref{ass:small-gain-global} can be easily verified.\\
On the other hand, for more general system structures, we need to retrieve a suitable ``decentralized" change of coordinates and, at the same time, a suitable ``auxiliary" decentralized linear observer, for which  for all  and the corresponding matrix  is stable. This amounts to a design problem, which can be cast, for example, as the following optimization.

\min_{\{H_{i},L_{i}\}_{i=1}^M} \sigma(\Gamma)

\sigma(\Gamma)&<1\\
\sigma(\hat{F}_i)&<1,\,\,i=1,\dots,M

To reduce the computational load of \eqref{eq:centralized_design} the values of  can be constrained. For example, one can try to minimize the terms  by constraining  to take values corresponding to which  is diagonal (provided that  is diagonalizable and has real eigenvalues), or one can set . The optimization \eqref{eq:centralized_design} is nonlinear, and therefore a suitable initialization is fundamental, for example selecting  as the Kalman predictor gains.\\
To reduce the computational complexity and to allow for flexible and reliable operation, in next Section we provide a distributed and scalable design procedure to be applied at each subsystem level.
\section{Distributed design and plug and play features}
\label{sec:d-design}
In many practical applications, it is of interest to perform the design of the DKF in a distributed fashion, i.e., to have a set of conditions to be verified locally by each subsystem, possibly using pieces of information provided by the neighboring subsystems.\\
Focusing on the main assumptions of Theorem \ref{prop:small-gain}, while Assumptions~\ref{ass:local-in} and~\ref{ass:small-gain-global} (i) are local conditions, to be verified at a single subsystem level, Assumption \ref{ass:small-gain-global} (ii) is centralized (although aggregate), since it involves information concerning the overall system.
We now introduce the following assumption, providing a conservative, yet distributed and very simple, condition, that must be verified at a single subsystem level by each subsystem, that implies the Schur stability of , as proved in Proposition~\ref{propo:Gamma-distributed} stated below.
\begin{assumption}
For all  and for some values of , , it holds that

\label{ass:small-gain-distributed}
\end{assumption}
\begin{proposition}
\label{propo:Gamma-distributed}
If Assumption \ref{ass:small-gain-distributed} holds, then Assumption \ref{ass:small-gain-global} (ii) is verified.\hfill
\end{proposition}
As it will be shown in the remainder of the section, this result allows for PnP operation.
The PnP scenario consists of the case when one or more subsystems (each described by \eqref{eq:subsystems00}) or devices (and specifically a transducer) is added to or removed from the interconnected system.\\
Before to proceed, the following standing assumption sets the scenario where PnP operations take place, assuming that the PnP event occurs at time instant .
\noindent
\begin{assumption}\hfill\\
- For , Assumptions~\ref{ass:small-gain-global} (i),~\ref{ass:small-gain-distributed}, and~\ref{ass:local-in} (for all ) hold.\\
- At  the updates \eqref{eq:riccati_distr01} are in steady state, i.e.,  for all .\hfill
\label{ass:PnP}
\end{assumption}
It is important to remark that, when PnP operations involving subsystems take place, the number of successors, for some subsystems, may change. Denote with  the set of successors of subsystem  after the PnP event and . In general it holds that . From this, it also follows that the matrices , , and  must be redefined, i.e., , , and .
Importantly, in case , this may prevent the detectability of the pair  to hold, which may jeopardize the verifiability of Assumption~\ref{ass:small-gain-global} (i). We also assume that  and  are not redefined, for  after the PnP event. From this it follows that, for all , .
\subsection{Plug-in of a subsystem}
Assume that, at step , the subsystem  is introduced. For each 
\begin{itemize}
\item if , then . Also, since , , then  and . In view of this, ;
\item if  but , then : Therefore , , and . However, since , . Therefore
;

\item if  but , then . However,  and therefore  may not be Schur stable. If  is stable,  but, at the same time, . In view of this  for all . Therefore
;
\item , the Schur stability of  is not guaranteed. If  is Schur stable, we can compute , in view of the fact that both  and .
\end{itemize}
The design of ,  can be addressed through the following optimization problem.

\min_{H_{M+1},L_{M+1}} \rho_{M+1}^+ + \sum_{j\in \mathcal{S}_{M+1}}\gamma_{j(M+1)}^+

\sigma(\hat{F}_{M+1}^+) < 1&
,\,\rho_{M+1}^+ < 1\\
\gamma_{j(M+1)}^+<1-\frac{1-\lambda_j^2}{1-\lambda_j^{+2}}\rho_j&\text{ for all } j\in \mathcal{S}_{M+1}

When a plug-in request is received from subsystem , the following design procedure must be adopted: (i) if \eqref{eq:scalable_design} admits a solution and if, for all ,  and , then allow the plug-in, otherwise deny it; (ii) properly initialize .\\
The following corollary of Theorem \ref{prop:small-gain} addresses the step (ii) and guarantees convergence of the system matrices ,  to steady state solutions.
\begin{corollary}
\label{cor:small-gain-Plugin}
If Assumption \ref{ass:local-in} holds also for  and if, after the plug-in event, Assumptions \ref{ass:small-gain-global} (i) and \ref{ass:small-gain-distributed} are verified,
then there exist  for all  such that,  as  if the following initialization is used:  for all  and (a) , or (b) if Assumption~\ref{ass:local-det1} holds for , , or (c) if Assumption~\ref{ass:local-det2} holds for , .\hfill
\end{corollary}
Note that the initializations (b) and (c) limit possible undesirable transients on the state estimates.
Note also that, at the -th subsystem level, to solve \eqref{eq:scalable_design}, the required data consist in (i) the local system matrices , (ii) the number  of successors of subsystem , (iii) , ,  for all , (iv) , ,  for all . It is therefore clear that this local design problem requires the transmission of a limited amount of information, i.e., through a neighbor-to-neighbor communication graph.\\
Also, remark that the optimization problem \eqref{eq:scalable_design} is a nonlinear one; to simplify it, an efficient strategy amounts, for example, to define  as the matrix such that  is diagonal (i.e., in case  is diagonalizable and has real eigenvalues), making  depend upon , or simply setting . In this way we can reduce the number of free variables of the problem.
\subsection{Unplug of a subsystem}
\label{sec:unplug}
Assume that, without loss of generality, at step , subsystem  is unplugged. Note that,
\begin{itemize}
\item if , then ;
\item if  but , then ;
\item if  but , then
 and therefore . From this it follows that  but, at the same time, . Also  for all . Therefore
;
\item , it follows that , in view of the fact that both  and .
\end{itemize}
In view of this, since Assumptions \ref{ass:small-gain-distributed} and~\ref{ass:small-gain-global} (i) hold before the unplug event, then they are guaranteed for the system deprived of the -th subsystem. Therefore, any unplug request can be accepted, without hampering the convergence properties of the estimator.\\
The following corollary of Theorem \ref{prop:small-gain} guarantees convergence of the system matrices ,  to new steady state solutions.
\begin{corollary}
\label{cor:small-gain-unPlug}
After the un-plug event, there exist  for all  such that,  as  if the following initialization is used:  for all .
\end{corollary}
\subsection{Plug and play of transducers}
In many practical applications, the sensors embedded in a subsystem can be added, removed, or replaced. We consider that changes occur to the -th subsystem for simplicity, but without loss of generality. Practically, this case consists in a change in the matrix  (and, consequently, ), while the topology of the system and its dynamics remain unchanged. Therefore, for all , , since , , do not depend on , but only on matrices  and on the number of successors, which remain unchanged.\\
On the other hand, focusing on subsystem 
\begin{itemize}
\item if a transducer is plugged in, this consists of adding a row (here denoted ) to matrix , i.e.,

This means that the detectability properties of the pairs  and  are not jeopardized by the plug-in event. Also, if  remains unchanged and if we take , then . This means that the addition of a new transducer does not compromize the convergence properties of the DKF scheme.
\item if a sensor is replaced or unplugged, this consists of a substantial variation of the matrix . This means that, before to allow the PnP operation, one must verify the existence of a gain  such that the following are verified: (I) Schur stability of ; (II) . Concerning the latter, note that  should remain unchanged, in order not to affect the values of , .
\end{itemize}
In the case considered in this section, however, it is not clear how the PnP operation impacts on the values of the matrices , . In order to guarantee the convergence of the matrices  to a steady state, the following practical procedure can be adopted, suggested by Corollaries \ref{cor:small-gain-Plugin} and \ref{cor:small-gain-unPlug}: (a) for , make ,  evolve as if subsystem  were unplugged; (b) after convergence is achieved (say, at instant ) make , ,  evolve as if subsystem  were plugged-in at time , i.e., by setting ,  and .
\section{Simulation results}
\label{sec:Exs}
In this section we provide some simulation results illustrating the application of DKF to two different examples, an academic one and the Hycon2 benchmark described in \cite{SR-GFT:12}, respectively.
\subsection{Academic example}
In this section we consider a system composed of interconnected subsystems. We set\\

and, for all , diag, where . Also  and  where  is the -dimensional identity matrix.
\subsubsection{Dependence on coupling and centralized design}\hfill\\
First consider , with .
In Figure \ref{fig:rho} we show the relationship between the coupling strength  and (\emph{i}) the spectral radius  of matrix ; (\emph{ii}) the spectral radius of  obtained through the LMI-based design procedure sketched in Section \ref{subsec:convergence:LMI}. The latter procedure has given numerically reliable results for .\\
To realize the upper plot, two different choices of  are adopted: (I) such that  is diagonal; (II)  for all . As it is apparent, the spectral radius of  does not significantly vary in the latter cases.\\
In both cases it is apparent that the small-gain procedure sketched in Section \ref{subsec:convergence:smallgain} is applicable when the coupling strength is sufficiently small. Also, from the lower panel it is apparent that  is just sufficient to guarantee that .
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{rho2.eps}
\caption{Upper panel: values of  as a function of  (setting  such that  is diagonal - dots; setting  - circles); lower panel: values of  as a function of .}
\label{fig:rho}
\end{figure}

\subsubsection{Plug and play scenario}\hfill\\
Now, assume that  and . At time ,  and , i.e., subsystem 3 is not connected with the network. In this case

Therefore, we have that  for .\\
At  subsystem 3 plugs in and connects with subsystem 2. More specifically
, , and . In this case

The plug-in request is accepted, since  for .\\
Finally, at , subsystem 1 unplugs, meaning that  and . As discussed in Section~\ref{sec:unplug}, the unplug request is automatically accepted, as it is witnessed by the values taken by the entries on  in this case: 
In Figure~\ref{fig:varspnp} the state trajectories are depicted, showing the different collective dynamical behaviours taken in correspondence with the different graph configurations.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{varspnp.eps}
\caption{State trajectories.  denotes the -th entry of .}
\label{fig:varspnp}
\end{figure}

In Figure~\ref{fig:errspnp} the trajectories of the root mean estimation errors rmse for all subsystems' states are depicted, showing that, in view of the proper matrix initializations, when plug and play operations occur the estimation error does not suffer from undesirable transients.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{errspnp.eps}
\caption{Root mean estimation error for each subsystem, obtained with DKF - solid line - and centralized KF - dotted line (the lines are practically overlapping).}
\label{fig:errspnp}
\end{figure}

\subsection{Power network benchmark}
In this section we consider a power network system including a number of power generation areas coupled through tie-lines. This system has been adopted also in \cite{Negenborn-Kalman13} where the authors proposed a partition-based distributed estimation scheme tailored to power networks applications and exhibiting promising numerical results (although without any theoretical guarantees).\\
Our contributions are two-fold: firstly, in Section \ref{ex:comparison} we compare DKF with the centralized Kalman filter and the distributed strategy proposed in \cite{Negenborn-Kalman13}; secondly, in Section \ref{ex:PlugPlay} we test the PnP features of DKF in case a new subsystem is plugged in the network during its operation.\\
The dynamics of each power generation area, equipped with primary control and linearized around the equilibrium value for all variables, is described by the following continuous time LTI model \cite{SR-GFT:12}

where  is the state,  is the control input of each area, and  is the local power load. Note that the letter  is used to denote the deviation from steady-state. The matrices of system \eqref{eq:SysTurbine} are


where the parameters and their numerical values are defined in \cite{SR-GFT:12}.
Since both  and  are assumed to be constant and known, for the sake of simplicity, we neglect them from our analysis.\\
We discretize the process \eqref{eq:SysTurbine} with a sampling interval  according to the technique proposed in \cite{FarinaAut2013}, leading to the discrete-time model \eqref{eq:subsystems00} where the matrices ,  can be easily constructed from \eqref{eq:SysTurbine}. The matrix  is

For ,  and  where  is the -dimensional identity matrix.

\subsubsection{Comparison test}\label{ex:comparison}\hfill\\
In this section we consider the scenario  in \cite{SR-GFT:12}, where  and where the adjacency matrix , defining the neighboring relationships between areas, is

namely,  if and only if . In Figure \ref{fig:PrimaComponente} we depict  and its estimate  generated by the DKF algorithm.\\
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{PrimaComponente.eps}
\caption{Trajectory of  (blue line) and its estimate (red line), obtained with DKF}
\label{fig:PrimaComponente}
\end{figure}
In Figures \ref{fig:Comparison} and \ref{fig:Comparison1} we compare the performance of DKF algorithm with that of the centralized Kalman predictor and of the distributed strategy proposed in \cite{Negenborn-Kalman13}. In Figure \ref{fig:Comparison} we plot the normalized estimation error  defined as

for the first  iterations. In Figure \ref{fig:Comparison1} we plot  from  up to  (i.e., in stationary conditions).
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{ComparisonNewIn.eps}
\caption{Trajectory of  obtained with DKF (red line), with a centralized Kalman predictor (blue line), and with the method proposed in \cite{Negenborn-Kalman13} (black line), with .}
\label{fig:Comparison}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{CompZoom.eps}
\caption{Trajectory of  obtained with DKF (red line), with a centralized Kalman predictor (blue line), and with the method proposed in \cite{Negenborn-Kalman13} (black line), with .}
\label{fig:Comparison1}
\end{figure}
In the Table \ref{table:TabellaMedia} we report the average value of the estimation error evaluated between iteration  and .
\begin{center}
\begin{tabular}{r|c|c|c|}\label{table:TabellaMedia}
&Centralized&\,\,\,DKF\,\,\,& Strategy in [29]\\ \hline
Error Mean&13.74&14.08&17.21 \\ \hline
\end{tabular}
\end{center}
Notice that the performance of DKF algorithm is quite close to the performance of the centralized Kalman filter and that it outperforms the performance of the strategy in \cite{Negenborn-Kalman13}. Additionally Assumption \ref{ass:small-gain-distributed} is satisfied with .

\subsubsection{Plug and play scenario}\label{ex:PlugPlay}\hfill\\
In this section we consider a PnP scenario. Specifically we assume that at time step  a new area (i.e., area ) is added to the power network, and that, in particular, it gets connected to area . Again the values of the parameters defining area  can be found in \cite{SR-GFT:12}. The adjacency matrix describing the interconnections after step  is

In Figure \ref{fig:PlugPlay} we depict the behavior of the estimation error for both the centralized Kalman filtering algorithm and DKF. Observe that, also in this plug and play scenario the performance of the DKF algorthm is comparable with that of the centralized Kalman filter.
As expected, when a new area is added to the network the value of  increases mainly due to the poor estimation quality concerning the state of the area . However, after few iterations the value of  settles around a value which is comparable to its value before the addition of the new area.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{PlugPlayInRic.eps}
\label{fig:PlugPlay}
\caption{Trajectory of  obtained with DKF (red line) and with a centralized Kalman predictor (blue line) in the PnP scenario.}
\end{figure}
\section{Conclusions}
\label{sec:conclusions}
In this paper a novel partition-based distributed observer based Kalman filter, named DKF, is proposed. The main advantages of the discussed state estimator are: (\emph{i}) scalability, in terms of both computational and communication loads required for the online operations; (\emph{ii}) the convergence properties can be proved under mild conditions; (\emph{iii}) distributed and plug and play design are allowed. In fact, not only centralized (although aggregate) but also distributed conditions for estimation convergence are given, which confer reconfigurability to the proposed estimation scheme. Simulation tests are provided to illustrate the effectiveness of DKF. For example, we have considered a well-known benchmark example, proposed in the framework of the Hycon2 Project.
Future work include the application of DKF to a real test case, e.g. smart grids.
\appendix
\section{Proofs}
\label{app:proofs}
The following preliminary result is needed for the proofs of both Lemma \ref{lemma:stability} and Proposition \ref{prop:stability}.

\begin{lemma}
Define . If   are updated according to~\eqref{eq:riccati_distr01}, then

\label{prop:distr}
\end{lemma}
\emph{Proof of Lemma~\ref{prop:distr}}\\
Since  is block-diagonal define

where

for all . Furthermore, it holds that

where

which is equivalent to \eqref{eq:riccati_distr01}. Inequality \eqref{eq:Riccati_ineq} can be proved as follows. Define a vector , where  for all . We compute that 

where . Remark that  identically iff , and that the number of nonzero vectors  is equal to . We compute that
. Note that, since , . Therefore
.\\
From this, it follows that

  from which \eqref{eq:Riccati_ineq} readily follows.\hfill{}

\medskip
\emph{Proof of Lemma~\ref{lemma:stability}}\\
From Lemma~\ref{prop:distr}, one has that

where the block entries of  are , which is equivalent to . The latter follows from the fact that , where
,
which is block-diagonal in view of the block-diagonality of , , and .
Assume, by contradiction, that  is not Schur stable. Therefore, there is at least an eigenvalue/eigenvector pair  of  such that  and . From \eqref{eq:riccati_ineq_01}

from which it follows that
.
Since the right hand side of the latter inequality is  and , the only possibility is that , , and . In view of this,  and  should hold at the same time which, recalling the PBH test, is in contradiction with the assumption that the pair  is stabilizable. This concludes the proof of Lemma~\ref{lemma:stability}.\hfill

\medskip
\emph{Proof of Proposition \ref{prop:stability}}

\medskip

As a preliminary step, we show that, if , then   for all . This can be proved using induction arguments. Assume that, at instant , it holds that . Recalling  Lemma \ref{prop:distr}, we have that
,
where .
From this and \eqref{eq:prediction_error_collective_var} it results that

Therefore, . By applying induction arguments, we can prove that  for all .\\
If  as , then  such that, in view of Lemma \ref{lemma:stability},  is Schur stable. Consider the evolution of matrix . From the stability of , then , for all initial conditions , where  is the unique solution to the Lyapunov equation
.
If we set , from the preliminary result then  for all  and . Noting that  is the unique steady-state attained for all possible initial conditions the proof is concluded. \hfill

\medskip
The proof of Theorem \ref{prop:small-gain} heavily relies on classical results on Kalman filters, e.g., \cite{Caines-Mayne-Riccati,AndMooFilt,GoodwinRiccati-84,GoodwinRiccati-86}.
Similarly to well known results on the discrete-time Riccati equation, we need two intermediate results.

\begin{lemma}
\label{lemma:riccati:monoton}
If  for all , then  where  and  are the matrix evolutions, obtained with \eqref{eq:riccati_distr01}, starting from  and , respectively.
\end{lemma}
\begin{proof}
Note that we can write \eqref{eq:riccati_distr01} as

where, according to the classical Kalman filter theory,  minimizes the term  for all . Therefore, consider the matrices , , where  for all , and optimal the gains  and  corresponding to  and , respectively, then for all , ,
and the proof is concluded.\hfill
\end{proof}

\begin{lemma}
\label{lemma:riccati:boundedness_small-gain}
If Assumptions \ref{ass:local-in} (for all ) and \ref{ass:small-gain-global} hold then, for all  ,
there exist  for all  such that  for all .
\end{lemma}
\begin{proof}
An alternative formulation of \eqref{eq:riccati_distr01} is

where  and , being  defined in \eqref{eq:PF_def}.
In view of Assumption \ref{ass:local-in}, we can write . Therefore

Since  is Schur stable (thanks to Assumption~\ref{ass:small-gain-global} (i)) and  is a suboptimal gain

Solving the latter we obtain:

Using the transformation matrices , we define
,  .
Note also that .
In view of this and \eqref{eq:Delta_def}, we can rewrite \eqref{eq:deltaP} as follows

Recalling that  for all , we have that

Therefore

Denoting , \eqref{eq:pseudoISS} implies that

where  and . Finally denote the vectors  and . We obtain that

According to \cite{Wirth-Small-gain07}, if the spectral radius of  is strictly smaller than one, for every initial condition (see, e.g., Lemma 13 for the general nonlinear case), the solution to the system \eqref{eq:riccati_distr02} exists and is uniformly bounded, since  does not depend on .\hfill{}\\
\end{proof}

\medskip
Now we are in the position to provide the proof of Theorem \ref{prop:small-gain}.

\medskip

\emph{Proof of Theorem \ref{prop:small-gain}}

\medskip
First consider all the initializations a, b, and c.

\medskip
a. In case  for all  then  for all .

\medskip
b. Set  for all .
Note that matrices  exist and are unique for all  in view of Assumption \ref{ass:local-det1}.
From \eqref{eq:riccati_distr01}, for all 



\medskip
c. Set  for all .
Note that matrices  exist and are unique (for all ) in view of Assumption \ref{ass:local-det2}.
Set  for all . Then, from \eqref{eq:riccati_distr01}, for all 


\medskip
In all cases, applying induction arguments and in view of the monotonicity property (i.e., Lemma \ref{lemma:riccati:monoton}),  for all  and for all . Therefore the sequence of matrices diag is monotonically increasing, in the sense that  for all . In view of the boundedness property  (i.e., Lemma \ref{lemma:riccati:boundedness_small-gain}), there exist  for all , such that  as .\hfill{}

\medskip
\emph{Proof of Proposition \ref{propo:Gamma-distributed}}\\
The proof easily follows from the Gershgorin circle theorem. Indeed, each eigenvalue of  lies in at least one of the  Gershgorin circles, i.e., since  for all , the values of  satisfying , for each . Then, if  for all , all eigenvalues verify .\hfill


\medskip
\emph{Proof of Corollary \ref{cor:small-gain-Plugin}}\\
When plug-in events take place, if  is a neighbor (also said \emph{predecessor} in \cite{RiversoFarinaGFT_PnP13}) of , then , otherwise . In view of this,  (for ), , and  for all ; otherwise , , and . Therefore, for all initializations and for all 

\begin{itemize}
\item[a.] if , .
\item[b.] if ,
     ,
\item[c.] if ,
     
\end{itemize}
In all cases it follows that, for all , . Applying an induction argument and in view of the monotonicity property,  for all  and for all . Therefore the sequence of matrices diag is monotonically increasing, in the sense that  for all . Since Assumption~\ref{ass:small-gain-global} (ii) holds in view of Assumption~\ref{ass:small-gain-distributed} and Proposition~\ref{prop:distr}, the boundedness property holds in view of Lemma \ref{lemma:riccati:boundedness_small-gain}, and therefore there exist  for all , such that  as .\hfill


\medskip
\emph{Proof of Corollary \ref{cor:small-gain-unPlug}}\\
When the unplug event takes place, if  is a neighbor of , then , otherwise . In view of this,  if , and  and  for all ; otherwise , , and .
Therefore, for all 

Then, applying an induction argument and in view of the monotonicity property,  for all  and for all . Therefore the sequence of matrices diag is monotonically decreasing, in the sense that  for all . In view of the fact that , there exist  for all , such that  as .\hfill

\bibliographystyle{plain}        \bibliography{DMHE_bib}          

\end{document} 