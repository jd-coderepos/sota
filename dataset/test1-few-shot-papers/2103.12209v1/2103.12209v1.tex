








\documentclass[journal]{IEEEtran}





\usepackage{ifpdf}


\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    linkcolor=black}
\usepackage{algorithm2e}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}  
\usepackage{booktabs}
\usepackage{caption}
\usepackage{color}
\usepackage{etoolbox}
\usepackage{epsfig}
\usepackage{filecontents}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{lmodern}
\usepackage{lipsum}
\usepackage{makecell}
\usepackage{siunitx}
\usepackage{soul}
\usepackage{slashbox}
\usepackage{subfig}
\usepackage{times}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage{xspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{hyphenat}
\usepackage[table]{xcolor}


\graphicspath{{./figures/}}
\DeclareGraphicsExtensions{.png , .jpg, .jpeg, .eps}

\usepackage[noadjust]{cite}
\renewcommand{\citepunct}{,\penalty\citepunctpenalty\,}
\renewcommand{\citedash}{--}

\definecolor{maroon}{cmyk}{0,0.87,0.68,0.32}
\definecolor{Gray}{gray}{0.9}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{Red}{rgb}{1,0,0}
\definecolor{Green}{rgb}{0,1,0}
\definecolor{Blue}{rgb}{0,0,1}

\newcommand{\correctthis}[1]{{\color{Red} #1}}
\newcommand{\checkthis}[1]{{\color{Blue} #1}}


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\Fig}[1]{Fig. \ref{fig:#1}}
\newcommand{\Eq}[1]{Eq. (\ref{eq:#1})}
\newcommand{\Sect}[1]{Sect. \ref{sec:#1}}
\newcommand{\sSect}[1]{Sect. \ref{ssec:#1}}
\newcommand{\Tab}[1]{Table \ref{tab:#1}}
\newcommand{\Alg}[1]{Alg. \ref{alg:#1}}

\newcommand{\itn}[1]{\emph{#1}}

\newcommand{\Exp}[1]{Exp\# \ref{exp:#1}}
\makeatletter
\newcommand{\textString}[1]{#1\renewcommand{\@currentlabel}{#1}}
\makeatother

\ifCLASSINFOpdf
\else
\fi


\usepackage{amsmath}


\usepackage{algorithmic}



















\hyphenation{op-tical net-works semi-conduc-tor}



\newcommand{\R}{\mathbb{R}}

\newcommand{\realdata}{r}
\newcommand{\syntheticdata}{s}
\newcommand{\inputimage}{x}
\newcommand{\inputsequence}{X}
\newcommand{\groundtruth}{Y}
\newcommand{\depthmap}{d}
\newcommand{\semsegmap}{c}
\newcommand{\supmask}{\varpi}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\supervised}{\mbox{{\scriptsize sp}}}
\newcommand{\selfsupervised}{\mbox{{\scriptsize sf}}}
\newcommand{\domainadaptation}{\mbox{{\tiny DA}}}
\newcommand{\edgesmooth}{\mbox{{\scriptsize es}}}
\newcommand{\retroprojection}{\mbox{{\scriptsize proj}}}
\newcommand{\mde}{\Psi}
\newcommand{\wmde}{\theta}
\newcommand{\waux}{\vartheta}
\newcommand{\wauxself}{\waux^{\selfsupervised}}
\newcommand{\wauxda}{\waux^{\domainadaptation}}
\newcommand{\wmdeparenc}{\wmde^{\mbox{{\scriptsize enc}}}}
\newcommand{\wmdeparpyr}{\wmde^{\mbox{{\scriptsize pyr}}}}
\newcommand{\wmdepardec}{\wmde^{\mbox{{\scriptsize dec}}}}
\newcommand{\wmdeparpyrdec}{\wmde^{\mbox{{\scriptsize pyde}}}}
\newcommand{\lossequalizer}{\omega}
\newcommand{\maxestidepth}{d^{\mbox{{\scriptsize max}}}}

\newcommand{\meangtdepth}{d^{\syntheticdata,\mbox{{\scriptsize m}}}}
\newcommand{\meanestidepth}{\hat{d}^{\syntheticdata,\mbox{{\scriptsize m}}}}

\newcommand{\Nr}{N^{\realdata}}
\newcommand{\Ns}{N^{\syntheticdata}}
\newcommand{\ir}[1]{\inputimage^{\realdata}_{#1}} \newcommand{\irh}[1]{\hat{\inputimage}^{\realdata}_{#1}}
\newcommand{\is}[1]{\inputimage^{\syntheticdata}_{#1}} \newcommand{\dr}[1]{\depthmap^{\realdata}_{#1}} \newcommand{\ds}[1]{\depthmap^{\syntheticdata}_{#1}} \newcommand{\cs}[1]{\semsegmap^{\syntheticdata}_{#1}} \newcommand{\ms}[1]{\supmask^{\syntheticdata}_{#1}} \newcommand{\totalloss}{\loss}
\newcommand{\totallosspar}[1]{\totalloss(#1)}
\newcommand{\suploss}{\loss^{\supervised}}
\newcommand{\selfloss}{\loss^{\selfsupervised}}
\newcommand{\selflosses}{\loss^{\selfsupervised,\edgesmooth}}
\newcommand{\selflossproj}{\loss^{\selfsupervised,\retroprojection}}
\newcommand{\daloss}{\loss^{\domainadaptation}}
\newcommand{\suplosspar}[1]{{\suploss(#1)}}
\newcommand{\selflosspar}[1]{{\selfloss(#1)}}
\newcommand{\selflossespar}[1]{{\selflosses(#1)}}
\newcommand{\selflossprojpar}[1]{{\selflossproj(#1)}}
\newcommand{\dalosspar}[1]{{\daloss(#1)}}
\newcommand{\wmdepar}[1]{{\mde(\wmde;#1)}}
\newcommand{\RWSequences}{\inputsequence^{\realdata}}
\newcommand{\VWSequences}{\inputsequence^{\syntheticdata}}
\newcommand{\VWSequencesGT}{\groundtruth^{\syntheticdata}}
\newcommand{\selflosseq}{\lossequalizer^{\selfsupervised}}
\newcommand{\MiniBatch}{B}
\newcommand{\BRWSequences}{\inputsequence^{\realdata}_{\MiniBatch}}
\newcommand{\BVWSequences}{\inputsequence^{\syntheticdata}_{\MiniBatch}}
\newcommand{\BVWSequencesGT}{\groundtruth^{\syntheticdata}_{\MiniBatch}}
\newcommand{\dalosseq}{\lossequalizer^{\domainadaptation}}
\newcommand{\LONEMETRIC}[1]{\|{#1}\|_1}
\newcommand{\lossgrad}[1]{{\nabla_{#1}}}
\newcommand{\lossgradvalue}[2]{{\Delta^{#1}_{#2}}}
\newcommand{\posenet}{\mathrm{T}}
\newcommand{\posetransf}{T}
\newcommand{\posenetpar}[1]{\posenet(#1)}
\newcommand{\posetransfpar}[1]{{\hat{\posetransf}}^{\realdata}_{#1}}
\newcommand{\irhpose}[1]{\hat{\inputimage}^{\realdata}_{#1}}
\newcommand{\cpe}{cpe}
\newcommand{\pe}{pe}
\newcommand{\cpepar}[1]{\cpe(#1)}
\newcommand{\pepar}[1]{\pe(#1)}
\newcommand{\averagedcpe}[1]{\overline{\cpe}(#1)}
\newcommand{\mr}[2]{\supmask^{\realdata}_{#1}(#2)}
\newcommand{\minphotoerror}[2]{P^{\realdata}_{#1}(#2)}
\newcommand{\mindepthedgeerror}[2]{S^{\realdata}_{#1}(#2)}
\newcommand{\averagedphotoerror}[2]{\Bar{\selflossmatrix_{#1}}(#2)}


\newcommand{\danet}{\mathrm{D}}

\newcommand{\depthscalefactor}{\psi}

\newcommand{\gan}{\mathcal{G}}
\newcommand{\ganpar}[1]{{\gan(#1)}}
\newcommand{\wauxgan}{{\waux^{\gan}}}
\newcommand{\igan}{{\inputimage^{\gan}}}




\begin{document}
\title{Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision}


\author{Akhil Gurram,
        Ahmet Faruk Tuna,
        Fengyi Shen,
        Onay Urfalioglu,
        and Antonio M. L\'opez
\thanks{Akhil and Antonio are with the Dpt. of Computer Science, Universitat Aut\`onoma de Barcelona (UAB), Spain. Akhil, Ahmet, Fengyi, and Onay are with the Huawei European Research Center, 80992 M{\"u}nchen, Germany. Fengyi is with the Dpt. of Informatics, Technische Universit\"at MÃ¼nchen (TUM), Germany. Antonio is also with the Computer Vision Center (CVC) at UAB, Spain. Corresponding author: {\tt\small akhil.gurram@huawei.com} }      
\thanks{Antonio acknowledges the financial support received for this research from the Spanish TIN2017-88709-R (MINECO/AEI/FEDER, UE) project. Antonio acknowledges the financial support to his general research activities given by ICREA under the ICREA Academia Program. Antonio acknowledges the support of the Generalitat de Catalunya CERCA Program as well as its ACCIO agency to CVC's general activities.}}










\maketitle

\begin{abstract}
Depth information is essential for on-board perception in autonomous driving and driver assistance. Monocular depth estimation (MDE) is very appealing since it allows for appearance and depth being on direct pixelwise correspondence without further calibration. Best MDE models are based on Convolutional Neural Networks (CNNs) trained in a supervised manner, {\ie}, assuming pixelwise ground truth (GT). Usually, this GT is acquired at training time through a calibrated multi-modal suite of sensors. However, also using only a monocular system at training time is cheaper and more scalable. This is possible by relying on structure-from-motion (SfM) principles to generate self-supervision. Nevertheless, problems of camouflaged objects, visibility changes, static-camera intervals, textureless areas, and scale ambiguity, diminish the usefulness of such self-supervision. In this paper, we perform \emph{mono}cular \emph{d}epth \emph{e}stimation by \emph{v}irtual-world \emph{s}upervision (MonoDEVS) and real-world SfM self-supervision. We compensate the SfM self-supervision limitations by leveraging virtual-world images with accurate semantic and depth supervision, and addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms previous MDE CNNs trained on monocular and even stereo sequences.
\end{abstract}

\begin{IEEEkeywords}
Self-supervised monocular depth estimation, on-board vision, domain adaptation, ADAS, autonomous driving.
\end{IEEEkeywords}

\section{Introduction}

Understanding the semantic content of images is enough to solve many vision-based applications. However, augmenting semantic information with depth is essential for many other applications too, as for on-board perception in autonomous driving and driver assistance. Due to cost and maintenance considerations, we wish to predict depth from the same single camera used to predict semantics, so having a direct pixelwise correspondence without further calibration. Therefore, in this paper, we focus on monocular depth estimation (MDE) on-board vehicles, thus, facing outdoor traffic environments. Recent advances on MDE rely on Convolutional Neural Networks (CNNs). Let  be a CNN architecture for MDE with weights , which takes a single image  as input, and estimates its pixelwise depth map  as output, {\ie}, . The  can be trained in a supervised manner, {\ie}, finding the values of  by assuming access to a set of images with pixelwise depth ground truth (GT). Usually, such a GT is acquired at training time through a multi-modal suite of sensors, at least consisting of a camera calibrated with a LiDAR or some type of 3D laser scanner variant \cite{Eigen:2014, Liu:2016, Roy:2016, Laina:2016, Cao:2017, Fu:2018DORN, Gurram:2018, He:2018, Xu:2018, Yin:2019}. Alternatively, we can use self-supervision based on either a calibrated stereo rig \cite{Saxena:2007, Garg:2016, Godard:2017, Pillai:2019}, or a monocular system and structure-from-motion (SfM) principles \cite{Zhou:2017, Yin:2018GeoNet, Zhao:2020, Guizilini:20203D}, or on a combination of both \cite{Godard:2019MonoDepth2}. Combining stereo self-supervision and LiDAR supervision has been also analyzed \cite{Kuznietsov:2017, He:2018wearable, Guizilini:2020}. In any case, the cheaper and simpler the suite of sensors used at training time, the better in terms of scalability and general access to the technology; however, the more challenging training a . Currently, supervised methods tend to outperform self-supervised ones \cite{De:2021},
thus, improving the latter is an open challenge worth to pursue.   

This paper focuses on the most challenging setting, namely, when at training time we only have a single on-board camera allowing for SfM based self-supervision. Using only such a self-supervision may give rise to depth estimation inaccuracies due to camouflage (objects moving as the camera may not be distinguished from background), visibility changes (occlusion changes, non-Lambertian surfaces), static-camera cases ({\ie}, stopped ego-vehicle), and textureless areas, as well as to scale ambiguity (depth could only be estimated up to an unknown scale factor). In fact, an interesting approach to compensate for these problems could be leveraging virtual-world images (RGB) with accurate pixelwise depth (D) supervision. Using virtual worlds \cite{Gaidon:2016, Cabon:2020, Ros:2016, Mayer:2016, Richter:2017, Shah:2017, Dosovitskiy:2017}, we can acquire as many RGBD virtual-world samples as needed. However, these virtual-world samples can only be useful provided we address the virtual-to-real domain gap \cite{Zheng:2018T2Net, Kundu:2018AdaDepth, Zhao:2019GASDA, Pnvr:2020SharinGAN, Cheng:2020S3Net}, which links MDE with visual domain adaptation (DA), a realm of research in itself \cite{Csurka:2017, Wang-Deng:2018, Wilson:2020}.

We propose to perform \emph{mono}cular \emph{d}epth \emph{e}stimation by \emph{v}irtual-world \emph{s}upervision (MonoDEVS) and real-world SfM self-supervision, estimating depth in absolute scale. By relying on standard benchmarks, we show that our MonoDEVSNet outperforms previous ones trained on monocular and even stereo sequences. 
We think our released code and models\footnote{\href{https://github.com/HMRC-AEL/MonoDEVSNet}{https://github.com/HMRC-AEL/MonoDEVSNet}} will help researchers and practitioners to address applications requiring on-board depth estimation, also establishing a strong baseline to be challenged in the future.


In the following, Section \ref{sec:relatedwork} summarizes previous works related to ours. Section \ref{sec:method} details our proposal. Section \ref{sec:experiments} describes the experimental setting and discusses the obtained results. Finally, Section \ref{sec:conclusion} summarizes the presented work and conclusions, and draws the work we target for the near future.

\section{Related Work}
\label{sec:relatedwork}




MDE was first addressed by combining hand-crafted features and shallow machine learning \cite{Saxena:2007, Liu:2010, Ladicky:2014, Srikakulapu:2015}. However, nowadays, best performing models are based on CNNs \cite{De:2021}. Therefore, we review the CNN-based approaches to MDE which are most related to ours. 


\subsection{Supervised MDE}
\label{sec:rw:supervised}

Relying on depth GT, Eigen {\etal} \cite{Eigen:2014} developed a  architecture for coarse-to-fine depth estimation with a scale-invariant loss function. This pioneering work inspired new CNN-based architectures to MDE \cite{Liu:2016, Laina:2016, Roy:2016, Cao:2017, He:2018, Xu:2018, Fu:2018DORN}, which also assume depth GT supervision. MDE has been also tackle as a task on a multi-task learning framework, typically together with semantic segmentation as both tasks aim at producing pixelwise information and, eventually, may help each other to improve their predictions at object boundaries. For instance, this is the case 
of some {'s} for indoor scenarios \cite{Mousavian:2016, Jafari:2017, Jiao:2018}. These proposals assume that pixelwise depth and class GT are simultaneously available at training time. 
However, this is expensive, being scarcely available for outdoor scenarios. In order to address this problem, Gurram {\etal} \cite{Gurram:2018} proposed a training framework which does not require depth and class GT to be available for the same images. Guizilini {\etal} \cite{Guizilini:2020semantic} used an out-of-the-box CNN for semantic segmentation to train semantically-guided depth features while training . 

The drawback of these supervised approaches is that the depth GT usually comes from expensive LiDARs, which must be calibrated and synchronized with the cameras; {\ie}, even if the objective is to use only cameras for the functionality under development. Moreover, LiDAR depth is sparse compared to available image resolutions.
Besides, surfaces like vehicle glasses or dark vehicles may be problematic for LiDAR sensing. Consequently, depth self-supervision and alternative sources of supervision are receiving increasing interest. 

\subsection{Self-supervised MDE}
\label{sec:rw:self-supervised}

Using a calibrated stereo rig to provide self-supervision for MDE is a much cheaper alternative to camera-LiDAR suites. Garg {\etal} \cite{Garg:2016} pioneered this approach by training  with a warping loss involving pairs of stereo images. 
Godard {\etal} \cite{Godard:2017} introduced epipolar geometry constraints with additional terms for smoothing and enforcing consistency between left-right image pairs.
Chen {\etal} \cite{Chen:2019} improved MDE results by enforcing semantic consistency between stereo pairs, via a joint training of  and semantic segmentation.
Pillai {\etal} \cite{Pillai:2019} implemented sub-pixel convolutional layers for depth super-resolution, as well as a novel differentiable layer to improve depth prediction on image boundaries, a known limitation of stereo self-supervision. 
Other authors \cite{Kuznietsov:2017, He:2018wearable} still complement stereo self-supervision with sparse LiDAR supervision.

SfM principles \cite{Ozyesil:2017} can be also followed to provide self-supervision for MDE. In fact, in this setting we can assume a monocular on-board system at training time. Briefly, the underlying idea is that obtaining a frame, , from consecutive ones, , can be decomposed into jointly estimating the scene depth for  and the camera pose at time  relative to its pose at time ; {\ie}, the camera ego-motion. Thus, we can train a CNN to estimate (synthesize)  from , where, basically, the photo-metric error between  and  acts as training loss, being  the output of this CNN ({\ie}, the synthesized view). After the training process, part of the CNN can perform MDE up to a scale factor (relative depth).


Zhou {\etal} \cite{Zhou:2017} followed this idea, adding an explainability mask to compensate for violations of SfM assumptions (due to frame-to-frame changes on the visibility of frame's content, textureless surfaces, {\etc}). This mask is estimated by a CNN jointly trained with  to output a pixelwise belief on the synthesized views. Later, Yin {\etal} \cite{Yin:2018GeoNet} proposed GeoNet, which aims at improving MDE by also predicting optical flow to explicitly consider the motion introduced by dynamic objects ({\eg}, vehicles, pedestrians), {\ie} a motion that violates SfM assumptions. However, this was effective on predicting occlusions, but not in significantly improving MDE accuracy. Godard {\etal} \cite{Godard:2019MonoDepth2} followed the idea of having a mask to indicate stationary pixels, which should not be taken into account by the loss driving the training. Such pixels typically appear on vehicles moving at the same speed as the camera, or can even correspond to full frames in case the ego-vehicle stops and, thus, the camera becomes stationary for a while. Pixels of similar appearance in consecutive frames are considered as stationary. A simple definition which can work because, instead of using a training loss based on absolute photo-metric errors ({\ie} on minimizing pairwise pixel differences), it is used the structure similarity index measurement (SSIM) \cite{Wang:2004}. Moreover, within the so-called  MonoDepth2 framework, Godard {\etal} \cite{Godard:2019MonoDepth2} combine SfM and stereo self-supervision to establish state-of-the-art results. Alternatively, Guizilini {\etal} \cite{Guizilini:2020semantic} addressed the presence of dynamic objects by a two-stage MDE training process. The first stage ignores the presence of such objects, returning a  trained with a loss based on SSIM. Then, before running the second stage, the training sequences are processed to filter out frames that may contain erroneous depth estimations due to moving objects. Such frames are identified by applying , a RANSAC algorithm to estimate the ground plane from their estimated depth, and determining if there is a significant number of pixels that would be projected far below the ground plane. Finally, in the second stage,  is retrained form scratch without the filtered frames. 

Zhao {\etal} \cite{Zhao:2020} focused on avoiding scale inconsistencies among different frames as produced under SfM self-supervision, specially when they are from sequences whose underlying depth range is too different. In this case, both depth and optical flow estimation CNNs are trained, but not a pose estimation one. Instead, the optical flow between two frames is used to find robust pixel correspondences between them, which are used to compute their relative camera pose, computing the fundamental matrix by the 8-point algorithm, and then performing triangulation between the corresponding pixels of these frames. Overall, a sparse depth pseudo-GT is estimated and used as supervision to train  . However, even robustifying scale consistency among frames, this method still outputs relative depth rather than absolute one. To avoid this problem, Guizilini {\etal} \cite{Guizilini:2020} used sparse LiDAR supervision with SfM self-supervision, relying on depth and pose estimation networks. More recently, Guizilini {\etal} \cite{Guizilini:20203D} relied on camera velocity ({\ie}, the available vehicle velocity) to solve scale ambiguity in a pure SfM self-supervision setting. In particular, a velocity supervision loss trains the pose estimation CNN to learn scale-aware camera translation which, in turn, enables scale-aware depth estimation. 

Overall, this literature shows the relevance of achieving MDE via SfM self-supervision and strategies to account for violation of SfM assumptions, as well as to obtain absolute depth values. Among these strategies, complementing SfM self-supervision with supervision (depth GT) coming from additional sensors such as a LiDAR and/or a stereo rig seems to be the most robust approach to address all the problems at once. However, then, a single camera would not be enough at training time. In this paper, we also complement SfM self-supervision with accurate depth supervision. However, instead of relying on additional sensors, we use virtual-world data.


\subsection{Virtual-world data for MDE}
\label{sec:rw:virtual-world-MDE}

Training  on virtual-world images to later perform on real-world ones, requires to address the virtual-to-real domain gap. Many approaches perform a virtual-to-real image-to-image translation coupled to the training of . This translation usually relies on generative adversarial networks (GANs) \cite{Goodfellow:2014, Choi:2020}, since to train them only unpaired and unlabeled sets of real- and virtual-world images are required.


Zheng {\etal} \cite{Zheng:2018T2Net} proposed Net. In this case, a GAN and  are jointly trained, where the GAN aims at performing virtual-to-real translation while acting as an auto-encoder for real-world images. The translated images are the input for  since they have depth supervision. Additionally, a GAN operating on the encoder weights (features) of  was incorporated during training to force similar depth feature distributions between translated and real-world images. However, this feature-level GAN worsen MDE results in outdoor scenarios. Kundu {\etal} \cite{Kundu:2018AdaDepth} proposed AdaDepth, which trains a common feature space for real- and virtual-world images, {\ie}, a space where it is not possible to distinguish the domain of the input images. Then, depth estimation is trained from this feature space. To achieve this, adversarial losses are used at the feature space level as well as at the estimated depth level. 

Cheng {\etal} \cite{Cheng:2020S3Net} proposed Net, which extends Net with SfM self-supervision. In this case, GAN training involves semantic and photo-metric consistency. Semantic consistency between the virtual-world images and their GAN-translated counterparts is required, which is measured via semantic segmentation (which involves also to jointly train a CNN for this task). Photo-metric consistency is required for consecutive GAN-translated images, which is measured via optical flow. Note that semantic segmentation and optical flow GT is available for virtual-world images.  uses the GAN-translated images as input and is trained end-to-end with the GAN. Then, a further fine-tuning step of  is performed using only the real-world sequence and SfM self-supervision, {\ie}, involving the training of a pose estimation CNN while fine-tuning. During this process, a masking mechanism inspired in \cite{Godard:2019MonoDepth2} is also used to compensate for SfM-adverse scenarios. Contrary to AdaDepth and Net, Net just outputs relative depth. 

Zhao {\etal} \cite{Zhao:2019GASDA} proposed GASDA, which leverages real-world stereo and virtual-world data. In this case, the CycleGAN idea \cite{Zhu:2017} is used to perform DA, which actually involves two GANs, one for virtual-to-real image translation and another for real-to-virtual. Two  are trained coupled to CycleGAN, one intended to process images with real-world appearance (actual real-wold images or GAN-translated from the virtual domain), the other to process images with synthetic appearance (actual virtual-world images or GAN-translated from the real domain). In fact, at testing time, the most accurate depth results are obtained by averaging the output of these two , which also involves to translate the real-world images to the virtual domain by the corresponding GAN. Thanks to the stereo data, left-right depth and geometry consistency losses are also included during training aiming at obtaining a more accurate . PNVR {\etal} \cite{Pnvr:2020SharinGAN} proposed SharinGAN for training a DA GAN coupled to a specific task. One of the selected tasks is MDE with stereo self-supervision, as in \cite{Zhao:2019GASDA}. In this case, real- and virtual-world images are transformed to a new image domain where their appearance discrepancies are minimized to perform MDE from them, {\ie} the GAN and the   are jointly trained end-to-end. SharinGAN outperformed GASDA. However, at testing time, before performing the MDE, the real-world images must be translated by the GAN to the new image domain. Both GASDA and SharinGAN produce absolute scale depth.

\subsection{Relationship of MonoDEVSNet with previous literature} 
In term of operational training conditions, the most similar paper to ours is Net \cite{Cheng:2020S3Net}. However, contrary to Net, our MonoDEVSNet can estimate depth in absolute scale. On the other hand, methods based on pure SfM self-supervision such as \cite{Zhou:2017}, \cite{Yin:2018GeoNet}, \cite{Godard:2019MonoDepth2} (only SfM setting), and \cite{Guizilini:2020semantic}, just report relative depth. In order to compare MonoDEVSNet with them, we have estimated relative depth too. We will see how we outperform these methods, proving the usefulness of leveraging depth supervision from virtual worlds. In fact, regarding relative depth, we also outperform Net. Methods leveraging virtual-world data such as GASDA \cite{Zhao:2019GASDA} and SharinGAN \cite{Pnvr:2020SharinGAN}, rely on real-world stereo data at training time, while we only require monocular sequences. On the other hand, our training framework can be extended to accommodate stereo data if available, although it is not our current focus. Net, GASDA, SharinGAN, Net \cite{Zheng:2018T2Net}, and AdaDepth \cite{Kundu:2018AdaDepth}, leverage ideas from GAN-based DA to reduce the virtual-to-real domain gap, either in image space (Net, GASDA, SharinGAN, Net) or in feature space (AdaDepth). We have analyzed both, image and feature based DA, finding that the later outperforms the former. In particular, by using the Gradient-Reversal-Layer (GRL) DA strategy \cite{Ganin:2015, Ganin:2016}, up to the best of our knowledge, not previously applied to MDE. Currently, we outperform the SfM self-supervision framework in \cite{Guizilini:20203D} thanks to the virtual-world supervision and our GRL DA strategy. However, using vehicle velocity to obtain absolute depth as in \cite{Guizilini:20203D}, is a complementary strategy that could be also incorporated in our framework, although it is not the focus on this paper.



\section{Methods}
\label{sec:method}
In this section, we introduce MonoDEVSNet, which aims at leveraging virtual-world supervision to improve real-world SfM self-supervision. Since we train from both real- and virtual-world data jointly, we describe our supervision and self-supervision losses, the loss for addressing the virtual-to-real domain gap, and the strategy to obtain depth in absolute scale. Our proposal is visually summarized in \Fig{overall_training}.

\subsection{Training data}
\label{ssec:trainingdata}
For training MonoDEVSNet, we assume two sources of  data. On the one hand, we have image sequences acquired by a monocular system on-board a vehicle while driving in real-world traffic. We denote as  one of such frames acquired at time . We denote these data as , where  is the number of frames from the real-world sequences. These frames do not have associated GT. On the other hand, we have analogous sequences but acquired on a virtual world, {\ie}, on-board a vehicle immersed in a traffic simulation. We denote as  one of such virtual-world frames acquired at time . We refer to these data as , where  is the number of frames from the virtual-world sequences. The images in  do have associated GT, since it can be automatically generated. In particular, as it is commonly available in today's simulators, we assume pixelwise depth and semantic class GT. We define  to be this GT; {\ie}, given ,  is its depth GT, and  its semantic class GT. 

\begin{figure}
    \centering
    \includegraphics[clip=True, trim=0 0 0 0,width=\columnwidth]{overall_netarch.png}
    \caption{Training framework for MonoDEVSNet, {\ie}, . We show the involved images, GT, weights, and losses. Red and blue lines are paths of real and virtual-world data, respectively. The discontinuous line is a common path. 
}
    \label{fig:overall_training}
\end{figure}

 
\subsection{MonoDEVSNet architecture: }
\label{ssec:MDECNN}

MonoDEVSNet, {\ie}, our , is composed of three main blocks: a encoding block of weights , a multi-scale pyramidal block, , and a decoding block inspired in \cite{Godard:2019MonoDepth2}, . Therefore, the total set of weights is . Here,  acts as a backbone of features. Moreover, since we aim at evaluating several encoders, the role of the multi-scale pyramid block is to adapt the bottleneck of the chosen encoder to the decoder. At testing time  will process any real-world image  acquired on-board the ego-vehicle, while at training time either  or .

\subsection{Problem formulation}
\label{ssec:formulation}

Training  consists in finding the optimum weight values, , by solving the problem:

\noindent where  is a loss function, and  indicates the use of the virtual-world frames with their GT. As we are going to detail,  relies on three different losses, namely,  and . The loss  focuses on training  based on SfM self-supervision, thus, only relying on real-world data sequences. The SfM self-supervision is achieved with the support of a camera pose estimation task performed by a CNN, , of weights . Thus, we have . The loss  focuses on training  with virtual-world supervision, in particular, using depth and semantic GT from virtual-world sequences. Therefore, we have . Finally,  focuses on creating domain-invariant features  as part of . In particular, we rely on a binary real/virtual domain-classifier CNN, , of weights . Thus, we have .


\subsection{SfM Self-supervised loss: }
\label{ssec:slfloss}
Since we focus on improving MDE by the additional use of virtual-world data, for the SfM self-supervision we leverage from the state-of-the-art proposal in \cite{Godard:2019MonoDepth2}, which we briefly summarize here for the sake of completeness as:

As previously introduced in \cite{Godard:2017}, the term  is a constant weighted loss to force local smoothness on , taking into account the edges of . The term  is the actual SfM-inspired loss. It involves the joint training of the depth estimation weights, , and the relative camera pose estimation weights, . Figure \ref{fig:overall_training} illustrates the CNN, , associated to these weights, which takes as input two consecutive frames, {\eg}, , and outputs the pose transform (rotation and translation), , between them. Then, as can be seen in \Fig{overall_training}, a projection module takes  and the depth estimation , to generate the synthesized frame  which, ideally, should match . In fact, both frames adjacent to  are considered for robustness. Accordingly, the SfM-inspired component of  can be defined as:

where  is a pixelwise conditioned photo-metric error and  its average over the pixels. Obtaining  starts by computing two pixelwise photo-metric error measurements,  and , where , and  is the pixelwise photo-metric error between  and  proposed in \cite{Godard:2017}, {\ie}, based on local structural similarity (SSIM) and pixelwise photo-metric absolute differences between  and . Thus,  applies pixelwise. Then, a pixelwise binary mask, called auto-mask in \cite{Godard:2019MonoDepth2}, is computed as:

\noindent where  denotes the Iverson bracket applied pixelwise. Finally,  is computed as:

\noindent where  stands for pixelwise multiplication. The auto-mask  conditions which pixels of  are considered during the gradient computation of , {\ie},  is computed in the forward pass of CNN training, but it is considered as a constant during back-propagation. As explained in \cite{Godard:2019MonoDepth2}, the aim of  is to remove, during training, the influence of pixels which remain the same between adjacent frames because they are assumed to often indicate SfM violations such as a static camera, objects moving as the camera, or low texture regions. 
Finally, we remark that the support of  is needed at training time, but not at testing time.

\subsection{Supervised loss: }
\label{ssec:suploss}
In this case, since we address an estimation problem and we have accurate GT, we base  on the L1 metric. On the other hand, MDE is specially interesting to determine how far is the ego-vehicle from vehicles, pedestrians, etc. Accordingly, since  includes semantic class GT, we use it to increase the relevance of accurately estimating the depth for such major traffic protagonists. Moreover, since virtual-world depth maps are based on the Z-buffer involved on image rendering, the range of depth values available as GT tend to be over-optimistic even for active sensors such as LiDAR. For instance, there can be depth values larger than  in the Z-buffer. Since we do not aim at estimating depth beyond a reasonable threshold (in ), , to compute  we will also discard pixels  with . For each , both the semantic class relevance and the out-of-range depth values, can be codified as real-valued weights running on  and arranged on a mask, . Thus,  depends on  and . However, contrarily to , we can compute  offline, {\ie}, before starting the training process. Taking all these details into account, we define our supervised loss as:


\subsection{Domain adaptation loss: }
\label{ssec:daloss}

As can be seen in \Fig{overall_training}, we aim at learning depth features, , so that it cannot be distinguished whether they were generated from a real-world input frame (target domain) or a virtual-world one (source domain); in other words, learning a domain invariant . Taking into account that we do not have accurate depth GT in the target domain, while we do have it for the source domain, we need to apply an unsupervised DA technique to train . In addition, as part of , the training of  must result on an accurate . Achieving this accuracy and domain invariance are adversarial goals. Accordingly, we propose to use the Gradient-Reversal-Layer (GRL) idea introduced in \cite{Ganin:2015}, which, up to the best of our knowledge, has not been applied before for DA in the context of MDE. In this approach, the domain invariance of  is measured by a binary target/source domain-classifier CNN, , of weights . In \cite{Ganin:2015}, a logistic loss is proposed to train the domain classifier. In our case, this is set as:

\noindent where we assume that  outputs 1 if  and 0 if  . The GRL has no parameters and connects  with  (see \Fig{overall_training}). Its behavior is exactly as explained in \cite{Ganin:2015}. This means that during forward passes of training, it acts as an identity function, while, during back-propagation, it reverses the gradient vector passing through it. Both the GRL and  are required at training time, but not at testing time.

\subsection{Overall training procedure} 
\label{ssec:traininprocedure}
Algorithm \ref{alg:gradientcomputation} summarizes the steps to compute the needed gradient vectors for mini-batch optimization. In particular, we need the gradients related to MonoDEVSNet weights, , and the weights of the auxiliary tasks, {\ie},  for SfM self-supervision, and  for DA. Regarding gradient computation, we do not need to distinguish  from , so we define . In \Alg{gradientcomputation}, we introduce an equalizing factor between supervised and self-supervised losses, , which aims at avoiding one loss dominating over the other along the training. A priori, we could set a constant factor. However, in practice, we have found that having an adaptive value is more useful. Therefore, inspired by the GradNorm idea \cite{Chen:2018GradNorm}, we use the ratio between the supervised and self-supervised losses. Algorithm \ref{alg:gradientcomputation} also introduces the scaling factor  which, following \cite{Ganin:2015}, controls the trade-off between optimizing  to obtain an accurate  model versus being domain invariant. Finally,  and  indicate whether this loss must be computed only using virtual- or real-world data, respectively.  

\subsection{Absolute depth computation} 
\label{ssec:scaling}

The virtual-world supervised data trains  on absolute depth values, while the real-world SfM self-supervised data trains  on relative depth values. Thanks to the unsupervised DA, the depth features  are trained to be domain invariant. However, according to our experiments, this is not sufficient for  producing accurate absolute depth values at testing time. Fortunately, thanks to the use of virtual-world data, we can still compute a global scaling factor, , so that  is accurate in absolute depth terms. For that, we assume that the sequences in  are acquired with a camera analogous to the one used to acquire the sequences in . Here analogous refers to using the same number of pixels, field of view, frame rate, and mounted on-board in similar heading directions. Note that simulators are flexible enough for setting these camera parameters as needed. Accordingly, we train a  model using only data from  and SfM self-supervision, {\ie} as if we would not have supervision for . Then, we find the median depth value produced by this model on the virtual-world data, . Finally, we set , where  is the median depth value of the GT. This pre-processing step is performed once and the model discarded afterwards. Other works follow a similar idea \cite{Zhou:2017, Godard:2019MonoDepth2, Cheng:2020S3Net, Guizilini:2020semantic, Zhao:2020} to compute absolute depth but relying on LiDAR data as GT reference, while we only rely on virtual-world data.




{
\begin{algorithm}
\caption{Computing the gradients , , ,  for a mini-batch .  refers to back-propagation on  with respect to weights .  is the empty set.}
\label{alg:gradientcomputation}
\end{algorithm}
\begin{minipage}{0.9\columnwidth}
{\centering Forward Passes with \par}
\vspace{-0.3cm}

\\
{\centering Back-propagation for Supervision \& DA\par}
\vspace{-0.3cm}

\\
{\centering Forward Passes with \par}
\vspace{-0.3cm}

\\
{\centering Back-propagation for Self-supervision \& DA\par}
\vspace{-0.3cm}

\\
{\centering Setting the final gradient vectors\par}
\vspace{-0.3cm}

\end{minipage}
}


\sisetup{detect-weight,mode=text}
\renewrobustcmd{\bfseries}{\fontseries{b}\selectfont}
\renewrobustcmd{\boldmath}{}
\newrobustcmd{\B}{\bfseries}
\newrobustcmd{\IL}{\underline}
\addtolength{\tabcolsep}{-4.1pt}


\section{Experimental Results}
\label{sec:experiments}
In this section, we start by defining the datasets and evaluation metrics used in our experiments. After, we provide relevant implementation and training details of MonoDEVSNet. Finally, we present and discuss our quantitative and qualitative results, comparing them with those from previous literature as well as performing an ablative analysis focused on the main components of MonoDEVSNet.

\subsection{Datasets and evaluation metrics}
\label{ssec:datasets}

We use publicly available datasets and metrics which are \emph{de facto} standards in MDE research. In particular, we use KITTI Raw (KR) \cite{Geiger:2013} and Virtual KITTI (VK) \cite{Cabon:2020} as real- and virtual-world sequences, respectively. We follow Zhou {\etal} \cite{Zhou:2017} training-testing split. From the training split we select 12K monocular triplets, {\ie}, samples of the form . The testing split consists of 697 isolated images with LiDAR-based GT, actually introduced by Eigen {\etal} \cite{Eigen:2014}. In addition, for considering the semantic content of the images in the analysis of results, we also use KITTI Stereo 2015 (KS) \cite{Menze:2015} for testing. This dataset consists of 200 isolated images with enhanced depth maps and semantic labels. VK is used only for training, we also use 12K monocular triplets (non-rotated camera subset) with associated depth GT. In this case, the triplets are used to calibrate the global scaling factor  (see \sSect{scaling}), while for actual training supervision only 12K isolated frames are used. As the depth GT of VK ranges up to m, to match the range of KR's LiDAR-based GT, we clip it to m (). VK includes similar weather conditions as KR/KS, and adds situations with fog, overcast, and rain, as well as sunrise and sunset illumination. 

Finally, as is common since \cite{Godard:2017}, we use Make3D dataset \cite{Saxena:2009} for assessing generalization since it is based on photographs at urban and natural areas. Therefore, Make3D shows views and content pretty much different from those on-board a vehicle as KR, KS, and VK. The images come with depth GT acquired by a 3D scanner. There are 534 images with depth GT, organized in a standard split of 400 for training and 134 for testing. We use the latter, since we rely on Make3D only for testing our proposal. 


In order to assess quantitative MDE results, we use the standard metrics introduced by Eigen {\etal} \cite{Eigen:2014}, {\ie}, the average absolute relative error (abs-rel), the average squared relative error (sq-rel), the root mean square error (rms), and the rms log error (rms-log). For these metrics, the lower the better. In addition, the accuracy (termed as ) under a threshold  is also used as metric. In this case, the higher the better. The abs-rel error and the  are percentage measurements, sq-rel and rms are reported in meters, and rms-log is similar (reported in meters) to rms but applied to logarithm depth values. 

These metrics are applied to absolute depth values for MDE models trained with depth supervision coming from either LiDAR  \cite{Eigen:2014, Liu:2016, Roy:2016, Laina:2016, Cao:2017, Fu:2018DORN, Gurram:2018, He:2018, Xu:2018, Yin:2019, Guizilini:2020}, stereo \cite{Saxena:2007, Garg:2016, Godard:2017, Godard:2019MonoDepth2, Pillai:2019}, real-world stereo and virtual-world depth \cite{Zhao:2019GASDA, Pnvr:2020SharinGAN}, or stereo and LiDAR \cite{Kuznietsov:2017, He:2018wearable}. 
However, MDE models trained on pure SfM self-supervision can only estimate depth in relative terms, {\ie}, up to scale. Moreover, the scale factor varies from image to image, a problem known as scale inconsistency. In this case, before computing the above metrics, it is applied a per-image correction factor computed at testing time \cite{Zhou:2017, Yin:2018GeoNet, Zhao:2020, Godard:2019MonoDepth2, Guizilini:2020semantic, Cheng:2020S3Net}. In particular, given a test image  with GT and estimated depth  and , respectively, the common practice consists of computing a scale  as the ratio , and then compare  with . On the other hand, SfM self-supervision with the help of additional information can train models able to produce absolute scale in testing time. For instance, \cite{Guizilini:20203D} uses the ego-vehicle speed and, in fact, virtual-world supervision can help too \cite{Zheng:2018T2Net, Kundu:2018AdaDepth}. The latter approach is the one followed in this paper, especially thanks to the procedure presented in \sSect{scaling}. Therefore,  will be evaluated in relative scale terms, and  in absolute terms. Please, note that our  scaling factor is constant for all the evaluated images and computed at training time. In the following, when presenting quantitative results, we will make clear if they are in relative or absolute terms.

\subsection{Implementation details}
\label{ssec:implementation}

We start by selecting the actual CNN layers to implement . Since we leverage the SfM self-supervision idea from \cite{Godard:2019MonoDepth2}, a straightforward implementation would be to use its ResNet-based architecture as it is. However, the High-Resolution Network (HRNet) architecture \cite{Wang:2020HrNet}, exhibits better accuracy in visual tasks such as semantic segmentation and object detection, suggesting that it can be a better backbone than ResNet. Thus, we decided to start our experiments by comparing ResNet and HRNet backbones using the SfM self-supervision framework provided in \cite{Godard:2019MonoDepth2}. In particular, we assess different ResNet/HRNet architectures for , while using the proposal in \cite{Godard:2019MonoDepth2} for . Then, when using ResNet we have , while for HRNet  consists of pyramidal layers adapting the  and  CNN architectures under test. For these experiments, we rely on KR. Table\ref{tab:compare_netarch} shows the accuracy (in relative scale terms) of the tested variants and their number of weights. We see how HRNet outperforms ResNet, being HRNet-W48 the best. Thus, for our following experiments, we will rely on HRNet-W48 although being the heaviest. We show the corresponding pyramidal architecture of  in \Fig{mde_net_arch}. It is composed of five blocks (), where each block is a pipeline of three consecutive layers consisting of convolution, batch normalization, and ReLU. As a deep learning framework, we use PyTorch 1.5v \cite{Paszke:2019pytorch}.

\begin{table}
\centering
\caption{Comparing ResNet and HRNet as backbone for our  model, training only on SfM self-supervision (relative scale) using the framework in \cite{Godard:2019MonoDepth2}. {MW} column stands for millions of  weights to be learnt. The  columns, , refer to the  in the usual  accuracy metrics. Here and in all the tables of \Sect{experiments}, bold stands for {\bf best}, and underline for \IL{second-best}.}
\label{tab:compare_netarch}
\begin{tabular}{|c||*{10}{c|}}\hline
\makebox[5em]{ Backb.}&\makebox[2em]{MW} &\makebox[2em]{abs-rel}&\makebox[2em]{sq-rel}&\makebox[2em]{rms}&\makebox[3em]{rms-log}&\makebox[3em]{}&\makebox[3em]{}&\makebox[3em]{}\\\hline \hline
ResNet-18 & 11.6  & 0.115 &  0.882 &  4.701 &  0.190 &  0.879 &  0.961 &  0.982 \\
ResNet-50 & 25.5  & 0.110 &  0.831 &  4.642 &  0.187 &  0.883 &  0.962 &  0.982 \\
ResNet-101 & 44.5 & 0.110 & 0.809 & 4.712 & 0.187 & 0.878 & 0.960 & 0.982 \\
ResNet-152 & 60.2  & 0.107 & \IL{0.800} & \IL{4.629} & 0.184 & 0.885 & 0.962 & 0.982 \\ \hline
HRNet-W18 & 9.5  & \IL{0.107} & 0.846 & 4.671 & \IL{0.184} & \IL{0.887} & \IL{0.962} & \IL{0.982}  \\ 
HRNet-W32 & 29.3  & 0.107 & 0.881 & 4.794 & 0.187 & 0.886 & 0.961 & 0.981  \\
HRNet-W48 & 65.3  & \B 0.105 & \B 0.791 & \B 4.590 & \B 0.182 & \B 0.888 & \B 0.963 & \B 0.982  \\\hline
\end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{test_time_detailed.png}
    \caption{Pyramidal architecture of .}
    \label{fig:mde_net_arch}
\end{figure}

In order to train the camera pose estimation network, , we follow \cite{Godard:2019MonoDepth2} but
using ResNet-50 instead of ResNet-18 since the former is more accurate. Four convolutional layers are used to convert the ResNet-50 bottleneck features to the 6-DoF relative pose vector (3D translation and rotation). For training the classification block of , {\ie}, , we use a standard classification pipeline based on convolutions, ReLU and fully connected layers. Finally, we remark that these networks are not required at testing time.


\subsection{Training details}
\label{ssec:training}

The input images are processed (at training and testing time) at a resolution of  ({}), where LANCZOS interpolation is performed from the  original resolution. As optimizer, we use ADAM with learning rate set as , and the rest of its hyper-parameters remain with default values. The weights  are initialized from available ImageNet pre-training, , and  are randomly initialized with Kaiming weights, while the ResNet-50 part of  is also initialized with ImageNet and the rest (convolutional layers to output the pose vector) following Kaiming. The mini-batch size is of 16 images, 50\%/50\% from real/virtual domains. To minimize over-fitting, we apply standard data augmentation such as horizontal flip, a 50\% chance of random brightness, contrast, saturation, and hue jitter with ranges of , , , and , respectively. Remaining hyper-parameters were set as  in \Eq{sf-loss},  in \Alg{gradientcomputation}, and in \Eq{sup-loss} our mask  is set to have values of  for traffic participants (vehicles, pedestrians, {\etc}),  for static infrastructure (buildings, road, vegetation, {\etc}), and  for the sky and pixels with depth over  (here m).



\subsection{Results and discussion}
\label{ssec:results}

\subsubsection{Relative depth assessment}
We start by assessing MDE in relative terms. Table \ref{tab:SOTA_KITTI_eigen_relative} presents MonoDEVSNet results (Ours) and those from previous works based on SfM self-supervision. From this table we can draw several observations. Regarding DA, MonoDEVSNet (VK\_v1) outperforms Net (VK\_v1) in all metrics. The new version of VK (VK\_v2) allows us to obtain even better results. MonoDEVSNet with virtual-world supervision outperforms the version with only SfM self-supervision (best result in \Tab{compare_netarch}) in all metrics, no matter the VK version we use. Overall, MonoDEVSNet outperforms most previous methods, being on pair with  \cite{Guizilini:2020semantic}.

\begin{table}
\centering
\caption{\emph{\underline{Relative depth}} results up to m on the (KR) Eigen {\etal} \cite{Eigen:2014} testing split. These methods rely on SfM self-supervision. In addition, methods in gray use DA supported by VK.  MonoDepth2 is based only on SfM self-supervision.}
\label{tab:SOTA_KITTI_eigen_relative} 
\begin{tabular}{|l||*{7}{c|}}\hline
Method &\makebox[2em]{abs-rel}&\makebox[2em]{sq-rel}&\makebox[2em]{rms}&\makebox[3em]{rms-log}&\makebox[2.5em]{}&\makebox[2.5em]{}&\makebox[2.5em]{}\\\hline \hline
\cite{Zhou:2017} (Zhou {\etal})            & 0.183      & 1.595      & 6.709      & 0.270      & 0.734      & 0.902      & 0.959 \\ \hline
\cite{Yin:2018GeoNet} GeoNet               & 0.149      & 1.060      & 5.567      & 0.226      & 0.796      & 0.935      & 0.975 \\ \hline
\cite{Godard:2019MonoDepth2}  MonoDepth2   
                                           & 0.115      & 0.903      & 4.863      & 0.193      & 0.877      & 0.959      & 0.981 \\ \hline
\cite{Zhao:2020} (Zhao {\etal})            & 0.113      & 0.704      & 4.581      & 0.184      & 0.871      & 0.961      & \B 0.984 \\ \hline
\cite{Guizilini:2020semantic} (Guizilini {\etal}) 
                                           & \B 0.102    & \IL{0.698} & \IL{4.381} & \B 0.178   & \B 0.896   & 0.964      & \B 0.984 \\ \hline
\rowcolor{Gray}
\cite{Cheng:2020S3Net} Net (VK\_v1)   & 0.124       & 0.826      & 4.981      & 0.200      & 0.846      & 0.955      & 0.982   \\ \hline
\rowcolor{Gray}
Ours (VK\_v1)                              & \IL{0.105}  &  0.753     & 4.389      & \IL{0.179} & 0.890      & \IL{0.965} & \IL{0.983} \\ \hline
\rowcolor{Gray}
Ours (VK\_v2)                              & \B 0.102    & \B 0.685   & \B 4.303   & \B 0.178   & \IL{0.894} & \B 0.966   & \B 0.984 \\ \hline
\end{tabular}
\end{table}


\subsubsection{Absolute depth assessment}
While assessing depth in relative terms is a reasonable option to compare methods purely based on SfM self-supervision, the most relevant evaluation is in terms of absolute depth. These are presented in \Tab{SOTA_KITTI_eigen_absolute}. The first (top) block of this table shows results based on depth supervision from LiDAR, thus, a priori they can be thought of as upper-bounds for methods based on self-supervision. The second block shows methods that only use virtual-world supervision. The third and fourth (bottom) blocks show results based on stereo and SfM self-supervision, respectively. Methods in gray use DA supported by VK. We can draw several observations from this table. MonoDEVSNet (Ours) is the best performing among those leveraging supervision from VK\_v1 and, consistently with the results on relative depth, by using VK\_v2 we improve MonoDEVSNet results. In fact, MonoDEVSNet based on VK\_v2 outperforms all self-supervised methods, including those using stereo rigs instead of monocular systems. We are not yet able to reach the performance of the best methods supervised with LiDAR data. However, it is clear that our proposal is able to successfully combine real-world SfM self-supervision and virtual-world supervision. Thus, we think it is worth to keep this line of research until reaching the LiDAR-based upper-bounds.

\begin{table}[!t]
\centering
\caption{\emph{\underline{Absolute depth}} results up to m on the (KR) Eigen {\etal} \cite{Eigen:2014} testing split. We divide the results into four blocks. From top to bottom, the blocks refer to: methods based on LiDAR supervision, only virtual-world supervision, stereo self-supervision, SfM self-supervision. In these blocks, we remark best and second-best results per block. Methods in gray use DA supported by VK. We remark some additional comments:  in addition to LiDAR supervision, it also uses stereo self-supervision;  it uses stereo and SfM self-supervision;  in this case, the MDE network is pre-trained on Cityscapes dataset \cite{Cordts:2016} and then fine-tuned on KITTI.}
\label{tab:SOTA_KITTI_eigen_absolute} 
\begin{tabular}{|l||*{7}{c|}}\hline
Method &\makebox[2em]{abs-rel}&\makebox[2em]{sq-rel}&\makebox[2em]{rms}&\makebox[3em]{rms-log}&\makebox[2.2em]{}&\makebox[2.2em]{}&\makebox[2.2em]{}\\\hline \hline
\cite{Eigen:2014} (Eigen {\etal})           & 0.203      & 1.548      & 6.307      & 0.282      & 0.702      & 0.890      & 0.890 \\ \hline
\cite{Liu:2016} (Liu {\etal})	            & 0.217	     & 1.841      & 6.986	   & 0.289	    & 0.647	     & 0.882      & 0.961 \\ \hline
\cite{Cao:2017} (Cao {\etal})               & 0.115	     & N/A        & 4.712      & 0.198	    & 0.887      & 0.963      & 0.982 \\ \hline
\cite{Kuznietsov:2017} (Kuzni. {\etal}) & 0.113	     & 0.741	  & 4.621	   & 0.189	    & 0.862	     & 0.960	  & 0.986 \\ \hline
\cite{Xu:2018} (Xu {\etal})                 & 0.122      & 0.897      & 4.677      & N/A        & 0.818      & 0.954      & 0.985 \\ \hline
\cite{Gurram:2018} (Gurram {\etal})         & 0.100      & \IL{0.601} & 4.298      & 0.174      & 0.874      & 0.966      & \IL{0.989} \\ \hline
\cite{Fu:2018DORN} DORN                     & \IL{0.098} & \B 0.582   & \IL{3.666} & \IL{0.160} & \IL{0.899} & \IL{0.967} & 0.986  \\ \hline
\cite{Yin:2019} VNL                       & \B 0.072   & N/A        & \B 3.258   & \B 0.117   & \B 0.938   & \B 0.990   & \B 0.998 \\ \hline
\Xhline{4\arrayrulewidth}
\rowcolor{Gray}
\cite{Kundu:2018AdaDepth} AdaDepth (VK\_v1) & \B 0.167   & \B 1.257   & \B 5.578   & \B 0.237   & \B 0.771   & \B 0.922   & \B 0.971 \\ \hline
\rowcolor{Gray}
\cite{Zheng:2018T2Net} Net (VK\_v1)       & \IL{0.174} & \IL{1.410} & \IL{6.046} & \IL{0.253} & \IL{0.754} & \IL{0.916} & \IL{0.966} \\ \hline
\Xhline{4\arrayrulewidth}
\cite{Garg:2016} (Garg {\etal}) 	        & 0.169	     & 1.512	  & 5.763	   & 0.236	    & 0.836	     & 0.935	  & 0.968 \\ \hline
\cite{Pillai:2019} SuperDepth               & 0.112      & 0.875      & \IL{4.958} & \IL{0.207} & 0.852      & 0.947      & 0.977 \\ \hline
\cite{Godard:2019MonoDepth2} MonoDepth2     & \IL{0.109} & \IL{0.873} & 4.960      & 0.209      &\IL{0.864}  & \IL{0.948} & 0.975 \\ \hline
\cite{Godard:2019MonoDepth2} MonoDepth2 & \B 0.106   & \B 0.806   & \B 4.630   & \B 0.193   & \B 0.876   & \B 0.958   & \B 0.980 \\ \hline
\rowcolor{Gray}
\cite{Zhao:2019GASDA} GASDA (VK\_v1)        & 0.120      & 1.022      & 5.162      & 0.215      & 0.848      & 0.944      & 0.974 \\ \hline
\rowcolor{Gray}
\cite{Pnvr:2020SharinGAN} SharinGAN (VK\_v1)& 0.116      & 0.939      & 5.068      & 0.203      & 0.850      & \IL{0.948} & \IL{0.978} \\ \hline
\Xhline{4\arrayrulewidth}
\cite{Guizilini:20203D} PackNet-SfM         & 0.111      & 0.829      & 4.788      & 0.199      & 0.864      & 0.954      & 0.980 \\ \hline
\cite{Guizilini:20203D} PackNet-SfM     & \IL{0.108} & 0.803      & 4.642      & 0.195      & \IL{0.875} & 0.958      & 0.980 \\ \hline
\rowcolor{Gray}
Ours (VK\_v1)                               & \IL{0.108} & \IL{0.775} & \IL{4.464} & \IL{0.188} & \IL{0.875} & \IL{0.961} & \IL{0.982} \\ \hline 
\rowcolor{Gray}
Ours (VK\_v2)                               & \B 0.104   & \B 0.721   & \B 4.396   & \B 0.185   & \B 0.880   & \B 0.962   & \B 0.983 \\ \hline
\Xhline{4\arrayrulewidth}
\end{tabular}
\end{table}
\begin{table}[!h]
\centering
\caption{\emph{\underline{Absolute depth}} ablative results of MonoDEVSNet (VK\_v2) up to m on the (KR) Eigen {\etal} \cite{Eigen:2014} testing split. Rows 1-6 show the progressive use of the components of our proposal (each row adds a new component).  refers to mini-batches of 50\% real-world samples and 50\% or virtual-world ones; not using  (rows 1-2) means that we alternate mini-batches of pure real- or virtual-world samples. Row 7 corresponds to a simplification of the SfM self-supervised loss.  (rows 8-9) refers to a GAN-based DA approach. LB (lower bound, row 10) indicates the use of only virtual-world data. UB (upper bound, row 12) indicates the use of KITTI LiDAR-based supervision instead of virtual-world data. Rows 11 and 13 show the difference of our best model (All) with respect to LB and UB, respectively.  means that All is  units better, while  means that it is  units worse. All/W18 (row 14) and All/W32 (row 15) refer to using the All configuration by relying on HRNet-W18 and HRNet-W32, respectively.
} 
\label{tab:Ablative_KITTI_eigen_absolute} 
\begin{tabular}{|l||*{7}{c|}}\hline
Configuration &\makebox[2em]{abs-rel}&\makebox[2em]{sq-rel}&\makebox[2em]{rms}&\makebox[2.8em]{rms-log}&\makebox[2em]{}&\makebox[2em]{}&\makebox[2em]{}\\\hline \hline
1.~           & 0.368    & 2.601    & 8.025    & 0.514    & 0.080    & 0.478    & 0.883 \\ \hline
2.~                                     & 0.140    & 0.876    & 4.915    & 0.217    & 0.828    & 0.950    & 0.980 \\ \hline
3.~                                               & 0.128    & 0.880    & 4.618    & 0.198    & 0.844    & 0.957    & 0.982 \\ \hline
4.~                                               & 0.110    & 0.724    & 4.450    & 0.187    & 0.873    & 0.960    & 0.983 \\ \hline
5.~                                           & 0.106    & 0.716    & 4.441    & 0.188    & 0.876    & 0.962    & 0.982 \\ \hline
6.~ (\textbf{All})                                 & \B 0.104 & \B 0.721 & \B 4.396 & \B 0.185 & \B 0.880 & \B 0.962 & \B 0.983 \\ \hline
7.~Simplified                                   & 0.105    & 0.736    &	4.471    & 0.190    & 0.875    & 0.960    &	0.981   \\ \hline
\Xhline{4\arrayrulewidth}
8.~All                                & 0.119    & 0.809    & 4.654    & 0.196    & 0.857    & 0.958    & 0.982 \\ \hline
9.~All                                           & 0.106    & 0.748    & 4.503    & 0.191    & 0.873    & 0.959    & 0.981 \\ \hline
\Xhline{4\arrayrulewidth}
10.~LB                                                       & 0.165    & 1.280    & 5.628    & 0.248    & 0.777    & 0.916    & 0.965   \\ \hline
11.~{{All} {\vs} {LB}}                & {\itn{0.061}}   & {\itn{0.559}}   & {\itn{1.232}}   & {\itn{0.063}}   & {\itn{0.103}}    & {\itn{0.046}}    & {\itn{0.018}}   \\ \hline
12.~UB                                                      & 0.088    & 0.583    & 3.978    & 0.164    & 0.906    & 0.970    & 0.986   \\ \hline
13.~{{All} {\vs} {UB}}                & {\itn{0.016}}   & {\itn{0.138}}   & {\itn{0.418}}   & {\itn{0.021}}   & {\itn{0.026}}    & {\itn{0.008}}    & {\itn{0.003}}   \\ \hline
\Xhline{4\arrayrulewidth}
14.~All/W18                                                 & 0.109    & 0.773    & 4.524    & 0.190    & 0.871    & 0.960    & 0.982   \\ \hline
15.~All/W32                                                 & 0.107    & 0.754    & 4.510    & 0.188    & 0.875    & 0.960    & 0.982   \\ \hline
\Xhline{4\arrayrulewidth}
\end{tabular}
\end{table}

\subsubsection{Ablative analysis of MonoDEVSNet}
It is also worth to analyze the contribution of the main components of our proposal. In rows 1-6 of \Tab{Ablative_KITTI_eigen_absolute}, we add one component at a time showing performance for absolute depth. The 1st row corresponds to using the real-world data with SfM self-supervision and the virtual-world images with only depth supervision, {\ie}, without using neither semantic supervision (), nor gradient equalization (), nor domain adaptation (), nor mixed mini-batches (), nor the global scaling factor (). By comparing 1st and 2nd rows ({\ie}, w/o  and w/ , resp.), we can see how relevant is obtaining a good global scaling factor to output absolute depth. In fact, adding  to the virtual-world depth supervision shows the higher improvement among all the components of our proposal. Then, using mixed mini-batches of real- and virtual-world data improves the performance over alternating mini-batches of only either real- or virtual-world data. This can be seen by comparing 2nd and 3rd rows ({\ie}, w/o  and w/ , resp.). If we alternate the domains, the optimization of a mini-batch is dominated by self-supervision (real-world data), and the optimization of the next mini-batch is dominated by supervision (virtual-world data). Thus, there is not an actual joint optimization of SfM self-supervised and supervised losses, which turns to be relevant. Yet, as can be seen in 4th row, when we add the DA component () we improve further the depth estimation results. As can bee seen in 5th row, adding the equalization () between gradients coming from supervision and self-supervision also improves the depth estimation results. Finally, adding the virtual-world mask () leads to the best performance in 6th row. Overall, this analysis shows how all the considered components are relevant in our proposal. We also remark that these components are needed only to train , but only  and  are required at testing time. Additionally, we have assessed the effect of simplifying the SfM self-supervised loss that we leverage from \cite{Godard:2019MonoDepth2}, here summarized in \sSect{slfloss}. In particular, we neither use the auto-mask (), nor the multi-scale depth loss, and we replaced the minimum re-projection loss by the usual average re-projection loss ({\ie}, we re-define  in \sSect{slfloss}). Results are shown in the 7th row. The metrics show worse values than in 6th row (All), but still outperforming or being on pair with PackNet-SfM and the stereo self-supervised methods of \Tab{SOTA_KITTI_eigen_absolute}. 


In addition to this ablative analysis, we did additional experiments changing the DA mechanism. In particular, instead of taking direct real- and virtual-world images as input to train , a GAN-based CNN, , processes these images to create a common image space in which (hopefully) it is not possible to distinguish the domain. In short, we train an overall CNN , where  can come from either the real or the virtual domain, and  are the weights of . These weights are jointly trained with all the rest () to optimize depth estimation and minimize the possibility of discriminating the original domain of a sample . Table \ref{tab:Ablative_KITTI_eigen_absolute} shows results using such a GAN when removing  (8th row) and when keeping it (9th row). As we can see, this approach does not improve performance. Moreover, it turns out in a more complex training and  would be required at testing time. Thus, we discarded it.

We also assessed the improvement of our proposal with respect a lower-bound model (LB) trained on virtual-world images and their depth GT (), but neither using real-world data (), nor DA (), nor the mask (). Results are shown in 10th row of \Tab{Ablative_KITTI_eigen_absolute}, and we explicitly show the improvement of our proposal over such LB in 11th row. Likewise, we have trained an upper-bound model (UB) replacing VK data by KR data with LiDAR-based supervision, so that DA is not required. Results are shown in 12th row, and the distance of our model to this UB is explicitly shown in 13th row. Comparing 11th and 13th rows we can see how we are clearly closer to the UB than to the LB. 

Finally, we have done experiments using HRNet-W18 and HRNet-W32. The results are shown in 14th and 15th rows of \Tab{Ablative_KITTI_eigen_absolute}, respectively. Indeed, as it happens with the results on relative depth (\Tab{compare_netarch}), HRNet-W48 outperforms these more lightweight versions of HRNet. However, by using HRNet-W18 and HRNet-W32 we still outperform or are on pair with the state-of-the-art self-supervised methods shown in \Tab{SOTA_KITTI_eigen_absolute}, {\ie}, those based on stereo self-supervision and PackNet-SfM.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{kitti_eigen_r7_asitis_2.png}
    \caption{Qualitative results on the (KR) Eigen {\etal} testing slit \cite{Eigen:2014}. From top  to bottom: input images, their LiDAR-based depth GT (interpolated for visualization using \cite{Premebida:2014}), DORN, SharinGAN, PackNet-SfM, ours VK\_v1, and ours VK\_v2.}
    \label{fig:kitti_eigen_qualitative}
\end{figure*}

\subsubsection{Qualitative results}
Figure \ref{fig:kitti_eigen_qualitative} presents qualitatively results relying on the depth color map commonly used in the MDE literature. We show results for representative methods in \Tab{SOTA_KITTI_eigen_absolute}, namely, DORN (LiDAR supervision), SharinGAN (stereo self-supervision and virtual-world supervision), PackNet-SfM (SfM self-supervision and ego-vehicle speed supervision), and MonoDEVSNet (Ours) using VK\_v1 and VK\_v2 (SfM self-supervision and virtual-world supervision). We also show the corresponding LiDAR-based GT. This GT shows that for LiDAR configurations such as the one used to acquire KITTI dataset, detecting some close vehicles may be problematic since only a few LiDAR points capture their presence. Despite being trained on LiDAR supervision, DORN provides more accurate depth information in these corner cases than the raw LiDAR, which is an example of the relevance of MDE in general. However, DORN shows worse results in these corner cases than the rest (SharinGAN/PackNet-SfM/Ours), even being more accurate in terms of MDE metrics, which focus on global assessment. SharinGAN has more difficulties than PackNet-SfM and our proposal for providing sharp borders in vertical objects/infra-structure ({\eg}, vehicles, pedestrians, traffic signs, trees). An interesting point to highlight is also the qualitative difference that we observe on our results depending on the use of VK version. In VK\_v1 data, vehicle windows appear as transparent to depth, like in many cases happens with LiDAR data, while in VK\_v2 they appear as solid. This is translated to the MDE results as we can observe comparing the two bottom rows of \Fig{kitti_eigen_qualitative}. Technically, we think the qualitative results of VK\_v2 make more sense since the windows are there at the given depth. However, what we would like to highlight is that we can select one option or another thanks to the use of virtual-world data. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{abs_rel_plot_binsize5.png}
    \caption{Abs-rel error as a function of depth, computed on (KR) Eigen {\etal} testing split \cite{Eigen:2014}. The height of each bar indicates the percentage of pixels whose depth GT falls within the depth range covered by the bar. We indicate five values as reference. We compare PackNet-SfM and MonoDEVSNet (Ours). }
    \label{fig:depth_error_as_a_function_of_depth}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{kitti_2015_ghost_cars.png}
    \caption{Qualitative results on KS data. From left to right: input images, PackNet-SfM, MonoDEVSNet  (Ours).}
    \label{fig:kitti_2015_ghost_cars_qualitative}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{per_class_analysis.png}
    \caption{Per-class abs-rel results of PackNet-SfM and MonoDEVSNet (Ours) on KS data. This per-class score is the average of the abs-rel metric only considering the pixels of each class at a time. We also indicate the percentage of pixels of each class over all the pixels from the images in KS.}
    \label{fig:per_class_analysis}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[clip=true, trim=10 0 2 10, width=\columnwidth]{pcd.png}
    \caption{Point cloud representation on KR Eigen test split \cite{Eigen:2014} and KS data from left to right. From top to bottom: input images, MonoDEVSNet textured point cloud.}
    \label{fig:pcd_representation}
\end{figure}

\subsubsection{Additional insights}
In terms of qualitative results we think the best performing and most similar approaches are PackNet-SfM and MonoDEVSNet, both relying only on real-world monocular systems. Thus, we perform a deeper comparison of them. First, following the analysis introduced in the PackNet-SfM article \cite{Guizilini:20203D}, \Fig{depth_error_as_a_function_of_depth} plots the abs-rel metric of both methods as a function of depth. Results are similar up to m, then our proposal clearly outperforms PackNet-SfM up to m, where both methods start to perform similarly and in the last part of the range, up to m, PackNet-SfM outperforms our proposal. How these differences translate to the abs-rel global metric depends on the number of pixels falling in each distance range, which we show as an histogram in the same plot. We see how for the KR testing set most of the pixels fall in the m depth range, where both methods perform more similarly. Second, we provide further comparative insights by using KS data since it has associated per-class semantic GT, which we are going to use for evaluation purposes. Note that, although KS is a different data split than the one used in the experiments shown so far (KR), still is KITTI data; thus, we are not yet facing experiments about generalization. Figure \ref{fig:kitti_2015_ghost_cars_qualitative} compares qualitative results of PackNet-SfM {\vs} MonoDEVSNet. We can see how PackNet-SfM misses some vehicles that our proposal does not. We believe that these vehicles may be moving at a similar speed w.r.t the ego-vehicle, which may be problematic for pure SfM-based approaches and we hypothesize that virtual-world supervision can help to avoid this problem. Figure \ref{fig:per_class_analysis} shows the corresponding abs-rel metric per-class, focusing on the most relevant classes for driving. Note how the main differences between PackNet-SfM and MonoDEVSNet are observed on vehicles, especially on cars. 

Additional qualitative results are added in \Fig{pcd_representation}, where we can see how original images from KR and KS can be rendered as a textured point cloud. In particular, the viewpoint of these renders can change with respect to the original images thanks to the absolute depth values obtained with MonoDEVSNet.

\subsubsection{Generalization results}
As done in the previous literature using VK to support MDE \cite{Kundu:2018AdaDepth, Zheng:2018T2Net, Zhao:2019GASDA, Pnvr:2020SharinGAN}, we assess generalization on Make3D dataset. As in this literature, we follow the standard data conditioning (cropping and resizing) for models trained on KR, as well as the standard protocol introduced in \cite{Godard:2017} to compute MDE evaluation metrics ({\eg} only depth below m is considered). Table \ref{tab:sota_make3D} presents the quantitative results usually reported for Make3D, and ours. Note how, in generalization terms, our method also outperforms the rest. Moreover, \Fig{make3d_qualitative} shows how our proposal captures the depth structure even better than the depth GT, which is build from  depth maps acquired by a 3D scanner.

\begin{table}
\centering
\caption{\emph{\underline{Absolute depth}} results on Make3D testing set. All the shown methods use Make3D only for testing (generalization), except () which fine-tunes on Make3D training set too.}
\label{tab:sota_make3D}
\begin{tabular}{|l|*{4}{c|}}\hline
Method &\makebox[3.5em]{abs-rel} &\makebox[3.5em]{sq-rel} &\makebox[3.5em]{rms} \\ \hline \hline
\cite{Zheng:2018T2Net} Net (VK\_v1)             & 0.508      & 6.589      & 8.935 \\ \hline
\cite{Kundu:2018AdaDepth} AdaDepth-S (VK\_v1) & 0.452      & 5.71       & 9.559 \\ \hline
\cite{Zhao:2019GASDA} GASDA (VK\_v1)              & 0.403      & 6.709      & 10.424 \\ \hline
\cite{Pnvr:2020SharinGAN} SharinGAN (VK\_v1)      & \IL{0.377} & 4.900      & 8.388 \\ \hline
Ours (VK\_v1)                                     & 0.381      & \IL{3.997} & \B 7.949 \\ \hline 
Ours (VK\_v2)                                     & \B 0.377   & \B 3.782   & \IL{8.011} \\ \hline 
\end{tabular}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{make3d_figure.png}
    \caption{Qualitative results of MonoDEVSNet on Make3D. From left to right: input images, depth GT, MonoDEVSNet. }
    \label{fig:make3d_qualitative}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
For on-board perception, we have addressed monocular depth estimation by virtual-world supervision (MonoDEVS) and real-world SfM-inspired self-supervision; the former compensating for the inherent limitations of the latter. This challenging setting allows to rely on a monocular system not only at testing time, but also at training time; a cheap and scalable approach. We have designed a CNN, MonoDEVSNet, which seamlessly trains on real- and virtual-world data, exploiting semantic and depth supervision from the virtual-world data, and addressing the virtual-to-real domain gap by a relatively simple approach which does not add computational complexity in testing time. We have performed a comprehensive set of experiments assessing quantitative results in terms of relative and absolute depth, generalization, and we show the relevance of the components involved on MonoDEVSNet training. Our proposal yields state-of-the-art results within the SfM-based setting, even outperforming stereo-based self-supervised approaches. Qualitative results also confirm that MonoDEVSNet properly captures the depth structure of the images. As a result, we show the usefulness of leveraging virtual-world supervision to ultimately reach the upper-bound performance of methods based on LiDAR supervision. Therefore, our next steps will focus on analyzing the detailed differences between LiDAR-based supervision methods and MonoDEVSNet to find better ways to benefit from virtual-world supervision.  


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi









\bibliographystyle{IEEEtran}
\bibliography{main}


\end{document}
