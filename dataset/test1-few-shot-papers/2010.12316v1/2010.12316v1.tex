\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{booktabs} 
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{cite}
\usepackage{caption}
\usepackage{subcaption}
\newcommand\todo[1]{\textcolor{red}{#1}}

\begin{document}
\appendix
\newpage
\section*{Appendix}

\section{MixMatch \& FixMatch -- algorithm details} \label{app:algorithms}

The foundation of both MixMatch and FixMatch is consistency regularization -- the idea that augmentations of the same data point should yield the same label. In this way, the model regularizes itself based on its predictions.

Let  be the batch of labeled examples.  denotes the set of \emph{weak} augmentations and  -- \emph{strong} augmentations.  is the prediction of backbone classifier, parametrized by .  denotes categorical cross-entropy and  is unsupervised loss weight.

\textbf{MixMatch} employs only weak augmentations  and MixUp \cite{zhang2017mixup}. Let  be the unlabeled data batch. The model outputs of  random weak augmentations  of the same unlabelled sample are treated as soft pseudo-labels . These soft pseudo-labels are averaged and sharpened with the temperature  for each image in  to yield a pseudo-label for that image. Then, images from both randomly augmented  and  are concatenated and shuffled, resulting in set . Afterwards, samples in  and  are weakly augmented and linearly interpolated with samples from . This results in  and  -- "mixed-up" versions of augmented labelled and  unlabelled batches. Coefficients of MixUp are sampled from  distribution. The final loss is the sum of categorical cross-entropy for images from  (supervised part) and Brier score for  images (unsupervised part): 

MixMatch linearly ramps up  from 0 to its maximum after each batch to reduce the influence of unsupervised part during early stages of training.

\textbf{FixMatch} is a more simplified method. Unlabeled data batch  is now -times bigger. Given the model's prediction  for a weakly augmented unlabelled sample , method yields hard pseudo-labels  and . Afterwards, the model predicts labels for both a batch of weakly augmented labelled images and a batch of strongly augmented unlabelled images. Only the confident predictions for unlabelled samples are used in the final unsupervised part of the loss. They are filtered with the threshold . The loss of FixMatch is then the sum of two categorical cross-entropies for labelled and unlabelled images:

       
As we use  for filtering confident pseudo-labels, we do not need the linear ramp-up for .

\section{Experiments}
\subsection{Transfer learning} \label{app:transfer-learning}
We took a version of Wide ResNet-50-2 pre-trained on ImageNet from PyTorch.\footnote{\href{https://pytorch.org/hub/pytorch_vision_wide_resnet/}{https://pytorch.org/hub/pytorch\_vision\_wide\_resnet/}.} Transfer learning was fine-tuned for every individual , as it did not require much computational budget: 
\begin{itemize}
    \item learning rate 
    \item optimizer weight decay 
    \item layers freezing Fine-tuning, \\ Feature extraction (see Section \ref{subsec:transfer-learning})
\end{itemize} 

Further hyperparameters are kept fixed, namely  we use Adam optimizer \cite{kingma2014adam}, , number of epochs = 50. Additionally, early stopping with the patience of 25 epochs was applied to avoid overfitting.

\subsection{MixMatch \& FixMatch} \label{app:SSL}

Hyperparameter fine-tuning for both SSL methods was two-fold: firstly, we fine-tuned more general parameters on 200 labelled samples () with respect to the validation loss (see Table \ref{tab:grid_search}). Secondly, for each specific , we tuned subset-size-dependent parameters. 

The labeled batch size was  for both algorithms. Additionally, we fix  for FixMatch. We omit using cosine learning rate decay.  

\begin{table}
    \centering
    \setlength{\tabcolsep}{2.5pt}
    \renewcommand{\arraystretch}{1}
    \begin{tabular}{l|c|c}
        \toprule
         Hyperparameter &  MixMatch & FixMatch\\ \midrule
         Learning rate &  &  \\
         Optimizer &  & Adam,  \\
         Number of epochs &  &  \\
          &  & --- \\
          &  & --- \\
          &  &  \\ \midrule
         Grid-search size & 320 & 8 \\
         \bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \caption{Primary hyperparameter search grid for SSL methods. Best value is marked with bold font. SGD -- stocastic gradient descent with momentum () \cite{nesterov1983method}. An epoch is defined by maximum number of batches in labelled subset.}
    \label{tab:grid_search}
\end{table}


Regarding secondary fine-tuning, after the increase of , each epoch becomes proportionally longer. Thus, we propose the following inverse formula to define the number of epochs:

where  denotes total number of labelled batches, used while training. 

While secondary fine-tuning, we vary:
\begin{itemize}
    \item  (EMA decay)
    \item  for MixMatch / \\  for FixMatch
\end{itemize} \end{document}