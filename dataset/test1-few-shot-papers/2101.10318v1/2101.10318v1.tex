
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{pdfpages}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\acro}{{mTAN}}

\title{Multi-Time Attention Networks \\for Irregularly Sampled Time Series}



\author{Satya Narayan Shukla \& Benjamin M. Marlin  \\
College of Information and Computer Sciences\\
University of Massachusetts Amherst\\
Amherst, MA 01003, USA \\
\texttt{\{snshukla,marlin\}@cs.umass.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}
\maketitle
\begin{abstract}
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of our framework on interpolation and classification tasks using multiple datasets. Our results show that our approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods.\footnote{Our implementation is available at : \url{https://github.com/satyanshukla/mTANs}}
\end{abstract} \section{Introduction}

Irregularly sampled time series occur in applications including healthcare, climate science, ecology, astronomy, biology and others.
It is well understood that irregular sampling poses a significant challenge to machine learning models, which typically assume fully-observed, fixed-size  feature representations \citep{yadav2018mining}. While recurrent neural networks (RNNs) have been widely used to model such data because of their ability to handle variable length sequences, basic RNNs assume regular spacing between observation times as well as alignment of the time points where observations occur for different variables (i.e., fully-observed vectors). In practice, both of these assumptions can fail to hold for real-world sparse and irregularly observed time series. To respond to these challenges, there has been significant progress over the last decade on building and adapting machine learning models that can better capture the structure of irregularly sampled multivariate time series \citep{marlin-ihi2012, li2015classification, li2016scalable, lipton2016directly,  futoma2017improved, che2016recurrent, shukla2019, Rubanova2019}.

In this work, we introduce a new model for multivariate, sparse and irregularly sampled time series that we refer to as \textit{Multi-Time Attention networks} or {\acro}s. {\acro}s are fundamentally continuous-time interpolation-based models. Their primary innovations are the inclusion of a learned continuous-time embedding mechanism coupled with a time attention mechanism that replaces the use of a fixed similarity kernel when forming representation from continuous time inputs. This gives {\acro}s significantly more representational flexibility than previous interpolation-based models.  

Our approach re-represents an irregularly sampled time series at a fixed set of reference points. The proposed time attention mechanism uses reference time points as queries and the observed time points as keys. We propose an encoder-decoder framework for end-to-end learning using an {\acro} module to interface with given  multivariate, sparse and irregularly sampled time series inputs. The encoder takes the irregularly sampled time series as input and produces a fixed-length latent representation over a set of reference points, while the decoder uses the latent representations to produce reconstructions conditioned on the set of observed time points. Learning uses methods established for variational autoencoders \citep{rezende14, kingma13}.

The main contributions of the mTAN model framework are:
(1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging 
a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. 
(2) It uses a temporally distributed latent representation to better capture local structure in time series data.
(3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.





 \section{Related Work}

An irregularly sampled time series is a sequence of samples with irregular time intervals between observations. 
In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records \citep{yadav2018mining}, climate science \citep{schulz1997spectrum},
ecology \citep{clark2004population},
biology \citep{ruf1999lomb},
and astronomy \citep{scargle-astro1982}. It is well understood that such data cause significant issues for standard 
supervised machine learning models that typically assume 
fully observed, fixed-size feature representations \citep{yadav2018mining}.



A basic approach to dealing with irregular sampling is fixed temporal discretization. For example, \citet{marlin-ihi2012} and \citet{lipton2016directly} discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.

The alternative to temporal discretization is to construct models with the ability to directly use
an irregularly sampled time series as input.  \citet{che2016recurrent} present several methods based on  gated recurrent unit networks (GRUs, \citet{gru}), including an approach that takes as input a sequence consisting of observed values, missing data indicators, and time intervals since the last observation. \citet{pham2016} proposed to capture time irregularity by modifying the forget gate of an LSTM \citep{lstm}, while  \citet{phased2016} introduced a new time gate that regulates access to the hidden and cell state of the LSTM. While these approaches allow the network to handle event-based sequences with irregularly spaced vector-valued observations, they do not support learning directly from vectors that are partially observed, which commonly occurs in the multivariate setting because of lack of alignment of observation times across different variables.
 



Another line of work has looked at using observations from the future as well as from the past for interpolation. \citet{Yoon2017,Yoon18} presented an approach based on the multi-directional RNN (M-RNN) that can leverage observations from the relative past and future of a given time point. \citet{shukla2019} proposed the interpolation-prediction network framework, consisting of several semi-parametric RBF interpolation layers that interpolate multivariate, sparse, and irregularly sampled input time series against a set of reference time points while taking into account all observed data in a time series. \citet{seft} proposed a set function based approach for classifying time-series with irregularly sampled and unaligned observation.
 
\citet{neural_ode2018} proposed a variational auto-encoder model \citep{kingma13, rezende14} for continuous time data based on the use of a neural network decoder combined with a latent ordinary differential equation (ODE) model. They model time series data via a latent continuous-time function which is defined via a neural network representation of its gradient field. Building on this, \citet{Rubanova2019} proposed a latent ODE model which consists of an ODE-RNN model as the encoder. ODE-RNNs use neural ODEs to model the  hidden state dynamics and uses an RNN to update the hidden state in the presence of a new observation. \citet{gruodebayes2019} proposed GRU-ODE-Bayes, a continuous-time version of the Gated Recurrent Unit \citep{gru}. Instead of the encoder-decoder architecture where the ODE is decoupled from the input processing, GRU-ODE-Bayes provides a tighter integration by interleaving the ODE and the input processing steps. 

Several recent approaches have also used attention mechanism to model irregularly sampled time series \citep{attendanddiagnose2018,datagru2020,attain2019} as well as medical concepts \citep{tempselfatt2019,medicalconcept2018}. Most of these approaches are similar to  \citet{transformer} where they replace the positional encoding with an encoding of time and model sequences using self-attention. Instead of adding the time encoding to the input representation as in \citet{transformer}, they concatenate it with the input representation. These methods use a fixed time encoding similar to the positional encoding of \citet{transformer}. \citet{selfatttimeemb2019}  learn a functional time representation and concatenate it with the input event embedding to model time-event interactions. 

Like \citet{selfatttimeemb2019} and \citet{time2vec2020}, our proposed method learns a time representation. However, instead of concatenating it with the input embedding, our model learns to attend to observations at different time points by computing a similarity weighting using only the time embedding. Our proposed model uses the time embedding as both the queries and keys in the attention formulation. It learns an interpolation over the query time points by attending to the observed values at key time points. Our proposed method is thus similar to kernel-based interpolations, but learning the time attention based similarity kernel gives our model significantly more flexibility compared to methods like that of \citet{shukla2019} that use similarity kernels with fixed functional forms. Another important difference relative to many of these previous methods is that our proposed approach attends only to the observed data dimensions at each time point and hence does not require a separate imputation step to handle vector valued observations with an arbitrary collection of dimensions missing at any given time point.




















%
 \section{The Multi-Time Attention Module}

In this section, we present the proposed Multi-Time Attention Module (\acro). The role of this module is to re-represent a sparse and irregularly sampled time series in a fixed-dimensional representation. This module uses multiple continuous-time embeddings and attention-based interpolation. We begin by presenting notation followed by the time embedding and attention components.

\textbf{Notation:}
In the case of a supervised learning task, we let  represent a data set containing
 data cases. An individual data case consists of
a single target value  (discrete for classification), as well as a -dimensional, sparse and irregularly
sampled multivariate time series . Different dimensions  of the multivariate
time series can have observations at different times, as well as different total
numbers of observations . Thus, we represent time series  for data case  as a tuple
  where 
is the list of time points at which observations are defined and 
 is the corresponding list of observed values. In the case of an unsupervised task such as interpolation, each data case consists of a multivariate time series  only. We drop the data case index  for brevity when the context is clear.



\textbf{Time Embedding:}
Time attention module is based on embedding continuous time points into a vector space. We generalize the notion of a positional encoding used in transformer-based models to continuous time. Time attention networks simultaneously leverage   embedding functions , each outputting a representation of size . Dimension  of embedding  is defined as follows:


where the 's and 's are learnable parameters. The periodic terms capture the periodicity in the time series. In this case,  and  represent the frequency and phase of the sine function. The linear term, on the other hand, can capture non-periodic patterns dependent on the progression of time. For a given difference ,  can be represented as a linear function of . 

Learning the periodic time embedding functions is equivalent to using a one-layer fully connected network with a sine function non-linearity to map the time values into a higher dimensional space. By contrast, the positional encoding used in transformer models is defined only for discrete positions.  We note that our time embedding functions subsume positional encodings when evaluated at discrete positions.



\textbf{Multi-Time Attention:}
\label{sec:tan}
The time embedding component described above takes a continuous time point and embeds it into  different -dimensional spaces. In this section, we describe how we leverage time embeddings to produce a continuous-time embedding module for sparse and irregularly sampled time series. This \textit{multi-time attention} embedding module  takes as input a query time point  and a set of keys and values in the form of a -dimensional multivariate sparse and irregularly sampled time series  (as defined in the notation section above), and returns a  dimensional embedding at time . This process leverages a continuous-time attention mechanism applied to the  time embeddings. The complete computation is described below.
\allowdisplaybreaks


As shown in Equation \ref{eq:attn}, dimension  of the mTAN embedding  is given by a linear combination of intermediate univariate continuous-time functions . There is one such function defined for each input data dimension  and each time embedding . The parameters  are learnable linear combination weights.


As shown in Equation \ref{eq:interp}, the structure of the intermediate continuous-time function  is essentially a kernel smoother applied to the  dimension of the time series. However, the interpolation weights  are defined based on a time attention mechanism that leverages time embeddings, as shown in Equation . As we can see, the same time embedding function  is applied for all data dimensions. The form of the attention mechanism is a softmax function over the observed time points  for dimension . The activation within the softmax is a scaled inner product between the time embedding  of the query time point  and the time embedding  of the observed time point, the key. The parameters  and  are each  matrices where . We use a scaling factor  to normalize the dot product to counteract the growth in the dot product magnitude with increase in the dimension . 

Learning the time embeddings provides our model with flexibility to learn complex temporal kernel functions . The use of multiple simultaneous time embeddings  and a final linear combination across time embedding dimensions and data dimensions means that the final output representation function  is extremely flexible. Different input dimensions can leverage different time embeddings via learned sparsity patterns in the parameter tensor . Information from different data dimensions can also be mixed together to create compact reduced dimensional representations. We note that all of the required computations can be parallelized using masking variables to deal with unobserved dimensions, allowing for efficient implementation on a GPU.

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{figures/mTAND-Page-2.pdf}
\caption{Architecture of the mTAND module. It takes irregularly sampled time points and corresponding values as keys and values and produces fixed dimensional representation at the query time points.}
\label{fig:mTAND}
\end{figure}

\textbf{Discretization:} Since the mTAN module defines a continuous function of  given , it can not be directly incorporated into neural network architectures that expect inputs in the form of fixed-dimensional vectors or discrete sequences. However, the mTAN module can easily be adapted to produce such an output representation by materializing its output at a set of reference time points . In some cases, we may have a fixed set of such points. In other cases, the set of reference time points may need to depend on  itself. In particular, we define the auxiliary function  to return the set of time points at which there is an observation on any dimension of .  

Given a collection of reference time points , we define the discretized mTAN module  as . This module takes as input the set of reference time points  and the time series  and outputs a sequence of mTAN embeddings of length , each of dimension . The architecture of the mTAND module is shown in Figure \ref{fig:mTAND}. The mTAND module can be used to interface sparse and irregularly sampled multivariate time series data with any deep neural network layer type including fully-connected, recurrent, and convolutional layers. In the next section, we describe the construction of a temporal encoder-decoder architecture leveraging the mTAND module, which can be applied to both classification and interpolation tasks. 






















 
\section{Encoder-Decoder Framework}

\begin{figure}[t]
\centering
\includegraphics[width=0.6\linewidth]{figures/mTAN.pdf}
\caption{Architecture of the proposed encoder-decoder framework \textbf{mTAND-Full}. Classifier component (faded region) is required only in classification experiments not in interpolation experiments. mTAND module is shown in Figure \ref{fig:mTAND}.}
\label{fig:complete_model}
\end{figure}

\label{sec:enc-dec}
As described in the last section, we leverage the discretized mTAN module in an encoder-decoder framework as our primary model in this paper, which we refer to as an mTAN network. We develop the encoder-decoder framework within the variational autoencoder (VAE) framework in this section. The architecture for the model framework is shown in Figure \ref{fig:complete_model}.

\textbf{Model Architecture:} As we are modeling time series data, we begin by defining a sequence of latent states . Each of these latent states are iid-distributed according to a standard multivariate normal distribution . We let the set of latent states be  defined at  reference time points. 

We define a three-stage decoder. First, the latent states are processed through an RNN decoder module to induce temporal dependencies, resulting in a first set of deterministic latent variables . Second, the output of the RNN decoder stage and the  time points  are provided to the mTAND module along with a set of  query time points . The mTAND module outputs a sequence of embeddings  of length . Third, the mTAN embeddings are independently decoded using a fully connected decoder  and the result is used to parameterize an output distribution. In this work, we use a diagonal covariance Gaussian distribution with mean given by the final decoded representation and a fixed variance . The final generated time series is given by  with all data dimensions observed. The full generative process  is shown below. We let  define the probability distribution over the values of the time series  given the time points  and the latent variables .  represents the parameters of all components of  the decoder.  

For an encoder, we simply invert the structure of the generative process. We begin by mapping the input time series  through the mTAND module along with a collection of  reference time points . We apply an RNN encoder to the mTAND model that outputs  to encode longer-range temporal structure. Finally, we construct a distribution over latent variables at each reference time point using a diagonal Gaussian distribution with mean and variance output by fully connected layers applied to the RNN outputs . The complete encoder architecture is described below. We define  to be the distribution over the latent variables induced by the input time series  and the reference time points .  represents all of the parameters in all of the encoder components. 

     



\textbf{Unsupervised Learning:}
To learn the parameters of our encoder-decoder model given a data set of sparse and irregularly sampled time series, we follow a slightly modified VAE training approach and maximize a normalized variational 
lower bound on the log marginal likelihood based on the evidence lower bound or ELBO. The learning objective is defined  below where  and  are defined in the previous section.
\allowdisplaybreaks


Since irregularly sampled time series can have different numbers of observations across different dimensions as well as across different data cases, it can be helpful to normalize the terms in the standard ELBO objective to avoid the model focusing more on sequences that are longer at the expense of sequences that are shorter. The objective above normalizes the contribution of each data case by the total number of observations it contains. The fact that all data dimensions are not observed at all time points is accounted for in Equation \ref{eq:loglik}. In practice, we use  samples from the variational distribution  to compute the learning objective.

\textbf{Supervised Learning:} We can also augment the encoder-decoder model with a supervised learning component that leverages the latent states as a feature extractor. We define this component to be of the form  where  are the model parameters. This leads to an augmented learning objective as shown in Equation \ref{eq:classif} where the  term trades off the supervised and unsupervised terms.

In this work, we focus on classification as an illustrative supervised learning problem. For the classification model , we use a GRU followed by a 2-layer fully connected network. We use a small number of samples to approximate the required intractable expectations during both learning and prediction. Predictions are computed by marginalizing over the latent variable as shown below.

 \vspace{-2em}
\section{Experiments}
In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset). Additional illustrative results on synthetic data can be found in Appendix \ref{sec:synthetic_data}. 

\textbf{Datasets:} The PhysioNet Challenge 2012 dataset \citep{physionet} consists of multivariate time series data with  variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first  hours after admission to ICU. We follow the procedures of \citet{Rubanova2019} and round the observation times to the nearest minute. This leads to  possible measurement times per time series. The data set includes  labeled instances and  unlabeled instances.  We use all  instances for interpolation experiments and the  labeled instances for classification experiments. We focus on predicting in-hospital mortality.  of examples are in the positive class.

The MIMIC-III data set \citep{johnson2016mimic} is a multivariate time series dataset consisting of sparse and irregularly sampled physiological signals collected at Beth Israel Deaconess Medical Center from 2001 to 2012. Following the procedures  of \citet{shukla2019}, we extract  records each containing  physiological variables. We focus on predicting in-hospital mortality using the first  hours of data.  of the instances have positive labels.


The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of \citet{Rubanova2019} and construct a dataset of  sequences with  channels and  time points.  We focus on classifying each time point in the sequence into one of eleven types of activities. 





\textbf{Experimental Protocols:} 
We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80\% of the instances, and a test set containing the remaining 20\% of instances. We use 20\% of the training data for validation.
In the interpolation task, we condition on a subset of available points and reconstruct/predict values for all points. We perform interpolation experiments with a varying percentage of observed points ranging from  to  of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at all available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).



We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for human activity dataset. We randomly divide each data set into a training set containing 80\% of the time series,  and a test set containing the remaining 20\% of instances. We use 20\% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy. 

For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix \ref{sec:hyperparamters}.

\begin{table}[t]
\centering
\scriptsize
\caption{Interpolation performance versus percent observed time points on Physionet}
\label{table:phy_interp}
\begin{tabular}[h]{l c c c c c}
     \toprule
     {\bf Model} & \multicolumn{5}{c}{{\bf Mean Squared Error} } \\
     \midrule
     RNN-VAE &  &  &  &  &  \\
     L-ODE-RNN &  &  &  &  &  \\
     L-ODE-ODE &  &  &  &  &  \\
     mTAND-Full &  &  &  &  &  \\
     \midrule
     Observed \%  &  &  &  &  &  \\ 
    \bottomrule
\end{tabular}
\end{table}

\textbf{Models:} The model we focus on is the encoder-decoder architecture based on the discretized multi-time attention module (\textbf{mTAND-Full}). 
In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (\textbf{mTAND-Enc}). 
We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) \citep{gru} module as the recurrent network throughout. Architecture details can be found in Appendix \ref{sec:arch}. 
\begin{itemize}[leftmargin=*]
     \item \textbf{RNN-Impute:} Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples \citep{che2016recurrent}.
     \item \textbf{RNN-:} Input is concatenated with masking variable and time interval  indicating how long the particular variable is missing.
     \item \textbf{RNN-Decay:} RNN with exponential decay on hidden states \citep{mozer2017,che2016recurrent}.
     \item \textbf{GRU-D:} combining hidden state decay with input decay \citep{che2016recurrent}.
     \item \textbf{Phased-LSTM:} Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM \citep{phased2016} with forward filling to handle partially observed vectors. 
     \item \textbf{IP-Nets:} Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU \citep{shukla2019}.
     \item \textbf{SeFT:} Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach \citep{seft}.
     \item \textbf{RNN-VAE:} A VAE-based model where the encoder and decoder are standard RNN models. 
     \item \textbf{ODE-RNN:} Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation \citep{Rubanova2019}.
    \item \textbf{L-ODE-RNN:}  Latent ODE where the encoder is an RNN and decoder is a neural ODE \citep{neural_ode2018}. 
    \item \textbf{L-ODE-ODE:} Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE \citep{Rubanova2019}.
\end{itemize}

\textbf{Physionet Experiments:}
Table \ref{table:phy_interp} compares the performance of all methods on the interpolation task where we observe  of the values in the test instances.
As we can see, the proposed method (mTAND-Full) consistently and substantially out-performs all of the previous approaches. Further, the interpolation performance of the proposed method conditioned on just  of the observed values  is similar to that of Latent-ODE model conditioned on all the observed values. 


Table \ref{table:classif} compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods. 

We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver \citep{neural_ode2018, Rubanova2019}. These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.









\begin{table}[t]
\centering
\footnotesize
    \caption{Classification Performance on PhysioNet, MIMIC-III and Human Activity dataset.}
    \label{table:classif}
     \begin{tabular}[h]{l c c c r}
        \toprule
        \multirow{2}{*}{\bf Model} & \multicolumn{2}{c}{\bf AUC Score} & {\bf Accuracy} & \multirow{2}{*}{\bf time}\\
        \cmidrule{2-4}
        & {\bf PhysioNet} & {\bf MIMIC-III} & {\bf Human Activity} & {\bf per epoch}\\
        \midrule
        RNN-Impute	    &  &  &  & \\
        RNN-	&  &  &  & \\
        RNN-Decay	    &  &  &  & \\
        RNN GRU-D	    &  &  &  & \\
        Phased-LSTM     &  &  &  & \\    
        IP-Nets	        &  &  &  & \\
        SeFT	        &  &  &  & \\
        RNN-VAE	        &  &  &  & \\
        ODE-RNN	        &  &  &  & \\
        L-ODE-RNN 	    &  &  &  & \\
        L-ODE-ODE	    &  &  &  & \\
        \midrule 
        {mTAND-Enc}	&  &  &  & \\
        {mTAND-Full}&  &  &  & \\
        \bottomrule
    \end{tabular}
\end{table}



\textbf{MIMIC-III Experiments:}
Table \ref{table:classif} compares the  predictive performance of the models on the mortality prediction task on MIMIC-III.  The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While 
ODE-RNN and L-ODE-ODE both have slightly better mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.

\textbf{Human Activity Experiments:}
Table \ref{table:classif} shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets. 

\textbf{Additional Experiments:} In Appendix \ref{sec:synthetic_data}, we demonstrate the effectiveness of learning  temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.

We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix \ref{sec:ablation}. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix \ref{sec:ablation} shows that  learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.
 \section{Discussion and Conclusions}

In this paper, we have presented the Multi-Time Attention (mTAN) module for learning from sparse and irregularly sampled data along with a VAE-based encoder-decoder model leveraging this module. Our results show that the resulting model performs as well or better than a range of baseline and state-of-the-art models on both the interpolation and classification tasks, while offering training times that are one to two orders of magnitude faster than previous state of the art methods. 
While in this work we have focused on a VAE-based encoder-decoder architecture, the proposed mTAN module can be used to provide an interface between sparse and irregularly sampled time series and many different types of deep neural network architectures including GAN-based models. Composing the mTAN module with convolutional networks instead of recurrent architectures may also provide further computational enhancements due to improved parallelism.  \bibliography{references}
\bibliographystyle{iclr2021_conference}
\appendix
\clearpage
\section{Appendix}

\subsection{Ablation Study}
\label{sec:ablation}
In this section, we perform  ablation experiments to show the performance gain achieved by learning similarity kernel and time embedding.
Table \ref{table:time_embedding} shows the ablation results by substituting
fixed positional encoding \citep{transformer} in place of learnable time embedding defined in Equation \ref{eq:time_embedding} in mTAND-Full model on PhysioNet and MIMIC-III dataset
for classification task. We report the average AUC score over  runs. As we can see from Table \ref{table:time_embedding}, learning the time embedding improves AUC score by  as compared to using fixed positional encodings. 
 
 \begin{table}[h]
\centering
    \caption{Ablation with time embedding}
    \label{table:time_embedding}
        \begin{tabular}[h]{l c c}
         \toprule
{\bf Dataset} & {\bf Time Embedding} &     {\bf AUC Score} \\
         \midrule
\multirow{2}{*} {PhysioNet} & Positional Encoding &        \\
  & Learned Time Embedding &        \\
  \midrule
  \multirow{2}{*} {MIMIC-III} & Positional Encoding & \\
  & Learned Time Embedding &  \\
\bottomrule
\end{tabular}  
\end{table}

Since mTANs are fundamentally continuous-time interpolation-based models, we perform an ablation study by comparing mTANs with the IP-nets \citep{shukla2019}. IP-Nets use several semi-parametric RBF interpolation layers, followed by a GRU to model irregularly sampled time series. In this framework, we replace the RBK kernel with a learnable similarity kernel using mTAND module, the corresponding model is mTAND-Enc. 
Table \ref{table:kernel} compares the performance of the two methods on classification task on PhysioNet, MIMIC-III and Human Activity dataset. We report the average AUC score over  runs. Table \ref{table:kernel} shows that learning the similarity kernel using mTAND module performs as well or better than using a fixed RBF kernel.

\begin{table}[h]
\centering
    \caption{Comparing interpolation kernels}
    \label{table:kernel}
        \begin{tabular}[h]{l c c}
         \toprule
 {\bf Dataset} &   {\bf Model} &      {\bf AUC Score} \\
         \midrule
 \multirow{2}{*}{PhysioNet} & {IP-Nets} &          \\
 & {mTAND-Enc} &           \\
 \midrule
 \multirow{2}{*}{MIMIC-III} & {IP-Nets} &          \\
 & {mTAND-Enc} &           \\
 \midrule
 \multirow{2}{*}{Human Activity} & {IP-Nets} &          \\
 & {mTAND-Enc} &           \\
\bottomrule
\end{tabular}  
\end{table}


\subsection{Synthetic Interpolation Experiments}
\label{sec:synthetic_data}
To demonstrate the capabilities of our model on the interpolation task, we generate a synthetic dataset consisting of  trajectories each of  time points sampled over . We fix  reference points and use RBF kernel with a fixed bandwidth of  for constructing local interpolations at  time points over . The values at the reference points are drawn from a standard normal distribution. 

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.75\textwidth}
   \includegraphics[width=\linewidth]{figures/syn_legend.pdf}
\end{subfigure}\\
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\linewidth]{figures/syn_kernel_mtan.pdf}
\end{subfigure}\\
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\linewidth]{figures/syn_kernel_lode.pdf}
\end{subfigure}
\caption{Interpolations on the synthetic interpolation dataset. The columns represent  different examples. First row: Ground truth trajectories with observed points, second row: reconstructions on the complete range  using the proposed model mTAN, third row: reconstructions on the complete range  using the Latent ODE model with ODE encoder.}
\label{fig:toy}
\end{figure}


We randomly sample  observations from each trajectory to simulate a sparse and irregularly sampled multivariate time series. We use  of the data for training and  for testing. At test time, encoder conditions on  irregularly sampled time points and the decoder generates interpolations on all  time points.  Figure \ref{fig:toy} illustrates the interpolation results on the test set for the Multi-Time Attention Network and Latent ODE model with ODE encoder \citep{Rubanova2019}. For both the models, we draw 100 samples from the approximate posterior distribution. As we can see from Figure \ref{fig:toy}, the ODE interpolations are much smoother and haven't been able to capture the local structure as well as mTANS.

Table \ref{table:syn_dataset} compares the proposed model with best performing baseline Latent-ODE with ODE encoder (L-ODE-ODE) on reconstruction and interpolation task. For both the tasks, we condition on the  irregularly sampled time points and reconstruct the input points (reconstruction) and the whole set of  time points (interpolation). We report the mean squared error on test set.


\begin{table}[t]
\centering
    \caption{Synthetic Data: Mean Squared Error}
    \label{table:syn_dataset}
        \begin{tabular}[h]{c c c c}
         \toprule
  {\bf Latent Dimension} &   {\bf Model} &     {\bf Reconstruction} & {\bf Interpolation} \\
         \midrule
  \multirow{2}{*}{} &   {L-ODE-ODE} &      &      \\
  	&   {\bf mTAND-Full} &   &    \\ \midrule
   \multirow{2}{*}{} &   {L-ODE-ODE} &      &      \\
   	&  {\bf mTAND-Full} &   &    \\
            \bottomrule
            \end{tabular}  
           \end{table}




\subsection{Architecture Details}




\label{sec:arch}
{\bf Multi-Time Attention Network (mTAND-Full)}: 
In our proposed encoder-decoder framework (Figure \ref{fig:complete_model}), we use bi-directional GRU as the recurrent model in both encoder and decoder. In encoder, we use a 2 layer fully connected network with 50 hidden units and ReLU activations to map the RNN hidden state at each reference point to mean and variance. Similarly in decoder, mTAN embeddings are independently decoded using a 2 layer fully connected network with 50 hidden units and ReLU activations, and the result is used to parameterize the output distribution. For classification tasks, we use a separate GRU layer on top of the latent states followed by a 2-layer fully connected layer with  units and ReLU activations to output the class probabilities.  

{\bf Multi-Time Attention Encoder (mTAND-Enc)}: As we show in the experiments, the proposed mTAN module can standalone be used for classification tasks. The mTAND-Enc consists of Multi-Time attention module followed by GRU to extract the final hidden state which is then passed to a 2-layer fully connected layer to output the class probabilities.   

{\bf Loss Function:} For computing the evidence lower bound (ELBO) during training, we use negative log-likelihood with fixed variance as the reconstruction loss. For all the datasets, we use a fixed variance of . For computing ELBO, we use 5 samples for interpolation task and 1 sample for classification tasks. We use cross entropy loss for classification. For the classification tasks, we tune the  parameter in the supervised learning loss function (Equation 15). We achieved best performance using  as   and  for Physionet, MIMIC-III respectively. For human activity dataset, we achieved best results without using the regulaizer or ELBO component.  We found that KL annealing with coeff  improved the performance of interpolation and classification tasks on Physionet.

\subsection{Hyperparameters}
\label{sec:hyperparamters}
{\bf Baselines:} For Physionet and Human Activity dataset, we use the reported hyperparameters for RNN baselines as well as ODE models from \citet{Rubanova2019}. For MIMIC-III dataset, we independently tune the hyperparameters of the baseline models on the validation set. We search for GRU hidden units, latent dimension, number of hidden units in fully connected network for ODE function in recognition and generative model over the range . For ODEs, we also searched the number of layers in fully connected network in the range . 

{\bf mTAN:} We learn time embeddings of size . The number of embeddings . The linear projection matrices used for projecting time embedding  are each   where  is the embedding size. We search the latent dimension and GRU encoder hidden size over the range \{32, 64, 128\}. We keep GRU decoder hidden size at . For the classification tasks, we use  reference points. For interpolation task, we search number of reference points over the range . We use Adam Optimizer for training the models. For classification, experiments are run for  iteration with learning rate , while for interpolation task experiments are run for  iterations with learning rate . Best hyperparameters are reported in the code.


\subsection{Visualizing Attention Weights}
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/attn_weights.pdf}
\caption{Visualization of attention weights. mTAN learns an interpolation over the query time points by attending to the observed values at key time points. The brighter edges correspond to higher attention weights.}
\label{fig:viz}
\end{figure}

In this section, we visualize the attention weights learned by our proposed model. We  experiment using synthetic dataset (described in \ref{sec:synthetic_data}) which consists of univariate time series. Figure \ref{fig:viz} shows the attention weights learned by the encoder mTAND module. The input shown in the figure is the irregularly sampled time points and the edges show how the output at reference points attends to the values on the input time points. The final output can be computed by substituting the attention weights in Equation \ref{eq:interp}. 
 
\subsection{Training Details}
\subsubsection{Data generation and preprocessing}
All the datasets used in the experiments are publicly available and can be downloaded using the following links:\\
PhysioNet: \url{https://physionet.org/content/challenge-2012/}\\
MIMIC-III: \url{https://mimic.physionet.org/}\\
Human Activity: \url{https://archive.ics.uci.edu/ml/datasets/Localization+Data+for+Person+Activity}.

We rescale each feature to be between  and  for Physionet and MIMIC-III dataset. We also rescale the time to be in  for all datasets. 
In case of MIMIC-III dataset, for the time series missing entirely, we follow the preprocessing steps of \citet{shukla2019} and assign the starting point (time t=0) value of the time series to the global mean for that variable. 

\subsubsection{Source Code}
The code for reproducing the results in this paper is available at \url{https://github.com/satyanshukla/mTANs}. 

\subsubsection{Computing Infrastructure} 
All experiments were run on a Nvidia Titan X GPU.





 \end{document}
