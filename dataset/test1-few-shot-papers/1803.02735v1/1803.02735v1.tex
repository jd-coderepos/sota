\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{color}
\usepackage{comment}

\def\Plus{\texttt{+}}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{3606} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\newcommand{\gscom}[1]{\textcolor{magenta}{\small {#1}}}

\newcommand{\changeurlcolor}[1]{\hypersetup{urlcolor=#1}}  

\title{Deep Back-Projection Networks For Super-Resolution}

\author{Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita\\
Toyota Technological Institute, Japan Toyota Technological Institute at Chicago, United States\\
{\tt\small \{mharis, ukita\}@toyota-ti.ac.jp, greg@ttic.edu}
}

\maketitle


\begin{abstract}
The feed-forward architectures of recently proposed deep
super-resolution networks learn representations of low-resolution
inputs, and the non-linear mapping from those to high-resolution
output. However, this approach does not fully address the mutual
dependencies of low- and high-resolution images. We propose Deep
Back-Projection Networks (\changeurlcolor{blue}\href{http://www.toyota-ti.ac.jp/Lab/Denshi/iim/members/muhammad.haris/projects/DBPN.html}{DBPN}), that exploit iterative up- and down-sampling
layers, providing an error feedback mechanism for projection errors at
each stage. We construct mutually-connected up- and down-sampling
stages each of which represents different types of image degradation
and high-resolution components. We show that extending this idea to
allow concatenation of features across up- and down-sampling stages 
(Dense DBPN) allows us to reconstruct further improve
super-resolution, yielding superior results and in particular
establishing new state of the art results for large scaling factors
such as  across multiple data sets. 
\end{abstract}


\section{Introduction}
Significant progress in deep learning for
vision~\cite{huang2017densely,he2015deep,denton2015deep,shrivastava2016learning,larsson2016fractalnet,radford2015unsupervised,IMKDB17}
has recently been propagating to the field of super-resolution (SR)~\cite{johnson2016perceptual,liao2015video,dong2016image,Haris17,kappeler2016video,Kim_2016_VDSR,LapSRN,Tai-DRRN-2017}. 
\begin{figure}[t]
\centering
\includegraphics[width=8cm]{intro_new}
\caption{Super-resolution result on  enlargement. PSNR: LapSRN~\cite{LapSRN} (15.25 dB), EDSR~\cite{Lim_2017_CVPR_Workshops} (15.33 dB), and Ours (16.63 dB)}
\label{figure:intro}
\end{figure}
\begin{figure*}
\centering
\includegraphics[width=14.5cm]{multi_upsampling}
\caption{Comparisons of Deep Network SR. (a) Predefined upsampling (e.g., SRCNN~\cite{dong2016image}, VDSR~\cite{Kim_2016_VDSR}, DRRN~\cite{Tai-DRRN-2017}) commonly uses the conventional interpolation, such as Bicubic, to upscale LR input images before entering the network. (b) Single upsampling (e.g., FSRCNN~\cite{dong2016accelerating}, ESPCN~\cite{shi2016real}) propagates the LR features, then construct the SR image at the last step. (c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images~\cite{LapSRN}. (d) Iterative up and downsampling approach is proposed by our DBPN which exploit the mutually connected up- (blue box) and down-sampling (gold box) stages to obtain numerous HR features in different depths.}
\label{figure:multi_upsampling}
\end{figure*}

Single image SR is an ill-posed inverse problem where the aim is to
recover a high-resolution (HR) image from a low-resolution (LR)
image. A currently typical approach is to construct an HR image by
learning non-linear LR-to-HR mapping, implemented as a deep neural
network~\cite{dong2016image, dong2016accelerating, shi2016real,
  LapSRN, Kim_2016_VDSR, kim2016deeply, Tai-DRRN-2017}. These networks
compute a sequence of feature maps from the LR image, culminating with
one or more upsampling layers to increase resolution and finally
construct the HR image. In contrast to this purely feed-forward
approach, human visual system is believed to use a feedback connection
to simply guide the task for the relevant
results~\cite{felleman1991distributed, kravitz2013ventral,
  lamme2000distinct}. Perhaps hampered by lack of such feedback,
the current SR networks with only feed-forward connections have difficulty in representing the LR to HR relation, especially for large scaling factors.

On the other hand, feedback connections were used effectively by one
of the early SR algorithms, the iterative
back-projection~\cite{irani1991improving}. It iteratively computes the
reconstruction error then fuses it back to tune the HR image
intensity. Although it has been proven to improve the image quality,
the result still suffers from ringing effect and chessboard
effect~\cite{dai2007bilateral}. Moreover, this method is sensitive to
choices of parameters such as the number of iterations and the blur
operator, leading to variability in results.







Inspired by~\cite{irani1991improving}, we construct an end-to-end
trainable architecture based on the idea of iterative up- and
down-sampling: Deep Back-Projection Networks (DBPN). Our networks
successfully perform large scaling factors, as shown
in~Fig.~\ref{figure:intro}. Our work provides
the following contributions:


\noindent(1) \textbf{Error feedback}. We propose an iterative
error-correcting feedback mechanism for SR, which calculates both up- and down-projection errors to guide the reconstruction for obtaining better results. Here, the projection errors are used to characterize or constraint the features in early layers. Detailed explanation can be seen in Section~\ref{sec:proposed}.

\noindent(2) \textbf{Mutually connected up- and down-sampling stages}. 
Feed-forward architectures, which is considered as a one-way mapping, only map rich representations of the input to the output space. This approach is unsuccessful to map LR and HR image, especially in large scaling factors, due to limited features available in the LR spaces. Therefore, our networks focus not only generating variants of the HR features using upsampling layers but also projecting it back to the LR spaces using downsampling layers. This connection is shown in~Fig.~\ref{figure:multi_upsampling} (d), alternating between up- (blue box) and down-sampling (gold box) stages, which represent the mutual relation of LR and HR image.

\noindent(3) \textbf{Deep concatenation}. Our networks represent different types of image degradation and HR components. This ability enables the networks to reconstruct the HR image using deep concatenation of the HR feature maps from all of the up-sampling steps. Unlike other networks, our reconstruction directly utilizes different types of LR-to-HR features without propagating them through the sampling layers as shown by the red arrow in~Fig.~\ref{figure:multi_upsampling} (d).


\noindent(4) \textbf{Improvement with dense connection}. We improve the accuracy of our network by densely connected~\cite{huang2017densely} each up- and down-sampling stage to encourage feature reuse.


\section{Related Work}
\subsection{Image super-resolution using deep networks}
Deep Networks SR can be primarily divided into four types as shown in~Fig.~\ref{figure:multi_upsampling}.

(a) \textbf{Predefined upsampling} commonly uses interpolation as the upsampling operator to produce middle resolution (MR) image. This schema was firstly proposed by SRCNN~\cite{dong2016image} to learn MR-to-HR non-linear mapping with simple convolutional layers. Later, the improved networks exploited residual learning~\cite{Kim_2016_VDSR,Tai-DRRN-2017} and recursive layers~\cite{kim2016deeply}. However, this approach might produce new noise from the MR image.


(b) \textbf{Single upsampling} offers simple yet effective way to increase the spatial resolution. This approach was proposed by FSRCNN~\cite{dong2016accelerating} and ESPCN~\cite{shi2016real}. 
These methods have been proven effective to increase the spatial resolution and replace predefined operators. However, they fail to learn complicated mapping due to limited capacity of the networks.
EDSR~\cite{Lim_2017_CVPR_Workshops}, the winner of NTIRE2017~\cite{timofte2017ntire}, belongs to this type. However, it
requires a large number of filters in each layer and lengthy training time, around eight days as stated by the authors. These problems open the opportunities to propose lighter networks that can preserve HR components better.

(c) \textbf{Progressive upsampling} was recently proposed in
LapSRN~\cite{LapSRN}. It progressively reconstructs the multiple SR
images with different scales in one feed-forward network. For the sake
of simplification, we can say that this network is the stacked of single upsampling networks which only relies on limited LR features. 
Due to this fact, LapSRN is outperformed even by our shallow networks especially for large scaling factors such as  in experimental results.


(d) \textbf{Iterative up and downsampling} is proposed by our networks. We focus on increasing the sampling rate of SR features in different depths and distribute the tasks to calculate the reconstruction error to each stage. This schema enables the networks to preserve the HR components by learning various up- and down-sampling operators while generating deeper features.

\subsection{Feedback networks} 
Rather than learning a non-linear mapping of input-to-target space in one step, the feedback networks compose the prediction process into multiple steps which allow the model to have a self-correcting procedure. Feedback procedure has been implemented in various computing tasks~\cite{carreira2016human,ross2011learning,tu2010auto,li2016iterative,zamir2016feedback, shrivastava2016contextual,lotter2016deep}.

In the context of human pose estimation, Carreira et
al.~\cite{carreira2016human} proposed an iterative error feedback by
iteratively estimating and applying a correction to the current
estimation. PredNet~\cite{lotter2016deep} is an unsupervised recurrent
network to predictively code the future frames by recursively feeding
the predictions back into the model. For image segmentation, Li et
al.~\cite{li2016iterative} learn implicit shape priors and use them to
improve the prediction. However, to our knowledge, feedback procedures
have not been implemented to SR.

\subsection{Adversarial training}
Adversarial training, such as with Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} has been applied to various image reconstruction problems~\cite{ledig2016photo, sajjadi2016enhancenet, radford2015unsupervised, denton2015deep, johnson2016perceptual}. For the SR task, Johnson et al.~\cite{johnson2016perceptual} introduced perceptual losses based on high-level features extracted from pre-trained networks. Ledig et al.~\cite{ledig2016photo} proposed SRGAN which is considered as a single upsampling method. It proposed the natural image manifold that is able to create photo-realistic images by specifically formulating a loss function based on the euclidian distance between feature maps extracted from VGG19~\cite{simonyan2014very} and SRResNet. 

Our networks can be extended with the adversarial loss as generator network. However, we optimize our network only using an objective function such as mean square root error (MSE). Therefore, instead of training DBPN with the adversarial loss, we can compare DBPN with SRResNet which is also optimized by MSE.

\subsection{Back-projection} 
Back-projection~\cite{irani1991improving} is well known as the efficient iterative procedure to minimize the reconstruction error. Previous studies have proven the effectivity of back-projection~\cite{zhao2017iterative, haris2017first, dong2009nonlocal, timofte2016seven}. Originally, back-projection is designed for the case with multiple LR inputs. However, given only one LR input image, the updating procedure can be obtained by upsampling the LR image using multiple upsampling operators and calculate the reconstruction error iteratively~\cite{dai2007bilateral}. Timofte et al.~\cite{timofte2016seven} mentioned that back-projection can improve the quality of SR image. Zhao et al.~\cite{zhao2017iterative} proposed a method to refine high-frequency texture details with an iterative projection process. However, the initialization which leads to an optimal solution remains unknown. Most of the previous studies involve constant and unlearnable predefined parameters such as blur operator and number of iteration. 

To extend this algorithm, we develop an end-to-end trainable architecture which focuses to guide the SR task using mutually connected up- and down-sampling stages to learn non-linear relation of LR and HR image. The mutual relation between HR and LR image is constructed by creating iterative up and down-projection unit where the up-projection unit generates HR features, then the down-projection unit projects it back to the LR spaces as shown in~Fig.~\ref{figure:multi_upsampling} (d).
This schema enables the networks to preserve the HR components by learned various up- and down-sampling operators and generates deeper features to construct numerous LR and HR features.  


\section{Deep Back-Projection Networks} 
\label{sec:proposed}
Let  and  be HR and LR image with   and , respectively, where  and . The main building block of our proposed DBPN architecture is the
projection unit, which is trained (as part of the end-to-end training
of the SR system) to map either an LR feature map to an HR map
(up-projection), or an HR map to an LR map (down-projection). 

\subsection{Projection units}


The up-projection unit is defined as follows:

where * is the spatial convolution operator,  and  are, respectively, the up- and
down-sampling operator with scaling factor , and  are (de)convolutional layers at stage .

This projection unit takes the previously computed LR feature map
 as input, and maps it to an (intermediate) HR map ;
then it attempts to map it back to LR map 
(``back-project''). The residual (difference)  between the observed LR map
 and the reconstructed  is mapped to HR again,
producing a new intermediate (residual) map ; the final output of the unit,
the HR map , is obtained by summing the two intermediate HR maps. This step is illustrated in the upper part of~Fig.~\ref{figure:projection_unit}.

The down-projection unit is defined very similarly, but now its job is
to map its input HR map  to the LR map  as illustrated in the lower part of~Fig.~\ref{figure:projection_unit}.


We organize projection units in a series of \emph{stages}, alternating between  and .
These projection units can be understood as a self-correcting procedure which feeds a projection error to the sampling layer and iteratively changes the solution by feeding back the projection error.

\begin{figure}[t!]
\centering
\includegraphics[width=7.5cm]{projection_unit}
\caption{Proposed up- and down-projection unit in the DBPN.}
\label{figure:projection_unit}
\end{figure}

The projection unit uses large sized filters such as  and . In other existing networks, the use of large-sized filter is avoided because it slows down the convergence speed and might produce sub-optimal results. However, iterative utilization of our projection units enables the network to suppress this limitation and to perform better performance on large scaling factor even with shallow networks.

\subsection{Dense projection units}
The dense inter-layer connectivity pattern in DenseNets~\cite{huang2017densely} has been shown to 
alleviate the vanishing-gradient problem, produce improved feature,
and encourage feature reuse. Inspired by this we propose to improve
DBPN, by introducing dense connections in the projection units called,
yielding Dense DBPN (D-DBPN).

Unlike the original DenseNets, we avoid dropout and batch norm, which are not suitable for SR, because they remove the range flexibility of the features~\cite{Lim_2017_CVPR_Workshops}.
Instead, we use  convolution layer as feature pooling and dimensional reduction~\cite{szegedy2015going,Haris17} before entering the projection unit.


In D-DBPN, the input for each unit is the concatenation of the outputs from all previous units. Let the  and  be the input for dense up- and down-projection unit, respectively. They are generated using  which is used to merge all previous outputs from each unit as shown in~Fig.~\ref{figure:D_DBPN}. This improvement enables us to generate the feature maps effectively, as shown in the experimental results.

\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{D_DBPN}
\caption{Proposed up- and down-projection unit in the D-DBPN. The feature maps of all preceding units (i.e.,  and  in up- and down-projections units, respectively) are concatenated and used as inputs, and its own feature maps are used as inputs into all subsequent units.}
\label{figure:D_DBPN}
\end{figure}

\subsection{Network architecture}
\begin{figure*}[t]
\centering
\includegraphics[width=14cm]{proposed_network}
\caption{An implementation of D-DBPN for super-resolution. Unlike the original DBPN, D-DBPN exploits densely connected projection unit to encourage feature reuse.}
\label{figure:proposed_network}
\end{figure*}

The proposed D-DBPN is illustrated in~Fig.~\ref{figure:proposed_network}. It can be divided into three
parts: initial feature extraction, projection, and reconstruction, as
described below. Here, let  be a convolutional layer,
where  is the filter size and  is the number of filters. 

\begin{enumerate}
\item \textbf{Initial feature extraction}. We construct initial LR
  feature-maps  from the input using . 
  Then  is used to reduce the dimension from  to  before
  entering projection step where  is the number of filters used in the initial LR features extraction and  is the number of filters used in each projection unit.
\item \textbf{Back-projection stages}. Following initial feature
  extraction is a sequence of projection units, alternating between
  construction of LR and HR feature maps ,
  ; each unit has access to the outputs of all previous
  units.

\item \textbf{Reconstruction}. Finally, the target HR image is
  reconstructed as  where  use  as
  reconstruction and  refers to the concatenation of the feature-maps
  produced in each up-projection unit.
\end{enumerate}

Due to the definitions of these building blocks, our network
architecture is modular. We can easily define and train networks with
different numbers of stages, controlling the depth. For a network with
 stages, we have the initial extraction stage (2 layers), and then
 up-projection units and  down-projection units, each with 3
layers, followed by the reconstruction (one more layer). However, for the dense network, we add  in each projection unit, except the first three units.


\section{Experimental Results}
\subsection{Implementation and training details}
In the proposed networks, the filter size in the projection unit is
various with respect to the scaling factor. For  enlargement,
we use  convolutional layer with two striding and two
padding. Then,  enlargement use  convolutional
layer with four striding and two padding. Finally, the 
enlargement use  convolutional layer with eight striding
and two padding.\footnote{We found these settings to work well based
  on general intuition and preliminary experiments.}

We initialize the weights based on~\cite{he2015delving}. Here, std is computed by  where ,  is the filter size, and  is the number of filters. For example, with  and , the std is . All convolutional and deconvolutional layers are followed by parametric rectified linear units (PReLUs).

We trained all networks using images from DIV2K \cite{timofte2017ntire}, Flickr \cite{Lim_2017_CVPR_Workshops}, and ImageNet dataset \cite{russakovsky2015imagenet} without augmentation.\footnote{The comparison on DIV2K only are available in the supplementary material.} To produce LR images, we downscale the HR images on particular scaling factors using Bicubic. We use batch size of 20 with size  for LR image, while HR image size corresponds to the scaling factors. The learning rate is initialized to  for all layers and decrease by a factor of 10 for every  iterations for total  iterations. For optimization, we use Adam with momentum to  and weight decay to . All experiments were conducted using Caffe, MATLAB R2017a on NVIDIA TITAN X GPUs.

\subsection{Model analysis}
\label{subsec:modelanalysis}
\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{modular_comparison_4x}
\caption{The depth analysis of DBPNs compare to other networks (VDSR~\cite{Kim_2016_VDSR}, DRCN~\cite{kim2016deeply}, DRRN~\cite{Tai-DRRN-2017}, LapSRN~\cite{LapSRN}) on Set5 dataset for 4 enlargement.}
\label{figure:modular_comparison_4x}
\end{figure} 

\textbf{Depth analysis}. To demonstrate the capability of our projection unit, we construct multiple networks  (),  (), and  () from the original DBPN. In the feature extraction, we use  followed by . Then, we use  for the reconstruction. The input and output image are luminance only. 

The results on  enlargement are shown in~Fig.~\ref{figure:modular_comparison_4x}. DBPN outperforms the state-of-the-art methods. Starting from our shallow network, the  network gives the higher PSNR than VDSR, DRCN, and LapSRN. The  network uses only 12 convolutional layers with smaller number of filters than VDSR, DRCN, and LapSRN. At the best performance,  networks can achieve  dB which better  dB,  dB,  dB than VDSR, DRCN, and LapSRN, respectively. The  network shows performance improvement which better than all four existing state-of-the-art methods (VDSR, DRCN, LapSRN, and DRRN). At the best performance, the  network can achieve  dB which better  dB,  dB,  dB,  dB than VDSR, DRCN, LapSRN, and DRRN respectively. In total, the  network use 24 convolutional layers which has the same depth as LapSRN. Compare to DRRN (up to 52 convolutional layers), the  network undeniable shows the effectiveness of our projection unit. Finally, the  network outperforms all methods with  dB which better  dB,  dB,  dB,  dB than VDSR, DRCN, LapSRN, and DRRN, respectively.

The results of  enlargement are shown in~Fig.~\ref{figure:modular_comparison_8x}. The  networks outperform the current state-of-the-art for  enlargement which clearly show the effectiveness of our proposed networks on large scaling factors. However, we found that there is no significant performance gain from each proposed network especially for  and  networks where the difference only  dB. 
\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{modular_comparison_8x}
\caption{The depth analysis of DBPN on Set5 dataset for 8 enlargement. S (), M (), and L ()}
\label{figure:modular_comparison_8x}
\end{figure} 



\textbf{Number of parameters}. 
We show the tradeoff between performance
and number of network parameters from our networks and existing deep
network SR in~Fig.~\ref{figure:psnr_vs_param_4x}~and~\ref{figure:psnr_vs_param_8x}. 

For the sake of low computation for real-time processing, we construct  network which is the lighter version of the  network, . We only use  followed by  for the initial feature extraction. However, the results outperform SRCNN, FSRCNN, and VDSR on both  and  enlargement. Moreover, our  network performs better than VDSR with  and  fewer parameters on  and  enlargement, respectively. 

Our  network has about  fewer parameters and higher PSNR than LapSRN on  enlargement. 
Finally, D-DBPN has about  fewer parameters, and
approximately the same PSNR, compared to EDSR on  enlargement. On the  enlargement, D-DBPN has about  fewer parameters with better PSNR compare to EDSR. This evidence show that our networks has the best trade-off between performance and number of parameter.
\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{psnr_vs_param_4x}
\caption{Performance vs number of parameters. The results are evaluated with Set5 dataset for  enlargement.}
\label{figure:psnr_vs_param_4x}

\includegraphics[width=8.5cm]{psnr_vs_param_8x}
\caption{Performance vs number of parameters. The results are evaluated with Set5 dataset for  enlargement.}
\label{figure:psnr_vs_param_8x}
\end{figure} 

\begin{comment}
\begin{table}[h!]
\small
\caption{Performance (PSNR) of the DBPN-SS network and other networks on 4 and 8 enlargement. {\color{red}Red} indicates the best performance.}
\centering
\label{tab:filter_number}
\begin{tabular}{*1l*1c*1c*1c|*1c*1c}
\hline\noalign{\smallskip}
Algorithm & Scale &Depth&Parameter&Set5 &Set14 \\         
\noalign{\smallskip}\hline\noalign{\smallskip}
SRCNN~\cite{dong2016image}&4&3&57k			&&\\
FSRCNN~\cite{dong2016accelerating}&4&8&12k		&&\\
VDSR~\cite{Kim_2016_VDSR}&4&20&665k			&&\\
DBPN-SS&4&12&188k		&{\color{red}}&{\color{red}}\\
\noalign{\smallskip}\hline\noalign{\smallskip}
SRCNN~\cite{dong2016image}&8&3&57k			&&\\
FSRCNN~\cite{dong2016accelerating}&8&8&12k			&&\\
VDSR~\cite{Kim_2016_VDSR}&8&20&665k			&&\\
DBPN-SS&8&12&422k		&{\color{red}}&{\color{red}}\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}
\end{comment}



\textbf{Deep concatenation}. Each projection unit is used to
distribute the reconstruction step by constructing features
which represent different details of the HR components. Deep concatenation is also well-related with the number of  (back-projection stage), 
which shows more detailed features generated from the projection units will also increase the quality of the results. In~Fig.~\ref{figure:result_up_projection}, it is shown that each stage successfully generates diverse features to reconstruct SR image.

\begin{figure}[t]
\centering
\includegraphics[width=8.5cm]{result_up_projection}
\caption{Sample of activation maps from up-projection units in D-DBPN where . Each feature has been enhanced using the same grayscale colormap for visibility.}
\label{figure:result_up_projection}
\end{figure} 

\textbf{Dense connection}. We implement D-DBPN-L which is a dense connection of the  network to show how dense connection can improve the network's performance in all cases as shown in~Table~\ref{tab:dense}. On  enlargement, the dense network, D-DBPN-L, gains  dB and  dB higher than DBPN-L on the Set5 and Set14, respectively. On , the gaps are even larger. The D-DBPN-L has  dB and  dB higher that DBPN-L on the Set5 and Set14, respectively.
\begin{table}[h!]
\small
\caption{Comparison of the DBPN-L and D-DBPN-L on 4 and 8 enlargement. {\color{red}Red} indicates the best performance.}
\centering
\label{tab:dense}
\begin{tabular}{*1l*1c|*2c*2c}
\hline\noalign{\smallskip}
\smallskip & &\multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14} \\         
Algorithm & Scale & PSNR&SSIM & PSNR&SSIM   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
DBPN-L&4		&&&&\\
D-DBPN-L&4		&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\noalign{\smallskip}\hline\noalign{\smallskip}
DBPN-L&8		&&&&\\
D-DBPN-L&8		&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}



\subsection{Comparison with the-state-of-the-arts}
\begin{figure*}
\centering
\includegraphics[width=15cm]{4x_result}
\caption{Qualitative comparison of our models with other works on  super-resolution.}
\label{figure:4x_result}
\end{figure*}

To confirm the ability of the proposed network, we performed several experiments and analysis. We compare our network with eight state-of-the-art SR algorithms: A+~\cite{timofte2014a+}, SRCNN~\cite{dong2016image}, FSRCNN~\cite{dong2016accelerating}, VDSR~\cite{Kim_2016_VDSR}, DRCN~\cite{kim2016deeply}, DRRN~\cite{Tai-DRRN-2017}, LapSRN~\cite{LapSRN}, and EDSR~\cite{Lim_2017_CVPR_Workshops}. We carry out extensive experiments using 5 datasets: Set5~\cite{bevilacqua2012low}, Set14~\cite{zeyde2012single}, BSDS100~\cite{arbelaez2011contour}, Urban100~\cite{huang2015single} and Manga109~\cite{matsui2016sketch}. Each dataset has different characteristics. Set5, Set14 and BSDS100 consist of natural scenes; Urban100 contains urban scenes with details in different frequency bands; and Manga109 is a dataset of Japanese manga. Due to computation limit of Caffe, we have to divide each image in Urban100 and Manga109 into four parts and then calculate PSNR separately.

Our final network, D-DBPN, uses  then  for the initial feature extraction and  for the back-projection stages. In the reconstruction, we use . RGB color channels are used for input and output image. It takes less than four days to train.

PSNR~\cite{irani93} and structural similarity (SSIM)~\cite{wang04}
were used to quantitatively evaluate the proposed method. Note that higher PSNR
and SSIM values indicate better quality. As used by existing networks,
all measurements used only the luminance channel (Y). For SR by factor
, we crop  pixels near image boundary before evaluation as in~\cite{Lim_2017_CVPR_Workshops, dong2016accelerating}. Some of the existing networks such as SRCNN, FSRCNN, VDSR, and EDSR did not perform  enlargement. To this end, we retrained the existing networks by using author's code with the recommended parameters. 

We show the quantitative results in the~Table~\ref{tab:psnr}. Our D-DBPN outperforms the existing methods by a large margin in all scales except EDSR. For the  and  enlargement, we have comparable PSNR with EDSR. However, the result of EDSR tends to generate stronger edge than the ground truth and lead to misleading information in several cases. The result of EDSR for eyelashes in~Fig.~\ref{figure:4x_result} shows that it was interpreted as a stripe pattern. On the other hand, our result generates softer patterns which subjectively closer to the ground truth. On the butterfly image, EDSR separates the white pattern which shows that EDSR tends to construct regular pattern such ac circle and stripe, while D-DBPN constructs the same pattern as the ground truth. The previous statement is strengthened by the results from the Urban100 dataset which consist of many regular patterns from buildings. In Urban100, EDSR has  dB higher than D-DBPN.

Our network shows it's effectiveness in the  enlargement. The D-DBPN outperforms all of the existing methods by a large margin. Interesting results are shown on Manga109 dataset where D-DBPN obtains  dB which is  dB better than EDSR. While on the Urban100 dataset, D-DBPN achieves 23.25 which is only  dB better than EDSR. The results show that our networks perform better on fine-structures images such as manga characters, even though we do not use any animation images in the training.

The results of  enlargement are visually shown in~Fig.~\ref{figure:8x_result}. Qualitatively, D-DBPN is able to preserve the HR components better than other networks. It shows that our networks can extract not only features but also create contextual information from the LR input to generate HR components in the case of large scaling factors, such as  enlargement.



\begin{figure*}
\centering
\includegraphics[width=14.5cm]{8x_result}
\caption{Qualitative comparison of our models with other works on  super-resolution.  line: LapSRN~\cite{LapSRN} (19.77 dB), EDSR~\cite{Lim_2017_CVPR_Workshops} (19.79 dB), and Ours (19.82 dB).  line: LapSRN~\cite{LapSRN} (16.45 dB), EDSR~\cite{Lim_2017_CVPR_Workshops} (19.1 dB), and Ours (23.1 dB).  line: LapSRN~\cite{LapSRN} (24.34 dB), EDSR~\cite{Lim_2017_CVPR_Workshops} (25.29 dB), and Ours (28.84 dB)}
\label{figure:8x_result}
\end{figure*}

\begin{table*}[t!]
\scriptsize
\caption{Quantitative evaluation of state-of-the-art SR algorithms: average PSNR/SSIM for scale factors 2, 4 and 8. {\color{red}Red} indicates the best and {\color{blue}blue} indicates the second best performance. (* indicates that the input is divided into four parts and calculated separately due to computation limitation of Caffe)}
\centering
\label{tab:psnr}
\begin{tabular}{*1l*1c*2c*2c*2c*2c*2c}
\hline\noalign{\smallskip}
\smallskip & &\multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14}& \multicolumn{2}{c}{BSDS100}& \multicolumn{2}{c}{Urban100}&\multicolumn{2}{c}{Manga109} \\         
Algorithm & Scale & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Bicubic&2								&&&&&&&&&&\\
A+~\cite{timofte2014a+}&2					&&&&&&&&&&\\
SRCNN~\cite{dong2016image}&2			&&&&&&&&&&\\
FSRCNN~\cite{dong2016accelerating}&2		&&&&&&&&&&\\
VDSR~\cite{Kim_2016_VDSR}&2			&&&&&&&&&&\\
DRCN~\cite{kim2016deeply}&2				&&&&&&&&&&\\
DRRN~\cite{Tai-DRRN-2017}&2				&&&&&&&&&&\\
LapSRN~\cite{LapSRN}&2					&&&&&&&&&&\\
EDSR~\cite{Lim_2017_CVPR_Workshops}&2	&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{blue}}\\
D-DBPN &2							&{\color{blue}}&{\color{red}}&{\color{blue}}&{\color{red}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{red}}\\
\noalign{\smallskip}\hline\noalign{\smallskip}
Bicubic&4								&&&&&&&&&&\\
A+~\cite{timofte2014a+}&4					&&&&&&&&&&\\
SRCNN~\cite{dong2016image}&4			&&&&&&&&&&\\
FSRCNN~\cite{dong2016accelerating}&4		&&&&&&&&&&\\
VDSR~\cite{Kim_2016_VDSR}&4			&&&&&&&&&&\\
DRCN~\cite{kim2016deeply}&4				&&&&&&&&&&\\
DRRN~\cite{Tai-DRRN-2017}&4				&&&&&&&&&&\\
LapSRN~\cite{LapSRN}&4					&&&&&&&&&&\\
EDSR~\cite{Lim_2017_CVPR_Workshops}&4	&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{red}}&{\color{blue}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{blue}}&{\color{red}}\\
D-DBPN &4							&{\color{red}}&{\color{red}}&{\color{red}}&{\color{blue}}&{\color{red}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{red}}&{\color{blue}}\\
\noalign{\smallskip}\hline\noalign{\smallskip}
Bicubic&8								&&&&&&&&&&\\
A+~\cite{timofte2014a+}&8					&&&&&&&&&&\\
SRCNN~\cite{dong2016image}&8			&&&&&&&&&&\\
FSRCNN~\cite{dong2016accelerating}&8		&&&&&&&&&&\\
VDSR~\cite{Kim_2016_VDSR}&8			&&&&&&&&&&\\
LapSRN~\cite{LapSRN}&8					&&&&&&&&&&\\
EDSR~\cite{Lim_2017_CVPR_Workshops}&8	&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}\\
D-DBPN &8							&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

\section{Conclusion}
We have proposed Deep Back-Projection Networks for Single Image Super-resolution. Unlike the previous methods which predict the SR image in a feed-forward manner, our proposed networks focus to directly increase the SR features using multiple up- and down-sampling stages and feed the error predictions on each depth in the networks to revise the sampling results, then, accumulates the self-correcting features from each upsampling stage to create SR image. We use error feedbacks from the up- and down-scaling steps to guide the network to achieve a better result. The results show the effectiveness of the proposed network compares to other state-of-the-art methods.  Moreover, our proposed network successfully outperforms other state-of-the-art methods on large scaling factors such as  enlargement. 





{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
