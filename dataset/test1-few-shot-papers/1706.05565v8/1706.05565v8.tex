

\documentclass{article}
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure} 

\usepackage{booktabs}

\usepackage{natbib}

\usepackage[utf8]{inputenc}
\usepackage[T1,T5]{fontenc}




\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow,multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mdwlist}


\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\newcommand{\setS}{\mathcal{S}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}









\iclrfinalcopy 

\title{Towards Neural Phrase-based Machine \\ Translation}

\author{Po-Sen Huang, Chong Wang\thanks{Work performed while CW, DZ, and LD were at Microsoft Research and SH was interning at Microsoft Research.}, Sitao Huang\footnotemark[1], Dengyong Zhou\footnotemark[1], Li Deng\footnotemark[1] \\
	Microsoft Research, Google, University of Illinois at Urbana-Champaign, Citadel\\
	{\small\texttt{pshuang@microsoft.com, \{chongw, dennyzhou\}@google.com},}\\ {\small\texttt{shuang91@illinois.edu, l.deng@ieee.org}}
	\\
}

\begin{document} 
\maketitle






















\begin{abstract} 
  In this paper, we present Neural Phrase-based Machine Translation (NPMT).\footnote{The source code is available at \url{https://github.com/posenhuang/NPMT}.} Our
  method explicitly models the phrase structures in output sequences using
  Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence
  modeling method. To mitigate the monotonic alignment requirement of SWAN, we
  introduce a new layer to perform (soft) local reordering of input sequences.
  Different from existing neural machine translation (NMT) approaches, NPMT does
  not use attention-based decoding mechanisms. 
  Instead, it directly outputs phrases in a sequential order and can decode in linear time. 
  Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT
  2015 English-Vietnamese machine translation tasks compared with strong NMT baselines.
  We also observe that our method produces meaningful phrases in output
  languages.
\end{abstract} 

\section{Introduction}
\label{sec:intro}

A word can be considered as a basic unit in languages. However, in many cases, we
often need a phrase to express a concrete meaning.
For example, consider understanding the following sentence, 
``machine learning is a field of computer science''. It may become easier to comprehend if we segment it
as ``[machine learning] [is] [a field of] [computer science]'', where the
words in the bracket `[]' are regarded as ``phrases''.
These phrases have their own meanings,  and can often be reused in other contexts. 

The goal of this paper is to explore the use of phrase structures aforementioned
for neural network-based machine translation
systems~\citep{Sutskever:2014,Bahdanau:2014}.
To this end, we develop a neural machine translation method that explicitly
models phrases in target language sequences. Traditional
phrase-based statistical machine translation (SMT) approaches have been shown to
consistently outperform word-based
ones~\citep{koehn2003statistical,koehn2009statistical,lopez2008statistical}. However, modern neural machine translation (NMT)
methods~\citep{Sutskever:2014,Bahdanau:2014,luong2015effective} do not have an
explicit treatment on phrases, but they still work surprisingly well and have
been deployed to industrial systems~\citep{TACL863,wu2016google}. 
The proposed Neural Phrase-based Machine Translation (NPMT) method tries to
explore the advantages from both kingdoms. It builds upon Sleep-WAke Networks
(SWAN), a segmentation-based sequence modeling technique described
in~\cite{wang2017sequence}, where segments (or phrases) are automatically
discovered given the data. However, SWAN requires monotonic alignments between
inputs and outputs. This is often not an appropriate assumption in many language
pairs. To mitigate this issue, we introduce a new layer to perform (soft) local
reordering on input sequences. Experimental results show that NPMT outperforms
attention-based NMT baselines in terms of the BLEU
score~\citep{papineni2002bleu} on IWSLT 2014 German-English/English-German and
IWSLT 2015 English-Vietnamese translation tasks. We believe our method is one
step towards the full integration of the advantages from neural machine
translation and phrase-based SMT.

This paper is organized as follows. Section~\ref{sec:model} presents the neural
phrase-based machine translation model. Section~\ref{sec:experiment}
demonstrates the usefulness of our approach on several language pairs. We
conclude our work with some discussions in Section~\ref{sec:conclusion}.

\tikzstyle{block} = [rectangle,draw,text width=10em, text centered, rounded corners, minimum height=2em]
\tikzstyle{textblock} = [rectangle, text width=10em, text centered, rounded corners, minimum height=1em]
\tikzstyle{line} = [draw, -latex']

\begin{figure} \centering
	\subfigure[]{\label{fig:npmt_architecture}\begin{tikzpicture}[node distance = 1.1cm, auto]
\node [textblock] (output) {Output sequence}; 
		\node [block, below of = output] (swan){SWAN};
		\node [block, below of=swan] (rnn) {Bi-directional RNN};
		\node [block, below of=rnn] (reordering) {Soft reordering};
		\node [block, below of= reordering] (embedding) {Word embedding};
		\node [textblock, below of = embedding] (input){Input sequence}; 
		
\path [line] (swan) -- (output); 
		\path [line] (rnn) -- (swan);
		\path [line] (reordering) -- (rnn) ;
		\path [line] (embedding) -- (reordering) ;
		\path [line] (input) -- (embedding); 
		\end{tikzpicture}}
  \subfigure[]{\label{fig:npmt_example}\includegraphics[width=0.66\columnwidth]{figures/npmt_example_box}}
  \caption{\small{(a) The overall architecture of NPMT. (b) An illustration of using
  NPMT in German-English translation. Ideally, phrases in the source sentence
  (German) are first reordered. Given the new order, phrases can be translated
  one by one to the target phrases. These translated phrases then compose the
  target sentence (English). Phrase boundaries in the target language are not
  predefined, but automatically discovered by the model. No attention-based
  decoders are used here.}}
	\label{fig:npmt}
\end{figure}




\section{Neural phrase-based machine translation}
\label{sec:model}

We first give an overview of the proposed NPMT architecture and some related
work on incorporating phrases into NMT. We then describe the two key building
blocks in NPMT: 1) SWAN, and 2) the soft reordering layer which alleviates the
monotonic alignment requirement of SWAN. In the context of machine translation,
we use ``segment'' and ``phrase'' interchangeably.

\subsection{The overall architecture of NPMT}
Figure~\ref{fig:npmt_architecture} shows the overall architecture of NPMT. The
input sequence is first turned into embedding representations and then they go
through a (soft) reordering layer (described below in
Section~\ref{sec:reorder}). We then pass these ``reordered'' activations to the
bi-directional RNN layers, which are finally fed into the SWAN layer to directly
output target language in terms of segments (or phrases). While it is possible
to replace bi-directional RNN layers with other
layers~\citep{gehring2017convolutional}, in this paper, we have only explored
this particular setting to demonstrate our proposed idea.

There have been several works that propose different ways to incorporate phrases
into attention-based neural machine translation, such
as~\cite{Tang2016neural,Wang:2017emnlp,dahlmann-EtAl:2017:EMNLP2017}. These
approaches typically use predefined phrases (obtained by external methods, e.g.,
phrase-based SMT) to guide or modify the existing attention-based decoder. The
major difference from our approach is that, in NPMT, we do not use 
attention-based decoding mechanisms, and our phrase structures for the target
language are automatically discovered from the training data. Another line of
related work is the segment-to-segment neural transduction model (SSNT)
~\citep{yu-buys-blunsom:2016:EMNLP2016}, which shows promising results on a
Chinese-to-English translation task under a noisy channel
framework~\citep{yu2016neural}. In SSNT, the segments are implicit, and the
monotonic alignments between the inputs and outputs are achieved using latent
variables. The latent variables are marginalized out during training using dynamic programming.

\subsection{Modeling phrases with SWAN}
\label{sec:swan}
Here we review the SWAN model proposed in~\cite{wang2017sequence}. SWAN defines
a probability distribution for the output sequence given an input sequence. It
models all valid output segmentations of the output sequence as well as the
monotonic alignments between the output segments and the input sequence. Empty
segments are allowed in the output segmentations. It does not make any
assumption on the lengths of input or output sequence. 

Assume input sequence for SWAN is , which is the outputs from bi-directional RNN of Figure \ref{fig:npmt_architecture}, 
and output sequence is . Let
 denote the set containing all valid segmentations of , with
the constraint that the number of segments in a segmentation is the same as the
input sequence length, . Let  denote a segment or phrase in the
target sequence. Empty segments are allowed to ensure that we can correctly
align segment  to input element . Otherwise, we might not have
a valid alignment for the input and output pair. See Figure~\ref{fig:wasm} for
an example of the emitted segmentation of .
\begin{figure}[t]
\begin{center}
\vskip -0.2in
\centerline{\includegraphics[width=0.48\columnwidth]{figures/wasm}}
\vskip -0.2in
\caption{\small{Courtesy to~\citet{wang2017sequence}.  Symbol x_1, \ldots, x_5y_{1:3}=\pi(a_{1:5})\{a_1=\{y_1, \\}, a_3=\{\\},
  a_5=\{\.  Here  wakes (emitting segment ) and  wakes
  (emitting segment ) while ,  and  sleep (emitting empty
  segments ,  and  respectively).}}
\label{fig:wasm}
\end{center}
\end{figure} 
The probability of the sequence  is  defined as the sum of the
probabilities of all the segmentations in ,\footnote{If predefined phrase structure information
is provided for the target language in advance, we can incorporate it into SWAN
by restricting the size of . We leave the exploration of this option as
future work.}
 
where the  is the segment probability given input element ,
which is modeled using a recurrent neural network (RNN) with an additional
softmax layer.  is the concatenation operator and the symbol \pi(\cdot)\. This gives us a segment .
    \end{enumerate}
  \item Concatenate  to obtain the output sequence via
  .
\end{enumerate}
Since there are more than one way to obtain the same  using the
generative process above, the probability of observing  is obtained by summing
over all possible ways, which is Eq.~\ref{eq:model-2}.

Note that  is exponentially large, direct summation quickly becomes
infeasible when  or  is not small. Instead, \citet{wang2017sequence}
developed an exact
dynamic programming algorithm to tackle the computation challenges.\footnote{The
computational complexity of SWAN is still high even with the dynamic programming
algorithm.  This is the reason that it takes a longer time to train our
method for larger datasets such as in WMT translation tasks (weeks for a
moderate model size).  In the meantime, we are actively looking into the
algorithms that can significantly speed up SWAN.} The key idea is that although
the number of possible segmentations is exponentially large, the number of
possible segments is polynomial---. In other words, it is possible to
first compute all possible segment probabilities, , , and then use dynamic programming to calculate the output sequence
probability  in Eq.~\eqref{eq:model-2}. The feasibility of
using dynamic programming is due to a property of segmentations---a segmentation
of a subsequence is also part of the segmentation of the entire sequence. In
practice, a maximum length  for a segment  is enforced to reduce the
computational complexity, since the length of useful segments is often not very
long. 
\cite{wang2017sequence} also discussed a way to carry over information
across segments using a separate RNN, which we will not elaborate here. 
We refer the readers to the original paper for the algorithmic details.

SWAN defines a conditional probability for an output sequence given an input
one. It can be used in many sequence-to-sequence tasks. In practice, a sequence
encoder like a bi-directional RNN can be used to process the raw input sequence
(like speech signals or source language) to obtain  that is to be
passed into SWAN for decoding. For example, \cite{wang2017sequence} demonstrated
the usefulness of SWAN in the context of speech recognition.

Greedy decoding for SWAN is straightforward. We first note that  is
modeled as an RNN with an additional softmax layer. 
Given each , , is independent of each other, we can run the RNN in parallel to produce an output segment (possibly empty) for each . 
We then concatenate these output segments to form the greedy
decoding of the entire output sequence.  
The decoding satisfies the non-autoregressive property \citep{gu2018non-autoregressive} and the decoding complexity is .
See~\cite{wang2017sequence} for the
algorithmic details of the beam search decoder.


We finally note that, in SWAN (thus in NPMT), only output segments are explicit;
input segments are implicitly modeled by allowing empty segments in the
output.  This is conceptually different from the traditional phrase-based SMT
where both inputs and outputs are phrases (or segments). We leave the option of
exploring explicit input segments as future work.

\subsection{Local reordering of input sequences}
\label{sec:reorder}

SWAN assumes a monotonic alignment between the output segments and the input
elements. For speech recognition experiments in~\citet{wang2017sequence}, this
is a reasonable assumption. However, for machine translation, this is usually too
restrictive.  In neural machine translation literature, attention mechanisms were
proposed to address alignment
problems~\citep{Bahdanau:2014,luong2015effective,raffel2017online,vaswani2017attention}.
But it is not clear how to apply a similar attention mechanism to SWAN due to
the use of segmentations for output sequences.

One may note that in NPMT, a bi-directional RNN encoder for the
source language can partially mitigate the alignment issue for SWAN, since it can access
every source word. However, from our empirical studies, it is not enough to obtain the best
performance.  
Here we augment SWAN with a reordering layer that does (soft) local 
reordering of the input sequence.  
This new model leads to promising
results on the IWSLT 2014 German-English/English-German, and IWSLT 2015
English-Vietnamese machine translation tasks. One additional advantage of using
SWAN is that since SWAN does not use attention mechanisms, decoding can be done in parallel with linear complexity, as now we remove the need to query the entire input source for every output
word~\citep{raffel2017online, gu2018non-autoregressive}.

We now describe the details of the local reordering layer shown in
Figure~\ref{fig:reorder_a}. Denote the input to the local reordering layer by
, which is the output from the word embedding layer of Figure \ref{fig:npmt_architecture}, 
and the output of this layer by , which is fed as inputs to the bi-directional RNN of Figure \ref{fig:npmt_architecture}.
We compute  as

where  is the sigmoid function, and  is the local
reordering window size. Notation 
is the concatenation of vectors .
For , notation  is the parameter for the gate function at
position  of the input window. It decides the weight of 
through the gate . The final output  is a weighted linear combination of
the input elements, , in the window
followed by a nonlinear transformation by the  function.



\begin{figure} \centering
	\subfigure[]{\label{fig:reorder_a}\includegraphics[width=0.40\columnwidth]{figures/reordering_center}}
	\subfigure[]{\label{fig:reorder_b}\includegraphics[width=0.29\columnwidth]{figures/reordering_example}}
  \caption{\small{(a) Example of a local reordering layer of window size 5 () to
  compute . Here , , are the
  gates that decides how much information  should accept from those
  elements from this input window. Note that all information available in this
  input window helps decides each gate. (b) An illustration of the reordering
  layer that swaps information between  and  and contributes to 
  and , respectively.}}
	\label{fig:reordering}
\end{figure}

Figure~\ref{fig:reorder_b} illustrates how local reordering works. Here we want
to (softly) select an input element from a window given all information available
in this window.  Suppose we have two adjacent windows,  and
. 
If  gets the largest weight ( is {\it picked}) in the first window and  gets the largest weight ( is {\it picked}) in the second window,
 and  are effectively reordered.
Our layer is different from
the attention
mechanism~\citep{Bahdanau:2014,luong2015effective,raffel2017online,vaswani2017attention}
in following ways.  
First, we do not have a query to begin with as in standard attention mechanisms. 
Second, unlike standard attention, which is top-down from a decoder state to encoder states, the reordering operation is bottom-up. 
Third, the weights  capture the relative positions of the input elements, whereas the weights are the same for different queries and encoder hidden states in the attention mechanism (no positional information). 
The reordering layer performs locally similar to a convolutional layer and the positional information is encoded by a different parameter  for each relative position  in the window. 
Fourth, we do not normalize the weights for the input elements . 
This provides the reordering capability and can potentially turn off everything if needed. 
Finally, the gate of any position  in the reordering window is determined by all input elements  in the window. 
We provide a visualizing example of the reordering layer
gates that performs input swapping in Appendix \ref{sec:reordering_analysis}.

One related work to our proposed reordering layer is the Gated Linear Units
(GLU)~\citep{dauphin2016language} which can control the information flow of the
output of a traditional convolutional layer. But GLU does not have a mechanism
to decide which input element from the convolutional window to choose. From our
experiments, neither GLU nor traditional convolutional layer helped our NPMT.
Another related work to the window size of the reordering layer is the distortion limit in traditional phrase-based statistical machine translation methods \citep{brown1993mathematics}. 
Different window sizes restrict the context of each position to different numbers of neighbors. 
We provide an empirical comparison of different window sizes in Appendix \ref{sec:window_reorder}. 






\section{Experiments}
\label{sec:experiment}

In this section, we evaluate our model on the IWSLT 2014
German-English~\citep{cettolo2014report}, IWSLT 2014 English-German, and IWSLT 2015
English-Vietnamese~\citep{cettolo2015iwslt} machine translation tasks. We note that, in this paper, we
limit the applications of our model to relatively small datasets to demonstrate
the usefulness of our method. We plan to conduct more large scale experiments in
future work.


\subsection{IWSLT14 German-English}
\label{sec:iwslt_de-en}
We evaluate our model on the German-English machine translation track of the IWSLT 2014
evaluation campaign~\citep{cettolo2014report}.
The data comes from translated TED talks, and the dataset contains
roughly 153K training sentences, 7K development sentences, and 7K test
sentences. We use the same preprocessing and dataset splits as in
\citet{DBLP:journals/corr/RanzatoCAZ15,
wiseman2016sequence,Bahdanau2016}. 
The German and English vocabulary sizes are 32,010 and 22,823 respectively.

We report our IWSLT 2014 German-English experiments using one reordering layer
with window size 7, two layers of bi-directional GRU encoder (Gated recurrent unit,
\cite{Chung:2014}) with 256 hidden units, and two layers of unidirectional GRU decoder
with 512 hidden units.  We add dropout with a rate of  in the GRU layer.
We choose GRU since baselines for comparisons were using GRU.  The maximum
segment length is set to 6.  Batch size is set as 32 (per GPU) and the Adam
algorithm~\citep{kingma2014adam} is used for optimization with an initial learning
rate of 0.001.  For decoding, we use greedy search and beam search with a beam
size of 10.  As reported in \citet{bd_rnn_asr,Bahdanau2016}, we find that
penalizing candidate sentences that are too short was required to obtain the
best results. 
We add the middle term of Eq. \eqref{eq:augment_lm} to encourage longer candidate sentences.
All hyperparameters are chosen based on the development set. NPMT takes about 2--3
days to run to convergence (40 epochs) on a machine with four M40 GPUs. The results are summarized
in Table \ref{tab:iwslt_de-en}.  In addition to previous reported baselines in
the literature, we also explored the best hyperparameter using the same model
architecture (except the reordering layer) using sequence-to-sequence model with
attention as reported as LL of Table \ref{tab:iwslt_de-en}. 

NPMT achieves state-of-the-art results on this dataset as far as we know.
Compared to the supervised sequence-to-sequence model, LL~\citep{Bahdanau2016},
NPMT achieves 2.4 BLEU gain in the greedy setting and 2.25 BLEU gain using
beam-search. Our results are also better than those from the actor-critic based
methods in~\citet{Bahdanau2016}. But we note that our proposed method is
orthogonal to the actor-critic method. So it is possible to further improve our
results using the actor-critic method. 


We also run the following two experiments to verify the sources of the gain.  The
first is to add a reordering layer to the original sequence-to-sequence model
with attention, which gives us BLEU scores of 25.55 (greedy) and 26.91 (beam
search). 
Since the attention mechanism and reordering layer capture similar information, adding the reordering layer to the sequence-to-sequence model with attention does not improve the performance.
The second is to remove the reordering layer from NPMT, which gives us
BLEU scores of 27.79 (greedy) and 29.28 (beam search). This shows that the
reordering layer and SWAN are both important for the effectiveness of NPMT.






\begin{table}[t!]
	\centering
	\begin{tabular}{lcc}
		\toprule		
		& \multicolumn{2}{c}{BLEU } \\ 
		&  Greedy & Beam Search \\ 	
		\midrule
MIXER \citep{DBLP:journals/corr/RanzatoCAZ15} & 20.73 & 21.83 \\    
		LL \citep{wiseman2016sequence} & 22.53  & 23.87 \\
		BSO \citep{wiseman2016sequence} & {23.83} & {25.48} \\
		LL \citep{Bahdanau2016} & 25.82  & 27.56 \\
		LL & 26.17  & 27.61 \\
\midrule
		RF-C+LL \citep{Bahdanau2016} & {27.70} & {28.30} \\
AC+LL \citep{Bahdanau2016} & {27.49} & {28.53} \\		
		\midrule


		NPMT (this paper) & \textbf{28.57} & \textbf{29.92} \\				
		NPMT+LM (this paper) & {--} & \textbf{30.08} \\				
		\bottomrule
	\end{tabular}
	\caption{\small{Translation results on the IWSLT 2014 German-English test set. MIXER
		\cite{DBLP:journals/corr/RanzatoCAZ15} uses a  convolutional encoder and
		simpler attention. LL (attention model with log likelihood) and BSO (beam
		search optimization) of \citet{wiseman2016sequence}, and LL, RF-C+LL, and
		AC+LL of \citet{Bahdanau2016} use a one-layer GRU encoder and decoder with
    attention. (RF-C+LL and AC+LL are different settings of actor-critic
    algorithms combined with LL.) LL stands for a well-tuned attention
    model with log likelihood with the same word embedding size, and encoder and
    decoder size as NPMT.}
	}
\label{tab:iwslt_de-en}
\end{table}



\begin{figure}[t!]
	\begin{center}
		\centerline{\includegraphics[width=0.75\columnwidth]{figures/de-en_example}}
		\vskip -0.1in
		\caption{\small{An example of NPMT greedy decoding output for German-English
				translation. The example corresponds to the first example of Table
				\ref{tab:de-en_examples}. Note that for illustrating the input and output
				segments, we do not take into account of the behavior of the reordering
				layer and bi-directional RNN---the index mappings from source to target
				assumes monotonic alignments so some of them might be inaccurate.}}
		\label{fig:de-en_examples}
	\end{center}
	
\end{figure} 

\begin{table*}[h!] \begin{small}
		{
			\begin{center}
				\begin{tabular}{rl}
					\hline


					


					source &  danke , aber das beste kommt noch .				
					\\
					greedy decoding& thank you  ,  but  the best thing  is still coming  .   \\
					target ground truth & thanks . i haven 't come to the best part .				
					\\			
\hline
					source &  sie k\"onnen einen schalter dazwischen einf\"ugen und so haben \\& sie einen kleinen UNK erstellt .\\
					greedy decoding&  you can put  a switch  in between  and  so  they created  a little  UNK . \\
					target ground truth & you can put a knob in between and now you 've made a little UNK .
					\\			
					\hline
source &  sie wollen die entscheidung wirklich richtig treffen , wenn es f\"ur \\& alle ewigkeit ist , richtig ?
					\\
					greedy decoding&  you really want to make  the decision  right  ,  if  it 's  for  \\& all  eternity  ,  right  ?   \\
					target ground truth & you really want to get the decision right if it 's for all eternity , right ?
					\\			
					\hline




source & es gibt zehntausende maschinen rund um die welt die kleine st\"ucke von  \\ & dna herstellen k\"onnen ,  30 bis 50  buchstaben  lang aber  es \\ &ist ein UNK prozess , also je l\"anger man ein st\"uck macht ,\\ & umso mehr fehler passieren .							
					\\
					greedy decoding&   there are   tens of thousands of   machines   around   the world   can make  \\ & little  pieces  of  dna  ,    30  to  50  letters  long  ,  \\ &but  it 's  a more UNK  process  ,  so  the longer  you make  \\ &a piece  ,   the more  mistakes  happen  .    \\
					target ground truth & there are tens of thousands of machines around the world that make small pieces of dna \\ &-- 30 to 50 letters - in length - and it 's a UNK process , \\ &so the longer you make the piece , the more errors there are .								
					\\			
					\hline
				\end{tabular}
			\end{center}
		}
	\end{small}
	\vspace{-2mm}
	\caption{\small{Examples of German-English translation outputs with their
			segmentations. We label the indexes of the words in the source sentence and we
			use those indexes to indicate where the output segment is emitted. For
			example, in greedy decoding results, ``'' denotes -th word in the source sentence emits words
			 during decoding (assuming monotonic
			alignments).  The ``'' represents the segment boundary in the target
			output. See Figure \ref{fig:de-en_examples} for a visualization of row 1 in this table.}}
	\label{tab:de-en_examples}
\end{table*}

In greedy decoding, we can estimate the {\it average segment
length}\footnote{The average segment length is defined as the length of the
output (excluding end of segment symbol \
 Q(y) = \log p(y|x)  + \lambda_1 {\rm
  word\_count}(y)+ \lambda_2 \log p_{\rm lm}(y), 
\label{eq:augment_lm}

where we empirically find that  and  give good
performance, which are tuned on the development set. The results with the external language model are denoted by NPMT+LM in Table \ref{tab:iwslt_de-en}. If no external language
models are used, we set . This scoring function is similar to the
one for speech recognition in~\citet{hannun2014deep}. 





\subsection{IWSLT14 English-German}
\label{sec:iwslt_en-de}
We also evaluate our model on the opposition direction, English-German, which
translates from a more segmented text to a more inflectional one.  Following the
setup in Section \ref{sec:iwslt_de-en}, we use the same dataset with the
opposite source and target languages.  We use the same model architecture,
optimization algorithm and beam search size as the German-English translation
task. NPMT takes about 2--3 days to run to convergence (40 epochs) on a machine with four M40 GPUs.

Given there is no previous sequence-to-sequence attention model baseline for
this setup, we create a strong one and tune hyperparameters on the development
set.  The results are shown in Table \ref{tab:iwslt_en-de}. Based on the
development set, we set  and  in Eq.
\eqref{eq:augment_lm}.  Our model outperforms sequence-to-sequence model with
attention by 2.46 BLEU and 2.49 BLEU in greedy and beam search cases.  We can
also use a 4th-order language model trained using the KenLM
implementation for German target training data, which
further improves the performance.  Some sampled examples are shown in
Table \ref{tab:en-de_examples}.  Several informative segments/phrases can be
found in the decoding results, e.g., ``some time ago''  ``vor
enniger zeit''.

\begin{table}[h]
	\centering
	\begin{tabular}{lcc}
		\toprule		
		& \multicolumn{2}{c}{BLEU } \\ 
		&  Greedy & Beam Search \\ 	
		\midrule
Sequence-to-sequence with attention \cite{} & 21.26  & 22.59 \\
		NPMT (this paper) & \textbf{23.62} & \textbf{25.08} \\	
	    NPMT+LM (this paper) & {--} & \textbf{25.36} \\									
		\bottomrule
	\end{tabular}
	\caption{\small{Translation results on the IWSLT 2014 English-German
		test set.}}
\label{tab:iwslt_en-de}
\end{table}



\begin{table*}[h!] 

	\begin{small}
		{
			\begin{center}
				\begin{tabular}{rl}
					\hline
source &  how would you guys describe your brand ?
					\\
					greedy decoding& wie  w\"urdet  sie  ihre marke  beschreiben ? \\
					target ground truth & wie w\"urdet ihr eure marke beschreiben ?				
					\\			
\hline
					source &  if the museum has given us the image , you click on it .\\
					greedy decoding&  wenn  das museum  uns  das bild  gegeben hat , klicken sie  darauf  . \\
					target ground truth & wenn das museum uns das bild gegeben hat , klicken sie darauf .
					\\			
					\hline
source &  they are frustrated as hell with it , but they 're  not complaining\\& about it , they 're fixing it .
					\\
					greedy decoding&  sie sind  frustriert  ,  aber  sie UNK sich  nicht  \\& dar\"uber  ,   sie reparieren  es  .   \\
					target ground truth & sie sie sind f\"urchterlich frustriert mit ihr , aber sie beschweren sich nicht dar\"uber ,\\& sie reparieren sie . ?\\			
					\hline			
source & now some time ago , if you wanted to win a formula  \\ & 1 race , you  take a budget  ,  and you  bet \\ &your budget on a good driver and a good car .					
					\\
					greedy decoding&   vor einiger zeit   wenn   man   eine formel   gewinnen will ,   ein budget  \\ & und   , dass  ihr budget  auf einem guten  fahrer    und  ein gutes  \\& auto  .\\
					target ground truth & vor einiger zeit war es so , dass wenn sie ein formel 1 rennen gewinnen wollten , \\& dann nahmen sie ihr budget und setzten ihr geld auf einen guten fahrer und ein gutes auto . 					
					\\			
					\hline
				\end{tabular}
			\end{center}
		}
	\end{small}
  \caption{\small{Examples of English-German translation outputs with their
  segmentations.  The meanings of the superscript indexes and the ``''
  symbol are the same as those in Table~\ref{tab:de-en_examples}.}}
  \label{tab:en-de_examples}
\end{table*}

\subsection{IWSLT15 English-Vietnamese}
In this section, we evaluate our model on the IWSLT 2015 English to Vietnamese
machine translation task.
The data is from translated TED talks, and the dataset contains roughly 133K
training sentence pairs provided by the IWSLT 2015 Evaluation
Campaign~\citep{cettolo2015iwslt}.  Following the same preprocessing steps in
\cite{luong_mt_2015,raffel2017online}, we use the TED tst2012 (1553 sentences)
as a validation set for hyperparameter tuning and TED tst2013 (1268 sentences)
as a test set.
The Vietnamese and English vocabulary sizes are 7,709 and 17,191 respectively.

We use one reordering layer with window size 7, two layers of bi-directional LSTM (Long
short-term memory, \cite{Hochreiter:1997}) encoder with 512 hidden units, and
three layers of unidirectional LSTM decoder with 512 hidden units.  We add dropout with a
rate of  in the LSTM layer. We choose LSTM since baselines for comparisons
were using LSTM. The maximum segment length is set to 7.  Batch size is set as 48 (per GPU)
and the Adam algorithm~\cite{kingma2014adam} is used for optimization with an
initial learning rate of 0.001. For decoding, we use greedy decoding and beam
search with a beam size of 10. The results are shown in Table
\ref{tab:iwslt_en-vi}.  Based on the development set, we set  and
 in Eq. \eqref{eq:augment_lm}.  
NPMT takes about one day to run to convergence (15 epochs) on a machine with 4 M40 GPUs. 
Our model outperforms sequence-to-sequence model with attention by 1.41 BLEU and 1.59 BLEU in greedy
and beam search cases.  We also use a 4th-order language model trained using the KenLM
implementation for Vietnamese target training data,
which further improves the BLEU score. Note that our reordering layer relaxes
the monotonic assumption as in \cite{raffel2017online} and is able to decode in
linear time. Empirically we outperform models with monotonic attention.  Table
\ref{tab:en_vi_examples} shows some sampled examples. 
\begin{table}[h!]
	\centering
	\begin{tabular}{lcc}
		\toprule		
		& \multicolumn{2}{c}{BLEU } \\ 
		&  Greedy & Beam Search \\ 	
		\midrule


		Hard monotonic \citep{raffel2017online} & 23.00  & - \\
		\citet{luong_mt_2015} & - &  23.30 \\    
		Sequence-to-sequence model with attention & 25.50 &  26.10 \\  NPMT (this paper) & \textbf{26.91} & \textbf{27.69} \\				
		NPMT+LM (this paper) & -- & \textbf{28.07} \\				
		\bottomrule
	\end{tabular}
  \caption{\small{Translation results on the IWSLT 2015 English-Vietnamese tst2013 test
  set. The result of the sequence-to-sequence model with attention is obtained
  from an open source model provided by the authors.\protect\footnotemark}}
	\label{tab:iwslt_en-vi}
\end{table}


\begin{table*}[th!] \begin{small}
		{
			\begin{center}
				\begin{tabular}{rl}
					\hline
source &  And I figured , this has to stop .
					\\
greedy decoding & V\`a  t\^oi  nh\d\acircumflex n ra r\`\abreve ng  ,  \dj i\`\ecircumflex u n\`ay  ph\h ai d\`\uhorn  ng  l\d ai . \\
target ground truth & V\`a t\^oi nh\d\acircumflex n ra r\`\abreve ng  \dj i\`\ecircumflex u \dj\'o ph\h ai ch\'\acircumflex m d\h\uhorn t . \\	\hline
source &  So great progress and treatment has been made over the years .
					\\
greedy decoding & V\`i v\d \acircumflex y ,  ti\'\ecircumflex n b\d \ocircumflex  v\`a  \dj i\`\ecircumflex u tr\d i  \dj \~a  \dj \uhorn \d \ohorn c   t\d ao ra   trong   nh\~\uhorn ng   \\& n\abreve m  .  \\
target ground truth & Trong su\'\ocircumflex t nh\~\uhorn ng n\abreve m qua \dj \~a c\'o s\d \uhorn~ ti\'\ecircumflex n b\d \ocircumflex ~to l\'\ohorn n trong qu\'a tr\`inh \dj i\`\ecircumflex u tr\d i .
					\\	\hline
				
						
														


source &  The passion that the person has for her own growth is the \\& most  important  thing .
\\
greedy decoding & Ni\`\ecircumflex m \dj am m\^e  r\`\abreve ng  ng\uhorn \`\ohorn
i   c\'o   cho  
s\d \uhorn ~ph\'at tri\h \ecircumflex n  c\h ua c\ocircumflex ~\'\acircumflex y  l\`a  \\& \dj i\`\ecircumflex u  quan tr\d ong   nh\'\acircumflex t  .
\\
target ground truth & 	C\'ai kh\'at v\d ong c\h ua ng\uhorn\`\ohorn i ph\d u n\~\uhorn~c\'o cho s\d \uhorn ~ph\'at tri\h \ecircumflex n c\h ua b\h an th\acircumflex n l\`a th\'\uhorn ~quan tr\d ong nh\'\acircumflex t .
\\	\hline
					


source &  We have eight species of UNK that occur  in Kenya , of which six \\& are highly threatened with  extinction .
				\\
greedy decoding & Ch\'ung ta  c\'o  8  lo\`ai   UNK  x\h ay ra  \h\ohorn~ Kenya  ,~ 6  \\ & b\d i  \dj e do\d a  
				tuy\d \ecircumflex t ch\h ung  .\\
target ground truth &  Ch\'ung ta c\'o 8 lo\`ai k\`\ecircumflex n k\`\ecircumflex n xu\'\acircumflex t hi\d \ecircumflex n t\d ai Kenya , trong \dj\'o c\'o 6 lo\`ai b\d i \dj e do\d a v\'\ohorn i nguy \\ & c\ohorn~ tuy\d \ecircumflex t ch\h ung cao .
				\\	
					
											
					\hline
				\end{tabular}
			\end{center}
		}
	\end{small}
  \caption{\small{Examples of English-Vietnamese translation outputs with their
  segmentations.  The meanings of the superscript indexes and the ``''
  symbol are the same as those in Table~\ref{tab:de-en_examples}.}}
  \label{tab:en_vi_examples}
\end{table*}
\footnotetext{https://github.com/tensorflow/nmt}




\section{Conclusion}
\label{sec:conclusion}
We proposed NPMT, a neural phrase-based machine translation system that models
phrase structures in the target language using SWAN. We also introduced a local
reordering layer to mitigate the monotonic alignment requirement in SWAN.  Our
experimental results showed promising results on IWSLT 2014 German-English,
English-German, and IWSLT 2015 English-Vietnamese machine translation tasks. 
The results suggest that NPMT can potentially be extended to explore the structures in other challenging sequence-to-sequence problems.
In future work, we will explore two directions: 1) speed up NPMT and apply it to larger datasets
and more language pairs; 2) investigate how to learn input and output phrases
simultaneously.



\section{Acknowledgments}
We thank Jacob Devlin, Adith Swaminathan, Frank Seide, Xiaodong He, and anonymous reviewers for their valuable feedback. 

\bibliography{bib}


\bibliographystyle{iclr2018_conference}

\clearpage
\appendix

\section{Reordering Layer Analysis}
\label{sec:reordering_analysis}
To further understand the behavior of the reordering layer, we examine the
values of the gate  in Eq. \eqref{eq:reordering}.  We study the NPMT
English-German model in Section \ref{sec:iwslt_en-de}.  In Figure
\ref{fig:reordering_analysis_en-de}, we show an example that translates from
``can you translate it ?'' to ''k\"onnen man es \"ubersetzen ?'', where the
mapping between words are as follows: ``can  k\"onnen'', ``you
 man'', ``translate  \"ubersetzen'', ``it
 es'' and ``?  ?''.  Note that the example needs to be
reordered from ``translate it'' to ''es \"ubersetzen''.  Each row of Figure
\ref{fig:reordering_analysis_en-de} represents a window of size 7 that is
centered at a source sentence word. The values in the matrix represent	the gate
values for the corresponding words.  The gate values will later be multiplied
with the embedding  of  Eq. \eqref{eq:reordering} and contribute
to the hidden vector .  The y-axis represents the word/phrases emitted from
the corresponding position.  We can observe that the gates mostly focus on the
central word since the first part of the sentence only requires monotonic
alignment. Interestingly, the model outputs ``\'' symbol are within the same group as the next non-'\\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow^*\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow^*\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow\rightarrow$ that can be done   \\ \hline
	\end{tabular}
  \caption{\small{German-English longer phrase mapping results. We show the top 5
	input, output phrase mappings for two categories: input and output phrases
	with three words, and input and output phrases with four words.}}
\end{table}

 
\end{document} 
