
\documentclass{article}

\usepackage{graphicx} \usepackage{subfigure}

\usepackage{pstricks}
\usepackage{tikz}

\usepackage{natbib}

\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2013}

\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}

\newcommand{\kmin}{\mathop{\mathrm{kmin}}}
\newcommand{\kmax}{\mathop{\mathrm{kmax}}}

\renewcommand{\algorithmicforall}{\textbf{for each}}

\icmltitlerunning{Tree-Independent Dual-Tree Algorithms}

\begin{document}

\twocolumn[
\vspace*{-1.8em}
\icmltitle{Tree-Independent Dual-Tree Algorithms}

\vspace*{-1.2em}
\icmlauthor{Ryan R. Curtin}{rcurtin@cc.gatech.edu}
\icmlauthor{William B. March}{march@gatech.edu}
\icmlauthor{Parikshit Ram}{p.ram@gatech.edu}
\icmlauthor{David V. Anderson}{anderson@gatech.edu}
\icmlauthor{Alexander G. Gray}{agray@cc.gatech.edu}
\icmlauthor{Charles L. Isbell, Jr.}{isbell@cc.gatech.edu}
\icmladdress{Georgia Institute of Technology,
            266 Ferst Drive NW, Atlanta, GA 30332 USA}

\icmlkeywords{some keywords}

\vskip 0.05in
]

\begin{abstract}
\vspace*{-0.1em}
{\em Dual-tree algorithms} are a widely used class of branch-and-bound
algorithms.  Unfortunately, developing dual-tree algorithms for use with
different trees and problems is often complex and burdensome.  We
introduce a four-part logical split: the tree, the traversal,
the point-to-point base case, and the pruning rule.  We provide a meta-algorithm
which allows development of dual-tree algorithms in a tree-independent manner
and easy extension to entirely new types of trees.  Representations are provided
for five common algorithms; for -nearest neighbor search, this
leads to a novel, tighter pruning bound. The meta-algorithm also allows
straightforward extensions to massively parallel settings.
\end{abstract}

\vspace*{-2.2em}
\section{Introduction}
\label{sec:intro}
\vspace*{-0.2em}


In large-scale machine learning applications, algorithmic scalability is
paramount.  Hence, much study has been put into fast algorithms for machine
learning tasks.  One commonly-used approach is to build trees on data and then
use branch-and-bound algorithms to minimize runtime.  A popular example of a
branch-and-bound algorithm is the use of the trees for nearest neighbor search,
pioneered by Bentley \yrcite{bentley1975}, and subsequently modified to use two
trees (``dual-tree'') \cite{nbody}.  Later, an optimized tree structure,
the cover tree, was designed \cite{langford2006}, giving provably
linear scaling in the number of queries \cite{ram2009}---a significant
improvement over the quadratically-scaling brute-force algorithm.

Asymptotic speed gains as dramatic as described above are common for dual-tree
branch-and-bound algorithms.  These types of algorithms can be applied to a
class of problems referred to as `-body problems' \cite{nbody}.  The
-point correlation, important in astrophysics, is an -body problem and can
be solved quickly with trees \cite{march2012}.  In addition, Euclidean minimum
spanning trees can be found quickly using tree-based algorithms
\cite{march2010}.  Other dual-tree algorithms include kernel density estimation
\cite{gray2003}, mean shift \cite{wang2007}, Gaussian summation \cite{lee2006},
kernel density estimation, fast singular value decomposition
\cite{holmes2008quic}, range search, furthest-neighbor search, and many others.


The dual-tree algorithms referenced above are each quite similar, but no formal
connections between the algorithms have been established.  The
types of trees used to solve each problem may differ, and in addition, the
manner in which the trees are traversed can differ (depending on the
problem or the tree).  In practice, a researcher may have to
implement entirely separate algorithms to solve the same problems with different
trees; this is time-consuming and makes it difficult to explore the
properties of tree types that make them more suited for particular problems.
Worse yet, parallel dual-tree algorithms are difficult to
develop and appear to be far more complex than serial implementations; yet, both
solve the same problem.  We make these contributions to address these
shortcomings:


\begin{itemize}
  \vspace*{-1em}
  \item A {\bf representation} of dual-tree algorithms as {\bf four separate
components}: a space tree, a traversal, a base case, and a pruning rule.
  \vspace*{-0.5em}
  \item A {\bf meta-algorithm} that produces dual-tree algorithms, given those
four separate components.
  \vspace*{-1.5em}
  \item Base cases and pruning rules for {\bf a variety of dual-tree
algorithms}, which can be used with \textbf{any} space tree and \textbf{any}
traversal.
  \vspace*{-0.5em}
  \item A {\bf theoretical framework}, used to prove the correctness of these
meta-algorithms and develop a new, tighter bound for -nearest neighbor
search.
  \vspace*{-0.5em}

  \item Implications of our representation, including {\bf easy creation of
large-scale distributed dual-tree algorithms} via our meta-algorithm.


\end{itemize}


\section{Overview of Meta-Algorithm}
\label{sec:overview}

In other works, dual-tree algorithms are described as standalone algorithms that
operate on a query dataset  and a reference dataset .  By observing
commonalities in these algorithms, we propose the following logical split of any
dual-tree algorithm into four parts:

\vspace*{-0.7em}
\begin{itemize}
  \item A \textit{space tree} (a type of data structure).

  \item A \textit{pruning dual-tree traversal}, which visits nodes in two space
trees, and is parameterized by a \texttt{BaseCase()} and a \texttt{Score()}
function.

  \item A \texttt{BaseCase()} function that defines the action to take on a
combination of points.

  \item A \texttt{Score()} function that determines if a subtree should be
visited during the traversal.
\end{itemize}
\vspace*{-0.7em}

We can use this to define a {\bf meta-algorithm}:

\vspace*{-0.7em}
\begin{quote}
\textit{Given a type of space tree, a pruning dual-tree traversal, a
}\texttt{BaseCase()}\textit{ function, and a }\texttt{Score()}\textit{ function,
use the pruning dual-tree traversal with the given }\texttt{BaseCase()}\textit{
and }\texttt{Score()}\textit{ functions on two space trees 
(built on ) and  (built on ).}
\end{quote}
\vspace*{-0.7em}

In Sections \ref{sec:trees} and \ref{sec:traversers}, space trees, traversals,
and related quantities are rigorously defined.  Then, Sections
\ref{sec:knn}--\ref{sec:kde} define \texttt{BaseCase()} and \texttt{Score()}
for various dual-tree algorithms.  Section \ref{sec:discussion} discusses
implications and future possibilities, including large-scale parallelism.

\section{Space Trees}
\label{sec:trees}

To develop a framework for understanding dual-tree algorithms, we must
introduce some terminology.

\begin{defn}
\label{def:spacetree}
A \textbf{space tree} on a dataset  is an undirected,
connected, acyclic, rooted simple graph with the following properties:

\vspace*{-0.5em}
\begin{itemize}
  \item Each \textit{node} (or vertex), holds a number of points (possibly
zero) and is connected to one parent node and a number of child nodes (possibly
zero).
  \item There is one node in every space tree with no parent; this
is the \textit{root node} of the tree.
  \item Each point in  is contained in at least one node of the
tree.
  \item Each node  of the tree has a convex subset of
 that contains each of the points in that node as well as the convex
subsets represented by each child of the node.
\end{itemize}
\end{defn}

\vspace*{-0.6em}
Notationally, we use the following conventions:
\vspace*{-0.5em}

\begin{itemize}
  \item The set of child nodes of a node  is denoted
 or .
  \item The set of points held in a node  is denoted
 or .
  \item The convex subset represented by node  is denoted
 or .
  \item The set of descendant nodes of a node , denoted
 or , is the set of nodes
 .
  \item The set of descendant points of a node , denoted
 or , is the set of points .
  \item The parent of a node  is denoted
.
\end{itemize}

\vspace*{-0.6em}

\begin{figure}[t!]
  \centering
  \subfigure[Abstract representation.]{

\begin{tikzpicture}[>=latex,line join=bevel,scale=0.6]
  \pgfsetlinewidth{1bp}
\pgfsetcolor{black}
\draw [thin] (70.107bp,72.571bp) .. controls (64.745bp,63.992bp) and (58.173bp,53.476bp)  .. (46.793bp,35.269bp);
\draw [thin] (81bp,143.83bp) .. controls (81bp,136.13bp) and (81bp,126.97bp)  .. (81bp,108.41bp);
\draw [thin] (91.893bp,72.571bp) .. controls (97.255bp,63.992bp) and (103.83bp,53.476bp)  .. (115.21bp,35.269bp);
\begin{scope}
  \definecolor{strokecol}{rgb}{0.0,0.0,0.0};
  \pgfsetstrokecolor{strokecol}
  \draw [thin] (126bp,18bp) ellipse (42bp and 18bp);
  \draw (126bp,18bp) node {};
\end{scope}
\begin{scope}
  \definecolor{strokecol}{rgb}{0.0,0.0,0.0};
  \pgfsetstrokecolor{strokecol}
  \draw [thin] (36bp,18bp) ellipse (42bp and 18bp);
  \draw (36bp,18bp) node {};
\end{scope}
\begin{scope}
  \definecolor{strokecol}{rgb}{0.0,0.0,0.0};
  \pgfsetstrokecolor{strokecol}
  \draw [thin] (81bp,162bp) ellipse (60bp and 18bp);
  \draw (81bp,162bp) node {};
\end{scope}
\begin{scope}
  \definecolor{strokecol}{rgb}{0.0,0.0,0.0};
  \pgfsetstrokecolor{strokecol}
  \draw [thin] (81bp,90bp) ellipse (60bp and 18bp);
  \draw (81bp,90bp) node {};
\end{scope}
\end{tikzpicture}
     \label{fig:sptree_abstract}
  }
  \subfigure[ representation.]{
    \begin{tikzpicture}
\coordinate (Origin) at (0, 0);
  \coordinate (XAxisMin) at (0, 0);
  \coordinate (XAxisMax) at (3, 0);
  \coordinate (YAxisMin) at (0, 0);
  \coordinate (YAxisMax) at (0, 4);
  \draw [thin, gray, -latex] (XAxisMin) -- (XAxisMax);
  \draw [thin, gray, -latex] (YAxisMin) -- (YAxisMax);

\filldraw[fill=gray] (0.25, 0.15) -- (0.25, 3.25) -- (2.75, 3.25) -- (2.75, 0.15) -- cycle;
  \node [ ] at (3.1, 0.35) {  };

\filldraw[fill=lightgray] (1, 2) -- (1, 3) -- (2, 3) -- (2, 2) -- cycle;
  \node [ ] at (2.3, 2.5) {  };

\node [draw, circle, inner sep=1pt, fill] at (2, 2) { };
  \node [ ] at (2.2, 1.8) {  };

  \node [draw, circle, inner sep=1pt, fill] at (1, 2.5) { };
  \node [ ] at (1.2, 2.3) {  };

  \node [draw, circle, inner sep=1pt, fill] at (0.5, 0.5) { };
  \node [ ] at (0.7, 0.3) {  };

  \node [draw, circle, inner sep=1pt, fill] at (1, 3) { };
  \node [ ] at (1.2, 2.8) {  };

  \node [draw, circle, inner sep=1pt, fill] at (1.5, 2.5) { };
  \node [ ] at (1.7, 2.3) {  };

\end{tikzpicture}
     \label{fig:sptree_r2}
  }
  \vspace*{-1em}
  \caption{An example space tree.}
  \label{fig:sptree}
  \vspace*{-0.7em}
\end{figure}

An abstract representation of an example space tree on a five-point dataset in
 is shown in Figure~\ref{fig:sptree_abstract}.  In this illustration,
 is the root node of the tree; it has no parent and it contains
the points  and  (that is, .  The node
 contains points  and  and has children 
and  (which each have no children and contain points  and
, respectively).  In Figure~\ref{fig:sptree_r2}, the points in the tree and
the subsets  (darker rectangle) and  (lighter
rectangle) are plotted.  and  are not labeled.

\vspace*{0.5em}
\begin{defn}
The \textbf{minimum distance} between two nodes  and
 is defined as

\vspace*{-1.8em}

\end{defn}

\begin{defn}
The \textbf{maximum distance} between two nodes  and
 is defined as

\vspace*{-1.8em}

\end{defn}

\begin{defn}
The \textbf{maximum child distance} of a node  is defined as the
maximum distance between the centroid  of  and each point in
:

\vspace*{-1.2em}

\end{defn}

\begin{defn}
The \textbf{maximum descendant distance} of a node  is defined as
the maximum distance between the centroid  of  and points in :

\vspace*{-1.3em}

\end{defn}
\vspace*{-1.0em}



It is straightforward to show that -trees, octrees, metric trees, ball
trees, cover trees \cite{langford2006}, R-trees, and vantage-point trees all
satisfy the conditions of a space tree.  The quantities , ,
, and  are easily derived (or
bounded, which in many cases is sufficient) for each of these types of trees.

\vspace*{-0.2em}
For a -tree,  is bounded below by
the minimum distance between  and ;
 is bounded above similarly by the
maximum distance between  and .  Both
 and  are bounded above by
.


\vspace*{-0.2em}
For the cover tree \cite{langford2006}, each node  contains only
one point  and has `scale' .  
is bounded below by  and
 is bounded above by .  Because  is the centroid of ,
.   is simply .

\vspace*{-0.5em}
\section{Tree Traversals}
\label{sec:traversers}

In general, the nodes of each space tree can be traversed in a number of ways.
However, there have been no attempts to formalize tree traversal.  Therefore, we
introduce several definitions which will be useful later.

\vspace*{0.4em}
\begin{defn}
A \textbf{single-tree traversal} is a process that, given a space tree, will
visit each node in that tree once and perform a computation on any points
contained within the node that is being visited.
\end{defn}

As an example, the standard depth-first traversal or breadth-first traversal
are single-tree traversals.  From a programming perspective, the computation in
the single-tree traversal can be implemented with a simple callback
\texttt{BaseCase(point)} function.  This allows the computation to be entirely
independent of the single-tree traversal itself.  As an example, a simple
single-tree algorithm to count the number of points in a given tree would
increment a counter variable each time \texttt{BaseCase(point)} was called.
However, this concept by itself is not very useful; without pruning branches, no
computations can be avoided.

\vspace*{0.4em}
\begin{defn}
A \textbf{pruning single-tree traversal} is a process that, given a space tree,
will visit nodes in the tree and perform a computation to assign a score to that
node.  If the score is above some bound, the node is ``pruned'' and none of its
descendants will be visited; otherwise, a computation is performed on any points
contained within that node.  If no nodes are pruned, then the traversal will
visit each node in the tree once.
\end{defn}
\vspace*{-0.3em}

Clearly, a pruning single-tree traversal that does not prune any nodes is a
single-tree traversal.  A pruning single-tree traversal can be implemented with
two callbacks: \texttt{BaseCase(point)} and \texttt{Score(node)}.  This allows
both the point-to-point computation and the scoring to be entirely independent
of the traversal.  Thus, single-tree branch-and-bound algorithms can be
expressed in a tree-independent manner.  Extensions to the dual-tree case are
given below.

\vspace*{0.4em}
\begin{defn}
A \textbf{dual-tree traversal} is a process that, given two space trees
 (query tree) and  (reference tree), will visit
every combination of nodes  once, where
 and .  At
each visit , a computation is performed between
each point in  and each point in .
\end{defn}


\vspace*{0.2em}
\begin{defn}
\label{def:dtpt}
A \textbf{pruning dual-tree traversal} is a process which, given two space
trees  (query tree) and  (reference
tree), will visit combinations of nodes  such
that  and  no
more than once, and perform a computation to assign a score to that combination.
If the score is above some bound, the combination is pruned and no combinations
 such that  and  will be visited;
otherwise, a computation is performed between each point in  and
each point in .
\end{defn}
\vspace*{-0.3em}

Similar to the pruning single-tree traversal, a pruning dual-tree algorithm can
use two callback functions \texttt{BaseCase(, )} and
\texttt{Score(, )}.  An example implementation of
a depth-first pruning dual-tree traversal is given in
Algorithm \ref{alg:depth_traversal}.  The traversal is started on the root of
the  and the root of .

Algorithm \ref{alg:depth_traversal} provides only one example of a commonly-used
pruning dual-tree traversal.  Other possibilities not explicitly documented here
include breadth-first traversals and the unique cover tree dual-tree traversal
described by Beygelzimer et al. \yrcite{langford2006}, which can be adapted to
the generic space tree case.

\begin{algorithm}[tb]
\begin{algorithmic}
    \STATE \textbf{if} \texttt{Score(, ) } \textbf{then return}
    \medskip
    \FORALL{}
      \STATE \texttt{BaseCase(, )}
    \ENDFOR
    \medskip
    \FORALL{}
      \STATE \texttt{DepthFirstTraversal(,
)}
    \ENDFOR
  \end{algorithmic}

  \caption{\texttt{DepthFirstTraversal(, )}.}
  \label{alg:depth_traversal}
\end{algorithm}

The rest of this work is devoted to using these concepts to represent existing
dual-tree algorithms in the four parts described in Section \ref{sec:overview}.



\section{-Nearest Neighbor Search}
\label{sec:knn}


-nearest neighbor search is a well-studied problem with a plethora of
algorithms and results.  The problem can be stated as follows:

Given a query dataset , a reference dataset , and an integer , for each point ,
find the  nearest neighbors in  and their distances from .  The
list of nearest neighbors for a point  can be referred to as  and
the distances to nearest neighbors for  can be referred to as .
Thus, the -th nearest neighbor to point  is  and .

This can be solved using a brute-force approach: compare every possible point
combination and store the  smallest distance results for each .
However, this scales poorly -- ; hence the importance of fast algorithms
to solve the problem.  Many existing algorithms employ tree-based
branch-and-bound strategies \cite{langford2006, cover1967, friedman1977,
fukunaga1975, nbody, ram2009}.

We unify all of these branch-and-bound strategies by defining methods
\texttt{BaseCase(, )} and \texttt{Score(,
)} for use with a pruning dual-tree traversal.

At the initialization of the tree traversal, the lists  and 
are empty lists for each query point .  After the traversal is complete,
for a query point , the set  is the
ordered set of  nearest neighbors of the point , and each .  If we assume that  if  is
greater than the length of , we can formulate \texttt{BaseCase()} as
given in Algorithm \ref{alg:knn_base_case}\footnote{
In practice, -nearest-neighbors is often run with identical reference and
query sets.  In that situation it may be useful to modify this implementation of
\texttt{BaseCase()} so that a point does not return itself as the nearest
neighbor (with distance 0).}.

With the base case established, only the pruning rule remains.  A valid pruning
rule will, for a given query node  and reference node
, prune the reference subtree rooted at  if
and only if it is known that there are no points in  that are
in the set of  nearest neighbors of any points in .
Thus, at any point in the traversal, we can prune the combination
 if and only if , where

\vspace*{-2em}

\vspace*{-2em}

Now, we can describe this bound recursively.  This is important for
implementation; a recursive function can cache previous calculations for large
speedups.

\vspace*{-1em}

\vspace*{-2em}

\begin{algorithm}[tb]
\begin{algorithmic}
    \STATE {\bfseries Input:} query point , reference point , list of
 nearest candidate points  and  candidate distances 
(both ordered by ascending distance)
    \STATE {\bfseries Output:} distance  between  and 

    \medskip
    \STATE 
    \medskip

    \IF{ \AND \texttt{BaseCase(, )} not yet called}
    \STATE  insert  into ordered list  and truncate list to length

    \STATE  insert  into  such that  is ordered by
distance and truncate list to length 
    \ENDIF

    \RETURN 
  \end{algorithmic}

  \caption{-nearest-neighbors \texttt{BaseCase()}}
  \label{alg:knn_base_case}
\end{algorithm}

Suppose we have, at some point in the traversal, two points  for some node , with
 and .  This means there exist  points  in
 such that  for .
Because , we can apply the triangle inequality to
see that .  Therefore,  for . Using
this observation we can construct an alternate bound function
:



which can, like , be rearranged to provide a recursive
definition.  In addition, if  and , we can bound  more tightly with
 instead of .  These observations yield



Both  and  provide valid pruning rules.
We can combine both to get a tighter pruning rule by taking the tighter
of the two bounds.  In addition,  and
 for all .  Therefore, we can prune  if


\newpage
These observations are combined for a better bound:

\vspace*{-1.3em}

\vspace*{-0.3em}

As a result of this bound function being expressed recursively, previous bounds
can be cached and used to calculate the bound  quickly.  We
can use this to structure \texttt{Score()} as given in Algorithm
\ref{alg:knn_score}.

\begin{algorithm}[tb]
  \begin{algorithmic}
    \STATE {\bfseries Input:} query node , reference node 
    \STATE {\bfseries Output:} a score for the node combination , or  if the combination should be pruned

    \medskip

    \IF{}
      \RETURN 
    \ENDIF

    \medskip
    \RETURN 
  \end{algorithmic}

  \caption{-nearest-neighbors \texttt{Score()}}
  \label{alg:knn_score}
\end{algorithm}

Applying the meta-algorithm in Section \ref{sec:overview} with any tree type and
any pruning dual-tree traversal gives a correct implementation of -nearest
neighbor search.  Proving the correctness is straightforward; first, a (non-pruning) dual-tree traversal which uses \texttt{BaseCase()} as
given in Algorithm \ref{alg:knn_base_case} will give correct results for any
space tree.  Then, we already know that  is a bounding
function that, at any point in the traversal, will not prune any subtrees which
could contain better nearest neighbor candidates than the current candidates.
Thus, the true nearest neighbors for each query point will always be visited,
and the results will be correct.







We now show that this algorithm is a generalization of the standard
-tree -NN search, which uses a pruning dual-tree depth-first traversal.
The archetypal algorithm for all-nearest neighbor search (-nearest neighbor
search with ) given for -trees in Alex Gray's Ph.D. thesis
\yrcite{gray2003phd} is shown here in Algorithm \ref{alg:gray_knn} with
converted notation.   is the bound for a node  and
is initialized to ;  represents the nearest distance for a
query point , and  represents the nearest neighbor for a query
point .  .left represents the left child of 
and is defined to be  if  has no children;
.right is similarly defined.

The structure of the algorithm matches Algorithm \ref{alg:depth_traversal}; it
is a dual-tree depth-first recursion.  Because this is a depth-first recursion,
 for a node  if no descendants of
 have been recursed into.  Otherwise,  is the
maximum of  for all .  That is, .  Thus, the comparison in the first line of Algorithm
\ref{alg:gray_knn} is equivalent to Algorithm \ref{alg:knn_score} with
 instead of .


The first two lines of the inside of the \textbf{for each} loop (the base case)
are equivalent to Algorithm \ref{alg:knn_base_case} with .  -trees
only hold points in leaves; therefore, the base case is called for all
combinations of points in each node combination, identically to the depth-first
traverser (Algorithm \ref{alg:depth_traversal}).

A -tree is a space tree and the dual depth-first recursion is a pruning
dual-tree traversal.  Also, we showed the equivalency of the pruning rule (that
is, the \texttt{Score()} function) and the equivalency of the base case.  So, it
is clear that Algorithm \ref{alg:gray_knn} is produced using our meta-algorithm
with these parameters.  In addition, because  is always less
than , Algorithm \ref{alg:knn_score} provides a {\bf tighter
bound} than the pruning rule in Algorithm \ref{alg:gray_knn}.

\begin{algorithm}[tb]
  \begin{algorithmic}
    \STATE \textbf{if} , \textbf{then return}
    \IF{ is leaf \AND  is leaf}
      \FORALL{}
        \STATE .
        \STATE \textbf{if}  \textbf{then} ; 
        \STATE \textbf{if}  \textbf{then} 
      \ENDFOR
    \ENDIF
    \STATE \texttt{AllNN(.}left, closer-of.left,
.right\texttt{)}
    \STATE \texttt{AllNN(.}left, farther-of.left,
.right\texttt{)}
    \STATE \texttt{AllNN(.}right, closer-of.left,
.right\texttt{)}
    \STATE \texttt{AllNN(.}right, farther-of.left, .right\texttt{)}
    \STATE 
  \end{algorithmic}

  \caption{\texttt{AllNN(, )} \cite{gray2003phd}}
  \label{alg:gray_knn}
\end{algorithm}

This algorithm is also a generalization of the standard cover tree -NN search
\cite{langford2006}.  The cover tree search is a pruning dual-tree
traversal where the query tree is traversed depth-first while the reference
tree is simultaneously traversed breadth-first.  The pruning rule (after
simple adaptation to the -nearest-neighbor search problem instead of the
nearest-neighbor search problem) is equivalent to

\vspace*{-0.8em}

\vspace*{-1.8em}

where  is the point contained in  (remember, each node of a
cover tree contains one point).  This is equivalent to 
because  for cover trees.  The transformation from the
algorithm given by Beygelzimer et al.  \yrcite{langford2006} to our
representation is made clear in Appendix A (supplementary material) and in the
-nearest neighbor search implementation of the C++ library MLPACK
\cite{curtin2011}; this is implemented in terms of our meta-algorithm.

\vspace*{-0.1em}
Specific algorithms for ball trees, metric trees, VP trees, octrees, and other
space trees are trivial to create using the \texttt{BaseCase()} and
\texttt{Score()} implementation given here (and in MLPACK).  Note also that this
implementation will work in any metric space.

\vspace*{-0.1em}
An extension to -furthest neighbor search is straightforward.  The bound
function must be `inverted' by changing `max' to `min' (and vice versa); in
addition, the distances  must be initialized to  instead of
, and the lists  and  must be sorted by descending distance
instead of ascending distance.  Lastly, the comparison  must be
changed to .  With these simple changes, we have solved an
entirely different problem using our meta-algorithm with very little effort.  A
-furthest neighbor search using our meta-algorithm for both -trees and
cover trees is also available in MLPACK.


\vspace*{-0.5em}
\section{Range Search}
\label{sec:range}

\vspace*{-0.1em}
Range search is another popular neighbor searching problem related to
-nearest neighbor search.  In addition to being a fairly standard machine
learning task, it has numerous uses in applications such as databases and
geographic information systems (GIS).  A treatise on the history of the problem
and solutions is given by Agarwal \& Erickson \yrcite{agarwal1999}.  The problem
is:

\vspace*{-0.1em}
Given query and reference datasets  and a range , for each point , find all points in  such that
.  As with -nearest neighbor
search, refer to the list of neighbors for each query point  as 
and the corresponding distances as .  These lists are not sorted in any
particular order, and at initialization time, they are empty.

\vspace*{-0.1em}
In different settings, the problem of range search may not be stated
identically; however, our results are easily adaptable.  A \texttt{BaseCase()}
implementation is given in Algorithm \ref{alg:rs_base_case}, and a
\texttt{Score()} implementation is given in Algorithm \ref{alg:rs_score}.  The
only bounds to consider are , so no complex bound handling
is necessary.

\vspace*{-0.1em}
While range search is sometimes mentioned in the context of
dual-tree algorithms \cite{nbody}, the focus is usually on -nearest neighbor
search.  As a result, we cannot find any explicitly published dual-tree
algorithms to generalize; however, a single-tree algorithm was proposed by
Bentley and Friedman \yrcite{bentley1979data}.  Thus, the \texttt{BaseCase()}
and \texttt{Score()} proposed here can be used with our meta-algorithm to
produce entirely novel range search implementations; MLPACK has -tree and
cover tree implementations.

\begin{algorithm}[tb]
  \begin{algorithmic}
    \STATE \textbf{Input:} query point , reference point , neighbor list
, distance list 
    \STATE \textbf{Output:} distance  between  and 
    \medskip
    \STATE 
    \medskip
     \IF{ \AND \texttt{BaseCase(, )} not
yet called}
       \STATE 
       \STATE 
     \ENDIF
     \RETURN 
  \end{algorithmic}
  \caption{Range search \texttt{BaseCase()}.}
  \label{alg:rs_base_case}
\end{algorithm}

\begin{algorithm}[tb]
  \begin{algorithmic}
    \STATE \textbf{Input:} query node , reference node

    \STATE \textbf{Output:} a score for , or
 if the combination should be pruned
    \medskip
    \IF{}
      \RETURN 
    \ENDIF
    \RETURN 
  \end{algorithmic}
  \caption{Range search \texttt{Score()}.}
  \label{alg:rs_score}
\end{algorithm}


























\vspace*{-2em}
\section{Bor\r{u}vka's Algorithm}
\label{sec:dtb}


\vspace*{-0.3em}
Finding a Euclidean minimum spanning tree has been a relevant problem since
Bor\r{u}vka's algorithm was proposed in 1926.  Recently, a dual-tree
version of Bor\r{u}vka's algorithm was developed \cite{march2010} for -trees
and cover trees.  We unify these two algorithms and generalize to other types of
space tree by formulating \texttt{BaseCase()} and \texttt{Score()} functions.

\vspace*{-0.3em}
For a dataset , Bor\r{u}vka's algorithm connects each
point to its nearest neighbor, giving many `components'.  For each component
, the nearest point in  to any point of  that is not part of  is
found.  The points are connected, combining those components.  This process
repeats until only one component---the minimum spanning tree---remains.

\vspace*{-0.3em}
During the algorithm, we maintain a list  made up of  components  where  is the list of edges and  is the list of vertices
in the component  (these are points in ).  Each point in  belongs
to only one .  At initialization,  and  for , where  is the
'th point in .  For  we define  if  is the
component containing .  During the algorithm, we maintain  as the
candidate nearest neighbor of component  and  as the point in
component  nearest to .  Then, .
Remember that .

\begin{algorithm}[t!]
\begin{algorithmic}
    \STATE {\bfseries Input:} query point , reference point , nearest
candidate point  and distance 
    \STATE {\bfseries Output:} distance  between  and 

    \medskip
    \IF{}
      \RETURN 
    \ENDIF
    \IF{ \AND }
      \STATE 
      \STATE ;  
    \ENDIF
    \RETURN 
  \end{algorithmic}

  \caption{Bor\r{u}vka's algorithm \texttt{BaseCase()}.}
  \label{alg:dtb_base_case}
\end{algorithm}

\begin{algorithm}[t]
  \begin{algorithmic}
    \STATE {\bfseries Input:} query node , reference node 
    \STATE {\bfseries Output:} a score for the node combination , or  if the combination should be pruned

    \medskip

    \IF{}
      \IF{}
        \RETURN 
      \ENDIF
      \RETURN 
    \ENDIF

    \RETURN 
  \end{algorithmic}

  \caption{Bor\r{u}vka's algorithm \texttt{Score()}.}
  \label{alg:dtb_score}
\end{algorithm}

\vspace*{-0.4em}
To run Bor\r{u}vka's algorithm with a space tree  built on the
set of points , a pruning dual-tree traversal is run with
\texttt{BaseCase()} as Algorithm \ref{alg:dtb_base_case},
\texttt{Score()} as Algorithm \ref{alg:dtb_score},  as
\textit{both} of the trees, and  as initialized before.  Note that
\texttt{Score()} uses  from Section \ref{sec:knn} with .  Upon traversal completion, we have a list  of nearest neighbors
of each component .  The edge  is added to  for
each .  Then, any components in  with shared edges are merged into a new
list  where .  The pruning dual-tree traversal is then run again
with  and the traversal-merge process repeats until .  When
, then  is the minimum spanning tree of .

\vspace*{-0.4em}
To prove the correctness of the meta-algorithm, see Theorem 4.1 in March et~al.
\yrcite{march2010}.  That proof can be adapted from -trees to
general space trees.  Our representation is a generalization of their
algorithms; our meta-algorithm to produces their -tree and cover
tree implementations with a tighter distance bound .  Our
meta-algorithm produces a provably correct dual-tree algorithm with any type of
space tree.











\vspace*{-0.7em}
\section{Kernel Density Estimation}
\label{sec:kde}

\vspace*{-0.3em}
Much work has been produced regarding the use of dual-tree algorithms
for kernel density estimation (KDE), including by Gray \& Moore \yrcite{nbody,
gray2003nonparametric} and later by Lee et al. \yrcite{lee2005, lee2008fast}.
KDE is an important machine learning problem with a vast range of applications,
such as signal processing to econometrics.  The
problem is, given query and reference sets , to estimate a probability
density  at each point  using each point  and a
kernel function .  The exact probability density at a point  is the
sum of  for all .

\begin{algorithm}[b]
  \begin{algorithmic}
    \STATE \textbf{Input:} query node , reference node

    \STATE \textbf{Output:} a score for  or
 if the combination should be pruned
    \medskip
    \IF{}
      \FORALL{}
        \STATE 
      \ENDFOR
      \RETURN 
    \ENDIF
    \RETURN 
  \end{algorithmic}
  \caption{KDE \texttt{Score(, )}.}
  \label{alg:kde_score}
\end{algorithm}

\begin{algorithm}[b!]
  \begin{algorithmic}
    \STATE \textbf{Input:} query point , reference point , density
estimate 
    \STATE \textbf{Output:} distance between  and 
    \medskip
    \STATE \textbf{if} \texttt{BaseCase(, )} already called
\textbf{then return}
    \STATE 
    \RETURN 
  \end{algorithmic}
  \caption{KDE \texttt{BaseCase(, )}.}
  \label{alg:kde_base_case}
\end{algorithm}

In general, the kernel function is some zero-centered probability density
function, such as a Gaussian.  This means that when  is very
large, the contribution of  to  is very small.  Therefore, we can
approximate small values using a dual-tree algorithm to avoid unnecessary
computation; this is the idea set forth by Gray \& Moore \yrcite{nbody}.
Because  is a function which is decreasing with distance, the maximum
difference between  values for a given combination  can be bounded above with

\vspace*{-2.3em}

\vspace*{-2.3em}

The algorithm takes a parameter ; when  is less than , the kernel
values are approximated using the kernel value of the centroid  of the
reference node.  The division by  ensures that the total approximation
error is bounded above by .  The base case on  and  merely
needs to add  to the existing density estimate .
When the algorithm is initialized,  for all query points.
\texttt{BaseCase()} is Algorithm \ref{alg:kde_base_case} and
\texttt{Score()} is Algorithm \ref{alg:kde_score}.


\vspace*{-0.3em}
Again we emphasize the flexibility of our meta-algorithm.  To our knowledge
cover trees, octrees, and ball trees have never been used to perform
KDE in this manner.  Our meta-algorithm can produce these implementations with
ease.


\vspace*{-1.1em}
\section{Discussions}
\label{sec:discussion}

\vspace*{-0.2em}
We have now shown five separate algorithms for which we have taken existing
dual-tree algorithms and constructed a \texttt{BaseCase()} and \texttt{Score()}
function that can be used with any space tree and any dual-tree traversal.
Single-tree extensions of these four methods are straightforward
simplifications.

This modular way of viewing tree-based algorithms has several useful immediate
applications.  The first is implementation.  Given a tree implementation and a
dual-tree traversal implementation, all that is required is \texttt{BaseCase()}
and \texttt{Score()} functions.  Thus, code reuse can be maximized, and new
algorithms can be implemented simply by writing two new functions.  More
importantly, the code is now modular.  MLPACK \cite{curtin2011}, written in C++,
uses templates for this.  One example is the \texttt{DualTreeBoruvka} class,
which implements the meta-algorithm discussed in Section \ref{sec:dtb}, and has
the following arguments:

\vspace*{-0.7em}
\begin{verbatim}
    template<typename MetricType,
             typename TreeType,
             typename TraversalType>
    class DualTreeBoruvka;
\end{verbatim}
\vspace*{-0.6em}

This means that any class satisfying the constraints of the \texttt{TreeType}
template parameter can be designed without any consideration or knowledge of the
\texttt{DualTreeBoruvka} class or of the \texttt{TraversalType} class; it is
\textit{entirely independent}.  Then, assuming a \texttt{TreeType} and
\texttt{TraversalType} without bugs, the dual-tree Bor\r{u}vka's algorithm is
guaranteed to work.  An immediate example of the advantage of this is that cover
trees were implemented for MLPACK for use with -nearest neighbor search.
This cover tree implementation could, without any additional work, be used with
\texttt{DualTreeBoruvka}---which was never an intended goal during the cover
tree implementation but still a particularly valuable result!






\vspace*{-0.2em}
Of course, the utility of these abstractions are not limited implementation
details.  Each of the papers cited in the previous sections describe algorithms
in terms of one specific tree structure.  March et~al.  \yrcite{march2010}
discuss implementations of Bor\r{u}vka's Algorithm on both -trees and cover
trees and give algorithms for both.  Each algorithm given is quite different and
it is not easy to see their similarities.  Using our meta-algorithm, any of
these tree-based algorithms can be expressed with less effort---especially for
more complex trees like the cover tree---and in a more general sense.

\vspace*{-0.2em}
In addition, correctness proofs for our algorithms tend to be quite simple.  The
proofs for each algorithm here can be given in two simple sub-proofs:
\textit{(1)} prove the correctness of \texttt{BaseCase()} when no prunes are
made, and \textit{(2)} prove that \texttt{Score()} does not prune any subtrees
which the correctness of the results depends on.

\vspace*{-0.2em}
The logical split of base case, pruning rule, tree type, and traversal can also
be advantageous.  A strong example of this is the function 
devised in Section \ref{sec:knn}, which is a novel, tighter bound.  When not
considering a particular tree, the path to a superior algorithm can often be
simpler (as in that case).

\subsection{Parallelism}

Nowhere in this paper has parallelism been discussed in any detail.  In fact,
all of the given algorithms seem to be suited to serial implementation.
However, the pruning dual-tree traversal is entirely separate from the rest of
the dual-tree algorithm; therefore, a parallel pruning dual-tree traversal can
be used without modifying the rest of the algorithm.

\vspace*{-0.2em}
For instance, consider -nearest neighbor search.  Most large-scale parallel
implementations of -NN do not use space trees but instead techniques like
LSH for fast (but inexact) search.  To our knowledge, no freely available
software exists that implements distributed dual-tree -nearest neighbor
search.

\vspace*{-0.2em}
As a simple (and not necessarily efficient) proof-of-concept idea for a
distributed traversal, suppose we have  machines and a ``master'' machine
for some .  Then, for a query tree  and a reference tree
, we can split  into  subtrees and one ``master
tree'' .  The reference tree  is split the same
way.  Each possible combination of query and reference subtrees is stored on one
of the  machines, and the master trees are stored on the master machine.
The lists  and  can be stored on the master machine and can be updated or
queried by other machines.

\vspace*{-0.2em}
The traversal starts at the roots of the query tree and reference tree and
proceeds serially on the master.  When a combination in two subtrees is reached,
\texttt{Score()} and \texttt{BaseCase()} are performed on the machine containing
those two subtrees and that subtree traversal continues in parallel.  This idea
satisfies the conditions of a pruning dual-tree traversal; thus, we can use it
to make any dual-tree algorithm parallel.

\vspace*{-0.2em}
Recently, a distributed dual-tree algorithm was developed for
kernel summation \cite{lee2012distributed}; this work could be adapted to a
generalized distributed pruning dual-tree traversal for use with our
meta-algorithm.

\vspace*{-0.5em}
\section{Conclusion}


\vspace*{-0.2em}
We have proposed a tree-independent representation of dual-tree algorithms and a
meta-algorithm which can be used to create these algorithms.  A dual-tree
algorithm is represented as four parts: a space tree, a pruning dual-tree
traversal, a \texttt{BaseCase()} function, and a \texttt{Score()} function.  We
applied this representation to generalize and extend five example dual-tree
algorithms to different types of trees and traversals.  During this process, we
also devised a novel bound for -nearest neighbor search that is tighter than
existing bounds.  Importantly, this abstraction can be applied to help approach
the problem of parallel dual-tree algorithms, which currently is not well
researched.

\bibliography{paper}
\bibliographystyle{icml2013}

\end{document}
