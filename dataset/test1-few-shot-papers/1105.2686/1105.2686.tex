\documentclass[a4paper,11pt,fleqn]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[pdftex,paper=a4paper,left=3.0cm,right=3.0cm,top=4cm,bottom=4cm]{geometry}
\usepackage{graphicx}
\usepackage{subfig}

\usepackage{xspace}


\newlength{\addressminus}
\setlength{\addressminus}{-1.3mm}
\newlength{\addressminusB}
\setlength{\addressminusB}{-0.7mm}

\title{Smoothed Performance Guarantees for Local Search \footnote{A preliminary version of this paper appeared in the proceedings of ESA 2011.} }



\author{Tobias Brunsch\\addressminus]
      	\footnotesize University of Bonn, Germany\\addressminusB]
        \footnotesize Dept.~of Computer Science\\addressminus]
        \footnotesize {\tt heiko@roeglin.org}
 \and Cyriel Rutten\\addressminus]
  	\footnotesize Maastricht University, The Netherlands\\addressminusB]
	\footnotesize Dept.~of Quantitative Economics\\addressminus]
  	\footnotesize {\tt t.vredeveld@maastrichtuniversity.nl}
}

\date{}

\begin{document}

\maketitle

\allowdisplaybreaks

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{claim}{Claim}

\newtheorem{definition}[theorem]{Definition}

\newtheorem*{remark}{Remark}
\newtheorem{property}{Property}

\newenvironment{artclfig}{\begin{figure}[htbp]\begin{center}}{\end{center}\end{figure}}

\newcommand{\sched}{\ensuremath{\sigma}\xspace}
\newcommand{\cmax}{\ensuremath{C_{\max}}\xspace}
\newcommand{\copt}{\ensuremath{\cmax^*}\xspace}
\newcommand{\csched}[1][\sched]{\ensuremath{\cmax(#1)}\xspace}
\newcommand{\load}[2][]{\ensuremath{L#1_{#2}}\xspace}


\newcommand{\Prob}{\ensuremath{\mathop{\mathbf{Pr}}}\xspace}
\newcommand{\Probe}[1]{\ensuremath{\Prob[#1]\,}\xspace}
\newcommand{\Probl}[2][]{\ensuremath{\Prob\limits_{#1}\left[#2\right]}\xspace}

\newcommand{\E}{\ensuremath{\mathop{\mathbf{E}}}\xspace}
\newcommand{\Ee}[1]{\ensuremath{\E[#1]}\xspace}
\newcommand{\El}[2][]{\ensuremath{\E\limits_{#1}\left[#2\right]}\xspace}

\newcommand{\opt}{\ensuremath{\sched^*}\xspace}
\newcommand{\critmach}{\ensuremath{i_{\max}}\xspace}
\newcommand{\loadsched}[1]{\ensuremath{L_{#1}}(\sched)\xspace}
\newcommand{\jobset}[1][]{\ensuremath{J_{#1}}\xspace}
\newcommand{\jobsetsched}[1]{\ensuremath{J_{#1}}(\sched)\xspace}
\newcommand{\pmax}{\ensuremath{p_{\max}}\xspace}
\newcommand{\event}{\ensuremath{\mathcal{E}}\xspace}
\newcommand{\e}{\ensuremath{e}\xspace}
\newcommand{\allow}[1]{\ensuremath{{\cal M}_{#1}}\xspace}
\newcommand{\psmooth}{\ensuremath{\mathbf{p}}\xspace}

\newcommand{\jobs}[2][\sched]{\ensuremath{J_{#2}({#1})}\xspace}
\newcommand{\schedopt}{\sched^*}

\newcommand{\jobclass}[1]{\ensuremath{\mathcal{J}_{#1}}}

\newcommand{\smin}{s_{\min}}
\newcommand{\smax}{s_{\max}}


\newcommand{\JUMP}[1]{\mathrm{Jump}(#1)}
\newcommand{\LEX}[1]{\mathrm{Lex}(#1)}
\newcommand{\LIST}[1]{\mathrm{List}(#1)}
\newcommand{\NL}[1]{\mathrm{NL}(#1)}

\newcommand{\SET}[1]{\left\{#1\right\}}
\newcommand{\MIN}[1]{\min \SET{#1}}
\newcommand{\MAX}[1]{\max \SET{#1}}
\renewcommand{\d}{\mathrm{d}}

\newcommand{\smalljobs}{{\jobset}_{\text{small}}}
\renewcommand{\H}[1]{R_{#1}}

\newcommand{\WHERE}{\,\colon\,}
\newcommand{\FLOOR}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\CEIL}[1]{\left\lceil#1\right\rceil}
\newcommand{\COMMA}{\,,}
\newcommand{\DOT}{\,.}

\newcommand{\ProofText}[1]{Proof of #1}

\providecommand{\qedhere}{\tag*{\qed}}

\begin{abstract}

We study popular local search and greedy algorithms for standard machine
scheduling problems.
The performance guarantee of these algorithms is well understood,
but the worst-case lower bounds seem somewhat contrived and it is
questionable whether they arise in practical applications. To find out how
robust these bounds are, we study the algorithms in the framework  
of smoothed analysis, in which instances are subject to some
degree of random noise.

While the lower bounds for all scheduling variants with restricted machines
are rather robust, we find out that the bounds are fragile for unrestricted
machines. In particular, we show that the smoothed performance guarantee
of the jump and the lex-jump algorithm are (in contrast to the worst case)
independent of the number of machines. They are~ and~,
respectively, where~ is a parameter measuring the magnitude 
of the perturbation. The latter immediately implies that also the smoothed
price of anarchy is~ for routing games on parallel links.
Additionally, we show that for unrestricted machines
also the greedy list scheduling algorithm has an approximation guarantee
of~.

\end{abstract}


\section{Introduction}
\label{sec:intro}

The performance guarantee of local search and greedy algorithms for scheduling
problems is well studied and understood. For most algorithms, matching upper and
lower bounds on their approximation ratio are known. The lower bounds are often
somewhat contrived, however, and it is questionable whether they resemble typical
instances in practical applications. For that reason, we study these algorithms
in the framework of smoothed analysis, in which instances are subject to some
degree of random noise. By doing so, we find out for which heuristics and
scheduling variants the lower bounds are robust and for which they are fragile and
not very likely to occur in practical applications. Since pure Nash equilibria
can be seen as local optima, our results also imply a
new bound on the smoothed price of anarchy, showing that known worst-case
results are too pessimistic in the presence of noise.

Let us first describe the scheduling problems that we study. We assume that
there is a set  of jobs each of which needs to
be processed on one of the machines from the set .
All jobs and machines are available for processing at time~. The goal is to
schedule the jobs on the machines such that the \emph{makespan}, i.e., the time
at which the last job is completed, is minimized. Each machine~ has a
speed~ and each job~ has a processing
requirement~. 
The time~ it takes to fully process job~ on machine~ depends on
the machine environment. We consider two machine environments. The first one is
the one of \emph{uniform parallel machines}, also known as
\emph{related machines}:
. The second machine environment that we consider is the
one of \emph{restricted related machines}: a job~ is only allowed to be
processed on a subset  of the machines. The processing
time is therefore  if  and  if . An instance~ of a scheduling problem
consists of the machine speeds , the processing requirements
, and in the restricted case the allowed machine
set~ for every job~.

A special case for both machine environments is when all speeds are
equal, i.e.,  for all . In this case, we say that the
machines are identical. In the notation of Graham et
al.~\cite{graham:etal:79} these problems are denoted by  and
 for the related machine problems and  and
 in case of identical machines.
In these problems, makespan minimization is equivalent to
minimizing the maximum machine finishing time. Once the assignment of
the jobs to the machines is known, the order in which the jobs are
processed is of no importance to determine the machine finishing times,
as long as the jobs are processed without any idle time in between.
Therefore, we assume that the
jobs that are scheduled on a machine  share this processor in
such a way that they all finish at the same time.

Even in the case that all speeds are equal, the problems under
 consideration are known to be
strongly NP-hard when  is part of the input (see, e.g., Garey and
Johnson~\cite{Garey+Johnson:1979}). This has motivated a lot of research in the
previous decades on approximation algorithms for scheduling problems. Since some
of the theoretically best approximation algorithms are rather involved, a lot of
research has focused on simple heuristics like \emph{greedy algorithms} and \emph{local
search algorithms} which are easy to implement. While greedy algorithms
make reasonable ad hoc decisions to obtain a schedule, local search algorithms start with
some schedule and iteratively improve the current schedule by performing some
kind of local improvements until no such is possible anymore. 
In this article, we consider the following algorithms that can be applied to all scheduling variants that we have described above:
\begin{itemize}

  \item \emph{List scheduling} is a greedy algorithm that starts from an empty schedule and a list of jobs. Then, it repeatedly selects the next unscheduled job from the list and assigns it to the machine on which it will be completed the earliest with respect to the current partial schedule. We call any schedule that can be generated by list scheduling a \emph{list schedule}.

  \item The \emph{jump} and the \emph{lex-jump} algorithms are local
search algorithms that start with an arbitrary schedule and iteratively
perform a local improvement step. In each improvement step, one job is
reassigned (jumped) from a machine~ to a different machine~ where it
finishes earlier. In the jump algorithm, only jobs on \emph{critical}
machines~, i.e., machines that have maximum finishing time, are
considered to be improving. In the lex-jump algorithm, the jobs can
be arbitrary. Note that a local step is lex-jump improving if and only if the sorted vector of machine finishing times decreases lexicographically, hence the term lex-jump.
A schedule for which there is no jump improvement step or no lex-jump improvement step is called \emph{jump optimal} or \emph{lex-jump optimal}, respectively.

\end{itemize}
For each of these three algorithms, we are interested in  their performance guarantees, i.e., the worst case bound on the ratio of the makespan of a schedule to be returned by the algorithm over the makespan of an optimal schedule.
The final schedule returned by a local search algorithm is called a \emph{local optimum}.
Usually, there are multiple local optima for a given scheduling
instance both for the jump and the lex-jump algorithm with varying quality.
As we do not know which local optimum is found by the local search, we 
will always bound the quality of the worst local optimum. Since local optima
for lex-jump and pure Nash equilibria are the same, see e.g.~\cite{Voecking:2007:AGT}, this corresponds
to bounding the price of anarchy in the scheduling game that is obtained if 
jobs are selfish agents trying to minimize their own completion time and if the
makespan is considered as the welfare function.
Similarly, list scheduling can produce different schedules
depending on the order in which the jobs are inserted into the list. Also for
list scheduling we will bound the quality of the worst schedule that can be
obtained. 


\paragraph{Notation.}

Consider an
instance~ for the scheduling problem and a schedule~ for this
instance. By~ we denote the set of jobs
assigned to machine~ according to~. The \emph{processing requirement
on a machine~} is defined as 
and the \emph{load} of a machine is defined by .
The makespan~ of~ can be written as . The optimal makespan, i.e., the
makespan of an optimal schedule is denoted by~. By~,
, and~ we denote the set of all feasible jump optimal
schedules, lex-jump optimal schedules, and list schedules, respectively,
according to instance~.

If the instance~ is clear from the context, we simply
write~ instead of~, 
instead of , and~ instead of~. If the
schedule~ is clear as well, we simplify our notation further
to~ and~ and we write~ instead
of~. By appropriate scaling, we may assume w.l.o.g.\
that the slowest machine has speed~ and that all processing 
requirements are bounded by .
In Appendix~\ref{sec:appendix-table}, the notation is summarized in a table.

\paragraph{Smoothed analysis.} 
As can be seen in Table~\ref{tab:results}, the worst-case
approximation guarantee of jump and lex-jump is known for all scheduling
variants and it is constant only for the simplest case with unrestricted
and identical machines. In all other cases it increases with the number~
of machines. For list scheduling, the case with unrestricted and
related machines has been considered. Cho and Sahni~\cite{Cho+Sahni:1980} and Aspnes et al.~\cite{DBLP:journals/jacm/AspnesAFPW97} showed that the performance guarantee of list scheduling is  in this case.

In order to analyze the robustness of the worst-case bounds, we turn to the
framework of smoothed analysis, introduced by Spielman and
Teng~\cite{Spielman+Teng:SA:2004} to explain why certain algorithms perform
well in practice in spite of a poor worst-case running time. Smoothed analysis is a hybrid of
average-case and worst-case analysis: First, an adversary chooses an instance.
Second, this instance is slightly randomly perturbed. The smoothed performance
is the expected performance, where the expectation is taken over the random
perturbation. The adversary, trying to make the algorithm perform as bad as
possible, chooses an instance that maximizes this expected performance. This
assumption is made to model that often the input an algorithm gets is subject to
imprecise measurements, rounding errors, or numerical imprecision.
If the smoothed performance guarantee of an algorithm is small, then bad worst-case
instances might exist, but one is very unlikely to encounter them if instances are
subject to some small amount of random noise. 

We follow the more general model of smoothed analysis introduced by Beier and
V\"{o}cking~\cite{DBLP:journals/jcss/BeierV04}.
In this model, the adversary is even allowed to
specify the probability distribution of the random noise. The influence he can
exert is described by a parameter~ denoting the maximum density of the noise. This model is formally defined as follows.
\begin{definition}
\label{def:phi-smooth}
In a \emph{-smooth} instance~,
the adversary chooses the following input data:
\begin{itemize}
\item the number~ of machines;
\item arbitrary machine speeds ,
in the case of non-identical machines;
\item the number~ of jobs;
\item an arbitrary set  for each job~,
in the case of restricted machines;
\item for each~, a probability density  according
to which  is chosen independently of the processing requirements of the other jobs.
\end{itemize}
Note that the only perturbed part of the instance are the processing
requirements.
Formally, a -smooth instance is not a single instance but a
distribution over instances.
We write  to denote that the instance  is
drawn from the -smooth instance .
\end{definition}

The parameter~ specifies how close the analysis is to a worst case analysis. The adversary can, for example, choose for every~ an interval of length~ from which  is drawn uniformly at random. For~, every processing requirement is uniformly distributed over~, and hence the input model equals the average case for uniformly distributed processing times. When~ gets larger, the adversary can specify the processing requirements more and more precisely, and for~ the smoothed analysis approaches a worst-case analysis.
 
In this article, we analyze the \emph{smoothed performance guarantee}
of the jump, the lex-jump, and the list scheduling algorithm. As mentioned above, 
to define the approximation guarantee of these algorithms on a given instance, we
consider the worst local optimum (for the jump and the lex-jump algorithm) or the worst order in which the jobs are inserted into the list (for the list scheduling algorithm). 
Now, the smoothed performance is defined to be the worst expected approximation guarantee of any
-smooth instance.  

\paragraph{Our results.}

\begin{table}
\begin{center}\footnotesize
\begin{tabular}{|l|cc|cc|}\hline
& \multicolumn{2}{c|}{worst case} & \multicolumn{2}{c|}{-smooth}\\
& jump & lex-jump & jump & lex-jump \\
\hline

\begin{tabular}{l}unrestricted\\identical\end{tabular} 
&  \cite{finn:horowitz:1979,Schuurman:Vredeveld:2007} 
&  \cite{finn:horowitz:1979,Schuurman:Vredeveld:2007} 
&  
& \\


\begin{tabular}{l}unrestricted\\related\end{tabular}
&  \cite{Cho+Sahni:1980,Schuurman:Vredeveld:2007}
&  \cite{Voecking:2007}
&  [\ref{subsec:jump}]
&  [\ref{subsec:ub-list-lex-jump}, \ref{subsec:lb-list-lex-jump}]\\


\begin{tabular}{l}restricted\\identical\end{tabular}
&  \cite{Rutten:etal:2012}
&  \cite{Awerbuch+etal:2006}
&  [\ref{subsec:lb-jump-restricted}]
&  [\ref{subsec:lb-lex-jump-restricted}]\\


\begin{tabular}{l}restricted\\related\end{tabular}
&  \cite{Rutten:etal:2012}
&  \cite{Rutten:etal:2012}
&  [\ref{subsec:lb-jump-restricted}]
&  [\ref{subsec:lb-lex-jump-restricted}]\\
\hline
\end{tabular}
\vspace{0.2cm}
\caption{Worst-case and smoothed performance guarantees for jump and lex-jump optimal
schedules. Here, , and we assume w.l.og.~that . With [X.Y] we refer to 
the section in this article where the bound is shown.
\label{tab:results}}
\end{center}
\vspace{-0.8cm}
\end{table}

Our results for the jump and lex-jump algorithm are summarized in
Table~\ref{tab:results}. The first
remarkable observation is that the smoothed performance guarantees for all
variants of restricted machines are robust against random noise. We show that
even for large perturbations with constant~, the worst-case lower bounds
carry over. This can be seen as an indication that neither the jump algorithm nor the lex-jump algorithm
yield a good approximation ratio for scheduling with restricted machines in practice.

The situation is much more promising for the unrestricted variants.
Here, the worst-case bounds are fragile and do not carry over to the smoothed
case. The interesting case is the one of unrestricted and related machines. Even though
both for jump and for lex-jump the worst-case lower bound is not robust,
there is a significant difference between these two: while the smoothed
approximation ratio for jump grows linearly with the perturbation parameter~,
it grows only logarithmically in~ for lex-jump optimal schedules.
This proves that also in the presence of random noise lex-jump optimal schedules
are significantly better than jump optimal schedules. As mentioned earlier, this also implies
that the smoothed price of anarchy is~. Additionally, we
show that the smoothed approximation ratio of list scheduling
is~ as well, even when the order of the list may be
specified after the realizations of the processing times are known.
 This indicates that both the lex-jump algorithm and the list scheduling algorithm should yield good
approximations on practical instances.

\paragraph{Related work.}
The approximability of  is well understood. Cho and
Sahni~\cite{Cho+Sahni:1980} showed that list scheduling has a
performance guarantee of at most  for~ and
that it is at least . Aspnes et al.~\cite{DBLP:journals/jacm/AspnesAFPW97} improved the upper bound to  matching the lower bound asymptotically. Hochbaum and
Shmoys~\cite{Hochbaum+Shmoys:1988} designed a polynomial time
approximation scheme for this problem. Polynomial time approximation algorithms and polynomial time
approximation schemes for special cases of the problem on restricted
related machines are given in, among others,~\cite{Li06,Glass07,Ou08}.
More work on restricted related parallel machines is discussed in the
survey of Leung and Li~\cite{Leung08}.

In the last decade, there has been a strong interest in understanding
the worst-case behavior of local optima. We
refer to the survey~\cite{Angel:2006} and the
book~\cite{Michiels:etal:2007} for a comprehensive overview of the worst-case analysis
and other theoretical aspects of local search. 
It follows from the work of Cho and Sahni~\cite{Cho+Sahni:1980} that for
the problem on unrestricted related machines the performance guarantee
of the jump algorithm is  and this bound is
tight~\cite{Schuurman:Vredeveld:2007}. For lex-jump optimal
schedules, Czumaj and V\"ocking~\cite{Voecking:2007} showed that the performance guarantee is
.
For the problem on restricted related machines, Rutten et
al.~\cite{Rutten:etal:2012} showed that the performance guarantee of locally optimal schedules with
respect to the jump neighborhood is  and
that this bound is tight up to a constant factor. Moreover, they showed
that the performance guarantee of lex-jump optimal schedules is
, where .
When all speeds are equal, Awerbuch et al.~\cite{Awerbuch+etal:2006}
showed that the performance guarantee for lex-jump optimal schedules is
.

Up to now, smoothed analysis has been mainly applied to running time
analysis (see, e.g.,~\cite{Spielman+Teng:CACM2009} for a survey). The first
exception is the paper by Becchetti et
al.~\cite{Becchetti+etal:Smoothed:MOR} who introduced the concept of
smoothed competitive analysis, which is equivalent to smoothed performance
guarantees for online algorithms. Sch\"afer and
Sivadasan~\cite{Schaefer+Sivadasan:2005} performed a smoothed
competitive analysis for metrical task systems. Englert et al.~\cite{Englert+etal:SODA2007} considered the -Opt algorithm for the traveling salesman problem and determined, among others, the
smoothed performance guarantee of local optima of the -Opt algorithm.
Hoefer and Souza~\cite{Hoefer+Souza:2010} presented one of the first
average case analyses for the price of anarchy.


The remainder of this article is organized as follows.
In Section~\ref{sec:relatedmachines}, we
provide asymptotically matching upper and lower bounds on the smoothed performance guarantees of
jump optimal,  lex-jump optimal, and list schedules in case of unrestricted related machines. In Section~\ref{sec:restricted}, we show that smoothing does not help for the setting of restricted machines.




\section{Unrestricted Related Machines}
\label{sec:relatedmachines}

\subsection{Jump Optimal Schedules}
\label{subsec:jump}


We show that the smoothed performance guarantee grows
linearly with the smoothing parameter~ and is independent
of the number of jobs and machines. In particular, it is constant if the
smoothing parameter is constant. In proving our results, we make use of the 
following proposition which follows from Cho and Sahni~\cite{Cho+Sahni:1980}.

\begin{prop}
For any scheduling instance~ with~ unrestricted related machines and~ jobs

\label{prop:ChoSahni}
\end{prop}

\begin{theorem}
\label{thm:JumpRelatedUpper}
For any -smooth instance~ with unrestricted and related machines,

\end{theorem}

\begin{proof}
First note that if~, then there exist an optimal schedule and a worst
jump-optimal schedule that do not schedule any job on any of the
slowest~ machines. We ignore these slowest~ machines, and therefore
we assume that .
We will prove an upper bound on the performance guarantee of jump optimal schedules
that decreases when the sum of processing requirements  increases and that is valid for every instance. Then, we will argue that for -smooth instances~ is usually not too small, which
yields the theorem.

Let~ denote an arbitrary jump optimal schedule for some arbitrary processing requirements~. Let~ be an arbitrary machine, let machine~ be a critical machine in schedule~, and let~ be a job assigned to machine~ by schedule~\sched. By jump optimality of~\sched it follows that

where~ denotes the processing requirement of the largest job. The previous inequality yields that  for
all machines~.
Summing over all machines from  and adding
 to both sides of the inequality, we find that

since . Noting that  yields the following upper bound on the makespan of any jump optimal schedule~:

where the last inequality follows since~. Using the well-known bound  we obtain

Hence,

The performance guarantee of any jump optimal schedule can only be bad if~ is small. Since the instance is -smooth, the processing requirements are random
variables in~ with bounded densities.
Let~ denote the failure event that .
We define~ to be independent random variables drawn uniformly from~ for all~. Then,
 for any~. 
Let . Then, for any~,
it follows that . Hence,

where the last inequality follows from Hoeffding's bound~\cite{Hoeffding:1963} (see also Theorem~\ref{thm:app:hoeffding} in the appendix). Consider the random variable

and let . Due to Inequalities~\eqref{ChoSahni} and~\eqref{UB_jump_optimality} we have . We denote by~ the complement of~ and obtain
 
For the third inequality, we used  if event~ does not hold. The last inequality holds since 

where the maximum is attained for .
\end{proof}


\begin{cor}
Consider an instance of scheduling with unrestricted and related machines in which the processing requirement of every job is chosen independently and uniformly at random from . The expected performance guarantee of the worst jump optimal schedule is~.
\end{cor}



Next, we show that the upper bound on the smoothed performance guarantee provided in Theorem \ref{thm:JumpRelatedUpper} is tight up to constant factor when~.

\begin{theorem}
\label{thm:JumpRelatedLower}
There is a class of -smooth instances~ with unrestricted and related machines such that

\end{theorem}

\begin{proof}
For any~ we construct a -smooth instance~ with  and~ machines.
Let

We assume that the processing requirement~ is chosen uniformly from the interval 
while the processing requirements of all other jobs are chosen uniformly from the interval~.
In an optimal schedule, job~ is scheduled on machine~, and all other
machines process exactly one job (see Figure~\ref{fig:jump-unrestricted-related-opt}). Hence,

We show that with high probability there exists a jump optimal schedule~
with . In order to find such a schedule~, 
we first schedule job~ on machine~.
Then, we consider the remaining jobs one after another and schedule unassigned jobs to machine~ until either
 or all jobs
are scheduled. Any job that remains unscheduled is then exclusively assigned to one
empty machine. Let~\event denote the event that . Note that . We will see that event~\event holds with high probability with respect to~.

\begin{artclfig}\newcommand{\height}{11em}
  \begin{minipage}[t]{0.55\textwidth}\begin{center}
    \includegraphics[height=\height]{JumpUnrestrictedRelatedLB1.pdf}
    \caption{Optimal schedule}
    \label{fig:jump-unrestricted-related-opt}
  \end{center}\end{minipage}\hfill
  \begin{minipage}[t]{0.4\textwidth}\begin{center}
    \includegraphics[height=\height]{JumpUnrestrictedRelatedLB2small.pdf}
    \caption{Machines~ and~ of schedule~\sched if event~\event occurs}
    \label{fig:jump-unrestricted-related-jump-opt}
  \end{center}\end{minipage}
\end{artclfig}

Consider the case that event~\event occurs. Then, schedule~ is such that  since  and  for
all jobs  (see Figure~\ref{fig:jump-unrestricted-related-jump-opt}). Now, we argue that schedule~ is jump optimal.
First observe that machine~ defines the makespan since . Job~, which is the only job assigned to
that machine, cannot jump to a machine~ because these have the same speed as machine~. Furthermore,
it cannot jump to machine~ because

as~. Hence,  is a jump optimal schedule with 

It remains to determine the probability of event~. Recalling , , and , this can be bounded with Hoeffding's bound~\cite{Hoeffding:1963} (see also Theorem~\ref{thm:app:hoeffding}) as follows:

Let . Applying Inequality~\eqref{eq:lb-jump-unrestricted} the smoothed performance guarantee can be bounded from below as follows: 

where the last inequality follows because  for~.
\end{proof}

\subsection{Upper Bounds for List Schedules and Lex-jump Optimal Schedules}
\label{subsec:ub-list-lex-jump}


Although the worst case performance bound on unrestricted related machines for list scheduling is slightly worse than the one for lex-jump scheduling, we show that the smoothed performance guarantee of both schedules is~. In the next subsection, we show that this bound is asymptotically tight.

\begin{theorem}
\label{thm.mainI.list.lex}
Let~ be an arbitrary positive real. For~ and any -smooth instance~ with unrestricted and related machines

and

\end{theorem}

Note that the assumption~ in
Theorem~\ref{thm.mainI.list.lex} is no real restriction as for  any -smooth instance is a -smooth instance. Hence,
for these values we can apply all bounds from
Theorem~\ref{thm.mainI.list.lex} when substituting~ by~. In
particular, the expected value is a constant.

In the remainder of this section, we will use the following notation (see also Appendix~\ref{sec:appendix-table}).
Let  denote the set
of all jobs that are scheduled on machine~ and have index at most~, i.e.,
. If~ is clear
from the context, then we just write~.
We start with observing an essential property that both lex-jump optimal
schedules and list schedules have in common.

\begin{definition}
We call a schedule~ on machines  with speeds  a \emph{near list schedule}, if we can index the jobs in
such a way that

for all machines~ and all jobs~. With  we denote the set of all near list schedules for instance~.
\end{definition}

Inequality~\eqref{eq.key.property} can be interpreted as follows. Assume that the jobs are already indexed correctly and imagine that on each machine the jobs form a stack, ordered from top to bottom ascendingly according to their index. Now, consider an arbitrary job~ on machine~ (see Figure~\ref{fig:near-list-schedule1}). Inequality~\eqref{eq.key.property} states that the completion time of job~ after removing all jobs above~ is minimized on machine~ in case only job~ is allowed to move (see Figure~\ref{fig:near-list-schedule2}).

\begin{artclfig}\newcommand{\height}{10em}
  \subfloat[Jobs on machine~, including job~, visualized as a stack]{
    \includegraphics[height=\height, page=1]{NearListSchedule.pdf}
    \label{fig:near-list-schedule1}
  }
  \hspace{5ex}
  \subfloat[Job~ does not benefit from jumping to machine~]{
    \includegraphics[height=\height, page=2]{NearListSchedule.pdf}
    \label{fig:near-list-schedule2}
  }
  \caption{Interpretation of Inequality~\eqref{eq.key.property}}
\end{artclfig}

\begin{lemma}
\label{lemma.NL}
For any instance~ the relation  holds.
\end{lemma}

Note that in general neither  nor  holds (see Figure~\ref{fig:strict-supset}). Moreover, there also exist near list schedules that are neither in  nor in  (see Figure~\ref{fig:strict-supset generalization}), i.e., near list schedules are a non-trivial generalization of both lex-jump optimal schedules and list schedules.

\begin{artclfig}
\newcommand{\height}{10em}
  \subfloat[A list schedule which is not lex-jump optimal]{
    \includegraphics[height=\height]{ListNoSubsetLex.pdf}
  }
  \hspace{5ex}
  \subfloat[A lex-jump optimal schedule which is no list schedule]{
    \includegraphics[height=\height]{LexNoSubsetList.pdf}
  }
  \hspace{5ex}
  \subfloat[A near list schedule which is neither lex-jump optimal nor a list schedule]{
    \includegraphics[height=\height]{NearListStrictSupset.pdf}
    \label{fig:strict-supset generalization}
  }
  \caption{Relationship between , , and }
  \label{fig:strict-supset}
\end{artclfig}

\begin{proof}[\ProofText{Lemma~\ref{lemma.NL}}]
For any schedule~, we can index the jobs
arbitrarily and, by definition, even the stronger inequality  holds. For~ we can
index the jobs in reverse order in which they appear in the list that
was used for list scheduling.
Consider an arbitrary job  and a machine . Let  and  denote
the loads of machines~ and~ before assigning job~ to
machine~ and the loads of~ and~ in the final schedule,
respectively. Then, 
as~ is assigned to machine~ according to list scheduling. Since
 and , this implies .
\end{proof}

In the remainder, we fix an instance~ and consider an arbitrary
schedule~ with appropriate indices of the jobs
such that Inequality~\eqref{eq.key.property} holds.
To prove Theorem~\ref{thm.mainI.list.lex}, we show
that in case the ratio of  over  is large,
then instance~ needs to have many very small jobs,
see~Corollary~\ref{corol.many.small.jobs}.
This holds even when  the instance  is deterministically picked by some adversary.
This observation allows us to prove the main theorem of this subsection by showing that for any -smooth instance, there are only ``few'' small jobs in expectation. The latter implies that a large ratio only happens with (exponentially) small probability.



In our proofs, we adopt some of the notation also used by Czumaj and
V\"{o}cking~\cite{Voecking:2007} (see also Appendix~\ref{sec:appendix-table}). 
Given a schedule~, we set . Recall that the
machines are ordered such that . For any
integer~ let  where . Note that  for all~ and hence~ for such~ (see Figure~\ref{fig:machine-classification}). Further, define  for all  and .
Note that this classification always refers to schedule~ even if
additionally other schedules are considered. Some properties follow
straightforwardly.

\begin{artclfig}\newcommand{\height}{14em}
\includegraphics[height=\height]{MachineClassification.pdf}
\caption{Machine classification by Czumaj and V{\"o}cking}
\label{fig:machine-classification}
\end{artclfig}

\begin{property}
\label{prop:minimum-load}
For each machine~, .
\end{property}

\begin{property}
Machine~, if it exists, is the first machine in , i.e., the machine with the least index,
and, hence, a fastest machine in .
\end{property}

\begin{property}
\label{prop:maximum-load}
 for all , and .
\end{property}

As mentioned, we need to show that there are many small jobs. To do so,
we will show that the the speeds of the machines in low classes, i.e.,  and
, are exponentially small with respect to the machines in the
highest class  (Lemma~\ref{lemma.no.neighboring.classes.empty})
and that the machines in low classes
need to process high volume (Lemma~\ref{lemma.job.redistribution}).
We start by showing that the highest class is nonempty.

\begin{lemma}
\label{lem:nonemptyHc}
Machine~ is in class~.
\end{lemma}

\begin{proof}
Let~ be a critical machine. If~, then we obtain . Otherwise we apply Inequality~\eqref{eq.key.property} for the job~ with the smallest index on machine~ and for machine~. This yields . Hence,
,
where the second inequality is due to the fact that any job can
contribute at most~ to the makespan of a fastest machine.
\end{proof}

Let  and  be integers satisfying . Several times we will consider the first many jobs 
on some machine~ which contribute at least~ to the load of machine~. We denote the set of those jobs
by~. Formally, 

Using this notation, Lemma \ref{lemma:main-prop-near-list-schedule} and Corollary \ref{corol.optimal.assignment} restrict the machines on which a job in  can be scheduled in an optimal schedule.

\begin{lemma}
\label{lemma:main-prop-near-list-schedule}
Let~ and~ be positive integers, let  and  be  machines in~ and not in~, respectively, and let  be a job on machine~. Then, the load job~ would contribute to machine~ is bounded from below by .
\end{lemma}

\begin{proof}
We apply Inequality~\eqref{eq.key.property} for machine~, for the first machine~ that does not belong to~, and for job~ to obtain

which implies

By the choice of the machines~ and~ and Properties~\ref{prop:minimum-load} and~\ref{prop:maximum-load} we obtain  and . Furthermore,  yields . Hence, . The claim follows since .
\end{proof}

\begin{cor}
\label{corol.optimal.assignment}
Let~ be an arbitrary machine and let~ be an integer. Then, in any optimal schedule any job  is assigned to machines from .
\end{cor}

\begin{proof}
Assume, for contradiction, that there is a job  which is assigned to a machine~ by an optimal schedule. By Lemma~\ref{lemma:main-prop-near-list-schedule} this job causes a load of more than  on this machine contradicting the assumption that the considered schedule is optimal.
\end{proof}

Czumaj and V\"{o}cking~\cite{Voecking:2007} showed that in a lex-jump optimal
schedule the speeds of any two machines which are at least two classes apart
differ by a factor of at least~. Aspnes et~al.~\cite{DBLP:journals/jacm/AspnesAFPW97} showed a similar property. In general, near list schedules have a
slightly weaker property. 

\begin{lemma}
\label{lemma.decreasing.speeds}
Let~ and assume . The speed of any machine in
class~ is at least twice the speed of any machine in~.
\end{lemma}


\begin{proof}
We may assume that , since otherwise all machines have a load larger than  as . Let  and  be arbitrary machines and consider the jobs from . If we would assign only these jobs to machines in~, then
there would be a machine with load at least . Consequently, in an optimal schedule at least one job in  is assigned to some machine , say job . Since job~ contributes at most~ to the load of machine~ in this optimal schedule, this implies  and, hence,

as .
Due to Lemma~\ref{lemma:main-prop-near-list-schedule}, the load that would be contributed by job~ on machine~ is bounded by . Inequality~\eqref{eq:fast-processing} yields  as claimed in the lemma.
\end{proof}

We want to show that machines in low classes, i.e., machines in , have exponentially small speeds
(with respect to~) compared to the speeds of the machines in a high class, i.e., those in~.
Lemma~\ref{lemma.decreasing.speeds} already implies that the machine speeds
would double every five classes if no class~ was empty. Although some classes~ can be empty,
we show that not too many of these machine classes are empty. This is done in Lemma~\ref{lemma.no.neighboring.classes.empty}
which follows from the next lemma.

The machines , , are overloaded compared to an optimal schedule,
even if we just consider the first few jobs  on them (where ). On the other hand, in Corollary~\ref{corol.optimal.assignment} we showed that in any optimal schedule these jobs are not assigned to machines in much lower classes, i.e., to machines from . Consequently, in any optimal schedule the machines in  consume the current overload of~.

\begin{lemma}
\label{lemma.job.redistribution}
Let~ be positive integers. In any optimal schedule the total processing requirement on all machines in~ is at least

\end{lemma}

Note that Lemma~\ref{lemma.job.redistribution} also holds for the case  where .

\begin{proof}
Applying Corollary~\ref{corol.optimal.assignment} with  for arbitrary integers~ yields that in any optimal schedule~ all jobs in
 are assigned to machines in~ as
 for any index~. Furthermore, in~
the processing requirement on any machine~ is at most , i.e., the machines in  must consume the remainder. Hence, these machines must process jobs with total processing requirement at least

This yields the claimed bound as .
\end{proof}

Although some machine classes~ might be empty, we are able to show that this cannot be the case for two consecutive classes.

\begin{lemma}
\label{lemma.no.neighboring.classes.empty}
 for any .
\end{lemma}

\begin{proof}
Let~ be a slowest machine in~. In any optimal schedule~
the processing requirement on any machine~
is at most .
Applying Lemma~\ref{lemma.job.redistribution} with~ implies

It follows that 

since~ and since  due to Lemma~\ref{lem:nonemptyHc}.
\end{proof}

We can now show that machine speeds double every six classes. To be more formal:

\begin{lemma}
\label{lemma.exponentially.decreasing.speeds}
Let  be integers, let~ be any machine of~
and let~. Then,  where .
\end{lemma}

\begin{proof}
We prove the claim by induction. For , the claim trivially holds as . Assume that the claim holds up to some integer . We show that it is also true for . Note that for such~ we have . According to Lemma~\ref{lemma.no.neighboring.classes.empty} the class  contains at least one machine. Let~ be the fastest machine in . Then . Lemma~\ref{lemma.decreasing.speeds} and the induction hypothesis imply  and , respectively. Hence, .
\end{proof}

Since the machines in low classes are exponentially slower than the machines in high classes (with respect to~) and as their aggregated total processing requirement in an optimal schedule is large (Lemma~\ref{lemma.job.redistribution}), it follows that many jobs have processing requirements exponentially small in~. 

\begin{lemma}
\label{lemma.small.jobs}
Let  be an arbitrary machine. Then each job~ assigned to machine~ by an optimal schedule has processing requirement at most .
\end{lemma}

\begin{proof}
For  the claim is true since we rescale all processing requirements to be at most~. Assume . Consider an optimal schedule~ and
let~ be a job processed on a machine~ according to~. Note that  due to Lemma~\ref{lemma.no.neighboring.classes.empty}. Then, , i.e.,

To bound , consider the job  with the smallest index on machine~ of schedule~ and consider the first machine~ which exists due to Lemma~\ref{lemma.no.neighboring.classes.empty} and . Applying Inequality~\eqref{eq.key.property}, we obtain , i.e., . Since machine~ belongs to~ due to Lemma~\ref{lem:nonemptyHc} and since machine~ is the first machine that does not belong to~, we have  and , which implies . Lemma~\ref{lemma.exponentially.decreasing.speeds} yields . Applying Inequality~\ref{eq:low-processing-requirement} and  according to our input model we obtain

\end{proof}

\begin{cor}
\label{corol.many.small.jobs}
The processing requirement of at least~ jobs is at most .
\end{cor}

\begin{proof}
Lemma~\ref{lemma.job.redistribution} for  implies that the total
processing requirement of all jobs assigned to machines from  according to~ is at least 
which is an upper bound for the total processing requirement of all jobs
assigned to machines in~ according to~. Since all jobs assigned to machines from  by an optimal schedule have processing requirement at most  due to Lemma~\ref{lemma.small.jobs}, at
least half of the jobs have processing requirement at most~.
\end{proof}

Since having many so small jobs is unlikely when the processing
requirements have been smoothed, it follows that the smoothed
performance guarantee, which is between  and , cannot be too high, yielding Theorem~\ref{thm.mainI.list.lex}.

\begin{proof}[\ProofText{Theorem~\ref{thm.mainI.list.lex}}]
If , then at least~ jobs have processing requirement at most  due to Corollary~\ref{corol.many.small.jobs} and . The probability that one specific job is that small is bounded by  in the smoothed input model. Hence, the probability that the processing requirement of at least~ jobs is at most , is bounded from above by

Note that the first inequality holds if . Otherwise, the bound is trivially true. This yields

As for~ any schedule~ is optimal, we just consider the case~. For  let , i.e., . If , then we obtain

as . Since  we obtain

\end{proof}





\subsection{Lower Bounds for List Schedules and Lex-jump Optimal Schedules}
\label{subsec:lb-list-lex-jump}


In this subsection, we show that the upper bound,
given in Theorem~\ref{thm.mainI.list.lex},
on the smoothed performance guarantee on  lex-jump optimal as well as list schedules is tight up to a constant factor.
We provide a -smooth instance such that the worst lex-jump optimal schedule as well as the worst schedule that can be obtained by list scheduling has a lower bound on the performance guarantee of , for any realization of the processing times.

\begin{theorem}
\label{thm.mainII.list.lex}
There is a class of -smooth instances~ with unrestricted and related machines such that, for any ,

\end{theorem}



To prove this theorem, we first present a -smooth instance and in
Algorithm~1, we implicitely give a permutation of the jobs such that list
scheduling using this permutation results in a schedule  which
we will show is also lex-jump optimal. The schedule  resembles
the worst case example constructed by Czumaj and
V\"{o}cking~\cite{Voecking:2007}: Machines are partitioned into classes
indexed by . We will show that in , each machine
in class  has a load of approximately , whereas the optimal
makespan is bounded by . Hence, we can lower bound the performance
guarantee in the order of the number of classes. Whereas Czumaj and
V\"{o}cking needed  classes, we only need
 classes.


As scaling of all processing requirements does not change the approximation ratio, for sake of simplicity we do not consider probability densities  but scaled densities  for an appropriate integer~.

Let~ and consider an integer ,
i.e., . The machines are partitioned into machine
classes~ for , such that machine class~
contains~ machines of speed~. Also the jobs are partitioned into
job classes~ for  such that a job class
 contains~ jobs each having a processing
requirement uniformly drawn from . Note that the
density of this instance is bounded by  which is valid in
the variant of our model that we use in this subsection. The permutation
of the jobs is such that list scheduling constructs the schedule \sched in
the following way:

\begin{center}
{\parbox{0.95\linewidth}{\textbf{Algorithm 1:}\\
   \quad 1. \quad \textbf{for}  \textbf{to}  \textbf{do}\\
   \quad 2. \quad\quad \textbf{for}  \textbf{down to}  \textbf{do}\\
   \quad 3. \quad\quad\quad Schedule  arbitrary jobs of
class~ according to list scheduling.\\
   \quad 4. \quad\quad \textbf{end for}\\
   \quad 5. \quad \textbf{end for}
 }}\end{center}
Note that for any job class~ all  jobs have been scheduled. Let~ be the resulting schedule. First, we show a key property of~.

\begin{lemma}
\label{lemma.list.schedule}
For any index  each machine in~ is assigned
exactly~ jobs of job class~ and no other jobs. The machines in~ remain empty.
\end{lemma}

\begin{proof}
Let~ denote the partial schedule after processing line~3
of iteration~ of Algorithm~1. Within the 
iteration, we call a machine~ \emph{used} if a job of
class~ has already been assigned to~ during that iteration. Otherwise, we call machine~ \emph{unused}. We show the two claims below inductively and simultaneously. The lemma then follows straightforwardly from the second claim since the last iteration is~.

\begin{claim}
During iteration~,  jobs of class~ are
assigned to  distinct machines (i.e.\ all machines) of
class~.
\end{claim}

\begin{claim}
\label{claim:nrjobs-partialschedule}
In the partial schedule~ each machine in class~ is assigned

jobs of class~ and no other jobs.
\end{claim}

Figure~\ref{fig:list-lex-lb1} visualizes the partial schedule
. Machine~ with speed  is a representative
for all machines in class~. With~ we refer to the current
load of machine~ and with  to the load of machine~ at
the end of iteration , i.e., in the partial schedule
. In phase ,  jobs of size
roughly~ are being assigned to the  machines
in~. All machines in~ for  just received
a job of roughly size~. All machines in~ for
 will still receive a single job of
size roughly~ during iteration~ of the outer loop. Figure~\ref{fig:list-lex-lb1} follows from the observations.

\begin{artclfig}
  \includegraphics[width=0.7\textwidth]{ListLexLB1.pdf}
  \caption{The partial schedule }
  \label{fig:list-lex-lb1}
\end{artclfig}

First, we validate the claims for the first iteration . As only
 job of class~ has to be scheduled and since all machines
are still empty, the job will be scheduled on the fastest machine which
is the single machine in~. Hence, both claims hold true for the
first iteration. Now, consider an arbitrary iteration~ and
assume both claims hold true for all previous iterations. Consider a
job~ which needs to be assigned to a machine during
iteration~. We show that job~ will always be assigned to
an unused machine~. To see this, first note that the previous
iteration was either  or .

Let  be an unused machine. By the second claim, we know
that this machine carries~ jobs of class~. Consequently, we
can upper bound its load by

where we used that~,~, and~ for all integers~.

Consider a machine machine~ which is either used (in that case let ) or in class~ for some . By Claim~\ref{claim:nrjobs-partialschedule}, this machine carries~ jobs of class~ and thus


Finally, consider a machine~ for some
. Again by
Claim~\ref{claim:nrjobs-partialschedule}, it carries~ jobs
of class~ and thus

where the second inequality follows from  and the third inequality follows from  for all positive integers~.


With this complete case analysis we have shown that job~ will be
assigned to an unused machine~. We conclude that during
iteration~, each of the~ jobs to be assigned will be
assigned to an unused machine in~. Note that , and hence for each job there always exists such an unused machine. The first claim and the second claim follow immediately.
\end{proof}

\begin{lemma}
Schedule~ is lex-jump optimal.
\end{lemma}

\begin{proof}
It follows from Lemma~\ref{lemma.list.schedule} that the load of any
machine~ can be bounded by

If a job~ assigned to machine~ would jump to another
machine~, then

where the last inequality follows from  for all integers~. Thus, any job would be worse off by jumping to another machine, and hence schedule~ is lex-jump optimal.
\end{proof}

We conclude this subsection by proving Theorem~\ref{thm.mainII.list.lex}.

\begin{proof}[\ProofText{Theorem~\ref{thm.mainII.list.lex}}]
We consider schedule~ constructed above which is both a list
schedule and a lex-jump optimal schedule. By
Lemma~\ref{lemma.list.schedule} the load of the single machine in~
is at least~. Hence, . Now, consider a
schedule~ in which each machine in~ processes a single
job from job class~, . The single
machine in~ remains empty. Then, the load of any machine~ with job~ assigned to it is bounded as follows:

Hence,  and the theorem follows: .
\end{proof}



\section{Restricted Machines}
\label{sec:restricted}

In this section, we provide lower bound examples showing that the
worst-case performance guarantees for all variants of the restricted
machines are robust against random noise. Our lower bounds are in the
order of the worst-case bounds and hold in particular
for . In our lower bound constructions all processing
requirements are chosen uniformly at random from intervals of length~.
This means that even with large perturbations the worst-case
lower bounds still apply.

\subsection{Jump Neighborhood on Restricted Machines}
\label{subsec:lb-jump-restricted}

Rutten et al.~\cite{Rutten:etal:2012} showed that the makespan of a
jump optimal schedule is at most a factor of  away from
the optimal makespan on restricted identical machines. 
On restricted related machines they
showed that the makespan of a jump optimal
schedule is not more than a factor of 
away from the makespan of an optimal schedule, assuming that .
They provided two examples showing that the bound on identical machines
is tight and the one on related machines is tight up to a constant
factor.
We show that even on -smooth
instances these bounds are tight up to a constant factor.
As in~\cite{Rutten:etal:2012}, we construct an example with two job
classes and three machine classes. The first machine class consists of
only one machine and this machine is the slowest among all machines. The
first class of jobs can only be scheduled on machines in the first two
classes, whereas the jobs in the second class are allowed on all
machines. To construct a bad example, we schedule all jobs in the first
class on the slowest machine and use the jobs of the second class to
fill the machines in the second machine class so that the schedule will
be jump optimal, with high probability. 

\begin{theorem}\label{thm:RestrictedJump}
For every~ there exists a class of -smooth instances~ on
restricted related machines such that

assuming without loss of generality that .
\end{theorem}

\begin{proof}
It suffices to show the theorem for~ and . W.l.o.g.\ we
assume~ and set . Let~ be an
arbitrary integer, let

In the remainder we assume
that~. This is possible because we only want to derive
an asymptotic bound. We consider the following -smooth
instance~. The set~ of machines is partitioned into three
classes~, , and~ such that

The machine in~ has speed~, the
machines in~ have speed

and the machines in~ have speed~. Let the set~ of jobs be
partitioned into two subsets~ and~, consisting of

jobs whose processing requirements are independently and uniformly drawn from
 and from , respectively. The jobs in~
are only allowed to be scheduled on the machines in~, whereas the
jobs in~ are allowed to be scheduled on any machine.

First, we construct a schedule~ to bound the optimal makespan: Use the
list scheduling algorithm to schedule all jobs in~ on the machines in~, and all jobs
in~  on the machines in~. Figure~\ref{fig:jump-restricted-opt} depicts schedule~. Machine~ is a representative for all machines in class~.

\begin{artclfig}\newcommand{\height}{12em}
  \includegraphics[height=\height]{JumpRestrictedLB.pdf}
  \caption{Schedule~}
  \label{fig:jump-restricted-opt}
\end{artclfig}

Along the same lines as in~\cite{graham:1966}, it follows that for all machines~

Similarly,  for all machines~


Hence, . Before we proceed with
constructing a `bad' jump optimal schedule~, we observe that

due to .

We construct a jump optimal schedule~ on the -smooth
instance~ such that the corresponding makespan exceeds  with
high probability: Schedule all jobs in~ on the single machine
in~. Then, . Next, start assigning jobs
from~ to the machines in~ according to the list scheduling
algorithm with an arbitrary job permutation, until
\begin{itemize}
\item[(a)] either~ becomes empty, or until
\item[(b)]  for all~. If
there remain unscheduled jobs in~, then we assign them to
the machines in~ using list scheduling.
\end{itemize}
Let~ and let  denote the event that 
. If  occurs, then

due to Inequality~\eqref{eqref:UpperboundHats}, i.e., the
algorithm will end up in case~(b) as~ for any job~. This shows that no machine~ is critical. Using the
same argument as for the analysis of~ we can show that the load of any
machine~ is bounded from above by , i.e., the machine in~ is the
unique critical machine. As each job on this machine has processing requirement
at least~ and due to the property of the loads of the machines in~ in
case~(b), schedule~ is jump optimal and .

It remains to determine the probability~. For this, note that

as  by our initial assumption. On the other hand, .
Applying Hoeffding's Inequality~\cite{Hoeffding:1963} (see also Theorem~\ref{thm:app:hoeffding}), we obtain

which becomes arbitrarily close to~ when  increases. Hence, for sufficiently large integers~

\end{proof}


\begin{cor}
For every~ there exists a class of -smooth instances~ on
restricted identical machines such that

\end{cor}

\begin{remark}
In the proof of Theorem \ref{thm:RestrictedJump} we introduce an arbitrary integer . We argue that there exists a sufficiently large value for  such that the desired result follows. Choosing an even larger value for  implies that the results above not only hold in expectation but also with high probability.
\end{remark}












\subsection{Lex-jump Optimal Schedules on Restricted Identical Machines}
\label{subsec:lb-lex-jump-restricted}
In this subsection, we show that there exist instances with~ such that the smoothed performance guarantee for lex-jump optimal schedules in the restricted setting is in the same order as the worst case performance guarantee.

As in Section~\ref{subsec:lb-list-lex-jump}, we construct an instance with
several job classes and machine classes and the loads of the machines
are gradually decreasing with increasing machine class.
By setting the sets~ of allowed machines equal to the union of
only one or two machine classes and choosing to schedule the jobs on the
\emph{wrong} machines, we can enforce that jobs cannot leave the machine class
on which they are scheduled in the lex-jump
optimal solution, whereas the optimal makespan is still small.

\begin{theorem}
\label{thm:rest-id-lj}
For every~ there exists a class of -smooth instances~ on
restricted identical machines such that

\end{theorem}

First, we introduce the -smooth instance~ for~. 
Given an integer~, consider the following recurrence formula:

Starting with , the fraction  decreases with increasing index~ until it is less or equal~.
To see this, note that  implies that . Therefore, we know that . Furthermore, we can bound the ratio  from above by .
Let~ be the smallest integer~ such that . Hence,  is a strictly increasing sequence. We will bound the number~ from above later in the analysis.

We consider~ job classes  and as many machine
classes . Each machine class~ contains  machines with speed~. Each job class~ consists of two subclasses~ and~ of size~ and of size , respectively. The jobs in class~ are called type~ jobs, have processing requirements independently and uniformly distributed in~, and can be processed on machines in~. As a convention let . Jobs in class~ are called type~ jobs, have processing requirements independently and uniformly distributed in~, and can only be processed on machines in~.

The schedule  for an instance~ is obtained by scheduling the jobs in~ on the machines in~ using LPT (longest processing time) scheduling, i.e., list scheduling with a list in which the jobs are ordered according to non-increasing processing requirements. Note that the LPT algorithm first schedules all type~A jobs and then all type~B jobs. Schedule~ is visualized in Figure~\ref{fig:lex-restricted-bad}. Machine~ represents all machines in class~.

\begin{artclfig}\newcommand{\height}{12em}
  \includegraphics[height=\height]{LexRestrictedLB1.pdf}
  \caption{Schedule }
  \label{fig:lex-restricted-bad}
\end{artclfig}


We show that schedule~ is lex-jump optimal with high probability. To be more specific, we show lex-jump optimality when the values~ and~ are close to their expectations for all . Let~ and~ denote the events that

Moreover, let~ denote the event that the events~ and~ are simultaneously true for all . By~, , and~ we refer to the complement of~, , and~.

First, we analyze the sequence  to obtain bounds for the number~ of machine and job classes and for the number~ of machines.

\begin{lemma}
\label{lem:rest-id-lj:frac}
For any  the following inequality holds:

\end{lemma}

\begin{proof}
The claim is true for~. By definition of~,

for any  as . The claim follows by induction.
\end{proof}

Now, we can bound the number~ of job classes.

\begin{cor}
\label{cor:rest-id-lj:zk}
The number~ of machine classes and job classes is bounded by~.
\end{cor}

\begin{proof}
Applying Lemma~\ref{lem:rest-id-lj:frac} for  we obtain

Hence,

\end{proof}

\begin{lemma}
\label{lem:rest-id-lj:m}
The number~ of machines is bounded by  where~ denotes the gamma function and where .
\end{lemma}

\begin{proof}
By induction we show that

for any . Note that  due to Corollary~\ref{cor:rest-id-lj:zk}. For~ the claim holds since . For~ we apply Lemma~\ref{lem:rest-id-lj:frac} to get

The induction hypothesis for~ yields

Recalling  we can bound the number~ of machines by using

Hence, .
\end{proof}

\begin{lemma}
\label{lem:rest-id-lj:succ-prob}
Event~ occurs with probability at most~.
\end{lemma}

\begin{proof}
We bound the probability for the events~ and~ to occur. Recalling ,  (see Lemma~\ref{lem:rest-id-lj:frac}), , and  we obtain

and

Each of the first inequalities stems from Hoeffding's bound~\cite{Hoeffding:1963} (see also Theorem~\ref{thm:app:hoeffding}). A union bound yields

due to Corollary~\ref{cor:rest-id-lj:zk}.
\end{proof}

As event~ occurs with high probability and as

to prove Theorem~\ref{thm:rest-id-lj} it suffices to bound the expected value conditioned on event~ by . Therefore, in the remainder of this section we assume that event~ happens.

\begin{lemma}
\label{lemma:rest-id-lj:load-dif}
The loads of the machines within the same class differ only slightly. In particular,  for any machines .
\end{lemma}

\begin{proof}
Suppose to the contrary that there exist two machines 
such that . Recall that according to the LPT rule all type~ jobs will be assigned to the machines before the type~ jobs are assigned. After all type~ jobs  have been assigned to the machines in~, the difference in load between any two machines in~ is at most~ since~ for all jobs~.

Since the processing time of all type~ jobs is bounded by~,  implies that  no type~ job is assigned to machine~ nor to any machine that has load at least~. Hence, all type~ jobs are assigned to the machines that have load less than~. Note that there are at most  such machines.

As the difference in load between machine~ and any other machine in~ is
at most~, the total amount of processing requirements of type~ jobs in
class~ is bounded by 
contradicting the assumption that event~ holds.
\end{proof}

\begin{lemma}
\label{lemma:rest-id-lj:load}
For any machine~ the inequality

holds, i.e., the load of machine~ is close to the expected average machine load in class~.
\end{lemma}

\begin{proof}
By applying the triangle inequality we obtain

where the second inequality holds since~ and~ are true. The last inequality is due to Lemma~\ref{lemma:rest-id-lj:load-dif}.
\end{proof}

\begin{lemma}
\label{lem:rest-id-lj:lexjumpopt2}
Schedule~ is lex-jump optimal.\end{lemma}

\begin{proof}
We need to show that  holds for any machine~, any job~, and any machine~. Let~ be an arbitrary machine. First, consider the last job~ that has been assigned to~. Then,  for any machine~ as this job was assigned to machine~ by list scheduling. Furthermore, job~ is a smallest job on machine~ due to the LPT rule. Hence,  for any machine~ and any job~ assigned to machine~.

For type~ jobs on machine~ the set of allowed machines equals~. It just remains to show that  for any machine~ and any type~ job  with . Recalling  for~, , and  we observe that

for any . This implies

where the first and the last inequality are due to Lemma~\ref{lemma:rest-id-lj:load}.
\end{proof}

Finally, we can prove Theorem~\ref{thm:rest-id-lj}.

\begin{proof}[\ProofText{Theorem~\ref{thm:rest-id-lj}}]
As mentioned before, due to Lemma~\ref{lem:rest-id-lj:succ-prob} it suffices to bound the expected value conditioned on event~. If event~ holds, then schedule~ is lex-jump optimal (see Lemma~\ref{lem:rest-id-lj:lexjumpopt2}), i.e., , and has makespan

where the third inequality is due to the occurrence of~ and~. Now, consider the following schedule~:
\begin{itemize}

  \item For  spread the jobs of class~ evenly among the machines in class~. As , each machine is assigned exactly one type~ job.

  \item Spread the jobs of class~ evenly among the machines in class~. As , each machine is assigned at most one type~ job.

  \item For  spread the jobs of class~ evenly among the machines in class~. As , each machine is assigned exactly ~type~ jobs.

\end{itemize}
Note that with `evenly' we refer to the number of jobs on each machine and not to the load. Figure~\ref{fig:lex-restricted-opt} shows schedule~ where each machine~ is a representative for all machines in class~.

\begin{artclfig}\newcommand{\height}{12em}
  \includegraphics[height=\height]{LexRestrictedLB2.pdf}
  \caption{Schedule }
  \label{fig:lex-restricted-opt}
\end{artclfig}

As each machine contains at most ~type~ jobs and ~type~ jobs, the makespan of schedule~ and hence~ is bounded by . This implies  due to Lemma~\ref{lem:rest-id-lj:m}. Hence,

\end{proof}

\begin{remark}
The worst case upper bound on the performance guarantee for
lex-jump optimal schedules on restricted related machines is
, where ~\cite{Rutten:etal:2012}.
As for identical machines , i.e., each machine has speed~, the
upper bound matches the lower bound of Theorem~\ref{thm:rest-id-lj} up
to a constant factor and smoothing does also not improve the performance
guarantee for the worst lex-jump optimal schedules on restricted related
machines.

Lemma \ref{lem:rest-id-lj:succ-prob} established that  occurs with high probability. Hence, if we choose  suitably large, the stated results not only hold in expectation, but also with high probability.
\end{remark}




\section{Concluding Remarks}
\label{sec:concluding}

We have proven that the lower bounds for all scheduling variants with restricted machines
are rather robust against random noise, not only in expectation but even with high probability. We have also shown that the situation looks much
better for unrestricted machines where we obtained performance guarantees of~ and~ for the jump and lex-jump algorithm, respectively.
The latter bound also holds for the price of anarchy of routing on parallel links and
for the list scheduling algorithm, even when the order in which the jobs are presented to the algorithm can be chosen by the adversary when the realization of the processing times are known.

There are several interesting directions of research and we view our results
only as a first step towards fully understanding local search and greedy algorithms
in the framework of smoothed analysis. For example, we have only
perturbed the processing requirements, and it might be the case that the worst-case
bounds for the restricted scheduling variants break down if also the sets~
are to some degree random. In general it would be interesting to study different
perturbation models where the sets~ and/or the speeds~ are perturbed. 
Lemma~\ref{lemma.exponentially.decreasing.speeds} and Corollary~\ref{corol.many.small.jobs} indicate that there need to exist many machines having exponentially small speeds. We conjecture that if speeds are being smoothed, then the smoothed performance guarantee of near list schedules on restricted related machines is  as well.

Another interesting question is the following:
since we do not know which local optimum is reached, we have always looked at
the worst local optimum. It might, however, be the case that the local optima
reached in practice are better than the worst local optimum. It would be interesting
to study the quality of the local optimum reached under some reasonable assumptions on how
exactly the local search algorithms work. An extension in this direction would be 
to analyze the quality of coordination mechanisms under smoothing.

\section*{Acknowledgments}
We thank three anonymous referees for their valuable comments and suggestions 
that helped to improve the writing of the paper.




\begin{thebibliography}{10}

\bibitem{Angel:2006}
E.~Angel.
\newblock A survey of approximation results for local search algorithms.
\newblock In E.~Bampis, K.~Jansen, and C.~Kenyon, editors, {\em Efficient
  Approximation and Online Algorithms}, volume 3484 of {\em LNCS}, pages
  30--73. Springer-Verlag, Heidelberg, Germany, 2006.

\bibitem{DBLP:journals/jacm/AspnesAFPW97}
J.~Aspnes, Y.~Azar, A.~Fiat, S.~A. Plotkin, and O.~Waarts.
\newblock On-line routing of virtual circuits with applications to load
  balancing and machine scheduling.
\newblock {\em Journal of the {ACM}}, 44(3):486--504, 1997.

\bibitem{Awerbuch+etal:2006}
B.~Awerbuch, Y.~Azar, Y.~Richter, and D.~Tsur.
\newblock Tradeoffs in worst-case equilibria.
\newblock {\em Theoretical Computer Science}, 361:200--209, 2006.

\bibitem{Becchetti+etal:Smoothed:MOR}
L.~Becchetti, S.~Leonardi, A.~Marchetti-Spaccamela, G.~Sch{\"a}fer, and
  T.~Vredeveld.
\newblock Average case and smoothed competitive analysis for the multi-level
  feedback algorithm.
\newblock {\em Mathematics of Operations Research}, 31(3):85--108, 2006.

\bibitem{DBLP:journals/jcss/BeierV04}
R.~Beier and B.~V{\"o}cking.
\newblock Random knapsack in expected polynomial time.
\newblock {\em Journal of Computer and System Sciences}, 69(3):306--329, 2004.

\bibitem{Cho+Sahni:1980}
Y.~Cho and S.~Sahni.
\newblock Bounds for list schedules on uniform processors.
\newblock {\em SIAM Journal on Computing}, 9:91--103, 1980.

\bibitem{Voecking:2007}
A.~Czumaj and B.~V\"ocking.
\newblock Tight bounds for worst-case equilibria.
\newblock {\em Transactions on Algorithms {ACM}}, 3(1), 2007.

\bibitem{Englert+etal:SODA2007}
M.~Englert, H.~R\"oglin, and B.~V\"ocking.
\newblock Worst case and probabilistic analysis of the 2-opt algorithm for the
  {TSP}.
\newblock In {\em Proceedings of the 18th {ACM-SIAM} Symposium on Discrete
  Algorithms ({SODA})}, pages 1295--13004, 2007.

\bibitem{finn:horowitz:1979}
G.~Finn and E.~Horowitz.
\newblock A linear time approximation algorithm for multiprocessor scheduling.
\newblock {\em {BIT}}, 19:312--320, 1979.

\bibitem{Garey+Johnson:1979}
M.~R. Garey and D.~S. Johnson.
\newblock {\em Computers and Intractibility: A Guide to the Theory of
  {NP}-Completeness}.
\newblock W.H.~Freeman \& Co., New York, NY, 1979.

\bibitem{Glass07}
C.~A. Glass and H.~Kellerer.
\newblock Parallel machine scheduling with job assignment restrictions.
\newblock {\em Naval Research Logistics}, 54(3):250--257, 2007.

\bibitem{graham:1966}
R.~L. Graham.
\newblock Bounds for certain multiprocessing anomalies.
\newblock {\em Bell System Technical Journal}, 45:1563--1581, 1966.

\bibitem{graham:etal:79}
R.~L. Graham, E.~L. Lawler, J.~K. Lenstra, and A.~H.~G. {Rinnooy Kan}.
\newblock Optimization and approximation in deterministic sequencing and
  scheduling: a survey.
\newblock {\em Annals of Discrete Mathematics}, 5:287--326, 1979.

\bibitem{Hochbaum+Shmoys:1988}
D.~S. Hochbaum and D.~B. Shmoys.
\newblock A polynomial approximation scheme for machine scheduling on uniform
  processors: using the dual approximation approach.
\newblock {\em {SIAM} Journal on Computing}, 17:539--551, 1988.

\bibitem{Hoefer+Souza:2010}
M.~Hoefer and A.~Souza.
\newblock Tradeoffs and average-case equilibria in selfish routing.
\newblock {\em {ACM} Transactions on Computation Theory}, 2(1):article 2, 2010.

\bibitem{Hoeffding:1963}
W.~Hoeffding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock {\em Journal of the American Statistical Association},
  58(301):13--30, 1963.

\bibitem{Leung08}
J.~Y.~T. Leung and C.~L. Li.
\newblock Scheduling with processing set restrictions: A survey.
\newblock {\em International Journal of Production Economics}, 116:251--262,
  2008.

\bibitem{Li06}
C.~L. Li.
\newblock Scheduling unit-length jobs with machine eligibility restrictions.
\newblock {\em European Journal of Operational Research}, 174:1325--1328, 2006.

\bibitem{Michiels:etal:2007}
W.~P. A.~J. Michiels, E.~H.~L. Aarts, and J.~H.~M. Korst.
\newblock {\em Theoretical Aspects of Local Search}.
\newblock Springer-Verlag, Heidelberg, Germany, 2007.

\bibitem{Ou08}
J.~Ou, J.~Y.-T. Leung, and C.~L. Li.
\newblock Scheduling parallel machines with inclusive set restrictions.
\newblock {\em Naval Research Logistics}, 55(4):328--338, 2008.

\bibitem{Rutten:etal:2012}
C.~Rutten, D.~Recalde, P.~Schuurman, and T.~Vredeveld.
\newblock Performance guarantees of jump neighborhoods on restricted related
  parallel machines.
\newblock {\em Operations Research Letters}, 40:287--291, 2012.

\bibitem{Schaefer+Sivadasan:2005}
G.~Sch\"afer and N.~Sivadasan.
\newblock Topology matters: Smoothed competitiveness of metrical task systems.
\newblock {\em Theoretical Computer Science}, 341(1--3):3--14, 2005.

\bibitem{Schuurman:Vredeveld:2007}
P.~Schuurman and T.~Vredeveld.
\newblock Performance guarantees of local search for multiprocessor scheduling.
\newblock {\em Informs Journal on Computing}, 19(1):52--63, 2007.

\bibitem{Spielman+Teng:SA:2004}
D.~A. Spielman and S.~H. Teng.
\newblock Smoothed analysis of algorithms: Why the simplex algorithm usually
  takes polynomial time.
\newblock {\em Journal of the {ACM}}, 51(3):385--463, 2004.

\bibitem{Spielman+Teng:CACM2009}
D.~A. Spielman and S.~H. Teng.
\newblock Smoothed analysis: an attempt to explain the behavior of algorithms
  in practice.
\newblock {\em Communications of the {ACM}}, 52(10):76--84, 2009.

\bibitem{Voecking:2007:AGT}
B.~V\"ocking.
\newblock Selfish load balancing.
\newblock In N.~Nisan, T.~Roughgarden, E.~Tardos, and V.~Vazirani, editors,
  {\em Algorithmic Game Theory}, chapter~20. Cambridge University Press, New
  York, NY, USA, 2007.

\end{thebibliography}

\newpage
\appendix




\section{Table of notation}
\label{sec:appendix-table}

In the table below, the notation used in this paper is summarized.
\vspace{1em}

\begin{tabular}{|ll|} \hline
 & set of jobs  \\
 & set of machines  \\
 & processing requirement of job~ \\
 & speed of machine~ \\
 & set of machines on which job  can be scheduled \\
 & maximum speed of the machines \\
 & minimum speed of the machines;  \\
& by scaling we assume w.l.o.g. it to be . \\
 & optimal makespan \\
 & makespan of schedule  \\
 & set of jobs scheduling on machine  in schedule
 \\
 &  \\
 & load of
machine  in schedule . \\
 &  \\
 &  \\
 &  \\
 &  \\
 & , \\
& assuming   \\
 &  \\
 &  \text{ for } \\
 & .
\\ \hline
\end{tabular}


\section{Hoeffding's bound}

On several occasions in this paper we use Hoeffding's
bound~\cite{Hoeffding:1963} to bound tail probabilities. For
completeness, we state the bound in the following theorem.

\begin{theorem}
\label{thm:app:hoeffding}
Let  be independent random variables.
Define  and . If each 
for some constants  and , , then for any 

\end{theorem}
\end{document}
