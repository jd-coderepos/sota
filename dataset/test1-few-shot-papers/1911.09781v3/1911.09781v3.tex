

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{subcaption}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumitem}

\usepackage{amsmath,amsfonts,bm}

\def\argmax{\mathop{\rm argmax}}\def\argmin{\mathop{\rm argmin}}


\def\bx{\mathbf{x}} \def\bxp{\mathbf{x}^{\prime}} \def\bxpp{\mathbf{x}^{\prime\prime}} \def\tbx{\tilde{\mathbf{x}}} \def\hbx{\hat{\mathbf{x}}} \def\ex{e(\mathbf{x})}
\def\tex{e({\tilde{\mathbf{x}}})}

\def\by{\mathbf{y}} \def\byp{\mathbf{y}^{\prime}} \def\bypp{\mathbf{y}^{\prime\prime}} \def\tby{\tilde{\mathbf{y}}} \def\ty{\tilde{{y}}} \def\hby{\hat{\mathbf{y}}} \def\ey{e(\mathbf{y})}
\def\tey{e({\tilde{\mathbf{y}}})}

\def\vw{\bm{w}}



\def\bv{\mathbf{v}} \def\tv{\tilde{v}} \def\tbv{\tilde{\mathbf{v}}} 

 
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\DeclareMathOperator{\diag}{diag}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem*{proof*}{Proof}
\newtheorem{remark}{Remark}

\SetAlgoCaptionSeparator{.}
\renewcommand\AlCapFnt{\small}

\newcommand{\eg}{\emph{e.g.}} \newcommand{\Eg}{\emph{E.g}}
\newcommand{\ie}{\emph{i.e.}} \newcommand{\Ie}{\emph{I.e}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\aka}{\emph{aka.}}

\usepackage[accepted]{icml2020}

\icmltitlerunning{Submission and Formatting Instructions for ICML 2020}

\begin{document}

\twocolumn[
\icmltitlerunning{Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels}

\icmltitle{Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels}







\begin{icmlauthorlist}
\icmlauthor{Lu Jiang}{goo}
\icmlauthor{Di Huang}{cloud}
\icmlauthor{Mason Liu}{cor}
\icmlauthor{Weilong Yang}{goo}
\end{icmlauthorlist}

\icmlaffiliation{goo}{Google Research, Mountain View, United States}
\icmlaffiliation{cloud}{Google Cloud AI, Sunnyvale, United States}
\icmlaffiliation{cor}{Cornell University, Ithaca, United States}

\icmlcorrespondingauthor{Lu Jiang}{lujiang@google.com}

\icmlkeywords{Machine Learning, Robust Learning, Deep Learning, Noisy Data, ICML}
\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings. The data and code are released at the following link {\footnotesize \url{http://www.lujiang.info/cnlw.html}}.
\end{abstract}

\vspace{-3mm}

\section{Introduction}
Performing experiments on controlled noise is essential in understanding Deep Neural Networks (DNNs) trained on noisy labeled data. Previous work performs controlled experiments by injecting a series of synthetic label noises into a well-annotated dataset such that the dataset's noise level can vary, in a controlled manner, to reflect different magnitudes of label corruption in real applications. Through studying controlled synthetic label noise, researchers have discovered theories and methodologies that have greatly fostered the development of this field.

However, due to the lack of suitable datasets, previous work has only examined DNNs on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This leads to two major issues. First, as synthetic noise is generated from an artificial distribution, a tiny change in the distribution may lead to inconsistent or even contradictory findings. For example, contrary to the common understanding that DNNs trained on synthetic noisy labels generalize poorly~\citep{zhang2017understanding}, \citet{rolnick2017deep} showed that DNNs can be robust to massive label noise when the noise distribution is made slightly different. Due to the lack of datasets, these findings, unfortunately, have not yet been verified beyond synthetic noise in a controlled setting. Second, the vast majority of previous studies prefer to verify robust learning methods on a spectrum of noise levels because the goal of these methods is to overcome a wide range of noise levels. However, current evaluations are limited because they are conducted only on synthetic label noise.
Although there do exist datasets of real label noise \eg~WebVision~\citep{li2017webvision}, Clothing-1M~\citep{xiao2015learning}, \etc, they are not suitable for controlled evaluation in which a method must be systematically verified on multiple different noise levels, because the training images in these datasets are not manually labeled and hence their data noise level is fixed and unknown.

In this paper, we study a realistic type of label noise in a controlled setting called web labels. ``Webly-labeled'' images are commonly used in the literature~\cite{bootkrajang2012label,li2017webvision,krause2016unreasonable,chen2015webly}, in which both images and labels are crawled from the web and the noisy labels are automatically determined by matching the images' surrounding text to a class name during web crawling or equivalently by querying the search index afterward. Unlike synthetic labels, web labels follow a realistic label noise distribution but have not been studied in a controlled setting.

We make three contributions in this paper. First, we establish the first benchmark of controlled web label noise, where each training example is carefully annotated to indicate whether the label is correct or not. Specifically, we automatically collect images by querying Google Image Search using a set of class names, have each image annotated by 3-5 workers, and create training sets of ten controlled noise levels. As the primary goal of our annotation is to identify images with incorrect labels, to obtain a sufficient number of these images we have to collect a total of about 800,000 annotations over 212,588 images. The new benchmark enables us to go beyond synthetic label noise and study web label noise in a controlled setting. For convenience, we will refer it as web label noise (or \textbf{\emph{red noise}}) to distinguish it from synthetic label noise (or \textbf{\emph{blue noise}})\footnote{From the red and blue pill in the movie ``The Matrix‘’ (1999), where the red pill is used to refer to the truth about reality.}.

Second, this paper introduces a simple yet highly effective method to overcome both synthetic and real-world noisy labels. It is based on a new idea of minimizing the empirical vicinal risk using curriculum learning. We show that it consistently outperforms baseline methods on our datasets and achieves state-of-the-art performance on two public benchmarks of synthetic and real-world noisy labels. Notably, on the challenging benchmark WebVision 1.0~\citep{li2017webvision} that consists of 2.2 million images of real-world noisy labels, it yields a significant improvement of  in the top-1 accuracy, achieving the best-published result under the standard training setting.

Finally, we conduct the \emph{largest study} by far into understanding DNNs trained on noisy labels across a variety of noise types (blue and red), noise levels, training settings, and network architectures. Our study confirms the existing findings of \citet{zhang2017understanding} and \citet{arpit2017closer} on synthetic labels, and brings forward new findings that may challenge our preconceptions about DNNs trained on noisy labels. See the findings in Section~\ref{sec:exp_understanding}. It is worth noting that these findings along with benchmark results are a result of conducting thousands of experiments using tremendous computation power (hundreds of thousands of V100 GPU hours). We hope our (i) benchmark, (ii) new method, and (iii) findings will facilitate future deep learning research on noisy labeled data. We will release our data and code.


\section{Related Work}

\subsection{Datasets of noisy training labels}
While many types of noises exist \eg~image corruption noise~\citep{hendrycks2019benchmarking}, image registration noise~\citep{mnih2012learning}, or noise from adversarial attacks~\citep{zhang2019theoretically}, this paper focuses on label noise, and in particular web label noise -- a common type of label noise used in the literature. To the best of our knowledge, there have been no datasets of controlled web label noise. The closest to ours is the datasets of two types of noises: controlled synthetic label noise and uncontrolled web label noise.

In the dataset of controlled synthetic label noise, a series of synthetic label noises are injected into a well-annotated dataset in a controlled manner to reflect different magnitudes of label corruption in real applications. The most common one is the symmetric label noise, in which the label of each example is independently and uniformly changed to a random class with a controlled probability. Many works studied the symmetric label in controlled settings and presented their findings, including famous ones, like~\citep{zhang2017understanding} and~\citep{arpit2017closer}. The symmetric label is also commonly used as a benchmark to evaluate robust learning methods in a noise-control setting \eg~in \citep{vahdat2017toward,shu2019meta,jiang2018mentornet,ma2018dimensionality,han2018co,van2015learning,li2019learning,arazo2019unsupervised,charoenphakdee2019symmetric}. Other types of synthetic label noise were also proposed, including class-conditional noises~\citep{patrini2017making,rolnick2017deep,reeve2019fast,han2018masking}, noises from other datasets~\citep{wang2018iterative,seo2019combinatorial}, \etc. However, these noises are still synthetically generated from artificial distributions. Moreover, different works might use different parameters to generate such synthetic noises, which may make their results incomparable. See the example of~\citep{rolnick2017deep} in the introduction. 

In the dataset of uncontrolled web label noise, both images and labels are crawled from the web and the noisy labels are automatically determined by matching the images' surrounding text to a class name. This can be achieved by querying a search index~\citep{krause2016unreasonable,li2017webvision,mahajan2018exploring}. For example, In WebVision, \citet{li2017webvision} collected web images with noisy labels by querying Google and Flickr image search using the 1,000 class names from ImageNet. \citet{mahajan2018exploring} gathered a large scale set of images with noisy labels by searching hashtags on Instagram. However, these datasets do not provide ground-truth labels for training examples. Their noise level is hence fixed and unknown. As a result, they are not suitable for controlled studies in which different noise levels must be systematically examined. Besides, to get controlled web noise, it may not be a feasible option to annotate images in these datasets due to their imbalanced class distribution.

\subsection{Robust deep learning methods}
Robust learning is experiencing a renaissance in the deep learning era. Nowadays training datasets usually contain noisy examples. The ability of DNNs to memorize all noisy training labels often leads to poor generalization on the clean test data. Recent contributions based on deep learning handle noisy data in multiple directions including dropout~\citep{arpit2017closer} and other regularization techniques~\citep{azadi2016auxiliary,noh2017regularizing}, label cleaning/correction~\citep{reed2014training,goldberger2017training,Li2017ICCV,veit2017learning,song2019selfie}, example weighting~\citep{jiang2018mentornet,ren2018learning,shu2019meta,jiang2015self,liang2016learning}, 
cross-validation~\citep{northcutt2019confident}, semi-supervised learning~\citep{hendrycks2018using,vahdat2017toward,li2020dividemix,zhang2020distilling}, data augmentation~\citep{zhang2018mixup,cheng2019robust,cheng2020advaug,liang2020simaug}, among others. Different from prior work, we introduce a simple yet effective method to overcome both synthetic and real-world noisy labels. Compared with semi-supervised learning methods, our method learns DNNs without using any clean label.

\section{Dataset}\label{sec:dataset}

\begin{table*}[ht]
\centering
\caption{\label{tab:dataset}Overview of our datasets of controlled red (web) label noise. Blue (synthetic) label noise is also included for comparison.}
\begin{tabular}{lccccc}
\toprule
{\bf Dataset} & \multicolumn{1}{l}{{\bf \#Class}} & {\bf Noise Source} & {\bf Train Size} & {\bf Val Size} & {\bf Controlled Noise Levels (\%)} \\
\midrule
Red Mini-ImageNet & \multirow{2}{*}{100} & image search label & 50,000 & \multirow{2}{*}{5,000} & 0, 5, 10, 15, 20, 30, 40, 50, 60, 80 \\
Blue Mini-ImageNet &  & symmetric label flipping & 60,000 &  & 0, 5, 10, 15, 20, 30, 40, 50, 60, 80 \\ \hline
Red Stanford Cars & \multirow{2}{*}{196} & image search label & 8,144 & \multirow{2}{*}{8,041} & 0, 5, 10, 15, 20, 30, 40, 50, 60, 80 \\
Blue Stanford Cars &  & symmetric label flipping & 8,144 &  & 0, 5, 10, 15, 20, 30, 40, 50, 60, 80 \\
\bottomrule
\end{tabular}
\vspace{-5mm}
\end{table*}

Our goal is to create a benchmark of controlled noise that resembles a realistic label noise distribution. Unlike existing datasets such as WebVision or Clothing1M, our benchmark provides \emph{controlled label noise} where every single training example is carefully annotated by several human annotators.

Our benchmark is built on top of two public datasets: Mini-ImageNet~\citep{vinyals2016matching} for coarse-grained image classification and Stanford Cars~\citep{krause2013collecting} for fine-grained image classification.
Mini-ImageNet has images of size 84x84 with 100 classes from ImageNet~\citep{deng2009imagenet}. We use all 60K images for training and the 5K images in the ILSVRC12 validation set for testing. Stanford Cars contain 16,185 high-resolution images of 196 classes of cars (Make, Model, Year) split 50-50 into training and validation set. The standard train/validation split is used.


\subsection{Dataset Construction}

We build our datasets to replace synthetic noisy labels with web noisy labels in a controlled manner. To recap, let us revisit the construction of existing datasets of noisy labels. Synthetic noisy datasets are generated beginning with a well-labeled dataset. The most common type of synthetic label noise is called \emph{symmetric label} noise in which the label of each training example is independently changed to a random incorrect class with a probability  called noise level.\footnote{This is slightly different from \citep{zhang2017understanding} and is the same as~\citep{jiang2018mentornet}. We do not allow examples to be label flipped to their true labels. It makes  denote the exact noise level and independent of the total number of classes.}.
The noise level indicates the percentage of training examples with incorrect labels. As the true labels for all images are known, one can enumerate  to obtain training sets of different noise levels and use them in noise-control studies. On class-balanced datasets, this process is equivalent to sampling \% training images from each class and then replacing their labels with the labels uniformly drawn from other classes. The drawback is that the synthetic labels are artificially created and do not follow the distribution of real-world label noise.

On the other hand, there exist a few datasets of uncontrolled web label noise~\citep{xiao2015learning,li2017webvision,krause2016unreasonable}. In these datasets, the images are crawled from the web and their labels are automatically assigned by matching the images' surrounding text to a class name. This can be achieved by querying a search index.
These datasets contain noisy web labels.
However, as their training images are not manually labeled, their label noise level is fixed and unknown, rendering existing datasets unsuitable for controlled studies.

For our benchmark, we follow the construction of synthetic datasets with one important difference -- instead of changing the labels of the sampled clean images, we replace the clean images with incorrectly labeled web images while leaving the label unchanged. The advantage of this approach is that we closely match the construction of synthetic datasets while still being able to introduce controlled web label noise. 


\subsection{Noisy Web Label Acquisition}
We collect images with incorrect web labels in three steps: (1) images collection, (2) deduplication, and (3) manual annotation. In the first step, we combine images independently retrieved by Google image search from two sources: text-to-image and image-to-image search. For the text-to-image search, we formulate a text query for each class using its class name and broader category to retrieve the top images. For image-to-image search, we query the search engine using every training image in Mini-ImageNet and Stanford Cars. 
Finally, we union the two search results, where the text-to-image search results account for 82\% of our final benchmark. Note that the rationale for including a small amount of image-to-image results is to enrich the types of web label noises in our benchmark. We show that an alternative way to construct the dataset by removing all image-to-image results leads to consistent results in the Appendix C.

In the second step of deduplication, following~\citep{kornblith2019better}, we run a CNN-based duplicate detector over all images to remove near-duplicates to any of the images in the validation set. All images are retrieved under the usage rights ``free to use or share''~\footnote{\scriptsize \url{https://support.google.com/websearch/answer/29508}}. But we still recommend checking their actual usage right for the image.

Finally, the images are annotated using the Google Cloud labeling service. The annotators are asked to provide a binary question: ``is the label correct for this image?''. Every image is independently annotated by 3-5 workers and the final label is reached by majority voting. Statistics show annotators disagree only on a small proportion (11\%) of the total images. The remainder have unanimous labels agreed by at least 3 annotators.

\subsection{Dataset Overview}
In total, we collect about 800,000 annotations over 212,588 images, out of which there are 54,400 images with incorrect labels on Mini-ImageNet and 12,629 images on Stanford Cars. The remainder of the images have correct labels.
Using web images with incorrect labels, we replace \% of the original training images in the two datasets, and enumerate  in 10 different levels to create the controlled web label noise: . Similar to synthetic noise,  is made uniform across classes, \eg~ means that every class has roughly 20\% incorrect labels. In Appendix C, we show constructing the dataset only using web images leading to consistent results.

Table~\ref{tab:dataset} summarizes our benchmark. For comparison, we also include synthetic (symmetric) labels of the same 10 noise levels. We use \textbf{\emph{blue noise}} to denote the synthetic label noise and \textbf{\emph{red noise}} for the web label noise. The sizes of the red and blue training sets are made similar to better compare their difference\footnote{As existing findings on synthetic noise hold on both the full (60K) and subset (50K) of Blue Mini-ImageNet, we choose to report the results on the 60K full set. This results in a slightly larger Blue Mini-ImageNet but may not affect our main contributions}. Only a subset of web images with incorrect labels is used in our dataset. But we release all 212,588 images along with their annotations, which can be downloaded at {\footnotesize \url{http://www.lujiang.info/cnlw.html}} under the license of Creative Commons.

For lack of space, we use Fig. 1 in the Appendix to illustrate noisy images in each dataset. In summary, there are three clear distinctions between images with the synthetic and web label noise. Images with label noise from the web (or red noise) (1) a higher degree of similarity to the true positive images, (2) exist at the instance-level, and (3) come from an open vocabulary outside the class vocabulary of Mini-ImageNet or Stanford Cars.

\section{Method}\label{sec:methods}
In this section, we introduce a simple method called \emph{MentorMix} to overcome both synthetic and web label noise. As its name suggests, our idea is inspired by MentorNet~\cite{jiang2018mentornet} and Mixup~\cite{zhang2018mixup}. The main idea is to design a new robust loss to overcome noisy labels using curriculum learning and vicinal risk minimization.

\subsection{Background on MentorNet and Mixup}

Consider a classification problem with training set , where  denotes the  training image and  is an integer-valued noisy label over  possible classes and  is the corresponding one-hot label. Note that no clean labels are allowed to be used in training.
Let  denote the prediction of a DNN, parameterized by . MentorNet~\citep{jiang2018mentornet} minimizes the following objective:

where , or  for short, is the cross-entropy loss with the softmax function.  is the weight decay parameter on the  norm of the model parameters. For convenience, we make the weight decay regularization, data augmentation and dropout all subsumed inside .

Eq.~\eqref{eq:mentornet_obj} introduces the latent weight variable  for every training example. The regularization term  determines a curriculum~\cite{jiang2015self,jiang2014self,fan2017self} or equivalently a weighting scheme to compute the latent weight  to each example. In~\citep{jiang2018mentornet}, the weighting scheme is computed by a neural network called MentorNet. In training,  and  are alternatively minimized inside a mini-batch, one at a time while the other is held fixed. Only  is used at test time.


Mixup~\citep{zhang2018mixup} minimizes the empirical vicinal risk calculated from:

 and  are computed by the mixup function:

where  is drawn from the Beta distribution  controlled by hyperparameter . In practice, only the examples from the same mini-batch are mixed up during training.


\subsection{MentorMix}
In the proposed MentorMix, we minimize the empirical vicinal risk using curriculum learning. For simplicity, the self-paced regularizer ~\cite{kumar2010self,jiang2015self} is used and we have:

where  is a hyperparameter. It is easy to derive the optimal weighting scheme when the network parameter  is fixed:

where  is the indicator function and  denotes the objective when  is fixed.

Although Eq.~\eqref{eq:vstarij} gives a closed-form solution for computing the optimal weight, it is intractable to compute as this requires enumerating all pairs of training examples. 
We therefore resort to importance sampling to find the ``important'' examples. To do so, we define a stratum for each  and draw an example from the following distribution:

where  is the temperature in the softmax function and is fixed to 1 in our experiments.  specifies a density function over individual training examples. In theory, the distribution is defined over all training examples but, in practice to enable mini-batch training, we compute the distribution within each mini-batch (See Algorithm 1).  is the optimal weight for .  is calculated from  in Eq.~\eqref{eq:mentornet_obj} and can be conveniently obtained by MentorNet. As the optimal  can only have binary values according to Eq.~\eqref{eq:vstarij}, under the importance sampling we rewrite part of the objective in Eq.~\eqref{eq:mentormix_obj} as:

where the constant  will be dropped during training. According to Eq.~\eqref{eq:vstarij}, our goal is to find the mixed-up examples of smaller loss. For a given example (, ), the loss of the mixed-up example  tends to be smaller when  is small. Inspired by this idea, we sample  from  with respect to the weight  that is monotonically decreasing with its loss  . In this way examples of lower loss are more likely to be selected in the mixup.

{
\begin{algorithm}
\small
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\LinesNumbered
\Input{mini-batch ; two hyperparameters  and }
\Output{the loss of the mini-batch}

For every  in  compute  \;

Set  to be the -th percentile of the loss . \;

 \; {\footnotesize\tcp{update the moving average}}

  \; {\footnotesize\tcp{MentorNet weight}}

Compute , where  \;

Stop gradient \;

\ForEach{ } {
    Draw a sample  with replacement from   \;
    
     \;
    
      \;
    


     \;
    
     \;
    
    Compute  \;
    
    Weight  using a separate MentorNet \; {\footnotesize\tcp{optional}}
}
\Return 
\caption{{The proposed MentorMix method.}}
\label{alg:overall}
\end{algorithm}
\vspace{-1mm}
}

Algorithm 1 shows the four key steps to compute the loss for a mini-batch: weight (Step 2-4), sample (Step 5 and 8), mixup (Step 9-12), and weight again (Step 14), where the weighting is achieved by the MentorNet. Following our previous work~\citep{jiang2018mentornet}, we avoid directly setting  and adopt a moving average (in Step 3) to track the exponential moving average of the -th percentile of the mini-batch loss, in which  becomes the new hyperparameter. Step 4 computes the weight for individual example using a fixed MentorNet. The simplified MentorNet is used in our paper which is equivalent to computing the weight by a thresholding function . In Step 10, we assign a bigger (binary) weight between  and  to  unless its  is small. 
This trick is to stabilize importance sampling by encouraging each stratum to receive a bigger weight in the mixup. It leads to marginal performance gains but makes the training more robust to the choice of hyperparameters. In Step 14, the second weighting can be applied optionally using the weights produced by a separate MentorNet. This step is optional for low noise levels but is useful for high noise levels. Our algorithm has the same time and space complexity as that of the Mixup algorithm.

\section{Experiments}\label{sec:experiments}
This section verifies the proposed method on four datasets and presents new findings on web label noise. Specifically, in Section~\ref{sec:exp_method} we first verify the proposed method on our dataset and then compare it with the state-of-the-art on two public benchmarks of synthetic and real-world noisy labels. In Section~\ref{sec:exp_understanding}, we empirically examine DNNs trained on controlled noisy labels under various settings and present our findings that challenge our previous understandings.

\begin{table*}[ht]
\centering
\caption{Peak accuracy (\%) of the best trial of each method averaged across 10 noise levels. -- denotes the method failed to train.}
\label{tab:baseline}
\begin{tabular}{|c|cc|cc|cc|cc|}
\toprule
\multicolumn{1}{|c|}{\multirow{4}{*}{Method}} & \multicolumn{4}{c|}{Mini-ImageNet} & \multicolumn{4}{c|}{Stanford Cars} \\
\multicolumn{1}{|c|}{} & \multicolumn{2}{c|}{Fine-tuned} & \multicolumn{2}{c|}{Trained from scratch} & \multicolumn{2}{c|}{Fine-tuned} & \multicolumn{2}{c|}{Trained from scratch} \\
\multicolumn{1}{|c|}{} & Blue & Red & Blue & Red & Blue & Red & Blue & \multicolumn{1}{c|}{Red} \\
\hline
Vanilla&	82.31.9&	81.61.9&	58.310.3&	64.95.2&	70.016.8&	82.46.9&	53.824.4&	77.710.4\\
WeightDecay&	81.91.8&	81.51.8&	---& ---&	72.217.5&	84.36.6&	---&	---\\
Dropout&	82.81.3&	81.81.8&	59.39.5&	65.75.0&	71.716.9&	83.86.6&	62.823.5&	84.16.7\\
S-Model&	82.31.8&	82.01.9&	58.710.2&	64.65.1&	69.716.8&	82.47.1&	53.923.5&	77.610.2\\
Bootstrap&	83.11.6&	82.71.8&	60.19.7&	65.54.9&	71.716.9&	82.86.7&	55.623.9&	78.99.6\\
Mixup&	81.71.8&	82.41.7&	60.79.8&	66.04.9&	73.116.6&	85.06.2&	64.221.6&	82.58.0\\
MentorNet&	82.91.7&	82.41.7&	61.810.3&	65.15.0&	75.916.8&	82.66.6&	56.823.1&	78.98.9\\
{Our MentorMix}&	{\bf 84.20.7}&	{\bf 83.31.9}&	{\bf 70.93.4}&	{\bf 67.05.0}&	{\bf 78.216.2}&	{\bf 86.95.5}&	{\bf 67.723.0} &	{\bf 83.67.5}\\
\bottomrule
\end{tabular}
\vspace{-3mm}
\end{table*}
\subsection{Method Comparison}\label{sec:exp_method}

\noindent\textbf{Evaluation metrics}: Following prior works, the \emph{peak accuracy} is used as the primary evaluation metric that denotes the maximum accuracy on the clean validation set throughout the training. In addition, the \emph{final accuracy} is also reported in the Appendix D \ie~the validation accuracy after training has converged at the final training step.

\noindent\textbf{Training setting}: On our benchmark, all methods are trained on the noisy training sets of two noise types (blue and red) under 10 noise levels (from 0\% to 80\%), and tested on the same clean validation set. Two training settings are considered: (i) training from scratch and (ii) fine-tuning from an ImageNet checkpoint where the checkpoint is pretrained on the ImageNet training data. See details in the Appendix. On our datasets, Inception-ResNet-v2~\citep{szegedy2017inception} is used as the default backbone for all methods, where we upsample the images in the Mini-ImageNet dataset from 84x84 to 299x299 so that we can keep using the same pretrained ImageNet checkpoint. On the public benchmarks in Section~\ref{exp:method_soa}, we train networks from scratch using ResNet-32 for CIFAR and Inception-ResNet-v2 for WebVision.

\noindent\textbf{Baselines and our method}: On our dataset, MentorMix is compared against the following baselines. 
We extensively search the hyperparameter for each method on every noise level.
\textit{\textbf{Vanilla}} is the standard training using  weight decay, dropout, and data augmentation. \textit{\textbf{Weight Decay}} and \textit{\textbf{Dropout}}~\citep{srivastava2014dropout} are classical regularization methods. We search the hyperparameter for the weight decay in  and the dropout ratio in  as suggested in~\citep{arpit2017closer}. \textbf{\emph{Bootstrap}}~\citep{reed2014training} corrects the loss with the learned label. The soft version is used and the hyperparameter for the learned label is tuned in . \textit{\textbf{S-model}}~\citep{goldberger2017training} is another way to ``correct'' the predictions by appending a new layer to a DNN to learn noise transformation. \textit{\textbf{MentorNet}}~\citep{jiang2018mentornet} is an example-weighting method. We employ the predefined MentorNet and search the hyperparameter -percentile in . \textit{\textbf{Mixup}}~\citep{zhang2018mixup} is a robust learning method that minimizes the empirical vicinal risk. Following the advice in~\citep{zhang2018mixup} its hyperparameter  is searched in .

We implement MentorMix in Algorithm~\ref{alg:overall} in TensorFlow. We search two hyperparameters  in the  in the range  and . The code will be released to reproduce our results.

\subsubsection{Baseline Comparison}
We first show the comparison to the baseline methods on our dataset. For the lack of space, Table~\ref{tab:baseline} shows a high-level summary of the comparison result, in which each cell shows a method's average best peak accuracy (and 95\% confidence interval) across all 10 noise levels. Table 4 - Table 7 in the Appendix D list detailed results on each noise level.

As shown, the proposed method consistently outperforms the baseline methods across noise types (red and blue) and training settings (finetuning and training from scratch). This is desirable as baseline methods that work well on blue noise may not show consistent improvement over red noise, or vice versa. The red noise appears to be less harmful. Yet it is more difficult to overcome, which suggests the need for a new benchmark for a more comprehensive evaluation. Nevertheless, our method yields consistent improvements on both synthetic and web noisy labels. 

\begin{figure}[ht]
\vspace{-2mm}
\centering
\begin{subfigure}[b]{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/mentormix_blue.pdf}
    \vspace{-5mm}
    \caption{Blue Mini-ImageNet}
\end{subfigure}
\begin{subfigure}[b]{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/mentormix_red.pdf}
    \vspace{-5mm}
    \caption{Red Mini-ImageNet}
\end{subfigure}
\vspace{-2mm}
\caption{\label{fig:mentormix_training}Comparison of training and validation accuracy during training. The dataset is Mimi-ImageNet at the 50\% noise level.}
\vspace{-3mm}
\end{figure}

The primary reason for our superior performance is that MentorMix can leverage MentorNet and Mixup in a complementary way. Technically, it uses MentorNet weight to identify examples with ``cleaner'' labels and encourages them to be used in the mixup operation. \cite{jiang2018mentornet} showed that MentorNet optimizes an underlying robust loss in empirical risk minimization. From this perspective, our MentorMix introduces a new robust loss to minimize the vicinal risk which turns out to be more resilient to noisy training labels. For example, Fig.~\ref{fig:mentormix_training} compares Vanilla and MentorMix on the blue and red Mini-ImageNet at 50\% noise level. It shows that MentorMix's loss is more robust to noisy labels, and MentorMix improves the peak accuracy of Vanilla by 16.3\% on blue noise and 2.4\% on red noise. 

As the noise levels span across a wide range, we find the hyperparameters of robust
learning methods are important. For the same method, a careful hyperparameter search could well be the difference between good and bad performance. As shown in the Appendix, we find that our method is relatively less sensitive to the choice of hyperparameters.


\begin{figure*}[ht!]
\vspace{-3mm}
\centering
\includegraphics[width=0.9\textwidth]{figures/vanilla.pdf}
\vspace{-3mm}
\caption{\label{fig:vanilla}DNNs trained on synthetic (blue) and web label noise (red) on Mini-ImageNet (top) and Stanford Cars (bottom).}
\vspace{-3mm}
\end{figure*}


\subsubsection{Comparison to the state-of-the-art}\label{exp:method_soa}
In this subsection, we compare MentorMix on two public benchmarks of synthetic and real-world noisy labels. Table~\ref{tab:cifar} compares with the state-of-the-art on the CIFAR dataset with symmetric label noise, where the top shows the classification accuracy on the clean validation set of CIFAR-100 and the bottom is for CIFAR-10. As all methods are trained using networks of similar capacity (ours is ResNet-32), we cite the numbers reported in their papers except for MentorNet and Mixup. Our method achieves the best accuracy across all noise levels. The results validate that our method is effective for synthetic noisy labels.

\begin{table}[ht!]
\vspace{-3mm}
\centering
\small
\caption{Comparison with the state-of-the-art in terms of the validation accuracy on CIFAR-100 (top) and CIFAR-10 (bottom).}
\label{tab:cifar}
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{Data} & \multirow{2}{*}{ Method} & \multicolumn{4}{c}{ Noise level (\%)} \\
 &  & 20 & 40 & 60 & 80 \\ 
\midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{CIFAR100}} 
 & \citet{arazo2019unsupervised} & 73.7 & 70.1 & 59.5 & 39.5 \\
 & \citet{zhang2018generalized} & 67.6 & 62.6 &54.0 &29.6 \\
 & MentorNet~(\citeyear{jiang2018mentornet}) & 73.5 & 68.5 & 61.2 & 35.5 \\
 & Mixup~(\citeyear{zhang2018mixup}) & 73.9 & 66.8 & 58.8 & 40.1 \\
 & \citet{huang2019o2u}& 74.1&69.2&39.4&-\\
 & Ours (MentorMix) & {\bf 78.6} & {\bf 71.3} & {\bf 64.6} & {\bf 41.2} \\
 \hline
 \multirow{7}{*}{\rotatebox[origin=c]{90}{CIFAR10}} 
 & \citet{arazo2019unsupervised} & 94.0 & 92.8 & 90.3 & 74.1 \\
 & \citet{zhang2018generalized} & 89.7 & 87.6 &82.7 &67.9 \\
 & \citet{lee2019robust} & 87.1& 81.8& 75.4& -\\
 & \citet{chen2019understanding} & 89.7 & - & - & 52.3 \\
 & \citet{huang2019o2u}& 92.6&90.3&43.4&-\\
 & MentorNet~(\citeyear{jiang2018mentornet}) & 92.0 & 91.2 & 74.2 & 60.0 \\
 & Mixup~(\citeyear{zhang2018mixup}) & 94.0& 91.5& 86.8& 76.9\\
 & Ours (MentorMix) & {\bf 95.6} & {\bf 94.2}& {\bf 91.3} & {\bf 81.0} \\ 
\bottomrule
\end{tabular}
\vspace{-3mm}
\end{table}



We then compare with the state-of-the-art on the challenging benchmark WebVision 1.0~\cite{li2017webvision} that contains 2.4 million training images of noisy labels from the web. It uses the same 1,000 classes in the ImageNet ILSVRC12 challenge and thus is evaluated on the same validation set. Following prior studies, we train our method on both the full training set (2.4M images on 1K classes) and the mini subset (61K images on 50 classes), and test it on two clean validation sets from ImageNet ILSVRC12 and WebVision.


Table~\ref{tab:webvision} shows the comparison results, where the method marked by  uses extra clean labels during training. As shown, the proposed MentorMix improves the prior state-of-the-art by about 3\% in the top-1 accuracy on the ILSVRC12 validation set without using any extra labels. It is worth noting that  is a significant improvement on the ILSVRC12 validation~\citep{deng2009imagenet}. To the best of our knowledge, it achieves the best-published result on the WebVision 1.0 benchmark under the same training setting. The results show that our method is effective for real-world noisy labels. We also apply our method on the Clothing-1M dataset where we train only on the 1M noisy training examples. Our model gets 74.3\% accuracy which is competitive to recent published works.

\begin{table}[ht!]
\vspace{-3mm}
\centering
\caption{Comparison with the state-of-the-art on the clean validation set of ILSVRC12 and WebVision. The number outside (inside) the parentheses denotes the top-1 (top-5) classification accuracy(\%).  marks the method trained using extra clean labels.}
\label{tab:webvision}
\small
\begin{tabular}{c|lcc}
\toprule
{Data} & {Method} & {ILSVRC12} & {WebVision} \\
\midrule
Full& \citet{lee2017cleannet}& 61.0(82.0) & 69.1(86.7) \\
Full&Vanilla & 61.7(82.4) & 70.9(88.0) \\
Full& MentorNet~(\citeyear{jiang2018mentornet})& 64.2(84.8) & 72.6(88.9) \\
Full& \citet{guo2018curriculumnet}& 64.8(84.9) & 72.1(89.2) \\
Full& \citet{saxena2019data} & ---& 67.5(-----) \\
Full&Ours (MentorMix) & {\bf 67.5(87.2)} & {\bf 74.3(90.5)} \\
\hline
Mini& MentorNet~(\citeyear{jiang2018mentornet}) & 63.8(85.8) & --- \\
Mini& \citet{chen2019understanding} & 61.6(85.0) & 65.2(85.3) \\
Mini& Ours (MentorMix) & {\bf 72.9(91.1)} & {\bf 76.0(90.2)} \\
\bottomrule
\end{tabular}
\vspace{-3mm}
\end{table}

\subsection{Understanding DNNs trained on noisy labels} \label{sec:exp_understanding}
In this subsection, we conduct a large study into understanding DNNs trained on noisy labels across noise levels, noise types, training settings, and network architectures. We focus on three important findings~\citep{zhang2017understanding,arpit2017closer,kornblith2019better}. These works examine vanilla DNN training either on controlled synthetic labels (the former two) or on clean training labels (the last one). Our goal is to revisit them on our benchmark in a controlled setting where the noise in training sets varies from completely clean (0\%) to the level where 80\% of training labels are incorrect. 

As in these works, we learn DNNs using vanilla training which allows us to compare their findings. Training DNNs using robust learning methods would probably lead to different findings but that would make the findings undesirably depend on the specific method being used. Regarding the network architectures, by default we use Inception-ResNet-v2 and also compare six other architectures: EfficientNet-B5~\citep{tan2019efficientnet}, MobileNet-V2~\citep{sandler2018mobilenetv2}, ResNet-50 and ResNet-101~\citep{he2016deep}, Inception-V2~\citep{ioffe2015batch}, and Inception-V3~\citep{szegedy2016rethinking}. We select the above architectures to be representative of diverse capacities, the accuracy of which on the ILSVRC 2012 validation covers a wide range from 71.6\% to 83.6\%. See more details in the Appendix B.




{\bf DNNs generalize much better on red label noise.} \citet{zhang2017understanding} found that the generalization performance of DNNs drops sharply as the level of noisy training labels increases. This pivotal finding that DNNs generalize poorly on noisy training labels has influenced many works. We first confirm \citet{zhang2017understanding}'s finding on blue noise in Fig~\ref{fig:vanilla}, where the training and validation accuracies are shown along with the training steps. The dashed and solid curves represent the validation accuracies for 0\% (or clean) and 40\% noise levels, respectively, and the color belt plots the 95\% confidence interval of the 10 noise levels.

After the training converges in Fig~\ref{fig:vanilla}, the difference between the dashed and solid curves (in blue) indicates a palpable performance degradation between the clean (0\%) and noisy (40\%) labels. This can also be seen from the greater width of the blue belt which denotes the accuracy's confidence interval over all 10 noise levels. This confirms \citet{zhang2017understanding}'s finding on synthetic noisy labels. However, the difference is much smaller on the red curves, suggesting DNNs generalize much better on the red noise. This pattern is consistent across our two datasets using both fine-tuning and training from scratch. We hypothesize that DNNs are more robust to web labels because they are more relevant (visually or semantically) to the clean training images. See Fig. 1 in the Appendix.


\begin{figure}[ht]
\vspace{-3mm}
\centering
\includegraphics[width=0.35\textwidth]{figures/vannilla_training_drop.pdf}
\vspace{-5mm}
\caption{\label{fig:drop}Performance drop from the peak accuracy at different noise levels. Colors are used to differentiate noise types.}
\vspace{-3mm}
\end{figure}

{\bf DNNs may not learn patterns first on red label noise.} \citet{arpit2017closer} found that DNNs learn patterns first, revealing an interesting property that DNNs are able to automatically learn generalizable ``patterns'' in the early training stage before memorizing all noisy training labels. This can be manifested by the gap between the peak and final accuracy, as shown in Fig.~\ref{fig:vanilla}(a). A larger drop suggests a better pattern is found during the early training stage. For better visualization, Fig.~\ref{fig:drop} computes the relative difference, namely the drop, between the peak and final accuracy across noise levels. As shown, the blue curves show a significant drop as the noise level grows. This is consistent with \citet{arpit2017closer}'s finding on synthetic label noise.

Interestingly, the drop on the red noise is considerably smaller and even approaches zero on the Stanford Cars dataset. This suggests DNNs may not learn patterns first on red label noise at least for the fine-grained classification task. Our hypothesis is that images of real-world label noise are more complicated than those of the synthetic noise because they are sampled non-uniformly from an infinite number of classes. Therefore, it is much more difficult for DNNs to capture meaningful patterns automatically.

\textbf{ImageNet architectures generalize on noisy labels when the networks are fine-tuned.} \citet{kornblith2019better} found that fine-tuning better architectures trained on ImageNet tend to perform better on downstream tasks of clean training labels. It is important to verify whether this holds on noisy training labels because if so, one can conveniently transfer better architectures to better overcome the noisy labels.
\begin{figure}[ht]
\vspace{-1mm}
\centering
\begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{figures/backbone-imagenet.pdf}
    \vspace{-5mm}
    \caption{Mini-ImageNet ()}
\end{subfigure}
\begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{figures/backbone-cars.pdf}
    \vspace{-5mm}
    \caption{Stanford Cars ()}
    \vspace{-3mm}
\end{subfigure}
\vspace{-2mm}
\caption{\label{fig:backbones}Fine-tuning seven ImageNet architectures on the red and blue datasets. The number in parentheses is the Pearson correlation between the architecture's ImageNet accuracy and the fine-tuning accuracy on our dataset of red noise.}
\vspace{-3mm}
\end{figure}

Following~\citep{kornblith2019better}, in Fig.~\ref{fig:backbones}, we compare the fine-tuning performance using ImageNet architectures, where the -axis is the accuracy of the pretrained architectures on ImageNet, and the -axis denotes the peak accuracy on our datasets. The bar plots the 95\% confidence interval across 10 noise levels, where the center dot marks the mean. As it shows, there is a reasonable correlation between the ImageNet accuracy and the validation accuracy on both red and blue noisy labels. The Pearson correlation for the red noise is 0.91 on Mini-ImageNet and 0.88 on Stanford Cars. This indicates a better-pretrained architecture is likely to perform better even when it is fine-tuned on noisy training labels. We do not find such correlation when these architectures are trained from scratch. These results extend \citet{kornblith2019better}'s finding to noisy training data, and suggest when possible one may use more advanced pretrained architectures to overcome noisy training labels.

\section{Conclusions}
In this paper, we study web label noise in a controlled setting. We make three contributions. First, we establish the first benchmark of controlled web noise obtained from image search. Second, a simple but effective method is proposed to overcome both synthetic and real-world noisy labels. Our method achieves state-of-the-art results on multiple datasets. 

Finally, we conduct the largest study by far into understanding deep learning on noisy data across a variety of settings. Our studies reveal several new findings: (1) DNNs generalize much better on web label noise; (2) DNNs may not learn patterns first on web label noise in which early stopping may not be very effective; (3) when networks are fine-tuned, ImageNet architectures generalize well on both symmetric and web label noise; (4) methods that perform well on synthetic noise may not work as well on the real-world noisy labels from the web; (5) the proposed method yields consistent improvements on both synthetic and real-world noisy labels from the web.

Based on our observations, we arrive at the following recommendations for training deep neural networks on noisy training data.
\begin{itemize}
    \item A simple way to deal with noisy labels is to fine-tune a model that is pre-trained on clean datasets, like ImageNet. The better the pre-trained model is, the better it may generalize on downstream noisy training tasks.
    \item Early stopping may not be as effective on the label noise from the web, especially on the fine-grained classification task.
    \item Methods that perform well on synthetic noise may not work as well on the real-world noisy labels from the web. The proposed MentorMix can better overcome both synthetic and real-world web noisy labels.
    \item The real-world label noise from the web appears to be less harmful, yet it is more difficult for our current robust learning methods to tackle. This encourages more future research to be carried out on controlled label noise from the web. 
\end{itemize}

\section*{Acknowledgements}
The authors would like to thank anonymous reviewers for helpful comments and Boqing Gong and Fei Sha for meaningful discussions and kind support. The data labeling is supported by the Google Cloud labeling service.



\bibliography{cr_paper}
\bibliographystyle{icml2020}

\end{document}
