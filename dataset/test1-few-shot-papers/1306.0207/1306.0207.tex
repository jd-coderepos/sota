\documentclass[11pt]{article}






\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{cite}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}


\DeclareMathOperator*{\argmin}{\arg\!\min}

\widowpenalty=0 \clubpenalty=0

\allowdisplaybreaks







\newcommand{\john}[1]{{\textcolor{red}{({\sc John Says:} #1)}}}

\title{In pursuit of the dynamic optimality conjecture}



\author{
John Iacono\thanks{Research supported by NSF grants CCF-1018370 and 0430849.}
}

\date{
  Polytechnic Institute of New York University, Brooklyn, New York, USA 
  }




\begin{document}

\maketitle

\begin{abstract}
In 1985, Sleator and Tarjan introduced the splay tree, a self-adjusting binary search tree algorithm. Splay trees were conjectured to perform within a constant factor as any offline rotation-based search tree algorithm on every sufficiently long sequence---any binary search tree algorithm that has this property is said to be dynamically optimal. However, currently neither splay trees nor any other tree algorithm is known to be dynamically optimal.  Here we survey the progress that has been made in the almost thirty years since the conjecture was first formulated, and present a binary search tree algorithm that is dynamically optimal if any binary search tree algorithm is dynamically optimal.
\end{abstract}

\bibliographystyle{alpha}


\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\IRB}{\mathrm{IRB}}
\newcommand{\ALT}{\mathrm{ALT}}


\section{Introduction}

A \emph{binary search tree (BST)} is a classic structure of computer science. A binary search tree stores a totally ordered set of data and supports the operations of insert, delete, and predecessor queries. Here we focus only on predecessor queries which we call \emph{searches}.
Since there are no insertions and deletions, we can assume the tree contains the integers from 1 to .

To execute each search in the BST model, there is a single pointer that starts at the root of the tree, and at unit cost can move the pointer to the left child, right child, parent, or perform a rotation with the parent (we call these \emph{BST unit-cost primitives}). In order to properly execute the search it is required that the result of the search be touched by the pointer in the course of executing the search. This model was formalized in \cite{DBLP:journals/siamcomp/Wilber89}.

We consider search sequences  of length : . To avoid issues with small sequences and the initial state of the tree, we assume  is sufficiently long (often only  is needed) and that the tree is in some canonical initial state. 
A BST-model algorithm is simply a way of choosing a sequence of BST unit cost primitives to execute each search. A BST-model algorithm is \emph{online} if its choice of BST unit cost primitives to execute search  is a function of . The online BST model is still very permissive, as only BST-model unit cost operations are counted, and unlimited computation could be done to determine these operations. What is normally thought of as a BST is an online BST model algorithm that can be implemented on a BST where  bits of data can be augmented on every node, and where unit cost operations are chosen based on the current search, the contents of the node currently pointed to, including any augmented data, and  bits of global state. Such a BST algorithm is called a \emph{real-world BST}, a term coined by \cite{DBLP:conf/icalp/BoseCFL12}.
We let  denote the cost in the BST model to execute  using some BST-model algorithm .

Let  be the fastest runtime of any BST that can execute ; that is . Given enough time (i.e.~exponential in ),  can be computed exactly, and an offline algorithm  such that  can be produced.

Splay trees are a BST structure introduced by Sleator and Tarjan \cite{DBLP:journals/jacm/SleatorT85}. They use a simple restructuring heuristic, to move the searched item to the root. This heuristic has the following effect on nodes other than the one searched: if the node  is at depth  and  of the ancestors of  are on the search path, after the search  will be at depth . The work of \cite{DBLP:journals/jal/Subramanian96} explores a class of heuristics that have the same general properties of splay trees. Splay trees have been proven to have a number of amortized bounds on their performance, including such basic facts as  amortized runtime per search. However, the focus of this work is on the \emph{dynamic optimality conjecture}:


\begin{conjecture}[Dynamic Optimality Conjecture \cite{DBLP:journals/jacm/SleatorT85}]

\end{conjecture}

We refer to any BST algorithm  such that  for some  as \emph{dynamically optimal}.
Rather then focus on splay trees themselves, we focus on whether there are any dynamically optimal BSTs. We present several different formulations of this, from weakest to strongest.


\paragraph{Offline optimality.} It is possible to compute in polynomial time (in, say, the RAM model) an algorithm  such that ? As we have noted that computing such an algorithm is possible, given enough time, this question concerns only running time, and is the easiest of the questions presented. We believe that computing  exactly is likely to be NP-complete. NP completeness for this very closely related problem was presented in \cite{DBLP:conf/soda/DemaineHIKP09}: instead of a sequence of single searches to be executed on a BST, a sequence of sets of items to be searched are provided and the algorithm can order the searches in each set in whatever manner is beneficial to it. Computing the exact optimal cost for executing such a sequence of sets of searches was shown to be NP-complete. 

\paragraph{Online optimality.} Is there an online BST algorithm  such that ?
In this statement of the problem,  could do significant computation in order to determine which BST unit-cost operation to perform at every step, subject only to the requirement that it is online. This conjecture represents the claim that there is no asymptotic difference in power between online and offline algorithms in the BST model. Such equivalence in power between online and offline power is generally not possible in more permissive models, and is typically only found in very strict models such as the optimality of the move-to-front rule for search in a linked list \cite{DBLP:journals/cacm/SleatorT85}. In more permissive models like the RAM, an offline algorithm could fill an array  such that  and thus could trivially achieve offline performance that an online algorithm could never match. 


\paragraph{Online real-world optimality.} The end goal of this line of research is to obtain, not just an online BST algorithm  such that , but one where the runtime in the BST model dominates the runtime and which could be reasonably implemented. The real-world BST model gives a formalization of that goal, and data structures such as splay trees meet the definition of a real-world BST.

Our hope is that solving the offline optimality problem is the bottleneck, and that a solution to that could be transformed into an online real-world algorithm.


We begin this survey by reviewing a geometric view of the problem. We then summarize the results on the lower and upper bounds for . Finally we present a conditional result whereby a concrete online algorithm is presented which is dynamically optimal if any online algorithm is dynamically optimal. Throughout the presentation we try to identify possible avenues for improvement in as well as the challenges of each of the approaches presented.

\section{Geometric view}








We now present a geometric view of a binary search tree algorithm , first introduced in~\cite{DBLP:conf/soda/DemaineHIKP09}. This geometric view was created in the hope that reasoning with this view would be significantly simpler than reasoning with rotation-based trees.

Suppose you have a BST algorithm executing some search sequence . Let the set of points  be the \emph{geometric view} of this sequence. Consider the execution of a BST algorithm  on this sequence. Let  be all the nodes touched by the pointer in executing . Then let ; that is,  is in  if algorithm  when executing  touches . Observe that  for any algorithm , since the item to be searched must be touched in the execution of  by any valid algorithm. This thus gives  as a plot of everything touched in an execution, seemingly stripping away the specific pointer movements and rotations performed. Also, the runtime of  on  is .

Call two points  a set of points  \emph{arborally satisfied (AS)} if there is at least one other point in  in or on the orthogonal rectangle they define. Call 
a set of points  an \emph{arborally satisfied set (ASS)} if all pairs of points  that differ in both coordinates are AS.
We have shown that all point sets  are ASS. Moreover, we have shown that given a ASS point set  where , there is a BST algorithm  with runtime  such that . 

Thus, the following two problems are equivalent: (1) find an -competitive offline BST to execute , and (2) find an -factor approximation of a minimal ASS superset of .


One can observe a kind of duality between the time a key is searched and its value.
Suppose  is a permutation of . Then the transpose of  is also a permutation  and represents some sequence  where if  in  then  in . 
As the horizontal and vertical coordinates are treated symmetrically in the definition of ASS, it follows that . 

Note that this statement of the geometric view is offline. An algorithm for computing an ASS superset  of  is said to be online if the points of  are only a function of the points of  with the same or smaller -coordinate. Note that an online BST algorithm yields an online ASS superset algorithm directly from the preceding definitions. However, the conversion in the other direction does not preserve online-ness. The method used by Fox \cite{DBLP:conf/wads/Fox11} to turn a particular online ASS superset algorithm into an online BST algorithm could probably be adapted to turn any online ASS superset algorithm into an online BST algorithm.

While computing  offline seems like a clean geometric optimization problem, a solution has remained elusive. The main stumbling block is that it is fairly easy to come up with a minimal superset  of  such that all pairs of points in  are AS with respect to ; the problem is that the points in  must all also be pairwise AS for the set  to be ASS.


\section{Lower bounds}

One defining feature of this problem is the existence of non-trivial lower bounds on  for particular fixed , as opposed to lower bounds where  is drawn from a distribution. There are several known bounds, we present each of them separately.

\paragraph{Independent rectangle bound \cite{DBLP:conf/soda/DemaineHIKP09}.}
Given a set of rectangles , each of which is defined by two points in , we say  is an independent set of rectangles if no corner of one rectangle lies properly inside of another. Define  to be the size of the maximum set of independent rectangles possible with respect to point set . It has been shown that .

\begin{figure}
\begin{center}
\includegraphics[width=2in]{ir}
\end{center}
\caption{Here we consider all combinatorial cases of overlapping -rectangles are independent, and when they are not. The top three pairs of rectangles are independent; the bottom two are not.}
\label{fig:ir}
\end{figure}

Computing a value which  is  can be done with a simple sweep algorithm. We call an unsatisfied (i.e. not AS) rectangle defined by two points a -type rectangle if its upper point is to the right of its lower point. Let  to be the point set obtained from  by repeatedly looking at the lowest   type unsatisfied rectangle (with lowest upper coordinate), and adding a point to the set in the upper-right corner of this rectangle. This process is repeated until no more unsatisfied -type rectangles remain.  is defined in a symmetric way. See Figure~\ref{geoview} for an illustration of the result of this process. We have shown that .



The independent rectangle bound is the best known lower bound on , but there are two other bounds that predate it due to Wilber, which are interesting in their own right. They were both introduced at the same time in \cite{DBLP:journals/siamcomp/Wilber89} using the language of BSTs and we briefly state them here in the language of the geometric representation.



\paragraph{Alternation bound.} The alternation bound can be computed using a the geometric view  as follows: pick a vertical line , and sweep in order of -coordinate through the points of , counting the number of times the points of  in this sweep alternate between the left and right side of the line . Split the point set  into two sets using the line and repeat the process recursively on each side. See Figure~\ref{fig:wi} for an illustration of this process. The lower bound is the total number of alternations in all levels of the recursion. In computing this bound, there is freedom to choose the vertical line at each step; classically the line at each step is chosen to go though the median -coordinate, and we will call this lower bound . While  it can be shown that it is not always tight; for all  there is a sequence  of size  such that . Such a sequence can be created randomly by picking  nodes that are to the left of the dividing line in all levels of the recursion but one and randomly searching only them. Each random search must take time  in expectation but will only contribute  to the lower bound calculation. There are several possible avenues for improving this bound: one would be to figure out how to choose the lines best (and \cite{dhthesis} shows that you don't have to restrict the lines to vertical ones), or perhaps change them dynamically. Another avenue for improvement would be to somehow do another type of recursion that would directly reduce the gap exhibited above, possibly reducing it  or . The main interest in this lower bound is that is it the only bound that has been turned into an algorithm (see Tango trees below); thus improvements on this bound have a reasonable chance of being able to create a better algorithm than what is known.


\begin{figure}
\begin{center}
\includegraphics[width=2in]{wi}
\end{center}
\caption{The interleave bound. As each line is introduced we restrict out view to the cell bounded by previous lines in and containing the current one. In this cell, we connect the points, top to bottom, and a connection that crosses the current line is colored blue and contributes one unit to the lower bound. Green connections are do not cross the current dividing line and do not contribute to the lower bound. As all blue lines generated are distinct, we can visualize the entire bound using the right diagram.}
\label{fig:wi}
\end{figure}

\paragraph{Funnel bound.}
Given a point , the \emph{funnel} of ,  is the set of points below  that form unsatisfied rectangles with . For each funnel, look at the points in the funnel sorted by  coordinate, and count the number of alternations from the left to the right of  that occur; this is the amortized lower bound for ; computing and summing this value for all  in  gives the lower bound. A different, tree-based view of this bound is as follows: execute  on a binary search tree, and perform a series of single rotations to bring  to the root; this BST algorithm was first proposed by Allen and Munro in \cite{DBLP:journals/jacm/AllenM78}. The amortized lower bound for  is the number of turns on the path from the root to . It is worth noting that this will maintain at each step a treap where the heap value of each  is the last  such that , or equivalently the working set number of the item. This tree view gives an immediate idea for an algorithm---since only the turns in the tree contribute to the lower bound, is it possible to create a method to maintain a representation of the treap so that an item can be searched in a time proportional to the number of turns in the tree? The main obstacle to this approach is that there could be a linear number of items that are one turn away, thus some kind of amortization would be needed to show that that situation could not happen in each search.


\begin{figure}
\begin{center}
\includegraphics[angle=180,width=1.5in]{wii}
\end{center}
\caption{The funnel bound. For each yellow node, the blue nodes are the nodes in its funnel, and the lines correspond to pairs in the funnel which cross the yellow node and yield one unit of lower bound. }
\label{fig:wii}
\end{figure}





\paragraph{Relationship among these bounds}

All of these three bounds are lower bounds, that is all of them compute values which are .
No relationship is known among the alternation bound and the funnel bound. However, the alternation bound and the funnel bound have been shown to correspond to  sets of independent rectangles, and thus they are asymptotically implied by the independent rectangle bound \cite{DBLP:conf/soda/DemaineHIKP09}. We have mentioned that the alternation bound is not known to be tight. However, while the independent rectangle bound asymptotically implies the funnel bound, the converse is unknown, and we conjecture that they are equivalent.


\paragraph{Improving the bounds.}
In proving the independent rectangle bound, it was shown the need to put a point in each independent rectangle to satisfy the empty rectangles in the original set. Now, adding these points could cause new unsatisfied rectangles, including those that are defined entirely by added points. We call these problems \emph{secondary effects} and it is easy to show that they occur. The question is, are these secondary effects asymptotically significant or not? If one believes that the independent rectangle bound is tight, they could try to show that if one were to put points to satisfy the rectangles in phases, where the points in each phase satisfy the unsatisfied rectangles in the previous phase, the number of problems would form some kind of exponential progression. On the other hand, to show the bound is not tight one would 
need an example where these secondary effects dominate.



\section{Upper bounds}

In this section we survey the various progress towards the dynamic optimality conjecture by producing actual BSTs.


\subsection{Concrete bounds}

One approach has been to come up with concrete closed-form bounds that express the runtime of a particular BST data structure. These bounds initially began with the analysis of splay trees \cite{DBLP:journals/jacm/SleatorT85}, with the working set bound, which says a search is fast if it was searched recently, and the dynamic finger bound \cite{DBLP:journals/siamcomp/Cole00,DBLP:journals/siamcomp/ColeMSS00}  which says that a search is fast if it is close in key value to the previous search. In \cite{DBLP:journals/tcs/BadoiuCDI07}, we proposed combining these bounds into one which bounds a search as being fast if it is close in key value to some search that has been searched frequently; a BST-model algorithm with this bound was claimed in \cite{jdthesis} but may be buggy \cite{sleatortalk}. These bounds all can easily be seen to not be tight, that is there are classes of sequences  such that they are . Can refining bounds of this type have any hope of a closed form solution that is an asymptotically tight expression of ? For example, in the closely related problem of the runtime of the best static tree where each search beings where the previous one ended, a closed-form expression for the asymptotic runtime was obtained \cite{DBLP:journals/corr/abs-1304-6897}. However, for optimal BST's with rotations, this approach seems difficult as it is easy to come up with search sequences which can be executed quickly on a BST but which all known concrete upper bounds are not tight and which seem to defy a simple formulaic characterization.







\subsection{Tango trees \cite{DBLP:journals/siamcomp/DemaineHIP07}}
In the language of competitive analysis, the problem of finding a dynamically optimal binary search tree is to find one which is -competitive with the best offline tree. 
Any tree with  search time is trivially  competitive. Tango trees are a data structure that was created to have a non-trivial approximation factor of .
Specifically, they are created to be within a  factor of the alternation lower bound.


We present another view of computing the alternation bound, one which leads easily to the central idea of the Tango tree construction. This computation is presented algorithmically. To compute the alternation bound, envision a \emph{reference tree} which is a balanced binary containing . Each nonleaf node in the tree has one of its children designed as the preferred child---the preferred child is designated based on which subtree of that node has had the most recent search. Now to compute the bound, go though the sequence  and execute each search on the reference tree. The process of executing a search involves starting at the root and following children, which are either preferred or not; at the end of the search the nodes where the search did not follow the preferred child are updated to reflect that the preferred child has changed and is now aligned with the search path; the sum of these preferred child changes is equivalent to the alternation lower bound.
Given a node, call the preferred path the path in the reference tree obtained by following preferred children starting from that node until a leaf is reached. Due to the balanced nature of the reference tree, the size of any preferred path is . Given this setup, the idea behind the Tango tree is to store each preferred path in a balanced binary search tree of height . Thus a search in the reference tree that involved following  nodes among  preferred paths (and this has a lower bound of ) can be executed in time  time.
Details of the Tango tree involve arranging the BST's created from each preferred path into one BST, and using split and merge operations to maintain the changing of the preferred paths.
However, this method is limited by the fact that the alternation bound is sometimes tight and sometimes off by a  factor. Thus, no improvement in the competitive ratio is possible while still using the alternation bound as-is as the basis of the competitive ratio. Note that \cite{DBLP:conf/wads/DerryberryS09} improved Tango trees to have some additional desirable properties, such as  worst-case time per search, as opposed to  in their original presentation.



\subsection{Greedy}
If one were to come up with an idea for candidates for the best possible offline method there is one greedy method that stands out. Search for the current item. Then for (asymptotically) free one can rearrange the nodes on the search path into any tree. The heuristic that makes the most sense would be to place the node to be searched next at the root, or if the node to be searched next is not on the search path, place the subtree that contains it as close to the root at possible. Then, recurse on the the remaining nodes on the search path.  This method was first proposed by Lucas \cite{jltr}.

In the geometric view, there is a natural greedy method to find an ASS superset of : from bottom to top, add points on every row so that the point set is satisfied from that row down. See Figure~\ref{geoview} for an illustration of this process.
It turns out that by applying the geometric-to-BST conversion to this method, Lucas's greedy tree method is obtained; thus the two greedy methods are in fact identical.

While this method seems intuitively to be a good idea, basic facts like  amortized time per search were not known until the work of Fox \cite{DBLP:conf/wads/Fox11}, who also showed that there is an equivalent online BST to this method. Showing that this method which greedily looks into the future has an equivalent online method provides some support for the belief that the best online and offline BST algorithms have asymptotically the same runtime.

In the geometric view, recall that the independent rectangle lower bound can be computed by sweeping twice though  and satisfying the  and  unsatisfied rectangles separately. The greedy method is a single sweep though  satisfying both the  and the  rectangles at the same time (again, see Figure~\ref{geoview}). Proving that the point sets obtained though these two methods are within a constant factor of each other would be enough to show the greedy method is a dynamically optimal binary search tree. We have spent much time coding and looking for ways to prove such a correspondence to no avail.


\begin{figure}
\begin{center}
\includegraphics[trim=80 200 100 170, angle=180, width=4.5in]{geoview}
\end{center}
\caption{The geometric view of binary search trees.
A black dot at location  represents that we wish to search for key value  at time ; it is set set . The black dots, combined with the solid blue and red represent an execution of Lucas's greedy future algorithm to execute this search sequence; a dot at  represents the greedy algorithm touching item with key  at time . The solid dots are \emph{satisfied}, that is for every two dots not in the same row or column, you can find a third one on the rectangle they define. Color simply represents being to the left or to the right of black. The -shaped markers are a visual representation of a the incremental construction of the independent rectangle lower bound for the minimal satisfied superset of the black points. Showing Lucas's method is dynamically optimal is equivalent to showing the solid and 's are always within a constant factor of each other for any such diagram.} 
\label{geoview}
\end{figure}


\subsection{Combining trees}
In \cite{DBLP:journals/corr/abs-1304-7604}, we have shown that given any constant number of online BST algorithms (subject to certain technical restrictions described in the paper), there is an online BST algorithm that performs asymptotically as well on any sequence as the best input BST algorithm. If the output algorithm did not have to be in the BST model this would be trivial as the input algorithms could just be run in parallel; however the BST-model restriction makes this non-trivial. It is open whether or not it is possible to combine a superconstant number of BST algorithms. This would be of interest as a dynamically optimal BST could be viewed as combining \emph{all} algorithms and taking the minimum. As the number of BST algorithms can be limited to be a function of  (see next section), this opens the possibility of having an algorithm with a runtime of , for some possibly huge function .




\subsection{Search optimality.} In \cite{DBLP:journals/algorithmica/BlumCK03}, the notion of \emph{search optimality} was presented. The \emph{search cost} of a search BST algorithm is simply the depth of the node to be searched. Any rotations or pointer movements off the search path are free; in this way the BST can be arbitrarily reconfigured between searches at zero cost. It was shown that there is a BST model algorithm for which the search cost is . The general method was to use a machine learning approach to finding the best tree after each search based on the searches performed so far. If one were to try to adapt this method to the standard online BST model cost function, a reasonable starting point would be to try to determine if there is any cohesion of the trees produced by the method from one search to the next, and to try to figure out if one could use such cohesion to transform one tree to the next in time proportional the the search cost. 

\section{Online optimality}

In this section we present the only new result of this paper: we present the best possible online BST algorithm for sufficiently long search sequences. In particular, we prove the following:

\begin{theorem} \label{tha}
There is an online BST algorithm  such that if there is an online algorithm  such that  for some function , then there is an algorithm  and a function  such that .
\end{theorem}

 The algorithm is decidedly not in the real-world BST model, and is a relatively straightforward application of known methods from learning theory.

We begin by summarizing a classic result from learning theory (see \cite{DBLP:journals/toc/AroraHK12} for a survey of its origins, variants and applications). The setup is that there are a sequence of events  which are presented in an online manner---each event is an integer in the range . Before each event is revealed, one of  \emph{experts} numbered  is chosen. After the event is chosen a \emph{penalty} is determined based on an  table  which assigns penalties to each combination of event and expert;  is the penalty if expert  was chosen and event  happened.

Thus, if a single expert  were to be chosen for all events, the total penalty would be 
. The main result we will use is that it is possible to pick online an expert before each event such that the total penalty is asymptotically that of the best expert:

\begin{theorem}[Weighted Majority Algorithm]
For any , there is an online choice of expert  such that


\end{theorem}

Now we apply this theorem to BSTs.
Let  be a sequence of searches in a BST-model data structure containing the integers ; for convenience we assume  is a multiple of , and we assume . 
We let the events be the  different search sequences of length ; thus .
We divide the search sequence into \textit{epochs} of size , and denote the th epoch as . Note that each epoch  is an event.

How many BST-model algorithms are there to execute an epoch, assuming at the beginning and end of each epoch the BST is in a canonical state (e.g.~a left path)? There are at most 
. This is because you can encode an algorithm by encoding the   fundamental operations spent to execute each of the  possible epochs, and the 5 possible BST unit-cost operations at unit of time executing each epoch. We view the set of online BST epoch algorithms as the experts. Thus . 
This is a gross overestimate as this counts the offline algorithms, and does not cull those which do not properly execute each search. The cost  is simply the runtime of BST epoch algorithm  on epoch .


Plugging this into Theorem~\label{main} gives:

\begin{lemma} \label{bsta} There is a way to choose an algorithm  at each epoch such that:

\end{lemma}

Now, recall   is the fastest any offline BST-model algorithm can execute the search sequence . 


\begin{lemma}
Given a search sequence  of length  on a set of size , let  be a search sequence of size  which is the th epoch of . Then for any 

\end{lemma}

\begin{proof}
Follows directly from the fact that any BST can be converted into any other in  time. Thus you can be forced into a canonical state every  searches and this only changes the optimal time by a constant.
\end{proof}


These lemmas give a proof of Theorem~\ref{tha}. Specifically, if there is an unknown online BST algorithm with runtime , then using the weighted majority algorithm to pick an algorithm to run every  steps yields an online BST algorithm that runs in time , which is  for sufficiently long sequences .




\bibliography{bib,extrabibs}

\end{document}