

\documentclass{sig-alternate-05-2015}
\usepackage{paralist}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{color}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{epstopdf}
\usepackage{dsfont,pifont}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage{multicol,multirow}
\usepackage{CJK}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}







\usepackage[bookmarks=true,bookmarksnumbered=true,hypertexnames=false,breaklinks=true,colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{hyperref}


\newenvironment{ppl}{\fontfamily{ppl}\selectfont}{}




\def\ignore#1{}
\newcommand{\colortext}[1]{\textcolor{magenta}{#1}}
\newcommand{\ylcomment}[1]{\textcolor{red}{[#1---yl]}}
\newcommand{\qycomment}[1]{\textcolor{red}{[#1---qy]}}




\begin{document}



\CopyrightYear{2016} 
\setcopyright{acmcopyright}
\conferenceinfo{CIKM'16 ,}{October 24-28, 2016, Indianapolis, IN, USA}
\isbn{978-1-4503-4073-1/16/10}\acmPrice{\^1^1^2^1^1^2\mathbf{q}M\mathbf{a}NMN\mathbf{P}\mathbf{P}_{j,i}\mathbf{q}_j\mathbf{a}_i\mathbf{q}_j\mathbf{a}_i\mathbf{P}_{j,i}1j\mathbf{P}\mathbf{q}_j\mathbf{a}\mathbf{P}\mathbf{P}_{j,i} \in [-1,1]0.121\mathbf{P}_{j,i} = 1\mathbf{w}K+1x_{jk}k\mathbf{q}jj\mathbf{q}\mathbf{v}P\mathbf{q}_j\mathbf{v}\mathbf{w_k}\mathbf{v_p}(\mathbf{q}, \mathbf{a}^+, \mathbf{a}^-)\mathbf{q}\mathbf{a}^+\mathbf{a}^-S(\mathbf{q}, \mathbf{a})(\mathbf{q}, \mathbf{a})(\mathbf{q}, \mathbf{a}^+, \mathbf{a}^-)\Delta S = 1 - S(\mathbf{q}, \mathbf{a}^+) + S(\mathbf{q},\mathbf{a}^-) \Delta S \leq 0 e\mathbf{v}\frac{\partial g_j}{\partial v_p}e\mathbf{w}\eta = \eta_0 (1-\epsilon)\epsilon1\mathbf{w}\mathbf{w} \tau (\mathbf{v} \cdot \mathbf{q}_j) = \frac{\exp({\mathbf{v}\cdot \mathbf{q}_j)} }{\sum_{l=1}^{L}\exp({\mathbf{v}\cdot\mathbf{q}_l})}\tauTr_tw_{kt}r_tv_p\mathbf{v}h_j^+ = \delta(\sum_{t=0}^{T} r_t \cdot \delta (\sum_{k=0}^{K} w_{kt}x_{jk}^+))h_j^- = \delta(\sum_{t=0}^{T} r_t \cdot \delta (\sum_{k=0}^{K} w_{kt}x_{jk}^-))\mathbf{r}s_t^+ = \delta(\sum_{k=0}^{K} w_{kt}x_{jk}^+ )s_t^- = \delta(\sum_{k=0}^{K} w_{kt}x_{jk}^- )\mathbf{w}u_j^{'+} = \sum_{t=0}^{T} r_t \cdot \delta (\sum_{k=0}^{K} w_{kt}x_{jk}^+)u_j^{'-} = \sum_{t=0}^{T} r_t \cdot \delta (\sum_{k=0}^{K} w_{kt}x_{jk}^-)552001000600700200700MRR = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{rank(fa)}rank(fa)qMAP = \frac{1}{|Q|} \sum_{q \in Q} AP(q)AP(q)q \in \mathbf{Q}trec\_eval6001(0,1)(300,600)(0.5,1)(450,600)(0,0.5)(300,450)(0.5,1)(0,0.5)(-1,0)\tau (\mathbf{v} \cdot \mathbf{q}_j)14.679.154.133.831600700(300, 700)$ is a good choice as much lower or higher word vector dimensions will hurt the performance. The choice of word vector dimension also depends on the size of training corpus. Larger corpus requires higher dimension of word vectors to embed terms in vocabulary. (2) For the number of bins, we can see that MAP and MRR will decrease as the bin number increase. Too many bins will increase the model complexity, which leads aNMM to be more likely to overfit the training data. Thus choosing suitable number of bins by optimizing hyper-parameter on validation data can help improve the performance of aNMM.





%
 

\section{Conclusions and Future Work}
\label{sec:conclu}

In this paper,  we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combing different matching signals and incorporate question term importance learning using a question attention network. We perform a thorough experimental study with the TREC QA dataset from TREC QA tracks 8-13 and show promising results. Unlike previous methods including CNN as in \cite{Yu:2014, Severyn:2015:LRS:2766462.2767738} and LSTM as in \cite{DBLP:conf/acl/WangN15}, which only show inferior results without combining additional features, our model can achieve better performance than the state-of-art method using linguistic feature engineering without additional features. With a simple additional feature, our method can achieve the new state-of-the-art performance among current existing methods. For further work, we will study other deep learning architectures for answer ranking and extend our work to include non-factoid question answering data sets.

\section{Acknowledgments}
This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF IIS-1160894, and in part by NSF grant \#IIS-1419693. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. 








\bibliographystyle{abbrv}
\bibliography{cikm}  

\end{document}
