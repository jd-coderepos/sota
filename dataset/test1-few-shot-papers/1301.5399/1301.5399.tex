\documentclass{article}
\usepackage{spconf}
\usepackage{multirow}
\usepackage{amsmath, amssymb}
\usepackage{array}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{pseudocode}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{ctable}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{tabulary}
\usepackage{subfigure}
\usepackage{array}
\usepackage{pseudocode}


\begin{document}

\title{Incorporating Betweenness Centrality in Compressive Sensing for Congestion Detection}

\name{Hoda S. Ayatollahi Tabatabaii, Hamid R. Rabiee, Mohammad Hossein Rohban, Mostafa Salehi}
\address{Sharif University of Technology, Tehran, Iran}

\maketitle

\begin{abstract}
This paper presents a new Compressive Sensing (CS) scheme for detecting network congested links. We focus on decreasing the required number of measurements to detect all congested links in the context of network tomography. We have expanded the LASSO objective function by adding a new term corresponding to the prior knowledge based on the relationship between the congested links and the corresponding link Betweenness Centrality (BC). The accuracy of the proposed model is verified by simulations on two real datasets. The results demonstrate that our model outperformed the state-of-the-art CS based method with significant improvements in terms of F-Score. \end{abstract}
\keywords{Network tomography, Compressive sensing, Congestion detection, Prior knowledge.}

\section{Introduction}
The network tomography scheme is a method to calculate link variables such as delay or bandwidth, using the end-to-end measurements. In this paper, we intend to solve a congestion detection problem using the network tomography scheme, based on end-to-end measurements. Our goal is to decrease the number of required measurements in order to identify all congested links. Since only a small number of network links may become congested, it is assumed that the delay vector (which contains delays of all links) is sparse. 

To solve this problem, we employ Compressive Sensing (CS) \cite{CSIntro} which has received significant attention in the recent years. Since its inception, CS shows remarkable results in reconstructing a high dimensional sparse signal with a small set of measurements. A significant issue in this field is the sample complexity which is the number of required measurements for high quality signal recovery which is solvable through LASSO \cite{CSIntro}. Although CS is a new sampling scheme in signal processing, it has been applied to many network applications specially in the area of network tomography. Fault diagnosis \cite{NetTom2}, traffic estimation \cite{NetTom1}, data localization \cite{CSP2P}, and congestion detection \cite{CSoverGraph} are some well known applications in this field.

The state-of-the-art CS-based algorithm in network tomography \cite{CSoverGraph}, used random walks to gather end-to-end delays and employed LASSO in its model for congestion detection.  However, it has two major drawbacks; relatively high number of random walk measurements (much higher than the total network links), and low accuracy in detecting the congested links when the required number of measurements is small. 
In this paper, we try to solve those drawbacks by using the prior knowledge corresponding to the correlation of link betweenness centrality (the number of shortest paths that traverse from the link) and link congestion. We also introduce a new CS objective function in which we consider the dependency of congestion and betweenness centrality. We further show that the proposed objective function can be considered as the Elastic-Net model \cite{ENet}, which is a more stable alternative to LASSO.
We have simulated the proposed Compressive Sensing in Congestion Detection (CSCD) model, by using real data with various configurations. We have compared the F-Score (of the retrieved congested links) in our model with \cite{CSoverGraph}, in terms of the number of measurements (random walks) and various sparsity of the network delays in which we achieve significant improvements. 

The rest of the paper is organized as follow; in Section \ref{Related Work}, we describe the related work in network tomography, congestion detection, and compressive sensing. Section \ref{Problem Definition} shows an introduction to compressive sensing and the problem we try to solve in this paper. In Section \ref{The Proposed Model}, we introduce the proposed compressive sensing model. Comparison with the previous model is appeared in Section \ref{Evaluation}. Finally, we conclude the paper in Section \ref{Conclusion}. 
\section{Related Work}
\label{Related Work}
Compressive sensing in network tomography is first discussed in \cite{GroupTest1}. The authors discussed a group testing problem on the Erds-Rnyi random graphs. They have applied OR operation (instead of summation) on the gathered measurements and calculated the required number of measurements when the link variables are binary. In \cite{CSoverGraph}, the authors gathered end-to-end delay information by applying random walks to the network and recovered the sparse delay values of network links using compressive sensing. Although they have better theoretical order for the number of measurements compared to \cite{GroupTest1}, they have used high number of random walks to achieve good recovery percentage in practice. In their theoretical proof, they required that the graph is highly connected which may not be true in all of the real networks. Moreover, their practical results are accomplished according to specific graphs such as the complete graph.

In our model, however, we try to solve these practical issues by employing the relationship between link betweenness centrality and link congestion in the network. As noted in \cite{BC1}, link betweenness centrality and congestion are two measurements in the network which are highly correlated. In  \cite{BC1}, \cite{BC2}, and \cite{BC3} link betweenness centrality is used as the only element to detect the congested links in the network. Although betweenness centrality is effective to achieve this purpose, none of these papers consider end-to-end measurements in finding the congested links. Later in this paper, we show  how efficient it is to employ both link betweenness centrality and end-to-end measurement for congestion detection.

\section{Problem Definition}
\label{Problem Definition}
We consider a real communication network as an undirected graph with the set of links , and vertices . We assume each link can transfer data in both directions between two connected nodes. We are going to measure the network congested links using end-to-end delays of  random walk paths in the network (). Thus, we should first measure the delay of each network link. Let the vector  denote the observed end-to-end delays where  represents the end-to-end delay of the path . We also define  as the links' delay vector where  represents the delay of link . Since the number of congested links in a network are sparse,  is considered as a -sparse vector ( and ). The goal is to recover vector  which contains the delay of all  links of the network. Moreover, by recovering , the congested links can be recognized through their high amount of delay. To recover this vector, compressive sensing method has been used. The recovery process starts with solving the following equation: 

where  is the noise vector that is experienced in the random walk measurements, and  is an  matrix. The elements of  row and  column of  is equal to , if the  link of the network is available in the  random walk path. We employ  random walks over the network graph and gather information regarding the end-to-end delay of each path (). Having  and , we may reconstruct  by minimizing 
under the constraint of Eq. \eqref{y_Ax}. However, as stated in \cite{l0norm}, this is an NP-hard optimization problem. In compressive sensing, the aforementioned optimization problem is solved by minimizing the 
 instead of 
which results in an optimization problem with computational complexity of  \cite{l0norm}, known as LASSO: 

\section{The Proposed Model}
\label{The Proposed Model}
We have employed the Bayesian theory in our model which is used in many compressive sensing applications \cite{Bayesian1} \cite{Bayesian2} \cite{Bayesian3}. The previous applications, however, have not considered prior knowledge on  other than its sparsity. The main goal of our model is congestion detection with a few number of random walk measurements. We have employed the betweenness centrality \cite{BetweenC} of the congested links as the prior knowledge in  minimization. 

Let  denote the maximum delay that each link can tolerate in the network and , , denote the link betweenness of the link  where  is the maximum link betweenness centrality in the network. We use linear interpolation to capture the relation between the betweenness centrality and the prior belief about the delay values of network links. In this way, we get the scaled version of link betweenness centralities (). 

With a high probability, a link with the maximum link betweenness () has the highest delay () in the network \cite{CongBetween}. Moreover, a link with the lowest link betweenness (0) is more likely to have no delay. Thus, (0,0) and (,) are two points on the interpolating line. Therefore, by considering the link betweenness values as the X-axis () and our prior belief about links' delay as the Y-axis () in a two dimensional space,  is given by: 

In order to recover  from , it is critical that  be sparse. Since  is also a sparse vector (because it is highly correlated with ),  should be sparse too. Therefore, we define the probability density function of  as follows:

where ,. It penalizes non-sparse choices of  (by the first term) and also vectors  which are not similar to  (by the second term).
By observing , we intend to find  with the highest probability. Thus, we may maximize  by using the Maximum a Posteriori probability (MAP) estimation as follows:

Since the goal of Eq. \eqref{PX_Y} is to find the maximum value of  in , by eliminating the terms that do not depend on , and taking logarithm on both sides, we obtain:

On the other hand, by observing , we intend to find  from Eq. \eqref{y_Ax}. We assume  has a normal distribution with zero mean and covariance matrix . Therefore:   

According to Eq.s \eqref{ProbX} and \eqref{PY_X}, Eq. \eqref{LogP_X} may be expanded 
as follows:

where  and  is the number of random walk measurements. Therefore, the optimization problem becomes:
 
where , , and .
An important goal in CS is to have model consistency \cite{ENet}, which shows that the support of recovered  converges to the support of original  as the number of random walk measurements goes to . To verify this property, we show that our problem is similar to the one discussed in \cite{ENet} which is known as Elastic-Net:
 
To increase the recovery accuracy, achieve model consistency, and overcome LASSO limitations \cite{ENet}, Elastic-Net is used as an alternative to LASSO.

Consider  and . Since  is a constant matrix, it is easy to show that Eq. \eqref{OurCSEq} and Eq. \eqref{ENetEq} are two equivalent optimizations. Thus, our model is in the form of Elastic-Net.


As mentioned before, we have  random walk measurements,  network links and  is the sparsity value. 
Without loss of generality, assume that only the first  elements of  are non-zero. Then , ,  contains the first  columns of , and  contains the last  columns of .

Therefore, by having , , , and , Irrepresentable Condition (IC) can be shown to be a necessary and sufficient condition for LASSO's model consistency \cite{ENet}:
 
Moreover, according to the Corollary 1 in \cite{ENet}, if the Elastic Irrepresentable Condition (EIC) is satisfied, the Elastic-Net model has model consistency by choosing right values for , , and . The EIC is as follows:
 
where .
In the next section, we show that in our model EIC is satisfied with a higher probability compared to IC.
\section{Evaluation}
\label{Evaluation}
\subsection{Simulation Framework}
In order to evaluate the proposed model, we have performed extensive simulations (30 runs) in MATLAB. We have used two real datasets. The first dataset contains the information of a mobile operator which has 273 links and 158 nodes corresponding to the network devices in Mobile Switching Centers (MSC) placed in 40 cities. The second dataset contains the information of a data network with 366 links and 277 nodes which are located in more than 50 cities. We have also considered the variance of the results by measuring the related error bars. For the first dataset, we consider  , , and . For the second dataset we have  . Through several simulations, these are almost the best configurations for both LASSO and our model. The number of steps in each random walk is assumed to be 15.
\subsection{Validation}
\label{Validation}
To evaluate the proposed model, named Compressive Sensing in Congestion Detection (CSCD), we have used  measure which corresponds to the harmonic mean of precision and recall. Precision measures the percentage of correctly detected congested links to the sum of correctly and wrongly detected congested links, and recall refers to the percentage of correctly detected congested links to the total detected congested links. 

First, we have evaluated the proposed CSCD model and LASSO through model consistency. Assuming , EIC in Eq. \eqref{EIC_eq} and IC in Eq. \eqref{EIC_eq} are verified for our model and LASSO, respectively. At the end of all 30 simulation runs, the percentage of the times that EIC and IC are satisfied, is computed.
\begin{figure}[h]
\centering
\includegraphics[width=0.27\textwidth]{EIC-IC.eps}~ 
\caption{Comparison of probability that EIC holds in Elastic-Net and IC holds in LASSO in various number of random walks in the first dataset}
\label{ENet-LASSO}
\end{figure}
It has to be mentioned that CS-over-Graphs in \cite{CSoverGraph} has used LASSO in its model. Moreover, in the theoretical proof of \cite{CSoverGraph}, they required that the graph is highly connected which may not be true in all of the real networks. Choosing ,  as sparsity, and  as the number of links in the first dataset, by increasing  such that , Fig.\ref{ENet-LASSO} shows that in CSCD model model consistency holds with higher probability.

The rest of this section shows the evaluation of F-Score in our model in various settings. Fig. \ref{F-ScoreRW} illustrates an improvement of the F-Score performance of the CSCD model by an average of 5\% (Dataset 1) and 10\% (Dataset 2) for the various number of random walks compared to the Compressive Sensing over Graphs method (CS-over-Graph) presented in \cite{CSoverGraph}. The number of random walks changes from 10\% to 90\% of the total network links. Although, we have evaluated F-Score for various random walk steps, the results were similar to those shown in Fig. \ref{F-ScoreRW}. 
Clearly, for lower number of random walks, we have lower number of measurements, and thus less samples. Since the proposed model simultaneously employs the prior knowledge based on the link betweenness centrality, it performs better than CS-over-Graphs model \cite{CSoverGraph} where only the sparsity information () is used. However, as the number of random walks grows, our F-Score gets closer to CS-over-Graphs'. Because at higher number of random walk the prior knowledge becomes less significant. \begin{figure}[h]
\centering
\includegraphics[width=0.28\textwidth]{FScore-RW_D1D2.eps}
\caption{F-Score versus random walk number in two datasets}
\label{F-ScoreRW}
\end{figure}
As illustrated in Fig. \ref{F-ScoreSparsity}, we have also evaluated the F-Score of CSCD in terms of sparsity of the congested links in the network (). The F-Score of the CSCD model is improved by an average of 11\% in the first dataset, and 9\% in the second one compared to CS-over-Graphs model.
\begin{figure}[h]
\centering
\includegraphics[width=0.28\textwidth]{FScore-Sparsity_D1D2.eps} 
\caption{F-Score versus sparsity in two datasets}
\label{F-ScoreSparsity}
\end{figure}
In Fig. \ref{RWSparsity}, we have measured the required number of random walks in terms of the sparsity of the congested links in the network (). Considering the sparsity varies from 5\% to 30\% of the network links, we have calculated the least required number of random walks when F-Score equals to 50\%. The number of random walks in CSCD is decreased by an average of 16\% in the first dataset and 15\% in the second one compared to CS-over-Graphs. \begin{figure}[h]
\centering
\includegraphics[width=0.28\textwidth]{Sparsity_RW_D1D2.eps}~ 
\caption{Sparsity versus various number of random walks in two datasets }
\label{RWSparsity}
\end{figure}
We also compare the F-Score of our model with the algorithm that is used only Betweenness Centrality (BC) measurement for congestion detection (BC algorithm) as employed in \cite{BC1}, \cite{BC2}, and \cite{BC3}. The result is illustrated in Fig. \ref{MereBC}.
\begin{figure}[h]
\centering
\subfigure[]{\label{RW-BC}\includegraphics[width=0.27\textwidth]{BC-FScore-RW_D1.eps}}~
\subfigure[]{\label{Sparsity-BC}\includegraphics[width=0.27\textwidth]{BC-FScore-Sparsity_D1.eps}}
\caption{Comparison of our model with BC algorithm in terms of (a) the required number of random walks (b) sparsity}
\label{MereBC}
\end{figure}
Since betweenness centrality is independent from the number of random walks, it remains constant in Fig. \ref{RW-BC}.
As shown in Fig. \ref{MereBC}, CSCD outperformed the algorithms based on mere betweenness centrality, by an average of 12\% on F-Score for different sparsities and 69\% on F-Score for various random walk numbers.
\section{Conclusion} 
\label{Conclusion}
In this paper, we introduced a new objective function based on the concepts of compressive sensing in a network tomography application. We used the link betweenness prior knowledge in our objective function which results in a decrease on the required number of measurements for detecting the network congested links. Based on extensive simulation results, we verified significant improvement in accuracy of detecting the network congested links in two real datasets. \bibliography{mybib}
\bibliographystyle{IEEEbib}
\end{document}