
\documentclass{article}










\usepackage[preprint]{neurips_2019}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}  
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{pgfplots.groupplots}
\usepackage{pgfplots}
\usepackage{textcomp}
\usepackage{subfig}
\usepackage{caption}
\usepackage{tikzscale}
\usepackage{appendix}
\usepackage{numprint}
\usepackage{tabularx}
\usepackage{verbatim}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{matrix,calc,fit}
\usepackage{booktabs}
\usepackage{footnote}
\newcommand{\bftab}{\fontseries{b}\selectfont}
\newcommand\ind{\textbf{1}}
\newcommand\complex{\mathbb{C}}
\newcommand\real{\mathbb{R}}
\newcommand\rational{\mathbb{Q}}
\newcommand\nat{\mathbb{N}}
\newcommand\integer{\mathbb{Z}}
\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\F{\mathcal{F}}
\newcommand\pn{p_{f_0}}
\newcommand\eps{\varepsilon}
\newcommand\bayes{\hat{\theta}_{\text{Bayes}}}
\newcommand\mle{\hat{\theta}_{\text{mle}}}
\newcommand\var{\text{Var}}
\newcommand\cov{\text{Cov}}
\newcommand\bias{\text{Bias}}
\newcommand{\norm}[3]{\frac{1}{\sqrt{2\pi#3}}\exp{\Big\{-\frac{(#1-#2)^2}{2#3}}\Big\}}
\newcommand{\te}{\theta}
\newcommand{\hyp}{\mathcal{H}}
\newcommand{\vcd}{\text{VCdim}}
\def\GG{E}
 \def\xx{\mathbf{x}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{}\mkern2mu{#1#2}}}
\pgfplotsset{compat=1.14}
\begin{document}
\title{Generative Latent Flow}

\author{Zhisheng Xiao\thanks{equal contribution}, Qing Yan \footnotemark[1]\\
University of Chicago\\
Chicago, IL 60637, USA \\
\texttt{\{zxiao, yanq\}@uchicago.edu} \\
\And
Yali Amit \\
University of Chicago\\
Chicago, IL 60637, USA \\
\texttt{amit@marx.uchicago.edu} \\
}

\maketitle



\begin{abstract}
  In this work, we propose the Generative Latent Flow (GLF), an algorithm for generative modeling of the data distribution. GLF uses an Auto-encoder (AE) to learn latent representations of the data, and a normalizing flow to map the distribution of the latent variables to that of simple i.i.d noise. In contrast to some other Auto-encoder based generative models, which use various regularizers that encourage the encoded latent distribution to match the prior distribution, our model explicitly constructs a mapping between these two distributions, leading to better density matching while avoiding over regularizing the latent variables. We compare our model with several related techniques, and show that it has many relative advantages including fast convergence, single stage training and minimal reconstruction trade-off. We also study the relationship between our model and its stochastic counterpart, and show that our model can be viewed as a vanishing noise limit of VAEs with flow prior.  Quantitatively, under standardized evaluations, our method achieves state-of-the-art sample quality among AE based models on commonly used datasets, and is competitive with GANs' benchmarks. 
\end{abstract}

\section{Introduction} \label{intro}
Deep generative models have recently attracted much attention in the deep learning literature. These models are used to formulate the distribution of complex data as a function of random noise passed through a network, so that rendering samples from the distribution is particularly easy. Deep generative models can be roughly classified into explicit and
implicit models. The former class assumes explicit parametric specification of the distribution, whereas the latter does not. Implicit models are dominated by Generative Adversarial Networks (GANs) \citep{GAN,DCGAN}. GANs have exhibited impressive performance in generating high quality images \citep{biggan} and other vision tasks \citep{cycgan,super}. Despite their successes, training GANs can be challenging, partly because they are trained by solving a saddle point optimization problem formulated as an adversarial game. It is well known that training GANs is unstable and extremely sensitive to hyper-parameter settings \citep{improve,equi}, and sometimes training leads to mode collapse \citep{tutorial}, where most of the samples share some common properties. Although there have been multiple efforts to overcome the difficulties in training GANs, by modifying the objective functions or introducing normalization \citep{wgan,unroll,VEE,spectralnorm}, researchers are also actively studying non-adversarial methods that are known to be less affected by these issues.  

Some explicit methods directly model , the distribution of data, and training is guided by maximizing the data likelihood. For example, auto-regressive models \citep{MAF,PIXELRNN}, which assume the data distribution can be expressed in an auto-regressive pattern, have a simple and stable training process, and currently give the best likelihood results; however, they cannot provide low-dimensional representations of images, and their sampling procedure is inefficient. Normalizing flows \citep{NICE,REAL,GLOW} model  as an invertible transformation from a simple distribution through a change of variables. While being mathematically clear, normalizing flows have one major drawback: computational complexity. Flows have to keep the dimensionality of the original data in order to maintain bijectivity, and this makes training computationally expensive. Considering the prohibitively long training time and advanced hardware requirements in training large scale flow models such as \citep{GLOW}, we believe that it is worth exploring the application of flows in the low dimensional representation spaces rather than in the original data.  

Other explicit generative models often adopt low dimensional latent representations, which are usually obtained from auto-encoders, and generate samples by decoding 's sampled from a pre-defined prior distribution . We call this type of method AE based models. Variational Auto-encoders (VAEs) \citep{VAE,rezende} are perhaps the most influential AE based models. VAEs are trained to minimize a variational bound of the data log likelihood, which is composed of the reconstruction loss plus the KL divergence between , the approximate posterior distribution returned by the probabilistic encoder, and the prior . AE based models are easy to train, and they provide low dimensional codes for the data, but unfortunately, their generation quality still lies far below that of GANs, especially on large datasets. For example, it is observed that VAEs tend to generate blurry images, an effect that is usually attributed to the failure to match the marginal distribution in the latent space \citep{TwoVAE,match}. Some modifications to VAEs \citep{IWAE,impFlow1} improve the estimated test data likelihood. However, it is known that higher likelihood is not directly related to better sample quality \citep{note,flowgan}. Some other modifications have recently been proposed  to better match the distribution of latent variables and the prior distribution \citep{TwoVAE,wae,From, NAIS}, and they are shown to have the potential to generate high quality samples. These are discussed in greater detail in Section \ref{rw}. 

Our work pursues the same goal of improving the generation quality of AE based models. To this end, we propose Generative Latent Flow (GLF), which uses a deterministic auto-encoder to learn a mapping to and from a latent space, and a normalizing flow that serves as an invertible transformation between the latent space distribution and a simple noise distribution. Our contributions are summarized as follows: i) we propose Generative Latent Flow, which is an AE based generative model that can generate high quality samples. ii) through standardized evaluations, we show that our model achieves state-of-the-art sample quality among competing models, and can match the benchmarks of GANs. Moreover it has the advantage of one stage training and faster convergence. iii) we carefully study some variants of our method and show its relationship to other methods.

\section{Motivation and related works} \label{rw}
Consider an AE based generative model that can generate samples from the data space . Ideally, the auto-encoder defines a low dimensional latent space , where each image  is associated with a latent vector  through the decoder . The marginal distribution over , denoted by , is unknown and possibly complicated, and depends on the encoder . In some cases, the model also has a predefined prior distribution  on . Since samples are generated by sampling  from the prior and feeding  to the decoder,  in order to generate high quality samples, AE based models need to have: (a) a good decoder  that can output realistic images given latent variables sampled from , and (b) a good match between  and . 

Criterion (a) is easily ensured by minimizing the reconstruction loss of the auto-encoders, and there are different ways to ensure criterion (b). Intuitively, criterion (b) can be achieved by either modifying the encoder so that  is close to , or conversely modifying  to match some observed distribution on the latent space. The classic VAE model adopts the first approach indirectly using an approximation  for . It assumes a simple prior, and the KL regularizer in the ELBO objective penalizes  plus a mutual information term as shown in \citep{surgery}. The approximation  is called the aggregated approximate posterior. Several modifications to VAE's objective \citep{surgery,tcvae, factorsing}, which are designed for the task of unsupervised disentanglement, further decompose the KL term in ELBO, put a stronger penalty specifically on the mismatch between  and . There are also attempts to use normalizing flows as VAEs' posterior distributions \citep{impFlow1,impFlow2,impFlow3}. Although similar in name, they are completely different from our models, as they aim to complicate the approximate posterior  thus modifying the distribution . As of yet, these modifications to VAEs have not been shown to improve generation quality. In particular, empirically VAEs with flow posterior have been shown to improve neither the matching of  and  \citep{match}, nor the generation quality \citep{TwoVAE}. Adversarial auto-encoders \citep{AAE} and Wasserstein auto-encoders \citep{wae} use adversarial regularizer or MMD regularizers \citep{MMD} to force the  to be close to . These regularizations can be applied to both deterministic and probabilistic auto-encoders, and are shown to improve generation quality, as they generate sharper images than VAEs do.  

One problem with regularizing  is that it introduces a trade off with reconstruction, i.e. criterion (a). This motivates the use of learnable priors optimized to match . \citep{vamp, hiera,resample} propose different ways to approximate the aggregated posterior  during training, and use the approximated  as their VAEs' prior distributions. This is a natural way to modify the prior to match , however, these methods have not been shown to improve generation quality. Two-stage-VAE\citep{TwoVAE} introduces another VAE on the latent space defined by the first VAE to learn the distribution of its latent variables. GLANN \citep{NAIS} learns a latent representation by GLO \citep{GLO} and matches the densities of the latent variables with an implicit maximum likelihood estimator \citep{IMLE}. VQ-VAE \citep{VQVAE} first learns an AE with discrete latent variables that are stored in a code-book, and then fits an auto-regressive prior on the latent space. RAE+GMM\citep{From} trains a regularized auto-encoder \citep{RAE} and fits a mixture of Gaussian distribution on the latent space. These methods significantly improve the generation quality, but they all involve two-stage training procedure, which adds a computational overhead.

Motivated by above works, we wish to design an AE based generative model that enjoys the best of both worlds: it can be trained end-to-end in a single stage, and it can greatly improve the generation quality without over regularizing the latent variables. We accomplish these goals by using normalizing flow on the latent space of a deterministic AE. More details will be presented in Section \ref{model}. Note that our method is closely related VAEs with normalizing flow as a learnable prior and the connections are discussed in Section \ref{connect}.

\begin{figure}[ht]
\centering
\subfloat[\label{fig:1a}]{\includegraphics{plot_tikz/g1}}\hspace{4pt}
\subfloat[\label{fig:1b}]{\includegraphics{plot_tikz/g2}}
\caption{(a) Illustration of GLF model. The red arraw contains a stop gradient operation. See Section \ref{strategies}. (b) Structure of one flow block. It splits the input into two parts , goes through the coupling layer , and applies the random permutation .} \label{fig:1}
\end{figure}

\section{The Generative latent flow (GLF) model} \label{model}
Our model uses a deterministic auto-encoder composed of an encoder  and decoder (generator) . In addition we have a transformation   from the distribution on the noise space , which is assumed to be the standard Gaussian distribution, to the distribution on . The transformation  is defined in terms of a normalizing flow and all three components are learned simultaneously end to end using a loss that combines the reconstruction quality and the likelihood of the encoded data  with respect to the transformation .
 
\subsection{Normalizing flows for the transformation \texorpdfstring{}{F}}
 The core of normalizing flows is carefully-designed invertible networks that map the training data to a simple distribution. Let  be an observation from an unknown target distribution  and  be the unit Gaussian prior distribution on . Given a bijection , we define a model  with parameters  on , and we can compute the negative log likelihood (NLL) of  by the change of variable formula: 
 
where  is the Jacobian matrix of . In order to learn the flow , the NLL objective of  is minimized, which is equivalent to maximize the likelihood of . Since the mapping is a bijection, sampling from the trained model  is trivial: simpy sample  and compute . In our method, we use the normalizing flow to model the transformation .

The key to designing a tractable flow model is defining the transformation  so that the inverse transformation and the determinant of the Jacobian matrix can be efficiently computed. Based on \citep{REAL}, we adopt the following layers to form the flows used in our model.

{\bf Affine coupling layer:}
Given a  dimensional input data  and , we partition the input into two vectors  and . The output of one affine coupling layer is given by
,  where  and  are functions from  and  is the element-wise product. The inverse of the transformation is explicitly given by
, . The determinant of the Jacobian matrix of this transformation is simply  . Since computing both the inverse and the Jacobian does not require computing the inverse and Jacobian of  and , both functions can be arbitrarily complex. 

{\bf Combining coupling layers with random permutation:}
Affine coupling layers leave some components of the input data unchanged. In order to transform all the components, two coupling layers are combined in an alternating pattern to form a coupling block, so the unchanged components in the first layer can be transformed in the second layer. In particular, we add a fixed random permutation of dimensions of the input data after each coupling layer. See Figure \ref{fig:1b} for an illustration of a coupling block used in our model. 

 \subsection{The objective function} \label{objective}
 Having defined the invertible flow , we also need to train a deterministic auto-encoder composed of an encoder  and a decoder . The auto-encoder is trained to minimize the reconstruction loss, which we set to be the common MSE loss. The overall training objective is a combination of the reconstruction loss and the NLL loss for the flow transformation:

where  are the parameters of the encoder and decoder respectively,  is the parameter of the flow,  is the stop gradient operation, and  is a hyper-parameter that controls the relative weight of the reconstruction loss and the NLL loss in equation \eqref{flow}. Note that when assuming the decoder distribution is Gaussian with variance  at each pixel,  can be viewed as the inverse variance.
 
After training the model, the generating process is easy: first sample a noise  and then obtain a generated sample , where . Since the highlight of our model is applying a flow on latent variables, we name it \textbf{Generative Latent Flow} (GLF). See Figure \ref{fig:1a} for an illustration of the GLF model.
 
\subsection{Why Stop The Gradients?}\label{strategies}
The stop gradient operation in \eqref{glf_loss} is important. If we let gradients of the NLL loss back propagate into the latent variables, it can lead to degenerate 's. This is because  has to transform the 's to unit Gaussian noise, so the smaller the scale of the 's, the more negative the log-determinant of the Jacobian becomes. Since there is no constraint on the scale of latent variables, the Jacobian term can dominate the entire objective, driving the NLL loss to negative infinity through shrinking  towards . While the latent variables cannot become exactly  because of the presence of reconstruction loss in the objective, the extremely small scale of  may cause numerical issues that cause severe fluctuations. Therefore, we propose to stop the gradient of the NLL loss at the latent variables so that it cannot modify the values of  or affect the parameters of the encoder. We experimentally verify the problems of latent regularization in Section \ref{comparison}.

We call our original model with  stopped gradients \textbf{GLF} and without stopped gradients \textbf{regularized GLF}, since the flow acts as a regularizer on the auto-encoder. Note that for  GLF, the value of  in \eqref{glf_loss} does not matter, since the reconstruction loss and the NLL loss are independent. Note also that GLF can also be trained two stages, namely an auto-encoder is trained first, and then the flow is trained to map the distributions. Empirically, we find that the two-stage training strategy leads to similar performance, so we only focus on one-stage training. 

\subsection{Connection to VAEs with Flow Prior} \label{connect}
Our method is closely related to VAEs with normalizing flow priors. To see this, consider the ELBO objective of plain VAEs with Gaussian prior and posterior ( denote the parameters of encoder and decoder, respectively):

The first term is related to the reconstruction loss and depends on the precision  of the observationd at each pixel, while the last two terms can be combined as . 

If we introduce a normalizing flow  for the prior distribution, then the prior  becomes , where  is the standard Gaussian density. Substituting this prior into \eqref{elbo}, we obtain the  for VAEs with flow prior:


Comparing \eqref{elbo_flow} and \eqref{glf_loss}, we observe that if the expectation over  is estimated by sampling,  is precisely the negative of GLF's objective (without stopping gradients) plus an additional entropy term that corresponds to the entropy of encoder distribution. As  increases two things occur as demonstrated empirically in Section \ref{comparison}.
First the estimated variances from the encoder decrease, and second the contribution of the reconstruction loss
to the gradient of the encoder parameters becomes larger than the contribution of the flow's likelihood loss.
Thus as  increases the VAE+flow converges to GLF. Furthermore Gaussian VAEs with flow prior does not suffer from the degeneracy of regularized GLF
because of the presence of the entropy term. It is the negative sum of the log variances of the latent variables, and thus it encourages the encoder to output large posterior variance, preventing latent variables from collapsing to . 



VAEs with flow prior have attracted very little attention \citep{explicit}, and they have only focused on improvement of the data likelihood. Our work is differs in two ways: 1.  we are the first to evaluate the effects of normalizing flow prior on generation quality; 2. we use deterministic AEs rather than VAEs, thus avoiding the need to choose .


\section{Experiments}\label{experiment}
To demonstrate the performance of our method, we present both quantitative and qualitative evaluations on four commonly used datasets for generative models: MNIST \citep{MNIST}, Fashion MNIST \citep{fashion}, CIFAR-10 \citep{CIFAR} and CelebA \citep{CELEB}. Throughout the experiments, we use 20-dimensional latent variables for MNIST and Fashion MNIST, and 64-dimensional latent variables for CIFAR-10 and CelebA. 

\citep{AreGan} adopted a common network architecture based on InfoGAN \citep{InfoGAN} to evaluate GANs. In order to make fair comparisons without designing arbitrarily large networks to achieve better performance, we use the generator architecture of InfoGAN as our decoder's architecture, and we make the encoder to be symmetric to the decoder. For details of the AE network structures, see Appendix \ref{AppA}. For the flow applied on latent variables, we use 4 affine coupling blocks defined as in Figure \ref{fig:1b}, where each block contains 3 fully connected layers each with  hidden units. For MNIST and Fashion MNIST, , while for CIFAR-10 and CelebA, . Note that the flow only adds a small parameter overhead on the auto-encoder (less than ).

\subsection{Metrics}
We use the Fr\'echet Inception Distance (FID) \citep{FID} as a metric for image generation quality. FID is computed by first extracting features of a set of real images  and a set of generated images  from an intermediate layer of the Inception network \citep{inception}. Each set of features is fitted with a Gaussian distribution, yielding means ,  and co-variances matrices . The FID score is defined to be the Fréchet distance between these two Gaussians:

It is claimed that the FID score is sensitive to mode collapse and correlates well with human perception of generator quality. Recently, \citep{PRD} proposed using Precision and Recall for Distributions (PRD) which can assess both the quality and diversity of generated samples. We also include PRD in our studies. See Appendix \ref{appD}.

\begin{figure}
    \centering
    \subfloat[MNIST]{\includegraphics[width=0.245\textwidth]{fgures/sample_mnist.png}}
    \hfill
    \subfloat[Fashion MNIST]{\includegraphics[width=0.245\textwidth]{fgures/sample_fmnist.png}}
    \hfill
    \subfloat[CIFAR-10]{\includegraphics[width=0.245\textwidth]{fgures/cifar_pick.png}}
     \hfill
    \subfloat[CelebA]{\includegraphics[width=0.245\textwidth]{fgures/celeba_pick.png}}
 
    \hfill
    \subfloat[CelebA-HQ\label{fig:3e}]{\includegraphics[width=0.44\textwidth]{fgures/celebahq_pick.png}}
     \hfill
    \subfloat[Noise interpolation \label{fig:3f}]{\includegraphics[width=0.48\textwidth]{fgures/interpolatemse.png}}
\caption{\label{fig:sample_percept}
    (a)-(e): Randomly generated samples from our method trained on different datasets.
     (f): Random noise interpolation on CelebA.}
\end{figure}

\begin{savenotes}
 \begin{table}[ht]
  \caption{FID scores obtained from different models. For our reported results, we executed 10 independent trials and report the mean and standard deviation of the FID scores. Each trail is computing the FID between 10k generated images and 10k real images.}
  \label{FID}
  \centering
  \begin{tabular}{lllll}
    \toprule
        & MNIST  &Fashion   & CIFAR-10 & CelebA \\
    \midrule
	 VAE &  &  &  & \\
	 WAE-GAN  &  &  &  & \\
	 Two-Stage VAE  &  &  & \footnote{Note that there is a large discrepancy between this and the result reoprted in the original paper. See Appendix \ref{bug} for explanation} &  \\
	 RAE + GMM  &  &  &  & \\
VAE+flow prior &  &  &  & \\
	 VAE+flow posterior &  &  &  & \\
	 GLF (ours) & \textbf{8.2}  0.1 & \textbf{21.3}  0.2 & \textbf{88.3}  0.4 & \textbf{53.2}  0.2 \\
	 \midrule
	 GLANN with perceptual loss &  &  &  &  \\
	 GLF+perceptual loss (ours) & \textbf{5.8}  0.1 & \textbf{10.3}  0.1 & \textbf{44.6}  0.3 & \textbf{41.8}  0.2\\
    \bottomrule
  \end{tabular}
\end{table}
\end{savenotes}

\subsection{Results}
Table \ref{FID} summarizes the main results of this work. We compare the FID scores obtained by our method with the scores of the VAE baseline and several existing AE based models that are claimed to produce high quality samples. Instead of directly citing their reported results, we re-ran the experiments because we want to evaluate them under standardized settings so that all models adopt the same AE architectures, latent dimensions and image pre-processing. We use GLF and VAE+flow prior with  to report the results in the table. For other methods, we largely follow their proposed experimental settings. Details of each experiment are presented in Appendix \ref{AppB}.

Note that the authors of WAE propose two variants, namely WAE-GAN and WAE-MMD. We only report the results of WAE-GAN, as we found it consistently outperforms WAE-MMD. Note also that, GLANN \citep{NAIS} obtains impressive FID scores, but it uses perceptual loss \citep{perceptual} as the reconstruction loss. The perceptual loss is obtained by feeding both training images and reconstructed images into a pre-trained network such as VGG \citep{VGG}, and computing the  distance between some of the intermediate layers' activation. We also train our method with perceptual loss and compare with GLANN in the last two rows of Table \ref{FID}.

As shown in Table \ref{FID}, our method obtains significantly lower FID scores than competing AE based models across all four datasets. In particular, GLF greatly outperforms VAE+flow prior in the default setting. A more detailed analysis and comparison between the two methods will be done in Section \ref{comparison}. We also confirm that VAE+flow posterior cannot improve generation quality. Perhaps the competing model with the closest performances to ours is RAE+GMM, which shares some similarity with GLF in that both methods fit the density of the latent variables of an AE explicitly. To compare our method with GANs, we also include the results from \citep{AreGan} in Appendix \ref{GAN compare}. In \citep{AreGan}, the authors conduct standardized and comprehensive evaluations of representative GAN models with large-scale hyper-parameter searches, and therefore, their results can serve as a strong baseline. The results indicate that our method's generation quality is competitive with that of carefully tuned GANs. 


Qualitative results are shown in Figure \ref{fig:sample_percept}. Besides samples of the datasets used for quantitative evaluation, samples of CelebA-HQ \citep{celebahq} with the larger size of  are also included in Figure \ref{fig:3e} to show our method's potential of scaling up to images with higher resolution. Qualitative results show that our model can generate sharp and diverse samples in each dataset. In Figure \ref{fig:3f}, we show CelebA images generated by linearly interpolating two samples of random noise. The smooth interpolation indicates that our method fits the distribution of latent variables well. For more qualitative results, including samples from the models trained with perceptual loss, see Appendix \ref{AppC}. We see that samples from models trained with perceptual loss have higher quality.

\subsubsection{Comparisons: GLF vs. Regularized GLF and VAE+flow Prior.}\label{comparison}




 
As discussed in Section \ref{strategies}, regularized GLF is unstable because of the degeneracy of latent variables created by the NLL loss. We empirically study the effect of latent regularization as a function of  on CIFAR-10. For low values of  and , the NLL loss completely dominates the learning signal and the reconstruction loss quickly diverges. Even for larger values of 
the NLL loss decreases to a very small negative value, and although overall performance is reasonable it oscillates quite strongly as training proceeds. The relevant plots are shown in Figure \hyperref[fig:cifar glf]{4} in Appendix \ref{additional}.
In contrast, for GLF, where the flow does not modify , the NLL loss does not degenerate, resulting in stable improvements of FID scores as training progress. 

We also trained VAEs+flow prior with different choices of decoder variances (equivalently, different choices of ), plus one with learnable decoder variance as done in \citep{TwoVAE}. We record the progression of FID scores of these models on CIFAR-10 in Figure \hyperref[fig:4]{3a}.
In Figure \hyperref[fig:4]{3b}, we plot the entropy loss, which is one term in VAE+flow prior's minimization objective. The entropy loss is expressed as , where  is the standard deviation of the  latent variable. Higher entropy loss means that the latent variables have lower variances. In Figure \hyperref[fig:4]{3c}, we plot the NLL loss. 

\begin{figure}[ht]
\centering
\includegraphics{plot_tikz/two_plots_vae}
\caption{(a) Record of FID scores on CIFAR-10 for VAEs+flow prior with different values of  and  GLF. (b) Record of entropy losses for corresponding models. (c) Record of NLL losses for corresponding models.} \label{fig:4}. 
\end{figure}

From Figure \hyperref[fig:4]{3a}, we see that GLF converges faster and obtains lower FID score than VAEs+flow prior. The performance gap closes as  increases, however, even with large , GLF still slightly outperforms VAE+flow prior. We also find that the learnable  for VAE+flow prior is not effective, probably due to relatively small values of  in early time. When  is large, as indicated before, the posterior variances become very small, so that effectively we are training an AE. For example, as shown in Figure \hyperref[fig:4]{3b}, when , the corresponding average posterior variance is around . 

In contrast to regularized GLF, there is no degeneracy of latent variables observed thanks to the noise introduced by VAEs and the corresponding entropy term. Indeed, Figure \hyperref[fig:4]{3c} shows that the training of VAE+flow prior does not over-fit the NLL loss, as opposed to regularized GLF where severe over-fitting to NLL loss occurs as shown in Figure \hyperref[fig:cifar glf]{4c}. Comparing Figure \hyperref[fig:4]{3a} and \hyperref[fig:cifar glf]{4a}, we observe that unlike regularized GLF, VAE+flow prior does not suffer from divergence or fluctuations in FID scores, even with relatively small . In general, the results of FID scores show that regularized GLF is unstable, while as  increases, the performance of VAE+flow prior converges to that of GLF, which outperforms them all. 


\subsection{Training time}\label{training}
Besides better performances, our method also has the advantage of faster convergence among competing methods such as GLANN and Two-stage VAE. In Table \ref{epoch} we compare the number of training epochs to obtain the FID scores in Table \ref{FID}. We also compare the per epoch training clock time in Appendix \ref{time}. The results indicate that GLF requires much less training time to generate high quality samples. 

\begin{table}
  \caption{Number of training epochs for Two-stage VAE, GLANN, and GLF}
  \label{epoch}
  \centering
  \begin{tabular}{llll}
    \toprule
        & MNIST/Fashion   & CIFAR-10 & CelebA \\
    \midrule
	 Two-stage VAE First/Second & 400/800 & 1000/2000 &  120/300 \\
	 GLANN First/Second & 500/50  & 500/50 & 500/50 \\
	 GLF & 100 & 200 &  40  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusion}
In this paper, we introduce Generative Latent Flow, a novel generative model which uses an auto-encoder to learn a latent space from training data and a normalizing flow to match the distribution of the latent variables with the prior. Under standardized evaluations, our model achieves state-of-the-art results in image generation among several recently proposed Auto-encoder based models. Besides higher generation quality, our method also enjoys advantages such as faster training time and end-to-end single stage training. While we are not claiming that our GLF model is superior to GANs, we do believe that it opens the door to realize the potential of AE based models to produce high quality samples just as GANs do. The comparison between our method and its stochastic counterparts briefly examines the question about the effects of adding noise during the training of generative models, which is a topic that deserves further studies.  

\bibliographystyle{plain}
\newpage
\begin{thebibliography}{9}
\bibitem[Goodfellow,(2014)]{GAN}
    Goodfellow, Ian, et al. "Generative adversarial nets." In \textit{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Kingma,(2013)]{VAE}
    Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." \textit{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Dinh,(2014)]{NICE}
    Dinh, Laurent, David Krueger, and Yoshua Bengio. "Nice: Non-linear independent components estimation." \textit{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem[Dinh,(2016)]{REAL}
    Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. "Density estimation using real nvp." \textit{arXiv preprint arXiv:1605.08803}, 2016.
    
\bibitem[Bojanowski, (2017)]{GLO}
    Bojanowski, Piotr, Armand Joulin, David Lopez-Paz, and Arthur Szlam. "Optimizing the latent space of generative networks." \textit{arXiv preprint arXiv:1707.05776}, 2017.
    
\bibitem[Li,(2018)]{IMLE}
   Li, Ke, and Jitendra Malik. "Implicit maximum likelihood estimation." \textit{arXiv preprint arXiv:1809.09087}, 2018.

\bibitem[Hoshen, (2018)]{NAIS}
   Hoshen, Yedid, and Jitendra Malik. "Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors." \textit{arXiv preprint arXiv:1812.08985}, 2018.

\bibitem[Liu,(2015)]{CELEB}
   Liu, Ziwei, et al. "Deep learning face attributes in the wild." In \textit{Proceedings of the IEEE international conference on computer vision}, 2015.

\bibitem[Szegedy,(2015)]{inception}
  Szegedy, Christian, et al. "Going deeper with convolutions." \textit{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2015.

\bibitem[Dai,(2019)]{TwoVAE}
  Dai, Bin, and David Wipf. "Diagnosing and enhancing vae models." \textit{arXiv preprint arXiv:1903.05789}, 2019.

\bibitem[Radford,(2015)]{DCGAN}
    Radford, Alec, Luke Metz, and Soumith Chintala. "Unsupervised representation learning with deep convolutional generative adversarial networks." \textit{arXiv preprint arXiv:1511.06434}, 2015.

\bibitem[Kingma, (2018)]{GLOW}
    Kingma, Durk P., and Prafulla Dhariwal. "Glow: Generative flow with invertible 1x1 convolutions." In \textit{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Brock,(2018)]{biggan}
   Brock, Andrew, Jeff Donahue, and Karen Simonyan. "Large scale gan training for high fidelity natural image synthesis." \textit{arXiv preprint arXiv:1809.11096}, 2018.

\bibitem[Zhu, (2017)]{cycgan}
   Zhu, Jun-Yan, et al. "Unpaired image-to-image translation using cycle-consistent adversarial networks." In \textit{Proceedings of the IEEE international conference on computer vision}, 2017.
   
\bibitem[Ledig, (2017)]{super}
    Ledig, Christian, et al. "Photo-realistic single image super-resolution using a generative adversarial network." In \textit{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2017.
    
\bibitem[Salimans, (2016)]{improve}
   Salimans, Tim, et al. "Improved techniques for training gans." In \textit{Advances in neural information processing systems}, 2016.
   
\bibitem[Goodfellow, (2017)]{tutorial}
   Goodfellow, Ian. "NIPS 2016 tutorial: Generative adversarial networks." \textit{arXiv preprint arXiv:1701.00160}, 2016.
   
\bibitem[Srivastava, (2017)]{VEE}
   Srivastava, Akash, et al. "Veegan: Reducing mode collapse in gans using implicit variational learning." In \textit{Advances in Neural Information Processing Systems}, 2017.
   
\bibitem[Miyato, (2018)]{spectralnorm}
   Miyato, Takeru, et al. "Spectral normalization for generative adversarial networks." \textit{arXiv preprint arXiv:1802.05957}, 2018.
   
\bibitem[Arjovsky, (2017)]{wgan}
   Arjovsky, Martin, Soumith Chintala, and Léon Bottou. "Wasserstein gan." \textit{arXiv preprint arXiv:1701.07875}, 2017.
   
\bibitem[Metz, (2016)]{unroll}
   Metz, Luke, et al. "Unrolled generative adversarial networks." \textit{arXiv preprint arXiv:1611.02163}, 2016.


\bibitem[Rezende, (2014)]{rezende}
   Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. "Stochastic backpropagation and approximate inference in deep generative models." \textit{arXiv preprint arXiv:1401.4082}, 2014.

\bibitem[Yan, (2016)]{attri}
    Yan, Xinchen, et al. "Attribute2image: Conditional image generation from visual attributes." In \textit{European Conference on Computer Vision}, Springer, Cham, 2016.
    
\bibitem[Makhzani, (2015)]{adae}
    Makhzani, Alireza, et al. "Adversarial autoencoders." \textit{arXiv preprint arXiv:1511.05644}, 2015.
    
\bibitem[Johnson, (2016)]{perceptual}
    Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. "Perceptual losses for real-time style transfer and super-resolution." In \textit{European conference on computer vision}, Springer, Cham, 2016.
    
\bibitem[Simonyan, (2014)]{VGG}
    Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." \textit{arXiv preprint arXiv:1409.1556}, 2014.
    
\bibitem[Hou, (2017)]{featureconsistent}
    Hou, Xianxu, et al. "Deep feature consistent variational autoencoder." In \textit{2017 IEEE Winter Conference on Applications of Computer Vision (WACV)}, IEEE, 2017.

\bibitem[Ghosh, (2019)]{From}
  Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael J. Black and Bernhard Scholkopf. "From Variational to Deterministic Autoencoders", \textit{arxiv preprint arxiv 1903.12436}, 2019
  
\bibitem[Bauer, (2018)]{resample}
    Bauer, Matthias, and Andriy Mnih. ``Resampled Priors for Variational Autoencoders." \textit{arXiv preprint arXiv:1810.11428}, 2018
    
\bibitem[Tolstikhin,(2017)]{wae}
    Tolstikhin, Ilya, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. ``Wasserstein auto-encoders." \textit{arXiv preprint arXiv:1711.01558}, 2017.

\bibitem[Kingma, (2016)]{impFlow1}
    Kingma, Durk P., et al. ``Improved variational inference with inverse autoregressive flow." In \textit{Advances in neural information processing systems}, 2016.
    
\bibitem[Rezende,(2015)]{impFlow2}
    Rezende, Danilo Jimenez, and Shakir Mohamed. ``Variational inference with normalizing flows." \textit{arXiv preprint arXiv:1505.05770}, 2015.

\bibitem[Berg,(2018)]{impFlow3}
    Berg, Rianne van den, et al. ``Sylvester normalizing flows for variational inference." \textit{arXiv preprint arXiv:1803.05649}, 2018.
    
\bibitem[Burda,(2015)]{IWAE}
   Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. \textit{arXiv:1509.00519}, 2015.
   
\bibitem[LeCun,(2010)]{MNIST}
    Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010.
    
\bibitem[Xiao, (2017)]{fashion}
    Xiao, Han, Kashif Rasul, and Roland Vollgraf. "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms." \textit{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Krizhevsky,(2009)]{CIFAR}
    A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009
    
\bibitem[Lucic, (2016)]{AreGan}
    Lucic, Mario, et al. "Are GANs created equal." A Large-Scale Study. \textit{arXiv preprint arXiv:1711.10337}, 2017.

\bibitem[Chen,(2016)]{InfoGAN}
    Chen, Xi, et al. "Infogan: Interpretable representation learning by information maximizing generative adversarial nets." In \textit{Advances in neural information processing systems}, 2016.

\bibitem[Theis,(2015)]{note}
    Theis, Lucas, Aäron van den Oord, and Matthias Bethge. "A note on the evaluation of generative models." \textit{arXiv preprint arXiv:1511.01844}, 2015.
    
\bibitem[Heusel,(2017)]{FID}
    Heusel, Martin, et al. "Gans trained by a two time-scale update rule converge to a nash equilibrium." \textit{arXiv preprint arXiv:1706.08500}, 2017.

\bibitem[Sajjadi, (2018)]{PRD}
    Sajjadi, Mehdi SM, et al. "Assessing generative models via precision and recall." In \textit{Advances in Neural Information Processing Systems}, 2018.
    
\bibitem[Kingma, (2014)]{adam}
    Kingma, Diederik P., and Jimmy Ba. "Adam: A method for stochastic optimization." \textit{arXiv preprint arXiv:1412.6980}, 2014.
    
\bibitem[Papamarkarios,(2017)]{MAF}
   Papamakarios, George, Theo Pavlakou, and Iain Murray. "Masked autoregressive flow for density estimation." \textit{Advances in Neural Information Processing Systems}, 2017.
   
\bibitem[Oord, (2016)]{PIXELRNN}
   Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. "Pixel recurrent neural networks." \textit{arXiv preprint arXiv:1601.06759}, 2016.
   
\bibitem[Rosca, (2018)]{match}
   Rosca, Mihaela, Balaji Lakshminarayanan, and Shakir Mohamed. "Distribution matching in variational inference." \textit{arXiv preprint arXiv:1802.06847}, 2018.
   
\bibitem[Hoffman, (2016)]{surgery}
   M. D. Hoffman and M. Johnson. Elbo surgery: yet another way to carve up the variational evidence lower bound. \textit{NIPS Workshop on Advances in Approximate Bayesian Inference}, 2016.
   
\bibitem[Tomczak, (2017)]{vamp}
  Tomczak, Jakub M., and Max Welling. "VAE with a VampPrior." \textit{arXiv preprint arXiv:1705.07120}, 2017.

\bibitem[Klushyn, (2019)]{hiera}
  Klushyn, Alexej, et al. "Learning Hierarchical Priors in VAEs." \textit{arXiv preprint arXiv:1905.04982}, 2019.

\bibitem[Chen,(2018)]{tcvae}
  Chen, Tian Qi, et al. "Isolating sources of disentanglement in variational autoencoders." \textit{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Kim,(2018)]{factorsing}
  Kim, Hyunjik, and Andriy Mnih. "Disentangling by factorising." \textit{arXiv preprint arXiv:1802.05983}, 2018.

\bibitem[Makhzani,(2015)]{AAE}
 Makhzani, Alireza, et al. "Adversarial autoencoders." \textit{arXiv preprint arXiv:1511.05644}, 2015.

\bibitem[van den Oord,(2017)]{VQVAE}
  van den Oord, Aaron, and Oriol Vinyals. "Neural discrete representation learning." \textit{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Huang,(2017)]{explicit}
  Huang, Chin-Wei, et al. "Learnable explicit density for continuous latent space and variational inference." arXiv preprint arXiv:1710.02248 (2017).

\bibitem[Gretton.(2007)]{MMD}
  Gretton, Arthur, Borgwardt, Karsten M, Rasch, Malte, Schölkopf, Bernhard, and Smola, Alex J. A kernel method
  for the two-sample-problem. \textit{In Advances in neural information processing systems}, 2007.

\bibitem[Arora,(2017)]{equi}
   Arora, Sanjeev, et al. "Generalization and equilibrium in generative adversarial nets (gans)." \textit{Proceedings of the 34th International Conference on Machine Learning}, 2017.

\bibitem[Grover,(2017)]{flowgan}
   Grover, Aditya, Manik Dhar, and Stefano Ermon. "Flow-gan: Bridging implicit and prescribed learning in generative models." \textit{arXiv preprint arXiv:1705.08868}, 2017.

\bibitem[Alain,(2014)]{RAE}
  Alain, G. and Bengio, Y. What regularized auto-encoders learn from the data-generating distribution. \textit{Journal of Machine
Learning Research}, 15(1):3563–3593, 2014.

\bibitem[Karras,(2017)]{celebahq}
 Karras, Tero, et al. "Progressive growing of gans for improved quality, stability, and variation." \textit{arXiv preprint arXiv:1710.10196}, 2017.

\bibitem[Hoshen,(2018)]{NAM}
  Hoshen, Yedid, and Lior Wolf. "Nam: Non-adversarial unsupervised domain mapping." \textit{Proceedings of the European Conference on Computer Vision (ECCV)}, 2018.
\end{thebibliography}

\newpage
\appendix
\appendixpage
\section{Network Architectures} \label{AppA}
In this section we provide Table \ref{netstruct} that summarizes the auto-encoder network structure. The network structure is adopted from InfoGAN\citep{InfoGAN}, and the difference between the networks we used for each dataset is the size of the fully connected layers, which depends on the size of the image. All convolution and deconvolution layers have  and  to ensure the spatial dimension decreases/increases by a factor of 2.  is simply the size of an input image divided by . Specifically, for MNIST and Fashion MNIST, ; for CIFAR-10, ; for CelebA, . BN stands for batch normalization.

For VAEs, the final FC layer of the encoder will have doubled output size to return both the mean and standard deviation of latent variables. 

\begin{table}
  \caption{Network structure for auto-encoder based on InfoGAN}
  \label{netstruct}
  \centering
  \begin{tabular}{ll}
    \toprule
    Encoder   & Decoder    \\
    \midrule
     Input    &      Input  \\
      , ReLU   &  FC , BN, ReLU\\
      , BN, ReLU &  FC , BN, ReLU \\
     Flatten, FC , BN, ReLU &  , BN, ReLU \\
     FC  &  , Sigmoid\\
    \bottomrule
  \end{tabular}
\end{table}



\section{Experiment Settings} \label{AppB}
In this section, we present the details of our experimental settings for results in Table \ref{FID}. Since the settings for MNIST and Fashion MNIST are the same, we only mention MNIST for simplicity. For GLANN, we directly cite the results from \citep{NAIS}, as their experimental settings is very similar to ours. 

We use the original images in the training sets for MNIST, Fashion MNIST and CIFAR-10. For CelebA, we follow the same pre-processing as in \citep{AreGan}: center crop to  and then resize to . 

\subsection{Settings for training GLF} \label{AppB1}
For all datasets (except CelebA-HQ), we use batch size 256 and Adam \citep{adam} optimizer with initial learning rate  for the parameters of both the AE and the flow. We add a weight decay  to the optimizers for the flow. For MNIST, we train our model for  epochs, with learning rate decaying by a factor of  after  epochs. For CIFAR-10, we train our model for 200 epochs, with the learning rate decaying by a factor of  every  epochs. For CelebA, we train our model for  epochs with no learning rate decay. 

For GLF with perceptual loss as the reconstruction loss, we compute the perceptual loss as suggested in \citep{NAM}. See \url{https://github.com/facebookresearch/NAM/blob/master/code/perceptual_loss.py} for their implementation. Other settings are the same. 

For CelebA-HQ dataset, we adopt our AE network structure based on DCGAN \citep{DCGAN}. Note that this is a relatively simple network for high resolution imgaes. We use batch size 64, with initial learning rate  for both the AE and the flow. We train our model for  epochs, with learning rate decaying by a factor of  after  epochs. 


\subsection{Settings for training VAEs and VAE variants}
We adopt common settings for our reported results of VAE, VAE+flow prior and VAE+flow posterior. We still use batch size 256, and Adam optimizer with initial learning rate  for both the VAE and the flow, if applicable. We find VAEs need longer time to converge, so we double the training epochs. We train MNIST for  epochs, with learning rate decaying by a factor of  after  epochs. We train CIFAR-10 for  epochs, with the learning rate decaying by a factor of  every  epochs. We train CelebA for  epochs with learning rate decaying by a factor of  after  epochs. 

\subsection{Settings for training WAE-GAN}
We follow the settings introduced in the original WAE paper\citep{wae}. The adversary in WAE-GAN has the following architecture:

where  is the dimension of the latent variables. 

WAE has two major hyper-parameters:  which controls the weight coefficient of the adversarial regularizer, and  which is the variance of the prior. Batch size is 100 for all datasets. For MNIST,  and , and the model is trained for 100 epochs. The initial learning rate is  for the AE and  for the adversary. After 30 epochs both learning rates decreased both by factor of 2, and after first 50 epochs further by factor of 5. For CIFAR,  and  and the model is trained for 200 epochs. The initial learning rates are the same as training MNIST, and the learning rate decays by a factor of 2 after first 60 epochs, and further by a factor of 5 after 120 epochs. For CelebA,  and . The model is trained for 55 epochs. The initial learning rate is  for the AE and  for the adversary. Both learning rates decays by factor of 2 after 30 epochs, further by factor of 5 after 50 first epochs.

\subsection{Settings for training Two stage VAE} \label{bug}
We adopt the settings in the original paper \citep{TwoVAE}. For all datasets, the batch size is set to be 100, and the initial learning rate for both the first and the second is . For MNIST, the first VAE is
trained for 400 epochs, with learning rate halved every 150 epochs; the second VAE is trained for 800 epochs with learning rate halved every 300 epochs. For CIFAR-10, 1000 and 2000 epochs are trained for the two VAEs respectively, and the learning rates are halved every 300 and 600 epochs for the two stages. For CelebA, 120 and 300 epochs are trained for the two VAEs respectively, and the learning rates are halved every 48 and 120 epochs for the two stages.

\textbf{Explaining the discrepancy between our reported results and the results in the original paper:} The original Two stage VAE paper adopt similar settings with our experiments, but we observe large discrepancies on the results of CIFAR-10 and CelebA. After carefully reviewing their published codes, we find that there is an issue in their FID score computation particularly for CIFAR-10 dataset. Specifically, the true images used for computating the FID on CIFAR-10 is obtained from saving the original data in .jpg format and reading them back, and the saving will cause some errors in pixel values. After fixing this issue, we re-ran their published codes and obtained similar results as we reported. We also run through their testing protocol using samples from our models, and we ontain scores around 65. For CelebA, one particular detail worth noting is that, \citep{TwoVAE} applies  center-crop before re-sizing on CelebA, while  center-crop is used in our evaluations. With smaller center-crops the human faces occupy a larger portion of the image with less background, making the generative modeling easier.

\subsection{Settings for training RAE+GMM}
The settings of batch size, learning rate scheduling and number of epochs for training RAE are the same as those of GLF. The objective of the RAE is reconstruction loss plus a penalty on the norm of the latent variable. Since the author does not report their choices for the penalty coefficient , we search over , and we find that  leads to the best overall performances, and therefore we let . After training the RAE, we fit a 10-component Gaussian mixture distribution on the latent variables.

\section{Comparison with GANs} \label{GAN compare}
In Table \ref{FID_GAN} we combine our reported results of AE based models and the FID scores of GANs cited from \citep{AreGan}.

\begin{table}
  \caption{FID score comparisons of GANs and various AE based models}
  \label{FID_GAN}
  \centering
  \begin{tabular}{lllll}
    \toprule
        & MNIST  &Fashion   & CIFAR-10 & CelebA \\
    \midrule
      MM GAN &  &  &  &  \\
	 NS GAN &  &  &  &   \\
	 LSGAN &  &  &  &   \\
	 WGAN &  &  &  &    \\
	 WGAN GP &  &  &  &   \\
	 DRAGAN &  &  &  &   \\
	 BEGAN &  &  &  &   \\
	 \midrule
	 VAE &  &  &  & \\
	 WAE-GAN  &  &  &  & \\
	 Two-Stage VAE &  &  &  &  \\
	 RAE + GMM  &  &  &  & \\
	 GLANN (with perceptual loss) &  &  &  &  \\
	 VAE+flow prior &  &  &  & \\
	 VAE+flow posterior &  &  &  & \\
	 GLF (ours) &  &  &  &  \\
	 GLF+perceptual loss (ours) & &  &  & \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Issues with latent regularization}\label{additional}

\begin{figure}[ht]
\centering
\includegraphics{plot_tikz/two_plots_glf}
\caption{(a) Record of FID scores on CIFAR-10 for regularized GLF with different values of  and  GLF.  and  are omitted because they leads to divergence in reconstruction loss. (b) Record of reconstruction losses for corresponding models. (c) Record of NLL losses for corresponding models.}  \label{fig:cifar glf}
\end{figure}

\section{Clock time comparisons}\label{time}
In Table \ref{clock}, we report the clock training time per epoch of our method, two-stage VAE and GLANN. Note that for methods using perceptual loss, the per epoch training time is longer because activations need to be computed. This, together with Table \ref{epoch}, shows that we need much shorter training time while obtaining better performances. In our method, training the flow does not add much computational time due to the low dimensionality.We record the clock time on a platform with a single GTX 1080 GPU. 

\begin{table}[ht]
  \caption{Per-epoch training time in seconds}
  \label{clock}
  \centering
  \begin{tabular}{llll}
    \toprule
        & MNIST & CIFAR-10 & CelebA \\
    \midrule
	 
     2-stage VAE 1st/2nd  &  &  & \\
     GLF &  &  &    \\
     GLANN with perceptual loss&  &  & \\
     GLF with perceptual loss &  &  &  \\
    \bottomrule
  \end{tabular}
\end{table}



\section{Precision and Recall}\label{appD}
In this section, we report the precision and recall (PRD) evaluation of our randomly generated samples on each dataset in Table \ref{prd}. The two numbers in each entry are  that captures recall and precision. See \citep{PRD} for more details. We report the PRD for our models under the setting of obtaining the results in Table \ref{FID}. 

\begin{table}[ht]
  \caption{ Evaluation of random sample quality by precision / recall. Higher numbers are better. }
  \label{prd}
  \centering
  \begin{tabular}{llll}
    \toprule
         MNIST  &Fashion & CIFAR-10 & CelebA \\
    \midrule
      
	  &   &   &   \\
    \bottomrule
  \end{tabular}
\end{table}

\section{More qualitative results}\label{AppC}
In Figure \ref{fig:more_sample_percept1}, we show more samples of each dataset generated by GLF, using either MSE or perceptual loss as reconstruction loss. In Figure \ref{fig:hq2}, we show samples of CelebA-HQ datasets from GLF trained with perceptual loss. In Figure \ref{fig:inter}, we show examples of interpolations between two randomly sampled noises on CelebA from GLF trained with perceptual loss. 

\begin{figure}[ht]
    \centering
    


  \centering
    \subfloat[MNIST]{\includegraphics[width=0.245\textwidth]{fgures/mnist_large_mse.png}}
    \hfill
    \subfloat[Fashion MNIST]{\includegraphics[width=0.245\textwidth]{fgures/fmnist_large_mse.png}}
    \hfill
    \subfloat[CIFAR-10]{\includegraphics[width=0.245\textwidth]{fgures/cifar_large_mse.png}}
     \hfill
    \subfloat[CelebA]{\includegraphics[width=0.245\textwidth]{fgures/celeb_large_mse.png}}
    
    \hfill
    
    \subfloat[MNIST]{\includegraphics[width=0.245\textwidth]{fgures/mnist_large.png}}
    \hfill
    \subfloat[Fashion MNIST]{\includegraphics[width=0.245\textwidth]{fgures/FMNIST_percept_2.jpg}}
    \hfill
    \subfloat[CIFAR-10]{\includegraphics[width=0.245\textwidth]{fgures/cifarp_pick.png}}
     \hfill
    \subfloat[CelebA]{\includegraphics[width=0.245\textwidth]{fgures/celeba_large.png}}
    \caption{\label{fig:more_sample_percept1}
    (a)-(d) Randomly generated samples from our method with MSE loss.
    (e)-(h) Randomly generated samples from our method with perceptual loss.}
\end{figure}


\clearpage

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{fgures/celebahqp_pick.png}
\caption{Randomly generated samples from our method with perceptual loss on CelebA-HQ dataset} \label{fig:hq2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{fgures/interpolate4.png}
\caption{Noise interpolation on CelebA} \label{fig:inter}
\end{figure}
\end{document} 
