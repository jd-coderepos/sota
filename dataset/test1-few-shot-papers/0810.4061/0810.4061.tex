\documentclass{article}

\usepackage[sort]{natbib}
\usepackage{epsfig,amsmath,amssymb}

\newcommand{\todostyle}{\scriptsize\sffamily}
\setlength{\marginparwidth}{0.65\marginparwidth}
\newcommand{\todo}[1]{\marginpar[\todostyle To do: #1\par]
   {\todostyle To do: #1\par}}
\newcommand{\ignore}[1]{}

\renewcommand{\deg}[1]{d_{#1}}
\newcommand{\matr}[1]{\mathbf #1}
\newcommand{\A}{\matr{A}}
\newcommand{\I}{\matr{I}}
\newcommand{\D}{\matr{D}}
\newcommand{\DQ}{\matr{R}}
\renewcommand{\P}{\matr{P}}
\newcommand{\Q}{\matr{Q}}
\newcommand{\M}{\matr{M}}
\renewcommand{\L}{\matr{L}}
\newcommand{\U}{\matr{U}}
\newcommand{\W}{\matr{W}}
\newcommand{\CP}{{\cal P}}
                      \newcommand{\CQ}{\cal{Q}}
\newcommand{\CL}{\cal{L}}
\newcommand{\HP}{\hat{\P}}
\newcommand{\vect}[1]{\mathbf{#1}}
\renewcommand{\u}{\vect{u}}
\newcommand{\vv}{\vect{v}}
\newcommand{\m}{\vect{m}}
\renewcommand{\d}{\vect{d}}
\newcommand{\dq}{\vect{r}}
\newcommand{\inv}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\scriptscriptstyle\mathrm T}}
\renewcommand{\cal}[1]{\mathcal{#1}}
\renewcommand{\sqrt}[1]{{#1}^{\frac{1}{2}}}
\newcommand{\invsqrt}[1]{{#1}^{-\frac{1}{2}}}
\newcommand{\superscript}[1]{\ensuremath{^\textrm{#1}}}
\renewcommand{\th}{\superscript{th}}
\newcommand{\eig}{\lambda}
\newcommand{\eigval}[2]{\eig^{(#1)}_{#2}}
\newcommand{\eigvect}[2]{\vect{v}^{(#1)}_{#2}}
\newcommand{\leig}{\mu}
\newcommand{\leigval}[2]{\leig^{(#1)}_{#2}}
\newcommand{\leigvect}[2]{\vect{u}^{(#1)}_{#2}}
\newcommand{\heig}{\hat{\eig}}
\newcommand{\heigval}[2]{\heig^{(#1)}_{#2}}
\newcommand{\heigvect}[2]{\hat{\vect{v}}^{(#1)}_{#2}}
\newcommand{\spec}[1]{\textrm{Spec}(#1)}
\newcommand{\eigmat}[1]{\Lambda^{(#1)}}

\newcommand{\argmin}{\textrm{argmin}}
\newcommand{\diag}{\textrm{diag}}
\newcommand{\constant}{c}
\newcommand{\abs}[1]{|#1|}
\newcommand{\Chat}{\hat{C}}
\newcommand{\Droot}{D^{-1/2}}
\newcommand{\Dsqr}{D^{1/2}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Sbar}{\bar{S}}
\newcommand{\Scut}{(S,\Sbar)}
\newcommand{\vf}{\vv^f}
\newcommand{\Vol}{\mbox{Vol}}

\title{Locally computable approximations for spectral clustering and
  absorption times of random walks}

\author{Pekka Orponen, Satu Elisa Schaeffer, and Vanesa Avalos-Gayt{\'{a}}n \\
 {\small Department of Information and Computer Science,} \\
{\small Helsinki University of Technology TKK,}\\
{\small FI-02015 TKK Espoo, Finland} \\
{\small \tt pekka.orponen@tkk.fi} \\
 {\small School of Mechanical and Electrical Engineering,} \\
{\small Universidad Aut\'{o}noma de Nuevo Le\'{o}n (UANL),} \\
{\small Ciudad Universitaria, San Nicol\'{a}s de los Garza, NL 66450, Mexico,} \\
{\small \tt elisa,vanesa@yalma.fime.uanl.mx} \\
 {\small Corresponding author, tel.\ +52 81 1340 4000, fax +52 81 1052 3321}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  We address the problem of determining a natural local neighbourhood
  or ``cluster'' associated to a given seed vertex in an undirected
  graph.  We formulate the task in terms of absorption times of random
  walks from other vertices to the vertex of interest, and observe
  that these times are well approximated by the components of the
  principal eigenvector of the corresponding fundamental matrix of the
  graph's adjacency matrix. We further present a locally computable
  gradient-descent method to estimate this Dirichlet-Fiedler vector,
  based on minimising the respective Rayleigh quotient. Experimental
  evaluation shows that the approximations behave well and yield
  well-defined local clusters.
\end{abstract}

{\bf Key words}:
  graph clustering, spectral clustering, random walk, absorption time,
  gradient method

{\bf AMS Classification:} 05C50, 05C85, 68R10, 68W25, 90C27, 90C52, 90C59, 94C15

\pagestyle{plain} 

\section{Introduction and motivation}

\subsection{Nonuniform networks}

The field of natural-network study became popular when Watts and
Strogatz \cite{WaSt98} published their observations on the short
average path length and the high clustering coefficient of many
natural graphs, followed by the observations of scale-free
distributions \cite{BaAl99,FFFa99} in the degrees and other structural
properties of such networks. As a consequence of the resulting wide
interest in the properties of natural networks, there now exist
numerous models to meet the observations made on natural networks
\cite{DoMe03,Newm03,Virt03}. 

\subsection{Graph clustering}

One of the properties of interest in the field of natural graphs is
the presence of \emph{clusters} or \emph{communities} \cite{NeGi03},
that is, the existence of dense induced subgraphs that have relatively
few connections outside compared to the internal density
\cite{Klei01}.

\emph{Graph clustering} is the task of grouping the vertices of the
graph into clusters taking into consideration the edge structure of
the graph in such a way that there should be many edges \emph{within}
each cluster and relatively few \emph{between} the clusters. For an
artificial example, see Figure \ref{fig:caveman} that illustrates a
small graph with a clear six-cluster structure. Another classic
example is a small real-world social network studied by Zachary
\cite{Zach77} and often referred to in graph clustering papers
\cite{WuHu04,OrSc05,Newm03}. It is a social network of a small karate
club that was just about to split into two (see Figure
\ref{fig:karate}), making it an ideal case for two-classification
algorithms. For a survey on graph-clustering algorithms, see
\cite{Scha07}.

\begin{figure}
\centerline{\includegraphics[width=50mm]{fig_1_caveman_graph.eps}}
  \caption{A \emph{caveman graph} \cite{Watt99} composed of six
    near-cliques of five vertices each that have been connected into a
    circulant graph by ``opening'' one edge from each clique (the
    removed edge is shown with a dotted line).}
\label{fig:caveman}
\end{figure}

\begin{figure}
\centerline{\includegraphics{fig_2_karate_club.eps}}
\caption{The karate club social network studied by Zachary
  \cite{Zach77}. The two groups into which the club split are
  indicated by the shape with which the vertices are drawn: the
  squares later formed their own club, and the circles formed another
  club.}
\label{fig:karate}
\end{figure}

\subsection{Local clustering}

In \emph{local clustering}, the goal is to find the cluster of a given
\emph{seed vertex} . Hence, essentially, it is the task of
finding a \emph{bipartition} of the graph  into two vertex sets 
and  such that  and  makes a good cluster
in some predefined sense. Common cluster quality criteria include {\em
  cut capacity} and related measures such as \emph{conductance}
\cite{SiSc06} or density-based measures \cite{Scha05}. Also methods
motivated by electric networks have been proposed for global and local
clustering alike \cite{WuHu04,OrSc05,NeGi04}.

\subsection{Spectra of graphs}

Let  be an unweighted undirected connected graph with at
least two vertices.  For simplicity, we focus on unweighted graphs,
although much of what follows can easily be generalised to the
weighted case.  Denote the order of , i.e.\ its number of vertices,
by  and identify each vertex  with a label in .  Denote the seed vertex by .  The \emph{adjacency matrix} of
 is the binary matrix , where  if edge  is
in , and otherwise .  

For a weighted graph, one would consider instead the analogous
\emph{edge-weight matrix}. Note also that for multigraphs, edge
multiplicities can in the present context be considered simply as
integer weights. For an undirected graph, the adjacency (resp.\ edge
weight) matrix is symmetric, whereas directed graphs pose further
complications in the algebraic manipulation --- we refer the reader to
the textbook and other works of Chung
\cite{Chun97,dirlocal,AnCh07,heatkernel} for properties and local
clustering of directed graphs.

The \emph{degree}  of a vertex  is the number (resp.\ total
weight) of its incident edges; thus the components of the \emph{degree
vector}  of  are the row sums of .  Denote by  the
diagonal  matrix formed by setting the diagonal elements
to  and all other elements to zero.

Let  be the  unit matrix. The \emph{Laplacian} matrix
of  is  and the \emph{normalised Laplacian} matrix of
 is .  Since both  and  are symmetric, all their
eigenvalues are real.  It turns out that  is in some respects a
more natural object of study than , and we shall mostly focus on
that. It is easy to see that zero is an eigenvalue of both  and
, and for  it can be shown that all the other 
eigenvalues (counting multiplicities) lie in the interval .
Denote these in increasing order as , and let  be some right
eigenvector associated to . We may assume that the distinct
eigenvectors  are orthogonal to each other.  For more
information on the spectral and algebraic properties of graphs, see
e.g.\ the excellent monographs of Biggs \cite{Bigg94} and Chung
\cite{Chun97}.


\subsection{Random walks}
\label{sec:random walks}

The \emph{simple random walk} on a graph  is a Markov chain where
each vertex  corresponds to a state and the transition
probability from state  to state  is  if
 and zero otherwise. For a weighted graph,  is
the ratio of the weight of edge  to the total weight of edges
incident to .

Denote the transition probability matrix of this Markov chain by .  Note that even for undirected graphs,  is not
in general symmetric. However, it is similar to the matrix

which \emph{is} symmetric because  is the adjacency matrix of an
undirected graph. Thus,  and  have the same spectrum of
eigenvalues, which are all real. Moreover,

Consequently,  is an eigenvalue of the normalised transition matrix 
if and only if  is an eigenvalue of the normalised
Laplacian matrix .  Thus, ,  and  have the following
correspondence:  is a right eigenvector associated to eigenvalue  in
 if and only if  is a right eigenvector associated
to the same eigenvalue in , and to eigenvalue  in
.

Since in the case of Markov chains, \emph{left} eigenvectors are also
of interest, let us note in passing that the analogous correspondence
holds between each left eigenvector  of  and left eigenvector
 of  or .

Denote the eigenvalues of  in decreasing order as .  Since  is
a stochastic matrix, it always has eigenvalue ,
corresponding to the smallest Laplacian eigenvalue . All the other eigenvalues of  satisfy . If moreover  is connected and not bipartite, the Markov chain
determined by  is ergodic, in which case 
for all . Without much loss of generality, we shall
assume this condition, and moreover that all the eigenvalues 
are nonnegative. Both of these conditions can be enforced by
considering, if necessary, instead of  the ``lazy random walk''
with transition matrix

For a connected graph  this chain is ergodic, and has
nonnegative eigenvalues

with the same eigenvectors as .

Let us then consider a transition matrix  obtained from 
by making a given state, or vertex  \emph{absorbing}.
Thus,  is otherwise equal to , but all
 except for .
We shall henceforth assume, for simplicity of notation, that
, so that in particular  has the block structure:


The \emph{absorption time}  from vertex  to the seed
vertex  is the expected number of steps that a walk initiated at
 will take before hitting . Intuitively, as the absorption time
measures in a certain sense the proximity of vertex  to vertex ,
vertices belonging to a good cluster  for , if such a cluster
exists, should have characteristically smaller absorption times to 
than vertices in . Note that not all graphs exhibit a
clustered structure, in which case no clustering method will be able
to pinpoint a high-quality cluster \cite{Scha07}.

It is well known that the absorption times to vertex 
can be calculated
as row sums

from the {\em fundamental matrix}

where  is the matrix obtained from  (or equivalently from )
by eliminating the row and column corresponding to vertex 
(as shown above in Equation~(\ref{eq:HPblocks})),

\begin{figure}
\centerline{\includegraphics[width=60mm]{fig_3_abstime_matrix.eps}}
\caption{The absorption time matrix composed of 30 absorption-time
  vectors using each vertex of the caveman graph of Figure
  \ref{fig:caveman} in turn as a seed vertex, with white corresponding
  to the maximum  thus obtained and black corresponding to
  the minimum  \emph{and} the diagonal zeroes.}
\label{fig:caveman_abstime}
\end{figure}

In Figure \ref{fig:caveman_abstime}, we illustrate the absorption
times in the caveman graph of Figure~\ref{fig:caveman}: we computed
with Matlab the absorption times from all vertices to a given seed vertex
, repeated the computation for each , and formed a matrix
where each column represents the absorption-time vector for the
corresponding vertex .
The columns are ordered so that all absorption-time vectors associated
to a given cave are grouped together, before those of the next cave,
and so forth. The matrix is visualised as a gray-scale colour map by
placing a tiny \emph{black} square where either  (that is, along
the diagonal) or  (the minimal off-diagonal absorption
time observed), a \emph{white} square where 
(the maximum observed), and discretising the
intermediate values to 254 gray-scale colours correspondingly. The
caves can be distinguished as dark five-by-five blocks along the diagonal,
although the matrix is somewhat too noisy to be trivially clustered.

Now consider the eigenvalue spectra of matrices  and .
Matrix  is still stochastic, so it has largest eigenvalue
, and since the chain is absorbing, all the other
eigenvalues satisfy , .

Denote , where . As  is symmetric (it is obtained by
eliminating the last row and column from the symmetric matrix )
and  is similar to , both have a spectrum of
real eigenvalues .  This spectrum is properly contained in the
interval , because for any vertex  adjacent to ,
, and so the \th\ row sum of  is less than .

We claim that in fact 
  
To prove this claim, let namely  be any non-principal
eigenvalue of  and  a corresponding eigenvector, so that
.  Since the \th\ row of  is zero
except for , it follows that , and since  that necessarily . Then for the -dimensional vector  and for any  it holds that:

Consequently,  is an eigenvector associated to
eigenvalue  of . Since  was chosen
arbitrarily from , this
establishes that .
For the converse direction, a similar argument shows that if
 is an eigenvector associated
to an eigenvalue  of , then the vector
 is an
eigenvector associated to eigenvalue  of .

\section{Spectral methods for bipartitioning}

\subsection{Fiedler vectors}

Spectral clustering of points in space, often modelled as (complete)
weighted graphs, is a widely studied topic \cite{HKK07, KVV04}. In the
context of graphs, the technique is usually applied so that some right
eigenvector associated to the smallest nonzero eigenvalue
 of  is used to produce a bipartitioning of the
graph such that those vertices that have negative values in the
eigenvector form one side of the bipartition  and the vertices with
positive values are the other side .  These
eigenvectors are called \emph{Fiedler vectors} following
\cite{Fied73,Fied75}, where the technique was first proposed.  The
corresponding eigenvectors based on  are called \emph{normalised}
Fiedler vectors. The works on Fiedler-vector based spectral clustering
are numerous and go back for decades \cite{SpTe96,QiHa06,HoSu99}

For our example graph illustrated in Figure \ref{fig:caveman}, such a
bipartition based on  puts three of the caves in  such that it
assigns negative values to every other cave along the cycle of six
caves. Using the eigenvector of , however, assigns only negative
values in the vector and does not yield an intuitive division that
preserves the caves. The two vectors are visualised in Figure
\ref{fig:laplace}.

\begin{figure}
\centerline{\includegraphics[width=128mm]{fig_4_laplace_caveman.eps}}
\caption{The components of the Fiedler vector (left) and the
  normalised Fiedler vector (right) for the caveman graph of Figure
  \ref{fig:caveman}. For the human eye, the six-cluster structure is
  evident in the Fiedler vectors, whereas in the normalised Fiedler vector
  the vertices are grouped into four clusters (two of them consisting
  of two caves).}
\label{fig:laplace}
\end{figure}

If there are only two natural clusters in the graph, such bipartition
works nicely. An example is the Zachary karate club network of Figure
\ref{fig:karate}: the corresponding Fiedler vectors are shown in
Figure \ref{fig:lapkarate}. Also, recursively performing bipartitions
on the subgraphs induced by  and  will help cluster
the input graph  in more than two clusters, but a \emph{stopping
  condition} needs to be imposed to determine when to stop
bipartitioning the resulting subgraphs further.

\begin{figure}
\centerline{\includegraphics[width=128mm]{fig_5_laplace_karate.eps}}
\caption{The components of the Fiedler vector (left) and the
  normalised Fiedler vector (right) for the karate club graph of Figure
  \ref{fig:karate}. The vertices can be classified in two groups:
  those with positive values in the Fiedler vector and those with
  negative values.}
\label{fig:lapkarate}
\end{figure}



\subsection{Spectral partitioning as integer program relaxation}

The use of Fiedler vectors for graph bipartitioning can be motivated
as follows (see for example \cite{HKK07}). Denote a \emph{cut}
(bipartition) of a graph  into vertex sets  and  as .  The {\em capacity} of a cut  is
defined as

A cut  can be conveniently represented by an indicator vector
, where  if , and 
if . 

Then

where the sum is over all the (undirected) edges .

For simplicity, assume now that  is even, and
consider the task of finding an {\em optimal bisection} of ,
i.e.\ a cut  that satisfies 
and minimises  subject to this condition.

This is equivalent to finding an indicator vector
 that satisfies  and
minimises the quadratic form ,
or equivalently (since  is fixed) minimises the ratio:

Since the all-ones vector  is associated to the
eigenvalue , we have by the
Courant-Fischer characterisation of the smallest
nonzero eigenvalue :

where the minimum is taken over all vectors 
satisfying the given condition. Since we can without
loss of generality also constrain the minimisation to,
say, the vectors of norm , we see that
the task of finding a Fiedler vector of  is in fact
a fractional relaxation of the combinatorial problem of
determining an optimal bisection of .

This correspondence motivates the previously indicated 
spectral approach to bisectioning a connected graph 
\cite{DoHo73,Fied73}:
\begin{enumerate}
\item Compute Fiedler vector  of .
\item Determine cut  by rule:
  
\end{enumerate}
where  is the median value of the 's.

The use of \emph{normalised} Fiedler vectors to graph
bipartitioning was explored in \cite{ShMa00},
where it was shown that Fiedler vectors of 
yield fractionally optimal graph bipartitions
according to the {\em normalised cut capacity} measure:

where .

\ignore{
Note that this is quite close to the notion of {\em conductance}
of cut :

}

Since  is an eigenvector of  with
eigenvalue  if and only if 
is eigenvector of  with eigenvalue ,
the eigenvalue  can be characterised
in terms of a ``degree-adjusted'' Rayleigh quotient:

\ignore{
Note that

}

Since  is an eigenvector of  with eigenvalue  if and
only if  is eigenvector of  with
eigenvalue , the eigenvalue  can be
characterised in terms of a ``degree-adjusted'' Rayleigh quotient:


A natural extension of the spectral clustering idea to the local
clustering context is to consider the Laplacian  or  together
with the \emph{Dirichlet boundary condition} that only clustering
vectors  with the seed vertex  fixed to some particular
value are acceptable solutions.

We follow \cite{Chun97,ChEl02} in using the normalised Laplacian 
and choosing , or equivalently 
as the boundary condition.  We thus aim to cluster according to the
``Dirichlet-Fiedler vector'' minimising the constrained Rayleigh
quotient:


For notational simplicity, assume again that , and observe
that for every vector , the value of
the Rayleigh quotient in equation~(\ref{eq:rayleigh}) is the same
as the value of the -dimensional quotient with respect to vector
 and Laplacian  which equals
 with its \th\ row and column removed. Thus, our clustering
vector  is, except for the final zero, the one minimising: 

i.e.\  for the principal eigenvector 
 of the Laplacian . Let us denote  and call
this the \emph{local Fiedler vector} associated to graph  and
seed vertex . 

\section{Local Fiedler vectors and absorption times of random walks}

We shall now show that the components of the local Fiedler vector  are in fact approximately proportional to the
absorption times  discussed in Section~\ref{sec:random walks}.
The connection between the absorption time provides a natural
interpretation to the notion of the local Fiedler vector, and yields
further support to the idea of local clustering by constrained
spectral techniques. Previously random walks and spectral clustering
have been jointly addressed by Meila and Shi \cite{MiSh01} and local
clustering by PageRank by Andersen, Chung, and Lang \cite{dirlocal}.
Important papers linking structural properties of graphs to convergence
rates of random walks via spectral techniques are \cite{Alon86,SiJe89}.

Observe first, from equation~(\ref{eq:lapprob}), that:

where  is as in Equation~(\ref{eq:HPblocks})
and .

Since  is similar to , its spectrum satisfies:

Thus,  is an eigenvalue of 
if and only if 
is an eigenvalue of both  and .
Moreover, if  is an eigenvector associated to eigenvalue
 in , then  is an
eigenvector associated to the same eigenvalue in .


Let then the eigenvalues of  (or equivalently ) be .  Since  is symmetric, it
has a corresponding orthonormal system of eigenvectors
 and a representation:

Denoting the component matrices
, we observe that by
orthogonality of the eigenvectors we have
 for , and by normality .
From these two observations it follows that:

Since ,
we obtain from this for  the representation:

where  is an eigenvector associated
to eigenvalue  in .

Substituting this to Equation~(\ref{eq:fundamental}) and denoting
the -dimensional all-ones vector by , we thus obtain
an expression for the vector  of absorption times 
in terms of the eigenvalues and eigenvectors of , or
equivalently :

where .

Now if the principal eigenvalue  is well-separated
from the others, i.e.\ if the ratio  is small
for , this yields a good approximation for :

Even in cases where there is no evident gap in the spectrum and hence
near-equality cannot be assumed, we have found in our experiments that
the approximations obtained are near-perfectly correlated with the exact
absorption times for a variety of graphs.

We study three example graphs to point out the strengths and
weaknesses of the proposed approximation. The first example graph is
the clustered but highly symmetric caveman graph of Figure
\ref{fig:caveman}, where the symmetries present cause problems for the
proposed approximation.  Our second example is the karate club network
shown in Figure \ref{fig:karate}. The third example graph is a uniform
random graph , with  and 
\cite{Gilb59}, which by definition has no clear cluster structure, and
hence the absorption times cannot be expected to have interesting patterns.

In Figure~\ref{fig:examples}, we show comparisons of some approximate
and exact spectral computations for three example graphs.
In each case, the highest-numbered vertex
of the graph has been chosen as the unique seed vertex.
It can be noted, from the top row of plots in Figure~\ref{fig:examples},
that the spectra of the graphs'  matrices do not exhibit
large gaps between their second and third largest eigenvalues.
Thus, it can not be expected \emph{a priori} that the Fiedler-vector
based approximations to the absorption times, from
Equation~(\ref{eq:approxabsvect}), would be even
of the same magnitude as the exact ones, as calculated from
Equations~(\ref{eq:rowsums}) and~(\ref{eq:fundamental}).
(Observe also how the structure of the caveman graph is reflected
in the corresponding  spectrum: a notable eigenvalue gap
occurs after the six largest eigenvalues, each representing the
dominant convergence behaviour of one of the clusters.)

Correlations between the approximate and exact absorption times
are apparent in the quantile-quantile plots presented in the
second row of Figure~\ref{fig:examples}: here the values
group diagonally when a linear dependency exists.
The correlation is very high in all
cases:  for the caveman graph,  for the karate club
network, and  for the uniform random graph.

The two lowest rows in Figure~\ref{fig:examples} present the
actual values of the exact and approximate absorption-time vectors,
indexed by vertex number. These plots illustrate the usefulness
of these quantities for performing a two-classification of
the vertices into the local cluster of the seed vertex
(low values) versus the other vertices (high values).
In fact, for the caveman graph, the full
six-cluster structure is visible. In the karate club network it can be
seen that two groups are present: one with high values and another one
with low values. (Cf.\ Figure \ref{fig:karateclass}, which indicates
the ``ground truth'' clustering of the vertices in this graph.)
As expected, the uniform random graph
reveals no significant cluster structure, but the vertices near the
seed vertex can be identified by their lower values, whereas most of
the graph has another, higher value. 

\begin{figure}
\centerline{\includegraphics[width=128mm]{fig_6_examples.eps}}
\caption{Comparisons of approximate and exact spectral computations
  for three
  example graphs: the small graphs of Figures \ref{fig:caveman} and
  \ref{fig:karate}, and a uniform random graph ,
  using a random vertex as the seed vertex.
  The top row presents the sorted spectra of the
   matrices of the graphs,
  the second row plots the approximate and exact absorption-time
  values for the given seed vertex against each other,
  and the lowest two rows indicate the exact and approximate
  absorption-time values as ordered by vertex number.
  The bottom rows can be seen as illustrating the quantities'
  capability of distinguishing the cluster of the seed vertex
  (low values) from the other vertices (high values).}
\label{fig:examples}
\end{figure}

In practice, it is not always interesting to compute the absorption
times for all vertices, especially in local computation, in which case
we may only have approximated some of the components of the Fiedler
vector. For these situations, we may write the \th\ component of
the result vector as

From this we obtain for the absorption time from vertex 
to vertex  the expression

Now for a given graph ,  is a constant and so we obtain
the very simple approximate correspondence  between the absorption time vector  and the local
Fiedler vector .

In order to compare the quality of the approximation as well as to
illustrate the computational load in approximating by summing term by
term the series of Equation~(\ref{eq:abssum}), we calculated for each
cutoff length the sum of squares of the differences between the
partial sums and the exact absorption times, divided by the order of
each of the three example graphs: the graph of Figure
\ref{fig:caveman}, the Zachary karate club graph of Figure
\ref{fig:karate}, and the uniform random graph . The resulting values over the set of vertices are shown in
Figure \ref{fig:absconv} (on the left) together with the Pearson
correlations (on the right) achieved at each iteration.  In both
plots, mean and standard deviation are shown.

\begin{figure}
\centerline{\includegraphics[width=128mm]{fig_7_correlations.eps}}
\caption{The sum of squares of the difference from the exact
  absorption-time (on the left) of estimate vectors with different
  cutoff values for approximating through Equation~(\ref{eq:abssum}) and
  Pearson correlation between the exact and the estimate vectors (on
  the right) for the three example graphs: the small graphs of Figures
  \ref{fig:caveman} and \ref{fig:karate}, and the . The values shown are averaged over the vertex sets of the two
  small graphs and over a set of 30 vertices selected uniformly at
  random for the  graph. The smallest standard
  deviation corresponds to the caveman graph and the largest to the
  uniform random graph. The horizontal lines (all three overlap
  between  and ) correspond to the average correlation
  coefficients between the exact and the approximate absorption times
  of Equation~(\ref{eq:approxabsvect}).}
\label{fig:absconv}
\end{figure}

\section{Local approximation of Fiedler vectors}

We take as a starting point the Rayleigh quotient of
Equation~(\ref{eq:rayleighp}). Since we are free to normalise our
eventual Fiedler vector  to any length we wish, we can constrain
the minimisation to vectors  that satisfy, say, .  Thus, the task becomes one of finding a vector  that
satisfies for a given :

We can solve this task approximately by reformulating the requirement
that  as a ``soft constraint'' with weight ,
and minimising the objective function

by gradient descent. Since the partial derivatives of  have
the simple form

the descent step can be computed locally at each vertex at time , based on information about the values of the vector  at time
, denoted by , for the vertex itself and its
neighbours:

where  is a parameter determining the speed of the descent.

Assuming that the natural cluster of vertex  is small compared to
the order of the graph , the normalisation  entails
that most vertices  in the network will have .  Thus
the descent iterations~(\ref{eq:grad_desc}) can be started from an
initial vector  that has  for the
seed vertex  and  for all .
The estimates need then to be updated at time  only for those
vertices  that have at least one neighbour  such that
.

Balancing the constraint weight  against the speed of gradient
descent  naturally requires some care.  We have obtained
reasonably stable results with the following heuristic: given an
estimate  for the average degree of the vertices in the
network, set  and . The gradient
iterations (\ref{eq:grad_desc}) are then continued until all the
changes in the -estimates are below .  We
leave the calibration of these parameters to future work.

The (approximate) Fiedler values thus obtained represent
proximity-values of the vertices in  to the cluster of vertex .
Determining a bisection into  and  is now a
one-dimensional two-classification task that can in principle be
solved using any of the standard pattern classifiers, such as
variations of the basic -means algorithm~\cite{HaWo79}.

We illustrate the applicability approximate absorption times for
clustering the karate club network (Figure \ref{fig:karate}).  The
approximate absorption times shown in Figure \ref{fig:karateclass} are
computed directly with Equation~(\ref{eq:fiedlerabsapprox}): the group
structure is seen to be strong when the seed vertex is one of the
central members of the group, whereas the classification task is
harder for the ``border'' vertices, as can be expected. For more
extensive examples of clustering with the locally computed
approximates, we refer the reader to previous work \cite{OrSc05}.

\begin{figure}
\centerline{\includegraphics[width=128mm]{fig_8_groups.eps}}
\caption{Four examples of two-classifying vertices of the Zachary
  karate club graph. The examples on the left have the seed vertex
  among the ``rectangles'' of Figure \ref{fig:karate} and the examples
  of the right have the seed vertex among the ``circles''. The
  vertices are ordered by their label in Figure \ref{fig:karate} and a
  zero has been inserted to represent the absorption time to the seed
  vertex itself. The group in which the seed belongs is drawn in black
  and the other group in white.}
\label{fig:karateclass}
\end{figure}

\section{Conclusions and further work}

In this work we have derived an expression for the absorption times to
a single absorbing vertex  in a simple random walk in an
undirected, unweighted graph in terms of the spectrum of the
normalised Laplacian matrix of the graph. We have shown that by
only knowing the Fiedler vector corresponding to  on the boundary and
the corresponding eigenvalue provides an approximation of the
absorption times if the spectrum of the graph presents a gap after the
first eigenvalue. Experimentally we have confirmed that the values given
by the approximation are nearly perfectly correlated with the exact
absorption times even in the absence of such a gap. 

Our motivation is to use the absorption times into a seed vertex 
as a measure of proximity in two-classifying the graph into two
partitions: vertices that are ``relevant'' to the seed vertex and
other vertices. Hence, not knowing the exact values but rather another
vector of perfectly correlated values is sufficient for separating
between the vertices with higher values from those with lower values
(which is the classical two-classification task).

Such a two-partition of a graph is known as local clustering. In order
for the proposed values to be locally computable, we have also presented
a gradient-descent method to approximate the Fiedler vector using only
local information in the graph. The method iteratively processes the
neighbourhoods of vertices starting from the seed vertex and expanding
outwards within the group of potentially ``relevant'' vertices,
without any need to process other parts of the graph. We have
illustrated the potential of these vectors in two-classification for local
clustering on a classical example graph representing a social network.

In further work, we seek to study further the effects of the presence
or absence of a spectral gap in the input graph into the approximation
proposed. We also want to calibrate the parameters of the locally
computable approximation in such a way that no a priori knowledge of
the input graph would be needed, but that the method would rather
adapt to the structure of the graph at runtime by dynamic parameter
adjustment. Of additional interest are extensions of this work to
weighted and directed graphs as well as case studies of applications
of local clustering. We also contemplate possible uses for approximate
absorption times in resolving other problems of interest that involve
complex systems represented as graphs.

\section*{Acknowledgements}

The work of Orponen and Schaeffer was supported by the Academy of
Finland under grant 206235 (ANNE, 2004--2006). Schaeffer and Avalos
received support from the UANL under grant CA1475-07 and from PROMEP
under grant 103,5/07/2523. Avalos also thanks CONACYT for support.

A preliminary report on parts of this work was presented as ``Local
clustering of large graphs by approximate Fiedler vectors'' by
P. Orponen and S. E. Schaeffer, at the Fourth International Workshop
on Efficient and Experimental Algorithms in Santorini, Greece, May
2005. The current work was presented at The Fifteenth Conference of the
International Linear Algebra Society (ILAS) in Canc\'{u}n, Quintana
Roo, Mexico, in June 2008.



\begin{thebibliography}{10}

\bibitem{WaSt98}
Duncan~J.\ Watts and Steven~H.\ Strogatz.
\newblock Collective dynamics of 'small world' networks.
\newblock {\em Nature}, 393(6684):440--442, June 1998.

\bibitem{BaAl99}
Albert-L\'{a}szl\'{o} Barab\'{a}si and R\'{e}ka Albert.
\newblock Emergence of scaling in random networks.
\newblock {\em Science}, 286:509--512, October 1999.

\bibitem{FFFa99}
Michalis Faloutsos, Petros Faloutsos, and Christos Faloutsos.
\newblock On power-law relationships of the {I}nternet topology.
\newblock In {\em Proceedings of the {ACM} {SIGCOMM}'99 Conference on
  Applications, Technologies, Architectures, and Protocols for Computer
  Communication}, pages 251--262, New York, NY, USA, 1999. {ACM} Press.

\bibitem{DoMe03}
Sergey~N. Dorogovtsev and Jos\'{e} Ferreira~F. Mendes.
\newblock {\em Evolution of Networks: From Biological Nets to the {I}nternet
  and {WWW}}.
\newblock Oxford University Press, Oxford, UK, January 2003.

\bibitem{Newm03}
Mark~E.J. Newman.
\newblock The structure and function of complex networks.
\newblock {\em SIAM Review}, 45(2):167--256, 2003.

\bibitem{Virt03}
Satu~Elisa Virtanen.
\newblock Properties of nonuniform random graph models.
\newblock Research Report A77, Helsinki University of Technology, Laboratory
  for Theoretical Computer Science, Espoo, Finland, May 2003.

\bibitem{NeGi03}
Mark E.~J.\ Newman and Michelle Girvan.
\newblock Mixing patterns and community structure in networks.
\newblock In Romualdo Pastor-Satorras, Miguel Rubi, and Albert Diaz-Guilera,
  editors, {\em Statistical Mechanics of Complex Networks}, volume 625 of {\em
  Lecture Notes in Physics}, pages 66--87, Berlin, Germany, 2003.
  Springer-Verlag GmbH.

\bibitem{Klei01}
Jon~M.\ Kleinberg and Steve Lawrence.
\newblock The structure of the web.
\newblock {\em Science}, 294(5548):1849--1850, November 2001.

\bibitem{Zach77}
Wayne~W. Zachary.
\newblock An information flow model for conflict and fission in small groups.
\newblock {\em Journal of Anthropological Research}, 33:452--473, 1977.

\bibitem{WuHu04}
Fang Wu and Bernardo~A. Huberman.
\newblock Finding communities in linear time: a physics approach.
\newblock {\em The European Physical Journal B}, 38(2):331--338, 2004.

\bibitem{OrSc05}
Pekka Orponen and Satu~Elisa Schaeffer.
\newblock Local clustering of large graphs by approximate {F}iedler vectors.
\newblock In Sotiris Nikoletseas, editor, {\em Proceedings of the Fourth
  International Workshop on Efficient and Experimental Algorithms (WEA'05)},
  volume 3505 of {\em Lecture Notes in Computer Science}, pages 524--533,
  BerlinHeidelberg, Germany, 2005. Springer-Verlag GmbH.

\bibitem{Scha07}
Satu~Elisa Schaeffer.
\newblock Graph clustering.
\newblock {\em Computer Science Review}, 1(1):27--64, 2007.

\bibitem{Watt99}
Duncan~J. Watts.
\newblock {\em Small Worlds}.
\newblock Princeton University Press, Princeton, NJ, USA, 1999.

\bibitem{SiSc06}
Ji{\v{r}}{\'{\i}} {\v{S}}{\'{\i}}ma and Satu~Elisa Schaeffer.
\newblock On the {NP}-completeness of some graph cluster measures.
\newblock In Ji{\v{r}}{\'{i}} Wiedermann, Gerard Tel, Jarslav Pokorn{\'{y}},
  M{\'{a}}ria Bielikov{\'{a}}, and J{\'{u}}lius {\v{S}}tuller, editors, {\em
  Proceedings of the Thirty-second International Conference on Current Trends
  in Theory and Practice of Computer Science (Sofsem 06)}, volume 3831 of {\em
  Lecture Notes in Computer Science}, pages 530--537, BerlinHeidelberg,
  Germany, 2006. Springer-Verlag GmbH.

\bibitem{Scha05}
Satu~Elisa Schaeffer.
\newblock Stochastic local clustering for massive graphs.
\newblock In T.~B. Ho, D.~Cheung, and H.~Liu, editors, {\em Proceedings of the
  Ninth Pacific-Asia Conference on Knowledge Discovery and Data Mining
  (PAKDD-05)}, volume 3518 of {\em Lecture Notes in Computer Science}, pages
  354--360, BerlinHeidelberg, Germany, 2005. Springer-Verlag GmbH.

\bibitem{NeGi04}
Mark~E.J. Newman and Michelle Girvan.
\newblock Finding and evaluating community structure in networks.
\newblock {\em Physical Review E}, 69:026113, 2004.

\bibitem{Chun97}
Fan~R.K. Chung.
\newblock {\em Spectral Graph Theory}.
\newblock American Mathematical Society, Providence, RI, USA, 1997.

\bibitem{dirlocal}
Reid Andersen, Fan Chung, and Kevin Lang.
\newblock Local partitioning for directed graphs using {P}age{R}ank.
\newblock In {\em Procee dings of WAW 2007}, pages 166--178, 2007.

\bibitem{AnCh07}
Reid Andersen and Fan Chung.
\newblock Detecting sharp drops in {P}age{R}ank and a simplified local
  partitioning algorithm.
\newblock In {\em Proceedings of the Conference on the Theory and Applications
  of Models of Computation (TAMC)}, pages 1--12, 2007.

\bibitem{heatkernel}
Fan Chung.
\newblock The heat kernel as the pagerank of a graph.
\newblock {\em PNAS}, 105(50):19735--19740, 2007.

\bibitem{Bigg94}
Norman Biggs.
\newblock {\em Algebraic Graph Theory}.
\newblock Cambridge University Press, Cambridge, UK, second edition, 1994.

\bibitem{HKK07}
Desmond~J. Higham, Gabriela Kalna, and Milla Kibble.
\newblock Spectral clustering and its use in bioinformatics.
\newblock {\em Journal of Computational and Applied Mathematics},
  204(1):25--37, July 2007.

\bibitem{KVV04}
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
\newblock On clusterings --- good, bad and spectral.
\newblock {\em Journal of the ACM}, 51(3):497--515, 2004.

\bibitem{Fied73}
Miroslav Fiedler.
\newblock Algebraic connectivity of graphs.
\newblock {\em Czechoslovak Mathematical Journal}, 23:298--305, 1973.

\bibitem{Fied75}
Miroslav Fiedler.
\newblock A property of eigenvectors of nonnegative symmetric matrices and its
  application to graph theory.
\newblock {\em Czechoslovak Mathematical Journal}, 25:619--633, 1975.

\bibitem{SpTe96}
Daniel~A.\ Spielman and Shang-Hua Teng.
\newblock Spectral partitioning works: planar graphs and finite element meshes.
\newblock In {\em Proceedings of the Thirty-seventh {{IEEE}} Symposium on
  Foundations of Computing ({FOCS})}, pages 96--105, Los Alamitos, CA, {USA},
  1996. {{IEEE}} Computer Society Press.

\bibitem{QiHa06}
Huaijun Qiu and Edwin~R. Hancock.
\newblock Graph matching and clustering using spectral partitions.
\newblock {\em Pattern Recognition}, 39(1):22--34, January 2006.

\bibitem{HoSu99}
Michael Holzrichter and Suely Oliveira.
\newblock A graph based method for generating the fiedler vector of irregular
  problems.
\newblock In {\em Proceedings of the 11 IPPS/SPDP'99 Workshops Held in
  Conjunction with the 13th International Parallel Processing Symposium and
  10th Symposium on Parallel and Distributed Processing}, volume 1586 of {\em
  Lecture Notes In Computer Science}, pages 978--985, London, UK, 1999.
  Springer-Verlag.

\bibitem{DoHo73}
Wilm~E. Donath and A.~J. Hoffman.
\newblock Lower bounds for the partitioning of graphs.
\newblock {\em IBM Journal of Research and Development}, 17(5):420--425,
  September 1973.

\bibitem{ShMa00}
Jianbo Shi and Jitendra Malik.
\newblock Normalized cuts and image segmentation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  22(8):888--905, August 2000.

\bibitem{ChEl02}
Fan~R.K. Chung and Robert~B. Ellis.
\newblock A chip-firing game and {D}irichlet eigenvalues.
\newblock {\em Discrete Mathematics}, 257:341--355, 2002.

\bibitem{MiSh01}
Marina Meila and Jianbo Shi.
\newblock A random walks view of spectral segmentation.
\newblock In {\em Proceedings of the Eighth International Conference on
  Artificial Intelligence and Statistics (AISTATS 2001)}, 2001.

\bibitem{Alon86}
Noga Alon.
\newblock Eigenvalues and expanders.
\newblock {\em Combinatorica}, 6(2):83--96, 1986.

\bibitem{SiJe89}
Alistair~J. Sinclair and Mark~R. Jerrum.
\newblock Approximative counting, uniform generation and rapidly mixing markov
  chains.
\newblock {\em Information and Computation}, 82(1):93--133, July 1989.

\bibitem{Gilb59}
E.~N.\ Gilbert.
\newblock Random graphs.
\newblock {\em Annals of Mathematical Statistics}, 30(4):1141--1144, December
  1959.

\bibitem{HaWo79}
John~A.\ Hartigan and Manchek~A.\ Wong.
\newblock Algorithm {AS} 136: A -means clustering algorithm.
\newblock {\em Applied Statistics}, 28:100--108, 1979.

\end{thebibliography}


\end{document}
