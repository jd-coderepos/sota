\section{Experimental Evaluation}\label{sec:eval}
We performed a set of experiments to investigate the performance of the proposed programming model and runtime policies, using different benchmark codes that were re-written using the task-based pragma directives. In particular, we evaluate our approach in terms of: 
\begin{inparaenum}[(i)]
	\item The potential for performance and energy reduction;
    \item The potential to allow graceful quality degradation;
    \item The overhead incurred by the runtime mechanisms.
\end{inparaenum}
In the sequel, we introduce the benchmarks and the overall evaluation approach, and discuss the results achieved for various degrees of approximation under different runtime policies. 

\subsection{Approach}\label{sec:eval_approach}

\begin{table}[tb]
\resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Benchmark} & {\bf A}pproximate & \multicolumn{3}{|c|}{Approx Degree} &\multirow{2}{*} {Quality} \\ \cline{3-5}
		 & or {\bf D}rop  & Mild & Med & Aggr & \\ 						\hline
{\hfil Sobel} & A 		& 80\% & 30\% & 0\% & PSNR\\ \hline
{\hfil DCT} & D 		& 80\% & 40\% & 10\% & PSNR\\ \hline
{\hfil MC} & D, A 		& 100\% & 80\% & 50\% & Rel. Err.\\ \hline
{\hfil Kmeans} & A 		& 80\% & 60\% & 40\% & Rel. Err.\\ \hline
{\hfil Jacobi} & D, A 	&  &  &  & Rel. Err.\\ \hline
{\hfil Fluidanimate}& A	& 50\% & 25\% & 12.5\% & Rel. Err.\\ \hline
\end{tabular}
}
\caption{Benchmarks used for the evaluation. For all cases, except Jacobi, the approximation degree is given by the percentage of accurately executed tasks. In Jacobi, it is given by the error tolerance in convergence of the accurately executed iterations/tasks ( in the native version).} 
\label{tab:benchmarks}
\end{table}




\begin{figure}[tb]
\centering
\includegraphics[scale=0.4]{figures/lenasobel.png}
\caption{ Different levels of approximation for the Sobel benchmark}\label{fig:lena_sobel}
\end{figure}

We use a set of six benchmarks, outlined in Table~\ref{tab:benchmarks}, where we apply different approximation approaches, subject to the nature/characteristics of the respective computation. 

\textit{Sobel} is a 2D filter used for edge detection in images. The approximate version of the tasks uses a lightweight Sobel stencil with just 2/3 of the filter taps. Additionally, it substitutes the costly formula  with its approximate counterpart . The way of assigning significance to tasks ensures that the approximated pixels are uniformly spread throughout the output image.

Discrete Cosine Transform (\textit{DCT}) is a module of the JPEG compression and decompression ~\cite{skodras2001jpeg} algorithm. We assign higher significance to tasks that compute lower frequency coefficients. 

\textit{MC}~\cite{Vavalis_hpde} applies a Monte Carlo approach to estimate the boundary of a subdomain within a larger partial differential equation (PDE) domain, by performing random walks from points of the subdomain boundary to the boundary of the initial domain. Approximate configurations drop a percentage of the random walks and the corresponding computations. A modified, more lightweight methodology is used to decide how far from the current location the next step of a random walk should be.

\textit{K-means} clustering aims to partition \textit{n} observations in a multi-dimensional space into \textit{k} clusters by minimizing the distance of cluster members to a cluster representative. In each iteration the algorithm spawns a number of tasks, each being responsible for a subset of the entire problem. All tasks are assigned the same significance value. The degree of approximation is controlled by the {\it ratio} used at {\it taskwait} pragmas. Approximated tasks compute a simpler version of the euclidean distance, while at the same time considering only a subset (1/8) of the dimensions. Only accurate results are considered when evaluating the convergence criteria. 

\textit{Jacobi} is an iterative solver of diagonally dominant systems of linear equations. We execute the first 5 iterations approximately, by dropping the tasks (and computations) corresponding to the upper right and lower left areas of the matrix. This is not catastrophic, due to the fact that the matrix is diagonally dominant and thus most of the information is within a band near the diagonal. All the following steps, until convergence, are executed accurately, however at a higher target error tolerance than the native execution (see Table~\ref{tab:benchmarks}).


\textit{Fluidanimate}, a code from the PARSEC benchmark suite~\cite{Bienia:2008:PBS:1454115.1454128}, applies the smoothed particle hydrodynamics (SPH) method to compute the movement of a fluid in consecutive time steps. The fluid is represented as a number of particles embedded in a grid. Each time step is executed as either fully accurate or fully approximate, by setting the {\it ratio} clause of the {\it omp taskwait} pragma to either 0.0 or 1.0. In the approximate execution, the new position of each particle is estimated assuming it will move linearly, in the same direction and with the same velocity as it did in the previous time steps. 



Three different degrees of approximation are studied for each benchmark: \textit{Mild, Medium, } and \textit{Aggressive} (see Table~\ref{tab:benchmarks}). They correspond to different choices in the quality vs. energy and performance space. No approximate execution led to abnormal program termination. It should be noted that, with the partial exception of Jacobi, quality control is possible solely by changing the {\it ratio} parameter of the {\it taskwait} pragma. This is indicative of the flexibility of our programming model. As an example, Figure~\ref{fig:lena_sobel} visualizes the results of different degrees of approximation for \textit{Sobel}: the upper left quadrant is computed with no approximation, the upper right is computed with \textit{Mild} approximation, the lower left with \textit{Medium} approximation, whereas the lower right corner is produced when using \textit{Aggressive} approximation.

The quality of the final result is evaluated by comparing it to the output produced by a fully accurate execution of the respective code. The appropriate metric for the quality of the final result differs according to the computation. For benchmarks involving image processing (\textit{DCT, Sobel}), we use the peak signal to noise ratio (\textit{PSNR}) metric, whereas for \textit{MC, Kmeans, Jacobi} and \textit{Fluidanimate} we use the relative error.

In the experiments, we measure the performance of our approach for the different benchmarks and approximation degrees, for the two different runtime policies GTB and LQH. For GTB, we investigate two cases: the buffer size is set so that tasks are buffered until the synchronization barrier (referred to as Max Buffer GTB); the buffer size is set to a smaller value, depending on the computation, so that task execution can start earlier (referred to as GTB). 

As a reference, we compare
our approach against:
\begin{itemize}
	\item A fully accurate execution of each application, using a significance agnostic version of the runtime system. 
    \item An execution using loop perforation~\cite{Sidiroglou-Douskos:2011:MPV:2025113.2025133}, a simple yet usually effective compiler technique for approximation. Loop perforation is also applied in three different degrees of aggressiveness. The perforated version executes the same number of tasks as those executed accurately by our approach.
\end{itemize}

The experimental evaluation is carried out on a system equipped with 2 \textit{Intel(R) Xeon(R) CPU E5-2650} processors clocked at 2.00 GHz, with 64 GB shared memory. Each CPU consists of 8 cores. Although cores support SMT execution (hyper-threading), we deactivated this feature during our experiments. We use Centos 6.5 Linux Operating system with the  2.6.32 Linux kernel. Each execution pinned 16 threads on all 16 cores.

Finally the energy and power are measured using likwid~\cite{treibig2010likwid} to access the Running Average Power Limit (RAPL) registers of the processors.

 \begin{comment} 
 The front end policies are evaluated using 4 metrics and the results are demonstrated in figure \ref{fig:policies}:
 \begin{enumerate}
 \item The absolute distance of the user requested ratio from the runtime provided ratio.
 \item The priority inversion, the number of tasks that have executed as significant however should execute as approximate and vice verse.
 \item The output quality of each application. 
 \item The execution time of each application. Comparing different policies demonstrates the overhead of each policy.
 \end{enumerate}
 
 The delay dag policy can be viewed as the reference point for comparing the first three metrics. Since the run time waits until the entire group of tasks to be created, afterwards all the information is present, therefore the corresponding mapping is optimal. However this policy trades of the fourth metric, the performance of the application, since the runtime stalls until all tasks are created.
 
 The probabilistic policy is the best in terms of performance however, a lot of tasks are mapped to a wrong significance value, this observation can be viewed from the increased number of priority inverted tasks and the "wrong" runtime choices lead to a poor quality of the output.
 
 Finally the window policy seems to average the benefits of the other two policies, since it allows to reach an acceptable ratio and few tasks have their priority inverted. The performance of this policy is acceptable, however it is slightly increased in comparison with the probabilistic policy, this is because the runtime waits for a subset of tasks to be created until the  tasks are scheduled to the corresponding cores.
 

\subsection{Evaluation conclusions}
Our runtime behaves comparatively close to OpenMP\cite{openmp4_2013}. When BDDT is instructed to execute the benchmarks 100\% accurate it marginally fails short at delivering the same time and energy performance by <I think this is  I need to check this out >. We note that the benchmarks demonstrate the following two trends:
\begin{enumerate}
\item The energy-benefits of the approximation seem to maximize when the processing cores are set on high frequency.
\item The approximation speedup gain deteriorates as the number of parallel threads increases.
\end{enumerate}
We could elaborate on both observations. For the second one we can present data where we increase the number of tasks. As the number of tasks increase the granularity of significance is finer however the overhead would be immense therefore the programmer should create tasks with the performance as first constraint and as a second the significance.

\end{comment}

\subsection{Experimental Results}

Figure \ref{fig:exp_results} depicts the results of the experimental evaluation of our system.  For each benchmark we present execution time, energy consumption and the corresponding error metric. 

\begin{figure*}
\begin{tabular}{ccccc}
& Execution time (secs) & Energy (Joules) & Quality \\
&  lower is better & lower is better & lower is better \\
\rotatebox{90}{\hspace{0.5in}Sobel} & 
\includegraphics[width=0.3\textwidth]{figures/sobel_time.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/sobel_energy.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/sobel_reverse_psnr.pdf} &
\hspace{-0.22in}\rotatebox{90}{\hspace{0.4in}} \\

\rotatebox{90}{\hspace{0.5in}DCT} & 
\includegraphics[width=0.3\textwidth]{figures/dct_idct_time.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/dct_idct_energy.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/dct_idct_quality.pdf} &
\hspace{-0.22in}\rotatebox{90}{\hspace{0.4in}} \\

\rotatebox{90}{\hspace{0.5in}MC} & 
\includegraphics[width=0.3\textwidth]{figures/monte_time.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/monte_energy.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/monte_quality.pdf} &
\hspace{-0.22in}\rotatebox{90}{\hspace{0.4in}Rel.Error}  \\

\rotatebox{90}{\hspace{0.5in}Kmeans} & 
\includegraphics[width=0.3\textwidth]{figures/kmeans_time.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/kmeans_energy.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/kmeans_quality.pdf} &
\hspace{-0.22in}\rotatebox{90}{\hspace{0.4in}Rel.Error}  \\

\rotatebox{90}{\hspace{0.5in}Jacobi} & 
\includegraphics[width=0.3\textwidth]{figures/jacobi_time.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/jacobi_energy.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/jacobi_quality.pdf} &
\hspace{-0.22in}\rotatebox{90}{\hspace{0.4in}Rel.Error}  \\

\rotatebox{90}{\hspace{0.3in}Fluidanimate} & 
\includegraphics[width=0.3\textwidth]{figures/fluid_time.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/fluid_energy.pdf}  &
\includegraphics[width=0.3\textwidth]{figures/fluid_quality.pdf}  &
\hspace{-0.22in}\rotatebox{90}{\hspace{0.4in}Rel.Error} \\

 & \multicolumn{3}{c}{\includegraphics[width=0.7\textwidth]{figures/label.eps} }\\

\end{tabular}
\caption{Execution time, energy and quality of results for the benchmarks used in the experimental evaluation under different runtime policies and degrees of approximation. In all cases lower is better. Quality is depicted as PSNR for Sobel and DCT, relative error (\%) is used in all others benchmarks. The accurate execution and the approximate execution using perforation are visualized as lines. Note that perforation was not applicable for Fluidanimate.}
\label{fig:exp_results}
\end{figure*}

\begin{figure}[tb]
\centering
\includegraphics[scale=0.4]{figures/lena_perfor.png}
\caption{ Different levels of perforation for the Sobel benchmark. Accurate execution, Perforation of 20\%, 70\% and 100\% of loop iterations on the upper left, upper right, lower left and lower right quadrants respectively.}\label{fig:lena_perf}
\end{figure}

The approximated versions of the benchmarks execute significantly faster and with less energy 
consumption compared to their accurate counterparts. Although the quality of the application output deteriorates as the approximation level increases, this is typically done in a graceful manner, as it can be observed in Figure~\ref{fig:lena_sobel} and the '{\it Quality}' column of Figure~\ref{fig:exp_results}.

The GTB policies with different buffer sizes are comparable with each other. Even though Max buffer GTB postpones task issue until the creation of all tasks in the group, this does not seem to penalize the policy. In most applications tasks are coarse-grained and are organized in relatively small groups, thus minimizing the task creation overhead and the latency for the creation of all tasks within a group. LQH is typically faster and more energy-efficient than both GTB flavors, except for \textit{Kmeans}. 

In the case of \textit{Sobel}, the perforated version seems to significantly outperform our approach in terms of both energy consumption and execution time. However the cost of doing so is unacceptable output quality, even for the mild approximation level as shown in Figure~\ref{fig:lena_perf}. Our programming model and runtime policies achieve graceful quality degradation, resulting in acceptable output even with
aggressive approximation, as illustrated in Figure~\ref{fig:lena_sobel}.

\textit{DCT} is friendly to approximations: it produces visually acceptable results even if a large percentage of the computations is dropped. Our policies, with the exception of the Max Buffer version of GTB, perform comparably to loop perforation in terms of performance and energy consumption, yet resulting in higher quality results\footnote{Note that PSNR is a logarithmic metric}. This is due to the fact that our model offers more flexibility than perforation in defining the relative significance of code regions in DCT. The problematic performance of GTB(Max Buffer) is discussed later in this Section, when evaluating the overhead of the runtime policies and mechanisms.

The approximate version of \textit{MC} significantly outperforms the original accurate version, without suffering much of a penalty on its output quality. Randomized algorithms are inherently susceptible to approximations without requiring much sophistication. It is characteristic that the performance of our approach is almost identical to that of blind loop perforation. We observe that the LQH policy attains slightly better results. In this case, we found that the LQH policy undershoots the requested ratio, evidently executing fewer tasks \footnote{4.6\% and 5.1\% more that requested tasks are approximated for the aggressive and the medium case respectively.}. This affects quality, which is lower than that achieved by the rest of the policies.

\textit{Kmeans} behaves gracefully as the level of approximation increases. Even in the aggressive case, all policies demonstrate relative errors less than 0.45\%. The GTB policies are superior in terms of 
execution time and energy consumption in comparison with the perforated version of the benchmark. 
Noticeably, the LQH policy exhibits slow convergence to the termination criteria. The application terminates when the number of objects which move to another cluster is less than 1/1000 of the total object population. As mentioned in the Section~\ref{sec:eval_approach}, objects which are computed approximately do not participate in the termination criteria. GTB policies behave deterministically, therefore always selecting tasks corresponding to specific objects for accurate executions. On the other hand, due to the effects dynamic load balancing in the runtime and its localized perspective, LQH tends to evaluate accurately different objects in each iteration. Therefore, it is more challenging for LQH to achieve the termination criterion. Nevertheless, LQH produces results with the same quality as a fully accurate execution with significant performance and energy benefits.

\textit{Jacobi} is a particular application, in the sense that approximations can affect its rate of convergence in deterministic, yet hard to predict and analyze ways. The blind perforation version requires fewer  iterations to converge, thus resulting in lower energy consumption than our policies. Interestingly enough, it also results in a solution closer to the real one, compared with the accurate execution.

The perforation mechanism could not be applied on top of the \textit{Fluidanimate} benchmark. This is because if the evaluation of the movement of part of the particles during a time-step is totally dropped, the physics of the fluid are violated leading to completely wrong results. Our programming model offers the programmer the expressiveness to approximate the movement of the liquid for a set of time-steps. Moreover, in order to ensure stability, in is necessary to alternate accurate and approximate time steps. In our programming model this is achieved in a trivial manner, by alternating the parameter of the {\it ratio} clause at {\it taskbarrier} pragmas between 100\% and the desired value in consecutive time steps. It is worth noting that \textit{Fluidanimate} is so sensitive to errors that only the mild degree of approximation leads to acceptable results. Even so, the LQH policy requires less than half the energy of the accurate execution, with the 2 versions of the GTB policy being almost as efficient. 

\begin{figure}[tb]
\centering
\includegraphics[width=0.45\textwidth]{figures/exec_times.eps}
\caption{The normalized execution time of benchmarks under different task categorization policies, with respect to that over the significance-agnostic runtime system}\label{fig:execution_times}
\end{figure}

Following, we evaluate the overhead of the runtime policies and mechanisms discussed in Sections~\ref{sec:overhead} and \ref{sec:overhead2}. We measure the performance of each benchmark when executed with a significance-agnostic version of the runtime system, which does not include the execution paths for classifying and executing tasks according to significance. We then compare it with the performance attained when executing the benchmarks with the significance-aware version of the runtime. All tasks are created with the same significance and the ratio of tasks executed accurately is set to 100\%, therefore eliminating any benefits of approximate execution. Figure~\ref{fig:execution_times} summarizes the results. It is evident that the significance-aware runtime system typically incurs negligible overhead. The overhead reaches in the order of 7\% in the worst case (DCT under the GTB Max Buffer policy). DCT creates many lightweight tasks, therefore stressing the runtime. At the same time, given that for DCT task creation is a non-negligible percentage of the total execution time, the latency between task creation and task issue introduced by the Max Buffer version of the GTB policy results in a measurable overhead.


\begin{table}[tb]
\resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Benchmark} & \multicolumn{3}{|c|}{(\%) Inversed Significance Tasks}  & \multicolumn{3}{|c|}{Average Ratio Diff}  \\ \cline{2-7}
  & LQH & GTB(UD) & GTB (MB)& LQH & GTB(UD) & GTB (MB) \\ \hline
  Sobel &  2.7  &  0 &  0   & 0.07&  0   &  0\\ \hline
  DCT  &  2.7  &  0 &  0   & 0.18    &  0   & 0 \\ \hline
  MC   &  4.8  &  0 &  0   & 0.17  & 0    &0  \\ \hline
  KMeans  &  0   & 0 &  0   & 0.9  &  0  &  0\\ \hline
  Jacobi  &  0  &  0 &  0   & 0.12 &0     &  0\\ \hline
  FluidAnimate  &  0  &  0 &  0  & 0 &  0   & 0 \\ \hline
\end{tabular}
}
\caption{Degree of accuracy of the proposed policies.}\label{tab:cat_policies}
\end{table}

The last step of our evaluation focuses on the accuracy of the policies in terms of respecting the significance of tasks and the user-supplied ratio of accurate tasks to be executed. Table~\ref{tab:cat_policies} summarizes the results. The average offset in the ratio of accurate tasks executed is calculated by the following formula:\\
\begin{center}

\end{center}

The two versions of GTB respect perfectly task significance and the user-specified ratio. This is totally expected for the Max Window version of GTB. The version of GTB using a limited window benefits by the relatively small task groups created by the applications and the smoothly distributed significance values in tasks of each group. LQH, in turn, is inherently more inaccurate, due to its localized perspective. It manages to avoid significance inversion only in cases where all tasks within each task group have the same significance ({\it Kmeans, Jacobi, Fluidanimate}). Even in these cases, LQH may slightly deviate from the specified ratio, due to the loose collaboration of policy modules active on different workers. 
