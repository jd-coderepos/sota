

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{booktabs}
\usepackage{amsbsy}
\usepackage{footmisc}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{todonotes}
\usepackage{arydshln}

\usepackage[normalem]{ulem}

\aclfinalcopy 




\usepackage{algorithmicx} 
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage[T1]{fontenc}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algnewcommand\algorithmicdefinitions{\textbf{Definitions:}}
\algnewcommand\Definitions{\item[\algorithmicdefinitions]}
\algnewcommand\algorithmicindices{\textbf{Indices:}}
\algnewcommand\Indices{\item[\algorithmicindices]}
\newlength{\commentindent}
\setlength{\commentindent}{.5\textwidth}
\renewcommand{\algorithmiccomment}[1]{\unskip\hfill\makebox[\commentindent][l]{//~#1}\par}
\captionsetup[algorithm]{font=footnotesize}
\algrenewcommand\alglinenumber[1]{\footnotesize #1:}



\newcommand{\dline}{\hdashline[0.5pt/1pt]}
\newcommand{\ddline}[1]{\cdashline{#1}[0.5pt/1pt]}

\newcommand{\xxcomment}[4]{\textcolor{#1}{[ #4]}}
\newcommand{\tl}[1]{\xxcomment{blue}{T}{L}{#1}}
\newcommand{\ya}[1]{\xxcomment{red}{Y}{A}{#1}}
\newcommand{\eat}[1]{}

\newcommand{\edit}[3]{\textcolor{#1}{#2\sout{#3}}}
\newcommand{\tle}[2]{\edit{green}{#1}{#2}}
\newcommand{\yae}[2]{{#1}{#2}}

\newcommand{\x}{\mathbf{x}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\cc}{\mathbf{c}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\X}{\mathbf{X}}

\title{When Attention Meets Fast Recurrence:\\Training Language Models with Reduced Compute}


\author{
Tao Lei\\
ASAPP, Inc.\\
{\tt tao@asapp.com}\\
}


\date{}

\begin{document}
\maketitle


\begin{abstract}
Large language models have become increasingly difficult to train because of the required computation time and cost.
In this work, we present SRU++, a recurrent unit with optional built-in attention that exhibits state-of-the-art modeling capacity and training efficiency. 
On standard language modeling benchmarks such as \textsc{enwik8}, \textsc{Wiki-103} and \textsc{billion word} datasets, our model obtains better perplexity and bits-per-character (bpc) while using 3x-10x less training time and cost compared to top-performing Transformer models.
Our results reaffirm that attention is not all we need and can be complementary to other sequential modeling modules~\cite{merity2019single,gulati2020}.
Moreover, fast recurrence with little attention can be a leading model architecture.
\end{abstract} \section{Introduction}
\label{sec:intro}


Many recent advances in language modeling have come from leveraging massive data and training models with significantly increased model size.
Not surprisingly, the associated computation for training has grown enormously, requiring hundreds of GPU hours or days for an experiment.
As a consequence, it has become imperative to build \emph{computationally efficient} models that retain top modeling power with reduced or accelerated computation.


The Transformer architecture~\cite{vaswani2017attention} was proposed to accelerate model training and has become the predominant architecture in NLP.
Specifically, it is built entirely upon self-attention and avoids the use of recurrence to enable strong parallelization.
While this change has led to many empirical success and improved computational efficiency, we are interested in revisiting the architectural question:
\emph{\bf Is attention all we need for modeling?}

\begin{figure}[t]
\includegraphics[width=3.0in]{figures/example.v2.pdf}
\caption{Bits-per-character on \textsc{enwik8} dev set vs. GPU hours used for training. SRU++ obtains better BPC by using 1/8 of the resources. 
We compare with Transformer-XL as it is one of the strongest models on the datasets tested and has the closest implementation to our model (e.g. local attention, state reuse etc.).
Models are trained with fp32 precision and comparable training settings.
}
\label{fig:intro}
\end{figure}


The attention mechanism permits learning dependencies between any parts of the input, making it an extremely powerful neural component in many machine learning applications.
We hypothesize that this advantage can still be complemented with other computation that is directly designed for sequential modeling.
Indeed, several recent works have studied and confirmed the same hypothesis by leveraging recurrence in conjunction with attention.
For example, \citet{merity2019single} demonstrates that single-headed attention LSTMs can produce competitive results compared to Transformer models in language modeling.
Other work have incorporated RNNs into Transformer, and obtain better results in machine translation~\cite{lei2018sru} and language understanding benchmarks~\cite{huang2020transblstm}.
These results highlight one possibility -- we could build more efficient models by combining attention and fast recurrent networks~\cite{bradbury2016quasi,skiprnn,zhang-sennrich-2019-lightweight}.


In this work, we validate this idea and present a recurrent unit with built-in self-attention that achieves strong computation efficiency.
Our work builds upon the SRU~\cite{lei2018sru}, a highly parallelizable RNN implementation that has been shown effective in NLP and speech applications~\cite{park2018fully,shangguan2020optimizing,lin-etal-2020-autoregressive}. 
We incorporate attention into the SRU by simply replacing the linear projection of input with a self-attention component.
The proposed architecture, called SRU++, enjoys high parallelization and enhanced context modeling capacity.
Figure~\ref{fig:intro} compares its performance with the Transformer-XL model~\cite{dai-etal-2019-transformer} on the \textsc{enwik8} dataset.
SRU++ achieves better dev results while using a fraction of the training resources needed by the baseline.

We evaluate SRU++ on several language modeling benchmarks, including the \textsc{enwik8}, \textsc{Wiki-103} and \textsc{billion word} datasets.
We use SRU++ as a replacement of the Transformer layer and stack multiple layers to construct our models.
Our results demonstrate that SRU++ consistently outperforms various Transformer models, delivering on par or better results while using 2.5x-10x less computation.
We open source our implementation to facilitate future research at \url{https://github.com/asappresearch/sru}.

















 
\begin{figure*}[!t]
\centering
\includegraphics[width=5.7in]{figures/arch_illustration.pdf}
\caption{An illustration of SRU and SRU++ networks: (a) the original SRU network, (b) the SRU variant using a projection trick to reduce the number of parameters, experimented in~\citet{lei2018sru} and (c) SRU++ proposed in this work. Numbers indicate the hidden size of intermediate inputs/outputs for  and .}
\label{fig:arch}
\end{figure*}

\section{Background: SRU}
\label{sec:sru}
We first describe the Simple Recurrent Unit (SRU) in this section. 
A single layer of SRU involves the following computation:

where  is the element-wise multiplication, ,  and  are parameter matrices and , ,  and  are parameter vectors to be learnt during training.
The SRU architecture consists of a \emph{light recurrence} component 
which successively reads the input vectors  and computes the sequence of states  capturing sequential information.
The computation resembles other recurrent networks such as LSTM and GRU, by using a forget gate  to control the information flow.
The state vector  is determined by averaging the previous state  and the current observation  according to .
Once the internal state  is produced, SRU uses skip connection implemented via a \emph{highway network} to compute the final output state .
Similarly, the information flow in a highway network is controlled by a reset gate .











Two code-level optimizations are performed to enhance the parallelism and therefore the speed of SRU.
First, given the input sequence  where each  is a -dimensional vector, we group the three matrix multiplications across all time steps as a single multiplication.
This significantly improves the computation intensity (e.g. GPU utilization).
Specifically, the batched multiplication is a linear projection of the input tensor :

where  is the sequence length,  is the output tensor and  is the hidden state size.



The second optimization performs all element-wise operations in an efficient way.
This involves

Note that each dimension of the hidden vectors are independent once  is computed. 
As a result, these operations can be done in parallel across hidden dimension  (and batch size  in a mini-batch scenario).
Similar to other operations such as attention and LSTM, this step is implemented as a CUDA kernel to accelerate computation.





\section{SRU++}
The key modification of SRU++ is to incorporate more expressive non-linear operations into the recurrent network.
Note that the computation of  (Equation 1) is a linear transformation of the input sequence .
We can use other parameterized neural operators to replace this transformation.

Specifically, SRU++ builds in self-attention to enhance its modeling capacity.
Given the input sequence being represented as a tensor or matrix , the attention component computes the query, key and value representations using the following multiplications,

where , ,  are model parameters, and  is the attention dimension (typically smaller than ).
Next, a weighted average output  is computed using the scaled dot-product attention introduced in~\citet{vaswani2017attention},

Finally, the output  required by the element-wise kernel is obtained by another linear projection,

where  is a learned scalar and  is the projection matrix applied to a residual connection  which improves gradient propagation and stablizes training.
 is initialized to zero and as a result

initially falls back to a linear mapping of the input  skipping the attention transformation.
Compared to Equation (1), the linear mapping here can be interpreted as applying a factorization trick  with a small inner dimension  that can reduce the total number of parameters.
Figure~\ref{fig:arch} compares the differences of SRU, factorized SRU and SRU++.






\paragraph{Layer normalization}
We also experiment with adding layer normalization~\cite{ba2016layer} to each SRU++ layer. 
In our implementation, we apply normalization after the attention operation and before the matrix multiplication with ,

Another option is to apply normalization over the hidden states  once they are produced.
Applying either one or both normalizations all work well. 
Note that these variants are post-layer normalizations in which the normalization happens after the skip connection is added.
In contrast, pre-layer normalization~\cite{pmlr-v119-xiong20b} is applied within each non-linear layer.
We use post normalization for better results following the empirical observations in~\citet{liu-etal-2020-understanding}.
%
 
\section{Experimental setup}
\label{sec:setup}

\paragraph{Datasets}
We evaluate our model on three language model benchmarks.
\begin{itemize}
    \item \textsc{Enwik8}~\cite{hutter2012human} is a character-level language modeling dataset consisting of 100 million tokens taken from Wikipedia. 
    The vocabulary size of this dataset is slightly more than 200.
    We use the standard 90M/5M/5M splits as the training, dev and test sets, and report bits-per-character (bpc) as the evaluation metric.
\item \textsc{Wiki-103}~\cite{merity2016pointer} is a word-level language modeling dataset. The training data contains 100 million tokens extracted from Wikipedia articles. 
    Following prior work, we use a vocabulary that has about 260K tokens, and adaptive embedding and softmax layers~\cite{grave2017efficient,baevski2018adaptive}.
\item \textsc{Billion word}~\cite{billionword} is a much larger dataset containing 768 million word tokens for training.
    Unlike \textsc{Wiki-103} in which sentences in the same article are treated as consecutive inputs to model long context, the sentences in \textsc{billion word} are randomly shuffled.
    We follow~\citep{baevski2018adaptive} and use adaptive embedding and softmax layers for this dataset as well. 
    The vocabulary size is about 800K, being the same as prior work.
\end{itemize}

\begin{table}[t!]
    \centering
    \begin{tabular}{lccr}
        \toprule
\bf Model~ & \bf Tok/batch & \bf BPC  & \bf GPUhrs\\
        \hline
        Trans-XL & 24512 & 1.06 & 356\\
        SRU++ &  24512 & 1.03 & 94\\
        SRU++ &  16768 & 1.02 & 37\\
        \bottomrule
    \end{tabular}
    \caption{Comparison between SRU++ and Transformer-XL on \textsc{Enwik8} dataset. We train SRU++ using the same setting as Transformer-XL base model. 
The metrics are smaller the better.  indicates training using automatic mixed precision and distributed data parallel in PyTorch.}
    \label{tab:enwik8_base}
\end{table}

\paragraph{Model}
All our language models are constructed with a word embedding layer, multiple layers of SRU++ and an output linear layer followed by softmax operation. 
We set the hidden dimensions , the number of SRU++ layers to 10 and use single-head attention in each layer.
We use the same dropout probability for all layers and tune this value according to the model size and the results on the dev set.

For simplicity, we do not use additional techniques that are shown useful such as compressed memory~\cite{Rae2020Compressive}, nearest-neighbor interpolation~\cite{Khandelwal2020Generalization}, relative position~\cite{shaw-etal-2018-self,press2020shortformer} and attention variants to handle very long context~\cite{sukhbaatar-etal-2019-adaptive,roy2020efficient}.


\paragraph{Optimization}
We use RAdam~\cite{liu2019radam}\footnote{\url{https://github.com/LiyuanLucasLiu/RAdam}}, a variant of the Adam optimizer~\cite{Kingma:14adam} for training.\footnote{RAdam is reported to be less sensitive to the choice of learning rate and warmup steps, while achieving similar results at the end.}
We use the default  values and a fixed weight decay of  for all experiments.
We use a cosine learning rate schedule following~\citep{dai-etal-2019-transformer} and an initial learning rate of 0.0003.
We do not change the learning rate unless otherwise specified.
See Appendix~\ref{sec:appendix:train_details} for the detailed training configuration of each model.

\begin{table}[!t]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf Param & \bf BPC  & \bf GPU hrs\\
    \hline
    Trans-XL & 41M & 1.06 & 356\\
    SHA-LSTM & 54M & 1.07 & 28\\
    \hline
     & 42M & 1.022 & 37\\
     & 41M & 1.025 & 29\\
     & 41M & 1.032 & 24\\
     & 42M & 1.033 & 22\\
    No attention & 42M & 1.190 & 20\\
    \bottomrule
    \end{tabular}
    \caption{Analyzing SRU++ on \textsc{enwik8} by enabling attention every  layers. We adjust the hidden size so the number of parameters are comparable.  indicates mixed precision training.}
    \label{table:analyze_attention}
\end{table}

For \textsc{Enwik8} and \textsc{Wiki-103} datasets, the training data is partitioned into  chunks by concatenating articles and ignoring the boundaries between articles.
Each training batch contains  consecutive tokens from each chunk (i.e. the unroll size), which gives an effective size of  tokens per batch.
Following previous work, the previous training batch is provided as additional context for attention, which results in a maximum attention length of .
For \textsc{billion word} dataset, we follow~\citep{dai-etal-2019-transformer} that concatenates sentences to create the training batches (instead of adding pad tokens).
Sentences are randomly shuffled and separated by a special token <s> indicating sentence boundaries.

\section{Results}
\label{sec:results}

\begin{table*}[!t]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf Parameters  & \bf Test BPC  & \bf GPU days \\
    \hline
    Longformer 30L~\cite{Beltagy2020Longformer} & 102M & 0.99 & 104\\
    All-attention network 36L~\cite{sukhbaatar2019augmenting} & 114M & 0.98 & 64\\
    Transformer-XL 24L~\cite{dai-etal-2019-transformer}& 277M & 0.99 & - \\
      + Compressive memory~\cite{Rae2020Compressive} & - & 0.97 & -\\
    \hline
    SRU++ Base & 108M & 0.97 & 6 \\
    SRU++ Base () & 98M & 0.98 & 4 \\
    SRU++ Base (w/o layernorm) & 108M & 0.96 & 13 \\
    \hdashline
    SRU++ Large & 191M & 0.96 & 12 \\
\bottomrule
    \end{tabular}
    \caption{Comparison between models on \textsc{enwik8} dataset. We include the training cost (measured by the number of GPUs used  the number of days) if it is reported in the previous work. Our results are obtained using an AWS p3dn instance with 8 V100 GPUs.  indicates mixed precision training.}
    \label{tab:enwik8_sota}
\end{table*}

\paragraph{Comparison with Transformer-XL}
We first conduct a comparison with the PyTorch implementation of Transformer-XL~\cite{dai-etal-2019-transformer} on \textsc{enwik8} dataset\footnote{\url{https://github.com/kimiyoung/transformer-xl/tree/master/pytorch}}.
Their base model consists of 41M parameters and 12 Transformer layers.
Following the official instructions, we reproduced the reported test BPC of 1.06 by training with 4 Nvidia 2080 Ti GPUs.
The training took about 4 days or 360 total GPU hours equivalently.

We train a 10-layer SRU++ model with 42M parameters. For a fair comparison, we use the same hyperparameter setting including the batch size, unroll size, attention context length, learning rate and training iterations as the Transformer-XL base model.
Notably, our base model can be trained using 2 GPUs due to less memory usage. 
After training, we set the attention context length to 2048 for testing, similarly to the Transformer-XL baseline.
Table~\ref{tab:enwik8_base} presents the results.
Our model achieves a test BPC of 1.03, outperforming the baseline by 0.03.
By extending training attention context / unroll size from 512 to 768, we further obtain a test BPC of 1.02.

\begin{figure}[t!]
\includegraphics[width=3.03in]{figures/example.v3.pdf}
    \caption{Dev BPC vs. total GPU hours used on \textsc{enwik8} for each model. 
Using automatic mixed precision (amp) and only one attention sub-layer achieves 16x reduction.
    To compute the dev BPC, the maximum attention length is the same as the unroll size  during training.}
    \label{fig:enwik8_base}
\end{figure}

\paragraph{How much attention is needed?}
\citet{merity2019single} demonstrated that using a single attention layer with LSTM retains most of the modeling capacity compared to using multiple attention layers.
We conduct a similar analysis to understand how much attention is needed in conjunction with the simple fast recurrence.
To do so, we only enable the attention operation every  layers.
The other layers without attention become the SRU variant with dimension projection illustrated in Figure~\ref{fig:arch} (b).
Note that  means the default SRU++ model with attention in each layer, and  means only the last layer has attention in a 10-layer SRU++ model.

Table~\ref{table:analyze_attention} presents the results by varying .
Our baseline model is a 10-layer model with 42M parameters.
We see that using 50\% less attention () achieves almost no increase in test BPC.
Using only one attention module () leads to a marginal loss of 0.01 BPC, but reduces the training time by 40\%. 
Our results still outperform Transformer-XL model and single-headed attention LSTM~\cite{merity2019single} by 0.03 BPC.
Figure~\ref{fig:enwik8_base} showcases the training efficiency of our model.
In comparison with Transformer-XL, SRU++ is 5x more efficient to reach a dev BPC of 1.09.
Furthermore, using automatic mixed precision training and only one attention sub-layer () achieves 16x reduction on training time and cost.

\begin{table*}[t!]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf Parameters  & \bf Test PPL  & \bf GPU days \\
    \hline
    All-attention network 36L~\cite{sukhbaatar2019augmenting} & 133M & 20.6 & -\\
    Feedback Transformer~\cite{fan2020accessing} & 139M & 18.2 & 214\\
    Transformer~\cite{baevski2018adaptive} & 247M & 18.7 & 22\\
    Transformer-XL 18L~\cite{dai-etal-2019-transformer} & 257M & 18.3 & - \\
    + Compressive memory~\cite{Rae2020Compressive} & - & 17.1 & -\\
    \hline
SRU++ Base & 148M & 18.3 & 8 \\
    \hdashline
    SRU++ Large & 232M & 17.4 & 14 \\
    SRU++ Large () & 215M & 17.6  & 10 \\
\bottomrule
    \end{tabular}
    \caption{Comparison between models on \textsc{wiki-103} dataset. We include the training cost (measured by the number of GPUs used  the number of days) if it is reported in the previous work. Our results are obtained using an AWS p3dn instance with 8 V100 GPUs.  indicates mixed precision training.}
    \label{tab:wit103_sota}
\end{table*}

\paragraph{\textsc{enwik8}}
Table~\ref{tab:enwik8_sota} compares our model with other top-performing models on the \textsc{enwik8} dataset.
We train a base model with  and a large model with  using 400K training steps.
The unroll size and attention context length are set to 1024 during training and 3072 during evaluation.
To compare the computation efficiency we report the effective GPU days -- the number of GPUs multiplied by the number of days needed to finish training. 
Our base model achieves an order of magnitude reduction on training time compared to previous reported numbers.
Furthermore, our big model achieves a test BPC of 0.96, being on par with the state-of-the-art results.
Interestingly, we found that turning off layer normalization achieves better generalization on the evaluation sets.
We present an additional analysis for layer normalization later in this section.


\begin{table}[t!]
    \centering
    \begin{tabular}{lccr}
    \toprule
    \bf Model & \bf Param & \bf PPL & \bf GPUdays\\
    \hline
    \multirow{3}{*}{Transformer} & \multirow{2}{*}{331M} & 25.6 & 57\\
    & & 25.2 & 147\\
    & 465M & 23.9 & 192 \\
    \hline
    SRU++ & 328M & 25.1 & 36\\
    SRU++ & 467M & 23.5 & 63\\
\bottomrule
    \end{tabular}
    \caption{Comparison between SRU++ and the Transformer model of~\citet{baevski2018adaptive} on \textsc{billion word} dataset.}
    \label{tab:1blm}
\end{table}

\paragraph{\textsc{Wiki-103}}
Table~\ref{tab:wit103_sota} presents the result of SRU++ models and other top results on the \textsc{Wiki-103} dataset. 
We train one base model with  and 148M parameters, and a large model with  and 232M parameters.
As shown in the table, our base model obtains a test perplexity of 18.3 using 8 GPU days of training, about 3x reduction compared to the Transformer model in~\cite{baevski2018adaptive} and over 10x reduction compared to Feedback Transformer~\cite{fan2020accessing}.
Our big model achieves a test perplexity of 17.4 or 17.6 when trained with 2 attention layers.
The required training cost remains significantly lower.


\paragraph{\textsc{billion word}}
Finally, we evaluate SRU++ models on the \textsc{billion word} dataset. 
We double our training iterations to 800K and use a learning rate of 0.0002 compared to the setting used for \textsc{Wiki-103} and \textsc{enwik8} datasets. 
We train a base model using ,  and an effective batch size of 65K tokens per gradient update.
We also train a large model by increasing the hidden size  to 7616 and the batch size to 98K.
In addition, we only use 2 attention layers () for the large model.
Both models use 10 layers and adopt the adaptive word embedding and softmax output layers as described in~\citet{baevski2018adaptive}.


Table~\ref{tab:1blm} presents the test perplexity and associated training cost (measured by GPU days needed).
Our base and large model obtain a test perplexity of 25.1 and 23.7 respectively, outperforming the Transformer model of~\citet{baevski2018adaptive} given similar model size.
Moreover, SRU++ achieves 3-4x training cost reduction and is trained using 8 GPUs.
In comparison, the Transformer model uses 32 or 64 V100 GPUs for the experiments.

\begin{table}[!t]
    \centering
    \begin{tabular}{lcc}
    \toprule
    \bf Model & \bf Speed & \bf PPL\\
    \hline
    kNN-LM~(\citeauthor{Khandelwal2020Generalization}) & 145 & 15.8\\
    Trans~(\citeauthor{baevski2018adaptive}) & 2.5k & 18.7\\
    Trans-XL~(\citeauthor{dai-etal-2019-transformer}) & 3.2k & 18.3 \\
    Shortformer~(\citeauthor{press2020shortformer}) & 15k & 18.2\\
    \hline
    SRU++ Large & 15k & 17.4\\
    SRU++ Large () & 22k & 17.6\\
    \bottomrule
    \end{tabular}
    \caption{Inference speed (tokens per second) on \textsc{Wiki-103} test set. Results of baseline models are taken from~\citet{press2020shortformer}. We use a single V100 GPU, a batch size of 1 and maximum attention length 2560 for consistency.}
    \label{tab:inference_speed}
\end{table}

\paragraph{Inference speed}
Recurrent networks enjoy a computational benefit because the associated computation is linear with respect to the input length, while a standard attention implementation scales quadratically. 
Table~\ref{tab:inference_speed} compares the inference speed of SRU++ with other strong models on \textsc{wiki-103} test set.
We use a single V100 GPU for inference.
Our large model runs at least 4.5x faster than all baseline models except Shortformer~\cite{press2020shortformer}.
In addition, our model achieves 0.6-0.8 perplexity lower than Shortformer and runs 50\% faster by using attention every 5 layers.

\begin{figure}
   \centering
    \includegraphics[width=2.83in]{figures/layernorm_2.pdf}
    \caption{Understanding the empirical effect of layer normalization. We show the training and dev loss of the small and big SRU++ models on \textsc{enwik8} dataset. Models with layer normalization fit the training data better, but achieve worse generalization in over-parameterized cases.}
    \label{fig:analyze_layernorm}
\end{figure}

\paragraph{The effectiveness of layer normalization}
In our experiments, we have always observed that layer normalization stabilizes training.
For large models however, layer normalization can lead to worse generalization on the evaluation sets, even if we tune dropout carefully.
On the other hand, turning off layer normalization can achieve better dev and test results but makes training sensitive to learning rate and initialization.
For example, we have to use a smaller learning rate of 0.00025 or lower to avoid sudden gradient explosion.

Figure~\ref{fig:analyze_layernorm} showcases our empirical observation on the \textsc{enwik8} dataset. 
Similar results are observed on \text{Wiki-103} dataset, and also in previous work~\cite{xu2019understanding}.
These results suggest possible future work by improving the normalization method~\cite{shen2020powernorm,brock2021characterizing}.
 \section{Conclusion}
We present a recurrent architecture with optional built-in attention and evaluate its effectiveness on various language modeling datasets.
We demonstrate that highly expressive and efficient neural models can be designed using not just attention.
In fact, by incorporating fast recurrent networks, very little attention computation is needed to achieve both top-performing modeling results as well as training speed.
As future work, we believe the self-attentive recurrent models can be further improved using improved attention implementations, normalization and optimization techniques. \section*{Acknowledgement}
We thank Hugh Perkins, Joshua Shapiro, Sam Bowman and Danqi Chen for providing invaluable feedback for this work.
In addition, we thank Jeremy Wohlwend, Jing Pan, Prashant Sridhar and Kyu Han for helpful discussions, and ASAPP Language Technology and Infra teams for the compute cluster setup for our research experiments.
Finally, we'd like to thank Tao Ma, Will Robinson and Gustavo Sapoznik for their support of this work and all research work at ASAPP in general. \bibliography{emnlp2018}
\bibliographystyle{acl_natbib_nourl}

\clearpage
\newpage
\appendix

\section{Changes after the first version of this paper}
\label{sec:appendix:changes_since_v1}
The first version of this paper is submitted to arXiv in February 2021.
The current version reports slightly improved perplexity results and reduced training time in the result section due to a few changes to our SRU++ implementation and the experiment code.
The changes include:
\begin{itemize}
    \item Using half precision for the elementwise kernel during mixed precision training. This improves the training speed by about 10\% without hurting the final results.\footnote{\url{https://github.com/asappresearch/sru/pull/166}}
    \item Fixing a precision issue that the loss tensor is half precision when using apdative softmax and mixed precision training.\footnote{\url{https://github.com/asappresearch/sru/pull/168}}
    \item Fixing an implementation issue of data loading and batching in \textsc{billion word} experiments, which makes GPUs highly under-utilized. This speeds up \textsc{billion word} training significantly.\footnote{\url{https://github.com/asappresearch/sru/pull/169}}
\end{itemize}
In addition, we include more results on the \textsc{billion word} dataset by training a large SRU++ model with 467M parameters.

\begin{table*}[!h]
\centering
\begin{tabular}{lcccc}
\hline
 & Base model & Base model & Base model & Large model \\
 & () & & (w/o layernorm) & \\
\hline
Attention / unroll size - train & 1024 & 1024 & 1024 & 1024 \\
Attention / unroll size - test & 3072 & 3072 & 3072 & 3072 \\
Batch size  Num of GPUs & 48 & 48 & 88 & 88 \\
Dropout & 0.22 & 0.22 & 0.25 & 0.32 \\
Gradient clipping & 1.0 & 1.0 & 1.0 & 1.0 \\
Hidden size  & 3072 & 3072 & 3072 & 4096 \\
Hidden size  & 768 & 768 & 768 & 1024 \\
Learning rate & 0.0003 & 0.0003 & 0.0003 & 0.0004 \\
LR warmup steps & 16K & 16K & 16K & 16K \\
Training steps & 400K & 400K & 600K & 400K \\
Weight decay & 0.1 & 0.1 & 0.1 & 0.1 \\
\hline
Model size & 98M & 108M & 108M & 191M \\
Dev BPC & 1.002 & 0.997 & 0.983 & 0.985 \\
Test BPC & 0.980 & 0.974 & 0.961 & 0.963\\
\bottomrule
\end{tabular}
\caption{Training details of SRU++ models on \textsc{enwik8} dataset.}
\label{tab:config_enwik8}
\end{table*}
\begin{table*}[!h]
\centering
\begin{tabular}{lccc}
\hline
 & Base model & Large model & Large model \\
 & & () & \\
\hline
Attention / unroll size - train & 768 & 1024 & 1024 \\
Attention / unroll size - test & 2560 & 2560 & 2560\\
Batch size  Num of GPUs & 88 & 88 & 88 \\
Dropout & 0.15 & 0.2 & 0.2 \\
Gradient clipping & 1.0 & 1.0 & 1.0 \\
Hidden size  & 3072 & 4096 & 4096 \\
Hidden size  & 768 & 1024 & 1024\\
Learning rate & 0.0003 & 0.0003 & 0.0003\\
LR warmup steps & 16K & 16K & 16K \\
Training steps & 400K & 400K & 400K \\
Weight decay & 0.1 & 0.1 & 0.1 \\
\hline
Model size & 148M & 215M & 232M \\
Dev PPL & 17.5 & 16.9 & 16.7\\
Test PPL & 18.3 & 17.6 & 17.4\\
\bottomrule
\end{tabular}
\caption{Training details of SRU++ models on \textsc{Wiki-103} dataset.}
\label{tab:config_wiki103}
\end{table*}

\section{Training details}
\label{sec:appendix:train_details}

We use the RAdam optimizer with the default hyperparameters  and  for all our experiments.
We use a cosine learning rate schedule with only 1 cycle for simplicity.
For faster training, we also leverage the native automatic mixed precision (AMP) training and distributed data parallel (DDP) of Pytorch in all experiments, except those in Table~\ref{tab:enwik8_base} and Figure~\ref{fig:intro} for a fair comparison with the Transformer-XL implementation.

Table~\ref{tab:config_enwik8} shows the detailed training configuration of SRU++ models on \textsc{enwik8} dataset.
Most training options are kept the same for all models. 
We tune the dropout probability more carefully as we found training is more prone to over-fitting and under-fitting for this dataset.
The large model is trained with 2x batch size.
We increase the learning rate by a factor of  following the suggestion in~\citet{hoffer2017}, which results in a rounded learning rate of 0.0004.

We additionally train a model without layer normalization to showcase that it can achieve state-of-the-art results, following our analysis in the result section.
The model without layer normalization uses the smaller learning rate 0.0003, as large learning rates can lead to gradient explosion.
We use an increased number of training steps of 600K for this experiment.

Table~\ref{tab:config_wiki103} presents the detailed training configuration on \textsc{Wiki-103} dataset.
Similarly we use  and  for the base and large model respectively and fix .
Following~\cite{baevski2018adaptive}, we use an adaptive word embedding layer and an adaptive softmax layer for our models, and we tie the weight matrices of the two layers.
 
\end{document}
