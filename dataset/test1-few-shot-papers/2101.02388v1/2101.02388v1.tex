\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{changepage}
\usepackage{bm}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=1.0in]{geometry}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{mathtools}
\usepackage{float}

\usepackage[backend=bibtex, maxcitenames=2]{biblatex}
\addbibresource{references.bib}
\DeclareCiteCommand{\citeauthor} 
  {\boolfalse{citetracker}\boolfalse{pagetracker}\usebibmacro{prenote}}
  {\ifciteindex
     {\indexnames{labelname}}
     {}\printtext[bibhyperref]{\printnames{labelname}}}
  {\multicitedelim}
  {\usebibmacro{postnote}}
\DeclareCiteCommand{\citeyear}
    {}
    {\bibhyperref{\printdate}}
    {\multicitedelim}
    {}
\newcommand{\citep}[1]{\citeauthor{#1}, \citeyear{#1}}
\newcommand{\cited}[1]{\citeauthor{#1} (\citeyear{#1})}
\newcommand{\citebracket}[1]{\cite{#1}}

\pdfpageheight=11in
\pdfpagewidth=8.5in
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}\let\endchangemargin=\endlist 
\def\subsectionautorefname{section}
\SetArgSty{textnormal}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\graphicspath{ {./} }
\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}

\newcommand{\x}{\textbf{x}}
\newcommand{\z}{\textbf{z}}
\newcommand{\ptheta}{p_\theta}
\newcommand{\boldmu}{\boldsymbol{\mu}}
\newcommand{\constantA}{\sqrt{\frac{1 - \alpha_{t-1}}{1-\alpha_t}}}
\newcommand{\etheta}{\epsilon_{\theta}}
\newcommand{\Qnextstep}{q(\x_{t-1} \vert \x_t, \x_0)}
\newcommand{\Pnextstep}{\ptheta(\x_{t-1} \vert \x_t)}
\newcommand{\fxt}{f_\theta(\x_t)}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\constantB}{\sqrt{\alpha_t}\x_0 + \sqrt{1-\alpha_t}\epsilon_t}
\newcommand{\pstudent}{p_{\text{student}}}
\newcommand{\pteacher}{p_{\text{teacher}}}
\newcommand{\Fteacher}{\mathcal{F}_\text{teacher}}
\newcommand{\Fstudent}{\mathcal{F}_\text{student}}
\newcommand{\stdnormal}{\mathcal{N}(\textbf{0}, \textbf{I})}



\begin{document}

\begin{center}
\begin{huge}
Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed\\
\end{huge}
\end{center}

\vspace{0.25cm}

\begin{center}
\begin{large}

\textbf{Eric Luhman}\footnote{\label{note0}Equal contribution} \hspace{4cm} \textbf{Troy Luhman}\footnotemark[1] \\
ericluhman2@gmail.com \hspace{4cm} troyluhman@gmail.com
\end{large}
\end{center}
\vspace{0.25cm}

\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}





\begin{center}
\textbf{Abstract}
\\

\end{center}
\begin{changemargin}{1.75cm}{1.75cm} 
Iterative generative models, such as noise conditional score networks and denoising diffusion probabilistic models, produce high quality samples by gradually denoising an initial noise vector. However, their denoising process has many steps, making them 2-3 orders of magnitude slower than other generative models such as GANs and VAEs. In this paper, we establish a novel connection between knowledge distillation and image generation with a technique that distills a multi-step denoising process into a single step, resulting in a sampling speed similar to other single-step generative models. Our \textit{Denoising Student} generates high quality samples comparable to GANs on the CIFAR-10 and CelebA datasets, without adversarial training. We demonstrate that our method scales to higher resolutions through experiments on  LSUN. Code and checkpoints are available at \url{https://github.com/tcl9876/Denoising_Student}


\end{changemargin}



\section{Introduction}

Image Generation is an important and well studied problem in computer vision. There are a variety of approaches to image generation which yield high quality results, such as Generative Adversarial Networks (GANs, \citep{goodfellow2014generative}), Variational Autoencoders (VAEs, \citep{kingma2014autoencoding}), and energy-based models (\citep{lecun2006tutorial}). GANs generally have been able to generate the highest quality images, especially at higher resolutions (\citep{stylegan2}; \citep{stylegan2ada}; \citep{biggan}). However, their adversarial training procedure makes them more difficult to train than other methods of generative modeling. 

Two increasingly popular methods of generative modeling are Denoising score-matching (\citep{vincent2011connection}; \citep{song2019generative}) and Denoising Diffusion Probabilistic Models (DDPMs, \citep{sohldickstein2015deep}; \citep{ddpm}), both of which model data by gradually reversing a noise-adding process. Denoising score-matching methods use a neural network such as a Noise conditional score network (NCSN) to estimate the score, or gradient of the logarithmic data density.  DDPMs are trained to reverse each step of the noise-adding (a.k.a. diffusion) process, and can also be parameterized to implicitly estimate scores (\citep{ddpm}; \citep{sde}b). We refer to both models as \textit{score-based generative models}. Both approcahes use MCMC methods similar to Langevin dynamics during sampling time. Score based models are capable of producing samples that rival even the best GAN methods, without adversarial training  (\citep{ddpm}; \citep{sde}b).

A major downside to score-based generative models is that they require performing expensive MCMC sampling, often with a thousand steps or more. As a result, they can be up to three orders of magnitude slower than GANs, which only require a single network evaluation. To address this issue, Denoising Diffusion Implicit Models, or DDIMs, have been proposed (\citep{ddim}a). DDIMs use a generative Markov chain to reverse a non-Markovian inference process, and are a type of implicit probabilistic model (\citep{mohamed2017learning}). Unlike DDPMs and NCSNs, they have a \textit{deterministic} generative process that only depends on an initial latent variable. Nevertheless, DDIMs are still rather inefficient, requiring 20-100 function evaluations to produce good samples.

In this work, we propose a method of approximating a multi-step generative process with only \textit{a single function evaluation}, through the use of knowledge distillation (\citep{bucilua2006model}; \citep{hinton2015distilling}), a technique of compressing a computationally expensive teacher into a smaller student model that learns to approximate the teacher's output distribution. Our \textit{Denoising Student} synthesizes data directly from Gaussian noise without any intermediate denoising steps, and is trained to predict the output of a DDIM given the same initial noise vector. 

Our approach has a number of desirable properties. Firstly, it is far more efficient ( to ) than existing score-based approaches, making it similar to GANs and VAEs in sampling speed. Secondly, it has a simple, stable objective that does not involve any adversarial training or surrogate losses (e.g. the encoder in VAEs). Thirdly, it can be easily applied to any iterative model with a deterministic generative process, as it involves no additional architectural considerations or hyperparameters (e.g. the noise schedule, the stepsize of Langevin dynamics). Lastly, it retains the abilities of other implicit models, such as semantically meaningful interpolations through the latent space.

Despite these advantages, it produces high fidelity samples comparable to GANs on lower resolution datasets such as CIFAR-10 and CelebA . It also scales to higher resolutions, generating decent quality samples of size . To the best of our knowledge, Denoising Student is the first non-adversarial model that can produce  LSUN images in only a single step.


\section{Knowledge Distillation in Deterministic Generative Models}

\subsection{Knowledge Distillation}
Knowledge distillation (\citep{bucilua2006model}; \citep{hinton2015distilling}) involves compressing an expensive but high-performing teacher model into a smaller student model. The student model is trained to minimize the cross-entropy between its output distribution and the output distribution of the teacher, which results in better performance than if it was trained normally. In supervised learning tasks, the motivation for this approach comes from the information hidden in the teacher's output, which isn't present in a one-hot label. For instance, an image with two objects could be plausibly classified as either one, which is reflected in the teacher's output distribution, but not the original label. 

One condition of knowledge distillation is that the function to be learned must be deterministic, otherwise, it will be difficult, if not impossible to learn. In supervised learning tasks, this is a rather trivial condition, since models produce the same output given the same input. In generative modeling, however, this condition is no longer trivial. Score-based and energy-based models use a stochastic MCMC procedure such as Langevin dynamics, making their usage as a teacher infeasible. We could apply knowledge distillation to a GAN, which is deterministic given the same latent variable, but we would gain only a minor speedup over an already fast model.  

Recently, \citeauthor{ddim} (\citeyear{ddim}a) proposed a new class of generative models called \textit{denoising diffusion implicit models} (DDIMs), which are similar to other iterative generative models in terms of sample quality but have somewhat faster sampling speed. Unlike other iterative models\footnote{An even more recent work (\citep{sde}b) proposed a neural \textit{probability flow ODE} that also has a deterministic, multi-step generative process. However, pretrained models are unavailable as of writing, and we wanted to avoid retraining the teacher.}, their generative process is deterministic, which motivates their usage as a teacher model. We will discuss them below. For further explanation and proofs related to DDIMs, the reader is referred to \citeauthor{ddim} (\citeyear{ddim}a).

\subsection{Denoising Diffusion Implicit Models}

Consider an inference process with latent variables  of the same dimensionality as the data , parameterized by a decreasing sequence of constants . The inference process  is defined as:

10pt]
\Qnextstep = \delta(\x_{t-1} - \boldmu(\x_t, \x_0))
\label{eq4}
\x_t = \sqrt{\alpha_t} \x_0 + \sqrt{1-\alpha_t} \epsilon, \epsilon \sim \stdnormal
\label{eq5}
\ptheta(\x_0) \coloneqq \int \ptheta(\x_{0:T}) \text{d} \x_{1:T}, \text{ with }
\ptheta(\x_{0:T}) \coloneqq p(\x_T) \prod_{t=1}^{T} \Pnextstep
\label{eq6}
\Pnextstep =
 \begin{cases}
 q(\x_{t-1} \vert \x_t, \fxt) & \text{if } \\
 \delta(\x_0 - f_\theta(\x_1)) & \text{if } 
 \end{cases}
\label{eq7}
\sqrt{\frac{1}{\alpha_{t-1}}} \x_{t-1} = \sqrt{\frac{1}{\alpha_t}} \x_t - \left(\sqrt{\frac{1-\alpha_{t}}{\alpha_{t}}} - \sqrt{\frac{1-\alpha_{t-1}}{\alpha_{t-1}}} \right) \etheta(\x_t) , \text{ or }
\label{eq8}
\x_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}}(\x_t - \sqrt{1-\alpha_t}\etheta(\x_t)) + \sqrt{1-\alpha_{t-1}}\etheta(\x_t)
\label{eq9}
L = \sum_{t=1}^{T} \gamma_t \mathbb{E}_{\x_0, \epsilon_t} \left[ \norm{\epsilon_t - \etheta(\constantB, t)}_2^2 \right] + C,
\label{eq10}
L_\text{student} = \mathbb{E}_{\x_T} \left[D_{\text{KL}}( \pteacher(\x_0 \vert \x_T) \ \Vert \ \pstudent(\x_0 \vert \x_T)) \right]
\label{eq11}
L_\text{student} = \frac{1}{2} \mathbb{E}_{\x_T} [\norm{\Fstudent(\x_T) - \Fteacher(\x_T)}_{2}^{2}]+C

Training of the student is done by sampling random  from the prior  and computing the corresponding  with a DDIM, then minimizing Eq. \eqref{eq11}. Unlike other generative models, our method does not require the joint training of two networks (e.g. discriminator in GANs, inference model in VAEs). In theory, one could train the student on as many unique examples as desired, since both the prior and the teacher model are known. In our experiments however, we use only 1.024 million synthetic examples for each dataset and iterate over these same ones multiple times.

\subsection{Model Architecture}
Unlike normalizing flows and autoregressive models, our method does not have any significant architectural restrictions. So, one could use an arbitrary neural network as the student, then randomly initialize weights.
But we observe that the entirety of the teacher's knowledge is contained in a much smaller neural network  that is reused many times. As such, we initialize the student to have \textit{the same architecture and weights} as . This allows the student to inherit knowledge from the teacher, which speeds up training. 

In the remaining sections, we use the term ``teacher'' to refer to the DDIM's sampling procedure from  to , and the term ``teacher network'' to refer to the neural network  that parameterizes this sampling procedure.

There are a couple minor concerns with this approach that we will address here. Firstly, the student is trained to model the data , so it may not be ideal to initialize it to a teacher network that models the noise . To resolve this, we have the student predict the noise as well, and subtract the predicted noise from the input noise  to produce samples and compute loss. Secondly, the teacher network (and therefore the student as well) is conditioned on time, so we condition the student on timestep \textit{T}, corresponding to the highest noise level.

\section{Experiments}
In this section, we examine the proposed method through multiple experiments. We consider four datasets: CIFAR-10  \citebracket{cifar}, CelebA  \citebracket{celeba}, LSUN \citebracket{lsun} Bedroom , and LSUN Church . We use the pretrained models from \cited{ddpm} as our teacher network, except for CelebA, where we use the one from \citeauthor{ddim} (\citeyear{ddim}a). For our experiments, we use a 100-step DDIM as our teacher for CIFAR-10 and CelebA, and a 50-step DDIM for LSUN. Further details can be found in \autoref{experimentaldetails}.

\subsection{Image Generation}
We show random samples from our CIFAR-10 and CelebA model in Figures \ref{fig1cifar_samples} and \ref{fig2celeba_samples} respectively. The samples are both high quality and diverse, demonstrating the efficacy of our method. Note that unlike VAEs, we do not reduce the temperature (standard deviation) of our prior when showing qualitative samples, which hinders diversity. 

For quantitative evaluation, we show FID \citebracket{heusel2018gans} and Inception scores \citebracket{salimans2016improved} in Table \ref{tab:table1} for various methods on CIFAR-10, as well as the number of steps (network evaluations) each needs. On CIFAR-10, our model achieves an FID of 9.36, which is lower than several GANs and far lower than NVAE \citebracket{nvae}, a state of the art VAE.\footnote{While \cited{vdvae} has slightly lower negative log-likelihood than NVAE (2.87 vs 2.91), we do not believe that this improvement would fully account for the disparity between our model and NVAE.} Our CIFAR-10 scores are also similar to state-of-the-art EBMs, even though our method uses only a single network evaluation. On CelebA, we obtain a competitive FID of 10.68. 

\begin{table}[t!]
  \parbox{.6\linewidth}{
  	\begin{small}
    \caption{Results for CIFAR-10. "Steps" refers to the number of neural network evaluations needed to generate a sample, and a model labeled with "cond." is class-conditional. Scores marked with an asterisk were computed by us and are not official.} 
    \label{tab:table1}
    \begin{tabular}{l c c c} 
      \\
      Model & FID  & IS  & Steps \\
      \hline \\
      \textbf{Single-Step} \\
      Denoising Student (\textbf{Ours}) & 9.36 & 8.36 & 1 \\
      NVAE \citebracket{nvae} & 51.67 & 5.51 & 1\\
      MoLM \citebracket{molm} & 18.9 & 7.90 & 1 \\
      \\ 
      \textbf{Single-Step, GAN} \\
      SNGAN \citebracket{sngan} & 21.7 & 8.22 & 1 \\
      BigGAN (cond.) \citebracket{biggan} & 14.73 & 9.22 & 1  \\
      PPOGAN \citebracket{ppogan} & 10.87 & 8.69 & 1 \\
      StyleGAN2+ADA \citebracket{stylegan2ada} & 2.92 & 9.83 & 1 \\
      StyleGAN2+ADA (cond.) \citebracket{stylegan2ada} & 2.42 & \textbf{10.14} & 1  \\
      \\
      \textbf{Many-Step} \\ 
      DDIM (100 step, \textbf{Teacher}) & 4.16 & 8.96* & 100 \\
      EBM \citebracket{du2019implicit} & 38.2 & 6.78 & 60 \\
      VAEBM \citebracket{vaebm} & 12.19 & 8.43 & 16  \\
      EBM+recovery likelihood \citebracket{gao2020learning} & 9.60 & 8.58 & 180  \\
      NCSNv2 \citebracket{ncsn2} & 10.87 & 8.40 & 1160  \\
      DDPM \citebracket{ddpm} & 3.17 & 9.46 & 1000  \\
      NCSN++ (8 blocks/res) \citebracket{sde} & \textbf{2.20} & 9.89 & 2000
    \end{tabular}
    \end{small}
  }
  \hfill
  \parbox{.3\linewidth}{
  	\begin{small}
    \caption{Time (s) to generate 50k CIFAR-10 images} 
    \label{tab:table2}
    \begin{tabular}{l|c} 
      \\
      Model & Time\\
      \hline \\
      Denoising Student & \textbf{51.5} \\
      NVAE  & 146.5 \\
      DDIM (Teacher)  &  \\
      DDPM & 
    \end{tabular}
    \end{small}
  }
\end{table}

To see how our method scales to higher resolutions, we evaluated it on the  LSUN Bedroom and Church datasets. We show random samples in Figure \ref{fig3lsun_bedroom_samples} (Bedroom) and \ref{fig4lsun_church_samples} (Church). We find that our model learns structure and color quite well, as well as larger details such as lamps and windows in Bedroom, and clouds and trees in Church. We do find, however, that our model produces rather blurry samples, without very defined textures.  We believe this bluriness arises from the choice of objective. While GANs are designed to produce images that fool the discriminator, our model has to replicate the teacher's output down to the pixel, which is difficult for higher dimensional data.

\vspace{-0.25cm}
\begin{minipage}{0.45\linewidth} 
\begin{figure}[H]
\includegraphics[height=4.5cm, width=6.0cm]{./figures/cifar_samples.png}
\caption{Uncurated samples from our CIFAR-10 model}
\label{fig1cifar_samples}
\end{figure}
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\linewidth} 
\begin{figure}[H]
\includegraphics[height=4.5cm, width=5.63cm]{./figures/celeba_samples.png}
\caption{Uncurated samples from our CelebA model}
\label{fig2celeba_samples}
\end{figure}
\end{minipage}

\vspace{-0.25cm}
\begin{minipage}{0.45\linewidth} 
\begin{figure}[H]
\includegraphics[height=5cm, width=7.5cm]{./figures/lsun_bedroom_samples.png}
\caption{Uncurated samples from our LSUN Bedroom model}
\label{fig3lsun_bedroom_samples}
\end{figure}
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.45\linewidth} 
\begin{figure}[H]
\includegraphics[height=5cm, width=7.5cm]{./figures/lsun_church_samples.png}
\caption{Uncurated samples from our LSUN Church-Outdoor model}
\label{fig4lsun_church_samples}
\end{figure}
\end{minipage}

To assess the sampling speed of Denoising Student, we measured the time it took to generate 50k CIFAR-10 images, and compared it in Table \ref{tab:table2}. Notably, it is about  faster than the teacher, and  faster than a DDPM, indicating that for the same architecture, sampling time is proportional to number of steps. When compared with NVAE, we find ours is slightly faster, showing that our model is indeed similar to other single-step models in speed. Timing experiments were done with a batch size of 250 on a single Nvidia V100 GPU. 

\subsection{Latent Space Manipulation}

\citeauthor{ddim} (\citeyear{ddim}a) observed that in DDIMs, the latent variable  is an effective representation of the data, where interpolations in the latent space result in meaningful image interpolations. Our model was capable of learning the latent mapping of a DDIM, so we hypothesize it would be able to produce meaningful interpolations as well. To test this hypothesis, we perform spherical interpolation between 2 random , and show the result in Figure \ref{fig5celeba_interpolation}. See Figure \ref{fig11interpolation_results} for more.

To gain some further insight into the learned latent representation of Denoising Student, we decided to apply it to \textit{different sized} images than it was trained on, by simply changing the size of the latent variable. Remarkably, it still manages to replicate the teacher quite well, producing coherent images of a different size (Figure \ref{fig12different_resolution} in Appendix). This would indicate that the student retains the teacher's advanced generalization capabilities.


\begin{figure}[H] 
\centering
\includegraphics[height=7cm, width=15.4cm]{./figures/celeba_interpolation.png}
\caption{Interpolation results on the CelebA dataset.}
\label{fig5celeba_interpolation}
\end{figure}


\section{Related Work}
Knowledge distillation (\citep{bucilua2006model}; \citep{hinton2015distilling}) has been explored extensively in supervised learning tasks such as image classification \citebracket{romero2015fitnets, zagoruyko2016paying, yim2017gift, furlanello2018born, tian2019contrastive, xu2020knowledge}, language modeling \citebracket{sun2019patient, turc2019wellread, sanh2020distilbert, jiao2019tinybert, sun2020contrastive}, and speech recognition \citebracket{chebotar2016distilling, Fukuda2017EfficientKD, watanabe2017student, kim2019knowledge, gao2020distilling}. Most existing work on knowledge distillation is concerned with finding the best way to transfer knowledge (e.g. best loss function) from the teacher to the student. Additionally, knowledge distillation is usually concerned with reducing the size (parameters) of a network, instead of the number of evaluations.

Our work is also based on denoising score-based models such as NCSNs (\citep{song2019generative}) and DDPMs (\citep{ddpm}), which estimate the score of the data density and use a denoising auto-encoder objective. \citeauthor{sde} (\citeyear{sde}b) observed that an NCSN / DDPM's generative process could be seen as solving a reverse time stochastic differential equation that reverses a noise-adding SDE. A DDIM's sampling procedure, on the other hand, more closely resembles integrating an ODE (see. Eq. \ref{eq7}). From this perspective, our student model could loosely be seen as approximating this integral with a function, instead of a more accurate but expensive numerical method (the teacher).

\section{Conclusion}
In this work, we presented a simple method to vastly reduce the sampling time of certain iterative generative models, with only a minor degradation in performance. Our knowledge distillation technique can be easily applied to any trained model with a deterministic generative process, since it has a stable training objective and requires no extra architectural considerations. Despite its simplicity and stability, our model is capable of producing significantly better samples than other non-adversarial, single-step methods such as VAEs. It also learns meaningful latent representations, allowing for easy data manipulation through the latent space. Future work includes bridging the gap between the teacher and student through more advanced distillation techniques, and producing sharper images at high resolutions.

\pagebreak

\printbibliography

\appendix

\section{Samples}
We show additional samples in Figure \ref{fig6cifar_uncurated} (CIFAR-10), \ref{fig7celeba_uncurated} (CelebA), \ref{fig8lsun_bedroom_uncurated} (Bedroom), and  \ref{fig9lsun_church_uncurated} (Church). Additional interpolation results can be seen in \ref{fig11interpolation_results}, and different size images can be seen in Figure \ref{fig12different_resolution}.

Since our model was not trained on real data, but instead on the teacher's output, we do not expect it to memorize training examples. To demonstrate this, we include nearest neighbor visualizations in Figure \ref{fig10inceptionspace_nearestneighbors}. 

\begin{figure}[H] 
\centering
\includegraphics[height=21cm, width=14cm]{./figures/cifar10_uncurated.png}
\caption{Uncurated Samples from our CIFAR-10 model}
\label{fig6cifar_uncurated}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[height=20cm, width=12cm]{./figures/celeba_uncurated.png}
\caption{Uncurated Samples from our CelebA model}
\label{fig7celeba_uncurated}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[height=20.8cm, width=13cm]{./figures/lsun_bedroom_uncurated.png}
\caption{Uncurated Samples from our LSUN Bedroom model}
\label{fig8lsun_bedroom_uncurated}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[height=20.8cm, width=13cm]{./figures/lsun_church_uncurated.png}
\caption{Uncurated Samples from our LSUN Church-Outdoor model.}
\label{fig9lsun_church_uncurated}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[height=8.4cm, width=16cm]{./figures/inceptionspace_nearestneighbors.png}
\caption{Inception feature space nearest neighbors for CIFAR-10. Images generated from our model are in the leftmost column, and to the right of the black line are the nearest neighbors in the training set.}
\label{fig10inceptionspace_nearestneighbors}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[height=9cm, width=16.5cm]{./figures/interpolation_results.png}
\caption{Extended Interpolation results. Rows 1 and 2: CelebA images, Rows 3 and 4: LSUN Bedroom, Rows 5 and 6: LSUN church}
\label{fig11interpolation_results}
\end{figure}

\begin{figure}[H] 
\centering
\includegraphics[height=10.3cm, width=16.0cm]{./figures/different_resolution.png}
\caption{Uncurated  images from a 50 step DDIM (teacher, left) and our model (right). Neither model has ever seen a  image in training, yet both produce decent samples, demonstrating that both the teacher and student generalize well.}
\label{fig12different_resolution}
\end{figure}

\section{Experimental Details} \label{experimentaldetails}
A list of the hyperparameters we used can be seen in Table \ref{tab:hparams}. We used the Adam (\citep{adam}) optimizer. We linearly increased the learning rate for a number of warmup steps, and used no decay. We did not use dropout or any other regularization, and used test loss evaluated on unseen images for early stopping. We made some interesting observations when training, and list them as follows:
\begin{itemize} 
\item{For CIFAR-10, we found that training on L1 distance yielded better results than L2.}
\item{For LSUN Bedroom, we initially clipped the values of the gradients to , but observed that the gradients were very large (norm consistently ). We found that \textit{not clipping} by value improved stability.} 
\item{For LSUN Bedroom, we initially chose , but found that  \textit{increasing}  to 0.98 and decreasing the learning rate to  helped stabilize  training. We applied these to Church without sweeping.}
\end{itemize}

\begin{table}[h!]
  \begin{center}
    \caption{Hyperparameters used in our experiments.} 
    \label{tab:hparams}
    \begin{tabular}{l c c c c} 
      \\ 
      Model & CIFAR-10 & CelebA & LSUN Bedroom & LSUN Church\\
      \hline \\
      Parameters (M) & 35.7 & 78.7 & 114 & 114 \\
      Batch size & 512 & 512 & 32 & 32 \\
      Learning Rate &  &  &  &  \\
      Adam  & 0.9 & 0.9 & 0.98 & 0.98 \\
      Adam  & 0.98 & 0.98 & 0.999 & 0.999 \\
      EMA decay rate & 0.995 & 0.995 & 0.9995 & 0.9995 \\
      Warmup steps & 5000 & 5000 & 1000 & 1000 \\
      Max training iterations & 50k & 64k & 320k & 256k \\
      
    \end{tabular}
  \end{center}
\end{table}

\end{document}
