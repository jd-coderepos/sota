

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color,cite}
\usepackage{multirow,bm,subfig}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}


\begin{document}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\title{PyramidBox: A Context-assisted Single Shot Face Detector.} 


\titlerunning{PyramidBox: A Context-assisted Single Shot Face Detector.}



\author{Xu Tang \and Daniel K. Du \and Zeqiang He \and
Jingtuo Liu}


\authorrunning{Xu Tang, Daniel K. Du, Zeqiang He and Jingtuo Liu}



\institute{Baidu Inc. \\
\email{tangxu02@baidu.com,daniel.kang.du@gmail.com,\{hezeqiang,liujingtuo\}@baidu.com}}
\maketitle              

\blfootnote{ Equal contribution.}
\blfootnote{ Corresponding author.}




\begin{abstract}
Face detection has been well studied for many years and one of remaining
challenges is to detect small, blurred and partially occluded faces in uncontrolled
environment. This paper proposes a novel context-assisted single shot face detector,
named \emph{PyramidBox} to handle the hard face detection problem.
Observing the importance of the context, we improve the utilization of contextual information in the following three aspects.
First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors.
Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot.
Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output.
In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces.
By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: \href{https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection}{\url{https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection}}.
\keywords{face detection, context, single shot, PyramidBox}
\end{abstract}


\section{Introduction}
\label{sec:intro}
Face detection is a fundamental and essential task in various face applications.
The breakthrough work by Viola-Jones~\cite{Viola2004} utilizes AdaBoost algorithm
with Haar-Like features to train a cascade of face vs. non-face classifiers.
Since that, numerous of subsequent
works~\cite{Brubaker2008,Pham2007,Liao2016,Lowe2004,Yang2014,Zhu2006}
are proposed for improving the cascade detectors.
Then, \cite{Mathias2014,Yan2014,Zhu2012} introduce deformable
part models (DPM) into face detection tasks by modeling the relationship of
deformable facial parts. These methods are mainly based on designed features which are less representable and trained by separated steps.

With the great breakthrough of convolutional neural networks(CNN),
a lot of progress for face detection
has been made in recent years due to utilizing modern CNN-based object detectors,
including R-CNN~\cite{Girshick2014,Girshick2016,Girshick2015,Ren2015},
SSD~\cite{Liu2016}, YOLO~\cite{Redmon2016}, FocalLoss~\cite{Lin2017b} and their extensions~\cite{zhang2017single}.
Benefiting from the powerful deep learning approach and end-to-end optimization,
the CNN-based face detectors have achieved much better performance and provided a new baseline for later methods.


Recent anchor-based detection frameworks aim at detecting hard faces in uncontrolled environment such as WIDER FACE~\cite{Barbu14}.
SSH~\cite{Najibi2017} and SFD~\cite{Zhang2017} develop scale-invariant networks to
detect faces with different scales from different layers in a single network.
Face R-FCN~\cite{Wang2017} re-weights embedding responses on score maps and eliminates
the effect of non-uniformed contribution in each facial part using a position-sensitive
average pooling. FAN~\cite{Wang2017b} proposes an anchor-level attention by highlighting the features from
the face region to detect the occluded faces.

Though these works give an effective way to design anchors and related networks to detect faces with different scales,
how to use the contextual information in face detection has not been paid enough attention, which should play a significant role in detection of hard faces.
Actually, as shown in Fig.~\ref{fig:context_of_face}, it is clear that faces never occur isolated in the real world, usually with shoulders or bodies, providing a rich
source of contextual associations to be exploited especially when the facial texture is not distinguishable for the sake of low-resolution, blur and occlusion.
We address this issue by introducing a novel framework of context assisted network to make full use of contextual signals as the following steps.

\begin{figure}[h]
\centering
\includegraphics[height=1.8cm]{face_head_body.pdf}
\caption{Hard faces are difficult to be located and classified due to the lack of visual consistency, while the larger regions which give hints to the position of face are easier to be located and classified, such as head and body.}
\label{fig:context_of_face}
\end{figure}

Firstly, the network should be able to learn features for not only faces, but also contextual parts such as heads and bodies. To achieve this goal, extra labels
are needed and the anchors matched to these parts should be designed. In this work, we use a semi-supervised solution to generate approximate labels for contextual
parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor-based architectures.

Secondly, high-level contextual features should be adequately combined with the low-level ones. The appearances of hard and easy faces can be quite different, which implies that
not all high-level semantic features are really helpful to smaller targets. We investigate the performance of
Feature Pyramid Networks (FPN) ~\cite{Lin2017} and modify it into a \emph{Low-level Feature Pyramid
Network (LFPN)} to join mutually helpful features together.

Thirdly, the predict branch network should make full use of the joint feature. We introduce the \emph{Context-sensitive prediction module (CPM)} to incorporate
context information around the target face with a wider and deeper network. Meanwhile, we propose a max-in-out layer for the prediction module to further improve the
capability of classification network.

In addition, we propose a training strategy named as \emph{Data-anchor-sampling} to make an adjustment on
the distribution of the training dataset. In order to learn more representable features, the diversity of hard-set samples is important and can be gained by data augmentation across samples.


For clarity, the main contributions of this work can be summarized as five-fold:
\begin{enumerate}
\item We propose an anchor-based context assisted method, called PyramidAnchors,
to introduce supervised information on learning contextual features for small, blurred and partially occluded faces.
\item We design the Low-level Feature Pyramid Networks (LFPN) to merge contextual features and facial features better. Meanwhile, the proposed method can handle faces with different scales well in a single shot.
\item We introduce a context-sensitive prediction module, consisting of a mixed network structure and max-in-out layer to learn accurate location and classification from the merged features.
\item We propose the scale aware Data-anchor-sampling strategy to change the distribution of training samples to put emphasis on smaller faces.
\item We achieve superior performance over state-of-the-art on the common face detection benchmarks
FDDB and WIDER FACE.
\end{enumerate}

The rest of the paper is organized as follows. Section~\ref{sec:relate} provides an
overview of the related works. Section~\ref{sec:pyramidbox} introduces the proposed method.
Section~\ref{sec:exper} presents the experiments and Section~\ref{sec:conclu} concludes the paper.


\section{Related Work}
\label{sec:relate}


\textbf{Anchor-based Face Detectors.}
Anchor was first proposed by Faster R-CNN \cite{Ren2015}, and then it was widely used in both two-stage and
one single shot object detectors.
Then anchor-based object detectors~\cite{Liu2016,Redmon2016} have achieved remarkable progress in recent years.
Similar to FPN~\cite{Lin2017}, Lin~\cite{Lin2017b} uses translation-invariant anchor boxes,
and Zhang~\cite{Zhang2017} designs scales of anchors to ensure that the detector can handle various scales of faces well.
FaceBoxes~\cite{Zhang2017b} introduces anchor densification to ensure different types of anchors
have the same density on the image. SFD~\cite{Zhang2017} proposed anchor matching strategy to improve the
recall rate of tiny faces.


\textbf{Scale-invariant Face Detectors.}
To improve the performance of face detector to handle faces of different scales,
many state-of-the-art works\cite{Yang2017,Zhang2017,Wang2017b,Najibi2017}
construct different structures in the same framework to detect faces with variant size, where
the high-level features are designed to detect large faces while low-level features for small faces.
In order to integrate high-level semantic feature into low-level layers with higher resolution,
FPN~\cite{Lin2017} proposed a top-down architecture to use high-level semantic feature maps
at all scales. Recently, FPN-style framework achieves great performance on both objection detection~\cite{Lin2017b}
and face detection~\cite{Wang2017b}.



\textbf{Context-associated Face Detectors.} Recently, some works
show the importance of contextual information for face detection,
especially for finding small, blurred and occluded faces.
CMS-RCNN~\cite{Zhu2016} used Faster R-CNN in face detection with body contextual information.
Hu et al.~\cite{Hu2017} trained separate detectors for different scales.
SSH~\cite{Najibi2017} modeled the context information by large filters on each prediction module.
FAN~\cite{Wang2017b} proposed an anchor-level attention, by highlighting the features from
the face region, to detect the occluded faces.



\section{PyramidBox}
\label{sec:pyramidbox}
\begin{figure}[t]
\centering
\includegraphics[height=9.0cm]{architecture.pdf}
\caption{Architecture of PyramidBox. It consists of \textbf{Scale-equitable Backbone Layers},
\textbf{Low-level Feature Pyramid Layers (LFPN)},
\textbf{Context-sensitive Predict  Layers}
and \textbf{PyramidBox Loss Layer}.}
\label{fig:architecture}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[]{\includegraphics[height=3.5cm]{lfpn.pdf}}\hfill
\subfloat[]{\includegraphics[height=3.5cm]{cpm.pdf}}\hfill
\subfloat[]{\includegraphics[height=3.5cm]{pll.pdf}}\hfill
\caption{(a) Feature Pyramid Net. (b) Context-sensitive Prediction Module. (c) PyramidBox Loss.}
\label{fig:architecture_detail}
\end{figure}


This section introduces the context-assisted single shot face detector, \emph{PyramidBox}.
We first briefly introduce
the network architecture in Sec.~\ref{sec:LFPN}.
Then we present a context-sensitive prediction module in Sec.~\ref{sec:cpm},
and propose a novel anchor method, named \emph{PyramidAnchors}, in Sec.~\ref{sec:pyramidanchors}.
Finally, Sec.~\ref{sec:training} presents the associated training methodology
including data-anchor-sampling and max-in-out.

\subsection{Network Architecture}
\label{sec:LFPN}

Anchor-based object detection frameworks with sophisticated design of anchors have been proved effective to handle faces of variable scales when predictions are made at different
levels of feature map ~\cite{Ren2015,Liu2016,Najibi2017,Zhang2017,Wang2017b}.
Meanwhile, FPN structures showed strength on merging high-level features with the lower ones.
The architecture of PyramidBox(Fig.~\ref{fig:architecture}) uses the same extended VGG16
backbone and anchor scale design as SFD~\cite{Zhang2017}, which can generate feature maps
at different levels and anchors with equal-proportion interval. Low-level FPN is added on this
backbone and a Context-sensitive Predict Module is used as a branch network from each pyramid
detection layer to get the final output. The key is that we design a novel pyramid anchor method
which generates a series of anchors for each face at different levels.
The details of each component in the architecture are as follows:

\textbf{Scale-equitable Backbone Layers.} We use the base convolution layers and extra convolutional layers
in SFD~\cite{Zhang2017} as our backbone layers, which keep layers of VGG from \emph{conv\,}
to \emph{pool\,}, then convert \emph{fc\,} and
\emph{fc\,} of VGG to \emph{convfc} layers,
and then add more convolutional layers to make it deeper.

\textbf{Low-level Feature Pyramid Layers.}
To improve the performance of face detector to handle faces of different scales, the low-level feature with high-resolution plays a key role.
Hence, many state-of-the-art works\cite{Yang2017,Zhang2017,Wang2017b,Najibi2017}
construct different structures in the same framework to detect faces with variant size, where
the high-level features are designed to detect large faces while low-level features for small faces.
In order to integrate high-level semantic feature into low-level layers with higher resolution,
FPN~\cite{Lin2017} proposed a top-down architecture to use high-level semantic feature maps
at all scales. Recently, FPN-style framework achieves great performance on both objection detection~\cite{Lin2017b}
and face detection~\cite{Wang2017b}.

As we know, all of these works build FPN start from the top layer, which should be argued that not all high-level features are undoubtedly helpful to small faces.
First, faces that are small, blurred and occluded have different texture feature from the large, clear and complete ones. So it is rude to directly use all high-level features to enhance the performance on small faces.
Second, high-level features are extracted from regions with little face texture and may introduce noise information.
For example, in the backbone layers of our PyramidBox, the receptive field~\cite{Zhang2017} of the top two
layers \emph{conv\,} and \emph{conv\,} are  and , respectively. Notice that the input size
of training image is , which means that the top two layers contain too much noisy context features, so they may not contribute to detecting medium and small faces.

Alternatively, we build the \emph{Low-level Feature Pyramid Network (LFPN)} starting a top-down structure from a middle layer,
whose receptive field should be close to the half of the input size, instead of the top layer.
Also, the structure of each block of LFPN, as same as FPN~\cite{Lin2017}, one can see
Fig.~\ref{fig:architecture_detail}(a) for details.


\textbf{Pyramid Detection Layers.} We select \emph{lfpn\,2}, \emph{lfpn\,1}, \emph{lfpn\,0},
\emph{conv\,fc\,}, \emph{conv\,} and \emph{conv\,} as detection layers with anchor size of
, , , ,  and , respectively.
Here \emph{lfpn\,2}, \emph{lfpn\,1} and \emph{lfpn\,0} are output layer of LFPN based on \emph{conv\,}, \emph{conv\,} and \emph{conv\,}, respectively.
Moreover, similar to other SSD-style methods, we use L normalization~\cite{Liu2016b} to rescale the norm of
LFPN layers.

\textbf{Predict Layers.} Each detection layer is followed by a \emph{Context-sensitive Predict Module (CPM)},
see Sec~\ref{sec:cpm}.
Notice that the outputs of CPM are used for supervising pyramid anchors, see Sec.~\ref{sec:pyramidanchors}, which
approximately cover face, head and body region in our experiments.
The output size of the -th CPM is ,
where  is the corresponding
feature size and the channel size  equals to  for  . Here the features of
each channels are used for classification and regression of faces, heads and bodies, respectively,
in which the classification of face need   channels, where  and 
are max-in-out of foreground and background label respectively, satisfying
7pt]
   3, &\mbox{ otherwise.}
 \end{array}\right.

\label{eq:pyramid_anchor_defi}
label_k(anchor_{i,j}) =
      \left\{ \begin{array}{ll}
              1, & \mbox{if }iou(anchor_{i,j}\cdot s_i/{s_{pa}}^k, region_{target}) > threshold,\
for , respectively,
where  is the stride of pyramid anchors.
 denotes the corresponding region in the original image
of , and  represents the corresponding
down-sampled region by stride .
The  is the same as other anchor-based detectors.
Besides, a PyramidBox Loss will be demonstrated in Sec.~\ref{sec:training}.

In our experiments,
we set the hyper parameter  since the stride of adjacent prediction modules is .
Furthermore, let  and .
Then ,  and  are labels of face, head and body respectively.
One can see that a face would generate  targets in three continuous prediction modules, which represent
for the face itself, the head and body corresponding to the face.
Fig.~\ref{fig:pyramidAnchors} shows an example.

Benefited from the PyramidBox, our face detector can handle small, blurred and partially occluded faces
better. Notice that the pyramid anchors are generated automatically without
any extra label and this semi-supervised learning help PyramidAnchors extract approximate contextual features.
In prediction process, we only use output of the face branch, so no additional computational
cost is incurred at runtime, compared to standard anchor-based face detectors.



\subsection{Training}
\label{sec:training}
In this section, we introduce the training dataset, data augmentation,
loss function and other implementation details.

\textbf{Train dataset.} We trained PyramidBox on  images of the WIDER FACE training set
with color distort,
random crop and horizontal flip.

\begin{figure}[t]
\centering
\subfloat[Pose.]{
\includegraphics[height=2cm]{1_pose.pdf}}\hfill
\subfloat[Occlusion.]{
\includegraphics[height=2cm]{2_occ.pdf}}\hfill
\subfloat[Blur.]{
\includegraphics[height=2cm]{3_illu.pdf}}\hfill
\subfloat[Illumination.]{
\includegraphics[height=2cm]{4_blur.pdf}}\hfill
\caption{Data-anchor-sampling changes the distribution of the train data.
Dotted lines show the
distribution of certain attribute, while solid lines represent the corresponding distribution of
those attribute after the data-anchor-sampling.}
\label{fig:dataanchorsampling}
\end{figure}
\textbf{Data-anchor-sampling.}
Data sampling~\cite{Thompson2012}  is a classical subject in statistics, machine learning and pattern recognition,
it achieves great development in recent years. For the task of objection detection,
Focus Loss~\cite{Lin2017b} address the class imbalance by reshaping the standard cross entropy loss.

Here we utilize a  data augment sample method named Data-anchor-sampling.
In short, data-anchor-sampling resizes train images by reshaping a random face in this image to a random smaller anchor size.
More specifically, we first randomly select a face of size  in a sample.
As previously mentioned that the scales of anchors in our PyramidBox,
as shown in Sec.~\ref{sec:LFPN},  are

let

be the index of the nearest anchor scale from the selected face, then we choose a random index  in the set

finally, we resize the face of size of  to the size of

Thus, we got the image resize scale

By resizing the original image with the scale 
and cropping a standard size of  containing the selected face randomly, we get
the anchor-sampled train data.
For example, we first select a face randomly, suppose its size is ,
then its nearest anchor-size is , then we need to choose a target size from
 and .
In general, assume that we select , then we resize the original
image by scale of .
Finally, by cropping a  sub-image from the last resized image containing
the originally selected face, we get the sampled train data.

As shown in Fig.~\ref{fig:dataanchorsampling},
data-anchor-sampling changes the distribution of the
train data as follows:
1) the proportion of small faces is larger than the large ones.
2) generate smaller face samples through larger ones to increase the diversity of face samples of smaller scales.




\textbf{PyramidBox loss.}
As a generalization of the multi-box loss in~\cite{Girshick2015},  we employ the \emph{PyramidBox Loss}
function for an image is defined as

where the -th pyramid-anchor loss is given by

Here  is the index of pyramid-anchors  (, and  represents for
face, head and body, respectively, in our experiments), and  is the index of an anchor and  is the predicted
probability of anchor  being the -th object (face, head or body). The ground-truth label defined by
7pt]
            0, & otherwise.
            \end{array}\right.

\begin{array}{ll}
t_{k,i}^* = (&t_x^* + \frac{1-s_{p_a}^k}{2}t_w^*s_{w,k}+\Delta_{x,k}, t_y^* +\frac{1-s_{p_a}^k}{2}t_h^*s_{h,k}+\Delta_{y,k}, \
where  and  denote offset of shifts, 
 and  are scale factors respect to width and height 
respectively. 
In our experiments, we set  for  
and  for .
The classification loss  is log loss over two classes ( face \emph{vs.} not face) and
the regression loss  is the smooth  loss defined in~\cite{Girshick2015}.
The term  means the regression loss is activated only for positive anchors and disabled
otherwise. The two terms are normalized with , , and balancing weights  and
 for .


\textbf{Optimization.} As for the parameter initialization, our PyramidBox use the pre-trained parameters
from VGG~\cite{Russakovsky2015}.
The parameters of \emph{convfc\,} and \emph{convfc\,} are initialized
by sub-sampling parameters from \emph{fc\,} and \emph{fc\,}
of VGG and the other additional layers are randomly initialized with ``xavier" in~\cite{Glorot2010}.
We use a learning rate of  for k iterations, and  for the next k iterations,
and  for the last k iterations on the WIDER FACE training set with batch size 16.
We also use a momentum of  and a weight decay of ~\cite{Krizhevsky2012}.



\section{Experiments}
\label{sec:exper}
In this section, we firstly analyze the effectiveness of our PyramidBox through a set of experiments,
and then evaluate the final model on WIDER FACE and FDDB
face detection benchmarks.

\subsection{Model Analysis}
We analyze our model on the WIDER FACE validation set by contrast experiments.


\textbf{Baseline.} Our PyramidBox shares the same architecture of SFD, so we directly use it as a baseline.

\textbf{Contrast Study.}
To better understand PyramidBox, we conduct contrast experiments to evaluate the contributions
of each proposed component, from which we can get the following conclusions.



\textbf{Low-level feature pyramid network (LFPN) is crucial for detecting hard faces.}
The results listed in Table~\ref{table:lfpn} prove that LFPN started from a middle layer,
using \emph{conv\,fc} in our PyramidBox,
is more powerful, which implies that features with large gap in scale may not help each other.
The comparison between the first and forth column of Table~\ref{table:lfpn} indicates that
LFPN increases the mAP by  on hard subset.
This significant improvement demonstrates the effectiveness of joining high-level semantic features with the low-level ones.

\begin{table}[h]
\begin{center}
\caption{Performances of LFPN starting from different layers.}
\label{table:lfpn}
\begin{tabular}{ll||c|c|c|c|c|c}
\hline
\multicolumn{2}{c||}{Start layer} & Baseline &\emph{conv\,}(\textbf{FPN}) & \emph{conv\,} & \emph{conv\,fc}(\textbf{LFPN}) & \emph{conv\,} & \emph{conv\,}\\
\hline
\hline
\multicolumn{2}{c||}{RF/InputSize} & &  &  &  &  & \\
\hline
\multirow{3}*{mAP} & easy   &  &  &  &  &  & \\
                 ~ & medium &  &  &  &  &  & \\
                 ~ & hard   &  &  &  &  &  &  \\
\hline
\end{tabular}
\end{center}
\end{table}

\textbf{Data-anchor-sampling makes detector easier to train.} We employ Data-anchor-sampling based on LFPN network and the result shows that our data-anchor-sampling
effectively improves the performance. The mAP is increased by ,  and  on easy, medium
and hard subset, respectively. One can see that Data-anchor-sampling
works well not only for small hard faces, but also for easy and medium faces.


\textbf{PyramidAnchor and PyramidBox loss is promising.}
\begin{table}[h]
\begin{center}
\caption{The Parameters of PyramidAnchors.}
\label{table:pyramidanchors_param}
\begin{tabular}{ll||c|c|c|c|c}
\hline
\multicolumn{2}{c||}{\textbf{Method}} & Baseline &  &  &  & \\
\multicolumn{2}{c||}{} & &  &  &  & \\
\hline
\hline
\multirow{3}*{mAP} & easy   &  &  &  &  & \\
                 ~ & medium &  &  &  &  & \\
                 ~ & hard   &  &  &  &  & \\
\hline
\end{tabular}
\end{center}
\end{table}
By comparing the first and last column in Table~\ref{table:pyramidanchors_param},
one can see that PyamidAnchor effectively improves the
performance, i.e., ,  and  on easy, medium and hard, respectively.
This dramatical improvement shows that learning contextual information is helpful to the task of detection,
especially for hard faces.



\textbf{Wider and deeper context prediction module is better.}
Table~\ref{table:cpm_res} shows that the performance of CPM is better than both DSSD module and SSH context module. Notice that the combination of SSH and DSSD gains very little compared to SSH alone, which indicates that large receptive field is more important to predict the accurate location and classification.
In addition, by comparing the last two column of Table~\ref{table:ablative}, one can find that
the method of Max-in-out improves the mAP on WIDER FACE validation set
about (Easy), (Medium) and (Hard), respectively.
\begin{table}[h]
\begin{center}
\caption{Context-sensitive Predict Module.}
\label{table:cpm_res}
\begin{tabular}{ll||c|c|c}
\hline
\multicolumn{2}{c||}{\textbf{Method}} & DSSD prediction module & SSH context module & \textbf{CPM} \\
\hline
\hline
\multirow{3}*{mAP} & easy   &  &  &  \\
                 ~ & medium &  &  &  \\
                 ~ & hard   &  &  &  \\
\hline
\end{tabular}
\end{center}
\end{table}


To conclude this section, we summarize our results in Table~\ref{table:ablative},
from which one can see that mAP increase ,  and  on easy, medium and \textbf{hard} subset, respectively.
This sharp increase demonstrates the effectiveness of proposed PyramidBox, especially for hard faces.
\begin{table}[h]
\begin{center}
\caption{Contrast results of the PyramidBox on WIDER FACE validation subset.}
\label{table:ablative}
\begin{tabular}{rc||c|cccc|c}
\hline
\multicolumn{2}{c||}{Contribution}                         & Baseline &   &  & & &\textbf{PyramidBox}\\
\hline
\hline
\multicolumn{2}{r||}{lfpn?}                    &  & ~  & ~ & ~ & ~ & ~\\
\hline
\multicolumn{2}{r||}{data-anchor-sampling?}    &  &  & ~ & ~ & ~ & ~\\
\hline
\multicolumn{2}{r||}{pyramid-anchors?}         &  &   &   &  ~ & ~ & ~\\
\hline
\multicolumn{2}{r||}{context-prediect-module?} &  &   &   & & ~ & ~\\
\hline
\multicolumn{2}{r||}{max-in-out?}              &  &   &   & &  & ~\\
\hline
    & easy   &  & ~ & ~ & ~ & ~ & ~ \\
mAP & medium &  & ~ & ~ & ~ &~ & ~\\
    & hard   &  & ~ & ~ & ~ &~ & ~\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Evaluation on Benchmark}
We evaluate our PyramidBox on the most popular face detection benchmarks, including
Face Detection Data Set and Benchmark (FDDB)~\cite{Jain2010} and
WIDER FACE~\cite{Yang2015b}.

\textbf{FDDB Dataset.}
\begin{figure}[t]
\centering
\subfloat[Discontinous ROC curves] {\includegraphics[height=3.8cm]{fddb_disc.pdf}}\hfill
\subfloat[Continous ROC curves] {\includegraphics[height=3.8cm]{fddb_cont.pdf}}\hfill
\caption{Evaluation on the FDDB dataset.}
\label{fig:fddbres}
\end{figure}
It has  faces in  images collected from the Yahoo! news website.
We evaluate our face detector on FDDB against the
other state-of-art methods~\cite{Zhang2017,Liao2016,Zhang2016,Zhang2017b,Yu2016,Barbu14,Triantafyllidou2016,Yang2015,Li2016,Farfade2015,Ghiasi2015,Kumar2015,Li2013,Li2013b,Li2014,Ohnbar2016,Ranjan2015,Ranjan2016,Cai2016,Wan2016}. The PyramidBox achieves state-of-art performance and the result is shown in Fig.~\ref{fig:fddbres}(a) and~Fig.~\ref{fig:fddbres}(b).

\textbf{WIDER FACE Dataset.}
\begin{figure}[t]
\centering
\subfloat[Val: Easy] {\includegraphics[height=3.0cm]{val_wider_pr_cruve_int_easy.pdf}}\hfill
\subfloat[Val: Medium] {\includegraphics[height=3.0cm]{val_wider_pr_cruve_int_medium.pdf}}\hfill
\subfloat[Val: Hard] {\includegraphics[height=3.0cm]{val_wider_pr_cruve_int_hard.pdf}}\hfill
\subfloat[Test: Easy] {\includegraphics[height=3.0cm]{test_wider_pr_cruve_int_easy.pdf}}\hfill
\subfloat[Test: Medium] {\includegraphics[height=3.0cm]{test_wider_pr_cruve_int_medium.pdf}}\hfill
\subfloat[Test: Hard] {\includegraphics[height=3.0cm]{test_wider_pr_cruve_int_hard.pdf}}\hfill
\caption{Precision-recall curves on WIDER FACE validation and test sets.}
\label{fig:widerface}
\end{figure}
It contains  images and  annotated faces with a high degree of variability in scale, pose and occlusion.
The database is split into training (), validation () and testing () set,
where both validation and test set are divided into ``easy", ``medium" and``hard" subsets,
regarding the difficulties of the detection.
Our PyramidBox is trained only on the training set and evaluated on both validation set
and testing set comparing with the state-of-the-art face detectors,
such as~\cite{Zhang2017,Yang2014,Zhang2016,Zhang2017b,Yang2015,Zhu2016,Yang2015b,Ohnbar2016,Zhang2018,Wang2017b,Cai2016,Wang2017,Najibi2017,Yang2017,Hu2017,wang2017c}.
Fig.~\ref{fig:widerface} presents the precision-recall curves and mAP values.
Our PyramidBox outperforms others across all three subsets,
i.e.  (easy),  (medium),  (hard)
for validation set, and  (easy),  (medium),  (hard) for testing set.



\section{Conclusion}
\label{sec:conclu}
This paper proposed a novel context-assisted single shot face detector,
denoted as PyramidBox, to handle the unconstrained face detection problem.
We designed a novel context anchor, named PyramidAnchor, to supervise face detector to
learn features from contextual parts around faces. Besides, we modified feature pyramid
network into a low-level feature pyramid network to combine features from high-level and
high-resolution, which are effective for finding small faces. We also proposed a wider
and deeper prediction module to make full use of joint feature. In addition, we introduced Data-anchor-sampling
to augment the train data to increase the diversity of train data for small faces.
The experiments demonstrate
that our contributions lead PyramidBox to the state-of-the-art performance on the common
face detection benchmarks, especially for hard faces.

\vspace{.2cm} \noindent{\bf Acknowledgments.}
 We wish to thank Dr. Shifeng Zhang and Dr. Yuguang Liu for many helpful discussions.
\clearpage






\begin{thebibliography}{10}

\bibitem{Viola2004}
Viola, P., Jones, M.J.:
\newblock Robust real-time face detection.
\newblock Internatinoal Journal of Computer Vision \textbf{57}(2) (2004)
  137--154

\bibitem{Brubaker2008}
Brubaker, S.C., Wu, J., Sun, J., Mullin, M.D., Rehg, J.M.:
\newblock On the design of cascades of boosted ensembles for face detection.
\newblock Internatinoal Journal of Computer Vision \textbf{77}(1-3) (2008)

\bibitem{Pham2007}
Pham, M.T., Cham, T.J.:
\newblock Fast training and selection of haar features using statistics in
  boosting-based face detection.
\newblock In: ICCV (2007)

\bibitem{Liao2016}
Liao, S., Jain, A.K., Li., S.Z.:
\newblock A fast and accurate unconstrained face detector.
\newblock IEEE Trans. Parttern Anal. Mach. Intell. \textbf{38} (2016)

\bibitem{Lowe2004}
Lowe, D.G.:
\newblock Distinctive image features from scale-invariant keypoints.
\newblock Internatinoal Journal of Computer Vision \textbf{60}(2) (2004)

\bibitem{Yang2014}
Yang, B., Yan, J., Lei, Z., Li, S.Z.:
\newblock Aggregate channel features for multi-view face detection.
\newblock In: IJCB (2014)  1--8

\bibitem{Zhu2006}
Zhu, Q., Yeh, M.C., Cheng, K.T., Avidan, S.:
\newblock Fast human detection using a cascade of histograms of oriented
  gradients.
\newblock In: CVPR \textbf{2} (2006)

\bibitem{Mathias2014}
Mathias, M., Benenson, R., Pedersoli, M., Gool, L.V.:
\newblock Face detection without bells and whistles.
\newblock In: ECCV (2014)

\bibitem{Yan2014}
Yan, J., Lei, Z., Wen, L., Li, S.Z.:
\newblock The fastest deformable part model for object detection.
\newblock In: CVPR (2014)

\bibitem{Zhu2012}
Zhu, X., Ramanan, D.:
\newblock Face detection, pose estimation, and landmark localization in the
  wild.
\newblock In: CVPR (2012)

\bibitem{Girshick2014}
Girshick, R., Donahue, J., Darrell, T., Malik, J.:
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In: CVPR (2014)

\bibitem{Girshick2016}
Girshick, R., Donahue, J., Darrell, T., Malik, J.:
\newblock Region-based convolutional networks for accurate object detection and
  segmentation.
\newblock TIEEE Trans. Parttern Anal. Mach. Intell. \textbf{38}(3) (2016)

\bibitem{Girshick2015}
R.Girshick:
\newblock Fastr-cnn.
\newblock In: ICCV (2015)

\bibitem{Ren2015}
Ren, S., Girshick, K.H.R., Sun, J.:
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In: NIPS (2015)

\bibitem{Liu2016}
Liu, W., Anguelov, D., D.Erhan, Christian, S., Reed, S., Fu, C.Y., Berg, A.C.:
\newblock Ssd: single shot multibox detector.
\newblock In: ECCV (2016)

\bibitem{Redmon2016}
Redmon, J., Divvala, S., Girshick, R., Farhadi, A.:
\newblock You only look once: Unified, real-time object detection.
\newblock In: CVPR (2016)

\bibitem{Lin2017b}
Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\'ar, P.:
\newblock Focal loss for dense object detection.
\newblock In: ICCV (2017)

\bibitem{Barbu14}
Barbu, A., Gramajo, G.:
\newblock Face detection with a d model.
\newblock arXiv preprint arXiv::1404.3596 (2014)

\bibitem{Najibi2017}
Najibi, M., Samangouei, P., Chellappa, R., Davis, L.S.:
\newblock Ssh: Single stage headless face detector.
\newblock In: ICCV (2017)

\bibitem{Zhang2017}
Zhang, S., Zhu, X., Lei, X., Shi, H., Wang, X., Li, S.Z.:
\newblock Sfd: Single shot scale-invariant face detector.
\newblock In: ICCV (2017)

\bibitem{Wang2017}
Wang, Y., Ji, X., Zhou, Z., Wang, H., Li, Z.:
\newblock Detecting faces using region-based fully convolutional networs.
\newblock arXiv preprint arXiv::1709.05256 (2017)

\bibitem{Wang2017b}
Wang, J., Y.Yuan, Yu, G.:
\newblock Face attention network: An effective face detector for the occluded
  faces.
\newblock arXiv preprint arXiv::1711.07246 (2017)

\bibitem{Lin2017}
Lin, T.Y., Doll\'ar, P., Girshick, R.:
\newblock Feature pyramid networks for object detection.
\newblock In: CVPR (2017)

\bibitem{Zhang2017b}
Zhang, S., Zhu, X., Lei, X., Shi, H., Wang, X., Li, S.Z.:
\newblock Faceboxes: A cpu real-time face detector with high accuracy.
\newblock arXiv preprint arXiv:: 1708.05234 (2017)

\bibitem{Yang2017}
Yang, S., Xiong, Y., Loy, C.C., Tang, X.:
\newblock Face detection through scale-friendly deep convolutional networks.
\newblock arXiv preprint arXiv::1706.02863 (2017)

\bibitem{Zhu2016}
Zhu, C., Zheng, Y., Luu, K., Savvides, M.:
\newblock Cms-rcnn: contextual multi-scale region-based cnn for unconstrained
  face detection.
\newblock arXiv preprint arXiv::1606.05413 (2016)

\bibitem{Hu2017}
Hu, P., Ramanan, D.:
\newblock Finding tiny faces.
\newblock In: CVPR (2017)

\bibitem{Liu2016b}
Liu, W., Rabinovich, A., Berg, A.C.:
\newblock Parsenet: Looking wider to see better.
\newblock ICLR (2016)

\bibitem{Cai2016}
Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N.:
\newblock A unified multiscale deep convolutional neural network for fast
  object detection.
\newblock In: ECCV (2016)

\bibitem{Fu2017}
Fu, C.Y., Liu, W., Ranga, A., Tyagi, A., Berg, A.C.:
\newblock Dssd: Deconvolutional single shot detector.
\newblock arXiv preprint arXiv:: 1701.06659

\bibitem{Szegedy2016}
Szegedy, C., Ioffe, S., Vanhoucke, V.:
\newblock Inception-v, inception-resnet and the impact of residual
  connections on learning.
\newblock arXiv preprint arXiv::1602.07261 (2016)

\bibitem{Goodfellow2013}
Goodfellow, I.J., Farley, D.W., Mirza, M., Courville, A., Bengio, Y.:
\newblock Maxout networks.
\newblock (2013)

\bibitem{Thompson2012}
Thompson, S.K.:
\newblock Sampling.
\newblock WILEY (Mar. 2012)

\bibitem{Russakovsky2015}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.:
\newblock Imagenet large scale visual recognition challenge.
\newblock Internatinoal Journal of Computer Vision \textbf{115}(3) (2015)

\bibitem{Glorot2010}
Glorot, X., Bengio, Y.:
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In: Aistats \textbf{9} (2010)

\bibitem{Krizhevsky2012}
Krizhevsky, A., Sutskever, I., Hinton, G.:
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In: NIPS (2012)

\bibitem{Jain2010}
Jain, V., Learned-Miller, E.G.:
\newblock Fddb: A benchmark for face detection in unconstrained settings.
\newblock UMass Amherst Technical Report (2010)

\bibitem{Yang2015b}
S.~Yang~and, P.L.n.C.C.L., Tang, X.:
\newblock Wider face: A face detection benchmark.
\newblock In: CVPR (2016)

\bibitem{Zhang2016}
Zhang, K., Zhang, Z., Li, Z., Qiao, Y.:
\newblock Joint face detection and alignment using multitask cascaded
  convolutional networks.
\newblock In: SPL \textbf{23}(10) (2016)

\bibitem{Yu2016}
Yu, J., Jiang, Y., Wang, Z., Cao, Z., Huang, T.:
\newblock Unitbox: An advanced object detection network.
\newblock In: ACMMM (2016)

\bibitem{Triantafyllidou2016}
Triantafyllidou, D., Tefas, A.:
\newblock A fast deep convolutional neural network for face detection in big
  visual data.
\newblock In INNS Conference on Big Data (2016)

\bibitem{Yang2015}
S.~Yang~and, P.L.n.C.C.L., Tang, X.:
\newblock From facial parts responses to face detection: A deep learning
  approach.
\newblock In: ICCV (2015)

\bibitem{Li2016}
Li, Y., Sun, B., Wu, T., Wang, Y.:
\newblock Facedetection with end-to-end integration of a convnet and a 3d
  model.
\newblock (2016)

\bibitem{Farfade2015}
Farfade, S.S., Saberian, M.J., Li, L.J.:
\newblock Multi-view face detection using deep convolutional neural networks

\bibitem{Ghiasi2015}
Ghiasi, G., Fowlkes, C.:
\newblock Occlusion coherence: Detecting and localizing occluded faces.
\newblock (2015)

\bibitem{Kumar2015}
Kumar, V., Namboodiri, A., Jawahar, C.:
\newblock Visual phrases for exemplar face detection.
\newblock In: ICCV (2015)

\bibitem{Li2013}
Li, H., Hua, G., Lin, Z., Brandt, J., Yang, J.:
\newblock Probabilistic elastic part model for unsupervised face detector
  adaptation.
\newblock In: ICCV (2013)

\bibitem{Li2013b}
Li, J., Zhang, Y.:
\newblock Learning surf cascade for fast and accurate object detection.
\newblock In: CVPR (2013)

\bibitem{Li2014}
Li, H., Lin, Z., Brandt, J., Shen, X., Hua, G.:
\newblock Efficient boosted exemplar-based face detection.
\newblock In: CVPR (2014)

\bibitem{Ohnbar2016}
Ohn-Bar, E., Trivedi, M.M.:
\newblock To boost or not to boost? on the limits of boosted trees for object
  detection.
\newblock In: ICPR (2016)

\bibitem{Ranjan2015}
Ranjan, R., Patel, V.M., Chellappa, R.:
\newblock A deep pyramid deformable part model for face detection.
\newblock In: BTAS (2015)

\bibitem{Ranjan2016}
Ranjan, R., Patel, V.M., Chellappa, R.:
\newblock Hyperface: A deep multi-task learning framework for face detection,
  landmark localization, pose estimation, and gender recognition.
\newblock arXiv preprint arXiv::1603.01249 (2016)

\bibitem{Wan2016}
Wan, S., Chen, Z., Zhang, T., Zhang, B., Wong, K.K.:
\newblock Bootstrapping face detection with hard negative examples.
\newblock arXiv preprint arXiv: 1608.02236 (2016)

\bibitem{Zhang2018}
Zhang, C., Xu, X., Tu, D.:
\newblock Face detection using improved faster rcnn.
\newblock arXiv preprint arXiv:: 1802.02142 (2018)

\bibitem{wang2017c}
Wang, H., Li, Z., Ji, X., Wang, Y.:
\newblock Face r-cnn. arxiv preprint.
\newblock arXiv preprint arXiv:1706.01061 \textbf{7} (2017)


\bibitem{zhang2017single}
Zhang, Shifeng and Wen, Longyin and Bian, Xiao and Lei, Zhen and Li, Stan Z:
\newblock Single-shot refinement neural network for object detection. 
\newblock arXiv preprint arXiv:1711.06897 (2017)





\end{thebibliography}

\clearpage

\section*{Appendix}


In this section, we show the robustness of our PyramidBox algorithm by testing it on the face images having large variance in scale, blur, pose and occlusion. Even in the images filled with small, blurred or partially occluded faces and big face with exaggerate expression, our PyramidBox can recall most of these faces, see Fig.~\ref{fig:demo_img}.  Besides, the robustness of scale, occlusion, blur, and pose is respectively described in the Fig.~\ref{fig:robustness_scale}, Fig.~\ref{fig:robustness_blur}, Fig.~\ref{fig:robustness_pose} and Fig.~\ref{fig:robustness_occlusion}.

\begin{figure}
\centering
\includegraphics[height=6.3cm]{demo_img.pdf}
\caption{An exampler which has extreme variability in the face regions. Our PyramidBox can find 880 faces out of the reportedly 1000 present in the above image. On the right of image, detector confidence is present to you directly by colorbar. Please zoom in for more details.}
\label{fig:demo_img}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[height=18cm]{robustness_scale.pdf}
\caption{Our PyramidBox can handle faces with a wide range of face scales. Blue represent the detector confidence above 0.8.}
\label{fig:robustness_scale}
\end{figure}


\begin{figure}
\centering
\includegraphics[height=9cm]{robustness_blur.pdf}
\caption{Our PyramidBox is able to handle various forms of blur, a key factor leading to the degradation of image quality. Blue represent the detector confidence above 0.8.}
\label{fig:robustness_blur}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=6.5cm]{robustness_pose.pdf}
\caption{The results of our PyramidBox across pose is shown in this figure, and blue represent the detector confidence above 0.8.}
\label{fig:robustness_pose}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[height=12cm,width=13cm]{robustness_occlusion.pdf}
\caption{Our PyramidBox can handle facial occlusions caused by sunglasses, mask, hat etc., and blue represent the detector confidence above 0.8.}
\label{fig:robustness_occlusion}
\end{figure}
\end{document}
