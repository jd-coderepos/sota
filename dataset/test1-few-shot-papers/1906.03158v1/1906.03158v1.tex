\begin{figure*}[p]
  \centering
  \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
  \begin{tabular}{P{0.45\textwidth}  P{0.45\textwidth}}
    \includegraphics[width=0.28\textwidth]{images/supervised_training.pdf} &
    \includegraphics[width=0.28\textwidth]{images/fewshot_training.pdf} \\ \hline
  \end{tabular}
  \caption{Illustration of losses used in our models. The left figure depicts
  a model suitable for supervised training, where the model is expected to classify
  over a predefined dictionary of relation types. The figure on the right depicts a pairwise
  similarity loss used for few-shot classification task.}
  \label{fig:loss-architecture}
\end{figure*}


\begin{figure*}[p]
  \centering
  \footnotesize
  \begin{tabular}{ccc}
    \includegraphics[width=0.25\textwidth]{images/cls_basic.pdf} &
    \includegraphics[width=0.25\textwidth]{images/entity_mention_pooling.pdf} &
    \includegraphics[width=0.25\textwidth]{images/positional_embeddings.pdf} \\
    (a) \textsc{standard} -- \textsc{[cls]} & 
    (b) \textsc{standard} -- \textsc{mention pooling} &
    (c) \textsc{positional emb.} -- \textsc{mention pool.} \\ [10pt] 
    \includegraphics[width=0.25\textwidth]{images/markers_and_cls.pdf} &
    \includegraphics[width=0.25\textwidth]{images/markers_and_entity_mentions.pdf} &
    \includegraphics[width=0.25\textwidth]{images/markers_and_markers_pooling.pdf} \\
    (d) \textsc{entity markers} -- \textsc{[cls]} &
    (e) \textsc{entity markers} -- \textsc{mention pool.} &
    (f) \textsc{entity markers} -- \textsc{entity start} \\ \hline
  \end{tabular}
  \caption{Variants of architectures for extracting relation representations from deep
  Transformers network. Figure~(a) depicts a model with \textsc{STANDARD} input and 
  \textsc{[CLS]} output, Figure~(b) depicts a model with \textsc{STANDARD} input and 
  \textsc{mention pooling} output and Figure~(c) depicts a model with
  \textsc{positional embeddings} input and \textsc{mention pooling} output.
  Figures (d), (e), and (f)  use \textsc{entity markers} input while using
  \textsc{[CLS]}, \textsc{mention pooling}, and \textsc{entity start} output, respectively.}
  \label{fig:model-variants}
\end{figure*}


\begin{table*}[p]
    \setlength{\tabcolsep}{5pt}
    \centering
    \footnotesize
    \begin{tabular}{|c|c|cc|cc|cc|c|}
    \hline
    \multicolumn{2}{|c|}{}
        & \multicolumn{2}{c|}{\textbf{SemEval 2010}}
        & \multicolumn{2}{c|}{\textbf{KBP37}}
        & \multicolumn{2}{c|}{\textbf{TACRED}} 
        & \textbf{FewRel} \\
    \multicolumn{2}{|c|}{}
        & \multicolumn{2}{c|}{\textbf{Task 8}}
        & \multicolumn{2}{c|}{}
        & \multicolumn{2}{c|}{}
        & \textbf{5-way-1-shot} \\        \hline
\multicolumn{2}{|c|}{\textbf{\# training annotated examples}} &
 \multicolumn{2}{c|}{8,000 (6,500 for dev)}    & 
 \multicolumn{2}{c|}{15,916}  & 
 \multicolumn{2}{c|}{68,120}  &
 44,800\\ \hline
\multicolumn{2}{|c|}{\textbf{\# relation types}} &
 \multicolumn{2}{c|}{19}  & 
 \multicolumn{2}{c|}{37}  & 
 \multicolumn{2}{c|}{42}  &
 100 \\ \hline
\multicolumn{2}{|c|}{} & Dev F1 & Test F1 & Dev F1 & Test F1 & Dev F1 & Test F1 & Dev Acc. \\ \hline
\multicolumn{2}{|c|}{\citet{P16-1123}*}  & -- & 88.0 & -- & -- & -- & -- & -- \\ \hline 
\multicolumn{2}{|c|}{\citet{ZhangW15a}*} & -- & 79.6 & -- & 58.8 & -- & -- & -- \\ \hline
\multicolumn{2}{|c|}{\citet{DBLP:journals/corr/abs-1807-03052}*} & -- & 84.8 & -- & -- & -- & 68.2 & -- \\ \hline
\multicolumn{2}{|c|}{\citet{han2018fewrel}}       & --    & -- & -- & -- & -- & -- & 71.6 \\ \hline \hline
\textbf{Input type}  & \textbf{Output type}       &        &    &       &     &       &   & \\ \hline
\textsc{standard} & \textsc{[CLS]}                & 71.6  & -- & 41.3  & --  & 23.4  & --  & 85.2 \\ \hline
\textsc{standard} & \textsc{mention pool.}        & 78.8  & -- & 48.3  & --  & 66.7  & --  & 87.5 \\ \hline
\textsc{positional emb.} & \textsc{mention pool.} & 79.1  & -- & 32.5  & --  & 63.9  & --  & 87.5 \\ \hline
\textsc{entity markers} & \textsc{[CLS]}          & 81.2  & --	& 68.7  & --  & 65.7  & --  & 85.2 \\ \hline
\textsc{entity markers} & \textsc{mention pool.}  & 80.4  & -- & 68.2  & --  & 69.5  & --  & 87.6 \\ \hline
\textsc{entity markers} & \textsc{entity start} & \textbf{82.1} & \textbf{89.2} & \textbf{70} & \textbf{68.3} & \textbf{70.1} & \textbf{70.1} & \textbf{88.9} \\ \hline
    \end{tabular}
    \caption{Results for supervised relation extraction tasks. Results on rows where the model name is marked with a~* symbol are reported as published, all other numbers have been computed by us.
    SemEval 2010 Task 8 does not establish a default split for development; for this work we use a random slice of the training set with 1,500 examples.}
    \label{tab:supervised-results}
\end{table*}

\section{Architectures for Relation Learning}
\label{sec:transformers-rel}

The primary goal of this work is to develop models that produce relation representations
directly from text. Given the strong performance of recent deep transformers trained on
variants of language modeling, we adopt \citet{devlin2018bert}'s \bert~model
as the basis for our work. In this section, we explore different 
methods of representing relations with the Transformer model.


\subsection{Relation Classification and Extraction Tasks}
\label{sec:relation-tasks}



We evaluate the different methods of representation on a suite of supervised relation extraction benchmarks. 
The relation extractions tasks we use can be broadly
categorized into two types: fully supervised relation extraction, and few-shot relation matching.

For the supervised tasks, the goal is to, given a relation statement , predict a relation type  where  is a fixed dictionary of relation types and  typically denotes a lack of relation between the entities in the relation statement.
For this type of task we evaluate on SemEval 2010 Task~8~\cite{hendrickx2009semeval}, KBP-37~\cite{ZhangW15a} and TACRED~\cite{zhang2017position}.
More formally,

In the case of few-shot relation matching, a set of candidate relation statements are ranked, and matched, according to a query relation statement. In this task, examples in the test and development sets typically contain relation types not present in the training set.
For this type of task, we evaluate on the FewRel~\cite{han2018fewrel} dataset. 
Specifically, we are given  sets of  labeled relation statements
 where 
is the corresponding relation type. The goal is to predict the 
for a query relation statement .





\subsection{Relation Representations from Deep Transformers Model}
\label{sec:transformers-results}


In all experiments in this section, we start with the \bert model made available by \citet{devlin2018bert} and train towards task-specific losses.
Since \bert{} has not previously been applied to the problem of relation representation, we aim to answer two primary modeling questions: (1) how do we represent entities of interest in the input to \bert{}, and (2) how do we extract a fixed length representation of a relation from \bert{}'s output.
We present three options for both the input encoding, and the output relation representation. Six combinations of these are illustrated in Figure~\ref{fig:model-variants}.

\begin{comment}
Unlike previous relation classification systems which typically use external
features as input, such as named entity recognition (NER) tags and
part-of-speech (POS) tags,  the classifiers  we train simply use input text as features.
\end{comment}




\subsubsection{Entity span identification}
Recall, from Section~\ref{sec:overview}, that the relation statement   contains the sequence of tokens  and the entity span identifiers  and .
We present three different options for getting information about the focus spans  and  into our \bert~encoder.

\paragraph{Standard input}
First we experiment with a \bert{} model that does not have access to any explicit identification of the entity spans  and . 
We refer to this choice as the \textsc{standard}~input.
This is an important reference point, since we believe that \bert{} has the ability to identify entities in , but with the \textsc{standard}~input there is no way of knowing which two entities are in focus when  contains more than two entity mentions. 

\paragraph{Positional embeddings}
For each of the tokens in its input, \bert~also adds a segmentation embedding, primarily used to add sentence segmentation information to the model.
To address the \textsc{standard} representation's lack of explicit entity identification, we introduce two new segmentation embeddings, one that is added to all tokens in the span , while the other is added to all tokens in the span . 
This approach is analogous to previous work where positional embeddings
    have been applied to relation extraction \cite{zhang2017position, DBLP:journals/corr/abs-1807-03052}.
    



\paragraph{Entity marker tokens}
Finally, we augment  with four reserved word pieces to mark the begin and end of each entity mention in the relation statement. 
We introduce the  , ,  and  and modify 
 to give

and we feed this token sequence into \bert~instead of . We also update the entity indices  and  to account for the inserted tokens.
We refer to this representation of the input as \textsc{entity markers}.

\subsection{Fixed length relation representation}
\label{sec:relation-rep}
We now introduce three separate methods of extracting a fixed length relation representation  from the \bert~encoder.
The three variants rely on extracting the last hidden layers of the transformer network, which we define as  for  (or  if entity marker tokens are used). 

\paragraph{\cls~token} Recall from Section~\ref{sec:overview} that each  starts with a reserved \cls~token. \bert's output state that corresponds to this token is used by \citet{devlin2018bert} as a fixed length sentence representation. We adopt the \cls~output, , as our first relation representation.

\paragraph{Entity mention pooling}
We obtain  by max-pooling the final hidden layers  corresponding to the word pieces in each entity mention, to get two vectors  and  representing the two entity mentions. We concatenate these two vectors to get the single representation 
 where  is the concatenation of  and . 
We refer to this architecture as \textsc{mention pooling}.

\paragraph{Entity start state}
Finally, we propose simply representing the relation between two entities with the concatenation of the final hidden states corresponding their respective start tokens, when \textsc{entity markers} are used. 
Recalling that \textsc{entity markers} inserts tokens in , creating offsets in  and , our representation of the relation is .  We refer to this output representation as
   \textsc{entity start} output. Note that this can only be applied to the \textsc{entity markers} input.

\begin{comment}
\paragraph{\cls~token representation} 

\textbf{Standard}
    Use the standard BERT input sequence, where the  sequence from the relation
     statement is pre-pended with a reserved  word piece token and appended
     with another reserved  token, modifying the relation statement into
     . We refer to this input architecture
     as \textsc{standard}.
     
     The main disadvantage of this input architectures is that it does not provide
     entity information ( and ) as part of the relation statement. The
     next two input architectures are included to provide the model with such information.

\textbf{Positional embedding} 
    Augment the BERT standard input by modifying the  "token type" embedding layer,
    defined in BERT, to encode positional information about the two entities in the relation
    statement. This approach is analogous to previous work where positional embeddings
    have been applied to relation extraction (e.g.~\citet{zhang2017position} and~\citet{DBLP:journals/corr/abs-1807-03052}).
    
    We reserve two extra token type indices and assign per word piece index
    values based on the overlap with word pieces of entities  and .
    Token types of word pieces between  and  have a value of 1, token
    types of word pieces between  and  have a value 2, and all other token
    types are indexed to 0. We refer to this input architecture as
    \textsc{positional embedding} input.
    
\textbf{Entity markers}
    Finally, we augment the standard BERT input sequence with four reserved word pieces
    to mark the begin and end of each entity mention in the relation statement:
    , ,  and .
    We insert  word pieces before their respective mentions and add  after
    their respective mentions, thus modifying the relation statement to:
    
    We refer to this input architecture as \textsc{entity markers}.


\textbf{CLS} Representation is extracted from the last hidden layer of the deep Transformers
    corresponding to the position of the reserved  word piece ().
    This is the standard representation for several of the classifiers described
    in~\cite{devlin2018bert}. We refer to this architecture as \textsc{[CLS]}.


\textbf{Entity mention pooling}
    Representation is obtained through max-pooling final hidden layers of corresponding
    to each word piece that belongs to the same entity mention, and
    concatenating these two vectors into a single representation: \\
    \\
    We refer to this architecture as \textsc{mention pooling}.

\textbf{Entity markers}
   Representation is obtained through concatenating the final hidden representations
   corresponding to the  and  positions of the statement
   ().
   Note that this representation is only possible for architectures where we add
   entity markers to the input of the model. We refer to this output representation as
   \textsc{entity markers} output.
   \end{comment}

Figure~\ref{fig:model-variants} illustrates a few of the variants we evaluated in this section.
In addition to defining the model input and output architecture, we fix the training loss
used to train the models (which is illustrated in Figure~\ref{fig:loss-architecture}).
In all models, the output representation from the Transformer network is fed into
a fully connected layer that either (1) contains a linear activation, or
(2) performs layer normalization~\cite{ba2016layer} on the representation. We treat the
choice of post Transfomer layer as a hyper-parameter and use the best performing layer
type for each task.

For the supervised tasks, we introduce a new classification layer
 where  is the size of the relation representation
and  is the number of relation types. The classification loss is the standard
cross entropy of the softmax of  with respect to the true relation type.

For the few-shot task, we use the dot product between relation representation of the query
statement and each of the candidate statements as a similarity score. In this case,
we also apply a cross entropy loss of the softmax of similarity scores with respect
to the true class.

We perform task-specific fine-tuning of the \bert{} model, for all variants, with the
following set of hyper-parameters:
\begin{itemize}[noitemsep]
    \small
    \item \textbf{Transformer Architecture}: 24 layers, 1024 hidden size, 16 heads 
    \item \textbf{Weight Initialization}: BERT
    \item \textbf{Post Transformer Layer}: Dense with linear activation (KBP-37 and TACRED), 
    or Layer Normalization layer (SemEval 2010 and FewRel).
    \item \textbf{Training Epochs}: 1 to 10
    \item \textbf{Learning Rate (supervised)}: 3e-5 with Adam
    \item \textbf{Batch Size (supervised)}: 64
    \item \textbf{Learning Rate (few shot)}: 1e-4 with SGD
    \item \textbf{Batch Size (few shot)}: 256
\end{itemize}



Table~\ref{tab:supervised-results} shows the results of model variants on the three
supervised relation extraction tasks and the 5-way-1-shot variant of the few-shot relation classification task. For all four tasks, the model using the
\textsc{entity markers} input representation and \textsc{entity start} output representation
achieves the best scores.

From the results, it is clear that adding  positional information in the input is critical
for the model to learn useful relation representations. Unlike previous work that have
benefited from positional embeddings~\cite{zhang2017position, DBLP:journals/corr/abs-1807-03052}, the deep Transformers benefits
the most from seeing the new entity boundary word pieces (\textsc{entity markers}). It is
also worth noting that the best variant outperforms previous published models on all
four tasks. For the remainder of the paper, we will use this architecture when further
training and evaluating our  models.