\documentclass{article}







\usepackage[final]{neurips_2020}
\newcommand{\MAT}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\VEC}[1]{\left[\begin{array}{c} #1 \end{array}\right]}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\keys}[1]{\left\{ #1 \right\}}
\newcommand{\sqbr}[1]{\left[ #1 \right]}
\newcommand{\brac}[1]{\left( #1 \right) }
\newcommand{\ml}[1]{\mathcal{ #1 } }
\newcommand{\op}[1]{ \operatorname{#1} }
\newcommand{\normInf}[1]{\left\| #1 \right\| _{\infty}}
\newcommand{\normTwo}[1]{\left\| #1 \right\| _{2}}
\newcommand{\normOne}[1]{\left\| #1 \right\| _{1}}
\newcommand{\normTV}[1]{\left\| #1 \right\| _{\op{TV}}}
\newcommand{\normF}[1]{\left\| #1 \right\| _{F}}
\newcommand{\atomicnorm}[1]{\left\| #1 \right| \right|_{\mathcal{A}}}
\newcommand{\der}[2]{\frac{\text{d}#2}{\text{d}#1}}
\newcommand{\derTwo}[2]{\frac{\mathrm{d} ^2#2}{\mathrm{d}#1^2}}
\newcommand{\PROD}[2]{\left \langle #1, #2\right \rangle}
\newcommand{\diff}[1]{ \, \text{d} #1 }
\newcommand{\real}{\op{Re}}
\newcommand{\imag}{\op{Im}}
\newcommand{\wt}[1]{\widetilde{ #1 }}
\newcommand{\R}{\mathbb{R}}
\newcommand\at[2]{\left.#1\right|_{#2}}
\newcommand{\RR}{\mathcal{R}}



\newcommand\EE{\mathbb{E}}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{enumitem, array}
\usepackage[flushleft]{threeparttable}
\usepackage{algpseudocode}

\newcommand{\Req}{\textbf{Require:}\hspace*{0.5em}}
\newcommand*\Let[2]{\State #1  #2}
\newcommand{\X}{\hspace*{3mm}}
\newcommand{\XX}{\X\X}
\newcommand{\XXX}{\X\X\X}
\newcommand{\XXXX}{\X\X\X\X}
\newcommand{\XXXXX}{\X\X\X\X\X}
\newcommand{\XXZ}{\XXX{} }
\newcommand{\cm}[1]{ #1}

\usepackage{hyperref}
\hypersetup{hidelinks,
backref=true,
pagebackref=true,
hyperindex=true,
breaklinks=true,
colorlinks=true,urlcolor=blue,
bookmarks=true,
bookmarksopen=false,
pdftitle={Title},
pdfauthor={Author}} \usepackage[utf8]{inputenc} \usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts,dsfont}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{amssymb}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\usepackage{amsthm}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{booktabs}

\newcommand{\carlos}[1]{\textcolor{blue}{#1}}
\newcommand{\jon}[1]{\textcolor{green}{#1}}
\newcommand{\sheng}[1]{\textcolor{violet}{#1}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vytrue}{\vy^*}
\newcommand{\va}{\mathbf{z}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\param}{\Theta}
\newcommand{\vone}{\mathbf{1}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vq}{\mathbf{t}}
\newcommand{\vv}{\mathbf v}
\newcommand{\vz}{\mathbf z}
\newcommand{\pmlabels}{\mathbf{\varepsilon}}
\newcommand{\pmlabelstrue}{\pmlabels^\ast}
\newcommand{\noise}{\Delta} \newcommand{\func}{\ml{N}}
\newcommand{\smax}{\ml{S}}
\newcommand{\coeff}{\kappa}
\newcommand{\correct}{C}
\newcommand{\wrong}{W}
\newcommand{\E}{\mathbb E}
\newcommand{\1}{\mathds{1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\todo}{\jon{TODO}}


\title{Early-Learning Regularization Prevents Memorization of Noisy Labels}



\author{Sheng Liu \\
  Center for Data Science\\
  New York University\\
  \texttt{shengliu@nyu.edu} \\
  \And
  Jonathan Niles-Weed\\
   Center for Data Science, and\\
   Courant Inst. of Mathematical Sciences\\
   New York University\\
   \texttt{jnw@cims.nyu.edu} \\
  \And
  Narges Razavian\\
  Department of Population Health, and\\
  Department of Radiology\\
  NYU School of Medicine\\
   \texttt{narges.razavian@nyulangone.org} \\
  \And
  Carlos Fernandez-Granda\\
  Center for Data Science, and\\
   Courant Inst. of Mathematical Sciences\\
   New York University\\
   \texttt{cfgranda@cims.nyu.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  We propose a novel framework to perform classification via deep learning in the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an ``early learning'' phase, before eventually memorizing the examples with false labels. We prove that early learning and memorization are fundamental phenomena in high-dimensional classification tasks, even in simple linear models, and give a theoretical explanation in this setting. Motivated by these findings, we develop a new technique for noisy classification tasks, which exploits the progress of the early learning phase. In contrast with existing approaches, which use the model output during early learning to detect the examples with clean labels, and either ignore or attempt to correct the false labels, we take a different route and instead capitalize on early learning via regularization. There are two key elements to our approach. First, we leverage semi-supervised learning techniques to produce target probabilities based on the model outputs. Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels. The resulting framework is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the state of the art.
 \end{abstract}

\section{Introduction}

Deep neural networks have become an essential tool for classification tasks~\citep{krizhevsky2012imagenet,he2016deep,girshick2014rich}. These models tend to be trained on large curated datasets such as CIFAR-10~\citep{krizhevsky2009learning} or ImageNet~\citep{deng2009imagenet}, where the vast majority of labels have been manually verified. Unfortunately, in many applications such datasets are not available, due to the cost or difficulty of manual labeling (e.g.~\citep{guan2018said,pechenizkiy2006class,liu20a,ait2010high}). However, datasets with lower quality annotations, obtained for instance from online queries~\citep{blum2003noise} or crowdsourcing~\citep{yan2014learning,yu2018learning}, may be available. Such annotations inevitably contain numerous mistakes or \emph{label noise}. It is therefore of great importance to develop methodology that is robust to the presence of noisy annotations.

 When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an \emph{early learning} phase, before eventually \emph{memorizing} the examples with false labels~\citep{arpit2017closer,zhang2016understanding}. In this work we study this phenomenon and introduce a novel framework that exploits it to achieve robustness to noisy labels. Our main contributions are the following:
 
\begin{itemize}[leftmargin=*]
\item In Section~\ref{sec:linear} we establish that early learning and memorization are fundamental phenomena in high dimensions, proving that they occur even for simple linear generative models.
\item In Section~\ref{sec:methodology} we propose a technique that utilizes the early-learning phenomenon to counteract the influence of the noisy labels on the gradient of the cross entropy loss. This is achieved through a regularization term that incorporates target probabilities estimated from the model outputs using several semi-supervised learning techniques.
\item In Section~\ref{sec:results} we show that the proposed methodology achieves results comparable to the state of the art on several standard benchmarks and real-world datasets. We also perform a systematic ablation study to evaluate the different alternatives to compute the target probabilities, and the effect of incorporating mixup data augmentation~\citep{zhang2017mixup}.
\end{itemize}

\begin{figure}[t]
    \begin{tabular}{>{\centering\arraybackslash}m{0.13\linewidth} >{\centering\arraybackslash}m{0.4\linewidth} >{\centering\arraybackslash}m{0.4\linewidth}}
    & {\small Clean labels} & {\small Wrong labels}\\
    {\small Cross Entropy}
    & \includegraphics[width=\linewidth]{images/True_label_behavior_04.png}&
    \includegraphics[width=\linewidth]{images/False_label_behavior_04.png}\\
    {\small \shortstack{Early-learning\\Regularization}}
    & \includegraphics[width=\linewidth]{images/True_label_behavior_ours_04.png}&
    \includegraphics[width=\linewidth]{images/False_label_behavior_ours_04.png}
  \end{tabular}
    
\caption{Results of training a ResNet-34~\citep{he2016deep} neural network with a traditional cross entropy loss (top row) and our proposed method (bottom row) to perform classification on the CIFAR-10 dataset where 40\% of the labels are flipped at random. The left column shows the fraction of examples with clean labels that are predicted correctly (green) and incorrectly (blue). The right column shows the fraction of examples with wrong labels that are predicted correctly (green), \emph{memorized} (the prediction equals the wrong label, shown in red), and incorrectly predicted as neither the true nor the labeled class (blue). The model trained with cross entropy begins by learning to predict the true labels, even for many of the examples with wrong label, but eventually memorizes the wrong labels. Our proposed method based on early-learning regularization prevents memorization, allowing the model to continue learning on the examples with clean labels to attain high accuracy on examples with both clean and wrong labels.}
    \label{fig:CE_vs_ELR}
\end{figure}

\vspace{-1mm}
\section{Related Work}
\label{sec:related_work}
In this section we describe existing techniques to train deep-learning classification models using data with noisy annotations. 
We focus our discussion on methods that do not assume the availability of small subsets of training data with clean labels (as opposed, for example, to~\citep{Hendrycks2018UsingTD,Ren2018LearningTR,veit2017learning}). We also assume that the correct classes are known (as opposed to~\citep{Wang2018IterativeLW}). 

\emph{Robust-loss} methods propose cost functions specifically designed to be robust in the presence of noisy labels. These include Mean Absolute Error (MAE)~\citep{ghosh2017robust}, Improved MAE~\citep{wang2019imae}, which is a reweighted MAE, Generalized Cross Entropy~\citep{zhang2018generalized}, which can be interpreted as a generalization of MAE, Symmetric Cross Entropy~\citep{Wang2019SymmetricCE}, which adds a reverse cross-entropy term to the usual cross-entropy loss, and ~\citep{Xu2019L_DMIAN}, which is based on information-theoretic considerations. \emph{Loss-correction} methods explicitly correct the loss function to take into account the noise distribution, represented by a transition matrix of mislabeling probabilities~\citep{patrini2017making,Goldberger2017TrainingDN,xia2019anchor, tanno2019learning}. 

Robust-loss and loss-correction techniques do not exploit the early-learning phenomenon mentioned in the introduction. This phenomenon was described in~\citep{arpit2017closer} (see also~\citep{zhang2016understanding}), and analyzed theoretically in~\citep{li2019gradient}. Our theoretical approach differs from theirs in two respects. First, Ref.~\citep{li2019gradient} focus on a least squares regression task, whereas we focus on the noisy label problem in classification. Second, and more importantly, we prove that early learning and memorization occur even in a \emph{linear} model.

Early learning can be exploited through \emph{sample selection}, where the model output during the early-learning stage is used to predict which examples are mislabeled and which have been labeled correctly. The prediction is based on the observation that mislabeled examples tend to have higher loss values. Co-teaching~\citep{Han2018CoteachingRT,Yu2019HowDD} performs sample selection by using two networks, each trained on a subset of examples that have a small training loss for the other network (see~\citep{Jiang2018MentorNetLD,malach2017decoupling} for related approaches). A limitation of this approach is that the examples that are selected tend to be \emph{easier}, in the sense that the model output during early learning approaches the true label. As a result, the gradient of the cross-entropy with respect to these examples is small, which slows down learning~\citep{chang2017active}. In addition, the subset of selected examples may not be rich enough to generalize effectively to held-out data~\citep{song2019selfie}.

An alternative to sample selection is \emph{label correction}. During the early-learning stage the model predictions are accurate on a subset of the mislabeled examples (see the top row of Figure~\ref{fig:CE_vs_ELR}). This suggests correcting the corresponding labels. This can be achieved by computing new labels equal to the probabilities estimated by the model (known as \emph{soft labels}) or to one-hot vectors representing the model predictions (\emph{hard labels})~\citep{Tanaka2018JointOF,yi2019probabilistic}. Another option is to set the new labels to equal a convex combination of the noisy labels and the soft or hard labels~\citep{Reed2015TrainingDN}. Label correction is usually combined with some form of iterative sample selection~\citep{Arazo2019unsup,Ma2018DimensionalityDrivenLW,song2019selfie,li2020dividemix} or with additional regularization terms~\citep{Tanaka2018JointOF}.  SELFIE~\citep{song2019selfie} uses label replacement to correct a subset of the labels selected by considering past model outputs. Ref.~\citep{Ma2018DimensionalityDrivenLW} computes a different convex combination with hard labels for each example based on a measure of model dimensionality. Ref.~\citep{Arazo2019unsup} fits a two-component mixture model to carry out sample selection, and then corrects labels via convex combination as in \citep{Reed2015TrainingDN}. They also apply mixup data augmentation~\citep{zhang2017mixup} to enhance performance. In a similar spirit, DivideMix~\citep{li2020dividemix} uses two networks to perform sample selection via a two-component mixture model, and applies the semi-supervised learning technique MixMatch~\citep{berthelot2019mixmatch}.  

Our proposed approach is somewhat related in spirit to label correction. We compute a probability estimate that is analogous to the soft labels mentioned above, and then exploit it to avoid memorization. However it is also fundamentally different: instead of modifying the labels, we propose a novel regularization term explicitly designed to correct the gradient of the cross-entropy cost function. This yields strong empirical performance, without needing to incorporate sample selection.


\vspace{-1mm}
\section{Early learning as a general phenomenon of high-dimensional classification}
\label{sec:linear}

As the top row of Figure~\ref{fig:CE_vs_ELR} makes clear, deep neural networks trained with noisy labels make progress during the early learning stage before memorization occurs.
In this section, we show that far from being a peculiar feature of deep neural networks, this phenomenon is intrinsic to high-dimensional classification tasks, even in the simplest setting.
Our theoretical analysis is also the inspiration for the early-learning regularization procedure we propose in Section~\ref{sec:methodology}.

We exhibit a simple \emph{linear} model with noisy labels which evinces the same behavior as described above: the \emph{early learning} stage, when the classifier learns to correctly predict the true labels, even on noisy examples, and the \emph{memorization} stage, when the classifier begins to make incorrect predictions because it memorizes the wrong labels. This is illustrated in Figure~\ref{fig:CE_vs_ELR_linear}, which demonstrates that empirically the linear model has the same qualitative behavior as the deep-learning model in Figure~\ref{fig:CE_vs_ELR}.
We show that this behavior arises because, early in training, the gradients corresponding to the correctly labeled examples dominate the dynamics---leading to early progress towards the true optimum---but that the gradients corresponding to wrong labels soon become dominant---at which point the classifier simply learns to fit the noisy labels.

We consider data drawn from a mixture of two Gaussians in .
The (clean) dataset consists of  i.i.d.~copies of . The label  is a one-hot vector representing the cluster assignment, and 

where  is an arbitrary unit vector in  and  is a small constant.
The optimal separator between the two classes is a hyperplane through the origin perpendicular to .
We focus on the setting where  is fixed while .
In this regime, the classification task is nontrivial, since the clusters are, approximately, two spheres whose centers are separated by 2 units with radii .

We only observe a dataset with noisy labels , 

where  are i.i.d.~random one-hot vectors which take values  and  with equal probability.

We train a linear classifier by gradient descent on the cross entropy:

where  is a softmax function. In order to separate the true classes well (and not overfit to the noisy labels), the rows of  should be correlated with the vector .

The gradient of this loss with respect to the model parameters  corresponding to class  reads

Each term in the gradient therefore corresponds to a weighted sum of the examples , where the weighting depends on the agreement between  and .

Our main theoretical result shows that this linear model possesses the properties described above.
During the early-learning stage, the algorithm makes progress and the accuracy on wrongly labeled examples increases.
However, during this initial stage, the relative importance of the wrongly labeled examples continues to grow; once the effect of the wrongly labeled examples begins to dominate, memorization occurs.

\begin{theorem}[Informal]\label{thm:main}
Denote by  the iterates of gradient descent with step size .
For any , there exists a constant  such that, if 
and , then with probability  as  there exists a  such that:
\begin{itemize}
\item \textbf{Early learning succeeds}:  For ,  is well correlated with the correct separator , and at  the classifier has higher accuracy on the wrongly labeled examples than at initialization.
\item \textbf{Gradients from correct examples vanish}: Between  and , the magnitudes of the coefficients  corresponding to examples with clean labels decreases while the magnitudes of the coefficients for examples with wrong labels increases. 
\item \textbf{Memorization occurs}: As , the classifier  memorizes all noisy labels.
\end{itemize}
\end{theorem}
Due to space constraints, we defer the formal statement of Theorem~\ref{thm:main} and its proof to the supplementary material.

The proof of Theorem~\ref{thm:main} is based on two observations. First, while  is still not well correlated with , the coefficients  are similar for all , so that  points approximately in the average direction of the examples. Since the majority of data points are correctly labeled, this means the gradient is still well correlated with the correct direction during the early learning stage. Second, once  becomes correlated with , the gradient begins to point in directions orthogonal to the correct direction ; when the dimension is sufficiently large, there are enough of these orthogonal directions to allow the classifier to completely memorize the noisy labels. 

This analysis suggests that in order to learn on the correct labels and avoid memorization it is necessary to (1) ensure that the contribution to the gradient from examples with clean labels remains large, and (2) neutralize the influence of the examples with wrong labels on the gradient. In Section 4 we propose a method designed to achieve this via regularization.



\section{Methodology}\label{sec:methodology}

\subsection{Gradient analysis of softmax classification from noisy labels}
\label{sec:motivation}
In this section we explain the connection between the linear model from Section~\ref{sec:linear} and deep neural networks. Recall the gradient of the cross-entropy loss with respect to  given in~\eqref{eq:linear_gradient}.
Performing gradient descent modifies the parameters iteratively to push  closer to . If  is the true class so that , the contribution of the th example to  is aligned with , and gradient descent moves in the direction of . However, if the label is noisy and , then gradient descent moves in the opposite direction, which eventually leads to memorization as established by Theorem~\ref{thm:main}. 

We now show that for nonlinear models based on neural networks, the effect of label noise is analogous. We consider a classification problem with  classes, where the training set consists of  examples ,  is the th input and  is a one-hot label vector indicating the corresponding class. The classification model maps each input  to a -dimensional encoding using a deep neural network  and then feeds the encoding into a softmax function  to produce an estimate  of the conditional probability of each class given , 

 denotes the parameters of the neural network. The gradient of the cross-entropy loss,

with respect to  equals

where  is the Jacobian matrix of the neural-network encoding for the th input with respect to . Here we see that label noise has the same effect as in the simple linear model. If  is the true class, but  due to the noise, then the contribution of the th example to  is reversed. The entry corresponding to the \emph{impostor} class , is also reversed because . As a result, performing stochastic gradient descent eventually results in memorization, as in the linear model (see Figures~\ref{fig:CE_vs_ELR} and~
\ref{fig:CE_vs_ELR_linear}). Crucially, the influence of the label noise on the gradient of the cross-entropy loss is restricted to the term  (see Figure~\ref{fig:Gradient_CE}). In Section~\ref{sec:ELR} we describe how to counteract this influence by exploiting the early-learning phenomenon. 



\subsection{Early-learning regularization}
\label{sec:ELR}
In this section we present a novel framework for learning from noisy labels called early-learning regularization (ELR). We assume that we have available a \emph{target}\footnote{The term target is inspired by semi-supervised learning where target probabilities are used to learn on unlabeled examples~\citep{yarowsky1995unsupervised, mcclosky2006effective, laine2016temporal}.} vector of probabilities  for each example , which is computed using past outputs of the model. Section~\ref{sec:targets} describes several techniques to compute the targets. Here we explain how to use them to avoid memorization. 

Due to the early-learning phenomenon, we assume that at the beginning of the optimization process the targets do not overfit the noisy labels. ELR exploits this using a regularization term that seeks to maximize the inner product between the model output and the targets,

The logarithm in the regularization term counteracts the exponential function implicit in the softmax function in . A possible alternative to this approach would be to penalize the Kullback-Leibler divergence between the model outputs and the targets. However, this does not exploit the early-learning phenomenon effectively, because it leads to overfitting the targets as demonstrated in Section~\ref{sec:kl}. 

The key to understanding why ELR is effective lies in its gradient, derived in the following lemma, which is proved in Section~\ref{sec:proof_lemma}.

\begin{lemma}[Gradient of the ELR loss]
\label{lemma:ELR_gradient}
The gradient of the loss defined in Eq.~\eqref{eq:ELR} is equal to

where the entries of  are given by

\end{lemma}

\begin{figure}[t]
\hspace{-0.4cm}
    \begin{tabular}{>{\centering\arraybackslash}m{0.31\linewidth} >{\centering\arraybackslash}m{0.31\linewidth} >{\centering\arraybackslash}m{0.31\linewidth}}
    \quad\quad & \quad\quad Clean labels & \quad\quad Wrong labels  \\
      \includegraphics[width=1.1\linewidth]{images/NN_grad_true_logit_ours_sum.png}
      &
     \includegraphics[width=1.1\linewidth]{images/NN_grad_true_logit_ours_True.png}&
     \includegraphics[width=1.1\linewidth]{images/NN_grad_true_logit_ours_False.png}
    \end{tabular}
    \caption{
    Illustration of the effect of the regularization on the gradient of the ELR loss (see Lemma~\ref{lemma:ELR_gradient}) for the same deep-learning model as in Figure~\ref{fig:CE_vs_ELR}. On the left, we plot the entry of  corresponding to the true class, denoted by , for training examples with clean (blue) and wrong (red) labels. The center image shows the th entry of the cross-entropy (CE) term  (dark blue) and the regularization term  (light blue) separately for the examples with clean labels. During early learning the CE term dominates, but afterwards it vanishes as the model learns the clean labels (i.e. ). However, the regularization term compensates for this, forcing the model to continue learning mainly on the examples with clean labels. On the right, we show the CE and the regularization term (dark and light red respectively) separately for the examples with wrong labels. The regularization cancels out the CE term, preventing memorization. In all plots the curves represent the mean value, and the shaded regions are within one standard deviation of the mean.
    }
    \label{fig:Gradient_simple}
\end{figure}

In words, the sign of  is determined by a weighted combination of the difference between  and the rest of the entries in the target. 

If  is the true class, then the th entry of  tends to be dominant during early-learning. In that case, the th entry of  is negative.
This is useful both for examples with clean labels and for those with wrong labels.
For examples with clean labels, the cross-entropy term  tends to vanish after the early-learning stage because  is very close to , allowing examples with wrong labels to dominate the gradient. Adding  counteracts this effect by ensuring that the magnitudes of the coefficients on examples with clean labels remains large. The center image of Figure~\ref{fig:Gradient_simple} shows this effect. For examples with wrong labels, the cross entropy term  is positive because . Adding the negative term  therefore dampens the coefficients on these mislabeled examples, thereby diminishing their effect on the gradient (see right image in Figure~\ref{fig:Gradient_simple}). Thus, ELR fulfils the two desired properties outlined at the end of Section~\ref{sec:linear}: boosting the gradient of examples with clean labels, and neutralizing the gradient of the examples with false labels.  



\subsection{Target estimation}
\label{sec:targets}
ELR requires a target probability for each example in the training set. The target can be set equal to the model output, but using a running average is more effective. In semi-supervised learning, this technique is known as temporal ensembling~\citep{laine2016temporal}. Let  and  denote the target and model output respectively for example  at iteration  of training. We set

where  is the momentum. The basic version of our proposed method alternates between computing the targets and minimizing the cost function~\eqref{eq:ELR} via stochastic gradient descent.


Target estimation can be further improved in two ways. First, by using the output of a model obtained through a running average of the model weights during training. In semi-supervised learning, this \emph{weight averaging} approach has been proposed to mitigate confirmation bias~\citep{tarvainen2017mean}. Second, by using two separate neural networks, where the target of each network is computed from the output of the other network. The approach is inspired by Co-teaching and related methods~\citep{Han2018CoteachingRT,Yu2019HowDD,li2020dividemix}. The ablation results in Section~\ref{sec:results} show that weight averaging, two networks, and mixup data augmentation~\citep{zhang2017mixup} all separately improve performance. We call the combination of all these elements ELR+. A detailed description of ELR and ELR+ is provided in Section~\ref{sec:algorithms} of the supplementary material. 



\begin{table}
\footnotesize
\begin{center}

\begin{threeparttable}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|cccc|cccc}
\toprule
\multirow{2}{*}{\shortstack{Datasets\ResNet34)}}  & Cross entropy & 86.98  0.12 & 81.88  0.29& 74.14  0.56 & 53.82  1.04 & 90.69  0.17 & 88.59  0.34 & 86.14  0.40 & 80.11  1.44 \\
& Bootstrap~\cite{Reed2015TrainingDN}& 86.23  0.23 & 82.23  0.37  & 75.12  0.56 & 54.12  1.32 & 90.32  0.21 & 88.26  0.24 & 86.57  0.35 & 81.21  1.47\\
 & Forward~\cite{patrini2017making}& 87.99  0.36 & 83.25  0.38 & 74.96  0.65 & 54.64  0.44 & 90.52  0.26 & 89.09  0.47 & 86.79  0.36& 83.55  0.58\\
  & GSE~\cite{zhang2018generalized}& 89.83  0.20 & 87.13  0.22 & 82.54  0.23 & 64.07  1.38 & 90.91  0.22 & 89.33  0.17 & 85.45  0.74 & 76.74  0.61\\
& SL~\cite{Wang2019SymmetricCE} & 89.83  0.32 & 87.13  0.26 & 82.81  0.61 & 68.12  0.81 & 91.72  0.31 & 90.44  0.27 & 88.48  0.46 & 82.51  0.45\\
 & ELR & \textbf{91.16  0.08} & \textbf{89.15  0.17 } & \textbf{86.12  0.49} & \textbf{73.86  0.61}  &\textbf{93.27  0.11} & \textbf{93.52  0.23} & \textbf{91.89  0.22} & \textbf{90.12  0.47} \\
  & ELR\tnote{} & \textbf{ 92.12  0.35} & \textbf{91.43  0.21} & \textbf{88.87  0.24} & \textbf{80.69  0.57}& \textbf{94.57  0.23} & \textbf{93.28  0.19} & \textbf{92.70  0.41} & \textbf{90.35  0.38} \\
\midrule
\multirow{7}{*}{\shortstack{CIFAR100\
    \pmlabels^{[i]} = \left\{\begin{array}{ll}
    1 & \text{ if } \\
    -1 & \text{ if }\,,
    \end{array}\right.

    \mathcal L_\text{CE}(\theta) = \frac 1n \sum_{i=1}^n  \log(1 + e^{-\pmlabels^{[i]}\theta^\top \vx^{[i]}})\,.

\nabla \mathcal{L}_\text{CE}(\theta) & = \frac{1}{2n}\sum_{i=1}^n \vx^{[i]} \left(\tanh(\theta^\top \vx^{[i]}) - \pmlabels^{[i]}\right), 

- \nabla \mathcal L_{\text{CE}}(\theta_t)^\top \vv/\|\nabla \mathcal L_{\text{CE}}(\theta_t)\| \geq 1/6\,.

- \nabla \mathcal L_{\text{CE}}(\theta_t) = \frac{1}{2n}\sum_{i=1}^n \pmlabels^{[i]} \vx^{[i]} - \frac{1}{2n}\sum_{i=1}^n \vx^{[i]} \tanh(\theta_t^\top \vx^{[i]})\,.

\vv^\top \Big(\frac{1}{2n}\sum_{i=1}^n \pmlabels^{[i]} \vx^{[i]}\Big) = \frac 12 (1 - \Delta) + o_P(1)\,.

\Big|\vv^\top\Big(\frac{1}{2n}\sum_{i=1}^n \vx^{[i]} \tanh(\theta_t^\top \vx^{[i]})\Big)\Big| &\leq \frac{1}{2} \Big(\frac{1}{n} \sum_{i=1}^n (\vv^\top \vx^{[i]})^2 \Big)^{1/2}\left(\frac{1}{n} \sum_{i=1}^n \tanh(\theta_t^\top \vx^{[i]})^2\right)^{1/2} \\
& \leq \frac 12 |\tanh(\theta_t^\top \vv)| + c\sigma(1 + \|\theta_t - \theta_0\|)\,.
\label{eq:recursive_bound}
- \nabla \mathcal L_{\text{CE}}(\theta_t)^\top v/\|\nabla \mathcal L_{\text{CE}}(\theta_t)\| \geq \frac 12 ( (1 - \Delta) - |\tanh(\theta_t^\top \vv)|) - c\sigma(1 + \|\theta_t - \theta_0\|)\,.

\theta_t - \theta_0 = \eta\sum_{s = 0}^{t-1} \mathbf g_s\,,

(\theta_t - \theta_0)^\top \vv/\|\theta_t - \theta_0\| \geq 1/6\,

    \frac 12 ( (1 - \Delta) - .1) - 2 c \sigma\,.

\hat{\mathcal A}(\theta_t) = \frac{1}{|\wrong|} \sum_{i \in \wrong} \1\{\mathrm{sign}(\theta_t^\top \vx^{[i]}) = (\pmlabelstrue)^{[i]}\}

    \hat{\mathcal A}(\theta_0) \leq .5001

    \hat{\mathcal A}(\theta_T) > .9999

\E[\1\{\mathrm{sign}(\theta_0^\top \vx^{[i]}) = (\pmlabelstrue)^{[i]}\}| \theta_0] = \PP[\sigma \theta_0^\top \vz^{[i]} < \theta_0^\top \vv | \theta_0] \leq 1/2 + O(|\theta_0^\top \vv|/\sigma)\,.

\hat{\mathcal A}(\theta_0) \leq 1/2 + O(|\theta_0^\top \vv|/\sigma) + o_P(1)\,,

    \E[\1\{\mathrm{sign}(\theta^\top \vx^{[i]}) = (\pmlabelstrue)^{[i]}\}] = \PP[\sigma \theta^\top \vz^{[i]} < \theta^\top \vv] = \Phi(\sigma^{-1} \theta^\top \vv/\|\theta\|)
\label{eq:small_ball_1}
    \hat{\mathcal{A}}(\theta) \geq \Phi(\sigma^{-1}\tau - \delta) - \frac{1}{|\wrong|} \sum_{i \in \wrong} \Phi(\sigma^{-1}\tau - \delta) - \1\{\theta^\top \vz^{[i]}/\|\theta\| < \sigma^{-1} \tau\}

    \phi(x) := \left\{\begin{array}{ll}
    1 & \text{if } \\
    \frac 1 \delta (\sigma^{-1} \tau - x) & \text{if } \\
    0 & \text{if .}
    \end{array}\right.

    \1\{x < \sigma^{-1} \tau - \delta\} \leq \phi(x) \leq \1\{x < \sigma^{-1} \tau\}

    \Phi(\sigma^{-1}\tau - \delta) - \1\{\theta^\top \vz^{[i]}/\|\theta\| < \sigma^{-1} \tau\} \leq \E [\phi(\theta^\top \vz^{[i]}/\|\theta\|)] - \phi(\theta^\top \vz^{[i]}/\|\theta\|)\,.

    \E \inf_{\theta \in \mathcal{C}_\tau} \hat{\mathcal{A}}(\theta) \geq \Phi(\sigma^{-1}\tau - \delta) - \E \sup_{\theta \in \mathcal{C}_\tau} \frac{1}{|\wrong|} \sum_{i \in \wrong} \E [\phi(\theta^\top \vz^{[i]}/\|\theta\|)] - \phi(\theta^\top \vz^{[i]}/\|\theta\|)\,.

    \E \sup_{\theta \in \mathcal{C}_\tau} \frac{1}{|\wrong|} \sum_{i \in \wrong} \E [\phi(\theta^\top \vz^{[i]}/\|\theta\|)] - \phi(\theta^\top \vz^{[i]}/\|\theta\|) & \leq \E \sup_{\theta \in \mathcal{C}_\tau} \frac{1}{|\wrong|} \sum_{i \in \wrong} \epsilon_i \phi(\theta^\top \vz^{[i]}/\|\theta\|) \\
    & \leq \frac 1 \delta \E \sup_{\theta \in \mathcal{C}_\tau} \frac{1}{|\wrong|} \sum_{i \in \wrong} \epsilon_i \theta^\top \vz^{[i]}/\|\theta\| \\
    & \leq \frac 1 \delta \E \sup_{\theta \in \R^p} \frac{1}{|\wrong|} \sum_{i \in \wrong} \epsilon_i \theta^\top \vz^{[i]}/\|\theta\| \\
    & = \frac 1 \delta \E \left\|\frac{1}{|\wrong|} \sum_{i \in \wrong} \epsilon_i \vz^{[i]}\right\|\,.

    \E \inf_{\theta \in \mathcal C_\tau} \hat{\mathcal A}(\theta) \geq \Phi(\sigma^{-1} \tau - \delta) - \frac 1 \delta \sqrt{p/|\wrong|}\,,

    \hat{\mathcal A}(\theta_T) \geq \Phi((50\sigma)^{-1} - \delta) - c_\Delta/\delta\,.

\frac{1}{|\correct|}\sum_{i \in \correct} (\coeff^{[i]}(\theta_T))^2 &< \frac{1}{|\correct|} \sum_{i \in \correct} (\coeff^{[i]}(\theta_0))^2 - .05\\
\frac{1}{|W|}\sum_{i \in W} (\coeff^{[i]}(\theta_T))^2 &> \frac{1}{|W|}\sum_{i \in W} (\coeff^{[i]}(\theta_0))^2 + .05 \,.

\frac{1}{|\correct|} \sum_{i \in \correct} (\tanh(\theta_0^\top \vx^{[i]}) - \pmlabels^{[i]})^2

\E_{\vx, \pmlabels} (\pmlabels\tanh(\theta_0^\top \vx) - 1)^2 \geq \left(\E_{\vx, \pmlabels}\pmlabels\tanh(\theta_0^\top \vx) - 1\right)^2\,.

\E_{\vx, \pmlabels}\pmlabels\tanh(\theta_0^\top \vx) \leq \E_{\vx, \pmlabels}\pmlabels\tanh(\pmlabelstrue \sigma \theta_0^\top \vz) + |\theta_0^\top \vv| = |\theta_0^\top \vv|\,,

\frac{1}{|\correct|} \sum_{i \in \correct} (\tanh(\theta_0^\top \vx^{[i]}) - \pmlabels^{[i]})^2 \geq 1 - o_P(1)\,.

    \left(\frac{1}{|C|} \sum_{i \in C} (\coeff^{[i]})^2 \right)^{1/2} & \leq \left(\frac{1}{|C|} \sum_{i \in C} ((\pmlabelstrue)^{[i]} \tanh(\theta_T^\top \vv) - \pmlabels^{[i]}) \right)^{1/2} + \sigma(2 + 3 c_\Delta) + o_P(1) \\
    &= |\tanh(\theta_T^\top \vv) - 1| + + \sigma(2 + 3 c_\Delta) + o_P(1) \\\
    & \leq |\tanh(.1) - 1| + \sigma(2 + 3 c_\Delta) + o_P(1)\,,

    \E \tanh(\theta_0^\top \vx)^2 \leq \E (\theta_0^\top \vx)^2 = 4 \sigma^2 + (\theta_0^\top \vv)^2\,,

\frac{1}{|\wrong|} \sum_{i \in \wrong} (\coeff^{[i]}(\theta_0))^2 \leq 1 + 4\sigma^2 + o_P(1)\,.

\left(\frac{1}{|\wrong|} \sum_{i \in \wrong} (\tanh(\theta_T^\top \vx^{[i]}) - \pmlabels^{[i]})^2\right)^{1/2} & \geq |\tanh(-\theta_T^\top \vv) - 1| - \sigma(2 + 3 c_\Delta) - o_P(1) \\
& \geq 1 + \tanh(.1) - \sigma(2 + 3 c_\Delta) - o_P(1)\,.

    1.05 + 4 \sigma_\Delta^2 < (1 + \tanh(.1) - \sigma_\Delta(2 + 3 c_\Delta))^2\,,

    C(n, p) := 2 \sum_{k = 0}^{p-1}\binom{n-1}{k}

    2^{n} - C(n, p) =  2 \sum_{k=p}^{n-1}\binom{n-1}{k} = 2 \sum_{k=0}^{n - p - 1} \binom{n-1}{k}

    \PP[S_+ = S | X] = (\Delta/2)^{|T_+ \triangle S|} (1-(\Delta/2))^{n - |T_+ \triangle S|}\,.

    \PP[S_+ \in B| X] & = \sum_{S \in B} (\Delta/2)^{|T_+ \triangle S|} (1-(\Delta/2))^{n - |T_+ \triangle S|} \nonumber \\
    & = \sum_{k = 0}^n |\{S \in B : |T_+ \triangle S| = k\}| \cdot (\Delta/2)^{k} (1-(\Delta/2))^{n - k} \label{eq:sum}\,.

    \max_{x_1, \dots x_n} \,\, & \sum_{k=0}^n x_k \cdot (\Delta/2)^{k} (1-(\Delta/2))^{n - k} \label{eq:prog}\\
    &\text{s.t.}\,\,\, x_k \in \left[0, \binom{n}{k}\right], \sum_{k=0}^n x_k = |B|\,.\nonumber

    2 \sum_{k=0}^{n-p} \binom{n}{k} (\Delta/2)^{k} (1-(\Delta/2))^{n - k} = 2 \cdot \PP[\mathrm{Bin}(n, \Delta/2) \leq (n-p)]\,.

    \PP[S_+ \in B | X] \leq 2 \cdot \PP[\mathrm{Bin}(n, \Delta/2) \leq (n-p)] = o(1)

|\theta_0^\top \vv| = o_P(1).

\sup_{\theta \in \R^d} \|\nabla \mathcal L_{\text{CE}}(\theta)\| \leq 1 + 2\sigma + o_P(1)\,.

\nabla \mathcal L_{\text{CE}}(\theta) = \frac{1}{\sqrt n} \sum_{i=1}^n \vx^{[i]}  \alpha_i \leq \left\|\frac{1}{\sqrt n}\mathbf X\right\|\,,

\left\|\frac{1}{\sqrt n}\mathbf X\right\| = \left\|\frac{1}{n}\mathbf X \mathbf X^\top \right\|^{1/2}\leq 1 + 2\sigma + o_P(1)\,.

\sup_{\theta: \|\theta - \theta_0\| \leq \tau} \left(\frac{1}{|I|} \sum_{i \in I} ((\pmlabelstrue)^{[i]}\tanh(\theta^\top \vx^{[i]}) - \tanh(\theta^\top \vv))^2\right)^{1/2} \leq \sigma(2 + c_\Delta \tau) + o_P(1)\,.

|(\pmlabelstrue)^{[i]}\tanh(\theta^\top \vx^{[i]}) - \tanh(\theta^\top \vv)| = |\tanh(\theta^\top \vv - \theta^\top \sigma \vz^{[i]}) - \tanh(\theta^\top \vv)| \leq \sigma |\theta^\top \vz^{[i]}|\,.

\Big(\frac{1}{|I|} \sum_{i \in I} ((\pmlabelstrue)^{[i]}\tanh(\theta^\top \vx^{[i]}) - \tanh(\theta^\top \vv))^2\Big)^{1/2} & \leq \sigma \Big(\frac{1}{|I|} \sum_{i \in I} (\theta^\top \vz^{[i]})^2\Big)^{1/2} \\
& \leq \sigma \Big(\frac{1}{|I|} \sum_{i \in I} (\theta_0^\top \vz^{[i]})^2\Big)^{1/2} + \sigma \Big(\frac{1}{|I|} \sum_{i \in I} ((\theta-\theta_0)^\top \vz^{[i]})^2\Big)^{1/2} \\
& \leq \sigma \Big(\frac{1}{|I|} \sum_{i \in I} (\theta_0^\top \vz^{[i]})^2\Big)^{1/2} + \sigma \|\theta - \theta_0\| \left\|\frac{1}{|I|} \sum_{i \in I} \vz^{[i]}(\vz^{[i]})^\top\right\|\,.

\frac{1}{|I|} \sum_{i \in I} (\theta_0^\top \vz^{[i]})^2 & \leq 2 + o_P(1)\\
\left\|\frac{1}{|I|} \sum_{i \in I} \vz^{[i]}(\vz^{[i]})^\top\right\|^{1/2} & \leq c_\Delta + o_P(1) \\
\left\|\frac{1}{|I|} \sum_{i \in I} \vx^{[i]}(\vx^{[i]})^\top\right\|^{1/2} & \leq 1 + \sigma c_\Delta + o_P(1)\,.

    \left\|\frac{1}{n} \sum_{i \in [n]} \vz^{[i]}(\vz^{[i]})^\top\right\|^{1/2} = \left\|\frac 1n \mathbf Z \mathbf Z^{\top}\right\|^{1/2} = \left\|\frac 1{\sqrt n} \mathbf Z\right\| \leq 1 + \sqrt{p/n} + o_P(1)\,,

    \left\|\frac{1}{|I|} \sum_{i \in I} \vz^{[i]}(\vz^{[i]})^\top\right\|^{1/2} \leq 2\sqrt{n/|I|} + o_P(1)\,.

    \mathbf X = \vv (\pmlabelstrue)^{\top} + \sigma \mathbf Z\,,

    \left\|\frac{1}{n} \sum_{i \in [n]} \vx^{[i]}(\vx^{[i]})^\top\right\|^{1/2} = \left\|\frac 1{\sqrt n}\mathbf X \right\| \leq \frac 1{\sqrt n} \left\|\vv (\pmlabelstrue)^{\top}\right\| + \sigma \left\|\frac 1{\sqrt n} \mathbf Z\right\| \leq 1 + 2 \sigma + o_P(1)\,.
 
\mathcal{L}_\text{CE}(\param) -  \frac{ \lambda}{n}\sum_{i=1}^n\sum_{c=1}^{C} \vq^{[i]}_c \log \vp^{[i]}_c. \label{eq:kl_loss}

    \frac{1}{n}\sum_{i=1}^n \nabla \func_{\vx^{[i]}}(\param) \left(\left(  \vp^{[i]} - \vy^{[i]}\right)+\lambda \left(  \vp^{[i]} - \vq^{[i]}\right) \right).
 
\mathcal{R}(\param) := \log \left(1-\langle \vp,\vq \rangle\right). 

    \nabla \mathcal{R}(\param) & = \frac{1}{1-\langle \vp,\vq \rangle} \nabla \left(1-\langle \vp,\vq \rangle\right).
    \label{eq:reg_gradient}

  \nabla \mathcal{R}(\param)  & = \sum_{i=1}^n\frac{1}{1-\langle \vp,\vq \rangle} \nabla \left(1-\frac{\langle e^{\func_{\vx}(\param)},\vq \rangle}{\sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c}}\right)\\
    &=\sum_{i=1}^n\frac{-1}{1-\langle \vp,\vq \rangle} \frac{\nabla\langle e^{\func_{\vx}(\param)},\vq \rangle \cdot \sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c} - \langle e^{\func_{\vx}(\param)},\vq \rangle\cdot \nabla  
    \sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c}
    }{\left(\sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c}\right)^2}\\
    & =\sum_{i=1}^n\frac{-\nabla \func_{\vx}(\param)}{1-\langle \vp,\vq \rangle} 
    \frac{ e^{\func_{\vx}(\param)}\odot \vq \cdot \sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c} - \langle e^{\func_{\vx}(\param)},\vq \rangle\cdot 
    e^{\func_{\vx}(\param)}
    }{\left(\sum_{c=1}^C
    e^{\left(\func_{\vx}(\param)\right)_c}\right)^2}\\
    & = \sum_{i=1}^n\frac{-\nabla \func_{\vx}(\param)}{1-\langle \vp,\vq \rangle}\left(
    \frac{e^{\func_{\vx}(\param)}\odot \vq}{\sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c}} - \frac{\langle e^{\func_{\vx}(\param)},\vq \rangle}{\sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c}}\cdot  \frac{e^{\func_{\vx}(\param)}}{\sum_{c=1}^C e^{\left(\func_{\vx}(\param)\right)_c}}\right).

     \nabla \mathcal{R}(\param) & = \frac{-\nabla \func_{\vx}(\param)}{1-\langle \vp,\vq \rangle}\left(\vp\odot \vq - \langle\vp,\vq\rangle\cdot \vp \right)\\
& =   \frac{\nabla \func_{\vx}(\param)}{1-\langle \vp,\vq \rangle} \begin{bmatrix} \vp_1 \cdot \left( \langle\vp,\vq\rangle - \vq_1\right)\\ \vdots \\ \vp_C\cdot \left( \langle\vp,\vq\rangle - \vq_C\right) \end{bmatrix}\\
    & =   \frac{\nabla \func_{\vx}(\param)}{1-\langle \vp,\vq \rangle} \begin{bmatrix} \vp_1 \cdot \sum_{k=1}^C\left(\vq_k-\vq_1\right)\vp_k\\ \vdots \\  \vp_C \cdot \sum_{k=1}^C\left(\vq_k-\vq_C\right)\vp_k \end{bmatrix}.

        \ell &\sim \text{Beta}(\alpha,\alpha),\\
        \ell' &= \max (\ell, 1-\ell),\\
        \tilde{\vx}^{[i]} &= \ell' \vx^{[i]} + (1-\ell')\vx^{[j]},\\
        \tilde{\vy}^{[i]} &= \ell' {\vy}^{[i]} + (1-\ell'){\vy}^{[j]},\\
        \tilde{\vq}^{[i]} &= \ell' {\vq}^{[i]} + (1-\ell'){\vq}^{[j]},
    
where  is a fixed hyperparameter used to choose the symmetric beta distribution from which we sample the ratio of the convex combination between data points.
    

\section{Description of the Computational Experiments}
Source code for the experiments is available at \url{https://github.com/shengliu66/ELR}.
\label{sec:experiments_appendix}
\begin{table}[t]
\footnotesize
\begin{center}
\begin{tabular}{c|c|c|c|c|c}
\toprule
Data set & Train & Val & Test & Image size & \# classes \\
\midrule
\multicolumn{6}{c}{Datasets with Clean Annotation}\\
\midrule
CIFAR-10 & 45K & 5k& 10K &  & 10 \\
CIFAR-100 & 45K & 5k & 10K &  & 100 \\
\midrule
\multicolumn{6}{c}{Datasets with Real World Noisy Annotation}\\
\midrule
Clothing-1M & 1M & 14K &10K &  & 14\\
Webvision1.0 & 66K & - &2.5K &  & 50\\
\bottomrule
\end{tabular}
\end{center}
\caption{Description of the datasets used in our computational experiments, including the training, validation and test splits.}
\label{tab:data_describ}
\end{table} 
\subsection{Dataset Information}
In our experiments we apply ELR and ELR+ to perform image classification on four benchmark datasets: CIFAR-10, CIFAR-100, Clothing-1M, and a subset of WebVision. Because CIFAR-10, CIFAR-100 do not have predefined validation sets, we retain 10\% of the training sets to perform validation. Table~\ref{tab:data_describ} provides a detailed description of each dataset.



\subsection{Data preprocessing}
We apply normalization and simple data augmentation techniques (random crop and horizontal flip) on the training sets of all datasets. The size of the random crop is set to be consistent with previous works~\cite{zhang2018generalized, Jiang2018MentorNetLD}:  for the CIFAR datasets,  for Clothing1M (after resizing to ), and  for WebVision.

\subsection{Training Procedure}
Below we describe the training procedure for ELR (i.e. the proposed approach with temporal ensembling) for the different datasets. The information for ELR+ is shown in Table~\ref{tab:ELR_plus_hyperparameters}. In ELR+ we ensemble the outputs of two networks during inference, as is customary for  methods that train two networks simultaneously~\cite{li2020dividemix,Han2018CoteachingRT}.

\textbf{CIFAR-10/CIFAR-100}: We use a ResNet-34~\cite{he2016deep} and train it using SGD with a momentum of 0.9, a weight decay of , and a batch size of . The network is trained for  epochs for CIFAR-10 and  epochs for CIFAR-100. We set the initial learning rate as 0.02, and reduce it by a factor of 100 after 40 and 80 epochs for CIFAR-10 and after 80 and 120 epochs for CIFAR-100. We also experiment with cosine annealing learning rate~\cite{Loshchilov2017SGDRSG} where the maximum number of epoch for each period is set to , the maximum and minimum learning rate is set to  and  respectively, total epoch is set to 150. 

\textbf{Clothing-1M}: We use a ResNet-50 pretrained on ImageNet same as Refs.~\cite{Wang2019SymmetricCE,xiao2015learning}. The model is trained with batch size 64 and initial learning rate 0.001, which is reduced by  after 5 epochs (10 epochs in total). The optimization is done using SGD with a momentum 0.9, and weight decay . For each epoch, we sample 2000 mini-batches from the training data ensuring that the classes of the noisy labels are balanced. 


\textbf{WebVision}: Following Refs.~\cite{Jiang2018MentorNetLD, li2020dividemix}, we use an InceptionResNetV2 as the backbone architecture. All other optimization details are the same as for CIFAR-10, except for the weight decay () and the batch size ().





\subsection{Hyperparameters selection\label{sec:hyperparameters_select}}
 We perform hyperparameter tuning on the CIFAR datasets via grid search: the temporal ensembling parameter  is chosen from  and the regularization coefficient  is chosen from  using the validation set. The selected values are  and  for symmetric noise,  and  for assymetric noise on CIFAR-10, and  and  CIFAR-100. For Clothing1M and WebVision we use the same values as for CIFAR-10. As shown in Section~\ref{sec:hyperparameters}, the performance of the proposed method seems to be robust to changes in the hyperparameters. For ELR+, we use the same values for  and . The   is set to 1 (chosen from  via grid search on the validation set) and the value of the weight averaging parameter  is set to  (which is the default value in the public code of Ref.~\cite{tarvainen2017mean}) except Clothing1M, which is set to 0.9999.

\begin{table}[t]
\footnotesize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\toprule
 & CIFAR-10 & CIFAR-100 & Clothing-1M & Webvision\\
\midrule

batch size & 128 & 128 & 64 & 32\\
architecture & PreActResNet-18 & PreActResNet-18 & ResNet-50 (pretrained) & InceptionResNetV2\\
training epochs & 200 & 250 & 15 & 100\\
learning rate (lr) & 0.02 & 0.02&  0.002 & 0.02\\
lr scheduler & divide 10 at 150th epoch & divide 10 at 200th epoch & divide 10 at 7th epoch & divide 10 at 50th epoch\\
weight decay & 5e-4  & 5e-4 & 1e-3 & 5e-4\\
\bottomrule
\end{tabular}}
\end{center}
\label{tab:ELR_plus_hyperparameters}
\caption{Training hyperparameters for ELR+ on CIFAR-10, Clothing-1M and Webvision.}
\end{table}



 
\section{Sensitivity to Hyperparameters \label{sec:hyperparameters}}
The main hyperparameters of ELR are the temporal ensembling parameter  and regularization coefficient . As shown in the left image of Figure~\ref{fig:ablation_hyperparameters}, performance is robust to the value of , although it is worth noting that this is only as long as the momentum of the moving average is large. The performance degrades to 38\% when the model outputs are used to estimate the target without averaging (i.e. ). The regularization parameter  needs to be large enough to neutralize the gradients of the falsely labeled examples but also cannot be too large, to avoid neglecting the cross entropy term in the loss. As shown in the center image of Figure~\ref{fig:ablation_hyperparameters}, the sensitivity to   is also quite mild. Finally, the right image of Figure~\ref{fig:ablation_hyperparameters} shows results for ELR combined with mixup data augmentation for different values of the mixup parameter . Performance is again quite robust, unless the parameter becomes very large, resulting in a peaked distribution that produces too much mixing.




\begin{figure}[t]
\resizebox{\linewidth}{!}{
    \begin{tabular}{>{\centering\arraybackslash}m{0.33\linewidth} >{\centering\arraybackslash}m{0.33\linewidth} >{\centering\arraybackslash}m{0.33\linewidth}}
    {\small \; Temporal ensembling momentum } & {\small \quad Regularization coefficient } & {\small mixup parameter }\\
    \includegraphics[width=1.1\linewidth]{images/ablation_agg.png} &
     \includegraphics[width=\linewidth]{images/ablation_coef.png} &{\vspace{3.5mm}\includegraphics[width=\linewidth]{images/ablation_mixup.png}}
     
    
  \end{tabular}
}
    \caption{Test accuracy on CIFAR-10 with symmetric noise level 60\%. The mean accuracy over four runs is reported, along with bars representing one standard deviation from the mean. In each experiment, the rest of hyperparameters are fixed to the values reported in Section~\ref{sec:hyperparameters_select}.}
    \label{fig:ablation_hyperparameters}
\end{figure}

\section{Training Time Analysis}
In Table~\ref{tab:training_time} we compare the training times of ELR and ELR+ with two state-of-the-art methods, using a single Nvidia v100 GPU. ELR+ is twice as slow as ELR. DivideMix takes more than 2 times longer than ELR+ to train. Co-teaching+ is about twice as slow as ELR+.  
\begin{table}[h]
\footnotesize
\begin{center}
\begin{tabular}{c| c|c|c|c}
\toprule
Co-teaching+\cite{Yu2019HowDD} & DivideMix\cite{li2020dividemix} & ELR & ELR+\\
\midrule
4.4h & 5.4h & 1.1h & 2.3h\\
\bottomrule
\end{tabular}
\end{center}
\caption{Comparison of total training time in hours on CIFAR-10 with 40\% symmetric label noise.}
\label{tab:training_time}
\end{table} 



\end{document}
