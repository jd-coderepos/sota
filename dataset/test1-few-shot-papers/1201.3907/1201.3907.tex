

\let\accentvec\vec
\documentclass[runningheads,a4paper]{llncs}
\let\llncsvec\vec
\let\vec\accentvec

\overfullrule3pt

\setcounter{tocdepth}{3}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[all]{xy}
\let\xyhole\hole
\let\hole\undefined

\usepackage{url}
\usepackage{betaneed}


\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\renewcommand{\labelitemi}{}

\begin{document}

\mainmatter  
\title{The Call-by-need Lambda Calculus, Revisited}

\titlerunning{The Call-by-need Lambda Calculus, Revisited}

\author{Stephen Chang \and Matthias Felleisen
}
\authorrunning{Stephen Chang \and Matthias Felleisen}

\institute{College of Computer Science\\
           Northeastern University\\
           Boston, Massachusetts, USA\\
\texttt{\{ stchang  matthias \} @ ccs.neu.edu}
}


\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle

\begin{abstract}
The existing call-by-need \lcs describe lazy evaluation via equational
 logics. A programmer can use these logics to safely ascertain whether one term
 is behaviorally equivalent to another or to determine the value of a lazy
 program. However, neither of the existing calculi models evaluation in a way
 that matches lazy implementations.
 
\hspace{10pt} Both calculi suffer from the same two problems. First, the
 calculi never discard function calls, even after they are completely
 resolved. Second, the calculi include re-association axioms even though these
 axioms are merely administrative steps with no counterpart in any
 implementation.

\hspace{10pt} In this paper, we present an alternative axiomatization of lazy
 evaluation using a single axiom. It eliminates both the function call
 retention problem and the extraneous re-association axioms. Our axiom uses a
 grammar of contexts to describe the exact notion of a \emph{needed
   computation}. Like its predecessors, our new calculus satisfies consistency
 and standardization properties and is thus suitable for reasoning about
 behavioral equivalence. In addition, we establish a correspondence between
 our semantics and Launchbury's natural semantics.

\keywords{call-by-need, laziness, lambda calculus}
\end{abstract}

\section{A Short History of the  Calculus}

Starting in the late 1950s, programming language researchers began to look
 to Church's \lc~\cite{Church41:CalcOfLambdaConv} for inspiration. Some
 used it as an analytic tool to understand the syntax and semantics of
 programming languages, while others exploited it as the basis for new 
 languages. By 1970, however, a disconnect had emerged in the form of
 call-by-value programming, distinct from the notion of  and
 normalization in Church's original calculus. Plotkin~\cite{Plotkin1975LC}
 reconciled the \lc and Landin's SECD machine for the ISWIM language~\cite{pl:iswim} with
 the introduction of a notion of correspondence and with a proof that two
 distinct variants of the \lc corresponded to two distinct variants of the
 ISWIM programming language: one for call-by-value and one for
 call-by-name.

In the early 1970s, researchers proposed  call-by-need~\cite{Friedman1976Cons,Henderson1976Lazy,Wadsworth1971Thesis},
 a third kind of parameter passing mechanism
 that could be viewed as yet another variant of the ISWIM
 language. Call-by-need is supposed to represent the best of both worlds. While
 call-by-value ISWIM always evaluates the argument of a function, the
 call-by-name variant evaluates the argument every time it is needed. Hence, if
 an argument (or some portion) is never needed, call-by-name wins; otherwise
 call-by-value is superior because it avoids re-evaluation of
 arguments. Call-by-need initially proceeds like call-by-name, evaluating
 a function's body before the argument---until the value of the argument is
 needed; at that point, the argument is evaluated and the resulting value is
 used from then onward. In short, call-by-need evaluates an argument at
 most once, and only if needed.

Since then, researchers have explored a number of characterizations of
 call-by-need~\cite{Danvy2010Cbneed,Friedman2007LazyKrivine,Garcia2009Delimited,Josephs1989Lazy,Nakata2009Cbneed,PeytonJones1989Spineless,Purushothaman1992Lazy}.
 Concerning this paper, three stand out. Launchbury's
 semantics~\cite{Launchbury1993Natural} specifies the meaning of complete
 programs with a Kahn-style natural semantics. The call-by-need \lcs of Ariola
 and Felleisen~\cite{Ariola1994Cbneed,Ariola1997Cbneed,Ariola1995Cbneed}, and
 of Maraist, Odersky, and
 Wadler~\cite{Ariola1995Cbneed,Maraist1994Cbneed,Maraist1998Cbneed} are
 equational logics in the spirit of the \lc.

The appeal of the \lc has several reasons. First, a calculus is sound
 with respect to the observational (behavioral) equivalence
 relation~\cite{Morris1968Thesis}. It can therefore serve as the starting
 point for other, more powerful logics. Second, its axioms are rich
 enough to mimic machine evaluation, meaning programmers can reduce
 programs to values without thinking about implementation details. Finally,
 the \lc gives rise to a substantial
 meta-theory~\cite{Barendregt1985LC,CurryFeys1958Combinatory} from which
 researchers have generated useful and practical results for its cousins.

Unfortunately, neither of the existing by-need calculi model lazy evaluation
 in a way that matches lazy language implementations. Both calculi suffer from
 the same two problems. First, unlike the by-name and by-value calculi, the
 by-need calculi never discard function calls, even after the call is resolved
 and the argument is no longer needed. Lazy evaluation does require some
 accumulation of function calls due to the delayed evaluation of arguments but
 the existing calculi adopt the extreme solution of retaining every
 call. Indeed, the creators of the existing calculi acknowledge that a solution
 to this problem would strengthen their work but they could not figure out a
 proper solution.

Second, the calculi include re-association axioms even though these axioms
 have no counterpart in any implementation. The axioms are mere
 administrative steps, needed to construct -like redexes. Hence, they
 should not be considered computationally on par with other axioms.

In this paper, we overcome these problems with an alternative
 axiomatization. Based on a single axiom, it avoids the retention of 
 function calls and eliminates the extraneous re-association axioms. The single axiom
 uses a grammar of contexts to describe the exact notion of a \emph{needed
   computation}. Like its predecessors, our new calculus satisfies consistency
 and standardization properties and is thus suitable for reasoning about
 behavioral equivalence. In addition, we establish an intensional
 correspondence with Launchbury's semantics.
 
The second section of this paper recalls the two existing by-need calculi in
 some detail. The third section presents our new calculus, as well as a way to
 derive it from Ariola and Felleisen's calculus. Sections~\ref{sec:essential}
 and \ref{sec:correctness} show that our calculus satisfies the usual
 meta-theorems and that it is correct with respect to Launchbury's
 semantics. Finally, we discuss some possible extensions.
 
\section{The Original Call-by-need  Calculi}

The original call-by-need \lcs are independently due to two groups: Ariola and
 Felleisen~\cite{Ariola1994Cbneed,Ariola1997Cbneed} and Maraist, et
 al.~\cite{Maraist1994Cbneed,Maraist1998Cbneed}. They were jointly presented at
 POPL in 1995~\cite{Ariola1995Cbneed}. Both calculi use the standard set of
 terms as syntax:

 Our treatment of syntax employs the usual
 conventions, including Barendregt's standard hygiene condition
 for variable bindings~\cite{Barendregt1985LC}. Figure~\ref{fig:lneeds}
 specifies the calculus of Maraist et al., \lmow, and \laf,
 Ariola and Felleisen's variant. Nonterminals in some grammar productions have
 subscript tags to differentiate them from similar sets elsewhere in the
 paper. Unsubscripted definitions have the same denotation in all systems.

\newcommand{\smallmid}{\hspace{-.5pt}\mid\hspace{-0.5pt}}
\begin{figure}[htbp]
\vspace{-10pt}
\hspace{-14pt}\begin{minipage}[b]{0.49\linewidth}
  

\end{minipage}
\rule[-6pt]{.1pt}{4.5cm}
\hspace{-4pt}\begin{minipage}[b]{0.55\linewidth}
  

\end{minipage}
\vspace{-10pt}
  \caption{Existing call-by-need \lcs (left: \lmow, right: \laf)}
  \label{fig:lneeds}
\vspace{-10pt}
\end{figure}

In both calculi, the analog to the  axiom---also called a
 \textit{basic notion of reduction}~\cite{Barendregt1985LC}---replaces variable
 occurrences, one at a time, with the value 
 of the function's argument. Value substitution means that there is no
 duplication of work as far as argument evaluation is concerned. The function
 call is retained because additional variable occurrences in the function
 body may need the argument.
 Since function calls may accumulate, the calculi come with axioms
 that re-associate bindings to
 pair up functions with their arguments. For example,
 re-associating  in
 \laf exposes a \derefnr redex:
{

}

The two calculi differ from each other in their timing of variable
 replacements. The \lmow calculus allows the replacement of a variable with its
 value anywhere in the body of its binding . The \laf calculus
 replaces a variable with its argument only if evaluation of the function body
 needs it, where ``need'' is formalized via so-called evaluation contexts
 (). Thus evaluation contexts in \laf serve the double purpose of
 specifying demand for arguments and the standard reduction strategy. The term
  illustrates this difference between the two calculi.
 According to \lmow, the term is a \Vnr redex and reduces to
 , whereas in \laf, the term is irreducible because the
  occurs in an inner, unapplied , and is thus not ``needed.''

Also, \lmow is more lenient than \laf when it comes to
 re-associations. The \laf calculus re-associates the left or right
 hand side of an application only if it has been completely reduced to an answer,
 but \lmow permits re-association as soon as one nested function layer is
 revealed. In short, \lmow proves more equations than \laf, i.e.,
 .

In \laf, programs reduce to answers:
  
 In contrast, Maraist et al. introduce a ``garbage collection'' axiom into 
 \lmow to avoid answers and to use values instead. This suggests the
 following definition: 
  
 This turns out to be incorrect, however. Specifically, let  be
 the analogous call-by-name evaluator. Then  but
 . Examples such as  confirm
 the difference.

In recognition of this problem, Maraist et al.\ use Ariola and Felleisen's
 axioms and evaluation contexts to create their Curry-Feys-style standard
 reduction sequences. Doing so reveals the inconsistency of \lmow with 
 respect to Plotkin's \emph{correspondence criteria}~\cite{Plotkin1975LC}. 
 According to Plotkin, a useful calculus \emph{corresponds to} a
 programming language, meaning its axioms (1) satisfy the 
 Church-Rosser and Curry-Feys Standardization properties, and (2) define a 
 standard reduction function that is equal to
 the evaluation function of the programming language. Both the call-by-name and
 the call-by-value \lcs satisfy these criteria with respect to call-by-name and
 call-by-value SECD machines for ISWIM, respectively. So does \laf with respect
 to a call-by-need SECD machine, but some of \lmow's axioms
 cannot be used as standard reduction relations.

Finally, the inclusion of \Gnr is a brute-force attempt to address the
 function call retention problem. Because \Gnr may discard arguments even before
 the function is called, both sets of authors consider it too coarse and
 acknowledge that a tighter solution to the function call retention issue would
 ``strengthen the calculus and its utility for reasoning about the
 implementations of lazy languages''~\cite{Ariola1995Cbneed}.

\section{A New Call-by-need  Calculus}

Our new calculus, \lneed, uses a single axiom, \betaneednr. The new axiom evaluates the argument when it
 is first demanded, replaces all variable occurrences with that result, and
 then discards the argument and thus the function call. In addition, the
 axiom performs the required administrative scope adjustments as part of
 the same step, rendering explicit re-association axioms unnecessary. In short,
 every reduction step in our calculus represents computational progress.

Informally, to perform a reduction, three components must be identified:
\begin{enumerate}
  \item the next demanded variable,
  \item the function that binds that demanded variable,
  \item and the argument to that function.
\end{enumerate}
In previous by-need calculi the re-association axioms rewrite a term so that
 the binding function and its argument are adjacent.
 
Without the re-association axioms, finding the function that binds the
 demanded variable and its argument requires a different kind of work. The
 following terms show how the demanded variable, its binding function, and its
 argument can appear at seemingly arbitrary locations in a program:
\begin{itemize}
\item 
\item 
\item 
\end{itemize}
Our \betaneednr axiom employs a grammar of contexts to describe the path
 from a demanded variable to its binding function and from there to its
 argument.

The first subsection explains the syntax and the contexts of \lneed in a
 gradual fashion. The second subsection presents the \betaneednr axiom and also
 shows how to derive it from Ariola and Felleisen's \laf calculus.




\subsection{Contexts}
\label{subsec:newevalcontexts}

Like the existing by-need calculi, the syntax of our calculus is
 that of Church's original calculus. In \lneed, calculations evaluate terms 
 to answers , which generalize answers from Ariola and Felleisen's
 calculus:


Following Ariola and Felleisen, the basic axiom uses evaluation contexts to
 specify the notion of demand for variables:

 The first two kinds, taken from \laf, specify that a variable is
 demanded, and that a variable in the operator position of an application
 is demanded, respectively.  

Since the calculus is to model program evaluation, we are primarily interested
 in demanded variables under a -abstraction. This kind of evaluation
 context is defined using an answer context :
 
Using answer contexts, this third evaluation context dictates that demand
 exists under a  if a corresponding argument exists for that
 . Note how \textit{an answer context descends under the same number
   of s as arguments for those s.} In particular, for any term
 ,  is always the argument of . The
 third evaluation context thus generalizes the function-is-next-to-argument
 requirement found in both call-by-name and call-by-value. The generalization
 is needed due to the retention of function calls in \lneed.

 Here are some example answer contexts that might be used: 
-20pt]
  \A_0 &= \underbrace{\app{\lx\hole}{\e_x}} \\
  \A_1 &= \underbrace{\app{\lx\underbrace{\app{\ly\hole}{\e_y}}}{\e_x}} \\
  \A_2 &= \underbrace{\app{\lx\underbrace{\app{\ly\underbrace{\app{\lz\hole}{\e_z}}}{\e_y}}}{\e_x}}
\
An underbrace matches each function to its argument. The examples
 all juxtapose functions and their arguments. In contrast, the next two
 separate functions from their arguments:
-20pt]
 \A_3 &= (\underbrace{\lx\ly\lz\hole)\,\e_x}\,\e_y\,\e_z\-13pt]
 \phantom{\A_3}
   &\phantom{=\;(\lx\ly}\underbrace{\phantom{\lz\hole)\,\e_x\,\e_y\,\e_z}}\\
\A_4 &= (\lx(\underbrace{\ly\lz\hole)\,\e_y})\,\e_x\,\e_z \-13pt]
 \phantom{\A_4}
   &\phantom{=\;(\lx(\ly}\underbrace{\phantom{\lz z)\,\e_y)\,\e_x\,\e_z}} 
\



To summarize thus far, when a demanded variable is discovered under a
 , the surrounding context looks like this:

 where both the function binding  and its argument are in . The
 decomposition of the surrounding context into  and  assumes that 
 encompasses as many function-argument pairs as possible; in other words, it is
 impossible to merge the outer part of  with  to form a larger answer
 context.

To know which argument corresponds to the demanded variable, we must 
 find the  that binds  in . To this end, we 
 split answer contexts so that we can ``highlight'' a function-argument
 pair within the context:
 
 Using these additional contexts, any answer context can be decomposed into

 where  is the argument of .  For a fixed
 function-argument pair in an answer context, this partitioning into ,
 , and  is unique. The  subcontext represents the part of the
 answer context around the chosen function-argument pair; the  subcontext
 represents the part of the answer context in its body; and  here is the
 subcontext between the function and its argument. Naturally we must demand
 that  composed with  is an answer context as well so that the
 overall context remains an answer context. The following table lists the
 various subcontexts for the example  for various function-argument
 pairs:



Now we can define the fourth kind of evaluation context:

 This final evaluation context shows how demand shifts to an
 argument when a function parameter is in demand within the function body.

\subsection{The \betaneednr Axiom and a Derivation}
 
Figure~\ref{fig:lneed} summarizes the syntax of \lneed as developed in the
 preceding section.\footnote{We gratefully acknowledge Casey Klein's help
   with the  production.} In this section we use these
 definitions to formulate the  axiom for our calculus.

\begin{figure}[htbp]
\vspace{-22pt}

\vspace{-10pt}
  \caption{The syntax and contexts of the new call-by-need \lc, \lneed.} \label{fig:lneed}
\vspace{-10pt}
\end{figure}

Here is the single axiom of \lneed:
-20pt]
  \tag{\betaneednr} 
  \inAp{\ap{\inA[_1]{\lx\inAm{\Ex}}}{\Av[_2]}} &= 
  \inAp{\inA[_1]{\inA[_2]{\substx{\inAm{\Ex}}{\val}}}},\-20pt]

 &           &  \app{\app{\lx\app{\ly\underline{\lz \mathbf{z}\;y\;x}}{\ly y}}{\lx x}}{\underline{\lz z}} \\
 & \needstep & \app{\lx\boxedapp{\ly \app{\lz \innerdemand{z}}{\mathbf{y}\;x}}{\ly y}}{\lx x} \\
 & \needstep & \app{\lx\app{\boxedapp{\lz \mathbf{z}}{\ly y}}{x}}{\lx x}  \\
 & \needstep & \boxedapp{\lx\app{\ly \innerdemand{y}}{\mathbf{x}}}{\lx x} 

    \tag{Values}
      \val &= \lxe \\
    \tag{Answers}
      \ansaf &= \Aafv \\
    \tag{Answer Contexts}
      \Aaf   &= \hole \mid \app{\lx\Aaf}{\e} \\
    \tag{Evaluation Contexts}
      \Eaf &= \hole \mid \ap{\Eaf}{\e} \mid \inAaf{\Eaf}
                    \mid \app{\lxEafx}{\Eaf}
  
    \tag{\betaneednrr} \app{\lxEafx}{\val}                & =   \substx{\Eafx}{\val} \\
    \tag{\liftnr[']}   \app{\lx\Aafv}{\ap{\e_1}{\e_2}}    & =   \app{\lx\inAaf{\ap{\val}{\e_2}}}{\e_1} \\
    \tag{\assocnr[']}  \appp{\lxEafx}{\app{\ly\Aafv}{\e}} & =   \app{\ly\inAaf{\app{\lxEafx}{\val}}}{\e}
  
   \tag{\betaneednrr['']}
   \app{\lxEafx}{\Aafv} &= \inAaf{\substx{\Eafx}{\val}}

  \tag{\betaneednrr[''']}
    \ap{\inA[_1]{\lxEx}}{\Av[_2]} &= \inA[_1]{\inA[_2]{\substx{\Ex}{\val}}}

\evalneed{\e} = \texttt{done} \textrm{ iff there exists an answer } \ans
\textrm{ such that } \pmb{\lneed} \vdash \e =\ans
\inE{\e} \srstep \inE{\e'}, \textrm{ where } \e \;\betaneednr\; \e'\evalneedsr{\e} = \texttt{done} \textrm{ iff there exists an answer } 
		   \ans\textrm{ such that }\e\srsteps\ans
\tag{Contexts} \C = \hole \mid \lx{C} \mid \ap{\C}{\e} \mid \ap{\e}{\C}
\xymatrixcolsep{13mm}\xymatrixrowsep{6mm}\xymatrix{
\textrm{store machine} \hspace{-12mm}
    \ar@<12mm>[d]_*+{\buildLname}
    &
    \ar@{|->}[r] & 
    \ar@{|->}[r]^{:=} \ar@{-->}[dl] & 
    \ar@{|->}[r] \ar@{-->}[drr] & 
    \ar@{|->}[r] & \\
\lstep \hspace{-25mm} 
    &
    \ar@{|->}[rrrr]^{\betastep} & & & & \\
\textrm{CK transitions} \hspace{-12mm} 
    \ar@<-12mm>[u]^*+{\buildtostepname}
    \ar@<12mm>[d]_*+{\buildname}
    &
    \ar@{|->}[r] & 
    \ar@{|->}[r]^{go\;under\;\lambda} \ar@{-->}[ul]   & 
    \ar@{|->}[r] \ar@{-->}[urr]  & 
    \ar@{|->}[r]^{\betaneedck} \ar@{-->}[dlll] & \ar@{-->}[d] \\
\lneed \hspace{-25mm}  &
    \ar@{|->}[rrrr]^{\betaneednr} & & & & \\
  }\begin{array}{lrcl@{\ }r}
 \multicolumn{2}{l}{\underline{Transitions}}\\
& \ckh{\ap{\e_1}{\e_2}}{\FLs}{\Gamma}    &\ckhstep & \ckhp{\e_1}{\argFe[_2],\FLs}{\Gamma}                     & \mbox{(\pushargckh)} \\
    & \ckhp{\lxe_1}{\argFe[_2],\FLs}{\Gamma} &\ckhstep & \ckh{\substx{\e_1}{y}}{\FLs}{(\Gamma,\heapelt{y}{\e_2})}, 
      					     	       \ \fresh{y}                                            & \mbox{(\descendlamckh)}\\
& \ckh{x}{\FLs}{(\Gamma,x\mapsto\e)}     &\ckhstep & \ckhp{\e}{\varFx,\FLs}{\Gamma}                           & \mbox{(\lookupvarckh)}\\
& \ckhp{\val}{\varFx,\FLs}{\Gamma}	     &\ckhstep & \ckh{\val}{\FLs}{(\Gamma,x\mapsto\val)}                  & \mbox{(\updateheapckh)}
  \end{array}
\tag{Terms}
  \estep &= \e \mid \labxe \\
\tag{Values}
  \vstep &= \val \mid \labx{\vstep}\\
\tag{Evaluation Contexts}
  \Estep &= \hole \mid \ap{\Estep}{\estep} \mid \labx{\Estep}

  \appp{\lx\ap{x}{x}}{\ap{I}{I}} \stepstep 
  \ap{\labx{(\ap{I}{I})}}{\labx{(\ap{I}{I})}} \stepstep 
  \ap{\labx{I}}{\labx{I}} \stepstep I

 \begin{array}{ll}
   \inEstep{\app{\labvec{y}{(\lx\estep_1)}}{\estep_2}}\stepstep
   \begin{cases}
    \inEstep{\estep},     \hspace{1.1cm} \textrm{if \hole\ is not under a label in } \Estep \\
    \substlab{ \inEstep{\estep} }{ z }{ \inEstep[_2]{\estep} }, \textrm{ if } \inEstephole = \inEstep[_1]{\labz{(\inEstephole[_2])}}\\
       \multicolumn{2}{r}{\hspace{1.6cm}\textrm{and \hole\ is not under a label in } \Estep_2 }
   \end{cases} &								\hspace{12pt} (\betastep)\\
   \multicolumn{2}{c}{\textrm{where } \estep = \substx{\estep_1}{\lab{w}{\estep_2}}, w \textrm{ fresh}}
 \end{array}

\substlabxe{\labx{\estep_1}} &= \labxe \\
\substlabye{\labx{\estep_1}} &= 
    \labx{(\substlabye{\estep_1})},
        \; x \neq y  \\
\substlabxe{(\lx\estep_1)} &= \lx(\substlabxe{\estep_1}) \\
\substlabxe{\apppp{\estep_1}{\estep_2}} &=
    \apppp{\substlabxe{\estep_1}}
          {\substlabxe{\estep_2}}\\
\textrm{otherwise, }
    \substlabxe{\estep_1} &= \estep_1
  
\tag*{\boxed{\buildLname : \SL \rightarrow \estep}} \
The operation , using overloaded notation, replaces all free
 variables in  with their corresponding terms in  and tags them with
 appropriate labels.


Lemma~\ref{lem:ckh->lstep} demonstrates the bulk of the equivalence of the
 store machine and \lstep.\footnote{The lemma relies on an extension of the
   typical -equivalence classes of terms to include variables in labels
   as well.} The rest of the equivalence proof is
 straightforward~\cite{Felleisen2009Redex}.

\begin{lemma}
\label{lem:ckh->lstep}
  If , then either:
  \begin{enumerate}
  \item 
  \item 
  \end{enumerate}
\end{lemma}


\subsection{A Transition System for Comparing \lneed and \lstep} \label{subsec:ck}

The CK layer in figure~\ref{fig:correctness} mediates between \lstep and
 \lneed. The corresponding transition system resembles a two-register CK 
 machine~\cite{Felleisen2009Redex}. Figure~\ref{fig:ck}
 describes the syntax and the transitions of the system.\footnote{The CK
 transition system is a proof-technical device. Unlike the original CK
 machine, ours is ill-suited for an implementation.}
\begin{figure*}[htbp]




\caption{A transition system for comparing \lneed and \lstep.}  \label{fig:ck}
\end{figure*}

States consist of a subterm and a list of frames representing the context. The
 first kind of frame represents the argument in an application and the second
 frame represents a -abstraction with a hole in the body. The last
 kind of frame has two frame list components, the first representing a context
 in the body of the , and the second representing the context between
 the  and its argument. The variable in this last frame is the
 variable bound by the  expression under evaluation. The initial state
 for a program  is , where () is an empty list of frames, and
 evaluation terminates when the control string is a value and the list of
 frames is equivalent to an answer context.

The \pushargck transition makes the operator in an application the new control
 string and adds a new \texttt{arg} frame to the frame list containing the
 argument. The \descendlamck transition goes under a , making the body
 the control string, but only if that  has a corresponding argument in
 the frame list, as determined by the \balancename function, defined as follows:

 The \balancename side condition for \descendlamck dictates that evaluation
 goes under a  only if there is a matching argument for it, thus
 complying with the analogous evaluation context. The \balancename function
 employs \texttt{\#arg-frames} and \texttt{\#lam-frames} to count the number of \texttt{arg} or
 \texttt{lam} frames, respectively, in a list of frames. Their definitions are
 elementary and therefore omitted.

The \lookupvarck transition is invoked if the control string is a variable,
 somewhere in a  body, and the rest of the frames have a certain shape
 consistent with the corresponding parts of a \betaneednr redex. With this
 transition, the argument associated with the variable becomes the next control
 string and the context around the variable in the  body and the
 context between the  and argument are saved in a new \texttt{bod}
 frame. Finally, when an argument is an answer, indicated by a value control
 string and a \texttt{bod} frame in the frame list---with the equivalent of an
 answer context in between---the value gets substituted into the body of the
  according to the \betaneedck transition. The \betaneedck transition
 uses a substitution function on frame lists, , which
 overloads the notation for regular term substitution and has the expected
 definition.






\begin{figure}[tbhp]
\begin{minipage}[b]{0.44\linewidth}
  -7pt]
\tag*{\boxed{\buildFname : \Fs \rightarrow \E}}\\
    \buildF{()} &= \hole \\
    \buildF{\lxF,\Fs} &= \inhole{\buildF{\Fs}}{\lx\hole} \\
    \buildF{\argFe,\Fs} &= \inhole{\buildF{\Fs}}{\ap{\hole}{\e}} \\
    \buildF{\bodyFx{\Fs_1}{\Fs_2},\Fs} &=\\
    &\hspace{-2.2cm}\inhole{\buildF{\Fs}}
                  {\ap{\inhole{\buildF{\Fs_2}}
                             {\lx\inhole{\buildF{\Fs_1}}
                                        {x}}}
                     {\hole}}\
\end{minipage}
\rule{0.5pt}{5.8cm}
\begin{minipage}[b]{0.54\linewidth}
  -7pt]
\tag*{\boxed{\buildtosteppname : \Fs \times \estep \rightarrow \estep }}\\
    \buildtosteppe{\;} &= \estep \\
    \buildtostepp{\argF{\estep_1},\Fs}{\estep}
        &=
    \buildtosteppnop{\Fs}{\ap{\estep}{\estep_1}} \\
    \buildtostepp{\bodyFx{\Fs_1}{\Fs_2},\Fs}{\estep} &=\-29pt]
  
  \buildtosteppe{\lxF,\Fs_1,\argF{\estep_1},\Fs_2}  =\hspace{15mm}\\
    \buildtostepp{\Fs_1,\Fs_2}{\substx{\estep}{\laby{\estep_1}}}\-12pt]
  
  \texttt{cons} = \lx\ly\lm{s}\apthree{s}{x}{y}, \quad
  \texttt{car}  = \lm{p}\ap{p}{\lx\ly x}, \quad
  \texttt{cdr}  = \lm{p}\ap{p}{\lx\ly y}

Adding true algebraic systems should also be straightforward.

\bigskip

\noindent\textbf{Recursion} Our \lneed calculus represents just a core \lc and
 does not include an explicit \texttt{letrec} constructor for cyclic
 terms. Since cyclic programming is an important idiom in lazy programming
 languages, others have extensively explored cyclic by-need calculi,
 e.g., Ariola and Blum~\cite{Ariola1997Cyclic}, and applying their solutions to our
 calculus should pose no problems.

\section{Conclusion}
 
Following Plotkin's work on call-by-name and call-by-value, we present a
 call-by-need \lc that expresses computation via a single axiom in the spirit
 of . Our calculus is close to implementations of lazy languages because
 it captures the idea of by-need computation without retaining every function
 call and without need for re-associating terms. We show that our calculus
 satisfies Plotkin's criteria, including an intensional correspondence
 between our calculus and a Launchbury-style natural semantics. Our future work
 will leverage our \lneed calculus to derive a new abstract machine for lazy
 languages.

\paragraph{Acknowledgments} We thank J. Ian Johnson, Casey Klein, Vincent 
 St-Amour, Asumu Takikawa, Aaron Turon, Mitchell Wand, and the ESOP
 2012 reviewers for their feedback on early drafts. This work was supported in
 part by NSF Infrastructure grant CNS-0855140 and AFOSR grant FA9550-09-1-0110.

\bibliographystyle{splncs03}
\bibliography{betaneed}

\end{document}
