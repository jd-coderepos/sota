

\documentclass[11pt,a4paper]{article}
\PassOptionsToPackage{breaklinks}{hyperref}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{url}
\usepackage{multirow}
\usepackage{url}
\usepackage{arydshln}
\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand\omer[1]{[\textcolor{brown}{Omer: {#1}}]}
\newcommand\luke[1]{[\textcolor{red}{Luke: {#1}}]}
\newcommand\dan[1]{[\textcolor{green}{Dan: {#1}}]}
\newcommand\mandar[1]{[\textcolor{blue}{MJ: {#1}}]}
\title{BERT for Coreference Resolution: Baselines and Analysis}

\author{Mandar Joshi  \quad Omer Levy \quad Daniel S. Weld \quad Luke Zettlemoyer \4pt]
Allen Institute of Artificial Intelligence, Seattle\\
{\tt \{danw\}@allenai.org} \
    P(y) = \frac{e^{s(x, y)}}{\sum_{y' \in Y} e^{s(x, y')}}

    s(x, y) &= s_m(x) + s_m(y) + s_c(x, y) \\
    s_m(x) &= \text{FFNN}_m(\mathbf{g_x}) \\
    s_c(x, y) &= \text{FFNN}_c(\mathbf{g_x}, \mathbf{g_y}, \phi(x, y))

    \mathbf{f} &= \sigma(\mathbf{w}^T [\mathbf{r_1}; \mathbf{r_2}]) \\
    \mathbf{r} &= \mathbf{f} \cdot \mathbf{r_1} + (\mathbf{1} - \mathbf{f}) \cdot \mathbf{r_2}

where  is a trained parameter and  represents concatenation. This variant allows the model to artificially increase the context window beyond the \texttt{max\_segment\_len} hyperparameter.

All layers in both model variants are then fine-tuned following \citet{devlin2018bert}.  \section{Experiments}
\begin{table*}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccc@{\hspace{0.5cm}}ccc@{\hspace{0.5cm}}ccc@{\hspace{0.5cm}}c}
\toprule
 & \multicolumn{3}{c}{MUC}& \multicolumn{3}{c}{}& \multicolumn{3}{c}{} \\
 & P & R & F1 & P & R & F1 & P & R & F1 & Avg. F1 \\
 \midrule
\midrule
\citet{Martschat2015Latent}  & 76.7 & 68.1 & 72.2 & 66.1 & 54.2 & 59.6 & 59.5 & 52.3 & 55.7 & 62.5 \\
~\cite{Clark2015Entity}  & 76.1  &69.4 & 72.6  &65.6 & 56.0 & 60.4 & 59.4 & 53.0 & 56.0 & 63.0 \\
\cite{Wiseman2015Learning} & 76.2 & 69.3  & 72.6 & 66.2 & 55.8 & 60.5 & 59.4 & 54.9 & 57.1 & 63.4 \\
\citet{Wiseman2016Global} & 77.5 & 69.8 & 73.4 & 66.8 & 57.0 & 61.5 & 62.1 & 53.9 & 57.7 & 64.2 \\
\citet{clark2016corefRL} &  79.2 & 70.4 & 74.6 & 69.9 & 58.0 & 63.4 & 63.5 & 55.5 & 59.2 & 65.7 \\
e2e-coref \cite{lee2017end}  & 78.4 & 73.4 & 75.8 & 68.6 & 61.8 & 65.0 & 62.7& 59.0 &60.8 &67.2\\
c2f-coref \cite{lee2018higher}  & 81.4 & 79.5 & 80.4 & 72.2 & 69.5 & 70.8 & 68.2 & 67.1 & 67.6 & 73.0 \\
\citet{fei2019end} & {\bf 85.4} & {77.9} & 81.4 & {\bf 77.9} & 66.4 & 71.7 &  70.6 & 66.3 & 68.4 & 73.8 \\
EE \cite{kantor2019coreference} & 82.6 & {\bf 84.1} & 83.4 & 73.3 & {\bf 76.2} & 74.7 & 72.4 & 71.1 & 71.8 & 76.6 \\
\midrule
BERT-base + c2f-coref (independent) & 80.2 & {82.4} & 81.3 & 69.6 & 73.8 & 71.6 & 69.0 & 68.6 & 68.8 & 73.9\\
BERT-base + c2f-coref (overlap) & 80.4 & 82.3 & 81.4 & 69.6 & 73.8 & 71.7 & 69.0 & 68.5 & 68.8 & 73.9 \\
BERT-large + c2f-coref (independent) & 84.7 & {82.4} & {\bf 83.5} & 76.5 & { 74.0} & {\bf 75.3} & {\bf 74.1} & {\bf 69.8} & {\bf 71.9} & \textbf{76.9}\\
BERT-large + c2f-coref (overlap) & { 85.1} & 80.5 & 82.8 & { 77.5} & 70.9 & 74.1 & 73.8 & 69.3 & 71.5 & 76.1\\
\bottomrule
\end{tabular}
\caption{OntoNotes:  BERT improves the c2f-coref model on English by 0.9\% and 3.9\% respectively for base and large variants.
The main evaluation is the average F1 of three metrics -- MUC, , and  on the test set.}
\label{tab:ontonotes}
\end{table*}



We evaluate our BERT-based models on two benchmarks: the paragraph-level GAP dataset~\cite{webster2018gap}, and the document-level English OntoNotes 5.0 dataset~\cite{Pradhan2012Ontonotes}. 
OntoNotes examples are considerably longer and typically require multiple segments to read the entire document.

\paragraph{Implementation and Hyperparameters}
We extend the original Tensorflow implementations of c2f-coref\footnote{\url{http://github.com/kentonl/e2e-coref/}}  and BERT.\footnote{\url{https://github.com/google-research/bert}} We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3, and learning rates of  and  with linear  decay for the BERT parameters and the task parameters respectively. We found that this made a sizable impact of 2-3\% over using the same learning rate for all parameters. 

We  trained separate models with \texttt{max\_segment\_len} of 128, 256, 384, and 512; the models trained on 128 and 384 word pieces performed the best for BERT-base and BERT-large respectively.
As span representations are memory intensive, we truncate documents randomly to eleven segments for BERT-base and 3 for BERT-large during training. Likewise, we use a batch size of 1 document following ~\cite{lee2018higher}. While training the large model requires 32GB GPUs, all models can be tested on 16GB GPUs. We use the cased English variants in all our experiments.

\paragraph{Baselines}
We compare the c2f-coref + BERT system with two main baselines: (1) the original ELMo-based c2f-coref system \cite{lee2018higher}, and (2) its predecessor, e2e-coref \cite{lee2017end}, which does not use contextualized representations. 
In addition to being more computationally efficient than e2e-coref, c2f-coref iteratively refines span representations using attention for higher-order reasoning.

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccc}
\toprule
Model & M & F & B & O \\
\midrule
e2e-coref & 67.7 & 60.0 & 0.89 & 64.0\\

c2f-coref & 75.8 & 71.1 & 0.94 & 73.5 \\
BERT + RR ~\citet{LiuRef2019} & 80.3 & 77.4 & \textbf{0.96} & 78.8 \\
BERT-base + c2f-coref   & 84.4 & 81.2 & \textbf{0.96} & 82.8 \\
BERT-large + c2f-coref   & \textbf{86.9} & \textbf{83.0} & 0.95 & \textbf{85.0} \\
\bottomrule
\end{tabular}
\caption{GAP:  BERT improves the c2f-coref model by 11.5\%. The metrics are F1 score on \textbf{M}asculine and \textbf{F}eminine examples, \textbf{O}verall, and a \textbf{B}ias factor (F / M).}
\label{tab:gap_res}
\end{table}




\subsection{Paragraph Level: GAP}
GAP \cite{webster2018gap} is a human-labeled corpus of ambiguous pronoun-name pairs derived from Wikipedia snippets.
Examples in the GAP dataset fit within a single BERT segment, thus eliminating the need for cross-segment inference.
Following ~\citet{webster2018gap}, we trained our BERT-based c2f-coref model on OntoNotes.\footnote{This is motivated by the fact that GAP, with only 4,000 name-pronoun pairs in its dev set, is not intended for full-scale training.}
The predicted clusters were scored against GAP examples according to the official evaluation script. Table~\ref{tab:gap_res} shows that BERT improves c2f-coref by  9\% and 11.5\% for the base and large models respectively. 
These results are in line with large gains reported for a variety of semantic tasks by BERT-based models \cite{devlin2018bert}.



\subsection{Document Level: OntoNotes}
 OntoNotes (English) is  a document-level dataset from the CoNLL-2012 shared task on coreference resolution. It consists of about one million words of newswire, magazine articles, broadcast news, broadcast conversations, web data and conversational
speech data, and the New Testament. The main evaluation is the average F1 of three metrics -- MUC, , and  on the test set according to the official CoNLL-2012 evaluation scripts.


Table~\ref{tab:ontonotes} shows that BERT-base offers an improvement of 0.9\% over the ELMo-based c2f-coref model.   
Given how gains on coreference resolution have been hard to come by as evidenced by the table, this is still a considerable improvement. However, the magnitude of gains is relatively modest considering BERT's arguably better architecture and many more trainable parameters. This is in sharp contrast to how even the base variant of BERT has very substantially improved the state of the art in other tasks. BERT-large, however, improves c2f-coref by the much larger  margin of 3.9\%.
We also observe that the \emph{overlap} variant offers no improvement over \emph{independent}.

Concurrent with our work, ~\citet{kantor2019coreference}, who use higher-order entity-level representations over ``frozen'' BERT features, also report large gains over c2f-coref. While their feature-based approach is more memory efficient, the fine-tuned model seems to yield better results. Also concurrent, SpanBERT ~\cite{joshi2019spanbert}, another self-supervised method, pretrains span representations achieving state of the art results (Avg. F1 79.6) with the \emph{independent} variant.

 
\begin{table*}[t]
\footnotesize
\centering
\begin{tabular}
{llcc}
\toprule
Category & Snippet & \#base & \#large \\
\midrule
\multirow{2}{1.5cm}{Related Entities} & Watch spectacular performances by dolphins and sea lions at the \emph{Ocean Theater}... & 12 & 7\\
& It seems the North Pole and the \underline{Marine Life Center} will also be renovated. & & \\
 \midrule
 Lexical & Over the past 28 years , \emph{the Ocean Park} has basically.. \textbf{The entire park} has been ... & 15 & 9\\
\midrule
Pronouns &  In the meantime , our children need \emph{an education}. \textbf{That}'s all we're asking. & 17 & 13 \\
\midrule
\multirow{2}{1.5cm}{Mention Paraphrasing}& And in case you missed it \emph{the Royals} are here. & 14 & 12\\
& Today Britain's \textbf{Prince Charles and his wife Camilla}... & & \\
\midrule
\multirow{2}{1.5cm}{Conversation} & (Priscilla:) \textbf{My} mother was Thelma Wahl . She was ninety years old ...  &  18 & 16\\
& (Keith:) \emph{Priscilla Scott} is mourning . \emph{Her} mother Thelma Wahl was a resident .. & & \\
\midrule
Misc. & He is \textbf{my}, She is \textbf{my} Goddess , ah  & 17 & 17 \\
\midrule
\emph{Total} & & 93 & 74\\
\bottomrule
\end{tabular}
\caption{Qualitative Analysis: \#base and \#large refers to the number of cluster-level errors on a subset of the OntoNotes English development set. \underline{Underlined} and \textbf{bold-faced} mentions respectively indicate incorrect and missing assignments to \emph{italicized} mentions/clusters. 
The miscellaneous category refers to other errors including (reasonable) predictions that are either missing from the gold data or violate annotation guidelines.
} 
\label{tab:errors}
\end{table*}

\section{Analysis}
We performed a qualitative comparison of ELMo and BERT models (Table \ref{tab:errors}) on the OntoNotes English development set by manually assigning error categories (e.g.,  pronouns, mention paraphrasing) to incorrect predicted clusters.\footnote{Each incorrect cluster can belong to multiple categories.} Overall, we found 93 errors for BERT-base and 74 for BERT-large from the same 15 documents.


\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{rcrcc}
\toprule
Doc length & \#Docs & Spread & F1 (base) & F1 (large)\\
\midrule
0 - 128 & 48 & 37.3 & 80.6 & 84.5 \\
128 - 256 & 54 & 71.7 & 80.0 & 83.0\\
256 - 512 & 74 & 109.9 & 78.2 & 80.0\\
512 - 768 & 64 & 155.3 & 76.8 & 80.2\\
768 - 1152 &  61 & 197.6 & 71.1 & 76.2\\
1152+ & 42 & 255.9& 69.9 & 72.8\\
\midrule
All & 343  & 179.1 & 74.3 & 77.3\\
\bottomrule
\end{tabular}
\caption{Performance on the English OntoNotes dev set generally drops as the document length increases. Spread is measured as the average number of tokens between the first and last mentions in a cluster.}
\label{tab:num_seg_res}
\end{table}

\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcc}
\toprule
Segment Length & F1 (BERT-base) & F1 (BERT-large)\\
\midrule
128 & 74.4 & 76.6 \\
256 & 73.9 & 76.9\\
384 & 73.4 & 77.3\\
450 &  72.2 & 75.3\\
512 &  70.7 & 73.6\\
\bottomrule
\end{tabular}
\caption{Performance on the English OntoNotes dev set with varying values for \texttt{max\_segment\_len}. Neither model is able  to effectively exploit larger segments; they perform especially badly  when  \texttt{maximum\_segment\_len} of 512 is used.}
\label{tab:seg_len_res}
\end{table}



\paragraph{Strengths}
 We did not find salient qualitative differences between ELMo and BERT-base models, which is consistent with the quantitative results (Table \ref{tab:ontonotes}). BERT-large improves over BERT-base in a variety of ways including pronoun resolution and lexical matching (e.g., \emph{race track} and \emph{track}). In particular,  the BERT-large variant is better at distinguishing related, but distinct, entities.  Table \ref{tab:errors} shows several examples where the BERT-base variant merges distinct entities (like \emph{Ocean Theater} and \emph{Marine Life Center}) into a single cluster.  BERT-large seems to be able to avoid such merging on a more regular basis.

\paragraph{Weaknesses}
An analysis of errors on the OntoNotes English development set suggests that better modeling of document-level context, conversations, and entity paraphrasing might further improve the state of the art.


\emph{Longer documents} in OntoNotes generally contain larger and more spread-out clusters. We focus on three observations -- (a) Table \ref{tab:num_seg_res} shows how models perform distinctly worse on longer documents, (b) both models are unable to use larger segments more effectively (Table \ref{tab:seg_len_res}) and perform worse when the \texttt{max\_segment\_len} of 450 and 512 are used, and, (c) using overlapping segments to provide additional context does not improve results (Table \ref{tab:ontonotes}). Recent work ~\cite{joshi2019spanbert} suggests that BERT's inability to use longer sequences effectively is likely a by-product pretraining on short sequences for a vast majority of updates.

Comparing preferred segment lengths for base and large variants of BERT indicates that larger models might better encode longer contexts. However, larger models also exacerbate the memory-intensive nature of span representations,\footnote{We required a 32GB GPU to finetune BERT-large.} which have driven recent improvements in coreference resolution. These observations suggest that future research in pretraining methods should look at more \emph{effectively} encoding document-level context using sparse representations ~\cite{child2019generating}. 

Modeling \emph{pronouns} especially in the context of conversations (Table \ref{tab:errors}), continues to be difficult for all models, perhaps partly because c2f-coref does very little to model dialog structure of the document.
Lastly, a considerable number of errors suggest that models are still unable to resolve cases requiring \emph{mention paraphrasing}. For example, bridging \emph{the Royals} with \emph{Prince Charles and his wife Camilla} likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 






















%
 \section{Related Work}
Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from ~\citet{lee2018higher} belongs to this family of models ~\cite{Ng2002Identifying,Bengtson2008Understanding,denis2008specialized,fernandes2012latest,durrett2013easy,Wiseman2015Learning,clark2016corefRL,lee2017end}. 

More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations ~\cite{Peters2018Elmo,devlin2018bert,mccann2017learned,joshi2019spanbert,liu2019roberta}. Of these, BERT ~\cite{devlin2018bert} notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT ~\cite{joshi2019spanbert} focuses on pretraining span representations achieving current state of the art results on  OntoNotes with the \emph{independent} variant. 



\bibliography{coref}
\bibliographystyle{acl_natbib}
\end{document}
