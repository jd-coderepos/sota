\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}

\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{booktabs}

\usepackage{caption}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}

\usepackage[numbers,sort]{natbib}

\usepackage{xspace}
\usepackage{bbold}

\newcommand{\samuel}[1]{\textcolor{red}{(S: #1)}}
\newcommand{\peter}[1]{\textcolor{green}{(P: #1)}}
\newcommand{\lorenzo}[1]{\textcolor{blue}{(L: #1)}}

\newcommand{\vct}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\mat}[1]{\mathtt{#1}}
\newcommand{\set}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\con}[1]{#1} \newcommand{\T}{\ensuremath{^\top}}
\newcommand{\ind}[1]{\ensuremath{\mathbb 1_{#1}}}
\newcommand{\argmax}{\operatornamewithlimits{\arg\,\max}}
\newcommand{\argmin}{\operatornamewithlimits{\arg\,\min}}
\newcommand{\Ex}[1]{\mathbb E[#1]}

\newcommand{\indep}{\emph{Ours Independent}\xspace}
\newcommand{\joint}{\emph{Ours Combined}\xspace}
\newcommand{\PQOurs}{PQ\xspace}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}

\newcommand{\aka}{\emph{a.k.a.}\xspace }



\newcommand{\iABN}{\textrm{iABN}\xspace}
\newcommand{\siABN}{\textrm{iABN}\xspace}

\definecolor{mapillarygreen}{RGB}{5,203,99}


\let\oldsection\section

\renewcommand{\paragraph}[1]{
        \vspace{3pt}
	\noindent\textbf{#1}}
	

\PassOptionsToPackage{hyphens}{url}\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{4003 \vspace{-10pt}} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\captionsetup[table]{position=below}
\title{
\vspace{-25pt}
Seamless Scene Segmentation
\vspace{-10pt}
}

\author{Lorenzo Porzi, Samuel Rota Bul\`o, Aleksander Colovic, Peter Kontschieder\\
Mapillary Research\\
{\tt\small research@mapillary.com}
\vspace{-10pt}
}

\maketitle


\begin{abstract}
In this work we introduce a novel, CNN-based architecture that can be trained end-to-end to deliver seamless scene segmentation results. Our goal is to predict consistent semantic segmentation and detection results by means of a panoptic output format, going beyond the simple combination of independently trained segmentation and detection models. The proposed architecture takes advantage of a novel segmentation head that seamlessly integrates multi-scale features generated by a Feature Pyramid Network with contextual information conveyed by a light-weight DeepLab-like module. As additional contribution we review the panoptic metric and propose an alternative that overcomes its limitations when evaluating non-instance categories. Our proposed network architecture yields state-of-the-art results on three challenging street-level datasets, \ie Cityscapes, Indian Driving Dataset and Mapillary Vistas.
	\vspace{-10pt}
\end{abstract}

\section{Introduction}
Scene understanding is one of the grand goals for automated perception that requires advanced visual comprehension of tasks like semantic segmentation (\textit{Which semantic category does a pixel belong to?}) and detection or instance-specific semantic segmentation (\textit{Which individual object segmentation mask does a pixel belong to?}). Solving these tasks has large impact on a number of applications, including autonomous driving or augmented reality. Interestingly, and despite sharing some obvious commonalities, both these segmentation tasks have been predominantly handled in a disjoint way ever since the rise of deep learning, while earlier works~\cite{Tu+05,YaoFU12,Tighe2014} already approached them in a joint manner. Instead, independent trainings of models, with separate evaluations using corresponding performance metrics, and final fusion in a post-processing step based on task-specific heuristics have seen a revival.

The work in~\cite{Kirillov18} introduces a so-called \textit{panoptic} evaluation metric for joint assessment of semantic segmentation of \textit{stuff} and instance-specific \textit{thing} object categories, to encourage further research on this topic. Stuff is defined as non-countable, amorphous regions of similar texture or material while things are enumerable, and have a defined shape. Few works have started adopting the panoptic metric in their methodology yet, but reported results remain significantly below the ones obtained from fused, individual models. All winning entries on designated panoptic Segmentation challenges like \eg the Joint COCO and Mapillary Recognition Workshop 2018\footnote{\scriptsize \url{http://cocodataset.org/workshop/coco-mapillary-eccv-2018.html}}, were based on combinations of individual (pre-trained) segmentation and instance segmentation models, rather than introducing streamlined integrations that can be successfully trained from scratch.

The use of separate models for semantic segmentation and detection obviously comes with the disadvantage of significant computational overhead. Due to a lack of cross-pollination of models, there is no way of enforcing labeling consistency between individual models. Moreover, we argue that individual models supposedly spend significant amounts of their capacity on modeling redundant information, whereas sensible architectural choices in a joint setting are leading to favorable or on par results, but at much reduced computational costs.

In this work we introduce a novel, deep convolutional neural network based architecture for \textit{seamless scene segmentation}. Our proposed network design aims at jointly addressing the tasks of semantic segmentation and instance segmentation. We present ideas for interleaving information from segmentation and instance-segmentation modules and discuss model modifications over vanilla combinations of standard segmentation and detection building blocks. With our findings, we are able to train high-quality, seamless scene segmentation models without the need of pre-trained recognition models. As result, we obtain a state-of-the-art, single model that jointly produces semantic segmentation and instance segmentation results, at a fraction of the computational cost required when combining independently trained recognition models.

We provide the following contributions in our work:
\begin{itemize}
\item Streamlined architecture based on a single network backbone to generate complete semantic scene segmentation for stuff and thing classes
	\item A novel segmentation head integrating multi-scale features from a Feature Pyramid Network~\cite{Lin2016}, with contextual information provided by a light-weight, DeepLab-inspired module
\item Re-evaluation of the panoptic segmentation metric and refinement for more adequate handling of stuff classes
\item Comparisons of the proposed architecture against individually trained and fused segmentation models, including analyses of model parameters and computational requirements
\item Experimental results on challenging driving scene datasets like Cityscapes~\cite{Cordts2016},  Indian Driving Dataset~\cite{Varma19}, and Mapillary Vistas~\cite{Neuhold2017}, demonstrating 	state-of-the-art performance.
\end{itemize}


\section{Related Works}


\paragraph{Semantic segmentation} is a long-standing problem in computer vision research~\cite{Shotton2007,Brostow2008,Krahenbuhl11,Kontschieder2014a} that has significantly improved over the past five years, thanks in great part to advances in deep learning. The works in~\cite{Badrinarayanan2015,Long2015} have introduced encoder/decoder CNN architectures for providing dense, pixel-wise predictions by taking \eg a fully convolutional approach. The more recent DeepLab~\cite{Chen2016} exploits multi-scale features via parallel filters from convolutions with different dilation factors, together with globally pooled features. Another recent Deeplab extension~\cite{Chen2018ECCV} integrates a decoder module for refining object boundary segmentation results. In~\cite{Chen2018NIPS}, a meta-learning technique for dense prediction tasks is introduced, that learns how to design a decoder for semantic segmentation. The pyramid scene parsing network~\cite{zhao2016pspnet} employs i) a pyramidal pooling module to capture sub-region representations at different scales, followed by upsampling and stacking with respective input features and ii) an auxiliary loss applied after the \textit{conv4} block of a ResNet-101 backbone. The works in~\cite{Yu2016,Yu_2017_CVPR} propose aggregation of multi-scale contextual information using dilated convolutions, which have proven to be particularly effective for dense prediction tasks, and are a generalization of the conventional convolution operator to expand its receptive field. RefineNet~\cite{Lin_2017_CVPR} proposes a multi-path refinement network to exploit multiple abstraction levels of features for enhancing the segmentation quality of high-resolution images. Other works like~\cite{Wu2016,RotNeuKon17cvpr} are addressing the problem of class sample imbalance by introducing loss-guided, pixel-wise gradient reweighting schemes.

\paragraph{Instance-specific semantic segmentation} has recently gained large attention in the field, with early, random-field-based works in~\cite{He2014,Tighe2014}. In~\cite{Hariharan2014} a simultaneous detection and segmentation algorithm is developed that classifies and refines CNN features obtained from regions under R-CNN~\cite{Gir+14} bounding box proposals. The work in~\cite{Hayder+17} emphasizes on refining object boundaries for binary segmentation masks initially generated from bounding box proposals. In~\cite{dai2016instance} a multi-task network cascade is introduced that, beyond sharing features from the encoder in all following tasks, subsequently adds blocks for i) bounding box generation, ii) instance mask generation and iii) mask categorization. Another approach~\cite{Dai2016} introduces instance fully convolutional networks that assemble segmentations from position-sensitive score maps, generated by classifying pixels based on their relative positions. The follow-up work in~\cite{li2016fully} builds upon Faster R-CNN~\cite{Ren+15} for proposal generation and additionally includes position-sensitive outside score maps. InstanceCUT~\cite{Kirillov17} obtains instance segmentations by solving a Multi-Cut problem, taking instance-agnostic semantic segmentation masks and instance-aware, probabilistic boundary masks as inputs, provided by a CNN. The work in~\cite{Arnab17} also introduces an approach where an instance Conditional Random Field (CRF) provides individual instance masks based on exploiting box, global and shape cues as unary potentials, together with instance-agnostic semantic information. In~\cite{Liu17}, sequential grouping networks are presented that run a sequence of simple networks for solving increasingly complex grouping problems, eventually yielding instance segmentation masks. DeepMask~\cite{Pinheiro15} first produces an instance-agnostic segmentation mask for an input patch, which is then assigned to a score corresponding to how likely this patch it to contain an object. At inference, their approach generates a set of ranked segmentation proposals. The follow-up work SharpMask~\cite{Pinheiro16} augments the networks with a top-down refinement approach. Mask R-CNN~\cite{He2017} forms the basis of current state-of-the-art instance segmentation approaches. It is a conceptually simple extension of Faster R-CNN, adding a dedicated branch for object mask segmentation in parallel to the existing ones for bounding box regression and classification. Due to its importance in our work, we provide a more thorough review in the next section. The work in~\cite{Liu2018} proposes to improve localization quality of objects in Mask R-CNN via integration of multi-scale information as bottom-up path augmentation. 

\paragraph{Joint segmentation and instance-segmentation} approaches date back to~\cite{Tu+05}, introducing a Bayesian approach for scene representation by establishing a scene parsing graph to explain both, segmentation of stuff and things. Other works before the era of deep learning often built upon CRFs where \cite{Tighe2014} alternatingly refined pixel labelings and object instance predictions, and~\cite{YaoFU12} framed holistic scene understanding as a structure prediction problem in a graphical model, defined over hierarchies of regions, scene types, \etc. The recently proposed work in~\cite{Kendall18} addresses automated loss balancing in a multi-task learning problem based on analysing the homoscedastic uncertainty of each task.  Even though their work addresses three tasks at the same time (semantic segmentation, instance segmentation and depth estimation), it fails to demonstrate consistent improvements over semantic segmentation and instance segmentation alone and lacks of comparisons to comparable baselines. The supervised variant in~\cite{Li2018Weaklypanoptic} generates panoptic segmentation results but \textit{i)} requires separate (external) input for bounding box proposals and \textit{ii)} exploits a CRF during inference, increasing the complexity of the model. The work in~\cite{Geus18} attempts to introduce a unified architecture related to our ideas, however, the reported results remain significantly below those of reported state-of-the-art methods. Independently and simultaneously to our paper, a number of works~\cite{Li2018,PanopticCOCOWinners2018,Xiong_UBER_2019,Kirillov19,Yang_Google_2019} have proposed panoptic segmentation provided by a single deep network, confirming the importance of this task to the field. While comparable in complexity and architecture, we obtain improved performance on challenging street-level image datasets like Cityscapes and Mapillary Vistas.

\section{Proposed Architecture}
\label{sec:method}

\begin{figure*}
	\centering
	\includegraphics[width=.7\textwidth]{figure01.pdf}
	\caption{Comparison of two architectures for panoptic segmentation. 
Left: Separate models (including bodies) for detection and segmentation. Both predictions are fused to obtain the final panoptic prediction.
		Right: Shared body between the heads. } \label{fig:basic}
		\vspace{-10pt}
\end{figure*}


The proposed architecture consists of a backbone working as feature extractor and two task-specific branches addressing semantic segmentation and instance segmentation, respectively. Hereafter, we provide details about each component and refer to Fig.~\ref{fig:basic} for an overview.

\subsection{Shared Backbone}
The backbone that we use throughout this paper is a slightly modified ResNet-50~\cite{He+16} with a Feature Pyramid Network (FPN)~\cite{Lin2016} on top. 
The FPN network is linked to the output of the modules conv2, conv3, conv4 and conv5 of ResNet-50, which yield different downsampling factors, namely , ,  and , respectively.
Akin to the original FPN architecture, we have a variable number of additional, lower resolution scales covering downsampling factors of  and , depending on the dataset.
The main modification in ResNet-50 is the replacement of all Batch Normalization (BN) + ReLU layers with synchronized Inplace Activated Batch Normalization (\siABN) proposed in~\cite{RotPorKon18a}, which uses LeakyReLU with slope  as activation function due to the need of invertible activation functions. 
This modification gives two important advantages: i) we gain up to 50\% additional GPU memory since the layer performs in-place operations, and ii) the synchronization across GPUs ensures a better estimate of the gradients in multi-GPU trainings with positive effects on convergence.


\subsection{Instance Segmentation Branch}
\label{sec:msk}
The instance segmentation branch follows the state-of-the-art Mask R-CNN~\cite{He2017} architecture.
This branch is structured into a region proposal head and a region segmentation head.

\paragraph{Region Proposal Head (RPH).}
The RPH introduces the notion of an \emph{anchor}. An anchor is a reference bounding box (\aka~region), centered on one of the available spatial locations of the RPH's input and having pre-defined dimensions. The set of pre-defined dimensions is chosen in advance, depending on the dataset and the scale of the FPN output (see details in Sec.~\ref{sec:results}).
We denote by  all anchors that can be constructed by combining a position on an available, spatial location and a dimension from the pre-defined set, and which are entirely contained in the image.
Given an anchor  we denote its position (in the image coordinate system) by  and its dimensions by . 
The role of RPH is to apply a transformation to each anchor in order to obtain a new bounding box proposal together with an objectness score, that assesses the validity of the region.
To this end, RPH applies a  convolution with  output channels and stride  to the outputs of the backbone, followed by \siABN, and a  convolution with  channels, which provide a bounding box proposal with an objectness score for each anchor in . In more details, for each anchor  the transformed bounding box has center , dimensions  and objectness score , where  represents the output from the  convolution for anchor , and  is the sigmoid function. 
The resulting set of bounding boxes are then fed to the region segmentation head, with distinct filtering steps for training and test time.

\paragraph{Region Segmentation Head (RSH).}
Each region proposal  obtained from RPH is fed to RSH, which applies ROIAlign~\cite{He2017}, pooling features directly from the th output of the backbone within region  with a  spatial resolution, where  is selected based on the scale of  according to the formula ~\cite{He2017}.
The result is forwarded to two parallel sub-branches: one devoted
to predicting a class label (or void) for the region proposal together with class-specific corrections of the proposal's bounding box, and the other devoted to providing class-specific mask segmentations. 
The first sub-branch of RSH is composed of two fully-connected layers with 1024 channels, each followed by Group Normalization (GN)~\cite{Wu_2018_ECCV} and LeakyReLU with slope , and a final 
fully-connected layer with  output units.
The output units encode, for each possible class , class-specific correction factors  that are used to compute a new bounding box centered in  with dimensions . This operation generates from  and for each class  a new class-specific region proposals given by .
In addition, we have  units providing logits for a softmax layer that gives a probability distribution over classes and void, the latter label assessing the invalidity of the proposal. The probability associated to class  is used as score function  for the class-specific region proposal .
The second sub-branch applies four  convolution layers each with  output channels. As for the first sub-branch each convolution is followed by GN and LeakyReLU.
This is followed by a  deconvolution layer with output stride  and  output channels, GN, LeakyReLU, and a final  convolution with  output channels. This yields, for each class,  logits that provide class-specific mask foreground probabilities for the given region proposal via a sigmoid.
The resulting mask prediction is combined with the output of the segmentation branch described below. 




\subsection{Semantic Segmentation Branch}
\label{sec:sem}
The semantic segmentation branch takes as input the outputs of the backbone corresponding to the first four scales of FPN. We apply independently to each input (not sharing parameters) a variant of the DeepLabV3 head~\cite{Chen2017} that we call \emph{Mini-DeepLab} (MiniDL, see Fig.~\ref{fig:minidl}) followed by an upsampling operation that yields an output downsampling factor of  and  output channels. All the resulting streams are concatenated and the result is fed to a final  convolution layer with  output channels. The output is bilinearly upsampled to the size of the input image. This provides the logits for a final softmax layer that provides class probabilities for each pixel of the input image.
Each convolution in the semantic segmentation branch, including MiniDL, is followed by \siABN akin to the backbone.

\paragraph{MiniDL.}
The MiniDL module consists of  parallel sub-branches. The first two apply a  convolution with  output channels with dilations  and , respectively. The third one applies a  average pooling operation with stride  followed by a padding with boundary replication to recover the spatial resolution of the input and a  convolution with  output channels. The outputs of the  sub-branches are concatenated and fed into a  convolution layer with  output channels, which delivers the final output of the MiniDL module. 

\paragraph{} As opposed to DeepLabV3, we do not perform the global pooling operation in our MiniDL module for two reasons: i) it breaks translation equivariance if we change the input resolution at test time, which is typically the case and ii) since we work with large input resolutions, it is preferable to limit the extent of contextual information. Instead, we replaced the global pooling operation with average pooling in the rd sub-branch with a fixed large kernel size and stride , but without padding. The lack of padding yields an output resolution which is smaller than the input resolution and we re-establish the input resolution by replicating the boundary of the resulting tensor, \ie we employ a padding layer with boundary replication. By doing so, we generalize the solution originally implemented in DeepLabV3, for we obtain the same output at training time if we keep the kernel size equal to the \emph{training} input resolution, but we preserve translation equivariance at test time, and can reduce the extent of contextual information by properly fixing the kernel size.

\begin{figure}[ht]
	\centering
	\includegraphics[width=.7\columnwidth]{figure02.pdf}
	\caption{Segmentation Head (top) and the architecture of the Mini Deeplab (MiniDL) module (bottom), which is used in the head.}
	\label{fig:minidl}
	\vspace{-10pt}
\end{figure}










\subsection{Training losses}\label{ss:losses}

The two branches of the architecture are supported with distinct losses, which are detailed below.
We denote by  the set of class labels, and assume for simplicity input images with fixed resolution .

\paragraph{Semantic segmentation branch.}
Let  be the semantic segmentation ground truth for a given image and pixel position  and let  denote the 
predicted probability for the same pixel to be assigned class .
The per-image segmentation loss that we employ is a weighted per-pixel log-loss that is given by

The weights are computed following the simplest version of~\cite{RotNeuKon17cvpr} with  and . This corresponds to having a pixel-wise hard negative mining, which selects the  worst predictions, \ie  for all  within the  pixels yielding the lowest probability , and  otherwise.


\paragraph{Instance segmentation branch.}
The losses for the instance segmentation branch and the training procedure are derived from the ones proposed in Mask R-CNN~\cite{He2017}. 
We start with the losses applied to the output of RPH. Let  be the set of ground truth bounding boxes for a given image , let  be the set of bounding boxes generated from  by RPH and let
 be the proposal originated from anchor .
We employ the same strategy adopted in Mask-RCNN to assign ground truth boxes in  to predictions in .
For each ground truth box , the closest anchor in , \ie the one with largest Intersection over Union (IoU), is selected and the corresponding prediction  is regarded as positively matching . For each anchor , we seek the closest ground truth box  and regard the match between  and , \ie the predicted box in  corresponding to anchor , as positive match if , or negative match if , where  are user-defined thresholds (we use ). We take a random subset  of all positive matches and a random subset  of all negative matches, where we typically set  and  in order to have at most  matches. 
The objectness loss for the given image is

where  is the objectness score of the predicted bounding box , and , while
the bounding box regression loss is defined only on positive matches  and is given by

where  is the smooth  norm~\cite{Ren+15}, ,  and  is the anchor that originated prediction .

We next move to losses that pertain to RSH.
Let again  be the ground truth boxes for a given image  and let  be the union of  and the set of bounding boxes generated by RPH from , filtered with Non-Maxima Suppression (NMS), and clipped to the image area. For each , we find the closest (in terms of IoU) ground truth box  and regard it as a positive match if  and as negative match otherwise (we use ). Let  and  be random subsets or all positive and negative matches, respectively, where we typically set  and  in order to have at most  matches.
The region proposal classification loss is given by

where ,  denotes the void class,  is the class of the ground truth bounding box ,  is the probability given by RSH of the input proposal  to take class .
The bounding box regression loss is defined only on positive matches  and is given by


Finally, the mask segmentation loss is given as follows.
Let  be a region proposal entering RSH matching a ground-truth region , \ie . Let  be the corresponding ground truth binary mask with associated class label , where  denotes a void pixel, and let  be the mask prediction for class  obtained from RSH with entries  denoting the probability of cell  to belong to the instance conditioned on class  and region . Then, the loss for the matched proposal  is given by

where  is the set of pixels being non-void in the ground-truth  mask .
The mask losses corresponding to all matched proposals are finally summed and divided by  akin to the bounding box loss .

All losses are weighted equally and gradient from RSH flows only to the backbone, \ie the gradient component flowing from RSH to RPH is blocked, akin to Mask-RCNN.


\subsection{Testing and Panoptic Fusion}\label{ssec:fusion}
 At test time, given an input image  we extract features  with the backbone and generate region proposals with corresponding objectness scores by applying RPH. We filter the resulting set of bounding boxes with Non-Maxima Suppression (NMS)
guided by the objectness scores. The surviving proposals are fed to the RSH (first sub-branch) together with  in order to generate class-specific region proposals with corresponding class probabilities. A second NMS pass is applied on the resulting set of bounding boxes, this time independently per class guided by the class probabilities. The resulting class-specific bounding boxes are fed again to RSH together with , but this time through the second sub-branch which provides the corresponding mask predictions. 
The extracted features  are fed in parallel to the segmentation branch, which provides class probabilities for each pixel. 
The output of RSH and the segmentation branch are finally fused using the strategy given below, in order to deliver the final panoptic segmentation. 



\paragraph{Fusion.} 
The fusion operation is inspired by the one proposed in~\cite{Kirillov18}.
We start iterating over predicted instances in reverse classification score order. For each instance we mark the pixels in the final output that belong to it and are still unassigned, provided that the latter number of pixels covers at least  of the instance. Otherwise we discard the instance thus resembling a NMS procedure.
Remaining unassigned pixels take the most likely class according to the segmentation head prediction, if it belongs to stuff, or void if it belongs to thing. Finally, if the total amount of pixels of any stuff class is smaller than a given threshold ( in our case) we mark all those pixels to void.







\section{Revisiting Panoptic Segmentation}
In this section we review the panoptic segmentation metric~\cite{Kirillov18} (\aka~PQ metric), which evaluates the performance of a so-called panoptic segmentation, and discuss a limitation of this metric when it comes to stuff classes.

\paragraph{PQ metric.} A panoptic segmentation assigns each pixel a stuff class label or an instance ID. Instance IDs are further given a thing class label (\eg pedestrian, car, \etc). As opposed to AP metrics used in detection, instances are not overlapping. The PQ metric is computed for each class independently and averaged over classes (void class excluded). This makes the metric insensitive to imbalanced class distributions. Given a set of ground truth segments  and predicted segments  for a given class , the metric collects a set of True Positive matches as \,.
This set contains all pairs of ground truth and predicted segments that overlap in terms of IoU more than . By construction, every ground truth segment can be assigned at most one predicted segment and vice versa. 
The PQ metric for class  is given by

where  is the set False Positives, \ie unmatched predicted segments for class , and  is the set False Negatives, \ie unmatched segments from ground truth for class .
The metric allows also specification of void classes, both in ground truth and actual predictions. 
Pixels labeled as void in the ground truth are not counted in IoU computations and predicted segments of any class  that overlap with void more than  are not counted in . Also, ground truth segments for class  that overlap with predicted void pixels more than  are not counted in . 
The final PQ metric is obtained by averaging the class-specific PQ scores:

We further denote by  and  the average of thing-specific and stuff-specific PQ scores, respectively.

\paragraph{The issue with stuff classes.}
\begin{figure}
    \includegraphics[width=\columnwidth]{figure03.png}
    \caption{Prediction on a Cityscapes validation set image, where light colored areas highlight conducted errors. Several classes, \eg pole (IoU 0.49) and traffic light (IoU 0.46), are just below the PQ acceptance threshold, while the sidewalk class (IoU 0.62) is just above it. Thus, the former will be overly penalized (PQ ), while the latter will contribute positively (PQ ), even if they look qualitatively similar. Best viewed in color and with digital zoom.}
    \label{fig:pq_example}
    \vspace{-10pt}
\end{figure}
One limitation of the PQ metric is that it over-penalizes errors related to stuff classes, which are by definition not organized into instances.
This derives from the fact that the metric does not distinguish stuff and thing classes and applies indiscriminately the rule that we have a true positive if the ground truth and the predicted segment have IoU greater than 0.5. De facto it regards all pixels in an image belonging to a stuff class as a single big instance.
To give an example of why we think this is sub-optimal, consider a street scene with two sidewalks and assume that the algorithm confuses one of the two with road (say the largest) then the segmentation quality on sidewalk for that image becomes . A real-world example is provided in Fig.~\ref{fig:pq_example}, where several stuff segments are severely penalized by the PQ metric, not reflecting the real quality of the segmentation.
The -IoU rule for thing classes is convenient because it renders the matching between predicted and ground truth instances easy, but this is a problem to be solved only for thing classes. Indeed, predicted and ground truth segments belonging to stuff classes can be directly matched independently from their IoU because each image has at most one instance of them. 


\paragraph{Suggested alternative.}
We propose to maintain the PQ metric only for thing classes, but change the metric for stuff classes.
Specifically, let  be the set of ground truth segments of a given class  and let  be the set of predicted segments for class .
Note that each image can have at most  ground truth segment and at most  predicted segment of the given stuff class.
Let  be the set of matching segments, then the updated metric for class  becomes:

We denote by  the final version of the proposed panoptic metric, which averages  over all classes, \ie

Similarly to PQ, the proposed metric is bounded in  and implicitly regards a stuff segment of an image as a single instance. However, we do not require the prediction of stuff classes to have IoU with the ground truth. 




\section{Experimental Results}
\label{sec:results}

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{figure04.png}
	\caption{Qualitative results obtained by our proposed combined architecture. Top row: Cityscapes. Middle row: IDD. Bottom row: Vistas. Best viewed in color and with digital zoom.} \label{fig:expresults}
	\vspace{-10pt}
\end{figure*}

We assess the benefits of our proposed network architecture on multiple street-level image datasets, namely Cityscapes~\cite{Cordts2016}, Mapillary Vistas~\cite{Neuhold2017} and the Indian Driving Dataset (IDD)~\cite{Varma19}. 
All experiments were designed to provide a fair comparison between baseline reference models and our proposed architecture design choices. To increase transparency of our proposed design contributions, we deliberately leave out model extensions like path aggregation network extensions~\cite{Liu2018,Chen2018}, deformable convolutions~\cite{Dai2017} or Cascade R-CNN~\cite{Cai2018}. We do not apply test time data augmentation (multi-scale testing or horizontal flipping) or explicit use of model ensembles, \etc, as we assume that such bells and whistles approximately equally increase recognition performances for all methods. All models were only pre-trained on ImageNet~\cite{Rus+15}. We use the following terminology in the remainder of this section: \indep~refers to fused, but individually trained models (Fig.~\ref{fig:basic} left) each following the proposed design, and \joint~refers to the unified architecture in Fig.~\ref{fig:basic} (right). 






\paragraph{Model and Training Hyperparameters.}
Unless otherwise noted, we take all the hyperparameters of the instance segmentation branch from~\cite{He2017}. These hyperparameters are shared by all the models we evaluate in our experiments.
We initialize our backbone model with weights extracted from PyTorch's ImageNet-pretrained ResNet-50 despite using a different activation function, motivated by findings in our prior work~\cite{RotPorKon18a}. We train all our networks with SGD, using a fixed schedule of 48k iterations and learning rate , decreasing the learning rate by a factor 10 after 36k and 44k iterations.
At the beginning of training we perform a warm-up phase where the learning rate is linearly increased from  to  in 200 iterations.\footnote{Note that the warm-up phase is not strictly needed for convergence. Instead, we adopt it for compatibility with~\cite{He2017}.}
During training the networks receive full images as input, randomly flipped in the horizontal direction, and scaled such that their shortest side measures  pixels, where  is randomly sampled from . Training is performed on batches of 8 images using a computing node equipped with 8 Nvidia V100 GPUs. At test time, images are scaled such that their shortest size measures 1024 pixels (preserving aspect ratio).


\paragraph{Differences with respect to~\cite{Por+19_cvpr}.}
Some scores reported in this document differ from our corresponding CVPR 2019 paper, due to mistakenly copying numbers from the experiment logs and improper handling of \textit{void} labels in Vistas. For fair comparisons please cite the numbers given here.



\subsection{Cityscapes}
\label{ssec:Cityscpes}
Cityscapes~\cite{Cordts2016} is a street-level driving dataset with images from 50 central-European cities. All images were recorded with a single camera type, image resolution of , and during comparable weather and lighting conditions. It has a total of 5,000 pixel-specifically annotated images (2,975/500/1,525 for training, validation and test, respectively), and additionally provides 19,998 images forming the \textit{coarse extra} set, where only coarse annotations per image are available (which we have not used in our experiments). Images are annotated into 19 object classes (11 stuff and 8 instance-specific). 

For \indep, we trained each recognition model independently, using the hyperparameter settings described above (again, each with a ResNet-50+FPN backbone), and fused the outputs into a single panoptic prediction as described in Section~\ref{ssec:fusion}. To assess the quality of our obtained panoptic results in terms of standard segmentation metrics, we drop the information about instance identity and retain only the pixel-wise class assignment. By doing so, we can evaluate the quality of \indep in terms of mIoU (mean Intersection-over-Union~\cite{Everingham2010}) against other state-of-the-art semantic segmentation methods. We obtain a result of \%, which is comparable or slightly better than \% reported in~\cite{Kreso_2017_ICCV} (using a DenseNet-169 backbone), \% using DeepLab2 in combination with a ResNet-101 backbone as reported in~\cite{RotNeuKon17cvpr}, or \% with a ResNet-152 in~\cite{Wu2016}. Similarly, we can evaluate the instance-segmentation results in terms of AP (mean average precision on masks)  by setting to void the stuff pixels. The result of our baseline is \%, which is slightly above the reported baseline score in Mask R-CNN~\cite{He2017} (31.5\% w/o COCO~\cite{LinMSCOCO2014} pre-training).

As for the PQ metrics, \indep delivers \%, PQ\%, PQ\% and PQ\%. We also provide results of \joint~in Tab.~\ref{tab:results}, performing slightly better on both PQ and PQ. This is remarkable, given the significantly reduced number of model parameters (see Section~\ref{ssec:computational}) and when assuming that the fusion of individually trained models could lead to an ensemble effect (often deliberately used to improve test results, at the cost of increased computational complexity).

In addition, we show results of jointly trained networks from independent, concurrently appearing works~\cite{Geus18,Li2018,Xiong_UBER_2019,Kirillov19,Yang_Google_2019}, with focus on comparability of network architectures and data used for pre-training. In Tab.~\ref{tab:results} we abbreviate the network backbones as R50, R101 or X71 for ResNet50, ResNet101 or Xception Net71, respectively, and provide datasets used for pre-training (\textbf{I} = ImageNet and \textbf{C} = COCO). All our proposed variants outperform the direct competitors by a considerable margin, \ie our baseline models as well as jointly trained architectures are better.
Finally, the top row in Fig.~\ref{fig:expresults} shows some qualitative seamless segmentation results obtained with our architecture.

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lll|cccccc|cccccc}
		\toprule
		& & & \multicolumn{6}{c|}{\textbf{Cityscapes}} & \multicolumn{6}{c}{\textbf{Mapillary Vistas}} \\
		Method & Body & Data & PQ & PQ & PQ & \PQOurs & AP & IoU & PQ & PQ & PQ & \PQOurs & AP & IoU\\
		\midrule
		de Geus \etal~\cite{Geus18} & R50 & \textbf{I} & - & - & - & - & - & - & 17.6 & 27.5 & 10.0 & - & - & 34.7  \\
		Supervised in~\cite{Li2018Weaklypanoptic} & R101 & \textbf{I} & 47.3 & 52.9 & 39.6 & - & 24.3 & 71.6  & - & - & - & - & - & - \\
		FPN-Panoptic~\cite{Kirillov19} & R50 & \textbf{I} & 57.7 & 62.2 & 51.6 & - & 32.0 & 75.0 & - & - & - & - & - & - \\
		TASCNet~\cite{Li2018} & R50 & \textbf{I+C} & 59.2 & 61.5 & 56.0 & - & 37.6 & 77.8 & 32.6 & 34.4 & 31.1 & - & 18.5 & - \\
		UPSNet~\cite{Xiong_UBER_2019} & R50 & \textbf{I} & 59.3 & 62.7 & 54.6 & - &  33.3 & 75.2  & - & - & - & - & - & - \\
		DeeperLab~\cite{Yang_Google_2019} & X71 & \textbf{I} & 56.3 & - & - & - & - & - & 32.0 & - & - & - & - & 55.3 \\
		\midrule 
\indep & R50 & \textbf{I} & 59.8 & 64.5 & 53.4 & 59.0 & 31.9 & 75.4 & 37.2 & 42.5 & 33.2 & 38.6 & 16.3 & 50.2 \\
		\rowcolor{mapillarygreen}
		\joint & R50 & \textbf{I} & 60.3 & 63.3 & 56.1 & 59.6 & 33.6 & 77.5 & 37.7 & 42.9 & 33.8 & 39.0 & 16.4 & 50.4 \\
\bottomrule
	\end{tabular} }
	\caption{Comparison of validation set results on Cityscapes and Vistas with related works. Used network bodies include R101, R50 and X71 for ResNet-101, ResNet-50 and Xception-71, respectively. \textit{Data} indicates datasets used for pre-training where \textbf{I} = ImageNet and \textbf{C} = COCO. All results in [\%]. }
	\label{tab:results}
	\vspace{-10pt}
\end{table*}


\subsection{Indian Driving Dataset (IDD)}
\label{ssec:IDD}
IDD~\cite{Varma19} was introduced for testing perception algorithm performance in India. It comprises 10,003 images from 182 driving sequences, divided in 6,993/981/2,029 images for training, validation and test, respectively. Images are either of 720p or 1080p resolution and were obtained from a front-facing camera mounted on a car roof. The dataset is annotated into 26 classes (17 stuff and 9 instance-specific), and we report results for \textit{level 3} labels. 

Following the same procedure described for Cityscapes, we evaluate \indep~on segmentation and instance segmentation yielding \% and AP\%, respectively. To contextualize the obtained results, the numbers reported as baselines in~\cite{Varma19} for semantic segmentation are \% using ERFNet~\cite{Romera18} and \% for dilated residual nets~\cite{Yu_2017_CVPR} and again Mask R-CNN for instance-specific segmentation on a ResNet-101 body yielding AP\%. Those numbers supposedly belong to the test set, while no numbers are reported for validation. In terms of panoptic metrics, \indep~yields \%, PQ\%, PQ\% and PQ\%. For \joint~we obtain \%, PQ\%, PQ\%, PQ\%, AP\% and \%. In the key metrics PQ and PQ the results differ by  points, and we again stress that the numbers for \joint~are provided from a network with significantly less parameters.




The middle row in Fig.~\ref{fig:expresults} shows seamless segmentation results obtained by our combined architecture.

\subsection{Mapillary Vistas}\label{ssec:vistas}
Mapillary Vistas~\cite{Neuhold2017} is one of the richest, publicly available street-level image datasets today. It comprises 25k high-resolution (on average 8.6 MPixels) images, split into sets of 18k/2k/5k images for training, validation and test, respectively. We only used the training set during model training while evaluating on the validation set. Vistas shows street-level images from all over the world, with images captured from driving cars as well as pedestrians taken them on a sidewalk. It also has large variability in terms of weather, lighting, capture time during day and season, sensor type, \etc., making it a very challenging road scene segmentation benchmark.
Accounting for this, we modify some of the model hyper-parameters and training schedule as follows: we use anchors with aspect ratios in  and area , where  is the FPN level downsampling factor; we train on images with shortest side scaled to , where t is randomly sampled from ; we train for a total of 192k iterations, decreasing the learning rate after 144k and 176k iterations.

The results obtained with \indep~and \joint~ are given in Tab.~\ref{tab:results}. The joint architecture besides using significantly less parameters achieves also slightly better results compared to the independently trained models (+\% PQ and +\% PQ). Compared to other state-of-the-art methods, we obtain \% and \% PQ score over DeeperLab~\cite{Yang_Google_2019} and TASCNet~\cite{Li2018}, respectively, despite using a weaker backbone compared to DeeperLab and not pre-training on COCO as opposed to TASCNet. 

Finally, we show seamless scene segmentation results in the bottom row of Fig.~\ref{fig:expresults}.












\subsection{Computational Aspects}
\label{ssec:computational}
We discuss computational aspects when comparing the two individually trained recognition models against our combined model architecture. 
When fused, the two task-specific models have  parameters, \ie \mbox{} more than our combined architecture (\mbox{}). The majority of saved parameters belong to the backbone. The amount of computation is similarly reduced, \ie the combined, independently trained models require  more FLOPs due to two inference steps per test image. In absolute terms, the individual models require  TFLOP while our combined architectures requires  TFLOP on  image resolution, respectively.

\subsection{Ablation of the New Pooling in DeeplabV3}
In Section~\ref{sec:sem}, we propose a modification of the DeepLabV3 global pooling operation in order to limit its extent and preserve translation equivariance. Specifically, DeepLabV3 applies global average spatial pooling to the backbone outputs and replicates the outcome to each spatial location. Instead, we replace the global pooling operation with average pooling with a fixed kernel size (in our experiments we use  corresponding to an image area of ), with stride  and no padding. This is equivalent to a convolution with a smoothing kernel that yields a reduced spatial output resolution. To recover the original spatial resolution we apply padding with replicated boundary. Our motivation is twofold: i) if we train without limiting the input size (\ie~without crops) the context that is captured by global pooling is too wide and degrades the performance compared to having a reduced context as the one captured by our modification and ii) in case one trains with crops, the use of global pooling also at test time breaks translation equivariance and induces a substantial change of feature representation between test and training time, while our modification is not affected by this issue.
To give an experimental evidence of the former motivation, we trained a segmentation model with the DeepLabV3 head and ResNet-50 backbone on Cityscapes at full resolution by keeping the original DeepLabV3 pooling strategy yielding  mIoU, whereas the same network and training schedule yields  (+1.8\% absolute) with our new pooling strategy. To support the second motivation, we trained our baseline segmentation model with random  crops using the standard DeepLabV3 pooling strategy, yielding  mIoU, whereas the same network with our pooling strategy yields  (+0.9\% absolute).


\section{Conclusions}
In this work we have introduced a novel CNN architecture for producing \textit{seamless scene segmentation results}, \ie semantic segmentation and instance segmentation modules jointly operating on top of a single network backbone. We depart from the prevailing approach of training individual recognition models, and instead introduce a multi-task architecture that benefits from interleaving network components as well as a novel segmentation module. We also revisit the panoptic metric used to assess combined segmentation and detection results and propose a relaxed alternative for handling stuff segments. Our findings include that we can generate state-of-the-art recognition results that are significantly more efficient in terms of computational effort and model sizes, compared to combined, individual models.




	

{\small
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{Arnab17}
Anurag Arnab and Philip~H.S. Torr.
\newblock Pixelwise instance segmentation with a dynamically instantiated
  network.
\newblock In {\em (CVPR)}, 2017.

\bibitem{Badrinarayanan2015}
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.
\newblock Segnet: {A} deep convolutional encoder-decoder architecture for image
  segmentation.
\newblock {\em CoRR}, abs/1511.00561, 2015.

\bibitem{Brostow2008}
Gabriel~J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla.
\newblock Segmentation and recognition using structure from motion point
  clouds.
\newblock In {\em (ECCV)}, pages 44--57. 2008.

\bibitem{Cai2018}
Zhaowei Cai and Nuno Vasconcelos.
\newblock Cascade r-cnn: Delving into high quality object detection.
\newblock In {\em (CVPR)}, June 2018.

\bibitem{Chen2016}
Liang{-}Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
  Alan~L. Yuille.
\newblock Deeplab: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected {CRF}s.
\newblock {\em CoRR}, abs/1606.00915, 2016.

\bibitem{Chen2017}
Liang{-}Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock {\em CoRR}, abs/1706.05587, 2017.

\bibitem{Chen2018}
Liang{-}Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
  Adam.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock {\em CoRR}, abs/1802.02611, 2018.

\bibitem{Chen2018NIPS}
Liang-Chieh Chen, Maxwell~D. Collins, Yukun Zhu, George Papandreou, Barret
  Zoph, Florian Schroff, Hartwig Adam, and Jonathon Shlens.
\newblock Searching for efficient multi-scale architectures for dense image
  prediction.
\newblock {\em (NIPS)}, 2018.

\bibitem{Chen2018ECCV}
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
  Adam.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock In {\em (ECCV)}, September 2018.

\bibitem{Cordts2016}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The {C}ityscapes dataset for semantic urban scene understanding.
\newblock In {\em (CVPR)}, 2016.

\bibitem{Dai2016}
Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, and Jian~Sun Sun.
\newblock Instance-sensitive fully convolutional networks.
\newblock In {\em (ECCV)}, 2016.

\bibitem{dai2016instance}
Jifeng Dai, Kaiming He, and Jian Sun.
\newblock Instance-aware semantic segmentation via multi-task network cascades.
\newblock In {\em (CVPR)}, 2016.

\bibitem{Dai2017}
Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen
  Wei.
\newblock Deformable convolutional networks.
\newblock In {\em (ICCV)}, Oct 2017.

\bibitem{Geus18}
Daan de Geus, Panagiotis Meletis, and Gijs Dubbelman.
\newblock Panoptic segmentation with a joint semantic and instance segmentation
  network.
\newblock {\em CoRR}, abs/1809.02110, 2018.

\bibitem{Everingham2010}
M. Everingham, L. Van~Gool, C.~K.~I. Williams, J. Winn, and A. Zisserman.
\newblock The {P}ascal visual object classes {(VOC)} challenge.
\newblock {\em (IJCV)}, 88(2):303--338, 2010.

\bibitem{Gir+14}
Ross~B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In {\em (CVPR)}, 2014.

\bibitem{Hariharan2014}
Bharath Hariharan, Pablo Arbel{\'a}ez, Ross Girshick, and Jitendra Malik.
\newblock Simultaneous detection and segmentation.
\newblock In {\em (ECCV)}, pages 297--312, 2014.

\bibitem{Hayder+17}
Zeeshan Hayder, Xuming He, and Mathieu Salzmann.
\newblock Boundary-aware instance segmentation.
\newblock In {\em (CVPR)}, 2017.

\bibitem{He2017}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'{a}}r, and Ross~B. Girshick.
\newblock Mask {R-CNN}.
\newblock In {\em (ICCV)}, 2017.

\bibitem{He+16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock {\em CoRR}, abs/1603.05027, 2016.

\bibitem{He2014}
Xuming He and Stephen Gould.
\newblock An exemplar-based crf for multi-instance object segmentation.
\newblock In {\em (CVPR)}, 2014.

\bibitem{Kendall18}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In {\em (CVPR)}, June 2018.

\bibitem{Kirillov19}
Alexander Kirillov, Ross~B. Girshick, Kaiming He, and Piotr Doll{\'{a}}r.
\newblock Panoptic feature pyramid networks.
\newblock {\em CoRR}, abs/1901.02446, 2018.

\bibitem{Kirillov18}
Alexander Kirillov, Kaiming He, Ross~B. Girshick, Carsten Rother, and Piotr
  Doll{\'{a}}r.
\newblock Panoptic segmentation.
\newblock {\em CoRR}, abs/1801.00868, 2018.

\bibitem{Kirillov17}
Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan Savchynskyy, and
  Carsten Rother.
\newblock Instancecut: From edges to instances with multicut.
\newblock In {\em (CVPR)}, 2017.

\bibitem{Kontschieder2014a}
Peter Kontschieder, Samuel {Rota~Bul\`o}, Marcello Pelillo, and Horst Bischof.
\newblock Structured labels in random forests for semantic labelling and object
  detection.
\newblock {\em (PAMI)}, 36, 2014.

\bibitem{Krahenbuhl11}
Philipp Kr\"{a}henb\"{u}hl and Vladlen Koltun.
\newblock Efficient inference in fully connected crfs with gaussian edge
  potentials.
\newblock In {\em (NIPS)}, 2011.

\bibitem{Kreso_2017_ICCV}
Ivan Kreso, Sinisa Segvic, and Josip Krapac.
\newblock Ladder-style densenets for semantic segmentation of large natural
  images.
\newblock In {\em ICCV Workshops}, Oct 2017.

\bibitem{Li2018}
Jie Li, Allan Raventos, Arjun Bhargava, Takaaki Tagawa, and Adrien Gaidon.
\newblock Learning to fuse things and stuff.
\newblock {\em CoRR}, abs/1812.01192, 2018.

\bibitem{Li2018Weaklypanoptic}
Qizhu Li, Anurag Arnab, and Philip~H.S. Torr.
\newblock Weakly- and semi-supervised panoptic segmentation.
\newblock In {\em (ECCV)}, 2018.

\bibitem{li2016fully}
Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
\newblock Fully convolutional instance-aware semantic segmentation.
\newblock {\em CoRR}, abs/1611.07709, 2016.

\bibitem{Lin_2017_CVPR}
Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid.
\newblock Refinenet: Multi-path refinement networks for high-resolution
  semantic segmentation.
\newblock In {\em (CVPR)}, 2017.

\bibitem{Lin2016}
Tsung{-}Yi Lin, Piotr Doll{\'{a}}r, Ross~B. Girshick, Kaiming He, Bharath
  Hariharan, and Serge~J. Belongie.
\newblock Feature pyramid networks for object detection.
\newblock {\em CoRR}, abs/1612.03144, 2016.

\bibitem{LinMSCOCO2014}
Tsung{-}Yi Lin, Michael Maire, Serge~J. Belongie, Lubomir~D. Bourdev, Ross~B.
  Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'{a}}r, and
  C.~Lawrence Zitnick.
\newblock Microsoft {COCO:} {C}ommon objects in context.
\newblock {\em CoRR}, abs/1405.0312, 2014.

\bibitem{PanopticCOCOWinners2018}
Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, Gang Yu, and Wei
  Jiang.
\newblock An end-to-end network for panoptic segmentation.
\newblock {\em CoRR}, abs/1903.05027, 2019.

\bibitem{Liu17}
Shu Liu, Jiaya Jia, Sandra Fidler, and Raquel Urtasun.
\newblock Sgn: Sequential grouping networks for instance segmentation.
\newblock In {\em (ICCV)}, 2017.

\bibitem{Liu2018}
Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
\newblock Path aggregation network for instance segmentation.
\newblock In {\em (CVPR)}, June 2018.

\bibitem{Long2015}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em (CVPR)}, pages 3431--3440, 2015.

\bibitem{Neuhold2017}
Gerhard Neuhold, Tobias Ollmann, Samuel Rota~Bul\`o, and Peter Kontschieder.
\newblock The {M}apillary {V}istas dataset for semantic understanding of street
  scenes.
\newblock In {\em (ICCV)}, 2017.

\bibitem{Pinheiro15}
Pedro H.~O. Pinheiro, Ronan Collobert, and Piotr Doll{\'{a}}r.
\newblock Learning to segment object candidates.
\newblock In {\em (NIPS)}, 2015.

\bibitem{Pinheiro16}
Pedro H.~O. Pinheiro, Tsung{-}Yi Lin, Ronan Collobert, and Piotr Doll{\'{a}}r.
\newblock Learning to refine object segments.
\newblock In {\em (ECCV)}, 2016.

\bibitem{Por+19_cvpr}
Lorenzo Porzi, Samuel~Rota Bul\`o, Aleksander Colovic, and Peter Kontschieder.
\newblock Seamless scene segmentation.
\newblock In {\em (CVPR)}, 2019.

\bibitem{Ren+15}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster {R-CNN}: Towards real-time object detection with region
  proposal networks.
\newblock In {\em (NIPS)}, 2015.

\bibitem{Romera18}
E. Romera, J.~M. Álvarez, L.~M. Bergasa, and R. Arroyo.
\newblock Erfnet: Efficient residual factorized convnet for real-time semantic
  segmentation.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems},
  19(1):263--272, 2018.

\bibitem{RotNeuKon17cvpr}
S. Rota~Bul\`o, G. Neuhold, and P. Kontschieder.
\newblock Loss max-pooling for semantic image segmentation.
\newblock In {\em (CVPR)}, July 2017.

\bibitem{RotPorKon18a}
Samuel Rota~Bul\`o, Lorenzo Porzi, and Peter Kontschieder.
\newblock In-place activated batchnorm for memory-optimized training of {DNN}s.
\newblock In {\em (CVPR)}, 2018.

\bibitem{Rus+15}
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A.
  Karphathy, A. Khosla, M. Bernstein, A.~C. Berg, and L. Fei-Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em (IJCV)}, 2015.

\bibitem{Shotton2007}
J. Shotton, J. Winn, C. Rother, and A. Criminisi.
\newblock Textonboost for image understanding: Multi-class object recognition
  and segmentation by jointly modeling texture, layout, and context.
\newblock {\em (IJCV)}, 81(1):2--23, 2007.

\bibitem{Tighe2014}
J. Tighe, M. Niethammer, and S. Lazebnik.
\newblock Scene parsing with object instances and occlusion ordering.
\newblock In {\em (CVPR)}, 2014.

\bibitem{Tu+05}
Z. Tu, X. Chen, A.L. Yuille, and S.-C. Zhu.
\newblock Image parsing: Unifying segmentation, detection, and recognition.
\newblock {\em (IJCV)}, 2005.

\bibitem{Varma19}
Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, and
  C~V Jawahar.
\newblock Indian driving dataset ({IDD}): A dataset for exploring problems of
  autonomous navigation in unconstrained environments.
\newblock In {\em (WACV)}, 2019.

\bibitem{Wu_2018_ECCV}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In {\em (ECCV)}, September 2018.

\bibitem{Wu2016}
Zifeng Wu, Chunhua Shen, and Anton van~den Hengel.
\newblock High-performance semantic segmentation using very deep fully
  convolutional networks.
\newblock {\em CoRR}, abs/1604.04339, 2016.

\bibitem{Xiong_UBER_2019}
Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and
  Raquel Urtasun.
\newblock Upsnet: {A} unified panoptic segmentation network.
\newblock {\em CoRR}, abs/1901.03784, 2019.

\bibitem{Yang_Google_2019}
Tien{-}Ju Yang, Maxwell~D. Collins, Yukun Zhu, Jyh{-}Jing Hwang, Ting Liu, Xiao
  Zhang, Vivienne Sze, George Papandreou, and Liang{-}Chieh Chen.
\newblock Deeperlab: Single-shot image parser.
\newblock {\em CoRR}, abs/1902.05093, 2019.

\bibitem{YaoFU12}
Jian Yao, Sanja Fidler, and Raquel Urtasun.
\newblock Describing the scene as a whole: Joint object detection, scene
  classification and semantic segmentation.
\newblock In {\em (CVPR)}, 2012.

\bibitem{Yu2016}
Fisher Yu and Vladlen Koltun.
\newblock Multi-scale context aggregation by dilated convolutions.
\newblock {\em (ICLR)}, 2016.

\bibitem{Yu_2017_CVPR}
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser.
\newblock Dilated residual networks.
\newblock In {\em (CVPR)}, 2017.

\bibitem{zhao2016pspnet}
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
\newblock Pyramid scene parsing network.
\newblock {\em CoRR}, abs/1612.01105, 2016.

\end{thebibliography}
 }

\end{document}
