\documentclass[letterpage]{article}
\usepackage{uai2019}


\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue}

\usepackage{natbib}
\usepackage[algoruled,algo2e]{algorithm2e}
\usepackage{tabularx}
\SetKwComment{Comment}{}{}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{enumitem}

\usepackage{amssymb}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}

\newcommand{\SM}[1]{\textbf{\color{blue}[SM: #1]}}

\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etY{{\etens{Y}}}
\def\etX{{\etens{X}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{times}

\title{Augmenting and Tuning Knowledge Graph Embeddings}



\author{
{\bf Robert Bamler}  \\
Department of Computer Science \\
University of California, Irvine \\
Irvine, CA 92617 \\
\And
{\bf Farnood Salehi\thanks{~~joint first authorship.}} \\
\'Ecole Polytechnique F\'ed\'erale \\
de Lausanne (EPFL), \\
Switzerland \\
\And
{\bf Stephan Mandt}   \\
Department of Computer Science \\
University of California, Irvine \\
Irvine, CA 9261
}

\begin{document}

\maketitle

\begin{abstract}
Knowledge graph embeddings rank among the most successful methods for link prediction in knowledge graphs, i.e., the task of completing an incomplete collection of relational facts.
A downside of these models is their strong sensitivity to model hyperparameters, in particular regularizers, which have to be extensively tuned to reach good performance~\citep{KBK2017}. We propose an efficient method for large scale hyperparameter tuning by interpreting these models in a probabilistic framework. After a model augmentation that introduces per-entity hyperparameters, we use a variational expectation-maximization approach to tune thousands of such hyperparameters with minimal additional cost. Our approach is agnostic to details of the model and results in a new state of the art in link prediction on standard benchmark data.
\end{abstract}

\section{INTRODUCTION}\label{sec:introduction}

In 2012, Google announced that it improved the quality of its search engines significantly by utilizing knowledge graphs \citep{E2012}.
A knowledge graph is a data set of relational facts represented as triplets (\emph{head}, \emph{relation}, \emph{tail}).
The \emph{head} and \emph{tail} symbols represent real-world entities, such as people, objects, or places.
The \emph{relation} describes how the two entities are related to each other, e.g., `head \emph{was founded by} tail' or `head \emph{graduated from} tail'.

While the number of true relational facts among a large set of entities can be enormous, the amount of data points in empirical knowledge graphs is often rather small.
It is therefore desirable to complete missing facts in a knowledge graph algorithmically based on patterns detected in the data set of known facts~\citep{nickel2016review}.
Such link prediction in relational knowledge graphs has become an important subfield of artificial intelligence~\citep{BUGWY2013,WZFC2014,LLSLZ2015,NRP2016,TWR2016,WL2016,JLHZ2016,SHCG2016,XHMZ2017,SW2017,LUO2018}.

A popular approach to link prediction is to fit an embedding model to the observed facts~\citep{KBK2017,N2017,wang2017knowledge}.
A knowledge graph embedding model represents each entity and each relation by a low-dimensional semantic embedding vector.
Over the past six years, these models have made significant progress on link prediction~\citep{BUGWY2013,BWXJL2015,NRP2016,TWR2016,LUO2018}.
However, \citet{KBK2017} pointed out that these models are highly sensitive to hyperparameters, specifically the regularization strength.
This is not surprising since even large knowledge graphs often contain only few data points per entity (i.e., per embedding vector), and so the regularizer plays an important role.
\citet{KBK2017} showed that a simple baseline model can outperform more modern models when using carefully tuned hyperparameters.

In addition to being highly sensitive to the regularization strength, knowledge graph embedding models also need vastly different regularization strengths for different embedding vectors.
Knowledge graph embedding models are typically trained by minimizing some function  of the embedding vectors for each triplet fact  (short or \emph{head}, \emph{relation}, and \emph{tail}) in the training set~.
One typically adds a regularizer with some strength  as follows,

Here, , , and~ is the embedding for entity~, entity~, and relation~, respectively.
Boldface~ and~ is shorthand for all entity and relation embeddings, respectively, and one typically uses a -norm regularizer with .

It was pointed out by~\citet{LUO2018} that Eq.~\ref{eq:loss_one_lambda} implicitly scales the regularization strength proportionally to the frequency of entities and relations in the data set~ since the regularizer is inside the sum over training points.
This implies vastly different regularization strengths for different embedding vectors since the frequencies of entities and relations vary over a wide range (Figure~\ref{fig:counts}).
As we show in this paper, the general idea to use stronger regularization for more frequent entities and relations can be justified from a Bayesian perspective (for empirical evidence, see~\citep{SS2010}).
However, the specific choice to make the regularization strength \emph{proportional} to the frequency seems more like a historic accident.

Rather than imposing a proprotional relationship between frequency and regularization strength, we propose to augment the model family such that each embedding~ and~ has its individual regularization strength  and , respectively.
This replaces the loss function from Eq.~\ref{eq:loss_one_lambda} with

Here, the last two sums run over each entity~ and each relation~ exactly once (there is only one sum over entities since the same entity embedding vector~ is used for an entity~ in either head or tail position).

The loss in Eq.~\ref{eq:loss_individual_lambdas} contains a macroscopic number of hyperparameters ~and~:
over  in our largest experiments.
It would be impossible to tune such a large number of hyperparameters with traditional grid search, which scales exponentially in the number of hyperparameters.
To solve this issue, we propose in this work a probabilistic interpretation of knowledge graph embedding models.
The probabilistic interpretation admits efficient hyperparameter tuning with variational expectation-maximization~\citep{dempster1977maximum,bernardo2003variational}.
This allows us to optimize over all hyperparameters in parallel, and it leads to models with better predictive performance.

Besides improving performance, our approach also has the potential to accelerate research on new knowledge graph embedding models.
Researchers who propose a new model architecture currently have to invest considerable resources into hyperparameter tuning to prove competitiveness with existing, highly tuned models.
Our cheap large-scale hyperparameter tuning speeds up iteration on new models.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figs/counts2}
  \caption{
    Number of training points (triplet facts) for each entity (left) and relation (right) in the FB15K data set (we sorted entity and relation IDs by frequency).
    The large variation on the -axis motivates entity/relation-dependent regularization strengths as proposed in Eq.~\ref{eq:loss_individual_lambdas}.
  }\label{fig:counts}
\end{figure}

In detail, our contributions are as follows:
\begin{itemize}
  \item
    We consider a broad class of knowledge graph embedding models containing ComplEx~\citep{TWR2016} and DistMult~\citep{BWXJL2015}.
    We interpret these models as generative models of facts with a corresponding generative process ( Figure~\ref{fig:generative}).
  \item
    We first \emph{augment} these models by introducing separate priors for each
    entity and relationship vector.
    In a nonprobabilistic picture, these correspond to regularizers.
    This augmentation makes the models more flexibile, but it introduces thousands of new hyperparameters (regularizers) that need to be optimized.
  \item
    We then show how to efficiently \emph{tune} such augmented models.
    The large number of hyperparameters rules out both grid search with cross validation and Bayesian optimization, calling for gradient-based hyperparameter optimization.
    Gradient-based hyperparameter optimization would lead to singular solutions in classical maximum likelihood training.
    Instead, we propose variational expectation-maximization (EM), which avoids such singularities.
  \item
    We evaluate our proposed hyperparameter optimization method experimentally for augmented versions of DistMult and ComplEx.\footnote{Source code: \url{https://github.com/mandt-lab/knowledge-graph-tuning}}
    The high tunability of the proposed models combined with our efficient hyperparameter tuning method improve the predictive performance over the previous state of the art.
\end{itemize}


The paper is structured as follows:
Section~\ref{sec:model} summarizes a large class of knowledge graph embedding models and presents our probabilistic perspective on these models in terms of a generative probabilistic process.
Section~\ref{sec:method} describes our algorithm for hyperparameter tuning.
We present experiments in
Section~\ref{sec:experiments}, compare our method to related work in Section~\ref{sec:related}, and conclude in Section~\ref{sec:conclusions}.
 \section{GENERATIVE KNOWLEDGE GRAPH EMBEDDING MODELS}
\label{sec:model}

In this section, we introduce our notation for a large class of knowledge graph embedding models (KG embeddings) from the literature (Section~\ref{sec:conventional_models}), and we then generalize these models in two aspects.
First, while conventional KG embeddings typically share the same regularization strength across all entities and relationship vectors, we lift this constraint and allow each embedding vector to be regularized differently (Section~\ref{sec:regularization}).
Second, we show that the loss functions of conventional KG embeddings as well as our augmented model class can be obtained as point estimates of a probabilistic generative process of the data (Section~\ref{sec:probabilistic_models}).
Drawing on this probabilistic perspective, we can optimize all hyperparameters efficiently using variational expectation-maximization (Section ~\ref{sec:method}).


\subsection{CONVENTIONAL KG EMBEDDINGS}
\label{sec:conventional_models}

We introduce our notation for a large class of knowledge graph embedding models (KG embeddings) from the literature, such as DistMult~\citep{BWXJL2015}, ComplEx~\citep{TWR2016}, and Holographic Embeddings~\citep{NRP2016}.

Knowledge graphs are sets of triplet facts  where the `head'~ and `tail'~ both belong to a fixed set of~ entities, and~ describes which one out of a set of~ relations holds between~ and~.
KG embeddings represent each entity  and each relation  by an embedding vector~ and~, respectively, that lives in a semantic embedding space~ with a low dimension~.
A model is defined by a real valued score function~.
One fits the embedding vectors such that~ assigns a high score to observed triplet facts~ in the training set~ and a low score to triplets that do not appear in~.


\paragraph{Examples.}
We give examples of the two models that reach highest predictive performance to the best of our knowledge.
For more models, see~\citep{KBK2017}.

In the DistMult model~\citep{BWXJL2015}, the embedding space  is real valued, and the score is defined as

where, e.g.,  is the \textsuperscript{th} entry of the vector~.

The ComplEx model~\citep{TWR2016} uses a complex embedding space , and defines the score

where  denotes the real part of a complex number, and~ is the complex conjugate of~.


\paragraph{Tail And Head Prediction.}
Typical benchmark tasks for KG embeddings are `tail prediction' and `head prediction', i.e., completing queries of the form  and , respectively, by ranking potential completions  by their score .
Most proposals for KG embeddings train a single model for both tail and head prediction.
Thus, the loss function is given by Eq.~\ref{eq:loss_one_lambda}, where  is a sum of two terms to train for tail and head prediction, respectively.
While early works (e.g.,~\citep{BUGWY2013,WZFC2014,BWXJL2015}) trained by maximizing a margin over negative samples, the more recent literature~\citep{KBK2017,LKHJ2018} suggests that the softmax loss leads to better predictive performance,


Here, the first line (with the sum over tails~) is the softmax loss for tail prediction, while the second line (with the sum over heads~) is the softmax loss for head prediction.


\subsection{REGULARIZATION IN KG EMBEDDINGS}
\label{sec:regularization}

Knowledge graph embedding models are highly sensitive to hyperparameters, especially to the strength of the regularizer~\citep{KBK2017}.
This can be understood since even large knowledge graphs typically contain only few data points per entity.
For example, the FB15K data set contains  data points, but  of all entities  appear fewer than  times as head or tail of a training point.
Moreover, the amount of training data varies strongly across entities and relations (see Figure~\ref{fig:counts}), suggesting that the regularization strength for embedding vectors~ and~ should depend on the entity~ and relation~.

The loss function for conventional KG embeddings in Eq.~\ref{eq:loss_one_lambda} regularizes all embedding vectors with the same strength~.
We propose to replace~ by individual regularization strengths~ and~ for each entity~ and relation~, respectively, and to fit models with the loss function in Eq.~\ref{eq:loss_individual_lambdas}.
It generalize Eq.~\ref{eq:loss_one_lambda}, which one obtains for

where  and  denote the number of times that entity~ or relation~ appears in the training data, respectively.
The proposed augmented models described by Eq.~\ref{eq:loss_individual_lambdas} are more flexible as they do not impose a linear relationship between~ and the regularization strength~.

The downside of the augmented KG embedding models is that one has to tune a macroscopic number of hyperparameters  and~:
more than  in the popular FB15K data set.
Tuning such a large number of hyperparameters would be far too computationally expensive in a conventional setup that fits point estimates by minimizing the loss function.
For point estimated models, it is well known that one cannot fit hyperparameters to the training data as this would lead to overfitting (see also Supplementary Material).
To avoid overfitting, knowledge graph embedding models are conventionally tuned by cross validation on heldout data.
This requires training a model from scratch for each new hyperparameter setting.
Cross validation does not scale beyond models with a handful of hyperparameters, and it is expensive even there (see, e.g.,~\citep{KBK2017,LUO2018}).

Probabilistic models, by contrast, allow tuning of many hyperparameters in parallel using the empirical Bayes method~\citep{dempster1977maximum,maritz2018empirical}.
We propose a probabilistic formulation of augmented KG embeddings in the next section, and we present a method for efficient hyperparameter tuning in these models in Section~\ref{sec:method}.


\subsection{PROBABILISTIC KG EMBEDDINGS}
\label{sec:probabilistic_models}


We now present our probabilistic version of KG embeddings.
The probabilistic formulation enables efficient optimization over thousands of hyperparameters, see Section~\ref{sec:method}.


\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figs/graphical_model}
  \caption{Two different generative processes of triplet facts .
  Hyperparameters (regularizer strengths) highlighted in blue.
  Left: proposed class of models augmented with individual regularizer strengths~.
  Right: probabilistic interpretation of conventional models~(Eq.~\ref{eq:loss_one_lambda}).
  The conventional models fix the relative strength of regularizers by reducing all~ to a single scalar~.}\label{fig:generative}
\end{figure}

\paragraph{Reciprocal Facts.}
The KG embedding models discussed in Sections~\ref{sec:conventional_models} and~\ref{sec:regularization} make a direct interpretation as a generative probabilistic process difficult.
Training a single model for both head and tail prediction introduces cyclic causal dependencies.
As will become clear below, the tail prediction part in Eq.~\ref{eq:individual_loss} (first line on the right-hand side) corresponds to a generative process where the head~ causes the tail~.
However, the head prediction part (second line) corresponds to a generative process where~ causes~.

To solve this issue, we employ a data augmentation due to~\citet{LUO2018} that goes as follows.
For each relation , one introduces a new symbol~, which has the interpretation of the inverse of~, but whose embedding vector is not tied to that of~.
One then constructs an augmented training set~ by adding the reciprocal facts,

One trains the model by minimizing the loss in Eq.~\ref{eq:loss_one_lambda} or Eq.~\ref{eq:loss_individual_lambdas}, where the sum over data points is now over~ instead of~, and where  is given by only the first line of Eq.~\ref{eq:individual_loss}.
When evaluating the model performance on a test set, one answers head prediction queries  by answering the corresponding tail prediction query .
This data augmentation was introduced in~\citep{LUO2018} to improve performance.
As we show next, it also has the advantage of enabling a probabilistic interpretation by establishing a causal order where~ comes before~.


\paragraph{Generative Process.}
With the above data augmentation, minimizing the loss function in Eq.~\ref{eq:loss_individual_lambdas} is equivalent to point estimating the parameters of the probabilistic graphical model shown in Figure~\ref{fig:generative} (left).
The generative process is:
\begin{itemize}
  \item
    For each entity  and each relation , draw an embedding  from the priors
    
    Here,  specifies the norm, and the omitted proportionality constant follows from normalization.
  \item
    Repeat for each data point to be generated:
    \begin{itemize}[topsep=-1pt,leftmargin=15pt]
      \item
        Draw a head entity  and a relation  from some discrete distribution .
        The choice of this distribution has no influence on inference since~ and~ are both directly observed.
      \item
        Draw a tail entity
        
        where  is the score (e.g., Eq.~\ref{eq:score_distmult} or~\ref{eq:score_complex}), and
        
      \item
        Add the triplet fact  to the data set~.
    \end{itemize}
\end{itemize}

This process defines a log joint distribution over~,~, and the data , conditioned on the hyperparameters~, which we denote collectively by the boldface symbol~,

Using Eq.~\ref{eq:categorical}, it is easy to see that  is the negative of the first line of Eq.~\ref{eq:individual_loss}.
Thus, up to an additive term that depends only on~, the log joint distribution in Eq.~\ref{eq:log_joint} is the negative of the loss function, and minimizing the loss over~ and~ is equivalent to a maximum a posteriori (MAP) approximation of the probabilistic model.

Figure~\ref{fig:generative} compares the generative process of the augmented KG embeddings proposed in Section~\ref{sec:regularization} (left part of the figure) to the generative process for conventional KG embeddings that one obtains by setting~ and~ as in Eq.~\ref{eq:proportional_lambdas} (right).
The augmented models are more flexible due to the large number of hyperparameters~.
We discuss next how the probabilistic interpretation allows us to efficiently optimize over this large number of hyperparameters.
 \section{HYPERPARAMETER OPTIMIZATION}
\label{sec:method}

We now describe the proposed method for hyperparameter tuning in the probabilistic knowledge graph embedding models introduced in Section~\ref{sec:probabilistic_models}.
The method is based on variational expectation-maximization (EM).
We first derive an approximate coordinate update equation for the hyperparameters (Section~\ref{sec:variational_em}) and then cover details of the parameter initialization (Section~\ref{sec:algorithm_phases}).

Variational EM optimizes a lower bound to the marginal likelihood of the model over hyperparameters , with model parameters  and  integrated out.
As we show in the supplementary material, the naive alternative of simultaneously optimizing the original model's loss function
over model parameters and hyperparameters would lead to divergent solutions.
Variational EM avoids such divergent solutions by keeping track of parameter uncertainty.
We elaborate on the role of parameter uncertainty in the supplementary material.


\begin{algorithm2e}[t!]
  \setlength{\hsize}{\dimexpr(\columnwidth-0.8em)}
  \SetKwComment{Comment}{}{}
  \DontPrintSemicolon
  \SetKwFor{Repeat}{repeat}{times}{end}
  \SetKwRepeat{Repeatuntil}{repeat}{until}
  \begin{tabularx}{\hsize}{@{}l@{}>{\raggedright\arraybackslash}X@{}}
    \textbf{Input:}
      & Augmented data set , see Eq.~\ref{eq:augmented_dataset}. \\
      & Model  as defined by Eqs.~\ref{eq:priors}-\ref{eq:log_joint}.\\
      & Hyperparameters .
        Learning rate . \2pt]
    \textbf{Output:} & Optimized hyperparameters , embedding vectors  with uncertainties.
  \end{tabularx}\\\vspace{2pt}
  \nl Initialize means~ and log standard deviations~ around a pretrained model, see Section~\ref{sec:algorithm_phases}.\;\vspace{2pt}
  \nl \For{ \KwTo }{
    \nl Draw a minibatch .\;
    \nl Draw Gaussian noise . \label{ln:draw_noise}\;
    \nl Calculate a minibatch estimate  of the loss with injected noise,  (``'' denotes elementwise multiplication). \label{ln:inject_noise}\;
    \nl Update .\;
    \nl Update . \label{ln:update_xi}\;
    \vspace{3pt}
    \nl \If{}{
      \nl Update . \label{ln:update_lambda}\;
      \Comment*[r]{\rm\textit{M-step (see Eq.~\ref{eq:optimal_lambda}).}}
      \vspace{-2.67ex}
    }
  }
  \caption{Variational EM for knowledge graph embedding models with coordinate updates for .}
  \label{alg:variational_em}
\end{algorithm2e}


\paragraph{Variational Expectation-Maximization.}
Our probabilistic interpretation of knowledge graph embedding models allows us to optimization over all hyperparameters  and  in parallel via the expectation-maximization (EM) algorithm~\citep{dempster1977maximum}.
This algorithm treats the model parameters~ and~ as latent variables that have to be integrated out.
The EM algorithm alternates between a step in which the latent variables are integrated out (`E-step'), and an update step for the hyperparameters~ (`M-step').
We use a version of EM based on variational inference, termed variational EM \citep{bernardo2003variational}, that avoids the integration step.
We further derive an approximate coordinate update equation for the hyperparameters~, which lead to a significant speedup over gradient updates in our experiments.

Each choice of hyperparameters~ defines a different variant of the model.
The marginal likelihood of the data,

quantifies how well a given model variant describes the data~.
Maximizing  over  thus yields the model variant that fits the data best.
However,  is unavailable in closed form as the integral in Eq.~\ref{eq:marginal_likelihood} is intractable.

To circumvent the problem of the intractable marginal likelihood, we use variational inference (VI) \citep{JGJS1999}.
Rather than integrating over the entire space of model parameters  and , we maximize a lower bound on the marginal likelihood.
We introduce a so-called variational family of Gaussian probability distributions,

with

and analogously for .
Here, the means  and the standard deviations  are so-called variational parameters over which we optimize.

Evoking Jensen's inequality, the log marginal likelihood is then lower-bounded by the \emph{evidence lower bound} \citep{BKM2017,ZBKM2017}, or ELBO:

Here, in the second step, we identified the log joint probability as the negative of the loss  of the corresponding point estimated model, and  is the entropy of .


The bound in Eq.~\ref{eq:elbo} is tight if the variational distribution~ is the true posterior of the model for given .
Since it is a lower bound, maximizing the ELBO over~ and  minimizes the gap and yields the best approximation of the marginal likelihood.
We thus take the ELBO as a proxy for the marginal likelihood, and we maximize it also over  to find near-optimal hyperparameters.


\paragraph{Gradient updates for  and .}
We maximize the ELBO concurrently over both variational parameters  and  as well as over hyperparameters .
Updating the variational parameters is called the ``E-step''.
Here, we use gradient updates using Black Box reparameterization gradients \citep{KW2014,RMW2014}.
This has the advantage of being agnostic to the model architecture as long as the score  (e.g., Eqs.~\ref{eq:score_distmult}-\ref{eq:score_complex}) is differentiable, and it requires only few changes compared to the standard SGD training loop in Algorithm~\ref{alg:sgd}.

To make sure that the standard deviations are always positive, we parameterize them by their logarithms~,

and we optimize over~ and~ using SGD.
We obtain an unbiased estimate of the term  in Eq.~\ref{eq:elbo} by drawing a single sample from  (lines~\ref{ln:draw_noise}-\ref{ln:inject_noise} in Algorithm~\ref{alg:variational_em}).
The reparameterization gradient trick uses the fact that for,~e.g., noise  from a standard normal distribution,  is distributed as .
The entropy part  of the ELBO (Eq.~\ref{eq:elbo}) can be calculated analytically.
Up to an additive constant, it is given by the sum over all log standard deviations ~and~.
Thus, its gradient with respect to  has the constant value of one in each coordinate direction, which we denote by the bold face term~``'' on line~\ref{ln:update_xi} of Algorithm~\ref{alg:variational_em}.

\paragraph{Coordinate updates for .}
Optimizing the ELBO over  leads to an improved set of hyperparameters provided that the ELBO is a good approximation of the marginal likelihood .
However, this is typically not the case at the beginning of the optimization when the variational distribution is still a poor fit of the posterior.
We therefore begin the optimization with some number  of pure ``E-step'' updates during which we keep  fixed.
After  ``E-steps'', we alternate between ``E'' and ``M'' steps, where the latter update the hyperparameters~.
In our experiments, we found that the optimization converged slowly when we used gradient updates for~.
To speed up convergence, we therefore derive approximate coordinate updates for .

To simplify the notation, we derive the update equation only for a single hyperparameter~.
Updates for~ are analogous.
The only term in the ELBO (Eq.~\ref{eq:elbo}) that depends on  is the expected log prior, .
Since this term is independent of the data we can write it out explicitly.
The omitted proportionality constant in the prior (Eq.~\ref{eq:priors}) is dictated by normalization.
We find,

where  for a real-valued embedding space  (as in DistMult) and  if  (as in ComplEx).
Setting  to zero we find that the regularizer strength  that maximizes the ELBO for given  and  satisfies


In moderately high embedding dimensions~, we can approximate the right-hand side of Eq.~\ref{eq:optimal_lambda} accurately by sampling from .
It is the expectation of the average of a large number of independent random variables, and therefore follows a highly peaked distribution.
The update step on line~\ref{ln:update_lambda} of Algorithm~\ref{alg:variational_em} uses a conservative weighted average between the current and the optimal value of  with a learning rate .
This effectively averages the estimates over past training steps with a decaying weight.
Note that, for , Eq.~\ref{eq:optimal_lambda} has a closed form solution for  and , but we found it unnecessary in our experiments to implement specialized code for these cases.

\paragraph{Absence of overfitting.}
While the variational EM algorithm keeps track of uncertainty of model parameters, it fits only point estimates for the hyperparameters~.
This is justified in our setup since there are much fewer hyperparameters than model parameters:
each entity~ and each relation~ has an embedding vector~ or~ with  scalar components in our experiments, but only a single scalar hyperparameter~ or~.
We therefore expect much smaller posterior uncertainty for~ than for~ and~, which justifies point estimating~.
Had we instead chosen a very flexible prior distribution with many hyperparameters per entity and relation, the EM algorithm would have essentially fitted the prior to the variational distribution, leading to an ill-posed problem.
Judging from learning curves on the validation set, we did not detect any overfitting in variational EM.


\subsection{PRE- AND RE-TRAINING}
\label{sec:algorithm_phases}

Variational EM (Algorithm~\ref{alg:variational_em}) converges more slowly than fitting point estimates (Algorithm~\ref{alg:sgd}) because the injected noise increases the variance of the gradient estimator.
To speed up convergence, we train the model in three consecutive phases: pre-training, variational EM, and re-training.

In the pre-training phase, we keep the hyperparameters~ fixed and fit point estimates ~and~ to the model using standard SGD (Algorithm~\ref{alg:sgd}).
We found the final predictive performance (after the variational EM and re-training phases) to be insensitive to the initial hyperparameters.
We use early stopping based on the mean reciprocal rank (see Eq.~\ref{eq:mrr} in Section~\ref{sec:experiments} below), evaluated on the validation set.

In the variational EM phase (Algorithm~\ref{alg:variational_em} and Section~\ref{sec:variational_em}), we initialize the variational distribution  around the pre-trained model parameters  and .
In detail, we initialize  and , and we initialize the components of  with a value that is small compared to the typical components of  (0.2 in our experiments).

In the re-training phase, we fit again point estimates~ and~ with Algorithm~\ref{alg:sgd}, this time using the optimized hyperparameters~.
We use the resulting models to evaluate the predictive performance, see results in Section~\ref{sec:experiments}.

Alternatively to re-training a point estimated model, one could also perform predictions by averaging predictive probabilities over samples from the variational distribution~.
If  is a good approximation of the model posterior then this results in an approximate Bayesian form of link prediction.
In our experiments, we found that, in low embedding dimensions~, predictions based on samples from  outperformed predictions based on point estimates.
In higher embedding dimensions, however, the point estimated models from the re-training phase had better predictive performance.
We interpret this somewhat counterintuitive observation as a failure of the fully factorized Gaussian variational approximation to adequately approximate the true posterior.
 \begin{table*}[tb]
  \caption{
  Model performances (in `filtered' setting; see Eqs.~\ref{eq:mrr}-\ref{eq:hits_at_10}).
  \citet{LUO2018} report performance metrics only with two decimals.
  In order to show three decimals, we reproduced their results using the code provided by the authors.
  When rounding to two digits, we recover all values reported in~\citep{LUO2018} except that the MRR for \hbox{FB15K-237} is reported there as 0.37.
  The small discrepancy in the last decimal may be explained by different random seeds.
  }
  \centering
  \begin{tabularx}{\textwidth}{llr@{}c@{}cc@{}cc@{}cc@{}c}
    \toprule
    \multicolumn{3}{r@{}}{data set }
      & \multicolumn{2}{c}{WN18RR}
      & \multicolumn{2}{c}{WN18}
      & \multicolumn{2}{c}{FB15K-237}
      & \multicolumn{2}{c}{FB15K} \\
     model &  variant & metric 
      & MRR & Hits@10
      & MRR & Hits@10
      & MRR & Hits@10
      & MRR & Hits@10 \\\midrule\midrule
    DistMult & \multicolumn{2}{l@{}}{\citet{BWXJL2015} (orig.)}
      & -- & --
      & 0.83 & 0.942
      & -- & --
      & 0.35 & 0.577 \\
    DistMult & \multicolumn{2}{l@{}}{\citet{KBK2017}}
      & -- & --
      & 0.790 & 0.950
      & -- & --
      & 0.837 & 0.904 \\
    DistMult & \multicolumn{2}{l@{}}{\citet{DMSR2017}}
      & 0.43 & 0.49
      & 0.822 & 0.936
      & 0.241 & 0.419
      & 0.654 & 0.824 \\
    DistMult & \multicolumn{2}{l@{}}{Ours (after variational EM)}
      & \textbf{0.455} & \textbf{0.544}
      & \textbf{0.911} & \textbf{0.961}
      & \textbf{0.357} & \textbf{0.548}
      & \textbf{0.841} & \textbf{0.914} \\\midrule
    ComplEx & \multicolumn{2}{l@{}}{\citet{TWR2016} (orig.)}
      & -- & --
      & 0.941 & 0.947
      & -- & --
      & 0.692 & 0.840 \\
    ComplEx & \multicolumn{2}{l@{}}{\citet{LUO2018}}
      & 0.478 & 0.569
      & 0.952 & 0.963
      & 0.364  & 0.555
      & \textbf{0.857} & 0.909 \\
    ComplEx & \multicolumn{2}{l@{}}{Ours (after variational EM)}
      & \textbf{0.486} & \textbf{0.579}
      & \textbf{0.953} & \textbf{0.964}
      & \textbf{0.365}& \textbf{0.560}
      & 0.854 & \textbf{0.915} \\\bottomrule
  \end{tabularx}\label{table:results}
\end{table*}


\section{EXPERIMENTAL RESULTS}
\label{sec:experiments}

We test the performance of the proposed model augmentation and the scalable hyperparameter tuning algorithm with two models and four different data sets.
In this section, we report results using standard benchmark metrics and we compare to the previous state of the art.
We also analyze the relationship between the optimized regularizer strengths and the frequency of entities and relations.


\paragraph{Model architectures and baselines.}
We report results for the DistMult model~\citep{BWXJL2015} and the ComplEx model~\citep{TWR2016}.
We follow \citep{LUO2018} for details of the model architecture: we use reciprocal facts as described at the end of Section~\ref{sec:probabilistic_models}, -norm regularizers, and an embedding dimension of .
We compare our results to the previous state of the art:
\citep{DMSR2017,KBK2017} for DistMult and \citep{LUO2018} for ComplEx.

\paragraph{Data sets.}
We used four standard data sets.
The first two are FB15K from the Freebase project~\citep{BEPST2008} and WN18 from the WordNet database~\citep{BGWB2014}.
The other two data sets, FB15K-237 and WN18RR, are modified versions of FB15K and WN18 due to~\citep{TC2015,DMSR2017}.
The motivation for the modified data sets is that FB15K and WN18 contain near duplicate relations that lead to leakage into the test set, which makes link prediction trivial for some facts, thus encouraging overfitting.
In FB15K-237 and WN18RR these near duplicates were removed.

\paragraph{Metrics.}
We report two standard metrics used in the KG embedding literature: mean reciprocal rank (MRR) and Hits@10.
We average over head and tail prediction on the test set~, which is equivalent to averaging only over tail prediction on the augmented test set~, see Eq.~\ref{eq:augmented_dataset}.

All results are obtained in the `filtered' setting introduced in~\citep{BUGWY2013}, which takes into account that more than one tail may be a correct answer to a query .
When calculating the rank of the target tail~ one therefore ignores any competing tails~ if the corresponding fact  exists in either the training, validation, or test set.
More formally, the fitlered rank, denoted below as , is defined as one plus the number of `incorrect' facts  with the the given~ and~ for which .
Here, candidate facts  are considered `incorrect' if they appear neither in the training nor in the validation or test set.

Given a test set~ with reciprocal facts (Eq.~\ref{eq:augmented_dataset}), the mean reciprocal rank (MRR) is (see, e.g., \citep{BWXJL2015}),

With `Hits@10', we denote the fraction of test queries for which the filtered rank is at most 10,



\paragraph{Quantitative results.}
Table~\ref{table:results} summarizes our quantitative results.
The top half of the table shows results for the DistMult model.
Our models with individually optimized regularization strengths significantly outperform the previous state of the art across all four data sets.

For the ComplEx model, the performance improvements are less pronounced (lower half of Table~\ref{table:results}).
This may be explained by the fact that the results in~\citep{LUO2018} were already obtained after large scale expensive hyperparameter tuning using grid search.
By contrast, the hyperparameter search with our proposed method required only a single run per data set.
Even for the largest data set FB15K, the variational EM phase took less than three hours on a single GPU.
Despite the much cheaper hyperparameter optimization, our models slightly outperform the previous state of the art on three out of the four considered data sets, with only a small degradation on the fourth.



\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{figs/lambdas.png}
  \caption{Relationship between the learned regularizer strengths  and the frequency of each entity.
  The conventional proportional scaling (red line) overregularizes frequent entities and underregularizes infrequent ones.}\label{fig:lambda_freq}
\end{figure}


\paragraph{Qualitative results.}
Finally, we study the relationship between optimized hyperparameters and frequencies of entities in the training data.
Figure~\ref{fig:lambda_freq} shows the learned  for all entities  as a function of the number of times  that each entity  appears in the training corpus of the FB15K data set.
The red line is the best proportional fit  to the results, as is imposed by conventional models (Eq.~\ref{eq:proportional_lambdas}).

Our findings confirm the general idea to use stronger regularization for entities with more training data.
The Bayesian interpretation can explain this observation:
a small amount of training data typically leads to high posterior uncertainty, which leads to small  in Eq.~\ref{eq:optimal_lambda}.
However, our results indicate that imposing a proportionality between  and  would be a poor choice that significantly underregularizes infrequent entities and overregularizes frequent entities (note the logarithmic scale in Figure~\ref{fig:lambda_freq}).
Our empirical findings may inspire future theoretical work that derives an optimal frequency dependency of the regularization strength in tensor factorization models.
 \section{RELATED WORK}\label{sec:related}
Related work to this paper can be grouped into link prediction algorithms and variational inference.

\paragraph{Link Prediction.}
Link prediction in knowledge graphs has gained a lot of attention as it may guide a way towards automated reasoning with real world data.
For a review see \citep{nickel2016review}.
Two different approaches for link prediction are predominant in the literature.
In statistical relational learning, one infers explicit rules about relations (such as transitivity or commutativity) by detecting statistical patterns in the training set.
One then uses these rules for logic reasoning~\citep{FGKP1999,KNP2011,NZRS2012,PMGC2015}.

Our work focuses on a complementary approach that builds on knowledge graph embedding models.
This line of research started with the proposal of the TransE model~\citep{BUGWY2013}, which models relational facts as vector additions in a semantic space.
More recently, a plethora of different knowledge graph embedding models based on tensor factorizations have been proposed.
We summarize here only the path that lead to the current state of the art.
Different models make different trade-offs between generality and effective use of training data.

Canonical tensor decomposition~\citep{H1927} uses independent embeddings for entities in the head or tail position of a fact.
DistMult~\citep{BWXJL2015,TC2015}, by contrast, uses the same embeddings for entities in head and tail position, thus making use of more training data per entity embedding, but restricting the model to symmetric relations.
The ComplEx model~\citep{TWR2016} lifts this restriction by multiplying the head, relation, and tail embeddings in an asymmetric way.
To the best of our knowledge, the current state of the art was presented by \citet{LUO2018}, who improved upon the ComplEx model by introducing reciprocal relations and using a better regularizer.

The sensitivity of KG embeddings to the choice of hyperparameters, such as regularizer strengths, was first pointed out in~\citep{KBK2017}.
A popular heuristic is to regularize each embedding every time it appears in a minibatch, thus effecitvely regularizing embeddings proportionally to their frequency~\citep{SS2010,LUO2018}.
In contrast, we propose to learn entity-dependent regularization strengths without relying on heuristics.

\cite{vilnis2018probabilistic} proposed a new model that is probabilistic in the sense that it assigns probabilities to the results of queries.
However, in contrast to our proposal, the model is not a probabilistic generative model of the data set.

\paragraph{Variational Inference.} Variational inference (VI) is a powerful technique to approximate a Bayesian posterior over latent variables given observations \citep{JGJS1999,BKM2017,ZBKM2017}. Besides approximating the posterior, VI also estimates the marginal likelihood of the data. This allows for iterative hyperparameter tuning, (variational EM) \citep{bernardo2003variational}, which is the main benefit of the Bayesian approach used in this paper.

Our paper builds on recent probabilistic extensions of embedding models to Bayesian models, such as word \citep{B2017} or paragraph \citep{JBSM2017} embeddings. In these works, the words are embedded into a -dimensional space. It has been shown that using a probabilistic approach leads to better performance on small data sets, and allows these models to be combined with powerful priors, such as for time series modeling~\citep{BM2017,bamler2018improving,JWKM2018}. Yet, the underlying probabilistic models in these papers are very different from the ones considered in our work.

\paragraph{Bayesian Optimization.}
An alternative method that can be used for hyperparameter optimization is Bayesian optimization.
However, Bayesian optimization does not scale to the large number of hyperparameters that we tune in this work.
Most practical applications of Bayesian optimization (e.g.,~\citep{snoek2012practical,wang2013bayesian}) tune only tens of hyperparameters, rather than ten thousands.
This is because Bayesian optimization treats the model as a black box, which it can only train and then evaluate for a given choice of hyperparameters at a time.
Each such evaluation contributes a single data point to fit an auxiliary model over the hyperparameters.
By contrast, variational EM has access to gradient information to train all hyperparameters in parallel, and concurrently with the model parameters.
 \section{CONCLUSIONS}
\label{sec:conclusions}

We augmented a large class of popular knowledge graph embedding models in such a way that every entity embedding and every relationship embedding vector has their own regularizer, and showed that it is possible to tune these potentially thousands of hyperparameters in a scalable way. Our approach is motivated by the observation that sharing a common regularization parameter across all embeddings leads to over-regularization.

We treat knowledge graph embeddings as generative probabilistic models, making them amenable to Bayesian model selection.
We derived approximate coordinate updates for the hyperparameters in the framework of variational EM.
We applied our method to generalizations of the DistMult and ComplEx models and outperformed the state of the art for link prediction. The approach
can be applied to a wide range of models with minimal modifications to the training routine. In the future, it would be interesting to investigate whether tighter variational bounds~\citep{burda2016importance,bamler2017perturbative} may further improve model selection.
 
\newpage


\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
