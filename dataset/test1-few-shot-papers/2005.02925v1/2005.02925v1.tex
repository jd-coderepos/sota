\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}

\usepackage{verbatim}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{capt-of}
\usepackage{tabularx}
\usepackage[caption=false]{subfig}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{scalerel}
\usepackage[inline]{enumitem}
\usepackage{listings}
\usepackage{varwidth}
\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\usepackage{todonotes}
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--}
\usepackage{stmaryrd}
\usepackage{bbm}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\tx}[1]{``\textit{#1}''}
\newcommand{\sptk}[1]{\texttt{[#1]}}
\newcommand{\eqform}[1]{Equation~(\ref{#1})}
 

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[ruled,noend]{algorithm2e}
\usepackage{graphicx}
\usepackage{url}

\usepackage{footnote}
\makesavenoteenv{table}

\aclfinalcopy \def\aclpaperid{812} 

\newcommand\refqa{\textsc{RefQA}}
\newcommand{\li}[1]{{\color{blue}{[{Li}: #1]}}}
\newcommand{\wang}[1]{{\color{red}{[{Wang}: #1]}}}
\newcommand{\zhongli}[1]{{\color{olive}{[{Zhongli}: #1]}}}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\linote}[1]{{\let\thempfn\relax \footnotetext[0]{\emph{#1}}}}

\title{Harvesting and Refining Question-Answer Pairs \\ for Unsupervised QA}
\author{Zhongli Li\thanks{~~Contribution during internship at Microsoft Research.},~~Wenhui Wang,~~Li Dong,~~Furu Wei,~~Ke Xu \\
  Beihang University \\
  Microsoft Research \\
  \texttt{\{lizhongli@,kexu@nlsde.\}buaa.edu.cn} \\
  \texttt{\{wenwan,lidong1,fuwei\}@microsoft.com}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available.
In this work, we introduce two approaches to improve unsupervised QA. 
First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as \refqa{}).
Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over \refqa{}.
We conduct experiments\footnote{The code and data are available at \url{https://github.com/Neutralzz/RefQA}.} on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data.
Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models.
We also show the effectiveness of our approach in the few-shot learning setting.
\end{abstract}


\section{Introduction}



Extractive question answering aims to extract a span from the given document to answer the question. 
Rapid progress has been made because of the release of large-scale annotated datasets~\citep{squad1, squad2, triviaqa}, and well-designed neural models~\citep{wang2016matchlstm, bidaf, qanet}. Recently, unsupervised pre-training of language models on large corpora, such as BERT~\citep{BERT}, has brought further performance gains.



However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is time-consuming and requires significant resources, especially for new domains or languages.
In order to tackle the setting in which no training data available,~\citet{lewis2019unsupervisedqa} 
leverage unsupervised machine translation to generate synthetic context-question-answer triples.
The paragraphs are sampled from Wikipedia. NER and noun chunkers are employed to identify answer candidates.
Cloze questions are first extracted from the sentences of the paragraph, and then translated into natural questions. 
However, there are a lot of lexical overlaps between the generated questions and the paragraph. 
Similar lexical and syntactic structures render the QA model tend to predict the answer just by word matching.
Moreover, the answer category is limited to the named entity or noun phrase, which restricts the coverage of the learnt model.




In this work, we present two approaches to improve the quality of synthetic context-question-answer triples. 
First, we introduce the \refqa{} dataset, 
which harvests lexically and syntactically divergent questions from Wikipedia by using the cited documents. 
As shown in Figure~\ref{fig:da}, the sentence (statement) in Wikipedia and its cited documents are semantically consistent, but written with different expressions. 
More informative context-question-answer triples can be created by using the cited document as the context paragraph and extracting questions from the statement in Wikipedia.
Second, we propose to iteratively refine data over~\refqa{}.
Given a QA model and some~\refqa{} examples, we first filter its predicted answers with a probability threshold. Then we refine questions based on the predicted answers, and obtain the refined question-answer pairs to continue the model training.
Thanks to the pretrained linguistic knowledge in the BERT-based QA model, there are more appropriate and diverse answer candidates in the filtered predictions, some of which do not appear in the candidates extracted by NER tools.
We also show that iteratively refining the data further improves model performance.














We conduct experiments on SQuAD 1.1~\citep{squad1}, and NewsQA~\citep{newsqa2017}. Our method yields state-of-the-art results against strong baselines in the unsupervised setting. 
Specifically, the proposed model achieves 71.4 F1 on the SQuAD 1.1 test set and 45.1 F1 on the NewsQA test set without using annotated data. We also evaluate our method in a few-shot learning setting. Our approach achieves 79.4 F1 on the SQuAD 1.1 dev set with only 100 labeled examples, compared to 63.0 F1 using the method of~\citet{lewis2019unsupervisedqa}.



To summarize, the contributions of this paper include: 
i) \refqa{}~constructing in an unsupervised manner, which contains more informative context-question-answer triples.
ii) Using the QA model to iteratively refine and augment the question-answer pairs in \refqa{}.


\begin{figure*}[t]
    \centering
    \includegraphics[width=16cm]{images/wikiref-example.pdf}
    \caption{Overview of \refqa{} construction.}
    \label{fig:da}
\end{figure*}

\section{Related Work}

\paragraph{Extractive Question Answering}
Given a document and question, the task is to predict a continuous sub-span of the document to answer the question. 
Extractive question answering has garnered a lot of attention over the past few years. 
Benchmark datasets, such as SQuAD~\citep{squad1,squad2}, NewsQA~\citep{newsqa2017} and TriviaQA~\citep{triviaqa}, play an important role in the progress.
In order to improve the performance on these benchmarks, several models have been proposed, including BiDAF~\citep{bidaf}, R-NET~\citep{wang-rnet}, and QANet~\citep{qanet}. 
Recently, unsupervised pre-training of language models such as BERT~\citep{BERT}, 
achieves significant improvement. 
However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct.

\paragraph{Semi-Supervised QA} 
Several semi-supervised approaches have been proposed to utilize unlabeled data.
Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models~\cite{yang-semiqa,unans-qg,syntheticqa,unilm}. However, the methods require labeled data to train the sequence-to-sequence QG model. 
\citet{dhingra2019semi} propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. 


\paragraph{Unsupervised QA} \citet{lewis2019unsupervisedqa} have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract ``fill-in-the-blank'' cloze-style questions given the candidate answer and context. iv) Translate cloze-style questions into natural questions by an unsupervised translator. Compared with~\citet{dhingra2019semi},~\citet{lewis2019unsupervisedqa} attempt to generate natural questions by training an unsupervised neural machine translation (NMT) model. They train the NMT model on non-aligned corpora of natural questions and cloze questions. The unsupervised QA model of~\citet{lewis2019unsupervisedqa} achieves promising results, even outperforms early supervised models.
However, their questions are generated from the sentences or sub-clauses of the same paragraphs, which may lead to a biased learning of word matching since its similar lexicons and syntactic structures. 
Besides, the category of answer candidates is limited to named entity or noun phrase, which restricts the coverage of the learnt QA model. 



\section{Harvesting \refqa{} from Wikipedia}
\label{sec:da}



In this section, we introduce \refqa{}, a question answering dataset constructed in an unsupervised manner.
One drawback of~\citet{lewis2019unsupervisedqa} is that questions are produced from the paragraph sentence that contains the answer candidate. So there are considerable expression overlaps between generated questions and context paragraphs.
In contrast, we harvest informative questions by taking advantage of Wikipedia's reference links, where lexical and syntactic differences exist between the article and its cited documents.



As shown in Figure~\ref{fig:da}, given statements in Wikipedia paragraphs and its cited documents, we use the cited documents as the context paragraphs and generate questions from the sub-clauses of statements.
In order to generate question-answer pairs, we first find answer candidates that appear in both sub-clauses and context paragraphs. 
Next, we convert sub-clauses into the cloze questions based on the candidate answers.
We then conduct cloze-to-natural-question translation by a dependency tree reconstruction algorithm.
We describe the details as follows.




\subsection{Context and Answer Generation}
Statements in Wikipedia and its cited documents often have similar content, but are written with different expressions.
Informative questions can be obtained by taking the cited document as the context paragraph, and generate questions from the statement.
We crawl statements with reference links from the English Wikipedia.
The cited documents are obtained by parsing the contents of reference webpages.







Given a statement and its cited document, we restrict the statement to its sub-clauses, and extract answer candidates (i.e., named entities) that appear in both of them by using a NER toolkit.
We then find the answer span positions in the context paragraph. If the candidate answer appears multiple times in the context, we select the position whose surrounding context has the most overlap with the statement.



\subsection{Question Generation}
\label{ssec:qg_section}

We first generate cloze questions~\cite{lewis2019unsupervisedqa} from the sub-clauses of Wikipedia statements.
Then we introduce a rule-based method to rewrite them to more natural questions, which utilizes the dependency structures.


\subsubsection{Cloze Generation}
Cloze questions are the statements with the answer replaced to a mask token.
Following~\citet{lewis2019unsupervisedqa}, we replace answers in statements with a special mask token, which depends on its answer category\footnote{We obtain the answer type labels by a NER toolkit, and group these labels to high-level answer categories, which are used as our mask tokens, e.g., \texttt{PRODUCT} corresponding to \texttt{THING}, \texttt{LOC} corresponding to \texttt{PLACE}.}.
Using the statement and the answer (with a type label \texttt{PRODUCT}) from Figure~\ref{fig:da}, this leaves us with the cloze question ``\textit{Guillermo crashed a Matt Damon interview, about his upcoming movie }\texttt{[THING]}".






\subsubsection{Translate Clozes to Natural Questions}
\label{sec:dep:rec}




We perform a dependency reconstruction to generate natural questions. We move answer-related words in the dependency tree to the front of the question, since answer-related words are important.
The intuition is that natural questions usually start with question words and question focus~\cite{qa:as:ie}.

As shown in Figure~\ref{fig:translate_cloze}, we apply the dependency parsing to the cloze questions, and translate them to natural questions by three steps: i) We keep the right child nodes of the answer and prune its lefts. ii) For each node in the parsing tree, if the subtree of its child node contains the answer node, we move the child node to the first child node.
iii) Finally, we obtain the natural question by inorder traversal on the reconstructed tree. We apply the same rule-based mapping as~\citet{lewis2019unsupervisedqa}, which replaces each answer category with the most appropriate wh* word. For example, the \texttt{THING} category is mapped to ``\textit{What}".



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/dtrc.pdf}
    \caption{Example of translating cloze questions to natural questions. The node with light yellow color indicates that its subtree contains the answer node.}
    \label{fig:translate_cloze}
\end{figure}

\begin{comment}
\begin{algorithm}
\caption{Dependency Reconstruction}\label{alg:tree}
\begin{algorithmic}[1]
    \Function{Reconstruction}{}
    \If { is }
        \State 
        \State \Return true
    \EndIf
    \State 
    \For {each  in }
            \If {\Call{Reconstruction}{}}
                \State 
                \State \Return true
            \EndIf
    \EndFor
    \State \Return false
    \EndFunction
\end{algorithmic}
\end{algorithm}
\end{comment}


\section{Iterative Data Refinement}
\label{sec:data_refinement}

In this section, we propose to iteratively refine data over \refqa{} based on the QA model. 
As shown in Figure~\ref{fig:data_refinement}, we use the QA model to filter \refqa{} data, find appropriate and diverse answer candidates, and use these answers to refine and augment \refqa{} examples.
Filtering data can get rid of some noisy examples in \refqa{}, and pretrained linguistic knowledge in the BERT-based QA model finds more appropriate and diverse answers. We produce questions for the refined answers, then continue to train the QA model on the refined and filtered triples.

\subsection{Initial QA Model Training}
The first step of iterative data refinement is to train an initial QA model.
We use the \refqa{} examples  to train a BERT-based QA model  by maximizing:

where the triple consists of context , question , and answer .


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/data_refinement.pdf}
\caption{Overview of our iterative data refinement process. ``QG" is the process of question generation as described in Section~\ref{ssec:qg_section}. We produce new training data and iteratively train the QA model.}
\label{fig:data_refinement}
\end{figure}

\subsection{Refine Question-Answer Pairs}
As shown in Figure~\ref{fig:data_refinement}, the QA model  is used to refine the \refqa{} examples. 
We first conduct inference on the unseen data (denoted as ), and obtain the predicted answers and their probabilities.  
Then we filter the predicted answers with a confidence threshold :

where  represents the predicted answer.

For each predicted answer , if it agrees with the gold answer , we keep the original question.
For the case that , we treat  as our new answer candidate. Besides, we use the question generator (Section~\ref{ssec:qg_section}) to refine the original question  to .


In this step, using the QA model for filtering helps us get rid of some noisy examples. The refined question-answer pairs  can also augment the \refqa{} examples.
The pretrained linguistic knowledge in the BERT-based QA model is supposed to find more novel answers, i.e., some candidate answers are not extracted by the NER toolkit.
With the refined answer spans, we then use the question generator to produce their corresponding questions.


\subsection{Iterative QA Model Training}

After refining the dataset, we concatenate them with the filtered examples whose candidate answers agree with the predictions.
The new training set is then used to continue to train the QA model.
The training objective is defined as:

where  is an indicator function (i.e.,  if the condition is true).


Using the resulting QA model, we further refine question-answer pairs and repeat the training procedure.
The process is repeated until the performance plateaus, or no new data available.
Besides, in order to obtain more diverse answers during iterative training, we apply a decay factor  for the threshold .
The pseudo code of iterative data refinement is presented in Algorithm~\ref{alg:refine}.







\begin{algorithm}[t]
\SetAlgoNoLine
\DontPrintSemicolon
\KwIn{synthetic context-question-answer triples , a threshold  and a decay factor . }
Sample a part of triples  from \;
Update the model parameters by \\ ~~~~maximizing \;
Split unseen triples into \;
\For{ \textbf{to} } {
    \;
    \For { \textbf{in} }{
        \;
        \For { \textbf{in} }{
            \uIf{}{
                \;
            }
            \Else{
                Refine question  to \;
\;
            }
        }
    }
    \;
    Update the model parameters by \\ ~~~~maximizing \;
}
\KwOut{the updated QA model }
\caption{Iterative Data Refinement}
\label{alg:refine}
\end{algorithm}



\section{Experiments}

We evaluate our proposed method on two widely used extractive QA datasets~\citep{squad1,newsqa2017}.
We also demonstrate the effectiveness of our approach in the few-shot learning setting.


\begin{table*}[t]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        & \multicolumn{2}{c}{SQuAD 1.1} & \multicolumn{2}{c}{NewsQA} \\
        Models & Dev Set & Test Set & Dev Set & Test Set \\ \midrule
        \multicolumn{5}{l}{\emph{Supervised Methods}} \\
\quad DCR~\citep{yu2016dcr} & 62.5 / 71.2 & 62.5 / 71.0  & - / - & - / - \\
        \quad mLSTM~\citep{wang2016matchlstm} & 64.1 / 73.9 & 64.7 / 73.7 & ~~34.4 / 49.6 & ~~34.9 / 50.0 \\
\quad FastQAExt~\citep{fastqa2017} & 70.3 / 78.5 & 70.8 / 78.9 & 43.7 / 56.1 & 42.8 / 56.1 \\
        \quad R-NET~\citep{wang-rnet} & 71.1 / 79.5 &  71.3 / 79.7  & - / - & - / - \\
        \quad BERT-Large~\citep{BERT} & 84.2 / 91.1 &  85.1 / 91.8  & - / - & - / - \\ 
        \quad SpanBERT~\citep{spanbert} & - / - & 88.8 / 94.6 & - / - & ~~~~~~- / 73.6 \\
        \midrule
        \multicolumn{5}{l}{\emph{Unsupervised Methods}} \\
        \quad~\citet{dhingra2019semi} & 28.4 / 35.8 & - / - & - / - & - / -\\
        \quad~\citet{lewis2019unsupervisedqa} & - / - & 44.2 / 54.7 & - / - & - / - \\
        \quad~\citet{lewis2019unsupervisedqa} & 45.4 / 55.6 & - / - & 19.6 / 28.5 & 17.9 / 27.0 \\
        \quad Our \refqa{}  & 57.1 / 66.8 & 55.8 / 65.5 & 29.0 / 42.2 & 27.6 / 41.0 \\
        \quad ~~~~+ Iterative Data Refinement & \textbf{62.5} / \textbf{72.6} & \textbf{61.1} / \textbf{71.4} & \textbf{33.6} / \textbf{46.3} & \textbf{32.1} / \textbf{45.1}\\
        \bottomrule
    \end{tabular}
    \caption{Results (EM / F1) of our method, various baselines and supervised models on SQuAD 1.1, and NewsQA. ``" means results taken from~\citet{newsqa2017}, ``" means results taken from~\citet{lewis2019unsupervisedqa}, and ``" means our reimplementation on BERT-Large (Whole Word Masking).}
    \label{tab:mainresult}
\end{table*}

\subsection{Configuration}

\paragraph{\refqa{} Construction} 
We collect the statements with references from English Wikipedia following the procedure in~\cite{wikiref}.
We only consider the references that are HTML pages, which results in 1.4M statement-document pairs.


In order to make sure the statement is relevant to the cited document, we tokenize the text, remove stop words and discard the examples if more than half of the statement tokens are not in the cited document. 
The article length is limited to 1,000 words for cited documents.
Besides, we compute ROUGE-2~\citep{lin-2004-rouge} as correlation scores between statements and context.
We use the score's median () as a threshold, i.e., half of the data with lower scores are discarded.
We obtain 303K remaining data to construct our \refqa{}.

We extract named entities as our answer candidates, using the NER toolkit of Spacy. We split the statements into sub-clauses with Berkeley Neural Parser~\citep{Kitaev-2018-SelfAttentive}.
The questions are generated as in Section~\ref{ssec:qg_section}.
We also discard sub-clauses that are less than  tokens, to prevent losing too much information of original sentences.
Finally, we obtain 0.9M \refqa{} examples.



\paragraph{Question Answering Model}
We adopt BERT as the backbone of our QA model.
Following~\cite{BERT}, we represent the question and passage as a single packed sequence.
We apply a linear layer to compute the probability of each token being the start or end of an answer span.
We use Adam~\citep{adam} as our optimizer with a learning rate of 3e-5 and a batch size of 24.
The max sequence length is set to 384.
We split the long document into multiple windows with a stride of 128. 
We use the uncased version of BERT-Large (Whole Word Masking). 
We evaluate on the dev set every 1000 training steps, and conduct early stopping when the performance plateaus. 


\paragraph{Iterative Data Refinement}
We uniformly sample 300k data from \refqa{} to train the initial QA model.
We split the remaining 600k data into 6 parts for iterative data refinement.
For each part, we use the current QA model to refine question-answer pairs. We combine the refined data with filtered data in a 1:1 ratio to continue training the QA model.
Specially, we keep the original answer if its prediction is a part of the original answer during inference.
The threshold  is set to 0.15 for filtering the model predictions. The decay factor  is set to 0.9. 



\subsection{Results}
We conduct evaluation on the SQuAD 1.1~\citep{squad1}, and the NewsQA~\citep{newsqa2017} datasets.
We compare our proposed approach with previous unsupervised approaches and several supervised models. 
Performance is measured via the standard Exact Match (EM) and F1 metrics.






\citet{dhingra2019semi} propose to train the QA model on the cloze-style questions. Here we take the unsupervised results that re-implemented by~\citet{lewis2019unsupervisedqa} with BERT-Large.
The other unsupervised QA system~\cite{lewis2019unsupervisedqa} borrows the idea of unsupervised machine translation~\citep{lample2017unsupervised} to convert cloze questions into natural questions.
For a fair comparison, we use their published data\footnote{\url{https://github.com/facebookresearch/UnsupervisedQA}} to re-implement their approach based on BERT-Large (Whole Word Masking) model.

Table \ref{tab:mainresult} shows the main results on SQuAD 1.1 and NewsQA. Training QA model on our~\refqa{} outperforms the previous methods by a large margin.
Combining with iterative data refinement, our approach achieves new state-of-the-art results in the unsupervised setting.
Our QA model attains 71.4 F1 on the SQuAD 1.1 test set and 45.1 F1 on the NewsQA test set without using their annotated data, outperforming all of the previous unsupervised methods.
In particular, the results are competitive with early supervised models.

\subsection{Analysis}

We conduct ablation studies on the SQuAD 1.1 dev set, in order to better understand the contributions of different components in our method.

\subsubsection{Effects of \refqa{}}

We conduct experiments on \refqa{} and another synthetic dataset (named as \textsc{Wiki}). The \textsc{Wiki} dataset is constructed using the same method as in~\citet{lewis2019unsupervisedqa}, which uses Wikipedia pages as context paragraphs for QA examples.
In addition to the dependency reconstruction method (Section~\ref{sec:dep:rec}), we compare three cloze translation methods proposed in~\citet{lewis2019unsupervisedqa}.

\begin{table}[t]
\small
\centering
\begin{tabular}{p{0.8cm}p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.2cm}<{\centering}}
\toprule
& Identity & Noise & UNMT & DRC \\
\midrule
\textsc{Wiki} & 20.8~/~30.5 & 36.6~/~45.6 & 40.5~/~49.1 & 26.3~/~35.7 \\
\refqa{} & \textbf{42.5}~/~\textbf{51.6} &  \textbf{45.1}~/~\textbf{53.5} 
& \textbf{43.4}~/~\textbf{52.0} & \textbf{49.2}~/~\textbf{58.8} \\
\bottomrule
\end{tabular}
\caption{Results (EM / F1) of \refqa{} and \textsc{Wiki} datasets with different cloze translation methods on the SDuAD 1.1 dev set. ``DRC" is short for dependency reconstruction.}
\label{tab:ref&top}
\end{table}

\begin{table}[t]
\small
\centering
\begin{tabular}{p{3.2cm}|p{3.2cm}}
\toprule
it finished first in the \textcolor{blue}{Arbitron} ratings in April 1990 & he was sold to Colin Murphy's Lincoln City for a fee of \textcolor{blue}{15,000} \\ 
\midrule
\textbf{UNMT:} Who finished it first in the ratings in April 1990 ? & \textbf{UNMT:} How much do we need Colin Murphy 's Lincoln City for a fee ? \\ 
\textbf{DRC:} Who ratings in it finished first in April 1990 & \textbf{DRC:} How much of a fee for he was sold to Colin Murphy 's Lincoln City \\
\bottomrule
\end{tabular}
\caption{Examples of generated questions using UNMT and our method. ``DRC" is short for our dependency reconstruction. The blue words indicate extracted answers.}
    \label{tab:example_q}
\end{table}

\begin{table}[t]
\small
\centering
\begin{tabular}{p{3.3cm}p{0.4cm}<{\centering}p{0.9cm}<{\centering}p{1.3cm}<{\centering}}
\toprule
 & Iter.  &  Size  & EM / F1   \\ \midrule
Initial QA Model  & &  300k       & 57.1~/~66.8 \\ \midrule
Training on \\
\quad Filtered Data & \xmark & 464k & 57.4~/~67.1 \\
\quad Refined Data & \xmark & 100k  & 61.0~/~70.7 \\ 
\quad Refined + Filtered Data & \xmark & 200k & 61.8~/~71.0 \\
\quad Refined Data & \cmark &  & 60.1~/~70.0 \\
\quad  Refined + Filtered Data & \cmark  &  & \textbf{62.5}~/~\textbf{72.6} \\ 
\bottomrule
\end{tabular}
\caption{Results of using filtered data, refined data, and the combination for data refinement on the SDuAD 1.1 dev set. ``Iter." is short for iterative training.}
\label{tab:merge}
\end{table}

\paragraph{Identity Mapping} generates questions by replacing the mask token in cloze questions with a relevant wh* question word.


\paragraph{Noise Cloze} first applies a noise model, such as permutation, and word drop, as in~\citet{lample2017unsupervised}, and then applies the ``Identity Mapping" translation.

\paragraph{UNMT} converts cloze questions into natural questions following unsupervised neural machine translation.
Here we directly use the published model of~\citet{lewis2019unsupervisedqa} for evaluation.



For a fair comparison, we sample 300k training data for each dataset, and fine-tune BERT-Base for 2 epochs.
As shown in Table~\ref{tab:ref&top}, training on our~\refqa{} achieves a consistent gain over all cloze translation methods.
Moreover, our dependency reconstruction method is also favorable compared with the ``Identity Mapping" method.
The improvement of DRC on \textsc{Wiki} is smaller than on \refqa{}. We argue that it is because \textsc{Wiki} contains too many lexical overlaps, while DRC mainly focuses on providing structural diversity.

We present the generated questions of our method (DRC) and UNMT in Table~\ref{tab:example_q}. Most natural questions follow a similar structure: question word (what/who/how), question focus (name/money/time), question verb (is/play/take) and topic~\citep{qa:as:ie}. Compared with UNMT, our method adjusts answer-related words in the dependency tree according to the linguistic characteristics of natural questions.





\subsubsection{Effects of Data Combination}


We validate the effectiveness of combining refined and filtered data for our data refinement.
We use only refined or filtered data to train our QA model, comparing with the combining approach.



The results are shown in Table~\ref{tab:merge}. We observe that both data can help the QA model to achieve better performance. Moreover, the combination of refined and filtered data is more useful than only using one of them.
Using iterative training, our combination approach further improves the model performance to 72.6 F1 (1.6 absolute improvement).
Besides, using our refined data contributes further improvement compared with filtered data.












\begin{table}[t]
    \small
    \centering
    \begin{tabular}{cccccccc}
        \toprule
         & 0.0 & 0.1 & 0.15 & 0.2 & 0.3 & 0.5 & 0.7  \\ \midrule
        EM & 54.3 & 61.2 & 61.8 & 61.1 & 59.7 & 59.2 & 58.5 \\
        F1 & 69.6 & 70.4 & 71.0 & 70.9 & 69.4 & 68.7 & 67.7 \\
        \bottomrule
    \end{tabular}
    \caption{Results of using different confidence thresholds during the construction of the refined data and filtered data.}
    \label{tab:threshold}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/threshold.pdf}
\caption{Comparison on filtered data and refined data with different confidence thresholds. ``F" is short for using filtered data, ``R" is short for using refined data. ``R+F" is short for the combination of refined and filtered data.}
\label{fig:threshold}
\end{figure}

\subsubsection{Effects of Confidence Threshold}
We experiment with several thresholds (0.0, 0.1, 0.15, 0.2, 0.3, 0.5 and 0.7) to filter the predicted answers. Their QA results on SQuAD 1.1 dev set are presented in Table~\ref{tab:threshold}. Using threshold of 0.15 achieves better performance. 



We also analyze the effects of threshold on refined data and filtered data.
As shown in Figure~\ref{fig:threshold}, for the filtered data, using a higher confidence threshold achieves better performance, suggesting that using the QA model for filtering makes our examples more credible.
For the refined data and the combination, we observe that the threshold 0.15 achieves a better performance than the threshold 0.3, but the EM is greatly reduced when the threshold is set to 0.0. Besides, there are 26,257 answers that do not appear in named entities using the threshold 0.15, compared to 15,004 for the threshold 0.3. Thus, an appropriate threshold can help us improve the answer diversity and get rid of some noisy examples.



\begin{comment}
\subsubsection{Effects of Iterative Training}
Table~\ref{tab:merge} shows that combining the refined data with filtered data, iterative training achieves the best performance with 72.6 F1 on the SQuAD 1.1 dev set. As shown in Figure~\ref{fig:multiround}, we also investigate the performance of iterative training with respect to the number of iterations.



\begin{figure}[t]
    \centering
    \includegraphics[width=7.5cm]{images/multiround.pdf}
    \caption{Performance on each iteration for the settings of using only refined data and combining with filtered data on the SQuAD 1.1 dev set.}
    \label{fig:multiround}
\end{figure}
\end{comment}

\subsubsection{Effects of Refinement Types}
\label{sec:eda}

For brevity, we denote the original answer and predicted answer by ``OA" and ``PA", respectively.
In order to analyze the contribution of our refined data, we categorize the data refinements into the following three types:


\noindent
\textbf{OAPA} The original answer contains the predicted answer.

\noindent
\textbf{OAPA} The predicted answer contains the original answer.

\noindent
\textbf{Others} The remaining data except for the above two types of refinement.


\begin{table}[t]
\centering
\begin{tabular}{lcccc}
\toprule
  & Refined & Size  & EM / F1   \\ \midrule
\refqa{} & - & 300k & 57.1 / 66.8 \\ \midrule
OAPA & \xmark & 90k & 59.4 / 69.0 \\
OAPA & \cmark & 90k & 50.9 / 64.6 \\
OAPA & \xmark & 35k & 47.5 / 61.2 \\
OAPA & \cmark & 35k & 60.3 / 69.9 \\
Others & \xmark& 75k & 52.2 / 62.3 \\
Others & \cmark& 75k & 58.8 / 69.7 \\
\bottomrule
\end{tabular}
\caption{Comparison between different types of data refinement on the SQuAD 1.1 dev set.}
\label{tab:case}
\end{table}

\begin{table*}[t!]
\centering
\begin{tabular}{lp{13cm}}
\toprule
\textbf{OAPA} 
& \textbf{S:} In 1938, E. Allen Petersen escaped the advancing Japanese armies by sailing a junk, ``Hummel Hummel", from Shanghai to California with his wife Tani and two White Russians (Tsar loyalists).  \\
& \textbf{Q:} Who escaped the advancing Japanese armies by sailing a junk   \\
& \textbf{OA:} E. Allen Petersen   \\
& \textbf{PA:} Petersen \\
& \textbf{RQ:} Who escaped the advancing Japanese armies by sailing a junk  \\ \midrule
\textbf{OAPA}
& \textbf{S:} Hyundai announced they would be revealing their future rally plans at the 2011 Chicago Auto Show on February 9 .  \\
& \textbf{Q:} What at they would be revealing their future rally plans on February 9  \\
& \textbf{OA:} Chicago Auto Show  \\
& \textbf{PA:} the 2011 Chicago Auto Show \\
& \textbf{RQ:} What at their future rally plans they would be revealing on February 9 \\ \midrule
\textbf{Others}
& \textbf{S:} In January 2017, she released the track ``That's What's Up" that re-imagines the spoken word segment on the Kanye West song ``Low Lights". \\
& \textbf{Q:} What the Kanye West song on the spoken word segment re-imagines  \\
& \textbf{OA:} Low Lights  \\
& \textbf{PA:} That's What's Up  \\
& \textbf{RQ:} What the track she released that re-imagines the spoken word segment on the Kanye West song ``Low Lights" . \\
\bottomrule
\end{tabular}
\caption{The generated and refined question-answer pairs. ``S" and ``Q" are short for statement and question. ``OA", ``PA" and ``RQ" are short for the original answer, predicted answer and the refined question.}
\label{tab:examples}
\end{table*}


For each type, we keep the original data or use refined data to train our QA model.
We conduct experiments on the non-iterative setting with the data combination.

As shown in Table~\ref{tab:case}, our refined data improves the QA model in most types of refinement except ``OAPA".
The results indicate that the QA model favors longer phrases as answer spans.
Moreover, for the ``OAPA" and ``Others" types, there are 47.8\% answers that are not extracted by the NER toolkit.
The iterative refinement extends the category of answer candidates, which in turn produces novel question-answer pairs.

We show a few examples of our generated data in Table~\ref{tab:examples}.
We list one example for each type.
For the ``OAPA" refinement, the predicted answer is a sub-span of the extracted named entity, but the complete named entity is more appropriate as an answer.
For the ``OAPA" refinement, the QA model can help us extend the original answer to be a longer span, which is more complete and appropriate.
Besides, for the ``Others" refinement, its prediction can be a new answer, and not appear in named entities extracted by the NER toolkit. 










\subsection{Few-Shot Learning}

Following the evaluation of~\cite{yang-semiqa,dhingra2019semi}, we conduct experiments in a few-shot learning setting. 
We use the best configuration of our approach to train the unsupervised QA model based on BERT-Large (Whole Word Masking).
Then we fine-tune the model with limited SQuAD training examples.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/few-shot.pdf}
\caption{F1 score on the SQuAD 1.1 dev set with various training dataset sizes.}
\label{fig:fewshot}
\end{figure}

As shown in Figure~\ref{fig:fewshot}, our method obtains the best performance in the restricted setting, compared with the previous state of the art~\cite{lewis2019unsupervisedqa} and directly fine-tuning BERT.
Moreover, our approach achieves  F1 (16.4 absolute gains than other models) with only  labeled examples.
The results illustrate that our method can greatly reduce the demand of in-domain annotated data.
In addition, we observe that the results of different methods become comparable when the labeled data size is greater than 10,000.

\section{Conclusion}

In this paper, we present two approaches to improve the quality of synthetic QA data for unsupervised question answering. We first use the Wikipedia paragraphs and its references to construct a synthetic QA data \refqa{} and then use the QA model to iteratively refine data over \refqa{}. Our method outperforms the previous unsupervised state-of-the-art models on SQuAD 1.1, and NewsQA, and achieves the best performance in the few-shot learning setting. 



\section*{Acknowledgements}
The work was partially supported by National Natural Science Foundation of China (NSFC) [Grant No. 61421003].


\bibliography{uqa}
\bibliographystyle{acl_natbib}

\end{document}
