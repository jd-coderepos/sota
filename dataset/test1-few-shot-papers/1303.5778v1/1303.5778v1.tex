\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{subfigure} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{setspace}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{multirow}
\usepackage{pifont}
\usepackage[capitalise]{cleveref}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}
\newcommand{\maybe}[1]{\textcolor{grey}{\textbf{MAYBE:}{#1}}}
\newcommand{\more}{\textcolor{red}{\textbf{MORE HERE!}}}

\newcounter{mnote}
\newcommand{\marginote}[1]{\addtocounter{mnote}{1}\marginpar{\themnote. \scriptsize #1}}
\setcounter{mnote}{0}
\newcommand{\ie}{i.e.\ }
\newcommand{\eg}{e.g.\ }
\newcommand{\cf}{c.f.\ }
\newcommand{\yes}{\checkmark}
\newcommand{\no}{\ding{55}}

\newcommand{\flabel}[1]{\label{fig:#1}}
\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\tlabel}[1]{\label{tab:#1}}
\newcommand{\elabel}[1]{\label{eq:#1}}
\newcommand{\alabel}[1]{\label{alg:#1}}
\newcommand{\fref}[1]{\cref{fig:#1}}
\newcommand{\sref}[1]{\cref{sec:#1}}
\newcommand{\tref}[1]{\cref{tab:#1}}
\newcommand{\eref}[1]{\cref{eq:#1}}
\newcommand{\aref}[1]{\cref{alg:#1}}

\newcommand*\idx[2][]
{ 
\def\next{#1}\ifx\empty\next
  (#2)
\else
  (#1, #2)
\fi
}
\newcommand*\elt[3][]
{ 
\def\next{#1}\ifx\empty\next
  #2\idx{#3}
\else
  #1\idx{#2,#3}
\fi
}
\newcommand*\pd[3][]
{ 
\def\next{#1}\ifx\empty\next
  \frac{\partial#2}{\partial #3}
\else
  \frac{{\partial^{#1} #2}}{\partial#3^{#1}}
\fi
}
\newcommand*\pdn[3]
{ 
\frac{{\partial#1}^{#3}}{\partial^{#3} #2}
}
\newcommand{\hfor}{\overrightarrow{h}}
\newcommand{\hback}{\overleftarrow{h}}
\newcommand{\igate}{i}
\newcommand{\fgate}{f}
\newcommand{\ogate}{o}
\newcommand{\state}{c}
\newcommand{\hiddenfn}{\mathcal{H}}
\newcommand{\outputfn}{\mathcal{O}}
\newcommand{\kronecker}[2]{\delta_{#1 #2}}
\newcommand{\len}[1]{|#1|}
\newcommand{\outelt}{k}
\newcommand{\wtmat}[2]{W_{#1 #2}}
\newcommand{\ihwts}{\wtmat{x}{h}}
\newcommand{\hhwts}{\wtmat{h}{h}}
\newcommand{\howts}{\wtmat{h}{y}}
\newcommand{\bias}[1]{b_{#1}}
\newcommand{\hbias}{\bias{h}}
\newcommand{\obias}{\bias{y}}
\newcommand{\seq}[1]{\boldsymbol #1}
\newcommand{\alignment}{\seq{a}}
\newcommand{\bestout}{\best{\outseq}}
\newcommand{\blank}{\varnothing}
\newcommand{\blankseq}{\seq{\blank}}
\newcommand{\slice}[3]{#1_{[#2:#3]}}
\newcommand{\exts}[1]{ext(#1)}
\newcommand{\invble}{x}
\newcommand{\outvble}{y}
\newcommand{\targvble}{z}
\newcommand{\inseq}{\seq{\invble}}
\newcommand{\outseq}{\seq{\outvble}}
\newcommand{\predinseq}{\hat{\outseq}}
\newcommand{\hidseq}{\seq{h}}
\newcommand{\targseq}{\seq{\targvble}}
\newcommand{\loss}{\mathcal{L}(\inseq,\targseq)}
\newcommand{\varspace}[1]{\mathcal{\MakeUppercase{#1}}}
\newcommand{\inspace}{\varspace{\invble}}
\newcommand{\outspace}{\varspace{\outvble}}
\newcommand{\blankoutspace}{\bar{\outspace}}
\newcommand{\seqsover}[1]{#1^*}
\newcommand{\outseqspace}{\seqsover{\outspace}}
\newcommand{\blankoutseqspace}{\seqsover{\blankoutspace}}
\newcommand{\range}[3]{#1 \leq #2 \leq #3}
\newcommand{\trange}{\range{1}{t}{T}}
\newcommand{\urange}{\range{0}{u}{U}}
\newcommand{\insum}{\sum_{t=1}^T}
\newcommand{\outsum}{\sum_{u=0}^U}
\newcommand{\infn}{f}
\newcommand{\outfn}{g}
\newcommand{\densityfn}{h}
\newcommand{\partition}{\mathcal{Z}}
\newcommand{\outprobvar}[3]{\Pr(#1| #2, #3)}
\newcommand{\outprobv}[1]{\outprobvar{#1}{t}{u}}
\newcommand{\outprob}{\outprobv{\outelt}}
\newcommand{\outseqprob}{\Pr(\outseq | \inseq)}
\newcommand{\targseqprob}{\Pr(\targseq | \inseq)}
\newcommand{\outslice}[2]{\slice{\outseq}{#1}{#2}}
\newcommand{\inslice}[2]{\slice{\inseq}{#1}{#2}}
\newcommand{\pseq}{\seq{b}}
\newcommand{\tseq}{\seq{f}}
\newcommand{\tslice}[2]{\slice{\tseq}{#1}{#2}}
\newcommand{\outpref}{\hat{\outseq}}
\newcommand{\outprefs}{pref(\outseq)}
\newcommand{\outlen}{\len{\outseq}}
\newcommand{\outpreflen}{\len{\outpref}}
\newcommand{\outfunc}[2]{\outfn(\outslice{#1}{#2})}
\newcommand{\infunc}[1]{\infn(#1, \inseq)}
\newcommand{\outfuncu}{\outfunc{1}{u}}
\newcommand{\infunct}{\infunc{t}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{\MakeUpperCase{#1}}}
\newcommand{\matelt}[3]{#1_{#2 #3}}
\newcommand{\vectseqelt}[2]{\vect{#1}(#2)}
\newcommand{\wt}{w}
\newcommand{\wts}{\wt}
\newcommand{\wtsa}{\theta}
\newcommand{\wtsb}{\psi}
\newcommand{\densityfunc}[1]{\densityfn(#1, t, u)}
\newcommand{\density}{\densityfunc{\outelt}}
\newcommand{\wtmatrix}{\mat{W}}
\newcommand{\topology}{\mathcal{N}}
\newcommand{\net}{\topology_{\wts}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\realn}[1]{\reals^{#1}}\newcommand{\fnvals}[2]{#1 \mapsto #2}
\newcommand{\fn}[3]{#1 : \fnvals{#2}{#3}}
\newcommand{\realfnvals}[2]{\realn{#1} \mapsto \realn{#2}}
\newcommand{\realfn}[3]{\fn{#1}{\realn{#2}}{\realn{#3}}}
\newcommand{\realseq}[1]{(\realn{#1})^{*}}
\newcommand{\realseqfn}[3]{\fn{#1}{\realseq{#2}}{\realseq{#3}}}
\newcommand{\compose}[2]{#1\circ #2}
\newcommand{\subelt}[2]{#1_{#2}}
\newcommand{\expo}[1]{\exp\left(#1\right)}
\newcommand{\kron}[2]{\delta_{#1,#2}}
\newcommand{\gauss}[2]{\mathcal{N}(#1,#2)}
\newcommand{\condgauss}[3]{\mathcal{N}(#1|#2,#3)}
\newcommand{\prodrange}[3]{\prod_{#1=#2}^{#3}}
\newcommand{\prodto}[2]{\prodrange{#1}{1}{#2}}
\newcommand{\sumrange}[3]{\sum_{#1=#2}^{#3}}
\newcommand{\sumto}[2]{\sumrange{#1}{1}{#2}}
\newcommand{\segs}{s}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\pair}[2]{(#1,#2)}
\newcommand{\deq}{\mathrel{\stackrel{\text{\tiny{def}}}{=}}} 
\newcommand{\opequals}[1]{\ #1\hspace{-3pt}=\hspace{1pt}}
\newcommand{\plusequals}{\opequals{+}} 
\newcommand{\minusequals}{\opequals{-}} 
\newcommand{\timesequals}{\opequals{*}} 
\newcommand{\front}[1]{beg(#1)}
\newcommand{\back}[1]{end(#1)}
\newcommand{\stddev}{\sigma}
\newcommand{\variance}{\stddev^2}
\newcommand{\ifelse}[3]{\begin{cases}#1 \text{ if } #2\\#3 \text{ otherwise}\end{cases}}

\newcommand{\jacobian}{\mat{J}}
\newcommand{\wtderivs}{\mat{D}}
\newcommand{\hessian}{\mat{H}}

\newcommand{\instep}[1]{\elt{\vect{\inputvble}}{#1}}
\newcommand{\outputvble}{y}
\newcommand{\out}{\seq{\outputvble}}
\newcommand{\outstep}[1]{\elt{\vect{\outputvble}}{#1}}

\newcommand{\inplen}{T}
\newcommand{\inpprod}{\prodto{t}{\inplen}}
\newcommand{\targpt}{\outelt}
\newcommand{\targ}{\seq{\targpt}}
\newcommand{\netin}{\seq{a}}
\newcommand{\supex}{\pair{\inp}{\targ}}
\newcommand{\obj}{\mathcal{O}}
\newcommand{\objex}{\obj\supex}
\newcommand{\wtjk}{\elt{\vect{w}}{j,k}}
\newcommand{\opt}[1]{#1^*}
\newcommand{\optwts}{\opt{\vect{w}}}
\newcommand{\suptargpw}{p(\targ|\inp,\wts)}
\newcommand{\suptargp}{p(\targ|\inp)}
\newcommand{\targi}{\elt{\targ}{t}}
\newcommand{\targs}{\elt{\targ}{s}}
\newcommand{\targk}{\elt{\targ}{k}}
\newcommand{\targik}{\elt{\targ}{t,k}}
\newcommand{\ppath}{\pi}
\newcommand{\pathi}{\elt{\ppath}{t}}
\newcommand{\targipathi}{\elt{\targ}{t,\pathi}}
\newcommand{\outipathi}{\elt{\out}{t,\pathi}}
\newcommand{\suppathp}{p(\ppath|\inp)}
\newcommand{\targsk}{\elt{\targ}{\segs,k}}
\newcommand{\suptargip}{p(\targi|\inp)}
\newcommand{\suptargikp}{p(\targik|\inp)}
\newcommand{\suptargsegp}{p(\elt{\targ}{s}|\inp)}
\newcommand{\condclassprobik}{\condclassprob{t}{k})}
\newcommand{\condclassprob}[2]{p(C(#1,#2)|\inp)}
\newcommand{\softnorm}[3]{\sum_{#2'=1}^#3{\expo{\elt{\netin}{#1,#2'}}}}
\newcommand{\softmax}[4]{\frac{\expo{\elt{\netin}{#1,#2}}}{\softnorm{#1}{#3}{#4}}}
\newcommand{\softmaxik}{\softmax{t}{k}{k}{K}}
\newcommand{\sigmoid}[1]{\frac{1}{1+\expo{-#1}}}
\newcommand{\bestk}{k^*}
\newcommand{\outi}{\elt{\out}{t}}
\newcommand{\optouti}{\elt{\opt{\out}}{t}}
\newcommand{\optoutik}{\elt{\opt{\out}}{t,k}}
\newcommand{\outik}{\elt{\out}{t,k}}
\newcommand{\outikdash}{\elt{\out}{t,k'}}
\newcommand{\outsk}{\elt{\out}{\segs,k}}
\newcommand{\netini}{\elt{\netin}{t}}
\newcommand{\netinik}{\elt{\netin}{t,k}}
\newcommand{\netinikdash}{\elt{\netin}{t,k'}}
\newcommand{\objpd}{\pd{\objex}{\netinik}}
\newcommand{\mapw}{\vect{w}_{MAP}}
\newcommand{\mlw}{\vect{w}_{ML}}
\newcommand{\subseq}[3]{\seq{#1}[#2:#3]}
\newcommand{\netins}{\elt{\netin}{\segs}}
\newcommand{\netinsk}{\elt{\netin}{\segs,k}}
\newcommand{\pdobjout}{\pd{\objex}{\outik}}
\newcommand{\pdobjnetin}{\pd{\objex}{\netinik}}
\newcommand{\pdoutnetin}{\pd{\outik}{\netinik}}
\newcommand{\pdoutdashnetin}{\pd{\outikdash}{\netinik}}
\newcommand{\outitargi}{\elt{\out}{t,\targi}}
\newcommand{\outnitargi}{\elt{\out}{n,t,\targi}}
\newcommand{\outprod}{\prod_{k=1}^K}
\newcommand{\outsumdash}{\sumto{k'}{K}}
\newcommand{\inpsum}{\sumto{t}{\inplen}}
\newcommand{\segsum}{\sumto{s}{S}}
\newcommand{\segprod}{\prodto{s}{S}}
\newcommand{\targni}{\elt{\targ_n}{t}}
\newcommand{\outnik}{\elt{\out_n}{t,k}}
\newcommand{\netinnik}{\elt{\netin_n}{t,k}}
\newcommand{\netinnikdash}{\elt{\netin_n}{t,k'}}
\newcommand{\outnitargni}{\elt{\out_n}{t,\targni}}
\newcommand{\trset}{S}
\newcommand{\dist}{\mc{D}}
\newcommand{\ispace}{\mc{X}}
\newcommand{\tspace}{\mc{Z}}
\newcommand{\supexnew}{\pair{\inpnew}{\targnew}}
\newcommand{\supspace}{\ispace\times\tspace}
\newcommand{\mixcoeff}{p(m|\inp, t)}
\newcommand{\mixmean}{\mu_m(\inp, t)}
\newcommand{\mixvar}{\variance_m(\inp, t)}
\newcommand{\mixstddev}{\stddev_m(\inp, t)}
\newcommand{\mixmeank}{\mu_{mk}(\inp, t)}
\newcommand{\mixvark}{\variance_{mk}(\inp, t)}
\newcommand{\mixstddevk}{\stddev_{mk}(\inp, t)}
\newcommand{\mixoutcoeff}{\elt{\out}{t,\pi_{m}}}
\newcommand{\mixincoeff}{\elt{\netin}{t,\pi_{m}}}
\newcommand{\mixoutvar}{\elt{\out^2}{t,\stddev_{mk}}}
\newcommand{\mixoutstddev}{\elt{\out}{t,\stddev_{mk}}}
\newcommand{\mixinstddev}{\elt{\netin}{t,\stddev_{mk}}}
\newcommand{\mixoutmean}{\elt{\out}{t,\mu_{mk}}}
\newcommand{\mixinmean}{\elt{\netin}{t,\mu_{mk}}}
\newcommand{\mixoutcoeffdash}{\elt{\out}{t,\pi_{{mk}'}}}
\newcommand{\mixoutvardash}{\elt{\out}{t,\stddev_{{mk}'}}}
\newcommand{\mixoutmeandash}{\elt{\out}{t,\mu_{{mk}'}}}
\newcommand{\mixcondgauss}{\condgauss{\targi}{\mixoutmean}{\mixoutvar}}
\newcommand{\mixsum}{\sum_{m=1}^M}
\newcommand{\mixrespo}{p(m|\inp,t,\targi)}
\newcommand{\sed}[2]{||#1-#2||^2}
\newcommand{\alphabet}{A}
\newcommand{\blankalph}{A'}
\newcommand{\alphstrings}[2]{#1^{#2}}
\newcommand{\paths}{\alphstrings{\blankalph}{\inplen}}
\newcommand{\labellings}{\alphstrings{\alphabet}{\leq\inplen}}
\newcommand{\labelling}{\mathbf{g}}
\newcommand{\blanklab}{\labelling'}
\newcommand{\suplabp}{p(\labelling|\inp)}
\newcommand{\blanklabs}{\alphstrings{\blankalph}{\leq\inplen}}
\newcommand{\aligncollapse}{\mathcal{B}}
\newcommand{\forward}[2]{\elt{\alpha}{#1,#2}}
\newcommand{\backward}[2]{\elt{\beta}{#1,#2}}
\newcommand{\prob}[2]{\elt{\gamma}{#1,#2}}
\newcommand{\forwardts}{\forward{t}{s}}
\newcommand{\backwardts}{\backward{t}{s}}
\newcommand{\numclasses}{K}
\newcommand{\blanklablen}{S'}
\newcommand{\lablen}{S}
\newcommand{\labelt}[1]{\subelt{\labelling}{#1}}
\newcommand{\blanklabelt}[1]{\subelt{\blanklab}{#1}}
\newcommand{\best}[1]{#1^*}
\newcommand{\bestlabelling}{\best{\labelling}}
\newcommand{\bestpath}{\best{\ppath}}
\newcommand{\targn}{\subelt{\targ}{n}}
\newcommand{\outn}{\subelt{\out}{n}}
\newcommand{\suptargn}{p(\subelt{\targ}{n}|\inp)}
\newcommand{\netinn}{\subelt{\netin}{n}}
\newcommand{\objexn}{\subelt{\obj}{n}\supex}
\newcommand{\inv}{\vect{\inputvble}}
\newcommand{\outv}{\vect{\outputvble}}
\newcommand{\kernelseq}{\seq{v}}
\newcommand{\kernelelt}[1]{\vectseqelt{v}{#1}}
\newcommand{\kerneltransseq}{\seq{\hat{v}}}
\newcommand{\kerneltranselt}[1]{\vectseqelt{\hat{v}}{#1}}
\newcommand{\predseq}{\seq{h^2}}
\newcommand{\transseq}{\seq{h^1}}

\newcommand{\figdir}{figures/}
\newcommand{\capt}[2]{\caption[#1]{#1#2}}
\newcommand{\qcapt}[1]{\capt{#1}{}}
\newcommand{\figheadingnospace}[1]{\center{\textbf{#1}}}
\newcommand{\figheading}[1]{\figheadingnospace{#1}\vspace{3mm}}
\newcommand{\fig}[5]
{
\begin{figure}
\begin{center}
\includegraphics[width=#3\columnwidth]{figures/#1}
\end{center}
\capt{#4}{#5}
\flabel{#2}
\end{figure}
}
\newcommand{\figt}[5]
{
\begin{figure}[t]
\begin{center}
\includegraphics[width=#3\columnwidth]{figures/#1}
\end{center}
\capt{#4}{#5}
\flabel{#2}
\end{figure}
}
\newcommand{\figstar}[5]
{
\begin{figure*}
\begin{center}
\includegraphics[width=#3\textwidth]{figures/#1}
\end{center}
\capt{#4}{#5}
\flabel{#2}
\end{figure*}
}
\newcommand{\qfig}[3]{\fig{#1}{#1}{#2}{#3}{}}

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Speech Recognition with Deep Recurrent Neural Networks}

\name{Alex Graves, Abdel-rahman Mohamed and Geoffrey Hinton}
\address{Department of Computer Science, University of Toronto}

\begin{document}
\maketitle

\begin{abstract}
Recurrent neural networks (RNNs) are a powerful model for sequential data.
End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown.
The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition.
However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks.
This paper investigates \emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. 
When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.
\end{abstract}
\begin{keywords}
recurrent neural networks, deep neural networks, speech recognition
\end{keywords}
\section{Introduction}
\label{sec:intro}
Neural networks have a long history in speech recognition, usually in combination with hidden Markov models~\cite{bourlard94hybrid,Zhu04tandem}.
They have gained attention in recent years with the dramatic improvements in acoustic modelling yielded by deep feedforward networks~\cite{5704567,6296526}.
Given that speech is an inherently dynamic process, it seems natural to consider recurrent neural networks (RNNs) as an alternative model.
HMM-RNN systems~\cite{robinson:1994} have also seen a recent revival~\cite{VinyalsICASSP12,maas2012denoisernn}, but do not currently perform as well as deep networks.

Instead of combining RNNs with HMMs, it is possible to train RNNs `end-to-end' for speech recognition~\cite{graves06icml,graves12supervised,graves12transducer}.
This approach exploits the larger state-space and richer dynamics of RNNs compared to HMMs, and avoids the problem of using potentially incorrect alignments as training targets.
The combination of Long Short-term Memory~\cite{hochreiter97lstm}, an RNN architecture with an improved memory, with end-to-end training has proved especially effective for cursive handwriting recognition~\cite{graves08nips,graves09nips}.
However it has so far made little impact on speech recognition.

RNNs are inherently deep in time, since their hidden state is a function of all previous hidden states.
The question that inspired this paper was whether RNNs could also benefit from depth in space; that is from stacking multiple recurrent hidden layers on top of each other, just as feedforward layers are stacked in conventional deep networks.
To answer this question we introduce \emph{deep Long Short-term Memory} RNNs and assess their potential for speech recognition.
We also present an enhancement to a recently introduced end-to-end learning method that jointly trains two separate RNNs as acoustic and linguistic models~\cite{graves12transducer}.
Sections~\ref{sec:rnn} and \ref{sec:training} describe the network architectures and training methods, \sref{experiments} provides experimental results and concluding remarks are given in~\sref{conclusion}.


\section{Recurrent Neural Networks}
\label{sec:rnn}

Given an input sequence , a standard recurrent neural network (RNN) computes the hidden vector sequence  and output vector sequence  by iterating the following equations from  to :
 
where the  terms denote weight matrices (\eg  is the input-hidden weight matrix), the  terms denote bias vectors (\eg  is hidden bias vector) and  is the hidden layer function.



 is usually an elementwise application of a sigmoid function.
However we have found that the Long Short-Term Memory (LSTM) architecture~\cite{hochreiter97lstm}, which uses purpose-built \emph{memory cells} to store information, is better at finding and exploiting long range context.
\fref{lstm} illustrates a single LSTM memory cell.
For the version of LSTM used in this paper~\cite{gers02peeps}  is implemented by the following composite function:

where  is the logistic sigmoid function, and , ,  and  are respectively the \emph{input gate}, \emph{forget gate}, \emph{output gate} and \emph{cell} activation vectors, all of which are the same size as the hidden vector .
The weight matrices from the cell to gate vectors (\eg ) are diagonal, so element  in each gate vector only receives input from element  of the cell vector.

\figt{lstm_variables}{lstm}{0.85}{Long Short-term Memory Cell}{}

One shortcoming of conventional RNNs is that they are only able to make use of previous context.
In speech recognition, where whole utterances are transcribed at once, there is no reason not to exploit future context as well.
Bidirectional RNNs (BRNNs)~\cite{schuster97bidirectional} do this by processing the data in both directions with two separate hidden layers, which are then fed forwards to the same output layer.
As illustrated in \fref{brnn}, a BRNN computes the \emph{forward} hidden sequence , 
the \emph{backward} hidden sequence  
and the output sequence  by iterating the backward layer from  to , the forward layer from  to  and then updating the output layer:
\figt{brnn_output}{brnn}{0.75}{Bidirectional RNN}{}

Combing BRNNs with LSTM gives bidirectional LSTM~\cite{graves05nn}, which can access long-range context in both input directions.

A crucial element of the recent success of hybrid HMM-neural network systems is the use of \emph{deep} architectures, which are able to build up progressively higher level representations of acoustic data.
\emph{Deep RNNs} can be created by stacking multiple RNN hidden layers on top of each other, with the output sequence of one layer forming the input sequence for the next. Assuming the same hidden layer function is used for all  layers in the stack, the hidden vector sequences  are iteratively computed from  to  and  to :

where we define . The network outputs  are 

Deep bidirectional RNNs can be implemented by replacing each hidden sequence  with the forward and backward sequences  and , and ensuring that every hidden layer receives input from both the forward and backward layers at the level below.
If LSTM is used for the hidden layers we get deep bidirectional LSTM, the main architecture used in this paper.
As far as we are aware this is the first time deep LSTM has been applied to speech recognition, and we find that it yields a dramatic improvement over single-layer LSTM.

\section{Network Training}
\label{sec:training}
We focus on end-to-end training, where RNNs learn to map directly from acoustic to phonetic sequences.
One advantage of this approach is that it removes the need for a predefined (and error-prone) alignment to create the training targets.
The first step is to to use the network outputs to parameterise a differentiable distribution  over all possible phonetic output sequences  given an acoustic input sequence .
The log-probability  of the target output sequence  can then be differentiated with respect to the network weights using backpropagation through time~\cite{rumelhart88backprop}, and the whole system can be optimised with gradient descent.
We now describe two ways to define the output distribution and hence train the network.
We refer throughout to the length of  as , the length of  as , and the number of possible phonemes as .


\subsection{Connectionist Temporal Classification}
The first method, known as Connectionist Temporal Classification (CTC)~\cite{graves06icml,graves12supervised}, uses a softmax layer to define a separate output distribution  at every step  along the input sequence.
This distribution covers the  phonemes plus an extra blank symbol  which represents a non-output (the softmax layer is therefore size ).
Intuitively the network decides whether to emit any label, or no label, at every timestep.
Taken together these decisions define a distribution over alignments between the input and target sequences.
CTC then uses a forward-backward algorithm to sum over all possible alignments and determine the normalised probability  of the target sequence given the input sequence~\cite{graves06icml}.
Similar procedures have been used elsewhere in speech and handwriting recognition to integrate out over possible segmentations~\cite{zweig09scarf,senior95nips}; however CTC differs in that it ignores segmentation altogether and sums over single-timestep label decisions instead.

RNNs trained with CTC are generally bidirectional, to ensure that every  depends on the entire input sequence, and not just the inputs up to .
In this work we focus on deep bidirectional networks, with  defined as follows:

where  is the  element of the length  unnormalised output vector , and  is the number of bidirectional levels.


\subsection{RNN Transducer}
CTC defines a distribution over phoneme sequences that depends only on the acoustic input sequence .
It is therefore an acoustic-only model.
A recent augmentation, known as an \emph{RNN transducer}~\cite{graves12transducer} combines a CTC-like network with a separate RNN that predicts each phoneme given the previous ones, thereby yielding a jointly trained acoustic and language model.
Joint LM-acoustic training has proved beneficial in the past for speech recognition~\cite{Mohamed10investigationof,5495227}.

Whereas CTC determines an output distribution at every input timestep, an RNN transducer determines a separate distribution  for every \emph{combination} of input timestep  and output timestep .
As with CTC, each distribution covers the  phonemes plus .
Intuitively the network `decides' what to output depending both on where it is in the input sequence and the outputs it has already emitted.
For a length  target sequence , the complete set of  decisions jointly determines a distribution over all possible alignments between  and , which can then be integrated out with a forward-backward algorithm to determine ~\cite{graves12transducer}.

In the original formulation  was defined by taking an `acoustic' distribution  from the CTC network, a `linguistic' distribution  from the prediction network, then multiplying the two together and renormalising.
An improvement introduced in this paper is to instead feed the hidden activations of both networks into a separate feedforward \emph{output network}, whose outputs are then normalised with a softmax function to yield .
This allows a richer set of possibilities for combining linguistic and acoustic information, and appears to lead to better generalisation.
In particular we have found that the number of deletion errors encountered during decoding is reduced.

Denote by  and  the uppermost forward and backward hidden sequences of the CTC network, and by  the hidden sequence of the prediction network.
At each  the output network is implemented by feeding  and  to a linear layer to generate the vector , then feeding  and  to a  hidden layer to yield , and finally feeding  to a size  softmax layer to determine :

where  is the  element of the length  unnormalised output vector.
For simplicity we constrained all non-output layers to be the same size (; however they could be varied independently.





RNN transducers can be trained from random initial weights.
However they appear to work better when initialised with the weights of a pretrained CTC network and a pretrained next-step prediction network (so that only the output network starts from random weights).
The output layers (and all associated weights) used by the networks during pretraining are removed during retraining.
In this work we pretrain the prediction network 
on the phonetic transcriptions of the audio training data; however for large-scale applications it would make more sense to pretrain on a separate text corpus.

\subsection{Decoding}
RNN transducers can be decoded with beam search~\cite{graves12transducer} to yield an n-best list of candidate transcriptions.
In the past CTC networks have been decoded using either a form of best-first decoding known as \emph{prefix search}, or by simply taking the most active output at every timestep~\cite{graves06icml}.
In this work however we exploit the same beam search as the transducer, with the modification that the output label probabilities  do not depend on the previous outputs (so ).
We find beam search both faster and more effective than prefix search for CTC.
Note the n-best list from the transducer was originally sorted by the \emph{length normalised} log-probabilty ; in the current work we dispense with the normalisation (which only helps when there are many more deletions than insertions) and sort by .


\subsection{Regularisation}
Regularisation is vital for good performance with RNNs, as their flexibility makes them prone to overfitting.
Two regularisers were used in this paper: early stopping and \emph{weight noise} (the addition of Gaussian noise to the network weights during training~\cite{chuen96noise}).
Weight noise was added once per training sequence, rather than at every timestep. 
Weight noise tends to `simplify' neural networks, in the sense of reducing the amount of information required to transmit the parameters~\cite{hinton93bitsback,graves11nips}, which improves generalisation.

\section{Experiments}
\label{sec:experiments}

Phoneme recognition experiments were performed on the TIMIT corpus~\cite{timit}.
The standard 462 speaker set with all SA records removed was used for training, and a separate development set of 50 speakers was used for early stopping. 
Results are reported for the 24-speaker core test set.
The audio data was encoded using a Fourier-transform-based filter-bank with 40 coefficients (plus energy) distributed on a mel-scale, together with their first and second temporal derivatives.
Each input vector was therefore size 123.
The data were normalised so that every element of the input vectors had zero mean and unit variance over the training set. 
All 61 phoneme labels were used during training and decoding (so ), then mapped to 39 classes for scoring~\cite{lee89timit39}.
Note that all experiments were run only once, so the variance due to random weight initialisation and weight noise is unknown.

As shown in \tref{timit}, nine RNNs were evaluated, varying along three main dimensions: the training method used (CTC, Transducer or pretrained Transducer), the number of hidden levels (1--5), and the number of LSTM cells in each hidden layer. 
Bidirectional LSTM was used for all networks except CTC-3l-500h-tanh, which had  units instead of LSTM cells, and CTC-3l-421h-uni where the LSTM layers were unidirectional.
All networks were trained using stochastic gradient descent, with learning rate , momentum  and random initial weights drawn uniformly from .
All networks except CTC-3l-500h-tanh and PreTrans-3l-250h were first trained with no noise and then, starting from the point of highest log-probability on the development set, retrained with Gaussian weight noise () until the point of lowest phoneme error rate on the development set.
PreTrans-3l-250h was initialised with the weights of CTC-3l-250h, along with the weights of a phoneme prediction network (which also had a hidden layer of 250 LSTM cells), both of which were trained without noise, retrained with noise, and stopped at the point of highest log-probability.
PreTrans-3l-250h was trained from this point with noise added.
CTC-3l-500h-tanh was entirely trained without weight noise because it failed to learn with noise added.
Beam search decoding was used for all networks, with a beam width of 100.


The advantage of deep networks is immediately obvious, with the error rate for CTC dropping from 23.9\% to 18.4\% as the number of hidden levels increases from one to five.
The four networks CTC-3l-500h-tanh, CTC-1l-622h, CTC-3l-421h-uni and CTC-3l-250h all had approximately the same number of weights, but give radically different results.
The three main conclusions we can draw from this are (a) LSTM works much better than  for this task, (b) bidirectional LSTM has a slight advantage over unidirectional LSTMand (c) depth is more important than layer size (which supports previous findings for deep networks~\cite{5704567}).
Although the advantage of the transducer is slight when the weights are randomly initialised, it becomes more substantial when pretraining is used.

\begin{table}
\centering
\capt{TIMIT Phoneme Recognition Results.}{ `Epochs' is the number of passes through the training set before convergence. `PER' is the phoneme error rate on the core test set.}
\tlabel{timit}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}\begin{tabular}{llll}
\hline
Network & Weights & Epochs & PER\\
\hline
CTC-3l-500h-tanh & 3.7M & 107 & 37.6\%\\
CTC-1l-250h & 0.8M & 82 & 23.9\%\\
CTC-1l-622h & 3.8M & 87 & 23.0\%\\
CTC-2l-250h & 2.3M & 55 & 21.0\%\\
CTC-3l-421h-uni & 3.8M & 115 & 19.6\%\\
CTC-3l-250h & 3.8M & 124 & 18.6\%\\
CTC-5l-250h & 6.8M & 150 & 18.4\%\\
Trans-3l-250h & 4.3M & 112 & 18.3\%\\
\textbf{PreTrans-3l-250h} & \textbf{4.3M} & \textbf{144} & \textbf{17.7\%}\\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}








\figt{spectrogram_sensitivity_dbl_ah_p}{sensitivity}{1}{Input Sensitivity of a deep CTC RNN.}{ The heatmap (top) shows the derivatives of the `ah' and `p' outputs printed in red with respect to the filterbank inputs (bottom). The TIMIT ground truth segmentation is shown below. 
Note that the sensitivity extends to surrounding segments; this may be because CTC (which lacks an explicit language model) attempts to learn linguistic dependencies from the acoustic data.}



\section{Conclusions and future work}
\label{sec:conclusion}

We have shown that the combination of deep, bidirectional Long Short-term Memory RNNs with end-to-end training and weight noise gives state-of-the-art results in phoneme recognition on the TIMIT database.
An obvious next step is to extend the system to large vocabulary speech recognition.
Another interesting direction would be to combine frequency-domain convolutional neural networks~\cite{mohamed12cnn} with deep LSTM.

\clearpage
\begin{spacing}{0.97}
\bibliographystyle{IEEEbib}
\bibliography{refs}
\end{spacing}
\end{document}
