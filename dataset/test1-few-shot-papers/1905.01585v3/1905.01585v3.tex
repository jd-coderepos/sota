\documentclass{article} \usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{setspace}
\def\ie{{\em i.e.}}
\def\eg{{\em e.g.}}
\def\etal{{\em et al.}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ignore}[1]{}

\nipsfinalcopy 


\begin{document}

\title{Accurate Face Detection for High Performance}
\author{
  Faen Zhang, Xinyu Fan, Guo Ai, Jianfei Song, Yongqiang Qin, Jiahong Wu\\
  AInnovation Technology Ltd, Beijing, China\\
  \texttt{\{zhangfaen,fanxinyu,aiguo\}@ainnovation.com}\\
  \texttt{\{songjianfei,qinyongqiang,wujiahong\}@ainnovation.com}\\
}
\maketitle

\begin{abstract}
Face detection has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs). Its central issue in recent years is how to improve the detection performance of tiny faces. To this end, many recent works propose some specific strategies, redesign the architecture and introduce new loss functions for tiny object detection. In this report, we start from the popular one-stage RetinaNet~\cite{DBLP:conf/iccv/LinPRK17} approach and apply some recent tricks to obtain a high performance face detector namely AInnoFace. Specifically, we apply the Intersection over Union (IoU) loss function~\cite{DBLP:conf/mm/YuJWCH16} for regression, employ the two-step classification and regression~\cite{DBLP:journals/corr/abs-1809-02693} for detection, revisit the data augmentation based on data-anchor-sampling~\cite{tang2018pyramidbox} for training, utilize the max-out operation~\cite{DBLP:conf/iccv/abs-1708-05237} for classification and use the multi-scale testing strategy~\cite{DBLP:conf/iccv/abs-1708-05237} for inference. As a consequence, the proposed face detection method achieves state-of-the-art performance on the most popular and challenging face detection benchmark WIDER FACE~\cite{DBLP:conf/cvpr/YangLLT16} dataset.
\end{abstract}


\section{Introduction}
Face detection is a tremendously important field in computer vision needed for face recognition, sentiment analysis, video surveillance, and many other fields. Given an arbitrary image, the goal of face detection is to determine whether there are any faces in the image, and if present, return the image location and extent of each face. The recent issue of face detection is how to improve the detection performance in unrestricted scenarios. Because detecting faces in real-world images has many difficulties including occlusion, significant scale variation, different illumination conditions, various facial poses, rich facial expressions, etc. Many works are devoted to solving this issue and great progress has been achieved with the development of deep convolutional neural networks (CNNs). For example, the average precision (AP) performance on the challenging WIDER FACE dataset~\cite{DBLP:conf/cvpr/YangLLT16} has been improved from  to  over recent years.

To improve the performance of face detection in unrestricted scenarios where exists plenty of tiny faces, some works~\cite{DBLP:conf/icb/YangYLL14,DBLP:conf/iccv/YangLLT15,DBLP:journals/spl/ZhangZLQ16,DBLP:conf/icpr/Ohn-BarT16a,DBLP:journals/corr/YangXLT17} combine traditional methods (\eg, cascade-based mechanism and part-based in DPM) with deep learning methods (\eg, CNN) to perform face detection. A number of works~\cite{DBLP:journals/corr/ZhuZLS16,DBLP:conf/cvpr/HuR17,DBLP:conf/iccv/NajibiSCD17} resort to the context information around the face region to find tiny faces based on the Faster R-CNN~\cite{DBLP:journals/pami/RenHG017} and SSD~\cite{DBLP:conf/eccv/LiuAESRFB16} detectors. Several works~\cite{DBLP:conf/eccv/CaiFFV16,DBLP:journals/corr/abs-1809-02693,DBLP:journals/corr/abs-1712-00721,DBLP:journals/corr/abs-1811-08557} redesign the architecture of modern object detection to better detect tiny faces. A series of works~\cite{DBLP:journals/corr/WangLJW17,DBLP:journals/corr/abs-1709-05256,DBLP:conf/iccv/abs-1708-05237,zhu2018seeing,DBLP:journals/corr/abs-1802-02142} propose some special strategies for tiny faces into the generic object detection methods to improve face detection performance. There are many works~\cite{tang2018pyramidbox,DBLP:journals/corr/abs-1901-06651,DBLP:journals/corr/abs-1810-10220} present some new data augmentations for tiny faces to improve the performance. Some works~\cite{wang2017fan,DBLP:journals/corr/abs-1901-02350} introduce the attention mechanism on the feature maps to focus on face regions for better detection performance.

In this work, we first modify the popular one-stage RetinaNet~\cite{DBLP:conf/iccv/LinPRK17} method to perform face detection as our baseline model. Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace: (1) Employing the two-step classification and regression for detection; (2) Applying the Intersection over Union (IoU) loss function for regression; (3) Revisiting the data augmentation based on data-anchor-sampling for training; (4) Utilizing the max-out operation for robuster classification; (5) Using the multi-scale testing strategy for inference. Consequently, we achieve some new state-of-the-art AP results on the challenging face detection benchmark WIDER FACE~\cite{DBLP:conf/cvpr/YangLLT16} dataset.


\section{Related Work}
\subsection{Traditional Method}
Face detection has been extensively studied from its emergence in the 1990s to the present because of its wide practical applications. The pioneering work~\cite{DBLP:journals/ijcv/ViolaJ04} of Viola and Jones uses the Haar-like feature and the AdaBoost strategy to train several cascaded face detectors, achieving a very good tradeoff between accuracy and efficiency in some simple and fixed scenarios. Afterwards, subsequent works~\cite{DBLP:journals/ijcv/BrubakerWSMR08,DBLP:journals/pami/LiaoJL16} have made great progress by developing more advanced features and more powerful classifiers. Apart from the boosted cascade methods, several studies~\cite{DBLP:conf/cvpr/YanLWL14,DBLP:conf/cvpr/ZhuR12,DBLP:conf/icb/YangYLL14} introduce another famous framework of Deformable Part Model (DPM)~\cite{DBLP:conf/eccv/MathiasBPG14} to the filed of face detection task, which detect faces by modelling the relationship of deformable facial parts and achieve promising performance in some simple application scenarios. However, these traditional face detectors are unreliable in complex scenarios because they depend on non-robust hand-crafted features and classifiers.

\subsection{Deep Learning Method}
Deep learning approaches significantly boost the recent progress in the face detection filed and the CNN-based face detectors have achieved the highest performance in the last few years. The cascade CNN-based methods~\cite{DBLP:conf/cvpr/LiLSBH15,DBLP:conf/cvpr/QinYLH16} train a series of CNN models separately or jointly to perform face detection, and achieve promising accuracy and efficiency simultaneously. After that, MTCNN~\cite{DBLP:journals/spl/ZhangZLQ16} and PCN~\cite{shi2018real} add another extra branch to detect five facial landmarks and predict face angles via the multi-task learning in a coarse-to-fine manner under a cascade-style structure. Faceness~\cite{DBLP:conf/iccv/YangLLT15} obtains different scores according to the spatial structure and arrangement of facial parts to detect faces under severe occlusion and unconstrained pose variations. LDCF+~\cite{DBLP:conf/icpr/Ohn-BarT16a} utilizes the boosted decision tree classifier to detect faces. UnitBox~\cite{DBLP:conf/mm/YuJWCH16} introduces an Intersection-over-Union (IoU) loss to directly minimize the IoUs of the predictions and the ground-truths for more accurate location. ScaleFace~\cite{DBLP:journals/corr/YangXLT17} detects different scales of faces via applying a specialized set of CNNs with different structures. SAFD~\cite{hao2017scale} develops a scale proposal stage to automatically normalize face sizes prior to detection. Hu~\etal~\cite{DBLP:conf/cvpr/HuR17} explore the contextual information with some separate detectors for different scales to find tiny faces. SAP~\cite{song2018beyond} finds face via paying attention to specific scales in image pyramid and valid locations in each scales layer. Zhu~\etal~\cite{zhu2018seeing} use the Expected Max Overlapping (EMO) score to evaluate the quality of anchor setting. Bai~\etal~\cite{bai2018finding} generate a clear super-resolution face from a blurry small one via to GAN~\cite{goodfellow2014generative} to detect blurry small faces.

Besides, many state-of-the-are face detectors are evolved from generic object detection methods including the two-stage approach (Faster R-CNN~\cite{DBLP:journals/pami/RenHG017}, R-FCN~\cite{DBLP:conf/nips/DaiLHS16} and FPN~\cite{DBLP:conf/cvpr/LinDGHHB17}) and the one-stage approach (SSD~\cite{DBLP:conf/eccv/LiuAESRFB16}, RefineDet~\cite{DBLP:journals/corr/abs-1711-06897} and RetinaNet~\cite{DBLP:conf/iccv/LinPRK17}). Based on Faster R-CNN and R-FCN, some face detection methods (\eg, Face R-CNN~\cite{DBLP:journals/corr/WangLJW17}, Face R-FCN~\cite{DBLP:journals/corr/abs-1709-05256}, CMS-RCNN~\cite{DBLP:journals/corr/ZhuZLS16} and FDNet~\cite{DBLP:journals/corr/abs-1802-02142}) design several specific training and testing strategies with the consideration of the characteristics of face detection. Similar to FPN, FANet~\cite{DBLP:journals/corr/abs-1712-00721} aggregates higher-level features to augment lower-level features for face detection. Based on SSD, DCFPN~\cite{DBLP:conf/ccbr/ZhangZLSWL17} and FaceBoxes~\cite{DBLP:conf/ijcb/abs-1708-05234} design a lightweight face detection network to achieve CPU real-time speed with promising result. In contrast, high performance face detectors including SFD~\cite{DBLP:conf/iccv/abs-1708-05237}, SFDet~\cite{zhang2019single}, SSH~\cite{DBLP:conf/iccv/NajibiSCD17}, PyramidBox~\cite{tang2018pyramidbox,li2019pyramidbox++} and DSFD~\cite{DBLP:journals/corr/abs-1810-10220} equip SSD with some specific strategies for better detection of small faces, such as architecture diagram, training strategy, contextual reasoning and multiple layers exploiting. Besides, FAN~\cite{wang2017fan} and DFS~\cite{DBLP:journals/corr/abs-1811-08557} use different types of attention mechanism on RetinaNet to handle hard faces. SRN~\cite{DBLP:journals/corr/abs-1809-02693} combines the multi-step detection in RefineDet~\cite{DBLP:journals/corr/abs-1711-06897} and the focal loss in RetinaNet~\cite{DBLP:conf/iccv/LinPRK17} to perform efficient and accurate face detection. After that, VIM-FD~\cite{DBLP:journals/corr/abs-1901-02350} and ISRN~\cite{DBLP:journals/corr/abs-1901-06651} combine many previous techniques on SRN and achieve new state-of-the-art performance.

\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\linewidth]{network.png}
\caption{Architecture of our face detector baseline built on RetinaNet~\cite{DBLP:conf/iccv/LinPRK17}. (a) Backbone: a feed-forward ResNet-152~\cite{DBLP:conf/cvpr/HeZRS16} architecture extracts the multi-scale feature maps. (b) Neck: a 6-level Feature Pyramid Network (FPN)~\cite{DBLP:conf/cvpr/LinDGHHB17} structure generates a richer multi-scale convolutional feature pyramid. After that, two shared subnetworks are attached, one for classifying anchor boxes (c) and one for regressing from anchor boxes to ground-truth object boxes (d). At the end, we use the focal loss~\cite{DBLP:conf/iccv/LinPRK17} for the binary classification and the IoU loss~\cite{DBLP:conf/mm/YuJWCH16} for the regression.}
\label{fig:framework}
\end{figure*}


\section{Method}
Starting from the RetinaNet~\cite{DBLP:conf/iccv/LinPRK17} face detector baseline, we apply some recently proposed strategies to achieve state-of-the-art performance on the challenging WIDER FACE~\cite{DBLP:conf/cvpr/YangLLT16} dataset.

\subsection{RetinaNet Baseline}
RetinaNet is one of the most popular one-stage object detection methods. One-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors because of extreme class imbalance encountered during training. To solve this issue, the focal loss is proposed in RetinaNet as follow:

and

where  specifies the ground-truth class,  is the model's estimated probability for the class with label ,  is a balanced factor and  a tunable focusing parameter. The focal loss is the reshaping of cross entropy loss such that it down-weights the loss assigned to well-classified examples. The novel focal loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.

In this report, we take the modified RetinaNet shown in Figure~\ref{fig:framework} as our baseline, which is a single and unified network composed of a backbone network, a neck network and two task-specific subnetworks. The backbone and neck networks are responsible for computing a multi-scale convolutional feature maps over an entire input image. The first subnet performs classification on the backbones output and the second subnet performs convolution bounding box regression. Specifically, we adopt the ResNet-152~\cite{DBLP:conf/cvpr/HeZRS16} with 6-level feature pyramid structure as our backbone network. We follow the way in FPN~\cite{DBLP:conf/cvpr/LinDGHHB17} to generate the 6-level feature maps (P2 to P7) for detection.

\subsection{IoU Regression Loss}
The object detection task consists of the classification subtask and the regression subtask. For regression, the smooth L1 loss~\cite{DBLP:conf/iccv/Girshick15} is the common loss function used to reduce the difference between anchor boxes and ground-truth bounding boxes, and the Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, as indicated in~\cite{rezatofighi2019generalized}, there is a gap between optimizing the commonly used smooth L1 distance losses for regressing the parameters of a bounding box and maximizing this IoU metric value. The optimal objective for a metric should be the metric itself, so we follow the UnitBox~\cite{DBLP:conf/mm/YuJWCH16} to minimize the difference between the predictions and the ground-truths via directly using IoU as the regression loss.

The IoU regression loss function is defined as below:

where  and  are the predicted bounding box and the ground-truth bounding box respectively,  and  indicate the intersection and union area between  and .

\subsection{Selective Refinement Network}
As described in Selective Refinement Network (SRN)~\cite{DBLP:journals/corr/abs-1809-02693}, using RetinaNet to perform face detection still exists two problems: (a) low recall efficiency: the precision is not high enough at high recall rates, \ie, the Precision-Recall curve extends far enough to the right but not steep enough; (b) low location accuracy: the performance drops dramatically as the IoU threshold increases, \ie, the accuracy of the bounding box location needs to be improved. To solve the aforementioned two issues, Selective Two-step Classification (STC) and Selective Two-step Regression (STR) are proposed in SRN and we follow these designs to further improve the performance of our face detector.

STC conducts two-step classification on three low level detection layers to filter out most simple negatives and reduce the search space for the subsequent classifier. Its loss function is:

STR performs is two-step regression on three high level detection layers to adjust anchors and provide better initialization for the subsequent regressor. Its loss function is: 

where  is the anchor index, / and / are the prediction of classification and regression in the first/second step, / are the ground truth of class/location, / are the number of positive anchors in the first/second step, / are the collection of classification/regression samples for the first step and  is the collection of samples for the second step,  is the sigmoid focal loss and  indicates that the IoU regression loss is computed only for positive anchors.

\subsection{Data Augmentation}
Date augmentation is of importance for one-stage detectors to construct a robust model to adapt to variations of objects, especially for face detection where has plenty tiny faces. Following most of the face detectors, we randomly expand and crop the original training images with additional random photometric distortion~\cite{DBLP:journals/corr/Howard13} and flipping to generate the training samples. Besides, with probability of , we replace the above random cropping operation with the anchor-based sampling like data-anchor-sampling in PyramidBox~\cite{tang2018pyramidbox} to diversify the scale distribution of training samples. The anchor-based sampling operation first randomly selects a face of size  in a batch, then finds its nearest anchor scale . After that, it chooses a random scale  around the nearest anchor scale. Finally, it resizes the image by  and randomly crops a standard size of the training size containing the selected face to get the anchor-sampled training data.

\subsection{Max-out Label}
To reduce the tiny false positives from background regions, the max-out operation for the background class is introduced in~\cite{DBLP:conf/iccv/abs-1708-05237}, and then the following work~\cite{tang2018pyramidbox} uses this max-out operation on both foreground and background classes. In the proposed face detection method, the max-out operation is applied in the classification subnet to recall more faces and reduce false positives simultaneously. To be more specific, the classification subnet first predicts  scores for each anchor, and then selects  and  as the final face and no-face confidence score to compute the classification loss. Empirically, we set  and  to train our final model.

\subsection{Multi-scale Testing}
Since there are plenty of tiny faces in the challenging WIDER FACE dataset, the multi-scale testing strategy is useful to improve the performance. We use the open source code\footnote{\url{https://github.com/sfzhang15/SFD/blob/master/sfd_test_code/WIDER_FACE/wider_test.py}} to conduct the multi-scale testing during inference. It inputs the image to the trained model multiple times with different sizes and then merge these detection results with the bounding box voting operation.


\section{Experiment}

\subsection{Experimental Dataset}
We verify the proposed AInnoFace detector on the WIDER FACE~\cite{DBLP:conf/cvpr/YangLLT16} dataset, which is a popular face detection benchmark dataset and whose images are selected from the publicly available WIDER~\cite{xiong2015wider} dataset. The WIDER FACE dataset contains  images and  annotated face bounding boxes with a high degree of variability in scale, pose, occlusion, expression, makeup and illumination as depicted in Figure~\ref{fig:widerface}. All the images are organized based on  event classes and are randomly selected from each event class by // as training, validation and testing subsets. Based on the detection rate of EdgeBox~\cite{DBLP:conf/eccv/ZitnickD14}, the validation and testing subsets are divided into three difficulty levels: Easy, Medium, Hard. The Average Precision (AP) is adopted as the evaluation metric. Following MALF~\cite{DBLP:conf/fgr/YangYLL15} and Caltech~\cite{dollar2012pedestrian} datasets, this dataset does not release bounding box ground truth for the testing images and researchers are required to submit final prediction files to get the AP performance on the testing subset. The proposed method is trained on the training subset and evaluated on both validation and testing subsets.
\begin{figure}[!h]
\centering
\includegraphics[width=1.0\textwidth]{widerface.jpg}
\caption{Some sample images for each attribute of WIDER FACE~\cite{DBLP:conf/cvpr/YangLLT16}.}
\label{fig:widerface}
\end{figure}

\subsection{Anchor Detail}
Following \cite{DBLP:journals/corr/abs-1809-02693} we set two  and  anchor scales ( is the downsampling factor of detection layer) and one  aspect ratio. Thus, there are  anchors at each location, covering the scale of  pixels in the  input image. During the training phase, anchors are assigned to ground-truth boxes using the  IoU threshold and to background if their IoU is in . The rest anchors in  are ignored. We set  and  for the first step, and  and  for the second step.

\begin{figure}[!h]
\centering
\subfigure[Val: Easy]{
\label{fig:ve}
\includegraphics[width=0.49\linewidth]{ve.jpg}}
\subfigure[Test: Easy]{
\label{fig:te}
\includegraphics[width=0.49\linewidth]{te.jpg}}
\subfigure[Val: Medium]{
\label{fig:vm}
\includegraphics[width=0.49\linewidth]{vm.jpg}}
\subfigure[Test: Medium]{
\label{fig:tm}
\includegraphics[width=0.49\linewidth]{tm.jpg}}
\subfigure[Val: Hard]{
\label{fig:vh}
\includegraphics[width=0.49\linewidth]{vh.jpg}}
\subfigure[Test: Hard]{
\label{fig:th}
\includegraphics[width=0.49\linewidth]{th.jpg}}
\caption{Precision-recall curves on WIDER FACE validation and testing subsets.}
\label{fig:wider-face-ap}
\end{figure}


\subsection{Optimization Detail}
The backbone network in the proposed AInnoFace detector is initialized by the pretrained model on the ImageNet~\cite{DBLP:journals/ijcv/RussakovskyDSKS15} dataset. We use the ``xavier''~\cite{DBLP:journals/jmlr/GlorotB10} method to randomly initialize the parameters in the newly added convolutional layers. The stochastic gradient descent (SGD) algorithm is used to fine-tune the model with  momentum,  weight decay and batch size . The warmup strategy~\cite{DBLP:journals/corr/GoyalDGNWKTJH17} is applied to gradually ramp up the learning rate from  to  at the first  epochs. After that, it switches to the regular learning rate schedule, i.e., dividing by  at  and  epochs and ending at  epochs. The full training and testing codes are built on the PyTorch library~\cite{paszke2017pytorch}.


\subsection{Evaluation Result}
Figure~\ref{fig:wider-face-ap} shows the comparison of the proposed AInnoFace detector with twenty-seven state-of-the-art methods~\cite{DBLP:conf/eccv/CaiFFV16,DBLP:journals/corr/abs-1809-02693,DBLP:conf/cvpr/HuR17,DBLP:journals/corr/abs-1810-10220,DBLP:conf/iccv/NajibiSCD17,DBLP:conf/icpr/Ohn-BarT16a,tang2018pyramidbox,DBLP:journals/corr/abs-1811-08557,DBLP:journals/corr/WangLJW17,wang2017fan,DBLP:journals/corr/abs-1709-05256,DBLP:conf/icb/YangYLL14,DBLP:conf/iccv/YangLLT15,DBLP:conf/cvpr/YangLLT16,DBLP:journals/corr/YangXLT17,DBLP:journals/corr/abs-1802-02142,DBLP:journals/corr/abs-1712-00721,DBLP:journals/spl/ZhangZLQ16,DBLP:conf/iccv/abs-1708-05237,zhu2018seeing,DBLP:journals/corr/ZhuZLS16,DBLP:journals/corr/abs-1901-02350,DBLP:journals/corr/abs-1901-06651,li2019pyramidbox++,zhang2019single} on both the validation and testing subsets based on the precision-recall curve and AP. As shown in Figure~\ref{fig:wider-face-ap}, our face detector sets some new state-of-the-art results based on the AP score across the three subsets on both validation and testing subsets, \ie,  (Easy),  (Medium) and  (Hard) for validation subset, and  (Easy),  (Medium) and  (Hard) for testing subset. These results outperform all the compared state-of-the-art methods and demonstrate the superiority of our AInnoFace detector.


\section{Conclusion}
In this report, we present a high performance face detector by equipping the popular one-stage RetinaNet~\cite{DBLP:conf/iccv/LinPRK17} method with some recent tricks: (1) Employing the two-step classification and regression for detection; (2) Applying the Intersection over Union (IoU) loss function for regression; (3) Revisiting the data augmentation based on data-anchor-sampling for training; (4) Utilizing the max-out operation for robuster classification; (5) Using the multi-scale testing strategy for inference. Experiments on the WIDER FACE dataset demonstrate that the proposed AInnoFace detector achieves the state-of-the-art detection performance.

\bibliographystyle{plain}
\bibliography{reference}

\end{document}