

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{tikz-dependency}
\usepackage{diagbox}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{array}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{afterpage}
\usepackage{float}
\usepackage[belowskip=5pt,aboveskip=5pt]{caption}
\usepackage{titlesec}
\usepackage{tablefootnote}

\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{1067} 



\newcommand\BibTeX{B\textsc{ib}\TeX}



\makeatletter
\newenvironment{smalleralign}[1][\small]
 {\par\nopagebreak\leavevmode\vspace*{-\baselineskip}\skip0=\abovedisplayskip
  #1\def\maketag@@@##1{\hbox{\m@th\normalfont\normalsize##1}}\abovedisplayskip=\skip0
  \align}
 {\endalign\ignorespacesafterend}
\makeatother



\renewcommand\theadgape{\Gape[0pt]}
\renewcommand\cellgape{\Gape[0pt]}

\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\newcolumntype{C}{@{\extracolsep{5pt}}c@{\extracolsep{}}}
\captionsetup[subtable]{labelformat=simple, labelsep=colon}

\newcommand\ul[1]{\underline{\smash{#1}}}
\newcommand\tline[2]{}
\newcommand\tlinesmash[2]{}

\newcommand\twoline[2]{\raisebox{-\baselineskip}{\shortstack{#1{\hspace{3cm}}\\#2}}}

\newcommand*{\scale}[2][4]{\scalebox{#1}{\ensuremath{#2}}}

\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\textcolor{lightgray}{\ding{55}}}

\newcommand{\better}[1]{\textcolor{blue}{\textbf{#1}}}
\newcommand{\worse}[1]{\textcolor{red}{#1}}

\newcommand{\MLP}{\text{MLP}}
\newcommand{\softmax}{\mathrm{softmax}}

\def\modelname{SpanRel\xspace}
\def\benchmarkname{GLAD\xspace}

\def\tightcol{\hskip 2pt}
\def\smallcol{\hskip 5pt}

\newcommand{\gn}[1]{\textcolor{red}{\bf\small [#1 --GN]}}
\newcommand{\zj}[1]{\textcolor{brown}{\bf\small [#1 --ZJ]}}
\newcommand{\jun}[1]{\textcolor{purple}{\bf\small [#1 --JUN]}}
\newcommand{\wx}[1]{\textcolor{magenta}{\bf\small [#1 --WX]}}

\title{Generalizing Natural Language Analysis through \\ Span-relation Representations}

\author{
Zhengbao Jiang, Wei Xu, Jun Araki, Graham Neubig \\
Language Technologies Institute, Carnegie Mellon University \\
Department of Computer Science and Engineering, Ohio State University \\
Bosch Research North America \\
\texttt{\{zhengbaj,gneubig\}@cs.cmu.edu} \\
\texttt{xu.1265@osu.edu, jun.araki@us.bosch.com}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures.
In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks.
We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models.
We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks.
Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.
\end{abstract}

\section{Introduction}
A large number of natural language processing (NLP) tasks exist to analyze various aspects of human language, including syntax (e.g., constituency and dependency parsing), semantics (e.g., semantic role labeling), information content (e.g., named entity recognition and relation extraction), or sentiment (e.g., sentiment analysis).
At first glance, these tasks are seemingly very different in both the structure of their output and the variety of information that they try to capture.
To handle these different characteristics, researchers usually use specially designed neural network architectures.
In this paper we ask the simple questions:
are the task-specific architectures really necessary?
Or with the appropriate representational methodology, can we devise \emph{a single model that can perform --- and achieve state-of-the-art performance on --- a large number of natural language analysis tasks}?

\begin{figure}
\begin{center}
\includegraphics[width=0.7\columnwidth]{brat2.png}
\end{center}
\caption{An example from BRAT, consisting of \textcolor{green}{POS}, \textcolor{orange}{NER}, and \textcolor{blue}{RE}.}
\label{fig:brat}
\end{figure}

\begin{table*}[tb]
\resizebox{\textwidth}{!}{\begin{tabular}{ccccccccccc}
\toprule
 & \multicolumn{4}{c}{\textbf{Information Extraction}} & \multirow{2}{*}{\textbf{POS}} & \multicolumn{2}{c}{\textbf{Parsing}} & \multirow{2}{*}{\textbf{SRL}} & \multicolumn{2}{c}{\textbf{Sentiment}} \\
\cline{2-5} 
\cline{7-8}
\cline{10-11}
 & NER & RE & Coref. & OpenIE  & & Dep. & Consti. & & ABSA & ORL \\
\toprule
\multicolumn{11}{c}{Different Models for Different Tasks} \\
\midrule
ELMo \citep{peters-etal-2018-deep} & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark \\
BERT \citep{devlin-etal-2019-bert} & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
SpanBERT \citep{joshi:19:spanbert} & \xmark & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
\midrule
\multicolumn{11}{c}{Single Model for Different Tasks} \\
\midrule
\citet{guo-etal-2016-unified} & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark \\
\citet{swayamdipta-etal-2018-syntactic} & \xmark & \xmark & \cmark & \xmark & \xmark & \xmark & \cmark & \cmark & \xmark & \xmark \\
\citet{strubell-etal-2018-linguistically} & \xmark & \xmark & \xmark & \xmark & \cmark & \cmark & \xmark & \cmark & \xmark & \xmark \\
\citet{clark-etal-2018-semi} & \cmark & \xmark & \xmark & \xmark & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark \\
\citet{luan:18:mtlie,luan-etal-2019-general} & \cmark & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
\citet{dixit-al-onaizan-2019-span} & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
\citet{marasovic-frank-2018-srl4orl} & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \cmark \\
\citet{hashimoto:17:mtlsocher} & \xmark & \xmark & \xmark & \xmark & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark \\
 This Work & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
\bottomrule
\end{tabular}
}
\caption{A comparison of the tasks covered by previous work and our work.
}
\vspace{-3mm}
\label{tab:related_work}
\end{table*}

Interestingly, in the domain of \emph{efficient human annotation interfaces}, it is already standard to use unified representations for a wide variety of NLP tasks.
\autoref{fig:brat} shows one example of the BRAT \citep{stenetorp-etal-2012-brat} annotation interface, which has been used for annotating data for tasks as broad as part-of-speech tagging, named entity recognition, relation extraction, and many others.
Notably, this interface has a single unified format that consists of spans (e.g., the span of an entity), labels on the spans (e.g., the variety of entity such as ``person'' or ``location''), and labeled relations between the spans (e.g., ``born-in'').
These labeled relations can form a tree or a graph structure, expressing the linguistic structure of sentences (e.g., dependency tree). 
We detail this BRAT format and how it can be used to represent a wide number of natural language analysis tasks in \autoref{sec:brat}. 

The simple hypothesis behind our paper is: \emph{if humans can perform natural language analysis in a single unified format, then perhaps machines can as well}.
Fortunately, there already exist NLP models that perform span prediction and prediction of relations between pairs of spans, such as the end-to-end coreference model of \citet{lee:17:e2ecoref}.
We extend this model with minor architectural modifications (which are \emph{not} our core contributions) and pre-trained contextualized representations (e.g., BERT; \citet{devlin-etal-2019-bert}\footnote{In contrast to work on pre-trained contextualized representations like ELMo \citep{peters-etal-2018-deep} or BERT \citep{devlin-etal-2019-bert} that learn unified \emph{features} to represent the \emph{input} in different tasks, we propose a unified \emph{representational methodology} that represents the \emph{output} of different tasks. Analysis models using BERT still use special-purpose output predictors for specific tasks or task classes.}) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference resolution (Coref.), open information extraction (OpenIE), part-of-speech tagging (POS), dependency parsing (Dep.), constituency parsing (Consti.), semantic role labeling (SRL), aspect based sentiment analysis (ABSA), and opinion role labeling (ORL).
While previous work has used similar formalisms to \emph{understand} the representations learned by pre-trained embeddings \citep{tenney:19:bertpipeline,tenney:19:bertprobe}, to the best of our knowledge this is the first work that uses such a unified model to actually \emph{perform analysis}.
Moreover, we demonstrate that despite the model's simplicity, it can achieve comparable performance with special-purpose state-of-the-art models on the tasks above (\autoref{tab:related_work}).
We also demonstrate that this framework allows us to easily perform multi-task learning (MTL), leading to improvements when there are related tasks to be learned from or data is sparse.
Further analysis shows that dissimilar tasks exhibit divergent attention patterns, which explains why MTL is harmful on certain tasks.
We have released our code and the \textbf{G}eneral \textbf{L}anguage \textbf{A}nalysis \textbf{D}atasets (\benchmarkname) benchmark with 8 datasets covering 10 tasks in the BRAT format at \url{https://github.com/neulab/cmu-multinlp},
and provide a leaderboard to facilitate future work on generalized models for NLP.

\begin{table*}[tb]
\begin{subtable}[t]{0.47\textwidth}
\renewcommand\thesubtable{Table \thetable a}
\renewcommand{\arraystretch}{1.35}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}[t]{ll}
\toprule
 \textbf{Task} & \textbf{\ul{Spans} annotated with labels} \\
\hline \16pt]
Consti. & \tline{S}{And \tline{NP}{\tlinesmash{NP}{their suspicions} \tline{PP}{of \tlinesmash{NP}{each other}}} \tline{VP}{run \tlinesmash{ADVP}{deep}}.} \vspace{8pt} \16pt]
 ABSA & Great laptop that offers many great \tlinesmash{positive}{features}! \
\text{sim}_k(t, t^\prime) = -\frac{1}{|\mathcal{X}_t|} \sum_{\mathbf{x} \in \mathcal{X}_t}{\left\Vert A_k^t(\mathbf{x}) - A_k^{t^\prime}(\mathbf{x})\right\Vert_F},

where  is the attention map extracted from the -th head by running the model trained from task  on sentence .
We select OpenIE as the target task because it shows the largest performance variation when paired with different source tasks (34.0 - 38.8) in Table~\ref{tab:mtl_pairwise}.
We visualize the attention similarity of all heads in BERT (12 layers  12 heads) between two mutually harmful tasks (OpenIE/POS on the left) and between two mutually helpful tasks (OpenIE/SRL on the right) in \autoref{fig:attn_vis}.
A common trend is that heads in higher layers exhibit more divergence, probably because they are closer to the prediction layer, thus easier to be affected by the end task.
Overall, it can be seen that OpenIE/POS has much more attention divergence than OpenIE/SRL.
A notable difference is that almost \textit{all} heads in the last two layers of the OpenIE/POS models differ significantly, while \textit{some} heads in the last two layers of the OpenIE/SRL models still behave similarly, providing evidence that failure of MTL can be attributed to the fact that dissimilar tasks requires different attention patterns.
We further compute average attention similarities for all source tasks in \autoref{fig:est}, and we can see that there is a strong correlation (Pearson correlation of 0.97) between the attentions similarity and the performance of pairwise MTL, supporting our hypothesis that attention pattern similarities can be used to predict improvements of MTL.

\begin{figure}[tb]
\centering
\begin{subfigure}{0.56\columnwidth}
\centering
\includegraphics[width=1.0\linewidth]{attn_vis.pdf}
\caption{Attention similarity between OpenIE/POS (left), and between OpenIE/SRL (right) for all heads.}
\label{fig:attn_vis}
\end{subfigure}
\hfill
\begin{subfigure}{0.42\columnwidth}
\centering
\includegraphics[width=1.0\linewidth]{attn.eps}
\caption{Correlation between attention similarity and MTL performance.}
\label{fig:est}
\end{subfigure}
\caption{Attention-based task relatedness analysis.}
\vspace{-3mm}
\end{figure}

\begin{figure*}[tb]
\begin{minipage}{\textwidth}
\begin{minipage}[b]{0.78\textwidth}
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c@{\smallcol}c@{\smallcol}c@{\smallcol}c@{\smallcol}c||c|c@{\smallcol}c@{\smallcol}c@{\smallcol}c@{\smallcol}c@{\smallcol}c@{\smallcol}c}
\toprule
 & \multicolumn{6}{c||}{\textbf{GloVe}} & \multicolumn{6}{c}{\textbf{BERT}} \\
\midrule
\backslashbox{\textbf{Target}}{\textbf{Source}} & \textbf{STL} & \textbf{POS} & \textbf{NER} & \textbf{Consti.} & \textbf{Dep.} & \textbf{SRL} & \textbf{STL} & \textbf{POS} & \textbf{NER} & \textbf{Consti.} & \textbf{Dep.} & \textbf{SRL} \\
\midrule
OpenIE & 28.3 & \better{29.9} & \worse{27.0} & \better{31.2} & \better{32.9} & \better{34.1} & 36.7 & \worse{34.0} & \worse{34.3} & \worse{35.2} & \better{37.8} & \better{38.3} \\
NER (WLP) & 77.6 & 77.8 & \better{78.3} & 77.9 & \better{78.6} & \better{78.1} & 78.1 & 78.0 & 78.1 & 78.1 & 77.7 & \better{78.8} \\
RE (WLP) & 64.9 & \better{65.5} & \better{65.6} & 64.9 & \better{66.5} & \better{65.9} & 64.7 & 64.4 & 64.7 & 64.3 & 64.9 & \better{65.3} \\
RE (SemEval10) & 50.7 & \better{52.3} & \better{52.8} & \worse{49.6} & \better{52.9} & \better{52.8} & 61.7 & 61.9 & \worse{60.2} & \worse{59.2} & 62.1 & \worse{59.9} \\
ABSA & 63.5 & 63.4 & \worse{62.8} & \worse{59.8} & 63.5 & \worse{60.2} & 70.8 & \worse{68.9} & \better{71.4} & 70.4 & \worse{69.9} & \worse{69.6} \\
ORL & 38.2 & \worse{35.7} & 37.9 & \worse{36.1} & 38.6 & \better{41.0} & 44.5 & \better{45.8} & 44.2 & 44.8 & \better{45.1} & \better{46.6} \\
SRL (10k) & 68.8 & \better{69.6} & 68.9 & \better{70.7} & \better{71.3} & - & 78.7 & \better{79.4} & \better{79.5} & \better{79.6} & \better{79.8} & - \\
\bottomrule
\end{tabular}
}
\captionof{table}{Performance of pairwise multi-task learning with GloVe and BERT. \better{blue} indicates results better than STL, \worse{red} indicates worse, and black means almost the same (i.e., a difference within 0.5).
We show the performance after fine-tuning.
Dataset of source tasks POS, Consti., Dep. is PTB and dataset of NER is CoNLL-2003.
}
\vspace{-3mm}
\label{tab:mtl_pairwise}
\end{minipage}
\hfill
\begin{minipage}[b]{0.2\textwidth}
\centering
\includegraphics[width=1.0\columnwidth, clip, keepaspectratio]{srl_size}
\captionof{figure}{MTL Performance of SRL wrt. the data size.}
\vspace{-3mm}
\label{fig:srl_size}
\end{minipage}
\end{minipage}
\end{figure*}

\paragraph{MTL under Different Settings}
We analyze how token representations and sizes of the target dataset affect the performance of MTL.
Comparing BERT and GloVe in Table~\ref{tab:mtl_pairwise}, the improvements become smaller or vanish as the token representation becomes stronger, e.g., improvement on OpenIE with SRL reduces from 5.8 to 1.6.
This is expected because both large-scale pre-training and MTL aim to learn general representations and their benefits tend to overlap in practice.
Interestingly, some helpful source tasks become harmful when we shift from GloVe to BERT, such as OpenIE paired with POS.
We conjecture that the gains of MTL might have already been achieved by BERT, but the task-specific characteristics of POS hurt the performance of OpenIE.
We did not observe many tasks benefitting from MTL for the GloVe-based model in \autoref{tab:mtl} because it is trained on \textit{all} tasks (instead of \textit{two}), which is beyond its limited model capacity.
The improvements of MTL shrink as the size of the SRL datasets increases, as shown in \autoref{fig:srl_size}, indicating that MTL is useful when the target data is sparse.

\paragraph{Time Complexity Analysis}
Time complexities of span and relation prediction are  and  respectively for a sentence of  tokens (\autoref{sec:spanrelationmodel}).
The time complexity of BERT is , dominated by its  self-attention layers.
Since the pruning threshold  is usually less than 1, the computational overhead introduced by the span-relation output layer is much less than BERT.
In practice, we observe that the training/testing time is mainly spent by BERT. For SRL, one of the most computation-intensive tasks with long spans and dense span/relation annotations, 85.5\% of the time is spent by BERT. For POS, a less heavy task, the time spent by BERT increases to 98.5\%.
Another option for span prediction is to formulate it as a sequence labeling task, as in previous works \citep{lample:16:ner,he-etal-2017-deep}, where time complexity is . Although slower than token-based labeling models, span-based models offer the advantages of being able to model overlapping spans and use span-level information for label prediction \citep{lee:17:e2ecoref}.

\section{Related Work}\label{sec:related}

\paragraph{General Architectures for NLP}
There has been a rising interest in developing general architectures for different NLP tasks, with the most prominent examples being sequence labeling framework \citep{collobert:11:nlpscratch,ma:16:lstmcnncrf} used for tagging tasks and sequence-to-sequence framework \citep{sutskever:14:seq2seq} used for generation tasks.
Moreover, researchers typically pick related tasks, motivated by either linguistic insights or empirical results, and create a general framework to perform MTL, several of which are summarized in \autoref{tab:related_work}.
For example, \citet{swayamdipta-etal-2018-syntactic} and \citet{strubell-etal-2018-linguistically} use constituency and dependency parsing to improve SRL.
\citet{luan:18:mtlie,luan-etal-2019-general,wadden-etal-2019-entity} use a span-based model to jointly solve three information-extraction-related tasks (NER, RE, and Coref.).
\citet{li-2019-mrcner} formulate both nested NER and flat NER as a machine reading comprehension task.
Compared to existing works, we aim to create an output representation that can solve \emph{nearly every} natural language analysis task in one fell swoop, allowing us to cover a far broader range of tasks with a single model.

In addition, NLP has seen a recent burgeoning of contextualized representations pre-trained on large corpora (e.g., ELMo \citep{peters-etal-2018-deep} and BERT \citep{devlin-etal-2019-bert}).
These methods focus on learning generic \emph{input} representations, but are agnostic to the \emph{output} representation, requiring different predictors  for different tasks. In contrast, we present a methodology to formulate the output of different tasks in a unified format.
Thus our work is orthogonal to those on contextualized embeddings.
Indeed, in \autoref{sec:exp_mtl}, we demonstrate that the \modelname model can benefit from stronger contextualized representation models, and even provide a testbed for their use in natural language analysis.

\paragraph{Benchmarks for Evaluating Natural Language Understanding} Due to the rapid development of NLP models, large-scale benchmarks, such as SentEval \citep{conneau-kiela-2018-senteval}, GLUE \citep{wang:19:glue}, and SuperGLUE \citep{wang:19:superglue} have been proposed to facilitate fast and holistic evaluation of models' understanding ability.
They mainly focus on sentence-level tasks, such as natural language inference, while our \benchmarkname benchmark focuses on token/phrase-level analysis tasks with diverse coverage of different linguistic structures.
New tasks and datasets can be conveniently added to our benchmark as long as they are in the BRAT standoff format, which is one of the most commonly used data format in the NLP community, e.g., it has been used in the BioNLP shared tasks \citep{kim:2009:bionlp} and the Universal Dependency project \citep{mcdonald-etal-2013-universal}.

\section{Conclusion}
We provide the simple insight that a large number of natural language analysis tasks can be represented in a single format consisting of spans and relations between spans.
As a result, these tasks can be solved in a single modeling framework that first extracts spans and predicts their labels, then predicts relations between spans.
We attempted 10 tasks with this \modelname model and show that this generic task-independent model can achieve competitive performance as state-of-the-art methods tailored for each tasks.
We merge 8 datasets into our \benchmarkname benchmark for evaluating future models for natural language analysis.
Future directions include (1) devising hierarchical span representations that can handle spans of different length and diverse content more effectively and efficiently; (2) robust multitask learning or meta-learning algorithms that can reconcile very different tasks.

\section*{Acknowledgments}
This work was supported by gifts from Bosch Research. We would like to thank Hiroaki Hayashi, Bohan Li, Pengcheng Yin, Hao Zhu, Paul Michel, and Antonios Anastasopoulos for their insightful comments and suggestions.

\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\newpage
\appendix

\section{Detailed Explanations of 10 Tasks}\label{ap:tasks}

\begin{itemize}[leftmargin=10pt]
\item \textbf{Span-oriented Tasks (\subref{tab:demo_span})}
\begin{itemize}[leftmargin=8pt]
\item \textbf{Named Entity Recognition} \citep{sang:03:conll2003}
NER is traditionally considered as a sequence labeling task. We model named entities as spans over one or more tokens.
\item \textbf{Constituency Parsing} \citep{collins-1997-three} Constituency parsing aims to produce a syntactic parse tree for each sentence. Each node in the tree is an individual span associated with a constituent label, and spans are nested.
\item \textbf{Part-of-speech Tagging} \citep{ratnaparkhi-1996-maximum,toutanova-etal-2003-feature} POS tagging is another sequence labeling task, where every single token is an individual span with a POS tag.
\item \textbf{Aspect-based Sentiment Analysis} \citep{pontiki:14:semeval2014} ABSA is a task that consists of identifying certain spans as aspect terms and predicting their associated sentiments.
\end{itemize}
\item \textbf{Relation-oriented Tasks (\subref{tab:demo_rel})}
\begin{itemize}[leftmargin=8pt]
\item \textbf{Relation Extraction} \citep{hendrickx:10:semeval2010} RE concerns the relation between two entities.
\item \textbf{Coreference} \citep{pradhan:12:conll2012} Coreference resolution is to link named, nominal, and pronominal mentions that refer to the same concept, within or beyond a single sentence. 
\item \textbf{Semantic Role Labeling} \citep{gildea:2002:srl} SRL aims to identify arguments of a predicate (verb or noun) and classify them with semantic roles in relation to the predicate.
\item \textbf{Open Information Extraction} \citep{banko:07:openie,niklaus-etal-2018-survey} In contrast to the fixed relation types in RE, OpenIE aims to extract open-domain predicates and their arguments (usually subjects and objects) from a sentence.
\item \textbf{Dependency Parsing} \citep{kubler2009dependency} Spans are single-word tokens and a relation links a word to its syntactic parent with the corresponding dependency type. 
\item \textbf{Opinion Role Labeling} \citep{yang:13:fgoe} 
ORL detects spans that are opinion expressions, as well as holders and targets related to these opinions.
\end{itemize}
\end{itemize}

\section{Results of BERT Large Model}\label{ap:bertlarge}

\autoref{tab:stl} shows the performance of single-task learning with different token representations.
BERT achieves the best performance on most of the tasks.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccc|ccccc}
\toprule
\textbf{Category} & \textbf{Task} & \textbf{Metric} & \textbf{Dataset} & \textbf{GloVe} & \textbf{ELMo} & \textbf{BERT} & \textbf{SpanBERT} & \textbf{BERT} \\
\midrule
\multirow{6}{*}{IE} & \multirow{2}{*}{NER} & \multirow{2}{*}{F} & CoNLL03 & 88.4 & 91.9 & 91.0 & 91.3 & 90.9 \\
 & & & WLP & 77.6 & 79.2 & 78.1  & 77.9 & 78.3 \\
\cmidrule{2-9}
 & \multirow{2}{*}{RE} & \multirow{2}{*}{F} & SemEval10 & 50.7 & 61.8 & 61.7 & 62.1 & 64.7 \\
 & & & WLP & 64.9 & 65.5 & 64.7 & 64.1 & 65.1 \\
\cmidrule{2-9}
 & \multirow{1}{*}{Coref} & Avg F & OntoNotes & 56.3 & 62.2 & 66.3 & 70.0 & - \\
\cmidrule{2-9}
 & \multirow{1}{*}{OpenIE} & F & OIE2016 & 28.3 & 35.2 & 36.7 & 36.5 & 36.5 \\
\midrule
\multicolumn{2}{c}{SRL} & F & OntoNotes & 78.0 & 82.4 & 83.3 & 83.1 & 84.4 \\
\midrule
\multirow{4}{*}{Parsing} & \multirow{2}{*}{Dep.} & \multirow{2}{*}{LAS} & PTB & 92.9 & 94.7 & 94.9 & 95.1 & 95.3 \\
 & & & OntoNotes & 90.4 & 92.3 & 94.1 & 94.2 & 94.5 \\
\cmidrule{2-9}
 & \multirow{2}{*}{Consti.} & \multirow{2}{*}{Evalb F} & PTB & 93.4 & 95.3 & 95.5 & 95.8 & 95.8 \\
 & & & OntoNotes & 91.0 & 93.2 & 93.6 & 94.3 & 93.9 \\
\midrule
\multirow{2}{*}{Sentiment} & \multirow{1}{*}{ABSA} & F & SemEval14 & 63.5 & 69.2 & 70.8 & 70.0 & 73.8 \\
\cmidrule{2-9}
 & \multirow{1}{*}{ORL} & F & MPQA 3.0 & 38.2 & 42.9 & 44.5 & 45.2 & 47.1 \\
\midrule
\multicolumn{2}{c}{\multirow{2}{*}{POS}} & \multirow{2}{*}{Accuracy} & PTB & 96.8 & 97.7 & 97.6 & 97.6 & 97.4 \\
 & & & OntoNotes & 97.0 & 98.2 & 97.7 & 98.3 & 97.9 \\
\bottomrule
\end{tabular}
}
\caption{Single-task learning performance of the \modelname model with different token representations. BERT requires a large amount of memory so we cannot feed the entire document to the model in coreference resolution.
}
\label{tab:stl}
\end{table*}

\begin{table*}[t]
\small
\centering
\begin{tabular}{ccccccccccc}
\toprule
 & \multicolumn{4}{c}{\textbf{Information Extraction}} & \multirow{2}{*}{\textbf{POS}} & \multicolumn{2}{c}{\textbf{Parsing}} & \multirow{2}{*}{\textbf{SRL}} & \multicolumn{2}{c}{\textbf{Sentiment}} \\
\cline{2-5} 
\cline{7-8}
\cline{10-11}
 & NER & RE & Coref. & OpenIE  & & Dep. & Consti. & & ABSA & ORL\\
\toprule
max span length  & 10 & 5 & 10 & 30 & 1 & 1 & - & 30 & 10 & 30 \\
pruning ratio  & - & 5 & 0.4 & 0.8 & - & 1.0 & - & 1.0 & - & 0.3 \\
\bottomrule
\end{tabular}
\caption{Task-specific hyperparameters. Span-oriented tasks do not need pruning ratio.}
\label{tab:hyperparam}
\end{table*}

\section{Task-specific Hyperparameters}\label{ap:hyper}

As shown in \autoref{tab:hyperparam}, a larger maximum span length is used for tasks with longer spans (e.g., OpenIE), and a larger pruning ratio is used for tasks with more spans (e.g., SRL).
Constituency parsing does not have span length limit because spans can be as long as the entire sentence.
Since relation extraction aims to extract exactly two entities and their relation from a sentence, we keep pruning ratio fixed (top 5 spans in this case) regardless of the length of the sentence.

\end{document}
