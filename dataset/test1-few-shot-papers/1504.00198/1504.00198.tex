\subsection{Proof of Lemma \ref{lem:ExpRew_is_wp} (i)}
\label{proofof:ExpRew_is_wp}

For proving Lemma \ref{lem:ExpRew_is_wp} (i) we rely on the fact that allowing a bounded while--loop to be executed for an increasing number of times approximates the behavior of an unbounded while--loop.
We first define bounded while--loops formally:

\begin{definition}[Bounded \texttt{while}--Loops]
Let . Then we define:

\end{definition}

We can now establish that by taking the supremum on the bound  we obtain the full behavior of the unbounded while--loop:

\begin{lemma}
\label{lem:whilekwhile}
Let  be a predicate, , and .
Then it holds that

\end{lemma}

\begin{proof}
For any predicate , any program , and any expectation  let

We first show by induction on  that

For the induction base we have .
In that case we have

As the induction hypothesis assume now that 

holds for some arbitrary but fixed .
Then for the induction step we have

We have by now established that

holds for every . Ergo, we can also establish that
\belowdisplayskip=-1\baselineskip

\end{proof}

With Lemma \ref{lem:whilekwhile} in mind, we can now restate and prove Lemma \ref{lem:ExpRew_is_wp} (i):

\begingroup
\def\thelemma{\ref{lem:ExpRew_is_wp} (i)}
\begin{lemma}
For , , and :

\end{lemma}
\addtocounter{lemma}{-1}
\endgroup



\begin{proof}

The proof goes by induction over all \cfpGCL programs. 
For the induction base we have:

\textbf{The Effectless Program \Skip.} The RMC for this program is of the following form:\footnote{If transitions have probability , we omit this in our figures. Moreover, all states---with the exception of ---are left out if they are not reachable from the initial state.}
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.5cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.6,-0.85) rectangle (6.5,.25);
   \node [state, initial, initial text=,label={[yshift=0.76cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.5cm, gray] 270:}] (exit) [right of=init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right of=exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In the above RMC we have  with .
Then we have for the expected reward:


\textbf{The Faulty Program \Abort.} The RMC for this program is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.75cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.5,-0.75) rectangle (5.15,.39);
   \node [state, initial, initial text=,label={[yshift=0.9cm, gray] 270:}] (init) {};  
      \node [state,label={[yshift=0.55cm, gray] 270:}] (sink) [right of=init] {};

  \path []
       (init) edge [loop right] (init)
       (sink) edge [loop right] (sink);
       ;
\end{tikzpicture}
 \end{center}
In this RMC we have .
Then we have for the expected reward:


\textbf{The Assignment .} The RMC for this program is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.5cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.6,-0.85) rectangle (6.4,.25);
   \node [state, initial, initial text=,label={[yshift=0.9cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.89cm, gray] 270:}] (exit) [right of=init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right of=exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In this RMC we have  with .
Then we have for the expected reward:


\textbf{The Observation .} For this program there are two cases: In Case 1 we have , so we have . 
The RMC in this case is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.85,-0.85) rectangle (6.2,.25);
   \node [state, initial, initial text=,label={[yshift=1.2cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.5cm, gray] 270:}] (exit) [right=.8 cm of init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right=.8 of exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In this RMC we have  with .
Then we have for the expected reward:

In Case 2 we have , so we have . 
The RMC in this case is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.9,-0.8) rectangle (6.2,.25);
   \node [state, initial, initial text=,label={[yshift=1.2cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.35cm, gray] 270:}] (undesired) [right=.95 cm of init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right=.8 of exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In this RMC we have  with .
Then for the expected reward we also have:



\textbf{The Concatenation .} For this program the RMC is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.5,-2.5) rectangle (6.8,.2);
   \node [state, initial, initial text=,label={[yshift=.9cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.9cm, gray] 270:}] (termp) [on grid, right=2.5 cm of init] {};
      \node [state,label={[yshift=.7cm, gray] 270:}] (q) [on grid, right=2 cm of termp] {};
      \node [state,label={[yshift=.9cm, gray] 270:}] (termp') [on grid, below=1.8 cm of termp] {};
       \node [state,label={[yshift=.7cm, gray] 270:}] (q') [on grid, right=2 cm of termp'] {};
\node [] (dummy) [below= 0.8 cm of init] {};
   
\node [] (dummy1) [on grid, right =2cm of q] {};
    \node [] (dummy2) [on grid, right =2cm of q'] {};
  \path [] 
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp)
      (init) edge [decorate,decoration={snake, post length=2mm}] (dummy)
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp')
      (termp) edge [] (q)
      (termp') edge [] (q')
      (q) edge [decorate,decoration={snake, post length=2mm}] (dummy1)
      (q') edge [decorate,decoration={snake, post length=2mm}] (dummy2)
;
\end{tikzpicture}
 \end{center}
In this RMC every path in  starts with , eventually reaches , and then immediately after that reaches  which is the initial state of  for which the expected reward is given by .
By this insight we can transform the above RMC into the RMC with equal expected reward below:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.2,-2.7) rectangle (6.1,.25);
   \node [state, initial, initial text=,label={[yshift=.8cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.6cm, gray] 280:}] (termp) [on grid, right=2.5 cm of init] {};
\node [state,label={[yshift=.6cm, gray] 280:}] (termp') [on grid, below=1.8 cm of termp] {};
\node [] (dummy) [below= 1.2 cm of init] {};


\path [] 
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp)
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp')
(init) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture}
 \end{center}But the above RMC is exactly  for which the expected reward is also known by the induction hypothesis. 
So we have



\textbf{The Conditional Choice .} For this program there are two cases: In Case 1 we have , so we have  and . 
The RMC in this case is of the following form:
\begin{center}
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-2.8,-2.2) rectangle (5.25,.2);
   \node [state, initial, initial text=,label={[yshift=1.8cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.6cm, gray] 260:}] (p) [right=1.8 cm of init] {};
\node [] (dummy) [below =.7cm of p] {};
  \path [] 
      (init) edge [] (p)
      (p) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture}
 \end{center}
In this RMC every path in  starts with .
As the state  collects zero reward, the expected reward of the above RMC is equal to the expected reward of the following RMC:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.3,-.6) rectangle (2.2,.2);
      \node [state, initial, initial text=,label={[yshift=.7cm, gray] 270:}] (p) [] {};
\node [] (dummy) [right =.7cm of p] {};
  \path [] 
(p) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture}
 \end{center}
But the above RMC is exactly  for which the expected reward is known by the induction hypothesis. 
So we have

In Case 2 we have , so we have  and . 
The RMC in this case is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-2.8,-2.2) rectangle (5.3,.2);
   \node [state, initial, initial text=,label={[yshift=1.8cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.6cm, gray] 260:}] (q) [right=1.8 cm of init] {};
\node [] (dummy) [below =.7cm of q] {};
  \path [] 
      (init) edge [] (q)
      (q) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture}
 \end{center}
In this RMC every path in  starts with .
As the state  collects zero reward, the expected reward of the above RMC is equal to the expected reward of the following RMC:
\begin{center}
    

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.3,-.65) rectangle (2.2,.2);
      \node [state, initial, initial text=,label={[yshift=.7cm, gray] 270:}] (q) [] {};
\node [] (dummy) [right =.7cm of q] {};
  \path [] 
(q) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture} \end{center}
But the above RMC is exactly  for which the expected reward is known by the induction hypothesis. 
So we also have




\textbf{The Probabilistic Choice }. For this program the RMC is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.9,-2.65) rectangle (5.2,.65);
   \node [state, initial, initial text=,label={[yshift=1.2cm, gray] 270:}] (init) {};  
   
   \node [state,label={[yshift=.7cm, gray] 270:}] (p) [on grid, right=3 cm of init] {};
      \node [state,label={[yshift=.7cm, gray] 270:}] (q) [on grid, below  =2 cm of p] {};
      
\node [] (dummy1) [on grid, right =2cm of p] {};
    \node [] (dummy2) [on grid, right =2cm of q] {};
  \path [] 
      (init) edge [] node [above] {\scriptsize{}} (p)
      (init) edge [] node [above] {\scriptsize{}} (q)
(p) edge [decorate,decoration={snake, post length=2mm}] (dummy1)
      (q) edge [decorate,decoration={snake, post length=2mm}] (dummy2)
;
\end{tikzpicture}
 \end{center}In this RMC every path in  starts with  and immediately after that reaches  with probability  or  with probability .
 is the initial state of  and  is the initial state of .
By this insight we can transform the above RMC into the RMC with equal expected reward below:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.9,-2.65) rectangle (6.45,.6);
   \node [state, initial, initial text=,label={[yshift=1.1cm, gray] 270:}] (init) {};  
   
   \node [state,label={[yshift=.8cm, gray] 280:}] (p) [on grid, right=3 cm of init] {};
      \node [state,label={[yshift=.8cm, gray] 280:}] (q) [on grid, below  =2 cm of p] {};
      
\path [] 
      (init) edge [] node [above] {\scriptsize{}} (p)
      (init) edge [] node [above] {\scriptsize{}} (q)
;
\end{tikzpicture}
 \end{center}The expected reward of the above RMC is given by , so in total we have for the expected reward:


\textbf{The Loop .} By Lemma \ref{lem:whilekwhile} we have

and as  is a purely syntactical construct (made up from \Abort, \Skip, conditional choice, and ) we can (using what we have already established on \Abort, \Skip, conditional choice, and using the induction hypothesis on ) also establish that

It is now left to show that

While the above is intuitively evident, it is a tedious and technically involved task to prove it. 
Herefore we just provide an intuition thereof:
For showing , we know that every path in the RMDP  either terminates properly or is prematurely aborted (yielding 0 reward) due to the fact that the bound of less than  loop iterations was reached. The RMDP  for the unbounded while--loop does not prematurely abort executions, so left--hand--side is upper bounded by the right--hand--side of the equation. 
For showing , we know that a path that collects positive reward is necessarily finite. Therefore there exists some  such that  includes this path. Taking the supremum over  we eventually include every path  in  that collects positive reward.
\end{proof}




\subsection{Proof of Lemma \ref{lem:ExpRew_is_wp} (ii)}

\label{proofof:LExpRew_is_wlp}

\begingroup
\def\thelemma{\ref{lem:ExpRew_is_wp} (ii)}
\begin{lemma}
For , , and :

\end{lemma}
\addtocounter{lemma}{-1}
\endgroup

\begin{proof}
The proof goes by induction over all \cfpGCL programs. 
For the induction base we have:
\textbf{The Effectless Program \Skip.} The RMC for this program is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.5cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.6,-0.85) rectangle (6.5,.25);
   \node [state, initial, initial text=,label={[yshift=0.76cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.5cm, gray] 270:}] (exit) [right of=init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right of=exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In this RMC we have  with .
Then we have for the liberal expected reward:


\textbf{The Faulty Program \Abort.} The RMC for this program is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.75cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.5,-0.75) rectangle (5.15,.39);
   \node [state, initial, initial text=,label={[yshift=0.9cm, gray] 270:}] (init) {};  
      \node [state,label={[yshift=0.55cm, gray] 270:}] (sink) [right of=init] {};

  \path []
       (init) edge [loop right] (init)
       (sink) edge [loop right] (sink);
       ;
\end{tikzpicture}
 \end{center}
In this RMC we have .
Then we have for the liberal expected reward:


\textbf{The Assignment .} The RMC for this program is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.5cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.6,-0.85) rectangle (6.4,.25);
   \node [state, initial, initial text=,label={[yshift=0.9cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.89cm, gray] 270:}] (exit) [right of=init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right of=exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In this RMC we have  with .
Then we have for the liberal expected reward:


\textbf{The Observation .} For this program there are two cases: In Case 1 we have , so we have . 
The RMC in this case is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.85,-0.85) rectangle (6.2,.25);
   \node [state, initial, initial text=,label={[yshift=1.2cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.5cm, gray] 270:}] (exit) [right=.8 cm of init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right=.8 of exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In this RMC we have  with .
Then we have for the liberal expected reward:

In Case 2 we have , so we have . 
The RMC in this case is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.9,-0.8) rectangle (6.2,.25);
   \node [state, initial, initial text=,label={[yshift=1.2cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=0.35cm, gray] 270:}] (undesired) [right=.95 cm of init] {};
   \node [state,label={[yshift=0.5cm, gray] 270:}] (sink) [right=.8 of exit] {};

  \path [] 
      (init) edge [] (exit)
      (exit) edge [] (sink)
      (sink) edge [loop right] (sink)
  ;
\end{tikzpicture}
 \end{center}
In this RMC we have  with .
Then we have for the liberal expected reward:



\textbf{The Concatenation .} 
For this program the RMC is of the following form:
\begin{center}
 \scalebox{.84}{\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm, inner sep=.5mm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.3,-2.5) rectangle (8.6,2.7);
   \node [state, initial, initial text=,label={[yshift=.9cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.9cm, gray] 270:}] (termp) [on grid, right=2.5 cm of init] {};
      \node [state,label={[yshift=.7cm, gray] 270:}] (q) [on grid, right=2 cm of termp] {};
      \node [state,label={[yshift=.9cm, gray] 270:}] (termp') [on grid, below=1.8 cm of termp] {};
       \node [state,label={[yshift=.7cm, gray] 270:}] (q') [on grid, right=2 cm of termp'] {};
       \node [state] (diverge1) [above=.7cm of termp] {\ldots};
       \node [state] (diverge2) [above right=1cm of dummy1] {\ldots};
   
\node [] (dummy1) [on grid, right =2cm of q] {};
    \node [] (dummy2) [on grid, right =2cm of q'] {};
  \path [] 
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp)
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp')
      (termp) edge [] (q)
      (termp') edge [] (q')
      (q) edge [decorate,decoration={snake, post length=2mm}] (dummy1)
      (q') edge [decorate,decoration={snake, post length=2mm}] (dummy2)
      (init) edge [decorate,decoration={snake, post length=2mm}] (diverge1)
      (diverge1) edge [decorate,decoration={snake, post length=2mm}, loop right] (diverge1)
      (q) edge [decorate,decoration={snake, post length=2mm}] (diverge2)
      (diverge2) edge [decorate,decoration={snake, post length=2mm}, loop right] (diverge2)
;
\end{tikzpicture}
 }
\end{center}
In this RMC every path in  starts with , eventually reaches , and then immediately after that reaches  which is the initial state of . 
Every diverging path either diverges because the program  diverges or because the program  diverges.
If we attempt to make the RMC smaller (while preserving the liberal expected reward) by cutting it off at states of the form , we have to assign to them the liberal expected reward  in order to not loose the non--termination probability caused by .
By this insight we can now transform the above RMC into the RMC with equal liberal expected reward below:
\begin{center}
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.15,-2.7) rectangle (6.3,1.95);
   \node [state, initial, initial text=,label={[yshift=.4cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.6cm, gray] 280:}] (termp) [on grid, right=2.5 cm of init] {};
\node [state,label={[yshift=.6cm, gray] 280:}] (termp') [on grid, below=1.8 cm of termp] {};
\node [state] (diverge1) [above=0cm of termp] {\ldots};
   
\path [] 
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp)
      (init) edge [decorate,decoration={snake, post length=2mm}] (termp')
(diverge1) edge [decorate,decoration={snake, post length=2mm}, loop right] (diverge1)
      (init) edge [decorate,decoration={snake, post length=2mm}] (diverge1)
  ;
\end{tikzpicture}
 \end{center}
But the above RMC is exactly  for which the liberal expected reward is known by the induction hypothesis. 
So we have for the liberal expected reward:



\textbf{The Conditional Choice .} For this program there are two cases: In Case 1 we have , so we have  and . 
The RMC in this case is of the following form:
\begin{center}
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-2.8,-2.2) rectangle (5.25,.2);
   \node [state, initial, initial text=,label={[yshift=1.8cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.6cm, gray] 260:}] (p) [right=1.8 cm of init] {};
\node [] (dummy) [below =.7cm of p] {};
  \path [] 
      (init) edge [] (p)
      (p) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture}
 \end{center}
As the state  collects zero reward, the expected reward of the above RMC is equal to the expected reward of the following RMC:
\begin{center}
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.3,-.6) rectangle (2.2,.2);
      \node [state, initial, initial text=,label={[yshift=.7cm, gray] 270:}] (p) [] {};
\node [] (dummy) [right =.7cm of p] {};
  \path [] 
(p) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture}
 \end{center}
But the above RMC is exactly  for which the expected reward is known by Lemma . 
A similar argument can be applied to the probability of not eventually reaching .
So we have for the liberal expected reward:

In Case 2 we have , so we have  and . 
The RMC in this case is of the following form:
\begin{center}
 \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-2.8,-2.2) rectangle (5.3,.2);
   \node [state, initial, initial text=,label={[yshift=1.8cm, gray] 270:}] (init) {};  
   \node [state,label={[yshift=.6cm, gray] 260:}] (q) [right=1.8 cm of init] {};
\node [] (dummy) [below =.7cm of q] {};
  \path [] 
      (init) edge [] (q)
      (q) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture}
 \end{center}
In this RMC every path in  starts with .
As the state  collects zero reward, the expected reward of the above RMC is equal to the expected reward of the following RMC:
\begin{center}
 

\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.3,-.65) rectangle (2.2,.2);
      \node [state, initial, initial text=,label={[yshift=.7cm, gray] 270:}] (q) [] {};
\node [] (dummy) [right =.7cm of q] {};
  \path [] 
(q) edge [decorate,decoration={snake, post length=2mm}] (dummy)
;
\end{tikzpicture} \end{center}
But the above RMC is exactly  for which the expected reward is known by the induction hypothesis. 
A similar argument can be applied to the probability of not eventually reaching .
So we also have for the liberal expected reward:




\textbf{The Probabilistic Choice .} For this program the RMC is of the following form:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.9,-2.65) rectangle (5.2,.65);
   \node [state, initial, initial text=,label={[yshift=1.2cm, gray] 270:}] (init) {};  
   
   \node [state,label={[yshift=.7cm, gray] 270:}] (p) [on grid, right=3 cm of init] {};
      \node [state,label={[yshift=.7cm, gray] 270:}] (q) [on grid, below  =2 cm of p] {};
      
\node [] (dummy1) [on grid, right =2cm of p] {};
    \node [] (dummy2) [on grid, right =2cm of q] {};
  \path [] 
      (init) edge [] node [above] {\scriptsize{}} (p)
      (init) edge [] node [above] {\scriptsize{}} (q)
(p) edge [decorate,decoration={snake, post length=2mm}] (dummy1)
      (q) edge [decorate,decoration={snake, post length=2mm}] (dummy2)
;
\end{tikzpicture}
 \end{center}In this RMC every path in  starts with  and immediately after that reaches  with probability  or  with probability .
 is the initial state of  and  is the initial state of .
The same holds for all paths that do not eventually reach \sink.
By this insight we can transform the above RMC into the RMC with equal liberal expected reward below:
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=2.7cm,semithick,minimum size=1cm]
\tikzstyle{every state}=[draw=none]
  \draw[white, use as bounding box] (-1.9,-2.65) rectangle (6.45,.6);
   \node [state, initial, initial text=,label={[yshift=1.1cm, gray] 270:}] (init) {};  
   
   \node [state,label={[yshift=.8cm, gray] 280:}] (p) [on grid, right=3 cm of init] {};
      \node [state,label={[yshift=.8cm, gray] 280:}] (q) [on grid, below  =2 cm of p] {};
      
\path [] 
      (init) edge [] node [above] {\scriptsize{}} (p)
      (init) edge [] node [above] {\scriptsize{}} (q)
;
\end{tikzpicture}
 \end{center}The liberal expected reward of the above RMC is given by , so in total we have for the liberal expected reward:



\textbf{The Loop .}

The argument is dual to the case for the (non--liberal) expected reward.
\end{proof}



\subsection{Proof of Lemma \ref{lem:Pr_not_bad_is_wlp_1}}
\label{proofof:lem:Pr_not_bad_is_wlp_1}

\begingroup
\def\thelemma{\ref{lem:Pr_not_bad_is_wlp_1}}
\begin{lemma}
For , , and :

\end{lemma}
\addtocounter{lemma}{-1}
\endgroup

\begin{proof}
First, observe that paths on reaching \exit or \bad imme{\-}di{\-}ate{\-}ly move to the state \sink. 
Moreover, all paths that never visit  either (a) visit a terminal \exit--state (which are the only states that can possibly collect positive reward) or (b) diverge and never reach \sink and therefore neither reach \exit nor \bad. 
Furthermore the set of ``(a)--paths" and the set of ``(b)--paths" are disjoint.
Thus:
-1.5\baselineskip]

\CExpRew{\Rdtmc{\sigma}{f}{P}}{\lozenge \sinklabel}{\neg\lozenge\bad} ~&=~ \qcwp[P](f)(\sigma) \\
\CLExpRew{\Rdtmc{\sigma}{g}{P}}{\lozenge \sinklabel}{\neg\lozenge\bad} ~&=~ \qcwlp[P](g) (\sigma)~.

&\CExpRew{\Rdtmc{\sigma}{f}{P}}{\lozenge \sinklabel}{\neg\lozenge\bad}\\
~=~ &\frac{\ExpRew{\Rdtmc{\sigma}{f}{P}}{\lozenge \sinklabel}}{\Prr{\Rdtmc{\sigma}{f}{P}}(\neg\lozenge \bad)}\\
~=~ &\frac{\wp[P](f)}{\wlp[P](\ExpOne)}\tag{Lemmas \ref{lem:ExpRew_is_wp}, \ref{lem:Pr_not_bad_is_wlp_1}}\\
~=~ &\frac{\cwp_1[P](f,\, \ExpOne)}{\cwp_2[P](f,\, \ExpOne)}\tag{Theorem \ref{thm:cwp-decuop}}\\
~=~ &\qcwp[P](f)

\end{proof}
