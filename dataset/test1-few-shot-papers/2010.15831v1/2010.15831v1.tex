\documentclass{article}

\PassOptionsToPackage{numbers,compress}{natbib}






\usepackage[final]{neurips_2020}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{threeparttable}
\usepackage{array,multirow}
\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{comment}

\usepackage{multicol}
\usepackage{multirow}

\usepackage{makecell}

\usepackage{amsmath}

\def\eg{{e.g.}}
\def\ie{{i.e.}}
\def\vs{{vs}}
\def\etal{{et al.}}

\usepackage{todonotes}

\usepackage{xcolor}


\title{RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder}



\author{Cheng Chi\thanks{The work is done when Cheng Chi is an intern at Microsoft Research Asia.}\\
  Institute of Automation, CAS\\
  \texttt{chicheng15@mails.ucas.ac.cn}  \\
\And
Fangyun Wei \\
Microsoft Research Asia \\
\texttt{fawe@microsoft.com} \\
\And
Han Hu \\
Microsoft Research Asia \\
\texttt{hanhu@microsoft.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Existing object detection frameworks are usually built on a single format of object/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and Faster R-CNN, center points in FCOS and RepPoints, and corner points in CornerNet. While these different representations usually drive the frameworks to perform well in different aspects, e.g., better classification or finer localization, it is in general difficult to combine these representations in a single framework to make good use of each strength, due to the heterogeneous or non-grid feature extraction by different representations. This paper presents an attention-based decoder module similar as that in Transformer~\cite{vaswani2017attention} to bridge other representations into a typical object detector built on a single representation format, in an end-to-end fashion. The other representations act as a set of \emph{key} instances to strengthen the main \emph{query} representation features in the vanilla detectors. Novel techniques are proposed towards efficient computation of the decoder module, including a \emph{key sampling} approach and a \emph{shared location embedding} approach. The proposed module is named \emph{bridging visual representations} (BVR). It can perform in-place and we demonstrate its broad effectiveness in bridging other representations into prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS, where about  AP improvements are achieved. In particular, we improve a state-of-the-art framework with a strong backbone by about  AP, reaching  AP on COCO test-dev. The resulting network is named RelationNet++. The code will be available at \url{https://github.com/microsoft/RelationNet2}.
\end{abstract}

\vspace{-.5em}

\section{Introduction}

Object detection is a vital problem in computer vision that many visual applications build on. While there have been numerous approaches towards solving this problem, they usually leverage a single visual representation format. For example, most object detection frameworks~\cite{girshick2014rich,girshick2015fast,ren2015faster,RetinaNet} utilize the rectangle box to represent object hypotheses in all intermediate stages. Recently, there have also been some frameworks adopting points to represent an object hypothesis, \eg, center point in CenterNet~\cite{CenterNet} and FCOS~\cite{FCOS}, point set in RepPoints~\cite{RepPoints,yang2019dense,chen2020reppointsv2} and PSN~\cite{wei2020point}. 
In contrast to representing whole objects, some keypoint-based methods, \eg, CornerNet~\cite{CornerNet}, leverage part representations of corner points to compose an object. In general, different representation methods usually steer the detectors to perform well in different aspects. For example, the bounding box representation is better aligned with annotation formats for object detection. The center representation avoids the need for an anchoring design and is usually friendly to small objects. The corner representation is usually more accurate for finer localization.

It is natural to raise a question: \emph{could we combine these representations into a single framework to make good use of each strength?} Noticing that different representations and their feature extractions are usually heterogeneous, combination is difficult. To address this issue, we present an \emph{attention based decoder module} similar as that in Transformer~\cite{vaswani2017attention}, which can effectively model dependency between heterogeneous features. The main representations in an object detector are set as the \emph{query} input, and other visual representations act as the auxiliary \emph{key}s to enhance the \emph{query} features by certain interactions, where both appearance and geometry relationships are considered. 

In general, all feature map points can act as corner/center \emph{key} instances, which are usually too many for practical attention computation. In addition, the pairwise geometry term is computation and memory consuming. To address these issues, two \emph{novel} techniques are proposed, including a \emph{key sampling} approach and a \emph{shared location embedding} approach for efficient computation of the geometry term. The proposed module is named \emph{bridging visual representations} (BVR).

Figure~\ref{fig:representation-1} illustrates the application of this module to bridge center and corner representations into an anchor-based object detector. The center and corner representations act as \emph{key} instances to enhance the anchor box features, and the enhanced features are then used for category classification and bounding box regression to produce the detection results. The module can work in-place. Compared with the original object detector, the main change is that the input features for classification and regression are replaced by the enhanced features, and thus the strengthened detector largely maintains its convenience in use.

The proposed BVR module is general. It is applied to various prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS. Extensive experiments on the COCO dataset~\cite{MSCOCO} show that the BVR module substantially improves these various detectors by  AP. In particular, we improve a strong ATSS detector by about  AP with small overhead, reaching  AP on COCO test-dev. The resulting network is named RelationNet++, which strengthens the relation modeling in \cite{hu2018relation} from bbox-to-bbox to across heterogeneous object/part representations. 

The main contributions of this work are summarized as:
\begin{itemize}
\setlength{\itemsep}{0pt}
\item A general module, named BVR, to bridge various heterogeneous visual representations and combine the strengths of each. The proposed module can be applied in-place and does not break the overall inference process by the main representations.
\item Novel techniques to make the proposed bridging module efficient, including a \emph{key sampling} approach and a \emph{shared location embedding} approach.
\item Broad effectiveness of the proposed module for four prevalent object detectors: RetinaNet, Faster R-CNN, FCOS and ATSS.
\end{itemize}

\begin{figure*}[t]
\setlength{\abovecaptionskip}{0.1cm}
\begin{subfigure}{.24\textwidth}
  \centering
\includegraphics[width=1.0\linewidth]{figures/Bridge.pdf}  
  \caption{Bridge representations.}
  \label{fig:representation-1}
\end{subfigure}
\begin{subfigure}{.75\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/Representation.pdf}
  \caption{Typical object/part representations.}
  \label{fig:representation-2}
\end{subfigure}
\caption{(a) An illustration of bridging various representations, specifically leveraging corner/center representations to enhance the anchor box features. (b) Object/part representations used in object detection (geometric description and feature extraction). The red dashed box denotes ground-truth.}
\label{fig:representation}
\vspace{-1em}
\end{figure*}
 
\vspace{-.5em}

\section{A Representation View for Object Detection}
\subsection{Object / Part Representations}

Object detection aims to find all objects in a scene with their location described by rectangle bounding boxes. To discriminate object bounding boxes from background and to categorize objects, intermediate geometric object/part candidates with associated features are required. We refer to the joint \emph{geometric description} and \emph{feature extraction} as the \emph{representation}, where typical representations used in object detection are illustrated in Figure~\ref{fig:representation-2} and summarized below.

{\noindent \textbf{Object bounding box representation}} \hspace{3pt} Object detection uses bounding boxes as the final output. Probably because of this, bounding box is now the most prevalent representation. Geometrically, a bounding box can be described by a -d vector, either as center-size  or as opposing corners . Besides the final output, this representation is also commonly used as initial and intermediate object representations, such as anchors~\cite{ren2015faster,SSD,YOLOv2,YOLOv3,RetinaNet} and proposals~\cite{girshick2014rich,dai2016r,FPN,Mask-rcnn}. For bounding box representations, features are usually extracted by pooling operators within the bounding box area on an image feature map. Common pooling operators include RoIPool~\cite{girshick2015fast}, RoIAlign~\cite{Mask-rcnn}, and Deformable RoIPool~\cite{DCN,DCNv2}. There are also simplified feature extraction methods, e.g., the box center features are usually employed in the anchor box representation~\cite{ren2015faster,RetinaNet}.

{\noindent \textbf{Object center representation}}\hspace{3pt} The -d vector space of a bounding box representation is at a scale of  for an image with resolution , which is too large to fully process. To reduce the representation space, some recent frameworks~\cite{FCOS,RepPoints,CenterNet,DBLP:journals/corr/abs-1904-03797,GuidedAnchoring} use the center point as a simplified representation. Geometrically, a center point is described by a 2-d vector , in which the hypothesis space is of the scale , which is much more tractable. For a center point representation, the image feature on the center point is usually employed as the object feature.

{\noindent \textbf{Corner representation}}\hspace{3pt} A bounding box can be determined by two points, \eg, a top-left corner and a bottom-right corner. Some approaches~\cite{Denet,CornerNet,DBLP:journals/corr/abs-1904-08900,duan2019centernet,DBLP:conf/cvpr/LuLYLY19,zhou2019bottom,samet2020houghnet} first detect these individual points and then compose bounding boxes from them. We refer to these representation methods as \emph{corner representation}. The image feature at the corner location can be employed as the part feature.

{\noindent \textbf{Summary and comparison}}\hspace{3pt} Different representation approaches usually have strengths in different aspects. For example, object based representations (bounding box and center) are better in category classification while worse in object localization than part based representations (corners). Object based representations are also more friendly for end-to-end learning because they do not require a post-processing step to compose objects from corners as in part-based representation methods. Comparing different object-based representations, while the bounding box representation enables more sophisticated feature extraction and multiple-stage processing, the center representation is attractive due to the simplified system design.

\begin{figure*}[t]
\centering
\setlength{\abovecaptionskip}{0.cm}
\includegraphics[width=1.0\linewidth]{figures/Evolution.pdf}
\caption{Representation flows for several typical detection frameworks.}
\label{fig:evolution}
\vspace{-1em}
\end{figure*}
\vspace{-.5em}
\subsection{Object Detection Frameworks in a Representation View}
\label{framework}

Object detection methods can be seen as evolving intermediate object/part representations until the final bounding box outputs. The representation flows largely shape different object detectors. Several major categorization of object detectors are based on such representation flow, such as \emph{top-down} (object-based representation) vs \emph{bottom-up} (part-based representation), \emph{anchor-based} (bounding box based) vs \emph{anchor-free} (center point based), and \emph{single-stage} (one-time representation flow) vs \emph{multiple-stage} (multiple-time representation flow). Figure~\ref{fig:evolution} shows the representation flows of several typical object detection frameworks, as detailed below.

{\noindent \textbf{Faster R-CNN}}~\cite{ren2015faster} employs bounding boxes as its intermediate object representations in all stages. At the beginning, multiple anchor boxes at each feature map position are hypothesized to coarsely cover the 4-d bounding box space in an image, \ie,  anchor boxes with different aspect ratios. The image feature vector at the center point is extracted to represent each anchor box, which is then used for foreground/background classification and localization refinement. After anchor box selection and localization refinement, the object representation is evolved to a set of proposal boxes, where the object features are usually extracted by an RoIAlign operator within each box area. The final bounding box outputs are obtained by localization refinement, through a small network on the proposal features.

{\noindent \textbf{RetinaNet}}~\cite{RetinaNet} is a one-stage object detector, which also employs bounding boxes as its intermediate representation. Due to its one-stage nature, it usually requires denser anchor hypotheses, \ie,  anchor boxes at each feature map position. The final bounding box outputs are also obtained by applying a localization refinement head network.

{\noindent \textbf{FCOS}}~\cite{FCOS} is also a one-stage object detector but uses object center points as its intermediate object representation. It directly regresses the four sides from the center points to form the final bounding box outputs. There are concurrent works, such as~\cite{CenterNet,DBLP:journals/corr/abs-1904-03797,RepPoints}. Although center points can be seen as a degenerated geometric representation from bounding boxes, these center point based methods show competitive or even better performance on benchmarks. 

{\noindent \textbf{CornerNet}}~\cite{CornerNet} is built on the intermediate part representation of corners, in contrast to the above frameworks where object representations are employed. The predicted corners (top-left and bottom-right) are grouped according to their embedding similarity, to compose the final bounding box outputs. The detectors based on corner representation usually reveal better object localization than those based on an object-level representation.
 
\vspace{-.5em}

\section{Bridging Visual Representations}

\vspace*{-0.5em}

For the typical frameworks in Section~\ref{framework}, mainly one kind of representation approach is employed. While they have strengths in some aspects, they may also fall short in other ways. However, it is in general difficult to combine them in a single framework, due to the heterogeneous or non-grid feature extraction by different representations. In this section, we will first present a general method to bridge different representations. Then we demonstrate its applications to various frameworks, including RetinaNet~\cite{RetinaNet}, Faster R-CNN~\cite{ren2015faster}, FCOS~\cite{FCOS} and ATSS~\cite{zhang2019bridging}.

\vspace*{-0.5em}

\subsection{A General Attention Module to Bridge Visual Representations}
\label{section:bvr}

\vspace*{-0.5em}

Without loss of generality, for an object detector, the representation it leverages is referred to as the \emph{master} representation, and the general module aims to bridge other representations to enhance this \emph{master} representation. Such other representations are referred to as \emph{auxiliary} ones.

Inspired by the success of the decoder module for neural machine translation where an attention block is employed to bridge information between different languages, \eg, Transformer~\cite{vaswani2017attention}, we adapt this mechanism to bridge different visual representations. Specifically, the \emph{master} representation acts as the \emph{query} input, and the \emph{auxiliary} representations act as the \emph{key} input. The attention module outputs strengthened features for the \emph{master} representation (\emph{queries}), which have bridged the information from \emph{auxiliary} representations (\emph{keys}). We use a general attention formulation as:

where  are the input feature, output feature, and geometric vector for a \emph{query} instance ;  are the input feature and geometric vector for a \emph{key} instance ;  is a linear \emph{value} transformation function;  is a similarity function between  and , instantiated as~\cite{hu2018relation}:

where  denotes the appearance similarity computed by a scaled dot product between \emph{query} and \emph{key} features~\cite{vaswani2017attention,hu2018relation}, and  denotes a geometric term computed by applying a small network on the relative locations between  and , \ie, cosine/sine location embedding~\cite{vaswani2017attention,hu2018relation} plus a -layer MLP. In the case of different dimensionality between the \emph{query} geometric vector and \emph{key} geometric vector (-d bounding box vs. -d point), we first extract a -d point from the bounding box, \ie, center or corner, such that the two representations are homogeneous in geometry description for subtraction operations. The same as in~\cite{vaswani2017attention,hu2018relation}, multi-head attention is employed, which performs substantially better than using single-head attention. We use an attention head number of  by default.

The above module is named \emph{bridging visual representations} (BVR), which takes \emph{query} and \emph{key} representations of any dimension as input and generates strengthened features for the \emph{query} considering both their appearance and geometric relationships. The module can be easily plugged into prevalent detectors as described in Section~\ref{BVRforRetina} and~\ref{BVRforOther}.

\vspace*{-0.5em}

\subsection{BVR for RetinaNet}
\label{BVRforRetina}
\vspace*{-0.5em}

We take RetinaNet as an example to showcase how we apply the BVR module to an existing object detector. As mentioned in Section~\ref{framework}, RetinaNet adopts anchor bounding boxes as its \emph{master} representation, where  bounding boxes are anchored at each feature map location. Totally, there are  bounding box instances for a feature map of  resolution. BVR takes the  feature map ( is the feature map channel) as \emph{query} input, and generates strengthened \emph{query} features of the same dimension.

We use two kinds of \emph{key} (\emph{auxiliary}) representations to strengthen the \emph{query} (\emph{master}) features. One is the object center, and the other is the corners. As shown in Figure \ref{fig:structure-1}, the center/corner points are predicted by applying a small point head network on the output feature map of the backbone. Then a small set of \emph{key} points are selected from all predictions, and are fed into the attention modules to strengthen the classification and regression feature, respectively. In the following, we provide details of these modules and the crucial designs.


{\noindent \textbf{Auxiliary (key) representation learning}}\hspace{3pt} The point head network consists of two shared  conv layers, followed by two independent sub-networks (a  conv layer and a sigmoid layer) to predict the scores and sub-pixel offsets for center and corner prediction, respectively~\cite{CornerNet}. The score indicates the probability of a center/corner point locating at the feature map bin. The sub-pixel offset  denotes the displacement between its precise location and the top-left (integer coordinate) of each feature bin, which accounts for the resolution loss by down-sampling of feature maps.

In learning, for the object detection frameworks with an FPN structure, we assign all ground-truth center/corner points to all feature levels. We find it performs better than the common practice where objects are assigned to a particular level~\cite{FPN,RetinaNet,FCOS,CornerNet,RepPoints}, probably because it speeds up the learning of center/corner representations due to more positive samples employed in each level. The focal loss~\cite{RetinaNet} and smooth L1 loss are employed for the center/corner score and sub-pixel offset learning, with loss weights of  and , respectively.

\begin{figure*}[t]
\begin{subfigure}{.45\textwidth}
  \centering
\includegraphics[width=0.85\linewidth]{figures/structure.pdf}  
  \caption{Apply BVR to RetinaNet.}
  \label{fig:structure-1}
\end{subfigure}
\begin{subfigure}{.55\textwidth}
  \centering
\includegraphics[width=0.8\linewidth]{figures/attention.pdf}
  \caption{Attention-based feature enhancement.}
  \label{fig:structure-2}
\end{subfigure}
\caption{Applying BVR into an object detector and an illustration of the attention computation.}
\label{fig:structure}
\vspace{-1em}
\end{figure*}

{\noindent \textbf{Key selection}}\hspace{3pt} We use corner points to demonstrate the processing of auxiliary representation selection since the principle is same for center point representation. We treat each feature map position as an object corner candidate. If all candidates are employed in the \emph{key} set, the computation cost of BVR module is unaffordable. In addition, too many background candidates may suppress real corners. To address these issues, we propose a top- ( by default) \emph{key} selection strategy. Concretely, a  MaxPool operator with stride 1 is performed on the corner score map, and the top-\emph{k} corner candidates are selected according to their corner-ness scores. For an FPN backbone, we select the top- \emph{key}s from all pyramidal levels, and the \emph{key} set is shared by all levels. This \emph{shared key} set outperforms that of independent \emph{key} set for different levels, as shown in Table~\ref{tab:sample}.

{\noindent \textbf{Shared relative location embedding}}\hspace{3pt} The computation and memory complexities for direct computation of the geometry term are  and , respectively, where  are the cosine/sine embedding dimension, inner dimension of the MLP network, head number of the multi-head attention module and the number of selected \emph{key} instances, respectively. As shown in Table~\ref{tab:share}, the default setting () is time-consuming and space-consuming.

Noting that the range of relative locations are limited, \ie, , we apply the cosine/sine embedding and the -layer MLP network on a fixed -d relative location map to produce a -channel geometric map, and then compute the geometric terms for a \emph{key}/\emph{query} pair by bilinear sampling on this geometric map. To further reduce the computation, we use a -d relative location map with the unit length  larger than , \eg, , where each location bin indicates a length of  in the original image. In our implementation, we adopt  ( indicates the stride of the pyramid level) and a location map of  resolution, which accounts for a  range on the original image for a pyramid level of stride S. Figure~\ref{fig:structure-2} gives an illustration. The computation and memory complexities are reduced to  and , respectively, which are significantly smaller than direct computation, as shown in Table~\ref{tab:share}.

{\noindent \textbf{Separate BVR modules for classification and regression}}\hspace{3pt} Object center representations can provide rich context for object categorization, while the corner representations can facilitate localization. Therefore, we apply separate BVR modules to enhance classification and regression features respectively, as shown in Figure\ref{fig:structure-1}. Such separate design is beneficial, as demonstrated in Table~\ref{tab:selection}.

\subsection{BVR for Other Frameworks}
\label{BVRforOther}
The BVR module is general, and can be applied to other object detection frameworks.

{\noindent \textbf{ATSS}}~\cite{zhang2019bridging} applies several techniques from anchor-free detectors to improve the anchor-based detectors, e.g. RetinaNet. The BVR used for RetinaNet can be directly applied.

{\noindent \textbf{FCOS}}~\cite{FCOS} is an anchor-free detector which utilizes center point as object representation. Since there is no corner information in this representation, we always use the center point location and the corresponding feature to represent the \emph{query} instance in our BVR module. Other settings are maintained the same as those for RetinaNet.

{\noindent \textbf{Faster R-CNN}}~\cite{ren2015faster} is a two-stage detector which employs bounding boxes as the inter-mediate object representations in all stages. We adopt BVR to enhance the features of bounding box proposals, the diagram is shown in Figure \ref{fig:frcnn}. In each of the proposals, RoIAlign feature is used to predict center and corner representations. Figure \ref{fig:head} shows the network structure of point (center/corner) head, which is similar with mask head in mask R-CNN~\cite{Mask-rcnn}. The selection of \emph{key}s is same with the process in RetinaNet, which is stated in Section \ref{BVRforRetina}. We use the features interpolated from the point head as the key features, center and corner features are also employed to enhance classification and regression, respectively. Since the number of the \emph{querys} is much smaller than that in RetinaNet, we directly compute the geometry term other than using the shared geometric map. 


\begin{figure*}[t]
\begin{subfigure}{.6\textwidth}
  \centering
\includegraphics[width=0.85\linewidth]{figures/structure_frcnn.pdf}  
  \caption{Apply BVR to faster R-CNN.}
  \label{fig:frcnn}
\end{subfigure}
\begin{subfigure}{.35\textwidth}
  \centering
\includegraphics[width=0.8\linewidth]{figures/head.pdf}
  \caption{Point head for center (corner) prediction in faster R-CNN.}
  \label{fig:head}
\end{subfigure}
\caption{Design of applying BVR to faster R-CNN.}
\vspace{-1em}
\end{figure*}

\subsection{Relation to Other Attention Mechanisms in Object Detection}

{\noindent \textbf{Non-Local Networks (NL)~\cite{DBLP:conf/cvpr/0004GGH18} and RelationNet~\cite{hu2018relation}}} are two pioneer works utilizing attention modules to enhance detection performance. However, they are both designed to enhance instances of a single representation format: non-local networks~\cite{DBLP:conf/cvpr/0004GGH18} use self-attention to enhance a pixel feature by fusing in other pixels' features; RelationNet~\cite{hu2018relation} enhance a bounding box feature by fusing in other bounding box features.

In contrast, BVR aims to bridge representations in different forms to combine the strengths of each. In addition to this conceptual difference, there are also new techniques in the modeling aspect. For example, techniques are proposed to enable homogeneous difference/similarity computation between heterogeneous representations, \ie, -d bounding box vs -d corner/center points. Also, there are new techniques proposed to effectively model relationship between different kinds of representations as well as to speed-up computation, such as \emph{key} representation selection, and the shared relative location embedding approach. The proposed BVR is actually complementary to these pioneer works, as shown in Table~\ref{tab:nonlocal} and \ref{tab:pr}.

{\noindent \textbf{Learning Region Features (LRF)~\cite{gu2018learning} and DeTr~\cite{carion2020endtoend}}} use an attention module to compute the features of object proposals~\cite{gu2018learning} or querys~\cite{carion2020endtoend} from image features. BVR shares similar formulation as them, but has a different aim to bridge different forms of object representations. 
\vspace{-.5em}

\section{Experiments}

We first ablate each component of the proposed BVR module using a RetinaNet base detector in Section~\ref{ablation}. Then we show benefits of BVR applied to four representative detectors, including two-stage (i.e., faster R-CNN), one-stage (i.e., RetinaNet and ATSS) and anchor-free (i.e., FCOS) detectors. Finally, we compare our approach with the state-of-the-art methods. 

Our experiments are all implemented on the MMDetection v1.1.0 codebase~\cite{mmdetection}. All experiments are performed on MS COCO dataset\cite{MSCOCO}. A union of  train images and a  subset of val images are used for training. Most ablation experiments are studied on a subset of  unused val images (denoted as {\tt minival}). Unless otherwise stated, all the training and inference details keep the same as the default settings in MMDetection, i.e., initializing the backbone using the ImageNet \cite{DBLP:journals/ijcv/RussakovskyDSKS15} pretrained model, resizing the input images to keep their shorter side being  and their longer side less than or equal to , optimizing the whole network via the SGD algorithm with  momentum,  weight decay, setting the initial learning rate as  with the  decrease at epoch  and . In the large model experiments in Table \ref{tab:verification} and \ref{tab:coco}, we train  epochs and decrease the learning rate at epoch  and . Multi-scale training is also adopted in large model experiments, for each mini-batch, the shorter side is randomly selected from a range of . 

\subsection{Method Analysis using RetinaNet}
\label{ablation}

Our ablation study is built on a RetinaNet detector using ResNet-50, which achieves  AP on COCO {\tt minival} ( settings). Components in the BVR module are ablated using this base detector.

{\noindent \textbf{Key selection}} As shown in Table \ref{tab:sample}, compared with independent keys across feature levels, sharing keys can bring  and  AP gains for  and  keys, respectively. Using  keys achieves the best accuracy, probably because that too few keys cannot sufficiently cover the representative keypoints, while too large number of keys include many low-quality candidates. 


On the whole, the BVR enhanced RetinaNet significantly outperforms the original RetinaNet by  AP, demonstrating the great benefit of bridging other representations.

{\noindent \textbf{Sub-pixel corner/center}} Table \ref{tab:value} shows the benefits of using sub-pixel representations for centers and corners. While sub-pixel representation benefits both classification and regression, it is more critical for the localization task.

\begin{table*}[t]
\centering
\makebox[0pt][c]{\parbox{1.0\textwidth}{\begin{minipage}[b]{0.49\linewidth}\centering
        \caption{Ablation on key selection approaches}
        \small \setlength{\tabcolsep}{7pt}
        \begin{tabular}{cc|ccc}
        \Xhline{1.0pt}
        \#keys & share & AP & AP & AP\\
        \hline
        - & - & 35.6 & 55.5 & 39.0 \\
        \hline
        20 & \xmark & 36.1 & 54.9 & 39.6\\
        50 & \xmark & 37.0 & 55.8 & 40.6\\
        20 & \cmark & 37.7 & 56.5 & 41.4\\
        50 & \cmark & \textbf{38.5} & \textbf{57.0} & \textbf{42.3} \\
        100 & \cmark & 38.3 & 56.9 & 42.0\\
        200 & \cmark & 38.2 & 56.7 & 41.9\\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:sample}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\linewidth}\centering
        \caption{Ablation of sub-pixel corner/centers}
        \renewcommand{\arraystretch}{1.3}
        \small \setlength{\tabcolsep}{3pt}
        \begin{tabular}{cc|cccc}
        \Xhline{1.0pt}
         CLS (ct.) & REG (cn.) & AP & AP & AP & AP  \\
        \hline
         - & - & 35.6 & 55.5 & 39.0 & 9.3\\
        \hline
         integer & integer    & 37.0 & 55.6 & 40.8 & 11.0 \\
         integer & sub-pixel      & 38.0 & 56.1 & 41.7 & 12.5 \\
         sub-pixel & integer      & 37.2 & 56.7 & 41.2 & 10.4 \\
         sub-pixel & sub-pixel        & \textbf{38.5} & \textbf{57.0} & \textbf{42.3} & \textbf{12.6} \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:value}
    \end{minipage}
}}
\end{table*}


\begin{table*}[t]
\centering
\makebox[0pt][c]{\parbox{1.0\textwidth}{\begin{minipage}[b]{0.52\linewidth}\centering
        \caption{Effect of shared relative location embedding}
        \small \setlength{\tabcolsep}{1pt}
        \begin{tabular}{c|cc|ccc}
        \Xhline{1.0pt}
         geometry & memory & FLOPs & AP & AP & AP \\
        \hline
         baseline   & M & 239G & 35.6 & 55.5 & 39.0 \\
         \hline
         appearance only   & M & 264G & 37.4 & 56.7 & 40.4 \\
         \hline
         \multirow{2}{*}{non-shared} & M & 468G  & \multirow{2}{*}{38.3} & \multirow{2}{*}{\textbf{57.2}} & \multirow{2}{*}{41.7} \\
         & (+5690M) & (+204G) & & & \\
         \hline
         \multirow{2}{*}{shared}     & M & 266G & \multirow{2}{*}{\textbf{38.5}} & \multirow{2}{*}{57.0} & \multirow{2}{*}{\textbf{42.3}} \\
         & (+134M) & (+2G) & & & \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:share}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.46\linewidth}\centering
        \caption{Comparison of different unit length and size of the shared location map}
        \small \setlength{\tabcolsep}{1pt}
        \begin{tabular}{cc|ccc}
        \Xhline{1.0pt}
         unit length & size & AP & AP & AP \\
        \hline
          &  & 38.2 & 56.7 & 41.8 \\
          &  & \textbf{38.5} & \textbf{57.0} & \textbf{42.3} \\
          &  & 38.4 & 56.8 & 42.2 \\
          &  & 38.3 & 56.9 & 42.1 \\
          &  & 38.1 & 56.7 & 41.8 \\
\Xhline{1.0pt}
        \end{tabular}
        \label{tab:map}
    \end{minipage}
}}
\end{table*}

{\noindent \textbf{Shared relative location embedding}} As shown in Table \ref{tab:share}, compared with direct computation of position embedding \cite{hu2018relation}, the proposed shared location embedding approach saves  memory cost (+M vs +M) and saves  FLOPs (+G vs +G) in the geometry term computation, while achieves slightly better performance ( AP vs  AP).

Ablation study of the unit length and the size of the shared location map in Table \ref{tab:map} indicates stable performance. We adopt a unit length of  and map size of  by default.

{\noindent \textbf{Separate BVR modules for classification and regression}} Table \ref{tab:selection} ablates the effect of using separate BVR modules for classification and regression, indicating the center representation is a more suitable auxiliary for classification and the corner representation is a more suitable auxiliary for regression. 

\iffalse
       \renewcommand{\arraystretch}{1.3}
        \caption{Comparison of GPU memory cost, training time and performance of using non-shared/shared relative location embedding.}
        \tiny \setlength{\tabcolsep}{1pt}
        \begin{tabular}{c|cccc|ccc}
        \Xhline{1.0pt}
         Embed & Mem & Time & Flops & Inference & AP & AP & AP \\
        \hline
         baseline   & M & s & 239B & 14fps & 35.6 & 55.5 & 39.0 \\
         -          & M & s & 264B & 11fps & 37.4 & 56.7 & 40.4 \\
         non-shared & M & s & 468B & 4fps  & 38.3 & \textbf{57.2} & 41.7 \\
         
         Shared     & M & s & 266B & 10fps & \textbf{38.5} & 57.0 & \textbf{42.3} \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:share}
\fi

{\noindent \textbf{Effect of appearance and geometry terms}} Table \ref{tab:appgeo} ablates the effect of appearance and geometry terms. Using the two terms together outperforms that using the appearance term alone by  AP and outperforms that using the geometry term alone by  AP. In general, the geometry term benefits more at larger IoU criteria, and less at lower IoU criteria.

{\noindent \textbf{Compare with multi-task learning}} Only including an auxiliary point head without using it can boost the RetinaNet baseline by  AP (from  to ). Noting the BVR brings a  AP improvement (from  to ) under the same settings, the major improvements are not due to multi-task learning.


\begin{table*}[t]
\centering
\makebox[0pt][c]{\parbox{1.0\textwidth}{\begin{minipage}[b]{0.52\linewidth}\centering
        \caption{Effect of different representations (`ct.': center, `cn.': corner) for classification and regression}
        \small \setlength{\tabcolsep}{4.5pt}
        \begin{tabular}{cc|cccc}
        \Xhline{1.0pt}
        CLS & REG & AP & AP & AP & AP \\
        \hline
        none & none             & 35.6 & 55.5 & 39.0 & 9.3  \\
        none & ct.              & 36.4 & 54.7 & 38.9 & 10.1  \\
        none & cn.              & 37.5 & 54.6 & 40.3 & 12.2  \\
        ct. & none              & 37.3 & 56.6 & 39.9 & 10.5  \\
        cn. & none              & 36.2 & 55.1 & 38.4 & 9.8  \\
        ct. & cn.               & \textbf{38.5} & \textbf{57.0} & \textbf{42.3} & \textbf{12.6} \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:selection}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.46\linewidth}\centering
        \renewcommand{\arraystretch}{1.4}
        \caption{Ablation of appearance and geometry terms}
        \small \setlength{\tabcolsep}{2pt}
        \begin{tabular}{cc|cccc}
        \Xhline{1.0pt}
         appearance & geometry & AP & AP & AP & AP \\
        \hline
         \xmark & \xmark & 35.6 & 55.5 & 39.0 & 9.3 \\
         \cmark & \xmark & 37.4 & 56.7 & 41.3 & 10.7 \\
         \xmark & \cmark & 37.6 & 55.8 & 41.5 & 12.0  \\
         \cmark & \cmark & \textbf{38.5} & \textbf{57.0} & \textbf{42.3} & \textbf{12.6} \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:appgeo}
    \end{minipage}
}}
\end{table*}

\begin{table*}[t]
\centering
\makebox[0pt][c]{\parbox{1.0\textwidth}{\begin{minipage}[b]{0.49\linewidth}\centering
        \caption{Compatibility with the non-local module (NL)~\cite{DBLP:conf/cvpr/0004GGH18}}
        \small \setlength{\tabcolsep}{6pt}
        \begin{tabular}{l|ccc}
        \Xhline{1.0pt}
         method & AP & AP & AP \\
        \hline
         RetinaNet & 35.6 & 55.5 & 39.0 \\
         RetinaNet + NL & 37.0 & 57.0 & 39.3 \\
         RetinaNet + BVR & 38.5 & 57.0 & 42.3 \\
         RetinaNet + NL + BVR & \textbf{39.4} & \textbf{58.2} & \textbf{42.5} \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:nonlocal}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\linewidth}\centering
        \caption{Compatibility with the object relation module (ORM)~\cite{hu2018relation}. ResNet-50-FPN is used}
        \small \setlength{\tabcolsep}{3pt}
        \begin{tabular}{l|ccc}
        \Xhline{1.0pt}
         method & AP & AP & AP \\
        \hline
         faster R-CNN & 37.4 & 58.1 & 40.4 \\
         faster R-CNN + ORM & 38.4 & 59.0 & 41.3 \\
         faster R-CNN + BVR & 39.3 & 59.5 & 43.1 \\
         faster R-CNN + ORM + BVR & \textbf{40.4} & \textbf{60.6} & \textbf{44.0} \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:pr}
    \end{minipage}
}}
\end{table*}

\begin{table*}[b]
\centering
\makebox[0pt][c]{\parbox{1.0\textwidth}{\begin{minipage}[b]{0.49\linewidth}\centering
        \caption{Complexity analysis}
        \small \setlength{\tabcolsep}{1pt}
        \begin{tabular}{c|cc|cc|c}
        \Xhline{1.0pt}
         method & \#conv & \#ch. & FLOP & param & AP \\
        \hline
         RetinaNet              & 4 & 256 & 239G & 38M & 35.6 \\
         RetinaNet (deep)       & 5 & 256 & 265G & 39M &35.2 \\
         RetinaNet (wide)       & 4 & 288 & 267G & 39M &35.6 \\
         RetinaNet+BVR        & 4 & 256 & 266G & 39M &\textbf{38.5} \\
         \hline
         RetinaNet+GN         & 4 & 256 & 239G & 38M &36.5 \\
         RetinaNet (deep)+GN  & 5 & 256 & 265G & 39M &36.8 \\
         RetinaNet (wide)+GN  & 4 & 288 & 267G & 39M &36.5 \\
         RetinaNet+GN+BVR   & 4 & 256 & 266G & 39M &\textbf{39.2} \\
        \Xhline{1.0pt}
        \end{tabular}
        \label{tab:flops}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.49\linewidth}\centering
        \caption{BVR for four representative detectors using a ResNeXt-64x4d-101-DCN backbone}
        \small \setlength{\tabcolsep}{3pt}
        \begin{threeparttable}
        \begin{tabular}{l|lll}
        \Xhline{1.0pt}
        method & AP & AP & AP \\
        \hline
        RetinaNet               & 42.9 & 63.4 & 46.9 \\
        RetinaNet + BVR         & 44.7 (+1.8) & 64.9 & 49.0 \\
        \hline
        faster R-CNN            & 45.0 & 66.2 & 48.8 \\
        faster R-CNN + BVR      & 46.5 (+1.5) & 67.4 & 50.5 \\
        \hline
        FCOS
                                & 46.1 & 65.0 & 49.6 \\
        FCOS + BVR              & 47.6 (+1.5) & 66.2 & 51.4 \\
        \hline
        ATSS
                                & 48.3 & 67.1 & 52.6 \\
        ATSS + BVR              & 50.3 (+2.0) & 69.0 & 55.0 \\
        \Xhline{1.0pt}
        \end{tabular}
        \end{threeparttable}
        \label{tab:verification}
    \end{minipage}
}}
\end{table*}

{\noindent \textbf{Complexity analysis}}
Table~\ref{tab:flops} shows the flops analysis. The input images are resized to . The proposed BVR module introduces about  more parameters (M vs M) and about  more computations (G vs G) than the original RetinaNet. We also conduct RetinaNet with heavier head network to have similar parameters and computations as our approach. By adding one more layer, the accuracy slightly drops to , probably due to the increasing difficulty in optimization. We introduce a GN layer after every head conv layer to alleviate it, and one additional conv layer improves the accuracy by  AP. These results indicate that the improvements by BVR are mostly not due to more parameters and computation.

The real inference speed of different models using a V100 GPU (fp32 mode is used) are shown in Table \ref{tab:inf}. By using a ResNet-50 backbone, the BVR module usually takes less than  overhead. By using a larger ResNeXt-101-DCN backbone, the BVR module usually takes less than  overhead.

\subsection{BVR is Complementary to Other Attention Mechanisms}
The BVR module acts differently compared to the pioneer works of the non-local module~\cite{DBLP:conf/cvpr/0004GGH18} and the relation module~\cite{hu2018relation} which also model dependencies between representations. While the BVR module models relationships between different kinds of representations, the latter modules model relationships within the same kinds of representations (pixels~\cite{DBLP:conf/cvpr/0004GGH18} and proposal boxes~\cite{hu2018relation}). To compare with the object relation module (ORM)~\cite{hu2018relation}, we first apply BVR to enhance RoIAlign features with corner/center representations, the process of which is same as Figure \ref{fig:frcnn}. Then the enhanced features are utilized to perform object relation between proposals. Different from~\cite{hu2018relation}, keys are sampled to make the module more efficient. Table 8 shows that the BVR module and the relation module are mostly complementary. On the basis of faster R-CNN baseline, ORM can obtain  AP improvement, while our BVR improves AP by . Applying our BVR on the basis of the ORM continually improves AP by . Table \ref{tab:nonlocal} and \ref{tab:pr} show that the BVR modules is mostly complementary with non-local and object relation module.


\subsection{Generally Applicable to Representative Detectors}
\vspace{-2mm}
We apply the proposed BVR to four representative frameworks, \ie, RetinaNet~\cite{RetinaNet}, Faster R-CNN~\cite{ren2015faster,FPN}, FCOS~\cite{FCOS} and ATSS~\cite{zhang2019bridging}, as shown in Table~\ref{tab:verification}. The ResNeXt-64x4d-101-DCN backbone, multi-scale and longer training (20 epochs) are adopted to test whether our approach effects on strong baselines. The BVR module improve these strong detectors by  AP.

\begin{table}[t]
  \caption{Time cost of the BVR module.}
  \small
  \label{tab:inf}
  \centering
  \begin{tabular}{l|c|c|c}
    \Xhline{1.0pt}
    method & backbone & FPS & FPS (+BVR)\\
    \Xhline{1.0pt}
    Faster R-CNN    & ResNet-50/ResNeXt-101-DCN & 21.3/7.5 & 19.5/7.3  \\ 
    RetinaNet       & ResNet-50/ResNeXt-101-DCN & 18.9/7.0 & 17.4/6.8 \\ 
    FCOS  & ResNet-50/ResNeXt-101-DCN & 22.7/7.4 & 20.7/7.2\\
    ATSS & ResNet-50/ResNeXt-101-DCN & 19.6/7.1 & 17.9/6.9\\
    \Xhline{1.0pt}
  \end{tabular}
\vspace{-1em}
\end{table}


\begin{table}[t]
\renewcommand\arraystretch{1.02}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\centering
\caption{Results on MS COCO {\tt test-dev} set, `' denotes the multi-scale testing}
\small \setlength{\tabcolsep}{6pt}
\begin{threeparttable}
\begin{tabular}{c|c|ccc|ccc}
\Xhline{1.0pt}
method &backbone &AP &AP &AP &AP &AP &AP\\
\hline
DCN v2*~\cite{DCNv2}                                     & ResNet-101-DCN & 46.0 & 67.9 & 50.8 & 27.8 & 49.1 & 59.5\\
SNIPER* \cite{SNIPER}                                    &ResNet-101 &46.5 &67.5 &52.2 &30.0 &49.4 &58.4 \\
RepPoints* \cite{RepPoints}                              &ResNet-101-DCN &46.5 &67.4 &50.9 &30.3 &49.7 &57.1 \\
MAL* \cite{DBLP:journals/corr/abs-1912-02252}            &ResNeXt-101 &47.0 &66.1 &51.2 &30.2 &50.1 &58.9 \\
CentripetalNet* \cite{DBLP:journals/corr/abs-2003-09119} &Hourglass-104 &48.0 &65.1 &51.8 &29.0 &50.4 &59.9 \\
ATSS* \cite{zhang2019bridging}                           &ResNeXt-64x4d-101-DCN &50.7 &68.9 &56.3 &33.2 &52.9 &62.4 \\
TSD* \cite{DBLP:journals/corr/abs-2003-07540}            &SENet154-DCN &51.2 &71.9 &56.0 &33.8 &54.8 &64.2 \\
\hline
RelationNet++ (our)                                          &ResNeXt-64x4d-101-DCN &50.3 &69.0 &55.0 &32.8 &55.0 &\bf{65.8} \\
RelationNet++ (our)*                                         &ResNeXt-64x4d-101-DCN & \bf{52.7} &\bf{70.4} &\bf{58.3} &\bf{35.8} &\bf{55.3} &64.7 \\
\Xhline{1.0pt}
\end{tabular}
\end{threeparttable}
\label{tab:coco}
\vspace{-1em}
\end{table}
\vspace{-2mm}
\subsection{Comparison with State-of-the-Arts}
\vspace{-2mm}
We build our detector by applying the BVR module on a strong detector of ATSS, which achieves  AP on COCO {\tt test-dev} using multi-scale testing based on the ResNeXt-64x4d-101-DCN backbone. Our approach improves it by  AP, reaching  AP. Table~\ref{tab:coco} shows the comparison with state-of-the-arts methods. 
\vspace{-3mm}
\section{Conclusion}
\vspace{-3mm}
In this paper, we present a new module, BVR, which bridge various other visual representations by an attention mechanism like that in Transformer~\cite{vaswani2017attention} to enhance the main representations in a detector. The BVR module can be applied plug-in for an existing detector, and proves broad effectiveness for prevalent object detection frameworks, i.e. RetinaNet, faster R-CNN, FCOS and ATSS, where about  AP improvements are achieved. We reach  AP on COCO test-dev by improving a strong ATSS detector. The resulting network is named RelationNet++, which advances the relation modeling in \cite{hu2018relation} from bbox-to-bbox to across heterogeneous object/part representations. 
\newpage
\section*{Broader Impact}

This work aims for better object detection algorithms. Any object oriented visual applications may benefit from this work, as object detection is usually an indispensable component for them. There may be unpredictable failures, similar as most other detectors. The consequences of failures by this algorithm are determined on the down-stream applications, and please do not use it for scenarios where failures will lead to serious consequences. The method is data driven, and the performance may be affected by the biases in the data. So please also be careful about the data collection process when using it.


\bibliographystyle{apalike}
\bibliography{ref}

\end{document}
