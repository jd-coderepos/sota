

\documentclass[10pt]{article}

\usepackage{amsfonts}
\usepackage{bm}
\usepackage{graphicx,subfigure}
\usepackage{epstopdf}
\usepackage{epsfig} \usepackage{mathptmx} 
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{amsmath,amssymb} 
\usepackage{cite}
\usepackage{color}
\usepackage{array}
\usepackage{floatflt}
\usepackage{lipsum}
\usepackage{amsfonts} 
\usepackage{lastpage}
\usepackage{latexsym}
\usepackage{booktabs}


\theoremstyle{plain}
\newtheorem{problem}{Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newcommand{\st}{\sigma_{\tau}}
\newcommand{\pt}{\psi}
\newcommand{\St}{\mathcal{S}_{\tau}}
\newcommand{\Mt}{\mathcal{M}_{\tau}}
\newcommand{\sigt}{s^{t:t+\tau}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage[noend]{algorithmic}

\begin{document}

\title{Active Requirement Mining of Bounded-Time Temporal Properties of Cyber-Physical Systems}
\author{Gang Chen \and Zachary Sabato \and Zhaodan Kong}

\maketitle

\begin{abstract}
This paper uses active learning to solve the problem of mining bounded-time signal temporal requirements of cyber-physical systems or simply the requirement mining problem. By utilizing robustness degree, we formulates the requirement mining problem into two optimization problems, a parameter synthesis problem and a falsification problem. We then propose a new active learning algorithm called Gaussian Process Adaptive Confidence Bound (GP-ACB) to help solving the falsification problem. We show theoretically that the GP-ACB algorithm has a lower regret bound thus a larger convergence rate than some existing active learning algorithms, such as GP-UCB. We finally illustrate and apply our requirement mining algorithm on two case studies, the Ackley's function and a real world automatic transmission model. The case studies show that our mining algorithm with GP-ACB outperforms others, such as those based on Nelder-Mead, by an average of 30\% to 40\%. Our results demonstrate that there is a principled and efficient way of extracting requirements for complex cyber-physical systems. 
\end{abstract}


\section{Introduction}

In this paper, we propose the use of active learning for mining bounded-time signal temporal logic (STL) requirements of cyber-physical systems (CPSs). CPSs are characterized by the tight interaction of a collection of digital computing devices (the Cyber part) and a continuous-time dynamical system (the Physical part) \cite{lee2011introduction,alur2015CPS}. They are used for modeling in many safety-critical domains, such as automotive, medical, and aerospace industries, where the correctness of the end product is of significant importance \cite{jin2014powertrain,rajkumar2010cyber,jiang2012cyber}. However, in many industrial settings, the requirements intended to enforce the correctness guarantee are vague and in many cases expressed in natural languages, such as ``smooth steering'' and ``good fuel efficiency.'' This prevents the application of formal verification tools, such as Statistical Model Checking (SMC) \cite{zuliani2010bayesian}. Further, due to the complex nature of many CPSs, writing down the appropriate requirements to reflect the desirable system properties can be challenging even for experienced designers.

Given a system , e.g., a Stateflow/Simulink model of a steering system, and a requirement template  with a set of unknown parameters , the goal of this paper is to develop an improved method which can automatically infer a requirement  (often called a specification in formal methods literature) written in signal temporal logic, i.e., the system  satisfies the requirement . Such a problem can be called either a \emph{requirement mining} problem or a \emph{temporal logic inference} problem. In the past few years with the introduction of the concept of \emph{robustness degree} \cite{donze2010robust,fainekos2009robustness}, the problem has received increased attention and has achieved significant progress. Robustness degree quantifies how strongly a given sample trajectory  of a system  exhibits a temporal logic property  as a real number rather than just providing a yes or no answer. A robustness degree of  means that the trajectory  can tolerate perturbations up to the size of  and still maintain its current Boolean truth value with respect to .

With the help of robustness degree, the requirement mining problem of a CPS can be converted to an optimization problem for the expected robustness. Various techniques, such as particle swarm optimization \cite{haghighi2015spatel}, simulated annealing \cite{kong2014temporal}, and stochastic gradient descent algorithm \cite{kong2016TAC} can then be used to solve the optimization problem\footnote{Other optimization algorithms that have been used to solve similar optimization problems related to verification/falsification include rapidly exploring random tree (RRT) \cite{dreossi2015efficient}, cross-entropy method \cite{sankaranarayanan2012falsification}, Tabu search \cite{abbas2011linear}, and Ant Colony Optimization (ACO) \cite{annpureddy2011s}.}. Based on the nature of the search space, the requirement mining problems can be classified into two categories, \emph{parameter estimation} problems and \emph{structural inference} problems. In the former case, the structure of the requirement is given but with some unknown parameter values, for example, in a vehicular application, a requirement structure may be known as , which means that ``eventually between time 0 and some unspecified time , the speed  is greater than some unspecified value .'' The goal of the parameter estimation problem is to identify appropriate values for the parameters \cite{bartocci2014data,jin2013mining}. In the latter example, even the structure of the requirement is unspecified. The structural inference problem is inherently hard. Candidate formulas have to be restricted to a certain template with which a partial order is well defined \cite{kong2014temporal,kong2016TAC,jones2014anomaly}. This paper focuses on the parameter estimation problem, but there are significant implications for structural inference as well.

Due to the hybrid nature of CPSs, the robustness degree function can be highly nonlinear. Furthermore, many CPSs are stochastic because of various uncertainties inherent to the system and its mathematical model. These complexities added together makes uniform sampling method inappropriate to solve the optimization problem (there are even cases in which the exact probability distribution over the parameter space is unknown a priori). Monte-Carlo techniques have been shown to be an effective sampling method to tackle the issue \cite{abbas2013probabilistic,jin2013mining}. It is worth pointing out that these techniques may suffer from slow convergence, meaning that the inference procedure may take a long time. In many applications, a quick verdict is needed, consider for instance an online diagnosis of a faulty safety-critical system.

In this paper, we use active learning to partly mitigate the need for a large number of iterations during optimization. The idea behind active learning is to accelerate convergence by actively selecting potentially ``informative'' samples, in contrast with random sampling from a predefined distribution \cite{Settles2010,ramdas2013algorithmic}. Active learning can be explained with a supervised video classification problem. With passive learning, a large amount of labeled data is needed for classification, e.g., cat videos vs. non cat videos. Labeling itself involves an oracle, e.g., a human, and it might be time consuming. Further, much of the data may be redundant, not contributing much to the classification. With active learning, based on the current information gathered about the data distribution, the learning algorithm only asks for the label of the most informative data. In our setting, the CPS model together with a stochastic model checker will serve as an oracle, providing the learner an approximate expected robustness degree based on the current most informative parameter. 

\paragraph{Contributions} The \emph{main} contribution of this paper is an active learning based scheme for requirement mining of cyber-physical systems. It unifies two complementary camps of requirement mining and verification philosophies: one is model based \cite{annpureddy2011s,abbas2013probabilistic,sankaranarayanan2012falsification,jin2013mining,asarin2012parametric}, and the other is data driven \cite{jones2014anomaly,kong2014temporal,bartocci2014data}. In our method, models are used as oracles, generating data which enables our method to gain knowledge of the system.  This helps focusing ongoing searches in promising parameter ranges, and thus eliminating unnecessary samples. The philosophy of our paper is similar to simulation based optimization \cite{gosavi2015simulation} but with a particular focus on utilizing machine learning techniques. \emph{Second}, instead of using existing learning algorithms, we develop a new active learning algorithm called Gaussian Process Adaptive Confidence Bound (GP-ACB). We prove that our GP-ACB algorithm converges faster than Gaussian Process Upper Confidence Bound (GP-UCP) algorithm \cite{srinivas2012information}, a state-of-the-art active learning algorithm. \emph{Third}, we integrate our GP-ACB algorithm with an existing verification tool, called Breach \cite{donze2010breach}. Using an automatic transmission controller as an example system, we show that the efficiency of Breach can be improved by as much as 40\%.

\paragraph{Organization} This paper is subdivided into the following sections. Section \ref{math} discusses the relevant background on signal temporal logic and Gaussian processes. Section \ref{problemForm} formally defines the requirement mining problem and shows how to transform the problem into an optimization problem with the help of robustness degree. Section \ref{active_learning} discusses our GP-ACB algorithm. Section \ref{caseStudies} provides two case studies to demonstrate our algorithm, an academic example with Ackley's function as the target function and an automotive example. Section \ref{conclusion} concludes the paper.


\section{Preliminaries}
\label{math}

\subsection{Signal Temporal Logic}
\label{sec:STL}
Given two sets  and ,  denotes the set of all functions from  to . Given a time domain , a \emph{continuous-time, continuous-valued signal} is a function . We use  to denote the value of signal  at time , and  to denote the suffix of signal  from time , i.e., .  

\em Signal temporal logic \em (STL) \cite{maler2004monitoring} is a temporal logic defined over signals.  STL is a predicate logic with interval-based temporal semantics. The  syntax of STL is defined as

where  and  are non-negative finite real numbers, and   is a predicate where  is a signal,  is a function, , and  is a constant. The Boolean operators  and  are negation (``not'') and conjunction (``and''), respectively. The other Boolean operators are defined as usual. The temporal operators  and  stand for ``Finally (eventually)'' and ``Globally (always)'', respectively. 


The \em semantics \em of STL is recursively defined as

In plain English,  means ``within  and  time units in the future,  is true'', and  means ``for all times between  and  time units in the future  is true''.

STL is equipped with a \em robustness degree \em 
\cite{fainekos2009robustness,donze2010robust} (also called ``degree of 
satisfaction'') that quantifies how well a given signal  satisfies a given formula .  The robustness is calculated recursively as follows
 
We use  to denote . If  is large and positive, then  would have to change by a large deviation in order to violate .  

\emph{Parametric signal temporal logic} (PSTL) is an extension of STL where the bound  and the endpoints of the time intervals  are parameters instead of constants \cite{asarin2012parametric}. We denote them as \emph{scale} parameters  and \emph{time} parameters , respectively. A full parameterization is given as . The syntax and semantics of PSTL are the same as those of STL. A \emph{valuation}  is a mapping that assigns real values to the parameters appearing in an PSTL formula. A valuation  of an PSTL formula  induces an STL formula . For example, if  and , then .

\subsection{Gaussian Processes}
\label{sec:GP}

Formally, a Gaussian process (GP) is defined as a collection of random variables, any finite linear combination of which have a joint Gaussian distribution \cite{Rasmussen2006}. A simple example of a Gaussian process is a linear regression model , where , , which maps an -dimensional  to an -dimensional feature space (a feature is an individual, measurable property of a function space), and , a zero mean Gaussian with covariance matrix . Any GP is completely specified by its mean function  and its covariance function or kernel 

As an example, for the linear regression model,




A flat (or even zero) mean function  is chosen in the majority of cases in the literature. Such a choice does not cause many issues since the mean of the posterior process in not confined to zero. There is a large set of available kernels . Two common ones, which are also used in this paper, are \cite{Anwar2015}
\begin{itemize}
\item Gaussian kernel with length-scale , where  is the Euclidean length;
\item Mat\'{e}rn kernel with length-scale  

where  is the modified Bessel function and  is a positive parameter.
\end{itemize}



\section{Requirement Mining Problem Formulation}
\label{problemForm}


In this section, we first provide some background on our cyber-physical system models. We then formally define the requirement mining problem. Finally, we show how to formulate the requirement mining problem as an optimization problem. 




\subsection{Cyber-Physical Systems}
\label{sec:system}

In this paper, we study autonomous (closed-loop) cyber-physical systems. Notations from \cite{abbas2013probabilistic,sankaranarayanan2012falsification} are adopted here. A system  maps an initial condition (or uncontrolled environmental conditions, e.g., road conditions)  to a discrete-time output signal  with  and  as the finite maximal simulation time. We assume both  and  can be represented as the Cartesian product of intervals , where . 





\subsection{Problem Statement}
\label{sec:problem}

Formally, in this paper, we solve the following problem.
\begin{problem}
\label{original_problem}
Given a system  with an initial condition set (or uncontrolled environmental condition set)  and a parametric signal temporal logic formula  with unknown parameters , where  is the set of feasible valuation,  find a valuation  such that 

That is to say, the output signal  of the system , starting from any initial condition , satisfies  at time 0. We write  as shorthand for .
\end{problem}



\subsection{Requirement Mining as Optimization}

Problem \ref{original_problem} is subject to the curse of dimensionality. If we solve Problem \ref{original_problem} directly, the dimension of the search space is . In this paper, we use the following three methods to mitigate the computational complexity.
\begin{itemize}
\item To reduce the search space's dimension, we follow the idea described in \cite{jin2013mining}. We divide Problem \ref{original_problem} into two sub-problems: a parameter synthesis problem (Problem \ref{ff:synthesis_problem}) having a dimension of  and a falsification problem (Problem \ref{falsificaton_problem}) having a dimension of .
\item We then develop an active learning algorithm to progressively reduce the volume of the search space. The active learning algorithm focuses only on ``informative'' initial states, meaning it does not need to search the entire initial condition set .
\item Finally, we take advantage of the monotonicity property of PSTL \cite{jin2013mining,kong2016TAC,kong2014temporal}. If a parameter has been found that solves Problem \ref{ff:synthesis_problem}, then there is no need to check for parameters that make the requirement more restrictive, as these parameters surely do not constitute an optimal solution.
\end{itemize}
The flowchart to solve Problem \ref{original_problem} is shown in Fig. 1.

\begin{figure}[!htbp]
\label{fig:Requirement mining flowchart}
\centering
\includegraphics[width=0.8\textwidth]{flowchart.pdf}
\caption{Flowchart for solving Problem \ref{original_problem}. Our algorithm is based on the Breach toolbox \cite{donze2010breach,jin2013mining}. Our new active learning algorithm is used in the falsification step (shown in blue).}
\end{figure}

\begin{problem}
\label{ff:synthesis_problem}
\em Parameter Synthesis\em  Given a system  with a set of inputs , where  is the number of counter-example traces obtained from Problem \ref{falsificaton_problem}, and a PSTL formula  with unknown parameters , find a valuation  to solve 

where  us a user-specified bound.
\end{problem}

\begin{problem}
\label{falsificaton_problem}
\em Falsification\em  
Given a system  with an initial condition set (or uncontrolled environmental condition set)  and a PSTL formula  with particular parameters , find an initial condition  to solve

\end{problem}


The max function  in Eqn. (3) is a modified hinge loss function. As the minimum of the robustness, , is positive,  the loss function rewards values that are close to the bound 0 and at the same time positive. It is utilized here to tackle the issue related to the non-uniqueness of solutions to the requirement mining problem, as pointed out in \cite{jin2013mining}. For a particular , the min functions in Eqn. (3) and Eqn. (4) reward initial states that lead to negative robustness degrees. Their goals are to find an initial state which leads to a trace that does not meet the requirement . The max-min function, Eqn. (3), then finds a parameter , and, in turn, a requirement  such that for any initial state , the output of the system has a positive robustness degree that is smaller than . This means that the system satisfies the requirement , but only barely. Formally,

\begin{lemma}
\label{lemma problem}
A formula obtained by solving Problem 2 and Problem 3 together is a solution to Problem 1. I.e., if a formula , where 

and furthermore , the statement (\ref{original requirment}) hold.
\end{lemma}

\begin{proof} \renewcommand{\qedsymbol}{}
Assume there is a parameter  obtained by solving Problem \ref{ff:synthesis_problem} and Problem \ref{falsificaton_problem}, and . According to Problem \ref{falsificaton_problem}, a  that meets the requirements cannot be found. Thus, Lemma \ref{lemma problem} has been proven. 
\end{proof}

The most time-consuming part of solving Problem \ref{original_problem} is the generation of a trace given an initial state . For the parameter synthesis problem, there is no need to generate traces. Moreover, according to \cite{jin2013mining}, since PSTL satisfies the important property of monotonicity, the solution is quite efficient, via a binary search algorithm \cite{dreossi2015efficient}, for example. The falsification problem, on the other hand, does need a large number of traces being generated. Therefore, finding a way to minimize the number of traces needed in the falsification step will benefit the requirement mining process greatly. In this paper, we solve the falsification problem with a new active learning algorithm called Gaussian Process Adaptive Confidence Bound (GP-ACB). We will show theoretically (in Section \ref{active_learning}) and empirically (in Section \ref{caseStudies}) the effectiveness of our algorithm in reducing the computational time.


\section{Active Learning Algorithm}
\label{active_learning}

In the section, we propose an active learning algorithm, called Gaussian Process Adaptive Confidence Bound (GP-ACB) algorithm, to solve the falsification problem mentioned in Section 3. It is inspired by Gaussian process upper confidence bound (GP-UCB) \cite{srinivas2012information}. We analyze the regret bound of GP-ACB and show that it can achieve the same regret bound with a higher probability than GP-UCB.




\subsection{Gaussian Process Adaptive Confidence Bound Active Learning Algorithm}



Active learning algorithms are originally developed to solve classification problems when an oracle is needed to provide labels \cite{Settles2010}. The process of obtaining labels from the oracle can be expensive in terms of both time and money. Thus, the goal of any active learning algorithm is to achieve high classification or regression accuracy by using the fewest labeled instances. The requirement mining problem suffers from a similar issue as elaborated at the end of Section \ref{problemForm}. Our oracle is a simulator, e.g., a Stateflow/Simulink model. Given the complexity of many CPS models, to obtain a trace from the simulator can be costly in time. Thus, we need to decrease the number of simulations needed to learn a formula.  

One active learning algorithm is called Gaussian process upper confidence bound (GP-UCB) \cite{srinivas2012information}. At each step , it solves the following problem

where  is the search space ( for the requirement mining problem),  is a function of  and independent of  (an example of  will be given later),  and  are the mean and covariance function of the Gaussian process, respectively, and  is the instance that will be inquired at step , meaning the label of  will be obtained from the oracle. The GP-UCB algorithm balances the classical exploitation-exploration trade-off by combining two strategies: the first term tends to pick those points that are expected to achieve high rewards (exploitation); and the second terms tend to pick those points that are uncertain (exploration). 

The second term of Eqn. (\ref{eq:GP-UCB}) only depends on the covariance function , which can potentially make the exploration somewhat random and inefficient. To address this problem, we propose an algorithm called Gaussian Process Adaptive Confidence Bound (GP-ACB) by adding a normalization term  to Eqn. (\ref{eq:GP-UCB}) as follows:

where  normalizes the mean  and can be written explicitly as

It is quite obvious that .  serves two main purpose here. First, it acts as an adaptive factor to uncertainty and favors exploration directions associated with increasing rewards. Thus, it can improve the exploration efficiency. Second, it provides an adaptive upper quantile of the marginal posterior , where  is a vector containing all the observations until time . In this paper, we assume that the observation at time , , is  with  and  known. Pseudocode for the GP-ACB algorithm is provided in Algorithm 1.

\begin{algorithm} \label{algorithm:GP-ACB}
 \caption{GP-ACB Algorithm}
 \KwIn{\\ Search space  ( for the requirement mining problem); \\
GP priors  and ;\\ 
Kernel function ;\\
Maximal simulation time }
\begin{algorithmic}[1]
  \FOR{}
  \STATE ;
  \STATE ;
  \STATE ;
  \STATE .
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Regret Bound of GP-ACB}

The goal of any learning algorithm can be stated as follows: given an unknown reward function , maximize the sum of rewards , which is equivalent to finding a  such that . A concept called regret bound can be used to quantify the convergence rate of a learning algorithm \cite{Niranjan2009,Settles2010,auer2009near}. First, the instantaneous regret at time  is defined as . Then, the cumulative regret  after  rounds is the sum of instantaneous regrets . A desired property of the learning algorithm is then to guarantee , implying the convergence to the global maximum . Finally, the bounds on the average regret  are directly related to the convergence rate of the learning algorithm. The lower the bound is, the faster the algorithm converges. This section investigates the regret bound of the GP-ACB algorithm.

Our proofs on the regret bound follow those in \cite{Niranjan2009}. Here we only consider the cases when the search space is finite, i.e., .

\begin{lemma}
\label{Lemma 1}
Pick  and set , where , . Then,

holds with probability .
\end{lemma}

\begin{proof} \renewcommand{\qedsymbol}{}
For  and . It is known that conditioned on ,  are deterministic. Further, . Now if , then

for , as  for . We have . Set  and . After applying the adaptive bound, we have

holds with probability . Choosing , e.g., with , and using the adaptive bound for , the statement holds.
\end{proof}

\begin{lemma}
\label{Lemma 2}
Fix , if , , then the regret  is bounded by .
\end{lemma}

\begin{proof} \renewcommand{\qedsymbol}{}
According to the definition of ,  \\ . Therefore, the instantaneous regret 

\end{proof}

\begin{lemma}
\label{Lemma 3}
Pick  and set , where . Then,

holds with probability .
\end{lemma}

\begin{proof} \renewcommand{\qedsymbol}{}
For  and . Conditioned on  are deterministic. Further, . According to Lemma \ref{Lemma 1}, . Since , and with the adaptive bound for , the statement holds.
\end{proof}

\begin{lemma}
\label{Lemma 4}
Set , , and let  be defined as in Lemma \ref{Lemma 3}, then 

\end{lemma}

\begin{proof} \renewcommand{\qedsymbol}{}
Set , , according to GP-ACB, , then

\end{proof}

\begin{remark}
\label{remark lemma 4}
Lemma \ref{Lemma 4} shows that when the scale of the function  is large,  will be close to 1, meaning the GP-ACB algorithm degrades to GP-UCB. Conversely, when the scale of function is large,  will play a more important role in the search process, driving the algorithm to be very greedy. To investigate the effects of the scaling function on the optimization performance, a numerical experiment has been conducted and the results are shown in Section 5.1.
\end{remark}

Define  as the maximum information gain after  rounds as follows \cite{Niranjan2009}:

According to  \cite{Niranjan2009}, if the search space  is compact and convex, where  is the dimension of the search space, with the assumption that the kernel function satisfies , we have 
\begin{itemize}
\item  for Gaussian kernel;
\item  for Mat\'{e}rn kernels with .
\end{itemize}
We can finally obtain the following bound for the GP-ACB algorithm.

\begin{theorem}
\label{theorem 1}
Let ,  ,  and . Running GP-ACB results in a regret bound as follows

where .
\end{theorem}

\begin{proof} \renewcommand{\qedsymbol}{}
Define the information gain  as follows:

where . According to Lemma  and Lemma , the regret bound ,  holds with probability . As  is non-decreasing, we have 

where , since , \\ and . As , for  we have

According to Cauchy-Schwarz inequality, . Theorem \ref{theorem 1} has been proven.
\end{proof}

\begin{remark}
\label{remark for regret bounds}
The regret bound of the GP-UCB algorithm is \cite{Niranjan2009} 

With the same parameter setting, the regret bound of our GP-ACB algorithm is shown as Eqn. (\ref{eq:regret_bound}). Based on the bound of  shown in Lemma \ref{Lemma 4} and the definition of the information gain , we know that  is close to 0 when  is large, which means  and  in Theorem 1 are close to one when iteration  is large and are close to zero when iteration  is small. This indicates that GP-ACB algorithm is greedy at the beginning and degrades to GP-UCB algorithm when iteration  is large. Since , we can get the conclusion that the GP-ACB algorithm can get the same regret bound more efficient than that of the GP-UCB algorithm. The regret bound can be easily translated into convergence rate. The maximum  in the first  iterations is no further from , where  is the global optimum, than the average regret . Thus, compared with GP-UCB, on average, the GP-ACB algorithm has a higher or equal convergence rate. 
\end{remark}

\section{Case Studies}
\label{caseStudies}

To demonstrate the capability of GP-ACB, in this section, we first use it to solve a global optimization problem with Ackley's function as the target function. The performance of GP-ACP is compared with those of other active learning algorithms. We then use GP-ACB to solve a full scale requirement mining problem of an automatic transmission system, a benchmark used widely in CPS community \cite{jin2013mining,jin2014powertrain,sankaranarayanan2012falsification}. In both cases, GP-ACB outperforms other active learning algorithms as well as some other state-of-art optimization algorithms such as Nelder-Mead. 

\subsection{Global Optimization of Ackley's Function}

To verify the performance of the proposed GP-ACB algorithm, we compare the GP-ACB algorithm with four types of Gaussian-Process-based strategy: (i) GP-UCB active learning, (ii) Batch-greedy UCB active learning \cite{Desautels2014}, (iii) pure exploration, i.e., choosing points of maximum variance at each step, and (iv) pure exploitation or greedy, i.e., choosing points of maximum mean at each step. We use Ackley's function (shown in Fig. 2.(a)) as the target function. Its formula is as follows,

where  is the observation noise with zero mean and variance  at 0.025. The search space  is randomly discretized into 1000 points. We run each algorithm for  iterations with sampling time . Since the global minimum of the Ackley's function  is known (unknown to the learning algorithms though), for the -th trial, if  is the solution obtained by running the algorithm for  iterations, then mean regret for the algorithm at time  is , where  is the number of trials. In this case study, we set . Each trial is initialized randomly. 

\begin{figure}[!htb]
\label{fig: comparison of regret}
\centering
\subfigure[]{\includegraphics[width=0.5\textwidth]{Ackleyfunction.pdf}}\\
\subfigure[]{\includegraphics[width=0.48\textwidth]{Regret_SE.pdf}}
\subfigure[]{\includegraphics[width=0.48\textwidth]{Regret_Matern.pdf}}
\caption{(a) Ackley's function. (b) and (c) compares the performance of GP-ACB with those of other strategies. The mean regret over 1000 trails  is chosen as the performance metric. (b) shows the comparison results with Gaussian kernel. (c) shows the comparison results with Mat\'{e}rn kernel.}
\end{figure}

Fig. 2(b) and Fig. 2(c) show the comparison of the mean regret  incurred by the different Gaussian Process based algorithms with Gaussian kernel and Mat\'{e}rn kernel, respectively. With both kernels, GP-ACB outperforms the others. For instance, GP-UCB arrives at its minimum regret in an average of 58 iterations; while GP-ACB arrives at its minimum regret in an average of 45 iterations. Another interesting observation is that pure exploitation (greedy) strategy on average converges quicker than others. But the regret it converges to is on average higher than others, implying the convergence to local minimums. As for kernels, for this particular case, Mat\'{e}rn kernel outperforms the Gaussian kernel. This is not supervising, given that the Ackley's function (as shown in Fig. 2(a)) is quite ``non-smooth'' and Mat\'{e}rn kernel is designed to capture non-smoothness. 

\begin{figure}[!htb]
\label{fig:comparison of scaling factors}
\centering
\subfigure[]{\includegraphics[width=0.48\textwidth]{Scaleperformance.pdf}}
\subfigure[]{\includegraphics[width=0.48\textwidth]{Scalingfactor.pdf}}
\caption{(a) The performance of the GP-ACB's algorithm with respect to the scaling factor . The mean regret over 1000 trails  is chosen as the performance metric. (b) The effects of the scaling factor  on different algorithms. The mean regret at time  over 1000 trails  is chosen as the performance metric. Here .}
\end{figure}

Remark \ref{remark lemma 4} shows that the range of the mean function  at time , 

affects the performance of the GP-ACB algorithm. When the range  is large, the GP-ACB algorithm degrades to the GP-UCB algorithm. In this paper, we use a scale factor  to change the range of the reward or cost function. Instead of optimizing  directly, we scale it by  first and then solve the optimization problem with a new reward or cost function . To demonstrate the effects, Fig. 3(a) shows the performance of GP-ACB with different scaling factors. It shows that when the scaling factor  is large, the algorithm prefers exploitation over exploration and has the danger of trapping in a local optimum; when the scaling factor  is small, the algorithm prefers exploration over exploitation and it may take more than necessary time to explore irrelevant regions. Thus, a proper  needs to be selected to balance the trade-off between exploration and exploitation. Fig. 3(b) shows the effects of the scaling factor  on different algorithms. It shows that when the scaling factor  is large, the GP-UCB and GP-ACB algorithms have the same mean regret after 58 iterations, implying GP-ACB has degraded to GP-UCB. The pure exploration and pure exploitation algorithms' performance are not effected by the scaling factor .


\subsection{Requirement Mining on an Automatic Transmission Model}

The model used in the following study is the same as the one used in \cite{jin2013mining}, a closed-loop model of a four-speed automatic transmission, shown in Fig 4. The model contains all necessary mechanical components, including engine, transmission and the longitudinal chassis dynamics. In the model, the throttle  position and brake torque are the input signals. With the current gear selection, the transmission ratio () can be computed through the transmission block, and the output torque can be obtained with the engine speed (), gear status and transmission RPM.

\begin{figure}[!htb]
\label{system model}
\centering
\includegraphics[width=0.8\textwidth]{simulinkDiagram.pdf}
\caption{Closed-loop Simulink model of an automatic transmission and controller, whose exogenous inputs are the throttle position and brake torque.}
\end{figure}

We tested three different template requirements which are the same with \cite{jin2013mining}, thus a comparison study can be conducted. The three template requirements are as follows:
\begin{enumerate}
\item Requirement , which has a strong correlation with safety requirements characterizing the operating region for the engine parameters \textit{speed} and \textit{RPM}, specifying that always the \textit{speed} is below  and \textit{RPM} is below . 

\item Requirement , which measures the performance of the closed loop system. The value for  specifies how fast the vehicle can reach a certain speed, and the value for  can specify the lowest \textit{RPM} needed to reach the above speed. The formula specifies that the vehicle cannot reach the speed of 100 mph in  seconds with \textit{RPM} always below :

\item Requirement , which encodes undesirable transient shifting of gears, specifying that whenever the system shifts to gear 2, it dwells in gear 2 for at least  seconds:

\end{enumerate}

Our requirement mining algorithm as shown in Fig. 1 and elaborated in Section 3.3 is implemented to mind requirements for the automatic transmission model. Our algorithm is based on the Breach toolbox \cite{donze2010breach} with the falsification problem solved by GP-ACB. The Breach toolbox uses the Nelder-Mead algorithm for falsification \cite{donze2010breach,jin2013mining}. Section 5.1 shows that the scaling factor affects the performance of the GP-ACB algorithm. We first run a few test trials in order to find the optimal scaling factor, the results of which are shown in Table 1. It can be seen that when scaling factor is set to 0.5, the time spent on falsification, the time spent on parameter synthesis and the number of simulations are all at their lowest values. Thus, the scaling factors are set to 0.5 for all the following experiments.  

\begin{table}[!htb]
\centering  
\label{scale factor}
\caption{Requirement mining results for GP-ACB with different scaling factors. In this table and Table 2, ``Parameter Values'' are the mined parameter values. For instance, parameter values  together with the formula template  means that the mined formula is . ``Fals.(s)'' is the CPU time spent on solving the falsification problem. ``Synth.(s)'' is the CPU time spent on solving the parameter synthesis problem. ``Sim.'' is the number of simulations.``Robustness'' is the robustness degree corresponding to the mined parameters.}
\begin{tabular}{c|ccccc}   
\hline
Scaling Factor &\multicolumn{5}{c}{Requirement Mining Results}\\     
 &Parameter Values  &Fals.(s)  &Synth.(s)  &Sim.  &Robustness\\ 
\hline  0.1   &(4849, 155) &112	&16.1	&393	&0.1541\\   
0.25  &(4846, 155)&109	&17.4	&385	&0.3510\\          
0.5   &(4849, 155)&67	&15.4	&255	&0.7108\\       
0.75  &(4846, 155)&79	&15.6	&276	&0.8197\\      
1     &(4847, 155)&107	&16.6	&367	&1.5098\\
\hline
\end{tabular}
\end{table}

The comparison results, averaged over 20 randomly started trails, for GP-ACB with Gaussian kernel, GP-ACB with Mat\'{e}rn kernel, GP-UCB and Nelder-Mead are shwon in Table 2. The results show GP-ACB with Mat\'{e}rn kernel outperforms the others in terms of the times spent on falsification, the times spent on parameter synthesis and the numbers of simulations. For instance, in mining the formula , GP-UCB saves on average 10\% of simulation number compared with Nelder-Mead, while GP-ACB with Mat\'{e}rn kernel saves up to about 40\%. The same level of improvement is observed for the other two requirements,  with a 40\% improvement and  with a 30\% improvement.

\begin{table}[!htb]
\centering  
\label{requirement mining}
\caption{Requirement mining results for GP-ACB with Gaussian kernel, GP-ACB with Mat\'{e}rn kernel, GP-UCB with Mat\'{e}rn kernel, and Nelder-Mead algorithm.}
\begin{tabular}{l|ccccc}   
\hline
Template &\multicolumn{5}{c}{Requirement Mining Results}\\     
 &Parameter Values  &Fals.(s)  &Synth.(s)  &Sim. &Robustness\\ \hline  GP-ACB(Gaussian)         &(4845, 155) &100 &15.6 &330   &1.2812\\   
GP-ACB(Mat\'{e}rn) &(4849, 155) &67	&15.4	&255	&0.7108\\          
GP-UCB(Mat\'{e}rn) &(4844, 155)	&97	&15.4	&334	&0.7425\\      
Nelder-Mead        &(4857, 155)&112&13.3	&380	&0.6738\\ \hline
 &Parameter Values  &Fals.(s)  &Synth.(s)  &im. &Robustness\\ \hline  GP-ACB(Gaussian)         &(5997, 12.20) &64 &2.0 &207 &0.1556\\   
GP-ACB(Mat\'{e}rn) &(5997, 12.20) &59 &1.9 &179 &0.0802\\          
GP-UCB(Mat\'{e}rn) &(5997, 12.20) &63 &2.0 &190 &0.1116\\      
Nelder-Mead        &(5997, 12.20) &104 &1.4 &340 &0.06453\\ \hline
 &Parameter Values  &Fals.(s)  &Synth.(s)  &Sim. &Robustness\\ \hline  GP-ACB(Gaussian)         &0.0586 &296 &10.6 &941 &0.05\\   
GP-ACB(Mat\'{e}rn) &0.0586 &294 &10.8 &940 &0.05\\          
GP-UCB(Mat\'{e}rn) &0.0586 &363 &10.6 &1056 &0.05\\      
Nelder-Mead        &0.0586 &422 &9.3  &1246 &0.1\\ \hline
\end{tabular}
\end{table}



\section{Conclusions and Future Work}
\label{conclusion}

In this paper, we introduced an active learning method, called Gaussian Process adaptive confidence bound (GP-ACB), for mining requirements of bounded-time temporal properties of cyber-physical systems. The theoretical analysis of the proposed algorithm showed that it had a lower regret bound thus a higher convergence rate than other Gaussian-Process-based active learning algorithms, such as GP-UCB. By using two case studies, one of which was an automatic transmission model, we showed that our requirement mining algorithm outperformed other existing algorithms, e.g., those based on GP-UCB or Nelder-Mead, by an average of 30\% to 40\%. Our results have significant implications for not only the requirement mining but also the validation and verification of cyber-physical systems. We are currently exploring the possibility of utilizing active learning to solve the structural inference problem, i.e., to mind a requirement without any given template. 



\bibliographystyle{abbrv}
\bibliography{references}     

\end{document}
