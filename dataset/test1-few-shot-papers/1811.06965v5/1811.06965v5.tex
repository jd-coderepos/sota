\documentclass{article}

\usepackage[preprint, nonatbib]{neurips_2019}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{todonotes}
\usepackage{url}            \usepackage[shortlabels]{enumitem}
\usepackage{wrapfig}

\newcommand*\comment[1]{\colorbox{yellow}{\textbf{#1}}}

\newcommand{\todob}[1]{\todo[inline,caption={},backgroundcolor=yellow]{#1}}

\makeatletter
\def\codename{GPipe}

\def\onedot{.}
\def\eg{\emph{e.g}\onedohttps://braintex.goog/project/5cb68f351a49981777fa8f87t} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}

\newif\ifarxiv
\arxivfalse




\begin{document}

\title{\codename{}: Easy Scaling with Micro-Batch Pipeline Parallelism}




\author{
Yanping Huang\\
\texttt{huangyp@google.com} \\
\And
Youlong Cheng\\
\texttt{ylc@google.com} \\
\And 
Ankur Bapna \\
\texttt{ankurbpn@google.com} \\
\And
Orhan Firat \\
\texttt{orhanf@google.com}
\And
Mia Xu Chen \\
\texttt{miachen@google.com}
\And
Dehao Chen\\
\texttt{dehao@google.com} \\
\And
HyoukJoong Lee\\
\texttt{hyouklee@google.com} \\
\And
Jiquan Ngiam\\
\texttt{jngiam@google.com}\\
\And
Quoc V. Le \\
\texttt{qvl@google.com}\\
\And
Yonghui Wu\\
\texttt{yonghui@google.com} \\
\And
Zhifeng Chen\\
\texttt{zhifengc@google.com}\\
}

\maketitle

\begin{abstract}
Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce \codename{}, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, \codename{} provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, \codename{} utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of \codename{} by training large-scale neural networks on two different tasks with distinct network architectures: (i) \textit{Image Classification}: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) \textit{Multilingual Neural Machine Translation}: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.
\end{abstract}


\section{Introduction}

Deep learning has seen great progress over the last decade, partially thanks to the development of methods that have facilitated scaling the effective capacity of neural networks. This trend has been most visible for image classification, as demonstrated by the accuracy improvements on ImageNet with the increase in model capacity (Figure~\ref{fig_size_accuracy}a). A similar phenomenon can also be observed in the context of natural language processing (Figure~\ref{fig_size_accuracy}b) where simple shallow models of sentence representations \cite{DBLP:journals/corr/abs-1708-00107,peters-etal-2018-deep} are outperformed by their deeper and larger counterparts \cite{Devlin2018bert,radford2019language}. 

While larger models have brought remarkable quality improvements to several fields, scaling neural networks introduces significant practical challenges. Hardware constraints, including memory limitations and communication bandwidths on accelerators (GPU or TPU), force users to divide larger models into partitions and to assign different partitions to different accelerators. However, efficient model parallelism algorithms are extremely hard to design and implement, which often requires the practitioner to make difficult choices among scaling capacity, flexibility (or specificity to particular tasks and architectures) and training efficiency. As a result, most efficient model-parallel algorithms are architecture and task-specific. With the growing number of applications of deep learning, there is an ever-increasing demand for reliable and flexible infrastructure that allows researchers to easily scale neural networks for a large variety of machine learning tasks.


\begin{figure}[!t]
\caption{(a) Strong correlation between top-1 accuracy on ImageNet 2012 validation dataset~\cite{deng2009imagenet} and model size for representative state-of-the-art image classification models in recent years~\cite{szegedy2015going, szegedy2016rethinking, he2016identity, xie2017aggregated, hu2017squeeze, zoph2017learning, real2018regularized}. There has been a  increase in the model capacity. Red dot depicts  top-1 accuracy for the 550M parameter AmoebaNet model. (b) Average improvement in translation quality (BLEU) compared against bilingual baselines on our massively multilingual in-house corpus, with increasing model size. Each point, , depicts the performance of a Transformer with  encoder and  decoder layers, a feed-forward hidden dimension of  and  attention heads. Red dot depicts the performance of a 128-layer 6B parameter Transformer.}
\label{fig_size_accuracy}
\sbox0{\begin{subfigure}[b]{0.51\textwidth}
    \label{size_vs_accuracy}
\includegraphics[width=\linewidth]{size_vs_accuracy.png}
\end{subfigure}}
\sbox1{\begin{subfigure}[b]{0.46\textwidth}
    \label{size_vs_bleu}
\includegraphics[width=\linewidth]{size_vs_bleu_4.png}
\end{subfigure}}
  \usebox0 \hspace{1mm} \usebox1 \\
\end{figure}

To address these challenges, we introduce \codename{}, a flexible library that enables efficient training of large neural networks. \codename{} allows scaling arbitrary deep neural network architectures beyond the memory limitations of a single accelerator by partitioning the model across different accelerators and supporting re-materialization on every accelerator~\cite{griewank2000algorithm,chen2016training}.
With \codename{}, each model can be specified as a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is then placed on a separate accelerator. Based on this partitioned setup, we propose a novel pipeline parallelism algorithm with batch splitting. We first split a \textit{mini-batch} of training examples into smaller \textit{micro-batches}, then pipeline the execution of each set of micro-batches over cells. We apply synchronous mini-batch gradient descent for training, where gradients are accumulated across all micro-batches in a mini-batch and applied at the end of a mini-batch. Consequently, gradient updates using \codename{} are consistent regardless of the number of partitions, allowing researchers to easily train increasingly large models by deploying more accelerators. \codename{} can also be complemented with data parallelism to further scale training.

We demonstrate the flexibility and efficiency of \codename{} on image classification and machine translation. For image classification, we train the AmoebaNet model on  input from the ImageNet 2012 dataset. By increasing the model width, we scale up the number of parameters to  million and achieve a top-1 validation accuracy of 84.4\%. On machine translation, we train a single 128-layer 6-billion-parameter multilingual Transformer model on 103 languages (102 languages to English). We show that this model is capable of outperforming the individually trained 350-million-parameter bilingual Transformer Big \cite{vaswani2017attention} models on 100 language pairs.

\section{The \codename{} Library}
We now describe the interface and the main design features of \codename{}. This open-source library is implemented  under the Lingvo~\cite{shen2019lingvo} framework. The core design features of \codename{} are generally applicable and can be implemented for other frameworks~\cite{jia2014caffe, chen2015mxnet,paszke2017automatic}.

\subsection{Interface}
Any deep neural network can be defined as a sequence of  layers. Each layer  is composed of a forward computation function , and a corresponding set of parameters . \codename{} additionally allows the user to specify an optional computation cost estimation function, . With a given number of partitions , the sequence of  layers can be partitioned into  composite layers, or cells. Let   consist of consecutive layers between layers  and . The set of parameters corresponding to  is equivalent to the union of , , \ldots, , and its forward function would be . The corresponding back-propagation function  can be computed from  using automatic symbolic differentiation. The cost estimator, , is set to .

The \codename{} interface is extremely simple and intuitive, requiring the user to specify: (i) the number of model partitions ,  (ii) the number of micro-batches , and (iii) the sequence and definitions of  layers that define the model. Please refer to supplementary material for examples.

\subsection{Algorithm}
Once the user defines the sequence of layers in their network in terms of model parameters , forward computation function , and the cost estimation function , \codename{}  partitions the network into  cells and places the -th cell on the -th accelerator. Communication primitives are automatically inserted at partition boundaries to allow data transfer between neighboring partitions. The partitioning algorithm minimizes the variance in the estimated costs of all cells in order to maximize the efficiency of the pipeline by syncing the computation time across all partitions.

During the forward pass, \codename{} first divides every mini-batch of size  into  equal micro-batches, which are pipelined through the  accelerators. During the backward pass, gradients for each micro-batch are computed based on the same model parameters used for the forward pass. At the end of each mini-batch, gradients from all  micro-batches are accumulated and applied to update the model parameters across all accelerators. This sequence of operations is illustrated in Figure \ref{pipeline_cnn_cartoon}c.


If batch normalization~\cite{ioffe2015batch} is used in the network, the sufficient statistics of inputs during training are computed over each \textit{micro-batch} and over replicas if necessary~\cite{peng2017megdet}. We also track the moving average of the sufficient statistics over the entire \textit{mini-batch} to be used during evaluation.

\begin{figure*}[!t]
\begin{small}
\sbox0{\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\linewidth]{WeightUpdate.png}
    \caption{}
    \label{fig_forward_backward}
\end{subfigure}}
\sbox1{\begin{subfigure}[b]{0.61\textwidth}
    \includegraphics[width=\linewidth]{NaiveParallelism.png}
    \caption{}
    \label{fig_naive_partition}
\end{subfigure}}
\sbox2{\begin{subfigure}[b]{0.61\textwidth}
    \includegraphics[width=\linewidth]{PipelineParallelism.png}
    \caption{}
    \label{fig_pipeline_partition}
\end{subfigure}}
\caption{
(a) An example neural network with sequential layers is partitioned across four accelerators.  is the composite forward computation function of the -th cell.  is the back-propagation function, which depends on both  from the upper layer and . (b) The naive model parallelism strategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipeline parallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators to work on different micro-batches simultaneously. Gradients are applied synchronously at the end.}
\label{pipeline_cnn_cartoon}
\begin{tabular}{cc} 
& \usebox1\\
\multirow[t]{2}{*}{\usebox0}  & \usebox2\\
\end{tabular}
\end{small}
\end{figure*}

\subsection{Performance Optimization}

In order to reduce activation memory requirements, \codename{} supports re-materialization ~\cite{chen2016training}. During forward computation, each accelerator only stores output activations at the partition boundaries. During the backward pass, the -th accelerator recomputes the composite forward function . As a consequence, peak activation memory requirement is reduced to , where  is the micro-batch size and  is the number of layers per partition. In comparison, memory requirement without re-materialization and partitioning would be , since computing the gradients  requires both the upper layer gradients  and the cached activations .

As illustrated in Figure~\ref{fig_pipeline_partition}, partitioning introduces some idle time per accelerator, which we refer to as the {\it bubble} overhead. This bubble time is  amortized over the number of micro-steps . In our experiments, we found the bubble overhead to be negligible when . This is also partly because re-computation during the backward pass can be scheduled earlier, without waiting for the gradients from earlier layers.

\codename{} also introduces low communication overhead, given that we only need to pass activation tensors at the partition boundaries between accelerators. Therefore, we can achieve efficient scaling performance even on accelerators without high-speed interconnects. 

Figure~\ref{fig_pipeline_partition} assumes partitions are evenly balanced. However, memory requirements and computation flops at different layers are often quite imbalanced. In such scenarios, imperfect partitioning algorithms might lead to load imbalance. Better partitioning algorithms can potentially improve the performance over our heuristic approach.

\section{Performance Analyses}
\begin{table*}[t]
\begin{small}
\begin{center}
\caption{Maximum model size of AmoebaNet supported by \codename{} under different scenarios. Naive-1 refers to the sequential version without \codename{}. Pipeline- means  partitions with \codename{} on  accelerators. AmoebaNet-D (L, D): AmoebaNet model with  normal cell layers and filter size  .  Transformer-L: Transformer model with  layers, 2048 model and 8192 hidden dimensions. Each model parameter needs  bytes since we applied RMSProp during training.}
\label{results_memory_tab}
\begin{tabular}{lccccc}
\toprule
 NVIDIA GPUs (8GB each) & Naive-1  & Pipeline-1 & Pipeline-2 & Pipeline-4 & Pipeline-8  \\
\midrule
AmoebaNet-D (L, D) &  (18, 208) & (18, 416) &  (18, 544) & (36, 544) & (72, 512) \\
\# of Model Parameters & 82M & 318M & 542M & 1.05B & 1.8B \\
Total Model Parameter Memory & 1.05GB & 3.8GB & 6.45GB & 12.53GB & 24.62GB \\ 
Peak Activation Memory & 6.26GB & 3.46GB & 8.11GB & 15.21GB & 26.24GB\\ 
\midrule
 Cloud TPUv3 (16GB each) & Naive-1  & Pipeline-1  & Pipeline-8 & Pipeline-32 & Pipeline-128  \\
\midrule
Transformer-L & 3   & 13 & 103 & 415 & 1663 \\
\# of Model Parameters & 282.2M & 785.8M & 5.3B  & 21.0B & 83.9B \\
Total Model Parameter Memory & 11.7G & 8.8G  &  59.5G & 235.1G & 937.9G \\ 
Peak Activation Memory & 3.15G & 6.4G  &  50.9G & 199.9G & 796.1G \\ 
\bottomrule
\end{tabular}
\end{center}
\end{small}
\end{table*}


We evaluate \codename{} performance with two very different types of model architectures: an AmoebaNet~\cite{real2018regularized} convolutional model and a Transformer~\cite{vaswani2017attention}  sequence-to-sequence model. We ran experiments to study their scalability, efficiency and communication cost.



We expect both re-materialization and pipeline parallelism to benefit memory utilization and thus make fitting giant models feasible. We report the biggest model size \codename{} can support under reasonably large input size in Table~\ref{results_memory_tab}. For AmoebaNet, we ran the experiments on Cloud TPUv2s with 8GB memory per accelerator. We used a fixed input image size of  and mini-batch size of .  Without \codename{}, a single accelerator can train up to an 82M-parameter AmoebaNet, constrained by device memory limits. Owing to re-materialization in back-propagation and batch splitting, \codename{} reduces the intermediate activation memory requirements from 6.26GB to 3.46GB, 
enabling a 318M-parameter model on a single accelerator.  With model parallelism, we were able to scale AmoebaNet to 1.8 billion parameters on 8 accelerators, 25x more than what is possible without \codename{}. In this case, the maximum model size did not scale perfectly linearly due to the imbalanced distribution of model parameters over different layers in AmoebaNet.


We next trained Transformer models using Cloud TPUv3s with 16GB memory per accelerator core. We used a fixed vocabulary size of 32k, sequence length 1024 and batch size 32. Each Transformer layer has 2048 for model dimension, 8192 for feed-forward hidden dimension and 32 attention heads. We scaled the model by varying the number of layers. Re-materialization allows training a  larger model on a single accelerator. With 128 partitions, \codename{} allows scaling Transformer up to 83.9B parameters, a  increase than what is possible on a single accelerator. Different from AmoebaNet, the maximum model size scales linearly with the number of accelerators for Transformer, since each layer has the same number of parameters and input sizes. 



\begin{wraptable}{r}{0.5\textwidth}
\begin{small}
\begin{center}
\caption{Normalized training throughput using \codename{} with different \# of partitions  and different \# of micro-batches  on TPUs.  Performance increases with more micro-batches. There is an almost linear speedup with the number of accelerators for Transformer model when . Batch size was adjusted to fit memory if necessary.}
\label{results_performance_tab}
\begin{tabular}{rcccccc}
\toprule
 TPU & \multicolumn{3}{c}{AmoebaNet} & \multicolumn{3}{c}{Transformer}  \\
\midrule
  & 2 & 4 & 8 & 2 & 4 & 8\\\hline
    & 1 & 1.13 & 1.38 & 1 & 1.07 & 1.3 \\
    & 1.07 & 1.26 & 1.72 & 1.7 & 3.2 & 4.8 \\
  & 1.21 & 1.84 & 3.48 &  1.8 & 3.4 & 6.3 \\
\bottomrule
\end{tabular}
\end{center}
\end{small}
\end{wraptable}To evaluate efficiency, we report the normalized training throughput of AmoebaNet-D (18, 256) and Transformer-48 using \codename{} with different numbers of partitions and different numbers of micro-batches in Table~\ref{results_performance_tab}. Each partition is assigned to a separate accelerator.  We observe that when the number of micro-batches  is at least  the number of partitions, the bubble overhead is almost negligible. For Transformer model, there is a  speedup when it is partitioned across four times more accelerators. Furthermore, training throughput scales almost linearly with the number of devices, thanks to the computation being evenly distributed across Transformer layers. In contrast, the AmoebaNet model achieves sub-linear speedup due to its imbalanced computation distribution. When  is relatively small, the bubble overhead can no longer be negligible. When  is , there is effectively no pipeline parallelism. We observe relatively constant throughput regardless of the number of accelerators used, indicating only one device is actively computing at any given time.  


To measure the effect of communication overhead with \codename{}, we ran our experiments on a single host with multiple NVIDIA P100 GPUs but without NVLinks. Data transfer across GPUs then has to involve the relatively slow device-to-host and host-to-device transfers through PCI-E. The number of micro-batches was fixed at 32. As shown in Table~\ref{results_comm_tab}, we observe  speedup for AmoebaNet-D (18, 128) when we increase the number of partitions from  to . For the -layer Transformer,
\begin{wraptable}{r}{0.5\textwidth}
\begin{small}
\begin{center}
\caption{Normalized training throughput using \codename{} on GPUs without high-speed interconnect.}
\label{results_comm_tab}
\begin{tabular}{rcccccc}
\toprule
GPU & \multicolumn{3}{c}{AmoebaNet} & \multicolumn{3}{c}{Transformer}  \\
 \midrule
  & 2 & 4 & 8 & 2 & 4 & 8\\\hline
  & 1 & 1.7 & 2.7 &  1 & 1.8 & 3.3 \\
\bottomrule
\end{tabular}
\end{center}
\end{small}
\end{wraptable}
the speedup is . There is similar linear speedup to what we observe on TPUs where high-speed interconnects are equipped. The communication bandwidth between devices is no longer a bottleneck for model parallelism since \codename{} only transfers activation tensors at the boundaries of partitions. 


\section{Image Classification}
\label{section:imagenet_results}
As a proof of concept, we first used \codename{} to scale AmoebaNet. We increased the number of channels in an AmoebaNet and scaled the input image size to . We trained this 557-million-parameter AmoebaNet-B(18, 512) on the ImageNet 2012 dataset, using the same hyper-parameters as described in~\cite{real2018regularized}. The network was divided into 4 partitions. This single model achieves  top-1 and  top-5 validation accuracy with single-crop.

We further demonstrate the effectiveness of giant convolution networks on other image datasets through transfer learning \cite{Razavian2014,Shelhamer2017}. Specifically, we used the pre-trained ImageNet model to fine-tune on a variety of target datasets ranging from general to fine-grained classification. We changed the number of output units in the last softmax classification layer to the number of classes in the target dataset and initialized the new softmax layer randomly. All the other layers were initialized from ImageNet pre-training. Input images to the network during training were resized to , horizontally flipped randomly and augmented using cutout~\cite{devries2017improved}. Training hyper-parameters were the same as those used for ImageNet (a detailed description of our training setup is provided in supplementary material). In Table~\ref{table:cv_results}, we report the average single-crop test accuracy over 5 fine-tuning runs for each dataset. Our giant models obtain competitive results on all target datasets. For example, CIFAR-10 error rate is reduced to  and CIFAR-100 error rate to . These results corroborate the findings by Kornblith \etal~\cite{Kornblith2018}, i.e., better ImageNet models transfer better.
\begin{table*}[t]
\begin{small}
\begin{center}
\caption{Image classification accuracy using AmoebaNet-B (18, 512) first trained on ImageNet 2012 then fine-tuned on others. Please refer to the supplementary material for a detailed description of our training setup.  Our fine-tuned results were averaged across 5 fine-tuning runs.  Baseline results from  Real \etal~\cite{real2018regularized} and Cubuk \etal~\cite{cubuk2018autoaugment} were directly trained from scratch.  *Mahajan \etal's model~\cite{mahajan2018exploring} achieved  top-1 accuracy but it was pretrained on non-public Instagram data. Ngiam \etal~\cite{ngiam2018domain} achieved better results by pre-training with data from a private dataset (JFT-300M).}
\label{table:cv_results}
\begin{tabular}{lccccl}
\toprule
Dataset & \# Train & \# Test & \# Classes & Accuracy () & Previous Best ()  \\ 
\midrule
 ImageNet-2012 & 1,281,167 & 50,000 & 1000 &  &  \cite{real2018regularized} (\cite{mahajan2018exploring}) \\
 CIFAR-10  & 50,000 & 10,000 & 10  &  &  \cite{cubuk2018autoaugment}  \\ 
 CIFAR-100 &  50,000 & 10,000 & 100       &  &  \cite{cubuk2018autoaugment}  \\ 
 Stanford Cars   & 8,144 & 8,041 & 196   &  &  \cite{cubuk2018autoaugment}  \\
 Oxford Pets &3,680 & 3,369 & 37  &  &  \cite{peng2018object}  \\ 
 Food-101    & 75,750 & 25,250 & 101      &  &  \cite{Cui2018}  \\
 FGVC Aircraft  & 6,667 &3,333 & 100   &  &  \cite{Yu18}  \\
 Birdsnap & 47,386 & 2,443  & 500     &  &  \cite{wei2018mask}  \\
\bottomrule
\end{tabular}
\end{center}
\end{small}
\end{table*}



\section{Massive Massively Multilingual Machine Translation}
\label{sec:mt}
Next, we demonstrate the flexibility of \codename{} by scaling up models used for Natural Language Processing (NLP). Due to an abundance of available parallel corpora, neural machine translation (NMT) has become a benchmark task for any architecture used for NLP \cite{DBLP:journals/corr/GehringAGYD17,vaswani2017attention,shazeer2018mesh,theBestofBothWorlds18,DBLP:journals/corr/abs-1901-10430}. For this reason, we continue our \codename{} experiments on a large-scale multilingual NMT task. We use a corpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from  to  per language \cite{arivazhagan2019massively}. This dataset creates a realistic test bed for experiments on scalability by spanning a diverse set of languages from data-scarce (low-resource) to data-rich (high-resource). For the first time in machine translation, we show that a large enough NMT model can learn the mapping between more than 100 language pairs simultaneously, while achieving better than bilingual model performance for all languages. This further brings out the importance of having efficient and flexible model-parallelism tools.


Our comparison is based on the performance of a single Transformer \cite{vaswani2017attention} trained on all language pairs in this corpus. We scale the architecture along two dimensions to stress the flexibility of \codename{}: (i) along the depth by increasing the number of layers in the model and (ii) along the width by increasing the hidden dimension in the feed-forward layers and the number of attention heads (as well as \# attention channels) in multi-head attention layers similar to Shazeer \etal{} \cite{shazeer2018mesh}. Please refer to the supplementary material for a detailed description of our dataset, baselines, training configuration and optimization hyper-parameters.





We start with a standard 400M-parameter Transformer Big model, \footnote{ is a Transformer model with  encoder layers and  decoder layers, a feed-forward hidden dimension of  and  attention heads. The model dimension is fixed to .}, as described in Chen \etal{} \cite{theBestofBothWorlds18}, with a vocabulary size of 64k. In Figure~\ref{fig:m4_xx_en}, we compare its performance against a 1.3B-parameter deep model, , a 1.3B-parameter wide model, , a 3B-parameter model,  and a 6B-parameter model, . All of the models are trained on all language pairs simultaneously, using temperature-based sampling as employed for multilingual BERT\footnote{https://github.com/google-research/bert/blob/master/multilingual.md}~\cite{Devlin2018bert}. , ,  and  are partitioned over , ,  and  accelerators respectively.

From Figure~\ref{fig:m4_xx_en}, we can observe that increasing the model capacity from 400M to 1.3B parameters significantly improves performance across all languages. Scaling up the model from 1.3B parameters to 6B parameters shows further improvement, especially for high-resource languages, although diminishing returns can be observed when scaling the model from 1.3B to 3B and 6B parameters.


Below we discuss some of our empirical findings based on these large-scale experiments. \begin{figure*}[h!]
\begin{center}
\caption{Translation quality across all languages with increasing multilingual model capacity. Languages are arranged in the order of decreasing training dataset size from left to right. , depicts the performance of a Transformer with  encoder and  decoder layers, a feed-forward hidden dimension of  and  attention heads. We notice that increasing the model capacity, from 400M params () to 1.3B (), and further, to 6B (), leads to significant quality improvements across all languages. We also notice huge quality improvements for low-resource languages (right side of the plot), when compared against bilingual baselines, highlighting the significant transfer gains resulting from training a multilingual model.}
\label{fig:m4_xx_en}
\includegraphics[width=\textwidth]{bleu_xx_en.png}
\end{center}
\end{figure*}

\textbf{Depth-Width Trade-off:} We study the trade-off between depth and width in our multilingual setup and compare the performance of 1.3B wide model  and 1.3B deep model . 
While the quality of these two models on high-resource languages (left of Figure~\ref{fig:m4_xx_en}) is very similar, the deeper model outperforms by huge margins on low-resource languages, suggesting that increasing model depth might be better for generalization. Further, the quality improvements for low-resource languages (right side of Figure~\ref{fig:m4_xx_en}), when comparing the 1.3B deep model against the 400M model, are almost as large as the improvements for high-resource languages, indicating that increasing depth might potentially increase the extent of transfer to low-resource tasks.

\textbf{Trainability Challenges with Deep Models:} Although depth increases the representational capacity of neural networks, it also complicates the optimization problem. In our large-scale experiments, we encountered severe trainability issues arising from a combination of sharp activations (positive kurtosis) and dataset noise. We observed that after training for a few thousand steps, the model predictions would become extremely peaky and vulnerable to noise, which frequently resulted in non-finite or large gradients that eventually destroyed the learning progress. To counter these problems, we apply two methods: (i) Following Zhang \etal{} \cite{zhang2019fixup}, we scale down the initialization of all transformer feed-forward layers by the number of layers. (ii) We clip the logit predictions (softmax pre-activations) whenever their magnitude exceeds a certain value. 
 \begin{wraptable}{r}{0.4\textwidth} 
\begin{center}
\caption{The Effect of Batch Size}
\label{tab:bsz}
\begin{tabular}{l|ccc}
\toprule
    Batch Size     & 260K  & 1M    & 4M    \\ \hline
    BLEU       & 30.92 & 31.86 & 32.71 \\
    Loss (NLL) & 2.58  & 2.51  & 2.46 \\    
\bottomrule    
\end{tabular}
\end{center}
\end{wraptable}
A combination of these two approaches allows us to mitigate the training instability posed by scaling model depth.

\textbf{Large Batches:} Due to its simplicity, data parallelism is the dominant approach to scale neural network training\cite{DBLP:journals/corr/KeskarMNST16, DBLP:journals/corr/abs-1711-00489}. We test the limits of large-batch training by significantly increasing the batch size used for standard Transformer Big training. Starting from 260K tokens per batch, we increase the effective batch size to 4M and observe the validation loss and BLEU scores on the high-resource language pair, German-English (similar trend can also be observed for other language pairs). Optimization parameters used here are identical to those for previous experiments. 
To our knowledge, 4M tokens per batch is the largest batch size that has ever been used in literature to date for training NMT models ~\cite{ott-etal-2018-scaling}. Table~\ref{tab:bsz} shows that both metrics improve significantly as we increase the batch size. We believe further increasing batch size can potentially yield more improvement.




\section{Design Features and Trade-Offs}


Several approaches have been proposed to enable efficient large-scale model parallelism. However, each approach chooses its own set of trade-offs, making it suitable for scaling specific architectures under particular hardware constraints. Here we highlight the various design choices and trade-offs involved with several model-parallelism approaches, and how they compare with \codename{} in terms of flexibility, scalability and efficiency under various hardware constraints and architecture variants.



The core idea of model parallelism involves partitioning a network into different computational units, which are then placed on different devices~\cite{krizhevsky2014one, lee2014model, mirhoseini2017device,dean12disbelief}. Conceptually this supports scaling a large spectrum of models to huge capacities. However these approaches typically suffer from low hardware utilization and device communication bottlenecks. Single Program Multiple Data (SPMD) and pipeline parallelism have been proposed as solutions to counter these challenges.


Mesh-Tensorflow~\cite{shazeer2018mesh} follows the SPMD paradigm, which extends the Single Instruction Multiple Data (SIMD) approach used for data parallelism to other tensor dimensions. SPMD allows splitting every computation across multiple devices, allowing the user to scale the size of individual matrix multiplications (and thus, the model parameters of individual layers) linearly with the number of accelerators. However, this also introduces high communication overhead between the accelerators due to an abundance of AllReduce-like operations used to combine the outputs of each parallelized matrix multiplication. This limits the applicability of the approach to scenarios where accelerators are connected with high speed interconnects. Further, SPMD limits the type of operations that can be efficiently scaled, restricting its use to a specific set of network architectures and machine learning tasks. For example, splitting along the channel dimension of convolution layers under this paradigm is not efficient given that channels are effectively fully connected, whereas splitting along the spatial dimension requires sophisticated techniques for the halo regions. While SPMD allows scaling the model depth by making each operation smaller, it requires splitting each layer over a larger number of accelerators, which in turn further increases the communication overhead across devices.

Other approaches have attempted to utilize pipeline-parallelism-based approaches to scale neural networks~\cite{petrowski93perf,wu2016google}. The most recent iteration of pipeline parallelism applied to neural network training is PipeDream~\cite{harlap2018pipedream}, which targets reducing the communication overhead for parameter servers~\cite{li2014scaling}. PipeDream pipelines the execution of forward passes and intersperses them with backward passes in an attempt to maximize hardware utilization. This design suffers from weight staleness introduced by asynchronous backward updates. To avoid optimization issues stemming from the weight staleness, PipeDream requires maintaining multiple versioned copies of the model parameters on each accelerator in order to compute the gradient updates accurately, preventing users from scaling to bigger models.

\codename{} introduces a new brand of pipeline parallelism that pipelines the execution of \textit{micro-batches} before applying a single synchronous gradient update for the entire \textit{mini-batch}. Our novel batch-splitting pipeline parallelism algorithm, when combined with re-materialization, allows scaling to a large number of micro-batches. This minimizes the \textit{bubble} overhead without the need for asynchronous gradient updates. 
\codename{} enables the user to scale model size linearly with the number of accelerators used. Unlike SPMD, pipeline parallelism introduces little additional communication overhead when scaling the model. Inter-device communication only takes place at partition boundaries for every micro-batch and the introduced communication overhead is marginal, extending the utility of \codename{} to situations where high-speed device interconnects are not available. However, \codename{} currently assumes that a single layer fits within the memory requirements of a single accelerator\footnote{One possible way around this limitation is splitting a single matrix-multiplication into smaller ones and spreading them sequentially across multiple layers.
}. Additionally, micro-batch splitting requires complicated strategies to support layers that require computations across the batch (for example, BatchNorm uses statistics over the micro-batch during training, but accumulates mini-batch statistics for evaluation).


\section{Conclusion}
In this work, we introduce \codename{}, a scalable model-parallelism library for training giant neural networks. We propose and implement a novel batch-splitting pipeline-parallelism algorithm that uses synchronous gradient updates, allowing model parallelism with high hardware utilization and training stability. We leverage \codename{} to train large-scale convolutional and transformer-based models and demonstrate strong empirical results on both image classification and multilingual machine translation. We highlight three key attributes of the \codename{} library: 1) Efficiency: Using a novel batch-splitting pipelining algorithm, \codename{} achieves almost linear speedup with the number of devices. 2) Flexibility: \codename{} supports any deep network that can be represented as a sequence of layers. 3) Reliability: \codename{} utilizes synchronous gradient descent and guarantees consistent training regardless of the number of partitions. 

{\small
}
\begin{thebibliography}{10}

\bibitem{DBLP:journals/corr/abs-1708-00107}
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher.
\newblock Learned in translation: Contextualized word vectors.
\newblock {\em CoRR}, abs/1708.00107, 2017.

\bibitem{peters-etal-2018-deep}
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock In {\em ACL}, 2018.

\bibitem{Devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}. IEEE, 2009.

\bibitem{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, and et~al.
\newblock Going deeper with convolutions.
\newblock In {\em CVPR}, pages 1--9, 2015.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em CVPR}, pages 2818--2826, 2016.

\bibitem{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em CVPR}, 2017.

\bibitem{hu2017squeeze}
Jie Hu, Li~Shen, and Gang Sun.
\newblock Squeeze-and-excitation networks.
\newblock {\em CVPR}, 2018.

\bibitem{zoph2017learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock {\em CVPR}, 2018.

\bibitem{real2018regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock {\em arXiv preprint arXiv:1802.01548}, 2018.

\bibitem{griewank2000algorithm}
Andreas Griewank and Andrea Walther.
\newblock Algorithm 799: revolve: an implementation of checkpointing for the
  reverse or adjoint mode of computational differentiation.
\newblock {\em ACM Transactions on Mathematical Software (TOMS)}, 26(1):19--45,
  2000.

\bibitem{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock {\em arXiv preprint arXiv:1604.06174}, 2016.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Neurips}, pages 5998--6008, 2017.

\bibitem{shen2019lingvo}
Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, Mia~Xu Chen, Ye~Jia,
  Anjuli Kannan, Tara Sainath, Yuan Cao, Chung-Cheng Chiu, et~al.
\newblock Lingvo: a modular and scalable framework for sequence-to-sequence
  modeling.
\newblock {\em arXiv preprint arXiv:1902.08295}, 2019.

\bibitem{jia2014caffe}
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross
  Girshick, Sergio Guadarrama, and Trevor Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock In {\em Proceedings of the 22nd ACM international conference on
  Multimedia}, pages 675--678. ACM, 2014.

\bibitem{chen2015mxnet}
Tianqi Chen, Mu~Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
  Bing Xu, Chiyuan Zhang, and Zheng Zhang.
\newblock Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock {\em arXiv preprint arXiv:1512.01274}, 2015.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em ICML}, 2015.

\bibitem{peng2017megdet}
Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu,
  and Jian Sun.
\newblock Megdet: A large mini-batch object detector.
\newblock {\em CVPR}, 7, 2017.

\bibitem{Razavian2014}
Ali~Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.
\newblock Cnn features off-the-shelf: An astounding baseline for recognition.
\newblock In {\em CVPR Workshops}, pages 512--519, 2014.

\bibitem{Shelhamer2017}
Evan Shelhamer, Jonathan Long, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 39(4):640--651,
  2017.

\bibitem{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{Kornblith2018}
Simon Kornblith, Jonathon Shlens, and Quoc~V. Le.
\newblock Do better imagenet models transfer better?
\newblock {\em CoRR}, abs/1805.08974, 2018.

\bibitem{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock {\em arXiv preprint arXiv:1805.09501}, 2018.

\bibitem{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens van~der Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock {\em ECCV}, 2018.

\bibitem{ngiam2018domain}
Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc Le, and
  Ruoming Pang.
\newblock Domain adaptive transfer learning.
\newblock 2018.

\bibitem{peng2018object}
Yuxin Peng, Xiangteng He, and Junjie Zhao.
\newblock Object-part attention model for fine-grained image classification.
\newblock {\em IEEE Transactions on Image Processing}, 27(3):1487--1500, 2018.

\bibitem{Cui2018}
Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie.
\newblock Large scale fine-grained categorization and domain-specific transfer
  learning.
\newblock In {\em CVPR}, 2018.

\bibitem{Yu18}
Fisher Yu, Dequan Wang, and Trevor Darrell.
\newblock Deep layer aggregation.
\newblock In {\em CVPR}, 2018.

\bibitem{wei2018mask}
Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu, and Chunhua Shen.
\newblock Mask-cnn: Localizing parts and selecting descriptors for fine-grained
  bird species categorization.
\newblock {\em Pattern Recognition}, 76:704--714, 2018.

\bibitem{DBLP:journals/corr/GehringAGYD17}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N. Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock {\em CoRR}, abs/1705.03122, 2017.

\bibitem{shazeer2018mesh}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn
  Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,
  et~al.
\newblock Mesh-tensorflow: Deep learning for supercomputers.
\newblock In {\em Neurips}, pages 10414--10423, 2018.

\bibitem{theBestofBothWorlds18}
Mia~Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey,
  George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui
  Wu, and Macduff Hughes.
\newblock The best of both worlds: Combining recent advances in neural machine
  translation.
\newblock {\em CoRR}, abs/1804.09849, 2018.

\bibitem{DBLP:journals/corr/abs-1901-10430}
Felix Wu, Angela Fan, Alexei Baevski, Yann~N. Dauphin, and Michael Auli.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock {\em CoRR}, abs/1901.10430, 2019.

\bibitem{arivazhagan2019massively}
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson,
  Maxim Krikun, Mia~Xu Chen, Yuan Cao, George Foster, Colin Cherry, et~al.
\newblock Massively multilingual neural machine translation in the wild:
  Findings and challenges.
\newblock {\em arXiv preprint arXiv:1907.05019}, 2019.

\bibitem{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock {\em arXiv preprint arXiv:1901.09321}, 2019.

\bibitem{DBLP:journals/corr/KeskarMNST16}
Nitish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and
  Ping~T.P. Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em CoRR}, abs/1609.04836, 2016.

\bibitem{DBLP:journals/corr/abs-1711-00489}
Samuel~L. Smith, Pieter{-}Jan Kindermans, and Quoc~V. Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock {\em CoRR}, abs/1711.00489, 2017.

\bibitem{ott-etal-2018-scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli.
\newblock Scaling neural machine translation.
\newblock In {\em Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 1--9, Belgium, Brussels, October 2018. Association
  for Computational Linguistics.

\bibitem{krizhevsky2014one}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1404.5997}, 2014.

\bibitem{lee2014model}
Seunghak Lee, Jin~Kyu Kim, Xun Zheng, Qirong Ho, Garth~A Gibson, and Eric~P
  Xing.
\newblock On model parallelization and scheduling strategies for distributed
  machine learning.
\newblock In {\em Neurips}, pages 2834--2842, 2014.

\bibitem{mirhoseini2017device}
Azalia Mirhoseini, Hieu Pham, Quoc~V Le, Benoit Steiner, Rasmus Larsen, Yuefeng
  Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean.
\newblock Device placement optimization with reinforcement learning.
\newblock {\em arXiv preprint arXiv:1706.04972}, 2017.

\bibitem{dean12disbelief}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang,
  Quoc~V. Le, and Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, {\em Neurips 25}, pages 1223--1231. Curran Associates, Inc., 2012.

\bibitem{petrowski93perf}
A.~Petrowski, G.~Dreyfus, and C.~Girault.
\newblock Performance analysis of a pipelined backpropagation parallel
  algorithm.
\newblock {\em IEEE Transactions on Neural Networks}, 4(6):970--981, Nov 1993.

\bibitem{wu2016google}
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V Le, Mohammad Norouzi, Wolfgang
  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock {\em Transactions of the Association for Computational Linguistics,},
  2017.

\bibitem{harlap2018pipedream}
Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil
  Devanur, Greg Ganger, and Phil Gibbons.
\newblock Pipedream: Fast and efficient pipeline parallel dnn training.
\newblock {\em arXiv preprint arXiv:1806.03377}, 2018.

\bibitem{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em OSDI}, volume~14, pages 583--598, 2014.

\end{thebibliography}
 

\end{document}
