

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{array}
\usepackage{booktabs}
\usepackage[scr=boondox]{mathalfa}
\usepackage[mathcal]{euscript}
\usepackage{dsfont}
\usepackage{subfig}
\usepackage{makeidx}


\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{nth}
\usepackage{dblfloatfix}
\usepackage{lipsum}\usepackage{multicol}\usepackage[export]{adjustbox}
\usepackage{textcomp}
\usepackage{graphicx}  \renewcommand{\algorithmicforall}{\textbf{for each}}
\usepackage{algcompatible}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \renewcommand{\textrightarrow}{}
\usepackage{pdfpages}
\newcommand{\subf}[2]{{\small\begin{tabular}[t]{@{}c@{}}
  #1\\#2
  \end{tabular}}}
\newcommand{\highlight}[1]{\colorbox{yellow}{#1}}


\urlstyle{rm}
\def\UrlFont{\rm}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}



\def\rvz{{\mathbf{z}}}
\def\rvx{\mathbf{x}}
\def\rvl{{\rm l}}
\def\sR{{\mathbb{R}}}
\renewcommand{\algorithmicforall}{\textbf{for each}}
\usepackage{yfonts}
\DeclareMathOperator*{\argminB}{argmin} 

\renewcommand\topfraction{0.85}
\renewcommand\bottomfraction{0.85}
\renewcommand\textfraction{0.1}
\renewcommand\floatpagefraction{0.85}

\newcommand{\var}[1]{\text{\texttt{#1}}}
\newcommand{\func}[1]{\text{\textsl{#1}}}

\usepackage[scr=boondox]{mathalfa}

\makeatletter
\newcommand{\printfnsymbol}[1]{\textsuperscript{\@fnsymbol{#1}}}
\makeatother


\usepackage[noadjust]{cite}
\renewcommand{\citepunct}{,\penalty\citepunctpenalty\,}
\renewcommand{\citedash}{--}



\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{6124}  \title{Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search}
\titlerunning{UDA for Semantic Segmentation of NIR Images through GLS}
\author{Prashant Pandey\thanks{equal contribution}\orcidID{0000-0002-6594-9685} \and
Aayush Kumar Tyagi\index{Tyagi, Aayush Kumar}\printfnsymbol{1}\orcidID{0000-0002-3615-7283} \and
Sameer Ambekar\orcidID{0000-0002-8650-3180} \and \\ Prathosh AP\orcidID{0000-0002-8699-5760}}
\authorrunning{P. Pandey et al.}
\institute{Indian Institute of Technology Delhi \\
\email{getprashant57@gmail.com, aayush16081@iiitd.ac.in,
ambekarsameer@gmail.com,
prathoshap@iitd.ac.in}}
\maketitle
\begin{abstract}
Segmentation of the pixels corresponding to human skin is an essential first step in multiple applications ranging from surveillance to heart-rate estimation from remote-photoplethysmography. However, the existing literature considers the problem only in the visible-range of the EM-spectrum which limits their utility in low or no light settings where the criticality of the application is higher. To alleviate this problem, we consider the problem of skin segmentation from the Near-infrared images. However, Deep learning based state-of-the-art segmentation techniques demands large amounts of labelled data that is unavailable for the current problem. Therefore we cast the skin segmentation problem as that of target-independent Unsupervised Domain Adaptation (UDA) where we use the data from the Red-channel of the visible-range to develop skin segmentation algorithm on NIR images. We propose a method for target-independent segmentation where the `nearest-clone' of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. We prove the existence of `nearest-clone' and propose a method to find it through an  optimization algorithm  over the latent space of a Deep generative model based on variational inference. We demonstrate the efficacy of the proposed method for NIR skin segmentation over  the  state-of-the-art UDA  segmentation  methods on  the two newly  created skin segmentation datasets in NIR domain despite not having access to the target NIR data. Additionally, we report state-of-the-art results for adaption from Synthia to Cityscapes  which is a popular setting in Unsupervised Domain Adaptation for semantic segmentation. The code and datasets are available at https://github.com/ambekarsameer96/GLSS.

\keywords{Unsupervised Domain Adaptation, Semantic segmentation, Near IR Dataset, VAE }
\end{abstract}

\section{Introduction}
\subsection{Background}
Human skin segmentation is the task of finding pixels corresponding to skin from images or videos. It serves as a necessary pre-processing step for multiple applications like video surveillance,
people tracking, human computer interaction, face detection and recognition, facial gesture detection and monitoring heart rate and respiratory rate \cite{prathosh2017estimation,mahmoodi2017high,chen2016skin,mahmoodi2016comprehensive} using remote photoplethysmography.  Most of the research efforts on skin detection have focused on visible spectrum images because of the challenges that it poses including, illumination change, ethnicity change and presence of background/clothes similar to skin colour. These factors adversely affect the applications where skin is used as conjugate information. Further, the algorithms that rely on visible spectrum images cannot be employed in the low/no light conditions especially during night times where the criticality of the application like human detection is higher. 
These problems which are  encountered in visible spectrum domain can be overcome by considering the images taken in the Near-infrared  (NIR) domain \cite{kong2005recent} or hyper spectral imaging \cite{pan2003face}. The information about the skin pixels is invariant of factors such as illumination conditions, ethnicity etc., in these domains. Moreover, most of the surveillance cameras that are used world-wide are NIR imaging devices. Thus, it is meaningful to pursue  the endeavour of detecting the skin pixels from the NIR images. 

\subsection{Problem setting and contributions}
The task of detection of skin pixels from an image is typically cast as a segmentation problem. Most of the classical approaches relied on the fact that the skin-pixels have a distinctive color pattern \cite{huynh2002skin,erdem2011combining} compared to other objects. In recent years, harnessing the power of Deep learning, skin segmentation problem has been dealt with using deep neural networks that show significant performance enhancement over the traditional methods \cite{long2015fully,ronneberger2015u,jones2002statistical}, albeit generalization across different illuminations still remains a challenge. 
While there exists sufficient literature on skin segmentation in the visible-spectrum, there is very little work done on segmenting the skin pixels in the NIR domain. Further, all the state-of-the-art Deep learning based segmentation algorithms demand large-scale annotated datasets to achieve good performance which is available in the case of visible-spectrum images but not the NIR images. Thus, building a fully-supervised skin segmentation network from scratch is not feasible for the NIR images because of the unavailability of the large-scale annotated data. However, the underlying concept of `skin-pixels' is the same across the images irrespective of the band in which they were captured. Additionally, the NIR and the Red-channel of the visible-spectrum are close in terms of their wavelengths.  Owing to these observations, we pose the following question in this paper - Can the labelled data (source) in the visible-spectrum (Red-channel) be used to perform skin segmentation in the NIR domain (target) \cite{pandey2020guided}? 
\par We cast the problem of skin segmentation from NIR images as a target-independent Unsupervised Domain Adaptation (UDA) task \cite{pandey2020target} where we consider the Red-channel of the visible-spectrum images as the source domain and NIR images as the target domain. The state-of-the-art UDA techniques demand access to the target data, albeit unlabelled, to adapt the source domain features to the target domain. In the present case, we do not assume existence of any data from the target domain, even unlabelled. This is an important desired attribute which ensures that a model trained on the Red-channel does not need any retraining with the data from NIR domain. The core idea is to sample the `nearest-clone' in the source domain to a given test image from the target domain. This is accomplished through a simultaneous sampling-cum-optimization procedure using a latent-variable deep neural generative network learned on the source distribution. Thus, given a target sample, its `nearest-clone' from the source domain is sampled and used as a proxy in the segmentation network trained only on the samples of the source domain. Since the segmentation network performs well on the source domain, it is expected to give the correct segmentation mask on the `nearest-clone' which is then assigned to the target image. Specifically, the core contributions of this work are listed as follows: 

\begin{enumerate}
    \item We cast the problem of skin segmentation from NIR images as a UDA segmentation task where we use the data from the Red-channel of the visible-range of the EM-spectrum to develop skin segmentation algorithm on NIR images.
    \item We propose a method for target-independent segmentation where the `nearest-clone' of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. 
    \item We theoretically prove the existence of the `nearest-clone' given that it can be sampled from the source domain with infinite data points. 
    \item We develop a joint-sampling and optimization algorithm using variational inference based generative model to search for the `nearest-clone' through implicit sampling in the source domain. 
    \item We demonstrate the efficacy of the proposed method for NIR skin segmentation over the state-of-the-art UDA segmentation methods on the two newly created skin segmentation datasets in NIR domain. The proposed method is also shown to reach SOTA performance on standard segmentation datasets like Synthia \cite{ros2016synthia} and Cityscapes \cite{cordts2016cityscapes}. 
\end{enumerate}
\section{Related Work}
In this section, we first review the existing methods for skin segmentation in the visible-range followed by a review of UDA methods for segmentation. 
\subsection{Skin Segmentation in Visible-range}
Methods for skin segmentation can be grouped into three categories, i.e. (i) Thresholding based methods \cite{kovac2003human, erdem2011combining, qiang2010robust}, (ii) Traditional machine learning techniques to learn a skin color model \cite{liu2010robust ,zaidan2014multi}, (iii) Deep learning based methods to learn an end-to-end model for skin segmentation \cite{al2013impact , wu2012skin , seow2003neural , chen2016skin , he2019semi}.  
The thresholding based methods focus  on  defining  a  specified  range  in  different color  representation spaces like (HSV)\cite{moallem2011novel} and orthogonal color space (YCbCr)\cite{hsu2002face,brancati2017human} to differentiate skin pixels. Traditional machine learning can be further divided into pixel based and region based methods. In pixel based methods, each pixel is classified as skin or non-skin without considering the neighbours \cite{taqa2010increasing} whereas region based approaches use spatial information to identify similar regions \cite{chen2007region}. 
In recent years, Fully convolutional neural networks (FCN) are employed to solve the problem \cite{long2015fully}.
\cite{ronneberger2015u} proposed a UNet architecture, consisting of an encoder-decoder structure with backbones like InceptionNet\cite{szegedy2016rethinking} and ResNet \cite{he2016deep}.
Holistic skin segmentation \cite{dourado2019domain} combine inductive transfer learning and UDA. They term this technique as cross domain pseudo-labelling and use it in an iterative manner  to  train and fine tune the model on the target domain. 
\cite{he2019semi} propose mutual guidance to improve skin detection with the usage of body masks as guidance. They use dual task neural network for joint detection with shared encoder and two decoders for detecting skin and body simultaneously. While all these methods offer different advantages, they do not generalize to low-light settings with NIR images, which we aim to solve through UDA. 
\subsection{Domain Adaptation for semantic segmentation}
Unsupervised Domain Adaptation aims to improve the performance  of deep neural networks on a target domain, using labels only from a source domain. UDA for segmentation task can be grouped into following  categories:
\subsubsection{Adversarial training based methods:}
These methods use the principles of adversarial learning \cite{hoffman2016fcns}, which generally consists of two networks. One predicts the segmentation mask of the input image coming from either source or target distribution while the other network acts as discriminator which tries to predict the domain of the images.
AdaptSegNet \cite{tsai2018learning} exploits structural similarity between the source and target domains in a multi-level adversarial network  framework. 
ADVENT \cite{vu2018advent} introduce entropy-based loss to directly penalize low-confident predictions on target domain. Adversarial training is used for structural adaptation of the target domain to the source domain.
CLAN \cite{luo2019taking}  considers category-level joint distribution and aligns each class with an adaptive adversarial loss. They reduce the weight of the adversarial loss for category-level aligned features while increasing the adversarial force for those that are poorly aligned. DADA \cite{vu2019dada} uses the geometry of the scene by simultaneously aligning the segmentation and depth-based information of source and target domains using adversarial training.
\subsubsection{Feature-transformation based methods:}
These methods are based on the idea of learning image-level or feature-level transformations between the source and the target domains. CyCADA \cite{Hoffman_cycada2017} adapts between domains using
both generative image space alignment and latent
representation space alignment. Image level adaptation is achieved with cycle loss, semantic consistency loss and pixel-level GAN loss while feature level adaptation employs feature-level GAN loss and task loss between true and predicted labels.
DISE \cite{chang2019all} aims to discover a domain-invariant structural feature by learning to  disentangle domain-invariant structural information of an image from its domain-specific texture information.
BDL \cite{li2019bidirectional}  involves  two  separated  modules a) image-to-image translation model b) segmentation adaptation  model, in two directions namely `translation-to-segmentation' and `segmentation-to-translation'. 
\section{Proposed method}
Most of the UDA methods assume access to the unlabelled target data which may not be available at all times. In this work, we propose a UDA segmentation technique by learning to find a data point from the source that is arbitrarily close (called the `nearest-clone') to a given target point so that it can used as a proxy in the segmentation network trained only on the source data. In the subsequent sections, we describe the methodology used to find the `nearest-clone' from the source distribution to a given target point. 
\subsection{Existence of nearest source point}
To start with, we show that for a given target data point, there exists a corresponding source data point,  that is arbitrarily close to, provided that infinite data points can be sampled from the source distribution. Mathematically, let  denotes the source distribution and  denotes any target distribution that is similar but not exactly same as  (Red-channel images are source and NIR images are target). Let the underlying random variable on which   and  are defined form a separable metric space  with  being some distance metric. Let  be i.i.d points drawn from  and  be a point from . With this, the following lemma shows the existence of the `nearest-clone'. 
\begin{lemma}
If  is the point such that , as  (in ),  converges to   with probability . 
\end{lemma}
\begin{proof}
Let  be a closed ball of radius  around  under the metric . Since  is a separable metric space \cite{cover1967nearest},  

With this, for any , the probability that  none of the points in  are within the ball  of radius  is:
Therefore, the probability of  (the closest point to ) lying within  is: 

Thus, given any infinitesimal , with probability ,  (`nearest-clone') that is within   distance from  as \qed
 \end{proof}
\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth,height=.4\textwidth]{PlotNetwork.pdf}
    \caption{VAE training. Edges of an input image are concatenated with the features from the decoder . Encoder and decoder parameters ,  are optimized with reconstruction loss , KL-divergence loss  and perceptual loss . Perceptual model  is trained on source samples. A zero mean and unit variance isotropic Gaussian prior is imposed on the latent space . }
    \label{fig:architecture}
\end{figure*} 
While Lemma 1 guarantees the existence of a `nearest-clone', it demands the following two conditions: 
\begin{itemize}
\item It should be possible to sample infinitely from the source distribution .
\item It should be possible to search for the `nearest-clone' in the , for a target sample  under the distance metric .
\end{itemize}
We propose to employ Variational Auto-encoding based sampling models on the source distribution to simultaneously sample and find the `nearest-clone' through an optimization over the latent space.  
\subsection{Variational Auto-Encoder for source sampling}
Variational Auto-Encoders (VAEs) \cite{kingma2013auto} are a class of latent-variable generative models that are based on the principles of variational inference where the variational distribution,  is used to approximate the intractable true posterior . The log-likelihood of the observed data is decomposed into two terms, an irreducible non-negative KL-divergence between   and   and the Evidence Lower Bound (ELBO) term which is given by Eq. \ref{elbo1}.
\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.69\textwidth,height=.42\textwidth]{inf_cond2.pdf}
\caption{Latent Search procedure during inference with GLSS. The latent vector  is initialized with a random sample drawn from . Iterations over the latent space  are performed to minimize the  loss between the input target image  and the predicted target image  (blue dotted lines). After convergence of  loss, the optimal latent vector , generates the closest clone  which is used to predict the mask of  using the segmentation network .}
\label{fig:LatentSearch}
\end{center}
\end{figure}
 where, 

The non-negative KL-term in Eq. \ref{elbo1} is irreducible and thus,   serves as a lower bound on the data log-likelihood which is maximized in a VAE by parameterizing  and  using probabilistic encoder  (that outputs the parameters  and  of a distribution) and decoder  neural networks. The latent prior  is taken to be arbitrary prior on  which is usually a  mean and unit variance Gaussian distribution. After training, the decoder network is used as a sampler for  in a two-step process: (i) Sample , (ii) Sample  from .
The likelihood term in Eq. \ref{elbo1} is approximated using norm based losses and it is known to result in blurry images. Therefore, we use the perceptual loss \cite{johnson2016perceptual} along with the standard norm based losses. Further, since the edges in images are generally invariant across the source and target domains, we extract edge of the input image and use it in the decoder of the VAE via a skip connection, as shown in Fig. \ref{fig:architecture}. This is shown to reduce the blur in the generated images. Fig. \ref{fig:architecture} depicts the entire VAE architecture used for training on the source data. 

\subsection{VAE Latent Search for finding the `nearest-clone'}
As described, the objective of the current work is to search for the nearest point in the source distribution, given a sample from target distribution. The decoder  of a VAE trained on the source distribution , outputs a new sample using the Normally distributed latent sample as input. That is, 


With this, our goal is to find the `nearest-clone' to a given target sample. That is, given a , find  as follows:



Since  is pre-defined and  is a deep neural network, finding  can be cast as an optimization problem over  with minimization of  as the objective. Mathematically, 




The optimization problem is Eq. \ref{obj} can be solved using gradient-descent based techniques on the decoder network   are the parameters of the decoder network trained only on the source samples   with respect to . This implies that given any input target image, the optimization problem in Eq. \ref{obj} will be solved to find its `nearest-clone' in the source distribution which is used as a proxy in the segmentation network trained only on . We call the iterative procedure of finding   through optimization using  as the Latent Search (LS). Finally, inspired by the observations made in \cite{hore2010image}, we propose to use structural similarity index (SSIM) \cite{wang2004image} based loss  for  to conduct the Latent Search. Unlike norm based losses, SSIM loss helps in preservation of structural information which is needed for segmentation. Fig. \ref{fig:LatentSearch} depicts the complete inference procedure employed in the proposed method named as the Generative Latent Search for Segmentation (GLSS).
\section{Implementation Details}
\subsection{Training}
Architectural details of the VAE used are shown in Fig. \ref{fig:architecture}. Sobel operator is used to extract the edge information of the input image which is concatenated with one of the layers of the Decoder via a \textit{tanh} non linearity as shown in Fig. \ref{fig:architecture}. The VAE is trained using (i) the Mean squared error reconstruction loss  and KL divergence  and (ii) the perceptual loss  for which the features are extracted from the  layer (a hyper-parameter) of the DeepLabv3+ \cite{deeplabv3plus2018} (Xception backbone \cite{chollet2017xception}) and the UNet \cite{ronneberger2015u} (EfficientNet backbone \cite{tan2019efficientnet}) segmentation networks. The segmentation network ( in Fig. \ref{fig:LatentSearch}) is either DeepLabv3+ or UNet  and is trained on the source dataset. For traning , we use combination of binary cross-entropy () and dice coefficient loss () for UNet with RMSProp (lr = 0.001) as optimizer and  binary focal loss () \cite{lin2017focal} with  = 2.0,  = 0.75 and RMSProp (lr=0.01) as optimizer for DeepLabv3+. 
For the VAE , the hidden layers of Encoder and Decoder networks use Leaky ReLU and \textit{tanh} as activation functions with the dimensionality of the latent space being 64. VAE is trained using standard gradient descent procedure with RMSprop (=0.0001) as optimizer. We train VAE for 100 to 150 epochs with batchsize 64. 
\subsection{Inference}
Once the VAE is trained on the source dataset, given an image  from the target distribution,
the Latent Search algorithm searches for an optimal latent vector  that generates its `nearest-clone'  from . The search is performed by minimizing the SSIM loss  between the input target image  and the VAE-reconstructed target image, using a gradient-descent based optimization procedure such as  ADAM \cite{kingma2014adam} with ,  and . The Latent Search is performed for  (hyper-parameter) iterations over the latent space of the source for a given target image. Finally, the segmentation mask for the input target sample is assigned the same as the one given by the segmentation network , which is trained on source data, on the `nearest-clone' . 
Latent Search for one sample takes roughly 450 ms and 120 ms on SNV and Hand Gesture datasets respectively. Please refer supplementary material for more details.
\section{Experiment and Results}
\subsection{Datasets}
We consider the Red-channel of the COMPAQ dataset \cite{jones2002statistical} as our source data. It consists of 4675 RGB images with the corresponding annotations of the skin. 
Since there is no publicly available dataset with NIR images and corresponding skin segmentation labels, we create and use two NIR datasets (publicly available) as targets. The first one named as the Skin NIR Vision (SNV), consists of 800 images of multiple human subjects taken in different scenes, captured using a WANSVIEW 720P camera in the night-vision mode. The captured images cover wide range of scenarios for skin detection task like presence of multiple humans, backgrounds similar to skin color, different illuminations, saturation levels and different postures of subjects to ensure diversity.
Additionally, we made use of the publicly available multi-modal Hand Gesture dataset\footnote{\url{https://www.gti.ssr.upm.es/data/MultiModalHandGesture_dataset}} as another target dataset which we call as Hand Gesture dataset. This dataset covers 16 different hand-poses of multiple subjects. We randomly sampled  500 images in order to cover illumination changes and diversity in hand poses.
Both SNV and Hand Gesture datasets are manually annotated with precision. 
\subsection{Benchmarking on SNV and Hand Gesture datasets}
To begin with, we performed supervised segmentation experiments on both SNV and Hand Gesture datasets with 80-20 train-test split using SOTA segmentation algorithms.
\bgroup
\setlength{\tabcolsep}{18pt}
\begin{table}[h]
\caption{Benchmarking Skin NIR Vision (SNV) dataset and Hand Gesture dataset on standard segmentation architectures with 80-20 train-test split.}
\begin{center}
\scalebox{0.77}{
\begin{tabular}{lcccc}
    \toprule
        \multicolumn{1}{c}{} & \multicolumn{2}{c}{SNV} & \multicolumn{2}{c}{Hand Gesture}\\
        Method&IoU&Dice&IoU&Dice
        \\
    \midrule
    FPN \cite{lin2017feature} &0.792&0.895&0.902&0.950\\
    UNet \cite{ronneberger2015u}&0.798&0.890&0.903&0.950\\
    DeepLabv3+ \cite{deeplabv3plus2018} & 0.750 & 0.850 & 0.860 & 0.924\\
    Linknet \cite{chaurasia2017linknet} & 0.768 & 0.872 & 0.907 & 0.952\\
    PSPNet \cite{zhao2017pyramid} & 0.757 & 0.850 & 0.905 & 0.949\\
    \bottomrule
  \end{tabular}
  }
\end{center}
\label{Table:Datasetbench}
\end{table}
\egroup

Table \ref{Table:Datasetbench}  shows the standard performance metrics such as IoU and Dice-coefficient calculated using FPN \cite{lin2017feature}, UNet \cite{ronneberger2015u}, LinkNet \cite{chaurasia2017linknet}, PSPNet \cite{zhao2017pyramid}, all with EfficientNet \cite{tan2019efficientnet} as backbone and DeepLabv3+ \cite{deeplabv3plus2018} with Xception network \cite{chollet2017xception} as backbone.
It is seen that SNV dataset (IoU  0.79) is slightly complex as compared to Hand Gesture dataset (IoU  0.90).
\bgroup
\setlength{\tabcolsep}{4.4pt}
\begin{table}[h]
\caption{Empirical analysis of GLSS along with standard UDA methods. IoU and Dice-coefficient are computed for both SNV and Hand Gesture datasets using UNet  and DeepLabv3+ as segmentation networks.}
\begin{center}
\scalebox{0.78}{
\begin{tabular}{l|cccc|cccc}
    \toprule
    \multicolumn{1}{c|}{} & \multicolumn{4}{c|}{SNV} & \multicolumn{4}{c}{Hand Gesture}\\
     & \multicolumn{2}{c|}{UNet} & \multicolumn{2}{c|}{DeepLabv3+} & \multicolumn{2}{c|}{UNet} &\multicolumn{2}{c}{DeepLabv3+}\\
     Models &\multicolumn{1}{c|}{IoU} & \multicolumn{1}{c|}{Dice} & \multicolumn{1}{c|}{IoU} &\multicolumn{1}{c|}{Dice} & \multicolumn{1}{c|}{IoU} & \multicolumn{1}{c|}{Dice} & \multicolumn{1}{c|}{IoU} &\multicolumn{1}{c}{Dice}\\
    \midrule
    Source Only & 0.295 & 0.426 & 0.215 & 0.426 & 0.601 & 0.711 & 0.505 & 0.680\\
    AdaptSegnet \cite{tsai2018learning} & 0.315 & 0.435 & 0.230 & 0.435 &0.641&0.716& 0.542 & 0.736\\
    Advent \cite{vu2018advent} & 0.341 & 0.571 & 0.332 &0.540 &0.612 & 0.729 & 0.508 & 0.689\\
    CLAN \cite{luo2019taking} & 0.248& 0.442 & 0.225 &0.426 &0.625 & 0.732 & 0.513 & 0.692\\
    BDL \cite{li2019bidirectional} & 0.320 & 0.518 & 0.301 &0.509 &0.647 & 0.720 & 0.536 & 0.750\\
    DISE \cite{chang2019all} & 0.341  & 0.557  & 0.339  &0.532  &0.672  & 0.789  & 0.563 & 0.769 \\
    DADA \cite{vu2019dada} & 0.332  & 0.534  & 0.314  &0.521  &0.643  & 0.743  &0.559  & 0.761 \\
    ours (GLSS) & \textbf{0.406}  & \textbf{0.597}  & \textbf{0.385}  & \textbf{0.597}  & \textbf{0.736}  & \textbf{0.844}  & \textbf{0.698}  & \textbf{0.824} \\
    \bottomrule
\end{tabular}
}
\end{center}
\label{Table:2}
\end{table} 
\egroup
\subsection{Baseline UDA Experiments}
\subsubsection{SNV and Hand Gesture dataset:}
We have performed the UDA experiments with the SOTA UDA algorithms using Red-channel of the COMPAQ Dataset  \cite{jones2002statistical} as the source and SNV and Hand Gesture as the target.
Table \ref{Table:2} compares the performance of proposed GLSS algorithm with six SOTA baselines along with the Source Only case (without any UDA). We have used entire target dataset for IoU and Dice-coefficient evaluation. 
\begin{figure}[h]
\centering
    \includegraphics[width=1.0\textwidth,height=0.672\textwidth]{Grid.pdf}
    \caption{Qualitative comparison of predicted segmentation skin masks on SNV and Hand Gesture datasets with standard UDA methods. Top four rows shows skin masks for SNV dataset and the last four are the masks for Hand Gesture dataset. It is evident that GLSS predicted masks are very close to the GT masks as compared to other UDA methods. (SO=Source Only, ASN=AdaptSegNet \cite{tsai2018learning}, GT=Ground Truth).
    }
    \label{fig:gridmasks}
\end{figure}
 Two architectures, DeepLabv3+ and UNet, are employed for the segmentation network (). It can be seen that although all the UDA SOTA methods improve upon the Source Only performance, GLSS offers significantly better performance despite not using any data from the target distribution. Hence, it may be empirically inferred that GLSS is successful in producing the `nearest-clone' through implicit sampling from the source distribution and thereby reducing the domain shift. It is also observed that the performance of the segmentation network  does not degrade on the source data with GLSS. 
The predicted masks with DeepLabv3+ are shown in Fig. \ref{fig:gridmasks} for SNV and Hand Gesture datasets, respectively. It can be seen that GLSS is able to capture fine facial details like eyes, lips and body parts like hands, better as compared to SOTA methods. It is also seen that the predicted masks for Hand Gesture dataset are sharper in comparison to other methods. 
Most of the methods work with the assumption of spatial and structural similarity between the source and target data. Since our source and target datasets do not have similar backgrounds, the methods that make such assumptions perform poorer on our datasets. We observed that for methods like BDL, the image translation between NIR images and Red channel images is not effective for skin segmentation task.

\subsubsection{Standard UDA task:}
We use standard UDA methods along with GLSS on standard domain adaptation datasets such as Synthia \cite{ros2016synthia} and Cityscapes \cite{cordts2016cityscapes}. As observed from Table \ref{tab:synthiatocityscape}, even with large domain shift, GLSS finds a clone for every target image that is sampled from the source distribution while preserving the structure of the target image.
\begin{table}[h]
\caption{Empirical analysis of GLSS on standard domain adaptaion task of adapting  Synthia \cite{ros2016synthia} to Cityscapes \cite{cordts2016cityscapes}. We calculate the mean IoU for 13 classes (mIoU) and 16 classes (mIoU*).}
\begin{center}
\scalebox{0.775}{
\begin{tabular}{c|c|c}
    \toprule
    Models  &  mIoU & mIoU*   \\
    \midrule
    AdaptsegNet \cite{tsai2018learning} &46.7& -\\
    Advent \cite{vu2018advent} &48.0&41.2\\
    BDL \cite{li2019bidirectional} &51.4&-\\
    CLAN \cite{luo2019taking} &47.8&-\\
    DISE \cite{chang2019all} &48.8&41.5\\
    DADA \cite{vu2019dada} &49.8&42.6\\
    ours(GLSS) &\textbf{52.3}&\textbf{44.5}\\
\bottomrule
\end{tabular}
}
\end{center}
\label{tab:synthiatocityscape}
\end{table}
\subsection{Ablation Study}
We have conducted several ablation experiments on GLSS using both SNV and Hand Gesture datasets using DeepLabv3+ as segmentation networks () to ascertain the utility of different design choices we have made in our method. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.75\textwidth,height=.37\textwidth]{ls_transform_mask_2.pdf}
    \caption{Illustration of Latent Search in GLSS. Real target is a ground truth mask. Source Only masks are obtained from target samples by training segmentation network  on source dataset. Prior to the LS, skin masks are obtained from VAE reconstructed target samples. It is evident that predicted skin masks improve as the LS progresses. The predicted masks for the `nearest-clones' are shown after every 30 iterations.}
    \label{fig:LStransformMask}
\end{center}
\end{figure}
\begin{figure}[h]

      \subfloat[SNV]{\includegraphics[width=0.48\linewidth,height=0.29\linewidth]{images/orignal_images/plot_SNV_iou.png}
        }
    \hfill
      \subfloat[Hand Gesture]{\includegraphics[width=0.48\linewidth,height=0.29\linewidth]{images/orignal_images/plot_Hand_iou.png}
      }
    \caption{Performance of gradient-based Latent Search during inference on target SNV and Hand Gesture images using different objective functions; MSE, MAE, SSIM loss. DeepLabv3+  is employed as segmentation network. It is evident that the losses saturate at around 90-100 iterations.}
    \label{fig:losa}
\end{figure}
\subsubsection{Effect of number of iterations on LS:}
The inference of GLSS involves a gradient-based optimization through the decoder network  to generate the `nearest-clone' for a given target image. In Fig. \ref{fig:LStransformMask}, we show the skin masks of the transformed target images after every 30 iterations. 
It is seen that with the increasing number of iterations, the predicted skin masks improves using GLSS as the `nearest-clones' are optimized during the Latent Search procedure. 
We plot the IoU as a function of the number of iterations during Latent Search as shown in Fig. \ref{fig:losa} where it is seen that it saturates around 90-100 iterations that are used for all the UDA experiments described in the previous section. 
\subsubsection{Effect of Edge concatenation:}
As discussed earlier, edges extracted using Sobel filter on input images are concatenated with one of the layers of decoder for both training and inference.  It is seen from Table \ref{tab:ablation} that IoU improves for both the target datasets with concatenation of edges.
\setlength{\tabcolsep}{12.4pt}
\begin{table}[h]
\caption{Ablation of different components of GLSS during training and inference; Edge, perceptual loss  and Latent Search (LS).}
\begin{center}
\scalebox{0.77}{
\begin{tabular}{ccc|c|c}
    \toprule
    Edge &  & LS & SNV IoU & Hand Gesture IoU   \\
    \midrule
    &&&0.112& 0.227\\
    \checkmark&&&0.178&0.560\\
    &\checkmark&&0.120&0.250\\
    &&\checkmark&0.128&0.238\\
    \checkmark&\checkmark&&0.330&0.615\\
    &\checkmark&\checkmark&0.182&0.300\\
    \checkmark&&\checkmark&0.223&0.58\\
    \checkmark&\checkmark&\checkmark&0.385&0.698\\
\bottomrule
\end{tabular}
}
\end{center}
\label{tab:ablation}
\end{table} 
It is observed that without the edge concatenation, the generated images (`nearest-clones') are blurry thus the segmentation network fails to predict sharper skin masks.
\subsubsection{Effect of Perceptual loss :}
We have introduced a perceptual model , trained on source samples. It ensures that the VAE reconstructed image is semantically similar to the input image unlike the norm based losses. Table \ref{tab:ablation} clearly demonstrates the improvement offered by the use of perceptual loss while training the VAE.
\subsubsection{Effect of SSIM for Latent Search:}
Finally, to validate the effect of SSIM loss for Latent Search, we plot the IoU metric using two norm based losses MSE (Mean squared error) and MAE (Mean absolute error) for the Latent Search procedure as shown in Fig. \ref{fig:losa}. On both the datasets, it is seen  that SSIM is consistently better than the norm based losses at all iterations affirming the superiority of the SSIM loss in preserving the structures while finding the `nearest-clone'.  











\section{Conclusion}
 In this paper, we addressed the problem of skin segmentation from NIR images. Owing to the non-existence of large-scale labelled NIR datasets for skin segmentation, the problem is casted as Unsupervised Domain Adaptation where we use the segmentation network trained on the Red-channel images from a large-scale labelled visible-spectrum dataset for UDA on NIR data. We propose a novel method for UDA without the need for the access to the target data (even unlabelled). Given a target image, we sample an image from the source distribution that is `closest' to  it under a distance metric. We show that such a `closest' sample exists and describe a procedure using an optimization algorithm over the latent space of a VAE trained on the source data. We demonstrate the utility of the proposed method along with the comparisons with SOTA UDA segmentation methods on the skin segmentation task on two NIR datasets that were created. Also, we reach SOTA performance on Synthia and Cityscapes datasets for semantic segmentation of urban scenes.
 \clearpage
\bibliographystyle{splncs04}
\bibliography{eccv2020submission}

\makeatletter
\newcounter{phase}[algorithm]
\newlength{\phaserulewidth}
\newcommand{\setphaserulewidth}{\setlength{\phaserulewidth}}
\newcommand{\phase}[1]{\vspace{-2.2ex}
\Statex\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}
  \Statex\strut\refstepcounter{phase}\textbf{#1}\vspace{-2.2ex}\Statex\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}}
\makeatother
\setphaserulewidth{.35pt}

\title{Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search
\\
Supplementary} 



\titlerunning{UDA for Semantic Segmentation of NIR Images through GLS}
\author{Prashant Pandey\thanks{equal contribution}\orcidID{0000-0002-6594-9685} \and
Aayush Kumar Tyagi\index{Tyagi, Aayush Kumar}\printfnsymbol{1}\orcidID{0000-0002-3615-7283} \and
Sameer Ambekar\orcidID{0000-0002-8650-3180} \and \\ Prathosh AP\orcidID{0000-0002-8699-5760}}
\authorrunning{P. Pandey et al.}
\institute{Indian Institute of Technology Delhi \\
\email{getprashant57@gmail.com, aayush16081@iiitd.ac.in, ambekarsameer@gmail.com, prathoshap@iitd.ac.in}}


\maketitle
\section{Datasets}
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth,height=.29\textwidth]{images/dataset_details_2.pdf}
    \caption{a) shows samples of COMPAQ dataset \cite{jones2002statistical} images with only Red-channel present b) contains samples from SNV dataset c) contains samples from Hand Gesture dataset.
    }
    \label{fig:DatasetExample}
\end{center}
\end{figure}
Each row of Fig. \ref{fig:DatasetExample} shows few images with the corresponding skin-mask pairs from  COMPAQ, SNV and Hand Gesture datasets respectively.
\begin{algorithm}[bth!]
\caption{\textbf{Generative Latent Search for Segmentation (GLSS)}}
    \label{alg:trainalgo}


    \hspace*{\algorithmicindent} 
    
    \begin{algorithmic}[1] \vspace*{.1cm}
    \phase{{Training VAE on source samples}}
    \textbf{Input}: Source dataset , Number of source samples , Encoder , Decoder , Trained Perceptual Model , Learning rate , Batchsize . \textbf{Output}: Optimal parameters , .
    \State Initialize parameters , 
    \REPEAT
\State sample batch  from dataset  for   
\State 
    \State sample  
\vspace*{0.02cm}
    \State  
    \vspace*{0.06cm}
    \State 
    \vspace*{0.02cm}
    \State 
    \State  
    \State  
    \State  
\UNTIL{convergence of ,  }
    \vspace*{.35cm}
    \phase{Inference - Latent Search during testing with Target}
     \textbf{Input}: Target sample , Trained decoder , Learning rate . \textbf{Output}: `nearest-clone'   for the target sample .  
    \State sample  from 
    \REPEAT


\State 
    \State 
    \UNTIL{convergence of }
    \State  \State 
\end{algorithmic}
\end{algorithm}
\section{Implementation details}
\subsection{GLSS on NIR images}
 is the segmentation model (as shown in Fig. 2 in the paper) implemented using DeepLabv3+ (XceptionNet) and UNet (EfficientNet).  is trained for 100-150 epochs with losses () as shown in Eq. 1 and Eq. 2  for UNet  and DeepLabv3+ respectively.




 is the dice coefficient loss which calculates the overlap between the predicted and the ground truth mask whereas  is the binary cross-entropy loss.
Binary focal loss () tries to down-weight the contribution
of examples that can be easily segmented so that the segmentation model focuses  more on learning hard examples.

 is a perceptual model (as shown in Fig. 1 in the paper) that uses perceptual loss . The perceptual features are taken from the 6th layer of UNet and the last concatenation layer of DeepLabv3+. VAE along with perceptual loss  is trained for  150-200 epochs.  is weighted  with a factor  (a hyper-parameter) as shown:

In order to improve the quality of VAE reconstructed images, we weighted the perceptual loss () with different values of .
For UNet, we have used  = 2 whereas   = 3 is used for DeepLabv3+.
The first part of Algorithm 1 shows the steps involved in training VAE and second part shows the steps involved in inference procedure. 

Using an Intel Xeon processor (6 Cores) with a base frequency of 2.0 GHz, 32GB RAM and NVIDIA Tesla K40 (12 GB Memory) GPU, Latent Search for one sample on SNV dataset takes 450 ms and 120 ms on Hand Gesture dataset. The time required is in the order of milliseconds on a basic GPU like K40 which is not very significant. However, this is the cost that is to be paid for being target independent which is a very significant advantage.
\subsection{Implementation details of UDA baseline methods for skin segmentation}
DeepLabv3+ was used as the segmentation model for all the baselines with images and corresponding masks of size . AdaptsegNet \cite{tsai2018learning} uses discriminative approach to predict the domain of the images.
For discriminator, we used a model with 5 convolutional layers (default implementation). We performed a grid search over   and  and reported the best IoU score for AdaptsegNet.
DISE \cite{chang2019all} uses image-to-image translation approach to translate one domain to another. It employs label transfer loss to optimize the segmentation model. Image-to-image translation based methods work well in cases where the structural similarity is more. 
We used 0.1, 0.25 and 0.5 for  and reported the best IoU using  while the learning rate was set to 2.5e-4.
Advent \cite{vu2018advent} proposes to leverage an entropy loss to directly penalize low-confident predictions on target domain. If  is large then the entropy drops too quickly and the model is strongly biased towards a few classes. We used 0.001 for  as suggested by the authors regardless of the network and dataset. Also, for adversarial training, 0.001 was used for . We trained with AdvEnt as it performed better that minEnt as stated in the paper. SGD and Adam were used as optimizers for segmentation and discriminator networks respectively.
In DADA \cite{vu2019dada}, authors make use of an additional depth information in the source domain. We performed a grid search over  using values 0.25, 0.5, 1. The learning rate was varied with values 2.5e-4, 1e-4 and 3e-4 and finally best IoU was reported with  and learning rate = 2.5e-4. 
CLAN \cite{luo2019taking} makes use of a category-level joint distribution and align each class with an adaptive adversarial loss, thus ensuring correct mapping of source and target. Compared to traditional adversarial training, CLAN introduces the discrepancy loss and the category-level adversarial loss. Hyperparameters like learning rate, weight decay,  and  were used with values 2.5e-4, 5e-4, 0.01 and 0.001 respectively during training. 
For training BDL \cite{li2019bidirectional}, we set the learning rate to 2.5e-4 for the segmentation network and 1e-4 for the discriminator. Grid search was performed for  with values 1e-3, 2e-3, 5e-3 and best IoU was reported with  1e-3.


\subsection{SSIM Loss}
SSIM loss compares pixels and their corresponding neighbourhoods between two images, preserving the luminance, contrast and structural information. To perform Latent Search, we used distance metric as SSIM loss, that helps to sample the nearest-clone in the source distribution for the target image from the generative latent space of VAE. Unlike norm-based losses, SSIM loss helps in the preservation of structural information as compared to discrete pixel-level information. We used 11x11 Gaussian filter in our experiments.

SSIM  is defined using the three aspects of similarities, luminance , contrast  and structure  that are measured for a pair of images  as follows:


where 's denote sample means and 's denote variances.  and  are constants. With these, SSIM and the corresponding loss function , for a pair of images  are defined as: 

where ,  and  are parameters used to adjust the relative importance of the three components.


\begin{table}[h]
\caption{IoU comparison for Target-Independence of GLSS with change in the amount of target data. GLSS performance is not affected by change in the amount of target data during training while other SOTA methods degrade.}
\begin{center}
\scalebox{0.77}{
\begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \% of Target data  &  Adaptsegnet & BDL & CLAN& Advent & DADA & GLSS \\
    \midrule
    
    60 & 0.23 & 0.30 & 0.22 & 0.33 & 0.31 & 0.37 \\ 
    40 & 0.22 & 0.26 & 0.22 & 0.29 & 0.28 & 0.37 \\ 
    20 & 0.21 & 0.22 & 0.21 & 0.24 & 0.23 & 0.38 \\ 
    
\bottomrule
\end{tabular}
}
\end{center}
\label{tab:vaevsgans}
\end{table} 

\subsection{Target-Independence of GLSS}
GLSS is a general-purpose target-independent UDA method. For UDA, target independence is a merit since a SINGLE source model can be used across multiple targets. However, even with target data (for VAE training) GLSS doesnt degrade while SOTA methods do, for skin segmentation on NIR images (Table below compares IoU). 

\subsection{GAN vs. VAE}
GLSS demands a generative model that has both generation and inference capabilities (mapping from latent to data space and vice versa), which is not the case with GANs. This leads to non-convergence of latent search. To validate this, we trained a SOTA BigGAN \cite{brock2018large} on COMPAQ Dataset [21] and performed GLSS. Although GAN had better generation quality (FID of 29.7 with BigGAN vs. 44 with VAE), the final IoU was worse as shown in Table \ref{tab:vaevsgans}.

\begin{table}[h]
\caption{IoU score comparison between BigGAN and VAE when trained on SNV and Hand Gesture datasets. VAE scores better in terms of IoU.}
\begin{center}
\scalebox{0.77}{
\begin{tabular}{c|c|c|c}
    \toprule
    SNV/BigGAN  &  Hand Gesture/BigGAN & SNV/VAE & Hand Gesture/VAE  \\
    \midrule
    
0.09&0.21&0.38&0.69\\
\bottomrule
\end{tabular}
}
\end{center}
\label{tab:vaevsgans}
\end{table} 

\clearpage
\section{Additional Results}

\begin{figure}
\begin{center}
    \includegraphics[width=0.7\textwidth,height=.74\textwidth]{ls_image_skin.pdf}
\caption{Illustration of Latent Search (LS) in GLSS for SNV dataset. Prior to the LS, VAE reconstructed target samples are obtained. It is evident that the `nearest-clones' (images generated using LS) improve as the LS progresses. Also the quality (empirically) of `nearest-clones' are better as compared to the VAE reconstructed images. The `nearest-clones' are shown after every 30 iterations.}
\label{fig:LatentSearch}
\end{center}
\end{figure}
\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.7\textwidth,height=.74\textwidth]{ls_image_gs.pdf}
\caption{Illustration of Latent Search (LS) in GLSS for Hand Gesture dataset. Prior to the LS, VAE reconstructed target samples are obtained. It is evident that the `nearest-clones' (images generated using LS) improve as the LS progresses. Also the quality (empirically) of `nearest-clones' are better as compared to the VAE reconstructed images. The `nearest-clones' are shown after every 30 iterations.}
\label{fig:LatentSearch}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
    \includegraphics[width=0.70\textwidth,height=.32\textwidth]{ablation_fig.pdf}
\caption{(a) the ground truth mask for SNV and Hand Gesture datasets, (b) the predicted mask of VAE reconstructed image without edge concatenation, (c) the predicted mask of VAE reconstructed  image without ,
(d) the predicted mask of VAE reconstructed with edge concatenation and perceptual loss when no Latent Search (LS) was performed, 
 (e) the predicted mask with GLSS.
 It is evident from the predicted masks that with edge concatenation, perceptual loss and Latent Search (LS), quality of predicted masks improve. Each component plays a significant role in improving the IoU. Hence, when all the components are employed (as in GLSS) we get the best IoU.}
\label{fig:LatentSearch}
\end{center}
\end{figure}
\begin{figure}[h]
\begin{center}
    \hspace*{-1cm}\includegraphics[scale=0.48]{ssim_snv.pdf}
\caption{(a) an NIR image  from SNV dataset (target), (b) `nearest-clone'  generated from GLSS, (c) Structural Similarity Index (SSIM) scores calculated between  and all the samples (having only Red-channel) of COMPAQ dataset (source) are shown with blue color in the plot. Similary, SSIM scores calculated between  and all the samples (having only Red-channel) of COMPAQ dataset are shown with red color. It is evident from the figure that the SSIM scores are higher for the `nearest-clone'  as compared to the scores with . It indicates that  is more closer to the source domain (COMPAQ) as compared to . Hence, the `nearest-clone'  generated by GLSS for target  is used as a proxy in the segmentation network  which is trained only on COMPAQ dataset, thereby increasing the IoU for .}
 \label{fig:LatentSearch}
\end{center}
\end{figure}
\begin{figure}[h]
\begin{center}
    \hspace*{-2cm}\includegraphics[scale=0.60]{ssim_hg.pdf}
\caption{(a) an NIR image  from Hand Gesture dataset (target), (b) `nearest-clone'  generated from GLSS, (c) Structural Similarity Index (SSIM) scores calculated between  and all the samples (having only Red-channel) of COMPAQ dataset (source) are shown with blue color in the plot. Similary, SSIM scores calculated between  and all the samples (having only Red-channel) of COMPAQ dataset are shown with red color. It is evident from the figure that the SSIM scores are higher for the `nearest-clone'  as compared to the scores with . It indicates that  is more closer to the source domain (COMPAQ) as compared to . Hence, the `nearest-clone'  generated by GLSS for target  is used as a proxy in the segmentation network  which is trained only on COMPAQ dataset, thereby increasing the IoU for .}
 
 \label{fig:LatentSearch}
\end{center}
\end{figure}



\end{document}
