\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage{makecell}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{multirow}
\usepackage{subcaption}

\interfootnotelinepenalty=10000
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\usepackage{mathtools}
\usepackage[ruled, linesnumbered]{algorithm2e}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}

\usepackage[table,x11names, dvipsnames]{xcolor}



\usepackage{multicol}
\usepackage{lipsum}
\usepackage{mwe}



\definecolor{mygray}{gray}{0.9}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}


\newlength\mylen
\newcommand\myinput[1]{\settowidth\mylen{\KwIn{}}\setlength\hangindent{\mylen}\hspace*{\mylen}#1\\}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\iccvfinalcopy 

\def\iccvPaperID{5417} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
 
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features}





\author{Juhong Min\affmark[1,2]\hspace{0.8cm}Jongmin Lee\affmark[1,2]\hspace{0.8cm}Jean Ponce\affmark[3,4]\hspace{0.8cm}Minsu Cho\affmark[1,2]\vspace{1.5mm}\\
\affmark[1]POSTECH \hspace{1.5cm} 
\affmark[2]NPRC\footnotemark[1] \hspace{1.5cm}  
\affmark[3]Inria\vspace{3.0mm} \hspace{1.5cm} 
\affmark[4]DI ENS\footnotemark[2] \\
{\tt\small \url{http://cvlab.postech.ac.kr/research/HPF/}}
}

\maketitle


\newcommand{\mcho}[1]{\textcolor{magenta}{#1}}
\newcommand{\jmin}[1]{\textcolor{blue}{#1}}
\newcommand{\jmlee}[1]{\textcolor{cyan}{#1}}





\begin{abstract}
Establishing visual correspondences under large intra-class variations requires analyzing images at different levels, from features linked to semantics and context to local patterns, while being invariant to instance-specific details.
To tackle these challenges, we represent images by ``hyperpixels'' that leverage a small number of relevant features selected among early to late layers of a convolutional neural network. Taking advantage of the condensed features of hyperpixels, we develop an effective real-time matching algorithm based on Hough geometric voting. 
The proposed method, hyperpixel flow, sets a new state of the art on three standard benchmarks as well as a new dataset, SPair-71k, which contains a significantly larger number of image pairs than existing datasets, with more accurate and richer annotations for in-depth analysis. \end{abstract} 
\footnotetext[1]{The Neural Processing Research Center, Seoul, Korea}
\footnotetext[2]{D\'epartement d'informatique de l'ENS, ENS, CNRS, PSL University, Paris, France}



\section{Introduction}
Establishing visual correspondences under large intra-class variations, \ie, matching scenes depicting different instances of the same object categories, remains a challenging problem in computer vision. It requires analyzing scenes at different levels, from features linked to semantics and context to local image patterns, while being invariant to irrelevant instance-specific details.
Recent methods have addressed this problem using deep convolutional features. 
Many of them~\cite{choy2016universal,han2017scnet,kim2017fcss,Rocco18} formulate this task as local region matching and learn to assign a local region in an image to a correct match in another image. Others~\cite{NIPS2018_7851, Rocco17, Rocco18, paul2018attentive} cast it as image alignment and learn to regress the parameters of global geometric transformation, \eg, using an affine or thin plate spline model~\cite{donato2002approximate}. These methods, however, mainly perform the prediction based on the output of the last convolutional layer, and fail to fully exploit the different levels of semantic features available to resolve the severe ambiguities in matching linked with intra-class variations.  


\begin{figure}[t]
	
    \centering
    \includegraphics[width=1.0\linewidth]{figures/teaser.pdf}
	\vspace{-3.0mm}
		
	\caption{Hyperpixel flow. {Top:} The {\em hyperpixel} is a multi-layer pixel representation created with selected levels of features optimized for semantic correspondence. It provides multi-scale features, resolving local ambiguities. {Bottom:} The proposed method, {\em hyperpixel flow}, establishes dense correspondences in real time using hyperpixels. } \label{fig:teaser}
	\vspace{-3.0mm}
\end{figure}







We propose a novel dense matching method, dubbed {\em hyperpixel flow} (Figure~\ref{fig:teaser}). Inspired by the hypercolumns~\cite{hariharan2015hypercolumns} used in object segmentation and detection, we represent images by ``hyperpixels'' that leverage different levels of features among early to late layers of a convolutional neural network and disambiguate parts of images in multiple visual aspects. The corresponding feature layers for hyperpixels are selected by a simple yet effective search process which requires only a small validation set of supervised image pairs.  
We show that the resultant hyperpixels provide both fine-grained and context-aware features suited for semantic correspondence and that only a few layers are sufficient and even better for the purpose, thus making hyperpixels an effective representation for light-weight computation. To obtain a geometrically consistent flow of hyperpixels, we present a real-time dense matching algorithm, regularized Hough matching (RHM), building on a recent region matching method using geometric voting~\cite{cho2015unsupervised}. 
Furthermore, we also introduce a new large-scale dataset, SPair-71k, with more accurate and richer annotations, which facilitates in-depth analysis for semantic correspondence.



Our paper makes four main contributions:\vspace{-5px}
\begin{itemize}
\setlength\itemsep{0em}
    \item We propose {\em hyperpixels} for establishing reliable dense correspondences between two images, which provide multi-layer features robust to local ambiguities.
    \item We present an efficient matching algorithm, regularized Hough matching (RHM), that achieves a speed of more than 50 fps on a GPU for  image pairs.
    \item We introduce a new dataset, {\em SPair-71k}, which contains a significantly larger number of image pairs with richer annotations than existing ones.
    \item The proposed method, {\em hyperpixel flow}, sets a new state of the art on standard benchmarks as well as SPair-71k.
\end{itemize} 

\begin{figure*}
    \begin{center}
        \includegraphics[width=1.0\linewidth]{figures/architecture.pdf}
    \end{center}
    \vspace{-2.0mm} 
       \caption{Overall architecture of the proposed method. Hyperpixel flow consists of three main steps: hyperpixel construction, regularized Hough matching, and flow formation. For details, see text.}
    \vspace{-3.0mm} 
\label{fig:architecture}
\end{figure*}

\section{Related Work}
\smallbreak
\noindent \textbf{Local region matching.}
Early methods commonly tackle semantic correspondence by matching two sets of local regions based on handcrafted features. Liu~\etal~\cite{liu2016sift} and Kim~\etal~\cite{kim2013deformable} use dense SIFT descriptors to establish a flow of local regions across similar but different scenes by leveraging a hierarchical optimization technique in a coarse-to-fine manner. Bristow~\etal~\cite{bristow2015dense} use LDA-whitened SIFT descriptors, making correspondence more robust to background clutter.
Cho~\etal~\cite{cho2015unsupervised} introduce an effective voting-based algorithm based on region proposals and HOG features~\cite{dalal2005histograms} for semantic matching and object discovery. Ham~\etal~\cite{ham2016proposal} further extend the work with a local-offset matching algorithm, and introduce a benchmark dataset with keypoint-level annotations. Taniai~\etal~\cite{taniai2016joint} tackle semantic correspondence jointly with cosegmentation, introducing a benchmark dataset annotated with dense flows and segmentation masks. 
All these hand-crafted representation fails to capture high-level semantics enough to discriminate complex patterns with large intra-class deformations. 

In this context, CNN features have emerged as good alternatives for semantic matching.
Long~\etal~\cite{long2014convnets} show that convolutional features from a CNN pretrained on classification are transferable to correspondence problems. 
Choy \etal~\cite{choy2016universal} attempt to learn a similarity metric based on a CNN using a contrastive loss with hard negative mining. Han~\etal~\cite{han2017scnet} propose to learn a CNN end-to-end with geometric matching, which uses region proposals as matching primitives. Kim~\etal~\cite{kim2017fcss} introduce a CNN-based self-similarity feature for semantic correspondence, and also use it to estimate dense affine-transformation fields by an iterative discrete-continuous optimization~\cite{kim2017dctm}.
Novotny~\etal~\cite{novotny2018self} train a geometry-aware feature in an unsupervised regime and use it for part matching and discovery by measuring confidence scores.
Rocco~\etal~\cite{rocco2018neighbourhood} propose a neighbourhood consensus network that computes robust matching similarity using 4D convolution filters.


\smallbreak
\noindent \textbf{Global image alignment.}
Some methods have cast semantic correspondence as global alignment.  
Rocco \etal~\cite{Rocco17} propose a CNN architecture which takes a correlation tensor and directly predicts global transformation parameters for geometric matching.
Seo \etal~\cite{paul2018attentive} improve it using offset-aware correlation kernels with attention.
Rocco~\etal~\cite{Rocco18} develop a weakly-supervised learning framework using differentiable soft-inlier count loss function.
Jeon \etal~\cite{jeon2018parn} propose a pyramidal affine transformation regression network to compute the correspondence hierarchically from high-level semantics to pixel-level points.
Kim \etal~\cite{NIPS2018_7851} introduce a recurrent alignment network that performs iterative local transformations with a global constraint. 

\smallbreak
\noindent \textbf{Multi-layer neural features.} 
Hariharan~\etal~\cite{hariharan2015hypercolumns} have shown that {\em hypercolumns} that combine features from multiple layers of CNN, improve object detection, segmentation, and part labeling. Following this work, several methods~\cite{kong2016hypernet, lin2017feature} have used multi-layer neural features with additional modules on object detection task.
Fathy~\etal~\cite{fathy2018hierarchical} propose coarse-to-fine stereo matching method that uses  multi-layer features in sequence. 
In semantic correspondence, multi-layer neural features have rarely been explored despite its relevance. Novotny~\etal~\cite{novotny2017anchornet} use residual hypercolumn features to learn a set of diverse filters for object parts.
Ufer and Ommer~\cite{ufer2017deep} employ pyramids of pre-trained CNN features to localize salient feature points guided by object proposals, and match them across images using sparse graph matching. In these methods, multi-layer features are mainly used to localize salient parts and the feature layers are manually selected following previous methods~\cite{girshick2015deformable, he2016deep}. 
Unlike these approaches and the hypercolumn~\cite{hariharan2015hypercolumns}, we use a multi-layer neural feature as a pixel representation for dense matching and optimize feature layers via layer search for the purpose.
We show that specific combinations of layers significantly affect matching performance and using only a small number of layers can achieve a remarkable performance. 

\smallbreak
\noindent \textbf{Neural architecture search (NAS).} 
The layer search for hyperpixels can be viewed as an instance of NAS~\cite{liu2018darts, randomwire, zoph2016neural, Zoph_2018_CVPR}. Unlike a general search space of network configurations in NAS, however, the search space in our work is limited to combinations of feature layers for visual correspondence.
\vspace{-2.0mm}



 

\section{Hyperpixel Flow}




Our method presented below, dubbed {\em hyperpixel flow}, can be divided into three steps: (1) hyperpixel construction, (2) regularized Hough matching, and (3) flow formation. Figure~\ref{fig:architecture} illustrates the overall architecture of our model aligned with the three steps. Each input image is fed into a convolutional neural network to create a set of hyperpixels. The hyperpixels are then used as primitives for the regularized Hough matching algorithm to build a tensor of matching confidences for all candidate correspondences. 
The confidence tensor is transformed into a hyperpixel flow in a post-processing step assigning a match to each hyperpixel.  
Three steps are detailed in this section. 

\subsection{Hyperpixel construction}
Given an image, a convolutional neural network produces a sequence of  feature maps  as intermediate outputs. We represent the original image by a {\em hyperimage} by pooling {\em a small} subset of  feature maps, optimized for semantic correspondence, and concatenating them along channels with upsampling: 

where  denotes a function that upsamples the input feature map to the size of , the {\em base} map. 
We can associate with each spatial position  of the hyperimage the corresponding image coordinates, a hyperpixel feature, and its multi-scale receptive fields.
Let us denote by  the image coordinate of position , and by   the corresponding hyperfeature, \ie, .
The hyperpixel at position  on the hyperimage is then defined as   

As will be seen in the next subsection, the hyperpixels are used as primitives for the subsequent matching process. 


\begin{algorithm}[t]
\SetKwInOut{Input}{Input}  
\Input{: all candidate layers\\
           :  candidate layers for the base ()\\: the beam size\\
           : the maximum number of layers allowed
           }
    \KwOut{: the set of selected layers}
    \SetKwBlock{Begin}{function}{end function}
    \Begin()
    {
\tcp{initialize memory buffers}
		 \ \ \ \; 
		\tcp{base layer search}
        \ForAll{}
        {
             evaluateLayerSet\;
            .insert()\;
        }
        
        .findBestN()\; 
        .findBest()\; 
        
        \tcp{layer search iterations}    
        \For{ to }
        {
            \;  
            \ForAll{}
            {
                \ForAll{}
                {
                    \If{}
                    {
                         evaluateLayerSet\;
                        .insert()\;
                    }
                }
            }
            .findBestN()\; 
            .findBest()\;
             \If{}
            {
                \;
            }
        }
        
        \Return{}
    }\vspace{-1.0mm}
\caption{Beam search for hyperpixel layers.}
\label{alg:layersearch}
\end{algorithm} 

To select the optimal set of feature maps for hyperpixels, we perform a search over all convolutional layers of a given CNN so that a subsequent matching algorithm achieves the best validation performance. In our case, we use regularized Hough matching (Sec.~\ref{sec:RHM}) for the matching algorithm and the probability of correct keypoints (PCK) (Sec.~\ref{sec:PCK}) for the performance metric. 
For the search algorithm, we use a variant of beam search~\cite{beamsearch}, which is a breadth-first search algorithm with a limited memory. 
Basically, at each iteration, it evaluates the effect of each candidate layer by adding it to current combinations of layers in the memory and then replaces them with a fixed number of top performing combinations.  
The search process is repeated until the number of selected layers reaches the maximum number of layers allowed. Finally, we use the best combination found along the search. 
The detailed procedure is summarized in Algorithm~\ref{alg:layersearch}, where we restrict base layer candidates,  only to layers with a sufficient spatial resolution. 


\subsection{Regularized Hough matching}\label{sec:RHM}
In order to establish visual correspondences, we adapt the probabilistic Hough matching (PHM), algorithm of Cho \etal ~\cite{cho2015unsupervised}, to hyperpixels. The key idea of PHM is to re-weight appearance similarity by Hough space voting to enforce geometric consistency. In our context, let  be two sets of hyperpixels, and  be a hyperpixel match where  and  are respectively elements of  and . Given a Hough space  of possible offsets (image transformation) between the two hyperpixels,  
the confidence for match , , is computed as  

where  represents the confidence for appearance matching and  is the confidence for geometric matching with an offset , measuring how close the offset induced by  is to . By sharing the Hough space  for all matches, PHM efficiently computes the match confidence with good empirical performance~\cite{cho2015unsupervised,ham2016proposal,han2017scnet}.

In this work we compute appearance matching confidence using hyperpixel features:    

where 
the ReLU function clamps negative values to zero and the exponent  is used to emphasize the difference between the hyperpixel features.
When combined with Hough voting, this similarity function with  improves matching performance by suppressing noisy activations. We set  in our experiments.

To compute , we construct a two-dimensional offset space, quantize it into a grid of bins, and use a set of center points of the bins for . For Hough voting, each match  is assigned to the corresponding offset bin to increment the score of the bin by the appearance similarity score, . Despite their (serial) complexity of , the operations are mutually independent, and can thus easily be parallelized on a GPU. 

Previous versions of PHM all use multi-scale region proposals~\cite{manen2013prime, pont2017multiscale, uijlings2013selective} as matching primitives described with HOG~\cite{cho2015unsupervised,ham2016proposal} or a single feature map from a CNN~\cite{ham2016proposal,han2017scnet}. While using irregular and multi-scale region proposals focuses attention on object-like regions, it requires creating a three-dimensional offset space for translation and scale changes with higher memory and computation. 
In contrast, the use of hyperpixels reduces the Hough space down to two dimensions and makes the voting procedure faster and simpler since all hyperpixels are homogeneous on a predefined regular grid.
In addition, unlike region proposals, hyperpixels provide (quasi-)dense image features and their multi-layer features improve performance in practice. 
In our GPU implementation, our algorithm, {\em regularized Hough matching} (RHM), runs 100 to 500 times faster than PHM (220 msecs vs. 12 secs), enabling real-time matching.
\iffalse

\fi


\begin{center}
\begin{table*}
    \centering
    \scalebox{0.72}{
    \begin{tabular}{C{3.15cm}|C{1.65cm}|c|C{3.5cm}|C{4.2cm}|C{4cm}|C{3cm}}\hline
        Dataset name & Size (pairs)  & Class &  Source datasets & Annotations & Characteristics & Users of the dataset\\
        \hline\hline
         Caltech-101~\cite{kim2013deformable} & 1,515  &  101  &  Caltech-101~\cite{fei2004learning,li2006one}  & object segmentation &  tightly cropped images of objects, little background & ~\cite{ham2016proposal, han2017scnet, jeon2018parn, kim2017fcss, laskar2019semantic, Rocco17, Rocco18, paul2018attentive}\\
         \hline
         PASCAL-PARTS~\cite{zhou2015flowweb} & 3,884  &  20  &      PASCAL-PARTS~\cite{chen_cvpr14}, PASCAL3D+~\cite{Xiang2014BeyondPA} & keypoints (012), azimuth, elevation, cyclo-rotation, body part segmentation & tightly cropped images of objects, little background, part and 3D infomation & ~\cite{choy2016universal,han2017scnet,kim2017fcss,kim2017dctm,novotny2017anchornet,ufer2017deep} \\
         \hline
         Animal-parts~\cite{novotny16i-have} &  7,000 & 100 &   ILSVRC 2012 ~\cite{krizhevsky2012imagenet} &  keypoints (16)  & keypoints limited to eyes and feet of animals & ~\cite{novotny2017anchornet} \\
         \hline
         CUB-200-2011~\cite{WahCUB_200_2011} & 120k & 200 & CUB-200-2011~\cite{WahCUB_200_2011}  & 15 part locations, 312 binary attributes, bbox & tightly cropped images of object, only bird images & ~\cite{choy2016universal, kanazawa2016warpnet}  \\

         \hline
         TSS~\cite{taniai2016joint} &  400 &   9  & FG3DCar~\cite{lin2014jointly}, JODS \cite{rubinstein2013unsupervised}, PASCAL~\cite{hariharan2011semantic} &  object segmentation, flow vectors &  cropped images of objects, moderate background & ~\cite{cho2015unsupervised,ham2016proposal,jeon2018parn,NIPS2018_7851,kim2017fcss,kim2017dctm,laskar2019semantic,Rocco17,Rocco18,paul2018attentive}  \\
         \hline
         PF-WILLOW~\cite{ham2016proposal} &   900 & 5 &  PASCAL VOC 2007~\cite{everingham2015pascal}, Caltech-256~\cite{cho2013learning, griffin2007caltech} &  keypoints (10) & center-aligned images, pairs with the same viewpoint & ~\cite{ham2016proposal,han2017scnet,NIPS2018_7851,kim2017fcss,kim2017dctm,novotny2017anchornet,Rocco17,paul2018attentive,ufer2017deep} \\
         \hline
         PF-PASCAL~\cite{ham2018proposal}  &  1,300  & 20 & PASCAL VOC 2007~\cite{everingham2015pascal} &  keypoints (417), bbox. & pairs with the same viewpoint & ~\cite{ham2016proposal,han2017scnet,jeon2018parn,NIPS2018_7851,laskar2019semantic,novotny2018self,Rocco17,Rocco18,rocco2018neighbourhood,paul2018attentive} \\
         \hline\hline
         SPair-71k (ours)  &   70,958 & 18 & PASCAL3D+~\cite{Xiang2014BeyondPA}, PASCAL VOC 2012~\cite{everingham2015pascal} & keypoints (330), azimuth, view-point diff., scale diff., trunc. diff., occl. diff., object seg., bbox. & large-scale data with diverse variations, rich annotations, clear dataset splits &  this work\\
         \hline
    \end{tabular} 
    }
    \vspace{-3.0mm} 
    \caption{Public benchmark datasets for semantic correspondence.  The datasets are listed in chronological order. Research papers using the datasets for evaluation are listed in the last column. See text for details.}
    \vspace{-5.0mm} 
    \label{tab:benchmark_survey}
\end{table*}
\end{center}

\vspace{-11.0mm}
\subsection{Flow formation and keypoint transfer}
The raw output of RHM is a tensor of confidences for all candidate matches. It can easily be transformed into a hyperpixel flow in a post-processing step of assigning a match to each hyperpixel, \eg, by nearest-neighbor assignment.
Since the base map of the hyperimage is selected among early layers, the flow is dense enough for many applications. 

Transferring keypoints from an image to the corresponding points in another image is commonly used for evaluating semantic correspondences.  
We use a simple method for keypoint transfer using hyperpixel flow; 
given a keypoint  in a source image, its neighbor hyperpixels  are collected whose base map receptive fields cover the keypoint, and the displacement vectors from the centers of the base map receptive fields to the keypoint, denoted by , are computed. Given the hyperpixel flow  of  predicted by our method, we apply the average of the displacements  to localize a corresponding keypoint in the target image.




 





\section{SPair-71k dataset}
With growing interest in semantic correspondence, several annotated benchmarks are now available. 
Some popular ones are summarized in Table~\ref{tab:benchmark_survey}.  
Due to the high expense of ground-truth annotations for semantic correspondence, early benchmarks~\cite{chen_cvpr14, kim2013deformable} only support indirect evaluation using a surrogate evaluation metric rather than direct matching accuracy.    
For example, the Caltech-101 dataset in~\cite{kim2013deformable} provides binary mask annotations of objects of interest for 1,515 pairs of images and the accuracy of mask transfer is evaluated as a rough approximation to that of matching. 
Recently, Ham~\etal~\cite{ham2016proposal,ham2018proposal} and Taniai~\etal~\cite{taniai2016joint} have introduced datasets with ground-truth correspondences. Since then, PF-WILLOW~\cite{ham2016proposal} and PF-PASCAL~\cite{ham2018proposal} have been used for evaluation in many papers. They contain 900 and 1,300 image pairs, respectively, with keypoint annotations for semantic parts. 


All previous datasets, however, have several drawbacks: 
First, the amount of data is not sufficient to train and test a large model. Second, image pairs do not display much variability in viewpoint, scale, occlusion, and truncation. 
Third, the annotations are often limited to either keypoints or object segmentation masks, which hinders in-depth analysis.
Fourth, the datasets have no clear splits for training, validation, and testing. Due to this, recent evaluations in~\cite{han2017scnet, Rocco18, rocco2018neighbourhood} have been done with different dataset splits of PF-PASCAL. Furthermore, the splits are disjoint in terms of image pairs, but not images: some images are shared between training and testing data. 


To resolve these issues, we introduce a new dataset, {\em SPair-71k}, consisting of total 70,958 pairs of images from PASCAL 3D+~\cite{Xiang2014BeyondPA} and PASCAL VOC 2012~\cite{everingham2015pascal}\footnote{We do not include `dining table' and `sofa' classes because they appear as background in most images and their semantic keypoints are too ambiguous to localize.}. The dataset is significantly larger with rich annotations and clearly organized for learning.   
In particular, several types of useful annotations are available: keypoints of semantic parts, object segmentation masks, bounding boxes, view-point, scale, truncation, and occlusion differences for image pairs, etc. Figure~\ref{fig:dataset_summary_example} presents the statistics of SPair-71k in pie chart forms and shows a sample image pair with its annotations. For details on our dataset, we refer the readers to the website: {\tt\small  \url{http://cvlab.postech.ac.kr/research/SPair-71k/}}. 
\vspace{-3mm}

\begin{figure}[t]
    \begin{center}
      \includegraphics[width=1.0\linewidth]{figures/Figure4-camera_ready.pdf}
    \end{center}
    \vspace{-6mm}
    \caption{SPair-71k data statistics and an example pair with its annotations. Best viewed in electronic form.}
    \label{fig:dataset_summary_example}
    \vspace{-5mm}
\end{figure}













\begin{center}
    \begin{table*}
        \begin{center}
            \scalebox{0.85}{
                \begin{tabular}{l|c|ccc|ccc|cc}
                \hline
                \multirow{2}{*}{Methods} &\multirow{2}{*}{Supervision}  & \multicolumn{3}{c|}{PF-PASCAL (PCK@)} & \multicolumn{3}{c|}{PF-WILLOW (PCK@)} & \multicolumn{2}{c}{Caltech-101} \\   
                 & & 0.05 & 0.1 & 0.15 & 0.05 & 0.1 & 0.15 & LT-ACC & IoU \\ 
                \hline
                \hline
                 Identity mapping                   & \multirow{2}{*}{-} & 12.7 & 37.0 & 60.8 & 12.2 & 27.0 & 41.7 & 0.77 & 0.44 \\
PF~\cite{ham2016proposal} &   & 31.4 & 62.5 & 79.5 & 28.4 & 56.8 & 68.2 & 0.78 & 0.50 \\  
                \hline CNNGeo~\cite{Rocco17}      & \multirow{2}{*}{\makecell{synthetic warp \\ \small{(self-supervised)}}} & 41.0 & 69.5 & 80.4 & 36.9 & 69.2 & 77.8 & 0.79 & 0.56 \\  
                 A2Net~\cite{paul2018attentive}    &  & 42.8 & 70.8 & 83.3 & 36.3 & 68.8 & 84.4 & 0.80 & 0.57 \\  
                \hline DCTM~\cite{kim2017fcss}   & \multirow{4}{*}{\makecell{image labels \\ \small{(weakly-supervised)}}}  & 34.2 & 69.6 & 80.2 & 38.1 & 61.0 & 72.1 & 0.83 & 0.52 \\  
                Weakalign~\cite{Rocco18}        &   &  49.0 & 74.8 & 84.0 & 37.0 & 70.2 & 79.9 & 0.85 & \underline{0.63} \\  
                NC-Net~\cite{rocco2018neighbourhood} & & 54.3 & 78.9 & 86.0 & 33.8 & 67.0 & 83.7 & 0.85 & 0.60  \\  
                RTNs~\cite{NIPS2018_7851} & & 55.2 & 75.9 & 85.2 & 41.3 & 71.9 & \underline{86.2} & - & - \\  
                \hline UCN~\cite{choy2016universal}  & \multirow{3}{*}{keypoints} & 29.9 & 55.6 & 74.0 & 24.1 & 54.0 & 66.5 & - & - \\  
                SCNet~\cite{han2017scnet} &  & 36.2 & 72.2 & 82.0 & 38.6 & 70.4 & 85.3 & 0.79 & 0.51 \\ NN-Cyc~\cite{laskar2019semantic}  & & 55.1 & \underline{85.7} & \underline{94.7} & 40.5 & \underline{72.5} & \underline{86.9} & 0.86 & 0.62 \\  
                \hline   
                HPF (ours)  & \multirow{3}{*}{\makecell{keypoints \\ \small{(validation only)}}}  & \underline{60.5} & 83.4 & 92.1 & \underline{46.5} & 72.4 & 84.7 & \textbf{0.88} & \textbf{0.64} \\
                HPF (ours)  &  & \underline{60.1} & \underline{84.8} & \underline{92.7} & \underline{45.9} & \underline{74.4} & 85.6 & \underline{0.87} & \underline{0.63} \\
                HPF (ours) & & \textbf{63.5} & \textbf{88.3} & \textbf{95.4} & \textbf{48.6} & \textbf{76.3} & \textbf{88.2} & \underline{0.87} & \underline{0.63} \\ 
\hline
                \hline
                HPF (=1)  & \multirow{3}{*}{\makecell{keypoints \\ \small{(validation only,} \\\small{small set)}}} & 59.4 & 83.9 & 92.2 & 44.5 & 72.5 & 84.8 & 0.87 & 0.63 \\
                HPF (=2)   &  & 58.3 & 84.5 & 92.9 & 44.7 & 73.1 & 85.4 & 0.87 & 0.63 \\
                HPF (=3)  & & 59.4 & 84.5 & 92.7 & 45.1 & 73.4 & 85.4 & 0.87 & 0.63 \\
                \hline
                HPF (random) & - & 44.5 & 74.7 & 87.3 & 32.8 & 62.4 & 78.2 & 0.85 & 0.55 \\
                \hline
                \end{tabular}
            }
            \vspace{-1.5mm}
            \caption{\label{tab:stdBenchmarkTable}Results on standard benchmarks of semantic correspondences. Subscripts of the method names indicate  backbone networks used.  The second column denotes supervisory information used for training or tuning.
            Numbers in bold indicate the best performance and underlined ones are the second and third best. Results of~\cite{ham2016proposal, han2017scnet, kim2017fcss, Rocco17, Rocco18} are borrowed from~\cite{NIPS2018_7851}.
            }
            \vspace{-3.5mm}
        \end{center}
    \end{table*}
\end{center}



















%
 

\section{Experimental Evaluation}
In this section we compare the proposed method with recent state-of-the-art methods and discuss the results.

\subsection{Implementation details}
We use two CNNs as main backbone networks for hyperpixel features, ResNet-50 and ResNet-101~\cite{he2016deep} pre-trained on ImageNet~\cite{deng2009imagenet}. All convolutional layers of the networks are used as candidate feature layers for hyperpixels. We extract the features at the end of each layer before a ReLU activation. The optimal set of hyperpixel layers, ,  
is determined by Algorithm~\ref{alg:layersearch} run with a validation split of a target dataset. For this beam search, we set the beam size 4 and the maximum number of layers allowed 8.
For the exponent value for hyperpixel similarity, we fix  based on search using PF-PASCAL validation split. 

\subsection{Evaluation metric}\label{sec:PCK}
For evaluation on PF-WILLOW, PF-PASCAL, and SPair-71k, we use a common evaluation metric of percentage of correct keypoints (PCK), which counts the average number of correctly predicted keypoints given a tolerance threshold. Given predicted keypoint  and ground-truth keypoint , the prediction is considered correct if Euclidean distance between them is smaller than a given threshold. The correctness  of each keypoint can be expressed as 

where  and  are the width and height of either an entire image or object bounding box, , and  is a tolerance factor (in most cases, ). Note that PCK with  is a more stringent metric than one with . The final PCK of a benchmark is evaluated by averaging PCKs of all input image pairs. Following recent papers~\cite{han2017scnet, Rocco17, Rocco18, rocco2018neighbourhood}, we evaluate PF-WILLOW with  and PF-PASCAL with  using the same dataset split as in~\cite{rocco2018neighbourhood}. For SPair-71k, we use , which is more stringent. 

\begin{figure}[t]
    \begin{center}
\includegraphics[width=1.0\linewidth]{figures/AblationHyperpixels.pdf}
    \end{center}
     \vspace{-5.5mm}
    \caption{Hyperpixel layer search with ResNet-101 backbone on PF-PASCAL and SPair-71k datasets. Hyperpixel layers are in the order of selection during beam search. Dashed lines indicate PCKs when all layers of a CNN are used for hyperpixels. Best viewed in electronic form.}
    \vspace{-5.0mm}
    \label{fig:hyperpixelAblation}
\end{figure}

\begin{center}
    \begin{table*}
        \begin{center}
            \scalebox{0.65}{
            \begin{tabular}{c|c|cccccccccccccccccc|c}
            \hline
             \multicolumn{2}{c|}{Methods} & aero & bike & bird & boat & bottle & bus & car & cat & chair & cow & dog & horse & moto & person & plant & sheep & train & tv & all\\
\hline
\hline
            \multirow{4}{*}{\shortstack[1]{Transferred \\ \\ models}}
            & CNNGeo~\cite{Rocco17} & 21.3 & 15.1 & 34.6 & 12.8 & 31.2 & 26.3 & 24.0 & 30.6 & 11.6 & 24.3 & 20.4 & 12.2 & 19.7 & 15.6 & 14.3 & 9.6 & 28.5 & 28.8 & 18.1  \\
            & A2Net~\cite{paul2018attentive} &  20.8 & 17.1 & 37.4 & 13.9 & 33.6 & \underline{29.4} & \underline{26.5} & 34.9 & 12.0 & 26.5 & 22.5 & 13.3 & 21.3 & 20.0 & 16.9 & 11.5 & 28.9 & 31.6 & 20.1 \\
            & WeakAlign~\cite{Rocco18} & 23.4 & 17.0 & 41.6 & 14.6 & 37.6 & \underline{28.1} & \underline{26.6} & 32.6 & 12.6 & 27.9 & 23.0 & 13.6 & 21.3 & 22.2 & 17.9 & 10.9 & \underline{31.5} & 34.8 & 21.1 \\
            & NC-Net~\cite{rocco2018neighbourhood} & \underline{24.0} & 16.0 & \underline{45.0} & 13.7 & 35.7 & 25.9 & 19.0 & \underline{50.4} & \underline{14.3} & \underline{32.6} & \underline{27.4} & \underline{19.2} & \underline{21.7} & 20.3 & 20.4 & \underline{13.6} & \textbf{33.6} & \textbf{40.4} & \underline{26.4} \\
\hline
            \multirow{4}{*}{\shortstack[1]{SPair-71k \\ \\ trained \\ \\  models}}
            & CNNGeo~\cite{Rocco17} &  23.4 & 16.7 & 40.2 & 14.3 & 36.4 & 27.7 & 26.0 & 32.7 & 12.7 & 27.4 & 22.8 & 13.7 & 20.9 & 21.0 & 17.5 & 10.2 & 30.8 & 34.1 & 20.6  \\
            & A2Net~\cite{paul2018attentive} & 22.6 & \underline{18.5} & 42.0 & \textbf{16.4} & \underline{37.9} & \textbf{30.8} & \underline{26.5} & 35.6 & 13.3 & 29.6 & 24.3 & 16.0 & 21.6 & \underline{22.8} & \underline{20.5} & 13.5 & 31.4 & \underline{36.5} & 22.3 \\
            & WeakAlign~\cite{Rocco18} &  22.2 & 17.6 & 41.9 & \underline{15.1} & \textbf{38.1} & 27.4 & \textbf{27.2} & 31.8 & 12.8 & 26.8 & 22.6 & 14.2 & 20.0 & 22.2 & 17.9 & 10.4 & \underline{32.2} & 35.1 & 20.9 \\
            & NC-Net~\cite{rocco2018neighbourhood} & 17.9 & 12.2 & 32.1 & 11.7 & 29.0 & 19.9 & 16.1 & 39.2 & 9.9 & 23.9 & 18.8 & 15.7 & 17.4 & 15.9 & 14.8 & 9.6 & 24.2 & 31.1 & 20.1   \\
\hline
\multicolumn{2}{c|}{ HPF (ours)}  & \textbf{25.3} & \underline{18.5} & \underline{47.6} & 14.6 & 37.0 & 22.9 & 18.3 & \underline{51.1} & \underline{16.7} & \underline{31.5} & \underline{30.8} & \underline{19.1} & \underline{23.7} & \underline{23.8} & \textbf{23.5} & \underline{14.4} & 30.8 & \underline{37.2} & \underline{27.2} \\
            \multicolumn{2}{c|}{HPF (ours)}  & \underline{25.2} & \textbf{18.9} & \textbf{52.1} & \underline{15.7} & \underline{38.0} & 22.8 & 19.1 & \textbf{52.9} & \textbf{17.9} & \textbf{33.0} & \textbf{32.8} & \textbf{20.6} & \textbf{24.4} & \textbf{27.9} & \underline{21.1} & \textbf{15.9} & \underline{31.5} & 35.6 & \textbf{28.2} \\
            \hline
            \end{tabular}}
            \vspace{-1.5mm}
        \caption{\label{tab:hpftable}Per-class PCK () results on SPair-71k dataset. For transferred model, the original models trained on PASCAL-VOC~\cite{Rocco17, paul2018attentive} and PF-PASCAL~\cite{Rocco18, rocco2018neighbourhood}, which are provided by the authors, are used for evaluation.
Note that, for SPair-71k trained models, the transferred models are further finetuned on SPair-71k dataset by ourselves with our best efforts. Numbers in bold indicate the best performance and underlined ones are the second and third best.}
\vspace{-4.5mm}
\end{center}
    \end{table*}
\end{center} 

\begin{table}[t]
    \begin{center}
        \scalebox{0.9}{
            \begin{tabular}{l|c|cc}
                \hline
                Approach & Model & PCK & Time (\em ms) \\
                \hline\hline
                \multirow{4}{*}{\shortstack[1]{Image \\ alignment}} 
                 & CNNGeo~\cite{Rocco17}                & 69.5 & \underline{40} \\
                 & WeakAlign~\cite{Rocco18}             & 74.8 & 41 \\
                 & A2Net~\cite{paul2018attentive}       & 70.8 & 53 \\
                 & RTNs~\cite{NIPS2018_7851}            & 75.9 & 376 \\ 
                \hline
                \multirow{7}{*}{\shortstack[1]{Local \\ \\ region \\ \\ matching}} 
                 & SCNet~\cite{han2017scnet}            & 72.2 &  1000 \\
                 & PF~\cite{ham2016proposal}            & 62.5 &  1000 \\
                 & NC-Net~\cite{rocco2018neighbourhood} & 78.9 & 261  \\\cline{2-4}
                 & HPF w/ all layers & 74.5 & 324 \\
                 & HPF w/ all layers & 70.1 & 130 \\
                 & HPF    & \textbf{84.8} & 63 \\
                 & HPF     & \underline{83.4} & \underline{34} \\
                 & HPF    & \underline{81.1} & \textbf{19} \\
                \hline
            \end{tabular}
        }
    \end{center}
    \vspace{-4mm}
    \caption{\label{tab:ablationSpeed}Inference time comparison on PF-PASCAL benchmark. Hyperpixel layers of HPF are (4,7,11,12,13).}
\vspace{-2mm}
\end{table}

\subsection{Results and analysis}

\smallbreak
\noindent \textbf{Hyperpixel layers.} 
For PF-PASCAL, the hyperpixel layer results are  with ResNet-50 and  with ResNet-101. For SPair-71k, the results are  with ResNet-50 and  with ResNet-101. In order to analyze the effect of each intermediate feature  on hyperpixel, we have measured PCK of our model on both PF-PASCAL and SPair-71k in the order of the layer selection during beam search as shown in Figure~\ref{fig:hyperpixelAblation}. The dashed lines represent PCKs using all layers. Interestingly, in both cases, adding the second layer significantly boosts the performance of PCK, and only a few more layers are sufficient to achieve a comparable performance with the best one. 
After reaching an optimized set of layers, adding more damages the performance. 
This result demonstrates the effectiveness of hyperpixels compared to conventional hypercolumn features. 
The result also implies that features resolving local-ambiguity lie in between particular layers, \eg, between layer 20 and 30 in our case. 



\smallbreak
\noindent \textbf{Benchmark comparisons.} 
Table~\ref{tab:stdBenchmarkTable} summarizes comparison to recent methods on three standard benchmarks: PF-PASCAL, PF-WILLOW, and Caltech-101. In this experiment, the hyperpixels tuned using the validation split of PF-PASCAL are evaluated on the test split of PF-PASCAL, and futher evaluated on PF-WILLOW and Caltech-101 for checking transferability as done in~\cite{han2017scnet, jeon2018parn, NIPS2018_7851, Rocco17, Rocco18, paul2018attentive}. 
The results clearly show that the proposed method sets new state-of-the-art results on all the three benchmarks, proving the effectiveness of our approach. Note that all recent neural methods for semantic correspondence rely 
on ImageNet-pretrained features, and thus their performance depends on the backbone networks (indicated by subscripts).
As expected, our method using the stronger backbone of ResNet-101 improves the performance compared to using ResNet-50. 
Furthermore, using the backbone of FCN~\cite{lin2017feature} pretrained with PASCAL VOC 2012~\cite{everingham2015pascal}, that is a superset of our target dataset~\cite{ham2016proposal}, significantly boosts performance.  
This shows that our method is flexible in using backbone networks and can further improve by adopting a better one.

\smallbreak
\noindent \textbf{Degree of supervision.} 
Different methods in our comparison require different degrees of supervision in training as indicated in the second column of Table~\ref{tab:stdBenchmarkTable}. 
The only supervised part of our method is layer selection using a validation set, which can be very small as revealed by small-set experiments, and does not require additional learning: 
Instead of using all the 308 pairs of the original validation split of PF-PASCAL, 
the layer search algorithm is performed on  random pairs per class, for a total of  validation pairs. 
The average performances over 10 trials are shown along with their standard deviations in the set of rows with  at the bottom of Table~\ref{tab:stdBenchmarkTable}.
Using as little as one sample per class (20 image pairs total) as supervisory signal gives results comparable as using all 308 pairs, outperforming the previous state of the art. 
Given the cost of data collection and the total amount of user-provided information in weakly-supervised methods, we thus believe that our algorithm with small  values (e.g., ) is more cost effective and practical.


\smallbreak
\noindent \textbf{Effect of layer search.} To check the effect of layer search, we take random combinations of 8 layers (the same number chosen by our layer search) as a baseline. 
The average results over 10 trials are shown with their standard deviations in the last row of Table~\ref{tab:stdBenchmarkTable}.
Their much worse performance shows that our layer search is crucial. 






















\begin{table}[t]
    \begin{center}
        \scalebox{0.95}{
        \begin{tabular}{l|c|c}
            \hline
            \multirow{2}{*}{Matching module} & PF-PASCAL & PF-WILLOW \\
             &  &  \\
            \hline
            \hline
            NN w/   & 69.0 & 60.9 \\
            RHM w/  & 81.4 & 68.6 \\
            RHM w/  & 84.4 & 73.3 \\
            \rowcolor{mygray} RHM w/ * & \textbf{84.8} & \textbf{74.4} \\
            RHM w/  & \underline{84.8} & \underline{74.1} \\
            RHM w/  & \underline{84.5} & \underline{73.9} \\
            \hline
        \end{tabular}}
        \vspace{-1.5mm}
    \caption{\label{tab:ablationModule}Ablation studies on RHM with ResNet-101.}
    \vspace{-8.0mm}
    \end{center}
\end{table}


\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.144\textwidth}
        \includegraphics[width=1.0\linewidth, keepaspectratio]{figures/qualitative/a_source_image.pdf} 
        \caption{Source image}
    \end{subfigure}
    \hspace{-2.0mm}
    \begin{subfigure}{0.135\textwidth}
        \includegraphics[width=1.0\linewidth, keepaspectratio]{figures/qualitative/b_target_image.pdf}
        \caption{Target image}
    \end{subfigure}
    \hspace{-2.0mm}
    \begin{subfigure}{0.144\textwidth}
        \includegraphics[width=1.0\linewidth, keepaspectratio]{figures/qualitative/c_hyperpixel_flow.pdf}
        \caption{HPF (ours)}
    \end{subfigure}
    \hspace{-2.0mm}
    \begin{subfigure}{0.144\textwidth}
        \includegraphics[width=1.0\linewidth, keepaspectratio]{figures/qualitative/d_CNNGeo.pdf}
        \caption{CNNGeo~\cite{Rocco17}}
    \end{subfigure}
    \hspace{-2.0mm}
    \begin{subfigure}{0.144\textwidth}
        \includegraphics[width=1.0\linewidth, keepaspectratio]{figures/qualitative/e_A2Net.pdf}
        \caption{A2Net~\cite{paul2018attentive}}
    \end{subfigure}
    \hspace{-2.0mm}
    \begin{subfigure}{0.144\textwidth}
        \includegraphics[width=1.0\linewidth, keepaspectratio]{figures/qualitative/f_WeakAlign.pdf}
        \caption{WeakAlign~\cite{Rocco18}}
    \end{subfigure}
    \hspace{-2.0mm}
    \begin{subfigure}{0.144\textwidth}
        \includegraphics[width=1.0\linewidth, keepaspectratio]{figures/qualitative/g_NCNet.pdf}
        \caption{NC-Net~\cite{rocco2018neighbourhood}}
    \end{subfigure}
    \hspace{-2.0mm}
    \vspace{-3.0mm}
    \caption{Qualitative results on SPair-71k. The source images are transformed to target images using correspondences.}
    \label{fig:figure4}
    \vspace{-3.0mm}
\end{figure*}


\begin{center}
    \begin{table*}
        \begin{center}
            \scalebox{0.75}{
            \begin{tabular}{c|c|ccc|ccc|cccc|cccc|c}
            \hline
            \multicolumn{2}{c|}{\multirow{2}{*}{Methods}} & \multicolumn{3}{c|}{View-point} & \multicolumn{3}{c|}{Scale} & \multicolumn{4}{c|}{Truncation} & \multicolumn{4}{c|}{Occlusion} & \multirow{2}{*}{All} \\
             \multicolumn{2}{c|}{ }  & easy & medi & hard & easy & medi & hard & none & src & tgt & both & none & src & tgt & both & \\
            \hline
            \hline
            \multicolumn{2}{c|}{Identity mapping} & 7.3 & 3.7 & 2.6 & 7.0 & 4.3 & 3.3 & 6.5 & 4.8 & 3.5 & 5.0 & 6.1 & 4.0 & 5.1 & 4.6 & 5.6 \\
\hline
            \multirow{4}{*}{\shortstack[1]{Transferred \\ \\ models}}
            & CNNGeo~\cite{Rocco17}                & 25.2 & 10.7 & 5.9 & 22.3 & 16.1 & 8.5 & 21.1 & 12.7 & 15.6 & 13.9 & 20.0 & 14.9 & 14.3 & 12.4 & 18.1 \\
            & A2Net~\cite{paul2018attentive}       & 27.5 & 12.4 & 6.9 & 24.1 & 18.5 & 10.3 & 22.9 & 15.2 & 17.6 & 15.7 & 22.3 & 16.5 & 15.2 & 14.5 & 20.1 \\ 
            & WeakAlign~\cite{Rocco18}             & 29.4 & 12.2 & 6.9 & 25.4 & 19.4 & 10.3 & 24.1 & 16.0 & 18.5 & 15.7 & 23.4 & 16.7 & 16.7 & 14.8 & 21.1  \\ 
& NC-Net~\cite{rocco2018neighbourhood} & \underline{34.0} & \underline{18.6} & \underline{12.8} & \underline{31.7} & \underline{23.8} & \underline{14.2} & \underline{29.1} & \underline{22.9} & \underline{23.4} & \underline{21.0} & \underline{29.0} & \underline{21.1} & \underline{21.8} & \underline{19.6} & \underline{26.4}  \\ 
\hline
\multirow{4}{*}{\shortstack[1]{SPair-71k \\ \\ trained \\ \\  models}}
            & CNNGeo~\cite{Rocco17}                & 28.8 & 12.0 & 6.4 & 24.8 & 18.7 & 10.6 & 23.7 & 15.5 & 17.9 & 15.3 & 22.9 & 16.1 & 16.4 & 14.4 & 20.6 \\
            & A2Net~\cite{paul2018attentive}       & 30.9 & 13.3 & 7.4 & 26.1 & 21.1 & 12.4 & 25.0 & 17.4 & 20.5 & 17.6 & 24.6 & 18.6 & 17.2 & 16.4 & 22.3 \\ 
            & WeakAlign~\cite{Rocco18}             & 29.3 & 11.9 & 7.0 & 25.1 & 19.1 & 11.0 & 24.0 & 15.8 & 18.4 & 15.6 & 23.3 & 16.1 & 16.4 & 15.7 & 20.9 \\ 
            & NC-Net~\cite{rocco2018neighbourhood} & 26.1 & 13.5 & 10.1 & 24.7 & 17.5 & 9.9 & 22.2 & 17.1 & 17.5 & 16.8 & 22.0 & 16.3 & 16.3 & 15.2 & 20.1 \\ 
\hline
            \multicolumn{2}{c|}{HPF (ours)} & \underline{35.0} & \underline{18.9} & \underline{13.6} & \underline{32.0} & \underline{25.1} & \underline{15.4} & \underline{29.7} & \underline{24.5} & \underline{23.5} & \underline{22.9} & \underline{29.6} & \underline{22.9} & \underline{22.1} & \underline{21.3} & \underline{27.2} \\
            \multicolumn{2}{c|}{HPF (ours)} & \textbf{35.6} & \textbf{20.3} & \textbf{15.5} & \textbf{33.0} & \textbf{26.1} & \textbf{15.8} & \textbf{31.0} & \textbf{24.6} & \textbf{24.0} & \textbf{23.7} & \textbf{30.8} & \textbf{23.5} & \textbf{22.8} &  \textbf{21.8} & \textbf{28.2}   \\
            \hline
            \end{tabular}}
        \vspace{-2.0mm}
        \caption{\label{tab:HPFanalysesTable} PCK analysis on SPair-71k. Difficulty levels of view points and scales are labeled easy, medium, and hard, while those of truncation and occlusion are indicated by none, source, target, and both.} 
\vspace{-4.0mm}
        \end{center}
    \end{table*}
\end{center}

\smallbreak
\noindent \textbf{Comparison to proposal flow approach}~\cite{ham2016proposal}. The core differences between hyperpixel flow and proposal flow~\cite{ham2016proposal} are the changes in (1) matching primitives, from per-proposal geometric descriptor to hyperpixels, in order to handle problems of local-ambiguity and (2) matching algorithms, from PHM to RHM, in order to leverage hyperpixel geometry for efficiency. In Table~\ref{tab:stdBenchmarkTable}, significant performance improvements on three different benchmarks demonstrate that our features encoding high-level semantics while being agnostic to instance-specific details are crucial to establish robust correspondences. In addition, as shown in Table~\ref{tab:ablationSpeed}, the proposed voting method, RHM, with hyperpixels shows an impressive improvement in speed compared to~\cite{ham2016proposal}.
 
\smallbreak
\noindent \textbf{Inference time comparison.} With RHM, predicting dense correspondences for a single pair of images turns out to be much faster compared to other recent models. Table~\ref{tab:ablationSpeed} demonstrates the comparison of per-pair inference time on PF-PASCAL. While having more than 5\% improvements over current state-of-the-art approach~\cite{rocco2018neighbourhood}, the proposed model runs 4 to 13 times faster. With a slight trade-off on performance, hyperpixels with fewer layers and larger receptive field sizes enables real-time matching.

\smallbreak
\noindent \textbf{Ablation studies on matching.} 
To analyze the effects of RHM and its exponent factor  in similarity , we experiment with replacing RHM with na\"ive nearest neighbor matching (NN) and also varying exponent  of similarity. As shown in Table~\ref{tab:ablationModule}, the significant PCK gap between NN and RHM demonstrates the effectiveness of geometry matching. The performance improvement with  shows its effect of suppressing noisy votes in RHM.


\smallbreak
\noindent \textbf{Model analyses on SPair-71k benchmark.} We evaluate several recent methods~\cite{Rocco17, Rocco18, rocco2018neighbourhood, paul2018attentive} on our new benchmark dataset. In this experiment, our method tuned using the validation split of SPair-71k is evaluated on the test split of SPair-71k. For each method in comparison, we run two versions of each model: a trained model provided by the authors and the other further finetuned by ourselves on SPair-71k training set. The results are shown in Table~\ref{tab:hpftable}. We fail to successfully train the method of~\cite{Rocco18,rocco2018neighbourhood} on SPair-71k so that their performances drop when trained. We guess that their original learning objectives for weakly-supervised learning is fragile in presence of large view-point differences as in SPair-71k. We leave this issue for further investigation and will update the results at our benchmark page. 



SPair-71k has several annotation types such as view-point, scale, truncation and occlusion differences. In-depth analyses of each model using these annotations are summarized in Table~\ref{tab:HPFanalysesTable}. All models perform better with pairs of small differences, and view-point and scale differences significantly affect the performances. Yet, our method shows more robust results in terms of those variations compared to the others.
Figure~\ref{fig:figure4} shows some examples where our method finds reliable correspondences even under a large view-point and scale difference.












 \section{Conclusion}
We have proposed a fast yet effective semantic matching method, hyperpixel flow, which leverages an optimized set of convolutional layer features pre-trained on a classification task. The impressive performance of the proposed method, which is only tuned with a small vadidation split without any end-to-end training, indicates that using relevant levels of multiple neural features is crucial in semantic correspondence. We believe further research in this direction is needed together with feature learning.  
To this end, we have also introduced a large-scale dataset, SPair-71k, with richer annotations for in-depth analyses, which is intended to resolve drawbacks of existing semantic correspondence datasets and to serve for supervised end-to-end learning of semantic correspondence.  




\smallbreak
\noindent \textbf{Acknowledgements.}
This work is supported by Samsung Advanced Institute of Technology (SAIT) and Basic Science Research Program (NRF-2017R1E1A1A01077999), and also in part by the Inria/NYU collaboration and the Louis Vuitton/ENS chair on artificial intelligence.






 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
