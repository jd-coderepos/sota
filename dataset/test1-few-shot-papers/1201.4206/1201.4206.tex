\documentclass[a4paper]{article}
\usepackage{ifthen}
\usepackage{microtype}\usepackage{boxedminipage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{authblk}


\usepackage{amssymb,amsfonts,amsthm,epsfig}
\newtheorem{theorem}{Theorem}
\newtheorem*{thm}{Theorem}
\newtheorem{conj}{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lm}{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{cond}{Condition}
\newtheorem{proofi}{Proof Idea}

\newcommand{\MA}{\mathcal{MA}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\pr}{{\bf Pr}}
\newcommand{\eps}{{\epsilon}}
\newcommand{\C}[1]{{{C}^{(#1)}}}
\newcommand{\itS}[1]{{{S}^{(#1)}}}
\newcommand{\itT}[1]{{{T}^{(#1)}}}

\newcommand{\commented}{no}

\ifthenelse{\equal{\commented}{yes}}{
\newcommand{\rnote}[1]{\footnote{{\bf [[Ragesh: {#1}\bf ]] }}}
\newcommand{\anote}[1]{\footnote{{\bf [[Amit: {#1}\bf ]] }}}
\newcommand{\snote}[1]{\footnote{{\bf [[Sandeep: {#1}\bf ]] }}}
}

\ifthenelse{\equal{\commented}{no}}{
\newcommand{\jnote}[1]{}
\newcommand{\snote}[1]{}
\newcommand{\rnote}[1]{}
}


\bibliographystyle{plain}

\title{A simple -sampling based PTAS for -means and other Clustering problems}




\author[1]{Ragesh Jaiswal}
\author[1]{Amit Kumar}
\author[1]{Sandeep Sen} \affil[1]{Department of Computer Science and Engineering \\ IIT Delhi\\ \texttt{\{rjaiswal,amitk,ssen\}@cse.iitd.ac.in}}





\begin{document}

\maketitle

\begin{abstract}
Given a set of points , the -means clustering problem is to find a set of  {\em centers}  such that the  objective function , where  denotes the distance between  and the closest center in , is
minimized. This is one of the most prominent objective functions that have been studied with respect to clustering.

-sampling \cite{ArthurV07} is a simple non-uniform sampling technique for choosing points from a set of points. It works as follows: given a set of points , the first point is chosen uniformly at random from . Subsequently, a point from  is chosen as the next sample with probability proportional to the square of the distance of this point to the nearest previously sampled points.

-sampling has been shown to have nice properties with respect to the -means clustering problem. Arthur and Vassilvitskii \cite{ArthurV07} show that  points chosen as centers from  using -sampling gives an  approximation in expectation. Ailon et. al. \cite{AJMonteleoni09} and Aggarwal et. al. \cite{AggarwalDK09} extended results of \cite{ArthurV07} to show that  points chosen as centers using -sampling give  approximation to the -means objective function with high probability. In this paper, we further demonstrate the power of -sampling by giving a simple randomized -approximation algorithm that uses
the -sampling in its core.
\end{abstract}


\section{Introduction}
Clustering problems arise in diverse areas including  machine learning, data mining, image processing and
web-search \cite{broder97,faloutsos,deer90,Swain}.
One of the most commonly used clustering problems is the -means problem. Here,
we are given a set of points  in a -dimensional Euclidean space, and a parameter . The goal is
to find a set  of  {\em centers} such that the objective function  is minimized,
where  denotes the distance from  to the closest center in . This naturally partitions  into
 clusters, where each cluster corresponds to the set of points of  which are closer to a particular center
than other centers. It is also easy to show that the center of any cluster must be  the mean of the
points in it.
 In most applications, the parameter  is a small constant. However, this problem turns out to be
 NP-hard even for  \cite{das08}.

One very popular heuristic for solving the -means problem is the Lloyd's algorithm~\cite{lloyd}. The heuristic is
as follows : start with an arbitrary set of  centers as seeds. Based on these  centers, partition the
set of points into  clusters, where each point gets assigned to the closest center. Now, we update the
set of centers as the means of each of these clusters. This process is repeated till we get convergence. Although,
this heuristic often performs well in practice, it is known that it can get stuck in local minima~\cite{ArthurV06}.
There has been lot of recent research in understanding why this heuristic works fast in practice, and how it can be modified such that we can guarantee that the solution produced by this heuristic is always close to the optimal solution.

One such modification  is to carefully choose the set of initial  centers.
Ideally, we would like to pick these centers such that we have a center close to each of the optimal clusters.
Since we do not know the optimal clustering, we would like to make sure that these centers are well separated from each
other and yet, are representatives of the set of points. A recently proposed idea~\cite{OstrovskyRSS06,ArthurV07} is to
pick the initial centers using -sampling which can be described as follows.
The first center is picked uniformly at random from the set of
points . Suppose we have picked a set of  centers -- call this set .
Then a point  is chosen as the next center
with probability proportional to . This process is repeated till we have a set of  centers.


There has been lot of recent activity in understanding how good a set of centers picked by -sampling are (even
if we do not run the Lloyd's algorithm on these seed centers).
Arthur and  Vassilvitskii~\cite{ArthurV07} showed that if we pick  centers with -sampling,
then the expected cost of the corresponding solution to the -means instance is within -factor of
the optimal value. Ostrovsky et. al.~\cite{OstrovskyRSS06} showed that if the set of points satisfied a separation 
condition (named -irreducible as defined in Section~\ref{sec:pre}), then these  centers give a constant factor approximation for the -means problem. Ailon et. al.~\cite{AJMonteleoni09}
proved a bi-criteria approximation property -- if we pick  centers by -sampling, then it is
a constant approximation, where we compare with the optimal solution that is allowed to pick  centers only.
Aggarwal et. al.~\cite{AggarwalDK09} give an improved result and show that it is enough to pick  centers by
-sampling to get a constant factor bi-criteria approximation algorithm.

In this paper, we give yet another illustration of the power of the -sampling idea. We give a simple randomized
-approximation algorithm for the -means algorithm, where  is an arbitrarily small constant.
At the heart of our algorithm is the idea of -sampling -- given a set of already selected centers, we pick
a small set of points by -sampling with respect to these selected centers. Then, we pick the next
center as the centroid of a subset of these small set of points. By repeating this process of picking  centers
sufficiently many times, we can guarantee that with high probability, we will get a set of  centers whose objective
value is close to the optimal value. Further, the running time of our algorithm is  \footnote{ 
notation hides a  factor which simplifies the expression.}--
for constant value of , this is a linear time algorithm.
 It is important to note that PTAS with better running time are known
for this problem. Chen \cite{Chen06} give an  algorithm for any  and Feldman et al. \cite{FeldmanMS07} give an  algorithm.
However, these results often are quite involved, and use the notion of coresets. Our algorithm is simple, and only uses the concept of -sampling.

\subsection{Other Related Work}
There has been significant research on exactly solving the -means algorithm (see e.g.,~\cite{inaba}), but all of these algorithms
take  time. Hence, recent research on this problem has focused on obtaining fast -approximation
algorithms for any . Matousek~\cite{Matousek00} gave  a PTAS
with running time  . Badoiu et al.~\cite{BadoiuHI02}
gave an improved PTAS with running time
. de la Vega et al.~\cite{VegaKKR03} gave a PTAS which works well for points
in high dimensions. The running time of this algorithm is  where
.  Har-Peled et al.~\cite{Har-PeledM04}  proposed a PTAS whose running time
is . Kumar et al.~\cite{KumarSS10} gave the first
linear time PTAS for fixed  -- the running time of their algorithm is . Chen~\cite{Chen06}
used the a new coreset construction to  give a PTAS with improved running time of .
Recently, Feldman et al.~\cite{FeldmanMS07} gave a PTAS with running time  -- this is the fastest known PTAS (for fixed ) for this problem.

There has also been work on obtaining fast constant factor approximation algorithms for the -means problem
based on some properties of the input points (see e.g.~\cite{OstrovskyRSS06,AwasthiBS10}).

\subsection{Our Contributions}
In this paper, we give a simple PTAS for the -means problem based on the idea of -sampling. Our work
builds on and simplifies the result of Kumar et al.~\cite{KumarSS10}. We briefly describe their algorithm first.
It is well known that for the 1-mean problem, if we sample a set of  points uniformly at random, then
the mean of this set of sampled points is close to the overall mean of the set of all points. Their algorithm
begins by sampling  points uniformly at random. With reasonable probability, we would sample 
points from the largest cluster, and hence we could get a good approximation to the center corresponding to this cluster
(their algorithm tries all subsets of size  from the randomly sampled points). However, the other clusters
may be much smaller, and we may not have sampled enough points from them. So, they need to prune a lot of points
from the largest cluster so that in the next iteration a random sample of  points will contain 
points from the second largest cluster, and so on. This requires a  non-trivial idea termed as {\em tightness}
 condition by the authors. In this paper,
we show that the pruning is not necessary if  instead of using uniform random sampling, one uses -sampling.

We can informally describe our algorithm as follows. We maintain a set of
candidate centers , which is initially empty.
Given a set , , we add a new center to  as follows. We sample a set  of  points using
-sampling with respect to . From this set of sampled points, we pick a
subset  and the new center is
the mean of this set . We add this to  and continue.

From the property of -sampling (\cite{AggarwalDK09,AJMonteleoni09}),
with some constant, albeit small probability , we pick up a point from
a hitherto untouched cluster  of the {\em optimal clustering}. Therefore
by sampling about  points using -sampling, we expect to hit
approximately  points from . If  is large enough,
(c.f. Lemma~\ref{lem:inaba}), then the centroid of these  points gives a
 approximation of the cluster .
Therefore, with reasonable probability, there will be
a choice of a subset  in each iteration such that the set of
centers chosen are from . Since we do not know , our algorithm will
try out all subsets of size  from the sample .
Note that our algorithm is very
simple, and can be easily parallelized.
Our algorithm has running time  which is an improvement over that of Kumar et al.~\cite{KumarSS10} who
gave a PTAS with running time .
\footnote{It can be used in conjunction with Chen~\cite{Chen06} to obtain
a superior running time but at the cost of the simplicity of our approach}

Because of the relative simplicity, our algorithm generalizes to measures
like Mahalanobis distance and -similar Bregman divergence. Note that
these do not satisfy triangle inequality and therefore not strict metrics.
Ackermann et al. \cite{ab09} have generalized the framework of Kumar et al.
\cite{KumarSS10} to Bregman divergences but we feel that the -sampling based
algorithms are simpler.

We formally define the problem and give some preliminary results in Section~\ref{sec:pre}.
In Section~\ref{sec:algo}, we describe our algorithm, and then analyze it subsequently. In Section~\ref{sec:other}, we discuss PTAS for other distance measures. 





\section{Preliminaries}
\label{sec:pre}
An instance of the -means problem consists of a set  of  points in -dimensional space and a parameter .
For a set of points (called centers) , let  denote  i.e., the cost of the
solution which picks  as the set of centers. For a singleton , we shall often abuse notation,
and use  to denote .
 Let  denote the cost of the optimal -means
solution for .

\begin{definition}
Given a set of points  and a set of centers , a point  is said to be sampled using {\em -sampling}
with respect to  if the probability of it being sampled, ,  is given by

\end{definition}

We will also need the following definition from~\cite{KumarSS10}.
\begin{definition}[Irreducibility or separation condition] Given  and , a set of points  is said to be -irreducible if

\end{definition}


We will often appeal to the following result~\cite{inaba} which shows that uniform random sampling works well for -means\footnote{It turns out
that even minor perturbations from uniform distribution can be catastrophic and indeed in this paper we had to work around this.}.
\begin{lemma}[Inaba et al.~\cite{inaba}]
\label{lem:inaba}
Let  be a set of points obtained by independently sampling  points with replacement uniformly at random from a point set . Then, for any ,

holds with probability at least . 
Here  denotes the centroid of a point set .
\end{lemma}

Finally, we will use the following property of the squared Euclidean metric. This is a standard result from linear algebra \cite{ps05}.

\begin{lemma}\label{lemma:2}
Let  be any point set and let  be any point. Then we have the following:

where  denotes the centroid of the point set.
\end{lemma}

Finally, we mention the simple approximate triangle inequality with respect to the squared Euclidean distance measure.

\begin{lemma}[Approximate triangle inequality]\label{lemma:3}
For any three points  we have:

\end{lemma}





\section{PTAS for -means}
\label{sec:algo}
We first give a high level description behind the  algorithm. We will also assume that the instance is -irreducible for a
suitably small parameter . We shall then get rid of this assumption later. The algorithm is described
in Figure~\ref{fig:k}. Essentially, the algorithm maintains a set  of centers,
where . Initially  is empty, and in each iteration of Step 2(b), it adds one center to  till
its size reaches . Given a set , it samples  a set of  points from  using -sampling with respect
to  (in Step 2(b)). Then it picks  a subset  of  of size ,
and adds the centroid of  to . The algorithm
cycles through all possible subsets of size  of  as choices for , and for each such choice,
repeats the above steps to find the next center, and so on.  To make the presentation clearer,
we pick a -tuple of -size subsets  in advance, and when , we pick  as the  subset of . 
In Step 2(i), we cycle through all such -tuples . In the analysis, we just need
to show that {\em one} such -tuple works with reasonable probability.


We develop some notation first. For the rest of the analysis, we will fix a tuple  --
this will be the ``desired tuple'', i.e., the one for which we can show that the set  gives a good solution.
As our analysis proceeds, we will argue what properties this tuple should have. Let  be the set 
at the beginning of the  iteration of Step 2(b). To begin with  is empty. Let 
be the set  sampled during the  iteration of Step 2(b), and  be the corresponding set 
(which is the  subset of ).




Let  be the optimal clusters, and  denote the centroid  of points in . Further, let  denote ,
and wlog assume that . Note that  is same as
.
 Let  denote the average cost paid by a point in , i.e.,

We will assume that the input set of points  are
-irreducible. We shall remove this assumption later. Now we show that any two optimal centers are far enough.

\begin{center}
\begin{figure}
\begin{boxedminipage}{5in}
{\bf Find-k-means(P)}

\hspace{0.1in} Let , , and 

\hspace{0.1in} 1. {\bf Repeat}  times and output the the set of centers  that give least cost

\hspace{0.3in} 2. {\bf Repeat} for all -tuples  and

\hspace{0.3in} \ \ \ \ \ pick the set of centers  that gives least cost

\hspace{0.5in} \ \ \ (a) 

\hspace{0.5in} \ \ \ (b) For  to 

\hspace{0.7in}\ \ \ \ \  \ \ Sample a set  of  points with -sampling (w.r.t. centers )

\hspace{0.7in} \ \ \ \ \ \ \ Let  be the  subset of . \footnote{For a set of size  we consider an arbitrary ordering of the subsets of size  of this set.}


\hspace{0.7in} \ \ \ \ \ \ \ . \footnote{ denote the centroid of the points in .}

\end{boxedminipage}
\caption{The -means algorithm that gives -approximation for any -irreducible data set. Note that the inner loop is executed at most 
 times. }
\label{fig:k}
\end{figure}
\end{center}
\vspace*{-0.5in}

\begin{lemma}
\label{lem:min}
For any ,

\end{lemma}
\begin{proof}
Suppose , and hence .
For the sake of contradiction assume . Then we have,

This implies that the centers  give a -approximation to the
-means objective. This contradicts the assumption that  is -irreducible.
\end{proof}

We give an outline of the proof. Suppose in the first  iterations, we have found centers which are close to the centers of some 
 clusters in the optimal solution. Conditioned on this fact, we show that in the next iteration, we are likely to sample enough 
number of points from one of the remaining clusters (c.f. Corollary~\ref{cor:sample}). Further, we show that the samples from this new
cluster are close to uniform distribution (c.f. Lemma~\ref{lem:key}). Since such a sample does not come from exactly uniform distribution, we
cannot apply Lemma~\ref{lem:inaba} directly. In fact, dealing with the slight non-uniformity turns out to be non-trivial (c.f. Lemmas~\ref{lem:clinaba} and ~\ref{lem:final}). 

We now show that the following invariant will hold for all iterations  : let  consist of centers
 (added in this order). Then, with probability at least , there exist distinct
indices  such that
for all ,

Suppose this invariant holds for  (the base case is easy since  is empty).
We now show that this invariant holds for  as well. In other words, we just show that in the  iteration,
with probability at least ,  the
algorithm finds a center   such that
 where  is an index distinct from
.
This will basically show that at the end of the last iteration, we will have  centers that give a -approximation
with probability at least .


We now show that the invariant holds for . We use the notation developed above for . Let  denote
the set of indices . Now let  be the index  for which 
is maximum. Intuitively, conditioned on sampling from clusters in  using -sampling, it is  likely that
enough points from  will be sampled.
The next lemma shows that there is good chance that elements from the sets  for 
will be sampled.

\begin{lemma}
\label{lem:sampled}
 
\end{lemma}
\begin{proof}
Suppose, for the sake of contradiction, the above statement does not hold. Then,


But this contradicts the fact that  is -irreducible.
\end{proof}

\noindent
We get the following corollary easily.
\begin{corollary}
\label{cor:sample}

\end{corollary}

The above Lemma and its Corollary say that with probability at least ,
points in the set  will be sampled. However the points within  are not sampled uniformly.
Some points in  might be sampled with higher probability than other points.
In the next lemma, we show that each point will be sampled with certain minimum probability.

\begin{lemma}
\label{lem:key}
For any  and any point , 
\end{lemma}
\begin{proof}
Fix a point .
Let  be the
 index such that  is closest to  among all centers in .
We have

where the second inequality follows from the invariant condition for .
Also, we know that

So, we get

\end{proof}

Recall that  is the sample of size  in this iteration. We would like to show that  that the invariant
will hold in this iteration as well. We first prove a simple corollary of Lemma~\ref{lem:inaba}.

\begin{lemma}
\label{lem:clinaba}
Let  be a set of  points, and  be a parameter, . Define a random variable  as follows :
with probability , it picks an element of  uniformly at random, and with  probability , it does not
pick any element (i.e., is null). Let  be  independent copies of , where 
Let  denote the (multi-set) of elements of  picked by . Then, with probability at least ,
 contains a subset  of size  which satsifies

\end{lemma}
\begin{proof}
 Define a random variable , which is a subset of the index set , as follows
Q. Conditioned on , note that the random variables  are independent uniform samples from . Thus if , then Lemma~\ref{lem:inaba} implies that with
probability at least 0.8, the desired event~(\ref{eq:clinaba}) happens. But the expected value of  is , and so,
 with high probability, and hence, the statement in the lemma is true.
\end{proof}

\noindent
We are now ready to prove the main lemma.
\begin{lemma}
\label{lem:final}
With probability at least ,
there exists a subset  of  of size at most  such that

\end{lemma}
\begin{proof}
Recall that  contains  independent samples of  (using -sampling). We are interested in .
Let  be   independent random variables defined as follows : for any , ,  picks an element of  using -sampling with respect to
. If this element is not in , it just discards it (i.e.,  is null). Let  denote . Corollary~\ref{cor:sample}
 and Lemma~\ref{lem:key} imply that
 picks a particular element of  with probability at least . We would now like to apply Lemma~\ref{lem:clinaba}
(observe that ). We can do this by a simple coupling argument as follows.
For a particular element ,
suppose  assigns probability  to it.
 One way of sampling a random variable
 as in Lemma~\ref{lem:clinaba} is as follows --  first sample using . If  is null then,  is also null. Otherwise, suppose
 picks an element  of . Then,  is equal to  with probability , null otherwise. It is easy
to check that with probability ,  is a uniform sample from , and null with probability . Now, observe that
the set of elements of  sampled by  is always a superset of . We can now use Lemma~\ref{lem:clinaba}
to finish the proof.
\end{proof}

 Thus, we will take the index  in Step 2(i) as the index of the set  as guaranteed by the Lemma above.
Finally, by repeating the entire process  times, we make sure that we get a -approximate solution
with high probability.
Note that the total running time of our algorithm is
.

\noindent
{\bf Removing the -irreducibility assumption :} We now show how to remove this assumption. First note
that we have shown the following result.

\begin{theorem}
If a given point set -irreducible, then
there is an algorithm that gives a -approximation to the -means objective
and that runs in time .
\end{theorem}
\begin{proof}
The proof can be obtained by replacing  by  in the above analysis. \end{proof}

Suppose the point set  is not -irreducible.
In that case it will be sufficient to find fewer centers that -approximate the -means objective.
The next lemma shows this more formally.

\begin{theorem}
There is an algorithm that runs in time  and
gives a -approximation to the -means objective.
\end{theorem}
\begin{proof}
Let  denote the set of points.  Let  be the largest index such that  is -irreducible.
If no such  exists, then 
and so picking the centroid of  will give a -approximation.

Suppose such an  exists. In that case, we consider the -means problem and from the previous lemma
we get that there is an algorithm that runs in time  and gives a
-approximation to the -means objective.
Now we have that 
Thus, we are done.
\end{proof}





\section{Other Distance Measures}\label{sec:other}
In the previous sections, we looked at the -means problem where the dissimilarity or distance measure was the square of Euclidean distance. There are numerous practical clustering problem instances where the dissimilarity measure is not a function of the Euclidean distance. In many cases, the points are not generated from a metric space. In these cases, it makes sense to talk about the general -median problem that can be defined as follows:

\begin{definition}[-median with respect to a dissimilarity measure]
Given a set of  objects  and a dissimilarity measure , find a subset  of  objects (called medians) such that the following objective function is minimized:

\end{definition}

In this section, we will show that our algorithm and analysis can be easily generalized and extended to dissimilarity measures that satisfy some simple properties. We will look at some interesting examples.
We start by making the observation that in the entire analysis of the previous section the only properties of the distance measure that we used were given in Lemmas~\ref{lem:inaba}, \ref{lemma:2}, and \ref{lemma:3}. We also used the symmetry property of the Euclidean metric implicitly. This motivates us to consider dissimilarity measures on spaces where these lemmas  (or mild relaxations of these) are true. For such measures, we may replace  (this is the square of the Euclidean distance) by  in all places in the previous section and obtain a similar result. We will now formalize these ideas.

First, we will describe a property that captures Lemma~\ref{lem:inaba}. This is similar to a definition by Ackermann et. al. \cite{abs10} who discuss PTAS for the -median problem with respect to metric and non-metric distance measures.

\begin{definition}[-Sampling property]
Given  and , a distance measure  over space  is said to have -sampling property if the following holds:
for any set , a uniformly random sample  of  points from  satisfies

where  denotes the mean of points in .
\end{definition}

\begin{definition}[Centroid property]
A distance measure  over space  is said to satisfy the centroid property if for any subset  and any point , we have:

where  denotes the mean of the points in .
\end{definition}

\begin{definition}[-approximate triangle inequality]
Given , a distance measure  over space  is said to satisfy -approximate triangle inequality if for any three points 
\end{definition}

\begin{definition}[-approximate symmetry]
Given , a distance measure  over space  is said to satisfy -symmetric property if for any pair of points , 
\end{definition}

The next theorem gives the generalization of our results for distance measures that satisfy the above basic properties. The proof of this theorem follows easily  from the analysis in the previous section. 
The proof of this theorem is given in Appendix~\ref{appendix:A}.

\begin{theorem}\label{thm:other}
Let . Let , , and  be constants and let . Let . Let  be a distance measure over space  that  follows:
\begin{enumerate}
\item -approximate symmetry property,

\item -approximate triangle inequality,

\item Centroid property, and

\item -sampling property.
\end{enumerate}
Then there is an algorithm that runs in time  and gives a -approximation to the -median objective for any point set .
\end{theorem}

The above theorem gives a characterization for when our non-uniform sampling based algorithm can be used to obtain a PTAS for a dissimilarity measure. The important question now is whether there exist interesting distance measures that satisfy the properties in the above Theorem. Next, we look at some distance measures other than squared Euclidean distance, that satisfy such properties.

\subsection{Mahalanobis distance}

Here the domain is  and the distance is defined with respect to a positive definite matrix . The distance between two points  is given by . Now, we discuss the properties in Theorem~\ref{thm:other}.
\begin{enumerate}
\item ({\it Symmetry}) For any pair of points , we have . So, the -approximate symmetry property holds for .

\item ({\it Triangle inequality}) \cite{ab09} shows that -approximate triangle inequality holds for .

\item ({\it Centroid}) The centroid property is shown to hold for Mahalanobis distance in \cite{ban05}.

\item ({\it Sampling}) \cite{abs10} (see Corollary 3.7) show that Mahalanobis distance satisfy the -sampling property for .
\end{enumerate}

\noindent
Using the above properties and Theorem~\ref{thm:other}, we get the following result.

\begin{theorem}[-median w.r.t. Mahalanobis distance]
Let . There is an algorithm that runs in time  and gives a -approximation to the -median objective function w.r.t. Mahalanobis distances for any point set .
\end{theorem}

\subsection{-similar Bregman divergence}

We start by defining Bregman divergence and then discuss the required properties.

\begin{definition}[Bregman Divergence]
Let  be a continuously-differentiable real-valued and strictly convex function defined on a closed convex set .The Bregman distance associated with  for points   is:

Where  denotes the gradient of  at point 
\end{definition}

Intuitively this can be thought of as the difference between the value of  at point  and the value of the first-order Taylor expansion of  around point  evaluated at point . Bregman divergence includes the following popular distance measures:
\begin{itemize}
\item {\em Euclidean distance.} . Here .

\item {\em Kullback-Leibler divergence.} . Here .

\item {\em Itakura-Saito divergence.} . Here .

\item {\em Mahalanobis distance.} For a symmetric positive definite matrix , the Mahalanobis distance is defined as:
.
Here .
\end{itemize}

Bregman divergences have been shown to satisfy the Centroid property by Banerjee et. al. \cite{ban05}. All Bregman divergences do not necessarily satisfy the symmetry property or the triangle inequality. So, we cannot hope to use our results for the class of all Bregman divergences. On the other hand, some of the Bregman divergences that are used in practice satisfy a property called {\em -similarity} (see \cite{a09} for an overview of such Bregman divergences). Next, we give the definition of -similarity.

\begin{definition}[-similar Bregman divergence]
A Bregman divergence  on domain  is called -similar for constant , if there exists a symmetric positive definite matrix  such that for Mahalanobis distance  and for each  we have:

\end{definition}

Now, a -similar Bregman divergence can easily be shown to satisfy approximate symmetry and triangle inequality properties. This is formalized in the following simple lemma. The proof of this lemma is given in the Appendix~\ref{appendix:B}.

\begin{lemma}\label{lemma:mu-similar}
Let . Any -similar Bregman divergence satisfies the -approximate symmetry property and -approximate triangle inequality.
\end{lemma}

Finally, we use the sampling property from Ackermann et. al. \cite{abs10} who show that any -similar Bregman divergence satisfy the -sampling property for .

Using all the results mentioned above we get the following Theorem for -similar Bregman divergences.

\begin{theorem}[-median w.r.t. -similar Bregman divergences]
Let  and . There is an algorithm that runs in time  and gives a -approximation to the -median objective function w.r.t. -similar Bregman divergence for any point set .
\end{theorem}

\bibliography{paper}





\appendix

\section{Proof of Theorem~\ref{thm:other}}\label{appendix:A}
Here we give a proof of Theorem~\ref{thm:other}. For the proof, we repeat the analysis in Section~\ref{sec:algo} almost word-by-word. One the main things we will be doing here is replacing all instances of  in Section~\ref{sec:algo} with . So, this section will look very similar to Section~\ref{sec:algo}. 
First we will restate Theorem~\ref{thm:other}.

\begin{thm}[Restatement of Theorem~\ref{thm:other}]
Let . Let , , and  be constants and let . Let . Let  be a distance measure over space  that  follows:
\begin{enumerate}
\item -approximate symmetry property,

\item -approximate triangle inequality,

\item Centroid property, and

\item -sampling property.
\end{enumerate}
Then there is an algorithm that runs in time  and gives a -approximation to the -median objective for any point set .
\end{thm}

We will first assume that the instance is -irreducible for a suitably small parameter . 
We shall then get rid of this assumption later as we did in Section~\ref{sec:algo}. 
The algorithm remains the same and is described in Figure~\ref{fig:k-repeat}. 

We develop some notation first. For the rest of the analysis, we will fix a tuple  --
this will be the ``desired tuple'', i.e., the one for which we can show that the set  gives a good solution.
As our analysis proceeds, we will argue what properties this tuple should have. Let  be the set 
at the beginning of the  iteration of Step 2(b). To begin with  is empty. Let 
be the set  sampled during the  iteration of Step 2(b), and  be the corresponding set 
(which is the  subset of ).

Let  be the optimal clusters, and  denote the respective optimal cluster centers. 
Further, let  denote , and wlog assume that . 
Let  denote the average cost paid by a point in , i.e.,


\begin{center}
\begin{figure}
\begin{boxedminipage}{5in}
{\bf Find-k-median(P)}

\hspace{0.1in} Let , , , and 

\hspace{0.1in} 1. {\bf Repeat}  times and output the the set of centers  that give least cost

\hspace{0.3in} 2. {\bf Repeat} for all -tuples  and

\hspace{0.3in} \ \ \ \ \ pick the set of centers  that gives least cost

\hspace{0.5in} \ \ \ (a) 

\hspace{0.5in} \ \ \ (b) For  to 

\hspace{0.7in}\ \ \ \ \  \ \ Sample a set  of  points with -sampling (w.r.t. centers )

\hspace{0.7in} \ \ \ \ \ \ \ Let  be the  subset of . \footnote{For a set of size  we consider an arbitrary ordering of the subsets of size  of this set.}


\hspace{0.7in} \ \ \ \ \ \ \ . \footnote{ denote the centroid of the points in .}

\end{boxedminipage}
\caption{The algorithm that gives -approximation for any -irreducible data set. 
Note that the inner loop is executed at most   times. }
\label{fig:k-repeat}
\end{figure}
\end{center}

First, we show that any two optimal centers are far enough.
\begin{lemma}
For any ,

\end{lemma}
\begin{proof}
Since , we have .
For the sake of contradiction assume . Then we have,

This implies that the centers  give a -approximation to the
-median objective. This contradicts the assumption that  is -irreducible.
\end{proof}

The above lemma gives the following Corollary that we will use in the rest of the proof.

\begin{corollary}\label{lem:min-repeat}
For any , .
\end{corollary}
\begin{proof}
If , then we have  from the above lemma and hence . In case , then the above lemma gives . Using -approximate symmetry property we get the statement of the corollary.
\end{proof}

We give an outline of the proof. Suppose in the first  iterations, we have found centers which are close to the centers of some  clusters in the optimal solution. Conditioned on this fact, we show that in the next iteration, we are likely to sample enough number of points from one of the remaining clusters (c.f. Corollary~\ref{cor:sample-repeat}). Further, we show that the samples from this new cluster are close to uniform distribution (c.f. Lemma~\ref{lem:key-repeat}). Since such a sample does not come from exactly uniform distribution, we cannot use the -sampling property directly. 
In fact, dealing with the slight non-uniformity turns out to be non-trivial (c.f. Lemmas~\ref{lem:clinaba-repeat} and ~\ref{lem:final-repeat}). 

We now show that the following invariant will hold for all iterations  : let  consist of centers
 (added in this order). Then, with probability at least , there exist distinct
indices  such that
for all ,

Where  is a fixed constant that depends on  and . With foresight, we fix the value of .
Suppose this invariant holds for  (the base case is easy since  is empty).
We now show that this invariant holds for  as well. In other words, we just show that in the  iteration,
with probability at least ,  the
algorithm finds a center   such that
 where  is an index distinct from
.
This will basically show that at the end of the last iteration, we will have  centers that give a -approximation
with probability at least .


We now show that the invariant holds for . We use the notation developed above for . Let  denote
the set of indices . Now let  be the index  for which 
is maximum. Intuitively, conditioned on sampling from clusters in  using -sampling, it is  likely that
enough points from  will be sampled.
The next lemma shows that there is good chance that elements from the sets  for 
will be sampled.

\begin{lemma}
\label{lem:sampled-repeat}
 
\end{lemma}
\begin{proof}
Suppose, for the sake of contradiction, the above statement does not hold. Then,


But this contradicts the fact that  is -irreducible.
\end{proof}

\noindent
We get the following corollary easily.
\begin{corollary}
\label{cor:sample-repeat}

\end{corollary}

The above Lemma and its Corollary say that with probability at least ,
points in the set  will be sampled. However the points within  are not sampled uniformly.
Some points in  might be sampled with higher probability than other points.
In the next lemma, we show that each point will be sampled with certain minimum probability.

\begin{lemma}
\label{lem:key-repeat}
For any  and any point , 
\end{lemma}
\begin{proof}
Fix a point .
Let  be the
 index such that  is closest to  among all centers in .
We have

where the last inequality follows from the invariant condition for .
Also, we know that the following inequalities hold:






Inequalities (\ref{u2}), (\ref{u3}), and (\ref{u4}) gives the following:

Using (\ref{u1}) and (\ref{u7}), we get the following:

Using the previous inequality and (\ref{u6}) we get the following:

So, we get

\end{proof}

Recall that  is the sample of size  in this iteration. We would like to show that  that the invariant
will hold in this iteration as well. We first prove a simple corollary of Lemma~\ref{lem:inaba}.

\begin{lemma}
\label{lem:clinaba-repeat}
Let  be a set of  points, and  be a parameter, . Define a random variable  as follows :
with probability , it picks an element of  uniformly at random, and with  probability , it does not
pick any element (i.e., is null). Let  be  independent copies of , where 
Let  denote the (multi-set) of elements of  picked by . Then, with probability at least ,
 contains a subset  of size  which satsifies

\end{lemma}
\begin{proof}
 Define a random variable , which is a subset of the index set , as follows
Q. Conditioned on , note that the random variables  are independent uniform samples from . 
Thus if , then sampling property wrt.  implies that with 
probability at least 0.8, the desired event~(\ref{eq:clinaba-repeat}) happens. 
But the expected value of  is , and so,  with high probability, and hence, the statement in the lemma is true.
\end{proof}

\noindent
We are now ready to prove the main lemma.
\begin{lemma}
\label{lem:final-repeat}
With probability at least , there exists a subset  of  of size at most  such that

\end{lemma}
\begin{proof}
Recall that  contains  independent samples of  (using -sampling). We are interested in .
Let  be   independent random variables defined as follows : for any , ,  picks an element of  using -sampling with respect to . 
If this element is not in , it just discards it (i.e.,  is null). 
Let  denote . Corollary~\ref{cor:sample-repeat} and Lemma~\ref{lem:key-repeat} imply that  picks a particular element of  with probability at least . 
We would now like to apply Lemma~\ref{lem:clinaba-repeat} (observe that ). 
We can do this by a simple coupling argument as follows.
For a particular element , suppose  assigns probability  to it.
One way of sampling a random variable  as in Lemma~\ref{lem:clinaba-repeat} is as follows --  first sample using . If  is null, then  is also null. Otherwise, suppose  picks an element  of . 
Then  is equal to  with probability , and null otherwise. 
It is easy to check that with probability ,  is a uniform sample from , and null with probability . 
Now, observe that the set of elements of  sampled by  is always a superset of . 
We can now use Lemma~\ref{lem:clinaba-repeat} to finish the proof.
\end{proof}

 Thus, we will take the index  in Step 2(i) as the index of the set  as guaranteed by the Lemma above.
Finally, by repeating the entire process  times, we make sure that we get a -approximate solution
with high probability.
Note that the total running time of our algorithm is .

\noindent
{\bf Removing the -irreducibility assumption :} We now show how to remove this assumption. First note
that we have shown the following result.

\begin{theorem}
If a given point set -irreducible, then
there is an algorithm that gives a -approximation to the -median objective
with respect to distance measure  and that runs in time .
\end{theorem}
\begin{proof}
The proof can be obtained by replacing  by  in the above analysis. \end{proof}

Suppose the point set  is not -irreducible.
In that case it will be sufficient to find fewer centers that -approximate the -median objective.
The next lemma shows this more formally.

\begin{theorem}
There is an algorithm that runs in time  and
gives a -approximation to the -median objective with respect to .
\end{theorem}
\begin{proof}
Let  denote the set of points.  Let  be the largest index such that  is -irreducible.
If no such  exists, then 
and so picking the centroid of  will give a -approximation.

Suppose such an  exists. In that case, we consider the -median problem and from the previous lemma
we get that there is an algorithm that runs in time  
and gives a
-approximation to the -median objective.
Now we have that 
Thus, we are done.
\end{proof} 

\section{Proof of Lemma~\ref{lemma:mu-similar}}\label{appendix:B}

Here we give the proof of Lemma~\ref{lemma:mu-similar}. For better readability, we first restate the Lemma.
\begin{lm}[Restatement of Lemma~\ref{lemma:mu-similar}]
Let . Any -similar Bregman divergence satisfies the -approximate symmetry property and -approximate triangle inequality.
\end{lm}

The above lemma follows from the next lwo sub-lemmas.

\begin{lemma}[Symmetry for -similar Bregman divergence]
Let . Consider a -similar Bregman divergence  on domain . For any two points , we have:

\end{lemma}
\begin{proof}
Using equation(\ref{eqn:similar}) we get the following:

\end{proof}

\begin{lemma}[Triangle inequality for -similar Bregman divergence]
Let . Consider a -similar Bregman divergence  on domain . For any three points , we have:

\end{lemma}
\begin{proof}
We have:

The first and third inequality is using equation~\ref{eqn:similar} and the second inequality is using the approximate triangle inequality for Mahalanobis distance.
\end{proof}
\end{document}
