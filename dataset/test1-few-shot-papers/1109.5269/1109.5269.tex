\documentclass[11pt,a4paper]{article}

\usepackage{comment}

\usepackage[paper=a4paper,margin=3.9cm]{geometry}


\usepackage[backend=bibtex,
firstinits,style=numeric-comp,maxnames=99]{biblatex} 
\renewcommand*{\multicitedelim}{\addcomma\space} 
\bibliography{shortnames.bib, bib-latest.bib}




\usepackage{graphics}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmiccomment}[1]{// #1}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{amsmath, amsfonts, amsthm}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage[T1]{fontenc}
\usepackage[normalem]{ulem}
\usepackage{braket}
\usepackage{tikz}
\usetikzlibrary{calc}

\newcommand{\QM}{\ensuremath{M}}
\newcommand{\QDloc}{\ensuremath{\calD}}
\newcommand{\Dloc}{\ensuremath{D}}
\newcommand{\NULL}{\textsc{Null}}
\newcommand{\TRUE}{\textsc{True}}
\newcommand{\FALSE}{\textsc{False}}



\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}[]
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}





\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\calA}{\ensuremath{\mathcal{A}}\xspace}
\newcommand{\calB}{\ensuremath{\mathcal{B}}\xspace}
\newcommand{\calD}{\ensuremath{\mathcal{D}}\xspace}
\newcommand{\calL}{\ensuremath{\mathcal{L}}\xspace}
\newcommand{\calM}{\ensuremath{\mathcal{M}}\xspace}
\newcommand{\calT}{\ensuremath{\mathcal{T}}\xspace}
\newcommand{\calQ}{\ensuremath{\mathcal{Q}}\xspace}
\newcommand{\bbN}{\ensuremath{\mathbb{N}}\xspace}
\newcommand{\defeq}{\stackrel{\scriptscriptstyle \text{def}}{=}}


\newcommand{\ileft}{\ensuremath{i_\text{left}}\xspace}
\newcommand{\iright}{\ensuremath{i_\text{right}}\xspace}


\newcommand{\SigmaT}{\ensuremath{\Sigma_{\textup{T}}}}
\newcommand{\SigmaP}{\ensuremath{\Sigma_{\textup{P}}}}
\newcommand{\Sm}{\ensuremath{S}}                        \newcommand{\SmFilt}{\ensuremath{S_{\textup{filt}}}}    \newcommand{\SigmaFilt}{\ensuremath{\Sigma_{\textup{filt}}}}
\newcommand{\SigmaLast}{\ensuremath{\Sigma_{\textup{last}}}}

\newcommand{\str}[5]{\ensuremath{#1[#2]\,#1[#3]\,#1[#4]\cdots #1[#5]}}

\newcommand{\strperm}[6]{\ensuremath{#1(#2[#3])\,#1(#2[#4])\,#1(#2[#5])\cdots #1(#2[#6])}}

\newcommand{\pref}{{\textup{pref}}}
\newcommand{\suf}{{\textup{suf}}}
\newcommand{\pareq}{\ensuremath{\stackrel{\scriptscriptstyle \textup{p}}{=}}}
\newcommand{\upto}{\ensuremath{,\,\,}} \newcommand{\pred}[1]{\ensuremath{\textup{pred\textbf{(}}#1{\textup{\textbf{)}}}}}
\newcommand{\wildcard}{\ensuremath{\star}}
\newcommand{\predwild}[1]{\ensuremath{\textup{pred}^\wildcard\textup{\textbf{(}}#1{\textup{\textbf)}}}}

\newcommand{\ID}{\ensuremath{\text{ID}}}
\newcommand{\peq}{\stackrel{\scriptscriptstyle p}{=}}

\newcommand{\mask}{\ensuremath{\text{mask}}}
\newcommand{\diff}{\ensuremath{\text{diff}}}
\newcommand{\lft}{\ensuremath{\text{left}}}
\newcommand{\rgt}{\ensuremath{\text{right} }}
\newcommand{\plog}{\ensuremath{\text{plog}}}

\newcommand{\pre}{\ensuremath{\textup{pred}}}
\newcommand{\phid}[1]{\ensuremath{\phi \text{\big(} #1 \text{\big)}}}
\newcommand{\Til}[2]{\ensuremath{T_{#2}^{#1}}}

\newcommand{\fplong}[1]{\ensuremath{\Phi_{#1}}\xspace}
\newcommand{\fphalf}[1]{\ensuremath{(\Phi_{#1} \ominus \Phi_{{#1}-1})}\xspace}
\newcommand{\fpinner}[1]{\ensuremath{\Phi^0_{#1}}\xspace}
\newcommand{\fpdiff}{\ensuremath{\Delta}}


\newcommand{\pmatch}{\mbox{p-match}\xspace}
\newcommand{\pmatches}{\mbox{p-matches}\xspace}
\newcommand{\pperiod}{\mbox{p-period}\xspace}

\newcommand{\prooflemma}[1]{\textbf{Proof of Lemma~\ref{#1}}}
\newcommand{\prooftheorem}[1]{\textbf{Proof of Theorem~\ref{#1}}}
\newcommand{\prooffact}[1]{\textbf{Proof of Fact~\ref{#1}}}
\newcommand{\paradistance}{\vspace{-3mm}}

\newcommand{\fsplit}{\ensuremath{\ominus}}
\newcommand{\fmerge}{\ensuremath{\oplus}}
\newcommand{\fzero}{\ensuremath{\circledcirc}}

\newcommand{\procA}{\textup{A}\xspace}
\newcommand{\procB}{\textup{B}\xspace}
\newcommand{\procBdelta}{\textup{B1}\xspace}
\newcommand{\procBphi}{\textup{B2}\xspace}
\newcommand{\procC}{\textup{C}\xspace}

\newcommand{\insertdiagram}[1]{
    \centering
    \includegraphics[scale=0.70]{./diagrams/#1}}

\newcommand{\insertfigure}[2]{
    \begin{figure}[t]
        \centering
        \insertdiagram{#1}
        \caption{#2}
    \end{figure}
    }




\newcommand{\margin}[1]{\marginpar{\fbox{\parbox{2.5cm}{\footnotesize \raggedright #1}}}}
\title{Parameterized Matching in the Streaming Model} 



\author{
    Markus Jalsenius\thanks{~Department of Computer Science, University of Bristol,  U.K.} \and
    Benny Porat\thanks{~Department of Computer Science, Bar-Ilan University, Israel.}\and
    Benjamin Sach\thanks{~Department of Computer Science, University of Warwick, U.K.}}





\date{}


\begin{document}

\maketitle

\begin{abstract}

We study the problem of parameterized matching in a stream where we want to output matches between a pattern of length  and the last  symbols of the stream before the next symbol arrives. Parameterized matching is a natural generalisation of exact matching where an arbitrary one-to-one relabelling of pattern symbols is allowed. We show how this problem can be solved in constant time per arriving stream symbol and sublinear, near optimal space with high probability. Our results are surprising and important: it has been shown that almost no streaming pattern matching problems can be solved (not even randomised) in less than  space, with exact matching as the only known problem to have a sublinear, near optimal space solution. Here we demonstrate that a similar sublinear, near optimal space solution is achievable for an even more challenging problem. The proof is considerably more complex than that for exact matching.
 \end{abstract}

\section{Introduction}

We consider the problem of pattern matching in a stream where we want to output matches between a pattern of length  and the last  symbols of the stream. Each answer must be reported before the next symbol arrives. The problem we consider in this paper is known as \emph{parameterized matching} and is a natural generalisation of exact matching where an arbitrary one-to-one relabelling of the pattern symbols is allowed (one per alignment). For example, if the pattern is \texttt{abbca} then there there is a parameterized match with \texttt{bddcb} as we can apply the relabelling \texttt{a}\texttt{b}, \texttt{b}\texttt{d}, \texttt{c}\texttt{c}. There is however no parameterized match with \texttt{bddbb}. We show how this streaming pattern matching problem can be solved in near constant time per arriving stream symbol and sublinear, near optimal, space with high probability. The space used is reduced even further when only a small subset of the symbols are allowed to be relabelled.  As discussed in the next section, our results demonstrate a serious push forward in understanding what pattern matching algorithms can be solved in sublinear space.



\subsection{Background}

Streaming algorithms is a well studied area and specifically finding patterns in a stream is a fundamental problem that has received increasing attention over the past few years. It was shown in~\cite{CEPP:2008} that many offline algorithms can be made online (streaming) and deamortised with a  factor overhead in the time complexity per arriving symbol in the stream, where  is the length of the pattern. There have also been improvements for specific pattern matching problems but they all have one property in common: space usage is  words. It is not difficult to show that we in fact \emph{need} as much as  space to do pattern matching, unless errors are allowed.
The field of pattern matching in a stream took a significant step forwards in 2009 when it was shown to be possible to solve exact matching using only  words of space and  time per new stream symbol~\cite{Porat:09}.  This method, which is based on fingerprints, correctly finds all matches with high probability. The initial approach was subsequently somewhat simplified~\cite{EJS:2010} and then finally improved to run in constant time~\cite{BG:2011} within the same space requirements.

Being able to do exact matching in sublinear space raised the question of what other streaming pattern matching problems can be solved in small space. In~2011 this question was answered for a large set of such problems~\cite{CJPS:2011}. The result was rather gloomy: almost no streaming pattern matching problems can be solved in sublinear space, not even using randomised algorithms. An  space lower bound was given for , , , Hamming, edit distance and pattern matching with wildcards as well as for any algorithm that computes the cross-correlation/convolution. So what other pattern matching problems could possibly be solved in small space? It seems that the only hope to find any is by imposing various restrictions on the problem definition. This was indeed done in~\cite{Porat:09} where a solution to -mismatch (exact matching where up to  mismatches are allowed) was given which uses  time per arriving stream symbol and  words of space. The solution involves multiple instances of the exact matching algorithm run in parallel. Note that the space bound approaches  as  increases, so the algorithm is only interesting for sufficiently small . Further, the space bound is very far from the known  lower bound.
We also note that it is straightforward to show that exact matching with ~wildcards in the pattern can be solved with the -mismatch algorithm. To our knowledge, no other streaming pattern matching have been solved in sublinear space so far.

In this paper we present the first push forward since exact matching by giving a sublinear, near optimal space and near constant time (or constant with a mild restriction on the alphabet) algorithm for parameterized matching in a stream. This natural problem turns out to be significantly more complicated to solve than exact matching and our results provide the first demonstration that small space and time bounds are achievable for a more challenging problem. Note that our space bound, as opposed to -mismatch, is essentially optimal like for exact matching.
One could easily argue that our results are surprising, and yet again the question of what other problems are solvable in sublinear space calls for an answer. In particular, given that restrictions to the problem have to be made, what restrictions should one make to break the  space barrier.



\subsection{Problem definition and related work}\label{sec:probdef}

A pattern  of length  is said to \emph{parameterize match}, or \emph{\pmatch} for short, an ~length string  if there is an injective (one-to-one) function  such that  for all .
In our streaming setting, the pattern is known in advance and
the symbols of the stream  arrive one at a time.  We use the letter  to denote the index of the latest symbol in the stream. Our task is to output whether there is a \pmatch between  and  before  arrives. The mapping  may be distinct for each .

One may view this matching problem as that of finding matches in a stream encrypted using a substitution cipher. In offline settings, parameterized matching has its origin in finding duplication and plagiarism in software code although has since found numerous
other applications.  Since the first introduction of the problem, a great deal of work has gone into
its study in both theoretical and practical settings (see
e.g.\@~\cite{Baker:1993,AFM:1994,
  Baker:1995,Baker:1996,Baker:1997,HLS:2007}). Notably, in an offline
setting, the exact parameterized matching problem can be solved in
near linear time using a variant~\cite{AFM:1994} of the classic linear
time exact matching algorithm KMP~\cite{Knuth:1977}.

When the sublinear space algorithm for exact matching was given in~\cite{Porat:09},
properties of the periods of strings formed a crucial part of their analysis. However, when considering parameterized matching the period of a string is a much less straightforward concept than it is for exact matching.
For example, it is no longer true that consecutive matches must either be separated by the period of the pattern or be at least  symbols apart. This property, which holds for exact but not parameterized matching, allows for an efficient encoding of the positions of the matches. This was crucial to reducing the space requirements of the previous streaming algorithms.  Unfortunately, parameterized matches can occur at arbitrary positions in the stream, requiring new insights. This is not the only challenge that we face.

A natural way to match two strings under parameterization is to consider
their \emph{predecessor strings}. For a string , the predecessor
string, denoted , is a string of length  such that
 is the distance, counted in numbers of symbols, to the
previous occurrence of the symbol  in . In other words,
, where  is the smallest positive value for
which . Whenever no such  exists, we set
.
As an example, if  then .
We can perform parameterized matching offline by only considering predecessor strings using the fundamental fact~\cite{Baker:1993} that two equal length strings  and  \pmatch iff .
A plausible approach for our streaming problem would now be to
translate the problem of parameterized matching in a stream to that of
exact matching.  This could be achieved by converting both pattern and
stream into their corresponding predecessor strings and maintaining
fingerprints of a sliding window of the translated input. However,
consider the effect on the predecessor string, and hence its
fingerprint, of sliding a window in the stream along by one.  The
leftmost symbol , say, will move out of the window and so the
predecessor value of the new leftmost occurrence of  in the new
window will need to be set to \texttt{0} and the corresponding fingerprint
updated. We cannot afford to store the positions of all
characters in a  length window. 
We will show a matching algorithm that solves these problems and others we
encounter en route using minimal space and in near constant time per arriving symbol. A number of technical innovations are required, including new uses of fingerprinting, a new compressed encoding of the positions of potential matches, a separate deterministic algorithm designed  for prefixes of the pattern with small parameterized period as well as the deamortisation of the entire matching process. Section~\ref{sec:overview} gives a more detailed overview of these main hurdles.









\subsection{Our new results}\label{sec:newresults}

Our main result is a fast and space efficient algorithm to solve the streaming parameterized matching problem.
It applies to \emph{dense} alphabets where we assume that both the pattern and streaming text alphabets are . The following theorem is proved over the subsequent sections of this paper.

\begin{theorem}\label{thm:main}
    Suppose the pattern and text alphabets are both  and the pattern has length~. There is a randomised algorithm for streaming parameterized matching that takes  worst-case time per character and uses  words of space. The probability that the algorithm outputs correctly at all alignments of an ~length text is at least , where  is any constant.
\end{theorem}

To fully appreciate this theorem we also give a nearly matching space lower bound which shows that our solution is optimal within logarithmic factors. The proof is based on communication complexity arguments and is deferred to Appendix~\ref{appendix:space}.

\begin{theorem}
\label{thm:space}
    There is a randomised space lower bound of   bits for the streaming parameterized problem, where  is the pattern alphabet.
\end{theorem}

Parameterized matching is often specified under the assumption that only some symbols are variable (allowed to be relabelled). The mapping  we used in Section~\ref{sec:probdef} has to reflect this constraint. More precisely, let the pattern alphabet be partitioned into fixed symbols  and variable symbols . For , we require that . The result from Theorem~\ref{thm:main} can be extended to handle \emph{general} alphabets with arbitrary fixed symbols. The idea is to apply a suitable reduction that was given in~\cite{AFM:1994} (Lemma~2.2) together with the streaming exact matching algorithm of Breslauer and Galil~\cite{BG:2011}, as well as applying a ``filter'' on the text stream, using for instance the
the dictionary of Andersson and Thorup~\cite{AT:2000} based on exponential search trees. The dictionary is used to map text symbols to the variable pattern symbols in . The proof of the following theorem is given in Appendix~\ref{appendix:filter}.

\begin{theorem}
    \label{thm:filter}
    Suppose  is the set of pattern symbols that can be relabelled under parameterized matching. All other pattern symbols are fixed. Without any constraints on the text alphabet, there is a randomised algorithm for streaming parameterized matching that takes  worst-case time per character and uses  words of space, where  is the length of the pattern. The probability that the algorithm outputs correctly at all alignments of an  length text is at least , where  is any constant.
\end{theorem}

As part of the proof of Theorem~\ref{thm:main} we had to develop an algorithm that efficiently solves streaming parameterized matching for patterns with small
\emph{parameterized period}, defined as follows. The parameterized period (\emph{\pperiod}) of the pattern , denoted , is the smallest positive integer such that  \pmatches . That is,  is the shortest distance that  must be slid by to parameterized match itself.
Our algorithm is deterministic and is interesting in its own right (see Section~\ref{sec:smallrho} for details). We also provide a matching space lower bound which is detailed in Appendix~\ref{appendix:space}.

\begin{theorem}
    \label{thm:kmp-main}
    Suppose the pattern and text alphabets are both  and the pattern has \pperiod~. There is a deterministic algorithm for streaming parameterized matching that takes  worst-case time per character and uses  words of space. Further, there is a deterministic space lower bound of  bits.
\end{theorem}






\subsection{Fingerprints}
We will make extensive use Rabin-Karp style fingerprints of strings
which are defined as follows. Let  be a string over the alphabet . Let  be a prime and choose
 uniformly at random.  The
fingerprint  is given by . A critical  property of the fingerprint function  is that the probability of achieving a false positive, , is at most  (see~\cite{KR:1987, Porat:09} for proofs). Let  denote the total length of the stream. Our randomised algorithm will make  (in fact near linear) fingerprint comparisons in total. Therefore, by the applying the union bound, for any constant , we can choose  so that with probability at least  there will be no false positive matches.

As we assume the RAM model with word size , a fingerprint fits in a constant number of words. We assume that all fingerprint arithmetic is performed within .
In particular we will take advantage of two fingerprint operations.

\begin{itemize}
\item[\fsplit]   \emph{Splitting:} Given ,  (where ) and the value of , we can compute  in  time.
\item[\fzero] \emph{Zeroing:} Let  be two equal length strings such that  is identical to  except for in positions  at which . We write  to denote . Given  and  for all , computing  takes  time.


\end{itemize}


\section{Overview, key properties and notation}\label{sec:overview}

The overall idea of our algorithm in Theorem~\ref{thm:main} follows that of previous work on streaming exact matching in small space, however for parameterized matching the situation is much more complex and calls for not only more involved details and methods but also a deep fundamental understanding of the nature of parameterized matching. We will now describe the overall idea, introduce some important notation and at the end of this section we will highlight  key facts about parameterized matching that are crucial for our solution. 

The main algorithm will try to match the streaming text with various prefixes of the pattern~. Let  denote the pattern alphabet.
We define  and
let  denote the shortest prefix of  that has
\pperiod greater than  (recall the definition of \pperiod given above Theorem~\ref{thm:kmp-main}).
We define  prefixes  of increasing length so that
 for , where  is the largest value such that . The final prefix  has length .
For all , we define , hence . 


In order to determine if there is a \pmatch between the text and a pattern prefix, we will compare the fingerprints of their predecessor strings (recall that two strings \pmatch iff their predecessor strings are the same).
We will need two related (but typically distinct) fingerprint definitions to achieve this. Figure~\ref{fig:fp-output} will be helpful when reading the following definitions which are discussed in an example below.
For any index~ and~,


\insertfigure{phi-fingerprints}{\label{fig:fp-output} The key fingerprints used by the randomised algorithm.
Characters contribute differently to  and  are highlighted.}

For each  the main algorithm runs a process whose responsibility for finding \pmatches between the text and  ( is handled separately as will be discussed later). The process responsible for  will ask the process responsible for  if it has found any \pmatches, and if so it will try to extend the matches to . As an example, suppose that the process for  finds a match at position  of the text (refer to Figure~\ref{fig:fp-output}). The process will then store this match along with the fingerprint  which has been built up as new symbols arrive. The process for  will be handed this information when the symbol at position  arrives. The task is now to work out if  is also a matching position with . With the fingerprint  available (built up as new symbols arrive), the process for  can use fingerprint arithmetics to determine if  is a matching position. This is one instance where the situation becomes more tricky than one might first think.

As position  is a \pmatch with  it suffices to compare the second half of the predecessor string of  with the second half of the predecessor string of . Fingerprints are used for this comparison. It is crucial to understand that  cannot be used directly here;
some predecessor values of the text might point very far back, namely to some position \emph{before} index . In Figure~\ref{fig:fp-output} we have shaded the three symbols for which this is true and we have drawn arrows indicating their predecessors. Thus, in order to correctly do the fingerprint comparison we need to set those positions to zero (we want the fingerprint of the predecessor string of the text substring starting at position , not the beginning of ). The fingerprint we defined as  above is the fingerprint we want to compare to the fingerprint of the second half of the predecessor string of . Using fingerprint operations, we have from the definitions that , where  is the set of positions that have to be set to zero. For a substring of  of length  consider the subset of positions which occur in  for at least one value of . Any such position has a predecessor value greater than . Therefore, by summing over all distinct symbols we have that the size of this subset is crucially only . Thus, we can maintain in small space every position in a suitable length window that will \emph{ever} have to be set to zero.

Let us go back to the example where the process for  had found a \pmatch at position~. The process stores  along with the fingerprint . This information is not needed by the process for  until  text symbols later. During the arrival of these symbols, the process for  might detect more \pmatches, in fact many more matches. Their positions and corresponding fingerprints have to be stored until needed by the process for . We now have a space issue: how do we store this information in small space? To appreciate this question, first consider exact matching. Here matches are known to be either an exact period length apart or very far apart. The matching positions can therefore be represented by an arithmetic progression. Further, the fingerprints associated with the matches in an arithmetic progression can easily be stored succinctly as one can work out each one of the fingerprints from the first one.
For parameterized matching the situation is much more complex: matches can occur more chaotically and, as we have seen above, fingerprints must be updated dynamically to reflect that symbols could be mapped differently in two distinct alignments. Handling these difficulties in small space (and small time complexity) is a main hurdle and is one point at which our work differ significantly from all previous work on streaming matching in small space.
We cope with this space issue in the next section.

\subsection{The structure of parameterized matches}
First recall that an \emph{arithmetic progression} is a sequence of numbers such that the (common) difference between any two successive numbers is constant. We can specify an arithmetic progression by its start number, the common difference and the length of the sequence.
In the next lemma we will see that the positions at which a string  of length  parameterize matches a longer string of length  can be stored in small memory: either a matching position belongs to an arithmetic progression or it is one of relatively few positions that can be listed explicitly in  space.
The proof of the lemma (consult Figure~\ref{fig:typ-matches}) is deferred to Section~\ref{sec:arithmetic-proof}.

\insertfigure{typ-matches}{\label{fig:typ-matches} Partitioning of positions (\!{\Large}\!) at which  \pmatches in a  length substring of .}

\begin{lemma}
    \label{lem:arithmetic}
     Let  be the set of positions at which  \pmatches within an  length substring of . The set  can be partitioned into two sets  and  such that ,  and  is an arithmetic progression with common difference , where  is the \pperiod of .
\end{lemma}

The lemma is incredibly important for the algorithm as it allows us to store all partial matches (that need to be kept in memory before being discarded) in a total of  space across all processes. The question of how to store their associated fingerprints remains, but is nicely resolved with the corollary below that follows immediately from the proof of Lemma~\ref{lem:arithmetic}. We can afford to store fingerprints explicitly for the positions that are identified to belong to the set  from Lemma~\ref{lem:arithmetic}, and for the matching positions in the arithmetic progression  we can, as for exact matching, work out every fingerprint given the first one.

\begin{corollary}
    \label{cor:arithmetic}
     For pattern , text  and arithmetic progression  as specified in Lemma~\ref{lem:arithmetic},  is the same for all .

\end{corollary}






































\subsection{Deamortisation}\label{sec:deamortisation}

So far we have described the overall approach but it is of course a major concern how to carry out computations in constant time per arriving symbol. In order to \emph{deamortise} the algorithm, we run a separate process responsible for the pattern prefix  that uses the deterministic algorithm of Section~\ref{sec:smallrho} (i.e.~Theorem~\ref{thm:kmp-main}). As  has  greater than , the \pmatches it outputs are at least this far apart. This enables the other processes to operate with a small delay: process  expects process  to hand over matches and fingerprints with a small delay, and it will itself hand over matches and fingerprints to  with a small delay. One of the reasons for the delays is that processes operate in a round-robin scheme -- one process per arriving symbol. The process that is responsible for  (which has length ) returns matches with a delay of up to  arriving symbols. Hence there is a gap of length  in which we can work out if the whole of  matches. To do this we have another process that runs in parallel with all other processes and explicitly checks if any match with  can be extended with the remaining  symbols by directly comparing their predecessor values with the last  predecessor values of the pattern. This job is spread out over  arriving symbols, hence matches with  are outputted in constant time.



\section{The main algorithm}



We are now in a position to describe the full algorithm of Theorem~\ref{thm:main}. Recall that the algorithm will find \pmatches with
each of the pattern prefixes  defined in the previous section. If a shorter prefix fails to match at a given
position then there is no need to check matches for longer
prefixes.
Our algorithm runs three main processes concurrently which we label \procA, \procB and \procC.
The term process had a slightly different meaning in the previous section, but hopefully this will cause no confusion.
Each process takes  time per arriving symbol. Recall that both the pattern and text alphabets are . \textbf{Process~\procA} finds \pmatches with prefix  which are inserted as they occur into a \emph{match queue} . \textbf{Process \procB} finds \pmatches for prefixes  which are inserted into the match queues , respectively.
The \pmatches are inserted with a delay of up to 
symbol arrivals after they occur. \textbf{Process~\procC} finds \pmatches with the whole pattern  which are outputted in constant time as they occur as described in Section~\ref{sec:deamortisation}.


It is crucial for the space usage that the match queues  will be stored in a compressed fashion.  The delay in detecting \pmatches with  in Process \procB is a consequence of deamortising the work required to find a prefix match, which we spread out over  arriving symbols. We can afford to spread out the work in this way because the \pperiod of  is at least  so any \pmatches are at least this far apart.


Throughout this section we assume that  so that  for .
If , or the \pperiod of  is  or less, we use the deterministic algorithm presented in Section~\ref{sec:smallrho} to solve the problem within the required bounds.




\subsection{Process \procA (finding matches with )}

From the definition of  we have that if
we remove the final character (giving the string ) then its \pperiod is at most .
The \pperiod of  itself could be much larger.
As part of process \procA we run the deterministic pattern matching algorithm from Section~\ref{sec:smallrho} (see Theorem~\ref{thm:kmp-main}) on . It returns \pmatches in constant time and uses  space.

In order to establish matches with the whole of  we handle the final character separately.
If the deterministic subroutine reports a match that ends in , when  arrives we have a \pmatch with  if and only if  (or  if ). As the alphabet is of the form , we can compute the value of  in  time by maintaining an array  of length  such that for all ,  gives the index of the most recent occurrence of symbol .

Whenever Process \procA finds a match with  at position  of the text, the pair  is added to a (FIFO) queue , which is queried by Process~\procB when handling prefix .

\subsection{Process \procB (finding matches with )}







We split the discussion of the execution of Process~\procB into  \emph{levels}, .
For each level~ the fingerprint  is computed for each position  at which  \pmatches.
Then, as discussed in Section~\ref{sec:overview}, if , there is also a match with  at . The algorithm will in this case add the pair  to the queue  which is subject to queries by level~.
To this end we compute  and , where  contains all the positions which should be zeroed in order to obtain .
In the example of Figure~\ref{fig:fp-output},
 (the \texttt{d}, \texttt{e} and \texttt{f}, respectively).

In order for process \procB to spend only constant time per arriving symbol, all its work must be scheduled carefully. The preparation of the  values takes place as a subprocess we name \procBdelta. Computing  and establishing matches takes place in another subprocess named \procBphi. The two subprocesses are run in sequence for each arriving symbol.
We now give their details.


\paragraph{Subprocess \procBdelta~(prepare zeroing)}
We use a queue  associated with each level~ which contains the most recent  positions with predecessor the values greater than . We will see below that  is a subset of the positions in  (adjusted to the offset ).

Unfortunately, in the worst case, for an arriving symbol ,  could belong to all of the  queues. Since we can only afford constant time per arriving symbol, we cannot insert  into more than a constant number of queues. The solution is to buffer arriving symbols.
When some  arrives we first check whether . If so, the pair  is added to a buffer  to be dealt with later.
Together with the pair we also store the value  which will be needed to perform the required zeroing operations.

In addition to adding a new element to the buffer , the Subprocess~\procBdelta will also process elements from . If is is currently not in the state of processing an element, it will now start doing so by removing an element from  (unless  is empty). Call this element . Over the next  arriving symbols the Subprocess~\procBdelta will do the following. For each of the  levels , if , add  to the queue . If  contains more than  elements, discard the oldest.


\paragraph{Subprocess \procBphi~(establish matches)} This subprocess schedules the work across the levels in a round-robin fashion by only considering level  when the symbol  arrives.  Potential matches may not be reported by this subprocess until up to  arriving symbols after they occur. As  has \pperiod at least , the processing of potential matches does not overlap.

The Subprocess \procBphi for level  is always in one of two states: either it is \emph{checking} whether a matching position  for  is also a match with , or it is \emph{idle}. If idle, level~ looks into queue  which holds matches with . If  is
non-empty, level~ removes an element from , call this element , and enters the checking state. Whenever ,
level~ will start checking if  is also a matching position with . It does so by first computing the fingerprint ,
which by definition equals . We can ensure the fingerprint  is always available when needed by maintaining a circular buffer of the most recent  fingerprints of the text. Similarly we can obtain  in  time by keeping a buffer of the most recent  values of  along with  for all . \label{page:buff}

Over the next at most  arriving symbols for which Subprocess~\procBphi is considering level~ (i.e. those with ),
 will be computed from
  by stepping through the elements of the queue . For any element , we have that  if and only if .
Further, as Subprocess~\procBdelta stored  with the element in  and  is obtained through the circular buffer as above, we can perform the zeroing in  time.

 Having computed , we then compare it to . If they are equal, we have a \pmatch with  at position  of the text, and the pair  is added to the queue . This occurs before  arrives.



\subsection{Correctness, time and space analysis}

The time and space complexity almost follow immediately from the description of our algorithm, but a little more attention is required to verify that the algorithm actually works. In particular one has to show that buffers do not overflow, elements in queues are dealt with before being discarded and every possible match will be found (disregarding the probabilistic error in the fingerprint comparisons). The proof of the next lemma is given in Appendix~\ref{appendix:correctness}.

\begin{lemma}
    \label{lem:correctness}
    The algorithm described above proves Theorem~\ref{thm:main}.
\end{lemma}









\section{The deterministic matching algorithm}\label{sec:smallrho}

We now describe the deterministic algorithm that solves Theorem~\ref{thm:kmp-main}. Its running time is  time per character and it uses  words of space, where  is the parameterized period
of . We require that both the pattern and text alphabets are .


We first briefly summarise the overall approach of \cite{AFM:1994} which our algorithm follows. It resembles the classic KMP algorithm. When  arrives, the overall goal is to calculate the largest  such that  \pmatches .
A \pmatch occurs iff .
When a new text character  arrives the algorithm compares
 to  in  time to determine whether  \pmatches .
    More precisely, the algorithm checks whether either , or . The second case covers the possibility that the previous occurrence in the text was outside the window.
If there is a match, we set  and  and continue with the next text character. If not, we shift the pattern prefix  along by its \pperiod, denoted , so that it is aligned with . This is the next candidate for a \pmatch. In the original algorithm, the p-periods of all prefixes are stored in an array of length  called a prefix table.

 The main hurdle we must tackle is to store both a prefix table suitable for parameterized matching as well as an encoding of the pattern in only  space, while still allowing efficient access to both. It is well-known that any string  can be stored in space proportional to its exact period. In Lemma~\ref{lem:pred-const}, which follows from Lemma~\ref{lem:split}, we show an analogous result for .
 See Appendix~\ref{appendix:smallrho} for proofs.

\begin{lemma}\label{lem:split}
For any  there is a constant  such that  is 0 for , and  for , where  is a constant that depends on .
\end{lemma}


\begin{lemma}
    \label{lem:pred-const}
    The predecessor string  can be stored in  space, where  is the \pperiod of~. Further, for any  we can obtain  from this representation in  time.
\end{lemma}








We now explain how to store the parameterized prefix table in only  space, in contrast to  space which a standard prefix table would require. The \pperiod  of  is, as a function of , non-decreasing in~. This property enables us to run-length encode the prefix table and store it as a doubly linked list with at most  elements, hence using only  space. Each element corresponds to an interval of prefix lengths with the same \pperiod, and the elements are linked together in increasing order (of the common \pperiod).
This representation does not allow  time random access to the \pperiod of any prefix, however, for our purposes it will suffice to perform sequential access.
To accelerate computation we also store a second linked list of the indices of the first occurrences of each symbol in  in ascending order, i.e. every  such that . This uses  space.





There is a crucial second advantage to compressing the prefix table which is that it allows us to upper bound the number of prefixes of  we need to inspect when a mismatch occurs. When a mismatch occurs in our algorithm, we repeatedly shift the pattern until a \pmatch between a text suffix and pattern prefix occurs. Naively it seems that we might have to check many prefixes within the same run.
However, as a consequence of Lemma~\ref{lem:split}
we are assured that if some prefix does not \pmatch, every prefix in the same run  with  will also mismatch (except possibly the longest). Therefore we can skip inspecting these prefixes.
This can be seen by observing (using Lemma~\ref{lem:split}) that for  such that , we have .
By keeping pointers into both linked lists, it is straightforward to find the next prefix to check in  time.
Whenever we perform a pattern shift we move at least one of the pointers to the left. Therefore the total number of pattern shifts inspected while processing  is at most . As each pointer only moves to the right by at most one when each  arrives, an amortised time complexity of  per character follows. The space usage is  as claimed, dominated by the linked lists.




We now briefly discuss how to deamortise our solution by applying Galil's KMP deamortisation argument~\cite{Galil:1981}. The main idea is to restrict the algorithm to shift the pattern at most twice when each text character arrives, giving a constant time algorithm. If we have not finished processing  by this point we accept  but place it on the end of a buffer, output `no match' and continue processing . The key property is that the number of text arrivals until the next \pmatch occurs is at least the length of the buffer. As we shift the pattern up to twice during each arrival we always clear the buffer before (or as) the next \pmatch occurs. Further, the size of the buffer is always . This follows from the observation above that the number of pattern shifts required to process a single text character is .
This concludes the algorithm of Theorem~\ref{thm:kmp-main}. Combining this result with the lower bound result of Appendix~\ref{appendix:space} proves Theorem~\ref{thm:kmp-main}.





\section{The proof of Lemma~\ref{lem:arithmetic}}\label{sec:arithmetic-proof}

    In this section we prove the important Lemma~\ref{lem:arithmetic}. Let \ileft denote an arbitrary position in  where  \pmatches. Let  be the set of positions at which  \pmatches within . We now prove that there exist disjoint sets  and  with the properties set out in the statement of the lemma.

     Let  be the smallest integer such that all distinct symbols in  occur in the prefix . We begin by showing that , the  of  is at least . From the minimality of , we have that  is the leftmost occurrence of some symbol. By the definition of the , we have that  \pmatches . Under this shift,  (in ) is aligned with  (in ) . Assume that  is not a leftmost occurrence and let  be the position of the previous occurrence of . As a parameterized match occurs, we have that , contradiction. By repeating this argument we have found distinct symbols at positions  for all . This immediately implies that .

     We first deal with two simple cases:  or  (which implies that ). In these two cases the number of \pmatches is easily upper bounded by , so all positions can be stored in the set .

     We therefore continue under the assumption that  and . As , there are at most  positions from the range  at which  can parameterize match . We can store these positions in the set . Next we will show that the positions from the range  at which  parameterize matches  can be represented by the arithmetic progression~.

First we show that  is an \emph{exact period} (not \pperiod) of  (but not necessarily the shortest period). Consider arbitrary positions  and  where . By the definition of the , we have that  \pmatches  and hence that .
In particular, ,
where the second equality follows because we take the predecessor string of a prefix of .
Also observe that  either equals  or  by definition. Further,  as  and all leftmost occurrences are before .
This implies that , hence, as required, 

Recall that  \pmatches  so  and hence  is an exact period of . Let 
and observe that by definition, .
However,   because  and all leftmost occurrences are in . This implies that . As  was arbitrary, we have that  and hence  is an exact period of .


Let  be the rightmost position in  where  \pmatches. By the same argument as for , we have that  is an exact period of .

Thus, both  and  has an exact period of . As these two strings overlap by at least  characters, we have that  is also an exact period of .

Let  be arbitrary such that  \pmatches . We now prove that if  then  \pmatches . As \pmatches must be at least  characters apart this is sufficient to conclude that all remaining matches form an arithmetic progression with common difference .

As  is an exact period of , we have that . By definition, this implies that  and hence a \pmatch also occurs at . This concludes the proof of Lemma~\ref{lem:arithmetic}.


\section{Acknowledgements}

The work described in this paper was supported by EPSRC. The authors would like to thanks Rapha\"el Clifford for many helpful and encouraging discussions.
















































\printbibliography

\newpage






\appendix

\markboth{APPENDIX}{APPENDIX}





\section{Space lower bounds}\label{appendix:space}


To complete the picture we give nearly matching space lower bounds which show that our
solutions are optimal to within log factors.  The proof is by a
communication complexity argument. In
essence one can show that in the randomised case Alice is able to
transmit any string of length  bits to Bob
using a solution to the matching problem by selecting a suitable pattern and streaming text.  Similarly in the deterministic case (see below) one can show
that she can send  bits.


\begin{proof}[\prooftheorem{thm:space}]
Consider first a pattern where all symbols are distinct, e.g.\@ .  Now let us assume Alice would like to send a bit-string to Bob. She can encode the bit-string as an instance of the parameterized matching problem in the following way. As an example, assume the bit-string is \texttt{01011}. She first creates the first half of a text stream \texttt{aBcDE} where we choose capitals to correspond to \texttt{1} and lower case symbols to correspond to \texttt{0} from the original bit-string.  She starts the matching algorithm and runs it until the pattern and the first half of the text have been processed and then sends a snapshot of the memory to Bob.  Bob then continues with the second half of the text which is fixed to be the sorted lower case symbols, in this case \texttt{abcde}.  Where Bob finds a parameterized match he outputs a \texttt{1} and where he does not, he outputs a \texttt{0}.  Thus Alice's bit-string is reproduced by Bob. In general, if we restrict the alphabet size of the pattern to be  then Alice can similarly encode a bit-string of length , and successfully transmit it to Bob, giving us an  bit lower bound on the space requirements of any streaming algorithm.
\end{proof}

If randomisation is not allowed, the lower bound increases to  bits of space. Here  is the parameterized period of the pattern. This bound follows by a similar argument by devising a one-to-one encoding of bit-strings of length  into . The key difference is that with a deterministic algorithm, Bob can enumerate all possible -length texts to recover Alice's bit-string from~.




\section{Correctness proof of the main algorithm}\label{appendix:correctness}




\begin{proof}[\prooflemma{lem:correctness}]
    Coupled with the discussion in Section~\ref{sec:overview}, the time and space complexity almost follow immediately from the description. It only remains to show that, at any time, . First observe that any symbol  is only inserted into  when  which can only happen at most once in every  arriving symbols. Further we remove one element every  arrivals and in particular remove the  occurrence after at most  arrivals. As  is initially empty, by induction it follows that no symbol occurs more than once in .


    For correctness, it remains to show that we correctly obtain the positions of  from . It follows from the description that all positions of  correspond to elements inserted into  at some point. However we need to prove that these elements are present in  while  is calculated. Any element inserted into  during  has cleared the buffer by the end of interval B (which has length ) by the argument above. Therefore any relevant element has been inserted into  by the start of interval C, during which we calculate . Any element inserted into  is at least  characters from its predecessor. Therefore, summing over all symbols in the alphabet, there are at most  positions in  which are inserted into . As  is a FIFO queue of size , the relevant elements are still present after interval C.

    As commented earlier, potential matches in  are separated by more than  arrivals because  has  more than . They are processed within  arrivals so  does not overflow. This completes the correctness.
\end{proof}





\section{Proof of Theorem~\ref{thm:filter} (general alphabets)} \label{appendix:filter}



Let  denote the text alphabet.
In order to handle general alphabets we perform two reductions in sequence on each arriving text symbol (and on  during preprocessing). The first reduces  and  to each contain only symbols from  and one additional variable symbol (which is different for  and ). A suitable such reduction is given in~\cite{AFM:1994} (Lemma 2.2). The reduction is presented for the offline version but immediately generalises by using the constant time exact matching algorithm of Breslauer and Galil~\cite{BG:2011}.

We now define  to be the pattern alphabet after the first reduction (and  respectively). Note that  and all pattern symbols are variables. However we have no guarantee on the bit representations of the alphabet symbols. Let  and  denote the text and pattern after the first reduction. The second reduction now maps each  into the range  as it arrives. The equivalent reduction for the pattern is a simplification which can be performed in preprocessing.

Let the strings  and  denote the last  characters of the unfiltered (post first reduction) and filtered (post second reduction) stream, respectively. Let  denote the up to  last distinct symbols in , hence  is never more than . Let  be a dynamic dictionary on  such that a symbol in  can be looked up, deleted and added in  time~\cite{AT:2000}. Every symbol that arrives in the stream is associated with its ``arrival time'', which is an integer that increases by one for every new symbol arriving in the stream. Let  be an ordered list of the symbols in  (together with their most recent arrival time) such that  is ordered according to the most recent arrival time. For example,

means that the symbols \texttt{b}, \texttt{d}, \texttt{e} and \texttt{g} are the last four distinct symbols that appear in  (for this example, ), where the last \texttt{e} arrived at time~102, the last \texttt{g} arrived at time~58, and so on.

    By using appropriate pointers between elements of the hash table~ and elements of  (which could be implemented as a linked list), we can maintain  and  in  time per arriving symbol. To see this, take the example in Equation~(\ref{eq:list}) and consider the arrival of a new symbol  at time~103 (following the last symbol \texttt{e}). First we look up  in  and if  already exists in , move it to the right end of  by deleting and inserting where needed and update the element to . Also check that the leftmost element of  is not a symbol that has been pushed outside of  when  arrived. We use its arrival time to determine this and remove the last element accordingly. If the arriving symbol  does not already exist in , then we add  to the right end of . To ensure that  does not contain more than  elements, we remove the leftmost element of  if necessary. We also remove the leftmost symbol if it has been pushed outside of . The hash table  is of course updated accordingly as well.

    Let  denote the symbols outputted by the filter. We augment the elements of  to maintain a mapping  from the symbols in  to distinct symbols in  as follows. Whenever a new symbol is added to , map it to an unused symbol in . If no such symbol exists, then use the symbol that is associated with the symbol of  that is to be removed from  (note that ). The mapping  specifies the filtered stream: when a symbol  arrives, the filter outputs . Finding  and updating  is done in  time per arriving character, and both the tree  and the list  can be stored in  space.

    It remains to show that the filtered stream does not induce any false matches or miss a potential match. Suppose first that the number of distinct symbols in  is  or fewer. That is,  contains all distinct symbols in . Every symbol  in  has been replaced by a unique symbol in  and the construction of the filter ensures that the mapping is one-to-one. Thus, . Suppose second that the number of distinct symbols in  is  or more. That is,  and therefore  contains  distinct symbols. Thus,  cannot equal . The claimed result then follows from Theorem~\ref{thm:main}.






\section{Proofs omitted from Section~\ref{sec:smallrho}} \label{appendix:smallrho}


\begin{proof}[\prooflemma{lem:split}]
    Let  be the \pperiod of . We prove the lemma by contradiction. Suppose, for some  and , that  is a position such that  and . Consider Figure~\ref{fig:pattern-rho} for a concrete example, where , ,  and .
\begin{figure}[t]
        \centering
        \includegraphics[scale=0.90]{./diagrams/{pattern-rho}}
        \caption{\label{fig:pattern-rho} An example demonstrating the structure of  used in the proof of Lemma~\ref{lem:split}.}
    \end{figure}
    Since  is a \pperiod of , we have that

Consider the alignment of positions  and  (positions 17 and~12 in Figure~\ref{fig:pattern-rho}). We have that  is either  or~0. In either case, it is certainly not  which is . Thus,  cannot be a \pperiod of .
\end{proof}


\begin{proof}[\prooflemma{lem:pred-const}]
    By Lemma~\ref{lem:split} we can encode  by storing the two values  and  for each . This takes  space. The value  is 0 if , otherwise it is .

\end{proof}



















\end{document}
