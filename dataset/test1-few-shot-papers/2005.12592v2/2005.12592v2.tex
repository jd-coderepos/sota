


\documentclass[11pt,a4paper]{article}

\usepackage{hyperref}
\usepackage{breakurl} 
\usepackage{acl2020}



\usepackage{times}
\usepackage{appendix}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{bm}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\usepackage{placeins}

\renewcommand{\UrlFont}{\ttfamily\small}

\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}


\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{GECToR -- Grammatical Error Correction: Tag, Not Rewrite}

\author{Kostiantyn Omelianchuk \qquad Vitaliy Atrasevych\thanks{~Authors  contributed equally to this work, names are given in an alphabetical order.} \quad Artem Chernodub\footnotemark[1] \qquad Oleksandr Skurzhanskyi\footnotemark[1]\\
Grammarly \\
{\tt \{firstname.lastname\}@grammarly.com} \\}


\date{}

\begin{document}
\maketitle

\begin{abstract}
{In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an  of 65.3/66.5 on CoNLL-2014 (test) and  of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system. The code and trained models are publicly available\footnote{\url{https://github.com/grammarly/gector}}.}
\end{abstract}

\section{Introduction}

Neural Machine Translation (NMT)-based approaches \cite{sennrich2016edinburgh} have become the preferred method for the task of Grammatical Error Correction (GEC)\footnote{\url{http://nlpprogress.com/english/grammatical_error_correction.html} (Accessed 1 April 2020).}. In this formulation, errorful sentences correspond to the source language, and error-free sentences correspond to the target language. Recently, Transformer-based \cite{vaswani2017attention} sequence-to-sequence (seq2seq) models have achieved state-of-the-art performance on standard GEC benchmarks \cite{bryant2019bea}. Now the focus of research has shifted more towards generating synthetic data for pretraining the Transformer-NMT-based GEC systems \cite{grundkiewicz2019neural, kiyono2019empirical}.
NMT-based GEC systems suffer from several issues which make them inconvenient for real world deployment: (i) slow inference speed, (ii) demand for large amounts of training data and  (iii) interpretability and explainability; they require additional functionality to explain corrections, e.g., grammatical error type classification \cite{bryant2017automatic}. 

In this paper, we deal with the aforementioned issues by simplifying the task from sequence generation to sequence tagging. Our GEC sequence tagging system consists of three training stages: pretraining on synthetic data, fine-tuning on an errorful parallel corpus, and finally, fine-tuning on a combination of errorful and error-free parallel corpora. 

\textbf{Related work.} LaserTagger \cite{malmi-etal-2019-encode} combines a BERT encoder with an autoregressive Transformer decoder to predict three main edit operations: keeping a token, deleting a token, and adding a phrase before a token. In contrast, in our system, the decoder is a softmax layer. PIE \cite{awasthi-etal-2019-parallel} is an iterative sequence tagging GEC system that predicts token-level edit operations. While their approach is the most similar to ours, our work differs from theirs as described in our contributions below:

\indent 1. We develop custom g-transformations: token-level edits to perform (g)rammatical error corrections. Predicting g-transformations instead of regular tokens improves the generalization of our GEC sequence tagging system. \\
\indent 2. We decompose the fine-tuning stage into two stages: fine-tuning on errorful-only sentences and further fine-tuning on a small, high-quality  dataset containing both errorful and error-free sentences. \\
\indent 3. We achieve superior performance by incorporating a pre-trained Transformer encoder in our GEC sequence tagging system. In our experiments, encoders from XLNet and RoBERTa outperform three other cutting-edge Transformer encoders (ALBERT, BERT, and GPT-2). \\

\section{Datasets}

\begin{table}
\footnotesize
\centering
\begin{tabular}{cccc}
\hline 
\textbf{Dataset}  & \textbf{\# sentences}  &\textbf{\% errorful} & \textbf{Training} \\
                 &                         &\textbf{sentences}   & \textbf{stage} \\
\hline 
PIE-synthetic & 9,000,000 & 100.0\% & I \\
Lang-8 & 947,344 & 52.5\% & II \\
NUCLE & 56,958 & 38.0\% & II \\
FCE & 34,490 & 62.4\% & II \\
W\&I+LOCNESS & 34,304 & 67.3\% & II, III  \\
\hline
\end{tabular}
\caption{\label{training-data-table} Training datasets. Training stage I is pretraining on synthetic data. Training stages II and III are for fine-tuning.}
\end{table}


Table \ref{training-data-table} describes the finer details of datasets used for different training stages.

\textbf{Synthetic data.} For pretraining stage I, we use 9M parallel sentences with synthetically generated grammatical errors \cite{awasthi-etal-2019-parallel}\footnote{\url{https://github.com/awasthiabhijeet/PIE/tree/master/errorify}}.

\textbf{Training data.} We use the following datasets for fine-tuning stages II and III: National University of Singapore Corpus of Learner English (NUCLE)\footnote{\url{https://www.comp.nus.edu.sg/~nlp/corpora.html}} \cite{dahlmeier2013building}, Lang-8 Corpus of Learner English (Lang-8)\footnote{\url{https://sites.google.com/site/naistlang8corpora}} \cite{tajiri2012tense}, FCE dataset\footnote{\url{https://ilexir.co.uk/datasets/index.html}} \cite{yannakoudakis2011new}, the publicly available part of the Cambridge Learner Corpus \cite{nicholls2003cambridge} and Write \& Improve + LOCNESS Corpus \cite{bryant2019bea}\footnote{\url{https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz}}.

\textbf{Evaluation data}. We report results on CoNLL-2014 test set \cite{ng2014CoNLL} evaluated by official  scorer \cite{dahlmeier2012better}, and on BEA-2019 dev and test sets evaluated by ERRANT \cite{bryant2017automatic}. 

\section{Token-level transformations}
\label{section:transformations}

We developed custom token-level transformations  to recover the target text by applying them to the source tokens . Transformations increase the coverage of grammatical error corrections for limited output vocabulary size for the most common grammatical errors, such as \textit{Spelling}, \textit{Noun Number}, \textit{Subject-Verb Agreement} and \textit{Verb Form} \cite[p. 28]{yuan2017grammatical}. 

The edit space which corresponds to our default tag vocabulary size = 5000 consists of 4971 \textit{basic transformations} (token-independent KEEP, DELETE and 1167 token-dependent APPEND, 3802 REPLACE) and 29 token-independent \textit{g-transformations}.

\textbf{Basic transformations} perform the most common token-level edit operations, such as: keep the current token unchanged (tag \textit{\DELETE}), append new token  next to the current token  (tag \textit{\t_{1}x_it_{2}REPLACE\_}).

\textbf{g-transformations} perform task-specific operations such as: change the case of the current token (\textit{CASE} tags), merge the current token and the next token into a single one (\textit{MERGE} tags) and split the current token into two new tokens (\textit{SPLIT} tags). Moreover, tags from \textit{NOUN NUMBER} and \textit{VERB FORM} transformations encode grammatical properties for tokens. For instance, these transformations include conversion of singular nouns to plurals and vice versa or even change the form of regular/irregular verbs to express a different number or tense.

To obtain the transformation suffix for the \textit{VERB\_FORM} tag, we use the verb conjugation dictionary\footnote{\url{https://github.com/gutfeeling/word_forms/blob/master/word_forms/en-verbs.txt}}. For convenience, it was converted into the following format:  (e.g., ). This means that there is a transition from  and  to the respective tags. The transition is unidirectional, so if there exists a reverse transition, it is presented separately.

The experimental comparison of covering capabilities for our token-level transformations is in Table \ref{transforms-cover-table}. All transformation types with examples are listed in Appendix, Table \ref{transforms-examples-table}.

 \textbf{Preprocessing.} To approach the task as a sequence tagging problem we need to convert each target sentence from training/evaluation sets into a sequence of tags where each tag is mapped to a single source token. Below is a brief description of our 3-step preprocessing algorithm for color-coded sentence pair from Table  \ref{iterations-examples-table}: \\


\indent Step 1). Map each token from source sentence to subsequence of tokens from target sentence.
[\textcolor{red}{A}  \textcolor{blue}{A}], [\textcolor{red}{ten}  \textcolor{blue}{ten}, \textcolor{blue}{-}], [\textcolor{red}{years}  \textcolor{blue}{year}, \textcolor{blue}{-}], 
[\textcolor{red}{old}  \textcolor{blue}{old}], [\textcolor{red}{go}  \textcolor{blue}{goes}, \textcolor{blue}{to}], [\textcolor{red}{school}  \textcolor{blue}{school}, \textcolor{blue}{.}].

For this purpose, we first detect the minimal spans of tokens which define differences between source tokens  and target tokens . Thus, such a span is a pair of selected source tokens and corresponding target tokens. We can't use these span-based alignments, because we need to get tags on the token level. So then, for each source token ,  we search for best-fitting subsequence ,  of target tokens by minimizing the modified Levenshtein distance (which takes into account that successful g-transformation is equal to zero distance).


Step 2). For each mapping in the list, find token-level transformations which convert source token to the target subsequence: [\textcolor{red}{A}  \textcolor{blue}{A}]: \\mapstoKEEP, \\mapstoNOUN\_NUMBER\_SINGULAR, \\mapstoKEEP, [\textcolor{red}{go}  \textcolor{blue}{goes}, \textcolor{blue}{to}]: \APPEND\_\textcolor{blue}{to}, [\textcolor{red}{school}  \textcolor{blue}{school}, \textcolor{blue}{.}]: \APPEND\_\{\textcolor{blue}{.}\}].

Step 3). Leave only one transformation for each source token:
\textcolor{red}{A}  \\LeftrightarrowMERGE\_HYPHEN,
\textcolor{red}{years}  \\LeftrightarrowKEEP,
\textcolor{red}{go}  \\LeftrightarrowAPPEND\_\{\textcolor{blue}{.}\}. 

The iterative sequence tagging approach adds a constraint because we can use only a single tag for each token. In case of multiple transformations we take the first transformation that is not a \x_i1 \leq i \leq N(x_1 \ldots x_N)T(x_i)\mathbf{F_{0.5}}\mathbf{F_{0.5}}\mathbf{F_{0.5}}dim = 300\mathbf{F_{0.5}}\mathbf{F_{0.5}}\mathbf{F_{0.5}}\mathbf{F_{0.5}}M^2KEEP tag which is responsible for not changing the source token. 
Second, we added a sentence-level \textit{minimum error probability} threshold for the output of the error detection layer. This increased precision by trading off recall and achieved better  scores (Table \ref{train-stages-xlnet-table}). 

Finally, our best single-model, GECToR (XLNet) achieves  = 65.3 on CoNLL-2014 (test) and  = 72.4 on BEA-2019 (test). Best ensemble model, GECToR (BERT + RoBERTa + XLNet) where we simply average output probabilities from 3 single models achieves  = 66.5 on CoNLL-2014 (test) and  = 73.6 on BEA-2019 (test), correspondingly (Table \ref{results-table}).


\textbf{Speed comparison}. We measured the model’s average inference time on NVIDIA Tesla V100 on batch size 128. For sequence tagging we don't need to predict corrections one-by-one as in autoregressive transformer decoders, so inference is naturally parallelizable and therefore runs many times faster. Our sequence tagger's inference speed is up to 10 times as fast as the state-of-the-art Transformer from \citet{zhao2019improving}, beam size=12 (Table \ref{speed-table}).


\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{l|c}
\hline
\multicolumn{1}{c|}{\textbf{GEC system}}     &  \textbf{Time (sec)} \\ \hline
Transformer-NMT, beam size = 12 &  4.35   \\
Transformer-NMT, beam size = 4   &  1.25   \\
Transformer-NMT, beam size = 1   &   0.71  \\ \hline
GECToR (XLNet), 5 iterations &  0.40           \\
GECToR (XLNet), 1 iteration &  0.20           \\ \hline
\end{tabular}
\caption{\label{speed-table} Inference time for NVIDIA Tesla V100 on CoNLL-2014 (test), single model, batch size=128.}
\end{table}

\section{Conclusions}
We show that a faster, simpler, and more efficient GEC system can be developed using a sequence tagging approach, an encoder from a pretrained Transformer, custom transformations and 3-stage training. 

Our best single-model/ensemble GEC tagger achieves an  of 65.3/66.5 on CoNLL-2014 (test) and  of 72.4/73.6 on BEA-2019 (test). We achieve state-of-the-art results for the GEC task with an inference speed up to 10 times as fast as Transformer-based seq2seq systems.

\section{Acknowledgements}
This research was supported by Grammarly. We thank our colleagues Vipul Raheja, Oleksiy Syvokon, Andrey Gryshchuk and our ex-colleague Maria Nadejde who provided insight and expertise that greatly helped to make this paper better. We would also like to show our gratitude to Abhijeet Awasthi and Roman Grundkiewicz for their support in providing data and answering related questions. We also thank 3 anonymous reviewers for their contribution. 


\bibliography{gector}
\bibliographystyle{acl_natbib}

\newpage
\onecolumn
\appendix
\section{Appendix}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2 \end{tabular}}

\begin{table}[ht]
\scriptsize
\centering
\begin{tabular}{ccccc}
\hline 
\\
\textbf{id} & \textbf{\specialcell{Core\\ transformation}} & \textbf{\specialcell{Transformation\\suffix}}  & \textbf{Tag} & \textbf{Example} \\
\\
basic-1 & KEEP &  & \\mathbf{\varnothing}DELETE & \ldots not sure if you are \{\textbf{you}  \} gifting \ldots \\
basic-3 & REPLACE & a & \\RightarrowREPLACE\_cause & \ldots hope it does not  \{\textbf{make}  \textbf{cause}\} any trouble \ldots \\
basic-3805 & APPEND & for & \\RightarrowAPPEND\_know & \ldots I \{\textbf{don't}  \textbf{don't know}\} which to choose\ldots
\\
\hline
g-1 & CASE & CAPITAL & \\RightarrowCASE\_CAPITAL\_1 & \ldots I want to buy an \{\textbf{iphone}  \textbf{iPhone}\} \ldots \\
g-3 & CASE & LOWER & \\RightarrowCASE\_UPPER & \ldots the \{\textbf{it}  \textbf{IT}\} department is concerned that\ldots  \\
g-5 & MERGE & SPACE & \\RightarrowMERGE\_HYPHEN & \ldots and needs \{\textbf{in depth}  \textbf{in-depth}\}  search \ldots \\
g-7 & SPLIT & HYPHEN & \\RightarrowNOUN\_NUMBER\_SINGULAR & \ldots a place to live for their \{\textbf{citizen}  \textbf{citizens}\} \\
g-9 & NOUN\_NUMBER & PLURAL & \\RightarrowVERB\_FORM\_VB\_VBZ & \ldots going through this \{\textbf{make}  \textbf{makes}\} me feel \ldots \\
g-11 & VERB FORM & VB\_VBN  &  \\RightarrowVERB\_FORM\_VB\_VBD & \ldots she sighed and \{\textbf{draw}  \textbf{drew}\} her \ldots \\
g-13 & VERB FORM & VB\_VBG & \\RightarrowVERB\_FORM\_VB\_VBZ & \ldots a small percentage of people \{\textbf{goes}  \textbf{go}\} by bike \ldots \\
g-15 & VERB FORM & VBZ\_VBN & \\RightarrowVERB\_FORM\_VBZ\_VBD & \ldots he \{\textbf{drinks}  \textbf{drank}\} a lot of beer last night \ldots \\
g-17 & VERB FORM & VBZ\_VBG & \\RightarrowVERB\_FORM\_VBN\_VB & \ldots going to \{\textbf{depended}  \textbf{depend}\} on who is hiring \ldots \\
g-19 & VERB FORM & VBN\_VBZ & \\RightarrowVERB\_FORM\_VBN\_VBD & \ldots he \{\textbf{driven}  \textbf{drove}\} to the bus stop and \ldots \\
g-21 & VERB FORM & VBN\_VBG & \\RightarrowVERB\_FORM\_VBD\_VB & \ldots each of these items will
 \{\textbf{fell}  \textbf{fall}\} in price \ldots \\
g-23 & VERB FORM & VBD\_VBZ & \\RightarrowVERB\_FORM\_VBD\_VBN & \ldots he has been went
 \{\textbf{went}  \textbf{gone}\} since last week \ldots \\
g-25 & VERB FORM & VBD\_VBG & \\RightarrowVERB\_FORM\_VBG\_VB & \ldots free time, I just \{\textbf{enjoying}  \textbf{enjoy}\} being outdoors \ldots \\
g-27 & VERB FORM & VBG\_VBZ & \\RightarrowVERB\_FORM\_VBG\_VBN  & \ldots people are afraid of being \{\textbf{tracking}  \textbf{tracked}\} \ldots \\
g-29 & VERB FORM & VBG\_VBD & \\Rightarrow\mathbf{F_{0.5}}\mathbf{F_{0.5}}$          
\\ \hline
Stage I.  &   57.8   &  33.0   &   50.2   &   40.8   &  22.1   &   34.9   \\ 
Stage II.   &   68.1   &  42.6    &   60.8   &   51.6   &  33.8   &   46.7   \\ 
Stage III. & 68.8&  \textbf{47.1} & 63.0 & 54.2 &  \textbf{41.0}   & 50.9   \\
Inf. tweaks &\textbf{73.9}&  41.5   &\textbf{64.0}&\textbf{62.3}&  35.1   &\textbf{54.0} \\
\hline
\end{tabular}

\caption{\label{train-stages-bert-table} Performance of GECToR (RoBERTa) after each training stage and inference tweaks. Results are given in addition to results for our best single model, GECToR (XLNet) which are given in Table \ref{train-stages-xlnet-table}.}

\end{table}

\begin{table}[!hb]
\footnotesize
\centering
\begin{tabular}{ccc}
\hline
\textbf{System name} & \textbf{Confidence bias} & \textbf{Minimum error probability} \\ 
GECToR (BERT) & 0.10 & 0.41  \\ 
GECToR (RoBERTa) & 0.20 & 0.50  \\ 
GECToR (XLNet) & 0.35 & 0.66 \\ 
GECToR (RoBERTa + XLNet) & 0.24 & 0.45  \\ 
GECToR (BERT + RoBERTa + XLNet) & 0.16 & 0.40  \\ 
\hline
\end{tabular}
\caption{\label{hyperparameters-table} Inference tweaking values which were found by random search on BEA-dev.} 
\end{table}


\end{document}
