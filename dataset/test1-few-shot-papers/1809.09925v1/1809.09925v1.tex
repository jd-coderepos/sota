\def\year{2019}\relax
\documentclass[letterpaper]{article} \usepackage{aaai19}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{graphicx}  \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{caption,subcaption}
\newcommand{\D}{\mathcal{D}}
\newcommand{\N}{\mathcal{N}}
\frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  

\pdfinfo{
/Title (2019 Formatting Instructions for Authors Using LaTeX)
/Author (AAAI Press Staff)}
\setcounter{secnumdepth}{2}  
 \begin{document}
\title{Every Node Counts: Self-Ensembling Graph Convolutional Networks \\ for Semi-Supervised Learning}

\author{Yawei Luo,\hspace{2mm} Tao Guan,\hspace{2mm} Junqing Yu,\hspace{2mm} Ping Liu,\hspace{2mm} Yi Yang \vspace{0.5cm} \\
School of Computer Science \& Technology, Huazhong University of Science \& Technology\\
CAI, University of Technology Sydney\\
JD.COM Silicon Valley Research Center, Big Data Group
}

\maketitle
\begin{abstract}
Graph convolutional network (GCN) provides a powerful means for graph-based semi-supervised tasks. However, as a localized first-order approximation of spectral graph convolution, the classic GCN can not take full advantage of unlabeled data, especially when the unlabeled node is far from labeled ones. To capitalize on the information from unlabeled nodes to boost the training for GCN, we propose a novel framework named Self-Ensembling GCN (SEGCN), which marries GCN with Mean Teacher -- another powerful model in semi-supervised learning. SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly classify the labeled nodes, but also tries to be consistent with the teacher on unlabeled nodes in more challenging situations, such as a high dropout rate and graph collapse. As a teacher, it averages the student model weights and generates more accurate predictions to lead the student. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for backpropagating effective gradients to train GCN. In three article classification tasks, \emph{i.e.} Citeseer, Cora and Pubmed, we validate that the proposed method matches the state of the arts in the classification accuracy.
\end{abstract}

\section{Introduction} \label{sec:introduction}
\noindent Semi-supervised learning (SSL) aims to build a better classifier, by utilizing huge amounts of unlabeled data which is readily accessible, together with a limited number of labeled data. Such line of work is of great significance because it achieves a high accuracy while requiring less human effort for data annotation. Recently, SSL has gained considerable attention when applied to deep learning-based methods, which are well known for their high demand on sizable and reliable labeled samples. Through distilling knowledge from unlabeled data, SSL boosts the deep learning-based methods to a new level in many tasks, \emph{e.g.,} speech recognition~\cite{dai2015semi}, image segmentation~\cite{papandreou2015weakly} and video understanding~\cite{caelles2017one}.

Inspired by the great success of SSL on regular Euclidean-based data such as speech, images, or video, a surge of recent approaches seek to apply SSL to data in a more general form -- graph. The motivation is natural: in many real problems, the data samples are on irregular grid or more generally in non-Euclidean domains, e.g. point cloud~\cite{wang2018dynamic}, chemical molecules~\cite{li2018adaptive}  and social networks~\cite{rahimi2018semi}. Instead of regularly shaped tensors, those data are better to be structured as graph, which is capable of handling varying neighborhood vertex connectivity. Similar to the original goal on regular data, SSL on graph-structured data aims to classify the all the nodes in a graph using a small subset of labeled nodes and large amounts of unlabeled nodes. The recently developed graph convolutional neural network 
\begin{figure}
\centering
\includegraphics[width=0.90\linewidth]{idea_full}
\caption{(Best viewed in color.) \textbf{(a)} Vanilla GCN, where the each node gathers features from its neighbors in limited scope and only labeled nodes are supervised under the classification loss. \textbf{(b)} Mean Teacher model, where student model operates under a perturbed setting and tries to be consistent with the predictions of teacher model.\\
Our method marries \textbf{(a)} and \textbf{(b)}, which utilizes both supervised classification loss and unsupervised consistency loss to train GCN. Such framework enables us to explore more unlabeled knowledge to boost the classification accuracy under the semi-supervised setting.}
\end{figure}
(GCNN)~\cite{defferrard2016GCNN} and the following graph convolutional network (GCN)~\cite{kipf2016semi} are successful attempts along this line, which generalize the powerful convolutional neural network (CNN) in Euclidean data to modeling graph-structured data. This line of work capitalizes on the relation between nodes and enables the features to propagate between neighboring vertices. During the training, the supervised loss acts upon the confluent features of labeled node and then distribute the gradient information across other unlabeled adjacent nodes. Such mechanism makes the features of both labeled and unlabeled vertices in the same cluster similar, thus largely easing the classification task.  

Although it has made great progresses, GCN fails to take full advantage of unlabeled data, especially when the unlabeled node is far away from labeled ones. This is due to a -layer GCN only captures node information up to -hop neighborhood, which cannot effectively propagate the information to the entire graph. Taking the two-layers GCN as an example. In such network, a vertex  would \emph{directly} aggregate and filter features in its -hop neighborhood . Considering that an adjacent vertex  also aggregates features from its own -hop neighborhood , the  can \emph{indirectly} discover farther vertices beyond . However, such indirect link is \emph{de facto} negligible which is represented with a tiny kernel weight. Consequently, any unlabeled vertex with shortest path distance  from labeled ones in graph would gain very limited attention and are prone to be underutilized during the training. A deeper network with more graph convolutional layers may help to discover information in such remote nodes. However, as mentioned in~\cite{li2018deeper}, GCN is essentially a special form of Laplacian smoothing. Therefore, a deeper GCN may bring potential concerns of over-smoothing, \emph{i.e.} the output features may be over-smoothed and the vertices from different clusters may become indistinguishable. In summary, the utilization of unlabeled information beyond the -hop neighborhood remains an open problem. 

In this paper, we propose a new architecture that can discover much more information within unlabeled vertices and learn from the global graph topology. A key innovation is to marry Mean Teacher framework~\cite{tarvainen2017mean} into the classic GCN. Instead of merely supervising the propagated features in labeled nodes, we directly give chances to unlabeled nodes to ``speak up for themselves''. Specifically, SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly predict the labeled nodes, but also tries to be consistent with teacher output on unlabeled nodes in more challenging settings, such as high dropout rates and graph collapse. As a teacher, it updates itself by averaging the student model weights. Since the teacher model operates under better settings such as low dropout rates and lossless graph, it generates more accurate predictions on both labeled and unlabeled nodes, thus being able to lead the student to learn in the next epoch. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for back-propagating effective gradient to train GCN.

To the best of our knowledge, this is the first time to introduce Mean Teacher strategy in the GCN design. Precisely, the main contributions of this work are summarized below. 
\begin{itemize}
\item By proposing to combine Mean Teacher with classic GCN, we emphasize the importance of exploitation of unlabeled nodes in graph-structured data classification.
\item Analogy to the noise added to student model in regular data, we successfully adapt Mean Teacher to graph-based data by designing new perturbation strategies for student model.
\item Our results are on par with the state-of-the-art methods on three node classification benchmarks in terms of accuracy, \emph{i.e.} Citeseer (), Core () and Pubmed ().
\end{itemize}

The rest of this paper is organized as follows. Section 2 discusses related work for GCN and Mean Teacher that provide the foundation for this paper. Then we propose the SEGCN model in Section 3. Section 4 presents an experimental study in which we compare our method with baseline and state-of-the-art results of benchmark datasets. Finally, we conclude with our contributions in Section 5.

\section{Preliminaries and Related Work}
We first provide a brief introduction to the required background. Then we review SSL with GCN and Mean Teacher, which provide fundamental theories for this paper.
\subsection{Spectral Graph Convolution}
There are two means to define convolution on graph, either from a spatial approach or from a spectral approach. This paper focuses on the latter. Based on the theory of Chung \emph{et al.}~\cite{chung1997spectral}, spectral GCNNs construct the convolution kernel on spectrum domain. They represent both the filter and the signal with the Fourier basis and multiply them, then transforms the result back into the discrete domain. However this model requires explicitly computing the eigenvectors of Laplacian matrix , which is impractical for real large graphs. To circumvent this problem, it was suggested in~\cite{hammond2009wavelets} that approximate the spectral filter  with Chebyshev polynomials up to  order:

where  is a vector of Chebyshev coefficients and  denotes the  item of Chebyshev polynomials. , where  is a  order diagonal matrix and  denotes the largest eigenvalue of .

Recent proposed GCN~\cite{kipf2016semi} further simplifies this model by limiting K = 1 and approximating . Given the adjacent matrix  and the input feature , the output of a single convolutional layer  can be represented as

where ,  and  denotes the trainable model parameters. Specifically, a two-layer GCN model can be defined as

where . This two-layer model forms the backbone of our SEGCN.

\subsection{Semi-supervised Learning with GCNs}
The above GCN model in Eq.~\ref{eq:two-layer-gcn} can be expediently used for SSL. However, as the analysis in Sec.~\ref{sec:introduction}, this method is limited to its small receptive field within few-hops neighborhood. Several recent methods are proposed to overcome such limitation~\cite{weston2012deep,abu2018N-GCN,verma2018feastnet,monti2017Monet}. 
Among these attempts, Random Walk is proven to be very effective to discover more remote cues~\cite{grover2016node2vec,perozzi2014deepwalk}. Moreover, attention mechanisms are introduced to emphasize the useful unlabeled information~\cite{Kiran2018attention,veličković2018GAT,shang2018edge}. To enable SSL on an extreme large graph, Liao \emph{et al.}~\cite{liao2018GPNN} extendse GCN by graph partitions.

\subsection{Semi-supervised Learning with Mean Teacher}
Mean Teacher~\cite{tarvainen2017mean} is one of the self-ensembling methods. The idea of a teacher model training a student is related to model compression~\cite{buciluǎ2006model} and distillation~\cite{hinton2015distilling}. Apart from other variants~\cite{bachman2014learning,laine2016temporal}, Mean Teacher averages model weights instead
of predictions and achieves excellent results in SSL. Other lines of work in self-ensembling focus on designing effective perturbation, including~\cite{gastaldi2017shake,huang2016stochasticdepth,wan2013dropconnect}.


\section{Proposed Method}


\subsection{Combining GCN with Mean Teacher}
Graph Convolutional Network and Mean Teacher model are individually powerful. However, as we present in early sections, the former explores the unlabeled information by halves while the latter has only shown its ability on Euclidean data. In this section, we propose a new framework called SEGCN that combines the merits of both models while overcomes their limitations. Particularly, SEGCN contains a student model  and a teacher model , where  and  are the weights of the respective models. Given the labeled data  and unlabeled data , we first construct a normalized adjacent matrix  upon these data according to their pairwise relations. Specific to our article classification task, the pairwise relations consist in a citation from one article to another. 

We formulate the overall loss of SEGCN from two aspects. On the one hand, the student should learn to minimize the cross-entropy loss under the supervision of labeled data in a noise-free environment. 
\newline

\newline
where  denotes a labeled sample. The  denotes the predicted probability from the student classifier on the class . The  denotes the ground truth probability of the class .

On the other hand, the student classifier should be consistent with teacher's predictions when operates under small perturbations. In classic Mean Teacher on Euclidean-based data, the perturbation is usually added to original inputs  to construct student inputs. Differently, the input fed in SEGCN are the \emph{Bag-of-Words} (\emph{BoW}) features distilled from articles. As a result, the traditional data augmentation strategies such as scaling, inversion or distortion are not available in our task. Instead, we innovatively construct such perturbation on both the graph structure and the model of student network. For the perturbation on graph structure , we generate a ``collapsed graph'' by  where  is our perturbation function on the normalized adjacent matrix . Similarly, we add noise to the original student model  by  where  is the perturbation function on model. These two perturbation functions will be detailed discussed in Sec.~\ref{sec:perturbation}. Given the \emph{disturbed} setting  for student and the \emph{original} setting  for teacher, the unsupervised consistency loss would penalizes the difference between the student's predicted probabilities  and the teacher's . In our paper, we formulate this loss as KL divergence.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{graph_collapse}
\caption{Perturbation on graph structure. The arrows indicate the feature aggregation directions. A deeper blue arrow represents a larger kernel weight. \textbf{(a)} In original complete graph, the propagation paths between vertices are abundant, \emph{e.g.,} the vertex  can propagate its feature to the labeled vertex  via four different routes. \textbf{(b)} On the premise of no isolated node appears,  randomly cuts off the redundant edges in order to partially block the information transmission between vertices. Our perturbation strategy enables the student to generate worse predictions while keeping the intrinsic \emph{BoW} features unchanged.}
\label{fig:perturbation}
\end{figure}



\newline
With the above loss terms in Eq.~\ref{eq:ce_loss} and Eq.~\ref{eq:cons_loss}, the overall loss function of our approach can be written as
\newline

\newline
where the parameter  controls the relative importance of the consistency term in the overall loss.

SEGCN is trained end-to-end to minimize the overall loss. In such scheme, the student can not only learn to distill knowledge from the labeled data under the supervised loss, but also pay much attention to the unlabeled vertices in order to come after the teacher. On the other side, through averaging the latest parameters in the student, the teacher is able to evolve itself since the gradient descent direction leading by consistency loss also applies to the update for the teacher model. As shown in Fig.~\ref{fig:phenomenon}, such a joint evolution between the dual GCNs paves the way for more thoroughly exploration on unlabeled information, which is otherwise not possible if solved alone in their traditional frameworks. 

\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\linewidth]{illustration}
\caption{(Best viewed in color.) \textbf{(a)} Vanilla GCN pushes decision boundaries away from the labeled sample. Since the labeled feature is mainly aggregated from adjacent vertices, the near samples can be well classified. However, a remote unlabeled node (red circle) is prone to be ignored by assigning a very small GCN kernel weight. Consequently, the vanilla model may be under-constrained and cannot be well adapted to these distant samples. \textbf{(b)} SEGCN pays more attentions on the unlabeled nodes than vanilla GCN. Via a mutual-promoting process leading by the consistency losses (\emph{e.g.,} the inconsistency between the solid/dotted red circles), SEGCN gives the chances to the unlabeled nodes as well to backpropagate effective gradient to train the model. Once the mutual-promoting process converges, even a remote unlabeled node can be well classified.}
\label{fig:phenomenon}
\end{figure*}

\subsection{Perturbation in Student Model} \label{sec:perturbation}
This subsection aims to design effective perturbations on graph-based data. We do not follow the traditional ways of adding noise to raw inputs~\cite{french2017self}, which is popularly used on images and videos. The reason is two-fold. Firstly, the traditional data augmentation strategies on Euclidean data such as scaling, inversion or distortion are not suitable in article classification task. Secondly, some keywords are closely related to the article category. If we directly modify the \emph{BoW} feature vectors, the intrinsic cues for classifying an article may be lost. As a result, we instead propose two perturbation strategies on the graph structure (represented by adjacent matrix ) and GCN model respectively. 

The perturbation operation on graph structure, which is denoted as , is shown in Fig.\ref{fig:perturbation}. On the premise of no isolated node appears,  randomly cuts off the redundant edges in order to partially block the information transmission between vertices. The decreased number of connection makes the feature aggregation from neighbor vertices becoming even harder, thus enabling the student to generate inconsistent predictions with the teacher.  

Another function  denotes the perturbation on model itself. We implement  by appending a dropout layer~\cite{srivastava2014dropout} to each model. The dropout layer can drop different nodes in each time and obtaining two different output vectors from student and teacher. To force the student to operate in a harder environment, we give a higher dropout rate to the student than the teacher.  
In a nutshell, the two specifically designed perturbations enable the student to generate inconsistent predictions while keeping the intrinsic \emph{BoW} features unchanged. 

\subsection{Self-training in Mean Teacher}
Self-training is an effective scheme where the model is bootstrapped with additional labeled data obtained from its own highly confident predictions. This process is repeated until some termination conditions are reached. Although these methods are heuristic and have achieved much progress in semi-supervised learning, seeking for an appropriate threshold to select those high confident predictions is by no means easy. A strict threshold may reject most of the right predictions, thus leading to a degenerated self-training scheme. On the contrary, a loose threshold may bring about a poorer classifier since the wrongly imported pseudo labels can reinforce poor predictions. Traditionally, this threshold is set empirically and any prediction with a softmax score larger than the threshold will be selected as an extended labeled sample.

Different from the traditional methods which generate predictions merely from a single classifier, SEGCN is born with the dual classifiers and able to make predictions from two views, \emph{i.e.} the student view and the teacher view. This property motivates us to combine the different views aiming to select more robust pseudo labels. Particularly, we regard a prediction as a high confident result only if \emph{both} student and teacher give larger softmax scores than the threshold  on the same class. Otherwise, if the two classifiers give inconsistent predictions, it indicates a probably incorrect prediction which will be excluded from the additional labeled data in this epoch. In our experiment, we verify that combining teacher and student can achieve a better self-training performance compared with the single model-based variant.

\begin{algorithm}[t]
    \caption{GCN in Mean Teacher framework}\label{alg:Mean Teacher}
    \label{alg:train SEGCN}
    \begin{algorithmic}[1]
    \STATE \textbf{Input :} \\
    \quad Two-layer GCN model  \\  
    \quad Normalized adjacent matrix  \\  		
    \quad \emph{BoW} features and labels  \& 
    \STATE \textbf{Initialization :} \\
    \quad High-noise model  \\
    \quad Collapsed graph  \\
    \quad Model weights  from scratch \\
    \STATE \textbf{Repeat :}
    \STATE \quad   \\
    \STATE \quad   \\
    \STATE \quad   \\
    \STATE \quad   \\
    \STATE \quad   \\
    \STATE \quad Add high confident pseudo labels to \\
    \STATE \textbf{Until}  converges.
    \end{algorithmic}
\end{algorithm}

\subsection{Training SEGCN} \label{sec:training}
The training of SEGCN framework is essentially a mutual-promoting process. We detail the training step in Algorithm~\ref{alg:train SEGCN}. The student weight  is initialized from scratch while the teacher weight  is initialized by copying from . Given the labeled \emph{BoW} features  and the unlabeled ones , the student is firstly trained to minimize the supervised cross-entropy loss on  using a \emph{complete graph}  and a \emph{low dropout model} . Then the student is forced to be consistent with the teacher predictions on all vertices  using a \emph{collapsed graph}  and a \emph{high dropout model} . Finally, the teacher updates its own model weights from the latest student model. In each iteration, the student network is optimized using ADAM~\cite{kingma2014adam}, while the weights of the teacher network are updated with an exponential moving average of those of the latest student, which is formulated as Eq.~\ref{eq:moving_average}.
\newline

\newline
where  is a smoothing coefficient hyperparameter and  denotes the current epoch.

Since the student and the teacher are both inaccurate in early epochs, the mutual learning between each other could lead to unstable predictions, which may deviate from our original intention. Therefore, traditional solution utilizes a two-stage training scheme. Namely, in first stage the student is merely trained on labeled data until it converges. Then the mutual learning is started up in the second stage. Differently, to enable an end-to-end scheme, we approximate the classic two-stage training process using a hyperparameter trick. We initialize small  and  in early epochs and progressively increase them during the training. In such one-stage scheme we can avoid the unwanted knowledge transmission between the immature partners in early epochs.

\section{Experiment} \label{sec:experiment}
\subsection{Datasets}
We experiment on three public available citation graph datasets: \textbf{Citeseer}, \textbf{Cora} and \textbf{Pubmed}. Table~\ref{tab:dataset} summarizes dataset statistics. A citation graph dataset consists of documents as nodes and citation links as directed edges. Each node has a human annotated topic from a finite set of classes and a feature vector. For Citeseer and Cora, the feature vector has binary entries indicating the presence/absence of the corresponding word from a dictionary. For the Pubmed dataset, the feature vector has real-values entries indicating Term Frequency-Inverse Document Frequency \emph{(TF-IDF)} of the corresponding word from a dictionary. Although the networks are directed, we use undirected versions of the graphs for all experiments, which is common in all baseline approaches.

Besides the three benchmarks above, we construct a new toy dataset called \textbf{Node5} in order to clearly showcase the effect of SEGCN on far-away nodes. It derives from a subset of Cora. For each class in Cora, we select 5 (1 labeled and 4 unlabeled) samples and link them as a complete graph structure presented in Fig.~\ref{fig:perturbation}. The experiment on this toy dataset aims to verify the phenomenon we present in Fig.~\ref{fig:phenomenon}.
\begin{table}[t]
\caption{Dataset statistics}
\label{tab:dataset}
\vspace{0.1cm}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\hline
\textbf{Dataset}    &    \textbf{\# Nodes}    &    \textbf{\# Edges}    &    \textbf{\# Classes}    &    \textbf{\# Feature Dim.} \\
\hline
Node5		&	35		&	42		&	7	&	1433	\\
Citeseer	&	3327	&	4732	&	6	&	3703    \\
Cora		&	2708	&	5429	&	7	&	1433    \\
Pubmed		&	19717	&	44338	&	3	&	500    \\
\hline
	
\end{tabular}
}
\end{table} 
\subsection{Experimental Setup}
We use PyTorch for implementation. we train both baseline and SEGCN as two-layer networks described in GCN~\cite{kipf2016semi}, where the first layer outputs 16 dimensions per node and the second layer outputs the number of classes. For the student model, we use ADAM optimizer~\cite{kingma2014adam} with , ,  and . While for the teacher, no dropout is applied. We fix the learning rate to 0.01 and train SEGCN for 1,000 epochs. As mentioned in Sec.~\ref{sec:training}, hyperparameter  in Eq.~\ref{eq:overall_loss} gradually increases from  to  while  in Eq.~\ref{eq:moving_average} increases from  to  in our best model. To import more pseudo label candidates when our model is stable, we gradually decrease the self-training threshold  from  to . In all experiments, we utilize 500 samples as a validation set and evaluate prediction accuracy on a test set of 1,000 examples. These validation samples are used for capturing the model parameters at peak validation accuracy to avoid overfitting.

\subsection{Comparative Study}
\textbf{Fixed splits.} In the first experiment, we use the fixed data splits from the work of Yang \emph{et al.}~\cite{yang2016revisiting} as it is the standard benchmark data splits in literatures. Specifically, these experiments are run on the same fixed split of 20 labeled nodes for each class.  We present the classification accuracy on the mentioned three benchmarks in Table~\ref{tab:main} with comparisons to our baseline as well as the state-of-the-art semi-supervised classification methods. Except our own implemented baseline GCN model, the accuracy of the other comparative methods are all taken from existing literature.

\begin{table*}[ht]
\caption{Accuracy comparison with the state-of-art-methods under the setting of \textbf{Fixed / Random} data splits.  indicates our own implemented baseline.}
\label{tab:main}
\vspace{0.1cm}
\centering
\begin{tabular}{l|ccc|ccc}
\hline
& \multicolumn{3}{c}{\textbf{Fixed splits}} & \multicolumn{3}{|c}{\textbf{Random splits}}\\
\textbf{Method}  & \textbf{Citeseer} & \textbf{Cora} & \textbf{Pubmed} & \textbf{Citeseer} & \textbf{Cora} & \textbf{Pubmed}\\ 
\hline






DeepWalk \small{(Perozzi et al. 2014)} &  &  &  &  &  & \\

node2vec \small{(Grover et al. 2016)} &  &  &  &  &  & \\



DCNN \small{(Atwood et al. 2016)} & - &  &   & - & - & - \\

Planetoid \small{(Yang et al. 2016)} &  &  &  & - & - & - \\

GCN \small{(Kipf et al. 2016)} &  &  &  &  &  & \\

Graph-CNN \small{(Such et al. 2017)} & - &  & -  & - & - & - \\

MoNet \small{(Monti et al. 2017)}  & - &  &   & - & - & - \\

Bootstrap \small{(Buchnik et al. 2017)} &  &  &  &  &  & \\


FeaStNet \small{(Verma et al. 2018)} & - &  &  & - & - & - \\

GPNN \small{(Liao et al. 2018)} &  &  &   &  &  &  \\

N-GCN \small{(Abu-El-Haija et al. 2018)} &  &  &  & - & - & - \\

F-GCN \small{(Vijayan et al. 2018)} & 72.3 & 79.0 & - & - & - & - \\

GAT \small{(Velikovi et al. 2018)} &   &   &   & - & - & - \\

\hline

GCN &  &  &  &  &  &  \\

SEGCN &  &  &  &  &  & \\

\hline
\end{tabular}
\end{table*}
 
On the one hand, we observe that SEGCN can significantly outperforms baseline GCN method, which brings ,  and  improvement on Citeseer, Cora and Pubmed respectively. It implies that Mean Teacher can actually boost the classifier training. On the other hand, we compare SEGCN with the state-of-the-art methods including Deepwalk~\cite{perozzi2014deepwalk}, node2vec~\cite{grover2016node2vec}, DCNN~\cite{atwood2016DCNN}, Planetoid~\cite{yang2016revisiting}, Monet~\cite{monti2017Monet}, Bootstrap~\cite{buchnik2017bootstrapped}, Graph-CNN~\cite{such2017GraphCNN}, FeaStNet~\cite{verma2018feastnet}, GPNN~\cite{liao2018GPNN}, N-GCN~\cite{abu2018N-GCN},  F-GCN~\cite{vijayan2018F-GCN} and GAT~\cite{veličković2018GAT}. As it can be seen SEGCN performs best on Citeseer and Cora which yields new state-of-the-art accuracies, while falling short by only 0.5\% from the best on the Pubmed dataset. The great performance not only relies on the high-performance baseline GCN method, but also due to the proposed self-ensembling strategy applied in the training scheme.

\textbf{Random splits.} Next, following the setting of GCN~\cite{kipf2016semi} , we run experiments keeping the same size in labeled, validation, and test sets as in fixed splits, but now selecting those nodes uniformly at random. This, along with the fact that different topics have different numbers of nodes in it, means that the labels might not be spread evenly across the topics. For 20 such randomly drawn dataset splits, the average accuracy is shown in Table~\ref{tab:main} with the standard error. As we do not force an equal number of labeled data for each class, we observe that the performance degrades for all methods compared to fixed splits except Deepwalk~\cite{perozzi2014deepwalk}. Besides, SEGCN achieves best results among the state-of-the-art methods on Citeseer and Cora, yielding  and  respectively in accuracy. We also note that the variances of the accuracies become larger and the performance falls short on Pubmed than baseline. We will discuss this observation in next subsection.


\begin{figure*} 

\begin{minipage}[b]{.24\textwidth}
\includegraphics[height=3.2cm]{Node5_Baseline_tSNE}
\subcaption{}
\end{minipage}
\begin{minipage}[b]{.24\linewidth}
\includegraphics[height=3.2cm]{Node5_SEGCN_tSNE}
\subcaption{}
\end{minipage}
\begin{minipage}[b]{.24\linewidth}
\includegraphics[height=3.2cm]{Cora_Baseline_tSNE}
\subcaption{}
\end{minipage}
\begin{minipage}[b]{.24\linewidth}
\includegraphics[height=3.2cm]{Cora_SEGCN_tSNE}
\subcaption{}
\end{minipage}
\caption{Feature distribution analysis on Node5 ((a), (b)) and Cora ((c), (d)). We map the high-dimensional features outputted from the second layer to a 2-D space with t-SNE. (a)\&(c) are the result of vanilla GCN while (b)\&(d) are ours. Different color indicates different class. The triangles in (a)\&(b) denote the remote nodes (same as the red circles in Fig.~\ref{fig:phenomenon}).}
\label{fig:tSNE}
\end{figure*} 
\textbf{More labeled samples on Pubmed.} We note that the improvement is relatively lower on Pubmed than that on Citeseer and Core. We suspect that the reason is due to the different label rates in the three benchmarks: the Pubmed dataset is relative large and the labeled rate is very low when we only select 20 training samples from each class. Consequently, the labeled nodes can only reach extremely limited scope in graph and can not give stable initial gradient directions to unlabeled vertices for the latter mutual-promoting process. Particularly, we observe that a few very low values appears under the random splits settings. These failure cases significantly decrease the average value and increase the variance. To clarify our hypothesis, we compare SEGCN with the state-of-the-art methods on Pubmed with more labeled samples over range \{50, 100, 200\}. As it can be seen SEGCN outperforms other methods in all cases with higher accuracies and lower variances. These results validate our hypothesis since a relative large labeled set can create stable initializations for mutual learning and self-training, which unlock the potential of SEGCN.

\subsection{Ablation Study}
To assess the importance of various aspects of the model, we run experiments on Cora under the setting of fixed splits, deactivating one or a few modules at a time while keeping the others activated. Table~\ref{tab:ablation} reports the classification accuracies under different ablations. To begin with, we test the two proposed perturbations  and  respectively, comparing with randomly erasing (\textbf{RE})~\cite{zhong2017random} the raw input features . We observe that the direct perturbation on  would hurt the final accuracy, dropping  from the baseline and bringing about larger variance. While our proposed perturbations are helpful in improving the accuracy and  is more effective than . When combining the  and , SEGCN yields  accuracy, which is higher than any single perturbation. It implies that  and  are complimentary and can lead the student to learn information from teacher. Then we test the self-training strategy with  and  fixed. We observe a significant improvement when combining student and teacher over a single model scheme, which implies that by using Mean Teacher we can produce more correct labeled sample candidates in self-training. Finally, SEGCN can yield new state-of-the-art accuracies when employing all the proposed modules. 

\begin{table}[t]
\caption{Accuracy comparison with the state-of-art-methods on Pubmed with varying \textbf{\# labeled sample} over range \{50, 100, 200\}.  indicates our own implemented baseline.}
\label{tab:pubmed}
\vspace{0.1cm}
\centering
\begin{tabular}{l|ccc}
\hline
& \multicolumn{3}{c}{\textbf{\# Labeled samples per class}}\\
\textbf{Method}  & \textbf{50} & \textbf{100} & \textbf{200}\\ \hline
DCNN	& -							&  			& - \\
N-GCN	& -							&  			& - \\
GCN	& 			&  			&  \\
SEGCN	&  	&  	&  \\
\hline
\end{tabular}
\end{table}
 
\subsection{Feature Distribution Analysis}
In this section, We aim to further prove the effectiveness of SEGCN via feature distribution analysis. To this end, we map the high-dimensional features distilled from the second layer into a 2-D space with t-SNE. Fig.~\ref{fig:tSNE} shows the t-SNE visualization, in which (a) and (b) represent the results of vanilla GCN and SEGCN respectively on Node5.  As depicted in (a) and (b), GCN maps the feature of the remote vertex far from the cluster center while SEGCN can  map it near. As a result, SEGCN significantly eases the classification task. The distribution distinction can be also observed on Cora shown in (c) and (d). These visualization results further validate the effectiveness of the mutual learning we described in Fig.~\ref{fig:phenomenon}. To sum up, SEGCN can capitalize on the every nodes' information to train a better classifier. It is effective for those remote unlabeled vertices, which are hard to be classified with vanilla GCN.

\section{Conclusion}
In this paper, we propose a self-ensembling framework called SEGCN for semi-supervised learning on graph-based data. Apart from the vanilla GCN, SEGCN can directly explore unlabeled information via the mutual learning between student and teacher, thus enabling every labeled and unlabeled nodes to backpropagate effective gradient to train the model. To the best of our knowledge, this is the first work that integrates the Mean Teacher model into GCN to boost the semi-supervised node classification. The extensive experiments on toy and public datasets show that SEGCN precedes the baseline model significantly and is on par with the state-of-the-art methods in tasks of article classification. 

\begin{table}[t]
\caption{Ablation study on Cora.}
\label{tab:ablation}
\vspace{0.095cm}
\centering
\begin{tabular}{ccc|cc|c}
\hline
\multicolumn{3}{c|}{\textbf{Perturbation}} & \multicolumn{2}{c|}{\textbf{Self-training}} & \\
 &   &  & \textbf{T} & \textbf{S\&T} & \textbf{Accuracy}\\ 
\hline
	&			&			&			&			&  \\
		& 	&			&			&			&  \\
		&			& 	&			&			&  \\
		&	& 	&			&			&  \\
		&	& 	& 	&			&  \\
		&	& 	&			& 	&  \\
\hline
\end{tabular}
\end{table}
 
\bibliographystyle{aaai}
\bibliography{ref}

\end{document}
