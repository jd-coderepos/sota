\def\year{2021}\relax
\documentclass[letterpaper]{article}
\usepackage{aaai21} \usepackage{times} \usepackage{helvet} \usepackage{courier} \usepackage[hyphens]{url} \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm} \usepackage{graphicx}  \usepackage{natbib}
\usepackage{caption}
\frenchspacing \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors
Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider,
Sunil Issar, J. Scott Penberthy, George Ferguson,
Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2021.1)
}


\usepackage[paper,cmrgreekup]{pdef}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}       \usepackage{caption}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{bm}
\usepackage{tikz}
\usepackage{xparse}


\setcounter{secnumdepth}{0} 







\title{Feature Space Singularity for Out-of-Distribution Detection}
\author{Haiwen Huang\textsuperscript{\rm 1}, Zhihan Li\textsuperscript{\rm 2}, Lulu Wang\textsuperscript{\rm 3}, Sishuo Chen\textsuperscript{\rm 2}\\
Bin Dong\textsuperscript{\rm 2, 4, 5}, Xinyu Zhou\textsuperscript{\rm 3}\\
}
\affiliations{
\textsuperscript{\rm 1} Department of Computer Science, University of Oxford\\
    \textsuperscript{\rm 2} Peking University \hspace{1cm}\textsuperscript{\rm 3} MEGVII Technology\\
     \textsuperscript{\rm 4}Beijing International Center for Mathematical Research\\
     \textsuperscript{\rm 5}Institute for Artificial Intelligence and Center for Data Science\\




haiwen.huang2@cs.ox.ac.uk,  zxy@megvii.com, dongbin@math.pku.edu.cn

}
\iffalse
\title{My Publication Title --- Single Author}
\author {
Author Name \\
}

\affiliations{
    Affiliation \\
    Affiliation Line 2 \\
    name@example.com
}
\fi

\iffalse
\title{My Publication Title --- Multiple Authors}
\author {
First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1} \\
}
\affiliations {
\textsuperscript{\rm 1} Affiliation 1 \\
    \textsuperscript{\rm 2} Affiliation 2 \\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi

\makeatletter
\def\sim@y@offset{.025}
\def\sim@x@scale{.35}
\def\sim@y@scale{.125}
\def\sim@y@thick{.008}

\newsavebox\sim@upper
\newsavebox\sim@lower

\NewDocumentCommand{\xSim}{ O{} m }{\TextOrMath{\PackageError{TEST}{`\string\xSim` is valid in math mode only.}{}}{
\sbox\sim@upper{\scalebox{0.7}{\hbox{}}}
    \sbox\sim@lower{\scalebox{0.7}{\hbox{}}}
    \pgfmathparse{min(max(\wd\sim@upper/1em, \wd\sim@lower/1em, 1.0), 1.5)}
    \edef\sim@ratio{\pgfmathresult}
    \def\sim@x {\sim@x@scale * \sim@ratio}
    \def\sim@@@y {\sim@y@offset * \sim@ratio}
    \def\sim@@@@y {-\sim@y@offset * \sim@ratio}
    \def\sim@y {\sim@y@scale * \sim@ratio}
    \def\sim@@y{\sim@y@thick * \sim@ratio}
    \pgfmathparse{floor(max(\wd\sim@upper/1em, \wd\sim@lower/1em)) - 0.5}
    \edef\sim@wd{\pgfmathresult em}
    \mathrel{
      \begin{tikzpicture}[baseline=-.7ex]
        \filldraw[line width=.2pt] 
          (0, \sim@@@y)
            .. controls +(\sim@x, \sim@y+\sim@@y) and +(-\sim@x, -\sim@y) .. 
          +(\sim@wd, 0) 
            node[midway, above] {\usebox\sim@upper}
            .. controls +(-\sim@x, -\sim@y-\sim@@y) and +(\sim@x, \sim@y) .. 
          (0, \sim@@@y);
          \filldraw[line width=.2pt] 
            (0, \sim@@@@y)
              .. controls +(\sim@x, \sim@y+\sim@@y) and +(-\sim@x, -\sim@y) .. 
            +(\sim@wd, 0)  
              node[midway, below] {\usebox\sim@lower}
              .. controls +(-\sim@x, -\sim@y-\sim@@y) and +(\sim@x, \sim@y) .. 
            (0, \sim@@@@y);
      \end{tikzpicture}
    }
  }}
\makeatother


\begin{document}

\maketitle

\begin{abstract}
Out-of-Distribution (OoD) detection is important for building safe artificial intelligence systems.
However, current OoD detection methods still cannot meet the performance requirements for practical deployment.
In this paper, we propose a simple yet effective algorithm based on a novel observation: in a trained neural network, OoD samples with bounded norms well concentrate in the feature space.
We call the center of OoD features the \emph{Feature Space Singularity (FSS)}, and denote the distance of a sample feature to FSS as \emph{FSSD}.
Then, OoD samples can be identified by taking a threshold on the FSSD.
Our analysis of the phenomenon  reveals why our algorithm works.
We demonstrate that our algorithm achieves state-of-the-art performance on various OoD detection benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test data and can be further enhanced by ensembling. These make FSSD a promising algorithm to be employed in real world. We release our code at \url{https://github.com/megvii-research/FSSD_OoD_Detection}.

\end{abstract}

\section{Introduction}\label{introduction}

Empirical risk minimization fits a statistical model on a training set which is independently sampled from the  data distribution.
As a result, the yielded model is expected to generalize to in-distribution data drawn from the same distribution.
However, in real applications, it is inevitable for a model to make predictions on \emph{Out-of-Distribution (OoD)} data instead of in-distribution data on which the model is trained.
This can lead to fatal errors such as over-confident or ridiculous
predictions ~\cite{whyRelu, FailingLoudly}.
Therefore, it is crucial to understand the uncertainty of models and automatically detect OoD data.
In applications like autonomous driving and medical services, if the model knows what it does not know, human intervention can be sought and security can be significantly improved.

Consider one particular example of OoD detection: some high-quality human face images are given as in-distribution data (\emph{training set} for OoD detector), and we are interested in filtering out non-faces and low quality faces from a large pool of data in the wild (\emph{test set}) in order to ensure reliable prediction.
One natural solution is to remove test samples far from the training data in some designated distances~\cite{mahalanobis, duq}.
However, calculating the distance to the whole training set needs a formidable amount of computation without some special design in feature and architecture, e.g., training a RBF network~\cite{duq}.
\begin{figure*}[t]
\begin{subfigure}{0.65\textwidth}
  \centering
\includegraphics[width=1\linewidth]{1a.pdf}  
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
\includegraphics[width=1\linewidth]{1b.pdf}

\end{subfigure}
\caption{Left: Histogram of FSS Distance (FSSD) of MS1M (in-distribution) and ImageNet (OoD). Exemplar images are shown at different FSSDs. We can see that FSSD reflects the OoD degree: as the FSSD increases, images change from non-faces and  pseudo-faces, to low-quality faces and high-quality faces. Right: Principle components of features from the penultimate layer. The spatial relationship among FSS, OoD data, and in-distribution data is shown.}
\label{fig:phenomena}
\end{figure*}
In this paper, we present a simple yet effective distance-based solution, which neither computes the distance to the training data nor performs extra model training than a standard classifier.

Our approach is based on a novel observation about OoD samples:
\begin{quote}
\emph{In a trained neural network, OoD samples with bounded norms well concentrate in the feature space of the neural network.}
\end{quote}
In Figure~\ref{fig:phenomena}, we show an example where OoD features from ImageNet~\cite{imagenet} concentrate in a neural network trained on the facial dataset MS-1M~\cite{ms1m}. Figure 2 and 3 provide more examples of this phenomenon. In fact, we find this phenomenon to be universal in most training configurations for most datasets.

To be more precise, for a given  feature extractor  trained on in-distribution data, the observation states that there exists a point  in the output space
of  such that  is small for , where  is the set of OoD samples.
We call  the \emph{Feature Space Singularity (FSS)}.
Moreover, we discover  the \emph{FSS Distance (FSSD)} 

can reflect the degree of OoD, and thus can be readily used as a metric for OoD detection.

Our analysis demonstrates that this phenomenon can be explained by the training dynamics. The key observation is that FSSD can be seen as an approximate movement of  during training, where  is the initial concentration point of the features. The difference in the moving speed  stems from the different similarity to the training data measured by the inner product of the gradients. Moreover, this similarity measure varies according to the architecture of the feature extractor. 


We demonstrate the effectiveness of our proposed method with multiple neural networks (LeNet~\cite{mnist}, ResNet~\cite{resnet}, ResNeXt~\cite{resnext}, LSTM~\cite{lstm}) trained on various datasets for classification (FashionMNIST~\cite{fmnist}, CIFAR10~\cite{cifar10}, ImageNet~\cite{imagenet}, CelebA~\cite{celeba}, MS-1M~\cite{ms1m}, bacteria genome dataset~\cite{ren2019likelihood}) with varying training set sizes.
We show that FSSD achieves state-of-the-art performance on almost all the considered benchmarks.
Moreover, the performance margin between FSSD and other methods increases as the size of the training set increases.
In particular, on large-scale benchmarks (CelebA and MS-1M), FSSD advances the AUROC by about 5\%.
We also evaluate the robustness of our algorithm when test images are corrupted.
We find that our algorithm can still reliably detect OoD samples under this circumstance.
Finally, we investigate the effects of ensembling FSSDs from different layers of a single neural network and multiple trained netowrks.


We summarize our contributions as follows.
\begin{itemize}
\item We observe that in feature spaces of trained networks OoD samples concentrate near a point (FSS), and the distance from a sample feature to FSS (FSSD) measures the degree of OoD (Section 1).
\item We analyze the concentration phenomenon by analyzing the dynamics of in-distribution and OoD features during training (Section 2).
\item We introduce the FSSD algorithm (Section 3) which achieves state-of-the-art performance on various OoD detection benchmarks with considerable robustness (Section 4).
\end{itemize}





\begin{figure*}[t]
    \centering
    \begin{subfigure}{\textwidth}
    	\centering
         \includegraphics[width=1\linewidth]{2a.pdf}
          \caption{The dynamics of features .}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{2b.pdf}
    \caption{The norm of the derivative, i.e., "moving speed", of last-layer feature vector  at different time steps. }
\end{subfigure}
     \caption{
     We show first two principle components of the feature vector and the L2 norm of the derivatives (Equation~\eqref{eq:DynFeat}). 
     Features and derivatives are calculated  from the last fully-connected layer of a LeNet trained on MNIST (in-distribution).
We feed in FashionMNIST data as OoD samples.  
    At initialization, features of both in-distribution and OoD samples concentrate near FSS . After training, features of in-distribution samples are pulled away from FSS , while features of OoD samples remain close to FSS . Similar dynamics of the softmax layer on in-distribution data was observed by \cite{grandtour}.
}
\end{figure*}


\section{Analyzing and Understanding the Concentration Phenomenon}

In this section, we analyze the concentration phenomenon. The key observation is that during training, the features of the training data are supervised to move away from the initial point, and \textit{the moving speeds of features of other data depend on their similarity to the training data}. Specifically, this similarity is measured by the inner product of the gradients. Therefore, data that are dissimilar to the training data will move little and concentrate in the feature space. This is how FSSD identifies OoD data.

To see this,  we derive the training dynamics of the feature vectors.
We denote  as the feature extractor which maps inputs to features  and  to be the map from features to outputs. 
The two mappings are parameterized by  and  respectively. 
The corresponding loss function can be denoted as . A popular choice is , where  is the cross entropy loss or the mean squared error. 
Then, 
the \emph{gradient descent} dynamics of  is 

where  is the backward propagation gradient and subscript  is the training time.
The dynamics of the feature extractor  as a function is therefore




From Equation~\eqref{eq:DynFeat}, we can see that the relative moving speed of the feature  depends on the inner product of the \emph{gradient on parameters} between  and the training data . Note here  is the same for all . Since FSSD defined in Equation~\ref{fssd} can be seen as the integration of  when the initial value  is  for all x, FSSD(x) will also be small when the derivative, i.e., the moving speed, is small. 



In Figure 2, we show both the  features and their moving speeds of in-distribution and OoD data at different steps during training. We can see that although in-distribution and OoD data are indistinguishable at step 0, they are quickly separated since the moving speeds of in-distribution data are larger than those of OoD data (Figure 2(b)) and thus the accumulated movements of in-distribution data are also larger than those of OoD data (Figure 2(a)).
In Figure~\ref{concentration}, we show examples of the initial concentration of features in LeNet and ResNet-34 for MNIST vs. FashionMNIST and CIFAR10 vs. SVHN dataset pairs respectively. Empirically, we find the concentration of both in-distribution and OoD features at the initial stage to be the common case for most popular architectures using random initialization. We show more examples on our Github page.

As we've mentioned, Equation~\eqref{eq:DynFeat} demonstrates that the difference in the moving speed of   stems from difference in  . We want to further point out that  is effectively acting as a kernel that measures the similarity between  and . In fact, when the network width is infinite,  will converge to a  time-independent term , which is called neural tangent kernel (NTK)~\cite{NTK, ntk1, ntk2}. 
In this way, FSSD can be seen as a kernel regression result: 



 
where . 

This indicates that the similarity described by the inner product  might enjoy similar properties to commonly used kernels such as RBF kernel, which diminishes as the distance between  and  increases. Moreover, since the neural tangent kernel depends on the neural architecture, this kernel interpretation also suggests that feature extractors of different architectures, including different layers, can have different properties and measure different aspects of the similarity between  and . We can see this more clearly later in the investigation of FSSD in different layers.








\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
         \centering
         \includegraphics[width=\linewidth]{3a.pdf}
          \caption{MNIST vs.\ FMNIST}
         \label{concentrate1}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
         \centering
         \includegraphics[width=\linewidth]{3b.pdf}
          \caption{CIFAR10 vs.\ SVHN}
         \label{concentrate3}
    \end{subfigure}
    \caption{Both in-distribution and OoD samples are clustered in the feature space of  at initialization. Moreover,  for .
}
    \label{concentration}
\end{figure}


\section{Our Algorithm}

Based on this phenomenon, we can now construct our OoD detection algorithm. 
Since the uniform noise input can be considered to possess the highest degree of OoD, we use the center of their features as the FSS . The FSSD can then be calculated using Equation~\eqref{fssd}. Note this calculation of FSS  is independent from the choice of in-distribution and OoD datasets. When such natural choice of uniform noise is unavailable, we can choose FSS  to be the center of features of OoD validation data instead.

Since a single forward-pass computation through the network  can give us features from each layer, it is also convenient to calculate FSSDs from different layers and ensemble them as .
The ensemble weights  can be determined using logistic regression on some validation data as in~\cite{mahalanobis} (see Evaluation section in Experiments).
In later experiments, if not specified, we use the ensembled FSSD from all layers.
We note that it is also possible to ensemble FSSDs from different architectures or multiple training snapshots~\cite{hor_and_ver_ensemble, snapshot}. This may further enhance the performance of OoD detection.
We investigate the effect of ensembling in the next section. 

Beside, we also  adopt input pre-processing as in ~\cite{odin, mahalanobis} .
The idea is to add small perturbations to the test samples in order to increase the in-distribution score.
It is shown in~\cite{odin, kamoi2020mahalanobis} that in-distribution data are more sensitive to such perturbation and it can therefore enlarge the score gap between in-distribution and OoD samples.
In particular, we perturb as  and take  as the final score.


We present the pseudo-code of computing  in Algorithm 1.

\begin{algorithm}[h]\SetAlgoLined
\DontPrintSemicolon
\KwIn{Test samples , noise samples , ensemble weights , perturbation magnitude , \\ \phantom{\textbf{Input:}\,} feature extractors }
\For{\text{each feature extractor} }
{
1. Estimate FSS , where , \;
2. Add perturbation to test sample:    \;
3. Calculate \;
}
\textbf{Return} 

\label{fss_alg1} 

\caption{Computation of FSSD-Ensem}
\end{algorithm}


\section{Experiments}\label{experiments}

In this section, we investigate the performance of our FSSD algorithm on various OoD detection benchmarks.
\begin{table*}
\centering
\caption{OoD detection benchmarks used in our experiments.}
\resizebox{0.7\linewidth}{!}{

\begin{tabular}{ccr@{/}lr@{/}lp{0pt}ccc}
\toprule
& \multicolumn{5}{c}{In-distribution} & & \multicolumn{2}{c}{OoD}  & \multirow{3}*{Data type}
\\
\cmidrule(lr){2-6} \cmidrule(lr){8-9}
& \multirow{2}*{Dataset} & \multicolumn{2}{c}{\#Classes} & \multicolumn{2}{c}{\#Samples} & & \multirow{2}*{Dataset} & \#Samples \\
& & (Train&Test) & (Train&Test) & & & (Test) \ \ \ \ \\
\midrule
A & FMNIST & 10&10 & 60k&10k & & MNIST & 10k & Image \\
B & CIFAR10 & 10&10 & 50k&10k & & SVHN & 26k & Image \\
C & ImageNet (dogs) & 50&50 & 50k&10k & & ImageNet (non-dogs) & 10k & Image \\
D & ImageNet & 1000&1000 & 1281.2k&50k & & ImageNet-C & 50k  & Image \\
E & CelebA (not blurry) & 10122&10122 & 153.8k&38.5k & & CelebA (blurry) & 10.3k  & Image \\
F & MS-1M & 64736&16184 & 2923.6k&50k & & IJB-C & 50k & Image \\
G & Genome (before 2011) & 10&10 & 1000k&1000k & & Genome (after 2016) & 6000k  & Sequence \\
\bottomrule
\end{tabular}}
\label{bmks}
\end{table*}

\subsection{Setup}

\paragraph{Benchmarks}
To conduct a thorough test of our method, we consider a wide variety of OoD detection benchmarks. In particular, we consider different scales of datasets and different types of data.
We consider different scales of datasets because large scale datasets tend to have more classes which can introduce more ambiguous data. The ambiguous data are of high classification uncertainty, but are not out-of-distribution.
We list the benchmarks in Table~\ref{bmks}. 

We first consider two common benchmarks from previous OoD detection literature~\cite{duq, ren2019likelihood}: \textbf{(A)} FMNIST~\cite{fmnist} vs.\ MNIST~\cite{mnist} and \textbf{(B)} CIFAR10~\cite{cifar10} vs.\ SVHN~\cite{svhn}.
They are known to be challenging for many methods~\cite{ren2019likelihood, generative_know}.
\textbf{(C)} We also construct ImageNet (dogs), a subset of ImageNet ~\cite{imagenet} , as in-distribution data. 
The OoD data are non-dog images from ImageNet.

For large-scale problems, we consider three benchmarks.
\textbf{(D)} We train models on ImageNet and detect corrupted  images from the ImageNet-C dataset~\cite{corruption}.
We test each method on 80 sets of corruptions (16 types and 5 levels).
\textbf{(E)} 
We train models on face images without the ``blurry'' attribute from CelebA~\cite{celeba} and detect face images with the ``blurry'' attribute.
\textbf{(F)} We train models on web images of celebrities from MS-Celeb-1M (MS-1M)~\cite{ms1m} and detect video captures from IJB-C~\cite{ijbc} which in general have lower quality due to pose, illumination, and resolution issues.
We also consider \textbf{(G)} the bacteria genome benchmark introduced by~\cite{ren2019likelihood}, which consists of sequence data.

To train models on  in-distribution datasets, we follow previous works~\cite{mahalanobis} to train LeNet on FMNIST and ResNet with 34 layers on CIFAR10, ImageNet, and ImageNet (dogs).
For two face recognition datasets (CelebA and MS-1M), we train ResNeXt with 50 layers.
For the genome sequence dataset, we use an character embedding layer and two Bidirectional LSTM layers~\cite{bilstm}.




\paragraph{Compared methods}
We compare our method with the following six common methods for OoD detection. \textbf{\textit{Base}}: the baseline method using the maximum softmax probability ~\cite{hendrycks17baseline}.
\textbf{\textit{ODIN}}:  temperature scaling on  logits  and input pre-processing~\cite{odin}.
\textbf{\textit{Maha}}: Mahalanobis distance of the sample feature to the closest class-conditional Gaussian distribution which is estimated from the training data~\cite{mahalanobis}.
In our experiments, we follow ~\cite{mahalanobis} to use both feature (layer) ensemble and input pre-processing. 
\textbf{\textit{DE}}: Deep Ensemble which averages the softmax probability predictions from multiple independently trained classifiers ~\cite{deepensemble}.
In our experiments, we take the average of 5 classifiers by default.
\textbf{\textit{MCD}}: Monte-Carlo Dropout that uses dropout during both training and inference~\cite{mcdropout}.
We follow ~\cite{Ovadia2019CanYT} to dropout convolutional layers.
For OoD detection, we calculate both the mean and the variance of 32 independent predictions and choose the better one to report.
\textbf{\textit{OE}}: Outlier exposure that explicitly enforces uniform probability prediction on an auxiliary dataset of outliers ~\cite{outlierexposure}.
For the choice of auxiliary datsets, we use KMNIST~\cite{kmnist} for benchmark A, CelebA~\cite{celeba} for benchmark C, and ImageNet-1K~\cite{imagenet} for benchmark B, E, F.
We do not evaluate \textit{OE} on the sequence benchmark, since we can not find a reasonable auxiliary dataset.
We remark here that \textit{Base}, \textit{ODIN}, and \textit{FSSD} can be deployed directly with a trained neural network, \textit{MCD} needs a trained neural network with dropout layers, while \textit{DE} needs multiple trained classifiers.
Besides, \textit{Maha} needs to use the training data during OoD detection on test data and \textit{OE} trains a neural network either from scratch or by fine-tuning to utilize the auxiliary dataset. 


\paragraph{Evaluation}
We follow \cite{ren2019likelihood, outlierexposure} to use the following metrics to assess the performance of OoD detection.
\textbf{AUROC}: Area Under the Receiver Operating Characteristic curve.
\textbf{AUPRC}: Area Under the Precision-Recall Curve.
\textbf{FPR80}: False Positive Rate when the true positive rate is 80\%.

For hyper-parameter tuning, we follow ~\cite{mahalanobis, ren2019likelihood, odin} to use a separate validation set, which consists of 1,000 images from each  in-distribution and OoD data pair.
Ensemble weights  for FSSD from different layers are extracted from a logistic regression model, which is trained using nested cross validation within the validation set as in~\cite{mahalanobis, ma2018characterizing}.
The same procedure is performed on \textit{Maha} for fair comparison.
The perturbation magnitude  of input pre-processing for \textit{ODIN}, \textit{Maha}, and \textit{FSSD} is searched from 0 to 0.2 with step size 0.01.
The temperature  of \textit{ODIN} is chosen from 1, 10, 100, and 1000, and the dropout rate of \textit{MCD} is chosen from 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, and 0.5.


\subsection{Main results}
The main results are presented in Table~\ref{tab:main_results} and Figure~\ref{fig:corruption}. 
In Table~\ref{tab:main_results}, we can see that larger datasets entail greater difficulty in OoD detection.
Notably, the advantage of \textit{FSSD} over other methods increases as the dataset size increases. Other methods like \textit{Maha} and \textit{OE} perform well on some small benchmarks, but have large variance across different datasets.
In comparison, \textit{FSSD} maintains great performance on these benchmarks.
On the genome sequence dataset, we also observe that \textit{FSSD} outperforms other methods.
These results show that \textit{FSSD} is a promising effective method  for a wide range of applications.


Inspired by~\cite{Ovadia2019CanYT}, we also evaluate the methods on the ability of detecting distributional dataset shift like Gaussian noise and JPEG artifacts.
Figure~\ref{fig:corruption} shows the means and quartiles of AUROC of the compared methods over 16 types of corruptions on 5 corruption levels.
We can observe that for each method, the performance of OoD detection increases as the level of corruption increases, while \textit{FSSD} enjoys the highest AUROC and much less variation over different types of corruptions.
The CelebA benchmark also evaluates the methods on detecting the dataset shift of the attribute ``blurry''.
However, all methods including \textit{FSSD} do not perform very well.
There are two possible reasons: (1) the attribute ``blurry'' of CelebA may be annotated not clearly enough; (2) the blurs in the wild may be more difficult to detect than the simulated blurs in ImageNet-C.
Overall, we can see that FSSD can more reliably detect different kinds of distributional shifts.





\begin{table*}[t]
\caption{Main results. All values are in \%.}
\centering
\resizebox{1.8\columnwidth}{!}{\begin{tabular}{ccrrrrrrrr}
\toprule
 
                                          &             Datasets (Architecture)       & Metrics & \textit{Base} & \textit{ODIN} & \textit{Maha} & \textit{DE} & \textit{MCD} & \textit{OE} &  \textit{FSSD}  \\
\midrule
\multirow{9}{*}{\makecell{Small-scale \\ benchmarks}}  &                                           
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}FMNIST vs.\ MNIST\\  (LeNet)\end{tabular}} & AUROC &  77.3    & 96.9  &  \textbf{99.6}    &  83.9  &  81.7 & \textbf{99.6} & \textbf{99.6}     \\
                      & & AUPRC &  79.2    &  93.0  & \textbf{99.7}  & 83.3 &   85.3 & 99.6  &  \textbf{99.7}    \\
                                    & & FPR80 &  43.5    & 2.5  &  \textbf{0.0}    &  27.5  & 36.8 & \textbf{0.0} & \textbf{0.0}   \\
\cmidrule(l){2-10}
& \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR10 vs.\ SVHN\\  (ResNet34)\end{tabular}} & AUROC & 89.9   & 96.7   &  99.1    &  93.7  &  96.7 & 90.4  &  \textbf{99.5}   \\
                                                                               & & AUPRC &  85.4    & 92.5 & 98.1     &  90.6  &  93.9   & 89.8  & \textbf{99.5}  \\
                                                                               & & FPR80 & 10.1     & 4.7     &   \textbf{0.3}   & 3.7   & 2.4 & 12.5 & 0.4     \\
\cmidrule(l){2-10}
& \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}ImageNet dogs vs.\ non-dogs\\  (ResNet34)\end{tabular}}                                               & AUROC &  88.5 & 90.8  & 83.3 & 89.0   & 67.2    & 92.5    & \textbf{93.1}  \\
                                                                               &  & AUPRC & 86.1     &  88.6    &  83.0    & 89.0  & 66.9   &  \textbf{92.6}  &  92.5    \\
                                                                               & & FPR80 &   19.5   &  15.2   &   30.1   & 18.8 & 59.2   &   \textbf{7.9}  &  10.2  \\
\midrule
\multirow{6}{*}{\makecell{Large-scale \\ benchmarks}}  &                                      \multirow{3}{*}{ \begin{tabular}[c]{@{}c@{}}CelebA non-blurry vs.\ blurry \\  (ResNeXt50)\end{tabular}}                                               & AUROC &     71.7 & 73.3     &      73.9 &  74.5  & 69.8    &  71.5 & \textbf{78.3}    \\
                                                                               & & AUPRC & 89.9     & 91.4  & 90.9 & 91.4   &  88.7   &  90.7 & \textbf{92.8}   \\
                                                                               & & FPR80 &  52.0    &  50.3     & 46.0 & 47.1   & 53.2    & 54.2 & \textbf{39.2}  \\
\cmidrule(l){2-10}
& \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}MS-1M vs.\ IJB-C\\  (ResNeXt50)\end{tabular}}                                               & AUROC &   60.0   & 61.3 & 82.5  & 63.0   & 65.5  & 52.6  & \textbf{86.7}  \\
                                                                               & & AUPRC &  53.3    &  55.9     & 80.6     &     56.1   &  59.4 & 46.6  & \textbf{86.1}    \\
                                                                               & & FPR80 &  61.8    &    59.4  &  29.6    &   56.7      & 58.8 & 64.2 & \textbf{22.1}    \\
\midrule
\multirow{3}{*}{\makecell{Sequence \\ benchmark}} & \multirow{3}{*}{ \begin{tabular}[c]{@{}c@{}} Bacteria Genome \\  (LSTM)\end{tabular}}                                               & AUROC &   69.6   &  70.6  &  70.4  & 70.0   & 69.3    &  NA    & \textbf{74.8} \\
                                                                               & & AUPRC &  69.9   & 71.9  &  69.3   &  56.0  & 70.2    & NA  & \textbf{75.8} \\
                                                                               & & FPR80 &  57.4  & 55.9 & 53.7  &  \textbf{30.0} & 58.3 & NA & 47.4   \\
\bottomrule
\end{tabular}}
\label{tab:main_results}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{4.pdf}
    \caption{Comparison of AUROC on ImageNet vs.\ ImageNet-C.  FSSD enjoys the highest mean and the least variance across all corruption levels.}
    \label{fig:corruption}
\end{figure}

\subsection{Robustness} 

In practice, it is possible that the test data are slightly corrupted or shifted due to the change of data source, e.g., from lab to real world.
We  evaluate the  ability to distinguish in-distribution data from OoD data when test data (both in-distribution and OoD) are slightly corrupted.
Note that we still use non-corrupted data during network training and hyper-parameter tuning.
We apply Gaussian noise and impulse noise, two typical corruptions, with varying levels.
Test results on CIFAR10 vs.\ SVHN and ImageNet dogs vs.\ non-dogs are shown in Figure~\ref{fig:robustness}.
We can see that \textit{FSSD} is robust to corruptions presented in test images, while  other methods may degrade.


\begin{figure}[t]
\centering
\begin{subfigure}{0.22\textwidth}
  \centering
\includegraphics[width=1\linewidth]{5a.pdf} 
\end{subfigure}
\begin{subfigure}{.22\textwidth}
  \centering
\includegraphics[width=1\linewidth]{5b.pdf}  
\end{subfigure}
\begin{subfigure}{0.22\textwidth}
  \centering
\includegraphics[width=1\linewidth]{  5c.pdf} 
\end{subfigure}
\begin{subfigure}{.22\textwidth}
  \centering
\includegraphics[width=1\linewidth]{ 5d.pdf}  


\end{subfigure}
\caption{Comparison of OoD detection robustness among methods on slightly corrupted test data.
}
\label{fig:robustness}
\end{figure}


\begin{figure*}
\centering
\begin{subfigure}{\textwidth}
  \centering
\includegraphics[width=1\linewidth]{6a.pdf}  
  \caption{ImageNet (dogs) vs.\ ImageNet (non-dogs)}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
\includegraphics[width=1\linewidth]{6b.pdf}  
  \caption{CIFAR10 vs.\ SVHN}
\end{subfigure}
\caption{FSSDs from different layers behave differently. Each row contains FSSD histograms extracted from different layers of a trained neural network. FSSDs of ImageNet (dogs) and ImageNet (non-dogs) are similar in early layers; while FSSDs of CIFAR10 and SVHN differ in all the layers. This can be explained by the fact that ImageNet (dogs) and ImageNet (non-dogs) are similar in low-level statistics since they are sampled from the same dataset, and that FSSDs in early layers capture more of the difference in low-level statistics.}
\label{fig:layer_ensemble}
\end{figure*}
\begin{figure*}[h]
    \centering
\begin{subfigure}{0.3\textwidth}
         \centering
         \includegraphics[width=\linewidth]{7a.pdf}
\label{dataset2}
    \end{subfigure}
    \begin{subfigure}{.3\textwidth}
         \centering
         \includegraphics[width=\linewidth]{7b.pdf}
\label{dataset2}
    \end{subfigure}
    \begin{subfigure}{.3\textwidth}
         \centering
         \includegraphics[width=\linewidth]{7c.pdf}
\label{dataset2}
    \end{subfigure}
\caption{Comparison of network ensembles of \textit{Base}, \textit{ODIN}, \textit{Maha}, and \textit{FSSD} scores.
}
    \label{ensemble}
\end{figure*}

\subsection{Effects of ensemble}

During our experiments, we find that the ensemble plays an important role in enhancing the performance of FSSD. 
Previous studies show that an important issue for ensemble-based algorithms is enforcing diversity~\cite{deepensemble}.
In our case, we find that FSSD has high diversity across different layers, and benefit from such diversity to reach higher performance. In Figure~\ref{fig:layer_ensemble}, we find that FSSD in different layers are working differently. 
This can be explained by previous works on understanding neural networks by visualizing the different representations learned by low and deep layers of a neural network~\cite{vis_eccv2014, scenecnn_iclr15}.
Generally, FSSDs from deep layers reflect more high-level features and FSSDs from early layers reflect more low-level statistics. ImageNet (dogs) and ImageNet (non-dogs) are from the same dataset (ImageNet), and are therefore similar in terms of low-level statistics; while the differences between CIFAR10 and SVHN are in all different levels.
From the perspective of kernel interpretation, this means that the neural tangent kernels of different layers diversify well and allow the ensemble of FSSD to capture different aspects of the discrepancy between the test data and training data.  We show more examples of FSSDs in different layers on our Github page.




Furthermore, Figure~\ref{ensemble}  demonstrates that the performance of FSSD can be further boosted by using network ensembles. Considering the low computational cost of ensembling FSSD, this shows FSSD can be a promising method in scenarios that require high-performance OoD detectors. 







\section{Related works}\label{related_work}

\subsection{Out-of-distribution detection}
According to different understandings of OoD samples, previous OoD detection methods can be summarized into four categories.

(1) Some methods regard OoD samples as those with uniform probability prediction across classes~\cite{whyRelu, hendrycks17baseline, odin, confidence-calibrated, adv_manipulation, provably} and treat the test samples with high entropy or low maximum prediction probability as OoD data.
Since these methods are based on prediction, they run the risk of mis-classifying ambiguous data as OoD samples, e.g., when there are thousands of classes in a large-scale dataset.

(2) OoD samples can also be characterized as samples with high epistemic uncertainty which reflects the lack of information on these samples~\cite{deepensemble, mcdropout, WeightUncertainty}.
Specifically, we can propagate the uncertainty of models to the uncertainty of predictions, which characterizes the level of OoD.
\textit{MCD} and \textit{DE} are two popular choices of this type.
However, it is reported that current epistemic uncertainty estimations may noticeably degrade under dataset distributional shift~\cite{Ovadia2019CanYT}.
Our experiments on detecting ImageNet-C from ImageNet (Figure~\ref{fig:corruption}) confirm this.

(3) When the density of data can be approximated, e.g., using generative models~\cite{kingma2018glow, pixelcnn++}, OoD samples can be classified as those with low density.
Recent works provide many inspiring insights on how to improve this idea~\cite{ren2019likelihood, typicality, inputcomplexity}. 
However, these methods typically have extra training difficulty incurred by large generative models.


(4) There are also works designing non-Euclidean metrics to compare test samples to training samples, and regard those with higher distances to training samples as OoD samples~\cite{mahalanobis,duq, kamoi2020mahalanobis, sngp}.
Our approach resembles this type most. Instead of comparing test samples to training samples, we compare the features of the test samples to the center of OoD features.



\subsection{Non-Lipschitz property of neural networks}
Many previous works have reported that the trained neural networks are not Lipschitz continuous~\cite{behrmann_invertible_2019, duq, sngp}. Some OoD detection methods claim that adding Lipschitz constraint may help improve OoD detection performance~\cite{duq, sngp}.
~\cite{bietti_inductive_2019} also demonstrated that the NTK kernel mappings of ReLU networks are  non-Lipschitz. 

Instead of adding Lipschitz constraint, this work utilizes the non-Lipschitz property of neural networks for OoD detection. In fact, the feature space singularity provides us with new understandings of the non-Lipschitz region in the input space, i.e., inputs that are dissimilar to the training data in terms of the NTK.
In the future, it is interesting to further investigate the non-Lipschitz properties of neural networks, e.g., non-Lipschitz behaviours in different layers and how the inductive bias of NTK influences the OoD detection using FSSD.

\section{Conclusion}
In this work, we propose a new OoD detection algorithm based on a novel observation that OoD samples concentrate in the feature space of a trained neural network.
We provide analysis and understanding of the concentration phenomenon by analyzing the training dynamics both theoretically and empirically and further interpreted the algorithm with the neural tangent kernel.
We demonstrate that our algorithm is state-of-the-art in detection performance and is robust to measurement noise. Our further investigation on the effect of ensemble reveals diversity in layer ensembles and shows promising performance of network ensembles.
In summary, we hope that our work can provide new insights for understanding properties of neural networks and add an alternative simple and effective OoD detection method to the safe AI deployment toolkits.


\section*{Acknowledgement}
Bin Dong is supported in part by Beijing Natural Science Foundation (No. 180001); National Natural Science Foundation of China (NSFC) grant No. 11831002 and Beijing Academy of Artificial Intelligence (BAAI).

\bibliography{FSS_bib.bib}

\end{document}
