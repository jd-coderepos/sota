\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{diagbox}


\usepackage{subfigure}


\def\ds3c{\textbf{DC}}
\def\s3c{C}


\def\b{\textbf{b}}
\def\c{\textbf{c}}
\def\e{\textbf{e}}
\def\g{\textbf{g}}
\def\h{\textbf{h}}
\def\q{\textbf{q}}
\def\w{\textbf{w}}
\def\x{\textbf{x}}
\def\y{\textbf{y}}
\def\z{\textbf{z}}
\def\0{\textbf{0}}
\def\1{\textbf{1}}
\def\ie{i.e.}
\def\eg{e.g.}
\def\st{\textrm{s.t.}}
\def\and{\textrm{and}}
\def\rank{\textrm{rank}}
\def\Diag{\textrm{Diag}}
\def\diag{\textrm{diag}}
\def\trace{\textrm{trace}}
\def\C{\mathcal{C}}
\def\D{\mathcal{D}}
\def\E{\mathcal{E}}
\def\H{\mathcal{H}}
\def\L{\mathcal{L}}
\def\Q{\mathcal{Q}}
\newcommand{\RR}{I\!\!R} 

\newcommand{\myparagraph}[1]{\smallskip\noindent\textbf{#1.}}
\newcommand{\mcr}{\color{red}}
\newcommand{\mcb}{\color{blue}}
\def\ie{i.e.}
\def\eg{e.g.}






\cvprfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}




\begin{document}

\title{Self-Supervised Convolutional Subspace Clustering Network}




\author{Junjian Zhang, Chun-Guang Li,~Chong You,~Xianbiao Qi,\\
~Honggang Zhang,~Jun Guo,~and~Zhouchen Lin\\
 SICE, Beijing University of Posts and Telecommunications \\
 EECS, University of California, Berkeley \ \ \ \  Shenzhen Research Institute of Big Data \\  Key Laboratory of Machine Perception (MOE), School of EECS, Peking University \\
}

\maketitle

\begin{abstract}

Subspace clustering methods based on data self-expression have become very popular for learning from data that lie in a union of low-dimensional linear subspaces. However, the applicability of subspace clustering has been limited because practical visual data in raw form do not necessarily lie in such linear subspaces. On the other hand, while Convolutional Neural Network (ConvNet) has been demonstrated to be a powerful tool for extracting discriminative features from visual data, training such a ConvNet usually requires a large amount of labeled data, which are unavailable in subspace clustering applications. To achieve simultaneous feature learning and subspace clustering, we propose an end-to-end trainable framework, called Self-Supervised Convolutional Subspace Clustering Network (SConvSCN), that combines a ConvNet module (for feature learning), a self-expression module (for subspace clustering) and a spectral clustering module (for self-supervision) into a joint optimization framework. Particularly, we introduce a dual self-supervision that exploits the output of spectral clustering to supervise the training of the feature learning module (via a classification loss) and the self-expression module (via a spectral clustering loss). Our experiments on four benchmark datasets show the effectiveness of the dual self-supervision and demonstrate superior performance of our proposed approach.

\end{abstract}



\section{Introduction}
\label{sec:intro}

In many real-world applications such as image and video processing, we need to deal with a large amount of high-dimensional data.
Such data can often be well approximated by a union of multiple low-dimensional subspaces, where each subspace corresponds to a class or a category. For example, the frontal facial images of a subject taken under varying lighting conditions approximately span a linear subspace of dimension up to nine \cite{Ho:CVPR03}; the trajectories of feature points related to a rigidly moving object in a video sequence span an affine subspace of dimension up to three \cite{CT:IJCV92}; the set of handwritten digit images of a single digit also approximately span a low-dimensional subspace \cite{Hastie:StatSci98}. In such cases, it is important to segment the data into multiple groups where each group contains data points from the same subspace. This problem is known as \textit{subspace clustering} \cite{ReVidal:ISM11,Vidal:Springer16}, which we formally define as follows.

{\textbf{Problem} (Subspace Clustering).} {\em Let  be a real-valued matrix whose columns are drawn from a union of  subspaces of , , of dimensions , for . The goal of subspace clustering is to segment the columns of  into their corresponding subspaces.}


In the past decade, subspace clustering has become an important topic in unsupervised learning and many subspace clustering algorithms have been developed \cite{Elham:CVPR09, Liu:ICML10, Favaro:CVPR11, Lu:ECCV12, Liu:TPAMI13, Lu:ICCV13-TraceLasso, Feng:CVPR14, Li:CVPR15, Peng:TCYB16, Zhang:VCIP16, You:CVPR16-EnSC, Li:TIP17}. These methods have been successfully applied to various applications such as motion segmentation \cite{Vidal:PAMI05, Rao:TPAMI10}, face image clustering~\cite{Elham:TPAMI13}, genes expression microarray clustering \cite{McWilliams:DMKD14, Li:ICPR18} and so on.

Despite the great success in the recent development of subspace clustering, its applicability to real applications is very limited because practical data do not necessarily conform with the linear subspace model.
In face image clustering, for example, practical face images are often not aligned and often contain variations in pose and expression of the subject.
Subspace clustering cannot handle such cases as images corresponding to the same face no longer lie in linear subspaces.
While there are recently developed techniques for joint image alignment and subspace clustering \cite{Li:PR16}, such a parameterized model is incapable of handling a broader range of data variations such as deformation, translation and so on.
It is also possible to use manually designed invariance features such as SIFT~\cite{Lowe:IJCV04}, HOG \cite{Dalal:CVPR05} and PRICoLBP~\cite{Qi:TPAMI14} of the images before performing subspace clustering, \eg, in \cite{Peng:IJCAI16, Peng:arxiv17}.
However, there has been neither theoretical nor practical evidence to show that such features follow the linear subspace model.

Recently, Convolutional Neural Networks (ConvNets) have demonstrated superior ability in learning useful image representations in a wide range of tasks such as face/object classification and detection \cite{Krizhevsky:NIPS2012,Parkhi:BMVC15}.
In particular, it is shown in \cite{Lezama:CVPR18} that when applied to images of different classes, ConvNets are able to learn features that lie in a union of linear subspaces.
The challenge for training such a ConvNet, however, is that it requires a large number of labeled training images which is often unavailable in practical applications.
	
In order to train ConvNet for feature learning without labeled data, many methods have been recently proposed by exploiting the self-expression of data in a union of subspaces \cite{Peng:IJCAI16, Ji:NIPS17, Peng:arxiv17,Zhou:CVPR18}.
Specifically, these methods supervise the training of ConvNet by inducing the learned features to be such that each feature vector can be expressed as a linear combination of the other feature vectors.
However, it is difficult to learn good feature representations in such an approach due to the lack of effective supervision.



\myparagraph{Paper contribution}
In this paper, we develop an end-to-end trainable framework for simultaneous feature learning and subspace clustering, called Self-Supervised Convolutional Subspace Clustering Network (SConvSCN).
In this framework, we use the current clustering results to self-supervise the training of feature learning and self-expression modules, which is able to significantly improve the subspace clustering performance.
In particular, we introduce the following two self-supervision modules:
\begin{enumerate}
\item We introduce a spectral clustering module which uses the current clustering results to supervise the learning of the self-expression coefficients. This is achieved by inducing the affinity generated from the self-expression to form a segmentation of the data that aligns with the current class labels generated from clustering.
\item We introduce a classification module which uses the current clustering results to supervise the training of feature learning.
This is achieved by minimizing the classification loss between the output of a classifier trained on top of the feature learning module and the current class labels generated from clustering.
\end{enumerate}
We propose a training framework where the feature representation, the data self-expression and the data segmentation are jointly learned and alternately refined in the learning procedure.
Conceptually, the initial clustering results do not align exactly with the true data segmentation, therefore the initial self-supervision incurs errors to the training.
Nonetheless, the feature learning is still expected to benefit from such self-supervision as there are data with correct labels that produce useful information. An improved feature representation subsequently helps to learn a better self-expression and consequently produce a better data segmentation (\ie, with less wrong labels).
Our experiments on four benchmark datasets demonstrate superior performance of the proposed approach. 


\myparagraph{Paper Outline} The remainder of this paper is organized as follows. Section~\ref{sec:related-work} reviews the relevant work. Section~\ref{sec:ConvSSCNet} presents our proposal---the components in SConvSCN, the used cost functions, and the training strategy. Section~\ref{sec:experiments} shows experimental results with discussions, and Section~\ref{sec:conclusion} concludes the paper.



\section{Related Work}
\label{sec:related-work}

In this section, we review the relevant prior work in subspace clustering. For clarity, we group them into two categories: a) subspace clustering in original space; and b) subspace clustering in feature space.


\begin{figure*}[t]
\vskip -5pt
\begin{center}
\centerline{\includegraphics[width=0.85\textwidth]{ConvSCNet17.eps}} \caption{Architecture of the proposed Self-Supervised Convolutional Subspace Clustering Network (SConvSCN). It consists of mainly five modules: a) stacked convolutional encoder module, which is used to extract convolutional features; b) stacked convolutional decoder module, which is used with the encoder module to initialize the convolutional module; c) self-expression module, which is used to learn the self-expressive coefficient matrix and also takes the self-supervision information from the result of spectral clustering to refine the self-expressive coefficients matrix; d) FC-layers based self-supervision module, which builds a self-supervision path back to the stacked convolutional encoder module; e) spectral clustering module, which provides self-supervision information to guide the self-expressive model and FC-layers module. The modules with solid line box are the backbone components; whereas the modules in dashed box are the auxiliary components to facilitate the training of the whole network.}
\label{fig:ConvSSCNet}
\end{center}
\vskip -25pt
\end{figure*}

\subsection{Subspace Clustering in Original Space}
In the past years, subspace clustering has received a lot of attention and many methods have been developed. Among them, methods based on spectral clustering are the most popular, \eg, \cite{Elham:CVPR09, Liu:ICML10, Favaro:CVPR11, Lu:ECCV12, Elham:TPAMI13, Liu:TPAMI13, Lu:ICCV13-TraceLasso, Feng:CVPR14, Li:CVPR15, Peng:TCYB16, You:CVPR16-EnSC, Zhang:VCIP16, Li:TIP17, You:ECCV18}. These methods divide the task of subspace clustering into two subproblems. The first subproblem is to learn a data affinity matrix from the original data, and the second subproblem is to apply spectral clustering on the affinity matrix to find the segmentation of the data. The two subproblems are solved successively in one-pass~\cite{Elham:CVPR09, Liu:ICML10, Favaro:CVPR11, Lu:ECCV12, Lu:ICCV13-TraceLasso, You:CVPR16-EnSC} or solved alternately in multi-pass~\cite{Feng:CVPR14, Li:CVPR15, Guo:IJCAI15, Zhang:VCIP16, Li:TIP17}. 


Finding an informative affinity matrix is the most crucial step. Typical methods to find an informative affinity matrix are based on the self-expression property of data \cite{Elham:CVPR09, Vidal:Springer16}, which states that a data point in a union of subspaces can be expressed as a linear combination\footnote{If data points lie in a union of affine subspaces \cite{Li:JSTSP18}, then the {\em linear} combination will be modified to {\em affine} combination.} of other data points, \ie, , where  is used to model the noise or corruption in data. It is expected that the linear combination of data point  uses the data points that belong to the same subspace as . To achieve this objective, different types of regularization terms on the linear combination coefficients are used. For example, in \cite{Elham:CVPR09} the  norm is used to find sparse linear combination; in \cite{Liu:ICML10} the nuclear norm of the coefficients matrix is used to find low-rank representation; in \cite{Wang:NIPS13-LRR+SSC, You:CVPR16-EnSC} the mixture of the  norm and the  norm or the nuclear norm is used to balance the sparsity and the denseness of the linear combination coefficients; and in \cite{Xin:TSP18} a data-dependent sparsity-inducing regularizer is used to find sparse linear combination. On the other hand, different ways to model the noise or corruptions in data have also been investigated, \eg, the vector  norm is used in \cite{Elham:CVPR09}, the  norm is adopted in \cite{Liu:ICML10}, and the correntropy term is used in \cite{He:TNNLS16}.



\subsection{Subspace Clustering in Feature Space}

For subspace clustering in feature space, we further divide the existing methods into two types. The first type uses \emph{latent feature space}, which is induced via a Mercer kernel, \eg, \cite{Patel:ICIP14, Patel:JSTSP15, Ngu:Neuc15, Xiao:TNNLS16}, or constructed via matrix decomposition, \eg, \cite{Liu:ICCV11}, \cite{Patel:ICCV13}. The second type use \emph{explicit feature space}, which is designed by manual feature extraction, \eg, \cite{Peng:IJCAI16}, or is learned from data, \eg, \cite{Ji:NIPS17, Zhou:CVPR18}.


\myparagraph{Latent Feature Space} Many recent works have employed the kernel trick to map the original data into a high-dimensional latent feature space, in which subspace clustering is performed, \eg, \cite{Patel:ICIP14, Patel:JSTSP15, Ngu:Neuc15, Xiao:TNNLS16}. For example, predefined polynomial and Gaussian kernels are used in the kernel sparse subspace clustering method \cite{Patel:ICIP14, Patel:JSTSP15} and the kernel low-rank representation method \cite{Ngu:Neuc15, Xiao:TNNLS16, Ji:AdaLRK-arXiv17}. Unfortunately, it is {\em not} guaranteed that the data in the latent feature space induced with such predefined kernels lie in low-dimensional subspaces.\footnote{In \cite{Ji:AdaLRK-arXiv17}, while the data matrix in the latent feature space is encouraged to be low-rank, it is not necessary that the data in feature space are encouraged to align with a union of linear subspaces.}




On the other hand, the latent feature space has also been constructed via matrix decomposition, \eg, \cite{Liu:ICCV11}, \cite{Patel:ICCV13}. In \cite{Liu:ICCV11}, a linear transform matrix and a low-rank representation are computed simultaneously; in \cite{Patel:ICCV13}, a linear transform and a sparse representation are optimized jointly. However, the representation power of the learned linear transform is still limited.


\myparagraph{Explicit Feature Space} Deep learning has gained a lot of research interests due to its powerful ability to learn hierarchical features in an end-to-end trainable way~\cite{Hinton:ISPM12, Krizhevsky:NIPS2012}. Recently, there are a few works that use techniques in deep learning for feature extraction in subspace clustering. For example, in \cite{Peng:IJCAI16,Peng:arxiv17}, a fully connected deep auto-encoder network with hand-crafted features (\eg, SIFT or HOG features) combined with a sparse self-expression model is developed; in \cite{Ji:NIPS17}, a stacked convolutional auto-encoder network with a plus-in self-expression model is proposed. While promising clustering accuracy has been reported, these methods are still suboptimal because \emph{neither} the potentially useful supervision information from the clustering result has been taken into the feature learning step \emph{nor} a joint optimization framework for fully combining feature learning and subspace clustering has been developed. More recently, in \cite{Zhou:CVPR18}, a deep adversarial network with a subspace-specific generator and a subspace-specific discriminator is adopted in the framework of \cite{Ji:NIPS17} for subspace clustering. However, the discriminator need to use the dimension of each subspace, which is usually unknown.



In this paper, we attempt to develop a joint optimization framework for combining feature learning and subspace clustering, such that the useful self-supervision information from subspace clustering result could be used to guide the feature learning and to refine the self-expression model. Inspired by the success of Convolutional Neural Networks in recent years for classification tasks on images and videos datasets~\cite{Krizhevsky:NIPS2012} and the recent work \cite{Ji:NIPS17}, we integrate the convolutional feature extraction module into subspace clustering to form an end-to-end trainable joint optimization framework, called Self-Supervised Convolutional Subspace Clustering Network (SConvSCN). In SConvSCN, both the stacked convolutional layers based feature extraction and the self-expression based affinity learning are effectively self-supervised by exploiting the feedback from spectral clustering.



\section{Our Proposal: Self-Supervised Convolutional Subspace Clustering Network} \label{sec:ConvSSCNet}

In this section, we present our SConvSCN for joint feature learning and subspace clustering.
We start with introducing our network formulation (see Fig.~\ref{fig:ConvSSCNet}), then introduce the self-supervision modules.
Finally, we present an effective procedure for training the proposed network.


\subsection{Network Formulation}

As aforementioned, our network is composed of a feature extraction module, a self-expression module and self-supervision modules for training the former two modules.

\myparagraph{Feature Extraction Module} A basic component of our proposed SConvSCN is the feature extraction module, which is used to extract features from raw data that are amenable to subspace clustering.
To extract localized features while preserving spatial locality, we adopt the convolutional neural network which is comprised of multiple convolutional layers.
We denote the input to the network as  where  is the image.
A convolutional layer  contains a set of filters  and the associated biases , , and produces  feature maps from the output of the previous layer. The feature maps  in the top layer  of the network are then used to form a representation of the input data . Specifically, the  feature maps  are vectorized and concatenated to form a representation vector , i.e.,

where  are row vectors denoting the vectorization of the feature maps .
These vectors are horizontally concatenated and then transposed to form the vector .

To ensure that the learned representation  contains meaningful information from the input data , the feature maps  are fed into a decoder network to reconstruct an image .
The loss function for this encoder-decoder network is the reconstruction error:

where  is the number of images in the training set.


\myparagraph{Self-Expression Module}
State-of-the-art subspace clustering methods are based on the self-expression property of data, which states that each data point in a union of subspaces can be expressed as a linear combination of other data points~\cite{Elham:CVPR09,Vidal:Springer16}. In order to learn feature representations that are suitable for subspace clustering, we adopt a self-expression module that imposes the following loss function:

where  is a matrix containing features from the feature extraction module as its columns,  is a properly chosen regularization term, the constraint  is optionally used to rule out a trivial solution of , and  is a tradeoff parameter.

\myparagraph{Self-Supervision Modules}
Once the self-expression coefficient matrix  is obtained,
we can compute a data affinity matrix as .
Subsequently, spectral clustering can be applied on  to obtain a segmentation of the data by minimizing the following cost:

where  is a set of all valid segmentation matrices with  groups, and  and  are respectively the -th and -th columns of  indicating the membership of each data point to the assigned cluster. In practice, since the search over all  is combinatorial, spectral clustering techniques usually relax the constraint  to .

Observe that the spectral clustering produces a labeling of the data set which, albeit is not necessarily the correct class label for all the data points, contains meaningful information about the data.
This motivates us to supervise the training of the feature extraction and self-expression modules using the output of spectral clustering.
In principle, the features learned from the feature extraction module should contain enough information for predicting the class labels of the data points.
Therefore, we introduce a classification layer on top of the feature extraction module which is expected to produce labels that aligns with the labels generated in spectral clustering. Furthermore, the segmentation produced by
spectral clustering can also be used to
construct a binary segmentation matrix, which contains information regarding which data points should be used in the expression of a particular data point.
Therefore, we incorporate the objective function of spectral clustering as a loss function in our network formulation, which has the effect of supervising the training of the self-expression module.
We present the details of these two self-supervision modules in the following two subsections.



\subsection{Self-Supervision for Self-Expression}
\label{sec:weak-supervision-self-express}

To exploit the information in the labels produced by spectral clustering, we incorporate spectral clustering as a module of the network which provides a feedback to the self-expression model (see Fig. \ref{fig:ConvSSCNet}).

To see how the objective function of spectral clustering in \eqref{eq:spectral-clustering-cost} provides such feedback, we rewrite \eqref{eq:spectral-clustering-cost} to a weighted  norm of  as in \cite{Li:CVPR15}, that is,

where we have used the fact that .
It can be seen from \eqref{eq:spectral-clu-cost-C-Q-norm} that  measures the discrepancy between the coefficients matrix  and the segmentation matrix .
When  is provided, minimizing the cost  has the effect of enforcing the self-expression matrix  to be such that an entry  is nonzero only if the -th and -th data points have the same class labels.
Therefore, incorporating the term  in the network formulation helps the training of the self-expression module. That is, the result of previous spectral clustering can be incorporated into the self-expression model to provide self-supervision for refining the self-expression matrix .


\subsection{Self-Supervision for Feature Learning}
\label{sec:weak-supervision-convolutional-layer}

We also use the class labels generated from spectral clustering to supervise the training of the feature extraction module.
Notice that the output of spectral clustering is an -dimensional vector which indicates the membership to  subspaces (\ie, clusters). Thus, we design FC layers as  , where  is the dimension of the extracted convolutional feature, which is defined as the concatenation of the different feature maps of the last convolutional layer in the encoder block, and  and  are the numbers of neurons in the two FC layers, respectively. 


Denote  as the -dimensional output of the FC layers, where .
Note that the output  of spectral clustering will be treated as the target output of the FC layers.
To exploit the self-supervision information to train the convolutional encoder, we define a mixture of \textit{cross-entropy loss} and \textit{center loss} (CEC) as follows:

where  is a normalization of  via \emph{softmax},
 denotes the cluster center which corresponds to ,  is to take the index of  from the output of spectral clustering, and  is a tradeoff parameter. The first term of   is effectively a cross-entropy loss and the second term of  is a center loss which compresses the intra-cluster variations.







An important issue in defining such a loss function is that the output of spectral clustering  provides merely \emph{pseudo labels} for the input data. That is, the label index assigned to a cluster in the returned result of spectral clustering is up to an unknown permutation.
Therefore, the class labels from two successive epochs might not be consistent.
To address this issue, we propose to perform a permutation of the new pseudo labels via Hungarian algorithm \cite{Munkres:JSIAM57} to find an optimal assignment between the pseudo labels of successive iterations
before feeding them into the self-supervision module with the cross-entropy loss in \eqref{eq:CAE-cost-Cross-Entropy}.

\myparagraph{Remark 1} Note that the output of spectral clustering is used in two interrelated self-supervision modules and thus we call it a \emph{dual self-supervision} mechanism.\footnote{
While it is also sensible to term our approach with ``self-training'', we prefer to use the term ``self-supervision'' in order to emphasizes on the mechanism of guiding the training of the whole framework, that is to make each component as consistent as possible (\ie, be separable, self-expressive, and block diagonal).}


\subsection{Training SConvSCN}
\label{sec:training-ConvSSCNet}

To obtain an end-to-end trainable framework, we design the total cost function of SConvSCN by putting together the costs in \eqref{eq:CAE-cost-0}, \eqref{eq:Self-expression}, \eqref{eq:spectral-clu-cost-C-Q-norm}, and \eqref{eq:CAE-cost-Cross-Entropy} as follows:

where ,
,
,
and , ,  and  are four tradeoff parameters.
The tradeoff parameters are set roughly to be inversely proportional to the value of each cost in order to obtain a balance amongst them.





\begin{algorithm}[tb]
\small
   \caption{Procedure for training SConvSCN}
   \label{alg:ConvSSCNet-train}
\begin{algorithmic}[1]
      \REQUIRE \texttt{Input data, tradeoff parameters, maximum iteration T, T, and t=1.} \STATE \texttt{Pre-train the stacked convolutional module via stacked CAE.} \STATE \texttt{(Optional) Pre-train the stacked convolutional module with the self-expressive layer.} \STATE \texttt{Initialize the FC layers.}
      \STATE \texttt{Run self-expressive layer.}
      \STATE \texttt{Run spectral clustering layer to get the segmentation Q.}
      \WHILE {\texttt{ t  T }} \STATE \texttt{Fixed Q, update the other parts T epoches.}
      \STATE \texttt{Run spectral clustering once to update Q and set t  t+1.}
      \ENDWHILE
\ENSURE \texttt{trained SConvSCN and Q.}
\end{algorithmic}
\end{algorithm}


To train SConvSCN, we propose a two-stage strategy as follows: a) pre-train the stacked convolutional layers to provide an initialization of SConvSCN;
b) train the whole network with the assistance of the self-supervision information provided by spectral clustering.

\myparagraph{Stage I: Pre-Training Stacked Convolutional Module} The pre-training stage uses the cost . In this stage, we set the weights in the two FC layers as zeros, which yield zeros output. Meanwhile, we also set the output of spectral clustering as zero vectors, \ie,  for . By doing so, the two FC layers are ``sleeping'' during this pre-training stage.
Moreover, we set the coefficient matrix  as an identity matrix, which is equivalent to training SConvSCN without the self-expression layer. As an optional pre-training, we can also use the pre-trained stacked CAE to train the stacked CAE with the self-expression layer. 


\myparagraph{Stage II: Training the Whole SConvSCN}
In this stage, we use the total cost  to train the whole SConvSCN as a stacked CAE assisted with the self-expression module and dual self-supervision. To be more specific, given the spectral clustering result , we update the other parameters in SConvSCN for  epoches, and then perform spectral clustering to update . For clarity, we provide the detailed procedure to train SConvSCN in Algorithm \ref{alg:ConvSSCNet-train}.


\myparagraph{Remark 2} In the total cost function as \eqref{eq:total-cost-ConvSSCNet}, if we set , then the two self-supervision blocks will disappear and our SConvSCN reduces to DSCNet \cite{Ji:NIPS17}. Thus, it would be interesting to add an extra pre-training stage, \ie, using the cost function  to train the stacked convolutional module and the self-expressive layer together before evoking the FC layers and the spectral clustering layer. This is effectively a DSCNet \cite{Ji:NIPS17}.
In experiments, as used in \cite{Ji:NIPS17}, we stop the training by setting a maximum number of epoches .

\begin{table}[t]
\center
\small
\begin{tabular}{c | cc | cc }
\hline
     & \multicolumn{2}{c|}{\textbf{Extended Yale B}} & \multicolumn{2}{c}{\textbf{ORL}}\\
\hline
    Layers & kernel size & channels & kernel size & channels \\
\hline
    encoder-1 &  & 10 &  & 3 \\
    encoder-2 &  & 20 &  & 3 \\
    encoder-3 &  & 30 &  & 5 \\
    decoder-1 &  & 30 &  & 5 \\
    decoder-2 &  & 20 &  & 3 \\
    decoder-3 &  & 10 &  & 3 \\
\hline
\end{tabular}
\caption{Network settings for Extended Yale B and ORL.}
\label{tab:Network-settings-ExYaleB-ORL-COIL20}
\end{table}



\begin{table*}[t]
\scriptsize
\centering
\begin{tabular}{c|c c c c c c c c c c c |c c}
\hline
\!Methods & \!LRR & \!LRSC & \!SSC & \!\!AE+ SSC & \!KSSC & \!SSC-OMP & \!soft SC & \!EDSC & \!AE+ EDSC & \!DSC- & \!DSC-
& {\bf Ours} () & {\bf Ours} () \\
\hline
\multicolumn{12}{l}{\textbf{10 subjects}} \\
\hline
Mean   &19.76 &30.95 & 8.80 &17.06& 14.49& 12.08   & 6.34 & 5.64 &5.46& 2.23& 1.59& \textbf{1.18} & \textbf{1.18} \\ Median &18.91 &29.38 & 9.06& 17.75& 15.78& 8.28 & 3.75 &5.47& 6.09& 2.03& 1.25& \textbf{1.09} & \textbf{1.09} \\\hline
\multicolumn{12}{l}{\textbf{15 subjects}} \\
\hline
Mean   &25.82  &31.47 & 12.89  & 18.65 &16.22 &14.05& 11.01    & 7.63 &6.70 &2.17 &1.69& \underline{1.14}& \textbf{1.12} \\Median &26.30  &31.64 & 13.23  &17.76  &17.34 &14.69& 10.89    & 6.41 &5.52 &2.03 &1.72& \textbf{1.14}& \textbf{1.14} \\\hline
\multicolumn{12}{l}{\textbf{20 subjects}}\\
\hline
Mean  &31.45 &28.76 &20.11& 18.23& 16.55& 15.16   & 14.07         &9.30 &7.67& 2.17 &1.73& \underline{1.31}&\textbf{1.30} \\Median&32.11 &28.91 &21.41& 16.80& 17.34 &15.23 & 13.98         &10.31& 6.56 &2.11 &1.80&\underline{1.32}&\textbf{1.25} \\\hline
\multicolumn{12}{l}{\textbf{25 subjects}} \\
\hline
Mean  &28.14& 27.81& 26.30 &18.72& 18.56 &18.89   &16.79      &10.67 &10.27 &2.53 &1.75& \underline{1.32}&\textbf{1.29} \\ Median&28.22& 26.81& 26.56 & 17.88& 18.03 &18.53 &17.13      &10.84& 10.22 &2.19 &1.81& \underline{1.34}&\textbf{1.28} \\\hline
\multicolumn{12}{l}{\textbf{30 subjects}} \\
\hline
Mean  &38.59 &30.64& 27.52 &19.99& 20.49& 20.75   & 20.46     &11.24& 11.56 &2.63 &2.07& \underline{1.71}&\textbf{1.67} \\ Median&36.98 &30.31& 27.97& 20.00 &20.94 &20.52 & 21.15     &11.09 &10.36 &2.81 &2.19& \underline{1.77} &\textbf{1.72} \\ \hline
\multicolumn{12}{l}{\textbf{35 subjects}}\\
\hline
Mean   &40.61&31.35& 29.19& 22.13 &26.07& 20.29   &20.38      &13.10& 13.28  & 3.09& 2.65&\underline{1.67}&\textbf{1.62} \\ Median &40.71&31.74& 29.51& 21.74& 25.92& 20.18&20.47       &13.10& 13.21& 3.10& 2.64& \underline{1.69}&\textbf{1.60} \\ \hline
\multicolumn{12}{l}{\textbf{38 subjects}} \\
\hline
Mean   &35.12 &29.89 & 29.36& 25.33& 27.75   & 23.52  &19.45      &11.64 &12.66 &3.33& 2.67 & \underline{1.56} & \textbf{1.52} \\ Median &35.12 &29.89 & 29.36& 25.33& 27.75& 23.52 &19.45      & 11.64 &12.66& 3.33& 2.67 & \underline{1.56} &\textbf{1.52} \\ \hline
\end{tabular}
\caption{Clustering Error (\%) on Extended Yale B. The best results are in bold and the second best results are underlined.} \label{table:extendedYaleB-DS3C}
\end{table*}


\section{Experimental Evaluations}
\label{sec:experiments}

To evaluate the performance of our proposed SConvSCN, we conduct experiments on four benchmark data sets: two face image data sets, the Extended Yale B \cite{Georghiades:TPAMI01} and ORL \cite{Sam:94}, and two object image data sets, COIL20 and COIL100~\cite{Nene:COIL}.
We compare our proposed SConvSCN with the following baseline algorithms, including Low Rank Representation (LRR)~\cite{Liu:ICML10}, Low Rank Subspace Clustering (LRSC) \cite{ReVidal:PRL14},
Sparse Subspace Clustering (SSC)~\cite{Elham:TPAMI13}, Kernel Sparse Subspace Clustering (KSSC) \cite{Patel:ICIP14}, SSC by Orthogonal Matching Pursuit (SSC-OMP)~\cite{You:CVPR16}, Efficient Dense Subspace Clustering (EDSC) \cite{Ji:WACV14},
Structured SSC (SC) \cite{Li:TIP17}, SSC with the pre-trained convolutional auto-encoder features (AE+SSC), EDSC with the pre-trained convolutional auto-encoder features (AE+EDSC), Deep Subspace Clustering Networks (DSCNet) \cite{Ji:NIPS17} and Deep Adversarial Subspace Clustering (DASC)~\cite{Zhou:CVPR18}. For EDSC, AE+EDSC, DSCNet and DASC, we directly cite the best results reported in \cite{Ji:NIPS17} and \cite{Zhou:CVPR18}. For SC, we use {\em soft} SC with a fixed parameter .

The architecture specification of SConvSCN used in our experiments for each dataset are listed in Table \ref{tab:Network-settings-ExYaleB-ORL-COIL20} and Table~\ref{table:setting for COIL}. In the stacked convolutional layers, we set the kernel stride as 2 in both horizontal and vertical directions, and use Rectified Linear Unit (ReLU) \cite{Krizhevsky:NIPS2012} as the activation function . In addition, the learning rate is set to  in all our experiments. The whole data set is used as one batch input. For the FC layers, we set  and .

To find informative affinity matrix, we adopt the vector  norm and the vector  norm to define  and denote as SConvSCN- and SConvSCN-, respectively. In the second training stage, we update the stacked convolutional layers, the self-expression model, and the FC layers for  epochs and then update the spectral clustering module once, where  is set to  in our experiments. 


\subsection{Experiments on Extended Yale B}


The Extended Yale B database \cite{Georghiades:TPAMI01} consists of face images of 38 subjects, 2432 images in total, with approximately 64 frontal face images per subject taken under different illumination conditions, where the face images of each subject correspond to a low-dimensional subspace. In our experiments, we follow the protocol used in \cite{Ji:NIPS17}: a) each image is down-sampled from  to  pixels; b) experiments are conducted using all choices of . 


To make a fair comparison, we use the same setting as that used in DSCNet \cite{Ji:NIPS17}, in which a three-layer stacked convolutional encoders is used with  channels, respectively. The detailed settings for the stacked convolutional network used on Extended Yale B are shown Table \ref{tab:Network-settings-ExYaleB-ORL-COIL20}.
The common parameters  and  are set the same as that in DSCNet, where  (for the term ) and . For the specific parameters used in SConvSCN, we set  for the term  and  for the cross-entropy term, respectively.  We set  and . 


The experimental results are presented in Table~\ref{table:extendedYaleB-DS3C}. We observe that our proposed SConvSCN- and SConvSCN- remarkably reduced the clustering errors and yield the lowest clustering errors with  than all the listed baseline methods. We note that DASC \cite{Zhou:CVPR18} reported a clustering error of  on Extended Yale B with , which is slightly better than our results.


To gain further understanding of the proposed dual self-supervision, we use SConvSCN- as an example and evaluate the effect of using the dual self-supervision modules via an ablation study. Due to space limitation, we only list the experimental results of using a single self-supervision via , using a single self-supervision via , and using dual self-supervision of  plus  on datasets Extended Yale B in Table \ref{table:ablation-study-extendedYaleB-DS3C}. As a baseline, we show the experimental results of DSCNet \cite{Ji:NIPS17}, which uses the loss . As could be read from Table \ref{table:ablation-study-extendedYaleB-DS3C} that, using only a single self-supervision module, \ie,  plus , or  plus , the clustering errors are reduced. Compared to using the self-supervision via a spectral clustering loss  in the self-expression module, using the self-supervision via the classification loss  in FC block is more effective. Nonetheless, using the dual supervision modules further reduces the clustering errors.


\begin{table*}[t]
\centering
\resizebox{2.08\columnwidth}{!}
{
\begin{tabular}{c|cc|cc|cc|cc|cc|cc|cc} \hline
\multirow{2}{*}{\diagbox{\textbf{Losses}}{\textbf{No. Subjects}}} & \multicolumn{2}{c|}{\textbf{10 subjects}} & \multicolumn{2}{c|}{\textbf{15 subjects}} & \multicolumn{2}{c|}{\textbf{20 subjects}} & \multicolumn{2}{c|}{\textbf{25 subjects}} & \multicolumn{2}{c|}{\textbf{30 subjects}} & \multicolumn{2}{c|}{\textbf{35 subjects}} & \multicolumn{2}{c}{\textbf{38 subjects}} \\
\cline{2-15}
 & Mean & Median & Mean & Median & Mean & Median & Mean & Median & Mean & Median & Mean & Median & Mean & Median \\
\hline
(DSC- \cite{Ji:NIPS17}) & 2.23 & 2.03 & 2.17 & 2.03 & 2.17 & 2.11 & 2.53 & 2.19 & 2.63 & 2.81 & 3.09 & 3.10 & 3.33 & 3.33 \\
\hline
 & 1.58 & 1.25 & 1.63 & 1.55 & 1.67 & 1.57 & 1.61 & 1.63 & 2.74 & 1.82 & 2.64 & 2.65 & 2.75 & 2.75 \\
 & 1.32 & \textbf{1.09} & 1.31 & 1.30 & 1.54 & 1.48 & 1.48 & 1.98 & 1.87 & \textbf{1.61} & 1.82 & 1.84 & 1.92 & 1.92\\
\hline
 & \textbf{1.18} & \textbf{1.09} & \textbf{1.12} & \textbf{1.14} & \textbf{1.30} & \textbf{1.25} & \textbf{1.29} & \textbf{1.28} & \textbf{1.67} & {1.72} & \textbf{1.62} & \textbf{1.60} & \textbf{1.52} & \textbf{1.52} \\
\hline

\end{tabular}
}
\caption{Ablation Study on SConvSCN- on Extended Yale B.}
\label{table:ablation-study-extendedYaleB-DS3C}
\end{table*}


\subsection{Experiments on ORL}


The ORL dataset \cite{Sam:94} consists of face images of 40 distinct subjects, each subjects having 10 face images under varying lighting conditions, with different facial expressions (open/closed eyes, smiling/not smiling) and facial details (glasses / no glasses) \cite{Sam:94}. As the images were took under variations of facial expressions, this data set is more challenging for subspace clustering due to the nonlinearity and small sample size per subject. 


In our experiments, each image is down-sampled from  to . We reduce the kernel size in convolution module to  due to small image size and set the number of channels to . The specification of the network structure is shown in Table \ref{tab:Network-settings-ExYaleB-ORL-COIL20}. For the tradeoff parameters, we set , and  for our SConvSCN. For the fine-tuning stage, we set  and .
Experimental results are shown in Table~\ref{Table:acc-for-orl-and-coil}. Again, our proposed approaches yield the best results.


\subsection{Experiments on COIL20 and COIL100}

To further verify the effectiveness of our proposed SConvSCN, we conduct experiments on dataset COIL20 and COIL100~\cite{Nene:COIL}. COIL20 contains 1440 gray-scale images of 20 objects; whereas COIL100 contains 7200 images of 100 objects. Each image was down-sampled to . The settings of the stacked convolutional networks used for COIL20 and COIL100 are listed in Table~\ref{table:setting for COIL}.

For the tradeoff parameters on COIL20, we set ,  as same as used in DSC-Net~\cite{Ji:NIPS17}, and , , , and  in our SConvSCN.
For the tradeoff parameters on COIL100, we set ,  as same as used in DSC-Net~\cite{Ji:NIPS17},
and , , , and  in our SConvSCN.



For experiments on COIL20 and COIL100, we initialize the convolutional module with stacked CAE at first, and then train a stacked CAE assisted with a self-expressive model. This is effectively DSCNet \cite{Ji:NIPS17}. And then, we train the whole SConvSCN. Experimental results are listed in Table \ref{Table:acc-for-orl-and-coil}. As could be read, our SConvSCN- and SConvSCN- reduce the clustering errors significantly. This result confirms the effectiveness of the designed dual self-supervision components for the proper use of the useful information from the output of spectral clustering.


\begin{figure}
\vspace{-10pt}
\small
\centering
\subfigure[ and ]{\includegraphics[clip=true,trim=0 5 5 2, width=0.156\textwidth]{TotalL_YaleB.eps}}
\subfigure[ and ]{\includegraphics[clip=true,trim=0 5 5 2, width=0.156\textwidth]{C_Q_YaleB.eps}}
\subfigure[]{\includegraphics[clip=true,trim=0 5 5 2, width=0.156\textwidth]{CrossEL_YaleB.eps}}\\
\subfigure[]{\includegraphics[clip=true,trim=0 5 5 2, width=0.156\textwidth]{CenterL_YaleB.eps}}
\subfigure[]{\includegraphics[clip=true,trim=0 5 5 2, width=0.156\textwidth]{acc_YaleB.eps}} \subfigure[]{\includegraphics[clip=true,trim=0 5 5 2, width=0.156\textwidth]{rho_YaleB.eps}}
\caption{The cost functions and clustering error of SConvSCN- during training period on Extended Yale B (). }
\label{fig:Acc-Costs-vs-epoch-ExYaleB}
\end{figure}

\subsection{Convergence Behaviors}

To show the convergence behavior during training iterations, we conduct experiments on Extended Yale B with . We record the clustering errors and each cost function during training period, and show them as a function of the number of epoches in Fig.~\ref{fig:Acc-Costs-vs-epoch-ExYaleB}. As could be observed from Fig.~\ref{fig:Acc-Costs-vs-epoch-ExYaleB}(a), (c), (d) and (e), the cost functions , , ,
and , and the cluster error decrease rapidly and tend to ``flat''. To show more details in the iterations, in Fig.~\ref{fig:Acc-Costs-vs-epoch-ExYaleB} (b) and (f), we show the curves of ,  and . Note that  and  are the cost and the relative cost of spectral clustering, respectively. Compared to , we argue that  is more indicative to the clustering performance. As could be observed, while  and  are increasing\footnote{The observation that the curves of  and  go up is because the entries of the extracted feature  are slowly shrinking and thus the absolute values of entries of  are slowly increasing, due to the absence of normalization step in feature learning at each epoch.}, the curve of  tends to ``flat''---which is largely consistent to the curve of the clustering error in Fig.~\ref{fig:Acc-Costs-vs-epoch-ExYaleB} (e).


\begin{table}[t]
\centering
\begin{tabular}{c|cc|cc}
\hline
    & \multicolumn{2}{c|}{COIL20} & \multicolumn{2}{c}{COIL100}\\
\hline
    Layers & kernel size & channels & kernel size & channels\\
    \hline encoder-1 &  & 15 &  & 50\\
    decoder-1 &  & 15 &  & 50\\
\hline \end{tabular}
\caption{Network settings for COIL20 and COIL100.} \label{table:setting for COIL}
\end{table}


\begin{table}[htbp]
\centering
\scriptsize
\resizebox{0.450\textwidth}{!}{
\begin{tabular}{c|ccc}
    \hline
    Methods & \text{ORL} & \text{COIL20} & \text{COIL100}\\
    \hline
LRR & 33.50 & 30.21 & 53.18 \\ LRSC & 32.50 & 31.25 & 50.67 \\
SSC & 29.50 & 14.83 & 44.90 \\ AE+SSC & 26.75 & 22.08 & 43.93 \\
    KSSC & 34.25 & 24.65 & 47.18 \\
SSC-OMP & 37.05 & 29.86 & 67.29 \\ EDSC & 27.25 & 14.86 & 38.13 \\
    AE+EDSC & 26.25 & 14.79 & 38.88 \\
soft SC & 26.00 & 11.87 & 41.71 \\ \hline
    DSC- & 14.25 & 5.65 & 33.62 \\
    DSC- & 14.00 & 5.42 & 30.96 \\
    DASC~\cite{Zhou:CVPR18} & 11.75 & 3.61 & - \\
    \hline
    SConvSCN- & \underline{11.25} & \underline{2.33} & \underline{27.83}\\
    SConvSCN- & \textbf{10.50} & \textbf{2.14} & \textbf{26.67} \\
    \hline
\end{tabular}
}
\caption{Clustering Error (\%) on ORL, COIL20 and COIL100.} \label{Table:acc-for-orl-and-coil}
\end{table}



\section{Conclusion}
\label{sec:conclusion}
We have proposed an end-to-end trainable framework for simultaneous feature learning and subspace clustering, called Self-Supervised Convolutional Subspace Clustering Network (SConvSCN). Specifically, in SConvSCN, the feature extraction via stacked convolutional module, the affinity learning via self-expression model, and the data segmentation via spectral clustering are integrated into a joint optimization framework. By exploiting a dual self-supervision mechanism, the output of spectral clustering are effectively used to improve the training of the stacked convolutional module and to refine the self-expression model, leading to superior performance. Experiments on benchmark datasets have validated the effectiveness of our proposed approach.


\section*{Acknowledgment} J. Zhang and C.-G. Li are supported by the National Natural Science Foundation of China (NSFC) under Grant No. 61876022, and the Open Project Fund from Key Laboratory of Machine Perception (MOE), Peking University. H. Zhang is partially supported by NSFC under Grant Nos. 61701032 and 61806184. X. Qi is supported by Shenzhen Fundamental Research Fund under Grants Nos. ZDSYS201707251409055 and 2017ZT07X152. Z. Lin is supported by 973 Program of China under Grant No. 2015CB352502, NSFC under Grant Nos. 61625301 and 61731018, Qualcomm, and Microsoft Research Asia.






\small
\bibliographystyle{ieee} \bibliography{zhjj,zhjj_v1,cgli,temp,sparse,learning,vidal,recognition} 

\end{document} 