

\documentclass{article}

\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure} 

\usepackage{natbib}
\setlength{\bibsep}{4.3pt}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{color}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{xcolor}





\usepackage[accepted]{icml2014}

\newcommand\x{\mathbf{x}}
\newcommand\w{\mathbf{w}}
\newcommand\uu{\mathbf{u}}
\newcommand\z{\mathbf{z}}
\newcommand\A{\mathbf{A}}
\newcommand\Z{\mathcal{Z}}
\newcommand\Real{\mathbb{R}}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\Bs}{\mathcal{B}_I} \newcommand{\Gs}{\mathcal{G}} \newcommand{\Es}{\mathcal{E}}
\newcommand{\Vs}{\mathcal{V}}
\newcommand{\Us}{\mathcal{U}}
\newcommand{\Ps}{\mathcal{P}}
\newcommand{\inter}{\cap}
\newcommand{\union}{\cup}

\newcommand\todo[1]{\textcolor{red}{#1}}

\newtheorem{lemma}{Lemma}

\newcommand{\mynote}[2]{{\bf \color{red}{#1}:~{#2}}}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{amsthm}
\usepackage{color}

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}


\icmltitlerunning{On learning to localize objects with minimal supervision}

\begin{document} 

\twocolumn[
\icmltitle{On learning to localize objects with minimal supervision}

\icmlauthor{Hyun Oh Song}{song@eecs.berkeley.edu}
\icmlauthor{Ross Girshick}{rbg@eecs.berkeley.edu}
\icmlauthor{Stefanie Jegelka}{stefje@eecs.berkeley.edu}
\icmlauthor{Julien Mairal}{julien.mairal@inria.fr}
\icmlauthor{Zaid Harchaoui}{zaid.harchaoui@inria.fr}
\icmlauthor{Trevor Darrell}{trevor@eecs.berkeley.edu}

\icmlkeywords{smooth optimization, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation.
The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50\% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection. 
\end{abstract} 

\section{Introduction}

The classical paradigm for learning object detection models starts by annotating each object instance, in all training images, with a bounding box. However, this exhaustive labeling approach is costly and error prone for large-scale datasets.
The massive amount of textually annotated visual data available online inspires a different, more challenging, research problem. Can weakly-labeled imagery, without bounding boxes, be used to reliably train object detectors? 

In this alternative paradigm, the goal is to learn to localize objects with minimal supervision \cite{perona1, perona2}. We focus on the case where the learner has access to binary image labels that encode whether an image contains the target object or not, without access to any instance level annotations (i.e., bounding boxes). 


Our approach starts by reducing the set of possible image locations that contain the object of interest from millions to thousands per image, using the selective search window proposal technique introduced by \citet{selectivesearch}. Then, we formulate a  discriminative submodular cover algorithm to discover an initial set of image windows that are likely to contain the target object. After training a detection model with this initial set, we refine the detector using a novel smoothed formulation of latent SVM \cite{misvm-nips,lsvm-pami}. We employ recently introduced object detection features, based on deep convolutional neural networks \cite{decafICML,girshick2014rcnn}, to represent the window proposals for clustering and detector training.

Compared to prior work on weakly-supervised detector training, we show substantial improvements on the standard evaluation metric (detection average precision on PASCAL VOC). Quantitatively, our approach achieves a 50\% relative improvement in mean average precision over the current state-of-the-art for weakly-supervised learning.

\section{Related work}

Our work is related to three active research areas: (1) weakly-supervised learning, (2) unsupervised discovery of mid-level visual elements, and (3) co-segmentation.

We build on a number of previous approaches for training object detectors from weakly-labeled data.
In nearly all cases, the task is formulated as a multiple instance learning (MIL) problem \cite{mil3}.
In this formulation, the learner has access to an image-level label indicating the presence or absence of the target class, but not its location (if it is present).
The challenge faced by the learner is to find the sliver of signal present in the positive images, but absent from the negative images.
The implicit assumption is that this signal will correspond to the positive class.

Although there have been recent works on convex relaxations \cite{li13, bach12}, most MIL algorithms start from an initialization and then perform some form of local optimization.
Early efforts, such as \cite{perona1,perona2,galleguillos2008weakly,fergus2007weakly,crandall2006weakly,chum2007exemplar, neil13}, focused on datasets with strong object-in-the-center biases (e.g. Caltech-101).
This simplified setting enabled clarity and focus on the MIL formulation, image features, and classifier design, but masked the vexing problem of finding a good initialization in data where such helpful biases are absent.

More recent work, such as \cite{siva1,siva2012defence}, attempts to learn detectors, or simply automatically generate bounding box annotations from much more challenging datasets such as PASCAL VOC \cite{PASCAL-ijcv}.
In this data regime, focusing on initialization is crucial and carefully designed heuristics, such as shrinking bounding boxes \cite{russakovsky}, are often employed.

Recent literature on unsupervised mid-level visual element discovery \cite{discovery1, discovery2, discovery4, discovery5, discovery6} uses weak labels to discover visual elements that occur commonly in positive images but not in negative images. Discovered visual element representation were shown to successfully provide discriminative information in classifying images into scene types. The most recent work \cite{discovery3} presents a discriminative mode seeking formulation and draws connections between discovery and mean-shift algorithms \cite{meanshift1}.





The problem of finding common structure is related to the challenging setting of co-segmentation \cite{rother06, joulin10, alexe10}, which is the unsupervised segmentation of an object that is present in multiple images. While in this paper we do not address pixel-level segmentation, we employ ideas from co-segmentation: the intuition behind our submodular cover framework in Section~\ref{sec:covering} is shared with CoSand \cite{kim11}. Finally, submodular covering ideas have recently been applied to (active) filtering of hypothesis after running a detector, and without the discriminative flavor we propose \cite{barinova12,chen14}.

















\section{Problem formulation}
Our goal is to learn a detector for a visual category from a set of images, each with a binary label.
We model an image as a set of overlapping rectangular windows and follow a standard approach to detection: reduce the problem of detection to the problem of binary classification of image windows.
However, at training time we are only given image-level labels, which leads to a classic multiple instance learning (MIL) problem.
We can think of each image as a ``bag'' of instances (rectangular windows) and
the binary image label  specifies that the bag contains at least one instance of the target category.
The label  specifies that the image contains no instances of the category.
During training, no instance labels are available.

MIL problems are typically solved (locally) by finding a local minimum of a non-convex objective function, such as MI-SVM \cite{misvm-nips}.
In practice, the quality of the local solution depends heavily on the quality of the initialization.
We therefore focus extensively on finding a good initialization.
In Section \ref{sec:covering}, we develop an initialization method by formulating a discriminative set multicover problem that can be solved approximately with a greedy algorithm.
This initialization, without further MIL refinement, already produces good object detectors, validating our approach.
However, we can further improve these detectors by optimizing the MIL objective.
We explore two alternative MIL objectives in Section \ref{sec:slsvm}.
The first is the standard Latent SVM (equivalently MI-SVM) objective function, which can be optimized by coordinate descent on an auxiliary objective that upper-bounds the LSVM objective.
The second method is a novel technique that smoothes the Latent SVM objective and can be solved more directly with unconstrained smooth optimization techniques, such as L-BFGS \cite{lbfgs}.
Our experimental results show modest improvements from our smoothed LSVM formulation on a variety of MIL datasets.

\section{Finding objects via submodular cover}
\label{sec:covering}

Learning with LSVM is a chicken and egg problem:
The model weights are needed to infer latent annotations, but the latent annotations are needed to estimate the model weights.
To initialize this process, we approximately identifying jointly present objects in a weakly supervised manner. The experiments show a significant effect from this initialization.
Our procedure implements two essential assumptions: (i) the correct boxes are similar, in an appropriate feature space, across positive images (or there are few modes), and (ii) the correct boxes do not occur in the negative images. In short, in the similarity graph of all boxes we seek dense subgraphs that only span the positive images. Finding such subgraphs is a nontrivial combinatorial optimization problem.

The problem of finding and encoding a jointly present signal in images is an old one, and has been addressed by clustering, minimum description length priors, and the concept of exemplar \cite{darrell90,leibe04,schiele06,kim11}.
These approaches share the idea that a small number of exemplars or clusters should well encode the shared information we are interested in. We formalize this intuition as a flexible \emph{submodular cover} problem. However, we also have label information at hand that can help identify correct boxes. We therefore integrate into our covering framework the relevance for positively versus negatively labeled images, generalizing ideas from \cite{discovery1}. This combination allows us to find multiple modes of the object appearance distribution.

Let  be the set of all positive images. Each image contains a set  of candidate bounding boxes generated from selective search region proposals \cite{selectivesearch}. In practice, there are about  region proposal boxes per image and about  training images in the PASCAL VOC dataset. Ultimately, we will define a function  on sets  of boxes that measures how well the set  represents .
For each box , we find its nearest neighbor box in each (positive \emph{and negative}) image. We sort the set  of all such neighbors of  in increasing order by their distance to . This can be done in parallel.
We will define a graph using these nearest neighbors that allows us to optimize for a small set of boxes  that are (i) \emph{relevant} (occur in many positive images); (ii) \emph{discriminative} (dissimilar to the boxes in the negative images); and (iii) \emph{complementary} (capture multiple modes).



We construct a bipartite graph  whose nodes  and  are all boxes occurring in  (each  occurs once in  and once in ). The nodes in  are partitioned into groups :  contains all boxes from image . The edges  are formed by connecting each node (box)  to its top  neighbors in  from positive images. 
Figure~\ref{fig:graph} illustrates the graph.
Connecting only to the top  neighbors (instead of all) implements discriminativeness: the neighbors must compete. If  occurs in positively and negatively labeled images equally, then many top- closest neighbors in  stem from negative images. Consequently,  will not be connected to many nodes (boxes from ) in . 
We denote the neighborhood of a set of nodes  by
.


\begin{figure}
  \centering
  \includegraphics[width=0.48\textwidth]{figures/graph} \vspace{-0.8cm}
  \caption{Illustration of the graph  with  (top row) and  (bottom row). Each box  is connected to its closest neighbors from positive images (one from each image). Non-discriminative boxes occur in all images equally, and may not even have any boxes from positive images among their closest neighbors -- and consequently no connections to . Picking the green-framed box  in  ``covers'' its (green) highlighted neighbors .}
  \label{fig:graph}
\end{figure}

Let  denote a set of selected boxes. We define a \emph{covering score}  for each  that is determined by a \emph{covering threshold}  and a scalar, nondecreasing concave function :

This score measures how many boxes in  are neighbors of  and thus ``covered''. We gain from covering up to  boxes from  -- anything beyond that is considered redundant.
The \emph{total covering score} of a set  is then 

The threshold  balances relevance and complementarity: let, for simplicity, . If , then a set that maximizes  contains boxes from many different images, and few from a single image. The selected neighborhoods are very complementary, but some of them may not be very relevant and cover outliers. If  is large, then any additionally covered box yields a gain, and the best boxes  are those with the largest degree. A box has large degree if many of its closest neighbors in  are from positive images. This also means  is discriminative and relevant for .
\begin{lemma}
  The function  defined in Equation~\eqref{eq:totalcover} is nondecreasing and submodular.
\end{lemma}
A set function is \emph{submodular} if it satisfies \emph{diminishing marginal returns}: for all  and , it holds that .
\vspace{-5pt}
\begin{proof}
  First, the function  is a covering function and thus submodular: let . Then  and therefore
  
  The same holds when intersecting with .
Thus,  is a nondecreasing concave function of a submodular function and therefore submodular. Finally,  is a sum of submodular functions and hence also submodular. Monotonicity is obvious.
\end{proof}

We aim to select a representative subset  with minimum cardinality:

for . We optimize this via a greedy algorithm: let  and, in each step , add the node  that maximizes the marginal gain .
\begin{lemma}
  \label{lem:bound}
  The greedy algorithm solves Problem~\eqref{eq:submodcover} within an approximation factor of 
  .
\end{lemma}
Lemma~\ref{lem:bound} says that the algorithm returns a set  with  and , where  is an optimal solution.
This result follows from the analysis by \citet{wolsey82} (Thm.~1) adapted to our setting.
To get a better intuition for the formulation~\eqref{eq:submodcover} we list some special cases:\\
\textbf{Min-cost cover.} With  and  being the identity, Problem~\ref{eq:submodcover} becomes a min-cost cover problem. Such straightforward covering formulations have been used for filtering after running a detector \cite{barinova12}.\\
\textbf{Maximum relevance.} A minimum-cost cover merely focuses on complementarity of the selected nodes , which may include rare outliers. At the other extreme ( large), we would merely select by the number of neighbors (\citet{discovery1} choose one single  that way).\\
\textbf{Multi-cover.} To smoothly move between the two extremes, one may choose  and  to be sub-linear. This trades off representation, relevance, and discriminativeness.



In Figure \ref{fig:cluster_visualization_figure}, we visualize top  nearest neighbors with positive labels in the first chosen cluster  for all  classes on the PASCAL VOC data.
Our experiments in Section~\ref{sec:exp} show the benefits of our framework. Potentially, the results might improve even further when using the complementary mode shifts of \citep{discovery3} as a pre-selection step before covering.



\begin{figure*}[htbp]
\centering
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_aeroplane_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_aeroplane_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_aeroplane_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_aeroplane_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_aeroplane_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bicycle_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bicycle_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bicycle_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bicycle_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bicycle_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bird_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bird_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bird_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bird_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bird_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_boat_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_boat_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_boat_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_boat_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_boat_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bottle_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bottle_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bottle_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bottle_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bottle_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bus_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bus_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bus_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bus_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_bus_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_car_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_car_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_car_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_car_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_car_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cat_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cat_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cat_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cat_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cat_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_chair_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_chair_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_chair_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_chair_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_chair_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cow_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cow_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cow_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cow_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_cow_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_diningtable_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_diningtable_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_diningtable_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_diningtable_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_diningtable_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_dog_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_dog_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_dog_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_dog_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_dog_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_horse_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_horse_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_horse_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_horse_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_horse_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_motorbike_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_motorbike_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_motorbike_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_motorbike_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_motorbike_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_person_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_person_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_person_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_person_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_person_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_pottedplant_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_pottedplant_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_pottedplant_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_pottedplant_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_pottedplant_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sheep_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sheep_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sheep_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sheep_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sheep_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sofa_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sofa_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sofa_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sofa_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_sofa_top_5.pdf}\vspace{0.2cm}\\
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_train_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_train_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_train_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_train_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_train_top_5.pdf}\hspace{0.2cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_tvmonitor_top_1.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_tvmonitor_top_2.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_tvmonitor_top_3.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_tvmonitor_top_4.pdf}\hspace{-0.12cm}
\includegraphics[width=0.1\textwidth, height=1cm]{figures/cluster_visualization_figure1/class_tvmonitor_top_5.pdf}\\
\caption{Visualizations of top  nearest neighbor proposal boxes with positive labels in the first cluster,  for all  classes in PASCAL VOC dataset. From left to right, aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, plant, sheep, sofa, train, and tvmonitor.} 
\label{fig:cluster_visualization_figure} 
\end{figure*}

\section{Iterative refinement with latent variables}
\label{sec:slsvm}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.35\textwidth]{figures/mil_figure_groundtruth.pdf} \hspace{0.1cm}
\includegraphics[width=0.35\textwidth]{figures/mil_figure_selectivesearch.pdf}
\caption{In the refinement stage, we formulate a multiple instance learning bag per image and bag instances correspond to each window proposals from selective search.  Binary bag labels correspond to image-level annotations of whether the target object exists in the image or not. (Left) ground truth bounding boxes color coded with category labels. green: person, yellow: dog, and magenta: sofa, (Right) visualization of  random subset of window proposals.} 
\label{fig:mil-figure} 
\end{figure*}

In this section, we review the latent SVM formulation, and we propose a simple
smoothing technique enabling us to use classical techniques for unconstrained
smooth optimization. Figure \ref{fig:mil-figure} illustrates our multiple
instance learning analogy for object detection with one-bit labels.

\subsection{Review of latent SVM}

For a binary classification problem, the latent SVM formulation consists of
learning a decision function involving a maximization step over a discrete set of 
configurations~. Given a data point~ in~ that we
want to classify, and some learned model parameters~ in~, 
we select a label~ in~ as follows:

where  is called a ``latent variable'' chosen among the set~. For
object detection,  is typically a set of bounding boxes, and maximizing
over  amounts to finding a bounding box containing the object.  In
deformable part models~\cite{lsvm-pami}, the set  contains all possible
part configurations, each part being associated to a position in the image.
The resulting set~ has exponential size, but~(\ref{eq:decision}) can be
solved efficiently with dynamic programming techniques for particular choices of~.

Learning the model parameters~ is more involved than solving a simple SVM
problem. We are given some training data , where the 
vectors  are  in~ and the scalars  are binary labels in~. Then, the latent SVM formulation becomes
\vspace{-4pt}

where  is the hinge loss defined as
, which encourages the decision
function for each training example to be the same as the corresponding label.
Similarly, other loss functions can be used such as the logistic or
squared hinge loss.

Problem~(\ref{eq:latentsvm}) is nonconvex and nonsmooth, making it hard to
tackle. A classical technique to obtain an approximate solution is to
use a difference of convex (DC) programming technique, called concave-convex
procedure ~\cite{yuille_cccp,Yu09}.  
We remark that the part of~(\ref{eq:latentsvm}) corresponding to negative
examples is convex with respect to~. It is indeed easy to show that each corresponding term can
be written as a pointwise maximum of convex functions, and is thus
convex~\citep[see][]{boyd}: when , .  On the other hand, the part corresponding to positive
examples is concave, making the objective~(\ref{eq:latentsvm}) suitable
to DC programming. Even though such a procedure does not have any
theoretical guarantee about the quality of the optimization, it monotonically
decreases the value of the objective and performs relatively well when the
problem is well initialized~\cite{lsvm-pami}.

We propose a \emph{smooth formulation} of latent SVM, with
 two main motives. First, smoothing the objective function of
latent SVM allows the use of efficient second-order optimization algorithms such as
quasi-Newton \cite{lbfgs} that can leverage curvature information to speed up convergence.
Second, as we show later, smoothing the latent SVM boils down to considering the
top- configurations in the maximization step in place of the top-
configuration in the regular latent SVM. As a result, the smooth latent SVM training
becomes more robust to unreliable configurations in the early stages, since a
larger set of plausible configurations is considered at each maximization step. 

\subsection{Smooth formulation of LSVM}









In the objective~(\ref{eq:latentsvm}), the hinge loss can be easily replaced by
a smooth alternative, e.g., squared hinge, or logistic loss. However, the
non-smooth points induced by the following functions are more difficult to handle

We propose to use a smoothing technique studied by~~\citet{nesterov} for convex functions.
\paragraph{Nesterov's smoothing technique}
We only recall here the
simpler form of Nesterov's results that is relevant for our purpose. 
Consider a non-smooth function that can be written in the following form:

where ,   is in~, and  denotes the probability simplex, . Smoothing here consists of 
adding a strongly convex function  in the maximization problem

The resulting function  is differentiable for all , and its gradient is

where  is the unique solution of~(\ref{eq:smoothedf}).
The parameter  controls the amount of smoothing. Clearly,  for all 
as . As~\citet{nesterov} shows, for a given target approximation accuracy~, 
there is an optimal amount of smoothing  that can be derived from a convex optimization perspective using the strong convexity parameter of  on  and the (usually unknown) Lipschitz constant of .
In the experiments, we shall simply learn
the parameter  from data. 



\paragraph{Smoothing the latent SVM}
We now apply Nesterov's smoothing technique to the latent SVM objective
function. As we shall see, the smoothed objective takes a simple form, which can
be efficiently computed in the latent SVM framework.  Furthermore, smoothing
latent SVM implicitly models uncertainty in the selection of the best
configuration~ in~, as shown by \citet{KumarPK12} for 
a different smoothing scheme. 

In order to smooth the functions  defined in~(\ref{eq:fmax}), 
we first notice that

where  is a matrix of size  such that the -th row of~ is
the feature vector  and  is the -th element of .
Considering any strongly convex function  and parameter , 
the smoothed latent SVM objective is obtained by replacing in~(\ref{eq:latentsvm}) \\
~ the functions  by their smoothed counterparts  obtained by applying~(\ref{eq:smoothedf}) to~(\ref{eq:simplex}); \\
~ the non-smooth hinge-loss function  by any smooth loss. 












\paragraph{Objective and gradient evaluations}
An important issue remains the computational tractability of the new
formulation in terms of objective and gradient evaluations, in order to
use quasi-Newton optimization techniques. The choice of the strongly convex
function  is crucial in this respect. 

There are two functions known to be strongly convex on the simplex: i) the
Euclidean norm, ii) the entropy.  In the case of the Euclidean-norm
, it turns out that the
smoothed counterpart can be efficiently computed using a projection on
the simplex, as shown below.

where  is the solution of~(\ref{eq:smoothedf}).  Computing
 requires a priori  operations.  The projection can be computed
in ~\citep[see, e.g.,][]{MAL-015}.  Once~ is obtained,
computing the gradient requires  operations, where
 is the number of non-zero entries in .

When the set  is large, these complexities can be improved by leveraging
two properties. First, the projection on the simplex is known to produce sparse
solutions, the smoothing parameter  controlling the sparsity of
; second, the projection preserves the order of the variables. As a
result, the following heuristic can be justified. Assume that for some , we can obtain the top-N entries of  without exhaustively
exploring~. Then, performing the projection on these reduced set of 
variables yields a vector  which can be shown to be optimal for the
original problem~(\ref{eq:proj}) whenever .  In other words,
whenever  is large enough and  small enough, computing the gradient of
 can be done in  operations.  We use this heuristic in
all our experiments.














\begin{figure*}[htbp]
\centering
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_bicycle_top_16.pdf} \hspace{0.04cm}
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_bus_top_63.pdf}  \hspace{0.04cm} 
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_motorbike_top_7.pdf}  \hspace{0.04cm}
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_horse_top_12.pdf}  \hspace{0.04cm}
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_motorbike_top_11.pdf} 

\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_horse_top_14.pdf} \hspace{0.04cm}
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_aeroplane_top_42.pdf} \hspace{0.04cm}
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_bicycle_top_93.pdf} \hspace{0.04cm}
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_bus_top_43.pdf} \hspace{0.04cm}
\includegraphics[width=0.19\textwidth, height=2.2cm]{figures/us_vs_siva_figure2/class_bicycle_top_127.pdf}
\caption{Visualization of some common failure cases of constructed positive windows by\cite{siva2012defence} vs our method. Red bounding boxes are constructed positive windows from \cite{siva2012defence}. Green bounding boxes are constructed positive windows from our method.} 
\label{fig:us_vs_siva} 
\end{figure*}

\begin{table*}[htbp]
\footnotesize
\centering
\renewcommand{\arraystretch}{1.1}
\renewcommand{\tabcolsep}{1.5mm}
\begin{tabular}{*{5}{l}}
\toprule
Dataset& LSVM w/o bias & SLSVM w/o bias &LSVM w/ bias & SLSVM w/ bias\\
\midrule
musk1    &  70.8  14.4 &  80.3  10.3  &  81.7  14.5  &  79.2  13.4 \\
musk2    &  51.0  10.9 &  79.5  10.4  &  80.5  9.9    &  84.3  11.4 \\
\midrule
fox           &  51.5  7.5   &  63.0  11.8  &  57.0  8.9   &  61.0  12.6 \\
elephant & 81.5  6.3   &  88.0  6.7    &  81.5  4.1    &  87.0  6.3 \\
tiger        &  79.5  8.6   &  85.5  6.4    &  86.0  9.1    &  87.5  7.9 \\
\midrule
trec1       &  94.3  2.9    &  95.5  2.6   &  95.3  3.0   &  95.3  2.8 \\
trec2       &  69.0  6.8    &  83.0  6.5   &  86.5  5.7   &  83.8  7.4 \\
trec3       &  77.5  5.8    &  90.0  5.8   &  85.5  6.3   &  86.0  6.5 \\
trec4       &  77.3  8.0    &  85.0  5.1   &  85.3  3.6   &  86.3  5.2  \\
trec7       &  74.5  9.8    &  83.8  4.0   &  82.5  7.0   & 81.5  5.8  \\
trec9       &  66.8  5.0    &  70.3  5.7   &  68.8  8.0   & 71.5  6.4 \\
trec10     &  71.0  9.9    &  84.3  5.4   &  80.8  6.6   & 82.8  7.3  \\
\bottomrule
\end{tabular}
\caption{10 fold average and standard deviation of the test accuracy on MIL dataset. The two methods start from the same initialization introduced in \cite{misvm-nips}}
\label{tab:mil-experiments}
\end{table*}

\begin{table*}[htbp]
\footnotesize
\centering
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{1.5mm}
\begin{tabular}{l*{14}{r}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{aeroplane} & \multicolumn{2}{c}{bicycle} & \multicolumn{2}{c}{boat} & \multicolumn{2}{c}{bus}& \multicolumn{2}{c}{horse} & \multicolumn{2}{c}{motorbike} & \multirow{2}{*}{~mAP}\\
  & left & right & left & right & left & right & left & right & left & right & left & right\\
\midrule
\cite{deselaers1}  & 9.1 & 23.6 & 33.4 & 49.4 & 0.0 & 0.0 & 0.0 & 16.4 & 9.6 & 9.1 & 20.9 & 16.1 & ~16.0\\
\midrule
\cite{pandey}  & 7.5 & 21.1 & 38.5 & 44.8 & 0.3 & 0.5 & 0.0 & 0.3 & 45.9 & 17.3 & 43.8 & 27.2 & ~20.8\\
\midrule
\cite{deselaers2}  & 5.3 & 18.1 & 48.6 & 61.6 & 0.0 & 0.0 & 0.0 & 16.4 & 29.1 & 14.1 & 47.7 & 16.2 & ~21.4\\
\midrule
\cite{russakovsky} & \multicolumn{2}{c}{30.8} & \multicolumn{2}{c}{25.0}  & \multicolumn{2}{c}{3.6} & \multicolumn{2}{c}{26.0}  & \multicolumn{2}{c}{21.3}  & \multicolumn{2}{c}{29.9} &~ 22.8\\
\midrule
\cite{siva2012defence} with our features &  \multicolumn{2}{c}{23.2} &  \multicolumn{2}{c}{15.4} &  \multicolumn{2}{c}{5.1} &  \multicolumn{2}{c}{2.0} &  \multicolumn{2}{c}{6.2} &  \multicolumn{2}{c}{17.4} & ~11.6\\
\midrule
Cover + SVM & \multicolumn{2}{c}{23.4} & \multicolumn{2}{c}{43.5}  & \multicolumn{2}{c}{8.1} & \multicolumn{2}{c}{33.9}  & \multicolumn{2}{c}{24.7}  & \multicolumn{2}{c}{40.2} &~ 29.0\\
\midrule
Cover + LSVM & \multicolumn{2}{c}{28.2} & \multicolumn{2}{c}{47.2}  & \multicolumn{2}{c}{9.6} & \multicolumn{2}{c}{34.7}  & \multicolumn{2}{c}{25.2}  & \multicolumn{2}{c}{39.8} &~ 30.8\\
\bottomrule
\end{tabular}
\caption{Detection average precision (\%) on PASCAL VOC 2007-6x2 test set. First three baseline methods report results limited to left and right subcategories of the objects.}
\label{tab:detection-6x2}
\end{table*}


\begin{table*}[htbp]
\footnotesize
\centering
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{0.35mm}
\begin{tabular}{l *{21}{c}}
\toprule
VOC2007 test & ~aero & bike & bird & boat & bottle & bus & car & cat & chair & cow & table & dog & horse & mbike & pson & plant & sheep & sofa & train & tv & ~mAP\\
\midrule
\cite{siva1} & ~13.4 & 44.0 & 3.1 & 3.1 & 0.0 & 31.2 & 43.9 & 7.1 & 0.1 & 9.3 & 9.9 & 1.5 & 29.4 & 38.3 & 4.6 & 0.1 & 0.4 & 3.8 & 34.2 & 0.0 & ~ 13.9\\
\midrule
Cover + SVM & ~23.4 & 43.5 & 22.4 & 8.1 & 6.2 & 33.9 & 33.8 & 30.4 & 0.1 &17.9 & 11.5 & 17.1 & 24.7 & 40.2 & 2.4 & 14.8 & 21.4 & 15.1 & 31.9 & 6.2 & ~20.3\\
\midrule
Cover + LSVM & ~28.2 & 47.2 & 17.6 & 9.6 & 6.5 & 34.7 & 35.5 & 31.5 & 0.3 & 21.7 & 13.2 & 20.7 & 25.2 & 39.8 & 12.6 & 18.6 & 21.2 & 18.6 & 31.7 & 10.2 & ~22.2\\
\midrule
Cover + SLSVM & ~27.6 & 41.9 & 19.7 & 9.1 & 10.4 & 35.8 & 39.1 & 33.6 & 0.6 & 20.9 & 10.0 & 27.7 & 29.4 & 39.2 & 9.1 & 19.3 & 20.5 & 17.1 & 35.6 & 7.1 & ~22.7\\
\bottomrule
\end{tabular}
\caption{Detection average precision (\%) on full PASCAL VOC 2007 test set.}
\label{tab:detection-full}
\end{table*}

\section{Experiments}
\label{sec:exp}

We performed two sets of experiments, one on a multiple instance learning dataset \cite{misvm-nips} and the other on the PASCAL VOC 2007 data \cite{PASCAL07}.  The first experiment was designed to compare the multiple instance learning bag classification performance of LSVM with Smooth LSVM (SLSVM). The second experiment evaluates detection accuracy (measured in average precision) of our framework  in comparison to baselines. 

\subsection{Multiple instance learning datasets}

We evaluated our method in Section 5 on standard multiple instance learning datasets \cite{misvm-nips}. For preprocessing, we centered each feature dimension and  normalize the data. For fair comparison with \cite{misvm-nips}, we use the same initialization, where the initial weight vector is obtained by training an SVM with all the negative instances and bag-averaged positive instances. For this experiment, we performed 10 fold cross validation on   and .  Table \ref{tab:mil-experiments} shows the experimental results. Without the bias, our method significantly performs better than LSVM method and with the bias, our method shows modest improvement in most cases.

\subsection{Weakly-supervised object detection}

To implement our weakly-supervised detection system we need suitable image features for computing the nearest neighbors of each image window in Section \ref{sec:covering} and for learning object detectors.
We use the recently proposed R-CNN \cite{girshick2014rcnn} detection framework to compute features on image windows in both cases. Specifically, we use the convolutional neural network (CNN) distributed with DeCAF \cite{decafICML}, which is trained on the ImageNet ILSVRC 2012 dataset (using only image-level annotations).
We avoid using the better performing CNN that is fine-tuned on PASCAL data, as described in \cite{girshick2014rcnn}, because fine-tuning requires instance-level annotations. 

We report detection accuracy as average precision on the standard benchmark dataset for object detection, PASCAL VOC 2007 \emph{test} \cite{PASCAL07}. We compare to five different baseline methods that learn object detectors with limited annotations. Note that other baseline methods use additional information besides the one-bit image-level annotations. \citet{deselaers1, deselaers2} use a set of  images with bounding box annotations as meta-training data. In addition to bounding box annotations, \citet{deselaers1, deselaers2, pandey} use extra instance level annotations such as \emph{pose}, \emph{difficult} and \emph{truncated}. \citet{siva2012defence,russakovsky} use \emph{difficult} instance annotations but not \emph{pose} or \emph{truncated}. First, we report the detection average precision on  subsets of classes in table \ref{tab:detection-6x2} to compare with \citet{deselaers1, deselaers2, pandey}. 
 
To evaluate the efficacy of our initialization, we compare it to the state-of-the-art algorithm recently proposed by \cite{siva2012defence}. Their method constructs a set of positive windows by looping over each positive image and picking the instance that has the maximum distance to its nearest neighbor over all negative instances (and thus the name negative data \emph{mining} algorithm). For a fair comparison, we used the same window proposals, the same features \cite{girshick2014rcnn}, the same L2 distance metric, and the same PASCAL 2007 detection evaluation criteria. The class mean average precision for the mining algorithm was  compared to  obtained by our initialization procedure. Figure \ref{fig:us_vs_siva} visualizes some command failure modes in our implementation of  \cite{siva2012defence}.  Since the negative mining method does not take into account the similarity among positive windows (in contrast to our method) our intuition is that the method is less robust to intra-class variations and background clutter. Therefore, it often latches onto background objects (i.e. hurdle in horse images, street signs in bus images), onto parts of the full objects (i.e. wheels of bicycles), or merges two different objects (i.e. rider and motorcycle). It is worth noting that \citet{pandey, siva2012defence} use the CorLoc metric\footnote{CorLoc was proposed by \cite{deselaers1} to evaluate the detection results on PASCAL \emph{train} set} as the evaluation metric to report results on PASCAL \emph{test} set. In contrast, in our experiments, we exactly follow the PASCAL VOC evaluation protocol (and use the PASCAL VOC devkit scoring software) and report detection average precision. 

Table \ref{tab:detection-full} shows the detection result on the full PASCAL 2007 dataset. There are two baseline methods \cite{siva1, russakovsky} which report the result on the full dataset. Unfortunately, we were not able to obtain the per-class average precision data from the authors of \cite{russakovsky} except the class mean average precision (mAP) of 15.0\%. As shown in Table \ref{tab:detection-full}, the initial detector model trained from the constructed set of positive windows already produces good object detectors but we can provide further improvement by optimizing the MIL objective.

\vspace{-2pt}
\section{Conclusion}

We developed a framework for learning to localize objects with one-bit object presence labels. Our results show that the proposed framework can construct a set of positive windows to train initial detection models and improve the models with the refinement optimization method. We achieve state-of-the-art performance for object detection with minimal supervision on the standard benchmark object detection dataset. Source code will be available on the author's website.

\vspace{-5pt}
\section*{Acknowledgement}
\small{
We thank Yong Jae Lee for helpful insights and discussions. H. Song was supported by Samsung Scholarship Foundation. J. Mairal and Z. Harchaoui were funded by the INRIA-UC Berkeley associated team ``Hyperion", a grant from the France-Berkeley fund, the Gargantua project under program Mastodons of CNRS, and the LabEx PERSYVAL-Lab (ANR-11-LABX-0025). This work was partially supported by  ONR N00014-11-1-0688, NSF, DARPA, and Toyota.}

\bibliography{refs}
\bibliographystyle{icml2014}

\end{document} 
