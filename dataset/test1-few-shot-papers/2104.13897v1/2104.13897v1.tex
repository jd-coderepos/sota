\documentclass[final,5p,times,twocolumn]{elsarticle}
\let\today\relax
\makeatletter
\def\ps@pprintTitle{\let\@oddhead\@empty
	\let\@evenhead\@empty
	\def\@oddfoot{}\let\@evenfoot\@oddfoot}
\makeatother







\usepackage{amssymb, hyperref}
\usepackage{amsmath,mathtools}
\usepackage{caption}





\begin{document}
	
	\renewcommand{\vec}[1]{\boldsymbol{\mathrm{#1}}}
	
	\begin{frontmatter}
		




		\title{Inpainting Transformer for Anomaly Detection}
		


		\author[fujitsu]{Jonathan Pirnay}
		\author[fujitsu]{Keng Chai}
		\address[fujitsu]{Digital Incubation, Fujitsu Technology Solutions GmbH, Germany}


		\begin{abstract}
Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When learning from scratch, InTra achieves better than state-of-the-art results on the MVTec AD \cite{MvtecAd2019} dataset for detection and localization.
		\end{abstract}
		
	\end{frontmatter}
	


\section{Introduction}
	\label{section:introduction}
	
	Anomaly detection and localization in vision describe the problem of deciding whether a given image is atypical with respect to a set of normal samples, and to identify the respective anomalous subregions within the image. Although often viewed separately, both problems have strong implications for industrial inspection \cite{MvtecAd2019} and medical applications \cite{fernando2020deep}. In practical industrial applications, anomalies occur rarely. Due to the lack of sufficient anomalous samples, and as anomalies can be of unexpected shape and texture, it is hard to deal with this problem with supervised methods. Current approaches follow unsupervised methods and try to model the distribution of normal data only. At test time an anomaly score is given to each image to indicate how much it deviates from normal samples. For anomaly localization a similar score is assigned to subregions or individual pixels of the image.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\columnwidth]{footage/graphical_abstract.jpg}
		\caption{Schematic overview of the proposed method.
			a.) The image is split into square patches. An inpainting transformer model (InTra) is trained to reconstruct a covered patch (black) from a long sequence of surrounding patches (red). Positional embeddings are added to the patches to include spatial context. b.) By reconstructing all patches of an image (left), a full reconstruction is obtained (middle). The difference of original and reconstruction is used to compute a pixel-wise anomaly score (right) for detection and localization.
		}
		\label{fig:graphical_abstract}
	\end{figure}
	
	A common approach following this paradigm is to use deep convolutional autoencoders or generative models such as variational autoencoders \cite{MvtecAd2019,Bergmann2019ssimanom,2018arXiv180701349L} and generative adversarial networks (GANs) \cite{gans2014,anogan2017,akcay2018ganomaly} in order to model the manifold of normal training data. The difference between the input and reconstructed image is then used to compute the anomaly scores. This is based on the idea that by training on normal images only, the model will not be able to properly reconstruct anomalous images, leading to higher anomaly scores. In practice this approach often suffers from the drawback that convolutional autoencoders generalize strongly and anomalies are reconstructed well, leading to misdetection \cite{gong2019memorizing}. Recent methods propose to mitigate this effect by posing the generative part as an inpainting problem: Parts of the input image are covered and the model is trained to reconstruct the covered parts in a self-supervised way \cite{Bhattad2018DetectingAF,8614226,ZAVRTANIK2021107706,DBLP:journals/corr/abs-2010-01942}. By conditioning on the neighborhood of the excluded part only, small anomalies get effectively retouched. Due to their limited receptive field, fully convolutional neural networks (CNNs) are partially ineffective in modeling distant contextual information, which makes the removal of larger anomalous regions difficult. In order to influence a pixel by information 64 pixels away, at least 6 layers of  convolutions with dilation factor 2 or equivalent are required \cite{Yu:2016:MCA,Yu_2018_CVPR}. For inpainting in general settings, this can be effectively addressed by introducing contextual attention in the model \cite{Yu_2018_CVPR}. For inpainting in the context of anomaly detection we suggest it to be beneficial to learn the relevant patterns alone by combining information from large regions around the covered image part via attention.
	
	Inspired by the recent success of self-attention based models such as Transformers \cite{NIPS2017_3f5ee243} in image recognition \cite{dosovitskiy2020}, we pose anomaly detection as a patch-inpainting problem and propose to solve it without convolutions: images are split into square patches, and a Transformer model is trained to reconstruct covered patches on the basis of a long sequence of neighboring patches. By recovering the whole image in this way, a full reconstructed image is obtained where the reconstruction of an individual patch incorporates a larger global context and not only the appearance of its immediate neighborhood. Thus patches are not reconstructed by simply mimicking the local neighborhood, leading to high anomaly scores even for spacious anomalous regions.
	
	Our contributions enfold the modeling of anomaly detection as a patch-sequence inpainting problem which we solve using a deep Transformer network consisting of a simple stack of multiheaded self-attention blocks. Within this network convolutional operations are removed entirely. Furthermore we propose to
	\begin{enumerate}[a.)]
		\item employ long residual connections between the Transformer blocks
		\item perform a dimension reduction for keys and queries with a small multilayer perceptron when computing self-attention
	\end{enumerate}
	in order to improve the network's reconstruction capabilities for difficult surfaces. By adding embeddings of the position of individual patches within an image to the sequence of patches, it is possible to perform the inpainting in a global context even if the sequence of patches does not cover the full image. This yields improved results especially for anomaly detection tasks where the composition of an inspected image may only differ slightly from the normal samples.
	
	We evaluate our method on the challenging MVTec AD dataset for both detection and segmentation. Although Transformer networks are usually trained on huge amounts of data, we effectively train our networks with 55M parameters from scratch only on the 200-300 images available for each category in MVTec AD. With our proposed method InTra we achieve better than state-of-the-art performance on MVTec AD in comparison to other methods \cite{Yi_2020_ACCV,ZAVRTANIK2021107706,li2021cutpaste} which are not using extra additional training data.
	
	\section{Related Work}
	\label{sec:rel_work}
	Image based anomaly detection and segmentation has improved significantly with the technological advances of deep learning \cite{MvtecAd2019}. The following section will give a summary about most recent approaches. While CNNs have shown to be highly successful for vision based tasks and also anomaly detection and segmentation \cite{MvtecAd2019,Goodfellow-et-al-2016, defard2020padim, ruff2018deeponeclass, 8614226, Bhattad2018DetectingAF}, sequence-to-sequence Transformer models, originating from Natural Language Processing (NLP) have found application in computer vision tasks \cite{dosovitskiy2020, NIPS2017_3f5ee243}.
	
	\subsection{Anomaly Detection and Segmentation}
	Anomaly detection is concerned about deciding if an image contains an unexpected deviation from a predefined norm, while in segmentation the goal is to find and localize these deviations accurately on a pixel level to extract the regions where a defect occurred.
	For explainability a good overall performance in both tasks is needed which is not guaranteed if a method achieves good results in one of the tasks.
	Existing methods can be roughly categorized into two different approaches.
	
	
	\paragraph{Reconstruction based}
	Reconstruction-based models try to model only normal, defect-free samples. For this, deep CNN autoencoders are widely used to model the manifold of defect-free images in a latent bottleneck. This has shown to achieve significant results in various domains. Given defective test data, these models should not be able to properly reconstruct the anomalous image since they only model normal data \cite{CHOW2020101105, Sakurada2014autencoder, Baur_2019, gong2019memorizing}. An anomaly map for segmentation is usually generated via pixel-wise difference between the input image and its model reconstruction, leading to noticeable anomalies.
	Modifications like integrating structural similarity index measure (SSIM) in the loss function during training are used to improve reconstruction quality by producing smoother images while focusing on retaining structural information such as edges \cite{Bergmann2019ssimanom, Wang2004ssim}.
	Variational Autoencoders (VAEs) have also found usage in anomaly detection and segmentation \cite{2018arXiv180701349L, vasilev2018qspace, An2015VariationalAB,Baur_2019}. Their probabilistic latent space tries to capture a distribution capable of generating normal samples. This generative approach allows for the inclusion of the Kullback-Leibler divergence in the anomaly score to incorporate a probabilistic scoring  \cite{2018arXiv180701349L, vasilev2018qspace, An2015VariationalAB}.
	In general VAEs are not automatically superior to traditional autoencoder methods \cite{MvtecAd2019,Baur_2019}.
	
	Adversarial models such as Generative Adversarial Networks (GANs) have been used for anomaly detection and segmentation \cite{MvtecAd2019,schlegl2017unsupervised, akcay2018ganomaly,akcay2019skipganomaly}.
	Although GANs often suffer from an unstable training procedure, highly realistic and almost natural images can be generated \cite{Goodfellow-et-al-2016, gans2014}.
	Akcay et al. propose to additionally take the discriminator network into account for the calculation of the anomaly score \cite{akcay2019skipganomaly}.
	For this a feature loss between the input image and its reconstructed counterpart is employed based on the last convolutional layer of the discriminator model.
	
	
	
	\paragraph{Embedding based}
	CNNs comprise of multiple hidden layers which produce meaningful latent features.
	Especially later stages lead to distinguishable and powerful intermediate representations.
	These feature vectors can be utilized to build the basis of an anomaly detection approach \cite{ruff2018deeponeclass, bergman2020deep, defard2020padim, rudolph2020differnet}.
	Popular CNN architectures such as ResNet have been used which were originally pretrained on the ImageNet classification benchmark \cite{he2015deep, Deng2009Imagenet, Napoletano2018cnnembedding}.
	
	Based on the feature representations e.g. a clustered codebook of good samples \cite{Napoletano2018cnnembedding} or a nearest neighbour algorithm can be used \cite{ruff2018deeponeclass,bergman2020deep}.
	Another approach proposed by Rudolph et al. uses normalizing flows to estimate a standard multivariate normal distribution of good samples only \cite{rudolph2020differnet}.
	
	While embedding based methods achieve good detection results, accurate localization of anomalies is not automatically included since no inherent mapping to the original image space exists.
	Defard et al. propose a patch based approach while maintaining the mapping of coordinates from original image space to feature maps \cite{defard2020padim}.
	Based on different convolutional architectures (Resnet  \cite{he2015deep}, Wide ResNet \cite{zagoruyko2017wide}, EfficientNet \cite{tan2020efficientnet}) feature vectors are  extracted for each patch and for each patch position a multivariate Gaussian is inferred to model the distribution of normal samples.
	
	
	\subsection{Inpainting in Anomaly Detection}
	Inpainting is a specific subtask in visual image understanding having found usage in various applications like image editing and synthesis.
	Given a partly covered image, the goal is to accurately reconstruct the original uncovered data.
	Earlier methods have found success using matching based on local image descriptors \cite{Hays2007SceneCompletion}.
	Complex scenes and objects are harder to get consistent and realistic results for
	as the model has to understand the context and content of the image \cite{Satoshi2017inpainting, Yu_2018_CVPR}.
	
	Even though in reconstruction-based methods for anomaly detection the models are trained on defect-free samples, they often generalize well to anomalies in practice \cite{gong2019memorizing}.
	An inpainting scheme can be used to effectively hide anomalous regions to further restrict a model's capability to reconstruct anomalies  \cite{Bhattad2018DetectingAF,8614226, ZAVRTANIK2021107706, DBLP:journals/corr/abs-2010-01942}.
	Since some part of the original image is covered, the reconstruction method needs to have semantic understanding of the image to be able to generate a coherent and realistic image.
	
	Zavrtanik et al. propose to use a U-Net architecture \cite{10.1007/978-3-319-24574-4_28} taking advantage of long residual connections. Their reconstruction-based method randomly selects multiple parts of the image to inpaint \cite{ZAVRTANIK2021107706}.
	The additional use of gradient magnitude similarity (GMS) \cite{10.1109/TIP.2013.2293423} in the loss function yields the current state-of-the-art results for anomaly detection via inpainting for different benchmarks \cite{ZAVRTANIK2021107706}. 
	
	Anomalies which span over a large area may still cause problems as these will not be covered up sufficiently enough. 
	As such we propose to add global context via replacing CNNs with a Transformer-based framework applied in vision. 
	


	
	\subsection{Transformers in Vision}
	Transformer models were originally introduced in NLP and have since evolved to be state-of-the-art design for various sequence tasks like text translation, generation and document classification \cite{NIPS2017_3f5ee243, devlin2019bert, radford2019language, yang2020xlnet}. Usually pretrained on huge amounts of data and afterwards finetuned on specific tasks such as human-like text synthesis impressive results can be achieved \cite{radford2019language}.
	
	Attention is used to relate elements of a sequence to each other. Based on the relative weighted importance a shared representation is calculated taking into account the relative dependencies between sequence elements.
	This is able to replace recurrent neural networks in sequence-to-sequence modeling because long-range dependencies are processed globally.
	In practice a single attention module may fail to capture more complex relations inside a sequence.
	As such multiple attention passes are run in parallel, concatenated and unified with a linear transformation, resulting in multihead attention \cite{NIPS2017_3f5ee243}.
	The encoder and decoder architecture of the Transformer consists of a stack of multiple blocks.
	Each encoder and decoder block contains a combination of multihead self-attention and fully connected layers while each decoder block additionally processes the encoder output through another attention component.
	The general architecture can be found in the original work \cite{NIPS2017_3f5ee243}.
	
	While Transformer architectures have been widely studied in NLP and sequence modeling, convolutional architectures have been essentially the standard tool in recent years due to weight sharing, translation equivariance and locality.
	Due to the induced bias in fully convolutional autoencoders, the restricted receptive field limits global context \cite{Yu_2018_CVPR}.
	Even though in theory the self-attention framework may mitigate this problem, running self-attention on the whole image without further simplifications is not feasible \cite{ho2019axial, parmar2018image}.
	First approaches restricted the neighborhood of pixels considered in the attention module \cite{parmar2018image} while Ho et al. proposed axial attention in which the computation is split along the spatial axes.
	
	Most recently Dosovitskiy et al. have proposed Vision Transformer (ViT) \cite{dosovitskiy2020}. The image data is split up into square non-overlapping uniform patches \cite{dosovitskiy2020}.
	Each patch and position gets embedded into a latent space and every image is treated as a sequence of these embedded patches.
	A Transformer architecture is applied on the restructured data achieving comparable results to state of the art CNNs and even surpassing them on some tasks while reducing model bias with a more generic framework. 
	
	
	\section{Inpainting Transformer for Anomaly Detection}
	
	Our approach is based on a simple stack of Transformer blocks which are trained to inpaint covered image patches based on neighboring patches. The network is trained on normal samples only. An anomaly score is computed for each pixel based on the difference of the original image and its reconstruction obtained by inpainting all patches. An overview of the method is shown in Figure \ref{fig:graphical_abstract}. In the following sections we describe the steps of our method in detail.
	
	\subsection{Embedding Patches and Positions}
	\label{seq:embedding_patches}
	
	We use a similar notation as in \cite{dosovitskiy2020}. Let  be an input image, where  denotes the (height, width)-resolution and  the number of channels of the image. Let  be the desired side length of a square patch and  (we resize the image such that  divides  and ). We split the image  into a  grid of flattened square patches
	
	where  is the patch in the -th row and -th column. Our aim is to choose square subgrids of some side length  in this patch grid and train a network to reconstruct any covered patch in the subgrid based on the rest of the subgrid's patches. Formally, this inpainting problem is as follows:
	
	Let
	
	be such a square subgrid ("window") of patches defined by some index set . Here  is the side length of the window, and  is the grid position of the window's upper left patch. If  is the position of some patch, the formal task to inpaint  given  is to approximate the patch  using only the content and positions of all other patches  in the window.
	
	As by definition Transformers are invariant with respect to reorderings of the input, we need to encode the positional information of the individual patches. The use of 2-dimensional-aware embeddings does not lead to significant performance gains \cite{dosovitskiy2020}, so we define two positional mappings
	
	and
	
	
	Informally speaking,  assigns to the patches in the window their 1-dimensional position when starting to count from the upper left patch in the whole image, whereas  assigns the position "locally" when counting only within the window (see Figure \ref{fig:window_locations}). Depending on the image domain, local or global positioning can be chosen. The intuition behind this is that in some settings such as textures, the exact position of a patch window within the full image does not play an important role, whereas in other settings, global positions do carry important information.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=6cm]{footage/local_global_grid.jpg}
		\caption{Illustration of  (left) and  (right) for a patch window (red) with .}
		\label{fig:window_locations}
	\end{figure}
	
	To use as a sequence input to the Transformer model, we map the window of patches and their positional information into some latent space of dimension . For position embeddings we use standard learnable one-dimensional position embeddings
	
	for  (resp.  for ). For ease of notation, in the following we assume  is chosen. The global case is analogous.
	
	For each patch  with  the position embeddings are added to a trainable linear projection via
	
	
	with weight matrix .
	
	To account for the patch at position  to inpaint, we use  as above and add a {\em single} learnable embedding , and set
	
	The embedding  is comparable to the class token in \cite{devlin2019bert}.
	
	The vectors in (\ref{eq:patch_embedding}) and (\ref{eq:inpainting_embedding}) build the final sequence of embedded patches
	
	which serves as an input sequence of length  to the Inpainting Transformer model.
	
	\begin{figure*}[t]
		\centering
		\includegraphics[width=14cm]{footage/diagram_network_and_block.jpg}
		\caption{Overview of the proposed architecture. Left: Parts of an individual Transformer block. Right: A stack of Transformer blocks builds the full architecture. Long residual connections are used to add information from earlier blocks to later ones.}
		\label{fig:network_architecture}
	\end{figure*}
	
	\subsection{Multihead Feature Self-Attention}
	\label{seq:mfsa}
	Self-attention is the main building block of the Transformer and will be successively applied to the projected patch sequence  in (\ref{eq:patch_sequence_projected}).
	
	We briefly revisit multihead self-attention (MSA) as in \cite{NIPS2017_3f5ee243} before proposing a dimension-reduction extension. For this let  be some input sequence of length . For standard MSA with  heads we compute queries , keys  and values  via
	
	with learnable weight matrices . Each vector in the sequence  is sliced into  pieces of
	dimension , i.e. we consider each  as elements in .
	
	For each head , self-attention is computed for the sequences  as a weighted sum over  as follows:
	
	
	
	The sequences  are concatenated back to a sequence of dimension  which is unified via a linear map to obtain the MSA output
	
	where  is another learnable weight matrix.
	
	For the inpainting problem MSA can be used as described above. In cases where the patches of the training images are very similar but indistinct the dot product of queries and keys in (\ref{eq:weighing_matrix}) and hence the entries of the weighing matrix  are close to each other, leading to an almost uniform weighted sum. To mitigate this, we propose to perform a nonlinear dimension reduction when computing  and . I.e. instead of only linear maps in (\ref{eq:qkv_msa}) we set
	
	Here
	
	are multilayer perceptrons with a single hidden layer with GELU non-linearity (in all our models we used  and a hidden layer dimension of ). The rest of the multihead attention mechanism is applied as in (\ref{eq:weighing_matrix}), (\ref{eq:msa_values}), (\ref{eq:msa_unify}), except that the vectors in  are sliced into  pieces of dimension  and  is replaced with  in (\ref{eq:weighing_matrix}). We refer to this modified MSA as multihead feature self-attention (MFSA). We experienced faster convergence and improved results with MFSA (see Section \ref{ablation:feature_attention}). However, depending on  and , the number of learnable parameters increases strongly with MFSA.
	
	\subsection{Network Architecture}
	
	Our network architecture for inpainting is composed of a simple stack of  Transformer blocks. Figure \ref{fig:network_architecture} illustrates the architecture. The structure of each Transformer block mainly follows \cite{dosovitskiy2020} and consists of MFSA followed by a multilayer perceptron (MLP). Layer normalization is applied before ("pre-norm" \cite{Wang2019LearningDT}), and residual connections after MFSA and MLP. Each MLP has a single hidden layer with GELU nonlinearity and maps . In particular the input and output of each Transformer block is a sequence in  (see Figure \ref{fig:network_architecture}). 
	
	To obtain the inpainted patch, we average over the output sequence of the last Transformer block to get a single vector in . This vector is mapped back to the pixel space of the flattened patches  via a learnable affine transformation. As an alternative to averaging, one could take the first vector of the sequence which corresponds to the patch-to-inpaint in the input sequence.
	
	In early experiments an inspection of the attention weights showed that a large spatial context is present in earlier layers. In addition to that, Attention Rollout \cite{attentionRollout} has been used in \cite{dosovitskiy2020} to illustrate that information across the entire input image is integrated already in the lowest layers. In order to carry this early information through deeper self-attention blocks of the network, we put additional long residual connections between early and late layers in a U-Net fashion \cite{10.1007/978-3-319-24574-4_28}. We found that the use of long residual connections leads to slightly more structural detail in the overall reconstruction, but also to more chromatic artifacts in the reconstruction of defective regions, improving detection (see Section \ref{ablation:skip_conns}). 
	
	\subsection{Training}
	
	The network is trained by randomly sampling batches of patch windows with a fixed side length  from normal image data. In each window a random patch position  is chosen, which is inpainted by the network as described in the previous sections. 
	
	For the loss function, we compare the original and reconstructed patch with pixel-wise  loss. To account for perceptual differences, we also include structural similarity (SSIM) \cite{Wang2004ssim} and gradient magnitude similarity (GMS) \cite{10.1109/TIP.2013.2293423}.
	Preliminary for this, given any two images , we define a gradient difference map
	
	Here  is a matrix of ones and  is the gradient magnitude similarity map for color channel . We define an analogous difference map for SSIM which we denote by . 
	
	Finally given an original and reconstructed patch , the full loss function  is given by
	
	where  are individual scaling parameters. 
	
	\subsection{Inference and Anomaly Detection}
	
	The inferencing process is divided into two steps: First a complete inpainted image is generated, afterwards the difference between the reconstruction and original is used to compute a pixel-wise anomaly map for localization. The maximum pixel value of the anomaly map is taken as a global anomaly score for detection on image level. 
	
	Let  be an input image with an  patch grid as introduced above. For each patch position , we choose an appropriate patch window of side length  which is used as a basis to inpaint the patch at position . In particular we define the window by its upper left patch  with
	
	where the map  is given by
	
	The above equations simply choose  such that  is as much centered in the  patch-window as possible. Using this window, the patch  is reconstructed with our model as described. By reconstructing all patches in the  grid, we obtain a full reconstruction  of the whole image. 
	
	The generation of an expressive anomaly map from  and  is by itself a nontrivial problem. This is not part of our contributions, in particular for comparability we use a simplified variant of the GMS-based scheme proposed in \cite{ZAVRTANIK2021107706}. 
	Generally speaking, we compute the gradient magnitude similarity of both images at half and quarter scale, smooth them with an averaging and gaussian blur operation and take the mean of the resulting maps resized to original scale. 
	
	Formally this translates to the following: We write  for an image  resized to scale . Now for original and reconstructed images  and scale , we set
	
	for a scaled and smoothed version of the gradient difference map  in (\ref{eq:gradient_difference_map}). To ease notation, we denote by  the application of an averaging filter followed by some Gaussian blur operation, both with a predefined kernel size and variance. As in \cite{ZAVRTANIK2021107706}, smoothing improves robustness with respect to small, poorly reconstructed anomalous regions. We resize the two-dimensional maps  and  back to the original size and take the pixel-wise mean which yields a difference map . 
	
	To finally obtain an anomaly map for  during inference, we take the squared deviation of the difference map to the normal training data, i.e.
	
	where  is the set of normal training samples. As a scalar anomaly score for detection on image level we take the pixel-wise maximum
	
	
	An example of an anomaly map can be seen in Figure \ref{fig:graphical_abstract}b.). 
	
	\section{Experiments}
	We evaluate our method on the MVTec AD dataset which contains high resolution samples of 5 texture and 10 object categories stemming from manufacturing \cite{MvtecAd2019}. For an overview of all categories we refer to Table \ref{table:results}. The dataset has been a widely used benchmark for anomaly detection and localization in the manufacturing domain. Each category consists of around 60 to 400 normal, defect-free samples for training, and a mixture of normal and anomalous images for testing. Additionally for each image there is a ground-truth binary image labeled on pixel-level for segmentation of anomalous test images.
	
	Based on an image's anomaly score (\ref{eq:anoscore}), we report standard ROC AUC as a detection metric. For localisation, the image's anomaly map (\ref{eq:anomap}) is used for an evaluation of pixel-wise ROC AUC.
	Each category is evaluated separately to give an insight of advantages and problems of our method. 
	Additionally we report mean AUC over all categories for detection and segmentation to give a comparison to current state-of-the-art results.
	
	\begin{table*}[h]
		\begin{center}
			\begin{tabular}{|l c c c c c|} 
				\hline
				Category & Patch-SVDD \cite{Yi_2020_ACCV} & RIAD \cite{ZAVRTANIK2021107706} & CutPaste \cite{li2021cutpaste} &PaDiM \cite{defard2020padim} &Ours \\
				\hline \hline
				Carpet & 92.9 & 84.2& 93.1 & & \textbf{98.8} \\
				Grid & 94.6 & 99.6 & 99.9 & &  \textbf{100.0} \\
				Leather& 90.9 & 100.0& 100.0 & & \textbf{100.0}  \\
				Tile & 97.8 & 93.4 & 93.4 & & \textbf{98.2} \\
				Wood & 96.5 & 93.0 & \textbf{98.6} & & 98.0 \\
				\hline
				avg. textures & 94.54 & 95.1 & 97.0 & 99.0 & \textbf{99.0} \\
				\hline 
				Bottle & 98.6 & 99.9 & 98.3 & & \textbf{100.0} \\
				Cable & \textbf{90.3 }& 81.9 &  80.6  & & 84.2  \\
				Capsule & 76.7 & 88.4 &  \textbf{96.2} & & 86.5 \\
				Hazelnut & 92.0 & 83.3 & \textbf{97.3} & & 95.7 \\
				Metal Nut & 94.0 & 88.5 & \textbf{99.3} & & 96.9 \\
				Pill & 86.1 & 83.8 & \textbf{92.4} & & 90.2  \\
				Screw & 81.3 & 84.5 & 86.3 & & \textbf{95.7}  \\
				Toothbrush & 100.0 & \textbf{100.0} & 98.3 & & 99.7 \\
				Transistor& 91.5 & 90.9 & 95.5 & & \textbf{95.8} \\
				Zipper & 97.9 & 98.1 & 99.4 & & \textbf{99.4} \\
				\hline
				avg. objects & 90.8 & 89.9 & 94.3 & \textbf{(97.2)} & \textbf{94.41} \\
				\hline 
				avg. all categories & 92.1& 91.7 & 95.2 &\textbf{(97.9)} & \textbf{95.94} \\
				\hline
			\end{tabular}
			\captionof{table}{Detection results for MVTec AD. Results are presented in ROC AUC . We compare our results to state-of-the-art methods. Note that PaDiM only reports mean AUC.}
			\label{table:results}
		\end{center}
	\end{table*}
	
	\begin{table*}[h]
		\begin{center}
			\begin{tabular}{|l c c c c c|} 
				\hline
				Category & Patch-SVDD \cite{Yi_2020_ACCV} & RIAD \cite{ZAVRTANIK2021107706} & CutPaste \cite{li2021cutpaste} &PaDiM \cite{defard2020padim} &Ours \\
				\hline \hline
				Carpet & 92.6 & 96.3 & 98.3 & 99.1 & \textbf{99.2} \\
				Grid & 96.2 & 98.8 & 97.5 & 97.3 &  \textbf{99.4} \\
				Leather& 97.4 & 99.4 & 99.5 & 99.2 & \textbf{99.5}  \\
				Tile & 91.4 & 89.1 & 90.5 &94.1  & \textbf{94.4} \\
				Wood & 90.8 & 85.8 & \textbf{95.5} & 94.9 & 90.5 \\
				\hline
				avg. textures & 93.7 & 93.9 & 96.3 & \textbf{96.9} & 96.6 \\
				\hline 
				Bottle & 98.1 & \textbf{98.4} & 97.6 & 98.3 & 97.1 \\
				Cable &\textbf{96.8}  & 84.2 &  90.0  & 96.7 & 93.2  \\
				Capsule & 95.8 & 92.8 & 97.4 & \textbf{98.5} & 97.7 \\
				Hazelnut & 97.5 & 96.1 & 97.3 & 98.2 & \textbf{98.3} \\
				Metal Nut & \textbf{98.0} & 92.5 & 93.1 & 97.2 & 93.3 \\
				Pill & 95.1 & 95.7 & 95.7 & 95.7 & \textbf{98.3}  \\
				Screw & 95.7 & 98.8 & 96.7 & 98.5 & \textbf{99.5}  \\
				Toothbrush & 98.1 & 98.9 & 98.1 & 98.8 & \textbf{99.0} \\
				Transistor& 97.0 & 87.7 & 93.0 & \textbf{97.5} & 96.1 \\
				Zipper & 95.1 & 97.8  & \textbf{99.3}  & 98.5 & 99.2 \\
				\hline
				avg. objects & 96.7 & 94.3 & 95.8& \textbf{(97.8)} & \textbf{97.17} \\
				\hline 
				avg. all categories & 95.7 & 94.2 & 96.0 & \textbf{(97.5)} & \textbf{96.98} \\
				\hline
			\end{tabular}
			\captionof{table}{Segmentation results for MVTec AD. Results are presented in pixel-wise ROC AUC .}
			\label{table:results_segmentation}
		\end{center}
	\end{table*}
	
	\subsection{Implementation Details}
	\label{seq:implementation_details}
	
	We train our model on each product category from scratch, with the same following strategy. We randomly choose 10\% of images from the normal training data (however a maximum of 20) and use them as a validation set to control the quality of reconstructions. In each epoch 600 patch windows are sampled randomly per image. To augment the dataset, random rotation and flipping is used.
	
	The choice of three parameters has an obvious significant impact on the performance: Side length  of square patches, side length  of a patch window and the choice of height  and width  (with , as all images are square) to which the original image is resized during training and inference. The patch size determines how much of the image is covered, the size of the patch window determines the dilation of context we include during inpainting, the image size implicitly influences both. For all models we choose , . Transformers in vision usually take a long time to train as they lack the inductive bias of CNNs. For the choice of image size in our pipeline, a balance needs to be struck between enlarging the image context of the  window, quality of patch reconstructions and computation time. The heuristics is to choose the image as small as possible while keeping patch reconstructions at a high level of detail. Hence we train the model with image dimensions  for 200 epochs and compare the best (epoch-wise) validation losses (averaged over  epochs). If there is no significant improvement in the validation loss of at least  for an image dimension with the next in size, the smaller dimension is chosen. Table \ref{table:image_sizes} shows the resulting image sizes for each category. For comparison and generality this simple pipeline is applied employing the validation set, we note however that in practice  could be tuned for the detection task at hand if prior knowledge about possible defects is present. For the patch position embeddings we use  as described in Section \ref{seq:embedding_patches}. 
	
	The Inpainting Transformer model trained consists of 13 blocks with 8 feature attention heads each. We set the latent dimension to . For multihead feature self-attention,  and  have an output dimension of  and one hidden layer of dimension . In total this amounts to 55M learnable parameters. 
	
	For computation of anomaly maps, given an image size of , a kernel size of  (resp. ) is used for averaging and Gaussian blur (with ) for  (resp. ) in (\ref{eq:bluravg}). The kernel sizes are simply scaled linearly for smaller image sizes. Before taking the maximum as anomaly score, the anomaly map is resized back to the original high image resolution for proper segmentation comparison.  
	All resizing is performed with bilinear interpolation. 
	
	For the loss function  in (\ref{eq:loss}) we set . The network is trained using the Adam optimizer with a learning rate of  and a batch size of 256. Training of the Transformer network may take a long time ( epochs in some cases), depending on the chosen resolution, available data and difficulty of reconstruction. In particular the network is trained until no improvement on the validation loss is observed for 50 consecutive epochs, and the model at the epoch with the best validation loss is chosen for evaluation.  
	Although common for training Transformers, we don't apply dropout at any point. 
	
	\subsection{Results and Discussion}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\columnwidth]{footage/overview_anomalies_selection.jpg}
		\caption{Examples of anomalies for different categories, each presented in a group of three images. For each group the original image is displayed on the left. The center image shows the reconstruction while the right image presents the calculated anomaly map, where the anomaly score for each pixel is linearly scaled by a factor of 50. Anomalous regions are displayed in brighter color.}
		\label{fig:overview_anomalies_selection}
	\end{figure}
	
	
	The following section presents the results of our method. In order to give an intuition of how well our approach performs on different tasks, we show the AUC scores for each category while discussing advantages and shortcomings of our model. We compare our method to current publicly available state-of-the-art results on the MVTec AD benchmark. RIAD \cite{ZAVRTANIK2021107706} is directly comparable to our approach, as it also uses an inpainting reconstruction-based method and computes anomaly maps which are also based on GMS. In contrast to our approach, RIAD is trained on an image resolution of  for all categories and does not include a differentiation of image sizes in their pipeline.
	
	Further we consider the embedding-based method Patch-SVDD \cite{Yi_2020_ACCV}. Their patch-based extension of support vector data description has been seen as state-of-the-art on MVTec AD until recently. In our comparison we also include most recent works such as PaDiM \cite{defard2020padim} (see Section \ref{sec:rel_work}) and CutPaste \cite{li2021cutpaste}. We note that PaDiM is based on pretrained CNNs using additional training data and is therefore not directly comparable to our method, however provides a good reference as it is to our knowledge the currently best performing model on MVTec AD both in detection and segmentation.
	The recent work CutPaste uses a special data augmentation strategy to train a one-class classifier in a self-supervised way. CutPaste also offers results using pretrained representations, however we focus on the results without extra training data in accordance to our training procedure. Also note that we use their best performing single model for a fair comparison instead of their ensemble.
	
	Figure \ref{fig:overview_anomalies_selection} shows a selection of qualitative results with reconstructions and anomaly maps. An extended selection can be seen in Figure \ref{fig:overview_anomalies_all_pt1} and \ref{fig:overview_anomalies_all_pt2}.
	
	The results for detection are reported in Table \ref{table:results}.
	For texture categories we observe perfect scores for \textit{grid}, which shows very structured patterns, and \textit{leather}, which exhibits anomalies which are easily detected as our method learns to inpaint the leather pattern sufficiently well. 
	The worst performing category with  AUC is \textit{wood}, which can be attributed to hard test cases with color-based anomalies. Although there is an obvious visual difference between reconstruction and original, it does not lead to a sufficient difference in gradient magnitude. 
	This can be seen in the first row of Figure \ref{fig:not_so_good_results}.
	We perform well on the category \textit{tile} which exhibits heavy random patterns. This makes a good choice of the input image size in our pipeline indispensable as a masked patch may not cover too much of the pattern.
	Aggregated over all texture categories we see an overall improvement of  over Patch-SVDD and  over RIAD. 
	Additionally we can outperform CutPaste and achieve on par results of  AUC with PaDiM while maintaining more restrictive requirements.
	
	For object categories there are two underperforming categories: \textit{Cable} contains many anomalous images where the defect lies in the overall constitution of the product (such as missing pieces). Combined with noise in large areas this makes these anomalies hard to detect via inpainting. 
	Even though Patch-SVDD achieved  AUC most recent methods struggle as well, e.g. CutPaste reports an AUC of .
	Although the defects in \textit{capsule} are per-se easily visible on the generated anomaly maps, our method does not learn the reconstruction of the writing sufficiently well, leading to high anomaly scores also on normal samples. In effect we observe a worse performance in comparison to the other categories.
	An example is shown in Figure \ref{fig:not_so_good_results}.
	
	Our method works well on structured, aligned data such as \textit{toothbrush}, \textit{zipper} and \textit{bottle}. It clearly has difficulties with learning to reconstruct noisy areas such as the heads in \textit{hazelnut}, leading to suboptimal results as seen in the third row of Figure \ref{fig:not_so_good_results}.
	
	In summary on object categories our method performs on par with the current state-of-the-art which is reflected in the overall mean object AUC of  surpassing Patch-SVDD, RIAD and CutPaste.
	
	Averaged over texture and object categories we report  AUC outperforming both Patch-SVDD and RIAD with a difference of  and  respectively.  
	Additionally we can see an absolute improvement of  in comparison to the current state-of-the-art results of CutPaste while PaDiM still achieves the best detection results which might be explained by the additional training data. 
	


	
	The performance on the segmentation task is shown in Table \ref{table:results_segmentation}.
	Our method outperforms the current state-of-the-art methods on all texture categories except \textit{wood} which we attribute to the same reason as in detection above. The same problem also affects \textit{tile}. 
	
	In addition to \textit{cable}, the localization performance for \textit{metal nut} is not on par. The reconstructions for flipped metal nuts are too close to the original, so not every pixel in the object is assigned a high anomaly score.
	Averaged over all categories, our method achieves a mean AUC of  leading to an absolute improvement of  over CutPaste, also outperforming Patch-SVDD and RIAD.
	We can see from the segmentation results that by integrating information from distant patches into the inpainting problem, even large anomalous regions are localized well. 
	
	Using InTra we have been able to obtain both very good detection and localization performance on the MVTec AD benchmark without extra training data while surpassing state-of-the-art approaches on detection and segmentation.
	
	
	\subsection{Ablation Studies}
	
	In the following we examine the influence of certain changes in the architecture. All chosen categories are trained for 200 epochs with settings as described in Section \ref{seq:implementation_details}, if not stated otherwise. As before, all results are reported in ROC AUC, using for evaluation the epoch with best validation loss. We note that training for only 200 epochs for most categories does not lead to results comparable to Table \ref{table:results}.  
	
	\subsubsection{Long residual connections}
	\label{ablation:skip_conns} 
	
	Long residual connections are used to add the output of earlier Transformer blocks to the input of later blocks (see Figure \ref{fig:network_architecture}). We train two object categories (\textit{toothbrush}, which is aligned, and \textit{hazelnut}) and three texture categories without residual connections between the blocks. The results are reported in Table \ref{table:skip_conns}, for examplary qualitative results see Figure \ref{fig:ablation_skipconns}. The aim is to benefit from the already large context of early self-attention in later layers. Comparable to what can be seen in U-Net, generally more structure can be found in the reconstructions when using long residual connections, which is helpful for low-level features. They do not help in noisy areas which are per-se difficult to inpaint due to noisy patterns, such as the head of hazelnuts.
	
	\begin{table}[ht]
		\begin{center}
			\resizebox{\columnwidth}{!}{\begin{tabular}{|c || c | c || c | c |} 
					\hline
					Category & Det. With & Det. Without & Seg. With & Seg. Without \\
					\hline \hline 
					Carpet & 98.8 & 98.8 & 99.2 & {\bf 99.3} \\
					Leather & {\bf 100.0} & 99.9 & {\bf 99.5} & 99.4 \\
					Wood & {\bf 96.8} & 95.6 & {\bf 89.9} & 88.9 \\
					Toothbrush & 99.7 & {\bf 100} & {\bf 99.0} & 98.9 \\
					Hazelnut & 91.0 & {\bf 91.4} & 97.3 & 97.3 \\
					\hline
				\end{tabular}
			}\captionof{table}{AUC score for detection and segmentation for the architecture with and without long residual connections.}\label{table:skip_conns}
		\end{center}
	\end{table}
	
	\subsubsection{Feature Self-Attention}
	\label{ablation:feature_attention}
	
	We examine the effect of using multihead feature self-attention (MFSA) and regular multihead self-attention (MSA) as described in Section \ref{seq:mfsa}. The results are listed in Table \ref{table:feature_sa}. Except in one case, we see no significant quantitative gain for texture categories. Qualitatively, we found that learning a dimension reduction on the normal data before computing self-attention leads to a slighty lower validation loss, more detailed reconstructions and better retouching of defective areas (see Figure \ref{fig:ablation_mfsa}). In effect we see a performance gain for object categories where consistent retouchings of defects have stronger segmentation implications. 
	
	\begin{table}[h]
		\begin{center}
			\resizebox{\columnwidth}{!}{\begin{tabular}{|c || c | c || c | c |} 
					\hline
					Category & Det. MFSA & Det. MSA & Seg. MFSA & Seg. MSA \\
					\hline \hline 
					Carpet & 98.8 & 98.8 & 99.2 & {\bf 99.3} \\
					Leather & 100.0 & 100.0 & 99.5 & 99.5 \\
					Wood & 96.8 & 96.8 & {\bf 89.9} & 89.3 \\
					Toothbrush & {\bf 99.7} & 98.9 & {\bf 99.0} & 98.8 \\
					Hazelnut & {\bf 91.0} & 89.7 & {\bf 97.3} & 96.6 \\
					\hline
				\end{tabular}
			}\captionof{table}{AUC score for detection and segmentation using multihead feauture self-attention (MFSA) and regular multihead self-attention (MSA).}\label{table:feature_sa}
		\end{center}
	\end{table}
	
	\subsubsection{Patch Position Embedding}
	\label{ablation:patch_position_embedding}
	
	We examine the effect of embedding the local position of a patch in a window with  versus embedding the position with respect to the full image with  (see Section \ref{seq:embedding_patches}). For categories such as textures, where the exact position of a patch window is not important, we experienced slightly faster training with . It takes 20 epochs longer until first details in the reconstructions start to show and the training loss drops significantly. There is however no difference in the final quantitative results. This comes as expected, as it takes a while until all global position embeddings have adjusted. Also as expected, the reverse is not true: Using  in cases where the position of a patch within the whole image does carry important information, the performance drops significantly. See Table \ref{table:local_global} for results on two such categories and Figure \ref{fig:ablation_position}, where this effect can be observed qualitatively. 
	
	\begin{table}[h]
		\begin{center}
			\resizebox{\columnwidth}{!}{\begin{tabular}{|c || c | c || c | c |} 
					\hline
					Category & Det. Local & Det. Global & Seg. Local & Seg. Global \\
					\hline \hline 
					Transistor & 80.4 & {\bf 95.8} & 82.6 & {\bf 96.1} \\
					Metal Nut & 86.4 & {\bf 92.5} & 77.4 & {\bf 80.5} \\
					\hline
				\end{tabular}
			}\captionof{table}{AUC score for detection and segmentation using  ("Local") and  ("Global").}\label{table:local_global}
		\end{center}
	\end{table}
	
	\subsubsection{Patch Window Size}
	\label{ablation:patch_window_size}
	
	We test how the side length  of a patch window influences the performance, using . We can see in Table \ref{table:patch_window} that detection and segmentation improve with growing patch windows, as more information from distant pixels can be used for the inpainting task. For a resolution of  pixels, a  window of patches with side length  already covers about one third of the image, and more than half of the image for a  window. This confirms the hypothesis that during inpainting context from distant patches is used, and not only the immediate neigborhood, resulting in improved detection and segmentation capabilities. It comes with high computational cost however, as the length of the patch sequence for the Transformer model is quadratic in , and the computation of the dot product in (\ref{eq:weighing_matrix}) is quadratic in the sequence length.
	
	\begin{table}[h]
		\begin{center}
			\resizebox{\columnwidth}{!}{\begin{tabular}{|c || c | c | c || c | c | c |} 
					\hline
					Category & Det.  & Det.  & Det.  & Seg.  & Seg.  & Seg.  \\
					\hline \hline 
					Hazelnut & 91.1 & 91.0 & {\bf 92.5} & 96.4 & {\bf 97.3} & {\bf 97.3} \\
					Wood & 96.1 & 96.8 & {\bf 97.3} & 89.4 & 89.9 & {\bf 90.3} \\
					\hline
				\end{tabular}
			}\captionof{table}{AUC score for detection and segmentation using different dimensions of the patch window.}\label{table:patch_window}
		\end{center}
	\end{table}
	
	
	\section{Conclusion}
	Inspired by the success of using attention-only methods in vision tasks, we have successfully used a Transformer model for visual anomaly detection by using an inpainting reconstruction approach while considering embeddings of patch sequences as input. We argued that by discarding convolutions and using only self-attention to incorporate global context into reconstructions, anomalies can be successfully detected and localized. 
	As in natural language processing, the position embeddings of patches play a significant role, especially in cases where positional information is important to understand the global structure of objects. For our task, applying a nonlinear dimension reduction before computing self-attention has shown to improve retouching of anomalies. In addition we adopted long residual connections in our architecture to take advantage of low-level features and their context in later Transformer blocks. 
	
	Although Transformer models are usually trained on large amounts of data, our model is trained from scratch on the basis of only 60-400 images, which is sufficient to detect irregularities with a high confidence. 
	Hyperparameters such as the input image size, patch sequence length and patch dimension have a strong impact on the overall performance, and including the detection of good values for them in the training pipeline is paramount. With a simple pipeline as proposed, we have shown that InTra can reach and outperform state-of-the-art results on the popular MVTec AD dataset. With more prior knowledge about potential defects and more computation time, the results might even be improved, possibly by large-scale pretraining.
	Further extensions may include the usage of hidden self-attention output embeddings in the anomaly map generation leading to a hybrid reconstruction-embedding based approach.
	As our anomaly map calculation is fairly basic we propose that a thorough investigation may improve the results even more.
	
	
\bibliographystyle{elsarticle-num}
	\bibliography{intra}
	
	\appendix
	
	\onecolumn
	
	\section{Experiment Details}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=16cm]{footage/overview_anomalies_all_pt1.jpg}
		\caption{Qualitative results on MVTec AD for our method across different categories. Each row shows examples of one category, each column a group of three images with original (left), reconstruction (center), anomaly map (right). The two left columns show examples of anomalous test images, the rightmost column shows an example of a good test image. Categories from top to bottom: \textit{carpet, grid, leather, tile, wood, bottle, cable, capsule}. The rest of the categories are shown in Figure \ref{fig:overview_anomalies_all_pt2}.}
		\label{fig:overview_anomalies_all_pt1}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=16cm]{footage/overview_anomalies_all_pt2.jpg}
		\caption{Examples of qualitative results continued from Figure \ref{fig:overview_anomalies_all_pt1}. Categories from top to bottom: \textit{hazelnut, metal nut, pill, screw, toothbrush, transistor, zipper}.}
		\label{fig:overview_anomalies_all_pt2}
	\end{figure}
	
	\begin{table}[h]
		\begin{center}
			\begin{tabular}{|c c|} 
				\hline
				Category & Size \\
				\hline \hline
				Carpet &  \\
				Grid &  \\
				Leather &  \\
				Tile &  \\
				Wood &  \\
				\hline
				Bottle &  \\
				Cable &  \\
				Capsule &  \\
				Hazelnut &  \\
				Metal Nut &  \\
				Pill &  \\
				Screw &  \\
				Tootbrush &  \\
				Transistor &  \\
				Zipper &  \\
				\hline
			\end{tabular}
			\captionof{table}{Image dimensions used for MVTec AD determined by the loss on the validation set (see Section \ref{seq:implementation_details}).}\label{table:image_sizes}
		\end{center}
	\end{table}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=6cm]{footage/not_so_good_results.jpg}
		\caption{ Examples where our model fails to properly compute anomaly maps.
			In the first row a test sample of wood is shown where color-based anomalies are not detected correctly due to only small differences in gradient  magnitude. Second row: As our model is not able to reconstruct certain text areas sufficiently enough, in certain cases also good samples are assigned strong anomaly maps.
			Third row: Difficult reconstructions of noisy areas such as the head of a hazelnut leads to incorrect results in the anomaly map.}
		\label{fig:not_so_good_results}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=12cm]{footage/ablation_skipconns.jpg}
		\caption{Reconstruction examples using long residual connections. The left column of the first two rows shows two examples from \textit{wood} using long residual connections in the architecture. The right column shows results for the same images without long residual connections. Anomalies are covered slightly better on the left hand side, which shows in the anomaly maps. Third row: Reconstruction examples from \textit{toothbrush}. In both columns from left to right: Original, with long residual connections, without long residual connections. Reconstructions in the center show more detailed low-level structure.}
		\label{fig:ablation_skipconns}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=12cm]{footage/ablation_mfsa.jpg}
		\caption{Reconstructions and anomaly maps from \textit{hazelnut} using different types of self-attention. The defects in the samples stretch over a larger object region. In each row, the left column shows an original, reconstruction and anomaly map when using multihead feature self-attention (MFSA). The right column shows results for the same images using regular multihead self-attention. Anomalies are covered slightly better using MFSA.}
		\label{fig:ablation_mfsa}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=12cm]{footage/ablation_positioning.jpg}
		\caption{Reconstructions and anomaly maps from \textit{transistor} (top row) and \textit{metal nut} (bottom row) using global (left column) and local (right column) position embeddings.}
		\label{fig:ablation_position}
	\end{figure}
	








\end{document}
