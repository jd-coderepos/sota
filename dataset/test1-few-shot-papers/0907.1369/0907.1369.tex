\documentclass [11pt,a4paper]{article}
\usepackage{amsthm,amssymb}
\usepackage[lined,algonl,boxed]{algorithm2e}
\oddsidemargin=0.0in
\voffset=-0.75in
\hoffset=-0.1in
\textwidth=6.5in
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{observation}{Observation}

\bibliographystyle{alpha}
\title{
	\textbf{ \Large Towards an -Approximation Algorithm for \\ {\sc Balanced Separator}}
}
\author{	\large{ Manjish Pal}\\\\
  \small{ Department of Computer Science and Engineering }\\
	\small{ Indian Institute of Technology Kanpur, INDIA. } \\
	\small{{\tt manjish@cse.iitk.ac.in}}\\
}
\date{}

\begin{document}

\pagestyle{empty}

\maketitle

\thispagestyle{empty}

\begin{abstract}
The {\sc -Balanced Separator} problem is a graph-partitioning problem in which given a graph 
, one aims to find a cut of minimum size such that both the sides of the cut
have at least  vertices. In this paper, we present new directions of progress in the {\sc -Balanced Separator} problem. 
More specifically, we propose a new family of mathematical programs, 
which depends upon a parameter , and extend
the seminal work of Arora-Rao-Vazirani ({\sf ARV}) \cite{ARV} to show that the polynomial time solvability 
of the proposed family of programs implies an improvement in the approximation factor
to  from the best-known factor of  due to {\sf ARV}.
In fact, for , the program we get is the SDP proposed by {\sf ARV}.
For , this family of programs is not convex but one can transform them 
into so called \emph{\textbf{concave programs}} in which one optimizes a concave function over a convex feasible set. 
The properties of concave programs allows one to apply techniques due to Hoffman \cite{H81} or Tuy \emph{et al} \cite{TTT85} 
to solve such problems with arbitrary accuracy. But the problem of finding of a method to solve these programs 
that converges in polynomial time still remains open. Our result, although conditional, 
introduces a new family of programs which is more powerful than semi-definite programming 
in the context of approximation algorithms and hence it will of interest to investigate 
this family both in the direction of designing efficient algorithms and proving hardness results.
\end{abstract}

\newpage

\section{Introduction}
Graph partitioning is a problem of fundamental importance both in practice and theory. Many problems belonging 
to the several areas of computer science namely clustering, PRAM emulation, VLSI layout, packet routing in networks
can be modeled as partitioning a graph into two or more parts ensuring that the number of edges in the cut is ``small''. The word ``small'' doesn't refer to finding the min-cut in the graph as it doesn't ensure that the number of vertices in both sides of the cut is large. To enforce this balance condition one needs to normalize the cut-size in some sense. For the known notions of normalization like \emph{conductance}, \emph{expansion} and \emph{sparsity}, finding optimal separators is NP-hard for general graphs. Hence, the objective 
is to look for efficient approximation algorithms. 

Because of the huge amount of work done to design good approximation algorithm for these problems, 
graph partitioning has become one of the central objects of study in the theory of geometric 
embeddings and random walks. The first approximation algorithm for {\sc Graph Conductance}
came out of the study of the Reimannian Manifolds in form of the well known Cheegar's Inequality \cite{C70}
which says that if  is the conductance of the graph and  is the second largest 
eigenvalue of graph Laplacian then . 
Because of the quadratic factor in the lower bound, the true approximation is  which 
in worst case can be  in worst case. The first true approximation algorithm for {\sc Sparsest Cut}
and {\sc Graph Conductance} was designed by Leighton and Rao \cite{LR99} whose approximation factor was . 
This also gave an   \textbf{\emph{pseudo-approximation algorithm}} for {\sc -Balanced Separator}. This
algorithm is referred to as a pseudo-approximation algorithm because 
instead of returning a -balanced cut, it returns a -balanced cut for some fixed  whose 
expansion is at most  times the optimum expansion of best -balanced cut. Their algorithm
was based on an LP framework motivated from the idea of Multi-commodity flows. Their main contribution
was to derive an approximate max-flow min-cut theorem corresponding to multi-commodity flow problem 
and the sparsest cut. Subsequently, a number of results
were discovered which showed that good approximation algorithms exist when one is considering
extreme cases such as the number of edges in the graphs is either very small or very large. In fact,
it is known for planar graphs one can find balanced cuts which are twice as optimal \cite{GSV94} and for 
graph with an average degree of , one can design -factor approximation algorithms
where  with running time polynomial in input size \cite{AKK} (such an algorithm is called a 
\emph{Polynomial Time Approximation Scheme} or PTAS). After 16 years the approximation factor
of  was improved to  in a breakthrough paper by Arora, Rao and Vazirani.
Their algorithm is based on semi-definite relaxations of these problems. 
The techniques and geometric structure theorems proved in their paper has subsequently led to breakthroughs
in the field of metric embeddings. The basic philosophy behind these approximation algorithms is to embed the
vertices of the input graph in an abstract space and derive a nice cut in this space. In the
linear programming approach one uses this abstract space as the  metric \cite{LR99, LLR95, V02}. In the semi-definite 
programming framework used in \cite{ARV} one embeds the vertices on the surface of an -dimensional unit sphere
such that they form an  metric. The  metric on the unit sphere translates into saying that for any three vectors
the angle subtended by any two among these at the third one is acute. 
One of the major tools used in this paper is the phenomenon of measure concentration on unit spheres. 

\textbf{Hardness Results:} Graph partitioning problems like {\sc Sparsest Cut} and {\sc Balanced Separator}
are considered to among the few NP-hard problems which have resisted various attempts to prove inapproximability 
results. After the result of {\sf ARV}, there has been 
a lot of impetus towards proving lower bounds on approximation factors. It has been 
shown by Ambuhl et al \cite{AMS07} that {\sc Sparsest Cut} can't have a PTAS unless NP-complete problems
can be solved in randomized sub-exponential time. Because of the strong connections 
between semi-definite programming and the {\sc Unique Games Conjecture (UGC)} of Khot \cite{Kh02}, certain inapproximability
results are also known which assume UGC. More specifically, Khot and Vishnoi \cite{KV05} show that 
UGC implies super-constant lower bounds on the approximation factor. In the following year,
Devanur et al \cite{DKSV06} showed that the integrality gap of the SDP relaxation of Arora-Rao-Vazirani is 
 thereby disproving the original conjecture of ARV that 
the integrality gap of their SDP relaxation is atmost a constant. This result did not rely upon the {\sc UGC}. 
The recent progress towards designing efficient and good solutions to {\sc Unique Games}
has also been motivated from designing a reduction from {\sc Unique Games} to {\sc Sparsest Cut} \cite{AKK*08}. 

\subsection{Concave Programming}
In order to define \emph{\textbf{Concave Programming}} one first needs to define a concave
function. A concave function is the reverse of a convex function. 
Formally, a function  with domain \textbf{dom} is said to be concave if 
\textbf{dom} is convex and for all  \textbf{dom},  for all . Therefore,  is concave iff  is a convex function. 
Based on this definition one defines concave programming as a form of mathematical programming 
in which one optimizes a concave function over a convex feasible set. 
More formally, a concave programming problem can be written as

where  is a convex set in  and  is a concave function. \\
Concave programming covers a broad range of 
non-linear global optimization problems which includes the well-known
DC(\emph{Difference of Convex Functions}) programming .
Due to its well-structured nature and wide applicability in economic problems as well as various other 
practical problems like allocation-location, water storage, standardization etc. \cite{TTT85},
there has been a lot of work in the field of optimization towards designing algorithms for various concave programming 
problems. One of the key properties of concave programming being exploited in these algorithms 
is a result that says that \emph{for every concave programming problem there is an extreme point
of the convex feasible set  which globally minimizes the optimization problem}. 
The first algorithm for concave programming was designed by Tuy \cite{T64} in a restricted scenario
when the feasible set is a polytope. A more general case, when the feasible set is convex
but not necessarily polyhedral, was solved by Horst \cite{H76} and subsequently by Hoffman \cite{H81}, Tuy and
Thai \cite{TT81}. General concave programming is NP-hard as -integer programming can
be cast as a concave program. There has been work towards designing efficient algorithms 
for some special class of concave programming. It has also been shown that some concave 
programs problems pertaining to Production-Transportation Problems can infact be solved in 
strongly polynomial time \cite{TGMV96}. A comprehensive list of works done in concave programming 
can be found in Vaserstein's homepage \cite{V}. \\
In this work, we introduce the use of a new family of concave programs towards designing
an improved approximation algorithm for the {\sc -Balanced Separator} problem. 
  
\subsection {Contributions and Outline}
In section 2, we formally introduce the notions of sparsity and balanced cuts and
sketch the Semi-Definite relaxation for -{\sc Balanced Separator}
of {\sf ARV}. We then start section 3 by introducing a family of relaxations for -{\sc Balanced Separator} 
which is generated by a parameter . In section 4, using the techniques from \cite{ARV}, 
we show that one can improve the approximation factor to  if
the proposed family of programs can be solved (by solving we mean getting a -approximate answer) 
in polynomial time. Our result, although conditional, proposes new directions of progress on this problem 
and also a family of optimization problems which are more powerful than semi-definite programs 
in the context of approximation algorithms. Then in Section 5 show that one can transform this 
family of programs into a concave program, a form of mathematical programming in which one seeks to minimize 
a concave function over a convex feasible set. There are a number of algorithms which can solve such 
programs with arbitrary accuracy \cite{H81,TTT85}, although one is not guaranteed to
achieve a polynomial time convergence using these algorithms. 
Since this family is a new form of mathematical programming 
that is being used in an approximation algorithm, progress both in the direction of hardness and algorithms 
will provide more insights into the nature of these concave programs and can potentially lead us to 
optimal inapproximability results for various graph-partitioning problems. We end the paper with Section 6
in which we present conclusions and open problems.

\section {Preliminaries}
We now define the two versions of balanced graph partitioning problem namely the 
{\sc Sparsest Cut} and {\sc -Balanced Separator}. It is well known that
upto constant factors approximating other versions of graph partitioning like
like {\sc Graph Conductance} and {\sc Uniform Sparsest Cut} are 
equivalent to approximating the {\sc Sparsest Cut}.
Although in this paper, we will mainly be concerned with the {\sc -Balanced Separator} problem. 

\textbf{{\sc Sparsest Cut}} \\
Given a graph  with , for each cut  define \emph{sparsity} of the cut to be the quantity . The sparsest cut problem is to find  where \\
. \\

\textbf{ -{\sc Balanced Separator}} \footnote{In \cite{ARV} -{\sc Balanced Separator} is defined as 
the minimum sparsity of -balanced cuts, we will be working with a definition which upto constant factors
is equivalent to their definition} \\
Given a graph  with , the -{\sc Balanced Separator} problem is to find  where 
.

\subsection{SDP Relaxation for {\sc -Balanced Separator}}
Unifying the spectral and the metric based (linear programming) approaches, {\sf ARV} used the following SDP
relaxation to get an improved (pseudo)-approximation algorithm
for the -{\sc Balanced Separator}. Let us call this program ,

\begin{center}
\\
\\
 \\

\end{center}

It is easy to see that this indeed is a \emph{vector program} (and hence an SDP)
and is a relaxation for the -{\sc Balanced Separator} 
problem. To show that this is a relaxation we have to show that for every cut we can get an assignment of vectors
such that all the constraints are satisfied and the value of the objective function
is the size of the cut. Given a cut  if one maps all the vertices in 
 to a unit vector  and the vertices in  to  then the value
of the function is indeed the cardinality of . 
The main idea behind their algorithm is to show that for any set of vectors which satisfy the constraints
of the SDP there always exist two disjoint subsets of ``large'' size such that for any two points belonging to 
different subsets the squared Euclidean distance between them is atleast .
The same idea is also used to get an improved approximation algorithm for {\sc Sparsest Cut} in \cite{ARV}.
Subsequently, this key idea has crucially been used in various other SDP based approximation algorithms and in 
solving problems related to metric embeddings.  

\section{A New Relaxation for -{\sc Balanced Separator}}
Consider the following family of optimization problems which depend on a parameter .
This family is essentially an extension of the semi-definite program proposed by {\sf ARV}.
Throughout the paper we will use  to represent the  norm. Let us call this 
family of programs .

\begin{center}
\\
\\
 \\

\end{center}

Note that for  this is the SDP relaxation proposed by {\sf ARV}. For ,               
we are mapping the points onto a unit sphere, therefore
we do not have to force the additional triangle inequality constraint of  metric. 
The same mapping described for  of the vertices of the graph
onto the unit sphere allows us to conclude that each program in this family 
is also a relaxation for {\sc -Balanced Separator}.
We will show that the techniques used in \cite{ARV} for lower bounding the optimum
value of their semi-definite program can be extended in this case as well by
appropriately modifying the ingredients of Theorem 1 of their paper. Under the assumption that we can solve  in polynomial time
for any , , we are able achieve an approximation factor of . 
We first show how to modify the results of \cite{ARV}, thereby reducing the problem of obtaining 
an improved approximation algorithm to that of finding a polynomial time algorithm for solving 
the family of programs mentioned above.

\section{ARV Proof Modifications}
We first modify the definition of -separated sets 
\begin{definition}[-{\sc Separated Sets}]
Two sets of vectors in ,  and  are said to be -separated if for all  and 
. 
\end{definition}

\begin{definition}
For , a set of vectors in  is said to be a unit -spread  representation if they
satisfy the last three constraints in the program .
\end{definition}

Under the new definition of -separated sets the main theorem of {\sf ARV} can 
be modified in the following way:
\begin{theorem}\label{separated}
For every , there are constants  such that every -spread unit-
representation with  points contains -separated subsets  of size , 
where . Also, there is a randomized polynomial-time
algorithm for finding these subsets .
\end{theorem}

This theorem immediately allows us to conclude the following result.
\begin{theorem}
Given a graph , if the program  can be solved in polynomial time for a fixed , then there 
exists a randomized -pseudo approximation algorithm for 
-{\sc Balanced Separator}. 
\end{theorem}

\begin{proof}\emph{(Sketch)}
For a fixed , let  is the optimum solution to the program  with the optimum value
value as . We construct the weighted graph  on the vertex set of the original graph, 
s.t. for every edge  
we impose a weight of . Now apply the algorithm of Theorem \ref{main}
to obtain two subsets  of  of size at least  which are 
-separated. 
Let  be the corresponding sets of vertices in . Pick a number  
randomly uniformly from the range [0-] and report the cut 
as the answer where  is the set of all vertices within distance  from . 
Performing a similar analysis as in Corollary 2 of \cite{ARV}, it is easy to show that with high 
probability  is a -approximate 
-balanced cut.
\end{proof}

\subsection{Proof of Theorem \ref{separated}}

\begin{algorithm}[ht]
\caption{\textbf{{\sc Modified}-Set-Find}}
\KwIn{A set of vectors  in 
which form a unit -spread  representation and parameters  and .}
\KwOut{Two sets  and  which are -separated with the desired balance .}
\begin{itemize}
\item[1.] Pick a random unit vector . 
\item[2.] Let  be the vector that realizes the median of the values taken by 
 for \\
 and let  be the median value. 
\item[3.] Let  be the set of vectors in  satisfying 
and  be the \\
vectors in  which satisfy .
\item[4.] If  or , HALT. Otherwise remove all the vectors  
 \\
and  which satisfy .
\end{itemize}
\textbf{Return}: Remaining sets as  and .
\end{algorithm}

Given a unit -spread  representation of vectors, 
the algorithm to find two -separated sets needs
a small modification over the Set-Find algorithm of {\sf ARV} which is presented as 
{\sc Modified}-\textbf{Set-Find}. Notice that the modification is made at the last step
when the algorithm is discarding pairs.

In order to prove our claim for {\sc Modified}-\textbf{Set-Find} 
we will borrow the definitions of -matching cover, 
-uniform matching cover and -cover directly from \cite{ARV}. Among these
we will only reproduce the definitions of -matching cover and -cover.
The basic idea is that the all these notions of covers do not depend on the triangle inequality of  metric and
hence they also make sense for  representations. 

\begin{definition}
For a set  of vectors, a -matching cover is a set of (partial) matchings 
such that for at least  fraction of directions  there exists a matching  
with size at least  such that for each pair  in the matching 
.
\end{definition}

Let  be the multi-graph obtained by the union of all the matchings in .

\begin{definition}
A set of vectors  is is said to be an -cover if  for all 
and for at least  fraction of the directions there exist   
. A set of vectors if said to -cover 
a point , if the set of vectors  is a -cover.
\end{definition}

The most important thing to note is that the well-separated constraint 
()
is the same for both  and . This
allows us to conclude that Lemmas 3-7 of \cite{ARV} all hold for any set 
of unit -spread  representations  as well. Only Theorem 8 of \cite{ARV}
needs considerable changes which we present as Theorem \ref{main}. For the sake of 
completeness we will reproduce the necessary ingredients used in the proof of 
Theorem 8 of \cite{ARV} namely the definition of -core and Lemma 7. 
But before going into the proof of our version of Theorem 8, let us recall the behavior of 
projection of a random unit vector onto a fixed vector.

\begin{lemma} \label{gaussian}
If  is a vector of length  in  and  is a randomly chosen unit vector 
\begin{itemize}
\item for , .
\item for , .
\end{itemize}
\end{lemma}

\begin{definition}
Given a set of  points about that is  matching covered by  with 
associated matching graph , define  to be in the -core,  if  is -covered
by points which are within  hops of  in the matching graph.
\end{definition}

The following lemma (Lemma 7 of {\sf ARV}) captures an important property of matching covers.
A crucial result used in its proof is Levy's iso-perimetric inequality and measure concentration
on spheres \cite{B97, Mat02}.

\begin{lemma}\label{metric-lemma}
For every set of  points that is -matching covered by  with associated 
matching graph , there are positive constants  and  such that for 
every  one of the following holds: 
\begin{itemize}
\item[1.] .
\item[2.] There is a pair with distance at most  in the matching graph  such that .
\end{itemize}
\end{lemma}

The following lemma can be used to prove the main theorem which shows that the algorithm
{\sc Modified}-\textbf{Set-Find} succeeds with constant probability. 

\begin{theorem}\label{main}
The \mbox{{\sc Modified}-\textbf{Set-Find}} algorithm finds a -separated set for a -spread  representation
with constant probability for  where .
\end{theorem}
\begin{proof}
If {\sc Modified}-\textbf{Set-Find} fails with probability ,
then according to the definition of matching covers it can be shown that,
the set of deleted points will be -matching covered. Let  be the 
associated matching graph. This implies that 
we can use Lemma \ref{metric-lemma} for the set of points. We will show that 
in such a situation both the cases of Lemma \ref{metric-lemma}
do not hold which in turn implies that {\sc Modified}-\textbf{Set-Find} does not fail with high probability. 
We first start with dispensing the case 2 of Lemma \ref{metric-lemma}. Since the points lie in a  metric
and  and  are within -hops in the corresponding matching graph  we will have
 which implies . Now let us 
choose  where  and 
. Under this choice of , 
one can verify that  
which will lead us to a contradiction. 

Now we consider the case 1 of Lemma \ref{metric-lemma}. This says that the number of vectors
which are  covered by points within -hops
of the matching graph , is at least some constant fraction of the total number of points. Consider 
a point  which is in the -core as defined above. Let  be a point that belongs to the 
set that -covers . Now by definition of -core  
with at least probability , .
But because of the fact that the points come from a  metric with in  hops 
. Now using the Gaussian behavior of projections for a 
vector  a randomly chosen unit vector  satisfies, 
. \\
Therefore taking  we get,

If we denote the exponent of  by , then we have
.
Since , we have the exponent of  as
.
Let .
Now one can choose a small constant  such that  for  and , 
which can be done because for a fixed   is a constant and we are going to set .
We therefore get the desired probability to be atmost
. Putting  we get the exponent of  as 
.
Therefore, if we choose , we can get this probability as atmost .
Now, clearly the probability that a vector  is covered by points within -hops of matching graph is atmost the probability
that there exists two points  and  such that for a random unit vector , the above event occurs.
From the above calculation, the probability that such an event occurs for any pair  is 
less that  via the union bound contradicting the condition of Lemma \ref{metric-lemma} that this probability is at least 1/2.
\end{proof}

\textbf{Some Discussion on the Result}: \\
It is not clear whether this method will receive 
benefits from the stronger version of Lemma \ref{metric-lemma} of \cite{ARV} in which they prove that 
for the second case  and use it along with other ideas
so that their algorithm works even for . The main
reason is that if we try to take a  that is of the form chosen in Theorem \ref{main}
then we don't get a dependence of  in terms of  and therefore we don't get a parametrization
of the approximation factor in terms of . Although one might come up with a method such that the above mentioned result 
can also be used to get an improvement over this bound of . One can also ask the 
question, why did we choose to set  as 1
because we could have improved the bound on probability if we had chosen a value greater than 1
but in that case one can easily notice that we would have to sacrifice with the approximation factor
and we would have got value a value of  which is worse than this value.

\section{A Concave Programming Formulation}
In this section, we consider the family of optimization problems  proposed above 
and transform it into a concave program. This formulation allows us to use the algorithms which have been developed
to solve a concave program with arbitrary accuracy. 
We now write  as a program with variables as matrix entries and not as -dimensional
vectors. The variables in the new program are of the form . Since all 's
are unit vectors we can write  as .
If we consider the matrix  with  entry as  we can write the above problem as

\begin{center}
\\
\\
 \\
 \\

\end{center}
where  means  is positive semi-definite. 

As we have seen earlier that in order to get an improved approximation factor 
we must have . Under such a restriction the problem becomes a non-convex 
feasibility problem as the function  is not convex. This 
is a crucial deviation from all the relaxations which have been studied till
now in the context of approximation algorithms. 
Because of the non-convex nature of the problem we can't use any of the 
well known techniques like the ellipsoid method and the interior point methods
and hence can't directly guarantee the polynomial time solvability of the program.
We therefore transform it into a form which allows us to prove some interesting properties. 
In the above program if we do a change of variable,   for all ,
the minimization problem looks as the following:

\begin{center}
\\
 \\
 \\
\\

\end{center}
where \textbf{1} is the matrix with all entries as 1. \\
Let us call the above program .
This formulation allows us to prove the following lemma:

\begin{lemma}\label{concave}
 is a concave program for . 
\end{lemma}
\begin{proof}
Since  is concave for  for , and the sum of concave functions
is also concave, the objective function is clearly concave. For the constraints
defining the feasible set,  and 
are convex. The constraint  can be shown to be convex 
as follows: Let  and  be two matrices corresponding to the variables 's
which lie in the feasible set. Therefore, they satisfy  and 
. Now, consider the line segment for 
 and the matrix . This
is positive semidefinite as it can be rewritten as  
which is a sum of two PSD matrices. \\
The only type of constraint left are the triangle inequality constraints. Consider 
an inequality of this type say .  
In general, let us look at the region  for .
If  for  then this region is same as .
Let  and  be two points which lie in this region, i.e. 
 and 
. To prove 
the convexity of the region we need to show that for any ,

also lies inside the region for all such points  and . Therefore, we have to show
.
Thus we will be done if we show 
.
which is equivalent to proving that the function 
is concave. We will prove this by showing that the Hessian of this function is negative-definite for all . We now
compute the entries of the Hessian matrix. The following calculations are easy to verify,


In order to show that the Hessian is negative-definite we have to show that for any ,
the following expression is always non-positive for all  (for  as 0 the derivatives do not exist):

which is non-positive for all 
This proves that the region  is a convex set for all . Hence the 
intersection of all the triangle inequality constraints is also convex.
\end{proof}

\section{Conclusion}
In this paper, we introduced a new family of mathematical programs inspired from the
well-known semi-definite program of {\sf ARV} that promises a 
 pseudo-approximation 
algorithm for -{\sc Balanced Separator} under the condition that the family 
of programs can be solved in polynomial time. Since this family provably gives better 
approximation guarantees than the celebrated linear and semi-definite relaxations
of Leighton-Rao \cite{LR99} and Arora-Rao-Vazirani \cite{ARV} respectively, 
investigation both in the direction of polynomial time solvability 
or hardness will be highly interesting. The formulation of the proposed family of programs
into a well-structured form of mathematical programming called concave programming
also gives us hope that for many of the problems for which optimal approximation factors
are not known one can possibly rely upon some ``nice'' programs 
which are although not convex but can be potential candidates for 
polynomial time solvability. Given that some algorithms for 
solving concave programs are easy simple to comprehend, it would be also 
interesting to know whether one can analyze their runs on  
and prove polynomial time convergence. Another area of investigation could be investigating the  
links of this family of programs with the {\sc UGC} 
which has been able to show optimality (close to optimality) of various approximation algorithms based on SDP. \\

\section{Acknowledgments}
I would like to thank Prof. Sumit Ganguly and Prof. Shashank K. Mehta
for having some stimulating discussions on the problem. Thanks to Purushottam 
Kar for having discussions at various points during the work.

\begin{thebibliography}{99}

\bibitem{AMS07} C. Ambuhl, M. Mastrolilli and O. Svensson, 
{\it Inapproximability Results for Sparsest Cut, Optimal Linear Arrangement, and Precedence Constrained Scheduling}
{\sf FOCS } 2007, pp. 329-337.

\bibitem{AKK} S. Arora, D. Karger and M. Karpinski,
{\it Polynomial Time Approximation Schemes for Dense Instances of NP-hard Problems}, 
{\sf Proceedings of the 27th ACM Symposium on Theory
Of Computing }, pp. 87-92, 1995.

\bibitem{AKK*08} S. Arora, S. Khot, A. Kolla, D. Steurer, M. Tulsiani and N. Vishnoi, 
{\it Unique Games on Expanding Constraint Graphs are Easy}, {\sf STOC} 2008, pp. 21-28

\bibitem{ARV} S. Arora, S. Rao and U. Vazirani,
{\it Expander Flows, Geometric Embeddings and Graph Partitioning}, {\sf JACM} 56, 2009, pp. 1-37 
(Preliminary version appeared in {\sf ACM STOC}, 2004, pp. 222-231.)

\bibitem{B97} K. Ball, 
{\it An elementary introduction to modern convex geometry, in Flavors of Geometry}, S. Levy (ed.),
{\sf Cambridge University Press}, 1997.

\bibitem{C70} J. Cheeger, {\it A lower bound for the smallest eigenvalue of the Laplacian}, {\sf Problem in Analysis}, 195-199,
{\sf Princeton Univ. Press}, 1970.

\bibitem{DKSV06} N. R. Devanur, S. Khot, R. Saket and N. K. Vishnoi, 
{\it Integrality gaps for sparsest cut and minimum linear arrangement problems}, STOC 2006, pp. 537-546

\bibitem{GSV94} N. Garg, H. Saran, V. V. Vazirani, {\it Finding separator cuts in planar graphs within twice the optimal}, {\sf FOCS} 1994, pp. 14-23

\bibitem{WG95} M.X. Goemans and D. Williamson,
{\it Improved approximation algorithms for maximum cut and satisfiability
problems using semidefinite programming}, {\sf JACM}, 42(6) 1995, pp. 1115-1145.


\bibitem{H81} K. L. Hoffman, {\it A Method for globally minimizing concave functions over convex sets }, {\sf Mathematical Programming}
(20), 1981, pp. 22-32.



\bibitem{H76} R. Horst, {\it An Algorithm for Non-Convex Programming Problem}, 
{\sf Mathematical Programming}(10)-3, 1985, pp. 498-514.

\bibitem{Kh02} S. Khot, {\it On the power of Unique 2-prover 1-round Games} {\sf STOC } 2002, pp. 767-775.

\bibitem{KV05} S. Khot and N. K. Vishnoi, {\it The Unique Games Conjecture, Integrality Gap for Cut Problems and Embeddability of Negative Type Metrics into  }, {\sf FOCS} 2005, pp. 53-62.

\bibitem{LR99}T. Leigton and S. Rao
{\it Multicommodity max-flow min-cut theorems and their use in designing approximation
algorithms}, {\sf JACM} 46 1999, pp. 787-832. Prelim. version in ACM STOC 1988.

\bibitem{LLR95} N. Linial, E. London and U. Rabinovich, {\it The Geometry of graphs and some of its algorithmic applications},
{\sf Combinatoria} (15) 2 1995, pp 215-245.

\bibitem{Mat02} J. Matousek. {\it Lectures on Discrete Geometry}, {\sf Springer Verlag}, 2002.

\bibitem{TTT85} H. Tuy, T. V. Theiu, and Ng. Q. Thai {\it A Conical Algorithm for Globally Minimizing a Concave Function over a Closed Convex Set }, 
{\sf Mathematics of Operation Research}(10)-3, 1985, pp. 498-514.


\bibitem{T64} H. Tuy, {\it Concave Programming under Linear Constraints}, {\sf Dokl. Akad. Nauk} (159), 1964, pp. 32-35. Translated {\sf Soviet Math.} (5), pp. 1437-1440.

\bibitem{TT81} H.Tuy and Ng. Q. Thai,{ \it Minimizing a Concave Function over a Compact Convex Set}, {\sf Proc. Conf. on Optimization Vitte/Hiddensee}, May, 1981.

\bibitem{TGMV96} H. Tuy, S. Ghannadan, A. Migdalas and P. Vabrand, {\it A strongly polynomial algorithm for a concave production-transportation problem with a fixed number of nonlinear variables}, {\sf Mathematical Programming} (72), 1996, pp. 229-258


\bibitem{V02} V. Vazirani, {\it Approximation algorithms}, {\sf Springer Verlag}, 2002.

\bibitem{V} {\it Concave Programming}, {\tt http://www.math.psu.edu/vstein/concave.html}

\end{thebibliography}

\end{document}