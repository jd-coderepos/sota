


\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{multirow}
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2019}

\icmltitlerunning{Self-Attention Graph Pooling}

\begin{document}

\twocolumn[
\icmltitle{Self-Attention Graph Pooling}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Junhyun Lee}{equal,ku}
\icmlauthor{Inyeop Lee}{equal,ku}
\icmlauthor{Jaewoo Kang}{ku}
\end{icmlauthorlist}

\icmlaffiliation{ku}{Department of Computer Science and Engineering, Korea University, Seoul, Korea}

\icmlcorrespondingauthor{Jaewoo Kang}{kangj@korea.ac.kr}
\icmlkeywords{Deep learning, Graph pooling, Graph neural network, Architecture}
\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution{}} 


\begin{abstract}
Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.
\end{abstract}


\section{Introduction}
\label{introduction}
The advent of deep learning has led to extensive improvements in technology used to recognize and utilize patterns in data \cite{lecun2015deep}. In particular, convolutional neural networks (CNNs) successfully leverage the properties of data such as images, speech, and video on Euclidean domains (grid structure)  \cite{hinton2012deep, krizhevsky2012imagenet, he2016deep, karpathy2014large}. CNNs consist of convolutional layers and downsampling (pooling) layers. The convolutional and pooling layers exploit the shift-invariance (also known as stationary) property and compositionality of grid-structured data \cite{simoncelli2001natural, bronstein2017geometric}. As a result, CNNs perform well with a small number of parameters.

In various fields, however, a large amount of data, such as graphs, exists on the non-Euclidean domain. For example, social networks, biological networks, and molecular structures can be represented by nodes and edges of graphs \cite{lazer2009life,davidson2002genomic,duvenaud2015convolutional}. Therefore, attempts have been made to successfully use CNNs in the non-Euclidean domain. Most previous studies have redefined the convolution and pooling layers to process graph data.

To define graph convolution, studies have used the spectral \cite{bruna2014spectral,henaff2015deep,defferrard2016convolutional,kipf2016semi} and non-spectral  \cite{monti2017geometric,hamilton2017inductive,Xu2018RepresentationLO,veličković2018graph,DBLP:journals/corr/abs-1810-02244} methods. The application of graph convolution has resulted in outstanding performance in a variety of fields which include recommender systems \cite{van2017graph, yao2018convolutional,monti2017geometric}, chemical researches \cite{NIPS2018_7877, Zitnik2018}, natural language processing \cite{bastings2017graph, peng2018large, yao2018graph}, and in many tasks as reported in \citeauthor{zhou2018graph}.


There are fewer methods for graph pooling than for graph convolution. Previous researches have adopted the pooling method that considers only graph topology \cite{defferrard2016convolutional, ijcai2018-490}. With growing interest in graph pooling, several improved methods have been proposed \cite{dai2016discriminative, duvenaud2015convolutional,gilmer2017neural,zhang2018end}. They utilize node features to obtain a smaller graph representation. Recently, \citeauthor{RexYing,gao2019graph,cangea2018towards} have proposed innovative pooling methods that can learn hierarchical representations of graphs. These methods allow Graph Neural Networks (GNNs) to attain scaled-down graphs after pooling in an end-to-end fashion.

However, the above pooling methods have room for improvement. For example, the differentiable hierarchical pooling method of \citeauthor{RexYing} has a quadratic storage complexity and the number of its parameters is dependent on the number of nodes. \citeauthor{gao2019graph,cangea2018towards} have addressed the complexity issue, but their method does not take graph topology into account.

Here, we propose SAGPool which is a Self-Attention Graph Pooling method for GNNs in the context of hierarchical graph pooling. Our method can learn hierarchical representations in an end-to-end fashion using relatively few parameters. The self-attention mechanism is exploited to distinguish between the nodes that should be dropped and the nodes that should be retained. 
Due to the self-attention mechanism which uses graph convolution to calculate attention scores, node features and graph topology are considered.
In short, SAGPool, which has the advantages of the previous methods, is the first method to use self-attention for graph pooling and achieve high performance.
The code is available on Github \footnote{\url{https://github.com/inyeoplee77/SAGPool}} 
\section{Related Work}
\label{related}
GNNs have drawn considerable attention due to their state-of-the-art performance on tasks in the graph domain. Studies on GNNs focus on extending the convolution and pooling operation, which are the main components of CNN, to graphs. 

\subsection{Graph Convolution} 
\label{related:graphconvolution}
Convolution operation on graphs can be defined in either the spectral or non-spectral domain. Spectral approaches focus on redefining the convolution operation in the Fourier domain, utilizing spectral filters that use the graph Laplacian. \citeauthor{kipf2016semi} proposed a layer-wise propagation rule that simplifies the approximation of the graph Laplacian using the Chebyshev expansion method \cite{defferrard2016convolutional}.
The goal of non-spectral approaches is to define the convolution operation so that it works directly on graphs. In general non-spectral approaches, the central node aggregates features from adjacent nodes when its features are passed to the next layer rather than defining the convolution operation in the Fourier domain. \citeauthor{hamilton2017inductive} proposed GraphSAGE which learns node embeddings through sampling and aggregation. While GraphSAGE operates in a fixed-size neighborhood, Graph Attention Network (GAT) \cite{veličković2018graph}, based on attention mechanisms \cite{DBLP:journals/corr/BahdanauCB14}, computes node representations in entire neighborhoods. Both approaches have improved performance on graph-related tasks. 

\subsection{Graph Pooling}
Pooling layers enable CNN models to reduce the number of parameters by scaling down the size of representations, and thus avoid overfitting. To generalize CNNs, the pooling method for GNNs is necessary. Graph pooling methods can be grouped into the following three categories: topology based, global, and hierarchical pooling.

\textbf{Topology based pooling}
Earlier works used graph coarsening algorithms rather than neural networks. Spectral clustering algorithms use eigendecomposition to obtain coarsened graphs. However, alternatives were needed due to the time complexity of eigendecomposition. Graclus\cite{graclus} computes clustered versions of given graphs without eigenvectors because of the mathematical equivalence between a general spectral clustering objective and a weighted kernel k-means objective. Even in recent GNN models \cite{defferrard2016convolutional,ijcai2018-490}, Graclus is employed as a pooling module. 




\textbf{Global pooling}
Unlike the previous methods, global pooling methods consider graph features. Global pooling methods use summation or neural networks to pool all the representations of nodes in each layer. Graphs with different structures can be processed because global pooling methods collect all the representations.
\citeauthor{DBLP:journals/corr/GilmerSRVD17} viewed GNNs as message passing schemes, and proposed a general framework for graph classification where entire graph representations could be obtained by utilizing the Set2Set\cite{SET2SET} method. SortPool\cite{zhang2018end} sorts embeddings for nodes according to the structural roles of a graph and feeds the sorted embeddings to the next layers. 


\textbf{Hierarchical pooling}
Global pooling methods do not learn hierarchical representations which are crucial for capturing structural information of graphs. The main motivation of hierarchical pooling methods is to build a model that can learn feature- or topology-based node assignments in each layer. \citeauthor{RexYing} proposed DiffPool which is a differentiable graph pooling method that can learn assignment matrices in an end-to-end fashion. A learned assignment matrix in layer ,  contains the probability values of nodes in layer  being assigned to clusters in the next layer . Here,  denotes the number of nodes in layer . Specifically, nodes are assigned by the following equation:


where  denotes the node feature matrix and  is the adjacency matrix.


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{figure/SAGPool.png}}
\caption{An illustration of the SAGPool layer.}
\label{SAGPool}
\end{center}
\vskip -0.2in
\end{figure*}


\citeauthor{cangea2018towards} utilized gPool\cite{gao2019graph} and achieved performance comparable to that of DiffPool. gPool requires a storage complexity of  whereas DiffPool requires   where , , and  denote vertices, edges, and pooling ratio, respectively. gPool uses a learnable vector  to calculate projection scores, and then uses the scores to select the top ranked nodes. Projection scores are obtained by the dot product between  and the features of all the nodes. The scores indicate the amount of information of nodes that can be retained. The following equation roughly describes the pooling procedure in gPool.


As in Equation (\ref{eq:gpool}), the graph topology does not affect the projection scores. 


To further improve graph pooling, we propose SAGPool which can use features and topology to yield hierarchical representations with a reasonable complexity of time and space.



 \section{Proposed Method}
\label{method}
The key point of SAGPool is that it uses a GNN to provide self-attention scores. In Section \ref{method:SAGP}, we describe the mechanism of SAGPool and its variants. Model architectures for the evaluations are described in Section \ref{method:architecture}. The SAGPool layer and the model architectures are illustrated in Figure \ref{SAGPool} and Figure \ref{model}, respectively.

\subsection{Self-Attention Graph Pooling}
\label{method:SAGP}

\textbf{Self-attention mask} 
Attention mechanisms have been widely used in the recent deep learning studies \cite{parikh2016decomposable, cheng2016long, zhang2018self, veličković2018graph}. Such mechanisms make it possible to focus more on important features and less on unimportant features. In particular, self-attention, commonly referred to as intra-attention, allows input features to be the criteria for the attention itself \cite{vaswani2017attention}. We obtain self-attention scores using graph convolution. For instance, if the graph convolution formula of \citeauthor{kipf2016semi} is used, the self-attention score  is calculated as follows.

where  is the activation function (e.g. ),  is the adjacency matrix with self-connections (i.e. ),  is the degree matrix of ,  is the input features of the graph with  nodes and -dimensional features, and  is the only parameter of the SAGPool layer. By utilizing graph convolution to obtain self-attention scores, the result of the pooling is based on both graph features and topology. We adopt the node selection method of \citeauthor{gao2019graph, cangea2018towards}, which retains a portion of nodes of the input graph even when graphs of varying sizes and structures are inputted. The pooling ratio  is a hyperparameter that determines the number of nodes to keep. The top  nodes are selected based on the value of .

where  is the function that returns the indices of the top  values,  is an indexing operation and   is the feature attention mask. 

\textbf{Graph pooling}
An input graph is processed by the operation notated as \textbf{masking} in Figure \ref{SAGPool}.

where  is the row-wise (i.e. node-wise) indexed feature matrix,  is the broadcasted elementwise product, and  is the row-wise and col-wise indexed adjacency matrix.  and  are the new feature matrix and the corresponding adjacency matrix, respectively. 

\textbf{Variation of SAGPool}
The main reason for using graph convolution in SAGPool is to reflect the topology as well as node features. The various formulas of GNNs can be substituted for Equation (\ref{eq:GCN_att}), if GNNs take the node feature and the adjacency matrix as inputs. The generalized equation for calculating the attention score  is as follows.

where  denotes the node feature matrix and  is the adjacency matrix.

There are several ways to calculate attention scores using not only adjacent nodes but also multi-hop connected nodes. In Equation (\ref{eq:augmentation}) and (\ref{eq:serial_mask}), we illustrate examples of using the two-hop connections which involve the augmentation of edges and the stack of GNN layers. Adding the square of an adjacency matrix creates edges between two-hop neighbors. 

The stack of GNN layers allows for the indirect aggregation of two-hop nodes. In this case, the nonlinearity and the number of parameters of the SAGPool layer increase. 

Equations (\ref{eq:augmentation}) and (\ref{eq:serial_mask}) can be applied to the multi-hop connections.

Another variant is to average multiple attention scores. The average attention score is obtained by  GNNs as follows: 


In this paper, the models using Equation (\ref{eq:augmentation}), (\ref{eq:serial_mask}), and (\ref{eq:parallel_mask}) are referred to as SAGPool, SAGPool , and SAGPool, respectively.


\subsection{Model Architecture}
\label{method:architecture}
According to \citeauthor{lipton2018troubling}, if numerous modifications are made to a model, it may be difficult to identify which modification contributes to improving performance. For a fair comparison, we adopted the model architectures from \citeauthor{zhang2018end} and \citeauthor{cangea2018towards}, and compared the baselines and our method using the same architectures.

\textbf{Convolution layer}
As mentioned in Section \ref{related:graphconvolution}, there are many definitions for graph convolution. Other types of graph convolution may improve performance, but we utilize the widely used graph convolution proposed by \citeauthor{kipf2016semi} for all the models. Equation (\ref{eq:GCN}) is the same as Equation (\ref{eq:GCN_att}), except for the dimension of .

where  is the node representation of -th layer and  is the convolution weight with input feature dimension  and output feature dimension . The Rectified Linear Unit (ReLU) \cite{nair2010rectified} function is used as an activation function.

\textbf{Readout layer}
Inspired by the JK-net architecture \cite{xu2018representation}, \citeauthor{cangea2018towards} proposed a readout layer that aggregates node features to make a fixed size representation. The summarized output feature of the readout layer is as follows:

where  is the number of nodes,  is the feature vector of -th node, and  denotes concatenation.


\begin{figure}[t!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure/model.png}}
\caption{The global pooling architecture (left) and the hierarchical pooling architecture (right). These architectures are applied to all the baselines and SAGPool for a fair comparison. In this paper, the architecture on the left side is referred to as  and the architecture on the right side is referred to as  with the  method (e.g. SAGPool, gPool).}
\label{model}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Global pooling architecture}
We implemented the global pooling architecture proposed by \citeauthor{zhang2018end}. As shown in Figure \ref{model}, the global pooling architecture consists of three graph convolutional layers and the outputs of each layer are concatenated. Node features are aggregated in the readout layer which follows the pooling layer. Then graph feature representations are passed to the linear layer for classification.


\begin{table*}[t!]
\caption{Statistics of data sets.}
\label{tab:statistics}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
\textbf{Data set} & \textbf{Number of Graphs} & \textbf{Number of Classes} & \textbf{Avg. \# of Nodes per Graph} & \textbf{Avg. \# of Edges per Graph}\\
\midrule
D\&D    & 1178 & 2 & 284.32 & 715.66 \\
PROTEINS    & 1113 & 2 & 39.06 & 72.82 \\
NCI1    & 4110 & 2 & 29.87 & 32.30 \\
NCI109    & 4127 & 2 & 29.68 & 32.13 \\
FRANKENSTEIN    & 4337 & 2 & 16.90 & 17.88 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\textbf{Hierarchical pooling architecture}
In this setting, we implemented the hierarchical pooling architecture from the recent hierarchical pooling study of \citeauthor{cangea2018towards}. As shown in Figure \ref{model}, the architecture is comprised of three blocks each of which consists of a graph convolutional layer and a graph pooling layer. The outputs of each block are summarized in the readout layer. The summation of the outputs of each readout layer is fed to the linear layer for classification. 
\section{Experiments}
\label{experiments}
We evaluate the global pooling and hierarchical pooling methods on the graph classification task. In Section \ref{ex:data}, we discuss the datasets used for evaluation. Section \ref{ex:train} describes how we train the models. The methods compared in the experiments are introduced in Sections \ref{ex:baseline} and \ref{ex:variation}.


\begin{table}
\caption{The grid search space for the hyperparameters. The pooling ratio is used only for the hierarchical pooling architecture because the the global pooling architecture uses the same node selection strategy as SortPool. The node selection strategy of SortPool does not require the pooling ratio.}
\label{tab:hyperparameter}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
\textbf{Hyperparameter} & \textbf{Range} \\

\midrule
Learning rate   & 1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4 \\
\midrule
Hidden size  & 16, 32, 64, 128 \\
\midrule
Weight decay  & \multirow{2}{*}{1e-2, 1e-3, 1e-4, 1e-5}\\
(L2 regularization) & \\
\midrule
Pooling ratio & 1/2, 1/4 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\subsection{Datasets}
\label{ex:data}
Five datasets with a large number of graphs (k) were selected among the benchmark datasets \cite{KKMMN2016}. The statistics of the datasets are summarized in Table \ref{tab:statistics}.

\textbf{D\&D} \cite{dobson2003distinguishing, shervashidze2011weisfeiler} contains graphs of protein structures. A node represents an amino acid and edges are constructed if the distance of two nodes is less than 6 . A label denotes whether a protein is an enzyme or non-enzyme. \textbf{PROTEINS} \cite{dobson2003distinguishing, borgwardt2005protein} is also a set of proteins, where nodes are secondary structure elements. If nodes have edges, the nodes are in an amino acid sequence or in a close 3D space. \textbf{NCI} \cite{wale2008comparison} is a biological dataset used for anticancer activity classification. In the dataset, each graph represents a  chemical compound, with nodes and edges representing atoms and chemical bonds, respectively. \textbf{NCI1} and \textbf{NCI109} are commonly used as benchmark datasets for graph classification. \textbf{FRANKENSTEIN} \cite{orsini2015graph} is a set of molecular graphs \cite{costa2010fast} with node features containing continuous values. A label denotes whether a molecule is a mutagen or non-mutagen.

\begin{table*}
\caption{Average accuracy and standard deviation of the 20 random seeds. The subscript  (e.g. ) denotes the global pooling architecture and the subscript  (e.g. ) denotes the hierarchical pooling architecture.}
\label{tab:results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccccc}
\toprule
\textbf{Models} & \textbf{D\&D} & \textbf{PROTEINS} & \textbf{NCI1} & \textbf{NCI109} & \textbf{FRANKENSTEIN} \\

\midrule
Set2Set     &  &  &  &  &  \\
SortPool    &  &  &  &  &  \\
SAGPool (Ours)  &  &  &  &  &   \\
\midrule
DiffPool            &  &  &  &  &   \\
gPool               &  &  &  &  &   \\
SAGPool  (Ours)     &  &  &  &  &   \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\begin{table}
\caption{Experimental results of SAGPool variants. We compare  ChebConv(K=2) \cite{defferrard2016convolutional}, GCNConv \cite{kipf2016semi}, SAGEConv \cite{hamilton2017inductive}, and GATConv(heads=6) \cite{veličković2018graph}. GCNConv is applied to SAGPool, SAGPool, SAGPool, and SAGPool.}
\label{tab:variance}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
\textbf{Graph Convolution} & \textbf{D\&D} & \textbf{PROTEINS} \\
\midrule
SAGPool               &  &   \\
\midrule
SAGPool              &  &   \\
SAGPool              &  &   \\
SAGPool               &  &  \\
\midrule
SAGPool           &  &   \\
SAGPool    &  &   \\
\midrule
SAGPool      &  &   \\
SAGPool      &  &  \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Evaluation of GNNs}  In addition, the same early stopping criterion and hyperparameter selection strategy are used for all the models to ensure a fair comparison.


\subsection{Training Procedures}
\label{ex:train}
\citeauthor{DBLP:journals/corr/abs-1811-05868} demonstrate that different splits of data can affect the performance of GNN models. In our experiments, we evaluated the pooling methods over 20 random seeds using 10-fold cross validation. A total of 200 testing results were used to obtain the final accuracy of each method on each dataset. 10 percent of the training data was used for validation in the training session. We used the Adam optimizer \cite{DBLP:journals/corr/KingmaB14}, early stopping criterion, patience, and hyperparameter selection strategy for the global pooling architecture and hierarchical pooling architecture. We stopped the training if the validation loss did not improve for 50 epochs in an epoch termination condition with a maximum of 100k epochs, as done in \cite{DBLP:journals/corr/abs-1811-05868}. The optimal hyperparameters are obtained by grid search. The ranges of grid search are summarized in Table \ref{tab:hyperparameter}.

\subsection{Baselines}
\label{ex:baseline}
We consider the following four pooling methods as baselines: Set2Set, SortPool, DiffPool, and gPool. DiffPool, gPool, and SAGPool were compared using the hierarchical pooling architecture while Set2Set, SortPool, and SAGPool were compared using the global pooling architecture. We used the same hyperparameter search strategy for all the baselines and SAGPool. The hyperparameters are summarized in Table \ref{tab:hyperparameter}.

\textbf{Set2Set} \cite{SET2SET} requires an additional hyperparameter which is the number of processing steps for the LSTM\cite{lstm} module. We use 10 processing steps for all the experiments. We assume that the readout layer is unnecessary because the LSTM module produces embeddings for graphs invariant to the order of nodes.

\textbf{SortPool} \cite{zhang2018end} is a recent global pooling method which uses sorting for pooling. The  number of nodes is set such that 60\% of graphs have more than  nodes. In the global pooling setting, SAGPool has the same  number of output nodes as SortPool.

\textbf{DiffPool} \cite{RexYing} is the first end-to-end trainable graph pooling method that can produce hierarchical representations of graphs. We did not use batch normalization for DiffPool, which is not related to the pooling method. For the hyperparameter search, the pooling ratio ranges from 0.25 to 0.5 for the following reasons. In the reference implementation, the cluster size is set to 25\% of the maximum number of nodes. DiffPool causes the out of memory error when the pooling ratio is larger  than 0.5.

\textbf{gPool} \cite{gao2019graph} selects top-ranked nodes for pooling, which makes it similar to our method. The comparison between our method and gPool demonstrates that considering topology can help improve performance on the graph classification task.



\subsection{Variations of SAGPool}
\label{ex:variation}
As mentioned in section \ref{method:SAGP}, three variations of SAGPool are used to obtain attention scores . In our experiments, we compared each variant on the two datasets. First, any kind of GNNs can be applied to Equation (\ref{eq:general_mask}). We compared the performance of the three most widely used GNNs (SAGPool, SAGPool, SAGPool).
Second, we made the following modifications to SAGPool so that it can consider the two-hop connection: an edge augmentation (SAGPool) in Equation (\ref{eq:augmentation}) and a stack of GNN layers (SAGPool) in Equation (\ref{eq:serial_mask}). Last, multiple GNNs calculate attention scores and the scores are averaged to obtain the final attention score (SAGPool). We evaluated the performance of  and  using Equation (\ref{eq:parallel_mask}). The results are summarized in Table \ref{tab:variance}.


\begin{figure}[t!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure/plot.pdf}}
\caption{The increase in the number of parameters according to the number of graph nodes. The -axis label denotes the number of input graph nodes and the -axis label denotes the number of parameters of the hierarchical pooling models: the number of input node features is 128, the hidden feature size is 128, and the number of classes is 2. Equation (\ref{eq:GCN_att}) is used as a graph convolution of SAGPool.  denotes the pooling ratio and  indicates that the entire node is preserved after pooling. gPool and SAGPool have a consistent number of parameters regardless of the input graph size and the pooling ratio.}
\label{plot}
\end{center}
\vskip -0.2in
\end{figure}


\subsection{Summary of Results}
\label{ex:summary}
The results are summarized in Table \ref{tab:results} and \ref{tab:variance}. The accuracies and standard deviations are given in percentages. From the comparison of the global pooling methods and SAGPool, the results demonstrate that SAGPool generally performs well, but it performs especially well on D\&D and PROTEINS. In the experiments, SAGPool outperformed the hierarchical pooling methods on all the datasets. We also compared variants of SAGPool with the hierarchical pooling architecture on the two benchmark datasets. The performance of the variants of SAGPool varied. The experimental results of the SAGPool variants show that SAGPool has the potential to improve performance. A detailed analysis of the experimental results is provided in the next section.




 
\section{Analysis}
\label{analysis}
In this section, we provide an analysis of the experimental results. In Section \ref{analysis:global-hierarhcical}, we compare global pooling and hierarchical pooling. Section \ref{analysis:effect-topology} provides an explanation on how the SAGPool method addresses the shortcomings of the gPool method. In the \ref{analysis:sparsity} and \ref{analysis:relation-num-nodes} sections, we compare the efficiency of SAGPool with that of DiffPool. We provide an analysis of SAGPool variants in Section \ref{analysis:variants}.


\subsection{Global and Hierarchical Pooling}
\label{analysis:global-hierarhcical}
It is difficult to determine whether the global pooling architecture or hierarchical pooling architecture is completely beneficial to graph classification. Since the global pooling architecture  (SAGPool, SortPool, Set2Set) minimizes the loss of information, it  performs better than the hierarchical pooling architecture  (SAGPool, gPool, DiffPool) on datasets with fewer nodes (NCI1, NCI109, FRANKENSTEIN).  However,  is more effective on datasets with a large number of nodes (D\&D, PROTEINS) because it efficiently extracts useful information from large scale graphs. Therefore, it is important to use the  pooling architecture that is the most suitable for the given data. Nonetheless, SAGPool tends to perform well with each architecture.


\subsection{Effect of Considering Graph Topology}
\label{analysis:effect-topology}
To calculate the attention scores of nodes, SAGPool utilizes the graph convolution in Equation (\ref{eq:GCN_att}). Unlike gPool, SAGPool uses the  term, which is the first order approximation of the graph Laplacian. This term allows SAGPool to consider graph topology. As shown in Table \ref{tab:results}, considering graph topology improves performance. In addition, the graph Laplacian does not have to be recalculated because it is the term used in a previous graph convolutional layer in the same block. Although SAGPool has the same parameters as gPool (Figure \ref{plot}), it achieves superior performance in the graph classification task.

\subsection{Sparse Implementation}
\label{analysis:sparsity}
Manipulating graph data with a sparse matrix is important for GNNs because the adjacency matrix is usually sparse. When graph convolution is calculated using a dense matrix, the computational complexity of multiplication  is  where  is the adjacency matrix,  is the feature matrix of nodes, and  denotes vertices. Pooling with a dense matrix causes the memory efficiency problem, as mentioned by \cite{cangea2018towards}. However, if a sparse matrix is used in the same operation, the computational complexity is reduced to  where  represents the edges. Since SAGPool is a sparse pooling method, it can reduce its computational complexity, unlike DiffPool which is a dense pooling method. 
Sparseness also affects space complexity. Since SAGPool uses GNN for obtaining attention scores, SAGPool requires 
 of storage for sparse pooling whereas dense pooling methods need .


\subsection{Relation with the Number of Nodes}
\label{analysis:relation-num-nodes}
In DiffPool, the cluster size has to be defined when constructing a model because a GNN produces an assignment matrix  as stated in Equation (\ref{eq:diffpool}). The cluster size has to be proportional to the maximum number of nodes according to the reference implementation. These requirements of DiffPool can lead to two problems. 
First, the number of parameters is dependent on the maximum number of nodes as shown in Figure \ref{plot}. Second, it is difficult to determine the right cluster size when the number of nodes varies greatly. For example, only 10 out of 1178 graphs have over 1000 nodes, where the maximum number of nodes is 5748 and the minimum is 30. The cluster size is 574 if the pooling ratio is 10\%, which expands the size of graphs after pooling for most of the data. On the other hand, in SAGPool, the number of parameters is independent of the cluster size. In addition, the cluster size can be changed based on the number of input nodes.




\subsection{Comparison of the SAGPool Variants}
\label{analysis:variants}
To investigate the potential of our method, we evaluated SAGPool variants on two datasets. SAGPool can be modified to perform the following: changing the type of GNN, considering the two-hop connections, and averaging the attention scores of multiple GNNs. As shown in Table \ref{tab:variance}, the performance on graph classification varies depending on which dataset and type of GNN in SAGPool are used.
We used two techniques to consider two-hop connections. The attention scores obtained by the two sequential GNN layers (SAGPool) reflect the information of two-hop neighbors. 
Another technique is to add the square of an adjacency matrix to itself, resulting in a new adjacency matrix that has two-hop connectivity. Without any modifications to the SAGPool layer, the new adjacency matrix can be processed in SAGPool. The information of two-hop neighbors may help improve performance.
The last variants of SAGPool is to average the attention scores from multiple GNNs. We found that choosing the right  for the dataset can help achieve stable performance.


\subsection{Limitations}
\label{analysis:limit}
We retain a certain percentage (pooling ratio ) of nodes to handle different input graphs of various sizes, which has also been done in previous studies \cite{gao2019graph,cangea2018towards}. 
In SAGPool, we cannot parameterize the pooling ratios to find optimal values for each graph. To address this limitation, we used binary classification to decide which nodes to preserve, but this did not completely solve the issue.
 
\section{Conclusion}
\label{conclusion}
In this paper, we proposed SAGPool which is a novel graph pooling method based on self-attention. Our method has the following features: hierarchical pooling, consideration of both node features and graph topology, reasonable complexity, and end-to-end representation learning. SAGPool uses a consistent number of parameters regardless of the input graph size. 
Extensions of our work may include using learnable pooling ratios to obtain optimal cluster sizes for each graph and studying the effects of multiple attention masks in each pooling layer, where final representations can be derived by aggregating different hierarchical representations.
Our experiments were run on a NVIDIA TitanXp GPU. We implemented all the baselines and SAGPool using PyTorch \cite{paszke2017automatic} and the geometric deep learning extension library provided by \citeauthor{Fey/etal/2018}.
%
 \begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and
  Bengio]{DBLP:journals/corr/BahdanauCB14}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{CoRR}, abs/1409.0473, 2014.
\newblock URL \url{http://arxiv.org/abs/1409.0473}.

\bibitem[Bastings et~al.(2017)Bastings, Titov, Aziz, Marcheggiani, and
  Simaan]{bastings2017graph}
Bastings, J., Titov, I., Aziz, W., Marcheggiani, D., and Simaan, K.
\newblock Graph convolutional encoders for syntax-aware neural machine
  translation.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1957--1967, 2017.

\bibitem[Borgwardt et~al.(2005)Borgwardt, Ong, Sch{\"o}nauer, Vishwanathan,
  Smola, and Kriegel]{borgwardt2005protein}
Borgwardt, K.~M., Ong, C.~S., Sch{\"o}nauer, S., Vishwanathan, S., Smola,
  A.~J., and Kriegel, H.-P.
\newblock Protein function prediction via graph kernels.
\newblock \emph{Bioinformatics}, 21\penalty0 (suppl\_1):\penalty0 i47--i56,
  2005.

\bibitem[Bronstein et~al.(2017)Bronstein, Bruna, LeCun, Szlam, and
  Vandergheynst]{bronstein2017geometric}
Bronstein, M.~M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P.
\newblock Geometric deep learning: going beyond euclidean data.
\newblock \emph{IEEE Signal Processing Magazine}, 34\penalty0 (4):\penalty0
  18--42, 2017.

\bibitem[Bruna et~al.(2014)Bruna, Zaremba, Szlam, and Lecun]{bruna2014spectral}
Bruna, J., Zaremba, W., Szlam, A., and Lecun, Y.
\newblock Spectral networks and locally connected networks on graphs.
\newblock In \emph{International Conference on Learning Representations
  (ICLR2014), CBLS, April 2014}, 2014.

\bibitem[Cangea et~al.(2018)Cangea, Veli{\v{c}}kovi{\'c}, Jovanovi{\'c}, Kipf,
  and Li{\`o}]{cangea2018towards}
Cangea, C., Veli{\v{c}}kovi{\'c}, P., Jovanovi{\'c}, N., Kipf, T., and Li{\`o},
  P.
\newblock Towards sparse hierarchical graph classifiers.
\newblock \emph{arXiv preprint arXiv:1811.01287}, 2018.

\bibitem[Cheng et~al.(2016)Cheng, Dong, and Lapata]{cheng2016long}
Cheng, J., Dong, L., and Lapata, M.
\newblock Long short-term memory-networks for machine reading.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  551--561, 2016.

\bibitem[Costa \& Grave(2010)Costa and Grave]{costa2010fast}
Costa, F. and Grave, K.~D.
\newblock Fast neighborhood subgraph pairwise distance kernel.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, pp.\  255--262. Omnipress,
  2010.

\bibitem[Dai et~al.(2016)Dai, Dai, and Song]{dai2016discriminative}
Dai, H., Dai, B., and Song, L.
\newblock Discriminative embeddings of latent variable models for structured
  data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2702--2711, 2016.

\bibitem[Davidson et~al.(2002)Davidson, Rast, Oliveri, Ransick, Calestani, Yuh,
  Minokawa, Amore, Hinman, Arenas-Mena, et~al.]{davidson2002genomic}
Davidson, E.~H., Rast, J.~P., Oliveri, P., Ransick, A., Calestani, C., Yuh,
  C.-H., Minokawa, T., Amore, G., Hinman, V., Arenas-Mena, C., et~al.
\newblock A genomic regulatory network for development.
\newblock \emph{science}, 295\penalty0 (5560):\penalty0 1669--1678, 2002.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{defferrard2016convolutional}
Defferrard, M., Bresson, X., and Vandergheynst, P.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3844--3852, 2016.

\bibitem[Dhillon et~al.(2007)Dhillon, Guan, and Kulis]{graclus}
Dhillon, I.~S., Guan, Y., and Kulis, B.
\newblock Weighted graph cuts without eigenvectors a multilevel approach.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 29\penalty0 (11), 2007.

\bibitem[Dobson \& Doig(2003)Dobson and Doig]{dobson2003distinguishing}
Dobson, P.~D. and Doig, A.~J.
\newblock Distinguishing enzyme structures from non-enzymes without alignments.
\newblock \emph{Journal of molecular biology}, 330\penalty0 (4):\penalty0
  771--783, 2003.

\bibitem[Duvenaud et~al.(2015)Duvenaud, Maclaurin, Iparraguirre, Bombarell,
  Hirzel, Aspuru-Guzik, and Adams]{duvenaud2015convolutional}
Duvenaud, D.~K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T.,
  Aspuru-Guzik, A., and Adams, R.~P.
\newblock Convolutional networks on graphs for learning molecular fingerprints.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2224--2232, 2015.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{Fey/etal/2018}
Fey, M. and Lenssen, J.~E.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem[Gao \& Ji(2019)Gao and Ji]{gao2019graph}
Gao, H. and Ji, S.
\newblock Graph u-net.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem[Gilmer et~al.(2017{\natexlab{a}})Gilmer, Schoenholz, Riley, Vinyals,
  and Dahl]{DBLP:journals/corr/GilmerSRVD17}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock \emph{CoRR}, abs/1704.01212, 2017{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1704.01212}.

\bibitem[Gilmer et~al.(2017{\natexlab{b}})Gilmer, Schoenholz, Riley, Vinyals,
  and Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1263--1272, 2017{\natexlab{b}}.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1024--1034, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Henaff et~al.(2015)Henaff, Bruna, and LeCun]{henaff2015deep}
Henaff, M., Bruna, J., and LeCun, Y.
\newblock Deep convolutional networks on graph-structured data.
\newblock \emph{arXiv preprint arXiv:1506.05163}, 2015.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, et~al.]{hinton2012deep}
Hinton, G., Deng, L., Yu, D., Dahl, G.~E., Mohamed, A.-r., Jaitly, N., Senior,
  A., Vanhoucke, V., Nguyen, P., Sainath, T.~N., et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal processing magazine}, 29\penalty0 (6):\penalty0
  82--97, 2012.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Karpathy et~al.(2014)Karpathy, Toderici, Shetty, Leung, Sukthankar,
  and Fei-Fei]{karpathy2014large}
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei,
  L.
\newblock Large-scale video classification with convolutional neural networks.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pp.\  1725--1732, 2014.

\bibitem[Kersting et~al.(2016)Kersting, Kriege, Morris, Mutzel, and
  Neumann]{KKMMN2016}
Kersting, K., Kriege, N.~M., Morris, C., Mutzel, P., and Neumann, M.
\newblock Benchmark data sets for graph kernels, 2016.
\newblock URL \url{http://graphkernels.cs.tu-dortmund.de}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{DBLP:journals/corr/KingmaB14}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2014.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:1609.02907}, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Lazer et~al.(2009)Lazer, Pentland, Adamic, Aral, Barabasi, Brewer,
  Christakis, Contractor, Fowler, Gutmann, et~al.]{lazer2009life}
Lazer, D., Pentland, A.~S., Adamic, L., Aral, S., Barabasi, A.~L., Brewer, D.,
  Christakis, N., Contractor, N., Fowler, J., Gutmann, M., et~al.
\newblock Life in the network: the coming age of computational social science.
\newblock \emph{Science (New York, NY)}, 323\penalty0 (5915):\penalty0 721,
  2009.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436, 2015.

\bibitem[Lipton \& Steinhardt(2018)Lipton and Steinhardt]{lipton2018troubling}
Lipton, Z.~C. and Steinhardt, J.
\newblock Troubling trends in machine learning scholarship.
\newblock \emph{arXiv preprint arXiv:1807.03341}, 2018.

\bibitem[Monti et~al.(2017)Monti, Boscaini, Masci, Rodola, Svoboda, and
  Bronstein]{monti2017geometric}
Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein,
  M.~M.
\newblock Geometric deep learning on graphs and manifolds using mixture model
  cnns.
\newblock In \emph{Proc. CVPR}, volume~1, pp.\ ~3, 2017.

\bibitem[Morris et~al.(2018)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{DBLP:journals/corr/abs-1810-02244}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G.,
  and Grohe, M.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock \emph{CoRR}, abs/1810.02244, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.02244}.

\bibitem[Nair \& Hinton(2010)Nair and Hinton]{nair2010rectified}
Nair, V. and Hinton, G.~E.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{Proceedings of the 27th international conference on machine
  learning (ICML-10)}, pp.\  807--814, 2010.

\bibitem[Orsini et~al.(2015)Orsini, Frasconi, and De~Raedt]{orsini2015graph}
Orsini, F., Frasconi, P., and De~Raedt, L.
\newblock Graph invariant kernels.
\newblock In \emph{Proceedings of the Twenty-fourth International Joint
  Conference on Artificial Intelligence}, pp.\  3756--3762, 2015.

\bibitem[Parikh et~al.(2016)Parikh, T{\"a}ckstr{\"o}m, Das, and
  Uszkoreit]{parikh2016decomposable}
Parikh, A., T{\"a}ckstr{\"o}m, O., Das, D., and Uszkoreit, J.
\newblock A decomposable attention model for natural language inference.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2249--2255, 2016.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Peng et~al.(2018)Peng, Li, He, Liu, Bao, Wang, Song, and
  Yang]{peng2018large}
Peng, H., Li, J., He, Y., Liu, Y., Bao, M., Wang, L., Song, Y., and Yang, Q.
\newblock Large-scale hierarchical text classification with recursively
  regularized deep graph-cnn.
\newblock In \emph{Proceedings of the 2018 World Wide Web Conference on World
  Wide Web}, pp.\  1063--1072. International World Wide Web Conferences
  Steering Committee, 2018.

\bibitem[Rhee et~al.(2018)Rhee, Seo, and Kim]{ijcai2018-490}
Rhee, S., Seo, S., and Kim, S.
\newblock Hybrid approach of relation network and localized graph convolutional
  filtering for breast cancer subtype classification.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence, {IJCAI-18}}, pp.\  3527--3534.
  International Joint Conferences on Artificial Intelligence Organization, 7
  2018.
\newblock \doi{10.24963/ijcai.2018/490}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2018/490}.

\bibitem[Shchur et~al.(2018)Shchur, Mumme, Bojchevski, and
  G{\"{u}}nnemann]{DBLP:journals/corr/abs-1811-05868}
Shchur, O., Mumme, M., Bojchevski, A., and G{\"{u}}nnemann, S.
\newblock Pitfalls of graph neural network evaluation.
\newblock \emph{CoRR}, abs/1811.05868, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.05868}.

\bibitem[Shervashidze et~al.(2011)Shervashidze, Schweitzer, Leeuwen, Mehlhorn,
  and Borgwardt]{shervashidze2011weisfeiler}
Shervashidze, N., Schweitzer, P., Leeuwen, E. J.~v., Mehlhorn, K., and
  Borgwardt, K.~M.
\newblock Weisfeiler-lehman graph kernels.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Sep):\penalty0 2539--2561, 2011.

\bibitem[Simoncelli \& Olshausen(2001)Simoncelli and
  Olshausen]{simoncelli2001natural}
Simoncelli, E.~P. and Olshausen, B.~A.
\newblock Natural image statistics and neural representation.
\newblock \emph{Annual review of neuroscience}, 24\penalty0 (1):\penalty0
  1193--1216, 2001.

\bibitem[van~den Berg et~al.(2017)van~den Berg, Kipf, and
  Welling]{van2017graph}
van~den Berg, R., Kipf, T.~N., and Welling, M.
\newblock Graph convolutional matrix completion.
\newblock \emph{stat}, 1050:\penalty0 7, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Veličković et~al.(2018)Veličković, Cucurull, Casanova, Romero,
  Liò, and Bengio]{veličković2018graph}
Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., and Bengio,
  Y.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Vinyals et~al.(2015)Vinyals, Bengio, and Kudlur]{SET2SET}
Vinyals, O., Bengio, S., and Kudlur, M.
\newblock Order matters: Sequence to sequence for sets.
\newblock \emph{arXiv preprint arXiv:1511.06391}, 2015.

\bibitem[Wale et~al.(2008)Wale, Watson, and Karypis]{wale2008comparison}
Wale, N., Watson, I.~A., and Karypis, G.
\newblock Comparison of descriptor spaces for chemical compound retrieval and
  classification.
\newblock \emph{Knowledge and Information Systems}, 14\penalty0 (3):\penalty0
  347--375, 2008.

\bibitem[Xu et~al.(2018{\natexlab{a}})Xu, Li, Tian, Sonobe, ichi Kawarabayashi,
  and Jegelka]{Xu2018RepresentationLO}
Xu, K., Li, C., Tian, Y., Sonobe, T., ichi Kawarabayashi, K., and Jegelka, S.
\newblock Representation learning on graphs with jumping knowledge networks.
\newblock In \emph{ICML}, 2018{\natexlab{a}}.

\bibitem[Xu et~al.(2018{\natexlab{b}})Xu, Li, Tian, Sonobe, Kawarabayashi, and
  Jegelka]{xu2018representation}
Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S.
\newblock Representation learning on graphs with jumping knowledge networks.
\newblock \emph{arXiv preprint arXiv:1806.03536}, 2018{\natexlab{b}}.

\bibitem[Yao \& Li(2018)Yao and Li]{yao2018convolutional}
Yao, K.-L. and Li, W.-J.
\newblock Convolutional geometric matrix completion.
\newblock \emph{arXiv preprint arXiv:1803.00754}, 2018.

\bibitem[Yao et~al.(2018)Yao, Mao, and Luo]{yao2018graph}
Yao, L., Mao, C., and Luo, Y.
\newblock Graph convolutional networks for text classification.
\newblock \emph{arXiv preprint arXiv:1809.05679}, 2018.

\bibitem[Ying et~al.(2018)Ying, You, Morris, Ren, Hamilton, and
  Leskovec]{RexYing}
Ying, R., You, J., Morris, C., Ren, X., Hamilton, W.~L., and Leskovec, J.
\newblock Hierarchical graph representation learning with differentiable
  pooling.
\newblock \emph{CoRR}, abs/1806.08804, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.08804}.

\bibitem[You et~al.(2018)You, Liu, Ying, Pande, and Leskovec]{NIPS2018_7877}
You, J., Liu, B., Ying, Z., Pande, V., and Leskovec, J.
\newblock Graph convolutional policy network for goal-directed molecular graph
  generation.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  6412--6422. Curran Associates,
  Inc., 2018.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Goodfellow, Metaxas, and
  Odena]{zhang2018self}
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.
\newblock Self-attention generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1805.08318}, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Cui, Neumann, and
  Chen]{zhang2018end}
Zhang, M., Cui, Z., Neumann, M., and Chen, Y.
\newblock An end-to-end deep learning architecture for graph classification.
\newblock In \emph{Proceedings of AAAI Conference on Artificial Inteligence},
  2018{\natexlab{b}}.

\bibitem[Zhou et~al.(2018)Zhou, Cui, Zhang, Yang, Liu, and Sun]{zhou2018graph}
Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., and Sun, M.
\newblock Graph neural networks: A review of methods and applications.
\newblock \emph{arXiv preprint arXiv:1812.08434}, 2018.

\bibitem[Zitnik et~al.(2018)Zitnik, Agrawal, and Leskovec]{Zitnik2018}
Zitnik, M., Agrawal, M., and Leskovec, J.
\newblock Modeling polypharmacy side effects with graph convolutional networks.
\newblock \emph{Bioinformatics}, 34\penalty0 (13):\penalty0 457–466, 2018.

\end{thebibliography}


\bibliographystyle{icml2019}
\end{document}
