\documentclass[11pt,draftcls,onecolumn]{IEEEtran}

\newcommand{\vx}{\mathbf x}
\newcommand{\zerob}{\mathbf 0}
\newcommand{\oneb}{\mathbf 1}
\newcommand{\vax}{\hat{\mathbf x}}
\newcommand{\vaa}{\hat{\mathbf a}}
\newcommand{\vaf}{\hat{\mathbf f}}
\newcommand{\vrh}{\hat{\mathbf r}}
\newcommand{\gc}{\widetilde{g}}
\newcommand{\gcc}{\overline{\widetilde{g}}}
\newcommand{\vs}{\mathbf s}
\newcommand{\va}{\mathbf a}
\newcommand{\vc}{\mathbf c}
\newcommand{\vd}{\mathbf d}
\newcommand{\vp}{\mathbf p}
\newcommand{\vDa}{\mathbf \Delta \va}
\newcommand{\MAh}{\hat{\mathbf A}}
\newcommand{\vah}{\hat{\mathbf a}}
\newcommand{\vn}{\mathbf n}
\newcommand{\vb}{\mathbf b}
\newcommand{\vh}{\mathbf h}
\newcommand{\vr}{\mathbf r}
\newcommand{\vg}{\mathbf g}
\newcommand{\vk}{\mathbf k}
\newcommand{\ve}{\mathbf e}
\newcommand{\vf}{\mathbf f}
\newcommand{\vv}{\mathbf v}
\newcommand{\vu}{\mathbf u}
\newcommand{\vm}{\mathbf m}
\newcommand{\vq}{\mathbf q}
\newcommand{\vw}{\mathbf w}
\newcommand{\vy}{\mathbf y}
\newcommand{\vz}{\mathbf z}
\newcommand{\MA}{\mathbf A}
\newcommand{\MB}{\mathbf B}
\newcommand{\MC}{\mathbf C}
\newcommand{\MS}{\mathbf S}
\newcommand{\MD}{\mathbf D}
\newcommand{\MH}{\mathbf H}
\newcommand{\MN}{\mathbf N}
\newcommand{\MM}{\mathbf M}
\newcommand{\MI}{\mathbf I}
\newcommand{\MJ}{\mathbf J}
\newcommand{\ME}{\mathbf E}
\newcommand{\MF}{\mathbf F}
\newcommand{\MG}{\mathbf G}
\newcommand{\MP}{\mathbf P}
\newcommand{\MQ}{\mathbf Q}
\newcommand{\MW}{\mathbf W}
\newcommand{\MR}{\mathbf R}
\newcommand{\MX}{\mathbf X}
\newcommand{\MY}{\mathbf Y}
\newcommand{\MU}{\mathbf U}
\newcommand{\MV}{\mathbf V}
\newcommand{\MZ}{\mathbf Z}
\newcommand{\MXi}{\mathbf{\underline{\Xi}}}
\newcommand{\MDelta}{\mathbf{\underline{\Delta}}}
\newcommand{\MGamma}{\mathbf{{\Gamma}}}
\newcommand{\MPhi}{\mathbf{{\Phi}}}
\newcommand{\vphi}{\boldsymbol{\varphi}}
\newcommand{\MSigma}{\mathbf{{\Sigma}}}
\def\norm{{\bigl | \bigl |}}
\def\Norm{{\Bigl | \Bigl |}}
\newcommand{\define}{{\,\stackrel{\Delta}{=}\,}}



 \usepackage{cite}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500
\usepackage{amssymb, amsfonts, mathrsfs}
\usepackage[tight,footnotesize, FIGTOPCAP]{subfigure}

\hyphenation{ }
\renewcommand{\topfraction}{1}	\renewcommand{\bottomfraction}{1}	\setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     \setcounter{dbltopnumber}{2}    \renewcommand{\dbltopfraction}{1}	\renewcommand{\textfraction}{0}	\renewcommand{\floatpagefraction}{1}	\renewcommand{\dblfloatpagefraction}{0.99}	

\begin{document}
\title{Sparse Vector Distributions and Recovery from Compressed Sensing}


\author{Bob L. Sturm \\ Department of Architecture, Design and Media Technology, Aalborg University Copenhagen, 
Lautrupvang 15, 2750 Ballerup, Denmark}

\maketitle

\begin{abstract}
\noindent It is well known that the performance of sparse vector recovery algorithms from 
compressive measurements can depend on the distribution 
underlying the non-zero elements of a sparse vector.
However, the extent of these effects has yet to be explored,
and formally presented.
In this paper, I empirically investigate this dependence
for seven distributions and fifteen recovery algorithms.
The two morals of this work are: 
1) any judgement of the recovery performance of one algorithm 
over that of another must be prefaced by the conditions
for which this is observed to be true, 
including sparse vector distributions,
and the criterion for exact recovery;
and 2) a recovery algorithm must be selected carefully based on
what distribution one expects to underlie the sensed sparse signal.
\end{abstract}

\section{Introduction}
Several researchers have observed that 
the distribution of the non-zero elements of a sparse vector
can impact its recoverability from its observation, e.g., 
\cite{Tropp2004,Dai2009,Donoho2009,Maleki2010,Qui2010}.
The degrees to which recovery algorithms are affected
by the underlying distribution of a sparse vector
have not been thoroughly investigated, both empirically and analytically,
except perhaps in the case of -regularization \cite{Donoho2009}
and OMP \cite{Jin2008}.
The worst case scenario appears to be 
sparse vectors distributed Bernoulli equiprobable in 
(or ``constant amplitude random sign'') \cite{Dai2009, Maleki2010}.
Maleki and Donoho \cite{Maleki2010} use these types of vectors to
tune three recovery algorithms.
They also briefly investigate the change in 
recovery performance by their tuned algorithms
for sparse vectors distributed Cauchy, uniform in , and Laplacian \cite{Maleki2010};
but this is just to show that vectors distributed
differently than Bernoulli have better recoverability.
Their reasoning goes, if one's sparse vector has a different distribution,
the possibility of recovery from compressive measurements 
will be better than expected.

In this article, I empirically compare the performance
of fifteen algorithms for the recovery of 
compressively sampled sparse vectors 
having elements sampled from seven distributions. 
The algorithms I test can be grouped into four categories: 
greedy iterative descent, thresholding, convex relaxation, and majorization.
The greedy approaches I test are:
orthogonal matching pursuit (OMP) \cite{Pati1993, Tropp2004},
stagewise OMP \cite{Donoho2006},
regularized OMP \cite{Needell2010}, 
and probabilistic OMP (PrOMP) \cite{Divekar2010}.
For the thresholding algorithms,
I test the recommended algorithms produced by Maleki and Donoho \cite{Maleki2010}
--- which includes iterative hard and soft thresholding (IHT, IST) \cite{Blumensath2009}, 
and two-stage thresholding (TST) ---
Approximate Message Passing (AMP) \cite{Donoho2009},
Subspace Pursuit (SP) \cite{Dai2009},
and CoSaMP \cite{Needell2009},
and Algebraic Pursuit with 1-memory (ALPS) \cite{Cevher2011}.
The convex relaxation methods I test inclde
-minimization (BP) \cite{Chen1998},
iteratively reweighted -minimization (IRl1) \cite{Candes2008b},
and Gradient Projection for Sparse Reconstruction (GPSR) \cite{Figueiredo2007}.
Finally, the majorization approach is the smoothed  technique (SL0) \cite{Mohimani2009}.
The seven distributions from which I sample sparse vectors are:
Normal; Laplacian; uniform;
Bernoulli; bimodal Gaussian; bimodal uniform; and bimodal Rayleigh.
I test several pairs of problem sparsities and indeterminacies
for sensing matrices sampled from the uniform spherical ensemble.

These comparisons provide many interesting observations,
many of which are known but yet to be formally stated.
First, I find that there is no one algorithm that
outperforms the others for all distributions I test
when using exact recovery of the full support.
SL0 and BP/AMP however, do appear to be the best in all cases.
Some algorithms are extremely sensitive to the distribution
underlying sparse vectors, and others are not.
Greedy methods and majorization perform better than
-minimization approaches
for vectors distributed with probability density concentrated at zero.
Recovery of Bernoulli distributed vectors shows the lowest rates.
Thus, choosing a recovery algorithm should be
guided by the expected underlying distribution of the sparse vector.
I also find that one can obtain an inflated recovery performance 
with a seemingly reasonable criterion for exact recovery.
For some distributions, such a criterion may not produce results
that accurately reflect the true performance of an algorithm.

In the next section,
I briefly review compressed sensing and my notation.
Then I describe each of the fifteen algorithms that I test.
Section 3 provides the main menu of results 
from my numerous simulations.
Here I look at the effects on performance of perfect recovery criterion,
and the effects of sparse vector distributions.
I also compare all algorithms for each of the
distributions I test through inspecting 
their phase transitions.
I conclude with a summary, and avenues for future research.

\section{Compressed Sensing and Signal Recovery}
Given a vector of ambient dimension , ,
we {\em sense} it by ,
producing a measurement vector .
Several methods have been developed and studied for
recovering  given  and ,
which are obviously sensitive to
both the size and content of  relative to .
The overdetermined problem () has been studied in
statistics, linear algebra, frame theory, and others, e.g., \cite{Christensen2003,Mallat2009}.
The underdetermined problem (), 
with  a {\em sparse} vector,
and  a random matrix,
has been studied in approximation theory \cite{DeVore1998, Mallat2009},
and more recently compressed sensing \cite{Candes2006,Donoho2006b, Tropp2010}.
From here on, we are working with vectors that have ambient dimensions 
larger than the number of measurements, i.e., .
Below, I briefly review the fifteen algorithms I test,
and provide details about their implementations.

\subsection{Notations}
We define the support of a vector  
as the indices of those elements with non-zero values, i.e., 

where  is the th component of the vector,
and .
A vector  is called {\em-sparse} 
when at most  of its entries are nonzero, i.e., .
As the work of this paper is purely empirical,
I only consider finite-energy complex vectors 
in a Hilbert space of dimension .
Here, the inner product of two vectors is defined

where  is the conjugate transpose;
and the -norm for 
of any vector in this space is defined
 

In compressed sensing, the sensing matrix 
maps a length- vector to a lower -dimensional space.
I define  as a matrix of the  columns of  
indexed by the ordered set ,
i.e., 
where  is the first element of .
I notate a set difference as .
Alternatively, one may speak of a {\em dictionary} of {\em atoms},
.
For the problem of recovering the sensed vector from its measurements,
one defines its {\em indeterminacy} as ,
and its {\em sparsity} as .
The problem indeterminacy
describes the undersampling of a vector in its measurements;
and the problem sparsity describes the proportion of the sensing matrix 
active in the measurements.

\subsection{Recovery by Greedy Pursuit}


Greedy pursuits entail the iterative augmentation of a set of 
atoms selected from the dictionary,
and an updating of the residual.
I initialize all the following greedy methods 
by , and ,
and define the stopping criteria to be 
, or ,
unless otherwise specified.

\subsubsection{Orthogonal Matching Pursuit (OMP) \cite{Pati1993}}
OMP augments the th index set 
 by selecting a new index according to

where the th residual is defined
as the projection of the measurements onto the
subspace orthogonal to that spanned by the 
dictionary elements indexed by , i.e.,

where ,
and  is the size- identity matrix.
OMP thereby ensures each residual is orthogonal
to the space spanned by the atoms indexed by .
OMP creates the th solution by

where the  square matrix , and zero elsewhere.
The implementation I use\footnote{{\tt SolveOMP.m} in SparseLab: http://sparselab.stanford.edu/}
involves a QR decomposition to efficiently perform the projection step.

\subsubsection{Probabilistic OMP (PrOMP) \cite{Divekar2010}}
PrOMP augments the th index set 
by sampling from ,
where the set  denotes the true indices of the non-zero entries of ,
and the residual is defined in (\ref{eq:OMPresidual}).
Divekar et al. \cite{Divekar2010} estimates this distribution by

where , and  is the th largest value in 
, .
In this way, PrOMP produces several candidate solutions (\ref{eq:OMPsolution}),
which can include that found by OMP,
and selects the one that produces the smallest residual .
In my implementation, I set , , 
and have PrOMP generate at most  solutions.
I determined these values by experimentation, but not formalized tuning.
I make PrOMP stop generating each solution 
using the same stopping criteria of OMP.



\subsubsection{Regularized OMP (ROMP) \cite{Needell2010}}
ROMP augments the th index set 
 
where  
is a set of indices determined in the following way.
First, ROMP defines the set 

where  is defined in (\ref{eq:OMPresidual}),
and because of this we know .
Next, ROMP finds the set of  disjoint sets  where

From this set, ROMP chooses the best defined by

The solution at a given iteration is defined in (\ref{eq:OMPsolution}).
Note that ROMP requires one to specify the solution sparsity desired.

\subsubsection{Stagewise OMP (StOMP) \cite{Donoho2006}}
StOMP augments the th index set 
 
where

where  is a threshold parameter, 
and  is defined in (\ref{eq:OMPresidual}).
The solution at a given iteration is defined in (\ref{eq:OMPsolution}).
In their work \cite{Donoho2006}, Donoho et al. define , and , 
motivated from either avoiding false alarms (which requires knowing the sparsity of the solution) 
or missed detection (false discovery rate).
I retain the defaults of the implementation I use\footnote{{\tt SolveStOMP.m} 
in SparseLab: http://sparselab.stanford.edu/},
which means the threshold is set by the false discovery rate with .
Donoho et al. recommend running this procedure only 5-10 times,
but here I ran it up to  times which I find does not degrade its performance. 

\subsection{Recovery by Thresholding}
Thresholding techniques entail the iterative refinement of
a solution, and an update of the residual.
I initialize all the following thresholding methods 
by ,
and restrict the number of refinements to , 
or when .

\subsubsection{Iterative Thresholding \cite{Blumensath2009}}
Iterative thresholding refines the solution  according to

where   is a thresholding function applied element-wise to ,
 is a threshold,
 is a relaxation parameter,
and the residual is .
For iterative {\em hard} thresholding (IHT), this function is defined

For iterative {\em soft} thresholding (IST), this function is defined

The implementations of IHT and IST I use 
are the ``recommended versions'' by Maleki and Donoho \cite{Maleki2010},
where they set  for IHT and  for IST,
and adaptively set  
according to a false alarm rate,
the problem indeterminacy,
and an estimate of the variance of the residual.\footnote{See http://sparselab.stanford.edu/OptimalTuning/main.htm}
These settings come from extensive empirical tests 
for sparse signals distributed Bernoulli.


\subsubsection{Compressive Sampling MP (CoSaMP) \cite{Needell2009}}
CoSaMP refines the solution  by two stages of thresholding.
First, given a sparsity  CoSaMP finds the support

where  nulls all elements of  
except for the  ones with the largest magnitudes above .
CoSaMP then thresholds again to find the new support

where  nulls all elements of  
except for the  ones with the largest magnitudes above .
The new solution  is then computed by (\ref{eq:OMPsolution}).
In my implementation of CoSaMP,
I set  to avoid numerical errors;
and in addition to the stopping criterion mentioned above,
I exit the refinements if ,
in which case I choose the previous solution.

\subsubsection{Subspace Pursuit (SP) \cite{Dai2009}}
SP operates in the same manner as CoSaMP,
but instead of retaining the  largest magnitudes in (\ref{eq:CoSaMPT1}),
it keeps only .
The stopping criteria of my implementation of SP
are the same as for CoSaMP.

\subsubsection{Two-stage Thresholding (TST) \cite{Maleki2010}}
Noting the similarity between the two,
Maleki and Donoho generalize CoSaMP and SP into TST.
Given , , and , TST with parameters  finds

where  is a relaxation parameter,
and  nulls all elements of  
except for the  ones with the largest magnitudes.
TST then thresholds again to find the new support

where  nulls all elements of  
except for the  ones with the largest magnitudes.
The new solution  is then computed by (\ref{eq:OMPsolution}).
Obviously, when  and , 
TST becomes similar to CoSaMP;
and to become similar to SP, .
The implementation of TST I use 
is that recommended by Maleki and Donoho \cite{Maleki2010},\footnote{See http://sparselab.stanford.edu/OptimalTuning/main.htm}
where they choose , define , and estimate the sparsity 

These values come from their extensive experiments.
Note that one iteration of SP (or CoSaMP) and one iteration of TST are only equivalent when

Since the thresholding is non-linear, TST will not produce the same results as SP (or as CoSaMP).

\subsubsection{Approximate Message Passing \cite{Donoho2009}}
AMP proceeds as iterative thresholding, 
but with a critical difference in how it defines the residual in (\ref{eq:thresholding}).
Given ,  and  as the th largest magnitude in , 
AMP with soft thresholding defines the -order residual 

where 
.
AMP then refines the solution  by soft thresholding (\ref{eq:softthresholding})

AMP repeats the procedure above until some stopping condition.
In the implementation I use,\footnote{http://people.epfl.ch/ulugbek.kamilov}
I make AMP stop refinement when  or .

\subsubsection{Algebraic pursuit (ALPS) with 1-memory \cite{Cevher2011}}
ALPS essentially entails accelerated iterative hard thresholding with memory.
Given  and a desired sparsity ,
ALPS with 1-memory refines the solution

where , and defining the expanded support set
 

where the optimal step size is given by

In the implementation I use,\footnote{http://lions.epfl.ch/ALPS}
ALPS computes the best weight at each step  according to FISTA \cite{Beck2009}.
Note that for ALPS I specify the correct sparsity, as I do for ROMP, CoSaMP and SP.

\subsection{Recovery by Conxex Relaxation}
The following methods attempt to solve the sparse recovery problem

by replacing the non-convex measure of strict sparsity 
with a relaxed and convex measure.

\subsubsection{-minimization \cite{Chen1998}} 
The principle of Basis Pursuit (BP) replaces strict sense sparsity
 with the relaxed and convex  norm.
We can rewrite (\ref{eq:sparseproblem}) in the following two ways

both of which can be solved by several methods,
e.g., simplex and interior-point methods \cite{Boyd2004},
as a linear program \cite{Chen1998},
and by gradient methods \cite{Figueiredo2007}.
In my implementation, I solve (\ref{eq:l1minimizationeq}) 
using the CVX toolbox \cite{Grant2011}.

\subsubsection{Gradient Projection for Sparse Reconstruction (GPSR) \cite{Figueiredo2007}}
GPSR provides a computationally light and iterative approach to solving
(\ref{eq:l1minimizationlag}), or at least taking one to the neighborhood of the solution,
by using gradient projection, thresholding, and line search.
The details of the implementation are out of the scope of this paper,
but suffice it to say I am using the ``Basic" implementation
provided by the authors \cite{Figueiredo2007} with their defaults,
and .

\subsubsection{Iteratively Reweighted -minimization (IRl1) \cite{Candes2008b}}
Given a diagonal  square matrix ,
IRl1 solves

Using the solution, IRl1 constructs a new weighting matrix

where  is set for stability.
IRl1 then solves (\ref{eq:weightedl1minimization}) with these new weights,
and continues in this way until some stopping criterion is met.
For initialization, .
In my implementation --- which uses CVX as for BP ---
I make  as done in \cite{Candes2008b},
and limit the number of iterations to 4, or until .

\subsubsection{Smoothed  (SL0) \cite{Mohimani2009}}
If we define

then the strict sense sparsity in (\ref{eq:sparseproblem}) can be expressed

For a decreasing set of variances 
for , SL0 finds solutions to

using steepest descent with soft thresholding,
and reprojection back to the feasible set.
Finally, depending on the last , 
SL0 arrives at a solution that will be quite close to the unique sparsest solution.
In the implementation I use provided by the authors,\footnote{http://ee.sharif.ir/SLzero}

where here .
I set , and restrict the last variance to be no larger than .

\section{Computer Simulations}
My problem suite is nearly the same as that used 
in the empirical work of Maleki and Donoho \cite{Maleki2010},
except here I make  instead of ,
and test 50 realizations (instead of 100) of each sparse vector
at each pair of problem sparsity and indetereminacy.\footnote{I am currently
running simulations at the original dimensions used by
Maleki and Donoho, but preliminary results do not show much
difference with those presented below.}
I sample each real sensing matrix from the {\em uniform spherical ensemble},
where each column of  is a point on the -dimensional unit sphere,
and each element of each column of  is 
independently and identically distributed zero-mean Gaussian.

I generate each real -sparse vector in the following way.
First, I determine the indices for the non-zero elements by drawing 
 elements at random (without replacement) from .
Then, for the vector elements at these indices,
I independently sample from the same distribution of the following:
\begin{enumerate}
\item Standard Normal (N)
\item Zero-mean Laplacian (L) , with parameter 
\item Zero-mean uniform (U) in the support 
\item Bernoulli (B) equiprobable in  (``Constant Amplitude Random Sign'' in \cite{Maleki2010})
\item Bimodal Gaussian (BG) as a sum of two unit variance Gaussians with means 
\item Bimodal uniform (BU) as a sum of two uniform distributions in  and 
\item Bimodal Rayleigh (BR), where the magnitude value is distributed Rayleigh with parameter .
\end{enumerate}
I sample from a distribution until I obtain a value with a magnitude greater than .
This ensures that all sparse components have magnitudes 100 times greater
than my specification of numerical precision () in CoSaMP and SP.
Figure \ref{fig:empiricalPDFs} shows the empirical distributions 
of all the signals I compressively sample.

\begin{figure}[t]
\centering
\subfigure{
\includegraphics[width=0.495\textwidth]{dists1_signals.pdf}}\hspace{-0.1in}
\subfigure{
\includegraphics[width=0.485\textwidth]{dists2_signals.pdf}}\\ \vspace{-0.1in}
\caption{Empirical probability distributions of all 
non-zero elements in the sparse signals of my test suite.}
\label{fig:empiricalPDFs}
\end{figure}

The problems I test have 30 different linearly-spaced sparsities .
For each of these, I test 16 different linearly-spaced indeterminacies .\footnote{I am testing a
wider range in my current simulations, but in my opinion, 
the most important part is when a signal is compressed to a high degree.}
For each sparsity and indeterminacy pair then,
I find the proportion of 50 vectors sensed by a  that are recovered 
by the fifteen algorithms I describe above.
Before I test for recovery, I ``debias'' \cite{Figueiredo2007}
each solution in the following way.
I first find the effective support of a solution 

In other words, I find a skinny submatrix of  that has the
largest (full column) rank of  associated with the  largest magnitudes in the solution.
Finally, I produce the debiased solution by solving (\ref{eq:OMPsolution})
with ,
and finally hard thresholding, i.e., 

At each problem indeterminacy,
I interpolate the recovery results over all sparsities to find 
where successful recovery occurs with a probability of 0.5.
This creates an empirical phase transition plot showing the boundary
above which most recoveries fail, and below which most recoveries succeed.

\subsection{Exact Recovery Criteria and Their Effects}
I consider two different criteria for exact recovery.
First, I consider a solution recovered exactly when 

for some .
We can relate () to the stopping criterion
of the algorithms above, i.e., when

with ,
and  is defined by (\ref{eq:OMPsolution}).
We can rewrite this as

and bound it considering the frame bounds of the sensing matrix, i.e., 

with .
Noticing that ,
and ,
we can bound the expression from below by

and thus

From these we see that given (\ref{eq:stoppingcriterion})
the recovery condition () must be true when

In my experiments, I specify  as done in \cite{Maleki2010}, 
and set  (but Maleki and Donoho set the latter ).

The second recovery condition I use is the recovery of the support,
in which case I consider a solution recovered exactly when 

When the criterion (\ref{eq:successcriterion2}) is true,
(\ref{eq:successcriterion1}) is necessarily true by virtue of (\ref{eq:OMPsolution})
with .
We might consider relating this recovery condition
to that in (\ref{eq:successcriterion1}) in the following sense.
Given that only a portion of the true support is recovered,
what are the conditions that criterion (\ref{eq:successcriterion1}) is true?
Consider without loss of generality
that an algorithm has recovered 
the true support except the first  elements, 
i.e., ,
and .
For simplicity I denote 
and .
If we assume that the atoms associated with the elements in 
are orthogonal to the rest of the atoms in the support ,
then we can write the solution as

Using the orthogonality assumption,
we can write the left hand side of (\ref{eq:successcriterion1}) as

as expected.
And thus, we see one way to guarantee condition (\ref{eq:successcriterion1}) is for
.
If we remove the orthogonality assumption, this becomes 


Now, consider that ,
i.e., that we have missed all but one of the elements,
but the recovery algorithm has found and precisely estimated 
the largest element with a magnitude .
Assume that all other non-zero elements have magnitudes 
less than or equal to .
Thus, ;
and .
From the above analysis, we see

We can see that the criterion (\ref{eq:successcriterion1}) is still guaranteed as long as


This analysis shows that we can substantially violate the criterion (\ref{eq:successcriterion2})
and still meet (\ref{eq:successcriterion1}) as long as the distribution of the sparse signal permits it.
For instance, if all the elements of  have unit magnitudes, 
then missing  elements of the support produces
;
and to satisfy (\ref{eq:successcriterion1}) this 
requires .
This is not likely unless  is very large and we miss only a few elements.
If instead our sparse signal is distributed such that it has
only a few extremely large magnitudes and the rest small,
then we can miss much more of the support and still satisfy (\ref{eq:successcriterion1}).

\begin{figure}[tb]
\centering
\subfigure[Normal]{
\includegraphics[width=0.49\textwidth]{normal_phaseSuppvsl2.pdf}}\hspace{-0.1in}
\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{laplacian_phaseSuppvsl2.pdf}}\\
\subfigure[Uniform]{
\includegraphics[width=0.49\textwidth]{uniform_phaseSuppvsl2.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.49\textwidth]{bimodalrayleigh_phaseSuppvsl2.pdf}}
\caption{Differences between the empirical phase transitions for 
five recovery algorithms using criterion ()
and criterion ().
Note different axis scaling in (d).}
\label{fig:Phasedifferencesrecoverycriterion}
\end{figure}

The following experiments test the variability of the phase transitions of
several algorithms depending on these success criteria,
and the distribution underlying the sparse signals.
Figure \ref{fig:Phasedifferencesrecoverycriterion} 
shows the differences in empirical phase transitions of five 
recovery algorithms using () or (\ref{eq:successcriterion1}).
Since () implies (\ref{eq:successcriterion1}),
the empirical phase transition of the latter will always be equal to or greater 
than that of the former.
The empirical phase transitions difference is zero across all problem indeterminacies
when there is no difference between these two criteria.
Figure \ref{fig:Phasedifferencesrecoverycriterion} shows 
a significant dependence of the empirical phase transition
and the success criteria for four sparse vector distributions.
For all other algorithms, and the three other distributions
(Bernoulli, bimodal uniform, and bimodal Gaussian),
the differences are nearly always zero.

I do not completely know the reason why only these five algorithms
out of the 15 I test show significant differences in their empirical phase transitions,
or why GPSR and StOMP appear the most volatile of these algorithms.
It could be that they are better than the others at estimating
the large non-zero components at the expense of modeling the small ones.
However, this experiment clearly reveals that for sparse vectors distributed with
probability density concentrated near zero, criterion (\ref{eq:successcriterion1}) does allow
many small components to pass detection without consequence.
The more probability density is distributed around zero,
the more criterion (\ref{eq:successcriterion1}) is likely to be satisfied
while criterion (\ref{eq:successcriterion2}) is violated.
It is clear from this experiment that we must take caution when
judging the performance of recovery algorithms 
by a success criterion that can be very lax.
In the following experiments,
I use criterion (\ref{eq:successcriterion2}) 
to measure and compare the success of the algorithms
since it also implies (\ref{eq:successcriterion1}).

\subsection{Sparse Vector Distributions and their Effects}
Figures \ref{fig:phasevsdistributions1} and \ref{fig:phasevsdistributions2}
show the variability of the empirical phase transitions for each algorithm
depending on the sparse vector distributions.
The empirical phase transitions of BP and AMP are exactly the same,
and so I only show one in Figure \ref{fig:phasevsdistributions1}(a).
Clearly, the performance of BP, AMP, and recommended TST and IST, 
appear extremely robust  to the distribution underlying sparse vectors.
In \cite{Donoho2009}, Donoho et al., prove AMP to have this robustness.
ROMP also shows a robustness,
but across all indeterminacies its performance
is relatively poor (note the difference in y-axis scaling).
IRl1 and GPSR shows the same robustness to 
Bernoulli and the bimodal distributions;
but they both show a surprisingly poor performance 
for Laplacian distributed vectors, even as the number of measurements increase.
GPSR appears to have poorer empirical phase transitions
as the probability density around zero increases.
I do not yet know why IRl1 fails so poorly for Laplacian vectors.
The specific results for recommended IST, IHT, and TST however, appear very different
to those reported in \cite{Maleki2010},
though the main findings of their work comport with what I show here.

The recovery performance of OMP, PrOMP, and SL0 
varies to the largest degree of all algorithms that I test.
It is clear that these algorithms perform in proportion to
the probability density around zero.
In fact, for eight of the algorithms I test (IHT, ALPS, StOMP, CoSaMP,
SP, OMP, PrOMP, and SL0) we can predict the order of performance
for each distribution by the amount of probability density 
concentrated near zero.
From Fig. \ref{fig:empiricalPDFs} we can see that
these are, in order from most to least concentrated:
Laplacian, Normal, Uniform, 
Bimodal Rayleigh, Bimodal Gaussian, 
Bimodal uniform, and Bernoulli.
This behavior is reversed for only recommended IST, GPSR, and ROMP,
where their performance increases 
the less concentrated probability density is around zero.

\begin{figure}[tb]
\centering
\subfigure[BP, AMP]{
\includegraphics[width=0.49\textwidth]{BPphasevsdist.pdf}}\hspace{-0.1in}
\subfigure[Recommended TST]{
\includegraphics[width=0.49\textwidth]{TSTphasevsdist.pdf}}\\ \vspace{-0.1in}

\subfigure[Recommended IST]{
\includegraphics[width=0.49\textwidth]{ISTphasevsdist.pdf}}\hspace{-0.1in}
\subfigure[IRl1]{
\includegraphics[width= 0.49\textwidth]{IRl1phasevsdist.pdf}}\\ \vspace{-0.1in}

\subfigure[GPSR]{
\includegraphics[width=0.49\textwidth]{GPSRphasevsdist.pdf}}\hspace{-0.1in}
\subfigure[ROMP]{
\includegraphics[width=0.49\textwidth]{ROMPphasevsdist.pdf}}\\ \vspace{-0.1in}

\subfigure[Recommended IHT]{
\includegraphics[width= 0.49\textwidth]{IHTphasevsdist.pdf}}\hspace{-0.1in}
\subfigure[ALPS]{
\includegraphics[width= 0.49\textwidth]{ALPSphasevsdist.pdf}}

\caption{Empirical phase transitions using criterion (\ref{eq:successcriterion2}) 
of nine recovery algorithms for a variety of sparse vector distributions:
Normal (N), Laplacian (L), Uniform (U), Bernoulli (B),
Bimodal Gaussian (BG), Bimodal Uniform (BU), Bimodal Rayleigh (BR).
Note different y-scaling for ROMP in (f).}
\label{fig:phasevsdistributions1}
\end{figure}

\begin{figure}[tb]
\centering
\subfigure[StOMP]{
\includegraphics[width=0.49\textwidth]{StOMPphasevsdist.pdf}}\hspace{-0.1in}
\subfigure[CoSaMP]{
\includegraphics[width= 0.49\textwidth]{CoSaMPphasevsdist.pdf}}\\ \vspace{-0.1in}

\subfigure[SP]{
\includegraphics[width=0.49\textwidth]{SPphasevsdist.pdf}} \hspace{-0.1in}
\subfigure[OMP]{
\includegraphics[width= 0.49\textwidth]{OMPphasevsdist.pdf}} \\ \vspace{-0.1in} 

\subfigure[PrOMP]{
\includegraphics[width=0.49\textwidth]{PrOMPphasevsdist.pdf}}\hspace{-0.1in}
\subfigure[SL0]{
\includegraphics[width= 0.49\textwidth]{SL0phasevsdist.pdf}}

\caption{Empirical phase transitions using criterion (\ref{eq:successcriterion2}) 
of six recovery algorithms for a variety of sparse vector distributions:
Normal (N), Laplacian (L), Uniform (U), Bernoulli (B),
Bimodal Gaussian (BG), Bimodal Uniform (BU), Bimodal Rayleigh (BR).
Note different y-scale in (f).}
\label{fig:phasevsdistributions2}
\end{figure}

\clearpage

\subsection{Comparison of Recovery Algorithms for Each Distribution}
Figure \ref{fig:phasevsalgorithms} shows the same information as
Figs. \ref{fig:phasevsdistributions1} and \ref{fig:phasevsdistributions2},
but compares all fifteen algorithms together for single distributions.
Here we can see that for sparse vectors distributed Bernoulli,
bimodal uniform, and bimodal Gaussian, -minimization methods (BP, IRl1, GPSR)
and the thresholding approach AMP (which uses soft thresholding
to approximate  minimization), perform better than all the
greedy methods and the other thresholding methods, and the majorization SL0.
For the other four distributions,
SL0, OMP and/or PrOMP outperform all the other algorithms I test,
with a significantly higher phase transition
in the case of vectors distributed Laplacian.
In every case, AMP and BP perform the same.
We can also see in all cases the phase transition for SP 
is higher than that for recommended TST, and sometimes much higher, 
even though for Maleki and Donoho's recommended algorithm
they find that the  pair that works best is that
that makes it closest in appearance to SP.
As I discuss about recommended TST above though, 
it is only similar to SP, 
and is not guaranteed to behave the same.
Furthermore, and most importantly,
in my simulations I provide SP (as well as CoSaMP, ROMP and ALPS)
with the exact sparsity of the sensed signal,
while recommended TST instead estimates it.
Finally, in the case of sparse vectors distributed Laplacian, Normal, and Uniform,
we can see that recommended IHT performs better than recommended TST,
and sometimes even BP (Normal and Laplacian), 
which is different from how Maleki and Donoho orders their performance
based on recovering sparse vectors distributed Bernoulli \cite{Maleki2010}.

\begin{figure}[htb]
\centering
\subfigure[Bernoulli]{
\includegraphics[width=0.49\textwidth]{bernoulli_phasevsdistSupp.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Uniform]{
\includegraphics[width=0.49\textwidth]{bimodaluniform_phasevsdistSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Gaussian]{
\includegraphics[width=0.505\textwidth]{bimodalgaussian_phasevsdistSupp.pdf}}\hspace{-0.1in} 
\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.485\textwidth]{bimodalrayleigh_phasevsdistSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Uniform]{
\includegraphics[width=0.485\textwidth]{uniform_phasevsdistSupp.pdf}}\hspace{-0.1in}
\subfigure[Normal]{
\includegraphics[width=0.50\textwidth]{normal_phasevsdistSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{laplacian_phasevsdistSupp.pdf}}

\caption{Comparison of phase transitions of
all algorithms I test for each distribution.
Note different y-scale in (g).}
\label{fig:phasevsalgorithms}
\end{figure}

Figure \ref{fig:bestphase} shows the empirical phase transitions
of the best performing algorithms for each distribution.
BP and AMP perform the same for Bernoulli and bimodal uniform distributions.
For the other five distributions,
SL0 performs the best at larger indeterminacies (),
and PrOMP performs better for these at smaller indeterminacies.
From this graph we also see the large difference
between the recoverability from compressive measurements
of signal distributed Laplacian and Bernoulli.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{phasebest.pdf}
\caption{Empirical phase transitions of the best performing algorithms
for each distribution.}
\label{fig:bestphase}
\end{figure}

\clearpage



\subsection{Effects of Distribution on Probability of Exact Recovery}
Empirical phase transitions only show the regions in -space 
where majority recovery does and does not hold.
Another interesting aspect of these algorithms is how fast this transition occurs,
and to what extents we can expect perfect recovery for all vectors.
Figures \ref{fig:RecoveryProbabilitiesBPPrOMP} -- \ref{fig:RecoveryProbabilitiesStOMPROMP}
compare for pairs of algorithms the probability of exact recovery
as a function of sparsity for several problem indeterminacies
for all distributions I test.

In Fig. \ref{fig:RecoveryProbabilitiesBPPrOMP} we see that the transitions of BP and OMP
are quite similar for all distributions except Normal and Laplacian, 
where OMP takes on a smaller transition slope than BP.
At one extreme, for Bernoulli distributed signals,
BP can perfectly recover signals with  ( for ),
while OMP can only recover all signals with  ( for ).
For Laplacian distributed signals, however,
these are reversed, with BP perfectly recovering signals with  ( for ),
while OMP recovers all signals up to  ( for ).

\begin{figure}[htb]
\centering
\subfigure[Bernoulli]{
\includegraphics[width=0.49\textwidth]{BPPrOMP_bernoulli_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Uniform]{
\includegraphics[width=0.49\textwidth]{BPPrOMP_bimodaluniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Gaussian]{
\includegraphics[width=0.49\textwidth]{BPPrOMP_bimodalgaussian_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Uniform]{
\includegraphics[width=0.49\textwidth]{BPPrOMP_uniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.49\textwidth]{BPPrOMP_bimodalrayleigh_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Normal]{
\includegraphics[width=0.49\textwidth]{BPPrOMP_normal_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{BPPrOMP_laplacian_probSupp.pdf}}
\caption{Probability of exact recovery using criterion (\ref{eq:successcriterion2}) 
for BP (solid) and PrOMP (dashed) as a function of problem sparsity
and six problem indeterminacies from thickest to thinest lines: 
.}
\label{fig:RecoveryProbabilitiesBPPrOMP}
\end{figure}

Figure \ref{fig:RecoveryProbabilitiesSL0SP} compares the transition
of probability for SL0 and SP.
They are quite similar for signals distributed 
Bernoulli, bimodal uniform, and bimodal Gaussian.
For all other distributions I test they are much less similar;
and perfect recovery for SL0 for Laplacian distributed signals
extends well beyond that of all the other algorithms.

\begin{figure}[htb]
\centering
\subfigure[Bernoulli]{
\includegraphics[width=0.49\textwidth]{SL0SP_bernoulli_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Uniform]{
\includegraphics[width=0.49\textwidth]{SL0SP_bimodaluniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Gaussian]{
\includegraphics[width=0.49\textwidth]{SL0SP_bimodalgaussian_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Uniform]{
\includegraphics[width=0.49\textwidth]{SL0SP_uniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.49\textwidth]{SL0SP_bimodalrayleigh_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Normal]{
\includegraphics[width=0.49\textwidth]{SL0SP_normal_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{SL0SP_laplacian_probSupp.pdf}}
\caption{Probability of exact recovery using criterion (\ref{eq:successcriterion2}) 
for SL0 (solid) and SP (dashed) as a function of problem sparsity
and six problem indeterminacies from thickest to thinest lines: 
.
Note change in y-scale in (f) and (g).}
\label{fig:RecoveryProbabilitiesSL0SP}
\end{figure}

In Fig. \ref{fig:RecoveryProbabilitiesCoSaMPTST} we see that 
the transitions of probability for CoSaMP and TST are quite similar for 
more distributions except Bernoulli,
and quite different from those of the other ``two-stage thresholding'' algorithm SP in
Fig. \ref{fig:RecoveryProbabilitiesSL0SP}.
Both CoSaMP and TST have very steep transitions
showing that the boundary between perfect recovery
and majority recovery is quite narrow.
A curious thing is that for most distributions I test, 
CoSaMP fails completely at .
In the case of Laplacian signals,
it is quite clear that for CoSaMP this sparsity 
cannot be exceeded no matter .
I do not know at this time what causes this behavior.

\begin{figure}[htb]
\centering
\subfigure[Bernoulli]{
\includegraphics[width=0.49\textwidth]{CoSaMPTST_bernoulli_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Uniform]{
\includegraphics[width=0.49\textwidth]{CoSaMPTST_bimodaluniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Gaussian]{
\includegraphics[width=0.49\textwidth]{CoSaMPTST_bimodalgaussian_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Uniform]{
\includegraphics[width=0.49\textwidth]{CoSaMPTST_uniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.49\textwidth]{CoSaMPTST_bimodalrayleigh_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Normal]{
\includegraphics[width=0.49\textwidth]{CoSaMPTST_normal_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{CoSaMPTST_laplacian_probSupp.pdf}}
\caption{Probability of exact recovery using criterion (\ref{eq:successcriterion2}) 
for CoSaMP (solid) and TST (dashed) as a function of problem sparsity
and six problem indeterminacies from thickest to thinest lines: 
.}
\label{fig:RecoveryProbabilitiesCoSaMPTST}
\end{figure}

For the two hard thresholding approaches,
Fig. \ref{fig:RecoveryProbabilitiesIHTALPS}
compares the transitions for IHT and ALPS.
These more than any other, have quite irregular
transitions for all distributions but Bernoulli and bimodal Uniform.
We see for ALPS that only for these distributions
can we expect perfect recovery for some sparsity.
In all the others, ALPS never reaches 100\% recovery,
which is extremely problematic.
The recommended IHT of Maleki and Donoho \cite{Maleki2010}
suffers no such problem, even though
it must estimate the sparsity of the signal,
while ALPS is given that information.

\begin{figure}[htb]
\centering
\subfigure[Bernoulli]{
\includegraphics[width=0.49\textwidth]{IHTALPS_bernoulli_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Uniform]{
\includegraphics[width=0.49\textwidth]{IHTALPS_bimodaluniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Gaussian]{
\includegraphics[width=0.49\textwidth]{IHTALPS_bimodalgaussian_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Uniform]{
\includegraphics[width=0.49\textwidth]{IHTALPS_uniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.49\textwidth]{IHTALPS_bimodalrayleigh_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Normal]{
\includegraphics[width=0.49\textwidth]{IHTALPS_normal_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{IHTALPS_laplacian_probSupp.pdf}}
\caption{Probability of exact recovery using criterion (\ref{eq:successcriterion2}) 
for CoSaMP (solid) and TST (dashed) as a function of problem sparsity
and six problem indeterminacies from thickest to thinest lines: 
.}
\label{fig:RecoveryProbabilitiesIHTALPS}
\end{figure}

\clearpage 

Figure \ref{fig:RecoveryProbabilitiesStOMPROMP}
shows that StOMP has transition regions that are
similar to those of ALPS, but over all distributions. 
We also see why in Fig. \ref{fig:phasevsalgorithms} StOMP has zero phase transition
at low indeterminacies for vectors distributed bimodal Gaussian, 
bimodal Rayleigh, uniform, and Normal.
I do not yet know the reason for these behaviors,
but it could be due to the necessity to tune
the false alarm rate of StOMP.

\begin{figure}[htb]
\centering
\subfigure[Bernoulli]{
\includegraphics[width=0.49\textwidth]{StOMPROMP_bernoulli_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Uniform]{
\includegraphics[width=0.49\textwidth]{StOMPROMP_bimodaluniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Gaussian]{
\includegraphics[width=0.49\textwidth]{StOMPROMP_bimodalgaussian_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Uniform]{
\includegraphics[width=0.49\textwidth]{StOMPROMP_uniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.49\textwidth]{StOMPROMP_bimodalrayleigh_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Normal]{
\includegraphics[width=0.49\textwidth]{StOMPROMP_normal_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{StOMPROMP_laplacian_probSupp.pdf}}
\caption{Probability of exact recovery using criterion (\ref{eq:successcriterion2}) 
for StOMP (solid) and ROMP (dashed) as a function of problem sparsity
and six problem indeterminacies from thickest to thinest lines: 
.}
\label{fig:RecoveryProbabilitiesStOMPROMP}
\end{figure}

Finally, Fig. \ref{fig:RecoveryProbabilitiesIRl1GPSR} compares 
the transitions of IRl1 and GPSR.
We see that the two are quite similar for vectors distributed
Bernoulli, bimodal uniform, and bimodal Gaussian;
but GPSR begins to break down as the probability density
becomes more concentrated around zero.
IRl1 has behavior that is inconsistent with what I would predict.
For the most part, it acts fine for Normally distributed vectors,
but its performance is very poor
for vectors distributed uniform and especially Laplacian.
At this time I do not know what troubles IRl1
for these distributions,
or why it cannot perfectly recover Normal vectors with low sparsity,
but it can recovery vectors with higher sparsity.

\begin{figure}[htb]
\centering
\subfigure[Bernoulli]{
\includegraphics[width=0.49\textwidth]{IRl1GPSR_bernoulli_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Bimodal Uniform]{
\includegraphics[width=0.49\textwidth]{IRl1GPSR_bimodaluniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Gaussian]{
\includegraphics[width=0.49\textwidth]{IRl1GPSR_bimodalgaussian_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Uniform]{
\includegraphics[width=0.49\textwidth]{IRl1GPSR_uniform_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Bimodal Rayleigh]{
\includegraphics[width=0.49\textwidth]{IRl1GPSR_bimodalrayleigh_probSupp.pdf}}\hspace{-0.1in}
\subfigure[Normal]{
\includegraphics[width=0.49\textwidth]{IRl1GPSR_normal_probSupp.pdf}}\\ \vspace{-0.1in}

\subfigure[Laplacian]{
\includegraphics[width=0.49\textwidth]{IRl1GPSR_laplacian_probSupp.pdf}}
\caption{Probability of exact recovery using criterion (\ref{eq:successcriterion2}) 
for IRl1 (solid) and GPSR (dashed) as a function of problem sparsity
and six problem indeterminacies from thickest to thinest lines: 
.}
\label{fig:RecoveryProbabilitiesIRl1GPSR}
\end{figure}

\clearpage











\section{Conclusion}
In this work I show that any judgement of which algorithm among several
is the best for recovering compressively sensed sparse vectors
must be prefaced by the conditions for which this is observed to be true.
It is clear from my computer simulations
that the performance of a recovery algorithm 
within the context of compressed sensing can be
greatly affected by the distribution underlying the sensed sparse vector,
and that the summary of performance is highly dependent on
the criterion of successful recovery.
These ``findings'' are certainly nothing novel,
and clearly not controversial.
It has already been stated in numerous pieces of research,
and is somewhat codified as ``folk knowledge'' in the 
compressed sensing research community,
that recovery algorithms are sensitive to the nature of a sparse vector,
and that sparse vectors distributed Bernoulli
appear to be the hardest to recover, e.g., \cite{Jin2008, Tropp2004,Dai2009,Donoho2009,Maleki2010,Qui2010}.
The extents to which this is true and meaningful,
and the variability of performance to the criterion
of successful recovery, however,
had yet to be formally and empirically studied and presented.

In light of this work, 
the important thing to ask moves from
what recovery algorithm is the best,
to what distribution underlies a compressively sensed vector,
{\em and} what is the measure of success.
In my experiments, we see that SL0 performs extremely well 
in the sense of recovering the support (and thus the sparse vector exactly)
for five distributions I test,
except for low indeterminacies of Laplacian, Normal, bimodal Rayleigh, 
uniform, and bimodal Gaussian, where the greedy approach PrOMP performs slightly better.
I do not yet know the reason for this,
but it could be that the parameters for SL0 must be adjusted.
SL0 does not perform as well as BP/AMP for 
sparse vectors distributed Bernoulli or bimodal uniform.
With their performance, and because of their speed, 
SL0 and AMP are together extremely attractive algorithms 
for recovering sparse vectors distributed in any of these seven ways.

A critical question to answer is how these results 
change when the sensed vector is corrupted by noise,
or when the sensing matrix is something other than
from the uniform spherical ensemble.
One potential problem with using the algorithms
tuned by Maleki and Donoho \cite{Maleki2010}
is that they are tuned to sparse vectors distributed Bernoulli.
It could be possible that they perform better for vectors distributed Laplacian
if they are tuned to such a distribution; however,
Maleki and Donoho argue that tuning on Bernoulli sparse vectors
is essentially maximizing the best performance for the worst case,
and that this translates to situations that are more forgiving.
In my experiments, I do see that recommended IHT can perform better
than recommended TST for sparse vectors distributed uniform, 
Normal, and Laplacian,
which subverts their ordering of their algorithms in terms of performance.
It stands to be reasoned then that we can better tune
these algorithms for those situations such that
recommended TST does outperform recommended IHT.
Finally, it is important to measure the performance of these algorithms
for real-world signals that are sparsely described only in 
coherent dictionaries \cite{Candes2010}.

\bibliographystyle{IEEEtran}
\bibliography{../bibliographies/BibAnnon}

\end{document}
