\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{bm}
\graphicspath{{Figures/}}
\usepackage{subfigure}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\tabincell}[2]{
\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage[flushleft]{threeparttable}




\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \cvprfinalcopy 

\def\cvprPaperID{3436} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN}

\author{
  Shuai Li \textsuperscript{*}, Wanqing Li \textsuperscript{*}, Chris Cook \textsuperscript{*}, Ce Zhu \textsuperscript{\dag}, Yanbo Gao \textsuperscript{\dag}\\
\textsuperscript{*}School of Computing and Information Technology, University of Wollongong\\
\textsuperscript{\dag}School of Electronic Engineering, University of Electronic Science and Technology of China\\
  \tt\small {\{sl669,wanqing,ccook\}@uow.edu.au,eczhu@uestc.edu.cn,yanbogao@std.uestc.edu.cn}}

\maketitle



\begin{abstract}
Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over  time steps), can be used to construct very deep networks ( layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM. {\color{red}{The code is available at \url{https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne.}}}
\end{abstract}

\section{Introduction}

Recurrent neural networks (RNNs) \cite{jordan1997serial} have been widely used in sequence learning problems such as action recognition \cite{donahue2015long}, scene labelling \cite{byeon2015scene} and language processing \cite{cho2014learning}, and have achieved impressive results. Compared with the feed-forward networks such as the convolutional neural networks (CNNs), a RNN has a recurrent connection where the last hidden state is an input to the next state. The update of states can be described as follows:

where  and  are the input and hidden state at time step , respectively. ,  and  are the weights for the current input and the recurrent input, and the bias of the neurons.  is an element-wise activation function of the neurons, and  is the number of neurons in this RNN layer.

Training of the RNNs suffers from the gradient vanishing and exploding problem due to the repeated multiplication of the recurrent weight matrix. Several RNN variants such as the long short-term memory (LSTM) \cite{greff2017lstm,jozefowicz2015empirical} and the gated recurrent unit (GRU) \cite{cho2014learning} have been proposed to address the gradient problems. However, the use of the hyperbolic tangent and the sigmoid functions as the activation function in these variants results in gradient decay over layers. Consequently, construction and training of a deep LSTM or GRU based RNN network is practically difficult. By contrast, existing CNNs using non-saturated activation function such as relu can be stacked into a very deep network (e.g. over 20 layers using the basic convolutional layers and over 100 layers with residual connections \cite{he2016deep}) and be still trained efficiently. Although residual connections have been attempted for LSTM models in several works \cite{wu2016google,pradhanexploring}, there have been no significant improvement (mostly due to the reason that gradient decays in LSTM with the use of the hyperbolic tangent and the sigmoid functions as mentioned above).

Moreover, the existing RNN models share the same component  in (\ref{RNN}), where the recurrent connection entangles all the neurons. This makes it hard to interpret and understand the roles of the trained neurons (e.g., what patterns each neuron responds to) since the simple visualization of the outputs of individual neurons~\cite{karpathy2015visualizing} is hard to ascertain the function of one neuron without considering the others.

In this paper, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed. In the proposed IndRNN, the recurrent inputs are processed with the Hadamard product as . This provides a number of advantages over the traditional RNN including:

\begin{itemize}
\item The gradient backpropagation through time can be regulated to effectively address the gradient vanishing and exploding problems.
\item Long-term memory can be kept with IndRNNs to process long sequences. Experiments have demonstrated that an IndRNN can well process sequences over  steps while LSTM could only process less than  steps.
\item An IndRNN can work well with non-saturated function such as relu as activation function and be trained robustly. 
\item Multiple layers of IndRNNs can be efficiently stacked, especially with residual connections over layers, to increase the depth of the network. An example of 21 layer-IndRNN is demonstrated in the experiments for language modelling.
\item Behaviour of IndRNN neurons in each layer are easy to interpret due to the independence of neurons in each layer.
\end{itemize}
Experiments have demonstrated that IndRNN performs much better than the traditional RNN and LSTM models on the tasks of the adding problem, sequential MNIST classification, language modelling and action recognition.

\vspace{0.5cm}
\section{Related Work}
To address the gradient exploding and vanishing problems in RNNs, variants of RNNs have been proposed and typical ones are the long short-term memory (LSTM) \cite{hochreiter1997long}, and the gated recurrent unit (GRU) \cite{cho2014learning}. Both LSTM and GRU enforce a constant error flow over time steps and use gates on the input and the recurrent input to regulate the information flow through the network. However, the use of gates makes the computation not parallelable and thus increases the computational complexity of the whole network. To process the states of the network over time in parallel, the recurrent connections are fixed in \cite{bradbury2016quasi,lei2017training}. While this strategy greatly simplifies the computational complexity, it reduces the capability of their RNNs since the recurrent connections are no longer trainable. In \cite{arjovsky2015unitary, wisdom2016full}, a unitary evolution RNN was proposed where the unitary recurrent weights are defined empirically. In this case, the norm of the backpropagated gradient can be bounded without exploding. By contrast, the proposed IndRNN solves the gradient exploding and vanishing problems without losing the power of trainable recurrent connections and without involving gate parameters. 

In addition to changing the form of the recurrent neurons, works on initialization and training techniques, such as initializing the recurrent weights to a proper range or regulating the norm of the gradients over time, were also reported in addressing the gradient problems. In \cite{le2015simple}, an initialization technique was proposed for an RNN with relu activation, termed as IRNN, which initializes the recurrent weight matrix to be the identity matrix and bias to be zero. In \cite{talathi2015improving}, the recurrent weight matrix was further suggested to be a positive definite matrix with the highest eigenvalue of unity and all the remainder eigenvalues less than 1. In \cite{neyshabur2016path}, the geometry of RNNs was investigated and a path-normalized optimization method for training was proposed for RNNs with relu activation. In \cite{krueger2016regularizing}, a penalty term on the squared distance between successive hidden states' norms was proposed to prevent the exponential growth of IRNN's activation. Although these methods help ease the gradient exploding, they are not able to completely avoid the problem (the eigenvalues of the recurrent weight matrix may still be larger than 1 in the process of training). Moreover, the training of an IRNN is very sensitive to the learning rate. When the learning rate is large, the gradient is likely to explode. The proposed IndRNN solves gradient problems by making the neurons independent and constraining the recurrent weights. It can work with relu and be trained robustly. As a result, an IndRNN is able to process very long sequences (e.g. over  steps as demonstrated in the experiments).

On the other hand, comparing with the deep CNN architectures which could be over 100 layers such as the residual CNN \cite{he2016deep} and the pseudo-3D residual CNN (P3D) \cite{qiu2017learning}, most of the existing RNN architectures only consist of several layers (2 or 3 for example \cite{krueger2016zoneout,shahroudy2016ntu,le2015simple}). This is mostly due to the gradient vanishing and exploding problems which result in the difficulty in training a deep RNN. Since all the gate functions, input and output modulations in LSTM employ sigmoid or hyperbolic tangent functions as the activation function, it suffers from the gradient vanishing problem over layers when multiple LSTM layers are stacked into a deep model. Currently, a few models were reported that employ residual connections \cite{he2016deep} between LSTM layers to make the network deeper \cite{wu2016google}. However, as shown in \cite{pradhanexploring}, the deep LSTM model with the residual connections does not efficiently improve the performance. This may be partly due to the gradient decay over LSTM layers. On the contrary, for each time step, the proposed IndRNN with relu works in a similar way as CNN. Multiple layers of IndRNNs can be stacked and be efficiently combined with residual connections, leading to a deep RNN.
\vspace{0.2cm}

\section{Independently Recurrent Neural Network}
In this paper, we propose an independently recurrent neural network (IndRNN). It can be described as:

where recurrent weight  is a vector and  represents Hadamard product. Each neuron in one layer is independent from others and connection between neurons can be achieved by stacking two or more layers of IndRNNs as presented later. For the -th neuron, the hidden state  can be obtained as 

where  and  are the -th row of the input weight and recurrent weight, respectively. Each neuron only receives information from the input and its own hidden state at the previous time step. That is, each neuron in an IndRNN deals with one type of spatial-temporal pattern independently. Conventionally, a RNN is treated as multiple layer perceptrons over time where the parameters are shared. Different from the conventional RNNs, the proposed IndRNN provides a new perspective of recurrent neural networks as independently aggregating spatial patterns (i.e. through ) over time (i.e. through ). The correlation among different neurons can be exploited by stacking two or multiple layers. In this case, each neuron in the next layer processes the outputs of all the neurons in the previous layer.

The gradient backpropagation through time for an IndRNN and how it addresses the gradient vanishing and exploding problems are described in the next Subsection \ref{BPTT}. Details on the exploration of cross-channel information are explained in Subsection \ref{relationRNN}. Different deeper and longer IndRNN network architectures are discussed in Subsection \ref{rnn_arcs}.


\vspace{0.3cm}
\subsection{Backpropagation Through Time for An IndRNN}
\label{BPTT}
For the gradient backpropagation through time in each layer, the gradients of an IndRNN can be calculated independently for each neuron since there are no interactions among them in one layer. For the -th neuron  where the bias is ignored, suppose the objective trying to minimize at time step  is . Then the gradient back propagated to the time step  is 
\begin{small}
{

}
\end{small}
where  is the derivative of the element-wise activation function. It can be seen that the gradient only involves the exponential term of a scalar value  which can be easily regulated, and the gradient of the activation function which is often bounded in a certain range. Compared with the gradients of an RNN ( where  is the Jacobian matrix of the element-wise activation function), the gradient of an IndRNN directly depends on the value of the recurrent weight (which is changed by a small magnitude according to the learning rate) instead of matrix product (which is mainly determined by its eigenvalues and can be changed significantly even though the change to each matrix entries is small \cite{parlett1964laguerre}). Thus the training of an IndRNN is more robust than a traditional RNN. To solve the gradient exploding and vanishing problem over time, we only need to regulate the exponential term ``'' to an appropriate range. This is further explained in the following together with keeping long and short memory in an IndRNN.

To keep long-term memory in a network, the current state (at time step ) would still be able to effectively influence the future state (at time step ) after a large time interval. Consequently, the gradient at time step  can be effectively propagated to the time step . By assuming that the minimum effective gradient is , a range for the recurrent weight of an IndRNN neuron in order to keep long-term memory can be obtained. Specifically, to keep a memory of  time steps,  according to (\ref{KIRNNgradient}) (ignoring the gradient backpropagated from the objective at time step ). That is, to avoid the gradient vanishing for a neuron, the above constraint should be met. In order to avoid the gradient exploding problem, the range needs to be further constrained to  where  is the largest gradient value without exploding. For the commonly used activation functions such as relu and tanh, their derivatives are no larger than , i.e., . Especially for relu, its gradient is either  or . Considering that the short-term memories can be important for the performance of the network as well, especially for a multiple layers RNN, the constraint to the range of the recurrent weight with relu activation function can be relaxed to . When the recurrent weight is 0, the neuron only uses the information from the current input without keeping any memory from the past. In this way, different neurons can learn to keep memory of different lengths. Note that the regulation on the recurrent weight  is different from the gradient clipping technique. For the gradient clipping or gradient norm clipping \cite{pascanu2013difficulty}, the calculated gradient is already exploded and is forced back to a predefined range. The gradients for the following steps may keep exploding. In this case, the gradient of the other layers relying on this neuron may not be accurate. On the contrary, the regulation proposed here essentially maintains the gradient in an appropriate range without affecting the gradient backprogated through this neuron.  
\vspace{0.1cm}

\begin{figure}[tbp]
	\centering
	\subfigure[]{
	\includegraphics[width=0.8\hsize]{RNNBN.pdf}\label{basic}}
	\subfigure[]{
	\includegraphics[width=0.8\hsize]{arc_5.pdf}\label{basic_rnnpreres}}
	\caption{Illustration of (a) the basic IndRNN architecture and (b) the residual IndRNN architecture.} 
\label{rnnillustration}
\end{figure}

\section{Multiple-layer IndRNN}
\label{relationRNN}
As mentioned above, neurons in the same IndRNN layer are independent of each other, and cross channel information over time is explored through multiple layers of IndRNNs. To illustrate this, we compare a two-layer IndRNN with a traditional single layer RNN. For simplicity, the bias term is ignored for both IndRNN and traditional RNN. Assume a simple -neuron two-layer network where the recurrent weights for the second layer are zero which means the second layer is just a fully connected layer shared over time. The Hadamard product () can be represented in the form of matrix product by . In the following,  is shortened as . Assume that the activation function is a linear function . The first and second layers of a two-layer IndRNN can be represented by (\ref{firstlayerKIRNN}) and (\ref{secondlayerKIRNN}), respectively. 

Assuming  is invertible, then 

Thus


By assigning  and , it becomes

which is a traditional RNN. Note that this only imposes the constraint that the recurrent weight () is diagonalizable. Therefore, the simple two-layer IndRNN network can represent a traditional RNN network with a diagonalizable recurrent weight (). In other words, under linear activation, a traditional RNN with a diagonalizable recurrent weight () is a special case of a two-layer IndRNN where the recurrent weight of the second layer is zero and the input weight of the second layer is invertible. 

It is known that a non-diagonalizable matrix can be made diagonalizable with a perturbation matrix composed of small entries. A stable RNN network needs to be robust to small perturbations (in order to deal with precision errors for example). It is possible to find an RNN network with a diagonalizable recurrent weight matrix to approximate a stable RNN network with a non-diagonalizable recurrent weight matrix. Therefore, a traditional RNN with a linear activation is a special case of a two-layer IndRNN. For a traditional RNN with a nonlinear activation function, its relationship with the proposed IndRNN is yet to be established theoretically. However, we have shown empirically that the proposed IndRNN can achieve better performance than a traditional RNN with a nonlinear activation function.

Regarding the number of parameters, for a -neuron RNN network with input of dimension , the number of parameters in a traditional RNN is , while the number of parameters using one-layer IndRNN is . For a two-layer IndRNN where both layers consist of  neurons, the number of parameters is , which is of a similar order to the traditional RNN. 

In all, the cross-channel information can be well explored with a multiple-layer IndRNN although IndRNN neurons are independent of each other in each layer. 

\subsection{Deeper and Longer IndRNN Architectures}
\label{rnn_arcs}
In the proposed IndRNN, the processing of the input () is independent at different timesteps and can be  implemented in parallel as in \cite{bradbury2016quasi,lei2017training}. The proposed IndRNN can be extended to a convolutional IndRNN where, instead of processing input of each time step using a fully connected weight (), it is processed with convolutional operation (, where  denotes the convolution operator).

The basic IndRNN architecture is shown in Fig. \ref{basic}, where ``weight'' and ``Recurrent+ReLU'' denote the processing of input and the recurrent process at each step with relu as the activation function. By stacking this basic architecture, a deep IndRNN network can be constructed. Compared with an LSTM-based architecture using the sigmoid and hyperbolic tangent functions decaying the gradient over layers, a non-saturated activation function such as relu reduces the gradient vanishing problem over layers. In addition, batch normalization, denoted as ``BN'', can also be employed in the IndRNN network before or after the activation function as shown in Fig. \ref{basic}.  

Since the weight layer () is used to process the input, it is natural to extend it to multiple layers to deepen the processing. Also the layers used to process the input can be of the residual structures in the same way as in CNN \cite{he2016deep}. With the simple structure of IndRNN, it is very easy to extend it to different networks architectures. For example, in addition to simply stacking IndRNNs or stacking the layers for processing the input, IndRNNs can also be stacked in the form of residual connections. Fig. \ref{basic_rnnpreres} shows an example of a residual IndRNN based on the ``pre-activation'' type of residual layers in \cite{he2016identity}. At each time step, the gradient can be directly propagated to the other layers from the identity mapping. Since IndRNN addresses the gradient exploding and vanishing problems over time, the gradient can be efficiently propagated over different time steps. Therefore, the network can be substantially deeper and longer. The deeper and longer IndRNN network can be trained end-to-end similarly as other networks.



\begin{figure*}[tbp]
	\centering
	\subfigure[]{
	\includegraphics[width=0.43\hsize]{indadding100.png}\label{adding_100}}
	\hspace{0.5cm}
	\subfigure[]{
	\includegraphics[width=0.43\hsize]{indadding500.png}\label{adding_500}}
	\subfigure[]{
	\includegraphics[width=0.43\hsize]{indadding1000.png}\label{adding_1000}}
	\hspace{0.5cm}
\subfigure[]{
	\includegraphics[width=0.43\hsize]{indadding5000.png}\label{adding_5000}}
	\caption{Results of the adding problem for different sequence lengths. The legends for all figures are the same and thus only shown in (a).} 
	\label{adding}
\end{figure*}


\section{Experiments}
In this Section, evaluation of the proposed IndRNN on various tasks are presented.

\subsection{Adding Problem}
\label{addingdesc}
The adding problem~\cite{hochreiter1997long, arjovsky2015unitary} is commonly used to evaluate the performance of RNN models. Two sequences of length  are taken as input. The first sequence is uniformly sampled in the range  while the second sequence consists of two entries being  and the rest being . The output is the sum of the two entries in the first sequence indicated by the two entries of  in the second sequence. Three different lengths of sequences, ,  and , were used for the experiments to show whether the tested models have the ability to model long-term memory.
 
The RNN models included in the experiments for comparison are the traditional RNN with tanh, LSTM, IRNN (RNN with relu). The proposed IndRNN was evaluated with relu activation function. Since GRU achieved similar performance as LSTM \cite{jozefowicz2015empirical}, it is not included in the report. RNN, LSTM, and IRNN are all one layer while the IndRNN model is two layers.  hidden units were used for all the models, and the number of parameters for RNN, LSTM, and two-layer IndRNN are ,  and , respectively. It can be seen that the two-layer IndRNN has a comparable number of parameters to that of the one-layer RNN, while many more parameters are needed for LSTM. As discussed in Subsection \ref{BPTT}, the recurrent weight is constrained in the range of  for the IndRNN. 

Mean squared error (MSE) was used as the objective function and the Adam optimization method \cite{kingma2014adam} was used for training. The baseline performance (predicting 1 as the output regardless of the input sequence) is mean squared error of 0.167 (the variance of the sum of two independent uniform distributions). The initial learning rate was set to  for models with tanh activation and set as  for models with relu activations. However, as the length of the sequence increases, the IRNN model do not converge and thus a smaller initial learning rate () was used. The learning rate was reduced by a factor of 10 every 20K training steps. The training data and testing data were all generated randomly throughout the experiments, different from \cite{arjovsky2015unitary} which only used a set of randomly pre-generated data. 

The results are shown in Fig. \ref{adding_100}, \ref{adding_500} and \ref{adding_1000}. First, for short sequences (), most of the models (except RNN with tanh) performed well as they converged to a very small error (much smaller than the baseline). When the length of the sequences increases, the IRNN and LSTM models have difficulties in converging, and when the sequence length reaches , IRNN and LSTM cannot minimize the error any more. However, the proposed IndRNN can still converge to a small error very quickly. This indicates that the proposed IndRNN can model a longer-term memory than the traditional RNN and LSTM. 

From the figures, it can also be seen that the traditional RNN and LSTM can only keep a mid-range memory (about 500 - 1000 time steps). To evaluate the proposed IndRNN model for very long-term memory, experiments on sequences with length  were conducted where the result is shown in Fig. \ref{adding_5000}. It can be seen that IndRNN can still model it very well. Note that the noise in the result of IndRNN is because the initial learning rate () was relatively large and once the learning rate dropped, the performance became robust. This demonstrates that IndRNN can effectively address the gradient exploding and vanishing problem over time and keep a long-term memory. 



\subsubsection{Analysis of Neurons' Behaviour}
\label{analysisneuron}
In the proposed IndRNN, neurons in each layer are independent of each other which allows analysis of each neuron's behaviour without considering the effect coming from other neurons. Fig. \ref{1layer} and \ref{2layer} show the activation of the neurons in the first and second layers, respectively, for one random input with sequence length . It can be seen that neurons in the first layer mainly pick up the information of the numbers to be added, where the strong responses correspond to the locations to be summed indicated by the sequence. It can be regarded as reducing noise, i.e., reducing the effect of other non-useful inputs in the sequence. For the second layer, one neuron aggregates inputs to long-term memory while others generally preserve their own state or process short-term memory which may not be useful in the testing case (since only the hidden state of the last time step is used as output). From this result, we conjecture that only one neuron is needed in the second layer to model the adding problem. Moreover, since neurons in the second layer are independent from each other, one neuron can still work with the others removed (which is not possible for the traditional RNN models).

To verify the above conjecture, an experiment was conducted where the first IndRNN layer is initialized with the trained weights and the second IndRNN layer only consists of one neuron initialized with the weight of the neuron that keeps the long-term memory. Accordingly, the final fully connected layer used for output is a neuron with only one input and one output, i.e., two scalar values including one weight parameter and one bias parameter. Only the final output layer was trained/fine-tuned in this experiment and the result is shown in Fig. \ref{resultoneneuron}. It can be seen that with only one IndRNN neuron in the second layer, the model is still able to model the adding problem very well for sequences with length  as expected. 


\begin{figure}[tbp]
	\centering
	\subfigure[]{
	\includegraphics[width=0.8\hsize]{1layer.png}\label{1layer}}
	\hspace{1cm}
	\subfigure[]{
	\includegraphics[width=0.8\hsize]{2layer.png}\label{2layer}}
	\caption{Neurons' behaviour in different layers of the proposed IndRNN for long sequences (5000 time steps) in the adding problem.} 
	\label{neuronbehav}
\end{figure}
\begin{figure}[tbp]
\begin{minipage}[t]{1\linewidth}
\centering
    \includegraphics[width=0.8\linewidth]{indadding5000_1neuron.png}\caption{Result of the adding problem with just one neuron in the second layer for sequences of length 5000.} 
    \label{resultoneneuron}
\end{minipage}\end{figure}



\subsection{Sequential MNIST Classification}
Sequential MNIST classification is another problem that is widely used to evaluate RNN models. The pixels of MNIST digits \cite{lecun1998gradient} are presented sequentially to the networks and classification is performed after reading all pixels. To make the task even harder, the permuted MNIST classification was also used where the pixels are processed with a fixed random permutation. Since an RNN with tanh does not converge to a high accuracy (as reported in the literature \cite{le2015simple}), only IndRNN with relu was evaluated. As explained in Section \ref{rnn_arcs}, IndRNN can be stacked into a deep network. Here we used a six-layer IndRNN, and each layer has 128 neurons. To accelerate the training, batch normalization is inserted after each layer. The Adam optimization was used with the initial learning rate  and reduced by a factor of 10 every 600K training steps. The results are shown in Table \ref{result_mnist} in comparison with the existing methods. It can be seen that IndRNN achieved better performance than the existing RNN models. 

\begin{table}
\centering
\caption{Results (in terms of error rate (\%)) for the sequential MNIST and permuted MNIST.} 
  \begin{tabular}{@{}l@{}cc}
  \hline
   & MNIST & pMNIST \\
  \hline
  IRNN \cite{le2015simple} &  &  \\
  uRNN \cite{arjovsky2015unitary} &  & \\
  RNN-path \cite{neyshabur2016path} &  & - \\
  LSTM \cite{arjovsky2015unitary} &  &  \\ 
  LSTM+Recurrent dropout \cite{semeniuta2016recurrent} & - &  \\ 
  LSTM+Recurrent batchnorm \cite{cooijmans2016recurrent} & - &  \\ 
  LSTM+Zoneout \cite{krueger2016zoneout} & - &  \\ 
  LSTM+Recurrent batchnorm+Zoneout & - &  \\ 
  \hline  
  \textbf{IndRNN (6 layers)} &  & \\
  \hline
  \end{tabular}
\label{result_mnist}
\end{table}

\subsection{Language Modeling}
\subsubsection{Char-level Penn Treebank}
In this subsection, we evaluate the performance of the proposed IndRNN on the language modelling task using the character-level Penn Treebank (PTB-c) dataset. The test setting is similar to \cite{cooijmans2016recurrent}. A six-layer IndRNN with  hidden neurons is used for the test. To demonstrate that the IndRNN network can be very deep with the residual connections, a 21-layer residual IndRNN as shown in Fig. \ref{basic_rnnpreres} in Subsection \ref{rnn_arcs} was adopted. The frame-wise batch normalization \cite{laurent2016batch} is applied, and the batch size is set to . Adam was used for training with initial learning rate set to  and dropped by a factor of 5 when performance on the validation set was no longer improved (with patience 20). Dropout \cite{gal2016theoretically} with a dropping probability of 0.25 and 0.3 were used for the 6-layer IndRNN and the residual IndRNN. The sequences are non-overlapping and length  and  were both tested in training and testing.

The results are shown in Table \ref{result_penntree} in comparison with the existing methods. Performance was evaluated using bits per character metric (BPC). It can be seen that the proposed IndRNN model achieved better performance than the traditional RNN and LSTM models. It can also been seen that with a deeper residual IndRNN, the performance can be further improved. Also an improvement can be achieved with longer temporal dependencies (from time step 50 to 150) as shown in Table \ref{result_penntree}. 

\begin{table}
\centering
\tabcolsep=12pt
\caption{Results of char-level PTB for our proposed IndRNN model in comparison with results reported in the literature, in terms of BPC.}\label{result_penntree}
\begin{threeparttable}
  \begin{tabular}{l c}
  \hline
   & Test \\
  \hline
  RNN-tanh \cite{krueger2016regularizing} &  \\
  RNN-relu \cite{neyshabur2016path} &   \\
  RNN-TRec \cite{krueger2016regularizing} &  \\
  RNN-path \cite{neyshabur2016path} &  \\
  HF-MRNN \cite{mikolov2012subword} &  \\ 
  LSTM \cite{krueger2016zoneout}  &  \\
  LSTM+Recurrent dropout \cite{semeniuta2016recurrent} &  \\  
  LSTM+Recurrent batchnorm \cite{cooijmans2016recurrent} &  \\
  LSTM+Zoneout \cite{krueger2016zoneout} &   \\
  HyperLSTM + LN \cite{ha2016hypernetworks} &  \\
  Hierarchical Multiscale LSTM + LN \cite{chung2016hierarchical} &  \\
  Fast-slow LSTM \cite{mujika2017fast} &  \\
  Neural Architecture Search \cite{zoph2016neural} &  \\
  \hline  
  \textbf{IndRNN (6 layers, 50 steps)} &  \\
  \hline 
  \textbf{IndRNN (6 layers, 150 steps)} &  \\
  \hline 
  \textbf{res-IndRNN (21 layers, 50 steps)} &  \\
  \hline  
  \textbf{res-IndRNN (11 layers\textsuperscript{*}, 150 steps)} &  \\
  \hline
  \end{tabular}
      \begin{tablenotes}
      \small
      \item \textsuperscript{*}Note that due to the limitation of GPU memory, an 11-layer residual IndRNN was used for time step 150 instead of 21 layers.
    \end{tablenotes}
\label{result_final}
\end{threeparttable}
\end{table}

\subsubsection{Word-level Penn Treebank}
In this subsection, the performance of the proposed IndRNN on the word-level Penn Treebank dataset is evaluated. The test setting is similar to \cite{krueger2016zoneout}. A 11-layer residual IndRNN was used for test and the weight tying \cite{inan2016tying,press2016using} of the input embedding and the final output weight is also adopted. The frame-wise batch normalization \cite{laurent2016batch} is applied, and the batch size is set to . Adam was used for training with initial learning rate set to  and dropped by a factor of 5 when performance on the validation set was no longer improved (with patience 20). The sequences are non-overlapping and length  was used in training and testing. Dropout \cite{gal2016theoretically} with a dropping probability of 0.35 were used among IndRNN layers (including embedding) while 0.8 is used after the last IndRNN layer. The recurrent weights are initialized with , which makes the network starts with learning more mid-range memory.

The results are shown in Table \ref{result_wPTB} in comparison with the existing methods. It can be seen that the proposed IndRNN model achieved better performance than most of the traditional RNN and LSTM models except the neural architecture search \cite{zoph2016neural} which constructs new models while learning. 

\begin{table}
\centering
\tabcolsep=12pt
\caption{Results of word-level PTB for our proposed IndRNN model in comparison with results reported in the literature, in terms of perplexity.}\label{result_wPTB}
\begin{tabular}{l c}
  \hline
   & Test \\\hline
  RNN-LDA + KN-5 + cache \cite{mikolov2012context}  &  \\
  Deep RNN \cite{pascanu2013construct}  &  \\
  CharCNN \cite{kim2016character}  &  \\
  LSTM \cite{krueger2016zoneout}  &  \\
  LSTM+Recurrent dropout \cite{semeniuta2016recurrent} &  \\  
  LSTM+Zoneout \cite{krueger2016zoneout} &   \\
  LSTM+Variational Dropout \cite{gal2016theoretically} &  \\
  Pointer Sentinel LSTM \cite{merity2016pointer} &  \\
  RHN \cite{zilly2016recurrent} &  \\
  Neural Architecture Search \cite{zoph2016neural} &  \\
  \hline  
  \textbf{res-IndRNN (11 layers)} &  \\
  \hline
  \end{tabular}
\end{table}

\subsection{Skeleton based Action Recognition}
The NTU RGB+D dataset \cite{shahroudy2016ntu} was used for the skeleton based action recognition. This dataset is currently the largest action recognition dataset with skeleton modality. It contains  sequences of 60 action classes, including Cross-Subject (CS) (40320 and 16560 samples for training and testing, respectively) and Cross-View (CV) (37920 and 18960 samples for training and testing, respectively) evaluation protocols \cite{shahroudy2016ntu}. In each evaluation protocol, 5\% of the training data was used for evaluation as suggested in \cite{shahroudy2016ntu} and 20 frames were sampled from each instance as one input in the same way as in \cite{liu2016spatio}. The joint coordinates of two subject skeletons were used as input. If only one is present, the second was set as zero. For this dataset, when multiple skeletons are present in the scene, the skeleton identity captured by the Kinect sensor may be changed over time. Therefore, an alignment process was first applied to keep the same skeleton saved in the same data array over time. A four-layer IndRNN and a six-layer IndRNN with  hidden neurons were both tested. Batch size was 128 and the Adam optimization was used with the initial learning rate  and decayed by  once the evaluation accuracy does not increase. Dropout \cite{gal2016theoretically} was applied after each IndRNN layer with a dropping probability of  and  for CS and CV settings, respectively. 

The final result is shown in Table \ref{result_ntu} including comparisons with the existing methods. It can be seen that the proposed IndRNN greatly improves the performance over other RNN or LSTM models on the same task. For CS, RNN and LSTM of 2 layers can only achieve accuracies of 56.29\% and 60.09\% while a 4-layer IndRNN achieved 78.58\%. For CV, RNN and LSTM of 2 layers only achieved accuracies of 64.09\% and 67.29\% while 4-layer IndRNN achieved 83.75\%. As demonstrated in \cite{liu2016spatio,shahroudy2016ntu}, the performance of LSTM cannot be further improved by simply increasing the number of parameters or increasing the number of layers. However, by increasing the 4-layer IndRNN to a 6-layer IndRNN, the performance is further improved to 81.80\% and 87.97\% for CS and CV, respectively. This performance is better than the state-of-the-art methods including those with attention models \cite{song2017end,baradel2017pose} and other techniques \cite{zhang2017geometric,liu2016spatio}.

\begin{table}
\caption{Results of all skeleton based methods on NTU RGB+D dataset.}
\label{result_ntu}
\begin{center}
  \begin{tabular}{l c c}
  \hline
  Method & CS & CV \\
  \hline
  Deep learning on Lie Group \cite{huang2016deep} & 61.37\% & 66.95\%  \\
  \hline
  JTM+CNN \cite{wang2016action} & 73.40\% & 75.20\%  \\
  \hline
  Res-TCN \cite{kim2017interpretable} & 74.30\% & 83.10\%  \\
  \hline
  SkeletonNet(CNN) \cite{ke2017skeletonnet} & 75.94\% & 81.16\%  \\
  \hline
  JDM+CNN \cite{li2017joint} & 76.20\% & 82.30\%  \\
  \hline
  Clips+CNN+MTLN \cite{ke2017new} & 79.57\% & 84.83\%  \\
  \hline
  Enhanced Visualization+CNN \cite{liu2017enhanced} & 80.03\% & 87.21\%  \\
  \hline
  1 Layer RNN \cite{shahroudy2016ntu} & 56.02\% & 60.24\%  \\
  \hline
  2 Layer RNN \cite{shahroudy2016ntu} & 56.29\% & 64.09\%  \\
  \hline
  1 Layer LSTM \cite{shahroudy2016ntu} & 59.14\% & 66.81\%  \\
  \hline
  2 Layer LSTM \cite{shahroudy2016ntu} & 60.09\% & 67.29\%  \\
  \hline
  1 Layer PLSTM \cite{shahroudy2016ntu} & 62.05\% & 69.40\%  \\
  \hline
  2 Layer PLSTM \cite{shahroudy2016ntu} & 62.93\% & 70.27\%  \\
  \hline
  JL\_d+RNN \cite{zhang2017geometric} & 70.26\% & 82.39\%  \\
  \hline
  STA-LSTM \cite{song2017end} & 73.40\% & 81.20\%  \\
  \hline
  ST-LSTM + Trust Gate \cite{liu2016spatio} & 69.20\% & 77.70\%  \\
  \hline
  Pose conditioned STA-LSTM\cite{baradel2017pose} & 77.10\% & 84.50\%  \\
  \hline
  \textbf{IndRNN (4 layers)} & 78.58\% & 83.75\%  \\
  \hline
  \textbf{IndRNN (6 layers)} & 81.80\% & 87.97\%  \\
  \hline
  \end{tabular}
\end{center}
\end{table}

\section{Conclusion}
In this paper, we presented an independently recurrent neural network (IndRNN), where neurons in one layer are independent of each other. The gradient backpropagation through time process for the IndRNN has been explained and a regulation technique has been developed to effectively address the gradient vanishing and exploding problems. Compared with the existing RNN models including LSTM and GRU, IndRNN can process much longer sequences. The basic IndRNN can be stacked to construct a deep network especially combined with residual connections over layers, and the deep network can be trained robustly. In addition, independence among neurons in each layer allows better interpretation of the neurons. Experiments on multiple fundamental tasks have verified the advantages of the proposed IndRNN over existing RNN models. 

{\small
\bibliographystyle{ieee}
\bibliography{reRNNreference}
}

\end{document}
