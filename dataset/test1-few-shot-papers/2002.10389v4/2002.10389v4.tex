\documentclass{article}



\usepackage[final,nonatbib]{neurips_2020}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{subfigure}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}

\title{Semi-Supervised Neural Architecture Search}



\author{
  Renqian Luo\thanks{The work was done when the first author was an intern at Microsoft Research Asia.}, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, Tie-Yan Liu\\
  University of Science and Technology of China, Hefei, China\\
  Microsoft Research Asia, Beijing, China\\
  lrq@mail.ustc.edu.cn,\; cheneh@ustc.edu.cn\\
  \{xuta, ruiwa, taoqin, tyliu\}@microsoft.com
}

\begin{document}

\maketitle

\begin{abstract}
Neural architecture search (NAS) relies on a good controller to generate better architectures or predict the accuracy of given architectures. However, training the controller requires both abundant and high-quality pairs of architectures and their accuracy, while it is costly to evaluate an architecture and obtain its accuracy. In this paper, we propose \emph{SemiNAS}, a semi-supervised NAS approach that leverages numerous unlabeled architectures (without evaluation and thus nearly no cost). Specifically, SemiNAS 1) trains an initial accuracy predictor with a small set of architecture-accuracy data pairs; 2) uses the trained accuracy predictor to predict the accuracy of large amount of architectures (without evaluation); and 3) adds the generated data pairs to the original data to further improve the predictor. The trained accuracy predictor can be applied to various NAS algorithms by predicting the accuracy of candidate architectures for them. SemiNAS has two advantages: 1) It reduces the computational cost under the same accuracy guarantee. On NASBench-101 benchmark dataset, it achieves comparable accuracy with gradient-based method while using only 1/7 architecture-accuracy pairs. 2) It achieves higher accuracy under the same computational cost. It achieves 94.02\% test accuracy on NASBench-101, outperforming all the baselines when using the same number of architectures. On ImageNet, it achieves 23.5\% top-1 error rate (under 600M FLOPS constraint) using 4 GPU-days for search. We further apply it to LJSpeech text to speech task and it achieves 97\% intelligibility rate in the low-resource setting and 15\% test error rate in the robustness setting, with 9\%, 7\% improvements over the baseline respectively.
\end{abstract}

\section{Introduction}
\label{intro}
Neural architecture search~(NAS) for automatic architecture design has been successfully applied in several tasks including image classification and language modeling~\cite{nasnet,evovledtransformer,nasfpn}. NAS typically contains two components, a controller~(also called generator) that controls the generation of new architectures, and an evaluator that trains candidate architectures and evaluates their accuracy\footnote{Although a variety of metrics including accuracy, model size, and inference speed have been used as search criterion, the accuracy of an architecture is the most important and costly one, and other metrics can be easily calculated with almost zero computation cost. Therefore, we focus on accuracy in this work.}. The controller learns to generate relatively better architectures via a variety of techniques~(e.g., reinforcement learning~\cite{nas,nasnet}, evolution~\cite{amoebanet}, gradient optimization ~\cite{darts,nao}, Bayesian optimization~\cite{bayesnas}), and plays an important role in NAS~\cite{nas,nasnet,enas,amoebanet,nao,darts,bayesnas}. To ensure the performance of the controller, a large number of high-quality pairs of architectures and their corresponding accuracy are required as the training data.

However, collecting such architecture-accuracy pairs is expensive, since it is costly for the evaluator to train each architecture to accurately get its accuracy, which incurs the highest computational cost in NAS. Popular methods usually consume hundreds to thousands of GPU days to discover eventually good architectures~\cite{nas,amoebanet,nao}. To address this problem, one-shot NAS~\cite{oneshot,enas,darts,snas} uses a supernet to include all candidate architectures via weight sharing and trains the supernet to reduce the training time. While greatly reducing the computational cost, the quality of the training data (architectures and their corresponding accuracy) for the controller is degraded~\cite{searchphasenas}, and thus these approaches suffer from accuracy decline on downstream tasks. 

In various scenarios with limited labeled training data, semi-supervised learning~\cite{introsemi} is a popular approach to leverage unlabeled data to boost the training accuracy. In the scenario of NAS, unlabeled architectures can be obtained through random generation, mutation~\cite{amoebanet}, or simply going through the whole search space~\cite{neuralpredictor}, which incur nearly zero additional cost. Inspired by semi-supervised learning, in this paper, we propose \emph{SemiNAS}, a semi-supervised approach for NAS that leverages a large number of unlabeled architectures. Specifically, SemiNAS 1) trains an initial accuracy predictor with a set of architecture-accuracy data pairs; 2) uses the trained accuracy predictor to predict the accuracy of a large number of unlabeled architectures; and 3) adds the generated architecture-accuracy pairs to the original data to further improve the accuracy predictor. The trained accuracy predictor can be incorporated to various NAS algorithms by predicting the accuracy of unseen architectures.

SemiNAS can be applied to many NAS algorithms. We take the neural architecture optimization (NAO)~\cite{nao} algorithm as an example, since NAO has the following advantages: 1) it takes architecture-accuracy pairs as training data to train a accuracy predictor to predict the accuracy of architectures, which can directly benefit from SemiNAS; 2) it supports both conventional methods which train each architecture from scratch~\cite{nasnet,amoebanet,nao} and one-shot methods which train a supernet with weight sharing~\cite{enas,nao}; and 3) it is based on gradient optimization which has shown better effectiveness and efficiency. Although we implement SemiNAS on NAO, it is easy to be applied to other NAS methods, such as reinforcement learning based methods~\cite{nasnet,enas} and evolutionary algorithm based methods~\cite{amoebanet}. 

SemiNAS shows advantages over both conventional NAS and one-shot NAS. Compared to conventional NAS, it can significantly reduce computational cost while achieving similar accuracy, and achieve better accuracy with similar cost. Specifically, on NASBench-101 benchmark, SemiNAS achieves similar accuracy~() as gradient based methods~\cite{nao} using only  architectures. Meanwhile it achieves  mean test accuracy surpassing all the baselines when evaluating the same number of architectures (with the same computational cost). Compared to one-shot NAS, SemiNAS achieves higher accuracy using similar computational cost. For image classification, within  GPU days for search, we achieve  top-1 error rate on ImageNet under the mobile setting. For text to speech (TTS), using  GPU days for search, SemiNAS achieves  intelligibility rate in the low-resource setting and  sentence error rate in the robustness setting, which outperforms human-designed model by  and  points respectively. To the best of our knowledge, we are the first to develop NAS algorithms on text to speech (TTS) task. We carefully design the search space and search metric for TTS, and achieve significant improvements compared to human-designed architectures. We believe that our designed search space and metric are helpful for future studies on NAS for TTS.



\section{Related Work}
From the perspective of the computational cost of training candidate architectures, previous works on NAS can be categorized into conventional NAS and one-shot NAS. 

Conventional NAS includes~\cite{nas,nasnet,amoebanet,nao}, which achieve significant improvements on several benchmark datasets. Obtaining the accuracy of the candidate architectures is expensive in conventional NAS, since they train every single architecture from scratch and usually require thousands of architectures to train. The total cost is usually more than hundreds of GPU days~\cite{nasnet,amoebanet,nao}.

To reduce the huge cost in NAS, one-shot NAS was proposed with the help of weight sharing mechanism.~\cite{oneshot} proposes to include all candidate operations in the search space within a supernet and share parameters among candidate architectures. Each candidate architecture is a sub-graph in the supernet and only activates the parameters associated with it. The algorithm trains the supernet and then evaluates the accuracy of candidate architectures by the corresponding sub-graphs in the supernet.~\cite{enas,nao,darts,pdarts,pcdarts,proxylessnas,singlepathnas,singlepathoneshot} also leverage the one-shot idea to perform efficient search while using different search algorithms. Such weight sharing mechanism successfully cuts down the computational cost to less than  GPU days~\cite{enas,darts,proxylessnas,pcdarts}. However, the supernet requires careful design and the training of supernet needs careful tuning. Moreover, it shows inferior performance and reproducibility compared to conventional NAS. One main cause is the short training time and inadequate update of individual architecture~\cite{random,searchphasenas}, which leads to an inaccurate ranking of the architectures, and provides relatively low-quality architecture-accuracy pairs for the controller.

To sum up, there exists a trade-off between computational cost and accuracy. We formalize the computational cost of the evaluator by , where  is the number of architecture-accuracy pairs for the controller to learn, and  is the training time of each candidate architecture. In conventional NAS, the evaluator trains each architecture from scratch and the  is typically several epochs\footnote{One epoch means training on the whole dataset for once.} to ensure the accuracy of the evaluation, leading to large . In one-shot NAS, the  is reduced to a few mini-batches, which is inadequate for training and therefore produces low-quality architecture-accuracy pairs. Our SemiNAS handles this computation and accuracy trade-off from a new perspective which reduces  by leveraging a large number of unlabeled architectures.

\section{SemiNAS}
In this section, we first describe the semi-supervised training of the accuracy predictor, and then introduce the implementation of the proposed SemiNAS algorithm.

\subsection{The Semi-Supervised Training of the Accuracy Predictor}
\label{sec:semitrain}
To learn from both labeled architecture-accuracy pairs and unlabeled architectures without corresponding accuracy numbers, SemiNAS trains an accuracy predictor via semi-supervised learning. Specifically, we utilize a large number of unevaluated architectures () to improve the accuracy predictor. To utilize numerous unlabeled data, we leverage self-supervised learning by predicting the accuracy of unevaluated architectures~\cite{pseudolabel} and then combine them with ground-truth data to further improve the accuracy predictor. Following~\cite{noisystudent}, we apply dropout as noise during the training.

However, a simple accuracy predictor is hard to learn information from architectures with pseudo labels via regression task although with techniques in~\cite{noisystudent}. Inspired by~\cite{nao}, we use an accuracy predictor framework consisting of an encoder , a predictor  and a decoder . The encoder is implemented as an LSTM network to map the discrete architecture  to continuous embedding representations , and the predictor uses fully connected layers to predict the accuracy of the architecture taking the continuous embedding  as input. The decoder is an LSTM to decode the continuous embedding back to discrete architecture in an auto-regressive manner. The three components are trained jointly via the regression task and the reconstruction task. The semi-supervised learning of the accuracy predictor can be decomposed into  steps:
\begin{itemize}
    \item Train the encoder , predictor  and the decoder  with  architecture-accuracy pairs where each architecture is trained and evaluated.
    \item Generate  unlabeled architectures and use the trained encoder  and predictor  to predict their accuracy.
    \item Use both the  architecture-accuracy pairs and the  self-labeled pairs together to train a better accuracy predictor.  
\end{itemize}

The accuracy predictor learns information from limited number of architecture-accuracy pairs, while there are still numerous unseen architectures. With the help of the decoder, the encoder and the decoder together act like an autoencoder to learn the hidden representation of architectures. Therefore it is able for the whole framework to learn the information of architectures in an unsupervised way without the requirement of ground-truth labels~(evaluated accuracy), and further improves the accuracy predictor as the three components are trained jointly. The trained accuracy predictor can be incorporated to various NAS algorithms by predicting the accuracy of unseen architectures for them.

SemiNAS brings advantages over both conventional NAS and one-shot NAS, which can be illustrated under the computational cost formulation . Compared to conventional NAS which is costly, SemiNAS can reduce the computational cost  with smaller  but using more additional unlabeled architectures to avoid accuracy drop, and can also further improve the performance with same computational cost. Compared to one-shot NAS which has inferior accuracy, SemiNAS can improve the accuracy by using more unlabeled architectures under the same computational cost . Specifically, in order to get more accurate evaluation of architectures and improve the quality of architecture-accuracy pairs, we can extend the average training time  for each individual architecture. Meanwhile, we reduce the number of architectures to be trained~(i.e., ) to keep the total budget  unchanged.


\subsection{The Implementation of SemiNAS} 
\label{sec:seminas}
We now describe the implementation of our SemiNAS algorithm. We take NAO~\cite{nao} as our implementation since it has following advantages: 1) it contains an encoder-predictor-decoder framework, where the encoder and the predictor can predict the accuracy for large number of architectures without evaluation, and is straightforward to incorporate our method; 2) it performs architecture search by applying gradient ascent which has shown better effectiveness and efficiency; 3) it can incorporate both conventional NAS~(whose evaluator trains each architecture from scratch) and one-shot NAS~(whose evaluator builds a supernet to train all the architectures via weight sharing). 

NAO~\cite{nao} uses an encoder-predictor-decoder framework as the controller, where the encoder  maps the discrete architecture representation  into continuous representation  and uses the predictor  to predict its accuracy . Then it uses a decoder  that is implemented based on a multi-layer LSTM to reconstruct the original discrete architecture from the continuous representation  in an auto-regressive manner.

After the controller is trained, for any given architecture  as the input, NAO moves its representation  towards the direction of the gradient ascent of the accuracy prediction  to get a new and better continuous representation  as follows: , where  is a step size.  can get higher prediction accuracy  after gradient ascent. Then it uses the decoder  to decode  into a new architecture , which is supposed to be better than architecture . The process of the architecture optimization is performed for  iterations, where newly generated architectures at the end of each iteration are added to the architecture pool for evaluation and further used to train the controller in the next iteration. Finally, the best performing architecture in the architecture pool is selected out as the final result. 

\begin{algorithm}[ht]
\caption{Semi-Supervised Neural Architecture Search}
\small
\label{alg:SemiNAS}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Number of architectures  to evaluate. Number of unlabeled architectures  to use. The set of architecture-accuracy pairs  to train the encoder-predictor-decoder. Number of architectures  based on which to generate better architectures. Training steps  to evaluate each architecture. Number of optimization iterations . Step size .
\STATE Generate  architectures. Use the evaluator to train each architecture for  steps~(in conventional way or weight sharing way). 
\STATE Evaluate the  architectures to obtain the accuracy and form the labeled dataset .
\FOR {}
\STATE Train ,  and  jointly using .
\STATE Randomly generate  architectures and use  and  to predict their accuracy and forming dataset .
\STATE Set .
\STATE Train ,  and  using .
\STATE Pick  architectures with top accuracy among . For each architecture, obtain a better architecture by applying gradient ascent optimization with step size .
\STATE Evaluate the newly generated architectures using the evaluator and add them to .
\ENDFOR
\STATE \textbf{Output}: The architecture in  with the best accuracy.
\end{algorithmic}
\end{algorithm}

With the semi-supervised method proposed in Section~\ref{sec:semitrain}, we propose our SemiNAS as shown in Alg.~\ref{alg:SemiNAS}. First we train the encoder-predictor-decoder on limited number~() of architecture-accuracy pairs~(line 5). Then we train the encoder-predictor-decoder with additional  unlabeled architectures~(line 6-8). Finally, we perform the step of generating new architectures as the same in~\cite{nao}~(line9-10).

\subsection{Discussions}
Although our SemiNAS is mainly implemented based on NAO in this paper, the key idea of utilizing the trained encoder  and predictor  to predict the accuracy of numerous unlabeled architectures can be extended to a variety of NAS methods. For reinforcement learning based algorithms~\cite{nas,nasnet,enas} where the controller is usually an RNN model, we can predict the accuracy of the architectures generated by the RNN and take the predicted accuracy as the reward to train the controller. For evolution based methods~\cite{amoebanet}, we can predict the accuracy of the architectures generated through mutation and crossover, and then take the predicted accuracy as the fitness of the generated architectures.

\section{Application to Image Classification}
\label{sec:image}
In this section, we demonstrate the effectiveness of SemiNAS on image classification tasks. We first conduct experiments on NASBench-101~\cite{nasbench101} and then on the commonly used large-scale ImageNet.

\subsection{NASBench-101}
\paragraph{Dataset} NASBench-101~\cite{nasbench101} designs a cell-based search space following the common practice~\cite{nasnet,nao,darts}. It includes  CNN architectures and trains each architecture CIFAR-10 for  times. Querying the accuracy of an architecture from the dataset is equivalent to training and evaluating the architecture. We hope to discover comparable architectures with less computational cost or better architectures with comparable computational cost. Specifically, on this dataset, reducing the computational cost can be regarded as decreasing the number of queries.

\paragraph{Setup} Both the encoder and the decoder consist of a single layer LSTM with a hidden size of , and the predictor is a three-layer fully connected network with hidden sizes of  respectively. We use Adam optimizer with a learning rate of . During search, only valid accuracy is used. After search, we report the mean test accuracy of the selected architecture over the  runs. We report two settings of SemiNAS. For the first setting, we use  and up-sample  labeled data by x~(directly duplicate the labeled data). We generate  new architectures based on top  architectures following line 9 in Alg.~\ref{alg:SemiNAS} at each iteration and run for  iterations. The algorithm totally evaluates  architectures. For the second setting, we set  and up-sample  labeled data by x. We generate  new architectures based on top  architectures at each iteration and run for  iterations. The algorithm totally queries  architectures. For comparison, we evaluate random search, regularized evolution~(RE)~\cite{amoebanet} and NAO as baselines, where RE is validated as the best-performing algorithm in the NASBench-101 publication. We limit the number of queries of the baselines to be  for fair comparison. Particularly, we run NAO with two settings using  and  architectures for better comparison considering our SemiNAS is mainly implemented based on NAO in this paper. Additionally, we also combine our semi-supervised trained accuracy predictor with RE and name it SemiNAS (RE) for comparison to show the potential of SemiNAS. All the experiments are conducted for  times and we report the averaged results. Since the best test accuracy in the dataset is  and several algorithms are reaching it, we also report test regret~(gap to ) following the guide by~\cite{nasbench101} and the ranking of the accuracy number among the whole dataset to better illustrate the improvements of our method.

\paragraph{Results}
All the results are listed in Table~\ref{tbl:nasbench}. Random search achieves  test accuracy with a confidence interval of  (alpha=). This implies that even  is a significant difference and there exists a large margin for improvement. We can see that, when using the same number of architectures-accuracy pairs~(), SemiNAS outperforms all the baselines with  test accuracy and corresponding  test regret, which ranks top  in the whole space. SemiNAS with only  architectures achieves  test accuracy and  test regret which is on par with NAO with  architectures. Moreover, NAO using  architectures only achieves  which is merely better than random search. This demonstrates that with the help of unlabeled data, SemiNAS indeed outperforms baselines when using the same number of labeled architectures, and can achieve similar performance while using much less resources. Further, SemiNAS~(RE) achieves  which is on par with baseline RE while using only a half number of labeled architectures, and outperforms RE with  when using same number of labeled architectures~(2000). This implies the potential of using semi-supervised learning in NAS for speeding up the search and applying to various NAS algorithms. We also conduct experiments to study the effect of different number of unlabeled architectures () and up-sampling ratio in SemiNAS, and the results are in Section~\ref{sec:hyperpara}.
\begin{table}[htbp]
\centering
\small
\begin{tabular}{lccccc}
\toprule
Method  & \#Queries & Test Acc. (\%) & SD (\%) & Test Regret (\%) & Ranking\\
\midrule
Random Search         & 2000 & 93.64 & 0.25 & 0.68 & 1749\\
\midrule
RE~\cite{amoebanet}   & 2000 & 93.96 & 0.05 & 0.36 & 89\\
SemiNAS (RE)          & 1000 & 93.97 & 0.05 & 0.35 & 76\\
SemiNAS (RE)          & 2000 & 94.03 & 0.05 & 0.29 & 37 \\
\midrule
NAO~\cite{nao}       & 300  & 93.69 & 0.06 & 0.63 & 1191\\
NAO~\cite{nao}        & 2000 & 93.90 & 0.03 & 0.42 & 169\\
SemiNAS               & 300  & 93.89 & 0.06 & 0.43 & 197\\
SemiNAS               & 2000 & 94.02 & 0.05 & 0.30 & 43\\
\bottomrule
\end{tabular}
\caption{Performances of different NAS methods on NASBench-101 dataset. ``\#Queries'' is the number of architecture-accuracy pairs queried from the dataset. ``SD'' is standard deviation.}
\label{tbl:nasbench}
\end{table}

\subsection{ImageNet}
Previous experiments on NASBench-101 dataset verify the effectiveness and efficiency of SemiNAS in a well-controlled environment. We further evaluate our approach to the large-scale ImageNet dataset.

\paragraph{Search space} We adopt a MobileNet-v2~\cite{mobilenetv2} based search space following ProxylessNAS~\cite{proxylessnas}. It consists of multiple stacked layers. We search the operation of each layer. Candidate operations include mobile inverted bottleneck convolution layers~\cite{mobilenetv2} with various kernel sizes  and expansion ratios , as well as zero-out layer.

\paragraph{Setup} We randomly sample  images from the training data as valid set for architecture search. Since training ImageNet is too expensive, we adopt weight sharing mechanism~\cite{enas,proxylessnas} to perform one-shot search. We train the supernet on  GPUs for  steps with a batch size of  per card. We set  and run the search process for  iterations. In each iteration,  new better architectures are generated based on top  architectures following line 9 in Alg.~\ref{alg:SemiNAS}. The search runs for  day on  V100 GPUs. To fairly compare with other works, we limit the FLOPS of the discovered architecture to be less than M. The discovered architecture 
is trained for  epochs with a total batch size of . We use the SGD optimizer with an initial learning rate of  and a cosine learning rate schedule~\cite{sgdr}. More training details are in Section~\ref{sec:expdetail}. For NAO, we use the open source code~\footnote{\url{https://github.com/renqianluo/NAO_pytorch}} and train it on the same search space used in this paper. In both SemiNAS and NAO, we train the supernet for  steps at each iteration to keep the same cost while NAO uses larger . For ProxylessNAS, since it also optimizes latency as additional target, for fair comparison, we use their open source code~\footnote{\url{https://github.com/mit-han-lab/proxylessnas}} and rerun the search while optimizing accuracy without considering latency. We limit the FLOPS of discovered architecture to be less than M. We run all the experiments for  times.
\begin{table}[htbp]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Model/Method                      & Top-1 (\%) & Top-5 (\%) & Params (Million) & FLOPS (Million) \\ \hline
MobileNetV2~\cite{mobilenetv2}               & 25.3      & -        & 6.9             & 585 \\
ShuffleNet 2 (v2)~\cite{shufflenet}  & 25.1      & -        &  5        & 591  \\
\midrule
NASNet-A~\cite{nas}               & 26.0      & 8.4       & 5.3       & 564 \\
AmoebaNet-A~\cite{amoebanet}      & 25.5      & 8.0       & 5.1       & 555 \\
PNAS~\cite{PNAS}                  & 25.8      & 8.1       & 5.1       & 588 \\
SNAS~\cite{snas}                  & 27.3      & 9.2       & 4.3       & 522\\
DARTS~\cite{darts}                & 26.9      & 9.0       & 4.9       & 595 \\
P-DARTS~\cite{pdarts}             & 24.4      & 7.4       & 4.9       & 557\\
PC-DARTS~\cite{pcdarts}           & 24.2      & 7.3       & 5.3       & 597\\
Efficienet-B0~\cite{efficientnet} & 23.7      & 6.8       & 5.3       & 390 \\
\midrule
Random Search                    & 25.2    & 8.0     & 5.1   & 578 \\
ProxylessNAS~\cite{proxylessnas}  & 24.0      & 7.1  & 5.8      & 595\\
NAO~\cite{nao}                  & 24.5      & 7.5       & 6.5       & 590 \\
\midrule
SemiNAS                           & \textbf{23.5}      & \textbf{6.8}       & 6.3      & 599\\
\bottomrule
\end{tabular}
\caption{Performances of different methods on ImageNet. For fair comparison, we run NAO on the same search space used in this paper, and run ProxylessNAS by optimizing accuracy without latency.}
\label{tbl:imagenet}
\end{table}
\paragraph{Results}
From the results in Table~\ref{tbl:imagenet}, SemiNAS achieves  top-1 test error rate on ImageNet under the M FLOPS constraint, which outperforms all the other NAS works. Specifically, it significantly \textbf{outperforms the baseline algorithm NAO based on which SemiNAS is mainly implemented by }, and outperforms ProxylessNAS where our search space is based on by . The discovered architecture is depicted in Section \ref{sec:arch}.


\section{Application to Text to Speech}
\label{sec:tts}
In this section, we further explore the application of SemiNAS to a new task: text to speech. 

Text to speech (TTS)~\cite{tacotron,tacotron2,deepvoice3,transformertts,fastspeech} is an import task aiming to synthesize intelligible and natural speech from text. The encoder-decoder based neural TTS~\cite{tacotron2} has achieved significant improvements. However, due to the different modalities between the input (text) and the output (speech), popular TTS models are still complicated and require many human experiences when designing the model architecture. Moreover, unlike many other sequence learning tasks (e.g., neural machine translation) where the Transformer model~\cite{transformer} is the dominate architecture, RNN based Tacotron~\cite{tacotron,tacotron2}, CNN based Deep Voice~\cite{deepvoice,deepvoice2,deepvoice3}, and Transformer based models~\cite{transformertts} show comparable accuracy in TTS, without one being exclusively better than others.

The complexity of the model architecture in TTS indicates great potential of NAS on this task. However, applying NAS on TTS task also has challenges, mainly in two aspects: 1) Current TTS model architectures are complicated, including many human designed components. It is difficult but important to design the network bone and the corresponding search space for NAS. 2) Unlike other tasks (e.g., image classification) whose evaluation is objective and automatic, the evaluation of a TTS model requires subject judgement and human evaluation in the loop (e.g., intelligibility rate for understandability and mean opinion score for naturalness). It is impractical to use human evaluation for thousands of architectures in NAS. Thus, it is difficult but also important to design a specific and appropriate objective metric as the reward of an architecture during the search process. Next, we design the search space and evaluation metric for NAS on TTS, and apply SemiNAS on two specific TTS settings: low-resource setting and robustness setting. 

\subsection{Experiment Settings}
\paragraph{Search space} After surveying the previous neural TTS models, we choose a multi-layer encoder-decoder based network as the network backbone for TTS. We search the operation of each layer of the encoder and the decoder. The search space includes  candidate operations in total: convolution layer with kernel size , Transformer layer~\cite{transformertts} with number of heads of  and LSTM layer. Specifically, we use unidirectional LSTM layer, causal convolution layer, causal self-attention layer in the decoder to avoid seeing the information in future positions. Besides, every decoder layer is inserted with an additional encoder-decoder-attention layer to catch the relationship between the source and target sequence, where the dot-product multi-head attention in Transformer~\cite{transformer} is adopted.

\paragraph{Evaluation metric} It has been shown that the quality of the attention alignment between the encoder and decoder is an important influence factor on the quality of synthesized speech in previous works~\cite{fastspeech,tacotron,tacotron2,transformertts,deepvoice3}, and misalignment can be observed for most mistakes~(e.g., skipping and repeating). Accordingly, we consider the diagonal focus rate~(DFR) of the attention map between the encoder and decoder as the metric of an architecture. DFR is defined as: , where  denotes the attention map,  and  are the length of the source input sequence and the target output sequence,  is the slope factor and  is the width of the diagonal area in the attention map. DFR measures how much attention lies in the diagonal area with width  in the attention matrix, and ranges in  which is the larger the better. In addition, we have also tried valid loss as the search metric, but it is inferior to DFR according to our preliminary experiments.

\paragraph{Task setting} Current TTS systems are capable of achieving near human-parity quality when trained on adequate data and tested on regular sentences~\cite{tacotron2,transformertts}. However, current TTS models have poor performance on two specific TTS settings: 1) low-resource setting, where only few paired speech and text data is available. 2) Robustness setting, where the test sentences are not regular~(e.g., too short, too long, or contain many word pieces that have the same pronunciations). Under these two settings, the synthesized speech of a human-designed TTS model is usually not accurate and robust (i.e., some words are skipped or repeated). Thus we apply SemiNAS on these two settings to improve the accuracy and robustness. We conduct experiments on the LJSpeech dataset~\cite{ljspeech} which contains  text and speech data pairs with approximately  hours of speech audio.

\subsection{Results on Low-Resource Setting}
\paragraph{Setup} To simulate the low-resource scenario, we randomly split out  paired speech and text samples as the training set, where the total audio length is less than  hours. We use . We adopt the weight sharing mechanism and train the supernet on 4 GPUs for  epochs. The search runs for  day on 4 P40 GPUs. Besides, we train vanilla NAO as a baseline where . The discovered architecture is trained on the training set for k steps on 4 GPUs, with a batch size of K speech frames on each GPU. More details are provided in Section~\ref{sec:expdetail}. In the inference process, the output mel-spectrograms are transformed into audio samples using Griffin-Lim~\cite{griffin1984signal}. We run all the experiments for  times.
\begin{table}[htbp]
\centering
\small
\begin{tabular}{lcc}
\toprule
Model/Method & Intelligibility Rate~(\%) & DFR~(\%) \\ 
\midrule
Transformer TTS~\cite{transformertts} & 88 & 86 \\
NAO~\cite{nao}             & 94 & 88 \\
SemiNAS         & \textbf{97} & \textbf{90} \\
\bottomrule
\end{tabular}
\caption{Results on LJSpeech under the low-resource setting. ``DFR'' is diagonal focus rate.}
\label{tbl:lowresource}
\end{table}
\paragraph{Results} We test the performance of SemiNAS, NAO~\cite{nao} and Transformer TTS~(following~\cite{transformertts}) on the  test sentences and report the results in Table~\ref{tbl:lowresource}. We measure the performances in terms of word level intelligibility rate~(IR), which is a commonly used metric to evaluate the quality of generated audio~\cite{unsuperttsasr}. IR is defined as the percentage of test words whose pronunciation is considered to be correct and clear by human. It is shown that SemiNAS achieves  IR, with significant improvements of  points over human designed Transformer TTS and  points over NAO. We also list the DFR metric for each method in Table~\ref{tbl:lowresource}, where SemiNAS outperforms Transformer TTS and NAO in terms of DFR, which is consistent with the results on IR and indicates that our proposed search metric DFR can indeed guide NAS algorithms to achieve better accuracy. We also use MOS~(mean opinion score)~\cite{mos} to evaluate the naturalness of the synthesized speech. Using Griffin-Lim as the vocoder to synthesize the speech, the ground-truth mel-spectrograms achieves  MOS, Transformer TTS achieves , NAO achieves  and SemiNAS achieves . SemiNAS outperforms other methods in terms of MOS, which also demonstrates the advantages of SemiNAS. We also attach the discovered architecture by SemiNAS in Section~\ref{sec:arch}.

\subsection{Results on Robustness Setting}
\paragraph{Setup} We train on the whole LJSpeech dataset as the training data. For robustness test, we select the  sentences as used in~\cite{deepvoice3}~(attached in Section~\ref{sec:robustnesssentences}) that are found hard for TTS models. Training details follow the same as in the low-resource TTS experiment. We also attach the discovered architecture in Section~\ref{sec:arch}. We run all the experiments for  times.
\begin{table}[htbp]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Model/Method    & DFR~(\%) & Repeat & Skip & Error~(\%)\\
\midrule
Transformer TTS\cite{transformertts} & 15 & 1 & 21 & 22\\ 
NAO~\cite{nao}  & 25 & 2 & 18 & 19 \\
SemiNAS         & \textbf{30} & 2 & 14 & \textbf{15}\\
\bottomrule
\end{tabular}
\caption{Robustness test on the 100 hard sentences. ``DFR'' stands for diagonal focus rate.}
\label{tbl:robust}
\end{table}
\paragraph{Results} We report the results in Table~\ref{tbl:robust}, including the DFR, the number of sentences with repeating and skipping words, and the sentence level error rate. A sentence is counted as an error if it contains a repeating or skipping word. SemiNAS is better than Transformer TTS~\cite{transformertts} and NAO~\cite{nao} on all the metrics. It reduces the error rate by  and  compared to Transformer TTS structure designed by human experts and the searched architecture by NAO respectively.

\section{Conclusion}
High-quality architecture-accuracy pairs are critical to NAS; however, accurately evaluating the accuracy of an architecture is costly. In this paper, we proposed SemiNAS, a semi-supervised learning method for NAS. It leverages a small set of high-quality architecture-accuracy pairs to train an initial accuracy predictor, and then utilizes a large number of unlabeled architectures to further improve the accuracy predictor. Experiments on image classification tasks (NASBench-101 and ImageNet) and text to speech tasks (the low-resource setting and robustness setting) demonstrate 1) the efficiency of SemiNAS on reducing the computation cost over conventional NAS while achieving similar accuracy and 2) its effectiveness on improving the accuracy of both conventional NAS and one-shot NAS under similar computational cost. In the future, we will apply SemiNAS to more tasks such as automatic speech recognition, text summarization, etc. Furthermore, we will explore advanced semi-supervised learning methods~\cite{unsuperviseddataaug,mixmatch} to improve SemiNAS.


\section*{Broader Impact}
This work focuses on neural architecture search. It has the following potential positive impact in the society: 1) Improve the performance of neural networks for better applications. 2) Reduce the human efforts in designing neural architectures. At the same time, it may have some negative consequences because architecture search may cost many resources. 




\bibliography{main}
\bibliographystyle{plain}

\section{Appendix}

\subsection{Experiment Details}
\label{sec:expdetail}
\subsubsection{NASBench-101}
In the first setting where ,  new architectures are generated based on top  architectures at each iteration. In the second setting where ,  new architectures are generated based on top  architectures at each iteration. We use  as the trade-off parameter to balance the regression loss and the reconstruction loss.
\subsubsection{ImageNet}
We build the supernet following~\cite{proxylessnas}. We train the supernet on  GPUs for  steps with a batch size of  per card. We use SGD optimizer with a learning rate of  and decay the learning rate by a factor of 0.97 per epoch. The discovered architecture is trained on 4 P40 cards for  epochs with a batch size of  per card. We use the SGD optimizer with an initial learning rate of  and a cosine learning rate schedule~\cite{sgdr}.
\subsubsection{TTS}
We adopt the weight sharing mechanism for search and train the supernet on 4 GPUs. The discovered architecture is trained on the training set for k steps on 4 GPUs, with a batch size of K speech frames on each GPU. We use the Adam optimizer with  and follow the same learning rate schedule in~\cite{transformertts} with  warmup steps.


\subsection{Study of SemiNAS}
\label{sec:hyperpara}
In this section, we conduct experiments on NASBench-101 to study SemiNAS, including the number of unlabeled architectures  and the up-sampling ratio of labeled architectures.

\begin{figure}[htbp]
\centering
\subfigure[]{
		\label{fig:m}
		\includegraphics[width=0.40\columnwidth]{fig1.png}
	}
	\subfigure[]{
		\label{fig:up}
		\includegraphics[width=0.40\columnwidth]{fig2.png}
	}
\caption{Study of SemiNAS on NASBench-101. (a): Performances with different . (b): Performances with different up-sampling ratios.}
\label{fig:studyseminas}
\end{figure}

\paragraph{Number of unlabeled architectures } We study the effect of different  on SemiNAS. Given , we range  within , and plot the results in Fig.~\ref{fig:m}. Notice that  is equivalent to NAO without using any additional evaluated architectures. We can see that the test accuracy increases as  increases, indicating that utilizing unlabeled architectures indeed helps the training of the controller and generating better architectures.


\paragraph{Up-sampling ratio} Since  is much smaller than , we do up-sampling to balance the data. We study how the up-sampling ratio affects the effectiveness of SemiNAS on NASBench-101. We set  and range the up-sampling ratio in  where  means no up-sampling. The results are depicted in Figure~\ref{fig:up}. We can see that the final accuracy would benefit from up-sampling but will not continue to improve when the ratio is high~(e.g., larger than ).



\subsection{Discovered Architectures}
\label{sec:arch}
We show the discovered architectures for the tasks by SemiNAS.
\subsubsection{ImageNet}
We adopt the ProxylessNAS~\cite{proxylessnas} search space which is built on the MobileNet-V2~\cite{mobilenetv2} backbone. It contains several different stages and each stage consists of multiple layers. We search the operation of each individual layer. There are  candidate operations in the search space:
\begin{itemize}
    \item MBConv (k=3, r=3)
    \item MBConv (k=3, r=6)
    \item MBConv (k=5, r=3)
    \item MBConv (k=5, r=6)
    \item MBConv (k=7, r=3)
    \item MBConv (k=7, r=6)
    \item zero-out layer
\end{itemize}
where MBConv is mobile inverted bottleneck convolution, k is the kernel size and r is the expansion ratio~\cite{mobilenetv2}. Our discovered architecture for ImageNet is depicted in Fig.~\ref{fig:img}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\columnwidth]{Arch_by_SemiNAS_for_ImageNet.jpg}
    \caption{Architecture for ImageNet discovered by SemiNAS. “MBConv3” and “MBConv6” denote mobile inverted bottleneck convolution layer with an expansion ratio of 3 and 6 respectively.}
    \label{fig:img}
\end{figure}

\subsubsection{TTS}
We adopt encoder-decoder based architecture as the backbone, and search the operation of each layer. Candidate operations include:
\begin{itemize}
    \item Convolution layer with kernel size of 1
    \item Convolution layer with kernel size of 5
    \item Convolution layer with kernel size of 9
    \item Convolution layer with kernel size of 13
    \item Convolution layer with kernel size of 17
    \item Convolution layer with kernel size of 21
    \item Convolution layer with kernel size of 25
    \item Transformer layer with head number of 2
    \item Transformer layer with head number of 4
    \item Transformer layer with head number of 8
    \item LSTM layer
\end{itemize}
\paragraph{Low-Resource Setting}
The discovered architecture by SemiNAS for low-resource setting is shown in Fig.~\ref{fig:lowresource}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\columnwidth]{Arch_by_SemiNAS_for_low_resource_TTS.png}
    \caption{Architecture for low-resource setting discovered by SemiNAS.}
    \label{fig:lowresource}
\end{figure*}

\paragraph{Robustness Setting}
The discovered architecture by SemiNAS for robustness setting is shown in Fig.~\ref{fig:robust}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\columnwidth]{Arch_by_SemiNAS_for_robust_TTS.png}
    \caption{Architecture for robustness setting discovered by SemiNAS.}
    \label{fig:robust}
\end{figure*}

\subsection{Robustness Test Sentences}
\label{sec:robustnesssentences}
We list the 100 sentences we use for robustness setting:\\
a b c.\\
x y z.\\
hurry.\\
warehouse.\\
referendum.\\
is it free?\\
justifiable.\\
environment.\\
a debt runs.\\
gravitational.\\
cardboard film.\\
person thinking.\\
prepared killer.\\
aircraft torture.\\
allergic trouser.\\
strategic conduct.\\
worrying literature.\\
christmas is coming.\\
a pet dilemma thinks.\\
how was the math test?\\
good to the last drop.\\
an m b a agent listens.\\
a compromise disappears.\\
an axis of x y or z freezers.\\
she did her best to help him.\\
a backbone contests the chaos.\\
two a greater than two n nine.\\
don't step on the broken glass.\\
a damned flips into the patient.\\
a trade purges within the b b c.\\
i'd rather be a bird than a fish.\\
i hear that nancy is very pretty.\\
i want more detailed information.\\
please wait outside of the house.\\
n a s a exposure tunes the waffle.\\
a mist dictates within the monster.\\
a sketch ropes the middle ceremony.\\
every farewell explodes the career.\\
she folded here handkerchief neatly.\\
against the steam chooses the studio.\\
rock music approaches at high velocity.\\
nine adam baye study on the two pieces.\\
an unfriendly decay conveys the outcome.\\
abstraction is often one floor above you.\\
a played lady ranks any publicized preview.\\
he told us a very exciting adventure story.\\
on august twenty eight mary plays the piano.\\
into a controller beams a concrete terrorist.\\
i often see the time eleven eleven on clocks.\\
it was getting dark and we weren't there yet.\\
against every rhyme starves a choral apparatus.\\
everyone was busy so i went to the movie alone.\\
i checked to make sure that he was still alive.\\
a dominant vegetarian shies away from the g o p.\\
joe made the sugar cookies susan decorated them.\\
i want to buy a onesie but know it won't suit me.\\
a former override of q w e r t y outside the pope.\\
f b i says that c i a says i'll stay way from it.\\
any climbing dish listens to a cumbersome formula.\\
she wrote him a long letter but he didn't read it.\\
dear beauty is in the heat not physical i love you.\\
an appeal on january fifth duplicates a sharp queen.\\
a farewell solos on march twenty third shakes north.\\
he ran out of money so he had to stop playing poker.\\
for example a newspaper has only regional distribution t.\\
i currently have four windows open up and i don't know why.\\
next to my indirect vocal declines every unbearable academic.\\
opposite her sounding bag is a m c's configured thoroughfare.\\
from april eighth to the present i only smoke four cigarettes.\\
i will never be this young again every oh damn i just got older.\\
a generous continuum of amazon dot com is the conflicting worker.\\
she advised him to come back at once the wife lectures the blast.\\
a song can make or ruin a person's day if they let it get to them.\\
she did not cheat on the test for it was not the right thing to do.\\
he said he was not there yesterday however many people saw him there.\\
should we start class now or should we wait for everyone to get here?\\
if purple people eaters are real where do they find purple people to eat?\\
on november eighteenth eighteen twenty one a glittering gem is not enough.\\
a rocket from space x interacts with the individual beneath the soft flaw.\\
malls are great places to shop i can find everything i need under one roof.\\
i think i will buy the red car or  i will lease the blue one the faith nests.\\
italy is my favorite country in fact i plan to spend two weeks there next year.\\
i would have gotten w w w w dot google dot com but my attendance wasn't good enough.\\
nineteen twenty is when we are unique together until we realise we are all the same.\\
my mum tries to be cool by saying h t t p colon slash slash w w w b a i d u dot com.\\
he turned in the research paper on friday otherwise he emailed a s d f at yahoo dot org.\\
she works two jobs to make ends meet at least that was her reason for no having time to join us.\\
a remarkable well promotes the alphabet into the adjusted luck the dress dodges across my assault.\\
a b c d e f g h i j k l m n o p q r s t u v w x y z one two three four five six seven eight nine ten.\\
across the waste persists the wrong pacifier the washed passenger parades under the incorrect computer.\\
if the easter bunny and the tooth fairy had babies would they take your teeth and leave chocolate for you?\\
sometimes all you need to do is completely make an ass of yourself and laugh it off to realise that life isn't so bad after all.\\
she borrowed the book from him many years ago and hasn't yet returned it why won't the distinguishing love jump with the juvenile?\\
last friday in three week's time i saw a spotted striped blue worm shake hands with a legless lizard the lake is a long way from here.\\
i was very proud of my nickname throughout high school but today i couldn't be any different to what my nickname was the metal lusts the ranging captain charters the link.\\
i am happy to take your donation any amount will be greatly appreciated the waves were crashing on the shore it was a lovely sight the paradox sticks this bowl on top of a spontaneous tea.\\
a purple pig and a green donkey flew a kite in the middle of the night and ended up sunburn the contained error poses as a logical target the divorce attacks near a missing doom the opera fines the daily examiner into a murderer.\\
as the most famous singer-songwriter jay chou gave a perfect performance in beijing on may twenty fourth twenty fifth and twenty sixth twenty three all the fans thought highly of him and took pride in him all the tickets were sold out.\\
if you like tuna and tomato sauce try combining the two it's really not as bad as it sounds the body may perhaps compensates for the loss of a true metaphysics the clock within this blog and the clock on my laptop are on hour different from each other.\\
someone i know recently combined maple syrup and buttered popcorn thinking it would taste like caramel popcorn it didn't and they don't recommend anyone else do it either the gentleman marches around the principal the divorce attacks near a missing doom the color misprints a circular worry across the controversy.\\

\subsection{Demo of TTS}
We provide demo for both low-resource setting and robustness setting of TTS experiments. Specifically, we provide 10 test cases for each setting respectively and provide their ground-truth audio (if exist), generated audio by Transformer TTS and generated audio by SemiNAS. We provide the demo at this link\footnote{https://speechresearch.github.io/seminas} which is a web page and one can directly listen to the audio samples.

\subsection{Implementation Details}
We implement all the code in Pytorch~\cite{pytorch} with version 1.2. We implement the core architecture search algorithm following NAO~\cite{nao}\footnote{https://github.com/renqianluo/NAO\_pytorch}. For downstream tasks, we implement the code following corresponding baselines. For ImageNet experiment, we build our code based on ProxylessNAS implementation~\footnote{https://github.com/mit-han-lab/proxylessnas}. For TTS experiment, we build the code following Transformer TTS~\cite{transformertts} which is originally in Tensorflow.


\end{document}