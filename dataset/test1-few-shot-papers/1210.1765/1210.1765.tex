\documentclass[prodmode,acmtalg]{acmsmall}
\usepackage{natbib}

\renewcommand{\log}{\lg}
\newcommand{\Oh}[1]
    {\ensuremath{\mathcal{O}\!\left( {#1} \right)}}
\newcommand{\rank}
    {\ensuremath{\mathrm{rank}}}
\newcommand{\select}
    {\ensuremath{\mathrm{select}}}
\newcommand{\occ}
    {\ensuremath{\mathrm{occ}}}
\newcommand{\ignore}[1]{}
    
\begin{document}

\markboth{D. Belazzougui, T. Gagie and G. Navarro}{Frequency-Sensitive Queries in Ranges}

\title{Frequency-Sensitive Queries in Ranges}
\author{DJAMAL BELAZZOUGUI \affil{Department of Computer Science, University of Helsinki}
    TRAVIS GAGIE \affil{Department of Computer Science, University of Helsinki}
    GONZALO NAVARRO \affil{Department of Computer Science, University of Chile}}

\begin{abstract}
Karpinski and Nekrich (2008) introduced the problem of parameterized range majority, which asks to preprocess a string of length  such that, given the endpoints of a range, one can quickly find all the distinct elements whose relative frequencies in that range are more than a threshold .  Subsequent authors have reduced their time and space bounds such that, when  is given at preprocessing time, we need either  space and optimal  query time or linear space and  query time, where  is the alphabet size.  In this paper we give the first linear-space solution with optimal  query time.  For the case when  is given at query time, we significantly improve previous bounds, achieving either  space and optimal  query time or compressed space and  query time.  Along the way, we consider the complementary 
problem of parameterized range minority that was recently introduced by Chan et al.\ (2012), who achieved linear space and  query time even for variable .  We improve their solution to use either nearly optimally compressed space with no slowdown, or optimally compressed space with nearly no slowdown.  Some of our intermediate results, such as density-sensitive query time for one-dimensional range counting, may be of independent interest.
\end{abstract}

\category{X.0.0}{Data Structures}{How is the 2012 classification supposed to work?}

\terms{Arrays, Range Queries}

\keywords{Parameterized range majority and minority, ...}

\acmformat{Djamal Belazzougui, Travis Gagie, and Gonzalo Navarro, 2014. Frequency-Sensitive Queries in Ranges.}

\begin{bottomstuff}
This work is supported by somebody.

Authors' addresses: D. Belazzougui and T. Gagie, Department of Computer Science, University of Helsinki; G. Navarro, Department of Computer Science, University of Chile.
\end{bottomstuff}

\maketitle

\section{Introduction} \label{sec:intro}

Finding frequent elements in a dataset is a fundamental operation in data mining.  Finding the most frequent elements can be challenging when all the distinct elements have nearly equal frequencies and we do not have the resources to compute all their frequencies exactly.  In some cases, however, we are interested in the most frequent elements only if they really are frequent.  For example, Misra and Gries~\cite{MG82} showed how, given a string and a threshold  with , with two passes and  words of space we can find all the distinct elements in a string whose relative frequencies are at least .  These elements are called the -majorities of the string.  Misra and Gries' algorithm was rediscovered by Demaine, L\'opez-Ortiz and Munro~\cite{DLM02}, who noted it can be made to run in  time per element on a word RAM with -bit words, where  is the length of the string, which is the model we use; it was then rediscovered again by Karp, 
Shenker and Papadimitriou~\cite{KSP03}.  As Cormode and Muthukrishnan~\cite{CM03} put it, ``papers on frequent items are a frequent item!''

Krizanc, Morin and Smid~\cite{KMS05} introduced the problem of preprocessing the string such that later, given the endpoints of a range, we can quickly return the mode of that range (i.e., the most frequent element).  They gave two solutions, one of which takes  space for any fixed positive , and answers queries in  time; the other takes  space and answers queries in  time.  Petersen~\cite{Pet08} reduced Krizanc et al.'s first time bound to  for any fixed non-negative , and Petersen and Grabowski~\cite{PG09} reduced the second space bound to .  Chan et al.~\cite{CDLMW12} recently gave a linear-space solution that answers queries in  time.  They also gave evidence suggesting we cannot easily achieve query time substantially smaller than  using linear space; however, the best known lower bound, 
by Greve et al.~\cite{GJLT10}, says only that we cannot achieve query time  using  words of  bits each.  Because of the difficulty of supporting range mode queries, Bose et al.~\cite{BKMT05} and Greve et al.~\cite{GJLT10} considered the problem of approximate range mode, for which we are asked to return an element whose frequency is at least a constant fraction of the mode's frequency.

Karpinski and Nekrich~\cite{KN08} took a different direction, analogous to Misra and Gries' approach, when they introduced the problem of preprocessing the string such that later, given the endpoints of a range, we can quickly return the -majorities of that range.  We refer to this problem as parameterized range majority.  Assuming  is given when we are preprocessing the string, they showed how we can store the string in  space and answer queries in  time.  They also gave bounds for dynamic and higher-dimensional versions.  Durocher et al.~\cite{DHMNS13} independently posed the same problem and showed how we can store the string in  space and answer queries in  time.  Notice that, because there can be up to  distinct elements to return, this time bound is worst-case optimal.  Gagie et al.~\cite{GHMN11} showed how to store the string in compressed space --- i.e.,  bits, where 
 is the entropy of the distribution of elements in the string --- such that we can answer queries in  time.  They also showed how to drop the assumption that  is fixed and simultaneously achieve optimal query time, at the cost of increasing the space bound by a -factor.  That is, they gave a data structure that stores the string in  space such that later, given the endpoints of a range and , we can return the -majorities of that range in  time.  Chan et al.~\cite{CDSW12} recently gave another solution for variable , which also has  query time but uses  space.  As far as we know, these are all the relevant bounds for Karpinski and Nekrich's original exact, static, one-dimensional problem, both for fixed and variable ; they are summarized in Table~\ref{tab:results} together with our own results.  Related work includes Elmasry et al.'s~\cite{EHMN11} solution for the dynamic 
version and Lai, Poon and Shi's~\cite{LPS08} and Wei and Yi's~\cite{WY11} approximate solutions for the dynamic version.

\begin{table}
\tbl{Results for the problem of parameterized range majority on a string of length  over an alphabet of size  in which the distribution of the elements has entropy .\label{tab:results}}
{\begin{tabular}{l@{\hspace{2ex}}|@{\hspace{2ex}}c@{\hspace{3ex}}c@{\hspace{3ex}}c}
source & space & time & variable \-2ex]
\cite{KN08} &  words &  & no\1ex]
\cite{GHMN11} &  bits &  & no\1ex]
\hline\1ex]
\cite{CDSW12} &  words &  & yes\1ex]
Theorem~\ref{thm:small maj better} &  bits &  & yes\1 + \epsilon) n H +o(n)1 + \epsilon) n H + \Oh{n}f (n) = \omega (1)n H + o(n)(H+1)n H + o (n)(H+1)1 + \epsilon) n H+o(n)S [1..n]S [k]S [1..k]S [k] = a\sigma = 2n H + \Oh{n / \log^c n}\sigma = \log^{\mathcal{O} (1)} nn H + o (n)\sigma < n1 + \epsilon) n H + o (n)o (n) (H + 1)f (n) = \omega (1)n H + o (n)(H + 1)o (\log (\log \sigma / \log \log n))n \log^{\mathcal{O} (1)} nS [1..n]C [1..n]C [k]S [k]S [1..k - 1]S [k]S [k]S [i..j]i \leq k \leq jC [k] < iS [i..j]C [i..j]C [m] < iS [m]C [i..m - 1]C [m + 1..j]C [k] = S.\select_{S [k]} \left( \rule{0ex}{2ex} S.\rank_{S [k]} (k) - 1 \right)1 + \epsilon) n H+o(n)occ=\occ (a, S [i..j])\lfloor 1 / \tau \rfloor + 1\lfloor 1 / \tau \rfloorS [1..n]1 + \epsilon) n H + \Oh{n}\lfloor 1 / \tau \rfloor + 1S [i..j]S [i..j]S [k]S [i..j]r + \lceil \tau (j - i + 1) \rceil - 1)S [j]S [i..j]S.\select_a \left( \rule{0ex}{2.5ex} S.\rank_a (k) + \lceil \tau (j - i + 1) \rceil - 1 \right) > j\,;S [k] = aS.\rank_a (k)1 + \epsilon) n H + \Oh{n}f (n) = \omega (1)n H + \Oh{n} + o (n H)f (n) = \omega (1)n H + \Oh{n} + o (n H)n H + o (n (H + 1))f (n) = \omega (1)n H + o(n)(H+1))o (\log (\log \sigma / \log w))n \log^{\mathcal{O} (1)} nS [1..n]0 \leq b \leq \lfloor \log n \rfloorF_b [1..n]F_b [k] = 1S [k]\tau 2^bS [k..k + 2^{b + 1} - 1]\lfloor \log (j
- i + 1) \rfloor = bS_b [F_b.\rank_1 (i)..F_b.\rank_1 (j)]S [i..j]S [i..j]\tau 2^bS [i..j + 2^{b + 1} - 1] \subset S [i..i + 2^{b + 2}]S [i..j]S [i..j]\tau (j - i + 1) \geq \tau 2^bS [i..j] \subset S [i..i + 2^{b + 1} - 1]S [i..j]S [i..j]\lfloor \log n \rfloor1, 1 / 2, 1 / 4, \ldots, 1 / 2^{\lceil \log n \rceil}1 / 2^{\lceil \log (1 / \tau) \rceil}S [i..j]1 / \tau \geq \sigma\Oh{\sigma} = \Oh{1 / \tau}S [i..j]S [k]1 / 2^t)1 / 2^{t'})t' \geq t1 / 2^{\lceil \log (1 / \tau) \rceil}1, 1 / 2, 1 / 4, \ldots, 1 / 2^{\lceil \log (1 / \tau) \rceil}\Oh{\sum_{t = 0}^{2^{\lceil \log (1 / \tau) \rceil}} 2^t} = \Oh{1 / \tau}0 \leq t \leq \lceil \log \sigma \rceil1 / 2^tF_b^t [k] = 1S [k]S [k..k + 2^{b + 1} - 1]0 \leq b \leq \lfloor \log n \rfloor1 \leq k \leq nF_b^t [k] = 1n (\lfloor \log n \rfloor + 1)n \lceil \log n \rceil \lceil
\log \sigma \rceilF_b^0, \ldots, F_b^{\lceil
\log \sigma \rceil}T_b [1..n]T_b [k] =
tF_b^t [k] = 1F_b^0, \ldots, F_b^{\lceil \log
\sigma \rceil}n H + o (n (H + 1))0 \leq t \leq \lceil \log \sigma \rceil\lfloor \log (2^t \log \log \sigma) \rfloor \leq b \leq \lfloor \log n \rfloorG_b^t [1..n]G_b^t [k] = 1S [k]S [k - 2^{b + 1}..k + 2^{b + 1}]S [k]\lceil \log \sigma \rceil\lfloor \log (2^t \log \log \sigma) \rfloor\lfloor \log n \rfloor\Oh{\frac{n \log \sigma \log \log \log \sigma}{\log \log \sigma} + \frac{n}{\log n}} = o (n \log \sigma)n H + o (n \log \sigma)\lfloor \log (j - i + 1) \rfloor
< \left\lfloor \log \left( 2^{\lceil \log (1 / \tau) \rceil} \log \log \sigma \right) \right\rfloor\,,S [i..j]\Oh{j - i} = \Oh{(1 / \tau) \log \log \sigma}t = \lceil \log (1 / \tau) \rceilb = \lfloor \log (j - i + 1) \rfloor \geq \lfloor \log (2^t \log \log \sigma) \rfloorS [i..j]S [i..j]S [i..j]S [i..j]S [i..j]S [i..j]i \leq k \leq jS [k - 2^{b + 1}..k + 2^{b + 1}]S [i..j]n H + o (n \log \sigma)n H + o(n)(H+1)1 / \tau \geq \sigma1 / \tau < \sigma1 + \epsilon) n H + o(n)1 / \tau = \log^{\mathcal{O} (1)} nS [i..j]S [i..j]\tau = 1\Oh{\left(1 + 2 + 4 + \ldots + 2^{\left\lceil \log \frac{j - i + 1}{\occ (x, S [i..j])} \right\rceil} \right) \log \log \sigma}
= \Oh{\frac{(j - i + 1) \log \log \sigma}{\occ (x, S [i..j])}}\frac{j - i + 1}{2^{\left\lceil \log \frac{j - i + 1}{\occ (x, S [i..j])} \right\rceil}}S [i..j]n H + o (n) (H + 1)S [i..j] in  time.
\end{theorem}

\section{Conclusions} \label{sec:conclusions}

We have given the first linear-space data structure for parameterized range majority with query time , which is worst-case optimal in terms of  and .  Moreover, we have improved the space bounds for parameterized range majority and minority in the important case of variable .  For parameterized range majority with variable , we have achieved nearly linear space and worst-case optimal query time, or compressed space with a slight slowdown.  For parameterized range minority, we have improved Chan et al.'s solution to use nearly compressed space with no slowdown or compressed space with nearly no slowdown.  We leave as an open problem achieving linear or compressed space with  query time for variable , or showing that this is impossible.

\begin{acks}
Many thanks to Patrick Nicholson for helpful comments.
\end{acks}

\bibliographystyle{authordate1}
\bibliography{wads13}

\end{document}
