\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{color}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{paralist}
\usepackage{enumitem}
\usepackage{xcolor}

\definecolor{mygray}{HTML}{C6C6C6}
\definecolor{myblue}{HTML}{1A6EAA}

\newcommand{\add}[1]{{\color{myblue}#1}}
\newcommand{\del}[1]{{\color{mygray}#1}}
\newcommand{\replace}[2]{\del{#1}\add{#2}}
\newcommand{\comment}[1]{{\color{orange}#1}}

\renewcommand{\add}[1]{#1}
\renewcommand{\del}[1]{}
\renewcommand{\replace}[2]{#2}
\renewcommand{\comment}[1]{}

\newcommand{\conv}[1]{\mathrm{conv_{#1\!\times\!#1}}}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{2343} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

\title{EAST: An Efficient and Accurate Scene Text Detector}

\author{Xinyu Zhou}
\author{Cong Yao}
\author{He Wen}
\author{Yuzhi Wang}
\author{Shuchang Zhou}
\author{Weiran He}
\author{Jiajun Liang}
\affil{Megvii Technology Inc., Beijing, China \\ \{zxy, yaocong, wenhe, wangyuzhi, zsc, hwr, liangjiajun\}@megvii.com}

\maketitle
\thispagestyle{empty}

\begin{abstract}

Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.
\vspace{-4mm}

\end{abstract}
 
\section{Introduction} \label{Sec:Introduction}

Recently, extracting and understanding textual information embodied in natural scenes have become increasingly important and popular, which is evidenced by the unprecedented large numbers of participants of the ICDAR series contests ~\cite{Ref:Shahab2011, Ref:Karatzas2013, Ref:Karatzas2015} and the launch of the TRAIT 2016 evaluation by NIST~\cite{Ref:TRAIT2015}.

Text detection, as a prerequisite of the subsequent processes, plays a critical role in the whole procedure of textual information extraction and understanding. Previous text detection approaches~\cite{Ref:Busta2015, Ref:Tian2015, Ref:Jaderberg2016B, gupta2016synthetic, Ref:Zhang2016} have already obtained promising performances on various benchmarks in this field. The core of text detection is the design of features to distinguish text from backgrounds. Traditionally, features are manually designed~\cite{Ref:Epshtein2010, Ref:Neumann2010, Ref:Yao2012, Ref:Huang2013, Ref:Neumann2012, Ref:Yin2014} to capture the properties of scene text, while in deep learning based methods~\cite{Ref:Coates2011, Ref:Jaderberg2014, Ref:Huang2014, Ref:Jaderberg2016B, gupta2016synthetic, Ref:Zhang2016} effective features are directly learned from training data.

\begin{figure}[!t]
\centering\begin{tikzpicture}
\pgfplotsset{compat=1.8}
\pgfplotsset{every tick label/.append style={font=\footnotesize}}
\pgfplotsset{grid style={dotted,gray}}
\begin{axis}[
xmin=0, xmax=17,
ymin=0.49, ymax=0.85,
grid=both,
axis lines=middle,
line width=.8pt,
width=1.1\linewidth,
height=.7\linewidth,
xlabel={\small Speed (FPS)},
ylabel={\small F-score}
]

\addplot [only marks, draw=black, fill=blue, mark size=3pt]
table {x     y
+7.14 +0.6085
+1.61 +0.6477
+0.476 +0.5358
};
\addplot [only marks, draw=black, fill=red, mark size=3pt]
table {x           y
+13.245e+00 +0.7820
+16.779e+00 +0.7571
};

\node at (axis cs:7.3,0.58)[
  anchor=base west,
  text=black,
  rotate=0.0,
  align=left
]{Tian~\etal~\cite{tian2016detecting}\0.648@1.61fps)};

\node at (axis cs:0.6,0.505)[
  anchor=base west,
  text=black,
  rotate=0.0,
  align=left
]{Zhang~\etal~\cite{Ref:Zhang2016}\0.782@13.2fps)};

\node at (axis cs:16.8,0.69)[
  anchor=base east,
  text=black,
  rotate=0.0,
  align=right
]{\bf Ours+PVANet\
g_i &= \begin{cases}
  \mathrm{unpool}(h_i) & \text{if}\quad i \le 3 \\
  \conv{3}(h_i) & \text{if}\quad i = 4
\end{cases} \\
h_i &= \begin{cases}
  f_i & \text{if}\quad i = 1 \\
  \conv{3}(\conv{1}([g_{i-1}; f_i])) & \text{otherwise}
\end{cases}

\begin{aligned}
  r_i =\min ( & \mathrm{D}(p_i, p_{(i \bmod 4) + 1}), \\
                & \mathrm{D}(p_i, p_{((i + 2) \bmod 4) + 1})) \\
\end{aligned}

  L = L_\text{s} + \lambda_\text{g} L_\text{g}

\begin{aligned}
    L_\text{s} & = \bxent(\mathbf{\hat{Y}}, \mathbf{Y^*}) \\
        & = -\beta \mathbf{Y^*} \log \mathbf{\hat{Y}} - (1 - \beta) (1 - \mathbf{Y^*}) \log (1 - \mathbf{\hat{Y}})
\end{aligned}

\begin{aligned}
  \beta = 1 - \dfrac{\sum_{y^* \in \mathbf{Y^*}} y^*}{|\mathbf{Y^*}|}.
\end{aligned}

\begin{aligned}
  L_\text{AABB} & = -\log \mathrm{IoU}(\mathbf{\hat{R}}, \mathbf{R^*})  = -\log \dfrac{|\mathbf{\hat{R}} \cap \mathbf{R^*}|}{|\mathbf{\hat{R}} \cup \mathbf{R^*}|}
\end{aligned}

\begin{aligned}
w_{\mathbf{i}} & = \min (\hat{d}_2, d^*_2) + \min(\hat{d}_4, d^*_4) \\
h_{\mathbf{i}} & = \min (\hat{d}_1, d^*_1) + \min(\hat{d}_3, d^*_3) \\
\end{aligned}

|\mathbf{\hat{R}} \cup \mathbf{R^*}| = |\mathbf{\hat{R}}| + |\mathbf{R^*}| - |\mathbf{\hat{R}} \cap \mathbf{R^*}|.

L_{\theta}(\hat{\theta}, \theta^*) = 1 - \cos (\hat{\theta} - \theta^*).

L_\text{g}= L_\text{AABB} + \lambda_{\theta}L_{\theta}.

  \mathrm{C}_{\mathbf{Q}} = \{x_1, y_1, x_2, y_2, \dots, x_4, y_4\}

\begin{aligned}
    L_\text{g}& = L_{\text{QUAD}}(\mathbf{\hat{\mathbf{Q}}}, \mathbf{Q^*}) \\
    & = \min_{\tilde{\mathbf{Q}} \in P_{\mathbf{Q}^*}}
        \sum_{\substack{c_i \in \mathrm{C}_{\mathbf{Q}}, \\ \tilde{c}_i \in \mathrm{C}_{\mathbf{\tilde{Q}}}}}
        \frac{\mathrm{smoothed}_{L1} (c_i - \tilde{c}_i)}{8\times{N}_{\mathbf{Q}^*}}  \\
\end{aligned}

    {N}_{\mathbf{Q}^*} = \min_{i=1}^4 D(p_i, p_{(i \bmod 4)+1}),

and  is the set of all equivalent quadrangles of  with different vertices ordering. This ordering permutation
is required since the annotations of quadrangles in the public training datasets are inconsistent.

\subsection{Training} \label{training}

The network is trained end-to-end using ADAM~\cite{kingma2014adam} optimizer.
To speed up learning, we uniformly sample 512x512 crops from images to form a minibatch of size 24. Learning rate of ADAM starts from 1e-3, decays to one-tenth every 27300 minibatches, and stops at 1e-5. The network is trained until performance stops improving.

\subsection{Locality-Aware NMS}

To form the final results, the geometries survived after thresholding should be merged by NMS. A na\"{i}ve NMS algorithm runs in ï¼Œ
where  is the number of candidate geometries, which is unacceptable as we are facing tens of thousands of geometries from dense predictions.

Under the assumption that the geometries from nearby pixels tend to be highly correlated, we proposed to merge the geometries row by row, and while merging geometries in the same row, we will iteratively merge the geometry currently encountered with the last merged one. This improved technique runs in  in best scenarios\footnote{Consider the case that only a single text line appears the image. In such case, all geometries will be highly overlapped if the network is sufficiently powerful}. Even though its worst case is the same as the na\"{i}ve one, as long as the locality assumption holds, the algorithm runs sufficiently fast in practice. The procedure is summarized in Algorithm~\ref{algo:nms-hier}

It is worth mentioning that, in , the coordinates of merged quadrangle are weight-averaged by the scores of two given quadrangles. To be specific, if , then  and , where  is one of the coordinates of  subscripted by , and  is the score of geometry .

In fact, there is a subtle difference that we are "averaging" rather than "selecting" geometries, as in a standard NMS procedure will do, acting as a voting mechanism, which in turn introduces a stabilization effect when feeding videos. Nonetheless, we still adopt the word "NMS" for functional description.









\iftrue
\begin{algorithm}[t]
\caption{Locality-Aware NMS}\label{algo:nms-hier}
\begin{algorithmic}[1]
\Function{NMSLocality}{}

\State ,~~
\For { in row first order}
  \If {}
    \State 
    \Else
    \If {}
      \State 
        \EndIf
        \State 
    \EndIf
\EndFor

\If {}
  \State 
\EndIf

\State \Return 

\EndFunction
\end{algorithmic}
\end{algorithm}
\vspace{-2mm}
\fi



 \section{Experiments}

To compare the proposed algorithm with existing methods, we conducted qualitative and quantitative experiments on three public benchmarks: ICDAR2015, COCO-Text and MSRA-TD500. 

\subsection{Benchmark Datasets}

\textbf{ICDAR 2015} is used in Challenge 4 of ICDAR 2015 Robust Reading Competition~\cite{Ref:Karatzas2015}. It includes a total of 1500 pictures, 1000 of which are used for training and the remaining are for testing. The text regions are annotated by 4 vertices of the quadrangle, corresponding to the QUAD geometry in this paper. We also generate RBOX output by fitting a rotated rectangle which has the minimum area. These images are taken by Google Glass in an incidental way. Therefore text in the scene can be in arbitrary orientations, or suffer from motion blur and low resolution. We also used the 229 training images from ICDAR 2013.


\textbf{COCO-Text}~\cite{Ref:Veit2016} is the largest text detection dataset to date. It reuses the images from MS-COCO dataset~\cite{lin2014microsoft}. A total of 63,686 images are annotated, in which 43,686 are chosen to be the training set and the rest 20,000 for testing. Word regions are annotated in the form of axis-aligned bounding box (AABB), which is a special case of RBOX. For this dataset, we set angle  to zero. We use the same data processing and test method as in ICDAR 2015.

\textbf{MSRA-TD500}~\cite{Ref:Yao2012} is a dataset comprises of 300 training images and 200 test images. Text regions are of arbitrary orientations and annotated at sentence level. Different from the other datasets, it contains text in both English and Chinese. The text regions are annotated in RBOX format. Since the number of training images is too few to learn a deep model, we also harness 400 images from HUST-TR400 dataset~\cite{yao2014unified} as training data.


\subsection{Base Networks}

As except for COCO-Text, all text detection datasets are relatively small compared to the datasets for general object detection\cite{krizhevsky2012imagenet,lin2014microsoft}, therefore if a single network is adopted for all the benchmarks, it may suffer from either over-fitting or under-fitting. We experimented with three different base networks, with different output geometries, on all the datasets to evaluate the proposed framework. These networks are summarized in Tab.~\ref{tab:base-models}.

\textbf{VGG16}~\cite{simonyan2014very} is widely used as base network in many tasks~\cite{ren2015faster,xie2015holistically} to support subsequent task-specific fine-tuning, including text detection~\cite{tian2016detecting,Ref:Zhang2016,zhong2016deeptext,gupta2016synthetic}. There are two drawbacks of this network: (1). The receptive field for this network is small. Each pixel in output of conv5\_3 only has a receptive field of 196. (2). It is a rather large network.

\textbf{PVANET} is a light weight network introduced in ~\cite{KimKH2016arXivPVANET}, aiming as a substitution of the feature extractor in Faster-RCNN~\cite{ren2015faster} framework. Since it is too small for GPU to fully utilizes computation parallelism, we also adopt~\textit{PVANET2x} that doubles the channels of the original PVANET, exploiting more computation parallelism while running slightly slower than PVANET. This is detailed in Sec.~\ref{sec:speed}. The receptive field of the output of the last convolution layer is 809, which is much larger than VGG16.

The models are pre-trained on the ImageNet dataset~\cite{krizhevsky2012imagenet}.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Network} & \textbf{Description} \\
\hline\hline
PVANET~\cite{KimKH2016arXivPVANET} & small and fast model \\\hline
PVANET2x~\cite{KimKH2016arXivPVANET} & PVANET with 2x number of channels \\\hline
VGG16~\cite{simonyan2014very} & commonly used model \\\hline
\end{tabular}
\end{center}
\caption{Base Models} \label{tab:base-models}
\vspace{-4mm}
\end{table}

\subsection{Qualitative Results}

\begin{figure*}
\begin{center}

\begin{tabular}{ccc}
	\begin{minipage}{0.3\linewidth}
		\includegraphics[width=\linewidth]{figs/all-figures/6.jpg}

		\vspace{2mm}

		\includegraphics[width=\linewidth]{figs/all-figures/7.jpg}

        \vspace{1mm}

        \centering{(a)}
	\end{minipage}
&
	\begin{minipage}{0.22\linewidth}
		\includegraphics[width=\linewidth]{figs/all-figures/9.jpg}

		\vspace{3mm}

		\includegraphics[width=\linewidth]{figs/all-figures/8.jpg}

        \vspace{1mm}

        \centering{(b)}
	\end{minipage}
&
	\begin{minipage}{0.22\linewidth}
		\includegraphics[width=\linewidth]{figs/all-figures/2.jpg}

		\vspace{5mm}

		\includegraphics[width=\linewidth]{figs/all-figures/1.jpg}

        \vspace{1mm}

        \centering{(c)}
	\end{minipage} \\

\end{tabular}

\end{center}
\caption{Qualitative results of the proposed algorithm. (a) ICDAR 2015. (b) MSRA-TD500. (c) COCO-Text.} \label{fig:qualitative}
\vspace{-3mm}
\end{figure*}


\begin{figure}[!t]
\begin{center}
\begin{tabular}{ccc}
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{figs/intermediate-result/box_xy0.png}
        \centering{(a)}


		\includegraphics[width=\linewidth]{figs/intermediate-result/angle.png}
       	\centering{(c)}

	\end{minipage}
&
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{figs/intermediate-result/box_xy1.png}
        \centering{(b)}


		\includegraphics[width=\linewidth]{figs/intermediate-result/bfh_illu.png}
        \centering{(d)}
	\end{minipage}
\end{tabular}
\end{center}

\vspace{-1mm}
\caption{Intermediate results of the proposed algorithm. (a) Estimated geometry map for  and . (b) Estimated geometry map for  and . (c) Estimated angle map for text instances. (d) Predicted rotated rectangles of text instances. Maps in (a), (b) and (c) are color-coded to represent variance (for  and ) and invariance (for angle) in an pixel-wise manner. Note that in the geometry maps only the values of foreground pixels are valid.}
\label{fig:intermediate-output}
\vspace{-4mm}
\end{figure}


Fig.~\ref{fig:qualitative} depicts several detection examples by the proposed algorithm. It is able to handle various challenging scenarios, such as non-uniform illumination, low resolution, varying orientation and perspective distortion. Moreover, due to the voting mechanism in the NMS procedure, the proposed method shows a high level of stability on videos with various forms of text instances\footnote{Online video: \url{https://youtu.be/o5asMTdhmvA}. Note that each frame in the video is processed independently.}.

The intermediate results of the proposed method are illustrated in Fig.~\ref{fig:intermediate-output}. As can be seen, the trained model produces highly accurate geometry maps and score map, in which detections of text instances in varying orientations are easily formed.

\subsection{Quantitative Results}

As shown in Tab.~\ref{tab:result-icdar15} and Tab.~\ref{tab:result-coco-text}, our approach outperforms previous state-of-the-art methods by a large margin on ICDAR 2015 and COCO-Text.

In ICDAR 2015 Challenge 4, when images are fed at their original scale, the proposed method achieves an F-score of 0.7820. When tested at multiple scales \footnote{At relative scales of 0.5, 0.7, 1.0, 1.4, and 2.0.} using the same network, our method reaches 0.8072 in F-score, which is nearly 0.16  higher than the best method~\cite{yao2016scene} in terms of absolute value (0.8072 vs. 0.6477).

Comparing the results using VGG16 network\cite{tian2016detecting,Ref:Zhang2016,yao2016scene}, the proposed method also outperforms best previous work~\cite{yao2016scene} by 0.0924 when using QUAD output, 0.116 when using RBOX output. Meanwhile these networks are quite efficient, as will be shown in Sec.\ref{sec:speed}.

In COCO-Text, all of the three settings of the proposed algorithm result in higher accuracy than previous top performer~\cite{yao2016scene}. Specifically, the improvement over~\cite{yao2016scene} in F-score is 0.0614 while that in recall is 0.053, which confirm the advantage of the proposed algorithm, considering that COCO-Text is the largest and most challenging benchmark to date. Note that we also included the results from~\cite{Ref:Veit2016} as reference, but these results are actually not valid baselines, since the methods (A, B and C) are used in data annotation.

The improvements of the proposed algorithm over previous methods prove that a simple text detection pipeline, which directly targets the final goal and eliminating redundant processes, can beat elaborated pipelines, even those integrated with large neural network models.


As shown in Tab.~\ref{tab:result-msra-td500}, on MSRA-TD500  all of the three settings of our method achieve excellent results. The F-score of the best performer (Ours+PVANET2x) is slightly higher than that of~\cite{yao2016scene}. Compared with the method of Zhang~\etal~\cite{Ref:Zhang2016}, the previous published state-of-the-art system, the best performer (Ours+PVANET2x) obtains an improvement of 0.0208 in F-score and 0.0428 in precision.

Note that on MSRA-TD500 our algorithm equipped with VGG16 performs much poorer than that with PVANET and PVANET2x (0.7023 vs. 0.7445 and 0.7608), the main reason is that the effective receptive field of VGG16 is smaller than that of PVANET and PVANET2x, while the evaluation protocol of MSRA-TD500 requires text detection algorithms output line level instead of word level predictions.


\begin{table}
\small
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Recall} & \textbf{Precision} & \textbf{F-score} \\\hline\hline

\begin{scriptsize} Ours + PVANET2x RBOX MS* \end{scriptsize}&
\begin{footnotesize} \textbf{0.7833} \end{footnotesize}&
\begin{footnotesize} 0.8327 \end{footnotesize}&
\begin{footnotesize} \textbf{0.8072} \end{footnotesize}\\\hline

\begin{footnotesize} Ours + PVANET2x RBOX \end{footnotesize} &
\begin{footnotesize} 0.7347 \end{footnotesize}&
\begin{footnotesize} \textbf{0.8357} \end{footnotesize}&
\begin{footnotesize} \textbf{0.7820} \end{footnotesize}\\\hline



\begin{footnotesize} Ours + PVANET2x QUAD \end{footnotesize}&
\begin{footnotesize} 0.7419 \end{footnotesize}&
\begin{footnotesize} 0.8018 \end{footnotesize}&
\begin{footnotesize} 0.7707 \end{footnotesize}\\\hline


\begin{footnotesize} Ours + VGG16 RBOX \end{footnotesize}&
\begin{footnotesize} 0.7275 \end{footnotesize}&
\begin{footnotesize} 0.8046 \end{footnotesize}&
\begin{footnotesize} 0.7641 \end{footnotesize}\\\hline






\begin{footnotesize} Ours + PVANET RBOX \end{footnotesize}&
\begin{footnotesize} 0.7135 \end{footnotesize}&
\begin{footnotesize} 0.8063 \end{footnotesize}&
\begin{footnotesize} 0.7571 \end{footnotesize}\\\hline






\begin{footnotesize} Ours + PVANET QUAD\end{footnotesize}&
\begin{footnotesize} 0.6856 \end{footnotesize}&
\begin{footnotesize} 0.8119 \end{footnotesize}&
\begin{footnotesize} 0.7434 \end{footnotesize}\\\hline






\begin{footnotesize} Ours + VGG16 QUAD \end{footnotesize}&
\begin{footnotesize} 0.6895 \end{footnotesize}&
\begin{footnotesize} 0.7987 \end{footnotesize}&
\begin{footnotesize} 0.7401 \end{footnotesize}\\\hline




\begin{footnotesize} Yao~\etal~\cite{yao2016scene} \end{footnotesize}&
\begin{footnotesize} 0.5869 \end{footnotesize}&
\begin{footnotesize} 0.7226 \end{footnotesize}&
\begin{footnotesize} 0.6477 \end{footnotesize}\\\hline
\begin{footnotesize} Tian~\etal~\cite{tian2016detecting} \end{footnotesize}&
\begin{footnotesize} 0.5156 \end{footnotesize}&
\begin{footnotesize} 0.7422 \end{footnotesize}&
\begin{footnotesize} 0.6085 \end{footnotesize}\\\hline
\begin{footnotesize}Zhang~\etal~\cite{Ref:Zhang2016} \end{footnotesize}&
\begin{footnotesize} 0.4309 \end{footnotesize}&
\begin{footnotesize} 0.7081 \end{footnotesize}&
\begin{footnotesize} 0.5358 \end{footnotesize}\\\hline
\begin{footnotesize} StradVision2~\cite{Ref:Karatzas2015} \end{footnotesize}&
\begin{footnotesize} 0.3674 \end{footnotesize}&
\begin{footnotesize} 0.7746 \end{footnotesize}&
\begin{footnotesize} 0.4984 \end{footnotesize}\\\hline
\begin{footnotesize} StradVision1~\cite{Ref:Karatzas2015} \end{footnotesize}&
\begin{footnotesize} 0.4627 \end{footnotesize}&
\begin{footnotesize} 0.5339 \end{footnotesize}&
\begin{footnotesize} 0.4957 \end{footnotesize}\\\hline
\begin{footnotesize} NJU~\cite{Ref:Karatzas2015} \end{footnotesize} &
\begin{footnotesize} 0.3625 \end{footnotesize} &
\begin{footnotesize} 0.7044 \end{footnotesize} &
\begin{footnotesize} 0.4787 \end{footnotesize} \\\hline
\begin{footnotesize} AJOU~\cite{Ref:Koo2013} \end{footnotesize}                  &
\begin{footnotesize} 0.4694 \end{footnotesize} &
\begin{footnotesize} 0.4726 \end{footnotesize} &
\begin{footnotesize} 0.4710 \end{footnotesize} \\\hline
\begin{footnotesize} Deep2Text-MO~\cite{Ref:Yin2014, Ref:Yin2015} \end{footnotesize}                  &
\begin{footnotesize} 0.3211 \end{footnotesize} &
\begin{footnotesize} 0.4959 \end{footnotesize} &
\begin{footnotesize} 0.3898 \end{footnotesize} \\\hline
\begin{footnotesize} CNN MSER~\cite{Ref:Karatzas2015} \end{footnotesize}                  &
\begin{footnotesize} 0.3442 \end{footnotesize} &
\begin{footnotesize} 0.3471 \end{footnotesize} &
\begin{footnotesize} 0.3457 \end{footnotesize} \\\hline
\end{tabular}
\end{center}

\caption{Results on ICDAR 2015 Challenge 4 Incidental Scene Text Localization task. MS means multi-scale testing.} \label{tab:result-icdar15}
\vspace{-2mm}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Recall} & \textbf{Precision} & \textbf{F-score} \\\hline\hline


\begin{footnotesize} Ours + VGG16\end{footnotesize} &
\begin{footnotesize} \textbf{0.324} \end{footnotesize} &
\begin{footnotesize} \textbf{0.5039} \end{footnotesize} &
\begin{footnotesize} \textbf{0.3945} \end{footnotesize} \\\hline



\begin{footnotesize} Ours + PVANET2x \end{footnotesize} &
\begin{footnotesize} 0.340 \end{footnotesize} &
\begin{footnotesize} 0.406 \end{footnotesize} &
\begin{footnotesize} 0.3701 \end{footnotesize} \\\hline



\begin{footnotesize} Ours + PVANET \end{footnotesize} &
\begin{footnotesize} 0.302 \end{footnotesize} &
\begin{footnotesize} 0.3981 \end{footnotesize} &
\begin{footnotesize} 0.3424 \end{footnotesize}  \\\hline



\begin{footnotesize} Yao~\etal~\cite{yao2016scene} \end{footnotesize} &
\begin{footnotesize} 0.271 \end{footnotesize} &
\begin{footnotesize} 0.4323 \end{footnotesize} &
\begin{footnotesize} 0.3331 \end{footnotesize} \\\hline
\hline
\multicolumn{4}{|c|}{Baselines from \cite{Ref:Veit2016}} \\\hline
\begin{footnotesize} A \end{footnotesize} &
\begin{footnotesize} 0.233 \end{footnotesize} &
\begin{footnotesize} 0.8378 \end{footnotesize} &
\begin{footnotesize} 0.3648 \end{footnotesize} \\\hline
\begin{footnotesize} B \end{footnotesize} &
\begin{footnotesize} 0.107 \end{footnotesize} &
\begin{footnotesize} 0.8973 \end{footnotesize} &
\begin{footnotesize} 0.1914 \end{footnotesize} \\\hline
\begin{footnotesize} C \end{footnotesize} &
\begin{footnotesize} 0.047 \end{footnotesize} &
\begin{footnotesize} 0.1856 \end{footnotesize} &
\begin{footnotesize} 0.0747 \end{footnotesize} \\\hline

\end{tabular}
\end{center}
\caption{Results on COCO-Text.} \label{tab:result-coco-text}
\vspace{-3mm}
\end{table}






\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Recall} & \textbf{Precision} & \textbf{F-score} \\\hline\hline




\begin{footnotesize} Ours + PVANET2x \end{footnotesize} &
\begin{footnotesize} 0.6743 \end{footnotesize} &
\begin{footnotesize} \textbf{0.8728} \end{footnotesize} &
\begin{footnotesize} \textbf{0.7608} \end{footnotesize} \\\hline

\begin{footnotesize} Ours + PVANET \end{footnotesize} &
\begin{footnotesize} 0.6713 \end{footnotesize} &
\begin{footnotesize} 0.8356 \end{footnotesize} &
\begin{footnotesize} 0.7445 \end{footnotesize} \\\hline


\begin{footnotesize} Ours + VGG16 \end{footnotesize} &
\begin{footnotesize} 0.6160 \end{footnotesize} &
\begin{footnotesize} 0.8167 \end{footnotesize} &
\begin{footnotesize} 0.7023 \end{footnotesize} \\\hline



\begin{footnotesize} Yao~\etal~\cite{yao2016scene} \end{footnotesize} &
\begin{footnotesize} \textbf{ 0.7531} \end{footnotesize} &
\begin{footnotesize} 0.7651 \end{footnotesize} &
\begin{footnotesize} 0.7591 \end{footnotesize} \\\hline
\begin{footnotesize} Zhang~\emph{et al.}~\cite{Ref:Zhang2016} \end{footnotesize}                  &
\begin{footnotesize} 0.67 \end{footnotesize} &
\begin{footnotesize} 0.83 \end{footnotesize} &
\begin{footnotesize} 0.74 \end{footnotesize} \\\hline
\begin{footnotesize} Yin~\emph{et al.}~\cite{Ref:Yin2015} \end{footnotesize}                  &
\begin{footnotesize} 0.63 \end{footnotesize} &
\begin{footnotesize} 0.81 \end{footnotesize} &
\begin{footnotesize} 0.71 \end{footnotesize} \\\hline
\begin{footnotesize} Kang~\emph{et al.}~\cite{Ref:Kang2014} \end{footnotesize}        &
\begin{footnotesize} 0.62 \end{footnotesize}  &
\begin{footnotesize} 0.71 \end{footnotesize} &
\begin{footnotesize} 0.66 \end{footnotesize} \\\hline
\begin{footnotesize} Yin~\emph{et al.}~\cite{Ref:Yin2014} \end{footnotesize}     &
\begin{footnotesize} 0.61 \end{footnotesize} &
\begin{footnotesize} 0.71 \end{footnotesize} &
\begin{footnotesize} 0.66 \end{footnotesize}\\\hline
\begin{footnotesize} TD-Mixture~\cite{Ref:Yao2012} \end{footnotesize}                         &
\begin{footnotesize} 0.63 \end{footnotesize}  &
\begin{footnotesize} 0.63 \end{footnotesize} &
\begin{footnotesize} 0.60 \end{footnotesize} \\\hline
\begin{footnotesize} TD-ICDAR~\cite{Ref:Yao2012} \end{footnotesize}   &
\begin{footnotesize} 0.52 \end{footnotesize} &
\begin{footnotesize} 0.53 \end{footnotesize} &
\begin{footnotesize} 0.50 \end{footnotesize} \\\hline
\begin{footnotesize} Epshtein~\etal~\cite{Ref:Epshtein2010} \end{footnotesize} &
\begin{footnotesize} 0.25 \end{footnotesize}  &
\begin{footnotesize} 0.25 \end{footnotesize} &
\begin{footnotesize} 0.25 \end{footnotesize} \\\hline
\end{tabular}
\end{center}
\caption{Results on MSRA-TD500.}
\label{tab:result-msra-td500}
\vspace{-3mm}
\end{table}



In addition, we also evaluated Ours+PVANET2x on the ICDAR 2013 benchmark. It achieves 0.8267, 0.9264 and 0.8737 in recall, precision and F-score, which are comparable with the previous state-of-the-art method~\cite{tian2016detecting}, which obtains 0.8298, 0.9298 and 0.8769 in recall, precision and F-score, respectively.

\subsection{Speed Comparison}\label{sec:speed}

The overall speed comparison is demonstrated in Tab.~\ref{tab:speed-comp}. The numbers we reported are averages from running through 500 test images from the ICDAR 2015 dataset at their original resolution (1280x720) using our best performing networks. These experiments were conducted on a server using a single NVIDIA Titan X graphic card with Maxwell architecture and an Intel E5-2670 v3 @ 2.30GHz CPU. For the proposed method, the post-processing includes thresholding and NMS, while others should refer to their original paper.

While the proposed method significantly outperforms state-of-the-art methods, the computation cost is kept very low, attributing to the simple and efficient pipeline. As can be observed from Tab.~\ref{tab:speed-comp}, the fastest setting of our method runs at a speed of 16.8 FPS, while slowest setting runs at 6.52 FPS. Even the best performing model Ours+PVANET2x runs at a speed of 13.2 FPS. This confirm that our method is among the most efficient text detectors that achieve state-of-the-art performance on benchmarks.

\begin{table}
\begin{center}
\small
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Approach} & \textbf{Res.} & \textbf{Device} & \textbf{T\textsubscript{1}/T\textsubscript{2} (ms)} & \textbf{FPS} \\\hline\hline
\begin{scriptsize} Ours + PVANET \end{scriptsize} &
\begin{footnotesize} 720p \end{footnotesize} &
\begin{footnotesize} Titan X \end{footnotesize} &
\begin{footnotesize} 58.1 / 1.5 \end{footnotesize} &
\begin{footnotesize} 16.8 \end{footnotesize}
\\\hline

\begin{scriptsize} Ours + PVANET2x \end{scriptsize} &
\begin{footnotesize} 720p \end{footnotesize} &
\begin{footnotesize} Titan X \end{footnotesize} &
\begin{footnotesize} 73.8 / 1.7 \end{footnotesize} &
\begin{footnotesize} 13.2 \end{footnotesize}
\\\hline

\begin{scriptsize} Ours + VGG16 \end{scriptsize} &
\begin{footnotesize} 720p \end{footnotesize} &
\begin{footnotesize} Titan X \end{footnotesize} &
\begin{footnotesize} 150.9 / 2.4 \end{footnotesize} &
\begin{footnotesize} 6.52 \end{footnotesize}
\\\hline

\begin{scriptsize} Yao~\etal~\cite{yao2016scene} \end{scriptsize} &
\begin{footnotesize} 480p \end{footnotesize} &
\begin{footnotesize} K40m \end{footnotesize} &
\begin{footnotesize} 420 / 200 \end{footnotesize} &
\begin{footnotesize} 1.61 \end{footnotesize}
\\\hline

\begin{scriptsize} Tian~\etal~\cite{tian2016detecting} \end{scriptsize} &
\begin{footnotesize} ss-600* \end{footnotesize} &
\begin{footnotesize} GPU \end{footnotesize} &
\begin{footnotesize} 130 / 10 \end{footnotesize} &
\begin{footnotesize} 7.14 \end{footnotesize}
\\\hline

\begin{scriptsize} Zhang~\etal~\cite{Ref:Zhang2016}* \end{scriptsize} &
\begin{footnotesize} MS* \end{footnotesize} &
\begin{footnotesize} Titan X \end{footnotesize} &
\begin{footnotesize} 2100 / N/A \end{footnotesize} &
\begin{footnotesize} 0.476 \end{footnotesize}
\\\hline




\end{tabular}
\end{center}
\caption{Overall time consumption compared on different methods. T\textsubscript{1} is the network prediction time, and T\textsubscript{2} accounts for the time used on post-processing.
For Tian~\etal~\cite{tian2016detecting}, \textit{ss-600} means short side is 600, and 130ms includes two networks. Note that they reach their best result on ICDAR 2015 using a short edge of 2000, which is much larger than ours. For Zhang~\etal~\cite{Ref:Zhang2016}, MS means they used 200, 500, 1000 three scales, and the result is obtained on MSRA-TD500. The theoretical flops per pixel for our three models are 18KOps, 44.4KOps and 331.6KOps respectively, for PVANET, PVANET2x and VGG16.
} \label{tab:speed-comp}
\vspace{-4mm}
\end{table}

\iftrue
\subsection{Limitations}

The maximal size of text instances the detector can handle is proportional to the receptive field of the network. This limits the capability of the network to predict even longer text regions like text lines running across the images.


Also, the algorithm might miss or give imprecise predictions for vertical text instances as they take only a small portion of text regions in the ICDAR 2015 training set.
\fi


 \section{Conclusion and Future Work}

We have presented a scene text detector that directly produces word or line level predictions from full images with a single neural network.
By incorporating proper loss functions, the detector can predict either rotated rectangles or quadrangles for text regions, depending on specific applications. The experiments on standard benchmarks confirm that the proposed algorithm substantially outperforms previous methods in terms of both accuracy and efficiency.

Possible directions for future research include: (1) adapting the geometry formulation to allow direct detection of curved text; (2) integrating the detector with a text recognizer; (3) extending the idea to general object detection.

 



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
