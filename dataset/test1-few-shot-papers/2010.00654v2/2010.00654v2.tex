
\documentclass{article} \usepackage{iclr2021_conference,times}
\iclrfinalcopy{\iclrfinaltrue}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rveps{{\pmb{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{float}
\usepackage{booktabs} 
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{amssymb}

\newcommand{\KK}[1]{{\color{blue}KK: #1}}
\newcommand{\AV}[1]{{\color{red}AV: #1}}
\newcommand{\ZX}[1]{{\color{green}ZX: #1}}
\newcommand{\JK}[1]{{\color{cyan}[JK: #1]}}

\title{VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models}




\author{Zhisheng Xiao \thanks{Work done during an internship at NVIDIA} \\
Computational and Applied Mathematics\\
The University of Chicago\\
\texttt{zxiao@uchicago.edu} \\
\And
Karsten Kreis, Jan Kautz, Arash Vahdat \\
NVIDIA\\
\texttt{\{kkreis,jkautz,avahdat\}@nvidia.com} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. 
\end{abstract}


\section{Introduction}

Deep generative learning is a central problem in machine learning. It has found diverse applications, ranging from image~\citep{brock2018large,karras2019style,razavi2019generating}, music~\citep{dhariwal2020jukebox} and speech \citep{ping2019waveflow,oord2016wavenet} generation, distribution alignment across domains \citep{zhu2017unpaired,liu2017unsupervised,tzeng2017adversarial} and semi-supervised learning \citep{kingma2014semi,izmailov2019semi} to 3D point cloud generation~\citep{yang2019pointflow}, light-transport simulation~\citep{mueller2019neural}, molecular modeling ~\citep{SanchezLengeling360,noe2019boltzmann} and equivariant sampling in theoretical physics~\citep{kanwar2020equivariant}.

Among competing frameworks, likelihood-based models include variational autoencoders (VAEs) \citep{kingma2014vae, rezende2014stochastic}, normalizing flows~\citep{rezendeICML15Normalizing, dinh2016density}, autoregressive models~\citep{oord2016pixel}, and energy-based models (EBMs) \citep{928a56b7d6f1473e930f282a0c4b534e,RBM}. These models are trained by maximizing the data likelihood under the model, and unlike generative adversarial networks (GANs)~\citep{NIPS2014_5423}, their training is usually stable and they cover modes in data more faithfully by construction. 







Among likelihood-based models, EBMs model the unnormalized data density by assigning low energy to high-probability regions in the data space~\citep{xie2016theory,du2019implicit}. EBMs are appealing because they require almost no restrictions on network architectures (unlike normalizing flows) and are therefore potentially very expressive.
They also exhibit better robustness and out-of-distribution generalization \citep{du2019implicit} because, during training, areas with high probability under the model but low probability under the data distribution are penalized explicitly. However, training and sampling EBMs usually requires MCMC, which can suffer from slow mode mixing and is computationally expensive when neural networks represent the energy function. 

On the other hand, VAEs are computationally more efficient for sampling than EBMs, as they do not require running expensive MCMC steps. VAEs also do not suffer from expressivity limitations that normalizing flows face \citep{dupont2019augmented, kong2020expressive}, and in fact, they have recently shown state-of-the-art generative results among non-autoregressive likelihood-based models~\citep{vahdat2020nvae}. Moreover, VAEs naturally come with a latent embedding of data that allows fast traverse of the data manifold by moving in the latent space and mapping the movements to the data space. However, VAEs tend to assign high probability to regions with low density under the data distribution. This often results in blurry or corrupted samples generated by VAEs. This also explains why VAEs often fail at out-of-distribution detection \citep{nalisnick2018deep}. 




In this paper, we propose a novel generative model as a symbiotic composition of a VAE and an EBM (VAEBM) that combines the best of both. VAEBM defines the generative distribution as the product of a VAE generator and an EBM component defined in pixel space. Intuitively, the VAE captures the majority of the mode structure in the data distribution. However, it may still generate samples from low-probability regions in the data space. Thus, the energy function focuses on refining the details and reducing the likelihood of non-data-like regions, which leads to significantly improved samples.


Moreover, we show that training VAEBM by maximizing the data likelihood easily decomposes into training the VAE and the EBM component separately. The VAE is trained using the reparameterization trick, while the EBM component requires sampling from the joint energy-based model during training. We show that we can sidestep the difficulties of sampling from VAEBM, by reparametrizing the MCMC updates using VAE's latent variables. 
This allows MCMC chains to quickly traverse the model distribution and it speeds up mixing.
As a result, we only need to run short chains to obtain approximate samples from the model, accelerating both training and sampling at test time. 

Experimental results show that our model outperforms previous EBMs and state-of-the-art VAEs on image generation benchmarks including CIFAR-10, CelebA 64, LSUN Church 64, and CelebA HQ 256 by a large margin, reducing the gap with GANs. We also show that our model covers the modes in the data distribution faithfully, while having less spurious modes for out-of-distribution data. To the best of knowledge, VAEBM is the first successful EBM applied to large images.

In summary, this paper makes the following contributions: i) We propose a new generative model using the product of a VAE generator and an EBM defined in the data space. ii) We show how training this model can be decomposed into training the VAE first, and then training the EBM component. iii) We show how MCMC sampling from VAEBM can be pushed to the VAE's latent space, 
accelerating sampling. iv) We demonstrate state-of-the-art image synthesis quality among likelihood-based models, confirm complete mode coverage, and show strong out-of-distribution detection performance.





 \section{Background}\label{sec:background}


\textbf{Energy-based Models:} An EBM assumes  to be a Gibbs distribution of the form , where  is the energy function with parameters  and  is the normalization constant. There is no restriction on the particular form of . Given a set of samples drawn from the data distribution , the goal of maximum likelihood learning is to maximize the log-likelihood , which has the derivative \citep{woodford2006notes}:

For the first expectation, the \textit{positive phase}, samples are drawn from the data distribution , and for the second expectation, the \textit{negative phase}, samples are drawn from the model  itself. 
However, sampling from  in the negative phase is itself intractable and approximate samples are usually drawn using MCMC. A commonly used MCMC algorithm is Langevin dynamics (LD)~\citep{neal1993probabilistic}. Given an initial sample , Langevin dynamics iteratively updates it as:

where  is the step-size.\footnote{In principle one would require an accept/reject step to make it a rigorous MCMC algorithm, but for sufficiently small stepsizes this is not necessary in practice \citep{neal1993probabilistic}.} In practice, Eq.~\ref{LD} is run for finite iterations, which yields a Markov chain with an invariant distribution approximately close to the original target distribution. 



\textbf{Variational Autoencoders:}
VAEs define a generative model of the form , where  is the latent variable with prior , and  is a conditional distribution that models the likelihood of data  given . The goal of training is to maximize the marginal log-likelihood  given a set of training examples. However since the marginalization is intractable, instead, the variational lower bound on  is maximized with  as the approximate posterior: 


The state-of-the-art VAE, NVAE \citep{vahdat2020nvae}, increases the expressivity of both prior and approximate posterior using hierarchical latent variables \citep{kingma2016improved} where  is decomposed into a set of disjoint groups, , and the prior  and the approximate posterior  are defined using autoregressive distributions over the groups. We refer readers to \citet{vahdat2020nvae} for more details.
 \section{Energy-based Variational Autoencoders}
\begin{figure*}[t]
    \centering
    \includegraphics[trim={2cm, 14.5cm, 4.cm, 4.6cm}, clip=true, scale=0.55]{Figures/teaser/Fig1_edited.pdf}
    \caption{Our VAEBM is composed of a VAE generator (including the prior and decoder) and an energy function that operates on samples  generated by the VAE. The VAE component is trained first, using the standard VAE objective; then, the energy function is trained while the generator is fixed. 
    Using the VAE generator, we can express the data variable  as a deterministic function of white noise samples  and . This allows us to reparameterize sampling from our VAEBM by sampling in the joint space of  and . We use this in the negative training phase (see Sec.~\ref{sec training}).
}
    \label{fig1}
\end{figure*}

One of the main problems of VAEs is that they tend to assign high probability to regions in data space that have low probability under the data distribution. To tackle this issue, we propose VAEBM, a generative model constructed by the product of a VAE generator and an EBM component defined in the data space. This formulation allows our model to capture the main mode structure of the data distribution using the VAE. 
But when training the joint VAEBM, in the negative training phase we sample from the model itself and can discover non-data-like samples, whose likelihood is then reduced by the energy function explicitly.
The energy function defined in the pixel space also shares similarities with discriminator in GANs, which can generate crisp and detailed images.

Formally, we define the generative model in VAEBM as  where  is a VAE generator and  is a neural network-based energy function, operating only in the  space, and  is the normalization constant.
VAEBM is visualized in Fig.~\ref{fig1}. Marginalizing out the latent variable  gives

Given a training dataset, the parameters of VAEBM, , are trained by maximizing the marginal log-likelihood on the training data:

where we replace  with the variational lower bound from Eq.~\ref{vae_loss}. Eq.~\ref{lower bd} forms the objective function for training VAEBM. The first term corresponds to the VAE objective and the second term corresponds to training the EBM component. Next, we discuss how we can optimize this objective.



\subsection{Training}\label{sec training}
The  term in Eq.~\ref{lower bd} is similar to the EBM training objective except that the log partition function depends on both  and . We show in Appendix~\ref{log Z gradient} that  has the gradients

\iffalse

\fi
The first gradient can be estimated easily by evaluating the gradient of the energy function at samples drawn from the VAEBM model  using MCMC.
However, the second term involves computing the intractable . In Appendix~\ref{log Z gradient}, we show that estimating  requires sampling from the VAE's posterior distribution, given model samples . To avoid the computational complexity of estimating this term, for example with a second round of MCMC, we propose a two-stage algorithm for training VAEBM. In the first stage, we train the VAE model in our VAEBM by maximizing the  term in Eq.~\ref{lower bd}. This term is identical to the VAE's objective, thus, the parameters  and  are trained using the reparameterized trick as in Sec.~\ref{sec:background}. In the second stage, we keep the VAE model fixed and only train the EBM component. Since  is now fixed, we only require optimizing  w.r.t. , the parameters of the energy function. The gradient of  w.r.t.  is:

which decomposes into a positive and a negative phase, as discussed in Sec.~\ref{sec:background}.


\textbf{Reparametrized sampling in the negative phase:} For gradient estimation in the negative phase, we can draw samples from the model using MCMC.
Naively, we can perform ancestral sampling, first sampling from the prior , then running MCMC for  in -space. 
This is problematic, since  is often sharp and MCMC cannot mix when the conditioning  is fixed.

In this work, we instead run the MCMC iterations in the joint space of  and . Furthermore, we accelerate the sampling procedure using reparametrization for both  and the latent variables . Recall that when sampling from the VAE, we first sample  and then . This sampling scheme can be reparametrized by sampling from a fixed noise distribution (e.g., () and deterministic transformations  such that 

Here,  denotes the transformation defined by the prior that transforms noise  into prior samples  and  represents the decoder that transforms noise  into samples , given prior samples .
We can apply the same reparameterization when sampling from . This corresponds to sampling  from the ``base distribution'': 

and then transforming them to  and  via Eq. \ref{reparam transformations} (see Appendix \ref{appendix reparam} for derivation). Note that  and  have the same scale, as  is a standard Normal distribution, while the scales of  and  can be very different. Thus, running MCMC sampling with this reparameterization in the -space has the benefit that we do not need to tune the sampling scheme (e.g., step size in LD) for each variable. This is particularly helpful when  itself has multiple groups, as in our case. 

\textbf{The advantages of two-stage training: }
Besides avoiding the difficulties of estimating the full gradient of , two-stage training has additional advantages. As we discussed above, updating  is computationally expensive, as each update requires an iterative MCMC procedure to draw samples from the model. The first stage of our training minimizes the distance between the VAE model and the data distribution, and in the second stage, the EBM further reduce the mismatch between the model and the data distribution.
As the pre-trained VAE  provides a good approximation to  already, we expect that a relatively small number of expensive updates for training  is needed. Moreover, the pre-trained VAE provides a latent space with an effectively lower dimensionality and a smoother distribution than the data distribution, which facilitates more efficient MCMC. 


\textbf{Alternative extensions: } During the training of the energy function, we fix the VAE's parameters. In Appendix \ref{gan loss}, we discuss a possible extension to our training objective that also updates the VAE.  

 \vspace{-0.5em}
\section{Related Work}

Early variants of EBMs include models whose energy is defined over both data and auxiliary latent variables \citep{salakhutdinov2009deep,hinton2012practical}, and models using only data variables \citep{hinton2002training, mnih2005learning}. Their energy functions are simple and they do not scale to high dimensional data. Recently, it was shown that EBMs with deep neural networks as energy function can successfully model complex data such as natural images \citep{du2019implicit, nijkamp2019learning,nijkamp2019anatomy}. They are trained with maximum likelihood and only model the data variable. Joint EBMs \citep{grathwohl2020your,liu2020hybrid} model the joint distribution of data and labels. In contrast, our VAEBM models the joint distribution of data and general latent variables.

Besides fundamental maximum likelihood training, other techniques to train EBMs exist, such as minimizing F-divergence \citep{yu2020training} or Stein discrepancy \citep{grathwohl2020cutting}, contrastive estimation \citep{gutmann2010noise, gao2020flow} and denoising score matching \citep{li2019annealed}. Recently, noise contrastive score networks and diffusion models have demonstrated high quality image synthesis \citep{song2019generative,song2020improved,ho2020denoising}. These models are also based on denoising score matching (DSM) \citep{vincent2011connection}, but do not parameterize any explicit energy function and instead directly model the vector-valued score function. We view score-based models as alternatives to EBMs trained with maximum likelihood. Although they do not require iterative MCMC during training, they need very long sampling chains to anneal the noise when sampling from the model ( steps). Therefore, sample generation is extremely slow.


VAEBM is an EBM with a VAE component, and it shares similarities with work that builds connections between EBMs and other generative models.  \citet{zhao2016energy,che2020your,song2020discriminator,Arbel2020GeneralizedEB} formulate EBMs with GANs, and use the discriminator to assign an energy.
\citet{xiao2020exponential,nijkamp2020learning} use normalizing flows that transport complex data to latent variables to facilitate MCMC sampling \citep{hoffman2019neutra}, and thus, their methods can be viewed as EBMs with flow component. However, due to their topology-preserving nature, normalizing flows cannot easily transport complex multimodal data, and their sample quality on images is limited. A few previous works combine VAEs and EBMs in different ways from ours. \citet{pang2020learning} and \citet{Vahdat2018DVAE++,vahdat2018dvaes, vahdat2019UndirectedPost} use EBMs for the prior distribution, and \citep{han2020joint, han2019divergence} jointly learn a VAE and an EBM with independent sets of parameters by an adversarial game. 



Finally, as we propose two-stage training, our work is related to post training of VAEs. Previous work in this direction learns the latent structure of pre-trained VAEs \citep{dai2019diagnosing,xiao2019generative,ghosh2019variational}, and sampling from learned latent distributions improves sample quality. These methods cannot easily be extended to VAEs with hierarchical latent variables, as it is difficult to fit the joint distribution of multiple groups of variables. Our purpose for two-stage training is fundamentally different: we post-train an energy function to refine the distribution in data space. 



%
 \section{Experiments}
In this section, we evaluate our proposed VAEBM through comprehensive experiments. Specifically, we benchmark sample quality in Sec.~\ref{img generation}, provide detailed ablation studies on training techniques in Sec.~\ref{ablation}, and study mode coverage of our model and test for spurious modes in Sec.~\ref{mode}. We choose NVAE~\citep{vahdat2020nvae} as our VAE, which we pre-train, and use a simple ResNet as energy function , similar to \citet{du2019implicit}. We draw approximate samples both for training and testing by running short Langevin dynamics chains on the distribution in Eq.~\ref{reparam model}. Note that in NVAE, the prior distribution is a group-wise auto-regressive Gaussian, and the conditional pixel-wise distributions in  are also Gaussian. Therefore, the reparameterization corresponds to shift and scale transformations.
For implementation details, please refer to Appendix \ref{setting}. 

\subsection{Image Generation}\label{img generation}
In Table \ref{table cifar}, we quantitatively compare the sample quality of VAEBM with different generative models on (unconditional) CIFAR-10. We adopt Inception Score (IS) \citep{salimans2016improved} and FID \citep{heusel2017gans} as quantitative metrics. Note that FID reflects the sample quality more faithfully, as potential problems have been reported for IS on CIFAR-10 \citep{barratt2018note}.

We observe that our VAEBM outperforms previous EBMs and other explicit likelihood-based models by a large margin. Note that introducing persistent chains during training only leads to slight improvement, while \citet{du2019implicit} rely on persistent chains with a sample replay buffer. This is likely due to the efficiency of sampling in latent space. Our model also produces significantly better samples than NVAE, the VAE component of our VAEBM, implying a significant impact of our proposed energy-based refinement. We also compare our model with state-of-the-art GANs and recently proposed score-based models, and we obtain comparable or better results. Thus, we largely close the gap to GANs and score-models, while maintaining the desirable properties of models trained with maximum likelihood, such as fast sampling and better mode coverage.      

Qualitative samples generated by our model are shown in Fig.~\ref{cifar fig main} and intermediate samples along MCMC chains in Fig.~\ref{MCMC visualizaton}. We find that VAEBM generates good samples by running only a few MCMC steps. Initializing MCMC chains from the pre-trained VAE also helps quick equilibration.

\begin{table}[t]
\small
\caption{IS and FID scores for unconditional generation on CIFAR-10.
}
\label{table cifar}
\centering
\begin{tabular}{llcc}
& Model & IS & FID
\\ \hline
\multirow{2}{*}{\textbf{Ours}}&VAEBM w/o persistent chain  & 8.21& 12.26\\
&VAEBM w/ persistent chain & 8.43 & 12.19\\
\hline
\multirow{7}{*}{\textbf{EBMs}} & IGEBM \citep{du2019implicit} & 6.02 & 40.58\\
&EBM with short-run MCMC \citep{nijkamp2019learning} & 6.21 & - \\
&F-div EBM \citep{yu2020training} & 8.61 & 30.86\\
&FlowCE \citep{gao2020flow} & - & 37.3\\
&FlowEBM \citep{nijkamp2020learning} &- & 78.12\\
&GEBM \citep{Arbel2020GeneralizedEB} & - & 23.02\\
&Divergence Triangle \citep{han2020joint} & - & 30.1 \\
\hline
\multirow{4}{2cm}{\textbf{Other Likelihood Models}}&Glow \citep{kingma2018glow} & 3.92 & 48.9 \\
&PixelCNN \citep{oord2016pixel} & 4.60 & 65.93\\
&NVAE \citep{vahdat2020nvae} & 5.51 & 51.67\\
& VAE with EBM prior \citep{pang2020learning} & - & 70.15\\
\hline
\hline
\multirow{4}{2cm}{\textbf{Score-based Models}}&NCSN \citep{song2019generative} &8.87 & 25.32\\
&NCSN v2 \citep{song2020improved} & - & 31.75\\
&Multi-scale DSM \citep{li2019annealed} & 8.31& 31.7\\
&Denoising Diffusion \citep{ho2020denoising} & 9.46 &3.17 \\ 
\hline
\multirow{5}{2cm}{\textbf{GAN-based Models}} &SNGAN \citep{miyato2018spectral} & 8.22 & 21.7 \\ 
&SNGAN+DDLS \citep{che2020your} & 9.09 & 15.42 \\
&SNGAN+DCD \citep{song2020discriminator} & 9.11& 16.24\\
&BigGAN \citep{brock2018large} & 9.22 & 14.73 \\ 
&StyleGAN2 w/o ADA \citep{karras2020training} & 8.99 & 9.9\\
\hline
\multirow{2}{*}{\textbf{Others}}&PixelIQN \citep{ostrovski2018autoregressive} &5.29 & 49.46\\
& MoLM \citep{ravuri2018learning} & 7.90 & 18.9 \\
\hline
\end{tabular}
\end{table}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.5]{Figures/main/cifar_100img.png}
    \caption{}\label{cifar fig main}
    \end{subfigure}
    \begin{subfigure}{.36\linewidth}
    \includegraphics[scale=0.5]{Figures/main/cifar_visualize_main.png}
    \caption{}\label{MCMC visualizaton}
    \end{subfigure}
\caption{
(a) CIFAR-10 samples generated by VAEBM. (b) Visualizing MCMC sampling chains. Samples are generated by running 16 LD steps. Chains are initialized with pre-trained VAE. We show intermediate samples at every 2 steps. See Appendix \ref{additional qualitative} for additional qualitative results.}\label{fig main sample}
\end{figure*}

\begin{figure}[h]
    \centering
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.35]{Figures/main/celeba64_32img.png}
    \caption{CelebA 64}
    \end{subfigure}
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.35]{Figures/main/lsun64_32img.png}
    \caption{LSUN Church 64}
    \end{subfigure}
    \begin{subfigure}{.98\linewidth}
    \includegraphics[scale=0.24]{Figures/main/celeba256_6img.png}
    \caption{CelebA HQ 256}\end{subfigure}
    
\caption{Qualitative results on CelebA 64, LSUN Church 64 and CelebA HQ 256. For CelebA HQ 256, we initialize the MCMC chains with low temperature NVAE samples () for better visual quality. On this dataset samples are selected for diversity. See Appendix \ref{additional qualitative} for additional qualitative results and uncurated CelebA HQ 256 samples obtained from higher temperature initializations. Note that the FID in Table \ref{table celeba 256} is computed with full temperature samples.}\label{fig larger dataset}
\end{figure}

We also train VAEBM on larger images, including CelebA 64, CelebA HQ 256 \citep{liu2015deep} and LSUN Church 64 \citep{yu2015lsun}. We report the FID scores for CelebA 64 and CelebA HQ 256 in Tables \ref{table celeba 64} and \ref{table celeba 256}. On CelebA 64, our model obtains results comparable with the best GANs. Although our model obtains worse results than some advanced GANs on CelebA HQ 256, we significantly reduce the gap between likelihood based models and GANs on this dataset. On LSUN Church 64, we obtain FID 13.51, which significantly improves the NVAE baseline FID 41.3. We show qualitative samples in Fig.~\ref{fig larger dataset}. Appendix \ref{additional qualitative} contains additional samples and MCMC visualizations. 

Our model can produce impressive samples by running very short MCMC chains, however, we find that when we run longer MCMC chains than training chains, most chains stay around the local mode without traversing between modes. We believe that the non-mixing is due to the long mixing time of Langevin Dynamics \cite{neal2011mcmc}, as \citet{nijkamp2019learning, nijkamp2019anatomy} also observe that models trained with short-run MCMC have non-mixing long-run chains. We conjecture that mixing can be improved by training and sampling with more advanced MCMC techniques that are known to mix faster, such as HMC \cite{neal2011mcmc}, and this will be left for future work.

\begin{table}[t]
\small
\begin{minipage}{.49\linewidth}
    \centering
\caption{Generative performance on CelebA 64}
\label{table celeba 64}
   \begin{tabular}{ll}
      Model & FID\\
      \hline
      VAEBM (ours) & 5.31\\
      \hline
      NVAE (\citeauthor{vahdat2020nvae}) & 14.74\\
      \hline
      Flow CE (\citeauthor{gao2020flow}) & 12.21\\
      Divergence Triangle (\citeauthor{han2020joint}) & 24.7 \\
      \hline
      \hline
      NCSNv2 (\citeauthor{song2020improved}) & 26.86 \\
      \hline
      COCO-GAN (\citeauthor{lin2019coco}) & 4.0 \\
      QA-GAN (\citeauthor{parimala2019quality}) & 6.42 \\
\hline
 \end{tabular}
\end{minipage}\hfill
\begin{minipage}{.5\linewidth}
    \centering
\caption{Generative performance on CelebA HQ 256}
\label{table celeba 256}
   \begin{tabular}{ll}
      Model & FID \\
      \hline
      VAEBM (ours) & 20.38\\
      \hline
      NVAE (\citeauthor{vahdat2020nvae}) & 45.11\\
      GLOW (\citeauthor{kingma2018glow})&68.93\\
      \hline
      \hline
      Advers. LAE (\citeauthor{pidhorskyi2020adversarial}) & 19.21\\
      PGGAN (\citeauthor{karras2017progressive}) & 8.03 \\
\hline
\end{tabular}
\end{minipage}\hfill
\end{table}

\begin{table}
\small
\begin{minipage}{.49\linewidth}
    \centering

\caption{Comparison for IS and FID on CIFAR-10 between several related training methods.}
\label{table ablation}
\begin{tabular}{lll}
Model & IS & FID
\\ \hline
NVAE (\citeauthor{vahdat2020nvae})  & 5.19 & 55.97\\
EBM on  (\citeauthor{du2019implicit}) & 5.85 & 48.89 \\
EBM on , MCMC init w/ NVAE & 7.28 & 29.32\\
WGAN w/ NVAE decoder & 7.41 & 20.39 \\
VAEBM (ours) & \textbf{8.15} & \textbf{12.96}\\
\hline
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{.49\linewidth}
    \centering
 \caption{Mode coverage on StackedMNIST. }
\label{table stackedmnist}
\begin{tabular}{lll}
Model & Modes  & KL
\\ \hline
VEEGAN (\citeauthor{srivastava2017veegan}) & 761.8 & 2.173\\
PacGAN (\citeauthor{lin2018pacgan}) & 992.0  & 0.277\\
PresGAN (\citeauthor{dieng2019prescribed}) & 999.6  & 0.115 \\
InclusiveGAN (\citeauthor{yu2020inclusive}) & 997 & 0.200  \\
StyleGAN2 (\citeauthor{karras2020analyzing}) & 940 & 0.424 \\
\hline
VAEBM (ours) & \textbf{1000} & \textbf{0.087} \\
\hline
\end{tabular}
\end{minipage}
\end{table}

\subsection{Ablation Studies}\label{ablation}
In Table \ref{table ablation}, we compare VAEBM to several closely related baselines. All the experiments here are performed on CIFAR-10, and for simplicity, we use smaller models than those used in Table \ref{table cifar}. Appendix \ref{ablation setting} summarizes the experimental settings and 
Appendix \ref{ablation qualitative} provides qualitative samples.

\textbf{Data space vs. augmented space:} One key difference between VAEBM and previous work such as \citet{du2019implicit} is that our model is defined on the augmented space , while their EBM only involves . Since we pre-train the VAE, one natural question is whether our strong results are due to good initial samples  from the VAE, which are used to launch the MCMC chains. To address this, we train an EBM purely on  as done in \citet{du2019implicit}. We also train another EBM only on , but we initialize the MCMC chains with samples from the pre-trained NVAE instead of noise. As shown in line 3 of Table \ref{table ablation}, this initialization helps the EBM which is defined only on . However, VAEBM in the augmented space outperforms the EBMs on  only by a large margin. 


\textbf{Adversarial training vs. sampling:} The gradient for  in Eq.~\ref{our derivative} is similar to the gradient updates of WGAN's discriminator \citep{arjovsky2017wasserstein}. The key difference is that we draw (approximate) samples from  by MCMC, while WGAN draws negative samples from a generator \citep{che2020your}. WGAN updates the generator by playing an adversarial game, while we only update the energy function . We compare these two methods by training  and  with the WGAN objective and initializing  with the NVAE decoder. As shown in line 4 of Table \ref{table ablation}, we significantly outperform the WGAN version of our model, implying the advantage of our method over adversarial training.





\subsection{Test for Spurious or Missing Modes}\label{mode}


We evaluate mode coverage on StackedMNIST. This dataset contains images generated by randomly choosing 3 MNIST images and stacking them along the RGB channels. Hence, the data distribution has  modes. Following \citet{lin2018pacgan}, we report the number of covered modes and the KL divergence from the categorical distribution over  categories from generated samples to true data (Table~\ref{table stackedmnist}). VAEBM covers all modes and achieves the lowest KL divergence even compared to GANs that are specifically designed for this task.
Hence, our model covers the modes more equally.
We also plot the histogram of likelihoods for CIFAR-10 train/test images (Fig.~\ref{histogram}, Appendix \ref{hist section}) and present nearest neighbors of generated samples (Appendix \ref{nn section}). We conclude that we do not overfit.






We evaluate spurious modes in our model by assessing its performance on out-of-distribution (OOD) detection. Specifically, we use VAEBM trained on CIFAR-10, and estimate unnormalized  on in-distribution samples (from CIFAR-10 test set) and OOD samples from several datasets. Following \citet{nalisnick2018deep}, we use area under the ROC curve (AUROC) as quantitative metric, where high AUROC indicates that the model correctly assigns low density to OOD samples. In Table \ref{table ood}, we see that VAEBM has significantly higher AUROC than NVAE, justifying our argument that the energy function reduces the likelihood of non-data-like regions. VAEBM also performs better than IGEBM and JEM, while worse than HDGE. However, we note that JEM and HDGE are classifier-based models, known to be better for OOD detection~\citep{liang2017enhancing}. 

\begin{table}[ht]
\small
\caption{Table for AUROC of  computed on several OOD datasets. In-distribution dataset is CIFAR-10. Interp. corresponds to linear interpolation between CIFAR-10 images.}
\label{table ood}
\centering
\begin{tabular}{llccccc}
& & SVHN & Interp. &CIFAR100 & CelebA
\\ \hline
\multirow{5}{2cm}{\textbf{Unsupervised Training}}&NVAE \citep{vahdat2020nvae} & 0.42 & 0.64&  0.56 & 0.68\\
&Glow \citep{kingma2018glow} & 0.05 & 0.51& 0.55 & 0.57 \\
&IGEBM \citep{du2019implicit} & 0.63 & \textbf{0.7} & 0.5 & 0.7\\
&Divergence Traingle \citep{han2020joint}& 0.68& -&- &0.56\\ 
&VAEBM (ours) & \textbf{0.83} & \textbf{0.7} & \textbf{0.62} & \textbf{0.77}\\ 
\hline
\hline
\multirow{2}{2cm}{\textbf{Supervised Training}}&JEM \citep{grathwohl2020your} & 0.67 &0.65 &0.67 & 0.75\\
&HDGE \citep{liu2020hybrid} & 0.96 & 0.82 &0.91 & 0.8\\
\hline
\end{tabular}
\end{table}


\subsection{Exact Likelihood Estimate on 2D Toy Data}
VAEBM is an explicit likelihood model with a parameterized density function. However, like other energy-based models, the estimation of the exact likelihood is difficult due to the intractable partition function . One possible way to estimate the partition function is to use Annealed Importance Sampling (AIS)~\citep{neal2001annealed}. However, using AIS to estimate  in high-dimensional spaces is difficult. In fact, \citet{du2019implicit} report that the estimation does not converge in 2 days on CIFAR-10. Furthermore, AIS gives a stochastic lower bound on , and therefore the likelihood computed with this estimated  would be an upper bound for the true likelihood. This makes the estimated likelihood hard to compare with the VAE's likelihood estimate, which is usually a lower bound on the true likelihood \citep{burda2015importance}.

As a result, to illustrate that our model corrects the distribution learned by the VAE and improves the test likelihood, we conduct additional experiments on a 2-D toy dataset. We use the 25-Gaussians dataset, which is generated by a mixture of 25 two-dimensional isotropic
Gaussian distributions arranged in a grid.
This dataset is also studied in \citet{che2020your}. The encoder and decoder of the VAE have 4 fully connected layers with 256 hidden units, and the dimension of the latent variables is 20. Our energy function has 4 fully connected layers with 256 hidden units.

In the 2-D domain, the partition function  can be accurately estimated by a numerical integration scheme. For the VAE, we use the IWAE bound \citep{burda2015importance} with 10,000 posterior samples to estimate its likelihood. We use 100,000 test samples from the true distribution to evaluate the likelihood. Our VAEBM obtains the average log likelihood of \textbf{-1.50} nats on test samples, which significantly improves the VAE, whose average test likelihood is \textbf{-2.97} nats. As a reference, we also analytically compute the log likelihood of test samples under the true distribution, and the result is \textbf{-1.10} nats.

We show samples from the true distribution, VAE and VAEBM in Figure \ref{toy samples}. We observe that the VAEBM successfully corrects the distribution learned by the VAE and has better sample quality.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.34\linewidth}
    \hspace{0.25cm}
    \includegraphics[scale=0.25]{Figures/toy/dataset.png}
    \caption{Samples from the true distribution}
    \end{subfigure}
    \begin{subfigure}{.3\linewidth}
    \includegraphics[scale=0.25]{Figures/toy/vae.png}
    \caption{Samples from VAE}
    \end{subfigure}
     \begin{subfigure}{.3\linewidth}
    \includegraphics[scale=0.25]{Figures/toy/sample_after_ld.png}
    \caption{Samples from VAEBM}
    \end{subfigure}
    \caption{\label{toy samples}
     Qualitative results on the 25-Gaussians dataset } \end{figure*}

\subsection{Sampling Efficiency}
Despite their impressive sample quality, denoising score matching models \citep{song2019generative,ho2020denoising} are slow at sampling, often requiring  MCMC steps. Since VAEBM uses short MCMC chains, it takes only  seconds to generate  CIFAR-10 samples, whereas NCSN~\citep{song2019generative} takes  seconds, which is about  slower (see Appendix \ref{sample speed} for details). 



 \section{Conclusions}
We propose VAEBM, an energy-based generative model in which the data distribution is defined jointly by a VAE and an energy network, the EBM component of the model.
In this joint model, the EBM and the VAE form a symbiotic relationship: the EBM component refines the initial VAE-defined distribution, while the VAE's latent embedding space is used to accelerate sampling from the joint model and therefore enables efficient training of the energy function.
We show that our model can be trained effectively in two stages with a maximum likelihood objective and we can efficiently sample it by running short Langevin dynamics chains. Experimental results demonstrate strong generative performance on several image datasets. Future work includes further scaling up the model to larger images, applying it to other domains, and using more advanced sampling algorithms. 

\newpage
\bibliography{reference}
\bibliographystyle{iclr2021_conference}
\newpage
\appendix

\section{Deriving the gradient of } \label{log Z gradient}
Recall that . For the derivative of  w.r.t. , we have:

Similarly, it is easy to show that . Intuitively, both gradients encourage reducing the likelihood of the samples generated by the VAEBM model. Since,  is an EBM, the expectation can be approximated using MCMC samples.

Note that Eq.~\ref{eq:grad_vae} is further expanded to:

which can be approximated by first sampling from VAEBM using MCMC (i.e., ) and then sampling from the true posterior of the VAE (i.e., ). The gradient term can be easily computed given the samples.  Two approaches can be used to draw approximate samples from . i) We can replace  with the approximate posterior . However, the quality of this estimation depends on how well  matches the true posterior on samples generated by , which can be very different from the real data samples. To bring  closer to , we can maximize the variational bound (Eq.~\ref{vae_loss}) on samples generated from  with respect to , the encoder parameters\footnote{Maximizing ELBO with respect to  corresponds to minimizing  while  is fixed.}. However, this will add additional complexity to training. ii) Alternatively, we can use MCMC sampling to sample . To speed up MCMC, we can initialize the  samples in MCMC with the original  samples that were drawn in the outer expectation (i.e., ). However, with this approach, the computational complexity of the gradient estimation for the negative phase is doubled, as we now require running MCMC twice, once for  and again for .

We can entirely avoid the additional computational complexity and the complications of estimating , if we assume that the VAE is held fixed when training the EBM component of our VAEBM. This way, we require running MCMC only to sample  to compute .

\section{Reparametrization for EBM}\label{appendix reparam}

Suppose we draw the re-parametrization variables . For convenience, we denote 


Since  is a deterministic and invertible transformation that maps  to , by the change of variables formula, we can write 

where  is the Jacobian of . Consider a Gaussian distribution as a simple example: if  and , then 

and 


Recall that the generative model of our EBM is

We can apply the change of variable to  in similar manner:

where  is the Jacobian of . 

Since we have the relation

for invertible function , we have that

which is the distribution in Eq.~\ref{reparam model}. 

After we obtained samples  from the distribution in Eq.~\ref{re-pram dist}, we obtain  by applying the transformation  in Eq.~\ref{transformation}.

\subsection{Comparison of Sampling in -space and in -space}
Above we show that sampling from  is equivalent to sampling from  and applying the appropriate variable transformation. Here, we further analyze the connections between sampling from these two distributions with Langevin dynamics. Since each component of  and  can be re-parametrzied with scaling and translation of standard Gaussian noise, without loss of generality, we assume a variable  ( can be a single latent variable in  or a single pixel in ) and write 

Suppose we sample in the  space with energy function  on  and step size . The update for  is 

Now we plug  into the expression of  while noting that . We obtain

Therefore, we see that running Langevin dynamics in -space is equivalent to running Langevin dynamics in -space with step size for each component of  and  adjusted by its variance. However, considering the high dimensionality of  and , the step size adjustment is difficult to implement. 

The analysis above only considers a variable individually. More importantly, our latent variable  in the prior follows block-wise auto-regressive Gaussian distributions, so the variance of each component in  depends on the value of . We foresee that because of this dependency, using a fixed step size per component of  will not be effective, even when it is set differently for each component. In contrast, all the components in -space have a unit variance. Hence, a universal step size for all the variables in this space can be used.

To further provide empirical evidence that adjusting the step size for each variable is necessary, we try sampling directly in -space without adjusting the step size (i.e., use a universal step size for all variables). Qualitative results are presented in Figure \ref{xz_sample}. We examine several choices for the step size and we cannot obtain high-quality samples. 

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.5]{Figures/xz_sample/sample_from_xz_8e-4.png}
    \caption{Step size 8e-4}
    \end{subfigure}
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.5]{Figures/xz_sample/sample_from_xz_8e-5.png}
    \caption{Step size 8e-5}
    \end{subfigure}
     \vskip1em
     \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.5]{Figures/xz_sample/sample_from_xz_8e-6.png}
    \caption{Step size 8e-6}
    \end{subfigure}
    \caption{\label{xz_sample}
     Qualitative samples obtained from sampling in -space with different step sizes.} \end{figure*}

In conclusion, the re-parameterization provides an easy implementation to adjust step size for each variable, and the adjustment is shown to be crucial to obtain good samples.

\section{Extension to Training Objective}\label{gan loss}

In the first stage of training VAEBM, the VAE model is trained by maximizing the training data log-likelihood which corresponds to minimizing an upper bound on  w.r.t. . In the second stage, when we are training the EBM component, we use the VAE model to sample from the joint VAEBM by running the MCMC updates in the joint space of  and .  Ideally, we may want to bring  closer to  in the second stage, because when , we will not need the expensive updates for . We can bring  closer to  by minimizing   with respect to  which was recently discussed in the context of an EBM-interpretation of GANs by \citet{che2020your}. To do so, we assume the target distribution  is fixed and create a copy of , named , and we update  by the gradient: 

In other words, one update step for  that minimizes  w.r.t.  can be easily done by drawing samples from  and minimizing the energy-function w.r.t. . Note that this approach is similar to the generator update in training Wasserstein GANs \citep{arjovsky2017wasserstein}. The above KL objective will encourage  to model dominants modes in . However, it may cause  to drop modes.

\subsection{Derivation}
Our derivation largely follows Appendix A.2 of \citet{che2020your}. Note that every time we update , we are actually taking the gradient w.r.t , which can be viewed as a copy of  and is initialized as . In particular, we should note that the  in  is fixed. Therefore, we have

where the second term in Eq.~\ref{score function} is 0 because the  does not depend on  and the expectation of the score function is :

Recall that  has the same value as  before the update, so 

Plug Eq.~\ref{log component} into Eq.~\ref{grad kl}, we have 

since 


\subsection{Results}
We train VAEBM with an additional loss term that updates the parameter  to minimize  as explained above. Our experiment uses the same initial VAE as in Sec. \ref{ablation}, and details of the implementation are introduced in Appendix \ref{ablation setting}. We obtain FID 14.0 and IS 8.05, which is similar to the results of plain VAEBM (FID  12.96 and IS 8.15). Therefore, we conclude that training the model by minimizing  does not improve the performance, and updating the decoder is not necessary. This is likely because the initial VAE is pulled as closely as possible to the data distribution already, which is also the target for the joint VAEBM .



\section{Comparing likelihoods on Train and Test Set}\label{hist section}
\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.8]{Figures/additional/hist.png}
    \caption{\label{histogram}
     Histogram of unnormalized log-likelihoods on 10k CIFAR-10 train and test set images}
\end{figure*}

In Figure \ref{histogram}, we plot a histogram of unnormalized log-likelihoods of 10k CIFAR-10 train set and test set images. We see that our model assigns similar likelihoods to both train and test set images. This indicates that VAEBM generalizes well to unseen data and covers modes in the training data well. 



\section{Implementation Details}
\label{setting}
In this section, we introduce the details of training and sampling from VAEBM. 

\textbf{NVAE: } VAEBM uses NVAE as the  component in the model. We train the NVAE with its official implementation\footnote{\url{https://github.com/NVlabs/NVAE}}. We largely follow the default settings, with one major difference that we use a Gaussian decoder instead of a discrete logistic mixture decoder as in \citet{vahdat2020nvae}. The reason for this is that we can run Langevin dynamics only with continuous variables. The number of latent variable groups for CIFAR-10, CelebA 64, LSUN Church 64 and CelebA HQ 256 are 30, 15, 15 and 20, respectively. 

\begin{table}[ht]
\small
  \caption{Network structures for the energy function }\label{table net struct}
     \centering
     \begin{tabular}{c}
     CIFAR-10\\
     \hline
      conv2d, 128\\
     ResBlock down 128\\
     ResBlock 128\\
     ResBlock down 256\\
     ResBlock 256\\
     ResBlock down 256\\
     ResBlock 256\\
     Global Sum Pooling\\
     FC layer  scalar\\
     \hline
     \end{tabular}
     \quad
     \begin{tabular}{c}
     CelebA 64\\
     \hline
      conv2d, 64\\
     ResBlock down 64\\
     ResBlock 64\\
     ResBlock down 128\\
     ResBlock 128\\
     ResBlock down 128\\
     ResBlock 256\\
     ResBlock down 256\\
     ResBlock 256\\
     Global Sum Pooling\\
     FC layer  scalar\\
     \hline
     \end{tabular}
         \quad
    \begin{tabular}{c}
     LSUN Church 64\\
     \hline
      conv2d, 64\\
     ResBlock down 64\\
     ResBlock 64\\
     ResBlock down 128\\
     ResBlock 128\\
     ResBlock 128\\
     ResBlock down 128\\
     ResBlock 256\\
     ResBlock 256\\
     ResBlock down 256\\
     ResBlock 256\\
     Global Sum Pooling\\
     FC layer  scalar\\
     \hline
     \end{tabular}
         \quad
     \begin{tabular}{c}
     CelebA HQ 256\\
     \hline
      conv2d, 64\\
     ResBlock down 64\\
     ResBlock 64\\
     ResBlock down 128\\
     ResBlock 128\\
     ResBlock down 128\\
     ResBlock 128\\
     ResBlock down 256\\
     ResBlock 256\\
     ResBlock down 256\\
     ResBlock 256\\
     ResBlock down 512\\
     ResBlock 512\\
     Global Sum Pooling\\
     FC layer  scalar\\
     \hline
     \end{tabular}
    
 \end{table}
 \textbf{Network for energy function: }We largely adopt the energy network structure for CIFAR-10 in \citet{du2019implicit}, and we increase the depth of the network for larger images. There are 2 major differences between our energy networks and the ones used in \citet{du2019implicit}: \textbf{1.} we replace the LeakyReLU activations with Swish activations, as we found it improves training stability, and \textbf{2.} we do not use spectral normalization \citep{miyato2018spectral}; instead, we use weight normalization with data-dependent initialization \citep{salimans2016weight}. The network structure for each dataset is presented in Table \ref{table net struct}. 
 
 \textbf{Training of energy function: }We train the energy function by minimizing the negative log likelihood and an additional spectral regularization loss which penalizes the spectral norm of each convolutional layer in . The spectral regularization loss is also used in training NVAE, as we found it helpful to regularize the sharpness of the energy network and better stabilize training. We use a coefficient  for the spectral regularization loss. 
 
 \begin{table}[ht]
\small
\caption{Important hyper-parameters for training VAEBM}
\label{table hyperparam}
\begin{center}
\begin{tabular}{lccccc}
Dataset & Learning rate & Batch size &  Persistent &  of LD steps & LD Step size
\\ \hline
CIFAR-10 w/o persistent chain &  & 32 & No &10 & \\
CIFAR-10 w/ persistent chain &  & 32 & Yes &6 & \\
CelebA 64 &  & 32& No &10 & \\
LSUN Church 64 &  & 32 & Yes & 10 &  \\
CelebA HQ 256 &  & 16&Yes &6 & \\
\hline
\end{tabular}
\end{center}
\end{table}

We summarize some key hyper-parameters we used to train VAEBM in Table \ref{table hyperparam}.

On all datasets, we train VAEBM using the Adam optimizer \citep{kingma2014adam} and weight decay . We use constant learning rates, shown in Table \ref{table hyperparam}. Following \citet{du2019implicit}, we clip training gradients that are more than 3 standard deviations from the 2nd-order Adam parameters. 

While persistent sampling using a sample replay buffer has little effect on CIFAR-10, we found it to be useful on large images such as CelebA HQ 256. When we do not use persistent sampling, we always initialize the LD chains with , sampled from a standard Gaussian. When we use persistent sampling in training, we keep a sample replay buffer that only stores samples of , while  is always initialized from a standard Gaussian. The size of the replay buffer is 10,000 for CIFAR-10 and LSUN Church 64, and 8,000 for CelebA HQ 256. At every training iteration, we initialize the MCMC chains on  by drawing  from the replay buffer with probability  and from standard Gaussian with probability . For CIFAR-10 and LSUN Church 64, we linearly increase  from  to  in 5,000 training iterations, and for CelebA HQ 256, we linearly increase  from  to  in 3,000 training iterations. The settings of Langevin dynamics are presented in Table \ref{table hyperparam}.

We do not explicitly set the number of training iterations. Instead, we follow \citet{du2019implicit} to train the energy network until we cannot generate realistic samples anymore. This happens when the model overfits the training data and hence energies of negative samples are much larger than energies of training data. Typically, training takes around 25,000 iterations (or 16 epochs) on CIFAR-10, 20,000 iterations (or 3 epochs) on CelebA 64, 20,000 iterations (or 5 epochs) on LSUN Church 64, and 9,000 iterations (or 5 epochs) on CelebA HQ 256.

\textbf{Test time sampling: }After training the model, we generate samples for evaluation by running Langvin dynamics with  initialized from standard Gaussian, regardless of whether persistent sampling is used in training or not. We run slightly longer LD chains than training to obtain the best sample quality. In particular, our reported values are obtained from running 16 steps of LD for CIFAR-10, 20 steps of LD for CelebA64 and LSUN Church 64, and 24 steps for CelebA HQ 256. The step sizes are the same as training step sizes. 

In CelebA HQ 256 dataset, we optionally use low temperature initialization for better visual quality. To do this, we first draw samples from the VAE with low temperature and readjusted the BN statistics as introduced by \citet{vahdat2020nvae}, and then initialize the MCMC chain by  obtained by encoding the low-temperature samples using VAE's encoder without readjusted BN statistics.

\textbf{Evaluation metrics: }We use the official implementations of FID\footnote{\url{https://github.com/bioinf-jku/TTUR}} and IS\footnote{\url{https://github.com/openai/improved-gan/tree/master/inception_score}}. We compute IS using 50k CIFAR 10 samples, and we compute FID between 50k generated samples and training images, except for CelebA HQ 256 where we use 30k training images (the CelebA HQ dataset contains only 30k samples).

\section{Settings for Ablation Study} \label{ablation setting}
In this section, we present the details of ablation experiments in Sec.~\ref{ablation}. Throughout ablation experiments, we use a smaller NVAE with 20 groups of latent variables trained on CIFAR-10. We use the same network architectures for the energy network as in Table \ref{table net struct}, with potentially different normalization techniques discussed below. We spent significant efforts on improving each method we compare against, and we report the settings that led to the best results. 

\textbf{WGAN initialized with NVAE decoder: }We initialize the generator with the pre-trained NVAE decoder, and the discriminator is initialized by a CIFAR-10 energy network with random weights. We use spectral normalization and batch normalization in the discriminator as we found them necessary for convergence. We update the discriminator using the Adam optimizer with constant learning rate , and update the generator using the Adam optimizer with initial learning rate  and cosine decay schedule. We train the generator and discriminator for 40k iterations, and we reach convergence of sample quality towards the end of training. 

\textbf{EBM on , w/ or w/o initializing MCMC with NVAE samples: }
We train two EBMs on data space similar to \citet{du2019implicit}, where for one of them, we use the pre-trained NVAE to initialize the MCMC chains that draw samples during training. The setting for training these two EBMs are the same except for the initialization of MCMC. We use spectral normalization in the energy network and energy regularization in the training objective as done in \citet{du2019implicit} because we found these modifications to improve performance. We train the energy function using the Adam optimizer with constant learning rate . We train for 100k iterations, and we reach convergence of sample quality towards the end of training. During training, we draw samples from the model following the MCMC settings in \citet{du2019implicit}. In particular, we use persistent sampling and sample from the sample replay buffer with probability 0.95. We run 60 steps of Langevin dynamics to generate negative samples and we clip gradients
to have individual value magnitudes of less than 0.01. We use a step size of 10 for each step of Langevin dynamics. For test time sampling, we generate samples by running 150 steps of LD with the same settings as during training. 

\textbf{VAEBM with  loss: }We use the same network structure for  as in VAEBM. We find persistent sampling significantly hurts the performance in this case, possibly due to the fact that the decoder is updated and hence the initial samples from the decoder change throughout training. Therefore, we do not use persistent training. We train the energy function using the Adam optimizer with constant learning rate . We draw negative samples by running 10 steps of LD with step size . We update the decoder with the gradient in Eq.~\ref{kl grad eq} using the Adam optimizer with initial learning rate  and cosine decay schedule. For test time sampling, we run 15 steps of LD with step size .

\textbf{VAEBM: }The training of VAEBM in this section largely follows the settings described in Appendix \ref{setting}. We use the same energy network as for CIFAR-10, and we train using the Adam optimizer with constant learning rate . Again, we found that the performance of VAEBM with or without persistent sampling is similar. We adopt persistent sampling in this section because it is faster.  The setting for the buffer is the same as in Appendix \ref{setting}. We run 5 steps of LD with step size  during training, and 15 steps of LD with the same step size in testing.

\section{Qualitative Results of Ablation Study} \label{ablation qualitative}

In Figure \ref{ablation samples}, we show qualitative samples from models corresponding to each item in Table \ref{table ablation}, as well as samples generated by VAEBM with additional  loss.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.48]{Figures/ablation/NVAE_plain.png}
    \caption{NVAE baseline}
    \end{subfigure}
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.48]{Figures/ablation/wgan.png}
    \caption{WGAN, initialized with NVAE decoder}
    \end{subfigure}
     \vskip1em
     \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.48]{Figures/ablation/data_EBM.png}
    \caption{EBM on , MCMC initialized with NVAE samples}
    \end{subfigure}
    \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.48]{Figures/ablation/ebvae_energyloss.png}
    \caption{VAEBM with  loss}
    \end{subfigure}
    \vskip1em
     \begin{subfigure}{.48\linewidth}
    \includegraphics[scale=0.48]{Figures/ablation/ours.png}
    \caption{VAEBM}
    \end{subfigure}
    \caption{\label{ablation samples}
     Qualitative results of ablation study in Sec. \ref{ablation}. and Appendix \ref{gan loss}} \end{figure*}

\section{Additional Qualitative Results}\label{additional qualitative}
We present additional qualitative results in this section.

Additional samples and visualizations of MCMC on CIFAR-10 are in Figures \ref{additional cifar} and \ref{additional visualization}, respectively. 

Additional samples on CelebA 64 are in Figure \ref{additional celeba64}.

Additional samples on LSUN Church 64 are in Figure \ref{additional lsun64}. We visualize the effect of running MCMC by displaying sample pairs before and after MCMC in Figure \ref{lsun compare}.

Additional samples on CelebA HQ 256 generated by initializing VAE samples with temperature 0.7 are shown in Figure \ref{additional celeba256}. Samples generated by initializing VAE samples with full temperature 1.0 are shown in Figure \ref{additional celeba256 full temp}. We visualize the effect of running MCMC by displaying sample pairs before and after MCMC in Figure \ref{celeba256 compare}. Note that the samples used to visualize MCMC are generated by initializing MCMC chains with VAE samples with full temperature 1.0.

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.59]{Figures/additional/cifar10_large.png}
    \caption{\label{additional cifar}
     Additional CIFAR-10 samples}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.45\linewidth}
    \includegraphics[scale=0.6]{Figures/additional/visual_app_1.png}
    \end{subfigure}
    \begin{subfigure}{.45\linewidth}
    \includegraphics[scale=0.6]{Figures/additional/visual_app_2.png}
    \end{subfigure}
\caption{Additional visualizations of MCMC chains when sampling from the model for CIFAR-10}\label{additional visualization}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.49]{Figures/additional/celeba_120img.png}
    \caption{\label{additional celeba64}
     Additional CelebA 64 samples}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.49]{Figures/additional/lsun64_120img.png}
    \caption{\label{additional lsun64}
     Additional LSUN Church 64 samples}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.7]{Figures/additional/lsun_compare1.png}
    \end{subfigure}
    \vskip1em
    \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.7]{Figures/additional/lsun_compare2.png}
    \end{subfigure}
     \vskip1em
     \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.7]{Figures/additional/lsun_compare3.png}
    \end{subfigure}
    \vskip1em
    \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.7]{Figures/additional/lsun_compare4.png}
    \end{subfigure}
    \vskip1em
     \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.7]{Figures/additional/lsun_compare5.png}
    \end{subfigure}
    \caption{\label{lsun compare}
     Visualizing the effect of MCMC sampling on LSUN Church 64 dataset. For each subfigure, the top row contains initial samples from the VAE, and the bottom row contains corresponding samples after MCMC. We observe that MCMC sampling fixes the corrupted initial samples and refines the details.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.25]{Figures/additional/celeba256_48img.png}
    \caption{\label{additional celeba256}
     Additional CelebA HQ 256 samples. Initial samples from VAE for MCMC initializations are generated with temperature 0.7. Samples are uncurated.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.25]{Figures/additional/celeba256_fulltemp_48img.png}
    \caption{\label{additional celeba256 full temp}
     Additional CelebA HQ 256 samples. Initial samples from VAE for MCMC initializations are generated with full temperature 1.0. Samples are uncurated.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.21]{Figures/additional/celeba_compare2.png}
    \end{subfigure}
    \vskip1em
    \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.21]{Figures/additional/celeba_compare1.png}
    \end{subfigure}
     \vskip1em
     \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.21]{Figures/additional/celeba_compare4.png}
    \end{subfigure}
    \vskip1em
    \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.21]{Figures/additional/celeba_compare3.png}
    \end{subfigure}
    \vskip1em
     \begin{subfigure}{.95\linewidth}
    \includegraphics[scale=0.21]{Figures/additional/celeba_compare5.png}
    \end{subfigure}
    \caption{\label{celeba256 compare}
     Visualizing the effect of MCMC sampling on CelebA HQ 256 dataset. Samples are generated by initializing MCMC with full temperature VAE samples. MCMC sampling fixes the artifacts of VAE samples, especially on hairs.}
\end{figure*}

\section{Nearest Neighbors}\label{nn section}
We show nearest neighbors in the training set with generated samples on CIFAR-10 (in Figure \ref{nn cifar pixel} and \ref{nn cifar feature}) and CelebA HQ 256 (in Figure \ref{nn celeba pixel} and \ref{nn celeba feature}). We observe that the nearest neighbors are significantly different from the samples, suggesting that our models generalize well.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.13\linewidth}
    \includegraphics[scale=0.8]{Figures/NN/origin_CIFAR.png}
    \end{subfigure}
    \begin{subfigure}{.8\linewidth}
    \includegraphics[scale=0.8]{Figures/NN/near_neigh_CIFAR.png}
    \end{subfigure}
    \caption{\label{nn cifar pixel}
    CIFAR-10 nearest neighbors in pixel distance. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.13\linewidth}
    \includegraphics[scale=0.8]{Figures/NN/origin_CIFAR.png}
    \end{subfigure}
    \begin{subfigure}{.8\linewidth}
    \includegraphics[scale=0.8]{Figures/NN/near_neigh_cifar_feature.png}
    \end{subfigure}
    \caption{\label{nn cifar feature}
    CIFAR-10 nearest neighbors in Inception feature distance. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.09\linewidth}
    \includegraphics[scale=0.13]{Figures/NN/origin_celeba.png}
    \end{subfigure}
    \begin{subfigure}{.9\linewidth}
    \includegraphics[scale=0.13]{Figures/NN/near_neigh_celeba.png}
    \end{subfigure}
    \caption{\label{nn celeba pixel}
     CelebA HQ 256 nearest neighbors in pixel distance, computed on a  center crop to focus more on faces rather than backgrounds. Generated samples are in the leftmost column, and training set nearest neighbors are in the
     remaining columns.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.09\linewidth}
    \includegraphics[scale=0.13]{Figures/NN/origin_celeba.png}
    \end{subfigure}
    \begin{subfigure}{.9\linewidth}
    \includegraphics[scale=0.13]{Figures/NN/near_neigh_celeba_feature.png}
    \end{subfigure}
    \caption{\label{nn celeba feature}
     CelebA HQ 256 nearest neighbors in Inception feature distance, computed on a  center crop. Generated samples are in the leftmost column, and training set nearest neighbors are in the
     remaining columns.}
\end{figure*}

\section{Settings of Sampling Speed Experiment}\label{sample speed}

We use the official implementation and checkpoints of NCSN at \url{https://github.com/ermongroup/ncsn}. We run the experiments on a computer with a Titan RTX GPU. We use PyTorch 1.5.0 and CUDA 10.2. 

 \end{document}
