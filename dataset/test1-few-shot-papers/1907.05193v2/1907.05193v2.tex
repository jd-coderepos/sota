








\documentclass[final]{IEEEtran}
\usepackage[noadjust]{cite}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{epsfig}
\usepackage{comment}
\usepackage{kevinlin}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{enumitem}
\usepackage[export]{adjustbox}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{xr}
\usepackage{pifont}
\usepackage[hyphens]{url}
\usepackage{hyperref}

\usepackage{sidecap}

\usepackage{amssymb}
\usepackage{amsmath}


\usepackage{ragged2e}




















\ifCLASSINFOpdf
\else
\fi



















































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Cross-Domain Complementary Learning Using Pose for Multi-Person Part Segmentation}


\author{Kevin~Lin,~\IEEEmembership{Student Member, IEEE}, Lijuan Wang,~\IEEEmembership{Senior Member, IEEE}, Kun Luo, Yinpeng Chen,~\IEEEmembership{Member, IEEE}, Zicheng Liu,~\IEEEmembership{Fellow, IEEE}, Ming-Ting Sun,~\IEEEmembership{Life Fellow, IEEE}
\vspace{-5mm}
\thanks{Manuscript received Sep. 30, 2019; revised Feb. 1, 2020 and Apr. 7, 2020; accepted May 3, 2020.}
\thanks{K.~Lin and M.-T.~Sun are with the Department of Electrical and Computer Engineering, University of Washington, Seattle, WA, 98195. E-mail: \{kvlin,~mts\}@uw.edu}\thanks{L. Wang, K. Luo, Y. Chen, and Z. Liu are with Microsoft Azure+AI, Redmond, WA, 98052. E-mail: \{lijuanw,~kun.luo,~yiche,~zliu\}@microsoft.com}}





\markboth{IEEE Transactions on Circuits and Systems for Video Technology}{Lin \MakeLowercase{\textit{et al.}}: Cross-Domain Complementary Learning Using Pose for Multi-Person Part Segmentation}





\IEEEpubid{\begin{minipage}{\textwidth}\ \
\label{eqn:abstract}
L = \alpha L_{pose}(D_r^{pose}) +  \beta L_{pose}(D_s^{pose})+ \gamma L_{part}(D_s^{part}),

\label{eqn:l2}
L_{kpts}(I, K, \hat{K}) = \sum_{j=1}^{J} \sum_{\theta} \mathcal{M}(\theta)||K(\theta) - \hat{K}(\theta)||_{2}^{2},

\label{eqn:l3}
L_{paf}(I, P, \hat{P}) = \sum_{c=1}^{C} \sum_{\theta} \mathcal{M}(\theta)||P(\theta) - \hat{P}(\theta)||_{2}^{2},

\label{eqn:l1}
L_{part}(I, B, \hat{B}) = -\sum_{z=1}^{Z} \sum_{\theta} \mathcal{M}(\theta) B(\theta)\log(\hat{B}(\theta)),

\begin{aligned}\label{eqn:overall-loss}
L
& = \alpha \Big( L_{kpts}(I_r, K_r, \hat{K}) + L_{paf}(I_r, P_r, \hat{P})\Big)\\
& + \beta \Big( L_{kpts}(I_s, K_s, \hat{K}) + L_{paf}(I_s, P_s, \hat{P}) \Big)\\
& + \gamma L_{part}(I_s, B_s, \hat{B}).
\end{aligned}
0.9cm]
	\rotatebox[origin=c]{90}{\small{Right Knee}} &
	\raisebox{-.5\height}{\includegraphics[height=.35\columnwidth]{./fig/fig8_synonly_Rkne_new.pdf}} &
	\raisebox{-.5\height}{\includegraphics[height=.35\columnwidth]{./fig/fig8_bridge_Rkne_new.pdf}}
	\end{tabular}
	\caption{t-SNE visualization~\cite{maaten2008visualizing} of the feature spaces of the real and synthetic body parts.}
	\label{fig:feat-visual}
\end{figure}


\label{sec:feat-visual}
\subsubsection{\textbf{Feature space visualization}}
 
We visualize the features of two different models (\textit{SYN} and \textit{CDCL}) from the real and synthetic images using the t-SNE visualization technique~\cite{maaten2008visualizing}. In Figure~\ref{fig:feat-visual}, the left column shows the features extracted with the model \textit{SYN} (trained with synthetic data only), and the right column are from the model \textit{CDCL}. The first row shows the features extracted at the left elbow position, and the second row shows the features extracted at the right knee position. In each plot, the red dots indicate the real data while the purple dots indicate the synthetic data. We can see that the red and purple dots in the right column are aligned very well, but they do not align well in the left column. This indicates that our complementary learning technique is effective at aligning the feature space of the real data with that of the synthetic data.


\begin{figure}[b]
 \centering
 \setlength{\tabcolsep}{1pt}
 \begin{tabular}{cccc}
 	\includegraphics[width=.24\columnwidth]{./fig/fig9_syn_analysis_rgb_new.pdf}
 	&\includegraphics[width=.24\columnwidth]{./fig/fig9_syn_analysis_no_bg_new.pdf} &
    \includegraphics[width=.24\columnwidth]{./fig/fig9_syn_analysis_gray_new.pdf}
 	&\includegraphics[width=.24\columnwidth]{./fig/fig9_syn_analysis_binary_new.pdf}\\
    \small{Original} & \small{No Background} & \small{Gray-scale} & \small{Binary Mask}\\
    \end{tabular}
 \caption{Different configurations of the synthetic data.}
\label{fig:ablation-syn} \end{figure}
\begin{table}[b]
	\centering
	\caption{Performance comparison (mIOU, \%) of our method using different synthetic training data.}
\label{tbl:syn-study}
	\resizebox{1.\columnwidth}{!}{
    \begin{tabular}{lcccc}
	\toprule
	 & Original & No Background & Gray-scale & Binary Mask\\
	 \midrule
	 Pascal-Person-Parts &  &   &  & \\
	 COCO-DensePose &  &  &  & \\
	\bottomrule
	\end{tabular}
	}
\end{table}

\subsubsection{\textbf{Synthetic training data analysis}}
\label{synthetic-data-analysis}
Since our method learns part segmentation from synthetic data, one may wonder what elements of the synthetic data are essential to be rendered. To answer the question, we ablate our synthetic training data by gradually removing the background, colors, and the human texture, and train our model with these configurations, respectively. Figure~\ref{fig:ablation-syn} shows the examples of different configurations of the synthetic training data, and Table~\ref{tbl:syn-study} shows the performance comparison on Pascal-Person-Parts and COCO-DensePose datasets. Firstly, we observe that removing the background from the synthetic data causes only a small drop on the segmentation performance. This is an indication that our framework is learning the background from the real data. Secondly, after we further remove the color of the synthetic data (Gray-scale), we again only see a small drop on the performance. Finally, when we degrade our synthetic data to the extreme by just using binary masks, our framework still works reasonably well. These studies indicate that our framework mainly requires the pose variations in the foreground data and the rendering quality is not as critical compared to the conventional approach of directly training from synthetic data.
 

\begin{table}
	\centering
	\caption{Ablation study (mIOU, \%) of our method using different number of synthetic human models for training.}
\label{tbl:different-human}
	\resizebox{1.\columnwidth}{!}{
    \begin{tabular}{lccccc}
	\toprule
	 Number of Human Models & 1 & 5 & 10 & 15 & 20\\
	 \midrule
	 Pascal-Person-Parts &  &   &  &  & \\
	 COCO-DensePose &  &   &  &  & \\
	\bottomrule
\end{tabular}
	}
\end{table}


\begin{table}
	\centering
	\caption{Ablation study (mIOU, \%) of our method when compositing synthetic humans with different number of backgrounds for training.}
\label{tbl:different-back}
    \begin{tabular}{lccc}
	\toprule
	 Number of Backgrounds & 1 & 100 & 1000\\
	 \midrule
	 Pascal-Person-Parts &  &   & \\
	 COCO-DensePose &  &   &  \\
	\bottomrule
	\end{tabular}
\end{table}
\begin{figure}[t]
 \centering
 \includegraphics[width=.99\columnwidth]{./fig/fig10_different_bk_new.pdf}
 \caption{Examples of compositing synthetic humans with a variety of real-world scenery images.}
\label{fig:real-bk} 
\end{figure}  
 
\subsubsection{\textbf{Influence of the synthetic human geometry}}
Since our synthetic humans each have their own geometry of body shape and clothing, we study the influence of the model geometry by training with different number of synthetic human models.
Table~\ref{tbl:different-human} shows such results on Pascal-Person-Parts and COCO-DensePose datasets. We see that as we increase the number of synthetic human models for training, it improves the performance for part segmentation. However, the impact becomes less prominent if we use more than  synthetic human models for training.

\subsubsection{\textbf{Compositing synthetic humans with different backgrounds}}
Since our synthetic dataset uses a single background of empty room, one may wonder what if we use more variety of backgrounds for training. Because it is time consuming to create a large variety of synthetic 3D background models, we composite the synthetic humans with a variety of real-world scenery images. We randomly select  scenery images from the Holidays dataset~\cite{jegou2008hamming} for data generation. Figure~\ref{fig:real-bk} shows a few examples of the composited images and Table~\ref{tbl:different-back} shows the results on Pascal-Person-Parts and COCO-DensePose datasets when we composite synthetic humans with different number of backgrounds. We can see that increasing the number of backgrounds improves the performance of part segmentation, but it does not work as well as using a simple background such as an empty room or a blank background. This is probably because the synthetic humans are not placed in realistic positions in the scene and there are lighting inconsistency between the synthetic humans and background. 
 
\begin{figure}
	 \centering
\setlength{\tabcolsep}{1pt}
 \begin{tabular}{c}
 	\includegraphics[width=.6\columnwidth]{./fig/fig11_param-search_new.pdf}
    \end{tabular}
	\caption{Performance comparison (mIOU, \%) of CDCL with different combinations of hyperparameters on Pascal-Person-Parts dataset.}
\label{fig:param-search}
\end{figure}

\subsubsection{\textbf{Influence of different losses}}
\label{sec:hyerparam}
We study the influence of the three terms in our learning objective (in Eq.(1)). The first two terms learn pose estimation from real and synthetic data, respectively. The third term learns part segmentation from synthetic data. We study the influence of the three terms by fixing the first weight  and iterating different combinations of  and . The reason we set  is that we can rewrite Eq.(1) as . Thus, we can omit the scaling factor by setting  and vary  and . As shown in Figure~\ref{fig:param-search}, our method performs more favorably when . Our method achieves the best performance when , ,  are set to ,  and , respectively. This indicates that the first two terms are equally important. We also observed that part segmentation loss is greater than pose estimation loss, thus the losses are better balanced when  is smaller than . It is worth noting that the hyperparameters (i.e., , , ) are used to control the quality of the learning process. Our design principle of the hyperparameters is to ensure the three losses should have a similar scale, so that the three loss terms can be balanced and contribute to the learning process.



 
\begin{figure*}[tb]
 \centering
    \includegraphics[height=.146\textwidth]{./fig/fig12_COCO_val2014_000000027476_new.pdf}
 	\includegraphics[height=.146\textwidth]{./fig/fig12_COCO_val2014_000000002867_new.pdf}
 	\includegraphics[height=.146\textwidth]{./fig/fig12_COCO_val2014_000000004243_new.pdf}
 	\includegraphics[height=.146\textwidth]{./fig/fig12_COCO_val2014_000000019444_new.pdf}
 	\includegraphics[height=.146\textwidth]{./fig/fig12_COCO_val2014_000000004554_new.pdf}
 	\includegraphics[height=.146\textwidth]{./fig/fig12_COCO_val2014_000000062657_new.pdf}\\
 	\includegraphics[height=.148\textwidth]{./fig/fig12_COCO_val2014_000000016961_new.pdf}
 	\includegraphics[height=.148\textwidth]{./fig/fig12_COCO_val2014_000000016491_new.pdf}
 	\includegraphics[height=.148\textwidth]{./fig/fig12_COCO_val2014_000000028881_new.pdf}
 	\includegraphics[height=.148\textwidth]{./fig/fig12_COCO_val2014_000000047516_new.pdf}
 	\includegraphics[height=.148\textwidth]{./fig/fig12_COCO_val2014_000000042507_new.pdf}\\
 	\includegraphics[height=.151\textwidth]{./fig/fig12_COCO_val2014_000000050746_new.pdf}
    \includegraphics[height=.151\textwidth]{./fig/fig12_COCO_val2014_000000021310_new.pdf}
 	\includegraphics[height=.151\textwidth]{./fig/fig12_COCO_val2014_000000021989_new.pdf}
 	\includegraphics[height=.151\textwidth]{./fig/fig12_COCO_val2014_000000021746_new.pdf}
 	\includegraphics[height=.151\textwidth]{./fig/fig12_COCO_val2014_000000015148_new.pdf}\\
\caption{Novel keypoint detection results on COCO validation images. Without any human labeling effort, our method learns to predict a new set of keypoints including those on the hands and feet.}
\label{fig:addition-kpts} \end{figure*}


\subsection{Novel keypoint detection}
\label{sec:novelkey}
Since our approach can easily create arbitrary annotations on synthetic data and transfer the knowledge to real domain, our method is highly scalable and flexible to users needs. For example, suppose we want to predict a new set of keypoints including hands and feet, it would be difficult to re-label the entire COCO dataset. With our technique, we can simply generate new labels on the synthetic data. We have performed an experiment to demonstrate this capability.


\begin{figure}
	\centering
	\includegraphics[height=.27\textwidth]{./fig/fig13_new-joint_new.pdf}
	\caption{The definition of our created novel keypoints. There is a total of  keypoints and  part associations for constructing fine-grained human skeleton.}
	\label{fig:joint-define}
\end{figure}



\begin{figure*}[h]
	 \centering
 \setlength{\tabcolsep}{4pt}
 \begin{tabular}{cc}
    \includegraphics[width=1\columnwidth]{./fig/fig14-left_new.pdf}
    &\includegraphics[width=1\columnwidth]{./fig/fig14-right_new.pdf}\\
(a) & (b)\\
\end{tabular}
	\caption{Qualitative comparison with the state-of-the-art approach~\cite{varol2017learning}. Previous method required additional preprocessing to normalize the input image, and failed to predict part segmentation for all the people due to occlusions. In contrast, our method does not require any preprocessing, and generated correct part segmentation for all the people even though some of them are heavily occluded by others.}
	\label{fig:demo}
\end{figure*}


\begin{figure*}
 \centering
 \setlength{\tabcolsep}{0.5pt}
 \begin{tabular}{cccc}
  	\includegraphics[width=.245\textwidth]{./fig/fig15_original_5610_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_synonly_5610_new.pdf}
    &\includegraphics[width=.245\textwidth]{./fig/fig15_adv_5610_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_ours_5610_new.pdf}\\
  	\includegraphics[width=.245\textwidth]{./fig/fig15_original_6435_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_synonly_6435_new.pdf}
    &\includegraphics[width=.245\textwidth]{./fig/fig15_adv_6435_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_ours_6435_new.pdf}\\
  	\includegraphics[width=.245\textwidth]{./fig/fig15_original_6371_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_synonly_6371_new.pdf}
    &\includegraphics[width=.245\textwidth]{./fig/fig15_adv_6371_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_ours_6371_new.pdf}\\
   	\includegraphics[width=.245\textwidth]{./fig/fig15_original_6999_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_synonly_6999_new.pdf}
    &\includegraphics[width=.245\textwidth]{./fig/fig15_adv_6999_new.pdf}
 	&\includegraphics[width=.245\textwidth]{./fig/fig15_ours_6999_new.pdf}\\
 	Input & SYN & ADV & Ours\\
    \end{tabular}
 \caption{Qualitative comparison on a challenging video~\cite{youtube-video}. The left column shows the input images. The second column from the left shows the results of training using synthetic data only (SYN). The third column from the left shows the results of adversarial training (ADV). The right column shows our results. We can see that SYN failed completely for the real-world images due to domain gap. ADV produces many false alarms in the background. In contrast, our approach performs better than the previous approaches on the tested frames. More video results can be found at \href{https://youtu.be/8QaGfdHwH48}{https://youtu.be/8QaGfdHwH48}}
\label{fig:visual} \end{figure*}



We create  novel keypoints for each avatar in the graphics simulator, and use the proposed method to learn the new set of keypoints. Figure~\ref{fig:joint-define} shows the definition of the novel keypoints. To enable our existing network to learn such a new task, we add two additional head networks in our framework to learn the newly created  keypoints and their Part Affinity Fields, resulting in a total of  head networks in our network architecture. Figure~\ref{fig:addition-kpts} shows the qualitative results of our novel keypoint detection.
With small modifications of the existing network, our method learns the novel skeleton representations from the synthetic data and transfers the knowledge to the real domain. It eliminates the needs of ground truth labeling of the additional joints on the real data. 



\section{Qualitative comparison}
Recent study~\cite{varol2017learning} proposed to estimate body part segmentation by learning with synthetic data, which is closely related to our method. 
Since MPII dataset~\cite{andriluka20142d} does not have part segmentation labels for quantitative evaluation, Varol~\etal~\cite{varol2017learning} showed qualitative results on selected images from MPII. 
Given a test image, Varol~\etal~\cite{varol2017learning} used additional preprocessing to normalize the input. From their results on MPII dataset with multiple people, it appears that they cropped each image centered at a specific person before feeding to their network. In contrast, our method does not require such preprocessing. Furthermore, our method produces better results as shown in Figure~\ref{fig:demo}. For each example, we show the original image from MPII dataset~\cite{andriluka20142d}, our part segmentation result on the original image, the cropped version which was used as the network input in~\cite{varol2017learning}, and the part segmentation result of~\cite{varol2017learning}.

We further conduct qualitative comparison with the adversarial training approach on a challenging video~\cite{youtube-video}. We compare our method with the adversarial network models presented in Sec~\ref{sec:adversarial}, and Figure~\ref{fig:visual} shows the results. We can see that our method performs consistently better than the previous baseline approaches on the tested frames. The results validate the effectiveness of the proposed approach.
\section{Conclusion}\label{sec:conclusion}
We presented a cross-domain complementary learning framework for multi-person part segmentation. Without using any real data part segmentation labels, our method is able to achieve a comparable or better performance than several state-of-the-art techniques that use real part segmentation data for training. We further demonstrated that our technique can also be used to learn novel keypoint detection from synthetic data.











\section*{Acknowledgment}

We would like to thank Alvin Chia, Jon Hanzelka, and Pedro Urbina for their help with the synthetic data generation. We would like to thank Jamie Shotton for his support.


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi







\bibliographystyle{IEEEtran}



\begin{thebibliography}{1}

\bibitem{chen2014semantic}
L.-C. Chen, G.~Papandreou, I.~Kokkinos, K.~Murphy, and A.~L. Yuille, ``Semantic
  image segmentation with deep convolutional nets and fully connected crfs,''
  in \emph{Proc. ICLR}, 2015.

\bibitem{chen2018deeplab}
------, ``Deeplab: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected crfs,'' \emph{IEEE Trans. Pattern
  Anal. Mach. Intell.}, vol.~40, no.~4, pp. 834--848, 2018.

\bibitem{chen2016attention}
L.-C. Chen, Y.~Yang, J.~Wang, W.~Xu, and A.~L. Yuille, ``Attention to scale:
  Scale-aware semantic image segmentation,'' in \emph{Proc. CVPR}, 2016.

\bibitem{xia2016zoom}
F.~Xia, P.~Wang, L.-C. Chen, and A.~L. Yuille, ``Zoom better to see clearer:
  Human and object parsing with hierarchical auto-zoom net,'' in \emph{Proc.
  ECCV}, 2016.

\bibitem{gu2018ava}
C.~Gu, C.~Sun, D.~Ross, C.~Vondrick, C.~Pantofaru, Y.~Li, S.~Vijayanarasimhan,
  G.~Toderici, S.~Ricco, R.~Sukthankar \emph{et~al.}, ``Ava: A video dataset of
  spatio-temporally localized atomic visual actions,'' in \emph{Proc. CVPR},
  2018.

\bibitem{Guler2018DensePose}
R.~A. G{\"u}ler, N.~Neverova, and I.~Kokkinos, ``Densepose: Dense human pose
  estimation in the wild,'' in \emph{Proc. CVPR}, 2018.

\bibitem{ionescu2014human3}
C.~Ionescu, D.~Papava, V.~Olaru, and C.~Sminchisescu, ``Human3. 6m: Large scale
  datasets and predictive methods for 3d human sensing in natural
  environments,'' \emph{IEEE Trans. Pattern Anal. Mach. Intell.}, vol.~36,
  no.~7, pp. 1325--1339, 2014.

\bibitem{tong2013upper}
R.~Tong, D.~Xie, and M.~Tang, ``Upper body human detection and segmentation in
  low contrast video,'' \emph{IEEE Trans. Circuits Syst. Video Technol.},
  vol.~23, no.~9, pp. 1502--1509, 2013.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Proc. NeurIPS}, 2012.

\bibitem{long2015fully}
J.~Long, E.~Shelhamer, and T.~Darrell, ``Fully convolutional networks for
  semantic segmentation,'' in \emph{Proc. CVPR}, 2015.

\bibitem{zheng2015conditional}
S.~Zheng, S.~Jayasumana, B.~Romera-Paredes, V.~Vineet, Z.~Su, D.~Du, C.~Huang,
  and P.~H. Torr, ``Conditional random fields as recurrent neural networks,''
  in \emph{Proc. ICCV}, 2015.

\bibitem{marin2010learning}
J.~Marin, D.~V{\'a}zquez, D.~Ger{\'o}nimo, and A.~M. L{\'o}pez, ``Learning
  appearance in virtual scenarios for pedestrian detection,'' in \emph{Proc.
  CVPR}, 2010.

\bibitem{richter2016playing}
S.~R. Richter, V.~Vineet, S.~Roth, and V.~Koltun, ``Playing for data: Ground
  truth from computer games,'' in \emph{Proc. ECCV}, 2016.

\bibitem{ros2016synthia}
G.~Ros, L.~Sellart, J.~Materzynska, D.~Vazquez, and A.~M. Lopez, ``The synthia
  dataset: A large collection of synthetic images for semantic segmentation of
  urban scenes,'' in \emph{Proc. CVPR}, 2016.

\bibitem{varol2017learning}
G.~Varol, J.~Romero, X.~Martin, N.~Mahmood, M.~Black, I.~Laptev, and C.~Schmid,
  ``Learning from synthetic humans,'' in \emph{Proc. CVPR}, 2017.

\bibitem{tobin2017domain}
J.~Tobin, R.~Fong, A.~Ray, J.~Schneider, W.~Zaremba, and P.~Abbeel, ``Domain
  randomization for transferring deep neural networks from simulation to the
  real world,'' in \emph{Proc. IROS}, 2017.

\bibitem{tremblay2018training}
J.~Tremblay, A.~Prakash, D.~Acuna, M.~Brophy, V.~Jampani, C.~Anil, T.~To,
  E.~Cameracci, S.~Boochoon, and S.~Birchfield, ``Training deep networks with
  synthetic data: Bridging the reality gap by domain randomization,'' in
  \emph{Proc. CVPR Workshops}, 2018.

\bibitem{vazquez2014virtual}
D.~Vazquez, A.~M. Lopez, J.~Marin, D.~Ponsa, and D.~Geronimo, ``Virtual and
  real world adaptation for pedestrian detection,'' \emph{IEEE Trans. Pattern
  Anal. Mach. Intell.}, vol.~36, no.~4, pp. 797--809, 2014.

\bibitem{ren2017cross}
Z.~Ren and Y.~J. Lee, ``Cross-domain self-supervised multi-task feature
  learning using synthetic imagery,'' in \emph{Proc. CVPR}, 2018.

\bibitem{Tsai_adaptseg_2018}
Y.-H. Tsai, W.-C. Hung, S.~Schulter, K.~Sohn, M.-H. Yang, and M.~Chandraker,
  ``Learning to adapt structured output space for semantic segmentation,'' in
  \emph{Proc. CVPR}, 2018.

\bibitem{tzeng2017adversarial}
E.~Tzeng, J.~Hoffman, K.~Saenko, and T.~Darrell, ``Adversarial discriminative
  domain adaptation,'' in \emph{Proc. CVPR}, 2017.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in
  context,'' in \emph{Proc. ECCV}, 2014.

\bibitem{liebelt2010multi}
J.~Liebelt and C.~Schmid, ``Multi-view object class detection with a 3d
  geometric model,'' in \emph{Proc. CVPR}, 2010.

\bibitem{nevatia1977description}
R.~Nevatia and T.~O. Binford, ``Description and recognition of curved
  objects,'' \emph{Artificial Intelligence}, vol.~8, no.~1, pp. 77--98, 1977.

\bibitem{shilane2004princeton}
P.~Shilane, P.~Min, M.~Kazhdan, and T.~Funkhouser, ``The princeton shape
  benchmark,'' in \emph{Proc. Shape Modeling Applications}, 2004.

\bibitem{chang2015shapenet}
A.~X. Chang, T.~Funkhouser, L.~Guibas, P.~Hanrahan, Q.~Huang, Z.~Li,
  S.~Savarese, M.~Savva, S.~Song, H.~Su \emph{et~al.}, ``Shapenet: An
  information-rich 3d model repository,'' \emph{arXiv preprint
  arXiv:1512.03012}, 2015.

\bibitem{wu2016learning}
J.~Wu, C.~Zhang, T.~Xue, B.~Freeman, and J.~Tenenbaum, ``Learning a
  probabilistic latent space of object shapes via 3d generative-adversarial
  modeling,'' in \emph{Proc. NeurIPS}, 2016.

\bibitem{peng2015learning}
X.~Peng, B.~Sun, K.~Ali, and K.~Saenko, ``Learning deep object detectors from
  3d models,'' in \emph{Proc. ICCV}, 2015.

\bibitem{Qi_2016_CVPR}
C.~R. Qi, H.~Su, M.~Niessner, A.~Dai, M.~Yan, and L.~J. Guibas, ``Volumetric
  and multi-view cnns for object classification on 3d data,'' in \emph{Proc.
  CVPR}, 2016.

\bibitem{pan2010survey}
S.~J. Pan and Q.~Yang, ``A survey on transfer learning,'' \emph{IEEE Trans. on
  Knowledge and Data Engineering}, vol.~10, no.~22, pp. 1345--1359, 2010.

\bibitem{torralba2011unbiased}
A.~Torralba and A.~A. Efros, ``Unbiased look at dataset bias,'' in \emph{Proc.
  CVPR}, 2011.

\bibitem{ganin2015unsupervised}
Y.~Ganin and V.~Lempitsky, ``Unsupervised domain adaptation by
  backpropagation,'' in \emph{Proc. ICML}, 2015.

\bibitem{bousmalis2016domain}
K.~Bousmalis, G.~Trigeorgis, N.~Silberman, D.~Krishnan, and D.~Erhan, ``Domain
  separation networks,'' in \emph{Proc. NeurIPS}, 2016.

\bibitem{yu2016learning}
J.~Yu and J.~Jiang, ``Learning sentence embeddings with auxiliary tasks for
  cross-domain sentiment classification,'' in \emph{Proc. EMNLP}, 2016.

\bibitem{zhao2019multi}
S.~Zhao, B.~Li, X.~Yue, Y.~Gu, P.~Xu, R.~Hu, H.~Chai, and K.~Keutzer,
  ``Multi-source domain adaptation for semantic segmentation,'' in \emph{Proc.
  NeurIPS}, 2019.

\bibitem{Tsai_2019_ICCV}
Y.-H. Tsai, K.~Sohn, S.~Schulter, and M.~Chandraker, ``Domain adaptation for
  structured output via discriminative patch representations,'' in \emph{Proc.
  ICCV}, 2019.

\bibitem{chen2019learning}
Y.~Chen, W.~Li, X.~Chen, and L.~V. Gool, ``Learning semantic segmentation from
  synthetic data: A geometrically guided input-output adaptation approach,'' in
  \emph{Proc. CVPR}, 2019.

\bibitem{chen2019crdoco}
Y.-C. Chen, Y.-Y. Lin, M.-H. Yang, and J.-B. Huang, ``Crdoco: Pixel-level
  domain transfer with cross-domain consistency,'' in \emph{Proc. CVPR}, 2019.

\bibitem{zhang2018fully}
Y.~Zhang, Z.~Qiu, T.~Yao, D.~Liu, and T.~Mei, ``Fully convolutional adaptation
  networks for semantic segmentation,'' in \emph{Proc. CVPR}, 2018.

\bibitem{luo2019taking}
Y.~Luo, L.~Zheng, T.~Guan, J.~Yu, and Y.~Yang, ``Taking a closer look at domain
  shift: Category-level adversaries for semantics consistent domain
  adaptation,'' in \emph{Proc. CVPR}, 2019.

\bibitem{Hoffmann:GCPR:2019}
D.~T. Hoffmann, D.~Tzionas, M.~J. Black, and S.~Tang, ``Learning to train with
  synthetic humans,'' in \emph{Proc. GCPR}, 2019.

\bibitem{du2017rpan}
W.~Du, Y.~Wang, and Y.~Qiao, ``Rpan: An end-to-end recurrent pose-attention
  network for action recognition in videos,'' in \emph{Proc. ICCV}, 2017.

\bibitem{gkioxari2014r}
G.~Gkioxari, B.~Hariharan, R.~Girshick, and J.~Malik, ``R-cnns for pose
  estimation and action detection,'' \emph{arXiv preprint arXiv:1406.5212},
  2014.

\bibitem{he2017mask}
K.~He, G.~Gkioxari, P.~Doll{\'a}r, and R.~Girshick, ``Mask r-cnn,'' in
  \emph{Proc. ICCV}, 2017.

\bibitem{li2014heterogeneous}
S.~Li, Z.-Q. Liu, and A.~B. Chan, ``Heterogeneous multi-task learning for human
  pose estimation with deep convolutional neural network,'' in \emph{Proc. CVPR
  Workshop}, 2014.

\bibitem{popa2017deep}
A.-I. Popa, M.~Zanfir, and C.~Sminchisescu, ``Deep multitask architecture for
  integrated 2d and 3d human sensing,'' in \emph{Proc. CVPR}, 2017.

\bibitem{zhou2017towards}
X.~Zhou, Q.~Huang, X.~Sun, X.~Xue, and Y.~Wei, ``Towards 3d human pose
  estimation in the wild: a weakly-supervised approach,'' in \emph{Proc. ICCV},
  2017.

\bibitem{ruder2017overview}
S.~Ruder, ``An overview of multi-task learning in deep neural networks,''
  \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem{zhang2017survey}
Y.~Zhang and Q.~Yang, ``A survey on multi-task learning,'' \emph{arXiv preprint
  arXiv:1707.08114}, 2017.

\bibitem{dong2014towards}
J.~Dong, Q.~Chen, X.~Shen, J.~Yang, and S.~Yan, ``Towards unified human parsing
  and pose estimation,'' in \emph{Proc. CVPR}, 2014.

\bibitem{gong2017look}
K.~Gong, X.~Liang, D.~Zhang, X.~Shen, and L.~Lin, ``Look into person:
  Self-supervised structure-sensitive learning and a new benchmark for human
  parsing,'' in \emph{Proc. CVPR}, 2017.

\bibitem{ladicky2013human}
L.~Ladicky, P.~H. Torr, and A.~Zisserman, ``Human pose estimation using a joint
  pixel-wise and part-wise formulation,'' in \emph{Proc. CVPR}, 2013.

\bibitem{xia2017joint}
F.~Xia, P.~Wang, X.~Chen, and A.~L. Yuille, ``Joint multi-person pose
  estimation and semantic part segmentation,'' in \emph{Proc. CVPR}, 2017.

\bibitem{xia2016pose}
F.~Xia, J.~Zhu, P.~Wang, and A.~L. Yuille, ``Pose-guided human parsing by an
  and/or graph using pose-context features,'' in \emph{Proc. AAAI}, 2016.

\bibitem{papandreou2018personlab}
G.~Papandreou, T.~Zhu, L.-C. Chen, S.~Gidaris, J.~Tompson, and K.~Murphy,
  ``Personlab: Person pose estimation and instance segmentation with a
  bottom-up, part-based, geometric embedding model,'' in \emph{Proc. ECCV},
  2018.

\bibitem{fang2018weakly}
H.-S. Fang, G.~Lu, X.~Fang, J.~Xie, Y.-W. Tai, and C.~Lu, ``Weakly and semi
  supervised human body part parsing via pose-guided knowledge transfer,'' in
  \emph{Proc. CVPR}, 2018.

\bibitem{bearman2016s}
A.~Bearman, O.~Russakovsky, V.~Ferrari, and L.~Fei-Fei, ``What’s the point:
  Semantic segmentation with point supervision,'' in \emph{Proc. ECCV}, 2016.

\bibitem{cmu-mocap}
``{Carnegie Mellon University Motion Capture Database},''
  \url{http://mocap.cs.cmu.edu/}.

\bibitem{arnoldrenderer}
``{Arnold},'' \url{https://www.arnoldrenderer.com/}.

\bibitem{maya}
``{Autodesk software},'' \url{https://www.autodesk.com/}.

\bibitem{cao2017realtime}
Z.~Cao, T.~Simon, S.-E. Wei, and Y.~Sheikh, ``Realtime multi-person 2d pose
  estimation using part affinity fields,'' in \emph{Proc. CVPR}, 2017.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proc. CVPR}, 2016.

\bibitem{lin2017feature}
T.-Y. Lin, P.~Doll{\'a}r, R.~Girshick, K.~He, B.~Hariharan, and S.~Belongie,
  ``Feature pyramid networks for object detection,'' in \emph{Proc. CVPR},
  2017.

\bibitem{chen2017cascaded}
Y.~Chen, Z.~Wang, Y.~Peng, Z.~Zhang, G.~Yu, and J.~Sun, ``Cascaded pyramid
  network for multi-person pose estimation,'' in \emph{Proc. CVPR}, 2018.

\bibitem{wei2016convolutional}
S.-E. Wei, V.~Ramakrishna, T.~Kanade, and Y.~Sheikh, ``Convolutional pose
  machines,'' in \emph{Proc. CVPR}, 2016.

\bibitem{chen_cvpr14}
X.~Chen, R.~Mottaghi, X.~Liu, S.~Fidler, R.~Urtasun, and A.~Yuille, ``Detect
  what you can: Detecting and representing objects using holistic models and
  body parts,'' in \emph{Proc. CVPR}, 2014.

\bibitem{liang2016semantic2}
X.~Liang, X.~Shen, D.~Xiang, J.~Feng, L.~Lin, and S.~Yan, ``Semantic object
  parsing with local-global long short-term memory,'' in \emph{Proc. CVPR},
  2016.

\bibitem{liang2016semantic}
X.~Liang, X.~Shen, J.~Feng, L.~Lin, and S.~Yan, ``Semantic object parsing with
  graph lstm,'' in \emph{Proc. ECCV}, 2016.

\bibitem{everingham2015pascal}
M.~Everingham, S.~A. Eslami, L.~Van~Gool, C.~K. Williams, J.~Winn, and
  A.~Zisserman, ``The pascal visual object classes challenge: A
  retrospective,'' \emph{Int. J. Comput. Vis}, vol. 111, no.~1, pp. 98--136,
  2015.

\bibitem{aic}
``{AI Challenger},'' \url{https://challenger.ai/}.

\bibitem{maaten2008visualizing}
L.~v.~d. Maaten and G.~Hinton, ``Visualizing data using t-sne,'' \emph{J. Mach.
  Learn. Res.}, vol.~9, no. Nov, pp. 2579--2605, 2008.

\bibitem{jegou2008hamming}
H.~Jegou, M.~Douze, and C.~Schmid, ``Hamming embedding and weak geometric
  consistency for large scale image search,'' in \emph{Proc. ECCV}, 2008.

\bibitem{youtube-video}
``{Crazy Uptown Funk flashmob in Sydney},''
  \url{https://www.youtube.com/watch?v=2DiQUX11YaY}.

\bibitem{andriluka20142d}
M.~Andriluka, L.~Pishchulin, P.~Gehler, and B.~Schiele, ``2d human pose
  estimation: New benchmark and state of the art analysis,'' in \emph{Proc.
  CVPR}, 2014.

\end{thebibliography}









\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./people/kevin_lin.jpg}}]{Kevin Lin} (S'10) received the M.S. degree from the Graduate Institute of Networking and Multimedia, National Taiwan University, in 2014. He is currently working toward the Ph.D. degree in electrical engineering at the University of Washington. During his study, he was a research intern at Microsoft Research. His research interests include computer vision, machine learning, and natural language processing.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./people/lijuan_wang.jpeg}}]{Lijaun Wang} received the B.E. degree from Huazhong University of Science and Technology, and the Ph.D. degree from Tsinghua University, in 2001 and 2006 respectively. She joined Microsoft Research in 2006, where she is currently a principle research manager. She has been the key contributor in developing technologies on computer vision, printed and handwritten text recognition, avatar animation, and speech synthesis and recognition, which have been shipped into various Microsoft products. Current research interests include vision-language pre-training, image captioning, human pose estimation and part segmentation, and machine learning. She was part of the team that developed Azure Kinect Body Tracking SDK. She has published more than 40 papers on the top conferences and journals, and she has been awarded more than 15 US patents. She is a senior member of IEEE.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./people/kun_luo.jpg}}]{Kun Luo} received his M.S. degree in computer science from University of California San Diego. He is currently a Research Software Development Engineer at Microsoft. He mainly works on building systems solving computer vision problems via deep learning methods. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./people/yinpeng_chen.jpg}}]{Yinpeng Chen} received the Ph.D. degree in electrical engineering from Arizona State University, Tempe, in 2009. He received the B.S. and M.S. degree in electrical engineering from Tsinghua University, Beijing, in 2000 and 2003, respectively. He is currently a researcher at Microsoft. His current research interests include computer vision and multimedia. 
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./people/zicheng_liu.jpg}}]{Zicheng Liu} (SM'05-F'15) received the B.S. degree in mathematics from Huazhong Normal University, Wuhan, China, in 1984, the M.S. degree in operations research from the Institute of Applied Mathematics, Chinese Academy of Sciences, in 1989, and the Ph.D. degree in computer science from Princeton University in 1996. He joined Microsoft Research in 1997, and he is currently a principal research manager. Before joining Microsoft Research, he was with Silicon Graphics, Inc., as a Member of Technical Staff for two years, where he developed the trimmed NURBS tessellator shipped in both OpenGL and the OpenGL Optimizer. Current research interests include human activity recognition, human pose estimation and part segmentation, and machine learning. He was part of the team that developed Azure Kinect Body Tracking SDK. 

Liu has co-authored three books. He served as the chair of the Multimedia Systems and Applications Technical Committee of IEEE CAS society. He was a steering committee member of \textit{IEEE Transactions on Multimedia}. He is the Editor-in- Chief of \textit{Journal of Visual Communications and Image Representation}. He is an affiliate professor in the department of Electrical and Computer Engineering, University of Washington. He was an IEEE distinguished lecturer from 2015-2016. He is a fellow of IEEE.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./people/ming-ting_sun.jpg}}]{Ming-Ting Sun} (S'79-M'81-SM'89-F'96-LF'20) received the B.S. degree from National Taiwan University and the Ph.D. degree from University of California, Los Angeles, all in electrical engineering.

Dr. Sun joined the University of Washington in 1996 where he is a Professor. Previously, he was the Director of Video Signal Processing Research at Bellcore. He has been a chaired/visiting professor at several universities. His main research interest is video and multimedia signal processing.

Dr. Sun holds 13 patents and has published about 300 technical papers, including 17 book chapters in the area of video and multimedia technologies.  He co-edited a book ``Compressed Video over Networks''.  He has guest-edited 12 special issues for various journals and given keynotes for several international conferences. He was an Editor-in-Chief of the \textit{Journal for Visual Communication and Image Representation (JVCI)} from 2012 to 2016, the Editor-in-Chief for \textit{IEEE Transactions on Multimedia (TMM)} from 2000 to 2001, and the Editor-in-Chief of the \textit{IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)} from 1995 to 1997. He was a Distinguished Lecturer of the Circuits and Systems Society from 2000 to 2001. He received an IEEE CASS Golden Jubilee Medal in 2000. He served as a General Co-Chair of ICME (International Conference on Multimedia and Expo) 2016, an Honorary Chair of VCIP (Visual Communication and Image Processing) 2015, a General Co-Chair chair of Visual Communications and Image Processing 2000. He served as the Chair of the VSPC (Visual Signal Processing and Communications) Technical Committee of IEEE CAS (Circuits and Systems) Society from 1993 to 1994. He received the \textit{TCSVT} Best Paper Award in 1993.  From 1988 to 1991, he was the chairman of the IEEE CAS Standards Committee and established the IEEE Inverse Discrete Cosine Transform Standard. He is a Life Fellow of IEEE.

\end{IEEEbiography}









\end{document}
