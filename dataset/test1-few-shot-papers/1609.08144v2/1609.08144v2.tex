In this section, we present our experimental results on
two publicly available corpora used extensively as
benchmarks for Neural Machine Translation systems:
WMT'14 English-to-French (WMT EnFr) and
English-to-German (WMT EnDe). On these two datasets, we
benchmark GNMT models with word-based, character-based, and wordpiece-based
vocabularies. We also present the improved accuracy of our models after
fine-tuning with RL and model ensembling. Our main objective
with these datasets is to show the contributions of various components
in our implementation, in particular the wordpiece model, RL
model refinement, and model ensembling.

In addition to testing on publicly available corpora, we also test GNMT on
Google's translation production corpora, which are two to three decimal orders of magnitudes bigger than the WMT corpora for a given language pair. We
compare the accuracy of our model against human accuracy and the 
best Phrase-Based Machine Translation (PBMT) production system for Google Translate.


In all experiments, our models consist of 8 encoder layers and 8 decoder layers.
(Since the bottom encoder layer is actually bi-directional, in total there are
9 logically distinct LSTM passes in the encoder.)
The attention network is a simple feedforward network with one hidden layer with 1024 nodes.
All of the models use 1024 LSTM nodes per encoder and decoder layers.



\subsection{Datasets}
We evaluate our model on the WMT EnFr dataset, the WMT
EnDe dataset, as well as many Google-internal
production datasets. On WMT EnFr, the training set
contains 36M sentence pairs.  On WMT EnDe, the training
set contains 5M sentence pairs. In both cases, we use newstest2014 as the test
sets to compare against previous
work~\cite{luong2015addressing,jean2015using,DBLP:journals/corr/ZhouCWLX16}.
The combination of newstest2012 and newstest2013 is used as the development set.

In addition to WMT, we also evaluate
our model on some Google-internal datasets representing a wider
spectrum of languages with distinct linguistic properties:
English  French, English  Spanish and
English  Chinese.

\subsection{Evaluation Metrics}
We evaluate our models using the standard BLEU score metric. To be
comparable to previous work~\cite{sutskever2014sequence,luong2015addressing,DBLP:journals/corr/ZhouCWLX16}, we report
tokenized BLEU score as computed by the \texttt{multi-bleu.pl} script,
downloaded from the public implementation of Moses (on Github), which is
also used in~\cite{luong2015addressing}.

As is well-known, BLEU score does not fully capture the quality of a
translation. For that reason we also carry out side-by-side (SxS)
evaluations where we have human raters evaluate and compare the
quality of two translations presented side by side for a given source
sentence. Side-by-side scores range from 0 to 6, with a score of 0
meaning \textsl{``completely nonsense translation''}, and a score of 6
meaning \textsl{``perfect translation: the meaning of the translation
is completely consistent with the source, and the grammar is
correct''}. A translation is given a score of 4 if \textsl{``the
sentence retains most of the meaning of the source sentence, but may
have some grammar mistakes''}, and a translation is given a score of 2
if \textsl{``the sentence preserves some of the meaning of the source
sentence but misses significant parts''}. These scores are generated
by human raters who are fluent in both languages and hence often
capture translation quality better than BLEU scores.





\subsection{Training Procedure}
The models are trained by a system we implemented using
TensorFlow\cite{tensorflow16}.
The training setup follows the classic
data parallelism paradigm. There are 12 replicas running
concurrently on separate machines. Every replica updates the shared
parameters asynchronously.

We initialize all trainable parameters uniformly between [-0.04, 0.04]. As
is common wisdom in training RNN models, we apply gradient clipping
(similar to \cite{sutskever2014sequence}): all gradients are uniformly
scaled down such that the norm of the modified gradients is no larger
than a fixed constant, which is  in our case. If the norm of the
original gradients is already smaller than or equal to the given
threshold, then gradients are not changed.

For the first stage of maximum likelihood training (that is, to
optimize for objective function \ref{eq:objml}), we use a
combination of Adam \cite{DBLP:journals/corr/KingmaB14} and simple SGD
learning algorithms provided by the TensorFlow runtime system.  We run Adam for
the first 60k steps, after which we switch to simple SGD. Each step in
training is a mini-batch of 128 examples. 

\begin{figure}[h!]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{adam_and_sgd}}
	\caption{Log perplexity vs. steps for Adam, SGD and
          Adam-then-SGD on WMT EnFr during maximum
          likelihood training.  Adam converges much faster than SGD at
          the beginning.  Towards the end, however, Adam-then-SGD is
          gradually better. Notice the bump in the red curve
          (Adam-then-SGD) at around 60k steps where we switch from
          Adam to SGD.  We suspect that this bump occurs due to
          different optimization trajectories of Adam vs. SGD. When we
          switch from Adam to SGD, the model first suffers a little,
          but is able to quickly recover afterwards.}
\label{sgd_adam}
\end{center}
\end{figure}

We find that Adam accelerates training at the beginning, but Adam
alone converges to a worse point than a combination of Adam first, followed by 
SGD (Figure~\ref{sgd_adam}). For the Adam part, we use a learning
rate of , and for the SGD part, we use a learning rate of
. We find that it is important to also anneal the learning rate
after a certain number of total steps. For the WMT EnFr dataset, 
we begin
to anneal the learning rate after 1.2M steps, after which we halve the
learning rate every 200k steps for an additional 800k steps. On WMT
EnFr, it takes around 6 days to train a basic model using 96 NVIDIA K80 GPUs.






Once a model is fully converged using the ML objective, we switch to RL
based model refinement, i.e., we further optimize the objective
function as in equation \ref{eq:mixed}. We refine a model until the BLEU
score does not change much on the development set.  For this model
refinement phase, we simply run the SGD optimization algorithm. The number
of steps needed to refine a model varies from dataset to dataset. For
WMT EnFr, it takes around 3 days to complete 400k steps. 

To prevent overfitting, we apply dropout during training with a scheme similar
to \cite{lstm_dropout}. For the WMT EnFr and EnDe datasets, we set the
dropout probability to be  and  respectively. Due to various technical reasons, dropout is
only applied during the ML training phase, not during the RL refinement phase.

The exact hyper-parameters vary from dataset to dataset and from model
to model. For the WMT EnDe dataset, since it is significantly
smaller than the WMT EnFr dataset, we use a higher dropout
probability, and also train smaller models for fewer steps overall. 
On the production data sets, we typically do not use dropout, and
we train the models for more steps.

\subsection{Evaluation after Maximum Likelihood Training}

The models in our experiments are word-based, character-based, mixed
word-character-based or several wordpiece models with varying
vocabulary sizes.

For the word model, we selected the most frequent 212K source words as
the source vocabulary and the most popular 80k target words as the
target vocabulary. Words not in the source vocabulary or the target
vocabulary (unknown words) are converted into special 
\texttt{<first\_char>\_UNK\_<last\_char>}
symbols. Note, in this case, there is more than one UNK (e.g., our
production word models have roughly 5000 different UNKs in this case). We then
use the attention mechanism to copy a corresponding word from the
source to replace these unknown words during decoding~\cite{jean2015using}.

The mixed word-character model is similar to the word model, except the
out-of-vocabulary (OOV) words are converted into sequences of
characters with special delimiters around them as described in section
\ref{Mixed Word/Character Model} in more detail. In
our experiments, the vocabulary size for the mixed word-character
model is 32K. For the pure character model, we simply split all words
into constituent characters, resulting typically in a few hundred basic
characters (including special symbols appearing in the data). For the
wordpiece models, we train 3 different models with vocabulary sizes of
8K, 16K, and 32K.







Table \ref{table:wmt_en_fr} summarizes our results on the WMT
EnFr dataset. In this table, we also compare against other
strong baselines without model ensembling.  As can be seen from the
table, ``WPM-32K'', a wordpiece model with a shared source and target
vocabulary of 32K wordpieces, performs well on this dataset and achieves the
best quality as well as the fastest inference speed.

The pure character model (char input, char output) works surprisingly
well on this task, not much worse than the best wordpiece models in BLEU 
score. However, these models are rather slow to train and slow to use as the 
sequences are much longer.

Our best model, WPM-32K, achieves a BLEU score of 38.95. Note that this
BLEU score represents the averaged score of 8 models we trained. The maximum
BLEU score of the 8 models is higher at 39.37.  We point out
that our models are completely self-contained, as opposed to previous models
reported in~\cite{DBLP:journals/corr/ZhouCWLX16}, which depend on
some external alignment models to achieve their best results. Also note that
all our test set numbers were achieved by picking an optimal model on the
development set which was then used to decode the test set.

Note that the timing numbers for this section are obtained on CPUs, not TPUs.
We use here the same CPU machine as described above, and run the decoder with
a batchsize of 16 sentences in parallel and a maximum of 4 concurrent
hypotheses at any time per sentence. The time per sentence is the total
decoding time divided by the number of respective sentences in the test set.







\begin{table}[h!]
\caption{Single model results on WMT EnFr (newstest2014)}
\label{table:wmt_en_fr}
\centering
\begin{tabular}{r c c c }
\hline\hline Model     & BLEU        & CPU decoding time    \\
                 &             & per sentence (s) \\ \hline
       Word      & 37.90       & 0.2226           \\
       Character & 38.01       & 1.0530           \\
       WPM-8K    & 38.27       & 0.1919           \\
       WPM-16K   & 37.60       & 0.1874           \\
       WPM-32K   & 38.95       & 0.2118           \\

       Mixed Word/Character& 38.39 & 0.2774    \\\hline
       PBMT~\cite{durrani2014edinburgh} & 37.0  & \\
       LSTM (6 layers)~\cite{luong2015addressing} & 31.5   & \\
       LSTM (6 layers + PosUnk)~\cite{luong2015addressing} & 33.1   & \\
       Deep-Att~\cite{DBLP:journals/corr/ZhouCWLX16} & 37.7 &  \\
       Deep-Att + PosUnk~\cite{DBLP:journals/corr/ZhouCWLX16} & 39.2  & \\
\hline \end{tabular}
\end{table}



Similarly, the results of WMT EnDe are presented in
Table~\ref{table:wmt_en_de}. Again, we find that wordpiece models
achieves the best BLEU scores.

\begin{table}[h!]
\caption{Single model results on WMT EnDe (newstest2014)} \label{table:wmt_en_de}
\centering
\begin{tabular}{r c c }
\hline\hline Model     & BLEU        & CPU decoding time    \\
                 &             & per sentence (s) \\ \hline
       Word                  & 23.12   & 0.2972           \\
       Character (512 nodes) & 22.62   & 0.8011           \\
       WPM-8K                & 23.50   & 0.2079           \\
       WPM-16K               & 24.36   & 0.1931           \\
       WPM-32K               & 24.61   & 0.1882           \\
       Mixed Word/Character  & 24.17   & 0.3268           \\ 
\hline
       PBMT~\cite{buck2014n}                           & 20.7     & \\
       RNNSearch~\cite{jean2015using} & 16.5            \\
       RNNSearch-LV~\cite{jean2015using} & 16.9          \\
       RNNSearch-LV~\cite{jean2015using} & 16.9          \\
       Deep-Att~\cite{DBLP:journals/corr/ZhouCWLX16} & 20.6    & \\
\hline \end{tabular}
\end{table}



WMT EnDe
is considered a more difficult task than WMT EnFr as
it has much less training data, and German, as a more morphologically
rich language, needs a huge vocabulary for word models.  Thus it is
more advantageous to use wordpiece or mixed word/character models,
which provide a gain of more than 2 BLEU points on top of the
word model and about 4 BLEU points on top of previously reported results
in~\cite{buck2014n,DBLP:journals/corr/ZhouCWLX16}.
Our best model, WPM-32K, achieves a BLEU score of 24.61, which is averaged over 8 runs.
Consistently, on the production corpora, wordpiece models tend
to be better than other models both in terms of speed and accuracy.



\subsection{Evaluation of RL-refined Models}
The models trained in the previous section are optimized for
log-likelihood of the next step prediction
which may not correlate well with translation quality, as discussed in
section~\ref{sec:RL}. We use RL training to fine-tune sentence BLEU scores
after normal maximum-likelihood training.

The results of RL fine-tuning on the best EnFr and
EnDe models are presented in
Table~\ref{table:rl_wmt_en_fr}, which show that fine-tuning the
models with RL can improve BLEU scores. On WMT EnFr,
model refinement improves BLEU score by close to 1 point. On EnDe,
RL-refinement slightly hurts the test performance even though we observe about 0.4 BLEU points
improvement on the development set. The results presented in
Table~\ref{table:rl_wmt_en_fr} are the average of 8 independent models.
We also note that there is an overlap between the wins from the RL refinement and the decoder 
fine-tuning (i.e., the introduction of length normalization and coverage penalty).
On a less fine-tuned decoder (e.g., if the decoder does beam search by
log-probability only), the win from RL would have been bigger (as is evident
from comparing results in Table~\ref{table:decoder} and
Table~\ref{table:decoder_after_rl}).

\begin{table}[h!]
\caption{Single model test BLEU scores, averaged over 8 runs, on WMT EnFr and
  EnDe}
\label{table:rl_wmt_en_fr}
\centering
\begin{tabular}{r c c c }
\hline\hline Dataset & Trained with log-likelihood   & Refined with RL  \\\hline
       EnFr  & 38.95                  & 39.92           \\ \hline
       EnDe  & 24.67                  & 24.60           \\
\hline \end{tabular}
\end{table}

\subsection{Model Ensemble and Human Evaluation}
We ensemble 8 RL-refined models to obtain a state-of-the-art
result of 41.16 BLEU points on the WMT EnFr dataset. Our results are
reported in Table~\ref{table:ensemble-wmt_en_fr}.
\begin{table}[h!]
\caption{Model ensemble results on WMT EnFr (newstest2014)}
\label{table:ensemble-wmt_en_fr}
\centering
\begin{tabular}{r c c }
\hline\hline Model                                                             & BLEU        \\
       \hline
       WPM-32K (8 models)                                                & 40.35 \\
       RL-refined WPM-32K (8 models)                                     & 41.16 \\ \hline
       LSTM (6 layers)~\cite{luong2015addressing}                        & 35.6 \\
       LSTM (6 layers + PosUnk)~\cite{luong2015addressing}               & 37.5 \\
       Deep-Att + PosUnk (8 models)~\cite{DBLP:journals/corr/ZhouCWLX16} & 40.4 \\
\hline \end{tabular}
\end{table}

We ensemble 8 RL-refined models to obtain a state-of-the-art
result of 26.30 BLEU points on the WMT EnDe dataset. Our results are
reported in Table~\ref{table:ensemble-wmt_en_de}.
\begin{table}[h!]
\caption{Model ensemble results on WMT EnDe (newstest2014). See Table~\ref{table:wmt_en_de} for a comparison against non-ensemble models.}
\label{table:ensemble-wmt_en_de}
\centering
\begin{tabular}{r c c }
\hline\hline Model                                                             & BLEU        \\
       \hline
       WPM-32K (8 models)                                                & 26.20 \\
       RL-refined WPM-32K (8 models)                                     & 26.30 \\
\hline \end{tabular}
\end{table}

Finally, to better understand the quality of our models and the effect
of RL refinement, we carried out a four-way side-by-side human
evaluation to compare our NMT translations against the reference translations
and the best phrase-based statistical machine translations.
During the side-by-side comparison,
humans are asked to rate four translations given a source sentence.
The four translations are:
1) the best phrase-based translations as downloaded
from \textsl{http://matrix.statmt.org/systems/show/2065},
2) an ensemble of 8 ML-trained models,
3) an ensemble of 8 ML-trained and then RL-refined models, and
4) reference human translations as taken directly from newstest2014,
Our results are presented in Table~\ref{table:wmt_en_fr_sxs}.

\begin{table}[h!]
\caption{Human side-by-side evaluation scores of WMT EnFr models.}
\centering
\begin{tabular}{r c c}
\hline\hline Model & BLEU & Side-by-side \\
      &      & averaged score \\ \hline
	PBMT~\cite{durrani2014edinburgh} & 37.0 & 3.87 \\ NMT before RL & 40.35 & 4.46 \\
	NMT after RL & 41.16 & 4.44 \\ \hline Human &  & 4.82 \\ \hline \end{tabular}
\label{table:wmt_en_fr_sxs}
\end{table}


The results show that even though RL refinement can achieve better
BLEU scores, it barely improves the human impression of the translation
quality.  This could be due to a combination of factors including: 1)
the relatively small sample size for the experiment (only 500
examples for side-by-side), 2) the improvement in BLEU score by RL
is relatively small after model ensembling (0.81), which may be at a
scale that human side-by-side evaluations are insensitive to, and 3) the
possible 
mismatch between BLEU as a metric and real translation quality as perceived by
human raters. Table~\ref{table:example_translations} contains some example
translations from PBMT, "NMT before RL" and "Human", along with the
side-by-side scores that human raters assigned to each translation
(some of which we disagree with, see the table caption).








\subsection{Results on Production Data}

We have carried out extensive experiments on many Google-internal production
data sets.
As the experiments above cast doubt on whether RL improves the real translation
quality or simply the BLEU metric, RL-based model
refinement is not used during these experiments.
Given the larger volume of training data available in the Google corpora,
dropout is also not needed in these experiments.

\begin{table}[h!]
\caption{Mean of side-by-side scores on production data} \centering
\begin{tabular}{l c c c c }
\hline\hline & PBMT & GNMT & Human & Relative\\ [0.5ex]
	&      &      &       & Improvement\\ [0.5ex]
\hline English  Spanish     & 4.885 & 5.428 & 5.504 & 87\%\\ English  French	  & 4.932 & 5.295 & 5.496 & 64\%\\ English  Chinese     & 4.035 & 4.594 & 4.987 & 58\%\\ Spanish  English     & 4.872 & 5.187 & 5.372 & 63\%\\ French   English	  & 5.046 & 5.343 & 5.404 & 83\%\\ Chinese  English     & 3.694 & 4.263 & 4.636 & 60\%\\ \hline \end{tabular}
\label{table:prod}
\end{table}

In this section we describe our experiments with human perception of the
translation quality. We asked human raters to rate translations in
a three-way side-by-side comparison.  The three sides are from: 1) translations 
from the production phrase-based statistical translation system used by Google, 
2) translations from our GNMT system, and 3) translations by humans fluent in 
both languages.  Reported here in Table \ref{table:prod} are averaged rated 
scores for English
 French, English  Spanish and
English 
Chinese. All the GNMT models are wordpiece models, without model
ensembling, and use a shared source and target vocabulary with 32K wordpieces. 
On each pair of languages, the evaluation data consist of 500
randomly sampled sentences from Wikipedia and news websites, and the
corresponding human translations to the target language. The
results show that our model reduces translation errors by more than 60\%
compared to the PBMT model on these major pairs of languages. A typical
distribution of side-by-side scores is shown in Figure \ref{fig:histogram}.

\begin{figure}[h!]
\centering
\includegraphics[width=6in]{histogram.png}
\caption{Histogram of side-by-side scores on 500 sampled sentences from Wikipedia and news websites for a typical language pair, here English  Spanish (PBMT blue, GNMT red, Human orange). It can be seen that there is a wide distribution in scores, even for the human translation when rated by other humans, which shows how ambiguous the task is. It is clear that GNMT is much more accurate than PBMT.}
\label{fig:histogram}
\end{figure}

As expected, on this metric the GNMT system improves also compared to the PBMT
system. In some cases human and GNMT translations are
nearly indistinguishable on the relatively simplistic and isolated sentences
sampled from Wikipedia and news articles for this experiment. Note that we have
observed that human raters, even though fluent in both languages, do not
necessarily fully understand each randomly sampled sentence sufficiently and 
hence cannot necessarily generate the best possible translation or rate a 
given translation accurately. Also note that, although the scale for the
scores goes from
0 (complete nonsense) to 6 (perfect translation) the human translations
get an imperfect score of only around 5 in Table \ref{table:prod}, which shows
possible ambiguities in the translations and also possibly non-calibrated raters
and translators with a varying level of proficiency.

Testing our GNMT system on particularly difficult translation cases and longer
inputs than just single sentences is the subject of future work.








