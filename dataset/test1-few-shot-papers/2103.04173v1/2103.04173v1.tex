

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage[utf8]{inputenc}
\usepackage[english]{babel}



\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{amsmath,amssymb} \usepackage{algorithm2e}
\SetKwInput{KwInput}{Input}                \SetKwInput{KwOutput}{Output}              

\newcommand{\gustavo}[1]{{\color{blue}GC: #1}}
\newcommand{\ragav}[1]{{\color{red}RS: #1}}
\newcommand{\vb}[1]{{\color{green}VB: #1}}
\newcommand{\filipe}[1]{{\color{cyan}FC: #1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}





\SetAlFnt{\footnotesize}

\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2021}



\icmltitlerunning{LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment}

\begin{document}

\twocolumn[
\icmltitle{LongReMix: Robust Learning with High Confidence Samples \\ in a Noisy Label Environment}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Filipe R. Cordeiro}{to}
\icmlauthor{Ragav Sachdeva}{goo}
\icmlauthor{Vasileios Belagiannis}{ed}
\icmlauthor{Ian Reid}{goo}
\icmlauthor{Gustavo Carneiro}{goo}
\end{icmlauthorlist}

\icmlaffiliation{to}{Universidade Federal Rural de Pernambuco, Recife, Brazil}
\icmlaffiliation{goo}{University of Adelaide, Adelaide, Australia}
\icmlaffiliation{ed}{Universit√§t Ulm, Ulm, Germany}

\icmlcorrespondingauthor{Filipe R. Cordeiro}{filipe.rolim@ufrpe.br}




\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Deep neural network models are robust to a limited amount of label noise, but their ability to memorise noisy labels in high noise rate problems is still an open issue.
   The most competitive noisy-label learning algorithms rely on a 2-stage process comprising an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning that minimises the empirical vicinal risk (EVR) using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy.
   In this paper, we hypothesise that the generalisation of such 2-stage  noisy-label learning methods depends on the precision of the unsupervised classifier and the size of the training set to minimise the EVR.
   We empirically validate these two hypotheses and propose the new 2-stage noisy-label training algorithm LongReMix.
We test LongReMix on the noisy-label benchmarks CIFAR-10, CIFAR-100, WebVision, Clothing1M, and Food101-N. The results show that our LongReMix generalises better than competing approaches, particularly in high label noise problems. Furthermore, our approach achieves state-of-the-art performance in most datasets. The code will be available upon paper acceptance. 
   
\end{abstract}

\section{Introduction}



Training Deep Neural Networks (DNNs) often requires large data sets to perform well on challenging problems such as image classification~\cite{litjens2017survey}. However, the larger the data set, the greater the likelihood for it to be contaminated with noisy labels due to reasons such as low-quality data, human failure, or challenging labelling tasks~\cite{frenay_survey}. The main issue is that DNNs can easily fit noisy labels, particularly for large rates of label noise, reducing their accuracy, as shown by Zhang et al.~\cite{zhang2016understanding}.


In the literature, several methods have been proposed to deal with noisy labels~\cite{kim2019nlnl, wang2019symmetric, ren2018learning,wang2019symmetric,nguyen2019self,li2020dividemix}, where the most successful methods explore a 2-stage process formed by an unsupervised learning method to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) to minimise the empirical vicinal risk (EVR) with a labelled set formed by the samples classified as clean, and an unlabelled set with the samples classified as noisy.
The unsupervised learning stage is generally based on the small-loss strategy~\cite{yu2019does}, where at every epoch, samples with small loss are classified as clean, and large loss as noisy. 
This strategy can lead to a low classification precision of clean samples, particularly in high noise rate scenarios, where the loss values can be unstable at different training epochs. The SSL stage~\cite{arazo2019unsupervised, nguyen2019self,li2020dividemix} is usually based on MixMatch~\cite{berthelot2019mixmatch} that minimises the empirical vicinal risk (EVR)~\cite{zhang2017mixup}, where a robust estimation of the vicinal distribution is critical for an effective optimisation that generalises well. 
Such robust estimation depends on a large training set to minimise the EVR~\cite{berthelot2019mixmatch,zhang2018generalization}, but problems with high noise rate usually cause the unsupervised learning stage to build a small training set to be used by this optimisation, affecting the generalisation of the SSL stage.


In this paper, we hypothesise that the generalisation of 2-stage noisy-label learning methods depends on the precision of the unsupervised learning stage to classify clean and noisy samples and a large training set to minimise the EVR at the SSL stage. We empirically validate these two hypotheses and propose a new 2-stage noisy-label training algorithm, called LongReMix. LongReMix is based on a theoretically sound unsupervised learning method to maximise the precision of the clean sample classification by considering the small-loss strategy over a range of epochs instead of a single one. Then, we artificially increase the training set size to improve the generalisation of MixMatch for the minimisation of the EVR during the SSL stage~\cite{berthelot2019mixmatch}.
We evaluate our approach on the noisy-label learning benchmarks of CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning}, WebVision~\cite{li2017WebVision}, Clothing1M~\cite{xiao2015learning}, and Food101-N~\cite{lee2018cleannet}, where LongReMix shows the best performance in the field in almost all of those data sets, particularly in problems with extremely large noise rates. We also show that LongReMix finds a set of clean samples with higher precision than the competing methods, and is robust to over-fitting in problems with high label noise.







\section{Prior Work}

Several methods have been proposed for the noisy-label problem, and they explore different strategies, such as robust loss functions~\cite{wang2019imae, wang2019symmetric}, label cleansing \cite{jaehwan2019photometric, yuan2018iterative}, sample weighting \cite{ren2018learning}, meta-learning \cite{han2018pumpout}, ensemble learning \cite{miao2015rboost}, and others \cite{yu2018learning, kim2019nlnl, zhang2019metacleaner}. Below, we focus on the prior work that is close to our approach and that show competitive results on the main benchmarks. It is important to mention that we do not consider methods that need a clean validation set, such as~\cite{zhang2020distilling}, because we believe this forms a less general experimental setup.





Several approaches explore the sample noise characterisation. \citet{xue2019robust} present a probabilistic Local Outlier Factor algorithm (pLOF) to estimate the probability that a sample is an outlier, which is assumed to have label noise. The idea explored by pLOF is that the density around a noisy sample is significantly different from the density around its (clean) neighbors. However, in high noise rate problems, the effectiveness of pLOF is reduced because it cannot find significant differences between the densities of noisy and clean samples. 
\citet{wang2018iterative} also use pLOF combined with a Siamese network to increase the dissimilarities between clean and noisy samples. Nevertheless, the incorrect classification of clean samples by pLOF can induce the learning of wrong feature representations. \citet{arazo2019unsupervised} propose the use of a Beta Mixture Model (BMM) to separate the clean and noise samples during training, based on the loss value of each sample. 
Similarly, \citet{li2020dividemix} use Gaussian Mixture Model (GMM) for the same goal.
Although the use of BMM and GMM applied on the loss values works well for low noise rate, for high noise regimes it becomes less precise. One of the issues affecting the precision of the classification of clean samples from the training set is that they usually rely on an estimation of clean and noisy sets using the loss from the latest training epoch and do not consider the stability of the classification of clean samples over several epochs.

Another technique being studied for noisy-label learning is the use of multiple models to improve the robustness of sample noise characterisation. \citet{han2018co} propose Co-teaching, which trains two models simultaneously, where each model estimates the clean sample set to be used by the other model. However, with an increase in the number of epochs, both networks converge to a consensus and show little difference between their estimated clean sets. Co-teaching+~\cite{yu2019does} relies on small loss samples that disagree on the predictions to select the data for the other model. Although this multiple model strategy shows better results for filtering clean samples, noisy samples are usually ignored during training, decreasing the effectiveness of the approach.

After distinguishing between clean and noisy samples, methods either disregard the noisy samples during training \cite{thulasidasan2019combating, han2018co}, or use both the clean and noisy samples in a semi-supervised learning (SSL) approach~\cite{li2020dividemix, arazo2019unsupervised, sachdeva2021evidentialmix}, where SSL-based methods tend to show better results on benchmarks.
One particularly successful technique that relies on SSL is DivideMix~\cite{li2020dividemix} that relies on MixMatch~\cite{berthelot2019mixmatch} to linearly combine training samples classified as clean or noisy for the EVR minimisation~\cite{zhang2017mixup}.
The generalisation of the EVR minimisation has been theoretically shown to depend on a large training set~\cite{zhang2018generalization}.
However, recent methods, such as DivideMix~\cite{li2020dividemix}, constrain this training set to be of the same size as the clean set, which tends to be small in large noise rate scenarios. Our approach removes this constraint, allowing a better generalisation of the EVR minimisation.










\section{Preliminaries}

\subsection{Problem Definition}
\label{sec:problem_definition}


Consider the training set , where  is the  image and  is a one-hot vector representing the noisy label, with  denoting the set of labels, and . 
The label  may differ from the unknown true label  as a result of a noise process represented by , with ,
where the  are the classes,  the probability of flipping the class  to , and . We assume that this noise process can be of three types, namely symmetric~\cite{kim2019nlnl}, asymmetric~\cite{patrini2017making}, and semantic~\cite{rog}.
The symmetric noise, also called uniform noise, refers to a noise type that the hidden label flips to a random class with a fixed probability , where the true label is included into the label flipping options, which means that in , and . 
The asymmetric noise is based on flipping labels between similar classes~\cite{patrini2017making}, where  depends only on the classes , but not on . For example, using CIFAR-10 data set \cite{krizhevsky2009learning}, the asymmetric noise maps \emph{truck}  \emph{automobile}, \emph{bird}  \emph{plane}, \emph{deer}  \emph{horse}, as mapped by \cite{zhang2018generalized}. 
The semantic noise~\cite{rog} depends on both the classes  and the image .

\vspace{-.1in}
\subsection{Background}
\label{sec:background}

We only consider 2-stage noisy-label learning approaches~\cite{li2020dividemix, ding2018semi, kong2019recycling} that hold state-of-the-art (SOTA) results on all benchmarks -- these approaches are based on: 1) an unsupervised learning classifier that characterises training samples as clean or noisy; and 2) a semi-supervised learning classifier that assumes that the training samples classified as clean are labelled, and the samples classified as noisy are unlabelled.
The SOTA noise-robust classifier~\cite{li2020dividemix,nguyen2019self} is formed by an ensemble of two classifiers, each represented by , where the classifier structure is the same, but their parameters are denoted by .  The training for  influences  and vice-versa, where this can be achieved by co-training~\cite{li2020dividemix} or student-teacher~\cite{nguyen2019self} approaches. Our training relies on co-training.

\textit{The unsupervised learning classifier} predicts the clean and noisy samples based on their loss values~\cite{arazo2019unsupervised, li2020dividemix,rog, jiang2020beyond}. 
Formally, assuming that the training is minimising the empirical risk , the set of clean and noisy samples are respectively defined by 
\vspace{-.05in}

where ,
 represents a classification loss (e.g., cross entropy),
and 
 is a function that computes the probability that the training sample  is clean based on its loss ~\cite{jiang2020beyond, li2020dividemix, zhang2020distilling,nguyen2019self} and parameterised by  (in this paper, this probability function computes the posterior of the smaller-mean component of a bi-modal GMM, where this smaller mean represents the clean GMM component~\cite{li2020dividemix}). To learn  and , co-training uses the clean and noisy sets from model  to train , and vice-versa.
 
\textit{The semi-supervised learning} based on MixMatch~\cite{berthelot2019mixmatch} mixes the elements of  and  to minimise the empirical vicinal risk (EVR)~\cite{zhang2017mixup}:

where  weights the noisy set loss,  and  denote the losses in the clean and noisy sets, respectively defined as

with

where  is a Dirac mass centered at , , and . In~\cite{li2020dividemix},  the noisy set size  and clean set size  are constrained to be equal to 
, which means that  .

\subsection{Our Hypothesis}
\label{sec:our_contribution}





We hypothesise that the generalisation of 2-stage noisy-label learning methods depend on: 1) the precision of the classification of clean samples to be included in  in~\eqref{eq:set_U_X}, and 2) the size of the clean set denoted by . 
In particular, a large  with a high proportion of positives will reduce the bound of the difference between the estimated and vicinal risks~\cite{zhang2018generalization}, improving the semi-supervised classification accuracy.  


Let us begin with our proposed method to increase the precision in the classification of clean samples in .  Our idea is to classify as clean, the samples that consistently show  for  epochs.  
Assuming that  denotes the probability of classifying a clean sample as clean,  the probability of classifying a clean sample as noisy (i.e., ). Similarly,  represents the probability of classifying a noisy sample as noisy,  the probability of classifying a noisy sample as clean (i.e., ). Also,  and  denote the proportion of clean and noisy samples in the training set, with .  The probability of a clean sample being in the clean set  after  epochs is , and 
the probability of a noisy sample being in the clean set after  epochs is .

\begin{lemma}
Assuming that  (so ) and  (so ), the classification precision of clean samples in  tends to 1 and recall tends to 0, as  increases.
\end{lemma}

\begin{proof}
The precision and recall are calculated with:

Given the assumption that  and  and that ,   tends to 1, and similarly, given that ,   tends to 0.
\end{proof}

In low noise rate problems,  tends to be large and , small, so even for small values of , Precision will be close to one with a relatively high Recall in~\eqref{eq:precision_recall}, allowing for a large .
According to Theorem 8 in~\cite{zhang2018generalization}, a large  will decrease the bound for vicinal risk minimisation.
On the other hand,  tends to be small and  large, in high noise rate scenarios, which means that  needs to increase to push the Precision to be close to one, but that can reduce the Recall to very low values, resulting in a potentially small , which will increase the vicinal risk minimisation bound~\cite{zhang2018generalization}.  
Therefore,  is a hyper-parameter that needs to be estimated to achieve a good trade-off between Precision and Recall. 
Nevertheless, for high noise rate scenarios, even with a careful estimation of ,  can still be small.
Hence, we propose that  must be sampled with replacement when mixing up  and  in~\eqref{eq:X_prime_U_prime}, such that . 




\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{figures/longremix_v2.pdf}
\caption{Our \textbf{proposed LongReMix method} is composed of two stages: the High Confidence Training and the Guided Training. The High Confidence Training is responsible for finding the high confidence samples. The Guided Training uses the high confidence samples to update the predicted clean set and train the model.}
\label{fig:longremix}
\end{figure*}



\section{LongReMix}
\label{sec:longremix}

Our proposed LongReMix algorithm is divided into two stages (Figure \ref{fig:longremix}). 
The first stage, comprising the High Confidence Training (HCT), trains the model to find a high confidence set of clean samples with high precision. 
Next, in the second stage, we build a core set of clean samples using the largest high confidence set obtained from the first stage.  With this core set, we retrain the model. Moreover, we propose a new way to build the data sets  and  in~\eqref{eq:X_prime_U_prime}, called LongMix,
which enables the number of MixUp operations to be proportional to  instead of  , as described in Sec.~\ref{sec:our_contribution}.

\subsection{First Stage: High Confidence Training}
\label{sec:HCT}

The \textbf{high confidence training (HCT)} aims to increase the precision of the unsupervised classification of clean and noisy training samples.  Following the idea presented in Sec.~\ref{sec:our_contribution}, 
we re-define how to form the sets of clean and noisy samples, originally defined in~\eqref{eq:set_U_X}, as follows:

where  represents the loss of sample  at training epoch  and  denotes the confidence window comprising the current and the previous  epochs -- this is represented by the block "filter" that produces the high confidence samples in Fig.~\ref{fig:longremix}.  Hence, a sample to be in the clean set  must be classified as clean for  epochs in a row, resulting in a more consistent, but smaller, set of clean samples, containing fewer noisy samples than the set in~\eqref{eq:set_U_X}.






\subsection{Second Stage: Guided Training}
\label{sec:guided_training}

The second stage of the training depends on the core set of clean samples estimated from the first training stage with

where  is the total number of training epochs for the first stage of training. In the second stage of training, we define the labelled and unlabelled sets as in~\eqref{eq:set_U_X}, but we use  to update these sets as follows:



During the second stage of LongReMix, we retrain the model from scratch\footnote{We compared 
if we should fine-tune the model trained from the first stage or train from scratch, and the latter approach showed the best results.} using the core set of clean samples  from~\eqref{eq:H} included in the predicted clean and with the original labels from . 









As explained in Sec.~\ref{sec:our_contribution}, we hypothesise that by sampling the clean set with replacement, we increase the number of MixUp operations in the EVR loss in~\eqref{eq:EVR}, resulting in a smaller bound of the difference be-tween estimated and vicinal risks~\cite{zhang2018generalization}.  
Therefore, we propose \textbf{LongMix} that increases the number of MixUp operations to be , instead of the number of predicted clean samples. 
A criticism faced by LongMix is that adding more MixUp iterations per epoch may be equivalent to a simple increase in the number of epochs, but we show in the experiments that this is not true.




\subsection{Training Loss}

The training loss for our proposed LongReMix is~\cite{li2020dividemix}:

where  denotes the empirical vicinal error defined in~\eqref{eq:EVR}, with
, ,  weights the regularisation loss, and

with  denoting a vector of  dimensions with values equal to , and 
 representing the Kullback Leibler divergence between  and .
The pseudo-code for the training of LongReMix is shown in Algorithm 1 in the supplementary material.



























\section{Experiments}\label{sec:experiments}


We compare LongReMix with related approaches on five noisy-label learning benchmarks. We also analyze the performance of LongReMix on a number of ablation studies. All comparisons are performed with the same network architecture and trained for the same number of epochs as the compared methods.

\subsection{Data Sets}
\label{sec:datasets}

We conduct our experiments on the data sets  CIFAR-10, CIFAR-100~\cite{krizhevsky2009learning}, Clothing1M~\cite{xiao2015learning}, WebVision~\cite{li2017WebVision} and Food101-N~\cite{lee2018cleannet}. CIFAR-10 and CIFAR-100 have 50000 training and 10000 testing images of size  pixels, where CIFAR-10 has 10 classes and CIFAR-100 has 100 classes and all training and testing sets have a perfectly balanced number of images per classes. As CIFAR-10 and CIFAR-100 data sets originally do not contain label noise, a common approach is to add synthetic noise to evaluate the models. For CIFAR-10/CIFAR-100 we investigated three  noise types: symmetric, asymmetric and semantic, as defined in Sec.~\ref{sec:problem_definition}. 
The symmetric noise is generated using , with   defined in Sec.~\ref{sec:problem_definition}.
The asymmetric noise is produced following the mapping used in~\cite{li2020dividemix, patrini2017making}, with  (note that we study  because it is close to the theoretical limit of 50\% for this type of noise).  We also evaluate the semantic noise scenario, where 
we follow the setup from~\cite{rog} to generate semantically noisy labels based on a trained VGG~\cite{vgg}, DenseNet~(DN), and ResNet~(RN) on CIFAR-10 and CIFAR-100.

Clothing1M consists of 1 million training images acquired from online shopping websites and it is composed of 14 classes.As the images from the data set vary in size, we resized the images to  for training, as used in \cite{li2020dividemix, han2019deep}.
The data set is heavily imbalanced and most of the noise is asymmetric~\cite{yi2019probabilistic}, with noise rate estimated to be around 40\%~\cite{xiao2015learning}. The data set provide additional clean sets for 
training, validation, and test of 50k, 14k and 10k images, respectively. For our experiments we do not use any of the clean training or validation sets, but we use the test set for evaluation.

WebVision contains 2.4 million images collected from the internet, with the same 1000 classes from ILSVRC12~\cite{deng2009imagenet} and images resized  to  pixels. It provides a clean test set of 50k images, with 50 images per class. We compare our model using the first 50 classes of the Google image subset, as used in \cite{li2020dividemix, chen2019understanding}.


Food101-N~\cite{lee2018cleannet} contains 310,009 training images of food recipes classified in 101 classes and 25,000 images for the testing set. The images from this data set were resized to . 
This data set is based on the Food101 data set~\cite{bossard2014food}, but it has more images with noisy labels. 
The test set is the same provided by the original Food101~\cite{bossard2014food}, which is a clean test set of 25K images.






\begin{figure}[t]
\centering
\subfloat[Prec. vs. Recall for  asy. noise]{\includegraphics[width=0.6\columnwidth]{figures/precision_recall.pdf} }\quad\quad
\subfloat[Precision vs. Noise ]{\includegraphics[width=0.42\columnwidth]{figures/barplot_precision.pdf} }\subfloat[Recall vs. Noise]{\includegraphics[width=0.44\columnwidth]{figures/barplot_recall.pdf} }\\
\caption{(a) Precision versus Recall for our proposed  from~\eqref{eq:redefine_clean_noisy_sets} (denoted by HCT) and  from~\eqref{eq:set_U_X} (Baseline), for 40\% asymmetric noise on CIFAR-10, where  (denoted by \emph{th}) for . (b) Precision  and (c) Recall for different noise rates, for CIFAR-10, using .}
\label{fig:precison}
\vspace{-.15in}
\end{figure}






\begin{table*}[ht]
\centering
\footnotesize
\scalebox{0.88}{
\begin{tabular}{cc|cccc|cc||cccc}
\toprule
\multicolumn{2}{c}{Data set} & \multicolumn{5}{c}{CIFAR-10} & \multicolumn{4}{c}{CIFAR-100}\\    
\midrule
\multicolumn{2}{c}{Noise type} & \multicolumn{4}{c}{sym.} & \multicolumn{2}{c}{asym.} &  \multicolumn{4}{c}{sym.} \\
\midrule

Method/ noise ratio & &  20\% & 50\% & 80\% & 90\% & 40\%& 49\% & 20\% & 50\% & 80\% & 90\% \\
\midrule

\multirow{2}{*}{Baseline~\cite{li2020dividemix} }& Best & \textbf{96.22} & 94.93 & 93.33 & 76.49& 93.24 & 82.90 & 78.03 & 74.87 & \textbf{62.74} & 29.79 \\
  & Last & \textbf{96.01} & 94.68 & 92.99 & 75.45& 91.79 & 75.57 & 77.43 & 74.23 & \textbf{62.01} & 29.37 \\
\midrule
\multirow{2}{*}{LongMix [ours] }& Best & 96.18 & \textbf{95.19} & \textbf{94.09} & \textbf{85.33} & \textbf{93.38} & \textbf{83.23} & \textbf{78.03} & \textbf{75.84} & 62,24 & \textbf{33.54}\\
    & Last & 95.98 & \textbf{94.79} & \textbf{93.73} & \textbf{84.71} &  \textbf{91.87} & \textbf{77.18} & \textbf{77.56} & \textbf{74.87} & 61.60 & \textbf{33.00}\\
\bottomrule \\
\end{tabular}
}
\caption{Comparison of the test accuracy between LongMix (where sizes of  and  in~\eqref{eq:X_prime_U_prime} are ) and the baseline in~\cite{li2020dividemix}  (with sizes of  and  being ), using the same number of iterations on CIFAR-10 and CIFAR-100 under symmetric (ranging from 20\% to 90\% and asymmetric (ranging from 40\% and 49\%) noise.}
\label{tab:longmix_sameiter}
\end{table*}



\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/acc_iters_v2.pdf}
\caption{Test accuracy versus number of training steps (or iterations) for CIFAR-10 at 90\% symmetric noise for our proposed LongMix (where sizes of  and  in~\eqref{eq:X_prime_U_prime} are ) and the baseline (with sizes of  and  being ~\cite{li2020dividemix}).
}

\label{fig:acc_iters}
\vspace{-.15in}
\end{figure}


\begin{table*}[ht]
\centering
\footnotesize
\scalebox{0.92}{
\begin{tabular}{cc|cccc|cc||cccc}
\toprule
\multicolumn{2}{c}{Data set} & \multicolumn{5}{c}{CIFAR-10} & \multicolumn{4}{c}{CIFAR-100}\\    
\midrule
\multicolumn{2}{c}{Noise type} & \multicolumn{4}{c}{sym.} & \multicolumn{2}{c}{asym.} &  \multicolumn{4}{c}{sym.} \\
\midrule

Method/ noise ratio & &  20\% & 50\% & 80\% & 90\% & 40\%& 49\% & 20\% & 50\% & 80\% & 90\% \\
\midrule
\multirow{2}{*}{Cross-Entropy \cite{li2020dividemix}}& Best & 86.8 & 79.4 & 62.9 & 42.7 & 85.0 & - & 62.0 & 46.7 & 19.9 & 10.1\\
    & Last & 82.7 & 57.9 & 26.1 & 16.8 & 72.3 & - & 61.8 & 37.3 & 8.8 & 3.5\\
\midrule
\multirow{2}{*}{Coteaching+ \cite{yu2019does}} & Best & 89.5 & 85.7 & 67.4& 47.9& - & -&65.6& 51.8 & 27.9 & 13.7\\
    & Last & 88.2 & 84.1 & 45.5& 30.1& - & -&64.1& 45.3 & 15.5 & 8.8\\
\midrule
\multirow{2}{*}{MixUp \cite{zhang2017mixup}} &Best & 95.6 & 87.1 & 71.6& 52.2& - & - & 67.8& 57.3 & 30.8 & 14.6\\
 &Last & 92.3 & 77.3 & 46.7& 43.9& - & - & 66.0& 46.6 & 17.6 & 8.1\\
\midrule
\multirow{2}{*}{Meta-Learning \cite{li2019learning}}&Best & 92.9 & 89.3 & 77.4& 58.7& 89.2 & - & 68.5& 59.2 & 42.4 & 19.5 \\
   &Last & 92.0 & 88.8 & 76.1& 58.3& 88.6 & - & 67.7& 58.0 & 40.1 & 14.3 \\
\midrule
\multirow{2}{*}{M-correction \cite{arazo2019unsupervised}}&Best& 94.0 & 92.0 & 86.8& 69.1& 87.4 & - & 73.9& 66.1 & 48.2 & 24.3 \\
   &Last& 93.8 & 91.9 & 86.6& 68.7& 86.3 & - & 73.4& 65.4 & 47.6 & 20.5 \\
\midrule
\multirow{2}{*}{DivideMix \cite{li2020dividemix}}& Best &96.1 & 94.6 & 93.2 & 76.0& 93.4 & 83.7 & 77.3 & 74.6 & 60.2 & 31.5 \\
  & Last & 95.7 & 94.4 & 92.9 & 75.4& 92.1 & \textbf{76.3} & 76.9 & 74.2 & 59.6 & 31.0 \\
\midrule
\multirow{2}{*}{LongReMix [ours] }& Best & \textbf{96.2} & \textbf{95.0} & \textbf{93.9} & \textbf{82.0} & \textbf{94.7} & \textbf{84.7} & \textbf{77.8} & \textbf{75.6} & \textbf{62.9} & \textbf{33.8}\\
    & Last & \textbf{96.0} & \textbf{94.7} & \textbf{93.4} & \textbf{81.3} & \textbf{94.3} & 76.1 & \textbf{77.5} & \textbf{75.1} & \textbf{62.3} & \textbf{33.2}\\
\bottomrule \\
\end{tabular}
}
\caption{Results using PRN18 on CIFAR-10 and CIFAR-100 under symmetric (ranging from 20\% to 90\% and asymmetric (ranging from 40\% and 49\%) noises. Results from related approaches are as presented in~\cite{li2020dividemix}.} 
\label{tab:results_cifar}
\end{table*}



\subsection{Implementation}

The model  is represented by a 18-layer PreAct ResNet (PRN18)~\cite{he2016identity} 
 for CIFAR-10 and CIFAR-100, InceptionV2~\cite{szegedy2017inception} for WebVision (this is the model used by competing approaches), and ResNet-50~\cite{he2016deep} for Clothing1M and Food-101N.
The models are trained with stochastic gradient descent with momentum of 0.8, weight decay of 0.0005 and batch size of 64. The learning rate is 0.02 which is reduced to 0.002 in the middle of the training. The WarmUp and total number of epochs is defined according to each data set, as defined in~\cite{li2020dividemix}.
For CIFAR-10 and CIFAR-100, PRN18 is based on a WarmUp stage of 30 epochs, with 300 epochs of total training.
For WebVision, the InceptionV2 is trained for 100 epochs, with a WarmUp stage of 1 epoch.
For Clothing1M, ResNet-50 is trained for 80 epochs with WarmUp stage of 1 epoch.
For Food-101N, we also use ResNet-50 and rely on the same training protocol as in~\cite{han2019deep}, consisting of training for 30 epochs, WarmUp stage of 1 epoch and reducing the learning rate by a factor of 10 every 10 epochs.
The MixMatch parameter is  in~\eqref{eq:v_x_y}, and 
the regularisation weight for the loss in~\eqref{eq:D_loss_full} is  for symmetric noise and  for asymmetric noise--these two parameters are as defined in~\cite{li2020dividemix}. 
We used a confidence window of  in~\eqref{eq:redefine_clean_noisy_sets}, which was defined empirically for all the experiments. In Table 1 of supplementary material we show that, in general, Precision increases and Recall decreases with larger  values.  Also, classification accuracy reaches a peak for large noise rates (symmetric at  and asymmetric ) at , and for lower noise rates, accuracy does not change much with different values of .




\subsection{Precision and Recall of the Clean Set}



We evaluate the precision and recall of the clean set  from~\eqref{eq:redefine_clean_noisy_sets} in the last epoch of the first stage of training (HCT), compared to the clean set  from~\eqref{eq:set_U_X} that relies on the small loss result from the last epoch (Baseline). 
We assess that by computing  and  of the sets  from~\eqref{eq:redefine_clean_noisy_sets} and  from~\eqref{eq:set_U_X}, where  refers to the samples correctly predicted as clean,   denotes the noisy samples incorrectly predicted as clean, and  denotes the clean samples incorrectly predicted as noisy.
Figure~\ref{fig:precison}-(a) shows the Precision vs Recall of predicted clean set for CIFAR-10 with 40\% asymmetric noise, where results are obtained by varying the threshold  applied to   to form  and . 
We highlight the value of , which is the default value~\cite{li2020dividemix} that we use to split the clean and noisy samples. Notice that in this highly asymmetric noise scenario, the curve from HCT shows a better trade-off than the Baseline.
Figure~\ref{fig:precison}-(b,c) shows that  from HCT trades off a higher precision for a lower recall, compared with  from the Baseline for several types of noise,  As shown below, this has a large influence on the training efficacy of LongReMix. 


\subsection{LongMix Analysis}

Figure~\ref{fig:acc_iters} shows the test accuracy versus training steps (iterations) for LongMix compared to the baseline~\cite{li2020dividemix}, for CIFAR-10 at 90\% symmetric noise -- this figure shows that adding more MixUp iterations per epoch, as in LongMix, is not equivalent to adding more epochs, as in baseline~\cite{li2020dividemix}.
This shows evidence for the claim in Sec.~\ref{sec:guided_training} that a simple increase in the number of epochs is not equivalent to adding more MixUp iterations, as we propose for LongMix.
Table~\ref{tab:longmix_sameiter} shows further evidence for this claim by comparing LongMix and baseline~\cite{li2020dividemix} using the same number of training iterations for different noisy rates on CIFAR-10 and CIFAR-100 -- results show that LongMix is more accurate for most cases. 



\subsection{Comparison with the State-of-the-Art}

\begin{table*}[t!]
\centering
\footnotesize
\scalebox{0.95}{
\begin{tabular}{@{}p{1.2cm}p{0cm}|p{0.5cm}p{0.5cm}p{0.5cm}p{0.6cm}|p{0.5cm}p{0.6cm}||p{0.5cm}p{0.5cm}p{0.5cm}p{0.6cm}||p{0.6cm}p{0.7cm}p{0.7cm} || c@{}}
\toprule
\multicolumn{2}{c}{Data set} & \multicolumn{6}{c}{CIFAR-10} & \multicolumn{4}{c}{CIFAR-100}& Webv.& Cloth. & Food. & Mean Rank\\    
\midrule
\multicolumn{2}{c}{Noise type} & \multicolumn{4}{c}{sym.} & \multicolumn{2}{c}{asym.} &  \multicolumn{4}{c}{sym.} & - & - & - & -  \\
\midrule

\multicolumn{2}{@{}p{2.17cm}}{Method/ n. ratio} &  20\% & 50\% & 80\% & 90\% & 40\%& 49\% & 20\% & 50\% & 80\% & 90\%& -& -& - & -  \\
\midrule
\multirow{2}{*}{LongReMix }& Best & \textit{\textbf{96.25}} & \textbf{95.01} & \textbf{93.88} & \textbf{81.98} & \textit{\textbf{94.64}} & \textbf{84.68} & \textbf{77.82} & \textbf{75.59} & \textit{\textbf{62.92}} & \textit{\textbf{33.80}}& \textit{\textbf{78.92}}&  \textbf{74.38} & \textit{\textbf{87.39}} & 1.46\\
    & Last & \textit{\textbf{96.02}} & \textbf{94.72} & \textbf{93.37} & \textbf{81.35} & \textit{\textbf{94.32}} & 76.08 & \textbf{77.52} & \textbf{75.11} & \textit{\textbf{62.34}} & \textit{\textbf{33.25}} & \textit{\textbf{78.00}} & 73.00 & \textit{\textbf{87.29}} & 1.69\\
\midrule
\multirow{2}{*}{LongMix} & Best & 96.18 & \textit{\textbf{95.19}} & \textit{\textbf{94.09}} & \textit{\textbf{85.33}}& 93.38 & 83.23 & \textit{\textbf{78.03}} & \textit{\textbf{75.84}} & \textbf{62.24} & \textbf{33.54}& \textbf{78.44} & 74.05 & \textbf{87.21} & 1.92\\
    & Last & \textbf{95.98} & \textit{\textbf{94.79}} & \textit{\textbf{93.73}} & \textit{\textbf{84.71}}& 91.87 & \textbf{77.18} & \textit{\textbf{77.56}}& \textit{\textbf{74.87}} & \textbf{61.60} & \textbf{33.00} & 77.72& \textit{\textbf{73.25}} & \textbf{87.12} & 1.69\\
\midrule
\multirow{2}{*}{Retrain} & Best & \textbf{96.23} & 94.85 & 92.86& 78.47& \textbf{94.59} & \textit{\textbf{85.10}} &77.20& 74.41 & 60.29 & 30.61 & 77.84 & \textbf{74.30} & 87.16 & 2.61\\
    & Last & 95.89 & 94.60 & 92.54& 77.51& \textbf{94.31} & \textit{\textbf{80.88}} &76.89& 73.89 & 59.88 & 30.37 & \textbf{77.84} & \textbf{73.21} & 86.98 & 2.61\\

\bottomrule  \\
\end{tabular}
}
\caption{Ablation Study Results. The italic bold, bold, and regular numbers represent respectively the ranking of first, second and third results in accuracy.  Last column shows the average rank of each approach (smaller is better).}
\label{tab:ablation}
\end{table*}




\begin{table}[ht]
\centering
\footnotesize
\scalebox{0.95}{
\begin{tabular}{@{}p{2.7cm}@{}p{0.5cm}p{0.5cm}p{0.6cm}p{0.5cm}p{0.5cm}p{0.5cm}}
\toprule
Data set & \multicolumn{3}{c}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-100}\\    
\midrule

Method/ noise ratio & DN (32\%) & RN (38\%) & VGG (34\%) & DN (34\%) & RN (37\%) & VGG (37\%) \\
\midrule
CE + RoG &  68.33 & 64.15 & 70.04 & 61.14 & 53.09 & 53.64\\
Bootstrap + RoG &  68.38 & 64.03 & 70.11 & 54.71 & 53.30 & 53.76\\
Forward + RoG &  68.20 & 64.24 & 70.09 & 53.91 & 53.36 & 53.63\\
Backward + RoG &  68.66 & 63.45 & 70.18 & 54.01 & 53.03 & 53.50\\
D2L + RoG &  68.57 & 60.25 & 59.94 & 31.67 & 39.92 & 45.42\\
DivideMix*  &  84.57 & 81.61 & 85.71 & 68.40 & 66.28 & 66.84\\
LongReMix [ours] &  \textbf{85.13} & \textbf{82.51} & \textbf{85.90} & \textbf{69.03} & \textbf{66.70} & \textbf{67.42}\\
\bottomrule \\
\end{tabular}
}
\caption{Results for Semantic Noise. Results from baseline methods are as presented in~\cite{rog}. Methods marked by * denote re-implementations based on public code.} 
\label{tab:res_semantic}
\end{table}

\begin{table}[!ht]
\footnotesize
\centering
\scalebox{0.95}{
\begin{tabular}{lcc}
\toprule
Method  & Top 1 & Top 5  \\
\midrule
Decoupling~\cite{malach2017decoupling}     & 62.54 & 84.74   \\
 D2L~\cite{ma2018dimensionality}            & 62.68 & 84.00   \\
 MentorNet~\cite{jiang2018mentornet}      & 63.00 & 81.40   \\
 Co-teaching~\cite{han2018co}    & 63.58 & 85.20  \\
 Iterative-CV~\cite{chen2019understanding}   & 65.24 & 85.34  \\
 DivideMix~\cite{li2020dividemix}      & 77.32 & 91.64  \\
 LongReMix~[ours]     & \textbf{78.92}  & \textbf{92.32} \\
 \bottomrule \\
 
\end{tabular}
}
\caption{Results for WebVision~\cite{li2017WebVision}. Results from baseline methods are as presented in \cite{li2020dividemix}.}
\label{tab:res_WebVision}
\end{table}



\begin{table}[t]
\footnotesize
\centering
\scalebox{0.95}{
\begin{tabular}{lcc}
\toprule
Method & Test Accuracy \\
\midrule
 Cross-Entropy~\cite{li2020dividemix}  & 69.21  \\
M-correction \cite{arazo2019unsupervised}   & 71.00 \\
PENCIL\cite{yi2019probabilistic} &   73.49 \\
DeepSelf~\cite{han2019deep} & 74.45 \\
CleanNet~\cite{lee2018cleannet}    & 74.69 \\
DivideMix~\cite{li2020dividemix}      & \textbf{74.76} \\
LongReMix~~[ours]      &  74.38 \\


 \bottomrule
 \\
\end{tabular}
}
\caption{Results for Clothing1M~\cite{xiao2015learning}. Results from baseline methods are as presented in \cite{li2020dividemix}. The marker  denotes the model is trained from scratch.}
\label{tab:sota_cl}
\end{table}


\begin{table}[t]
\footnotesize
\centering
\scalebox{0.95}{
\begin{tabular}{lcc}
\toprule
Method  & from pre-trained & from scratch \\
\midrule
Cross-Entropy   & 81.44& -  \\
CleanNet   & 83.95 & - \\
DeepSelf  & 85.10 & - \\
DivideMix*  & 86.91 &  75.53 \\
 LongReMix [ours]  & \textbf{87.39} &  \textbf{78.57} \\
\bottomrule
 \\
\end{tabular}
}
\caption{Results for Food-101N~\cite{lee2018cleannet}. Methods marked by * denote re-implementations based on public code.}
\label{tab:res_food}
\end{table}



For CIFAR-10 and CIFAR-100, we evaluate our model using different levels of symmetric label noise ranging from 20\% to 90\%. We also consider asymmetric noisy, with noise rates of 40\% and 49\%. 
We report both the best test accuracy across all epochs and the averaged test accuracy over the last 10 epochs of training, similar to~\cite{li2020dividemix}. Table~\ref{tab:results_cifar} shows that for CIFAR-10 and CIFAR-100 data sets, our method obtains better results for all evaluated noisy rates. LongReMix displays a higher improvement for large symmetric noise and asymmetric noise scenarios, which can be considered as the most challenging cases. We believe that the improvement over higher noise rates is due to the LongMix approach, which runs a large number of MixUp operations proportional to the size of the training set. The retraining with high confidence samples also improves the results for asymmetric noise. The results for semantic noise~\cite{rog} in
Table~\ref{tab:res_semantic} shows again the superiority of our approach compared to the related work.

Also, we evaluate our method on large-scale data sets. For WebVision, 
Table~\ref{tab:res_WebVision} shows the Top-1 and Top-5 accuracy, where LongReMix displays better results than competing methods. 
For the Clothing1M evaluation, 
the competing methods rely on a pre-trained ImageNet model for training on Clothing1M. In our experiments, we did not observe any improvement with pre-trained models, and therefore we trained from scratch with 128k images from Clothing1M. The results in Table~\ref{tab:sota_cl} show that our model, trained from scratch and with a reduced training set, obtained comparable results to the competing approaches. Lastly, Table~\ref{tab:res_food} summarizes the results for Food-101N. 
For this problem, we evaluate our approach with a pre-trained model and trained from scratch, and LongReMix outperforms all other approaches in both scenarios.





\subsection{Ablation Study}

We analyze the effect of the different components of our proposal in an ablation study, shown in Table~\ref{tab:ablation}. 
Below, "Retrain" denotes the high-confidence training explained in Sec.~\ref{sec:HCT} which increases the accuracy of the classifier that distinguishes between clean and noisy samples; and "LongMix" represents the guided training from Sec.~\ref{sec:guided_training} that increases the number of MixUp operations.
We first evaluate our approach without LongMix -- this approach is referred to as "Retrain". 
Then we evaluate training only with the LongMix, without the second stage of re-training, and the whole model is denoted as LongReMix. 
In general, we can observe that the LongReMix is competitive for all noise scenarios (being best or second best for all cases), but it is generally better for the large-scale data sets.
Considering
different data sets and noise rates, LongReMix shows the best average rank.




\section{Conclusion}

We presented LongReMix, 
a new 2-stage noisy-label learning algorithm based on an unsupervised learning stage to classify clean and noisy training samples, followed by an SSL stage to minimise the EVR using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy.
Our LongReMix improves the precision of the unsupervised learning stage and improves the generalisation of the EVR minimisation.
We show that LongReMix reaches state-of-the-art performance on several benchmarks, and is robust to over-fitting in high label noise problems.








\bibliography{egbib}
\bibliographystyle{icml2021}



\end{document}
