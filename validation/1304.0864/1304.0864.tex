In order to evaluate the viability of our solution, we compared experimentally our library (referred to as {\libpoly}) with mature implementations.

In addition to the efficiency of the polyhedra computation, we wished to measure the cost of the inclusion checker.
Our approach guarantees that,
if our certificate checker terminates successfully on a given verification,
the result of the operation which produced the certificate is correct.
However, this assertion currently only applies to the polyhedra as known to the \coq\ checker:
a translation occurs between the \ocaml\ representation of numbers, \zarith ,
and their representation in the \coq\ language as lists of bits.
This means that the checker has to compute on this inefficient representation, and thus we wished to ascertain whether the cost was tolerable.\footnote{An alternative would be to map, at checker extraction time, \coq\ numbers to \zarith\ numbers,
at the expense of having both \zarith\ and \gmp\ in the trusted computing base.
One may consider that we already make assumptions about {\zarith} and {\gmp}: we assume they respect memory safety, and thus will not corrupt the data of the {\ocaml} code extracted from {\coq}, or at least that, if they corrupt memory, they will cause a crash in the analyzer (probably in the garbage collector) instead of a silent execution with incorrect data.
This seems a much less bold assumption than considering that they always compute correctly, including in all corner cases.}

The best approach to evaluating \libpoly\ would have been to rely on it for
building a complete static analyzer.
Although this is our long-term goal, a less demanding method was needed for a
more immediate evaluation.
We chose to compare computation results from \libpoly\ to those of widely
used existing implementations of the abstract domain of polyhedra:
the \newpolka\ library and the PPL.
More precisely, we used them through their \apron\ front end~\cite{jeannet09}.
\subsection{The Method}
\label{expmethod}
As~\cite{Monniaux_CAV09} points out, randomly-generated polyhedra do not give a faithful evaluation:
a more realistic approach was needed.
Because of the lack of a static analyzer supporting both \apron\ and \libpoly ,
we carried out the comparison by logging and then
replaying with \libpoly\ the abstract domain operations done
by the existing \textsc{Pagai} analyzer~\cite{henry012} using \apron .

Technically, logging consists in intercepting calls to the \apron\ shared
library (using the wrap functionality of the GNU linker~\texttt{ld}),
analyzing the data structures passed as operands and
generating equivalent \ocaml\ code for \libpoly .
\newpolka\ and PPL results are logged too, for comparison purposes.
At the end of the analysis, the generated \ocaml\ code forms a complete program
which replays all the abstract domain operations executed by the \newpolka\ library or the PPL
on request of the analyzer.

The comparison was done for the following operations: parallel assignment,
convex hull, inclusion test and intersection on the analysis of the following programs:
\begin{enumerate}
\item \texttt{bf}: the Blowfish cryptographic cipher
\item \texttt{bz2}: the bzip2 compression algorithm
\item \texttt{dbz2}: the bzip2 decompress algorithm
\item \texttt{jpg}: an implementation of the jpeg codec
\item \texttt{re}: the regular expression engine of GNU~\texttt{awk}
\item \texttt{foo}: a hand-crafted program leading to polyhedra with many constraints,
	large coefficients and few equalities
\end{enumerate}
\subsection{Precision and Representation Size Comparison}
The result of each operator we evaluated is a well-defined geometrical object.
For every logged call,
the results from \newpolka, PPL and \libpoly\ were checked for equality (double inclusion).
The certificates generated by \libpoly\ were then systematically checked.
Furthermore, polyhedra have a minimal constraints re\-presentation,
up to the variable choices in the substitutions of equalities.
It was systematically checked
whether \libpoly, \newpolka\ and the PPL computed the same number of equalities and inequalities.
In all the cases we tried, the tests of correctness and precision passed.
It is to be noted that the PPL does not systematically minimize representations:
its results often have redundant constraints.\footnote{This is due to the lazy-by-default implementation of the operators of the PPL.
Since support for the eager version of the operators has been deprecated in and is being removed from the PPL
(see~\cite{ppldoc}, \S\ A Note on the Implementation of the Operators),
we could not configure the library to have the same behavior as \newpolka .}

Besides giving confidence in the results computed by \libpoly ,
ensuring that our results are identical to those of \newpolka\ or the PPL
lead us to believe that the analyzer behavior would not have been very different,
had it used the results from \libpoly .
There is no noticeable difference between the analyses carried out using \newpolka\ and the PPL.
\subsection{Timing Measurements}
Timing measurements were made difficult because of
the importance of the state of polyhedra in the double representation \newpolka\ and the PPL use.
We were concerned that logging and replaying as described above would be unfair towards these libraries, since it would force the systematic recomputation of generator representations that, in a real analyzer, would be kept internally. We thus opted for a different approach.

We measured the timings for \newpolka\ and the PPL directly inside \textsc{Pagai} by wrapping the function calls between calls to a high precision timer.
We made sure that the overhead of the timer system calls was sufficiently small so as to produce meaningful results.
For \libpoly, timing measurements were done during the replay and
exclude the time needed to parse and rebuild the operand polyhedra.

We present two views of the same timing measurements,
carried out on the programs introduced in~\pararef{expmethod}.
Table~\ref{restabprg} gives, for each benchmark program, the total time spent in each operation of the abstract domain.
Such a table does not inform us of the typical distribution of problem sizes and the relationship between problem size and computation time, thus we compiled~Table~\ref{restabsz} which
shows computation times aggregated according to the ``problem size'', defined as the sum of the number of constraints of all the operands of a given operation.

For the assignment and the convex hull,
all the constraints of the two operands are put together after renaming
and many projections follow.
The inclusion test~$\pol_1\inclb\pol_2$, in the worst case,
solves as many linear programming problems as there are constraints in~$\pol_2$,
but each is of size the number of constraints of~$\pol_1+1$.
Last, the intersection operator minimizes the result of the union of the sets of constraints.
Note that the sums in Table~\ref{restabprg} exclude operations on trivial problems of size zero or one.

\begin{table}[t]\setlength{\belowcaptionskip}{0pt}
\caption{Timing comparison between {\newpolka} (N), PPL (P), {\libpoly} (L) and {\libpoly} with certificate checker (C): total time (in milliseconds) spent in each of the operations; trivial problems are excluded.}
\label{restabprg}

\begin{center}\narrow
\begin{tabular}{|l|S[table-format=4.0]S[table-format=5.1]S[table-format=2.1]|S[table-format=4.0]S[table-format=4.1]S[table-format=3.1]S[table-format=3.1]|S[table-format=2.1]S[table-format=2.1]S[table-format=1.1]S[table-format=1.1]|S[table-format=4.0]S[table-format=5.1]S[table-format=2.1]|}
\hline
prog. & \multicolumn{3}{c|}{assignment} & \multicolumn{4}{c|}{convex hull} &
	\multicolumn{4}{c|}{inclusion} & \multicolumn{3}{c|}{intersection}\\
 & N & P & L & N & P & L & C & N & P & L & C & N & P & L\\
\hline
\texttt{bf} & 3.7 & 11.4 & 0.5 & 3.2 & 1.2 & 2.7 & 2.8 & 0.2 & 0.4 & 0.1 & 0.1 & 10.7 & 13.4 & 1.2\\
\texttt{bz2} & 14.6 & 54.1 & 2.9 & 23.5 & 11.5 & 66.8 & 68.7 & 1.6 & 2.8 & 0.7 & 1.2 & 52.3 & 61.1 & 7.9\\
\texttt{dbz2} & 1618 & 4182 & 83.8 & 1393 & 231.9 & 532.8 & 535.3 & 32.3 & 35.6 & 2.1 & 3.6 & 1687 & 1815 & 28.3\\
\texttt{jpg} & 23.7 & 68.3 & 3.8 & 28.2 & 7.5 & 24.0 & 24.9 & 1.2 & 1.8 & 0.5 & 0.8 & 39.7 & 51.0 & 6.0\\
\texttt{re} & 5.7 & 17.2 & 0.7 & 20.2 & 8.4 & 17.9 & 19.2 & 1.1 & 1.3 & 0.5 & 0.7 & 37.3 & 47.2 & 3.3\\ 
\texttt{foo} & 9.2 & 14.8 & 8.5 & 4.2 & 0.6 & 941.8 & 943.7 & 0.2 & 0.2 & 0.9 & 0.9 & 6.7 & 7.1 & 5.5\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]\setlength{\belowcaptionskip}{0pt}\verynarrow
\caption{Timing comparison between {\newpolka} (N), PPL (P) and {\libpoly} (L). Computation times (in milliseconds) are aggregated according to operation and problem size. (n) is the total number of problems of the size range in the benckmarks.}
\label{restabsz}

\newcommand{\totalnr}[1]{\multicolumn{1}{>{\em}r|}{#1}}
\begin{center}
\begin{tabular}{|l l |S[table-format=4.2]|S[table-format=4.2]|S[table-format=4.2]|S[table-format=4.2]|S[table-format=4.2]|S[table-format=4.2]|S[table-format=2.2]|S[table-format=3.2]|}
\hline
\multicolumn{2}{|r|}{problem size} & \multicolumn{1}{c|}{0--1} & \multicolumn{1}{c|}{2--5} & \multicolumn{1}{c|}{6--10} & \multicolumn{1}{c|}{11--15} & \multicolumn{1}{c|}{16--20} & \multicolumn{1}{c|}{21--25} & \multicolumn{1}{c|}{26--30} & \multicolumn{1}{c|}{31+}\\
\hline
\multirow{3}{*}{assignment}
	& N & 33.8 & 601.8 & 385.4 & 20.9 & 78.3 & 537.4 & 59.5 & 13.1 \\
	& P & 47.5 & 1176 & 519.7 & 87.4 & 247.6 & 2111 & 81.7 & 77.9 \\
	& L & 1.1 & 6.6 & 14.3 & 10.7 & 5.2 & 39.2 & 15.2 & 11.6 \\
	& n & \totalnr{539} & \totalnr{667} & \totalnr{381} & \totalnr{58} & \totalnr{64} & \totalnr{480} & \totalnr{30} & \totalnr{16} \\
\hline
\multirow{3}{*}{convex hull}
	& N & 687.9 & 679.7 & 434.1 & 119.5 & 68.8 & 37.9 & 6.4 & 3.5\\
	& P & 167.5 & 141.0 & 68.4 & 22.8 & 16.8 & 9.2 & 1.9 & 0.9 \\
	& L & 7.0 & 57.1 & 133.7 & 131.2 & 1050 & 106.4 & 50.1 & 27.8\\
	& n & \totalnr{3354} & \totalnr{3373} & \totalnr{1092} & \totalnr{354} & \totalnr{135} & \totalnr{65} & \totalnr{14} & \totalnr{7}\\
\hline
\multirow{3}{*}{inclusion}
	& N & 7.2 & 9.7 & 9.7 & 3.3 & 5.8 & 4.0 & 4.0 & 0 \\
	& P & 6.5 & 12.8 & 10.6 & 4.2 & 7.0 & 3.9 & 3.4 & 0 \\
	& L & 0.6 & 1.6 & 1.3 & 0.5 & 1.0 & 0.3 & 0.1  & 0\\
	& n & \totalnr{1482} & \totalnr{1881} & \totalnr{673} & \totalnr{277} & \totalnr{111} & \totalnr{52} & \totalnr{17} & \totalnr{4}\\
\hline
\multirow{3}{*}{intersection}
	& N & 1389 & 1752 & 52.3 & 27.4 & 1.3 & &  & \\
	& P & 1933 & 1740 & 158.6 & 91.4 & 4.8 & & & \\
	& L & 35.0 & 30.9 & 18.4 & 8.8 & 0.6 &  &  & \\
	& n & \totalnr{11458} & \totalnr{4094} & \totalnr{322} & \totalnr{156} & \totalnr{6} & \totalnr{0} & \totalnr{0} & \totalnr{0}\\
\hline
\end{tabular}
\end{center}
\end{table}

The presented results show that \libpoly\ is efficient on small problems.
Yet, the performance gap between \libpoly\ and the other implementations closes on bigger problems.
This is especially true for the convex hull, which is a costly operation in the constraint representation.
At least part of the difference in efficiency on small problems can be explained by the generality \apron\ provides:
it provides a unified interface to several abstract domains at the expense of an extra abstraction layer
which introduces a significant overhead on small problems.

More generally, the use of \zarith\ in \libpoly\ is likely to lower the cost of arithmetic
when compared to \newpolka\ and the PPL, which use \gmp\ directly.
The \texttt{foo} program illustrates this: the analysis creates constraints with big coefficients,
likely to overflow native number representation.
However, precise measurement of the effect of using \zarith\ would be a hard task.

Last, Table~\ref{restabprg} seems to show that problems are most often of rather small size,
but this may well be due to our limited experimentation means.

In spite of the shortcomings of our evaluation method,
these results seem promising for a constraints-only implementation of the abstract domain of polyhedra.
Some progress still needs to be made on the convex hull side (see \pararef{concl}).
It is also interesting to notice the performance differences between the {\newpolka} and the PPL, despite their design similarities; we ignore their cause.

\subsection{Certificate Checking Overhead}
The certificate checking overhead shown in Table~\ref{restabprg} includes
the translation between \ocaml\ and \coq\ representations.
Inside a certified static analyzer, this overhead could be reduced by only transferring the certificates, as opposed to the full polyhedra, and using them to simulate the polyhedra computations, without bothering to check after every call that the polyhedron inside the {\ocaml} library corresponds to the one inside the certified checker.
In addition to translation costs, there is the general inefficiency of computations on {\coq} integers, which are represented as lists of bits; this is considerably more expensive than using native integers, or even arrays of native integers as GMP would do.

However, it should be noted that the checking of inclusion certificates occurs
only during the final step of the certified static analysis which consists in
verifying that the inferred invariant candidates are indeed inductive invariants
for the program.

Last, the overhead of certificate checking is relatively greater for inclusion than for convex hull.
Although the actual checking burden is bigger for the convex hull,
due to certificate composition densifying the resulting certificate,
the inclusion test algorithm is much cheaper than the convex hull in terms of computations.
More precisely, the convex hull algorithm involves inclusion tests as part of representation minimization.

