
\documentclass[dvipsnames, svgnames, x11names, table]{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{enumitem}
\hyphenpenalty=5000
\tolerance=1000

\usepackage{setspace}

\title{CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation}

\author{Tongkun Xu$^{12}$\thanks{These authors contributed equally to this work.} , Weihua Chen$^{1\ast}$, Pichao Wang$^{1}$, Fan Wang$^{1}$, Hao Li$^{1}$, Rong Jin$^{1}$ \\
\text{$^{1}$Alibaba Group, $^{2}$Shandong University} \\
\texttt{xutongkun1208@gmail.com, kugang.cwh@alibaba-inc.com} \\ 
\texttt{\{pichao.wang,fan.w,lihao.lh,jinrong.jr\}@alibaba-inc.com}
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}
\begin{spacing}{1.0}
\maketitle
\vspace{-3mm}
\begin{abstract}
Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance.  With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way center-aware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. 
Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution.
Experiments show that our proposed method achieves the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet. Code and models are available at \url{https://github.com/CDTrans/CDTrans}.
\end{abstract}

\section{Introduction}
Deep neural network have achieved remarkable success in a wide range of application scenarios~\citep{wang2022adanets,qian2021learning,jiang2022giraffedet,tan2019lrt,chen2021tagperson,jiang2021exploring,chen2017beyond} but it still suffers poor generalization performance to other new domain because of the domain shift problem~\citep{csurka2017domain,zhao2020review,zhang2020unsupervised,oza2021unsupervised}.
To handle this issue and avoid the expensive laborious annotations, lots of research efforts~\citep{bousmalis2017unsupervised,kuroki2019unsupervised,wilson2020survey,vs2021mega} are devoted on Unsupervised Domain Adaptation (UDA). The UDA task aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. In UDA, most approaches focus on aligning distributions of source and target domain and learning domain-invariant feature representations. One kind of such UDA methods are based on category-level alignment~\citep{kang2019contrastive,zhang2019category,jiang2020implicit,li2021category}, which have achieved promising results on public UDA datasets using deep convolution neural networks (CNNs). The fundamental problems in category-level based alignment is the production of pseudo labels for samples in target domain to generate the input source-target pairs. However, the current CNNs-based methods are not robust to the generated noisy pseudo labels for accurate domain alignment~\citep{morerio2020generative,jiang2020implicit}. 






With the success of Transformer in natural language processing (NLP)~\citep{vaswani2017attention,devlin2018bert} and vision tasks~\citep{dosovitskiy2020image,han2020survey,he2021transreid,khan2021transformers}, it is found that cross-attention in Transformer is good at aligning different distributions, even from different modalities \textit{e.g.}, vision-to-vision~\citep{li2021trear}, vision-to-text~\citep{tsai2019multimodal,hu2021unit} and text-to-speech~\citep{li2019neural}. And we find that it is robust to noise in pseudo labels to some extent. Hence, in this paper, we apply transformers to the UDA task to take advantage of its robustness to noise and super power for feature alignment to deal with the problems as described above in CNNs.








In our experiment, we conclude that even with noise in the labeling pair, the cross-attention can still work well in aligning two distributions, thanks to the attention mechanism. To obtain more accurate pseudo labels, we designed a two-way center-aware labeling algorithm for samples in the target domain. The pseudo labels are produced based on the cross-domain similarity matrix, and a center-aware matching is involved to weight the matrix and weaken noise into the tolerable range. With the help of pseudo labels, we design the cross-domain transformer (CDTrans) for UDA. It consists of three weight-sharing transformer branches, of which two branches are for source and target data respectively and the third one is the feature alignment branch, whose inputs are from source-target pairs. The self-attention is applied in the source/target transformer branches and cross-attention is involved in the feature alignment branch to conduct domain alignment. Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously.
In summary, our contributions are three-fold:
\begin{itemize}
\item We propose a weight-sharing triple-branch transformer framework, namely, CDTrans, for accurate unsupervised domain adaptation, taking advantage of its robustness to noisy labeling data and great power for feature alignment.
\item To produce pseudo labels with high quality, a two-way center-aware labeling method is proposed, and it boosts the final performance in the context of CDTrans.
\item CDTrans achieves the best performance compared to state-of-the-arts with a large margin on VisDA-2017~\citep{visda2017} and DomainNet~\citep{peng2019moment} datasets.
\end{itemize}



\section{Related Work}
\subsection{Transformer for Vision}
Transformer is proposed in~\citep{vaswani2017attention} to model sequential data in the field of NLP. Many works have shown its effectiveness for computer-vision tasks~\citep{han2020survey,khan2021transformers,li2021transformer,han2021context,yu2021transrppg,li2021lifting,yang2021transformer,qian2022entroformer}. Pure Transformer based models are becoming more and more popular. For example, ViT \citep{dosovitskiy2020image} is proposed recently  by feeding transformer with sequences of image patches; Touvron et al.~\citep{touvron2021training} propose DeiT that introduces a distillation strategy for transformers to help with ViT training; many other ViT variants~
\citep{yuan2021tokens,wang2021pyramid,han2021transformer,chen2021psvit,ranftl2021vision,liu2021swin} 
are proposed from then, which achieve promising performance compared with its counterpart CNNs for both image classification and downstream tasks, such as object detection~\citep{liu2021swin}, semantic segmentation~\citep{yuan2021volo} and object ReID~\citep{he2021transreid}. For multi-modal based networks, there are several works~\citep{tsai2019multimodal,li2021trear,hu2021unit} that apply cross-attention for multi-modal feature fusion, which demonstrates that attention mechanism is powerful at distilling noise and feature alignment. This paper adopts cross-attention in the context of pure transformers for UDA tasks.
 

\subsection{Unsupervised Domain Adaptation}
There are mainly two levels for UDA methods: domain-level~\citep{tzeng2014deep,long2015learning,ghifary2016deep,tzeng2017adversarial,bousmalis2017unsupervised,hoffman2018cycada} and  category-level~\citep{saito2018maximum,kang2019contrastive,du2021cross,li2021cross}. Domain-level UDA mitigates the distribution divergence between the source and target domain by pulling them into the same distribution at different scale levels.  The commonly used divergence measures include Maximum Mean Discrepancy (MMD)~\citep{gretton2006kernel,tzeng2014deep,long2015learning} and Correlation Alignment (CORAL)~\citep{sun2016return,sun2016deep}.  Recently, some works \citep{saito2018maximum,du2021cross,li2021cross} focus on the fine-grained category-level label distribution alignment through an adversarial manner between the feature extractor and two domain-specific classifiers.  Unlike coarse-grained alignment at the domain scale, this approach aligns each category distribution between the source and target domain data by pushing the target samples to the distribution of source samples in each category. Obviously, the fine-grained alignment results in more accurate distribution alignment within the same label space. Although the adversarial approach achieves new improvements by fusing fine-grained alignment operations of source and target samples at the category level, it still does not solve the problem of noisy samples in the wrong category. Our method adopts Transformers for category-level UDA to solve the noise problem. 


\subsection{Pseudo Labeling}
Pseudo labeling~\citep{lee2013pseudo} is first introduced for semi-supervised learning and gains popularity in domain adaptation tasks. It learns to label unlabeled data using predicted probabilities and performs fine-tuning together with labeled data. In terms of using pseudo labeling for domain adaptation tasks, \citep{long2017deep,long2017conditional} adopt pseudo labels  to conduct conditional distribution alignment; \citep{zhang2018collaborative,choi2019pseudo} use pseudo labels as a regularization for domain adaptation; \citet{zou2018unsupervised} designs a self-training framework by alternately solving pseudo labels; \citet{caron2018deep} propose a deep self-supervised method by generating pseudo labels via $k$-means cluster to progressively train the model; \citet{liang2020we} develop a self-supervised pseudo labeling method to alleviate the effects of noisy pseudo labels. Based on~\citet{liang2020we}, in this work, we propose a two-way center-aware labeling algorithm to further filter the noisy pseudo pairs. 

\section{The Proposed Method}
We first introduce the cross attention module and analyze its robustness to the noise in Section~\ref{ssec:crossattn}. Then the two-way center-aware labeling method is presented in Section~\ref{ssec:pseudolabels}. With the produced pseudo labels as inputs, our cross-domain transformer (CDTrans) is proposed in Section~\ref{ssec:cdtrans}, consisting of three weight-sharing transformers.

\subsection{The Cross Attention in Transformer}
\label{ssec:crossattn}
\subsubsection{Preliminary}
Vision Transformer (ViT)~\citep{dosovitskiy2020image} has achieved comparable or even superior performance on computer vision tasks. One of the most important structures in ViT is the self-attention module~\citep{vaswani2017attention}. In ViT, an image $\mI\in \mathbb{R}^{H\times W\times C}$ is reshaped into a sequence of flattened 2D patches $x\in \mathbb{R}^{N\times (P^2\cdot C)}$, where ($H,W$) is the resolution of the original image, $C$ is the number of channels, ($P,P$) is the resolution of each image patch, and $N=HW/P^2$ is the resulting number of patches. For self-attention, the patches are first projected into three vectors, \textit{i.e.} queries $\mQ\in \mathbb{R}^{N\times d_k}$, keys $\mK\in \mathbb{R}^{N\times d_k}$ and values $\mV\in \mathbb{R}^{N\times d_v}$. $d_k$ and $d_v$ indicates their dimensions.
The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. The $N$ patches serve as the inputs for the self-attention module, and the process can be formulated as below. The self-attention module aims to emphasize relationships among patches of the input image.
\begin{gather}
Attn_{self}(\mQ,\mK,\mV)=softmax(\frac{\mQ\mK^T}{\sqrt{d_k}})\mV
\label{eq:selfattn}
\end{gather}



The cross-attention module is derived from the self-attention module. The difference is that the input of cross-attention is a pair of images, \textit{i.e.} $\mI_s$ and $\mI_t$. Its query and key/value are from patches of $\mI_s$ and $\mI_t$ respectively. The cross-attention module can be calculated as follows:
\begin{gather}
Attn_{cross}(\mQ_s,\mK_t,\mV_t)=softmax(\frac{\mQ_s\mK_t^T}{\sqrt{d_k}})\mV_t
\label{eq:crsattn}
\end{gather}
where $\mQ_s\!\in\!\mathbb{R}^{M\times d_k}$ are queries from $M$ patches of image $\mI_s$, and $\mK_t\!\in\!\mathbb{R}^{N\times d_k},\mV_t\!\in\!\mathbb{R}^{N\times d_v}$ are keys and values from $N$ patches of image $\mI_t$. 
The output of the cross-attention module holds the same length $M$ as the number of the queries. For each output, it is calculated by multiplying $\mV_t$ with attention weights, which comes from the similarity between the corresponding query in $\mI_s$ and all the keys in $\mI_t$. As a result, among all patches in $\mI_t$, the patch that is more similar to the query of $\mI_s$ would hold a larger weight and contribute more to the output. In other words, the output of the cross-attention module manages to aggregate the two input images based on their similar patches. 


So far, many researchers have utilized the cross-attention for feature fusion, especially in multi-modal tasks~\citep{tsai2019multimodal,li2019neural,hu2021unit,li2021trear}. In these works, the inputs of the cross-attention module are from two modalities, \textit{e.g.} vision-to-text~\citep{tsai2019multimodal,hu2021unit}, text-to-speech~\citep{li2019neural} and vision-to-vision~\citep{li2021trear}. They apply the cross-attention to aggregate and align the information from two modalities. Given its great power in feature alignment, we propose to use the cross attention module to solve the unsupervised domain adaptation problem.



\begin{figure}[!t]
\centering
\begin{subfigure}[c]{0.54\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{./fig/hotmap_case.pdf}
\vspace{-5mm}
\caption{}
\vspace{-3mm}
\label{fig:noisecase}
\end{subfigure}
\hfill
\begin{subfigure}[c]{0.44\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{./fig/noise_rate.pdf}
\vspace{-5mm}
\caption{}
\vspace{-3mm}
\label{fig:noisecurve}
\end{subfigure}
\caption{(a): The heatmap of the cross-attention weights for a false positive pair (Car vs. Truck). (b): The changes of UDA performance by the ratio of false positive pairs. The \textcolor{red}{red}/\textcolor{Green}{green} curves represent the model with and without the cross-attention module. The \textcolor{blue}{blue} curve means that only true positive pairs are involved in the cross-attention module.}
\vspace{-3mm}
\end{figure}

\subsubsection{Robustness to Noise}


As mentioned above, the input of the cross-attention module is a pair of images, which usually comes from two domains, and the cross-attention module aims to align these two images.
If label noise exists, there would be false positive pairs in the training data. Images in the false positive pairs would have dissimilar appearance, and forcibly aligning their features would inevitably injure the training and compromise the performance.
We assume that the dissimilar patches in false positive pairs are more harmful to the performance than the similar patches.
In the cross-attention module, two images are aligned based on their patch similarity. As shown in Fig.~\ref{fig:noisecase}, the cross-attention module would assign a low weight to the dissimilar patches in false positive pairs. Thus it weakens the negative effects of the dissimilar patches on the final performance to some extent.
\footnote{The patches in true positive pairs, either similar and dissimilar, would bring no noise to the final performance, which is out of the discussion of our paper.}

To further analyze this issue, an experiment is carefully designed. Specifically, we randomly sample true positive pairs from source and target domain in VisDA-2017 dataset~\citep{visda2017} as the training data. Then we manually replace the true positive pairs with random false positive pairs to increase the noise, and watch the changes of the performance as shown in Fig.~\ref{fig:noisecurve}. The x-axis indicates the rate of false positive pairs in the training data, and the y-axis shows the performance of different methods on the UDA task. The red curve represents the results by aligning pairs with the cross-attention module, while the green curve is that without cross-attention, \textit{i.e.} to directly train the target data with the label of corresponding source data in the pair. It can be seen that the red curve achieves a much better performance than the green one, which implies the robustness of the cross-attention module to the noise. We also provide another baseline shown as the blue curve in Fig.~\ref{fig:noisecurve}, which is to remove the false positive pairs from the training data and train the cross-attention with only true positive pairs. Without the noisy data, this baseline can be considered as the upper bound to our methods.
We can see the red curve is very close to the blue curve, and both of them are much better than the green one. It further implies that the cross-attention module is robust to the noisy input pair.





\subsection{Two-Way Center-Aware Pseudo Labeling}
\label{ssec:pseudolabels}



\subsubsection{Two-Way Labeling}

To build the training pairs for the cross-attention module, an intuitive method is that for each image in the source domain, we manage to find the most similar image from the target domain. The set $\sP_S$ of selected pairs is:
\begin{gather}
\sP_S=\{(s,t)|t=\min_k d(\vf_s,\vf_k),\forall k\in T,\forall s\in S\}
\label{eq:ps}
\end{gather}
where $S,T$ are the source and target data respectively. $d(\vf_i,\vf_j)$ means the distance between features of image $i$ and $j$. The advantage of this strategy is to make full use of source data, while its weakness is obvious that only a part of target data is involved. To eliminate this training bias from target data, we introduce more pairs $\sP_T$ from the opposite way, consisting of all the target data and their corresponding most similar 
images in the source domain.
\begin{gather}
\sP_T=\{(s,t)|s=\min_k d(\vf_t,\vf_k),\forall t\in T,\forall k\in S\}
\label{eq:pt}
\end{gather}
As a result the final set $\sP$ is the union of two sets, \textit{i.e.} $\sP=\{\sP_S\cup \sP_T\}$, making the training pairs include all the source and target data.

\subsubsection{Center-Aware Filtering}
The pairs in $\sP$ are built based on the feature similarities of images from both domains, thus the accuracy of the pseudo labels of pairs is highly dependent on the feature similarities. Inspired by \citet{liang2020we}, we find that the pre-trained model of the source data is also useful to further improve the accuracy.
Firstly, we send all the target data through the pre-trained model and obtain their probability distributions $\delta$ on the source categories from the classifier. Similar to \citet{liang2020we}, these distributions can be used to compute initial centers of each category in the target domain by weighted k-means clustering:
\begin{gather}
\vc_k=\frac{\sum_{t\in T}\delta_t^k\vf_t}{\sum_{t\in T}\delta_t^k}
\label{eq:initcenter}
\end{gather}
where $\delta_t^k$ indicates the probability of image $t$ on category $k$. Pseudo labels of the target data can be produced via the nearest neighbor classifier:
\begin{gather}
y_t=\mathop{\arg\min_k}d(\vc_k,\vf_t)
\label{eq:initlabel}
\end{gather}
where $t\in T$ and $d(i,j)$ is the distance of features $i$ and $j$. Based on the pseudo labels, we can calculate new centers:
\begin{gather}
\vc'_k=\frac{\sum_{t\in T}\mathbbm{1}(y_t=k)\vf_t}{\sum_{t\in T}\mathbbm{1}(y_t=k)}
\label{eq:newcenter}
\end{gather}

In \citet{liang2020we}, Eq.~\ref{eq:initlabel} and~\ref{eq:newcenter} could be updated for multiple rounds, and we only adopt one round in our paper. The final pseudo labels are then used to refine the selected pairs. Specifically, for every pair, if the pseudo label of the target image is consistent with the label of the source image, this pair would be kept for our training, otherwise it will be discarded as a noise.

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\linewidth]{./fig/framework.pdf}
\caption{The proposed CDTrans framework. It consists of three weight-sharing transformers fed by inputs from the selected pairs using the two-way center-aware labeling method. Cross-entropy is adopted to \textit{source branch} (H$_{S}$) and \textit{target branch} (H$_T$), while the distillation loss is applied between \textit{source-target branch} (H$_{S+T}$) and H$_T$. }
\vspace{-3mm}
\label{fig:framework}
\end{figure}

\subsection{CDTrans: Cross-domain Transformer}
\label{ssec:cdtrans}
The framework of the proposed Cross-domain Transformer (CDTrans) is shown in Fig.~\ref{fig:framework}, which consists of three weight-sharing transformers. There are three data flows and constraints for the weight-sharing branches.

The inputs of the framework are the selected pairs from our labeling method mentioned above. The three branches are named as \textit{source branch}, \textit{target branch}, \textit{source-target branch}. As shown in Fig.~\ref{fig:framework}, the source and target images in the input pair are sent to source branch and target branch respectively. In these two branches, the self-attention module is involved to learn the domain-specific representations. And the softmax cross-entropy loss is used to train the classification. It is worth noting that all three branches share the same classifier due to the same label of two images.

The cross-attention module is imported in the source-target branch. The inputs of the source-target branch are from the other two branches. In the $N$-th layer, the query of the cross-attention module comes from the query in the $N$-th layer of the source branch, while the keys and values are from those of the target branch. Then the cross-attention module outputs aligned features which are added with the output of the $(N\!-\!1)$-th layer.\footnote{The addition operation is not included in the 1st layer.}

The features of the source-target branch not only align distributions of two domains, but are robust to the noise in the input pairs thanks to the cross-attention module.
Thus we use the output of the source-target branch to guide the training of the target branch. Specifically, the source-target branch and target branch are denoted as teacher and student respectively. We consider the probability distribution of the classifier in source-target branch as a soft label that can be used to further supervise the target branch through a distillation loss~\citep{hinton2015distilling}:
\begin{gather}
L_{dtl}=\sum_kq_k\log p_k
\label{eq:distillloss}
\end{gather}
where $q_k$ and $p_k$ are the probabilities of category $k$ from the source-target branch and the target branch respectively.

During inference, only the target branch is used. The input is an image from testing data, and only the target data flow is triggered, \textit{i.e.} the blue lines in Fig.~\ref{fig:framework}. Its output of the classifier is utilized as the final predicted labels.





\section{Experiments}

\subsection{Datasets and Implementation}
The proposed method is verified on four popular UDA benchmarks, including VisDA-2017~\citep{visda2017}, Office-Home~\citep{venkateswara2017deep},
Office-31~\citep{saenko2010adapting}
and DomainNet~\citep{peng2019moment}.
In the DomainNet dataset, we follow the setup method of other datasets, using the entire data of the traget domain for training and testing.
The input image size in our experiments is 224$\times$224. Both the DeiT-small and DeiT-base~\citep{touvron2021training} are adopted as our backbone for fair comparison.
We use the Stochastic Gradient Descent algorithm with the momentum of 0.9 and weight decay ratio 1e-4 to optimize the training process.
The learning rate is set to 3e-3 for Office-Home, Office-31 and DomainNet, 5e-5 for VisDA-2017 since it can easily converge. The batch size is set to 64. 




\begin{table*}[!t]\small
\setlength\tabcolsep{3.4pt}
\begin{tabular}{c|cccccccccccc>{\columncolor{lightgray}}c}
\hline
Method    & plane & bcycl & bus   & car   & horse & knife & mcycl & person & plant & sktbrd & train & truck & Avg.  \\ \hline
ResNet-50    & 55.1  & 53.3    & 61.9  & 59.1  & 80.6  & 17.9  & 79.7  & 31.2   & 81.0  & 26.5       & 73.5  & 8.5   & 52.4 \\
DANN      & 81.9  & 77.7    & 82.8  & 44.3  & 81.2  & 29.5  & 65.1  & 28.6   & 51.9  & 54.6       & 82.8  & 7.8   & 57.4 \\
MinEnt    & 80.3  & 75.5    & 75.8  & 48.3  & 77.9  & 27.3  & 69.7  & 40.2   & 46.5  & 46.6       & 79.3  & 16.0  & 57.0 \\
MCD       & 87.0  & 60.9    & 83.7  & 64.0  & 88.9  & 79.6  & 84.7  & 76.9   & 88.6  & 40.3       & 83.0  & 25.8  & 71.9 \\
SWD       & 90.8  & 82.5    & 81.7  & 70.5  & 91.7  & 69.5  & 86.3  & 77.5   & 87.4  & 63.6       & 85.6  & 29.2  & 76.4 \\
CDAN+E    & 85.2  & 66.9    & 83.0  & 50.8  & 84.2  & 74.9  & 88.1  & 74.5   & 83.4  & 76.0       & 81.9  & 38.0  & 73.9 \\
BNM       & 89.6  & 61.5    & 76.9  & 55.0  & 89.3  & 69.1  & 81.3  & 65.5   & 90.0  & 47.3       & 89.1  & 30.1  & 70.4 \\
MSTN+DSBN & 94.7  & 86.7    & 76.0  & 72.0  & 95.2  & 75.1  & 87.9  & 81.3   & 91.1  & 68.9       & 88.3  & 45.5  & 80.2 \\
CGDM      & 93.7  & 82.7    & 73.2  & 68.4  & 92.9  & 94.5  & 88.7  & 82.1   & 93.4  & 82.5       & 86.8  & 49.2  & 82.3 \\
CGDM*  & 92.8 & 85.1 & 76.3 & 64.5 & 91.0 & 93.2 & 81.3 & 79.3 & 92.4 & 83.0 & 85.6 & 44.8 & 80.8 \\
SHOT      & 94.3  & 88.5    & 80.1  & 57.3  & 93.1  & 93.1  & 80.7  & 80.3   & 91.5  & 89.1   & 86.3  & 58.2  & 82.9 \\ 
SHOT* & 95.5 & 87.5 & 80.1 & 54.5 & 93.6 & 94.2 & 80.2 & 80.9 & 90.0 & 89.9 & 87.1 & 58.4 & 82.7 \\
TVT$^{\circ}$ & 92.9 & 85.6 & 77.5 & 60.5 & 93.6 & 98.2 & 89.4 & 76.4 & 93.6 & \textbf{92.0} & 91.7 & 55.7 & 83.9 \\
\hline
Baseline-B & 97.7 & 48.1 & 86.6 & 61.6 & 78.1 & 63.4 & \textbf{94.7} & 10.3 & 87.7 & 47.7 & \textbf{94.4} & 35.5 & 67.1 \\
CGDM-B* & 96.3 & 87.1 & \textbf{86.8} & \textbf{83.5} & 92.2 & 98.3 & 91.6 & 78.5 & 96.3 & 48.4 & 89.4 & 39.0 & 82.3 \\
SHOT-B* & \textbf{97.9} & 90.3 & 86.0 & 73.4 & \textbf{96.9} & \textbf{98.8} & 94.3 & 54.8 & 95.4 & 87.1 & 93.4 & 62.7 & 85.9 \\
Ours-B & 97.1 & \textbf{90.5} & 82.4 & 77.5 & 96.6 & 96.1 & 93.6 & \textbf{88.6} & \textbf{97.9} & 86.9 & 90.3 & \textbf{62.8} & \textbf{88.4} \\
\hline
\end{tabular}
\vspace{-2mm}
\caption{Comparison with SoTA methods on VisDA-2017. ``S/B'' implies the DeiT-small/DeiT-base backbone respectively. $*$ indicates the results are reproduced by ourselves. ${\circ}$ implies its pretrained model is trained on ImageNet21K instead of ImageNet1K. The best performance is marked as \textbf{bold}.}
\label{tab:visda}
\vspace{-1mm}
\end{table*}

\begin{table*}[!t]\small
\setlength\tabcolsep{1.0pt}
\scalebox{0.945}{
\begin{tabular}{c|cccccccccccc>{\columncolor{lightgray}}c}
\hline
Method &
  Ar$\to $Cl &
  Ar$\to $Pr &
  Ar$\to $Re &
  Cl$\to $Ar &
  Cl$\to $Pr &
  Cl$\to $Re &
  Pr$\to $Ar &
  Pr$\to $Cl &
  Pr$\to $Re &
  Re$\to $Ar &
  Re$\to $Cl &
  Re$\to $Pr &
  Avg. \\ \hline
ResNet-50      & 44.9 & 66.3 & 74.3 & 51.8 & 61.9 & 63.6 & 52.4 & 39.1 & 71.2 & 63.8 & 45.9 & 77.2 & 59.4 \\
MinEnt         & 51.0 & 71.9 & 77.1 & 61.2 & 69.1 & 70.1 & 59.3 & 48.7 & 77.0 & 70.4 & 53.0 & 81.0 & 65.8 \\
CDAN+E         & 54.6 & 74.1 & 78.1 & 63.0 & 72.2 & 74.1 & 61.6 & 52.3 & 79.1 & 72.3 & 57.3 & 82.8 & 68.5 \\
DCAN           & 54.5 & 75.7 & 81.2 & 67.4 & 74.0 & 76.3 & 67.4 & 52.7 & 80.6 & 74.1 & 59.1 & 83.5 & 70.5 \\
BNM            & 56.7 & 77.5 & 81.0 & 67.3 & 76.3 & 77.1 & 65.3 & 55.1 & 82.0 & 73.6 & 57.0 & 84.3 & 71.1 \\
ATDOC-NA       & 58.3 & 78.8 & 82.3 & 69.4 & 78.2 & 78.2 & 67.1 & 56.0 & 82.7 & 72.0 & 58.2 & 85.5 & 72.2 \\
SHOT           & 57.1 & 78.1 & 81.5 & 68.0 & 78.2 & 78.1 & 67.4 & 54.9 & 82.2 & 73.3 & 58.8 & 84.3 & 71.8 \\ 
SHOT* & 56.2 & 77.6 & 81.6 & 67.5 & 78.2 & 78.8 & 67.8 & 54.0 & 82.0 & 72.5 & 58.8 & 84.5 & 71.6\\
TVT$^{\circ}$ & \textbf{74.9} & \textbf{86.8} & \textbf{89.5} & \textbf{82.8} & \textbf{88.0} & \textbf{88.3} & \textbf{79.8} & \textbf{71.9} & \textbf{90.1} & \textbf{85.5} & \textbf{74.6} & \textbf{90.6} & \textbf{83.6} \\
\hline
Baseline-S & 55.6 & 73.0 & 79.4 & 70.6 & 72.9 & 76.3 & 67.5 & 51.0 & 81.0 & 74.5 & 53.2 & 82.7 & 69.8 \\
Ours-S & 60.6 & 79.5 & 82.4 & 75.6 & 81.0 & 82.3 & 72.5 & 56.7 & 84.4 & 77.0 & 59.1 & 85.5 & 74.7 \\
Baseline-B & 61.8 & 79.5 & 84.3 & 75.4 & 78.8 & 81.2 & 72.8 & 55.7 & 84.4 & 78.3 & 59.3 & 86.0 & 74.8 \\
CGDM-B* & 67.1 & 83.9 & 85.4 & 77.2 & 83.3 & 83.7 & 74.6 & 64.7 & 85.6 & 79.3 & 69.5 & 87.7 & 78.5 \\
SHOT-B* & 67.1 & 83.5 & 85.5 & 76.6 & 83.4 & 83.7 & 76.3 & 65.3 & 85.3 & 80.4 & 66.7 & 83.4 & 78.1 \\
Ours-B & 68.8 & 85.0 & 86.9 & 81.5 & 87.1 & 87.3 & 79.6 & 63.3 & 88.2 & 82.0 & 66.0 & \textbf{90.6} & 80.5 \\
\hline
\end{tabular}}
\vspace{-2mm}
\caption{Comparison with SoTA methods on Office-Home. The best performance is marked as \textbf{bold}.}
\label{tab:officehome}
\vspace{-3mm}
\end{table*}




\subsection{Comparison to SoTA}
We compare our method with state-of-the-art methods on UDA tasks, including MinEnt~\citep{grandvalet2005semi}, 
DANN~\citep{ganin2015unsupervised},  
CDAN+E~\citep{long2017conditional}, CDAN+BSP ~\citep{chen2019transferability}, CDAN+TN~\citep{wang2019transferable}, rRGrad+CAT~\citep{deng2019cluster}, MCD~\citep{saito2018maximum}, SWD~\citep{lee2019sliced}, 
MSTN+DSBN~\citep{chang2019domain}, 
SAFN+ENT~\citep{xu2019larger}, 
BNM~\citep{cui2020towards}, DCAN~\citep{li2020domain}, SHOT~\citep{liang2020we},
ATDOC-NA~\citep{liang2021domain}, CGDM~\citep{du2021cross} and TVT~\citep{yang2021tvt}.
The results are shown in Table~\ref{tab:visda},~\ref{tab:officehome},~\ref{tab:office31} and~\ref{tab:domainnet}. 

\begin{wraptable}{!t}{7.8cm}
\small
\setlength\tabcolsep{1.3pt}
\centering
\scalebox{0.95}{
\begin{tabular}{c|cccccc>{\columncolor{lightgray}}c}
\hline
Method & A$\to$D & A$\to$W & D$\to$A & D$\to$W & W$\to$A & W$\to$D & Avg \\ 
\hline
ResNet-50 & 68.9 & 68.4 & 62.5 & 96.7 & 60.7 & 99.3 & 76.1 \\
DANN & 79.7 & 82.0 & 68.2 & 96.9 & 67.4 & 99.1 & 82.2 \\
CDAN+E & 92.9 & 94.1 & 71.0 & 98.6 & 69.3 & \textbf{100.} & 87.7 \\
rRGrad+CAT & 90.8 & 94.4 & 72.2 & 98.0 & 70.2 & \textbf{100.} & 87.6 \\
SAFN+ENT & 90.7 & 90.1 & 73.0 & 98.6 & 70.2 & 99.8 & 87.1 \\
CDAN+BSP & 93.0 & 93.3 & 73.6 & 98.2 & 72.6 & \textbf{100.} & 88.5 \\
CDAN+TN & 94.0 & 95.7 & 73.4 & 98.7 & 74.2 & \textbf{100.} & 89.3\\
SHOT & 94.0 & 90.1 & 74.7 & 98.4 & 74.3 & 99.9 & 88.6 \\
SHOT* & 93.8 & 91.8 & 74.8 & 98.2 & 74.1 & 99.8 & 88.8 \\
TVT$^{\circ}$ & 96.4 & 96.4 & \textbf{84.9} & \textbf{99.4} & \textbf{86.1} & \textbf{100.} & \textbf{93.8} \\
\hline
Baseline-S & 87.6 & 86.9 & 74.9 & 97.7 & 73.5 & 99.6 & 86.7\\
Ours-S & 94.6 & 93.5 & 78.4 & 98.2 & 78.0 & 99.6 & 90.4 \\
Baseline-B & 90.8 & 90.4 & 76.8 & 98.2 & 76.4 & \textbf{100.} & 88.8 \\
CGDM-B* & 94.6 & 95.3 & 78.8 & 97.6 & 81.2 & 99.8 & 91.2\\
SHOT-B* & 95.3 & 94.3 & 79.4 & 99.0 & 80.2 & \textbf{100.} & 91.4 \\
Ours-B & \textbf{97.0} & \textbf{96.7} & 81.1 & 99.0 & 81.9 & \textbf{100.} & 92.6 \\
\hline
\end{tabular}}
\vspace{-2mm}
\caption{Comparison with SoTA methods on Office-31. The best performance is marked as \textbf{bold}.}
\vspace{-3mm}
\label{tab:office31}
\end{wraptable}

\begin{table*}[!t]\small
\setlength\tabcolsep{1.0pt}
\scalebox{0.845}{
\begin{tabular}{c|ccccccc||c|ccccccc||c|ccccccc}
\hline
MCD & clp & inf & pnt & qdr & rel & skt & Avg. & CDAN & clp & inf & pnt & qdr & rel & skt & Avg. & BNM & clp & inf & pnt & qdr & rel & skt & Avg. \\ 
\hline
clp & - & 15.4 & 25.5 & 3.3 & 44.6 & 31.2 & 24.0 & clp & - & 13.5 & 28.3 & 9.3 & 43.8 & 30.2 & 25.0 & clp & - & 12.1 & 33.1 & 6.2 & 50.8 & 40.2 & 28.5 \\
inf & 24.1 & - & 24.0 & 1.6 & 35.2 & 19.7 & 20.9 & inf & 18.9 & - & 21.4 & 1.9 & 36.3 & 21.3 & 20.0 & inf & 26.6 & - & 28.5 & 2.4 & 38.5 & 18.1 & 22.8 \\
pnt & 31.1 & 14.8 & - & 1.7 & 48.1 & 22.8 & 23.7 & pnt & 29.6 & 14.4 & - & 4.1 & 45.2 & 27.4 & 24.2 & pnt & 39.9 & 12.2 & - & 3.4 & 54.5 & 36.2 & 29.2 \\
qdr & 8.5 & 2.1 & 4.6 & - & 7.9 & 7.1 & 6.0 & qdr & 11.8 & 1.2 & 4.0 & - & 9.4 & 9.5 & 7.2 & qdr & 17.8 & 1.0 & 3.6 & - & 9.2 & 8.3 & 8.0 \\
rel & 39.4 & 17.8 & 41.2 & 1.5 & - & 25.2 & 25.0 & rel & 36.4 & 18.3 & 40.9 & 3.4 & - & 24.6 & 24.7 & rel & 48.6 & 13.2 & 49.7 & 3.6 & - & 33.9 & 29.8 \\
skt & 37.3 & 12.6 & 27.2 & 4.1 & 34.5 & - & 23.1 & skt & 38.2 & 14.7 & 33.9 & 7.0 & 36.6 & - & 26.1 & skt & 54.9 & 12.8 & 42.3 & 5.4 & 51.3 & - & 33.3 \\
Avg. & 28.1 & 12.5 & 24.5 & 2.4 & 34.1 & 21.2 & \cellcolor{lightgray}{20.5} & Avg. & 27.0 & 12.4 & 25.7 & 5.1 & 34.3 & 22.6 & \cellcolor{lightgray}{21.2} & Avg. & 37.6 & 10.3 & 31.4 & 4.2 & 40.9 & 27.3 & \cellcolor{lightgray}{25.3} \\ 
\hline
SWD & clp & inf & pnt & qdr & rel & skt & Avg. & CGDM & clp & inf & pnt & qdr & rel & skt & Avg. & Base-S & clp & inf & pnt & qdr & rel & skt & Avg. \\ 
\hline
clp & - & 14.7 & 31.9 & 10.1 & 45.3& 36.5 & 27.7 & clp & - & 16.9 & 35.3 & 10.8 & 53.5 & 36.9 & 30.7 & clp & - & 21.2 & 44.2 & 15.3 & 59.9 & 46.0 & 37.3 \\
inf & 22.9 & - & 24.2 & 2.5 & 33.2 & 21.3 & 20.0 & inf & 27.8 & - & 28.2 & 4.4 & 48.2 & 22.5 & 26.2 & inf & 36.8 & - & 39.4 & 5.4 & 52.1 & 32.6 & 33.3\\
pnt & 33.6 & 15.3 & - & 4.4 & 46.1 & 30.7 & 26.0 & pnt & 37.7 & 14.5 & - & 4.6 & 59.4 & 33.5 & 30.0  & pnt & 47.1 & 21.7 & - & 5.7 & 60.2 & 39.9 & 34.9\\
qdr & 15.5 & 2.2 & 6.4 & - & 11.1 & 10.2 & 9.1 & qdr &  14.9 & 1.5 & 6.2 & - & 10.9 & 10.2 & 8.7& qdr & 25.0 & 3.3 & 10.4 & - & 18.8 & 14.0 & 14.3\\
rel & 41.2 & 18.1 & 44.2 & 4.6 & - & 31.6 & 27.9 & rel & 49.4 & 20.8 & 47.2 & 4.8 & - & 38.2 & 32.0  & rel & 54.8 & 23.9 & 52.6 & 7.4 & - & 40.1 & 35.8\\
skt & 44.2 & 15.2 & 37.3 & 10.3 & 44.7 & - & 30.3 & skt & 50.1 & 16.5 & 43.7 & 11.1 & 55.6 & - & 35.4 & skt & 55.6 & 18.6 & 42.7 & 14.9 & 55.7 & - & 37.5\\
Avg. & 31.5 & 13.1 & 28.8 & 6.4 & 36.1 & 26.1 & \cellcolor{lightgray}{23.6} & Avg. & 36.0 & 14.0 & 32.1 & 7.1 & 45.5 & 28.3 & \cellcolor{lightgray}{27.2}  & Avg. & 43.9 & 17.7 & 37.9 & 9.7 & 49.3 & 34.5 & \cellcolor{lightgray}{32.2} \\
\hline
Ours-S & clp & inf & pnt & qdr & rel & skt & Avg. & Base-B & clp & inf & pnt & qdr & rel & skt & Avg. & Ours-B & clp & inf & pnt & qdr & rel & skt & Avg. \\ 
\hline
clp & - & 25.3 & 52.5 & 23.2 & 68.3 & 53.2 & 44.5& clp & - & 24.2 & 48.9 & 15.5 & 63.9 & 50.7 & 40.6 & clp & - & 29.4 & 57.2 & 26.0 & 72.6 & 58.1 & 48.7 \\
inf & 47.6 & - & 48.3 & 9.9 & 62.8 & 41.1 & 41.9 & inf & 43.5 & - & 44.9 & 6.5 & 58.8 & 37.6 & 38.3 & inf & 57.0 & - & 54.4 & 12.8 & 69.5 & 48.4 & 48.4 \\
pnt & 55.4 & 24.5 & - & 11.7 & 67.4 & 48.0 & 41.4 & pnt & 52.8 & 23.3 & - & 6.6 & 64.6 & 44.5 & 38.4 & pnt & 62.9 & 27.4 & - & 15.8 & 72.1 & 53.9 & 46.4 \\
qdr & 36.6 & 5.3 & 19.3 & - & 33.8 & 22.7 & 23.5 & qdr & 31.8 & 6.1 & 15.6 & - & 23.4 & 18.9 & 19.2 & qdr & 44.6 & 8.9 & 29.0 & - & 42.6 & 28.5 & 30.7 \\
rel & 61.5 & 28.1 & 56.8 & 12.8 & - & 47.2 & 41.3 & rel & 58.9 & 26.3 & 56.7 & 9.1 & - & 45.0 & 39.2 & rel & 66.2 & 31.0 & 61.5 & 16.2 & - & 52.9 & 45.6 \\
skt & 64.3 & 26.1 & 53.2 & 23.9 & 66.2 & - & 46.7 & skt & 60.0 & 21.1 & 48.4 & 16.6 & 61.7 & - & 41.6 & skt & 69.0 & 29.6 & 59.0 & 27.2 & 72.5 & - & 51.5 \\
Avg. & 53.1 & 21.9 & 46.0 & 16.3 & 59.7 & 42.4 & \cellcolor{lightgray}{39.9} & Avg. & 49.4 & 20.2 & 42.9 & 10.9 & 54.5 & 39.3 & \cellcolor{lightgray}{36.2} & Avg. & 59.9 & 25.3 & 52.2 & 19.6 & 65.9 & 48.4 & \cellcolor{lightgray}{\textbf{45.2}} \\
\hline
\end{tabular}}
\vspace{-2mm}
\caption{Comparison with SoTA methods on DomainNet. ``Base'' is the Baseline. 
}
\label{tab:domainnet}
\vspace{-3mm}
\end{table*}

For Office-Home, Office-31 and DomainNet, as most of the methods use ResNet-50 as their backbones,  we provide results with DeiT-small as our backbone for a fair comparison, which has a comparable model size as ResNet-50, but we also show the results using DeiT-base. And for VisDA-2017, we adopt the DeiT-base backbone for fair comparisons, where other methods utilize ResNet-101 for their results.

The ``Baseline-S/B'' indicates directly training a DeiT-small/DeiT-base on the source domain and testing on the target domain.
The baseline shows a competitive result even compared to other SoTA methods on most datasets.
It demonstrates that Transformers has better generalization ability over ConvNets. We also provide some insights about why transformers can generalize well from source domain to target domain in the supplementary materials.
To further eliminate the unfairness of using different backbones, we reproduce the results of SHOT and CGDM (marked as ``*''), and replace their backbones with DeiT-base as the same as ours, denoted as ``-B*''.

From Table~\ref{tab:visda},~\ref{tab:officehome},~\ref{tab:office31} and ~\ref{tab:domainnet}, it can be seen that our method outperforms the baseline with a large margin on all four datasets, \textit{e.g.} nearly 21\% on VisDA. 
With our improvements, the new Transformer with cross-attention module shows a much better generalization power, and achieves the best performance on VisDA-2017 compared to other SoTAs methods. It further implies the effectiveness of our method on the UDA task. 


Taking a closer look at the results, for the hard categories, such as ``person'' in VisDA-2017 dataset, the baseline is very low, which indicates the initial model of our method has a poor classification ability on this category, leading to the pseudo labels with  more noise. Even with such a poor baseline and poor quality of pseudo labels, our method can still achieve a much higher performance boost (from 10.3\% to 88.6\%). It suggests that our method has a great robustness to the labeling noise and can overcome the noise problem to some extent. 



We can see that TVT achieves a better result on Office-Home and Office-31. Because TVT utilizes ViT~\citep{dosovitskiy2020image} as backbone which is pretrained on ImageNet21K. While the pretrained model of our CDTrans and other UDA methods are trained on ImageNet1K.



\subsection{Ablation Study}


\subsubsection{Different Pseudo Labeling}
We have conducted experiments on different pseudo labeling methods to verify their influence on the final performance.
The results on VisDa-2017 are listed in Table.~\ref{tab:pseudoexpm}.
RPLL~\citep{zheng2021rectifying} and MRKLD+LRENT~\citep{zou2019confidence} are two commonly used pseudo-label generation methods, we reproduce their pseudo-label generations on our baseline to compare with our proposed pseudo labeling method.
$Rec_s,Rec_t$ means the recall of the selected training pairs in the source and target data, while $Prec$ represents the accuracy of the pairs. 
``One-way-source'' and ``One-way-target'' denote only using the pair set $\sP_S$ in Eq.~\ref{eq:ps} or $\sP_T$ in Eq.~\ref{eq:pt} for training. ``Two-way'' indicates results of using the union of $\sP_s$ and $\sP_t$ without the center-aware strategy. ``Tw+Ca'' implies our two-way center-aware labeling method, and
``Groundtruth'' means all training pairs are from groundtruthes.

By looking at $Rec_s$ and $Rec_t$ in Table.~\ref{tab:pseudoexpm}, it can be found that the one-way methods have an apparent bias on either source or target data, and its results are lower than the two-way method. By comparing ``Two-way'' and ``Tw+Ca'', we can conclude that although the center-aware method filters the training pair and slightly reduces the recall, it largely improves the precision and leads to a better final performance. We also find that our two-way center-aware labeling method achieves a very high result, not only better than other pseudo-label generation methods, but also very close to the upper bound trained with groundtruth pairs.

\subsubsection{Different Losses}
As there are three losses in our method, we conduct another experiment to verify the effectiveness of each loss on VisDa-2017, as shown in Table.~\ref{tab:lossexpm}. 
``cls'' in $L_{s+t}$ denotes that we replace the distillation loss with a classification loss for the source-target branch.
We can see that the 3rd row with both $L_s$ and $L_t$ having classification loss achieves a better result than the first row where only $L_s$ has the cls loss, which means the target branch with the pseudo labels is helpful to improve the UDA result. With the addition of ``cls'' in $L_{s+t}$, the performance is further improved, which demonstrates the advantages of using the cross-attention module for feature alignment. Using ``dtl'' instead of ``cls'' on the source-target branch can further improve the results, showing the effectiveness of our distillation loss.

\begin{table*}[!t]\small
\setlength\tabcolsep{1.pt}
\scalebox{0.985}{
\begin{tabular}{c|ccc|cccccccccccc>{\columncolor{lightgray}}c}
\hline
Pseudo labels & Rec$_{s}$ & Rec$_{t}$ & Prec & plane & bcycl & bus & car & horse & knife & mcycl & person & plant & sktbrd & train & truck & Avg. \\
\hline
One-way-source & 100. & 6.6 & 90.6 & 96.1 & 52.7 & 85.5 & 69.6 & 95.0 & 90.2 & 95.1 & 66.6 & 88.8 & 54.6 & 95.4 & 29.5 & 76.6 \\
One-way-target & 8.0 & 100. & 76.3 & 98.2 & 32.0 & 87.7 & 84.1 & 95.5 & 89.9 & 98.3 & 66.8 & 95.7 & 57.5 & 95.6 & 22.0 & 76.9 \\
Two-way & 100. & 100. & 81.8 & 97.5 & 49.6 & 88.7 & 73.9 & 94.6 & 85.8 & 96.6 & 58.6 & 93.3 & 63.6 & 94.8 & 27.9 & 77.1 \\
Tw + Ca & 97.8 & 94.8 & 91.3 & 98.1 & 86.9 & 87.9 & 80.9 & 97.9 & 97.3 & 96.8 & 85.3 & 97.6 & 83.2 & 94.0 & 54.4 & 88.4 \\
RPLL & - & - & - & 98.4 & 63.4 & 85.8 & 68.8 & 97.0 & 95.4 & 97.77 & 59.3 & 96.2 & 57.2 & 96.2 & 48.1 & 80.3 \\
MRKLD+LRENT & - & - & - & 97.8 & 77.3 & 81.4 & 64.3 & 94.6 & 93.9 & 93.3 & 77.5 & 93.1 & 74.9 & 92.6 & 59.0 & 83.3 \\
Groundtruth & 100. & 100. & 100. & 97.9 & 89.1 & 92.3 & 91.9 & 98.4 & 97.2 & 97.5 & 86.8 & 98.6 & 90.7 & 96.3 & 60.0 & 91.5\\
\hline
\end{tabular}}
\vspace{-2mm}
\caption{Comparison among different pseudo labeling methods on VisDa-2017. $Rec_s,Rec_t$ express the recall of pseudo labels in source and target data, while $Prec$ represents the accuracy of the pairs. ``One-way-source/target'' denotes only using the source/target pair set for training. ``Tw+Ca'' implies the proposed two-way center-aware labeling method.}
\vspace{-2mm}
\label{tab:pseudoexpm}
\end{table*}

\begin{table*}[!t]\small
\setlength\tabcolsep{3.4pt}
\begin{tabular}{ccc|cccccccccccc>{\columncolor{lightgray}}c}
\hline
$L_{s}$ & $L_{s+t}$ & $L_{t}$ & plane & bcycl & bus & car & horse & knife & mcycl & person & plant & sktbrd & train & truck & Avg. \\ 
\hline
\textit{cls} & - & - & 97.7 & 48.1 & 86.6 & 61.6 & 78.1 & 63.4 & 94.7 & 10.3 & 87.7 & 47.7 & 94.4 & 35.5 & 67.1 \\
- & - & \textit{cls} & 98.3 & 85.0 & 88.0 & 76.3 & 98.1 & 96.1 & 96.9 & 61.1 & 97.2 & 85.5 & 94.6 & 54.9 & 86.0 \\
\textit{cls} & - & \textit{cls} & 98.3 & 87.4 & 89.1 & 77.3 & 98.0 & 97.4 & 95.4 & 69.5 & 97.1 & 86.3 & 95.3 & 49.5 & 86.7 \\
\textit{cls} & \textit{cls} & \textit{cls} & 98.2 & 88.4 & 88.0 & 76.8 & 98.2 & 97.2 & 95.6 & 80.1 & 97.1 & 84.7 & 94.5 & 54.1 & 87.7 \\
\textit{cls} & \textit{dtl} & \textit{cls} & 98.0 & 86.9 & 87.9 & 80.9 & 97.9 & 97.3 & 96.8 & 85.3 & 97.6 & 83.2 & 94.0 & 54.4 & 88.4 \\
\hline
\end{tabular}
\vspace{-2mm}
\caption{Comparison among different losses on VisDa-2017. $L_{s}$,$L_{t}$ and $L_{s+t}$ represent the loss used in source, target and source+target branches respectively. $cls$ and $dtl$ imply the classification loss and the distillation loss.}
\vspace{-1mm}
\label{tab:lossexpm}
\end{table*}

\section{Conclusion}
In this paper, we tackle the problem of unsupervised domain adaptation by introducing the cross-attention module into Transformer in a novel way. We propose a new network structure CDTrans which is a pure transformer-based structure with three branches, and we also propose to generate high-quality pseudo labels using a two-way center-aware labeling method. Training CDTrans using the generated high-quality pseudo labels yields a robust solution and also achieves state-of-the-art results on four popular UDA datasets, outperforming previous methods by a large margin. We believe that transformer-based approaches will have great potential in the UDA community, and our work, as one of the first attempts along this direction, has pushed forward the frontiers and shed lights for future research.

\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}



\end{spacing}
\end{document}
