\newpage
\onecolumn
{\huge Appendix}


\def\E{{\bf E}}
\def\x{{\bf x}}
\def\r{{\bf r}}
\def\rhat{\hat{r}}

\section{Proofs}

\label{app:proofs}

\subsection{Proof of Theorem~\ref{thm:bijection}}
\label{app:proof-bij}
\thmbij*
\begin{proof}
Let $q_r(\rvx) \propto \int {r^D}/{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}p(\rvy)d\rvy$. We will show that $q_r\propto \int {r^D}/{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}p(\rvy)d\rvy$ is equal to the $r$-dependent marginal distribution $p_r$ by verifying (1) the starting distribution is correct when $r{=}0$; (2) the continuity equation holds, \ie $\partial_r q_r + \nabla_\rvx\cdot(q_r \mat{E}(\tilde{\mat{x}})_\rvx  /E(\tilde{\mat{x}})_{r})=0$. The starting distribution is $\lim_{r\to 0}q_r(\rvx) \propto \lim_{r\to 0}\int {r^D}/{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}p(\rvy)d\rvy\propto p(\rvx)$, which confirms that $q_r{=}p$. The continuity equation can be expressed as:
\begin{align*}
    &\partial_r q_r + \nabla_\rvx\cdot(q_r \mat{E}(\tilde{\mat{x}})_\rvx/E(\tilde{\mat{x}})_{r})\\
    &=  \partial_r\left(\int \frac{r^D}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}p(\rvy)d\rvy\right)+ \nabla_\rvx\cdot\left(\int \frac{r^D}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}{p}({\rvy}) d {\rvy}\frac{\int \frac{\tilde{\mat{x}}-\tilde{\rvy}}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}{p}({\rvy}) d {\rvy}}{\int \frac{r}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}{p}({\rvy}) d {\rvy}}\right)\\
    &=  \int \left(\frac{Dr^{D-1}}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}} - \frac{(N+D)r}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}}}\right)p(\rvy)d\rvy+\nabla_\rvx\cdot\left(r^{D-1}{\int \frac{\tilde{\mat{x}}-\tilde{\rvy}}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}{p}({\rvy}) d {\rvy}}\right)\\
    &=  \int \left(\frac{Dr^{D-1}}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}} - \frac{(N+D)r}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}}}\right)p(\rvy)d\rvy+\nabla_\rvx\cdot\left(r^{D-1}{\int \frac{\tilde{\mat{x}}-\tilde{\rvy}}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}{p}({\rvy}) d {\rvy}}\right)\\
    &=\int \left(\frac{Dr^{D-1}}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}} - \frac{(N+D)r}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}}}\right)p(\rvy)d\rvy\\
    &\qquad +r^{D-1}\sum_{i=1}^N{\int \frac{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D} - \|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}(\rvx_i -\rvy_i)^2(N+D)}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{2(N+D)}}{p}({\rvy}) d {\rvy}}\\
    &=\int \left(\frac{Dr^{D-1}}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}} - \frac{(N+D)r^{D+1}}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}}}\right)p(\rvy)d\rvy\\
    &\qquad +r^{D-1}{\int \frac{N\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D} - \|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}\|\rvx-\rvy\|^2(N+D)}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{2(N+D)}}{p}({\rvy}) d {\rvy}}\\
    &=r^{D-1}\int \frac{\|\tilde{\mat{x}}{-}\tilde{\rvy}\|^{N{+}D}D-(N{+}D)r^{2}\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N{+}D{-}2}+N\|\tilde{\mat{x}}{-}\tilde{\rvy}\|^{N{+}D}-\|\tilde{\mat{x}}{-}\tilde{\rvy}\|^{N{+}D{-}2}\|\rvx{-}\rvy\|^2(N{+}D)}{{\|\tilde{\mat{x}}{-}\tilde{\rvy}\|^{2(N{+}D)}}}p(\rvy)d\rvy\\
    &=r^{D-1}\int \frac{(N+D)(\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}-\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}\|\rvx-\rvy\|^2)-(N+D)r^{2}\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{2(N+D)}}}p(\rvy)d\rvy\\
    &=r^{D-1}\int \frac{(N+D)r^{2}\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}-(N+D)r^{2}\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D-2}}{{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{2(N+D)}}}p(\rvy)d\rvy\\
    &=0
\end{align*}
It means that $q_r$ satisfies the continuity equation for any $r\in \R_{\ge 0}$. Together, we conclude that $q_r=p_r$. Lastly, note that the terminal distribution is 
\begin{align*}
    \lim_{\rmax\to \infty}p_{\rmax}(\rvx) &\propto \lim_{\rmax\to \infty} \int \frac{\rmax^D}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}p(\rvy)d\rvy=\lim_{\rmax\to \infty}\int \frac{\rmax^D}{(\|\rvx-\rvy\|^2+\rmax^2)^\frac{N+D}{2}}p(\rvy)d\rvy\\
    &= \lim_{\rmax\to \infty}\frac{\rmax^D}{(\|\rvx\|^2+\rmax^2)^\frac{N+D}{2}}  + \lim_{\rmax\to \infty}\int \left(\frac{\rmax^D}{(\|\rvx-\rvy\|^2+\rmax^2)^\frac{N+D}{2}}-\frac{\rmax^D}{(\|\rvx\|^2+\rmax^2)^\frac{N+D}{2}}\right)p(\rvy)d\rvy\\
    &= \lim_{\rmax\to \infty}\frac{\rmax^D}{(\|\rvx\|^2+\rmax^2)^\frac{N+D}{2}} \qquad \text{($p$ has a compact support)}
\end{align*}
\end{proof}

\subsection{Proof of Theorem~\ref{thm:minimizer}}
\label{app:thm-minimizer}
\thmmin*
\begin{proof}
The minimizer at $\tx$ in \Eqref{eq:new-D-aug} is
\begin{align*}
    f^*_{\theta}(\tilde{\rvx}) &= \int p_r({\rvy}|\rvx) ({{\tx}-\tilde{\rvy}} )d\tilde{\rvy}= \frac{\int p_r(\rvx|{{\rvy}})({\tilde{\rvx}-\tilde{\rvy}})p({\rvy}) d{\rvy}}{p_r(\rvx)}\numberthis \label{eq:minimizer-D}
\end{align*}
The choice of perturbation kernel is
\begin{align*}
    p_r(\rvx|\rvy) \propto \frac{1}{\|\tx - \tilde{\rvy}\|^{N+D}} =  \frac{1}{({\|\rvx-{\rvy}\|_2^2+ r^2})^\frac{N+D}{2}}
\end{align*}
By substituting the perturbation kernel in \Eqref{eq:minimizer-D}, we have:
\begin{align*}
    f^*_{\theta}(\tilde{\rvx})  &=\frac{\int \frac{{\tilde{\rvx}-\tilde{\rvy}}}{({\|\rvx-{\rvy}\|_2^2+ r^2})^\frac{N+D}{2}} p({\rvy}) d{\rvy}}{p_r(\rvx)}\\
    &= \frac{\int \frac{{\tilde{\rvx}-\tilde{\rvy}}}{{\|\tx-\tilde{\rvy}\|_2}^{N+D}} p({\rvy}) d{\rvy}}{p_r(\rvx)}\\
    &= ({S_{N+D-1}(1)}/{p_r(\rvx)})\mat{E}(\tx)
\end{align*}
\end{proof}

\subsection{Proof of Theorem~\ref{thm:inf-D-field}}
\label{app:proof-thmfield}
\thmfield*
\begin{proof}
The $\rvx$ component in the Poisson field can be re-expressed as
\begin{align*}
    \mat{E}({\tx})_{\rvx} &= \frac{1}{S_{N+D-1}(1)}\int \frac{{\mat{x}}-{\rvy}}{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}{p}({\rvy}) d {\rvy} \\
    &\propto \int p_r(\rvx|\rvy)(\rvx-\rvy)p(\rvy)d\rvy
\end{align*}
 where the perturbation kernel $p_r({\rvx}|\rvy)  \propto  {1}/{({\|\rvx-{\rvy}\|_2^2+ r^2})^\frac{N+D}{2}}$. The direction of the score can also be written down in a similar form: 
 \begin{align*}
     \nabla_\rvx \log p_{\sigma}(\rvx)=\frac{\int p_\sigma(\rvx|\rvy)\frac{\rvy-\rvx}{\sigma^2}p(\rvy)d\rvy}{p_\sigma(\rvx)}\propto \int p_\sigma(\rvx|\rvy)(\rvx-\rvy)p(\rvy)d\rvy
 \end{align*}
where $ p_{\sigma}(\rvx|\rvy) \propto \exp-\frac{\|\rvx-{\rvy}\|_2^2}{ 2\sigma^2}$. Since $p\in \gC^1$, and obviously $p_r(\rvx|\rvy) \in C^1$, then $\lim_{D\to \infty}\int p_r(\rvx|\rvy)(\rvx-\rvy)p(\rvy)d\rvy=\int \lim_{D\to \infty}p_r(\rvx|\rvy)(\rvx-\rvy)p(\rvy)d\rvy$. It suffices to prove that the perturbation kernel $p_r({\rvx}|\rvy)$ point-wisely converge to the Gaussian kernel $p_{\sigma}(\rvx|\rvy)$, \ie $\lim_{D\to \infty}p_r({\rvx}|\rvy) = p_\sigma({\rvx}|\rvy) $, to ensure $\mat{E}({\mat{x}})_{\rvx} \propto \nabla_\rvx \log p_{\sigma}(\rvx)$. Given $\forall \rvx,\rvy \in \mathbb{R}^{N}$, 
\begin{align*}
    \lim_{D\to \infty}p_r({\rvx}|\rvy) &\propto \lim_{D\to \infty}\frac{1}{({\|\rvx-\rvy\|_2^2+ r^2})^\frac{N+D}{2}}\\
    &=\lim_{D\to \infty}{({\|\rvx-\rvy\|_2^2+ r^2})^{-\frac{N+D}{2}}}\\
    &\propto\lim_{D\to \infty}{({1+\frac{\|\rvx-\rvy\|_2^2}{ r^2}})^{-\frac{N+D}{2}}}\\
    &=\lim_{D\to \infty}{({1+\frac{\|\rvx-\rvy\|_2^2}{ D\sigma^2}})^{-\frac{N+D}{2}}}\qquad \tag{$r=\sigma\sqrt{D}$}\\
    &= \lim_{D\to \infty}\exp\left(-\frac{N+D}{2} {\rm ln}(1+\frac{\|\rvx-\rvy\|_2^2}{ D\sigma^2}) \right)\\
    &= \lim_{D\to \infty}\exp\left(-\frac{N+D}{2} \frac{\|\rvx-\rvy\|_2^2}{ D\sigma^2} \right) \qquad \tag{ $\lim_{D\to \infty}\frac{\|\rvx-\rvy\|_2^2}{D\sigma^2}= 0 $}\\
   &= \exp-\frac{\|\rvx-\rvy\|_2^2}{ 2\sigma^2}\\
   &\propto  p_\sigma({\rvx}|\rvy) 
\end{align*}    
Hence $\lim_{D\to \infty}p_r({\rvx}|\rvy)=p_\sigma({\rvx}|\rvy)$, and we establish that $\mat{E}(\tx)_{\rvx} \propto \nabla_\rvx \log p_{\sigma}(\rvx)$. We can rewrite the drift term in the PFGM++ ODE as 
\begin{align*}
\lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\sqrt{D}\mat{E}(\tx)_{\rvx} /E(\tx)_{r}&=\lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\frac{\sqrt{D}\int p_r(\rvx|\rvy)(\rvx-\rvy)p(\rvy)d\rvy}{\int p_r(\rvx|\rvy)(-r)p(\rvy)d\rvy}\\
&=\lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\frac{\sqrt{D}\int p_r(\rvx|\rvy)(\rvy-\rvx)p(\rvy)d\rvy}{rp_r(\rvx)}\\
&= \lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\frac{\sqrt{D}\int p_\sigma(\rvx|\rvy)(\rvy-\rvx)p(\rvy)d\rvy}{rp_\sigma(\rvx)}\\
&= {\sigma \nabla_\rvx\log p_{\sigma}(\rvx)}\qquad\qquad \text{($\nabla_\rvx \log p_{\sigma}(\rvx)=\frac{\int p_\sigma(\rvx|\rvy)\frac{\rvy-\rvx}{\sigma^2}p(\rvy)d\rvy}{p_\sigma(\rvx)}$)}\numberthis \label{eq:EEinv}
\end{align*}
which establishes the first part of the theorem. For the second part, by the change-of-variable $d\sigma = dr/\sqrt{D}$, the PFGM++ ODE is 
\begin{align*}
    \lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\frac{\mathrm{d}\rvx}{\mathrm{d}\sigma} &= \frac{\mathrm{d}\rvx}{\mathrm{d}r} \cdot \frac{\mathrm{d}r}{\mathrm{d}\sigma}\\
    &=\lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\mat{E}(\tx)_{\rvx} \cdot E(\tx)_{r}^{-1} \cdot \sqrt{D}\\
    &= \lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\frac{\sigma \nabla_\rvx\log p_{\sigma}(\rvx)}{\sqrt{D}}\cdot \sqrt{D}\qquad \text{(by \Eqref{eq:EEinv})}\\
    &= {\sigma \nabla_\rvx\log p_{\sigma}(\rvx)}
\end{align*}
which is equivalent to the diffusion ODE.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:obj}}
\label{app:proof-propobj}
\propobj*
\begin{proof}
    For $\forall \rvx \in \mathbb{R}^N$, the minimizer in PFGM++ objective~(\Eqref{eq:pfgmpp-obj}) at point $\tx=(\rvx, r)$ is
\begin{align*}
    f^*_{\theta,\textrm{PFGM++}}(\tilde{\rvx}) &= \lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\frac{\int p_r(\rvx|{{\rvy}})\frac{{{\rvx}-{\rvy}}}{r/\sqrt{D}}p({\rvy}) d{\rvy}}{p_r(\rvx)}\\
    &= \lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}\frac{\int p_\sigma(\rvx|{{\rvy}})\frac{{{\rvx}-{\rvy}}}{r/\sqrt{D}}p({\rvy}) d{\rvy}}{p_\sigma(\rvx)} \qquad \tag{By Theorem~\ref{thm:inf-D-field}, $\lim_{D\to \infty}p_r(\rvx|{{\rvy}})=p_\sigma(\rvx|{{\rvy}})$}\\
    &= \frac{\int p_\sigma(\rvx|{{\rvy}})\frac{{{\rvx}-{\rvy}}}{\sigma}p({\rvy}) d{\rvy}}{p_\sigma(\rvx)}\numberthis \label{eq:minimizer-pfgmpp}
\end{align*}

On the other hand, the minimizer in denoising score matching at point $\rvx$ in noise level $\sigma=r/\sqrt{N+D}$ is
\begin{align*}
    f^*_{\theta,\textrm{DSM}}({\rvx},\sigma) = \frac{\int p_\sigma(\rvx|{{\rvy}})\frac{{{\rvx}-{\rvy}}}{\sigma}p({\rvy}) d{\rvy}}{p_\sigma(\rvx)}\numberthis \label{eq:minimizer-diffusion}
\end{align*}

Combining \Eqref{eq:minimizer-pfgmpp} and \Eqref{eq:minimizer-diffusion}, we have 
\begin{align*}
\lim_{\substack{D\to \infty\\r=\sigma\sqrt{D}}}f^*_{\theta,\textrm{PFGM++}}(\rvx, \sigma\sqrt{N+D}) =  f^*_{\theta,\textrm{DSM}}(\rvx, \sigma)
\end{align*}

\end{proof}






















\section{Practical Sampling Procedures of Perturbation Kernel and Prior Distribution}
\label{app:sample-prior}
In this section, we discuss how to simple from the perturbation kernel $p_{r}(\rvx|{\rvy}) \propto {1}/{({\|\rvx-{\rvy}\|_2^2+ r^2})^\frac{N+D}{2}}$ in practice. We first decompose $p_r(\cdot|\rvy)$ in hyperspherical coordinates to $\gU_{\psi}(\psi)p_r(R)$, where $\gU_{\psi}$ is the uniform distribution over the angle component and the distribution of the perturbed radius $R=\|\rvx - \rvy\|_2$ is
\begin{align*}
    p_r(R) \propto \frac{R^{N-1}}{({R^2+ r^2})^\frac{N+D}{2}} \numberthis \label{eq:propto}
\end{align*}
The sampling procedure of the radius distribution encompasses three steps:
\begin{align*}
    &R_1 \sim \textrm{Beta}(\alpha=\frac{N}{2}, \beta=\frac{D}{2})\\
    &R_2=\frac{R_1}{1-R_1}\\
    &R_3= \sqrt{r^2 R_2}
\end{align*}
Next, we prove that $p(R_3) = p_r(R_3)$. Note that the pdf of the inverse beta distribution is
\begin{align*}
    p(R_2) \propto R_2^{\frac{N}{2}-1}(1+R_2)^{-\frac{N+D}{2}}
\end{align*}
By change-of-variable, the pdf of $R_3=\sqrt{r_{max}^2 R_2}$ is
\begin{align*}
    p(R_3) &\propto R_2^{{\frac{N}{2}-1}}(1+R_2)^{-\frac{N}{2}-\frac{D}{2}}*\frac{2R_3}{r_{max}^2}\\
    &\propto\frac{R_3R_2^{\frac{N}{2}-1}}{(1+R_2)^{\frac{N+D}{2}}}\\
    &= \frac{(R_3/r)^{N-1}}{(1+(R_3^2/r^2))^{\frac{N+D}{2}}}\\
    &\propto  \frac{R_3^{N-1}}{(1+(R_3^2/r^2))^{\frac{N+D}{2}}}\\
    &\propto  \frac{R_3^{N-1}}{(r^2+R_3^2)^{\frac{N+D}{2}}} \propto p_{r}(R_3)\qquad \textrm{(By \Eqref{eq:propto})}
\end{align*}

Note that $R_1$ has mean $\frac{N}{N+D}$ and variance $O(\frac{ND}{(N+D)^3})$. Hence when $D=O(N)$,  $p_{r}(R)$ would highly concentrate on a specific value, resolving the heavy-tailed problem. We can sample the uniform angel component by $\rvu = \rvw/\|\rvw\|, \rvw \sim \gN(\mathbf{0}, \mI_{N \times N})$. Together, sampling from the perturbation kernel $p_{r}(\rvx|{\rvy}) $ is equivalent to setting $\rvx = \rvy + R_3\rvu$. On the other hand, the prior distribution is 
\begin{align*}
    p_{\rmax}(\rvx) \propto \lim_{\rmax\to \infty} \int {\rmax^D}/{\|\tilde{\mat{x}}-\tilde{\rvy}\|^{N+D}}p(\rvy)d\rvy= \lim_{\rmax\to \infty}{\rmax^D}/{(\|\rvx\|^2+\rmax^2)^\frac{N+D}{2}}
\end{align*}
We observe that $p_{\rmax}(\rvx)$ the same as the perturbation kernel $p_{\rmax}(\rvx|{\rvy}=\bf{0})$. Hence we can sample from the prior following $\rvx = R_3\rvu$ with $R_3, \rvu$ defined above and $r=\rmax$.


\begin{comment}







\end{comment}

\section{$r=\sigma\sqrt{D}$ for Phase Alignment}
\subsection{Analysis}
\label{app:phase-align}



In this section, we examine the phase of intermediate marginal distribution $p_r$ under different $D$s to derive an alignment method for hyper-parameters. Consider a $N$-dimensional dataset $\gD$ in which the average distance to the nearest neighbor is about $l$. We consider an arbitrary datapoint $\rvx_1 \in \gD$ and denote its nearest neighbor as $\rvx_2$. We assume $\|\rvx_1 - \rvx_2\|_2=l$, and uniform prior on $\gD$.


To characterize the phases of $p_r, \forall r>0$, we study the perturbation point $\rvy \sim p_r(\rvy|\rvx_1)$. According to Appendix~\ref{app:sample-prior}, the distance $\|\rvx_1-\rvy\|$ is roughly $r\sqrt{\frac{N}{D-1}}$. Since $p_r(\rvy|\rvx_1)$ is isotropic, with high probability, the two vectors $\rvy-\rvx_1, \rvx_2-\rvx_1$ are approximately orthogonal. In particular, the vector product $(\rvy - \rvx_1)^T(\rvx_1 - \rvx_2)= O(\frac{1}{\sqrt{N}}\|\rvy - \rvx_1\|\|\rvx_1 - \rvx_2\|)=O(\frac{rl}{\sqrt{D}})$ w.h.p. It reveals that $\|\rvy-\rvx_2\| = \sqrt{l^2+r^2\frac{N}{D-1}+O(\frac{rl}{\sqrt{D}})}$. \Figref{fig:align-analysis} depicts the relative positions of $\rvx_1, \rvx_2$ and the perturbed point $\rvy$.

\begin{figure*}[t]
\centering    \includegraphics[width=0.8\textwidth]{img/align_plot.png}
    \caption{Illustration of the phase alignment analysis}
    \label{fig:align-analysis}
\end{figure*}

The ratio of the posterior of the $\rvx_2$ and $\rvx_1$  --- $\frac{p_r(\rvx_2|\rvy)}{p_r(\rvx_1|\rvy)}$ ---  is an indicator of different phases of field~\cite{Xu2023StableTF}: point in the nearer field tends to have a smaller ratio. Indeed, the ratio would gradually decay from $1$ to $0$ when moving from the far to the near field.  We can calculate the ratio of the coefficients after approximating the distance $\|\rvy-\rvx_2\|$:
\begin{align*}
    \frac{p_r(\rvx_2|\rvy)}{p_r(\rvx_1|\rvy)}=\frac{p_r(\rvy|\rvx_2)}{p_r(\rvy|\rvx_1)} &=  \left(\frac{l^2+r^2\frac{N}{D-1}+O(\frac{rl}{\sqrt{D}})+r^2}{r^2\frac{N}{D-1}+r^2}\right)^{\frac{N+D}{2}}\\
    &=\left(1 + \frac{l^2+O(\frac{rl}{\sqrt{D}})}{r^2\frac{N}{D-1}+r^2}\right)^{\frac{N+D}{2}}\\
    & = \exp{\left({\rm ln}(1+\frac{l^2+O(\frac{rl}{\sqrt{D}})}{r^2\frac{N}{D-1}+r^2})\cdot \frac{N+D}{2}\right)}\\
    & \approx \exp{\left(\frac{l^2+O(\frac{rl}{\sqrt{D}})}{r^2\frac{N}{D-1}+r^2} \cdot\frac{N+D}{2}\right)}\\
    &= \exp{\left(\frac{l^2+O(\frac{rl}{\sqrt{D}})}{r^2} \cdot\frac{N+D}{2(N+D-1)}\cdot(D-1)\right)}\\
    &\approx \exp{\left(\frac{l^2+O(\frac{rl}{\sqrt{D}})}{r^2}\cdot D\right)}\numberthis \label{eq:ratio}
\end{align*}

Hence the relation $r\propto \sqrt{D}$ should hold to keep the ratio invariant of the parameter $D$. On the other hand, by Theorem~\ref{thm:inf-D-field} we know that $p_{\sigma}$ is equivalent to $p_{r=\sigma\sqrt{D}}$ when $D\to \infty$. To achieve phase alignment on the dataset, one should roughly set $r=\sigma\sqrt{D}$.

\subsection{Practical Hyperparameter Transfer from Diffusion Models}
\label{app:transfer-diff}

\subsubsection{Transfer EDM training and sampling}

We list out and compare the EDM training algorithm~(Alg~\ref{algorithm-edm}) and the PFGM++ with transferred hyper-parameters~(Alg~\ref{algorithm-edm-pfgmpp}). The major modification is to replace the Gaussian noise $\rvn_i\sim \gN(0, \sigma^2 \mI)$ with the additive noise $R_i \rvv_i\sim \gU_{\psi}(\psi)p_r(R)$, where $r=\sigma\sqrt{D}$. We highlight the major modifications in \textcolor{blue}{blue}.

We also show the sampling algorithms of EDM~(Alg~\ref{algorithm-edm-sample}) and PFGM++~(Alg~\ref{algorithm-edm-pfgmpp-sample}). Note that we only change the prior sampling process while the for-loop is identical for both algorithms, since EDM~\citep{Karras2022ElucidatingTD} sets $\sigma=t$, and $\frac{\mathrm{d}\rvx}{\mathrm{d}r}=\frac{\rvx-f_{\theta}(\rvx, r)}{r}=\frac{\rvx-f_{\theta}(\rvx, r)}{\sigma\sqrt{D}}=\frac{\mathrm{d}\rvx}{\sqrt{D}\mathrm{d}\sigma}=\frac{\mathrm{d}\rvx}{\mathrm{d}\sigma}\frac{\mathrm{d}\sigma}{\mathrm{d}r}=\frac{\mathrm{d}\rvx}{\mathrm{d}\sigma}=\frac{\mathrm{d}\rvx}{\mathrm{d}t}$. Thus we can use the original samplers of EDM without further modification.

\begin{minipage}{0.46\textwidth}
\vspace{-46pt}
\begin{algorithm}[H]
    \centering
    \caption{EDM training}\label{algorithm-edm}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data $\{\rvy_i\}_{i=1}^\gB$ from $p(\rvy)$
        \STATE Sample standard deviations $\{\sigma_i\}_{i=1}^\gB$ from $p(\sigma)$ 
        \STATE Sample noise vectors $\{\rvn_i \sim \gN(0, \sigma_i^2 \mI)\}_{i=1}^\gB$
        \STATE Get perturbed data $\{\hat{\rvy}_i = \rvy_i + \rvn_i\}_{i=1}^\gB$
        \STATE Calculate loss $\ell(\theta) =  \sum_{i=1}^\gB \lambda (\sigma_i)\|f_\theta(\hat{\rvy}_i, \sigma_i)-\rvy_i\|_2^2$
        \STATE Update the network parameter $\theta$ via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ training with hyperparameter transferred from EDM}\label{algorithm-edm-pfgmpp}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data $\{\rvy_i\}_{i=1}^\gB$ from $p(\rvy)$
        \STATE Sample standard deviations $\{\sigma_i\}_{i=1}^\gB$ from $p(\sigma)$ 
        \STATE \textcolor{blue}{Sample $r$ from $p_r$: $\{r_i = \sigma_i\sqrt{D}\}_{i=1}^\gB$}
        \STATE  \textcolor{blue}{Sample radiuses $\{R_i \sim p_{r_i}(R)\}_{i=1}^\gB$}
        \STATE  \textcolor{blue}{Sample uniform angles $\{\rvv_i =\frac{\rvu_i}{\|\rvu_i\|_2}\}_{i=1}^\gB$, with $\rvu_i \sim \gN(\bf{0}, \mI)$}
        \STATE \textcolor{blue}{Get perturbed data $\{\hat{\rvy}_i = \rvy_i + R_i\rvv_i\}_{i=1}^\gB$}
        \STATE {Calculate loss $\ell(\theta) =  \sum_{i=1}^\gB \lambda (\sigma_i)\|f_\theta(\hat{\rvy}_i, \sigma_i)-\rvy_i\|_2^2$}
        \STATE Update the network parameter $\theta$ via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{minipage}{0.46\textwidth}
\vspace{-47pt}
\begin{algorithm}[H]
    \centering
    \caption{EDM sampling~(Heun’s $2^{\textrm{nd}}$ order method)}\label{algorithm-edm-sample}
    \begin{algorithmic}[1]
\STATE $\rvx_0 \sim \gN(\bm{0}, \sigma_{\textrm{max}}^2\mI)$
\FOR{$i=0, \dots, T-1$}
\STATE $\rvd_i = ({\rvx_i -f_{\theta}(\rvx_i,t_i)})/{t_i}$
\STATE $\rvx_{i+1} =\rvx_i + (t_{i+1}-t_i)\rvd_i$
\IF{$t_{i+1}>0$}
\STATE $\rvd_i' = ({\rvx_{i+1} -f_{\theta}(\rvx_{i+1},t_{i+1})})/{t_{i+1}}$
\STATE $\rvx_{i+1}=\rvx_i + (t_{i+1}-t_i)(\frac12\rvd_i+\frac12\rvd_i')$
\ENDIF
\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ training with hyperparameter transferred from EDM}\label{algorithm-edm-pfgmpp-sample}
    \begin{algorithmic}[1]
 \STATE \textcolor{blue}{Set  $\rmax = \sigma_{\textrm{max}}\sqrt{D}$}
        \STATE  \textcolor{blue}{Sample radius $R \sim p_{\rmax}(R)$
       and uniform angle $\rvv =\frac{\rvu}{\|\rvu\|_2}$, with $\rvu \sim \gN(\bf{0}, \mI)$}
        \STATE \textcolor{blue}{Get initial data $\rvx_0 = R\rvv$}
\FOR{$i=0, \dots, T-1$}
\STATE $\rvd_i = ({\rvx_i -f_{\theta}(\rvx_i,t_i)})/{t_i}$
\STATE $\rvx_{i+1} =\rvx_i + (t_{i+1}-t_i)\rvd_i$
\IF{$t_{i+1}>0$}
\STATE $\rvd_i' = ({\rvx_{i+1} -f_{\theta}(\rvx_{i+1},t_{i+1})})/{t_{i+1}}$
\STATE $\rvx_{i+1}=\rvx_i + (t_{i+1}-t_i)(\frac12\rvd_i+\frac12\rvd_i')$
\ENDIF
\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\subsubsection{Transfer DDPM~(continuous) training and sampling}

Here we demonstrate the ``zero-shot" transfer of hyperparameters from DDPM to PFGM++, using the $r=\sigma\sqrt{D}$ formula. We highlight the modifications in \textcolor{blue}{blue}. In particular, we list the DDPM training/sampling algorithms~(Alg~\ref{algorithm-ddpm}/Alg~\ref{algorithm-ddpm-sample}), and their counterparts in PFGM++~(Alg~\ref{algorithm-ddpm-pfgmpp}/Alg~\ref{algorithm-ddpm-pfgmpp-sample}) for comparions. Let $\beta_T$ and $\beta_1$ be the maximum/minimum values of $\beta$ in DDPM~\cite{Ho2020DenoisingDP}. Similar to \citet{Song2021ScoreBasedGM}, we denote $\alpha_{t}=e^{-\frac12t^2(\bar{\beta}_{\textrm{max}}-\bar{\beta}_{\textrm{min}}) - t\bar{\beta}_{\textrm{min}}}$, with $\bar{\beta}_{\textrm{max}}=\beta_T  \cdot T$ and $\bar{\beta}_{\textrm{min}}=\beta_1\cdot T$. For example, on CIFAR-10, $\bar{\beta}_{\textrm{min}}=1e-1$ and $\bar{\beta}_{\textrm{max}}=20$ with $T=1000$. We would like to note that the $t_i$s in the sampling algorithms~(Alg~\ref{algorithm-ddpm-sample} and Alg~\ref{algorithm-ddpm-pfgmpp-sample}) monotonically decrease from $1$ to $0$ as $i$ increases.

\begin{minipage}{0.46\textwidth}
\vspace{-69pt}
\begin{algorithm}[H]
    \centering
    \caption{DDPM training}\label{algorithm-ddpm}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data $\{\rvy_i\}_{i=1}^\gB$ from $p(\rvy)$
        \STATE Sample time $\{t_i{=}t_i'/T\}_{i=1}^\gB$ with $t_i'{\sim} \gU(\{1, \dots, T\})$ 
        \STATE Get perturbed data $\{\hat{\rvy}_i = \sqrt{\alpha_{t_i}}\rvy_i +\sqrt{1-\alpha_{t_i}}\bm{\epsilon}_i\}_{i=1}^\gB$,
        where $\bm{\epsilon}_i \sim \gN(\bf{0}, \mI)$
        \STATE Calculate loss $\ell(\theta) =  \sum_{i= 1}^\gB \lambda (t_i)\|f_\theta(\hat{\rvy}_i, t_i)-\bm{\epsilon}_i\|_2^2$
        \STATE Update the network parameter $\theta$ via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ training with hyperparameter transferred from DDPM}\label{algorithm-ddpm-pfgmpp}
    \begin{algorithmic}[1]
        \STATE Sample a batch of data $\{\rvy_i\}_{i=1}^\gB$ from $p(\rvy)$
       \STATE Sample time $\{t_i\}_{i=1}^\gB$ from $\gU[0,1]$
       \STATE \textcolor{blue}{Get $\sigma_{i}$ from $t_i$: $\{\sigma_{i}=\sqrt{\frac{1-\alpha_{t_i}}{\alpha_{t_i}}}\}$}
        \STATE \textcolor{blue}{Sample $r$ from $p_r$: $\{r_i = \sigma_i\sqrt{D}\}_{i=1}^\gB$}
        \STATE  \textcolor{blue}{Sample radiuses $\{R_i \sim p_{r_i}(R)\}_{i=1}^\gB$}
        \STATE  \textcolor{blue}{Sample uniform angles $\{\rvv_i =\frac{\rvu_i}{\|\rvu_i\|_2}\}_{i=1}^\gB$, with $\rvu_i \sim \gN(\bf{0}, \mI)$}
        \STATE \textcolor{blue}{Get perturbed data $\{\hat{\rvy}_i = \sqrt{\alpha_{t_i}}(\rvy_i + R_i\rvv_i)\}_{i=1}^\gB$}
        \STATE \textcolor{blue}{Calculate loss $\ell(\theta) =  \sum_{i=1}^\gB \lambda (t_i)\|f_\theta(\hat{\rvy}_i, t_i)-\frac{\sqrt{D}R_i\rvv_i}{r}\|_2^2$}
        \STATE Update the network parameter $\theta$ via Adam optimizer
    \end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{minipage}{0.46\textwidth}
\vspace{-44pt}
\begin{algorithm}[H]
    \centering
    \caption{DDIM sampling}\label{algorithm-ddpm-sample}
    \begin{algorithmic}[1]
\STATE $\rvx_T \sim \gN(\bm{0}, \mI)$
\FOR{$i=T, \dots, 1$}
\STATE $\rvx_{i-1} =\sqrt{\frac{\alpha_{t_{i-1}}}{\alpha_{t_i}}}\rvx_i $\\
$ \qquad +(\sqrt{1-\alpha_{t_{i-1}}}{-}\sqrt{\frac{\alpha_{t_{i-1}}}{\alpha_{t_i}}}\sqrt{1-\alpha_{t_i}})f_{\theta}(\rvx_i, t_i)$
\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.50\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{PFGM++ sampling transferred from DDIM}\label{algorithm-ddpm-pfgmpp-sample}
    \begin{algorithmic}[1]
     \STATE \textcolor{blue}{Set $\sigma_{\textrm{max}}=\sqrt{\frac{1-\alpha_{1}}{\alpha_{1}}}, \rmax = \sigma_{\textrm{max}}\sqrt{D}$}
        \STATE  \textcolor{blue}{Sample radius $R \sim p_{\rmax}(R)$
       and  uniform angle $\rvv =\frac{\rvu}{\|\rvu\|_2}$, with $\rvu \sim \gN(\bf{0}, \mI)$}
        \STATE \textcolor{blue}{Get initial data $\rvx_T = \sqrt{\alpha_1}R\rvv$}
\FOR{$i=T, \dots, 1$}
\STATE $\rvx_{i-1} =\sqrt{\frac{\alpha_{t_{i-1}}}{\alpha_{t_i}}}\rvx_i $\\
$ \qquad +(\sqrt{1-\alpha_{t_{i-1}}}{-}\sqrt{\frac{\alpha_{t_{i-1}}}{\alpha_{t_i}}}\sqrt{1-\alpha_{t_i}})f_{\theta}(\rvx_i, t_i)$
\ENDFOR
    \end{algorithmic}
\end{algorithm}
\end{minipage}






\section{Experimental Details}
\label{app:exp}
We show the experimental setups in section~\ref{sec:benefits}, as well as the training, sampling, and evaluation details for PFGM++. All the experiments are run on four NVIDIA A100 GPUs or eight NVIDIA V100 GPUs.


\subsection{Experiments for the Analysis in Sec~\ref{sec:benefits}}
\label{app:be-exp}
In the experiments of section~\ref{sec:diffusion} and section~\ref{sec:behavior}, we need to access the posterior $p_{0|r}(\rvy|\rvx)\propto p_r(\rvx|\rvy)p(\rvy)$ to calculate the mean TVD. We sample a large batch $\{\rvy_i\}_{i=1}^n$ with $n=1024$ on CIFAR-10 to empirically approximate the posterior:
\begin{align*}
    p_{0|r}(\rvy_i|\rvx) = \frac{p_r(\rvx|\rvy_i)p(\rvy_i)}{p_r(\rvx)}\approx \frac{p_r(\rvx|\rvy_i)}{\sum_{j=1}^n p_r(\rvx|\rvy_j)}=\frac{{1}/{({\|\rvx-{\rvy_i}\|_2^2+ r^2})^\frac{N+D}{2}}}{\sum_{j=1}^n {1}/{({\|\rvx-{\rvy_j}\|_2^2+ r^2})^\frac{N+D}{2}}}
\end{align*}

We sample a large batch of $256$ to approximate all the expectations in section~\ref{sec:benefits}, such as the average TVDs.
\subsection{Training Details}

We borrow the architectures, preconditioning techniques, optimizers, exponential moving average~(EMA) schedule, and hyper-parameters from previous state-of-the-art diffusion model EDM~\cite{Karras2022ElucidatingTD}. We apply the alignment method in section~\ref{sec:diffusion} to transfer their well-tuned hyper-parameters. 

For architecture, we use the improved NCSN++~\cite{Karras2022ElucidatingTD} for the CIFAR-10 dataset~(batch size $512$), and the improved DDPM++ for the FFHQ dataset~(batch size $256$). For optimizers, following EDM, we adopt the Adam optimizer with a learning rate of $10e-4$. We further incorporate the EMA schedule, learning rate warm-up, and data augmentations in EDM. Please refer to Appendix F in EDM paper~\cite{Karras2022ElucidatingTD} for details.

The most prominent improvements in EDM are the preconditioning and the new training distribution for $\sigma$, \ie $p(\sigma)$. Specifically, adding these two techniques to the vanilla diffusion objective~(\Eqref{eq:diffusion-obj}), their effective training objective can be written as:
\begin{align*}
    \E_{\sigma \sim p(\sigma)}\lambda(\sigma)c_{\textrm{out}}(\sigma)^2\E_{p({\rvy})}\E_{p_{\sigma}({\rvx}|{\rvy})}\left[\Big\lVert F_{\theta}({c_{\textrm{in}}(\sigma) \cdot \rvx}, c_{\textrm{noise}}(\sigma))- \frac{1}{c_{\textrm{out}}(\sigma)}(\rvy-c_{\textrm{skip}}(\sigma)\cdot \rvx)\Big\rVert_2^2\right] \numberthis \label{eq:edm-obj}
\end{align*}
with the predicted normalized score function in the vanilla diffusion objective~(\Eqref{eq:diffusion-obj}) re-parameterized as 
\begin{align*}
    f_{\theta}(\rvx, \sigma) = \frac{c_{\textrm{skip}}(\sigma)\rvx+c_{\textrm{out}}(\sigma)F_{\theta}({c_{\textrm{in}}(\sigma) \rvx}, c_{\textrm{noise}}(\sigma)) - x}{\sigma} \approx \sigma\nabla_\rvx \log p_\sigma(x)
\end{align*}
$c_{\textrm{in}}(\sigma)=1/\sqrt{\sigma^2+\sigma_{\textrm{data}}^2}, c_{\textrm{out}}(\sigma)=\sigma\cdot\sigma_{\textrm{data}}/\sqrt{\sigma^2+\sigma_{\textrm{data}}^2}, c_{\textrm{skip}}(\sigma)=\sigma_{\textrm{data}}^2/(\sigma^2+\sigma_{\textrm{data}}^2), c_{\textrm{noise}}(\sigma)=\frac14 \ln(\sigma)$, with $\sigma_{\textrm{data}}=0.5$. $\{c_{\textrm{in}}(\sigma),c_{\textrm{out}}(\sigma),c_{\textrm{skip}}(\sigma),c_{\textrm{data}},c_{\textrm{noise}}(\sigma)\}$ are all the hyper-parameters in the preconditioning. The training distribution $p(\sigma)$ is the log-normal distribution with $\ln(\sigma) \sim \gN(-1.2, {1.2}^2)$, and the loss weighting $\lambda(\sigma) = 1/c_{\textrm{out}}(\sigma)^2$. 

Recall that the hyper-parameter alignment rule $r=\sigma\sqrt{D}$ can transfer the hyper-parameter from diffusion models~($D{\to} \infty$) to finite $D$s. Hence we can directly set $\sigma=r/\sqrt{D}$ in those hyper-parameters for preconditioning. In addition, the training distribution $p(r)$ can be derived via the change-of-variable formula, \ie $p(r)=p(\sigma=r/\sqrt{D})/\sqrt{D}$. The final PFGM++ objective after incorporating these techniques into \Eqref{eq:pfgmpp-obj} is:
\begin{align*}
    \E_{r \sim p(r)}\lambda(r/\sqrt{D})c_{\textrm{out}}(r/\sqrt{D})^2\E_{p({\rvy})}\E_{p_{r}({\rvx}|{\rvy})}\left[\Big\lVert F_{\theta}({c_{\textrm{in}}(r/\sqrt{D}) \cdot \rvx}, c_{\textrm{noise}}(r/\sqrt{D}))- \frac{1}{c_{\textrm{out}}(\sigma)}(\rvy-c_{\textrm{skip}}(r/\sqrt{D})\cdot \rvx)\Big\rVert_2^2\right]
\end{align*}
with the predicted normalized electric field in the vanilla PFGM++ objective~(\Eqref{eq:pfgmpp-obj}) re-parameterized as 
\begin{align*}
    f_{\theta}(\tx) = \frac{c_{\textrm{skip}}(r/\sqrt{D})\rvx+c_{\textrm{out}}(r/\sqrt{D})F_{\theta}({c_{\textrm{in}}(r/\sqrt{D}) \rvx}, c_{\textrm{noise}}(r/\sqrt{D})) - x}{r/\sqrt{D}} \approx {\sqrt{D}}\frac{\mat{E}(\tilde{\mat{x}})_{\rvx}}{E(\tilde{\mat{x}})_{r}}
\end{align*}
\subsection{Sampling Details}

For sampling, following EDM~\cite{Karras2022ElucidatingTD}, we also use Heun's $2^{\textrm{nd}}$ method~(improved Euler method)~\cite{Ascher1998ComputerMF} as the ODE solver for $\mathrm{d}\rvx /\mathrm{d}r = \mat{E}(\tilde{\mat{x}})_\rvx /E(\tilde{\mat{x}})_{r}= f_\theta(\tx) / \sqrt{D}$.

We adopt the same parameterized scheme in EDM to determine the evaluation points during $N$-step ODE sampling:
\begin{align*}
    r_i = ({\rmax}^\frac{1}{\rho} + \frac{i}{N-1}({\rmin}^\frac{1}{\rho}-{\rmax}^\frac{1}{\rho}))^\rho \quad \textrm{and} \quad r_N=0
\end{align*}
where $\rho$ controls the relative density of evaluation points in the near field. We set $\rho=7$ as in EDM, and $\rmax=\sigma_{\textrm{max}}\sqrt{D}=80\sqrt{D}, \rmin=\sigma_{\textrm{min}}\sqrt{D}=0.002\sqrt{D}$~($\sigma_{\textrm{max}},\sigma_{\textrm{min}}$ are the hyper-parameters in EDM, controlling the starting/terminal evaluation points) following the $r=\sigma\sqrt{D}$ alignment rule.
\subsection{Evaluation Details}
\label{app:eval}
For the evaluation, we compute the Fréchet distance between 50000 generated samples and the pre-computed statistics of CIFAR-10 and FFHQ. On CIFAR-10, we follow the evaluation protocol in EDM~\cite{Karras2022ElucidatingTD}, which repeats the generation three times with different seeds for each checkpoint and reports the minimum FID score. However, we observe that the FID score has a large fluctuation across checkpoints, and the minimum FID score of EDM in our re-run experiment does not align with the original results reported in \cite{Karras2022ElucidatingTD}. \Figref{fig:ffhq} shows that the FID score could have a variation of $\pm 0.2$ during the training of a total of 200 million images~\cite{Karras2022ElucidatingTD}. To better evaluate the model performance, Table~\ref{tab:ffhq} reports the average FID over the Top-3 checkpoints instead. In \Figref{fig:ffhq-avg}, we further demonstrate the moving average of the FID score with a window of $10000$K images. It shows that $D=2048$ consistently outperforms other baselines in the same training iterations, in agreement with the results in Table~\ref{tab:ffhq}.

\begin{figure*}
\centering
    \subfigure[w/o moving average]{\includegraphics[width=0.8\textwidth]{img/ffhq.pdf}\label{fig:ffhq}}
        \subfigure[w/ moving average]{\includegraphics[width=0.8\textwidth]{img/ffhq_avg.pdf}\label{fig:ffhq-avg}}
        \caption{FID score in the training course when varying $D$, \textbf{(a)} w/o and \textbf{(b)} w/ moving average.}
\end{figure*}


\subsection{Experiments for Robustness}
\label{app:robust-exp}

\paragraph{Controlled experiments with $\alpha$} In the controlled noise setting, we inject noise into the intermediate point $\rvx_r$ in each of the $35$ ODE steps by $\rvx_r=\rvx_r + \alpha\bm\epsilon_r$ where $\bm\epsilon_r\sim \gN(\bm 0, r/\sqrt{D}\mI)$. Since $p_r$ has roughly the same phase as $p_{\sigma=r/\sqrt{D}}$ in diffusion models, we pick $r/\sqrt{D}$ standard deviation of $\bm \epsilon_r$ when the intermediate step is $r$.

\paragraph{Post-training quantization} In the post-training quantization experiments on CIFAR-10, we quantize the weights of convolutional layers excluding the $32\times 32$ layers, as we empirically observe that these input/output layers are more critical for sample quality.
\section{Extra Experiments}

\subsection{Stable Target Field}
\label{app:stf}

\citet{Xu2023StableTF} propose a Stable Target Field objective for training the diffusion models:
\begin{align*}
    \nabla_{\rvx}\log p_\sigma(\rvx) \approx \E_{\rvy_1 \sim p_{0|t}(\cdot|\rvx)}\E_{\{\rvy_i\}_{i=2}^{n} \sim p^{n-1}}\left[{\sum_{k=1}^n \frac{p_{t|0}(\rvx|\rvy_k)}{\sum_{j} p_{t|0}(\rvx|\rvy_j)} }\nabla_{\rvx}\log p_{t|0}(\rvx|\rvy_k)\right]
\end{align*}
where they sample a large batch of samples $\{\rvy_i\}_{i=2}^n$ from the data distribution to approximate the score function at $\rvx$. They show that the new target can enhance the stability of converged models in different runs/seeds. PFGM++ can be trained in a similar fashion by replacing the target $\frac{{\rvx}-{\rvy}}{{r}/{\sqrt{D}}}$ in perturbation-based objective~(\Eqref{eq:pfgmpp-obj}) with
\begin{align*}
    \frac{1}{{r}/{\sqrt{D}}}\left(\rvx - \E_{p_{0|r(\rvy|\rvx)}}[\rvy]\right) \approx \frac{1}{{r}/{\sqrt{D}}}\left(\rvx - \E_{\rvy_1 \sim p_{0|r}(\cdot|\rvx)}\E_{\{\rvy_i\}_{i=2}^{n} \sim p^{n-1}}\left[{\sum_{k=1}^n \frac{{1}/{({\|\rvx-{\rvy_k}\|_2^2+ r^2})^\frac{N+D}{2}}}{\sum_j{1}/{({\|\rvx-{\rvy_j}\|_2^2+ r^2})^\frac{N+D}{2}}} }\rvy_k\right]\right)
\end{align*}
When $n=1$, the new target reduces to the original target. Similar to \cite{Xu2023StableTF}, one can show that the bias of the new target together with its trace-of-covariance shrinks to zero as we increase the size of the large batch. This new target can alleviate the variations between random seeds. With the new STF-style target, Table~\ref{tab:cifar-stf} shows that when setting $D=3072000\gg N=3072$, the model obtains the same FID score as the diffusion models~(EDM~\cite{Karras2022ElucidatingTD}). It aligns with the theoretical results in Sec~\ref{sec:diffusion}, which states that PFGM++ recover the diffusion model when $D\to \infty$. 
\begin{table}[htbp]
    \small
    \centering
    \caption{FID and NFE on CIFAR-10, using the Stable Target Field~\cite{Xu2023StableTF} in training objective.}
    \begin{tabular}{l c c c}
    \toprule
         &FID $\downarrow$ & NFE $\downarrow$\\
        \midrule
        $D=3072000$  &1.90 & 35\\
        $D\to \infty$~\cite{Karras2022ElucidatingTD}  &1.90 & 35\\
        \bottomrule
    \end{tabular}
    \label{tab:cifar-stf}
\end{table}


\subsection{Extended CIFAR-10 Samples when varying $\alpha$}
\label{app:robust}


To see how the sample quality varies with $\alpha$, we visualize the generative samples of models trained with $D\in \{64, 128, 2048\}$ and $D\to \infty$. We pick $\alpha \in \{0, 0.1, 0.2\}$. \Figref{fig:robust_vis} shows that the smaller $D$s produce better samples compared to larger $D$. Diffusion models~($D\to\infty$) generate noisy images that appear to be out of the data distribution when $\alpha=0.2$, in contrast to the clean images by $D=64, 128$.
\begin{figure*}
    \centering
\subfigure[$D{=}64, \alpha=0$~(FID=1.96)]{\includegraphics[width=0.26\textwidth]{img/D_64_alpha_0.png}}\hfill
\subfigure[$D{=}64, \alpha=0.1$~(FID=1.97)]{\includegraphics[width=0.26\textwidth]{img/D_64_alpha_0.1.png}}\hfill
\subfigure[$D{=}64, \alpha=0.2$~(FID=2.07)]{\includegraphics[width=0.26\textwidth]{img/D_64_alpha_0.2.png}}

\subfigure[$D{=}128, \alpha=0$~(FID=1.92)]{\includegraphics[width=0.26\textwidth]{img/D_128_alpha_0.png}}\hfill
\subfigure[$D{=}128, \alpha=0.1$~(FID=1.95)]{\includegraphics[width=0.26\textwidth]{img/D_128_alpha_0.1.png}}\hfill
\subfigure[$D{=}128, \alpha=0.2$~(FID=2.19)]{\includegraphics[width=0.26\textwidth]{img/D_128_alpha_0.2.png}}

\subfigure[$D{=}2048, \alpha=0$~(FID=1.92)]{\includegraphics[width=0.26\textwidth]{img/D_2048_alpha_0.png}}\hfill
\subfigure[$D{=}2048, \alpha=0.1$~(FID=1.95)]{\includegraphics[width=0.26\textwidth]{img/D_2048_alpha_0.1.png}}\hfill
\subfigure[$D{=}2048, \alpha=0.2$~(FID=2.19)]{\includegraphics[width=0.26\textwidth]{img/D_2048_alpha_0.2.png}}

\subfigure[$D\to\infty, \alpha=0$~(FID=1.98)]{\includegraphics[width=0.26\textwidth]{img/D_inf_alpha_0.png}}\hfill
\subfigure[$D\to\infty, \alpha=0.1$~(FID=9.27)]{\includegraphics[width=0.26\textwidth]{img/D_inf_alpha_0.1.png}}\hfill
\subfigure[$D\to\infty, \alpha=0.2$~(FID=92.41)]{\includegraphics[width=0.26\textwidth]{img/D_inf_alpha_0.2.png}}
    \caption{Generated samples on CIFAR-10 with varied hyper-parameter for noise injection~($\alpha$). Images from top to bottom rows are produced by models trained with $D=64/128/2048/\infty$. We use the same random seeds for finite $D$s during image generation.}
    \label{fig:robust_vis}
\end{figure*}

\subsection{Extended FFHQ Samples}

In \Figref{fig:ffhq-vis}, we provide samples generated by the $D=128$ case and EDM~(the $D\to \infty$ case).
\begin{figure*}
\centering
    \subfigure[$D=128$~(FID=2.43)]{\includegraphics[width=0.45\textwidth]{img/ffhq_D_128.png}}\hfill
    \subfigure[EDM~($D\to \infty$)~(FID=2.53)]{\includegraphics[width=0.45\textwidth]{img/ffhq_edm.png}}
    \caption{Generated images on FFHQ $64\times 64$ dataset, by \textbf{(left)} $D=128$ and \textbf{(right)} EDM~($D\to \infty$).}
    \label{fig:ffhq-vis}
\end{figure*}


\section{Potential Negative Social Impact}
\label{app:impact}

The deep generative model is a burgeoning field and has significant potential for shaping our society. Our work presents a novel family of generative models, the PFGM++, which subsume previous high-performing models and provide greater flexibility. The PFGM++ have many potential applications, particularly in areas that require both robustness and high-quality output. However, it is important to note that the usage of these models can have both positive and negative implications, depending on the specific application. For instance, the PFGM++ can be used to create realistic image and audio samples, but it can also contribute to the development of deepfake technology and potentially lead to social scams. Additionally, the data-collecting process for generative models may infringe upon intellectual property rights. To address these concerns, further research is needed to provide robustness guarantees for generative models and to foster collaborations with experts in socio-technical fields.
